URL: ftp://ftp.cs.columbia.edu/reports/reports-1997/cucs-005-97.ps.gz
Refering-URL: http://www.cs.columbia.edu/~library/1997.html
Root-URL: http://www.cs.columbia.edu
Email: radev@cs.columbia.edu  
Title: Generating Natural Language Summaries from Multiple On-Line Sources  
Author: Dragomir R. Radev 
Degree: Ph.D. Thesis Proposal  
Date: March 28, 1997  
Address: New York, NY 10027  
Affiliation: Department of Computer Science Columbia University  
Pubnum: Technical Report CUCS-005-97  
Abstract-found: 0
Intro-found: 1
Reference: [Aberdeen et al., 1992] <author> Aberdeen, J., Burger, J., Connolly, D., Roberts, S., and Vilain, M. </author> <year> (1992). </year> <title> MITRE-Bedford: Description of the ALEMBIC system as used for MUC-4. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference (MUC-4), </booktitle> <pages> pages 215-222, </pages> <address> McLean, Virginia. </address>
Reference-contexts: the use of regular grammars to delimit and identify proper nouns [Mani et al., 1993, Paik et al., 1994], the use of extensive name lists, place names, titles and "gazetteers" in conjunction with partial grammars in order to recognize proper nouns as unknown words in close proximity to known words <ref> [Cowie et al., 1992, Aberdeen et al., 1992] </ref>, statistical training to learn, for example, Spanish names, from online corpora [Ayuso et al., 1992], and the use of concept based pattern matchers that use semantic concepts as pattern categories as well as part-of-speech information [Weischedel et al., 1993, Lehnert et al., 1993].
Reference: [Aho et al., 1997] <author> Aho, A., Chang, S.-F., McKeown, K., Radev, D., Smith, J., and Zaman, K. </author> <year> (1997). </year> <title> Columbia digital news system : An environment for briefing and search over multimedia information. </title> <booktitle> In Proceedings of ADL, </booktitle> <address> Washington, DC. </address>
Reference-contexts: Researchers in our department are currently working on an event tracking language <ref> [Aho et al., 1997] </ref>. It uses pattern-matching techniques to track changes to on-line news sources and provide a live feed of articles that relate to a changing event to the summarizer. <p> We will investigate using co-occurrence information 6.2. PLANNED WORK AND PROPOSED EVALUATION 35 to match acronyms to full organization names and alternative spellings of the same name with each other. As a final twist to the integration of non-textual sources, we will apply some ongoing work on image classification <ref> [Aho et al., 1997, Smith and Chang, 1996] </ref> to the problem of generating illustrated summaries.
Reference: [AltaVista, 1996] <institution> AltaVista (1996). Altavista. </institution> <note> WWW site, URL: http:// altavista.digital.com. </note>
Reference-contexts: Hence, there is a need for search and selection services, as well as for summarization facilities made available to the user. There currently exist more than 40 search and selection services on the World-Wide Web, such as DEC's AltaVista <ref> [AltaVista, 1996] </ref>, Lycos [Lycos, 1996], and DejaNews [DejaNews, 1997], all of which allow keyword searches for recent news. However, only recently have there been practical results in the area of summarization.
Reference: [Ayuso et al., 1992] <author> Ayuso, D., Boisen, S., Fox, H., Gish, H., Ingria, R., and Weischedel, R. </author> <year> (1992). </year> <title> BBN: Description of the plum system as used for MUC-4. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference (MUC-4), </booktitle> <pages> pages 169-176, </pages> <address> McLean, Virginia. </address>
Reference-contexts: the use of extensive name lists, place names, titles and "gazetteers" in conjunction with partial grammars in order to recognize proper nouns as unknown words in close proximity to known words [Cowie et al., 1992, Aberdeen et al., 1992], statistical training to learn, for example, Spanish names, from online corpora <ref> [Ayuso et al., 1992] </ref>, and the use of concept based pattern matchers that use semantic concepts as pattern categories as well as part-of-speech information [Weischedel et al., 1993, Lehnert et al., 1993]. In addition, some researchers have explored the use of both local context surrounding the hypothesized proper nouns 2.5.
Reference: [Berners-Lee, 1992] <author> Berners-Lee, T. </author> <year> (1992). </year> <title> World-Wide Web: The information universe. </title> <journal> Electronic Networking, </journal> <volume> 2(1) </volume> <pages> 52-58. </pages>
Reference-contexts: Introduction One of the major problems with the Internet is the abundance of information and the difficulty for the average computer user to read everything existing on a specific topic. There exist now more than 100 operational sources of live newswire on the Internet, mostly accessible through the World-Wide Web <ref> [Berners-Lee, 1992] </ref>. Some of the most popular sites include Reuters News [Reuters, 1996], CNN's Web site [CNN, 1996], ClariNet's e.News on-line newspaper [ClariNet, 1996], as well as the New York Times online edition [NYT, 1996].
Reference: [Bourbeau et al., 1990] <author> Bourbeau, L., Carcagno, D., Goldberg, E., Kittredge, R., and Polguere, A. </author> <year> (1990). </year> <title> Bilingual generation of weather forecasts in an operations environment. </title> <editor> In Karlgren, H., editor, </editor> <booktitle> Proceedings of the 13th International Conference on Computational Linguistics (COLING-90), </booktitle> <volume> volume 3, </volume> <pages> pages 318-320, </pages> <address> Helsinki, Finland. </address>
Reference-contexts: Among the most well-known systems, ana [Kukich, 1983], semtex [Rosner, 1987], fog <ref> [Bourbeau et al., 1990] </ref>, and lfs [Iordanskaja et al., 1994] need to be mentioned. All of them are domain-specific and their domains range from weather forecasts (fog) to stock-market reports (ana).
Reference: [Brandow et al., 1990] <author> Brandow, R., Mitze, K., and Rau, L. F. </author> <year> (1990). </year> <title> Automatic condensation of electronic publications by sentence selection. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 26 </volume> <pages> 135-170. </pages>
Reference-contexts: In more recent work, Kupiec's system is known for performing better than the baseline approach. Summaries that consist of sentences plucked from texts have been shown to be useful indicators of content, but they are also highly unreadable, often judged to readable at between 5% and 27% <ref> [Brandow et al., 1990] </ref>. The sentence-extraction approach has the problem that it uses sentences that were not intended to be used alone as a replacement for the text. Note, in any case, that these approaches cannot handle 7 8 CHAPTER 2.
Reference: [Church, 1988] <author> Church, K. W. </author> <year> (1988). </year> <title> A stochastic parts program and noun phrase parser for unrestricted text. </title> <booktitle> In Proceedings of the Second Conference on Applied Natural Language Processing (ANLP-88), </booktitle> <pages> pages 136-143, </pages> <institution> Austin, Texas. Association for Computational Linguistics. </institution>
Reference-contexts: Only when a suitable stored description cannot be found will the system initiate search of additional text. * Extraction of candidates for proper nouns. After tagging the corpus using the POS part-of-speech tagger <ref> [Church, 1988] </ref>, we used a CREP [Duford, 1993] regular grammar to first extract all possible candidates for entities. These consist of all sequences of words that were tagged as proper nouns (NP) by POS.
Reference: [ClariNet, 1996] <institution> ClariNet (1996). ClariNet. </institution> <note> WWW site, URL: http:// www.clari.net. </note>
Reference-contexts: There exist now more than 100 operational sources of live newswire on the Internet, mostly accessible through the World-Wide Web [Berners-Lee, 1992]. Some of the most popular sites include Reuters News [Reuters, 1996], CNN's Web site [CNN, 1996], ClariNet's e.News on-line newspaper <ref> [ClariNet, 1996] </ref>, as well as the New York Times online edition [NYT, 1996]. For the typical user, it is nearly impossible to go through megabytes of news every day to select articles he wishes to read.
Reference: [CNN, 1996] <institution> CNN (1996). CNN interactive. </institution> <note> WWW site, URL: http:// www.cnn.com. </note>
Reference-contexts: There exist now more than 100 operational sources of live newswire on the Internet, mostly accessible through the World-Wide Web [Berners-Lee, 1992]. Some of the most popular sites include Reuters News [Reuters, 1996], CNN's Web site <ref> [CNN, 1996] </ref>, ClariNet's e.News on-line newspaper [ClariNet, 1996], as well as the New York Times online edition [NYT, 1996]. For the typical user, it is nearly impossible to go through megabytes of news every day to select articles he wishes to read.
Reference: [Coates-Stephens, 1991] <author> Coates-Stephens, S. </author> <year> (1991). </year> <title> Automatic lexical acquisition using within-text descriptions of proper nouns. </title> <booktitle> In Proceedings of the Seventh Annual Conference of the UW Centre for the New OED and Text Research, </booktitle> <pages> pages 154-169. 45 46 BIBLIOGRAPHY </pages>
Reference-contexts: In addition, some researchers have explored the use of both local context surrounding the hypothesized proper nouns 2.5. ARCHITECTURES FOR INTELLIGENT INFORMATION PROCESSING SYSTEMS 9 <ref> [McDonald, 1993, Coates-Stephens, 1991] </ref> and the larger discourse context [Mani et al., 1993] to improve the accuracy of proper noun extraction when large known word lists are not available. Like this research, our work also aims at extracting proper nouns without the aid of large word lists.
Reference: [Cowie et al., 1992] <author> Cowie, J., Guthrie, L., Wilks, Y., Pustejovsky, J., and Waterman, S. </author> <year> (1992). </year> <title> CRL/NMSU and Brandeis: Description of the mucbruce system as used for MUC-4. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference (MUC-4), </booktitle> <pages> pages 223-232, </pages> <address> McLean, Virginia. </address>
Reference-contexts: the use of regular grammars to delimit and identify proper nouns [Mani et al., 1993, Paik et al., 1994], the use of extensive name lists, place names, titles and "gazetteers" in conjunction with partial grammars in order to recognize proper nouns as unknown words in close proximity to known words <ref> [Cowie et al., 1992, Aberdeen et al., 1992] </ref>, statistical training to learn, for example, Spanish names, from online corpora [Ayuso et al., 1992], and the use of concept based pattern matchers that use semantic concepts as pattern categories as well as part-of-speech information [Weischedel et al., 1993, Lehnert et al., 1993].
Reference: [Cuts, 1994] <author> Cuts, S. </author> <year> (1994). </year> <journal> Science and technology section. Economist, </journal> <volume> 17 </volume> <pages> 85-86. </pages>
Reference-contexts: However, only recently have there been practical results in the area of summarization. One currently existing Web-based summarization system, NetSumm, developed by the Language Group at British Telecom Laboratories <ref> [Preston and Williams, 1994, Cuts, 1994, NetSumm, 1996] </ref>, uses a statistical, language-independent approach to selecting relevant sentences from a news article.
Reference: [Dalianis and Hovy, 1993] <author> Dalianis, H. and Hovy, E. </author> <year> (1993). </year> <title> Aggregation in natural language generation. </title> <booktitle> Proceedings of the 4th European Workshop on Natural Language Generation. </booktitle>
Reference-contexts: In ongoing work at Carnegie Mellon University, Carbonell (1996, personal communication) is developing statistical techniques to identify similar sentences and phrases across articles and unlike other statistical approaches, this shows promise for summarization across multiple articles. In related work, Dalianis and Hovy <ref> [Dalianis and Hovy, 1993] </ref> have also looked at the problem of summarization, identifying eight aggregation operators (e.g., conjunction around noun phrases) that apply during generation to create more concise text. Recently, text summarizers have been announced by both Microsoft and Apple Computer.
Reference: [DejaNews, 1997] <institution> DejaNews (1997). Dejanews. </institution> <note> WWW site, URL: http:// www.dejanews.com. </note>
Reference-contexts: Hence, there is a need for search and selection services, as well as for summarization facilities made available to the user. There currently exist more than 40 search and selection services on the World-Wide Web, such as DEC's AltaVista [AltaVista, 1996], Lycos [Lycos, 1996], and DejaNews <ref> [DejaNews, 1997] </ref>, all of which allow keyword searches for recent news. However, only recently have there been practical results in the area of summarization.
Reference: [DeJong, 1979] <author> DeJong, G. </author> <year> (1979). </year> <title> Skimming stories in real time: an experiment in integrated understanding. </title> <type> PhD thesis, </type> <institution> Computer Science Department, Yale University. </institution>
Reference-contexts: The original idea was presented in the 60es [Luhn, 1958] and has been used extensively over the years, especially since the Internet came into wide use. Existing algorithms and systems range from domain-dependent <ref> [DeJong, 1979, Tait, 1983, Jacobs and Rau, 1990] </ref> techniques which make use of specialized knowledge to topic-independent such as NetSumm [Preston and Williams, 1994], or Xerox's summarizer [Kupiec et al., 1995].
Reference: [Duford, 1993] <author> Duford, D. </author> <year> (1993). </year> <title> Crep: a regular expression-matching textual corpus tool. </title> <type> Technical Report CUCS-005-93, </type> <institution> Columbia University. </institution>
Reference-contexts: Only when a suitable stored description cannot be found will the system initiate search of additional text. * Extraction of candidates for proper nouns. After tagging the corpus using the POS part-of-speech tagger [Church, 1988], we used a CREP <ref> [Duford, 1993] </ref> regular grammar to first extract all possible candidates for entities. These consist of all sequences of words that were tagged as proper nouns (NP) by POS.
Reference: [Elhadad, 1991] <author> Elhadad, M. </author> <year> (1991). </year> <title> FUF: The universal unifier user manual, version 5.0. </title> <type> Technical Report CUCS-038-91, </type> <institution> Columbia University. </institution>
Reference-contexts: Its linguistic component determines the phrases and surface syntactic form of the summary. The linguistic component consists of: * a lexical chooser, which determines the high level sentence structure of each sentence and the words which realize each semantic role, and * the FUF (Functional Unification Formalism) <ref> [Elhadad, 1991, Elhadad, 1993] </ref> sentence generator, which uses a large systemic grammar of English, called SURGE 3 to fill in syntactic constraints, build a syntactic tree, choose closed class words, and eventually linearize the tree as a sentence.
Reference: [Elhadad, 1993] <author> Elhadad, M. </author> <year> (1993). </year> <title> Using argumentation to control lexical choice: a unification-based implementation. </title> <type> PhD thesis, </type> <institution> Computer Science Department, Columbia University. </institution>
Reference-contexts: The content planner also receives input from the CIA World Factbook and possible descriptions of people or organizations to augment the base summary. The full content will be sent to a sentence generator, implemented using the FUF/SURGE language generation system <ref> [Elhadad, 1993, Robin, 1994] </ref>. The right side of the figure shows how proper nouns and their descriptions are extracted from past news. An entity extractor identifies proper nouns in the past newswire archives, along with descriptions. <p> Its linguistic component determines the phrases and surface syntactic form of the summary. The linguistic component consists of: * a lexical chooser, which determines the high level sentence structure of each sentence and the words which realize each semantic role, and * the FUF (Functional Unification Formalism) <ref> [Elhadad, 1991, Elhadad, 1993] </ref> sentence generator, which uses a large systemic grammar of English, called SURGE 3 to fill in syntactic constraints, build a syntactic tree, choose closed class words, and eventually linearize the tree as a sentence. <p> In particular, we used FUF to implement the lexical chooser, representing the lexicon as a grammar as we have done in many previous systems (e.g., Elhadad <ref> [Elhadad, 1993] </ref>, Robin [Robin, 1994], McKeown et al. [McKeown et al., 1993], Feiner and McKeown [Feiner and McKeown, 1991]), and thus the main effort was in identifying the words and phrases needed for the domain. The content planner features several stages, as does the PLANDoc system. <p> The content planner features several stages, as does the PLANDoc system. It first groups messages together, identifies commonalities between them, and notes how the discourse influences wording by setting realization flags. Before lexical choice, SUMMONS maps the templates into the FD <ref> [Elhadad, 1993] </ref> formalism expected as input to FUF and uses a domain ontology (derived from the ontologies represented in the message understanding systems) to enrich the input. 4 primary source usually a direct witness of the event, and secondary source most often a press agency or journalist, reporting the event. 4.2. <p> We attempt to identify a preeminent set of templates from the input to the system. This set needs to contain a large number of similar fields. If this holds, we can merge the set into a simpler structure, keeping the common features and marking the distinct features as Elhadad <ref> [Elhadad, 1993] </ref> and McKeown [McKeown et al., 1994a] suggest. At each step, a summary operator is selected based on existing similarities between messages in the database. This operator is then applied to the input templates, resulting in a new template which combines, or synthesizes, information from the old.
Reference: [Etzioni and Weld, 1994] <author> Etzioni, O. and Weld, D. </author> <year> (1994). </year> <title> A softbot-based interface to the internet. </title> <journal> Communications of the ACM, </journal> <volume> 37(7) </volume> <pages> 72-76. </pages>
Reference-contexts: is extracted may be merged, or regenerated, as part of a larger textual summary. 2.5 Architectures for Intelligent Information Processing Systems Some ideas in the overall architectural design of our system are derived from the concepts set forth by Genesereth and Ketchpel [Genesereth and Ketchpel, 1994] and Etzioni and Weld <ref> [Etzioni and Weld, 1994] </ref> and the work on the standardization of protocols and languages for agent communication such as KQML and KIF [Finin et al., 1994, Genesereth and Fikes, 1992]. The current proposed KQML specification is described in [Labrou, 1996].
Reference: [Feiner and McKeown, 1991] <author> Feiner, S. and McKeown, K. </author> <year> (1991). </year> <title> Automating the generation of coordinated multimedia explanations. </title> <journal> IEEE Computer, </journal> <volume> 24(10) </volume> <pages> 33-41. </pages>
Reference-contexts: In particular, we used FUF to implement the lexical chooser, representing the lexicon as a grammar as we have done in many previous systems (e.g., Elhadad [Elhadad, 1993], Robin [Robin, 1994], McKeown et al. [McKeown et al., 1993], Feiner and McKeown <ref> [Feiner and McKeown, 1991] </ref>), and thus the main effort was in identifying the words and phrases needed for the domain. The content planner features several stages, as does the PLANDoc system.
Reference: [Finin et al., 1994] <author> Finin, T., Fritzson, R., McKay, D., and McEntire, R. </author> <year> (1994). </year> <title> KQML a language and protocol for knowledge and information exchange. </title> <type> Technical Report CS-94-02, </type> <institution> Computer Science Department, University of Maryland and Valley Forge Engineering Center, Unisys Corporation. </institution>
Reference-contexts: ideas in the overall architectural design of our system are derived from the concepts set forth by Genesereth and Ketchpel [Genesereth and Ketchpel, 1994] and Etzioni and Weld [Etzioni and Weld, 1994] and the work on the standardization of protocols and languages for agent communication such as KQML and KIF <ref> [Finin et al., 1994, Genesereth and Fikes, 1992] </ref>. The current proposed KQML specification is described in [Labrou, 1996]. KQML (Knowledge Query and Manipulation Language) is a language and protocol for exchanging information and knowledge and is part of the ARPA Knowledge Sharing Effort [Patil et al., 1992].
Reference: [Genesereth and Fikes, 1992] <author> Genesereth, M. and Fikes, R. </author> <year> (1992). </year> <title> Knowledge interchange format, version 3.0 : Reference manual. </title> <type> Technical Report Logic-92-1, </type> <institution> Computer Science Department, Stanford University, Stanford, California. </institution>
Reference-contexts: ideas in the overall architectural design of our system are derived from the concepts set forth by Genesereth and Ketchpel [Genesereth and Ketchpel, 1994] and Etzioni and Weld [Etzioni and Weld, 1994] and the work on the standardization of protocols and languages for agent communication such as KQML and KIF <ref> [Finin et al., 1994, Genesereth and Fikes, 1992] </ref>. The current proposed KQML specification is described in [Labrou, 1996]. KQML (Knowledge Query and Manipulation Language) is a language and protocol for exchanging information and knowledge and is part of the ARPA Knowledge Sharing Effort [Patil et al., 1992].
Reference: [Genesereth and Ketchpel, 1994] <author> Genesereth, M. and Ketchpel, S. </author> <year> (1994). </year> <title> Software agents. </title> <journal> Communications of the ACM, </journal> <volume> 37(7) </volume> <pages> 48-53. </pages>
Reference-contexts: In our work, the string that is extracted may be merged, or regenerated, as part of a larger textual summary. 2.5 Architectures for Intelligent Information Processing Systems Some ideas in the overall architectural design of our system are derived from the concepts set forth by Genesereth and Ketchpel <ref> [Genesereth and Ketchpel, 1994] </ref> and Etzioni and Weld [Etzioni and Weld, 1994] and the work on the standardization of protocols and languages for agent communication such as KQML and KIF [Finin et al., 1994, Genesereth and Fikes, 1992]. The current proposed KQML specification is described in [Labrou, 1996]. <p> RELATED WORK All of these systems take data as input. The focus for these systems has been on linguistic summariza tion. SUMMONS, on the other hand, focuses on conceptual summarization. Chapter 3 System overview The overall architecture of our summarization system (Figure 3.1) draws on research in software agents <ref> [Genesereth and Ketchpel, 1994] </ref> to allow connections to a variety of different types of data sources. Facilities are used to provide a transparent interface to heterogeneous data sources which run on various machines and may be written in different programming languages.
Reference: [Grishman et al., 1992] <author> Grishman, R., Macleod, C., and Sterling, J. </author> <year> (1992). </year> <title> New York University: Description of the proteus system as used for muc-4. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference. </booktitle>
Reference-contexts: GENERATING THE BASE SUMMARY <ref> [Grishman et al., 1992] </ref> were also restricted to such topics only.
Reference: [Hovy, 1988] <author> Hovy, E. </author> <year> (1988). </year> <title> Planning coherent multisentential text. </title> <booktitle> In Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics, </booktitle> <address> Buffalo, N.Y. </address> <note> Association for Computational Linguistics. BIBLIOGRAPHY 47 </note>
Reference-contexts: We provide examples of the summarization markers we collected for the lexicon and show the demands that summarization creates for interpretation. 4.1 Overview of the Summarization Component The summarization component of SUMMONS is based on the traditional language generation system architecture <ref> [McKeown, 1985, McDonald and Pustejovsky, 1986, Hovy, 1988] </ref>.
Reference: [Iordanskaja et al., 1994] <author> Iordanskaja, L., Kim, M., Kittredge, R., Lavoie, B., and Polguere, A. </author> <year> (1994). </year> <title> Generation of extended bilingual statistical reports. </title> <booktitle> In Proceedings of the 15th International Conference on Computational Linguistics (COLING-94), </booktitle> <address> Kyoto, Japan. </address>
Reference-contexts: Among the most well-known systems, ana [Kukich, 1983], semtex [Rosner, 1987], fog [Bourbeau et al., 1990], and lfs <ref> [Iordanskaja et al., 1994] </ref> need to be mentioned. All of them are domain-specific and their domains range from weather forecasts (fog) to stock-market reports (ana).
Reference: [Jacobs and Rau, 1990] <author> Jacobs, P. and Rau, L. </author> <year> (1990). </year> <title> SCISOR: Extracting information from on-line news. </title> <journal> Communications of the ACM, </journal> <volume> 33(11) </volume> <pages> 88-97. </pages>
Reference-contexts: The original idea was presented in the 60es [Luhn, 1958] and has been used extensively over the years, especially since the Internet came into wide use. Existing algorithms and systems range from domain-dependent <ref> [DeJong, 1979, Tait, 1983, Jacobs and Rau, 1990] </ref> techniques which make use of specialized knowledge to topic-independent such as NetSumm [Preston and Williams, 1994], or Xerox's summarizer [Kupiec et al., 1995].
Reference: [Jing et al., 1997] <author> Jing, H., Hatzivassiloglou, V., Passonneau, R., and McKeown, K. </author> <year> (1997). </year> <title> Investigating complementary methods for verb sense pruning. </title> <booktitle> In Proceedings of the 5th Conference on Applied Natural Language Processing, </booktitle> <address> Washington, DC. </address>
Reference-contexts: Possibly, make use of such algorithms being developed by other research groups in the department. 6.3.2 Lexicon and Language Generation * Assess the feasibility of using a large-scale generation lexicon under development by other mem bers of our group <ref> [Jing et al., 1997] </ref>. * (*) Enhance the sentence-level generation grammar. The goal would be to cover 50% of all possible values for each of the different slots of the MUC templates. * (*) Enhance the paragraph-level generation grammar.
Reference: [Kukich, 1983] <author> Kukich, K. K. </author> <year> (1983). </year> <title> Design of a knowledge-based report generator. </title> <booktitle> In Proceedings of the 21st Annual Meeting of the ACL, </booktitle> <pages> pages 145-150, </pages> <address> Cambridge, </address> <institution> Massachusetts. Association for Computational Linguistics. </institution>
Reference-contexts: Among the most well-known systems, ana <ref> [Kukich, 1983] </ref>, semtex [Rosner, 1987], fog [Bourbeau et al., 1990], and lfs [Iordanskaja et al., 1994] need to be mentioned. All of them are domain-specific and their domains range from weather forecasts (fog) to stock-market reports (ana).
Reference: [Kupiec, 1993] <author> Kupiec, J. M. </author> <year> (1993). </year> <title> Murax: A robust linguistic approach for question answering using an on-line encyclopedia. </title> <booktitle> In Proceedings, 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. </booktitle>
Reference-contexts: We represent the descriptions in a format that facilitates symbolic generation. Another system, called murax <ref> [Kupiec, 1993] </ref>, is similar to ours from a different perspective. mu-rax also extracts information from a text to serve directly in response to a user question. murax uses lexico-syntactic patterns, collocational analysis, along with information retrieval statistics, to find the string of words in a text that is most likely to
Reference: [Kupiec et al., 1995] <author> Kupiec, J. M., Pedersen, J., and Chen, F. </author> <year> (1995). </year> <title> A trainable document summarizer. </title> <booktitle> In Proceedings, 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 68-73, </pages> <address> Seattle, Washington. </address>
Reference-contexts: It has an impressive user interface, and is practically domain-independent, but doesn't address two major issues: it only summarizes articles that the user has selected and it only summarizes a single article at a time. Other statistical systems <ref> [Kupiec et al., 1995] </ref>, [Rau et al., 1994], while using different algorithms for sentence extraction, have similar disadvantages as NetSumm. Another major unsolved problem involves conveying rapidly changing information to the end user in a sensible format. <p> Existing algorithms and systems range from domain-dependent [DeJong, 1979, Tait, 1983, Jacobs and Rau, 1990] techniques which make use of specialized knowledge to topic-independent such as NetSumm [Preston and Williams, 1994], or Xerox's summarizer <ref> [Kupiec et al., 1995] </ref>.
Reference: [Labrou, 1996] <author> Labrou, Y. </author> <year> (1996). </year> <title> Semantics for an Agent Communication Language. </title> <type> PhD thesis, </type> <institution> University of Maryland, Baltimore County. </institution>
Reference-contexts: The current proposed KQML specification is described in <ref> [Labrou, 1996] </ref>. KQML (Knowledge Query and Manipulation Language) is a language and protocol for exchanging information and knowledge and is part of the ARPA Knowledge Sharing Effort [Patil et al., 1992].
Reference: [Lehnert et al., 1993] <author> Lehnert, W., McCarthy, J., Soderlan, S., Riloff, E., Cardie, C., Peterson, J., and Feng, F. </author> <year> (1993). </year> <title> UMass/Hughes: Description of the CIRCUS system used for MUC-5. </title> <booktitle> In Proceedings of the Fifth Message Understanding Conference (MUC-5), </booktitle> <pages> pages 277-291, </pages> <address> Baltimore, Md. </address>
Reference-contexts: words in close proximity to known words [Cowie et al., 1992, Aberdeen et al., 1992], statistical training to learn, for example, Spanish names, from online corpora [Ayuso et al., 1992], and the use of concept based pattern matchers that use semantic concepts as pattern categories as well as part-of-speech information <ref> [Weischedel et al., 1993, Lehnert et al., 1993] </ref>. In addition, some researchers have explored the use of both local context surrounding the hypothesized proper nouns 2.5.
Reference: [Luhn, 1958] <author> Luhn, H. P. </author> <year> (1958). </year> <title> The automatic creation of literature abstracts. </title> <journal> IBM Journal, </journal> <pages> pages 159-165. </pages>
Reference-contexts: Paice [Paice, 1990] has given an overview of the different existing methods for summarization using sentences extracted from the original text. The original idea was presented in the 60es <ref> [Luhn, 1958] </ref> and has been used extensively over the years, especially since the Internet came into wide use.
Reference: [Lycos, 1996] <institution> Lycos (1996). Lycos, Inc. home page. </institution> <note> WWW site, URL: http:// www.lycos.com. </note>
Reference-contexts: Hence, there is a need for search and selection services, as well as for summarization facilities made available to the user. There currently exist more than 40 search and selection services on the World-Wide Web, such as DEC's AltaVista [AltaVista, 1996], Lycos <ref> [Lycos, 1996] </ref>, and DejaNews [DejaNews, 1997], all of which allow keyword searches for recent news. However, only recently have there been practical results in the area of summarization.
Reference: [Mani et al., 1993] <author> Mani, I., Macmillan, R. T., Luperfoy, S., Lusher, E., and Laskowski, S. </author> <year> (1993). </year> <title> Identifying unknown proper names in newswire text. </title> <booktitle> In Proceedings of the Workshop on Acquisition of Lexical Knowledge from Text, </booktitle> <pages> pages 44-54, </pages> <address> Columbus, Ohio. </address> <booktitle> Special Interest Group on the Lexicon of the Association for Computational Linguistics. </booktitle>
Reference-contexts: Techniques for proper noun extraction include the use of regular grammars to delimit and identify proper nouns <ref> [Mani et al., 1993, Paik et al., 1994] </ref>, the use of extensive name lists, place names, titles and "gazetteers" in conjunction with partial grammars in order to recognize proper nouns as unknown words in close proximity to known words [Cowie et al., 1992, Aberdeen et al., 1992], statistical training to learn, <p> In addition, some researchers have explored the use of both local context surrounding the hypothesized proper nouns 2.5. ARCHITECTURES FOR INTELLIGENT INFORMATION PROCESSING SYSTEMS 9 [McDonald, 1993, Coates-Stephens, 1991] and the larger discourse context <ref> [Mani et al., 1993] </ref> to improve the accuracy of proper noun extraction when large known word lists are not available. Like this research, our work also aims at extracting proper nouns without the aid of large word lists.
Reference: [McDonald and Pustejovsky, 1986] <author> McDonald, D. and Pustejovsky, J. </author> <year> (1986). </year> <title> Description-directed natural language generation. </title> <booktitle> In Proceedings of the 9th IJCAI, </booktitle> <pages> pages 799-805. IJCAI. </pages>
Reference-contexts: We provide examples of the summarization markers we collected for the lexicon and show the demands that summarization creates for interpretation. 4.1 Overview of the Summarization Component The summarization component of SUMMONS is based on the traditional language generation system architecture <ref> [McKeown, 1985, McDonald and Pustejovsky, 1986, Hovy, 1988] </ref>.
Reference: [McDonald, 1993] <author> McDonald, D. D. </author> <year> (1993). </year> <title> Internal and external evidence in the identification and semantic cateogrization of proper names. </title> <booktitle> In Proceedings of the Workshop on Acquisition of Lexical Knowledge from Text, </booktitle> <pages> pages 32-43, </pages> <address> Columbus, Ohio. </address> <booktitle> Special Interest Group on the Lexicon of the Association for Computational Linguistics. </booktitle> <address> 48 BIBLIOGRAPHY </address>
Reference-contexts: In addition, some researchers have explored the use of both local context surrounding the hypothesized proper nouns 2.5. ARCHITECTURES FOR INTELLIGENT INFORMATION PROCESSING SYSTEMS 9 <ref> [McDonald, 1993, Coates-Stephens, 1991] </ref> and the larger discourse context [Mani et al., 1993] to improve the accuracy of proper noun extraction when large known word lists are not available. Like this research, our work also aims at extracting proper nouns without the aid of large word lists.
Reference: [McKeown et al., 1994a] <author> McKeown, K., Kukich, K., and Shaw, J. </author> <year> (1994a). </year> <title> Practical issues in automatic documentation generation. </title> <booktitle> In Proceedings of the ACL Applied Natural Language Conference, </booktitle> <address> Stuttgart, Germany. </address>
Reference-contexts: No changes in the FUF sentence generator were needed. In addition, the lexical chooser and content planner were based on the design used in the PLANDoc automated documentation system, developed jointly with Bellcore to summarize the activities of telephone planning engineers <ref> [McKeown et al., 1994a] </ref>. <p> This set needs to contain a large number of similar fields. If this holds, we can merge the set into a simpler structure, keeping the common features and marking the distinct features as Elhadad [Elhadad, 1993] and McKeown <ref> [McKeown et al., 1994a] </ref> suggest. At each step, a summary operator is selected based on existing similarities between messages in the database. This operator is then applied to the input templates, resulting in a new template which combines, or synthesizes, information from the old. <p> It looks at consecutive messages in the database, marked as separate paragraphs from the previous stage, and assigns values to "realization switches" <ref> [McKeown et al., 1994a] </ref> which control local choices such as tense and voice. They also govern the presence or lack of certain constituents to avoid repetition of constituents and to satisfy anaphora constraints. 22 CHAPTER 4.
Reference: [McKeown et al., 1993] <author> McKeown, K., Robin, J., and Tanenblatt, M. </author> <year> (1993). </year> <title> Tailoring lexical choice to the user's vocabulary in multimedia explanation generation. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, </booktitle> <address> Columbus, Oh. </address>
Reference-contexts: In particular, we used FUF to implement the lexical chooser, representing the lexicon as a grammar as we have done in many previous systems (e.g., Elhadad [Elhadad, 1993], Robin [Robin, 1994], McKeown et al. <ref> [McKeown et al., 1993] </ref>, Feiner and McKeown [Feiner and McKeown, 1991]), and thus the main effort was in identifying the words and phrases needed for the domain. The content planner features several stages, as does the PLANDoc system.
Reference: [McKeown, 1985] <author> McKeown, K. R. </author> <year> (1985). </year> <title> Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Language Texts. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, England. </address>
Reference-contexts: We provide examples of the summarization markers we collected for the lexicon and show the demands that summarization creates for interpretation. 4.1 Overview of the Summarization Component The summarization component of SUMMONS is based on the traditional language generation system architecture <ref> [McKeown, 1985, McDonald and Pustejovsky, 1986, Hovy, 1988] </ref>.
Reference: [McKeown et al., 1994b] <author> McKeown, K. R., Kukich, K., and Shaw, J. </author> <year> (1994b). </year> <title> Practical issues in automatic documentation generation. </title> <booktitle> In Proceedings of the 4th Conference on Applied Natural Language Processing, </booktitle> <address> Stuttgart, Germany. </address> <institution> Association for Computational Linguistics. </institution>
Reference-contexts: Independently or in conjunction with Bellcore, the following systems have been developed over the course of the last few years. * STREAK [Robin and McKeown, 1993, Robin, 1994, Robin and McKeown, 1995] generates sum maries of basketball games using the concept of revisions. * PLANDoc <ref> [McKeown et al., 1994b, McKeown et al., 1995, Shaw, 1995] </ref> generates summaries of the activities of telephone planning engineers.
Reference: [McKeown and Radev, 1995] <author> McKeown, K. R. and Radev, D. R. </author> <year> (1995). </year> <title> Generating summaries of multiple news articles. </title> <booktitle> In Proceedings, 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 74-82, </pages> <address> Seattle, Washington. </address>
Reference-contexts: This information can come from a multitude of different sources which use different internal representations to store it. A summarizing program needs to be able to retrieve all this information in real time, process it and produce meaningful summaries in natural language. We present a system, called SUMMONS 1 <ref> [McKeown and Radev, 1995, Radev and McKeown, 1997b] </ref>, which introduces novel techniques in the following areas: * It combines information from multiple news articles into a coherent summary using symbolic 1 SUMMarizing Online NewS articles 3 4 CHAPTER 1.
Reference: [McKeown et al., 1995] <author> McKeown, K. R., Robin, J., and Kukich, K. </author> <year> (1995). </year> <title> Generating concise natural language summaries. </title> <journal> Journal of Information Processing and Management, </journal> <volume> 31(5) </volume> <pages> 703-733. </pages>
Reference-contexts: Independently or in conjunction with Bellcore, the following systems have been developed over the course of the last few years. * STREAK [Robin and McKeown, 1993, Robin, 1994, Robin and McKeown, 1995] generates sum maries of basketball games using the concept of revisions. * PLANDoc <ref> [McKeown et al., 1994b, McKeown et al., 1995, Shaw, 1995] </ref> generates summaries of the activities of telephone planning engineers.
Reference: [Miller et al., 1990] <author> Miller, G. A., Beckwith, R., Fellbaum, C., Gross, D., and Miller, K. J. </author> <year> (1990). </year> <title> Introduction to WordNet: An on-line lexical database. </title> <journal> International Journal of Lexicography (special issue), </journal> <volume> 3(4) </volume> <pages> 235-312. </pages>
Reference-contexts: Like this research, our work also aims at extracting proper nouns without the aid of large word lists. We use a regular grammar encoding part-of-speech categories to extract certain text patterns (descriptions) and we use WordNet <ref> [Miller et al., 1990] </ref> to provide semantic filtering. Our work on extracting descriptions is quite similar to the work carried out under the DARPA message understanding program for extracting descriptions. <p> Table 5.1 shows how many entities we retrieve at this stage, and of them, how many pass the semantic filtering test. * Weeding out of false candidates. Our system analyzed all candidates for entity names using WordNet <ref> [Miller et al., 1990] </ref> and removed from consideration those that contain words appearing in WordNet's dictionary. This resulted in a list of 421 unique entity names that we used for the automatic description extraction stage. All 421 entity names retrieved by the system are indeed proper nouns.
Reference: [MUC, 1992] <author> MUC, M. U. C. </author> <year> (1992). </year> <booktitle> Proceedings of the Fourth Message Understanding Conference (MUC-4). DARPA Software and Intelligent Systems Technology Office. </booktitle>
Reference-contexts: Another system, ZEDDoc, which summarizes Web access logs is described in 2.6. 2.4 Description Extraction As our work also involves generation using extracted descriptions, we also provide a comparison with work on proper noun extraction, extraction of people descriptions in various information extraction systems developed for the message understanding conferences <ref> [MUC, 1992] </ref>, and use of extracted information for question answering. <p> An example of a template produced by MUC systems and used in our system is shown in Figure 4.1. To test our system, we used the templates produced by systems participating in MUC-4 <ref> [MUC, 1992] </ref>, available from the Linguistic Data Consortium (LDC), as input. MUC-4 systems operate on the terrorist domain and extract information by filling fields such as perpetrator, victim, and type of event, for a total number of 25 fields.
Reference: [NetSumm, 1996] <institution> NetSumm (1996). Netsumm home page. </institution> <note> WWW site, URL: http:// www.labs.bt.com/ innovate/informat/netsumm/index.htm. </note>
Reference-contexts: However, only recently have there been practical results in the area of summarization. One currently existing Web-based summarization system, NetSumm, developed by the Language Group at British Telecom Laboratories <ref> [Preston and Williams, 1994, Cuts, 1994, NetSumm, 1996] </ref>, uses a statistical, language-independent approach to selecting relevant sentences from a news article.
Reference: [NYT, 1996] <institution> NYT (1996). </institution> <address> New York Times. </address> <note> WWW site, URL: http:// www.nytimes.com. </note>
Reference-contexts: Some of the most popular sites include Reuters News [Reuters, 1996], CNN's Web site [CNN, 1996], ClariNet's e.News on-line newspaper [ClariNet, 1996], as well as the New York Times online edition <ref> [NYT, 1996] </ref>. For the typical user, it is nearly impossible to go through megabytes of news every day to select articles he wishes to read.
Reference: [Paice, 1990] <author> Paice, C. </author> <year> (1990). </year> <title> Constructing literature abstracts by computer: Techniques and prospects. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 26 </volume> <pages> 171-186. </pages>
Reference-contexts: This chapter will provide some background on the research performed in these five areas and will conclude with a brief overview of prior research on summarization at Columbia. 2.1 Text Summarization All prior work on text summarization has been essentially done using statistical sentence extraction techniques. Paice <ref> [Paice, 1990] </ref> has given an overview of the different existing methods for summarization using sentences extracted from the original text. The original idea was presented in the 60es [Luhn, 1958] and has been used extensively over the years, especially since the Internet came into wide use.
Reference: [Paik et al., 1994] <author> Paik, W., Liddy, E. D., Yu, E., and McKenna, M. </author> <year> (1994). </year> <title> Interpretation of proper nouns for information retrieval. </title> <booktitle> In Proceedings of the Human Language Technology Workshop, </booktitle> <pages> pages 309-313, </pages> <address> Plainsboro, New Jersey. </address> <booktitle> ARPA Software and Intelligent Systems Technology Office, </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Techniques for proper noun extraction include the use of regular grammars to delimit and identify proper nouns <ref> [Mani et al., 1993, Paik et al., 1994] </ref>, the use of extensive name lists, place names, titles and "gazetteers" in conjunction with partial grammars in order to recognize proper nouns as unknown words in close proximity to known words [Cowie et al., 1992, Aberdeen et al., 1992], statistical training to learn,
Reference: [Passonneau et al., 1997] <author> Passonneau, R., Kukich, K., McKeown, K., Radev, D., and Jing, H. </author> <year> (1997). </year> <title> Summarizing web traffic: A portability exercise. </title> <type> Technical Report CUCS-009-97, </type> <institution> Columbia University, Department of Computer Science, </institution> <address> New York, NY, USA. </address>
Reference-contexts: It uses of conjunction, ellipsis and paraphrase to result in concise, yet fluent reports. * ZEDDoc <ref> [Passonneau et al., 1997] </ref> generates Web traffic summaries for advertisement manage ment software. 10 CHAPTER 2. RELATED WORK All of these systems take data as input. The focus for these systems has been on linguistic summariza tion. SUMMONS, on the other hand, focuses on conceptual summarization.
Reference: [Patil et al., 1992] <author> Patil, R. S., Fikes, R. E., Patel-Schneider, P. F., McKay, D., Finin, T., Gruber, T., and Neches, R. </author> <year> (1992). </year> <title> The DARPA knowledge sharing effort: Progress report. </title> <editor> In Rich, C., Swartout, W., and Nebel, B., editors, </editor> <booktitle> Knowledge Representation, </booktitle> <pages> pages 777-788. BIBLIOGRAPHY 49 </pages>
Reference-contexts: The current proposed KQML specification is described in [Labrou, 1996]. KQML (Knowledge Query and Manipulation Language) is a language and protocol for exchanging information and knowledge and is part of the ARPA Knowledge Sharing Effort <ref> [Patil et al., 1992] </ref>. KQML defines an extensible set of performatives which specify the actions that intelligent agents can perform or attempt to perform on themselves and on each other's knowledge bases.
Reference: [Preston and Williams, 1994] <author> Preston, K. and Williams, S. </author> <year> (1994). </year> <title> Managing the information overload. Physics in Business. </title>
Reference-contexts: However, only recently have there been practical results in the area of summarization. One currently existing Web-based summarization system, NetSumm, developed by the Language Group at British Telecom Laboratories <ref> [Preston and Williams, 1994, Cuts, 1994, NetSumm, 1996] </ref>, uses a statistical, language-independent approach to selecting relevant sentences from a news article. <p> Existing algorithms and systems range from domain-dependent [DeJong, 1979, Tait, 1983, Jacobs and Rau, 1990] techniques which make use of specialized knowledge to topic-independent such as NetSumm <ref> [Preston and Williams, 1994] </ref>, or Xerox's summarizer [Kupiec et al., 1995].
Reference: [Radev, 1994] <author> Radev, D. R. </author> <year> (1994). </year> <title> Rendezvous: A WWW synchronization system. </title> <note> Poster Session, Second International WWW Conference. </note>
Reference-contexts: It would be more efficient for the user to be notified automatically when a new article has been published <ref> [Radev, 1994] </ref>, and even better, to be sent a summary of the article directly. Such asynchronous summaries can be based on the specific interests of the user, contained in his user profile or the pending subscription.
Reference: [Radev, 1996] <author> Radev, D. R. </author> <year> (1996). </year> <title> An architecture for distributed natural language summarization. </title> <booktitle> In Proceedings of the 8th International Workshop on Natural Language Generation: Demonstrations and Posters, </booktitle> <pages> pages 45-48, </pages> <address> Herstmonceux, England. </address>
Reference-contexts: INTRODUCTION techniques. * It augments the resulting summaries using descriptions of entities obtained from on-line sources. * It features an agent-based architecture <ref> [Radev, 1996] </ref> that provides access to heterogeneous lexical and conceptual resources.
Reference: [Radev and McKeown, 1997a] <author> Radev, D. R. and McKeown, K. R. </author> <year> (1997a). </year> <title> Building a generation knowledge source using internet-accessible newswire. </title> <booktitle> In Proceedings of the 5th Conference on Applied Natural Language Processing, </booktitle> <address> Washington, DC. </address>
Reference-contexts: If the summarization system can find the needed information in other online sources, then it can produce an improved summary by merging information extracted from the input articles with information from other sources <ref> [Radev and McKeown, 1997a] </ref>. In the news domain, a summary needs to refer to people, places, and organizations and provide descriptions that clearly identify the entity for the reader. Such descriptions may not be present in the original text that is being summarized.
Reference: [Radev and McKeown, 1997b] <author> Radev, D. R. and McKeown, K. R. </author> <year> (1997b). </year> <title> Generating natural language summaries from multiple on-line sources. </title> <note> submitted to Computational Linguistics. </note>
Reference-contexts: This information can come from a multitude of different sources which use different internal representations to store it. A summarizing program needs to be able to retrieve all this information in real time, process it and produce meaningful summaries in natural language. We present a system, called SUMMONS 1 <ref> [McKeown and Radev, 1995, Radev and McKeown, 1997b] </ref>, which introduces novel techniques in the following areas: * It combines information from multiple news articles into a coherent summary using symbolic 1 SUMMarizing Online NewS articles 3 4 CHAPTER 1.
Reference: [Rau et al., 1994] <author> Rau, L., Brandow, R., and Mitze, K. </author> <year> (1994). </year> <title> Domain-Independent summarization of news. </title> <booktitle> In Summarizing Text for Intelligent Communication, </booktitle> <pages> pages 71-75, </pages> <address> Dagstuhl, Germany. </address>
Reference-contexts: It has an impressive user interface, and is practically domain-independent, but doesn't address two major issues: it only summarizes articles that the user has selected and it only summarizes a single article at a time. Other statistical systems [Kupiec et al., 1995], <ref> [Rau et al., 1994] </ref>, while using different algorithms for sentence extraction, have similar disadvantages as NetSumm. Another major unsolved problem involves conveying rapidly changing information to the end user in a sensible format. <p> Rau et al. <ref> [Rau et al., 1994] </ref> report that statistical summaries of individual news articles were rated lower by evaluators than simply using the lead sentence or two from the article. In more recent work, Kupiec's system is known for performing better than the baseline approach.
Reference: [Reuters, 1996] <institution> Reuters (1996). Reuters news. </institution> <note> WWW site, URL: http:// www.yahoo.com/headlines/. </note>
Reference-contexts: There exist now more than 100 operational sources of live newswire on the Internet, mostly accessible through the World-Wide Web [Berners-Lee, 1992]. Some of the most popular sites include Reuters News <ref> [Reuters, 1996] </ref>, CNN's Web site [CNN, 1996], ClariNet's e.News on-line newspaper [ClariNet, 1996], as well as the New York Times online edition [NYT, 1996]. For the typical user, it is nearly impossible to go through megabytes of news every day to select articles he wishes to read.
Reference: [Robin, 1994] <author> Robin, J. </author> <year> (1994). </year> <title> Revision-Based Generation of Natural Language Summaries Providing Historical Background. </title> <type> PhD thesis, </type> <institution> Computer Science Department, Columbia University. </institution>
Reference-contexts: Independently or in conjunction with Bellcore, the following systems have been developed over the course of the last few years. * STREAK <ref> [Robin and McKeown, 1993, Robin, 1994, Robin and McKeown, 1995] </ref> generates sum maries of basketball games using the concept of revisions. * PLANDoc [McKeown et al., 1994b, McKeown et al., 1995, Shaw, 1995] generates summaries of the activities of telephone planning engineers. <p> The content planner also receives input from the CIA World Factbook and possible descriptions of people or organizations to augment the base summary. The full content will be sent to a sentence generator, implemented using the FUF/SURGE language generation system <ref> [Elhadad, 1993, Robin, 1994] </ref>. The right side of the figure shows how proper nouns and their descriptions are extracted from past news. An entity extractor identifies proper nouns in the past newswire archives, along with descriptions. <p> In particular, we used FUF to implement the lexical chooser, representing the lexicon as a grammar as we have done in many previous systems (e.g., Elhadad [Elhadad, 1993], Robin <ref> [Robin, 1994] </ref>, McKeown et al. [McKeown et al., 1993], Feiner and McKeown [Feiner and McKeown, 1991]), and thus the main effort was in identifying the words and phrases needed for the domain. The content planner features several stages, as does the PLANDoc system. <p> Thus, the main problem was the identification of summarization strategies, which indicate how information is linked together to form a concise and cohesive summary. As we have found in other work <ref> [Robin, 1994] </ref>, what information is included is often dependent on the language available to make concise additions. <p> Use these fields to guide summarization. * Find and extend an existing ontology of event types. * Integrate domain ontologies from other sources. Such ontologies are available from the Message Understanding Conferences. 6.3.5 Integration of Sources * (*) Investigate the use of revisions to add background information. Robin <ref> [Robin, 1994] </ref> has successfully used such revisions in other domains.
Reference: [Robin and McKeown, 1993] <author> Robin, J. and McKeown, K. R. </author> <year> (1993). </year> <title> Corpus analysis for revision-based generation of complex sentences. </title> <booktitle> In Proceedings of the 11th National Conference on Artificial Intelligence, </booktitle> <address> Washington, D.C. </address>
Reference-contexts: Independently or in conjunction with Bellcore, the following systems have been developed over the course of the last few years. * STREAK <ref> [Robin and McKeown, 1993, Robin, 1994, Robin and McKeown, 1995] </ref> generates sum maries of basketball games using the concept of revisions. * PLANDoc [McKeown et al., 1994b, McKeown et al., 1995, Shaw, 1995] generates summaries of the activities of telephone planning engineers.
Reference: [Robin and McKeown, 1995] <author> Robin, J. and McKeown, K. R. </author> <year> (1995). </year> <title> Empirically designing and evaluating a new revision-based model for summary generation. </title> <journal> Artificial Intelligence Journal. </journal> <note> In press. </note>
Reference-contexts: Independently or in conjunction with Bellcore, the following systems have been developed over the course of the last few years. * STREAK <ref> [Robin and McKeown, 1993, Robin, 1994, Robin and McKeown, 1995] </ref> generates sum maries of basketball games using the concept of revisions. * PLANDoc [McKeown et al., 1994b, McKeown et al., 1995, Shaw, 1995] generates summaries of the activities of telephone planning engineers. <p> Also, evaluate the ease of adding a new service, knowledge source, or medium. * Develop a coverage benchmark for different message types and operators. * Evaluate generated summaries against real summaries from the newswire as well as summaries written by humans (as in <ref> [Robin and McKeown, 1995] </ref>). Such a corpus would allow, for example, scoring of recall and precision of the content of generated summaries against content of the human written summaries. It would also allow us to measure coverage of the generated phrasing against the corpus.
Reference: [Rosner, 1987] <author> Rosner, D. </author> <year> (1987). </year> <title> SEMTEX: A text generator for German. </title> <editor> In Kempen, G., editor, </editor> <booktitle> Natural Language Generation: New Results in Artificial Intellligence, Psychology, and Linguistics. </booktitle> <publisher> Martinus Ninjhoff Publishers. </publisher>
Reference-contexts: Among the most well-known systems, ana [Kukich, 1983], semtex <ref> [Rosner, 1987] </ref>, fog [Bourbeau et al., 1990], and lfs [Iordanskaja et al., 1994] need to be mentioned. All of them are domain-specific and their domains range from weather forecasts (fog) to stock-market reports (ana).
Reference: [Shaw, 1995] <author> Shaw, J. </author> <year> (1995). </year> <title> Conciseness through aggregation in text generation. </title> <booktitle> In Proceedings of the 33rd ACL (Student Session), </booktitle> <pages> pages 329-331. </pages>
Reference-contexts: Independently or in conjunction with Bellcore, the following systems have been developed over the course of the last few years. * STREAK [Robin and McKeown, 1993, Robin, 1994, Robin and McKeown, 1995] generates sum maries of basketball games using the concept of revisions. * PLANDoc <ref> [McKeown et al., 1994b, McKeown et al., 1995, Shaw, 1995] </ref> generates summaries of the activities of telephone planning engineers.
Reference: [Smadja and McKeown, 1991] <author> Smadja, F. and McKeown, K. R. </author> <year> (1991). </year> <title> Using collocations for language generation. </title> <journal> Computational Intelligence, </journal> <volume> 7(4). </volume>
Reference-contexts: The author hasn't been able to review these systems in time for this proposal. 2.2 Building Knowledge Sources for Generation The construction of a database of phrases for reuse in generation is quite novel. Previous work on extraction of collocations for use in generation <ref> [Smadja and McKeown, 1991] </ref> is related to ours in that full phrases are extracted and syntactically typed so that they can be merged with individual words in a generation lexicon to produce a full sentence. However, extracted collocations were used only to determine realization of an input concept.
Reference: [Smith and Chang, 1996] <author> Smith, J. R. and Chang, S.-F. </author> <year> (1996). </year> <title> Searching for images and videos on the world-wide web. </title> <note> submitted to ieee multimedia magazine, also cu/ctr technical report 459-96-25. </note>
Reference-contexts: We will investigate using co-occurrence information 6.2. PLANNED WORK AND PROPOSED EVALUATION 35 to match acronyms to full organization names and alternative spellings of the same name with each other. As a final twist to the integration of non-textual sources, we will apply some ongoing work on image classification <ref> [Aho et al., 1997, Smith and Chang, 1996] </ref> to the problem of generating illustrated summaries.
Reference: [Tait, 1983] <author> Tait, J. </author> <year> (1983). </year> <title> Automatic summarising of English texts. </title> <type> PhD thesis, </type> <institution> University of Cam-bridge, </institution> <address> Cambridge, England. 50 BIBLIOGRAPHY </address>
Reference-contexts: The original idea was presented in the 60es [Luhn, 1958] and has been used extensively over the years, especially since the Internet came into wide use. Existing algorithms and systems range from domain-dependent <ref> [DeJong, 1979, Tait, 1983, Jacobs and Rau, 1990] </ref> techniques which make use of specialized knowledge to topic-independent such as NetSumm [Preston and Williams, 1994], or Xerox's summarizer [Kupiec et al., 1995].
Reference: [Weischedel et al., 1993] <author> Weischedel, R., Ayuso, D., Boisen, S., Fox, H., Ingria, R., Matsukawa, T., Papageorgiou, C., MacLaughlin, D., Kitagawa, M., Sakai, T., Abe, J., Hosihi, H., Miyamoto, Y., and Miller, S. </author> <year> (1993). </year> <title> BBN: Description of the plum system as used for MUC-5. </title> <booktitle> In Proceedings of the Fifth Message Understanding Conference (MUC-5), </booktitle> <pages> pages 93-108, </pages> <address> Baltimore, Md. </address>
Reference-contexts: words in close proximity to known words [Cowie et al., 1992, Aberdeen et al., 1992], statistical training to learn, for example, Spanish names, from online corpora [Ayuso et al., 1992], and the use of concept based pattern matchers that use semantic concepts as pattern categories as well as part-of-speech information <ref> [Weischedel et al., 1993, Lehnert et al., 1993] </ref>. In addition, some researchers have explored the use of both local context surrounding the hypothesized proper nouns 2.5.
References-found: 69

