URL: ftp://ftp.cse.ucsc.edu/pub/tr/ucsc-crl-92-23.ps.Z
Refering-URL: ftp://ftp.cse.ucsc.edu/pub/tr/README.html
Root-URL: http://www.cse.ucsc.edu
Email: haussler@cse.ucsc.edu, krogh@cse.ucsc.edu  
Title: Protein Modeling using Hidden Markov Models: Analysis of  
Author: Globins David Haussler Anders Krogh I. Saira Mian Kimmen Sjolander 
Date: June 1992, revised Sept. 1992  
Address: Santa Cruz, CA 95064, USA.  
Affiliation: Computer and Information Sciences Sinsheimer Laboratories University of California,  
Pubnum: UCSC-CRL-92-23  
Abstract: We apply Hidden Markov Models (HMMs) to the problem of statistical modeling and multiple sequence alignment of protein families. A variant of the Expectation Maximization (EM) algorithm known as the Viterbi algorithm is used to obtain the statistical model from the unaligned sequences. In a detailed series of experiments, we have taken 400 unaligned globin sequences, and produced a statistical model entirely automatically from the primary (unaligned) sequences. We use no prior knowledge of globin structure. Using this model, we obtained a multiple alignment of the 400 sequences and 225 other globin sequences that agrees almost perfectly with a structural alignment by Bashford et al. This model can also discriminate all these 625 globins from nonglobin protein sequences with greater than 99% accuracy, and can thus be used for database searches. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D Bashford, C Chothia and A M Lesk, </author> <note> Journ Mol Biol 196 (1987) 199-216. </note>
Reference-contexts: case of globins, whose structure contains principally alpha helices, the 150 or so residues in a helical conformation have been named A1, A2, ..., A16, B1, ... etc., where the letter denotes which alpha helix the residue occurs in, and the number indicates the location within that helix (see e.g. <ref> [1] </ref>). For each of these positions there is a (distinct) probability distribution over the 20 amino acids that measures how likely it is that each amino acid will occur in that position in a typical globin, as well as the probability that there is no amino acid in that position. <p> We also obtain a multiple sequence alignment for all of the 625 globin sequences that basically reproduces the alignment given for the seven globins in Bashford et al. <ref> [1] </ref>, even though their method employed additional information on the three-dimensional structure of globins, whereas we have only used the primary sequence information. <p> These should naturally be considered minimum numbers, but they give a good picture of how skewed the sample is. Many of the globin sequences have been aligned by Bashford et al. <ref> [1] </ref> using seven different globins whose three-dimensional structures were known. This was done by aligning these seven sequences, and then aligning the rest (of the 226 they studied) to the closest of these seven. The alignment of the seven is shown in figure 2. globins from Bashford et al. [1]. <p> al. <ref> [1] </ref> using seven different globins whose three-dimensional structures were known. This was done by aligning these seven sequences, and then aligning the rest (of the 226 they studied) to the closest of these seven. The alignment of the seven is shown in figure 2. globins from Bashford et al. [1]. Above the alignments are indicated which alpha helix each column belongs to. Bashford et al. identified 8 helices (A to H), but some regions are not well defined (especially CD, D, and FG). <p> The alignments using the model from the first experiments are even better, and are almost identical to the structural alignments from <ref> [1] </ref>. This indicates that an extremely good model exists, and that the EM algorithm was able to find a good approximation to it. <p> As more sequences become available, the overall accuracy of the multiple alignment should improve, because better models will be found. Although it is not perfect, the alignment found from the second experiment (starting from an unbiased model) agrees very well with the structurally derived alignment by Bashford et al. <ref> [1] </ref>.
Reference: [2] <author> M Gribskov, R Luthy, and D Eisenberg, </author> <note> Methods in Enzymology 183 (1990) 146-59. </note>
Reference-contexts: These have been called profiles <ref> [2, 3, 4] </ref>. A profile of globins can be thought of as a statistical model for the family of globins, in that for any sequence of amino acids, it defines a probability for that sequence, such that globin sequences tend to have much higher probabilities than non-globin sequences. <p> One can choose any structure for the states and transitions that is appropriate for the problem at hand. 3 Estimating the HMM All the parameters in the model (i.e., probabilities) could in principle be chosen by hand from an existing alignment of protein sequences, as in <ref> [2] </ref>, or from information about the three-dimensional structure of proteins, as in [3]. The novel approach we take is to "learn" the parameters entirely automatically from a set of unaligned primary sequences, using an EM algorithm. <p> Indeed, this distance can be efficiently calculated in quadratic time by a straightforward generalization of the dynamic programming method used for sequence comparison (see also <ref> [2] </ref>). In addition, the most probable path can be found using the usual backtracking technique. The algorithm is known as the Viterbi algorithm in the HMM literature (see e.g. [10]).
Reference: [3] <author> J U Bowie, R Luthy, and D Eisenberg, </author> <note> Science 253 (1991) 164-70. </note>
Reference-contexts: These have been called profiles <ref> [2, 3, 4] </ref>. A profile of globins can be thought of as a statistical model for the family of globins, in that for any sequence of amino acids, it defines a probability for that sequence, such that globin sequences tend to have much higher probabilities than non-globin sequences. <p> and transitions that is appropriate for the problem at hand. 3 Estimating the HMM All the parameters in the model (i.e., probabilities) could in principle be chosen by hand from an existing alignment of protein sequences, as in [2], or from information about the three-dimensional structure of proteins, as in <ref> [3] </ref>. The novel approach we take is to "learn" the parameters entirely automatically from a set of unaligned primary sequences, using an EM algorithm.
Reference: [4] <author> M S Waterman and M D Perlwitz, </author> <title> Bull. </title> <journal> Math. Biol. </journal> <month> 46 </month> <year> (1986) </year> <month> 567-577. </month>
Reference-contexts: These have been called profiles <ref> [2, 3, 4] </ref>. A profile of globins can be thought of as a statistical model for the family of globins, in that for any sequence of amino acids, it defines a probability for that sequence, such that globin sequences tend to have much higher probabilities than non-globin sequences.
Reference: [5] <author> C E Lawrence and A A Reilly, </author> <note> Proteins 7 (1990) 41-51. </note>
Reference-contexts: In this paper we define a class of structures related to profiles and propose them as statistical models of protein families such as globins. Similar models can also be applied to other, non-protein sequence fam ilies, such as DNA <ref> [5, 6] </ref>. The novel aspect of this work is that we are able to "learn" or "estimate" the stochastic model directly from raw, unaligned sequences, rather than starting with a multiple alignment of the sequences. <p> From a statistical perspective, the procedure can be viewed as a computationally efficient approximation to maximum likelihood estimation, or, if prior probabilities for the parameters of the model are incorporated, as a maximum a posteriori or Bayesian approach. A related EM procedure is applied in <ref> [5, 6] </ref> to model protein binding sites, and an explicit 2-state HMM model is constructed in [12] to distinguish coding from non-coding regions in DNA. Here, in an attempt to model entire proteins, we explore a significantly richer class of models than these. <p> In this case we can find more training sequences by adding in the corresponding pieces of these other proteins. In fact, the EM method can be used to locate the appropriate pieces by itself, given the entire sequences <ref> [5, 6] </ref>. We are currently experimenting with an extension of the method described here that does this, applying it to the kinase and EF hand domains.
Reference: [6] <author> L R Cardon and G D Stormo, </author> <note> Journ Mol Biol 223 (1992) 159-170. </note>
Reference-contexts: In this paper we define a class of structures related to profiles and propose them as statistical models of protein families such as globins. Similar models can also be applied to other, non-protein sequence fam ilies, such as DNA <ref> [5, 6] </ref>. The novel aspect of this work is that we are able to "learn" or "estimate" the stochastic model directly from raw, unaligned sequences, rather than starting with a multiple alignment of the sequences. <p> From a statistical perspective, the procedure can be viewed as a computationally efficient approximation to maximum likelihood estimation, or, if prior probabilities for the parameters of the model are incorporated, as a maximum a posteriori or Bayesian approach. A related EM procedure is applied in <ref> [5, 6] </ref> to model protein binding sites, and an explicit 2-state HMM model is constructed in [12] to distinguish coding from non-coding regions in DNA. Here, in an attempt to model entire proteins, we explore a significantly richer class of models than these. <p> In this case we can find more training sequences by adding in the corresponding pieces of these other proteins. In fact, the EM method can be used to locate the appropriate pieces by itself, given the entire sequences <ref> [5, 6] </ref>. We are currently experimenting with an extension of the method described here that does this, applying it to the kinase and EF hand domains.
Reference: [7] <author> A Bairoch and B Boeckmann, </author> <title> Nucl. </title> <journal> Acids Res. </journal> <month> 19 </month> <year> (1991) </year> <month> 2247-2249. </month>
Reference-contexts: Finally, we can study the model we have found directly, and see what it reveals about the common structure underlying the various sequences in the family. We demonstrate the general technique on a set of 625 globin sequences from the SWISS-PROT database, release 19 <ref> [7] </ref>. By building a statistical model of globins based on 400 globin sequences selected at random, we are able to discriminate with very high accuracy between the remaining 225 globins and the 19,458 non-globins in the database.
Reference: [8] <author> M S Waterman, </author> <title> In Mathematical Methods for DNA Sequences, </title> <editor> ed M S Waterman, </editor> <publisher> CRC Press, </publisher> <year> 1989. </year>
Reference-contexts: Our method of multiple alignment is quite different from conventional methods, which are usually based on pairwise alignments using the standard dynamic programming scheme with gap penalties (see e.g. <ref> [8] </ref>). The alignments produced by conventional methods often depend strongly on the particular values of the parameters required by the method, in particular the gap penalties [9]. <p> This distance from sequence to model is very similar to the standard "edit distance" from one sequence to another (with gap penalties), see e.g. <ref> [8] </ref>. The log T (y i jy i1 ) corresponds to a penalty for using the transition from y i1 to y i in the model.
Reference: [9] <author> M Vingron and P Argos, dot-matrices. </author> <note> Journ Mol Biol 218 (1991) 33-43. </note>
Reference-contexts: The alignments produced by conventional methods often depend strongly on the particular values of the parameters required by the method, in particular the gap penalties <ref> [9] </ref>. Furthermore, a given set of sequences is likely to possess both fairly conserved regions and highly variable regions, yet in aligning them with the conventional global methods, the same penalties are used for all regions of the sequences.
Reference: [10] <author> L R Rabiner, </author> <note> Proc IEEE 77:2 (1989) 257-286. </note>
Reference-contexts: The models used there are smaller than ours, but have a more varied structure. 2 The protein HMM A hidden Markov Model describes a series of observations by a "hidden" stochastic process a Markov process; for an excellent review, see <ref> [10] </ref>. In speech recognition, where HMMs have been used extensively, the observations are sounds forming a word, and a model is one that by its "hidden" random process generates these sounds with high probability. <p> When modeling proteins, the observations are the amino acids in the primary sequence of a protein (corresponding to sounds in a word). A model for 1 Sometimes the algorithm is called the Viterbi algorithm, but following <ref> [10] </ref> we reserve that name for the estimation part of the algorithm. a set of proteins is one that assigns high probability to the sequences in that particular set. <p> Call the sequences s 1 : : : s n . Then, assuming independence Prob (s 1 : : : s n jmodel) = n Y Prob (s jmodel): (4) Equation (2) gives an expression for Prob (s jmodel), which can be calculated using the "forward algorithm" <ref> [10] </ref>. <p> In addition, the most probable path can be found using the usual backtracking technique. The algorithm is known as the Viterbi algorithm in the HMM literature (see e.g. <ref> [10] </ref>). Estimation algorithm There is no known efficient way to directly calculate the best model, i.e., the one that maximizes the posterior probability (3). <p> However, there are algorithms that iteratively re-estimate the model from some arbitrary starting point which guarantee that the posterior probability increases in each iteration, and thus find a local maximum. The most common one is the Baum-Welch or forward-backward algorithm <ref> [10] </ref>, which is a version of the general EM (Expectation-Maximization) method often used in statistics. The full Baum-Welch algorithm is computationally rather intensive, but using the Viterbi approximation (5) makes it somewhat faster. 2 The Viterbi-EM adaptation of the model to the sequences is an iterative process. <p> This can be done by starting with a better initial model, and by using more involved kinds of regularization. We are currently exploring a regularization method that we call "soft tying" of the distributions in the states of the HMM. This combines the idea of tying states, see e.g. <ref> [10] </ref>, in which the number of free parameters is reduced by having groups of states all share the same distribution on the output alphabet (the 20 amino acids in this case), and the idea of soft weight sharing from [18], in which the regularizer (in this case for the distribution of <p> Although this is not such a big problem with the data we used, one could consider using other methods if unknown amino acids are prevalent. The dynamic programming algorithm The Viterbi algorithm is described in the paper by Rabiner <ref> [10] </ref> page 264. Here we give a translation to our notation. The description in [10] is nice, general, and compact, so only the reader who wants to see the connection to sequence comparison with gap penalties, or who wants to actually implement the algorithm will benefit from this translation. <p> The dynamic programming algorithm The Viterbi algorithm is described in the paper by Rabiner <ref> [10] </ref> page 264. Here we give a translation to our notation. The description in [10] is nice, general, and compact, so only the reader who wants to see the connection to sequence comparison with gap penalties, or who wants to actually implement the algorithm will benefit from this translation. A negative log of a probability we will call a cost.
Reference: [11] <author> A P Dempster, N M Laird and D B Rubin, J. Roy. </author> <title> Statist. </title> <journal> Soc. B ,1977, </journal> <volume> Vol. 39, </volume> <pages> pp 1-38 </pages>
Reference-contexts: The algorithm we use to estimate the model is an approximation of the Baum-Welch or more general EM (expectation maximization) method 1 <ref> [11] </ref>. The basic idea is to start with some initial model, possibly randomly constructed, align all the sequences to this model, and then reestimate the probabilities in the model based on how the sequences align to it.
Reference: [12] <author> G A Churchill, </author> <note> Bull Math Biol 51 (1989) 79-94. </note>
Reference-contexts: A related EM procedure is applied in [5, 6] to model protein binding sites, and an explicit 2-state HMM model is constructed in <ref> [12] </ref> to distinguish coding from non-coding regions in DNA. Here, in an attempt to model entire proteins, we explore a significantly richer class of models than these. Independent work, closely related to ours, is also given in [13], where hidden Markov models for protein superfamilies are constructed.
Reference: [13] <author> White, Stultz and Smith, </author> <type> preprint, </type> <month> Nov. '91. </month>
Reference-contexts: Here, in an attempt to model entire proteins, we explore a significantly richer class of models than these. Independent work, closely related to ours, is also given in <ref> [13] </ref>, where hidden Markov models for protein superfamilies are constructed.
Reference: [14] <author> D Haussler, A Krogh, S Mian, </author> <title> and K Sjolander, </title> <type> UC Santa Cruz Tech. Rep. </type> <institution> UCSC-CRL-92-23. </institution>
Reference: [15] <author> N Abe and M K Warmuth, </author> <title> Machine Learn., Spec. </title> <note> issue for COLT90, to appear, </note> <year> 1992. </year>
Reference-contexts: It is basically a steepest-descent-type algorithm that climbs the nearest peak (local maximum) of the posterior probability function. Since finding the globally optimal model seems to be a difficult optimization problem in general <ref> [15] </ref>, we have experimented with various heuristic methods to improve the performance of the method. Probably the best method is to give the model a hint if something is already known about the sequences, which is often the case.
Reference: [16] <author> S Geman, E Bienenstock, and R Doursat, </author> <note> Neural Computation 4 (1992) 1-58. </note>
Reference-contexts: We say that the model does not "generalize" well to test sequences. This phenomenon has been well documented in statistics and machine learning (see e.g. <ref> [16] </ref>). One way to deal with this problem is to control the effective number of free parameters in the model by regularization, which can be viewed as a way of implementing MAP estimation.
Reference: [17] <author> S K Hanks and A M Quinn, </author> <note> Methods in Enzymology 200 (1991) 38-62. </note>
Reference-contexts: Furthermore, recently we tried the program with the exact same setting of all the regularization, noise rate and other parameters on an entirely different set of sequences: the set of 193 protein kinase sequences from the protein kinase catalytic domain database by Hanks and Quinn <ref> [17] </ref>. Preliminary analysis of these experiments indicate that the method works just as well on the kinases as it does on the globins, without any further tuning of the parameters. This is perhaps the best evidence for the robustness of the method.
Reference: [18] <author> S J Nowlan and G E Hinton, </author> <booktitle> In Advances Neural Information Processing Systems 4, </booktitle> <editor> ed. Moody, Hanson, and Lippmann, </editor> <publisher> Morgan Kauffmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1992. </year>
Reference-contexts: This combines the idea of tying states, see e.g. [10], in which the number of free parameters is reduced by having groups of states all share the same distribution on the output alphabet (the 20 amino acids in this case), and the idea of soft weight sharing from <ref> [18] </ref>, in which the regularizer (in this case for the distribution of amino acids) is also adaptively modified during learning. Even more complex, position-specific kinds of regularization are possible.
References-found: 18

