URL: ftp://theory.lcs.mit.edu/pub/people/danar/coins-colt95.ps.gz
Refering-URL: http://theory.lcs.mit.edu/~danar/papers.html
Root-URL: 
Title: Learning to model sequences generated by switching distributions  
Author: Yoav Freund Dana Ron 
Address: 600 Mountain Ave. Murray Hill, NJ, USA  Jerusalem, Israel  
Affiliation: AT&T Bell Labs  Computer Science Institute. Hebrew University  
Abstract: We study efficient algorithms for solving the following problem, which we call the switching distributions learning problem. A sequence S = 1 2 : : : n , over a finite alphabet is generated in the following way. The sequence is a concatenation of K runs, each of which is a consecutive subsequence. Each run is generated by independent random draws from a distribution ~p i over , where ~p i is an element in a set of distributions f~p 1 ; : : : ; ~p N g. The learning algorithm is given this sequence and its goal is to find approximations of the distributions ~p 1 ; : : : ; ~p N , and give an approximate segmentation of the sequence into its constituting runs. We give an efficient algorithm for solving this problem and show conditions under which the algorithm is guaranteed to work with high probability.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Abe and M. K. Warmuth. </author> <title> On the computational complexity of approximating distributions by probabilistic automata. </title> <journal> Machine Learning, </journal> <volume> 9(2/3), </volume> <year> 1992. </year> <note> Special issue for COLT90. </note>
Reference-contexts: The theoretical results regarding the problem of learning HMMs of which we are aware are mostly negative, Abe and Warmuth <ref> [1] </ref> and Gillman and Sipser [5] show that learning HMMs is NP-hard under various conditions. The model of sequences that we consider is similar to the HMM with the restriction that the transition probabilities assign a very high value to the transition from each hidden state to itself. <p> For j 2 <ref> [1; : : : ; N ] </ref> set ~p t+1 j to the empirical distribution of the elements of S for which ~ t (i) = j. 3. <p> We shall show that this threshold need not depend on the approximation parameter *, but rather is of the order of the smallest distance between any pair of target distributions. 4 Summary of Results Before we present our results, we add the following notation. For j 2 <ref> [1; : : : ; N ] </ref>, we use n j to denote the number of elements in S corresponding to the distribution ~p j , and define fl def min j (n j )=M . <p> Set ` = flM=K. (If L, the minimum length of any run in S, is known, set ` = max (flM=K; L)). 2. For each i 2 <ref> [1; : : : ; M ` + 1] </ref> and each 2 [1; : : : ; D], let f i () be the fraction of the elements in i i+1 : : : i+`1 which are equal to . <p> Set ` = flM=K. (If L, the minimum length of any run in S, is known, set ` = max (flM=K; L)). 2. For each i 2 [1; : : : ; M ` + 1] and each 2 <ref> [1; : : : ; D] </ref>, let f i () be the fraction of the elements in i i+1 : : : i+`1 which are equal to . Let ~ f i denote the vector hf i (1); f i (2); : : : ; f i (D)i. 3. <p> &lt; 1 =2, the length M of the sequence S satisfies M ffi ) ; then with probability at least 1 ffi, there exists a one-to-one mapping : f1; 2g ! f1; 2g such that for j 2 f1; 2g, k ~p 0 Proof: For a given index i 2 <ref> [1; : : : ; M ` + 1] </ref>, let S i;i+`1 = i : : : i+`1 , and let ff i be such that the frac tion of symbols in S i;i+l1 that were generated by ~p 1 and ~p 2 are (1 ff i ) and ff i <p> In the third lemma we show that if the segmentation ~ t small number of errors, then good new estimates of ~p 1 and ~p 2 can be computed using ~ t Lemma 10 (Reevaluation error for two distributions) Sup pose : <ref> [1; 2] </ref> ! [1; 2] is a one to one mapping such that fi = max (n 1;(2) ( ~ ); n 2;(1) ( ~ ))=M ; and fi &lt; fl. <p> In the third lemma we show that if the segmentation ~ t small number of errors, then good new estimates of ~p 1 and ~p 2 can be computed using ~ t Lemma 10 (Reevaluation error for two distributions) Sup pose : <ref> [1; 2] </ref> ! [1; 2] is a one to one mapping such that fi = max (n 1;(2) ( ~ ); n 2;(1) ( ~ ))=M ; and fi &lt; fl. <p> Initial estimates of the distributions. (general case) 1. Set ` = 1 2. Set = 2 2 (ln M+D ln (`+1)+ln 1 ` . 3. For each i 2 1 : : : M `+1 and each 2 <ref> [1; : : : ; D] </ref>, let f i () be the fraction of the elements in i ; i+1 ; : : : ; i+`1 which are equal to . <p> Thus step 4 generates a set of distributions with one representative per target distribution, as required in the statement of the lemma. Lemma 12 (Segmentation error for general N ) Let = min max k ~p (j) ~p j k 2 ; where ranges over all one-to-one mappings from <ref> [1; : : : ; N ] </ref> to [1; : : : ; N ]. Assume that is the mapping that achieves the minimum. <p> Lemma 12 (Segmentation error for general N ) Let = min max k ~p (j) ~p j k 2 ; where ranges over all one-to-one mappings from <ref> [1; : : : ; N ] </ref> to [1; : : : ; N ]. Assume that is the mapping that achieves the minimum. <p> The cost difference is a sum of the cost differences in the places where the segmentations disagree. These are independent random variables. It is easy to check that the coordinates of any cost vector are bounded in the range <ref> [0; 1] </ref> and thus the cost difference is bounded in [1; 1]. <p> The cost difference is a sum of the cost differences in the places where the segmentations disagree. These are independent random variables. It is easy to check that the coordinates of any cost vector are bounded in the range [0; 1] and thus the cost difference is bounded in <ref> [1; 1] </ref>. <p> Com bining this with the last equation we get the statement of the lemma. Lemma 13 (Reevaluation error for general N ) Suppose : <ref> [1; : : : ; N ] </ref> ! [1; : : : ; N ] is a one to one mapping such that fi = M If fi &lt; fl, then with probability at least 1 ffi, there exists a one-to-one mapping : [1; : : : ; N ] ! <p> Com bining this with the last equation we get the statement of the lemma. Lemma 13 (Reevaluation error for general N ) Suppose : <ref> [1; : : : ; N ] </ref> ! [1; : : : ; N ] is a one to one mapping such that fi = M If fi &lt; fl, then with probability at least 1 ffi, there exists a one-to-one mapping : [1; : : : ; N ] ! [1; : : : ; N ], such <p> error for general N ) Suppose : <ref> [1; : : : ; N ] </ref> ! [1; : : : ; N ] is a one to one mapping such that fi = M If fi &lt; fl, then with probability at least 1 ffi, there exists a one-to-one mapping : [1; : : : ; N ] ! [1; : : : ; N ], such that for every j 2 [1; : : : ; N ] k ~p t+1 fi 2 + var ; where var = 2 (K ln (N M ) + D ln (M + 1) <p> : : : ; N ] ! <ref> [1; : : : ; N ] </ref> is a one to one mapping such that fi = M If fi &lt; fl, then with probability at least 1 ffi, there exists a one-to-one mapping : [1; : : : ; N ] ! [1; : : : ; N ], such that for every j 2 [1; : : : ; N ] k ~p t+1 fi 2 + var ; where var = 2 (K ln (N M ) + D ln (M + 1) + ln (1=ffi) : The Proof of Lemma <p> ] is a one to one mapping such that fi = M If fi &lt; fl, then with probability at least 1 ffi, there exists a one-to-one mapping : <ref> [1; : : : ; N ] </ref> ! [1; : : : ; N ], such that for every j 2 [1; : : : ; N ] k ~p t+1 fi 2 + var ; where var = 2 (K ln (N M ) + D ln (M + 1) + ln (1=ffi) : The Proof of Lemma 13 is the same as the proof of Lemma 10 except for the <p> Where A is a constant independent of the hypothesis. The range of all of the c (i)'s is <ref> [0; 1] </ref> and they are independent random variables.
Reference: [2] <author> Leonard E. Baum and J. A. Eagon. </author> <title> An inequality with applications to statistical estimation for probabilistic functions of markov processes and to a model for ecology. </title> <journal> Bulletin of the American Mathematical Society, </journal> <volume> 73 </volume> <pages> 360-363, </pages> <year> 1967. </year>
Reference-contexts: HMMs are a popular model in the context of speech analysis. One can view the hidden state as representing the state of the vocal tract of the speaker, which is not directly observable but controls the distribution of the observable sounds. The Baum-Welch algorithm <ref> [2] </ref> is the predominant algorithm for learning HMMs from examples and produces. In many real-world cases, this algorithm produces accurate hypotheses after a small num ber of iterations. 1 There is almost no theory for explaining why Baum-Welch performs so well in some cases and badly in others. <p> In the third lemma we show that if the segmentation ~ t small number of errors, then good new estimates of ~p 1 and ~p 2 can be computed using ~ t Lemma 10 (Reevaluation error for two distributions) Sup pose : <ref> [1; 2] </ref> ! [1; 2] is a one to one mapping such that fi = max (n 1;(2) ( ~ ); n 2;(1) ( ~ ))=M ; and fi &lt; fl. <p> In the third lemma we show that if the segmentation ~ t small number of errors, then good new estimates of ~p 1 and ~p 2 can be computed using ~ t Lemma 10 (Reevaluation error for two distributions) Sup pose : <ref> [1; 2] </ref> ! [1; 2] is a one to one mapping such that fi = max (n 1;(2) ( ~ ); n 2;(1) ( ~ ))=M ; and fi &lt; fl.
Reference: [3] <author> Avrim Blum and Prasad Chalasani. </author> <title> Learning switching concepts. </title> <booktitle> In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 231-242, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: This problem is related to the problem of learning switching concepts studied by Blum and Chalasani <ref> [3] </ref>.
Reference: [4] <author> Thomas M. Cover and Joy A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference: [5] <author> David Gillman and Michael Sipser. </author> <title> Inference and minimization of hidden markov chains. </title> <booktitle> In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 147-158, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: The theoretical results regarding the problem of learning HMMs of which we are aware are mostly negative, Abe and Warmuth [1] and Gillman and Sipser <ref> [5] </ref> show that learning HMMs is NP-hard under various conditions. The model of sequences that we consider is similar to the HMM with the restriction that the transition probabilities assign a very high value to the transition from each hidden state to itself.
Reference: [6] <author> Nick Littlestone. </author> <title> Notes on the derivation of chernoff-type bounds for sums of random variables. </title> <type> Unpublished Manuscript, </type> <year> 1990. </year>
Reference-contexts: Our analysis does not support such a claim, and we shall later discuss this question briefly. 5 Useful Inequalities In the proofs of our theorems and lemmas we apply several well known inequalities that are given here as lemmas. The first is a Chernoff/Hoeffding type bound, derived by Littlestone <ref> [6] </ref>, and the second is due to Sanov ([4],page 292). Lemma 5 For m &gt; 0, let X 1 ; X 2 ; :::X m be m independent random variables where a i X i b i . Let p = P i E [X i ]=m.
Reference: [7] <author> L. R. Rabiner and B. H. Juang. </author> <title> An introduction to hidden markov models. </title> <journal> IEEE ASSP Magazine, </journal> <volume> 3(1) </volume> <pages> 4-16, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: However, in their setup the switching entities are concepts, i.e., mappings from some domain to f0; 1g, while in our setup the switching enti 1 For an introduction on HMMs and their use in speech analysis and the use of Hidden Markov Model see Rabiner and Juang <ref> [7] </ref>. ties are distributions over a single space. In this work we give an efficient algorithm for learning switching distributions. We describe several variants of the algorithm, each of which is guaranteed to succeed under slightly different conditions regarding the process which is generating the sequence.
Reference: [8] <author> A.J. </author> <title> Viterbi. Error bounds for convulutional codes and an asymptotically optimal decoding algorithm. </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> 13 </volume> <pages> 260-269, </pages> <year> 1967. </year>
Reference-contexts: Finding the segmentation with at most K runs that minimizes the total cost for a given set of cost vectors can be performed in time O (log (K)M 3 ) using a dynamic programming technique, which is essentially the same as the well known Viterbi algorithm <ref> [8] </ref>. The cost vectors are chosen so that with high probability the segmentation with the lowest total cost does not differ significantly from the target segmentation.
References-found: 8

