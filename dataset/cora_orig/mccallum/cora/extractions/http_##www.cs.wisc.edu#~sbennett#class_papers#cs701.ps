URL: http://www.cs.wisc.edu/~sbennett/class_papers/cs701.ps
Refering-URL: http://www.cs.wisc.edu/~sbennett/sbennett.html
Root-URL: 
Email: (sbennett, melski)@cs.wisc.edu  
Title: A Reason to Add Registers  
Author: Steve Bennett and David Melski 
Date: 12 May 1995  
Pubnum: CS701 Jim Larus Project Report  
Abstract: The increasing gap between CPU and memory performance forces us to reevaluate design choices made in the past to optimize the CPU-memory interface. Ideally, all CPU operand traffic would remain on-chip. It is obviously not feasible to have the number of registers be on the order of main memory size and it is not clear that increasing L1 cache sizes has significant benefit.[10] Current instruction set architectures (ISAs) provide at most 32 registers for the software to explicitly manage. There have been proposals to increase the number of registers, but it is not clear that more registers are useful, as traditional optimizations/transformations and register allocators were not designed to take advantage of a large number of registers. We have investigated the feasibility and utility of providing a larger area of fast memory which the compiler controls. The form of memory which we investigated in this project is registers. As an example of an transformation that takes advantage of a larger register set, we studied scalar replacement. Our goals in this project were to demonstrate three ideas: (1) there are realistic ways to increase the number of registers in an ISA without expanding the number of bits required to address the registers, (2) scalar replacement is an effective way of increasing the number of registers used, and (3) compared to results reported by Callahan, Carr and Kennedy[11] performance of the Livermore Loops transformed by scalar replacement improves when using a larger number of registers. Our experimental results do not allow us to justify all of these hypotheses, however. Our measured speedups on a number of kernels varied widely and did not match up well with those reported by Calahan, Carr and Kehnnedy. Though we question the performance changes we measured, all of our observations lead us to believe that the expanded register set has concrete performance benefits and is realizable. Based on our experience, we advocate further study into optimizations and transformations which improve performance while increasing register usage. 
Abstract-found: 1
Intro-found: 0
Reference: [8] <author> J. Amarasinghe, J. Anderson, M. Lam, C. Tseng, </author> <title> The SUIF Compiler for Scalable Parallel Machines, </title> <booktitle> Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Processing , July 1995. </booktitle>
Reference: [9] <author> T. Austin, T.N. Vijaykumar, Guri Sohi, </author> <title> Knapsack: A Zero-Cycle Memory Hierarchy Component, </title> <publisher> UW CS TR#93-1189, </publisher> <month> November </month> <year> 1993 </year>
Reference-contexts: Sohi and Hsu [20] discuss allocating array variables temporarily to vector register slots in Cray vector supercomputers. They use the vector registers as high speed, compiler (programmer) controlled cache. Their results indicate that this optimization is beneficial on the kernels studied. Austin, Vijaykumar and Sohis Knapsack study <ref> [9] </ref> showed that the compiler can find large numbers of variables in real programs which can be allocated to compiler controlled portions of the memory hierarchy, but it is not clear how this translates into improved system performance.
Reference: [10] <author> D. Burger, J. Goodman and A. Kagi, </author> <title> The Declining Effectiveness of Dynamic Caching for General-Purpose Microprocessors, </title> <institution> Univ. of Wisconsin Technical Report #TR-1261, </institution> <month> February </month> <year> 1995. </year>
Reference: [11] <author> D. Callahan, S. Carr, K. Kennedy, </author> <title> Improving Register Allocation for Subscripted Variables, </title> <booktitle> Proceedings of the ACM Conference on Programming Language Design and Implementation , June 1990. </booktitle>
Reference-contexts: (1) there are realistic ways to increase the number of registers in an ISA without expanding the number of bits required to address the registers, (2) scalar replacement is an effective way of increasing the number of registers used, and (3) compared to results reported by Callahan, Carr and Kennedy <ref> [11] </ref> performance of the Livermore Loops transformed by scalar replacement improves when using a larger number of registers. Our experimental results do not allow us to justify all of these hypotheses, however. <p> Examples of this concern for the number of registers available are limiting loop unrolling based on the number of registers required and limiting array accesses that are replaced when doing scalar replacement. <ref> [11] </ref> Some recent studies have considered using registers or other high speed, compiler controlled memory resource to temporarily hold values which traditional register allocation schemes do not allocate to registers. This includes array elements and dynamically allocated variables. <p> The fundamental idea is important: the compiler knows a great deal at compile time regarding the optimal placement of variables but is limited by hardware structures which it can not control, namely the cache. Callahan, Carr and Kennedy studied a source-to-source compiler transformation they named scalar replacement <ref> [11] </ref>. This transformation replaces array accesses with scalar temporary variables it introduces into the program. This increases register pressure since the target compiler attempts to allocate these new temporary variables to registers. <p> No virtual memory translation is performed. 3.4 Scalar Replacement Implementation We implemented the basic scalar replacement algorithm <ref> [11] </ref> using the SUIF compiler system.[8] SUIF provides a detailed dependence analysis framework. SUIF is a source-to-source translator, reading either C or FORTRAN code and emitting C code after optimizations are performed. This C code is then compiled with our modified version of gcc for use in the simulator. <p> It is clear that these results do not show us the whole picture. The wide variance leads us to believe that there is a problem in our measurement system or compiler implementation. The large difference between our 32 register implementation speedups and those in <ref> [11] </ref> also point to errors in our implementation. Even though the results reported in [11] are for a single-issue processor, we expected our relative speedups to be similar. We therefore do not place a great deal of emphasis on these values. <p> The wide variance leads us to believe that there is a problem in our measurement system or compiler implementation. The large difference between our 32 register implementation speedups and those in <ref> [11] </ref> also point to errors in our implementation. Even though the results reported in [11] are for a single-issue processor, we expected our relative speedups to be similar. We therefore do not place a great deal of emphasis on these values. <p> Problems with the absolute performance numbers aside, the trend that we see in the test results shown above is that of increasing performance as the register set expands. Though our absolute speedup numbers are less than those in <ref> [11] </ref>, we did not implement all optimizations that were used in that paper. It is not clear if the cost-performance tradeoffs make additonal registers desirable. <p> We measured speedups as large as 4.5 compared to results in <ref> [11] </ref> on certain Livermore loops compiled for an architecture with more than 32 registers after performing scalar replacement. However, our results displayed a wide variance and differed considerably from those reported by Calahan, Carr and Kennedy [11], therefore, we do not stress our experimental results. <p> We measured speedups as large as 4.5 compared to results in <ref> [11] </ref> on certain Livermore loops compiled for an architecture with more than 32 registers after performing scalar replacement. However, our results displayed a wide variance and differed considerably from those reported by Calahan, Carr and Kennedy [11], therefore, we do not stress our experimental results. Our goal in this project was to show that giving the compiler control over a larger block of the memory hierarchy can be beneficial if optimizations or transformations are available to take advantage of the available hardware resources. <p> Our experimental results do not allow us to verify or reject this hypothesis, they only lead us to ask more questions.. Loop Number Speedups 32 Registers 64 Registers Speedup As Reported in <ref> [11] </ref> 1 2.19 2.19 1.03 7 1.35 1.02 1.22 11 4.68 4.71 1.17 Harmonic Mean 1.65 1.79 1.78 Table 2: Speedups 6.0 Acknowledgments We would like to thank Todd Austin for his help with the simulator and our gcc modifications.
Reference: [12] <author> S. Carr, K. Kennedy, </author> <title> Scalar Replacement in the Presence of Conditional Control Flow, </title> <institution> Rice University Technical Report CRPC-TR92283, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: Calla-han, Carr and Kennedy indicate that limiting the number of registers consumed by their transformed code seems to control code size well. The initial study did not look at a large group of application packages, instead concentrating on the Liver more loops. A later study <ref> [12] </ref> looked at conditional control flow and the performance of larger group of application pro-grams. Performance improvements for these application programs was less encouraging. Average improvement in performance was 3%. Some work has indicated that more than 32 registers does not improve performance by a significant amount.
Reference: [13] <author> Chaitin, et. al. </author> <title> Register Allocation Via Coloring, </title> <booktitle> Computer Languages , 6 </booktitle> <pages> 47-57, </pages> <year> 1981 </year>
Reference: [14] <author> F. Chow, J. Hennessey, </author> <title> The Priority-Based Coloring Approach to Register Allocation, </title> <journal> ACM Transactions on Programming Languages and Systems , October 1990. </journal>
Reference: [15] <author> W. W. Hwu, et al, </author> <title> The Superblock: An Effective Technique for VLIW and Superscalar Compilation, </title> <publisher> Journal of Supercomputing , Kluwer Academic Publishers, </publisher> <year> 1993. </year>
Reference: [16] <author> Intel Corporation, </author> <title> i486 Microprocessor Programmers Reference Manual , 1990. </title>
Reference-contexts: In todays 32-bit RISC instructions and existing CISC instruction sets, the number of bits in an instruction for use in addressing registers is limited (see [19] and <ref> [16] </ref> for examples). Hence using traditional register design methodologies, an arbitrarily large set of registers is not feasible. As architectures move toward 64-bit instructions, there will be a call to expand the instruction formats to 64 bits as well.
Reference: [17] <author> T. Kiyohara, et al, </author> <title> Register Connection: A New Approach to Adding Registers into Instruction Set Architectures, </title> <booktitle> Proceedings of the International Symposium on Computer Architecture , 1993. </booktitle>
Reference: [18] <author> S. Mahlke, et al, </author> <title> Scalar Program Performance on Multiple-Instruction-Issue Processors with a Limited Number of Registers, </title> <booktitle> Proceedings of the 25th Annual Hawaii International Conference on System Sciences , 1992. </booktitle>
Reference: [19] <author> D. Patterson, J. </author> <title> Hennessey, </title> <booktitle> Computer Organization and Design The Hardware/Software Interface , Morgan-Kaufmann, </booktitle> <year> 1994. </year>
Reference-contexts: In todays 32-bit RISC instructions and existing CISC instruction sets, the number of bits in an instruction for use in addressing registers is limited (see <ref> [19] </ref> and [16] for examples). Hence using traditional register design methodologies, an arbitrarily large set of registers is not feasible. As architectures move toward 64-bit instructions, there will be a call to expand the instruction formats to 64 bits as well.
Reference: [20] <author> G. Sohi, </author> <title> Instruction Issue Logic for High-Performance, Interruptible, Multiple Functional Unit, Pipelined Computers, </title> <journal> IEEE Transactions on Computers , March 1990. </journal>
Reference-contexts: Both of these aspects are critical in todays computing environments; in supercomputers and even microprocessors a non-register operand fetch may cost hundreds of CPU cycles. Sohi and Hsu <ref> [20] </ref> discuss allocating array variables temporarily to vector register slots in Cray vector supercomputers. They use the vector registers as high speed, compiler (programmer) controlled cache. Their results indicate that this optimization is beneficial on the kernels studied. <p> The machine modeled by the simulator used in this project is configured as follows: Issue widths are set at 4 The state maintenance mechanism is an RUU <ref> [20] </ref> with full operand bypass. The register file, no matter how large, is accessible in a single cycle. Functional Units are fully pipelined, capable of starting a new instruction each cycle.
Reference: [21] <author> G. Sohi, W.C. Hsu, </author> <title> The Use of Intermediate Memories for Low-Latency Memory Access in Supercomputer Scalar Units, </title> <booktitle> The Journal of Supercomputing , 4,5-21 (1990) </booktitle>
Reference: [22] <author> D. Wall, </author> <title> Global Register Allocation at Link Time, </title> <booktitle> Proceedings of the ACM SIGPLAN 1986 Symposium on Compiler Construction , June 1986. </booktitle>
References-found: 15

