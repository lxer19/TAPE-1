URL: http://polaris.cs.uiuc.edu/reports/1012.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: ISSUES ON THE DESIGN OF PARALLELIZING COMPILERS  
Author: BY BRUCE PAUL LEUNG 
Degree: 1986 THESIS Submitted in partial fulfillment of the requirements for the degree of Master of Science in Computer Science in the Graduate College of the  
Address: 1990 Urbana, Illinois  
Affiliation: B.S., Purdue University,  University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers : Principles, Techniques and Tools. </booktitle> <publisher> Addison Wesley, </publisher> <month> march </month> <year> 1986. </year>
Reference-contexts: Finally, all function structures are on a doubly linked list. 9 Besides the basic information, additional information used to aid compiler optimizations is also maintained within the structure. Each function structure contains pointers to the call graph and control flow graph associated with the subprogram <ref> [1] </ref>. The call graph represents the order of function calls in a program, while the flow graph indicates the flow of control. As an aid to interprocedural analysis, reference information about parameters and global variables is stored [19].
Reference: [2] <author> Frances E. Allen, Michael Burke, Ron Cytron, Jeanne Ferrante, Wilson Hsieh, and Vivek Sarkar. </author> <title> A framework for determining useful parallelism. </title> <booktitle> In ICS 88, </booktitle> <year> 1988. </year>
Reference-contexts: There are several timing models to consider in order to achieve this goal. The first model considers a machine which has as many processors as needed and no memory access overheads or synchronization delays. By determining the maximum parallelism possible 33 <ref> [2] </ref>, timing with this model will give the theoretical best time for execution of a program, T 1 . We use T 1 as a basis for comparison with other models. For the next model, assume that memory access is the primary contributor to the execution time of a program.
Reference: [3] <author> J. R. Allen and K. Kennedy. </author> <title> Pfc: A program to convert fortran to parallel form. </title> <type> Technical Report MASC-TR82-6, </type> <institution> Rice University, Houston, Texas, </institution> <month> March </month> <year> 1982. </year>
Reference-contexts: This entails constructing the compiler in a way that allows rapid implementation of new ideas and techniques. Parafrase-2, a multilingual, restructuring, auto-scheduling compiler project at the University of Illinois, is the basis for much of the experience drawn upon for this work. Early restructuring compilers, such as Parafrase, PFC <ref> [3] </ref> and KAP [14], were primarily concerned with automatic program vectorization. As multiprocessor machines were developed, vendors usually provided run-time libraries for taking advantage of the multiple processors. Automatic concurrentization was provided on a limited basis, if at all.
Reference: [4] <author> J.R. Allen, D. Baumgartner, K. Kennedy, and A. Ploomfield. </author> <title> Ptool: A semi-automatic parallel programming system. </title> <type> Technical Report TR86-31, </type> <institution> Rice University, Houston, Texas, </institution> <month> January </month> <year> 1986. </year>
Reference-contexts: Early restructuring compilers, such as Parafrase, PFC [3] and KAP [14], were primarily concerned with automatic program vectorization. As multiprocessor machines were developed, vendors usually provided run-time libraries for taking advantage of the multiple processors. Automatic concurrentization was provided on a limited basis, if at all. More recent projects, <ref> [4] </ref>, [5], [12] and [20], employ sophisticated user interfaces to aid the programmer in discovering and exploiting many forms of parallelism. Graphical interfaces provide a means to conveniently view important structures such as the dependence graph or control flow graph.
Reference: [5] <author> Allen, Frances E., et. al. </author> <title> An overview of the PTRAN analysis system. </title> <booktitle> In Proceedings of the 1987 International Conference on Supercomputing. </booktitle> <publisher> Springer-Verlag, LNCS, </publisher> <month> February </month> <year> 1988. </year>
Reference-contexts: As multiprocessor machines were developed, vendors usually provided run-time libraries for taking advantage of the multiple processors. Automatic concurrentization was provided on a limited basis, if at all. More recent projects, [4], <ref> [5] </ref>, [12] and [20], employ sophisticated user interfaces to aid the programmer in discovering and exploiting many forms of parallelism. Graphical interfaces provide a means to conveniently view important structures such as the dependence graph or control flow graph.
Reference: [6] <author> Utpal Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: A (1:N:2). Finally, there are N-ary operators. These consist of array references and subprogram calls which do not have a fixed number of operands. 2.3.4 Data Dependence Graph The data dependence graph is used to keep the information regarding dependences among statements in a program <ref> [6] </ref>. The dependence graph is vital to virtually all program restructuring techniques. Dependence information specifies constraints on memory references within a program. A valid program transformation must maintain these dependences in order to preserve the original semantics of a program.
Reference: [7] <author> Mark Byler, James R.B. Davies, Christopher Huson, Bruce Leasure, and Michael Wolfe. </author> <title> Multiple version loops. </title> <booktitle> In Proceedings of the 1987 International Conference on Parallel Processing, </booktitle> <pages> pages 312-318, </pages> <month> August </month> <year> 1987. </year>
Reference-contexts: The choice of versions to generate, as well as the execution of the run-time decision, must also be given careful consideration. Although this technique may be applied to other code, we restrict our discussion to loops. Previous work in this area has been presented in <ref> [7] </ref>. Several cases where multiversion loops could be used are presented as well as a brief discussion of the implementation in the KAP Fortran translator [14]. <p> This lack of information is often due to the 21 unknown value range of a variable. By run-time checking, MVL allows these undetected parallel loops to be found and exploited. The KAP Fortran translator <ref> [7] </ref> augments its dependence graph with the conditions necessary to eliminate conservative dependences. We assume that such a facility is available. DO I = 1,N S 2 : D (I) = A (I-M) + 2 S 1 ? (=) if M = 0 S 2 to be unknown at compile-time.
Reference: [8] <author> Mike Chastain, Gary Gostin, Jim Mankovich, and Steve Wallach. </author> <title> The Convex C240 architecture. </title> <booktitle> In Supercomputing 88. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1988. </year>
Reference-contexts: The most general way to do this involves timing a section of code before and after performing a transformation or series of transformations. Inspection of the results determines the best transformation for decreasing execution time. At least one commercial compiler features some level of cost/performance analysis <ref> [8] </ref>. STA may be applied to various areas and transformations in a compiler.
Reference: [9] <author> Jacques Cohen. </author> <title> Computer-assisted microanalysis of programs. </title> <journal> Communications of the ACM, </journal> <volume> 25(10) </volume> <pages> 724-733, </pages> <month> October </month> <year> 1982. </year>
Reference-contexts: Other work involves using empirical results to estimate the execution time for a specific machine [15]. Some work in static analysis is not concerned with execution time, but rather, determining reachable program states [16]. Work closer to our interests involves deriving time-formulas based on quantitative information gathered during analysis <ref> [9] </ref>. This requires modeling the behavior of various language constructs on various architectures and deriving a time-formula that best represents the program. From this time-formula we can proceed to compute a more specific value for a particular machine or use the information to guide other aspects of compiler optimization. <p> The rest of this section presents an overview of some of this work and how each can be applied to static timing analysis. In <ref> [9] </ref>, time-formulas are presented which allow the machine specific details to be abstracted and only filled in when a specific result is required.
Reference: [10] <author> Kyle Gallivan, Dennis Gannon, William Jalby, Allen Malony, and Harry Wijshoff. </author> <title> Behavioral characterization of multiprocessor memory systems: A case study. </title> <institution> Center for 48 Supercomputing Research and Development, University of Illinois at Urbana Champaign, </institution> <year> 1988. </year>
Reference-contexts: Timing ex-periments are needed to compute actual values for specific machines, however, a more general approach, such as that presented in <ref> [10] </ref>, may prove more useful. 4.2 Application of Static Timing Analysis By having some quantitative information about a program, or section of code, the compiler can make informed decisions throughout the compilation process. Chapter 3 showed how STA may be used to help generate multiple versions of code. <p> For the next model, assume that memory access is the primary contributor to the execution time of a program. By counting the various types of memory references, a close approximation (within some constant factor) to the actual execution time is achieved <ref> [10] </ref>. Finally, we need a model to determine T 1 .
Reference: [11] <author> Milind B. Girkar, Mohammad-Reza Haghighat, Chia-Ling Lee, Bruce P. Leung, and Dale A. Schouten. </author> <title> Parafrase-2 manual. CSRD Internal document. </title>
Reference-contexts: S_F_ENTRY The representation of the Fortran ENTRY statement is used as a place holder for code generation since all the necessary information is stored in the symbol table and function structures. 46 S_F_DATA The Fortran DATA statement. A complete discussion of all Parafrase-2 internal data structures is found in <ref> [11] </ref>. 47
Reference: [12] <author> Vincent A. Guarna, Dennis Gannon, Yogesh Gaur, and David Jablonowski. </author> <title> Faust : an environment for programming parallel scientific applications. </title> <booktitle> In Supercomputing 88. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1988. </year>
Reference-contexts: As multiprocessor machines were developed, vendors usually provided run-time libraries for taking advantage of the multiple processors. Automatic concurrentization was provided on a limited basis, if at all. More recent projects, [4], [5], <ref> [12] </ref> and [20], employ sophisticated user interfaces to aid the programmer in discovering and exploiting many forms of parallelism. Graphical interfaces provide a means to conveniently view important structures such as the dependence graph or control flow graph.
Reference: [13] <author> Mark D. Guzzi. </author> <title> CEDAR Fortran Programmers Handbook, </title> <month> June </month> <year> 1987. </year>
Reference-contexts: Many 7 classes have additional information associated with them (see Appendix A). There is also a storage class associated with each entry which specifies if it is automatic, static or external. These are also supplemented by a global or cluster specification to accommodate Cedar Fortran <ref> [13] </ref>.
Reference: [14] <author> C. Huson, T. Macke, J. Davies, M. Wolfe, and B. Leasure. </author> <title> The kap/205: An advanced source-to-source vectorizer for the cyber 205 supercomputer. </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: Parafrase-2, a multilingual, restructuring, auto-scheduling compiler project at the University of Illinois, is the basis for much of the experience drawn upon for this work. Early restructuring compilers, such as Parafrase, PFC [3] and KAP <ref> [14] </ref>, were primarily concerned with automatic program vectorization. As multiprocessor machines were developed, vendors usually provided run-time libraries for taking advantage of the multiple processors. Automatic concurrentization was provided on a limited basis, if at all. <p> Although this technique may be applied to other code, we restrict our discussion to loops. Previous work in this area has been presented in [7]. Several cases where multiversion loops could be used are presented as well as a brief discussion of the implementation in the KAP Fortran translator <ref> [14] </ref>. These cases involve testing loop increments for stride one access, testing the data dependence graph for conditions under which a dependence does not exist, and testing loop bounds to determine the actual number of iterations.
Reference: [15] <author> Thomas H. Kong, Alfred Z. Spector, and Daniel P. Siewiorek. </author> <title> Measuring time in cm*. </title> <type> Technical Report CMU-CS-86-136, </type> <institution> Carnegie Mellon University Department of Computer Science, </institution> <month> May </month> <year> 1986. </year>
Reference-contexts: O (n). Other work involves using empirical results to estimate the execution time for a specific machine <ref> [15] </ref>. Some work in static analysis is not concerned with execution time, but rather, determining reachable program states [16]. Work closer to our interests involves deriving time-formulas based on quantitative information gathered during analysis [9]. <p> This model may provide useful information for determining which parts of a program are actually executed and how synchronization affects the execution characteristics of the program. 30 The work described in <ref> [15] </ref> involves actual timing experiments on the Cm*.
Reference: [16] <author> Charles E. McDowell. </author> <title> A practical algorithm for static analysis of parallel programs. </title> <type> Technical Report UCSC-CRL-87-23, </type> <institution> Baskin Center for Computer Engineering & Information Sciences, University of California Santa Cruz, </institution> <month> December </month> <year> 1987. </year>
Reference-contexts: O (n). Other work involves using empirical results to estimate the execution time for a specific machine [15]. Some work in static analysis is not concerned with execution time, but rather, determining reachable program states <ref> [16] </ref>. Work closer to our interests involves deriving time-formulas based on quantitative information gathered during analysis [9]. This requires modeling the behavior of various language constructs on various architectures and deriving a time-formula that best represents the program. <p> The authors present a communication model for determining the behavior of synchronization based on shared memory locks as well as a method for modeling the allocation of processors. Another useful model is presented in <ref> [16] </ref>. While not concerned with timing estimates, the author presents a method for determining reachable program states in a concurrent program.
Reference: [17] <author> C. D. Polychronopoulos. </author> <title> Parallel Programming and Compilers. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: Finally, each task may be serial or parallel where a serial task consists of a single process and a parallel task may be composed of several processes, each able to run on a separate processor <ref> [17] </ref>. In the past, most scheduling had been done manually by the programmer. This was usually in the form of some static assignment of loop iterations to processors. For complex programs or inexperienced programmers, an auto-scheduling compiler has the potential to outperform the user [17]. <p> to run on a separate processor <ref> [17] </ref>. In the past, most scheduling had been done manually by the programmer. This was usually in the form of some static assignment of loop iterations to processors. For complex programs or inexperienced programmers, an auto-scheduling compiler has the potential to outperform the user [17]. <p> For other schemes, such as Guided Self-Scheduling <ref> [17] </ref>, the scheduling overhead may be logarithmic in P . To simplify our analysis, we assume that overhead is a linear function of P (this is easily extended to the logarithmic case). <p> END DO If 0 &lt; M &lt; N , another alternative is to perform a DOALL nested inside a serial DO as follows: DO I = 1,N,M DOALL J = I,I+M-1 A (J) = A (J-M) + D (J) END DOALL END DO This is also known as cycle shrinking <ref> [17] </ref>. Yet another possibility is to execute a synchronized DOACROSS: initialization DOACROSS I = 1, N wait (I-M) A (I) = A (I-M) + D (I) signal (I) END DO Here the cost of synchronization versus the overhead of a fork and join must be taken into account.
Reference: [18] <author> Bin Qin, Howard A. Sholl, and Reda A. Ammar. </author> <title> Micro time cost analysis of parallel computations. </title> <type> Technical report, </type> <institution> University of Connecticut Computer Science & Engineering Department, </institution> <year> 1988. </year>
Reference-contexts: The formulas that are derived from this system are limited in that they only address sequential programs. This work, however, serves as a basis from which to start and may be expanded to include many of the parallel constructs available in many languages. A more recent paper, <ref> [18] </ref>, uses roughly the same idea and presents new methods for calculating time-formulas. The authors present a communication model for determining the behavior of synchronization based on shared memory locks as well as a method for modeling the allocation of processors. Another useful model is presented in [16].
Reference: [19] <author> Dale Schouten. </author> <title> An overview of interprocedural analysis techniques for high performance parallelizing compilers. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> Jan-uary </month> <year> 1990. </year> <month> 49 </month>
Reference-contexts: Each entry also contains a pointer to a union-find set for use in the Fortran EQUIVALENCE statement. If two entries are equivalenced, they will both be in the same union-find set. Finally, aliasing information (in the form of alias pairs <ref> [19] </ref>) gathered by interprocedural analysis is stored for use by other analysis phases. 2.3.2.2 Datatype Next to the symbol table entry, the datatype associated with each program identifier contains the most vital information. Every identifier has a datatype, though not necessarily unique. <p> The call graph represents the order of function calls in a program, while the flow graph indicates the flow of control. As an aid to interprocedural analysis, reference information about parameters and global variables is stored <ref> [19] </ref>. Reference information includes the parameters and/or global variables which are modified or used by the subprogram. <p> Thus each procedure performs a specific task and can hide the implementation details from the rest of the program. Accurate dependence analysis, i.e. aggressive parallelization, requires interprocedural information which summarizes the use of local and global variables across procedure calls. Interprocedural analysis techniques are discussed in <ref> [19] </ref>. The call graph provides a convenient means for these algorithms to track the effect of procedure calls. The call graph represents the calling sequence of a given program or function. Associated with each function is a callgraph node which contains a list of edges representing calls in that function.
Reference: [20] <editor> Sridharan, K., et. al. </editor> <title> An environment for parallel structuring of fortran programs. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <pages> pages II:98-117, </pages> <month> August </month> <year> 1989. </year> <month> 50 </month>
Reference-contexts: As multiprocessor machines were developed, vendors usually provided run-time libraries for taking advantage of the multiple processors. Automatic concurrentization was provided on a limited basis, if at all. More recent projects, [4], [5], [12] and <ref> [20] </ref>, employ sophisticated user interfaces to aid the programmer in discovering and exploiting many forms of parallelism. Graphical interfaces provide a means to conveniently view important structures such as the dependence graph or control flow graph.
References-found: 20

