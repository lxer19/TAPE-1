URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/MP-TR-97-05/MP-TR-97-05.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/MP-TR-97-05/
Root-URL: http://www.cs.wisc.edu
Title: Minimum-Support Solutions of Polyhedral Concave Programs  
Author: O. L. Mangasarian 
Abstract: This paper is dedicated to the memory of Karl-Heinz Elster Abstract Motivated by the successful application of mathematical programming techniques to difficult machine learning problems, we seek solutions of concave minimization problems over polyhedral sets with a minimum number of nonzero components. We prove that if such problems have a solution, they have a vertex solution with a minimal number of zeros. This includes linear programs and general linear complementarity problems. A smooth concave exponential approximation to a step function solves the minimum-support problem exactly for a finite value of the smoothing parameter. A fast finite linear-programming-based iterative method terminates at a stationary point, which for many important real world problems provides very useful answers. Utilizing the complementarity property of linear programs and linear complementarity problems, an upper bound on the number of nonzeros can be obtained by solving a single convex minimization problem on a polyhedral set.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. S. Bradley, Usama M. Fayyad, and O. L. Mangasarian. </author> <title> Data mining: Overview and optimization opportunities. </title> <type> Technical Report 98-01, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin, </institution> <month> January </month> <year> 1998. </year> <note> INFORMS Journal on Computing, submitted. Available from: ftp://ftp.cs.wisc.edu/math-prog/tech-reports/998-01.ps.Z. </note>
Reference-contexts: previous work on minimum-support solutions of mathematical programs, it is one of the primary purposes of this paper to bring to the attention of the mathematical programming community the importance and the richness of such problems from the point of view of machine learning [14, 3, 2] and data mining <ref> [15, 1] </ref>. A related area in these fields that has received recent wide attention is that of support vector machines [20, 7, 4] where instead of suppressing solution components, as we are doing here, fl Mathematical Programming Technical Report 97-05, April 1997. Revised March 1998.
Reference: [2] <author> P. S. Bradley, O. L. Mangasarian, and J. B. Rosen. </author> <title> Parsimonious least norm approximation. </title> <type> Technical Report 97-03, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin, </institution> <month> March </month> <year> 1997. </year> <note> Computational Optimization and Applications, to appear. ftp://ftp.cs.wisc.edu/math-prog/tech-reports/97-03.ps.Z. </note>
Reference-contexts: 1 Introduction This work is prompted by applications of mathematical programming to machine learning and other problems where a solution to a mathematical program is desired with as many of its components equal to zero as possible <ref> [14, 3, 15, 2] </ref>. <p> Since there is virtually no previous work on minimum-support solutions of mathematical programs, it is one of the primary purposes of this paper to bring to the attention of the mathematical programming community the importance and the richness of such problems from the point of view of machine learning <ref> [14, 3, 2] </ref> and data mining [15, 1]. A related area in these fields that has received recent wide attention is that of support vector machines [20, 7, 4] where instead of suppressing solution components, as we are doing here, fl Mathematical Programming Technical Report 97-05, April 1997.
Reference: [3] <author> P. S. Bradley, O. L. Mangasarian, and W. N. </author> <title> Street. Feature selection via mathematical programming. </title> <journal> INFORMS Journal on Computing, </journal> <note> 1998. To appear. Available at ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-21.ps.Z. </note>
Reference-contexts: 1 Introduction This work is prompted by applications of mathematical programming to machine learning and other problems where a solution to a mathematical program is desired with as many of its components equal to zero as possible <ref> [14, 3, 15, 2] </ref>. <p> This leads to better "generalization" [22], that is a plane that will discriminate better between new elements of A and B that were not involved in determining the original separating plane <ref> [21, 14, 3] </ref>. We are thus interested in obtaining minimum-support solutions to mathematical programs, that is solutions with as many components equal to zero as possible. <p> Since there is virtually no previous work on minimum-support solutions of mathematical programs, it is one of the primary purposes of this paper to bring to the attention of the mathematical programming community the importance and the richness of such problems from the point of view of machine learning <ref> [14, 3, 2] </ref> and data mining [15, 1]. A related area in these fields that has received recent wide attention is that of support vector machines [20, 7, 4] where instead of suppressing solution components, as we are doing here, fl Mathematical Programming Technical Report 97-05, April 1997.
Reference: [4] <author> C. J. C. Burges. </author> <title> Simplified support vector decision rules. </title> <editor> In L. Saita, editor, </editor> <booktitle> Machine Learning-Proceedings of the THirteenth International Conference (ICML '96)-Bari, </booktitle> <address> Italy July 3-6, </address> <year> 1996, </year> <pages> pages 71-77, </pages> <address> San Francisco, CA, 1996. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A related area in these fields that has received recent wide attention is that of support vector machines <ref> [20, 7, 4] </ref> where instead of suppressing solution components, as we are doing here, fl Mathematical Programming Technical Report 97-05, April 1997. Revised March 1998.
Reference: [5] <author> S.-J. Chung. </author> <title> NP-completeness of the linear complementarity problem. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 60 </volume> <pages> 393-399, </pages> <year> 1989. </year>
Reference-contexts: This problem includes linear programs of course, but also includes NP-hard problems, such as the general linear complementarity problem <ref> [6, 5] </ref> and others. We shall therefore be concerned with existence of solutions to problem (1) with minimum number of nonzero elements, and methods to for computing such solutions.
Reference: [6] <author> R. W. Cottle, J.-S. Pang, and R. E. Stone. </author> <title> The Linear Complementarity Problem. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: This problem includes linear programs of course, but also includes NP-hard problems, such as the general linear complementarity problem <ref> [6, 5] </ref> and others. We shall therefore be concerned with existence of solutions to problem (1) with minimum number of nonzero elements, and methods to for computing such solutions.
Reference: [7] <author> H. Drucker, C. J. C. Burges, L. Kaufman, A. Smola, and V. Vapnik. </author> <title> Support vector regression machines. </title> <editor> In M. C. Mozer, M. I. Jordan, and T. Petsche, editors, </editor> <booktitle> Advances in Neural Information Processing Systems -9-, </booktitle> <pages> pages 155-161, </pages> <address> Cambridge, MA, 1997. </address> <publisher> MIT Press. </publisher>
Reference-contexts: A related area in these fields that has received recent wide attention is that of support vector machines <ref> [20, 7, 4] </ref> where instead of suppressing solution components, as we are doing here, fl Mathematical Programming Technical Report 97-05, April 1997. Revised March 1998.
Reference: [8] <author> S.-P. Han and O. L. Mangasarian. </author> <title> Exact penalty functions in nonlinear programming. </title> <journal> Mathematical Programming, </journal> <volume> 17 </volume> <pages> 251-269, </pages> <year> 1979. </year>
Reference-contexts: a vertex solution of (6) for ff 2 fff i g ff 0 = ff 0 (fi). - We establish now our principal result, that of the existence of a minimum-support vertex solution to our original concave minimization problem, via a sufficient penalty function argument similar to that used in <ref> [8, Theorem 4.1] </ref>,[11, Theorem 2.3], for a finite value of the penalty parameter. This result will also be useful in specifying a finite successive linear approximation algorithm for obtaining a stationary point of the NP-hard minimum-support problem. Related finite penalty parameter results are also given in [13, 17].
Reference: [9] <author> O. L. Mangasarian. </author> <title> Characterization of linear complementarity problems as linear programs. </title> <journal> Mathematical Programming Study, </journal> <volume> 7 </volume> <pages> 74-87, </pages> <year> 1978. </year>
Reference-contexts: For the general linear complementarity problem 0 x ? M x + q 0; (22) where M is a given matrix in R nfin , it is easy to show <ref> [9, Lemma 1] </ref> that the problem is equivalent to the following concave minimization problem: 0 = min fe 0 minfx; M x + qg j M x + q 0; x 0g: (23) Again because problem (22) fits the hypotheses of our principal Theorem 2.3, the following result follows immediately.
Reference: [10] <author> O. L. Mangasarian. </author> <title> Simple computable bounds for solutions of linear complementarity problems and linear programs. </title> <journal> Mathematical Programming Study, </journal> <volume> 25 </volume> <pages> 1-12, </pages> <year> 1985. </year>
Reference-contexts: =) inf e 0 x fl n + inf e 0 w fl =) inf e 0 x fl n sup e 0 w fl : (29) By using the above lemma, the inequality (8) and the characterization of bounded solutions of monotone linear complementarity problems and linear programs of <ref> [10] </ref> we can obtain the following upper-bounds on the cardinality of the nonzero elements of solutions by solving a concave maxi mization problem. If the solution set is nondegenerate, then these upper bounds equal the actual cardinality. <p> In case of boundedness of the dual optimal set U 1 which is equivalent to <ref> [10, Theorem 3.1] </ref>: fx j Ax b; x &gt; 0g 6= ;; (32) then the inf of (31) can be replaced by a min. Proof The first inequality of (31) follows from the second implication of Lemma 4.1, and the second inequality follows from inequality (8). <p> When the primal-dual solution set is nondegenerate, then by the first implication of Lemma 4.1, the first inequality can be replaced by an equality. If (32) holds, then by Theorem 3.1 of <ref> [10] </ref>, the optimal dual solution set U 1 is bounded. Hence the continuous function e 0 " ff (A 0 u+c) which is bounded below on bounded compact set U 1 , attains its infimum on U 1 .
Reference: [11] <author> O. L. Mangasarian. </author> <title> Some applications of penalty functions in mathematical programming. </title> <editor> In R. Conti, E. De Giorgi, and F. Giannessi, editors, </editor> <booktitle> Optimization and Related Fields, </booktitle> <pages> pages 307-329. </pages> <publisher> Springer-Verlag, </publisher> <address> Heidelberg, </address> <year> 1986. </year> <note> Lecture Notes in Mathematics 1190. </note>
Reference: [12] <author> O. L. Mangasarian. </author> <title> A simple characterization of solution sets of convex programs. </title> <journal> Operations Research Letters, </journal> <volume> 7(1) </volume> <pages> 21-26, </pages> <year> 1988. </year>
Reference-contexts: Hence the inf in (33) can be replaced by a minimum. - If we make use of the polyhedral characterization of the solution set of a monotone linear complementarity problem <ref> [12, Corollary 2] </ref>, then the solution set S 2 in the above theorem can be explicitly given in terms of any solution point x as follows.
Reference: [13] <author> O. L. Mangasarian. </author> <title> Misclassification minimization. </title> <journal> Journal of Global Optimization, </journal> <volume> 5 </volume> <pages> 309-323, </pages> <year> 1994. </year>
Reference-contexts: This result will also be useful in specifying a finite successive linear approximation algorithm for obtaining a stationary point of the NP-hard minimum-support problem. Related finite penalty parameter results are also given in <ref> [13, 17] </ref>.
Reference: [14] <author> O. L. Mangasarian. </author> <title> Machine learning via polyhedral concave minimization. </title> <editor> In H. Fischer, B. Riedmueller, and S. Schae*er, editors, </editor> <booktitle> Applied Mathematics and Parallel Computing - Festschrift for Klaus Ritter, </booktitle> <pages> pages 175-188. </pages> <publisher> Physica-Verlag A Springer-Verlag Company, </publisher> <address> Hei-delberg, </address> <year> 1996. </year> <note> Available at ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-20.ps.Z. 9 </note>
Reference-contexts: 1 Introduction This work is prompted by applications of mathematical programming to machine learning and other problems where a solution to a mathematical program is desired with as many of its components equal to zero as possible <ref> [14, 3, 15, 2] </ref>. <p> This leads to better "generalization" [22], that is a plane that will discriminate better between new elements of A and B that were not involved in determining the original separating plane <ref> [21, 14, 3] </ref>. We are thus interested in obtaining minimum-support solutions to mathematical programs, that is solutions with as many components equal to zero as possible. <p> Since there is virtually no previous work on minimum-support solutions of mathematical programs, it is one of the primary purposes of this paper to bring to the attention of the mathematical programming community the importance and the richness of such problems from the point of view of machine learning <ref> [14, 3, 2] </ref> and data mining [15, 1]. A related area in these fields that has received recent wide attention is that of support vector machines [20, 7, 4] where instead of suppressing solution components, as we are doing here, fl Mathematical Programming Technical Report 97-05, April 1997. <p> We turn our attention to computational methods for finding minimum-support solutions. The methods are based on the penalty approach formulation described above that leads to minimizing a concave function on a polyhedral set. 3 Computational Algorithms We shall employ the finitely terminating fast successive linearization algorithm (SLA) proposed in <ref> [14] </ref> for differentiable concave function minimization and [16] for nondifferentiable concave function minimization on polyhedral sets. Since the linear complementarity minimum-support penalty reformulation (24) has a nondifferentiable concave objective function, we shall give the more general SLA version of [16].
Reference: [15] <author> O. L. Mangasarian. </author> <title> Mathematical programming in data mining. Data Mining and Knowledge Discovery, </title> <booktitle> 1(2) </booktitle> <pages> 183-201, </pages> <year> 1997. </year> <note> Available at ftp://ftp.cs.wisc.edu/math-prog/tech-reports/96-05.ps.Z. </note>
Reference-contexts: 1 Introduction This work is prompted by applications of mathematical programming to machine learning and other problems where a solution to a mathematical program is desired with as many of its components equal to zero as possible <ref> [14, 3, 15, 2] </ref>. <p> previous work on minimum-support solutions of mathematical programs, it is one of the primary purposes of this paper to bring to the attention of the mathematical programming community the importance and the richness of such problems from the point of view of machine learning [14, 3, 2] and data mining <ref> [15, 1] </ref>. A related area in these fields that has received recent wide attention is that of support vector machines [20, 7, 4] where instead of suppressing solution components, as we are doing here, fl Mathematical Programming Technical Report 97-05, April 1997. Revised March 1998.
Reference: [16] <author> O. L. Mangasarian. </author> <title> Solution of general linear complementarity problems via nondifferen-tiable concave minimization. </title> <journal> Acta Mathematica Vietnamica, </journal> <volume> 22(1) </volume> <pages> 199-205, </pages> <year> 1997. </year> <note> Available at ftp://ftp.cs.wisc.edu/math-prog/tech-reports/96-10.ps.Z. </note>
Reference-contexts: The methods are based on the penalty approach formulation described above that leads to minimizing a concave function on a polyhedral set. 3 Computational Algorithms We shall employ the finitely terminating fast successive linearization algorithm (SLA) proposed in [14] for differentiable concave function minimization and <ref> [16] </ref> for nondifferentiable concave function minimization on polyhedral sets. Since the linear complementarity minimum-support penalty reformulation (24) has a nondifferentiable concave objective function, we shall give the more general SLA version of [16]. We consider the penalty formulation (13) and apply the SLA algorithm of [16] to it. <p> employ the finitely terminating fast successive linearization algorithm (SLA) proposed in [14] for differentiable concave function minimization and <ref> [16] </ref> for nondifferentiable concave function minimization on polyhedral sets. Since the linear complementarity minimum-support penalty reformulation (24) has a nondifferentiable concave objective function, we shall give the more general SLA version of [16]. We consider the penalty formulation (13) and apply the SLA algorithm of [16] to it. This gives the following algorithm. <p> differentiable concave function minimization and <ref> [16] </ref> for nondifferentiable concave function minimization on polyhedral sets. Since the linear complementarity minimum-support penalty reformulation (24) has a nondifferentiable concave objective function, we shall give the more general SLA version of [16]. We consider the penalty formulation (13) and apply the SLA algorithm of [16] to it. This gives the following algorithm. Algorithm 3.1 Successive Linearization Algorithm (SLA) Choose ff &gt; 0; fi &gt; 0 and start with a random x 0 2 R n and y 0 = jx 0 j. <p> Stop if (x i ; y i ) 2 T and @f (x i )(x x i ) + fffi" ffy i 0 (y y i ) = 0 By Theorem 3 of <ref> [16] </ref> we can establish the following finite termination result of the SLA 3.1 at a stationary point satisfying the minimum principle necessary optimality condition.
Reference: [17] <author> O. L. Mangasarian and J.-S. Pang. </author> <title> Exact penalty functions for mathematical programs with linear complementarity constraints. </title> <journal> Optimization, </journal> <volume> 42 </volume> <pages> 1-8, </pages> <year> 1997. </year> <note> Available from: ftp://ftp.cs.wisc.edu/math-prog/tech-reports/96-06.ps.Z. </note>
Reference-contexts: This result will also be useful in specifying a finite successive linear approximation algorithm for obtaining a stationary point of the NP-hard minimum-support problem. Related finite penalty parameter results are also given in <ref> [13, 17] </ref>.
Reference: [18] <author> B. T. Polyak. </author> <title> Introduction to Optimization. Optimization Software, </title> <publisher> Inc., Publications Division, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: The set D (f (x)) of supergradients of f at the point x is nonempty, convex, compact and reduces to the ordinary gradient rf (x), when f is differentiable at x <ref> [18, 19] </ref>.
Reference: [19] <author> R. T. Rockafellar. </author> <title> Convex Analysis. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1970. </year>
Reference-contexts: The set D (f (x)) of supergradients of f at the point x is nonempty, convex, compact and reduces to the ordinary gradient rf (x), when f is differentiable at x <ref> [18, 19] </ref>. <p> This follows from the fact that a concave function bounded below on a polyhedral set not containing lines going to infinity in both directions attains a minimum at a vertex of the polyhedral set <ref> [19, Corollaries 32.3.3 & 32.3.4] </ref>. Hence for ff i ff 0 (fi): = min f (x) + fie 0 (e " ff i y ) x2S inf f (x) + fie 0 jxj fl ; where the last inequality follows from (8). <p> Proof By <ref> [19, Corollary 32.3.3] </ref> problem (1) has a solution because the concave function f is bounded below on the polyhedral set S. Furthermore, by [19, Corollary 32.3.4] it has a vertex solution because S contains no straight lines that go to infinity in both directions. <p> Proof By [19, Corollary 32.3.3] problem (1) has a solution because the concave function f is bounded below on the polyhedral set S. Furthermore, by <ref> [19, Corollary 32.3.4] </ref> it has a vertex solution because S contains no straight lines that go to infinity in both directions.
Reference: [20] <author> V. N. Vapnik. </author> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1995. </year>
Reference-contexts: A related area in these fields that has received recent wide attention is that of support vector machines <ref> [20, 7, 4] </ref> where instead of suppressing solution components, as we are doing here, fl Mathematical Programming Technical Report 97-05, April 1997. Revised March 1998.
Reference: [21] <author> D. H. Wolpert. </author> <title> On the connection between in-sample testing and generalization error. </title> <journal> Complex Systems, </journal> <volume> 6 </volume> <pages> 47-94, </pages> <year> 1992. </year>
Reference-contexts: This leads to better "generalization" [22], that is a plane that will discriminate better between new elements of A and B that were not involved in determining the original separating plane <ref> [21, 14, 3] </ref>. We are thus interested in obtaining minimum-support solutions to mathematical programs, that is solutions with as many components equal to zero as possible.
Reference: [22] <author> D. H. Wolpert, </author> <title> editor. The Mathematics of Generalization, </title> <address> Reading, MA, 1995. </address> <publisher> Addison-Wesley. </publisher> <pages> 10 </pages>
Reference-contexts: This leads to better "generalization" <ref> [22] </ref>, that is a plane that will discriminate better between new elements of A and B that were not involved in determining the original separating plane [21, 14, 3].
References-found: 22

