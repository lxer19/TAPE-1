URL: http://www.cs.huji.ac.il/~citron/hpca.ps.gz
Refering-URL: http://www.cs.huji.ac.il/~citron/index.html
Root-URL: http://www.cs.huji.ac.il
Title: Creating a Wider Bus Using Caching Techniques  
Author: Daniel Citron Larry Rudolph 
Address: Jerusalem 91904 Israel Jerusalem 91904 Israel  
Affiliation: Institute of Computer Science Institute of Computer Science Hebrew University Hebrew University  
Abstract: The effective bandwidth of a bus and external communication ports can be increased by using a variant of data compression techniques that compacts words instead of data streams. The compaction is performed by caching the high order bits into a table and sending the index into the table along with the low order bits. A coherent table at the receiving end expands the word into it original form. Compaction / expansion units can be placed between processor and memory, between processor and local bus, and between devices that access the system bus. Simulations have shown that over 90% of all information transferred can be sent in a single cycle when using a 32 bit processor connected by a 16 bit wide bus to a 32 bit memory module. This is for all forms of data, address, data, and instructions, and when a cache-based processor is used. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Becker, J., A. Park, and M. Farrens, </author> <title> "An Analysis of the Information Content of Address Reference Streams," </title> <booktitle> Proceedings of the 24th Annual Int. Sym. on Microarchitecture, </booktitle> <address> Albuquerque, New Mexico, </address> <month> Nov. </month> <year> 1991, </year> <pages> pp. 19-24. </pages>
Reference-contexts: In order to reduce the number of lines on a bus, researchers have proposed using compaction to reduce the number of address lines. Hammerstrom and Davidson [9] presented methods for computing the information content of memory reference patterns. Far-rens, Park, and their coauthors <ref> [14, 6, 1, 15] </ref> have investigated several compaction schemes for address line reduction. In particular, they cleverly propose [14, 6] to compact the high-order bits of an address into a smaller number of bits representing the index of a cache-like device of the high order bits of recently accessed addresses.
Reference: [2] <editor> Byte, </editor> <volume> Vol. 18, No. 6, </volume> <pages> pp. 92-110, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The compaction technique promises to increase the effective memory bandwidth in high performance computer systems. Although the current crop of microprocessors, e.g. Alpha, Pentium, PowerPC <ref> [5, 2, 3] </ref>, have increased their internal data paths to 64 or 128 bits, and cache lines to 256 bits or more, these advances have introduced a technological mismatch between the microprocessor and its immediate surrounding environment. <p> The spectrum of associativity from direct mapped to 8-way associativity was simulated. Addresses and data are stored separately in all cases. What is paramatized is the method of storing instructions. Simulations were made with a Harvard (split) architecture and a unified architecture. Most processors have on-chip caches <ref> [5, 2, 3, 12, 19] </ref>) which can have a profound impact on the performance of a Bus-Expander. Such a cache was simulated where the size, associativity, line size, architecture and write policy of the cache are paramatized. The traces were taken from several sources.
Reference: [3] <editor> Byte, </editor> <volume> Vol. 18, No. 9, </volume> <pages> pp. 56-90, </pages> <year> August1993. </year>
Reference-contexts: The compaction technique promises to increase the effective memory bandwidth in high performance computer systems. Although the current crop of microprocessors, e.g. Alpha, Pentium, PowerPC <ref> [5, 2, 3] </ref>, have increased their internal data paths to 64 or 128 bits, and cache lines to 256 bits or more, these advances have introduced a technological mismatch between the microprocessor and its immediate surrounding environment. <p> The spectrum of associativity from direct mapped to 8-way associativity was simulated. Addresses and data are stored separately in all cases. What is paramatized is the method of storing instructions. Simulations were made with a Harvard (split) architecture and a unified architecture. Most processors have on-chip caches <ref> [5, 2, 3, 12, 19] </ref>) which can have a profound impact on the performance of a Bus-Expander. Such a cache was simulated where the size, associativity, line size, architecture and write policy of the cache are paramatized. The traces were taken from several sources.
Reference: [4] <author> Denning P.J. </author> , <title> "On Modeling Program Behavior," </title> <booktitle> Proc. Spring Joint Computer Conference, </booktitle> <volume> vol. 40, </volume> <publisher> AFIPS Press, </publisher> <address> Arlington, Va., </address> <pages> pp. 937-944, </pages> <year> 1972. </year>
Reference-contexts: In particular, they cleverly propose [14, 6] to compact the high-order bits of an address into a smaller number of bits representing the index of a cache-like device of the high order bits of recently accessed addresses. Compaction is achieved by exploiting the "Principle of Locality" <ref> [4] </ref>: If an address was referenced by the program there is a good chance that addresses in its immediate area will also be referenced. This means that all the addresses in an immediate area will have the same high-order bits.
Reference: [5] <institution> DEC, "Alpha Architecture Handbook," Digital Equipment Corporation, </institution> <year> 1992. </year>
Reference-contexts: The compaction technique promises to increase the effective memory bandwidth in high performance computer systems. Although the current crop of microprocessors, e.g. Alpha, Pentium, PowerPC <ref> [5, 2, 3] </ref>, have increased their internal data paths to 64 or 128 bits, and cache lines to 256 bits or more, these advances have introduced a technological mismatch between the microprocessor and its immediate surrounding environment. <p> The spectrum of associativity from direct mapped to 8-way associativity was simulated. Addresses and data are stored separately in all cases. What is paramatized is the method of storing instructions. Simulations were made with a Harvard (split) architecture and a unified architecture. Most processors have on-chip caches <ref> [5, 2, 3, 12, 19] </ref>) which can have a profound impact on the performance of a Bus-Expander. Such a cache was simulated where the size, associativity, line size, architecture and write policy of the cache are paramatized. The traces were taken from several sources.
Reference: [6] <author> Farrens M. and Park A. </author> , <title> "Dynamic Base Register Caching: A Technique for Reducing Address Bus Width," </title> <booktitle> Proc. 18th Annual International Symp. on Computer Architecture, </booktitle> <pages> pp. 128-137, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: In order to reduce the number of lines on a bus, researchers have proposed using compaction to reduce the number of address lines. Hammerstrom and Davidson [9] presented methods for computing the information content of memory reference patterns. Far-rens, Park, and their coauthors <ref> [14, 6, 1, 15] </ref> have investigated several compaction schemes for address line reduction. In particular, they cleverly propose [14, 6] to compact the high-order bits of an address into a smaller number of bits representing the index of a cache-like device of the high order bits of recently accessed addresses. <p> Hammerstrom and Davidson [9] presented methods for computing the information content of memory reference patterns. Far-rens, Park, and their coauthors [14, 6, 1, 15] have investigated several compaction schemes for address line reduction. In particular, they cleverly propose <ref> [14, 6] </ref> to compact the high-order bits of an address into a smaller number of bits representing the index of a cache-like device of the high order bits of recently accessed addresses. <p> In a Bus-Expander the low-order bits are sent over the bus without modification and the higher order bits of the value are compacted. The LUT can be a fully associative memory, direct mapped, or something in between. As found in traditional caches [22] and suggested by Farrens and Park <ref> [6] </ref>, it seems best to organize it as a direct mapped cache with a small associative set size per data line, During a compact, the LUT is searched for the high order bits by dividing these bits into two fields, a tag and a key.
Reference: [7] <author> Goodman J.R. </author> , <title> "Using Cache Memory to Re--duce Processor-Memory Traffic,"Proc. </title> <booktitle> 10th Annual Int. Symp. on Computer Architecture, </booktitle> <pages> pp. 124-131, </pages> <year> 1983. </year>
Reference-contexts: We propose to use the technique of Snoopy Caching <ref> [7] </ref>. Snoopy Caching was devised for systems with multiple processors where each device has a memory bank and a cache. The caches must be updated to be consistent with changes in memory on other boards. Thus a scheme for listening on the bus (snooping) and updating ones cache was devised.
Reference: [8] <author> Hamacher V. , Vranesic Z. and Zaky G. </author> , <title> "Computer Organization," second ed. </title> <publisher> McGraw-Hill Book Company, </publisher> <address> Singapore, </address> <year> 1984. </year>
Reference: [9] <author> Hammerstrom, D. and E. Davidson, </author> <title> "Information Content of CPU Memory Referencing Behavior," </title> <booktitle> Proceedings of Fourth Annual Int. Symp. on Computer Architecture, </booktitle> <month> March </month> <year> 1977, </year> <pages> pp. 184-192. </pages>
Reference-contexts: In order to reduce the number of lines on a bus, researchers have proposed using compaction to reduce the number of address lines. Hammerstrom and Davidson <ref> [9] </ref> presented methods for computing the information content of memory reference patterns. Far-rens, Park, and their coauthors [14, 6, 1, 15] have investigated several compaction schemes for address line reduction.
Reference: [10] <author> Hennessy J. L. and Patterson D. A., </author> <title> "Computer Architecture: A Quantitative Approach," </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo CA, </address> <year> 1990. </year>
Reference-contexts: A Bus-Expander with 2 12 entries is comparable with a 8K cache. An on-chip cache can run at the speed of the processor and process a read within one cycle <ref> [10] </ref>. Thus a Bus--Expander would add, at the worst, one processor cycle. If the lookup on the Bus-Expander is short enough it might be included in the first cycle of the information transfer from the processor to the bus.
Reference: [11] <author> IBM, </author> <title> "IBM Technical Reference Personal Computer AT," IBM Personal Computer Hardware Reference Library, </title> <year> 1984. </year>
Reference-contexts: Floating point values too cause problems but by compacting only a few high bits (the exponent), they will cause no performance degradation. There are many needs for compaction of both data and address values. For example, the sixteen bit ISA (Industry Standard Architecture) bus <ref> [11] </ref>, e.g. the AT bus, is still being used and sold with an i80386 or i80486 [12] thirty-two bit processor [16]. Sending any thirty-two bit value over the bus requires two bus cycles. With our compaction techniques this can be reduced to one cycle in most cases. <p> The key, tag, and low order bits are assembled to form the device-word. There need not be a single LUT. Most devices and buses have the same bit width for addresses and data, but there are exceptions like the ISA bus <ref> [11] </ref>. Although it is possible to store address and data values in the same LUT, this requires performing two sequential lookups for a store operation.
Reference: [12] <author> Intel, </author> <title> "Microprocessor and Peripheral Handbook, Volume I Microprocessor," Intel Literature Sales, </title> <address> Santa Clara CA, </address> <year> 1991. </year>
Reference-contexts: There are many needs for compaction of both data and address values. For example, the sixteen bit ISA (Industry Standard Architecture) bus [11], e.g. the AT bus, is still being used and sold with an i80386 or i80486 <ref> [12] </ref> thirty-two bit processor [16]. Sending any thirty-two bit value over the bus requires two bus cycles. With our compaction techniques this can be reduced to one cycle in most cases. <p> The spectrum of associativity from direct mapped to 8-way associativity was simulated. Addresses and data are stored separately in all cases. What is paramatized is the method of storing instructions. Simulations were made with a Harvard (split) architecture and a unified architecture. Most processors have on-chip caches <ref> [5, 2, 3, 12, 19] </ref>) which can have a profound impact on the performance of a Bus-Expander. Such a cache was simulated where the size, associativity, line size, architecture and write policy of the cache are paramatized. The traces were taken from several sources.
Reference: [13] <author> Larus, J.R. , SPIM S20: </author> <title> A MIPS R2000 Simulator, </title> <month> August </month> <year> 1992. </year>
Reference-contexts: Thus consistency between all Bus-Expanders in a system is preserved with no additional bookkeeping and overhead. 3 Experimental Verification To verify the usefulness of the Bus-Expander technique, we performed a series of experiments with an architecturally detailed simulator: SPIM <ref> [13] </ref> a MIPS R2000/R3000 [19] simulator. It takes assembler or executable files and simulates their execution instruction by instruction. Traces of every memory reference were collected. There were several types of information traced, addresses of data or instructions and data composed of instructions and scalar and floating point values.
Reference: [14] <author> Park, A. and M. Farrens, </author> <title> "Address Compression Through Base Register Caching," </title> <booktitle> Proceedings of the 23rd Annual Symp. and Workshop on Microprogramming and Microarchitectures, </booktitle> <address> Or-lando, Florida, </address> <month> Nov. </month> <year> 1990, </year> <pages> pp. 193-199. </pages>
Reference-contexts: In order to reduce the number of lines on a bus, researchers have proposed using compaction to reduce the number of address lines. Hammerstrom and Davidson [9] presented methods for computing the information content of memory reference patterns. Far-rens, Park, and their coauthors <ref> [14, 6, 1, 15] </ref> have investigated several compaction schemes for address line reduction. In particular, they cleverly propose [14, 6] to compact the high-order bits of an address into a smaller number of bits representing the index of a cache-like device of the high order bits of recently accessed addresses. <p> Hammerstrom and Davidson [9] presented methods for computing the information content of memory reference patterns. Far-rens, Park, and their coauthors [14, 6, 1, 15] have investigated several compaction schemes for address line reduction. In particular, they cleverly propose <ref> [14, 6] </ref> to compact the high-order bits of an address into a smaller number of bits representing the index of a cache-like device of the high order bits of recently accessed addresses.
Reference: [15] <author> Park, A., M. Farrens, and G. Tyson, </author> <title> "Modifying VM Hardware to Reduce Address Pin Requirements," </title> <booktitle> Proceedings of the 25th Annual Int. Symp. on Microarchitecture, </booktitle> <address> Portland, Oregon, </address> <month> Dec. </month> <year> 1992, </year> <pages> pp. 210-213. </pages>
Reference-contexts: In order to reduce the number of lines on a bus, researchers have proposed using compaction to reduce the number of address lines. Hammerstrom and Davidson [9] presented methods for computing the information content of memory reference patterns. Far-rens, Park, and their coauthors <ref> [14, 6, 1, 15] </ref> have investigated several compaction schemes for address line reduction. In particular, they cleverly propose [14, 6] to compact the high-order bits of an address into a smaller number of bits representing the index of a cache-like device of the high order bits of recently accessed addresses.
Reference: [16] <author> PC Magazine, </author> <note> pp 31, </note> <month> March 17 </month> <year> 1992. </year>
Reference-contexts: There are many needs for compaction of both data and address values. For example, the sixteen bit ISA (Industry Standard Architecture) bus [11], e.g. the AT bus, is still being used and sold with an i80386 or i80486 [12] thirty-two bit processor <ref> [16] </ref>. Sending any thirty-two bit value over the bus requires two bus cycles. With our compaction techniques this can be reduced to one cycle in most cases.
Reference: [17] <editor> PC Magazine, </editor> <volume> Vol. 12, No. 13, </volume> <pages> pp. 183-200, </pages> <month> July </month> <year> 1993. </year>
Reference: [18] <author> Price W.J. </author> , <title> "A Benchmark Tutorial," </title> <booktitle> IEEE Micro, </booktitle> <pages> pp. 28-43, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Such a cache was simulated where the size, associativity, line size, architecture and write policy of the cache are paramatized. The traces were taken from several sources. Some are standard benchmarks (dhrystone, whetstone, lin-pack <ref> [18] </ref>), others are less standardized (Stanford University integer and floating point benchmarks [18] , Heapsort, Tower of Hanoi, Sieve of Eratosthenes), some are applications taken from a ULTRIX 4.2 operating system (diff, nm, lex, sort, dvips, compact), and the simulator itself SPIM was simulated as well. <p> Such a cache was simulated where the size, associativity, line size, architecture and write policy of the cache are paramatized. The traces were taken from several sources. Some are standard benchmarks (dhrystone, whetstone, lin-pack <ref> [18] </ref>), others are less standardized (Stanford University integer and floating point benchmarks [18] , Heapsort, Tower of Hanoi, Sieve of Eratosthenes), some are applications taken from a ULTRIX 4.2 operating system (diff, nm, lex, sort, dvips, compact), and the simulator itself SPIM was simulated as well.
Reference: [19] <author> Slater, M. (ed.), </author> <title> "A Guide to RISC Microprocessors," </title> <publisher> Academic Press, </publisher> <address> San Diego CA, </address> <year> 1992. </year>
Reference-contexts: Thus consistency between all Bus-Expanders in a system is preserved with no additional bookkeeping and overhead. 3 Experimental Verification To verify the usefulness of the Bus-Expander technique, we performed a series of experiments with an architecturally detailed simulator: SPIM [13] a MIPS R2000/R3000 <ref> [19] </ref> simulator. It takes assembler or executable files and simulates their execution instruction by instruction. Traces of every memory reference were collected. There were several types of information traced, addresses of data or instructions and data composed of instructions and scalar and floating point values. <p> The spectrum of associativity from direct mapped to 8-way associativity was simulated. Addresses and data are stored separately in all cases. What is paramatized is the method of storing instructions. Simulations were made with a Harvard (split) architecture and a unified architecture. Most processors have on-chip caches <ref> [5, 2, 3, 12, 19] </ref>) which can have a profound impact on the performance of a Bus-Expander. Such a cache was simulated where the size, associativity, line size, architecture and write policy of the cache are paramatized. The traces were taken from several sources.
Reference: [20] <author> Rudolph, L. </author> <title> "Exploiting the Low Entropy of Address and Data Values," </title> <type> Technical Report, </type> <institution> Institute of Computer Science, Hebrew University, Jerusalem, Israel, </institution> <year> 1994. </year>
Reference-contexts: To save pins, many chips use bit serial links for status, command, and other debugging information and compaction can speedup the use of these "extra" chip features. There are many more applications, some of which an be found in reference <ref> [20] </ref>. The next section presents a top-down description of the Bus-Expander.
Reference: [21] <author> Smith, A.J. </author> , <title> "Cache Memories," </title> <journal> ACM Computing Surveys, </journal> <volume> 14, no. 3, </volume> <pages> pp. 473-550, </pages> <month> September </month> <year> 1982. </year>
Reference: [22] <author> Stone, H.S. </author> , <title> "High-Performance Computer Architecture," </title> <publisher> Addison-Wesley, </publisher> <address> Reading MA, </address> <year> 1989. </year>
Reference-contexts: In a Bus-Expander the low-order bits are sent over the bus without modification and the higher order bits of the value are compacted. The LUT can be a fully associative memory, direct mapped, or something in between. As found in traditional caches <ref> [22] </ref> and suggested by Farrens and Park [6], it seems best to organize it as a direct mapped cache with a small associative set size per data line, During a compact, the LUT is searched for the high order bits by dividing these bits into two fields, a tag and a <p> Recall that the larger the LUT, the fewer direct bits are sent. 3.2.4 On-chip cache write policy The hit rates described above are based on the assumption that the on-chip cache uses a write-through policy <ref> [22] </ref>. A comparison of a write-through policy with a write-back policy is shown in Figure 9. As in previous simulations the on-chip cache is a 16K byte, split architecture, 2-way associative with a line size of 32 bytes cache. The Bus-Expander has a set size of 4.
References-found: 22

