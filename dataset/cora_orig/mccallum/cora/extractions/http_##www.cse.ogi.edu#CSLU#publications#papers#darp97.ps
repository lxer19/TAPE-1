URL: http://www.cse.ogi.edu/CSLU/publications/papers/darp97.ps
Refering-URL: http://www.cse.ogi.edu/CSLU/publications/publications.html
Root-URL: http://www.cse.ogi.edu
Email: fyan, xintian, johans, coleg@cse.ogi.edu  
Title: EVALUATION SYSTEM  for Spoken Language Understanding  
Author: Yonghong Yan Xintian Wu Johan Schalkwyk Ron Cole 
Address: P.O. Box 91000, Portland, OR 97291-1000  
Affiliation: Center  Oregon Graduate Institute of Science and Technology  
Note: DEVELOPMENT OF CSLU LVCSR: THE 1997 DARPA HUB4  
Abstract: This paper presents the CSLU Broadcast News transcription system used in the DARPA 1997 evaluation. The system was built using the softwares developed for the CSLU LVCSR project started in January 1997. This 25K-word vocabulary system used continuous HMMs for acoustic modeling and the standard backoff trigram as the language model. The search used a single pass decoder with MLLR based adaptation technique. Although on the standard DARPA 20k WSJ task our system obtained 11.6% word error, the 39% error on this year's evaluation suggests there are still many aspects need to be learned for a new comer like us. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Adda, L. Lamel, M. Adda-Decker, and J.L.Gauvain. </author> <booktitle> Language and lexical modeling in the limsi nov96 hub4 system. In Proc. of DARPA Speech Recognition Workshop, </booktitle> <year> 1997. </year>
Reference-contexts: All the filler words were removed from the text and the only context cue used was the sentence begin/end. Transcriptions for the BN acoustic data were copied twice as part of the training data as suggested in <ref> [1] </ref>. The good-turing method was employed to estimate the back-off trigram language model, which resulted in a model with 10M trigrams and 5M bigrams.
Reference: [2] <author> T. Anastasakos, J. McDonough, and J. Makhoul. </author> <title> Speaker adaptive training: a maximum likelihood approach to speaker normalization. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 1043-1046, </pages> <year> 1997. </year>
Reference-contexts: Language modeling, Maximum A Posteriori (MAP) [25, 10, 26] and Speaker Adaptive training (SAT) <ref> [2] </ref> were studied at this period. * From October to November, we received all the training/development data related to Broadcast News (BN) and started to build the eval uation system.
Reference: [3] <author> R. Bakis, S. Chen, P. Gopalakrishnan, S. Maes R. Gopinath, and L. Polymenakos. </author> <title> Transcription of braodcast news shows with the ibm large vocabulary speech recognition system. </title> <booktitle> In Proc. of DARPA Speech Recognition Workshop, </booktitle> <year> 1997. </year>
Reference-contexts: Development of the 1997 evaluation system At the end of September we received all the data related to the Hub4 task (acoustic and language modeling data). Due to the time constraints, all the decisions made for Hub4 specific components are based on the discussions in <ref> [3, 20, 18, 4, 8, 28, 11, 22, 13] </ref> and our understanding of these approaches. We basically adopted BBN's strategy in the 1996 evaluation: One set of acoustic models for all the BN conditions [17]. The system was planned as: 1. Monophone recognition and acoustic wave seg mentation 2.
Reference: [4] <author> C. Che, D. Yuk, S. Chennoukh, and J. Flana-gan. </author> <title> Development of the ru hub4 system. </title> <booktitle> In Proc. of DARPA Speech Recognition Workshop, </booktitle> <year> 1997. </year>
Reference-contexts: Development of the 1997 evaluation system At the end of September we received all the data related to the Hub4 task (acoustic and language modeling data). Due to the time constraints, all the decisions made for Hub4 specific components are based on the discussions in <ref> [3, 20, 18, 4, 8, 28, 11, 22, 13] </ref> and our understanding of these approaches. We basically adopted BBN's strategy in the 1996 evaluation: One set of acoustic models for all the BN conditions [17]. The system was planned as: 1. Monophone recognition and acoustic wave seg mentation 2.
Reference: [5] <author> P. Clarkson and R. Rosenfeld. </author> <title> Statistical language modeling using the cmu-cambridge toolkit. </title> <booktitle> In Proc. of EUROSPEECH 1997, </booktitle> <pages> pages 2707-2710, </pages> <year> 1997. </year>
Reference-contexts: Some of the results are summarized in Table 3.2. System WER FB: BN only 39.1% FB: WSJ SI284+BN 38.4% MLLR: BN 40.6% Table 2: Word Error Rates (WER) for different training methods, without adaptation in decoding 3.3. Language Modeling The CMU-Cambridge language model package V2.0 was used <ref> [19, 5] </ref>. The text materials include the WSJ LM data and BN LM data obtained from LDC. All the filler words were removed from the text and the only context cue used was the sentence begin/end.
Reference: [6] <author> E. Eide and H. Gish. </author> <title> A parametric approach to vocal tract length normalisation. </title> <booktitle> Conference Proceedings of ICASSP'96, </booktitle> <address> I:346-348, </address> <month> May 7-10 </month> <year> 1996. </year> <institution> Atlanta, Georgia. </institution>
Reference-contexts: During this period, we implemented the basic training and decoding software. * From April to July, we worked on the Wall Street Journal (WSJ) 5k task. During this period, we implemented the Maximum Likelihood Linear Regression (MLLR) [16, 9] and Vocal Tract Length Normalization (VTN) <ref> [15, 6] </ref>, and started to play with language models. * From August to September, we worked on the WSJ 20k system and implemented the parallel version of our training and decoding tools.
Reference: [7] <author> F.Kubala, A.Anastasakos, J.Makhoul, L.Nguyen, R.Schwartz, and G.Zavaliagkos. </author> <title> Comparative experiments on large vocabulary speech recognition. </title> <booktitle> Proceedings of the International Conference on Acoustic Speech and Signal Processing, </booktitle> <year> 1994. </year>
Reference-contexts: Our Resource Management system uses the standard Word-Pair grammar, and the rest use trigram models. These results compare favorably with other systems of equal complexity <ref> [24, 14, 7, 27] </ref>. 3. Development of the 1997 evaluation system At the end of September we received all the data related to the Hub4 task (acoustic and language modeling data).
Reference: [8] <author> F.Kubala, H. Jin, S. Matsoukas, L. Nguyen, R. Schwartz, and J. Makhoul. </author> <title> The 1996 bbn byblos hub4 transcription system. </title> <booktitle> In Proc. of DARPA Speech Recognition Workshop, </booktitle> <year> 1997. </year>
Reference-contexts: Development of the 1997 evaluation system At the end of September we received all the data related to the Hub4 task (acoustic and language modeling data). Due to the time constraints, all the decisions made for Hub4 specific components are based on the discussions in <ref> [3, 20, 18, 4, 8, 28, 11, 22, 13] </ref> and our understanding of these approaches. We basically adopted BBN's strategy in the 1996 evaluation: One set of acoustic models for all the BN conditions [17]. The system was planned as: 1. Monophone recognition and acoustic wave seg mentation 2.
Reference: [9] <author> M.J.F. </author> <title> Gales and P.C. Woodland. Variance compensation within the mllr frame work. </title> <type> Technical Report, </type> <address> CUED/F-INFENG/TR 242, </address> <month> February </month> <year> 1996. </year> <institution> Cambridge University, Engineering Department. </institution>
Reference-contexts: During this period, we implemented the basic training and decoding software. * From April to July, we worked on the Wall Street Journal (WSJ) 5k task. During this period, we implemented the Maximum Likelihood Linear Regression (MLLR) <ref> [16, 9] </ref> and Vocal Tract Length Normalization (VTN) [15, 6], and started to play with language models. * From August to September, we worked on the WSJ 20k system and implemented the parallel version of our training and decoding tools.
Reference: [10] <author> J.L. Gauvain and C.H. Lee. </author> <title> Maximum a posteriori estimation for multivariate gaussian mixture observations of markov chains. </title> <journal> IEEE Trans. Speech & Audio Process, </journal> <volume> 2 </volume> <pages> 291-298, </pages> <year> 1994. </year>
Reference-contexts: Language modeling, Maximum A Posteriori (MAP) <ref> [25, 10, 26] </ref> and Speaker Adaptive training (SAT) [2] were studied at this period. * From October to November, we received all the training/development data related to Broadcast News (BN) and started to build the eval uation system.
Reference: [11] <author> G.Cook, D. Kershaw, J. Christie, and T. Robin-son. </author> <title> The 1996 abbot broadcast news system. </title> <booktitle> In Proc. of DARPA Speech Recognition Workshop, </booktitle> <year> 1997. </year>
Reference-contexts: Development of the 1997 evaluation system At the end of September we received all the data related to the Hub4 task (acoustic and language modeling data). Due to the time constraints, all the decisions made for Hub4 specific components are based on the discussions in <ref> [3, 20, 18, 4, 8, 28, 11, 22, 13] </ref> and our understanding of these approaches. We basically adopted BBN's strategy in the 1996 evaluation: One set of acoustic models for all the BN conditions [17]. The system was planned as: 1. Monophone recognition and acoustic wave seg mentation 2.
Reference: [12] <author> H.Jin, F. Kubala, and R. Schwartz. </author> <title> Automatic speaker clustering. </title> <booktitle> In Proc. of DARPA Speech Recognition Workshop, </booktitle> <year> 1997. </year>
Reference-contexts: Segmentation and Clustering We experimented with the commonly adopted strategy: Use the silence segments located by a mono-phone recognizer as boundaries of presegments and then use some distortion measures to cluster these segments. The method proposed by <ref> [12] </ref> was implemented. We experimented with this method on concatenated WSJ utterances and found generally it worked quite well. When we experimented with the actual BN data with monophone recognition generated boundaries, we found the presegmentation generating too many very long (short) utterances.
Reference: [13] <author> J.L.Gauvain, G. Adda, L.F.Lamel, and M.Adda-Decker. </author> <title> Transcribing broadcast news: </title> <booktitle> The limsi nov96 hub4 system. In Proc. of DARPA Speech Recognition Workshop, </booktitle> <year> 1997. </year>
Reference-contexts: Development of the 1997 evaluation system At the end of September we received all the data related to the Hub4 task (acoustic and language modeling data). Due to the time constraints, all the decisions made for Hub4 specific components are based on the discussions in <ref> [3, 20, 18, 4, 8, 28, 11, 22, 13] </ref> and our understanding of these approaches. We basically adopted BBN's strategy in the 1996 evaluation: One set of acoustic models for all the BN conditions [17]. The system was planned as: 1. Monophone recognition and acoustic wave seg mentation 2.
Reference: [14] <author> J.L.Gauvain, L.F.Lamel, G.Adda, and M.Adda--Decker. </author> <title> The limsi continuous speech dictation system: </title> <booktitle> Evaluation on the arpa wall stree journal task. Proceedings of the International Conference on Acoustic Speech and Signal Processing, </booktitle> <year> 1994. </year>
Reference-contexts: Our Resource Management system uses the standard Word-Pair grammar, and the rest use trigram models. These results compare favorably with other systems of equal complexity <ref> [24, 14, 7, 27] </ref>. 3. Development of the 1997 evaluation system At the end of September we received all the data related to the Hub4 task (acoustic and language modeling data).
Reference: [15] <author> L. Lee and R. C. Rose. </author> <title> Speaker normalization using efficient frequency warping procedures. </title> <booktitle> Conference Proceedings of ICASSP'96, </booktitle> <address> I:353-356, </address> <month> May 7-10 </month> <year> 1996. </year> <institution> Atlanta, Georgia. </institution>
Reference-contexts: During this period, we implemented the basic training and decoding software. * From April to July, we worked on the Wall Street Journal (WSJ) 5k task. During this period, we implemented the Maximum Likelihood Linear Regression (MLLR) [16, 9] and Vocal Tract Length Normalization (VTN) <ref> [15, 6] </ref>, and started to play with language models. * From August to September, we worked on the WSJ 20k system and implemented the parallel version of our training and decoding tools.
Reference: [16] <author> C.J. Leggetter. </author> <title> Improved acoustic modelling for hmms using linear transformations. </title> <type> Ph.D thesis, </type> <institution> Cambridge University, </institution> <year> 1995. </year>
Reference-contexts: During this period, we implemented the basic training and decoding software. * From April to July, we worked on the Wall Street Journal (WSJ) 5k task. During this period, we implemented the Maximum Likelihood Linear Regression (MLLR) <ref> [16, 9] </ref> and Vocal Tract Length Normalization (VTN) [15, 6], and started to play with language models. * From August to September, we worked on the WSJ 20k system and implemented the parallel version of our training and decoding tools.
Reference: [17] <author> D. Pallett and J. Fiscus. </author> <title> 1996 preliminary broadcast news benchmark tests. </title> <booktitle> In DARPA 1997 speech recognition workshop, </booktitle> <year> 1997. </year>
Reference-contexts: We basically adopted BBN's strategy in the 1996 evaluation: One set of acoustic models for all the BN conditions <ref> [17] </ref>. The system was planned as: 1. Monophone recognition and acoustic wave seg mentation 2. Segment clustering 3. VTN adaptation based on the decoded mono phone string 4. Decode with the speaker-independent models with VTN 5. Decode with MLLR using the output from the previous step 3.1.
Reference: [18] <author> P. Placeway, S.Chen, M. Eskenazi, U. Jain, V.Parikh, B. Raj, M. Ravishankar, R. Rosen-feld, K. Seymore, M. Siegler, R. Stern, and E. Thayer. </author> <booktitle> The 1996 hub-4 sphinx-3 system. In Proc. of DARPA Speech Recognition Workshop, </booktitle> <year> 1997. </year>
Reference-contexts: Development of the 1997 evaluation system At the end of September we received all the data related to the Hub4 task (acoustic and language modeling data). Due to the time constraints, all the decisions made for Hub4 specific components are based on the discussions in <ref> [3, 20, 18, 4, 8, 28, 11, 22, 13] </ref> and our understanding of these approaches. We basically adopted BBN's strategy in the 1996 evaluation: One set of acoustic models for all the BN conditions [17]. The system was planned as: 1. Monophone recognition and acoustic wave seg mentation 2.
Reference: [19] <author> R. Rosenfeld. </author> <title> The cmu statistical language modeling toolkit, and its use in the 1994 arpa csr evaluation. </title> <booktitle> In ARPA Spoken Language Technology Workshop, </booktitle> <year> 1995. </year>
Reference-contexts: Some of the results are summarized in Table 3.2. System WER FB: BN only 39.1% FB: WSJ SI284+BN 38.4% MLLR: BN 40.6% Table 2: Word Error Rates (WER) for different training methods, without adaptation in decoding 3.3. Language Modeling The CMU-Cambridge language model package V2.0 was used <ref> [19, 5] </ref>. The text materials include the WSJ LM data and BN LM data obtained from LDC. All the filler words were removed from the text and the only context cue used was the sentence begin/end.
Reference: [20] <author> A. Sankar, A. Stolcks, L. Heck, and F. Weng. </author> <title> Sri h4-pe system overview. </title> <booktitle> In Proc. of DARPA Speech Recognition Workshop, </booktitle> <year> 1997. </year>
Reference-contexts: Development of the 1997 evaluation system At the end of September we received all the data related to the Hub4 task (acoustic and language modeling data). Due to the time constraints, all the decisions made for Hub4 specific components are based on the discussions in <ref> [3, 20, 18, 4, 8, 28, 11, 22, 13] </ref> and our understanding of these approaches. We basically adopted BBN's strategy in the 1996 evaluation: One set of acoustic models for all the BN conditions [17]. The system was planned as: 1. Monophone recognition and acoustic wave seg mentation 2.
Reference: [21] <author> R. Schwartz and S. Austin. </author> <title> A comparison of several approximate algorithms for finding multiple (n-best) sentence hypotheses. </title> <booktitle> In ICASSP'91, S10.4, </booktitle> <pages> pages 701-704, </pages> <year> 1991. </year>
Reference-contexts: Our search engine is a single pass decoder which supports word-dependent N -best results <ref> [21] </ref>, high order language models, and cross-word triphone for large vocabulary continuous speech recognition. This is an extension of the token passing algorithm [23]. By decoupling the search and the search space, the tree is re-entered conceptually instead of being copied.
Reference: [22] <author> S. Sekine, A Borthwick, R. Grishman, and S. Katz. </author> <title> Nyu language modeling experiment for 1996 csr evaluation. </title> <booktitle> In Proc. of DARPA Speech Recognition Workshop, </booktitle> <year> 1997. </year>
Reference-contexts: Development of the 1997 evaluation system At the end of September we received all the data related to the Hub4 task (acoustic and language modeling data). Due to the time constraints, all the decisions made for Hub4 specific components are based on the discussions in <ref> [3, 20, 18, 4, 8, 28, 11, 22, 13] </ref> and our understanding of these approaches. We basically adopted BBN's strategy in the 1996 evaluation: One set of acoustic models for all the BN conditions [17]. The system was planned as: 1. Monophone recognition and acoustic wave seg mentation 2.
Reference: [23] <author> S.J.Young, N.H.Russell, and J.H.S.Thornton. </author> <title> Token passing: A simple conceptual model for connected speech recognition systems. </title> <institution> Cambridge University Engineering Department Technical Report, </institution> <month> July 31 </month> <year> 1989. </year>
Reference-contexts: Our search engine is a single pass decoder which supports word-dependent N -best results [21], high order language models, and cross-word triphone for large vocabulary continuous speech recognition. This is an extension of the token passing algorithm <ref> [23] </ref>. By decoupling the search and the search space, the tree is re-entered conceptually instead of being copied. Cross-word triphone decoding is achieved by tagging tokens differently which are being passed along the same tree nodes.
Reference: [24] <author> S.J.Young and P.C.Woodland. </author> <title> Tree-based state-tying for high accuracy acoustic modeling. </title> <booktitle> Proc Human Language Technology Workshop, </booktitle> <pages> pages pp307-312, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: The CSLU Hub4-system is based on continuous HMMs, with a 25k-word vocabulary. WSJ SI-284 and BN training data were used for acoustic training. The decision tree-based state clustering algorithm <ref> [24] </ref> was used to cluster the phonetic contexts, which resulted in 5300 distinct states. The system was bootstrapped from our WSJ-20K system. The standard forward-backward algorithm was used for model estimation on the combined data set. The resulting cross-word triphone system has 12 Gaussians per state. <p> Our Resource Management system uses the standard Word-Pair grammar, and the rest use trigram models. These results compare favorably with other systems of equal complexity <ref> [24, 14, 7, 27] </ref>. 3. Development of the 1997 evaluation system At the end of September we received all the data related to the Hub4 task (acoustic and language modeling data).
Reference: [25] <author> R.M. Stern and M.J. Lasry. </author> <title> Dynamic speaker adaptation for isolated letter recognition using map estimation. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 734-737, </pages> <year> 1983. </year>
Reference-contexts: Language modeling, Maximum A Posteriori (MAP) <ref> [25, 10, 26] </ref> and Speaker Adaptive training (SAT) [2] were studied at this period. * From October to November, we received all the training/development data related to Broadcast News (BN) and started to build the eval uation system.
Reference: [26] <author> E. Thelen, X. Aubert, and P. Beyerlein. </author> <title> Speaker adaptation in the philips system for large vocabulary continuous speech recognition. </title> <booktitle> Conference Proceedings of ICASSP'97, </booktitle> <pages> pages 1035-1038, </pages> <address> April 21-24 1997. Munich, Germany. </address>
Reference-contexts: Language modeling, Maximum A Posteriori (MAP) <ref> [25, 10, 26] </ref> and Speaker Adaptive training (SAT) [2] were studied at this period. * From October to November, we received all the training/development data related to Broadcast News (BN) and started to build the eval uation system.
Reference: [27] <author> V.Digalakis and H.Murveit. Genones: </author> <title> Optimizing the degree of mixture tying in a large vocabulary hidden markov model based speech recog-nizer. </title> <booktitle> Proceedings of the International Conference on Acoustic Speech and Signal Processing, </booktitle> <year> 1994. </year>
Reference-contexts: Our Resource Management system uses the standard Word-Pair grammar, and the rest use trigram models. These results compare favorably with other systems of equal complexity <ref> [24, 14, 7, 27] </ref>. 3. Development of the 1997 evaluation system At the end of September we received all the data related to the Hub4 task (acoustic and language modeling data).
Reference: [28] <author> P. Woodland, M. Gales, D. Pye, and S. Young. </author> <title> Broadcast news transcription using htk. </title> <booktitle> In Proc. of DARPA Speech Recognition Workshop, </booktitle> <year> 1997. </year>
Reference-contexts: Development of the 1997 evaluation system At the end of September we received all the data related to the Hub4 task (acoustic and language modeling data). Due to the time constraints, all the decisions made for Hub4 specific components are based on the discussions in <ref> [3, 20, 18, 4, 8, 28, 11, 22, 13] </ref> and our understanding of these approaches. We basically adopted BBN's strategy in the 1996 evaluation: One set of acoustic models for all the BN conditions [17]. The system was planned as: 1. Monophone recognition and acoustic wave seg mentation 2.
References-found: 28

