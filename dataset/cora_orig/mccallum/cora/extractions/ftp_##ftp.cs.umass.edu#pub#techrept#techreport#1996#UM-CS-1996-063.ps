URL: ftp://ftp.cs.umass.edu/pub/techrept/techreport/1996/UM-CS-1996-063.ps
Refering-URL: http://laser.cs.umass.edu/abstracts/96-063.html
Root-URL: 
Title: Improving the Quality of Software Quality Determination Processes  
Author: Leon J. Osterweil 
Keyword: software process, software quality, process improvement, integrated software testing and analysis  
Address: Amherst, MA 01003 USA  
Affiliation: Department of Computer Science University of Massachusetts  
Abstract: This paper suggests a systematic, orderly, process-based approach to stating software quality objectives and knowing if and when they have been achieved. We suggest that quality in software is a complex, multifaceted array of characteristics, and that it is important to establish specific objectives along various software quality dimensions as requirements for software quality assurance determination processes. We propose that process technology be used to design, code, execute, evaluate, and migrate processes that are demonstrably effective in achieving required software product quality objectives. Recently there have been numerous highly visible efforts to codify the assessment of software processes, and to use assessment results to improve them. In this paper we argue that these efforts function as testplans for software processes. We borrow some of the notions proposed in these efforts, and indicate how they can be used to construct a discipline of measuring and evaluating how well processes can be expected to deliver specific knowledge about software product qualities. We look towards the gradual, but eventual, establishment of an orderly discipline of software quality demonstration process development that should ultimately support a marketplace in which definitive knowledge about the nature of software products can be bought and sold. 
Abstract-found: 1
Intro-found: 1
Reference: [AGB + 77] <author> A.L. Ambler, D.I. Good, J.C. Browne, W.F. Burger, R.M. Cohen, C.G. Hoch, and R.E. Wells. </author> <title> Gypsy: A Language for Specification and Implementation of Verifiable Programs. </title> <booktitle> In Proceedings of ACM Conference on Language Design for Reliable Software, </booktitle> <pages> pages 1-10, </pages> <month> March </month> <year> 1977. </year> <note> Published as SIGPLAN Notices, Vol.12,No.3. </note>
Reference-contexts: Experience suggests that these models help analysts study such qualities as safety and robustness, but are less helpful in studying functional properties. Formal verification <ref> [AGB + 77, Bjo87] </ref> uses such formalisms as predicate calculus to represent program functional behaviors. Inference techniques such as symbolic execution [Kin75, Cla76] then develop models of program functionality that are then compared to desired functional behavior specifications to determine the presence or absence of functional faults.
Reference: [BFG93] <author> Sergio Bandinelli, Alfonso Fuggetta, and Sandro Grigolli. </author> <title> Process Modeling in-the-large with SLANG. </title> <booktitle> In Proceedings of the Second International Conference on the Software Process, </booktitle> <address> Berlin, Germany, </address> <pages> pages 75-83, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Thus, quality software processes must also consist of non-code artifacts and demonstrations of consistency as well. Indeed, much software process technology focuses on using modeling and design <ref> [MA94, BFG93, ea92] </ref>. to create blueprints for process code implementation. But it is requirements specifications that are perhaps the key type of software artifact. Requirements specify what characteristics completed software must have, and are used to make decisions about how product software is to be architected and designed.
Reference: [Bjo87] <author> D. </author> <title> Bjorner. </title> <booktitle> On the Use of Formal Methods in Software Development. In Proceedings of the Ninth International Conference on Software Engineering, </booktitle> <pages> pages 17-29, </pages> <month> March </month> <year> 1987. </year> <note> Published by IEEE Computer Society Press. </note>
Reference-contexts: Experience suggests that these models help analysts study such qualities as safety and robustness, but are less helpful in studying functional properties. Formal verification <ref> [AGB + 77, Bjo87] </ref> uses such formalisms as predicate calculus to represent program functional behaviors. Inference techniques such as symbolic execution [Kin75, Cla76] then develop models of program functionality that are then compared to desired functional behavior specifications to determine the presence or absence of functional faults.
Reference: [Cla76] <author> Lori A. Clarke. </author> <title> A System to Generate Test Data and Symbolically Execute Programs. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-2(3):215-222, </volume> <month> September </month> <year> 1976. </year>
Reference-contexts: Experience suggests that these models help analysts study such qualities as safety and robustness, but are less helpful in studying functional properties. Formal verification [AGB + 77, Bjo87] uses such formalisms as predicate calculus to represent program functional behaviors. Inference techniques such as symbolic execution <ref> [Kin75, Cla76] </ref> then develop models of program functionality that are then compared to desired functional behavior specifications to determine the presence or absence of functional faults.
Reference: [DC94] <author> Matthew Dwyer and Lori Clarke. </author> <title> Data Flow Analysis for Verifying Properties of Concurrent Programs,. </title> <booktitle> In ACM SIGSOFT'94 Software Engineering Notes, Proceedings of the Second ACM Sigsoft Symposium on Foundations of Software Engineering 19(5), </booktitle> <pages> pages 62-75, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: More powerful static analyzers build increasingly sophisticated program models, and employ increasingly powerful mathematics to demonstrate the absence of wider classes of faults. Thus dataflow analyzers <ref> [OF76, DC94] </ref> construct annotated flowgraphs to represent program execution behaviors, and then compare them to regular expressions that describe sequences of events that represent desired behaviors. Experience suggests that these models help analysts study such qualities as safety and robustness, but are less helpful in studying functional properties.
Reference: [ea92] <author> Richard J. Mayer et al. </author> <title> IDEF Family of Methods for Concurrent Engineering and Business Re engineering Applications. </title> <type> Technical report, </type> <institution> Knowledge Based Systems, Inc., </institution> <year> 1992. </year>
Reference-contexts: Thus, quality software processes must also consist of non-code artifacts and demonstrations of consistency as well. Indeed, much software process technology focuses on using modeling and design <ref> [MA94, BFG93, ea92] </ref>. to create blueprints for process code implementation. But it is requirements specifications that are perhaps the key type of software artifact. Requirements specify what characteristics completed software must have, and are used to make decisions about how product software is to be architected and designed.
Reference: [How78] <author> W.E. Howden. </author> <title> Introduction to the Theory of Testing. </title> <editor> In E. Miller and W.E. Howden, editors, </editor> <booktitle> Tutorial: Software Testing and Validation Techniques, </booktitle> <pages> pages 16-19. </pages> <publisher> IEEE, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: They also fail to concentrate on implementation structures that may be error-prone. Thus a second major test data selection approach is so-called white box testing, in which test cases are chosen to assure that implementation structures are thoroughly exercised <ref> [How78, RT86] </ref>. Some combination of these approaches seems indicated. 2.2 Static Analysis. Static analysis complements dynamic testing in its ability to show the absence of certain classes of faults. It has the additional advantage of not requiring execution of the program, and hence the selection of test data sets.
Reference: [How87] <author> W.E. Howden. </author> <title> Functional Program Testing and Analysis. </title> <booktitle> McGraw-Hill Series in Software Engi neering and Technology, </booktitle> <year> 1987. </year>
Reference-contexts: As determining the absence of faults becomes the key goal, dynamic testing becomes increasingly inappropriate. When dynamic testing is used to show the absence of faults it is essential to select test data sets that thoroughly exercise the program. An extensive literature documents many approaches to selecting test data <ref> [How87] </ref>. Some emphasize systematic sampling of the program's input data space (black box, or requirements-based, testing) (eg. see [RAO92]). These approaches tend to emphasize modes in which actual users tend to use the program.
Reference: [Hum88] <author> Watts S. Humphrey. </author> <title> Characterizing the Software Process: A Maturity Framework. </title> <journal> IEEE Soft ware, </journal> <pages> pages 73-79, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: The most active and visible work in software process is that centering on the Capability Maturity Model (CMM) at the US Department of Defense Software Engineering Institute (SEI) at Carnegie Mellon University under the direction of Watts Humphrey <ref> [Hum88, Hum89] </ref>. The CMM is a set of five normative models of software development, augmented by elaborate evaluation instruments and institutions that help determine which of the five models most closely describes the process actually used by a software development organization.
Reference: [Hum89] <author> Watts S. Humphrey. </author> <title> Managing the Software Process. SEI Series in Software Engineering. </title> <publisher> Addison-Wesley, </publisher> <month> August </month> <year> 1989. </year>
Reference-contexts: The most active and visible work in software process is that centering on the Capability Maturity Model (CMM) at the US Department of Defense Software Engineering Institute (SEI) at Carnegie Mellon University under the direction of Watts Humphrey <ref> [Hum88, Hum89] </ref>. The CMM is a set of five normative models of software development, augmented by elaborate evaluation instruments and institutions that help determine which of the five models most closely describes the process actually used by a software development organization.
Reference: [Kat89] <author> Takuya Katayama. </author> <title> A Hierarchical and Functional Software Process Description and its Enaction. </title> <booktitle> In Proceedings of the Eleventh International Conference of Software Engineering, </booktitle> <pages> pages 343-353, </pages> <year> 1989. </year>
Reference-contexts: Other compelling parallels are put forth in [Ost87, Ost86]. Our conclusion is that software processes can and should be defined precisely and rigorously using pro whose execution results in the development of an application software product. gramming languages. Experimentation with a range of process programming languages <ref> [Sut95, KBS90, Kat89] </ref>, supports this premise.
Reference: [KBS90] <author> Gail E. Kaiser, Naser S. Barghouti, and Michael H. Sokolsky. </author> <title> Experience with Process Modelling in the MARVEL Software Development Environment Kernel. </title> <editor> In Bruce Shriver, editor, </editor> <booktitle> Proceedings of the Twenty-Third Annual Hawaii International Conference on System Sciences, Kona, Hawaii, </booktitle> <volume> volume II, </volume> <pages> pages 131-140, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Other compelling parallels are put forth in [Ost87, Ost86]. Our conclusion is that software processes can and should be defined precisely and rigorously using pro whose execution results in the development of an application software product. gramming languages. Experimentation with a range of process programming languages <ref> [Sut95, KBS90, Kat89] </ref>, supports this premise.
Reference: [Kin75] <author> James C. King. </author> <title> A New Approach to Program Testing. </title> <booktitle> In Proceedings of the International Con ference on Reliable Software, </booktitle> <pages> pages 228-233, </pages> <year> 1975. </year>
Reference-contexts: Experience suggests that these models help analysts study such qualities as safety and robustness, but are less helpful in studying functional properties. Formal verification [AGB + 77, Bjo87] uses such formalisms as predicate calculus to represent program functional behaviors. Inference techniques such as symbolic execution <ref> [Kin75, Cla76] </ref> then develop models of program functionality that are then compared to desired functional behavior specifications to determine the presence or absence of functional faults.
Reference: [MA94] <author> Carlo Montangero and Vincenzo Ambriola. OIKOS: </author> <title> Constructing Process-Centered SDEs. </title> <editor> In Anthony Finkelstein, Jeff Kramer, and Bashar Nuseibeh, editors, </editor> <booktitle> Software Process Modelling and Technology, chapter 6, </booktitle> <pages> pages 33-70. </pages> <publisher> John Wiley & Sons Inc., </publisher> <address> Taunton, Somerset, England, </address> <year> 1994. </year>
Reference-contexts: Thus, quality software processes must also consist of non-code artifacts and demonstrations of consistency as well. Indeed, much software process technology focuses on using modeling and design <ref> [MA94, BFG93, ea92] </ref>. to create blueprints for process code implementation. But it is requirements specifications that are perhaps the key type of software artifact. Requirements specify what characteristics completed software must have, and are used to make decisions about how product software is to be architected and designed.
Reference: [OF76] <author> Leon J. Osterweil and L.D. Fosdick. </author> <title> DAVE | A Validation, Error Detection, and Documentation System for Fortran Programs. </title> <journal> Software Practice and Experience, </journal> <volume> 6(4) </volume> <pages> 473-486, </pages> <month> October </month> <year> 1976. </year>
Reference-contexts: More powerful static analyzers build increasingly sophisticated program models, and employ increasingly powerful mathematics to demonstrate the absence of wider classes of faults. Thus dataflow analyzers <ref> [OF76, DC94] </ref> construct annotated flowgraphs to represent program execution behaviors, and then compare them to regular expressions that describe sequences of events that represent desired behaviors. Experience suggests that these models help analysts study such qualities as safety and robustness, but are less helpful in studying functional properties.
Reference: [Ost86] <author> Leon J. Osterweil. </author> <title> A Process-Object Centered View of Software Environment Architecture. </title> <editor> In R. Conradi, Didriksen T, and D. Wanvik, editors, </editor> <booktitle> Advanced Programming Environments, </booktitle> <pages> pages 156-174, </pages> <address> Trondheim, 1986. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Figure 1 suggests these parallels diagrammatically. Other compelling parallels are put forth in <ref> [Ost87, Ost86] </ref>. Our conclusion is that software processes can and should be defined precisely and rigorously using pro whose execution results in the development of an application software product. gramming languages. Experimentation with a range of process programming languages [Sut95, KBS90, Kat89], supports this premise.
Reference: [Ost87] <author> Leon J. Osterweil. </author> <title> Software Processes are Software Too. </title> <booktitle> In Proceedings of the Ninth International Conference of Software Engineering, </booktitle> <pages> pages 2-13, </pages> <address> Monterey CA, </address> <month> March </month> <year> 1987. </year>
Reference-contexts: The key to seeing how software process rigor and formality effects improved software product quality seems to us to be an appreciation of the nature of software processes as software themselves. Earlier <ref> [Ost87] </ref> we suggested that processes and applications share many key characteristics. Software processes aim to develop a complex information aggregate (the completed software product), much as application software aims to develop complex information structures. <p> Figure 1 suggests these parallels diagrammatically. Other compelling parallels are put forth in <ref> [Ost87, Ost86] </ref>. Our conclusion is that software processes can and should be defined precisely and rigorously using pro whose execution results in the development of an application software product. gramming languages. Experimentation with a range of process programming languages [Sut95, KBS90, Kat89], supports this premise.
Reference: [RAO92] <author> Debra J. Richardson, Stephanie Leif Aha, and T. Owen O'Malley. </author> <title> Specification-based Test Oracles for Reactive Systems. </title> <booktitle> In Proceedings of the Fourteenth International Conference on Software Engineering, </booktitle> <address> Melbourne, Australia, </address> <pages> pages 105-118, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: An extensive literature documents many approaches to selecting test data [How87]. Some emphasize systematic sampling of the program's input data space (black box, or requirements-based, testing) (eg. see <ref> [RAO92] </ref>). These approaches tend to emphasize modes in which actual users tend to use the program. Their weakness is that they pay less attention to less frequently-used modes, such as failure recovery, that may still be quite important. They also fail to concentrate on implementation structures that may be error-prone.
Reference: [RT86] <author> Debra J. Richardson and M.C. Thompson. </author> <title> A Formal Framework for Test Data Selection Criteria. </title> <type> Technical Report 86-56, </type> <institution> Computer and Information Science Department, University of Massachusetts at Amherst, </institution> <address> Amherst MA 01003, </address> <month> November </month> <year> 1986. </year>
Reference-contexts: They also fail to concentrate on implementation structures that may be error-prone. Thus a second major test data selection approach is so-called white box testing, in which test cases are chosen to assure that implementation structures are thoroughly exercised <ref> [How78, RT86] </ref>. Some combination of these approaches seems indicated. 2.2 Static Analysis. Static analysis complements dynamic testing in its ability to show the absence of certain classes of faults. It has the additional advantage of not requiring execution of the program, and hence the selection of test data sets.
Reference: [Ryd74] <author> B.G. Ryder. </author> <title> The PFORT Verifier. </title> <journal> Software | Practice and Experience, </journal> <volume> 4 </volume> <pages> 359-378, </pages> <year> 1974. </year>
Reference-contexts: The most familiar static analyzers are incorporated into compilers. Compiler syntax analyzers build and examine parse trees and determine the presence or absence of syntactic faults. More ambitious compilers, and similar standalone analysis tools, determine the presence or absence of certain semantic faults (eg. the PFORT Fortran Analyzer <ref> [Ryd74] </ref>, and the Lint C analyzer). More powerful static analyzers build increasingly sophisticated program models, and employ increasingly powerful mathematics to demonstrate the absence of wider classes of faults.
Reference: [Sut95] <author> Sutton, Jr., Stanley M. and Heimbigner, Dennis and Osterweil, Leon J. APPL/A: </author> <title> A Language for Software-Process Programming. </title> <journal> ACM Transactions on Software Engineering and Methodology, </journal> <volume> 4(3) </volume> <pages> 221-286, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: Other compelling parallels are put forth in [Ost87, Ost86]. Our conclusion is that software processes can and should be defined precisely and rigorously using pro whose execution results in the development of an application software product. gramming languages. Experimentation with a range of process programming languages <ref> [Sut95, KBS90, Kat89] </ref>, supports this premise.
References-found: 21

