URL: file://ftp.cs.ucsd.edu/pub/baden/tr/GA_ipps96.ps.gz
Refering-URL: http://www.cs.ucsd.edu/groups/hpcl/scg/tr.html
Root-URL: http://www.cs.ucsd.edu
Email: wehart@cs.sandia.gov  fbaden,rikg@cs.ucsd.edu  skohn@cs.ucsd.edu  
Title: Analysis of the Numerical Effects of Parallelism on a Parallel Genetic Algorithm  
Author: William E. Hart P. O. Scott Baden Richard K. Belew Scott Kohn 
Address: Box 5800, MS 1110 Albuquerque, NM 87185  U.C. San Diego La Jolla, CA 92093  U.C. San Diego La Jolla, CA 92093  
Affiliation: Sandia National Labs  Computer Science Eng'g. Dept.  Dept. Chem. Biochem.  
Abstract: This paper examines the effects of relaxed synchronization on both the numerical and parallel efficiency of parallel genetic algorithms (GAs). We describe a coarse-grain geographically structured parallel genetic algorithm. Our experiments provide preliminary evidence that asynchronous versions of these algorithms have a lower run time than synchronous GAs. Our analysis shows that this improvement is due to (1) decreased synchronization costs and (2) high numerical efficiency (e.g. fewer function evaluations) for the asynchronous GAs. This analysis includes a critique of the utility of traditional parallel performance measures for parallel GAs. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. C. Belding. </author> <title> The distributed genetic algorithm revisited. </title> <editor> In L. Eshelman, editor, </editor> <booktitle> Proceedings of the Sixth Intl. Conf. on Genetic Algorithms, </booktitle> <pages> pages 114-121, </pages> <address> San Mateo, CA, 1995. </address> <publisher> Morgan-Kaufmann. </publisher>
Reference-contexts: Given our critique of traditional parallel performance measures, we expect that previous analyses of parallel GAs will need to be reconsidered. For example, claims of su-perlinear speedup for parallel GAs <ref> [1, 12, 13] </ref> have used speedup measures that do not directly relate the the computation performed on p processors and the computation performed on p 0 processors. Consequently, the meaning of this type of superlinear speedup is not clear.
Reference: [2] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Extending HPF for advanced data parallel applications. </title> <type> Report 94-34, </type> <institution> ICASE, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Communication of grid elements is performed to update the neighborhoods of individuals whose neighborhoods span two (or more) processors. 3 Methods 3.1 Parallel Design of Coarse-Grain GSGAs Coarse grain GSGAs use a toroidal, two-dimensional population grid that is distributed across p processors. We examine GSGAs using HPF-style block decompositions <ref> [2] </ref>. Communication between processors is required to (1) perform selection and recombination, and (2) check for termination signals. Because the grid is toroidal, periodic boundary conditions are required. Hence, every processor communicates with the same number of processors.
Reference: [3] <author> D. Chazan and W. L. Miranker. </author> <title> Chaotic relaxation. </title> <journal> Lin. Algebra and Applic, </journal> <volume> 2 </volume> <pages> 199-222, </pages> <year> 1969. </year>
Reference-contexts: Synchronized GSGAs guarantee that all border regions for all of the processors have been communicated before any of the processors can proceed in the next iteration. By comparison, the asynchronous algorithm does not require that all border regions be filled prior to starting the next iteration <ref> [3] </ref>. Asynchronous GSGAs will probably have a higher utilization of the parallel hardware, but processors may frequently be using border regions that are inconsistent with the state of the neighboring processors. 3.2 Experimental Design The GSGAs used in our experiments are similar to the those described by Hart [8].
Reference: [4] <author> R. J. Collins and D. R. Jefferson. </author> <title> Selection in massively parallel genetic algorithms. </title> <booktitle> In Proc of the fourth Intl Conf on Genetic Algorithms, </booktitle> <pages> pages 249-256, </pages> <year> 1991. </year>
Reference-contexts: Unlike most other optimization procedures, GAs maintain a population of individuals (set of solutions) that are competitively selected to generate new candidates for the global optima. Parallel GAs have been developed for a variety of parallel architectures <ref> [4, 7, 14] </ref>. These GAs have been used to solve a variety of difficult optimization problems, and the algorithmic modifications needed to efficiently parallelize GAs have provided insight into the dynamics of GAs' search [6]. Several arguments are commonly used to justify parallelism in GAs. <p> Parallel GSGAs are typically implemented as fine grain algorithms in which each processor is responsible for evaluating and processing a single individual. Among fine grain GSGAs, two classes of algorithms can be distinguished: GSGAs designed for MIMD architectures [7] and GSGAs designed for SIMD architectures <ref> [4, 11, 14] </ref>. The method of structuring the competitive selection differs between these two classes based on differences in the hardware that are exploited by these algorithms to achieve high utilization. <p> For example, the most common method of structuring the competitive selection for SIMD fine grain GSGAs uses a toroidal grid like the one in Figure 3 <ref> [4, 11, 14] </ref>. Every individual in the population is assigned to a location on the grid in an arbitrary manner. The grid is used to determine subsets of solutions that compete for a location on the grid.
Reference: [5] <author> D. Goldberg. </author> <title> Genetic Algorithms in Search, Optimization, and Machine Learning. </title> <publisher> Addison-Wesley Publishing Co., Inc., </publisher> <year> 1989. </year>
Reference-contexts: 1. Introduction Genetic algorithms (GAs) are stochastic search algorithms that have been successfully applied to a variety of optimization problems <ref> [5] </ref>. Unlike most other optimization procedures, GAs maintain a population of individuals (set of solutions) that are competitively selected to generate new candidates for the global optima. Parallel GAs have been developed for a variety of parallel architectures [4, 7, 14].
Reference: [6] <author> V. S. Gordon and D. Whitley. </author> <title> Serial and parallel genetic algorithms as function optimizers. </title> <editor> In S. Forrest, editor, </editor> <booktitle> Proc of the Fifth Intl Conf on Genetic Algorithms, </booktitle> <pages> pages 177-183, </pages> <address> San Mateo, CA, 1993. </address> <publisher> Morgan-Kaufmann. </publisher>
Reference-contexts: Parallel GAs have been developed for a variety of parallel architectures [4, 7, 14]. These GAs have been used to solve a variety of difficult optimization problems, and the algorithmic modifications needed to efficiently parallelize GAs have provided insight into the dynamics of GAs' search <ref> [6] </ref>. Several arguments are commonly used to justify parallelism in GAs. First, parallelism can reduce the cost of function evaluations, which may dwarf other costs in the fl Scott Baden and Scott Kohn were supported by ONR contract N00014-93-1-0152. <p> A consequence of this localization is that parallel GAs often reliably converge to optimal solutions. The last two arguments for parallelism in GAs concern the observed superior numerical performance of these GAs. Gordon and Whitley <ref> [6] </ref> make a similar observation and argue that the algorithmic structure of parallel GAs is of interest, independent from their implementation on a particular piece of hardware. They experimentally compared the performance of several parallel GAs, all of which were emulated on a sequential architecture. <p> We used a minimal neighborhood that included the two immediate neighbors along each dimension of the population grid (see Figure 3). The crossover rate was 0.8 and the mutation rate was 0.01. Rank selection was used to competitively select individuals within each neighborhood. Following Gordon and Whit-ley <ref> [6] </ref>, we use the best individual in the neighborhood as one of the parents when performing crossover. Additional checks were added to avoid repeating a function evaluation on individuals that were not modified by the crossover or mutation operations. Communication in our parallel GSGAs was coded using LPARX [10].
Reference: [7] <author> M. Gorges-Schleuter. </author> <title> Explicit parallelism of genetic algorithms through population structures. </title> <booktitle> In Parallel Problem Solving from Nature, </booktitle> <pages> pages 150-159, </pages> <year> 1990. </year>
Reference-contexts: Unlike most other optimization procedures, GAs maintain a population of individuals (set of solutions) that are competitively selected to generate new candidates for the global optima. Parallel GAs have been developed for a variety of parallel architectures <ref> [4, 7, 14] </ref>. These GAs have been used to solve a variety of difficult optimization problems, and the algorithmic modifications needed to efficiently parallelize GAs have provided insight into the dynamics of GAs' search [6]. Several arguments are commonly used to justify parallelism in GAs. <p> An example of panmictic GAs are those that use proportional selection. Parallel GSGAs are typically implemented as fine grain algorithms in which each processor is responsible for evaluating and processing a single individual. Among fine grain GSGAs, two classes of algorithms can be distinguished: GSGAs designed for MIMD architectures <ref> [7] </ref> and GSGAs designed for SIMD architectures [4, 11, 14]. The method of structuring the competitive selection differs between these two classes based on differences in the hardware that are exploited by these algorithms to achieve high utilization.
Reference: [8] <author> W. E. Hart. </author> <title> Adaptive Global Optimization with Local Search. </title> <type> PhD thesis, </type> <institution> University of California, </institution> <address> San Diego, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: The dashed lines adjacent to point B illustrate the toroidal nature of neighborhoods on GAs to define population subsets. this grid. We call a neighborhood like that shown in Figure 3 a NEWS neighborhood, since it only uses neighbors to the North, East, West and South. Recently, Hart <ref> [8] </ref> has described a coarse grain design for parallel GSGAs. Like SIMD GSGAs, this parallel GA uses a two-dimensional toroidal grid that is distributed across the set of available processors. Thus each processor typically processes a set of solutions that is located on a subgrid of the entire grid. <p> Asynchronous GSGAs will probably have a higher utilization of the parallel hardware, but processors may frequently be using border regions that are inconsistent with the state of the neighboring processors. 3.2 Experimental Design The GSGAs used in our experiments are similar to the those described by Hart <ref> [8] </ref>. Mutation was performed by adding a value from a normally distributed random variable to a coordinate of a solution. We used a minimal neighborhood that included the two immediate neighbors along each dimension of the population grid (see Figure 3). <p> The angles ff represent the bond angles for a molecule, which can be used to calculate the coordinates x i and y i (see Hart <ref> [8] </ref>). A molecule with 37 atoms was used in our experiments, which requires 35 bond angles to specify its conformational space. The optimal conformation has an energy of approximately 98:3. The GSGAs were run on Sandia National Laboratories' 1824 node Intel Paragon under the SUNMOS operating system.
Reference: [9] <author> R. Judson, M. Colvin, J. Meza, A. Huffer, and D. Gutierrez. </author> <title> Do intelligent configuration search techniques outperform random search for large molecules? Intl J Quantum Chem, </title> <address> pages 277-290, </address> <year> 1992. </year>
Reference-contexts: LPARX provides distributed 2D grid classes that handle block copies and global synchronization. LPARX was extended to create labeled block copies that are used to perform asynchronous communication. The function that is optimized in our experiments is the two-dimensional molecular conformation problem described by Judson et al. <ref> [9] </ref>. This problem concerns a molecule composed of a chain of identical atoms that are connected with rigid rods of length one. The energy of the molecule is modeled van der Walls forces.
Reference: [10] <author> S. R. Kohn and S. B. Baden. </author> <title> A robust parallel programming model for dynamic non-uniform scientific computations. </title> <booktitle> In Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: Additional checks were added to avoid repeating a function evaluation on individuals that were not modified by the crossover or mutation operations. Communication in our parallel GSGAs was coded using LPARX <ref> [10] </ref>. LPARX provides distributed 2D grid classes that handle block copies and global synchronization. LPARX was extended to create labeled block copies that are used to perform asynchronous communication. The function that is optimized in our experiments is the two-dimensional molecular conformation problem described by Judson et al. [9].
Reference: [11] <author> J. M. McInerney. </author> <title> Biologically Influenced Algorithms and Parallelism in Non-Linear Optimization. </title> <type> PhD thesis, </type> <institution> University of California, </institution> <address> San Diego, </address> <year> 1992. </year>
Reference-contexts: Parallel GSGAs are typically implemented as fine grain algorithms in which each processor is responsible for evaluating and processing a single individual. Among fine grain GSGAs, two classes of algorithms can be distinguished: GSGAs designed for MIMD architectures [7] and GSGAs designed for SIMD architectures <ref> [4, 11, 14] </ref>. The method of structuring the competitive selection differs between these two classes based on differences in the hardware that are exploited by these algorithms to achieve high utilization. <p> For example, the most common method of structuring the competitive selection for SIMD fine grain GSGAs uses a toroidal grid like the one in Figure 3 <ref> [4, 11, 14] </ref>. Every individual in the population is assigned to a location on the grid in an arbitrary manner. The grid is used to determine subsets of solutions that compete for a location on the grid.
Reference: [12] <author> H. Muhlenbein, M. Schomisch, and J. Born. </author> <title> The parallel genetic algorithm as function optimizer. </title> <editor> In R. K. Belew and L. B. Booker, editors, </editor> <booktitle> Proc of the Fourth Intl Conf on Genetic Algorithms, </booktitle> <pages> pages 271-278, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan-Kaufmann. </publisher>
Reference-contexts: Given our critique of traditional parallel performance measures, we expect that previous analyses of parallel GAs will need to be reconsidered. For example, claims of su-perlinear speedup for parallel GAs <ref> [1, 12, 13] </ref> have used speedup measures that do not directly relate the the computation performed on p processors and the computation performed on p 0 processors. Consequently, the meaning of this type of superlinear speedup is not clear.
Reference: [13] <author> R. Shonkwiler. </author> <title> Parallel genetic algorithms. </title> <editor> In S. Forrest, editor, </editor> <booktitle> Proceedings of the Fifth Intl. Conf. on Genetic Algorithms, </booktitle> <pages> pages 199-205, </pages> <address> San Mateo, CA, 1993. </address> <publisher> Morgan-Kaufmann. </publisher>
Reference-contexts: Given our critique of traditional parallel performance measures, we expect that previous analyses of parallel GAs will need to be reconsidered. For example, claims of su-perlinear speedup for parallel GAs <ref> [1, 12, 13] </ref> have used speedup measures that do not directly relate the the computation performed on p processors and the computation performed on p 0 processors. Consequently, the meaning of this type of superlinear speedup is not clear.
Reference: [14] <author> P. Spiessens and B. Manderick. </author> <title> A massively parallel genetic algorithm: Implementation and first analysis. </title> <booktitle> In Proc of the Fouth Intl Conf on Genetic Algorithms, </booktitle> <pages> pages 279-285, </pages> <year> 1991. </year>
Reference-contexts: Unlike most other optimization procedures, GAs maintain a population of individuals (set of solutions) that are competitively selected to generate new candidates for the global optima. Parallel GAs have been developed for a variety of parallel architectures <ref> [4, 7, 14] </ref>. These GAs have been used to solve a variety of difficult optimization problems, and the algorithmic modifications needed to efficiently parallelize GAs have provided insight into the dynamics of GAs' search [6]. Several arguments are commonly used to justify parallelism in GAs. <p> Parallel GSGAs are typically implemented as fine grain algorithms in which each processor is responsible for evaluating and processing a single individual. Among fine grain GSGAs, two classes of algorithms can be distinguished: GSGAs designed for MIMD architectures [7] and GSGAs designed for SIMD architectures <ref> [4, 11, 14] </ref>. The method of structuring the competitive selection differs between these two classes based on differences in the hardware that are exploited by these algorithms to achieve high utilization. <p> For example, the most common method of structuring the competitive selection for SIMD fine grain GSGAs uses a toroidal grid like the one in Figure 3 <ref> [4, 11, 14] </ref>. Every individual in the population is assigned to a location on the grid in an arbitrary manner. The grid is used to determine subsets of solutions that compete for a location on the grid.
Reference: [15] <author> T. Starkweather, D. Whitley, and K. Mathias. </author> <title> Optimization using distributed genetic algorithms. </title> <booktitle> In Parallel Problem Solving from Nature, </booktitle> <pages> pages 176-185, </pages> <year> 1990. </year>
Reference-contexts: The decrease in communication overhead is a natural consequence of a decrease in the synchronization frequency. The total number of function evaluations exhibit a sharp decline as the synchronization frequency is increased. These results are consistent with research on Island Model GAs (IMGAs) <ref> [15] </ref>, which decompose the population into subpopulations, each of which is processed panmictically. The results confirm our hypothesis that delays in communication can improve the performance of parallel GAs, and they provide one explanation for the superior numerical efficiency of asynchronous GSGAs.
Reference: [16] <author> L. E. Toothaker. </author> <title> Multiple Comparisons for Researchers. </title> <publisher> Sages Publications, </publisher> <year> 1991. </year>
Reference-contexts: We tested this conjecture by comparing the distribution of the total number of function evaluations used on all processors for each of the processor size, using a method of multiple comparisons (the GH procedure <ref> [16] </ref>). This test confirmed that expected performance of the synchronized parallel GAs is not affected by randomization with a 5% confidence level. (a) asynchronous coarse-grain GSGAs: (a) plot of T p and (b) plot of pT p . GSGAs.
References-found: 16

