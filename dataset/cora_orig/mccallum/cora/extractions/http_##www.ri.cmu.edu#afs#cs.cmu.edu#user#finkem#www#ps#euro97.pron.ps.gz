URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/finkem/www/ps/euro97.pron.ps.gz
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/finkem/www/publications.html
Root-URL: 
Email: finkem@cs.cmu.edu, ahw@cs.cmu.edu  
Title: SPEAKING MODE DEPENDENT PRONUNCIATION MODELING IN LARGE VOCABULARY CONVERSATIONAL SPEECH RECOGNITION  
Author: Michael Finke and Alex Waibel 
Address: (USA)  
Affiliation: Interactive Systems Laboratories Carnegie Mellon University  
Abstract: In spontaneous conversational speech there is a large amount of variability due to accents, speaking styles and speaking rates (also known as the speaking mode) [3]. Because current recognition systems usually use only a relatively small number of pronunciation variants for the words in their dictionaries, the amount of variability that can be modeled is limited. Increasing the number of variants per dictionary entry is the obvious solution. Unfortunately, this also means increasing the confusability between the dictionary entries, and thus often leads to an actual performance decrease. In this paper we present a framework for speaking mode dependent pronunciation modeling. The probability of encountering pronunciation variants is defined to be a function of the speaking style. The probability function is learned through decision trees from rule based generated pronunciation variants as observed on the Switchboard corpus. The framework is successfully applied to increase the performance of our state-of-the-art Janus Recognition Toolkit Switchboard recognizer significantly. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Michael Finke, Jurgen Fritsch, Petra Geutner, Klaus Ries, Torsten Zeppenfeld, and Alex Waibel. </author> <title> The JanusRTk Switchboard/CallHome evaluation system. </title> <booktitle> In Proceedings of LVCSR Hub 5 Workshop, </booktitle> <month> May </month> <year> 1997. </year>
Reference-contexts: In this paper we present a new approach for modeling pronunciation variation dependent on the speaking mode as implemented in the Janus Recognition Toolkit (JRTk). We present first results based on the JRTk Switchboard Large Vocabulary Conversational Speech Recognizer <ref> [1] </ref>. 2. <p> In order to train our speech recognizer based on such unreliable transcriptions we implemented a Flexible Transcription Alignment (FTA) procedure in JRTk <ref> [1] </ref>. <p> The second component of the FTA approach is a label boosting procedure <ref> [5, 1] </ref>. Instead of relying on a speaker independent system to align the FTA utterance HMM, we adapt the recognizer using maximum likelihood linear re-gression (MLLR) to derive a speaker dependent recognizer for each speaker.
Reference: [2] <author> Michael Finke and Ivica Rogina. </author> <title> Wide Context Acoustic Modeling in Read vs. Spontaneous Speech. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <address> Munich, Germany, 1997. </address> <publisher> IEEE. </publisher>
Reference-contexts: All test runs used the JRTk Switchboard recognizer. The expanded dictionary that was used in these tests included 1.78 pronunciations variants/word, compared to 1.13 for the baseform dictionary (PronLex). The first list of results in Table 4 is based on a JRTk SWB recognizer whose polyphonic decision trees <ref> [2] </ref> were still trained on viterbi alignments based on the unexpanded dictionary. We compare a baseline system trained on the base dictionary with an expanded dictionary FTA trained system tested in two different ways: with the base dictionary and with the expanded one.
Reference: [3] <author> M. Ostendorf, B. Byrne, M. Bacchiani, M. Finke, A. Gunawardana, K. Ross, S. Roweis, E. Shriberg, D. Talkin, A.Waibel, , B. Wheatley, and T. Zeppen-feld. </author> <title> Systematic Variations in Pronunciation via a Language-Dependent Hidden Speaking Mode. </title> <booktitle> In International Conference on Spoken Language Processing, </booktitle> <address> Philadelphia, USA, </address> <year> 1996. </year>
Reference-contexts: Thus, the goal of the work presented in this paper is to develop a method for allowing pronunciation variations depending on a hidden speaking mode <ref> [3] </ref>. The speaking "mode" would vary within and across utterances and would reflect speaking "style", e.g. indicating the likelihood of reduced or sloppy speech vs. clear vs. exaggerated speech.
Reference: [4] <author> M. Riley. </author> <title> A Statistical Model for Generating Pronunciation Networks. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <address> Toronto, 1991. </address> <publisher> IEEE. </publisher>
Reference-contexts: pronunciation rules using decision tree prediction from word features p (rjw) (dotted lines), compared to chance performance (bars). pronunciation rules using decision tree prediction from word and mode features p (rjw; m) (dotted lines), compared to chance performance (bars). for pronunciation modeling is different from the approach proposed by Riley <ref> [4] </ref>. The main contribution of our approach is to include speaking mode related information into the procedure of predicting the probabilities. To evaluate the goodness of the resulting trees, we can compare classification performance of p (rjw; m) to using p (r) only as shown in Figure 2.
Reference: [5] <author> Torsten Zeppenfeld, Michael Finke, Klaus Ries, Martin Westphal, and Alex Waibel. </author> <title> Recognition of Conversational Telephone Speech using the JANUS Speech Engine. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <address> Munich, Germany, 1997. </address> <publisher> IEEE. </publisher>
Reference-contexts: The second component of the FTA approach is a label boosting procedure <ref> [5, 1] </ref>. Instead of relying on a speaker independent system to align the FTA utterance HMM, we adapt the recognizer using maximum likelihood linear re-gression (MLLR) to derive a speaker dependent recognizer for each speaker.
References-found: 5

