URL: http://www.cs.umn.edu/crisys/project/papers/intent-tse.ps
Refering-URL: http://www.cs.umn.edu/crisys/project/currwor.htm
Root-URL: http://www.cs.umn.edu
Title: Intent Specifications: An Approach to Building Human-Centered Specifications  
Author: Nancy G. Leveson 
Affiliation: Dept. of Computer Science and Engineering University of Washington  
Abstract: This paper examines and proposes an approach to writing software specifications, based on research in systems theory, cognitive psychology, and human-machine interaction. The goal is to provide specifications that support human problem solving and the tasks that humans must perform in software development and evolution. A type of specification, called intent specifications, is constructed upon this underlying foundation. 
Abstract-found: 1
Intro-found: 1
Reference: [AT90] <author> D. Ackermann and M. J. Tauber, </author> <title> editors. Mental Models and Human-Computer Interaction. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1990. </year>
Reference-contexts: Each of the users of a specification may (and probably will) have different mental models of the system, depending on such factors as prior experience, the task for which the model is being used, and their role in the system <ref> [AT90, Dun87, Luc87, Rea90] </ref>. The same person may have multiple mental models of a system, and even having two contradictory models of the same system does not seem to constitute a problem for people [Luc87] Strategies also seem to be highly variable.
Reference: [Ash62] <author> W.R. Ashby. </author> <title> Principles of the self-organizing system. </title> <editor> in H. Von Foerster and G.W. Zopf (eds.) </editor> <booktitle> Principles of Self-Organization, </booktitle> <address> Perga-mon, </address> <year> 1962. </year>
Reference-contexts: A basic and often noted principle of engineering is to keep things simple. This principle, of course, is easier to state than to do. Ashby's Law of Requisite Variety <ref> [Ash62] </ref> tells us that there is a limit to how simple we can make control systems, including those designs represented in software, and still have them be effective. In addition, basic human ability is not changing.
Reference: [BP87] <author> M. Beveridge and E. Parkins. </author> <title> Visual representation in analogical program solving. Memory and Cognition, </title> <editor> v. </editor> <volume> 15, </volume> <year> 1987. </year>
Reference-contexts: Recent research on problem-solving behavior consistently shows that experts spend a great deal of their time analyzing the functional structure of a problem at a high level of abstraction before narrowing in on more concrete details <ref> [BP87, BS91, GC88, Ras86, Ves85] </ref>. With other hierarchies, the links between levels are not necessarily related to goals.
Reference: [Bro83] <author> R. Brooks. </author> <title> Towards a theory of comprehension of computer programs. </title> <journal> Int. Journal of Man-Machine Studies, </journal> <volume> 18 </volume> <pages> 543-554, </pages> <year> 1983. </year>
Reference-contexts: Examples of strategies are hypothesis and test, pattern recognition, decision tree search, reasoning by analogy, and topological search. Some computer science researchers have proposed theories about the mental models and strategies used in program understanding tasks (examples of such models are <ref> [Bro83, Let86, Pen87, SM79, SE84] </ref>). Although this approach seems useful, it may turn out to be more difficult than appears on the surface.
Reference: [BL98] <author> M. Brown and N. G. Leveson. </author> <title> Modeling Controller Tasks for Safety Analysis. </title> <booktitle> Second Workshop on Human Error and System Development, </booktitle> <address> Seattle, </address> <month> April </month> <year> 1998. </year>
Reference-contexts: TCAS II (i.e., it fails a self-test and sends a status message to TCAS or it is not sending any output at all), or (3) the malfunctioning is undetected and it sends an incorrect radio altitude. and thus permits integrated simulation and analysis of the entire system, including human-computer interactions <ref> [BL98] </ref>. 4.2.4 Design Representation The two lowest levels of an intent specification provide the information necessary to reason about component design and implementation. The fourth level, Design Representation, contains design information. Its content will depend on whether the particular function is being implemented using analog or digital devices or both.
Reference: [BS91] <author> M.A. Buttigieg and P.M. Sanderson. </author> <title> Emergent features in visual display design for two types of failure detection tasks. </title> <booktitle> Human Factors, </booktitle> <volume> 33, </volume> <year> 1991. </year>
Reference-contexts: Recent research on problem-solving behavior consistently shows that experts spend a great deal of their time analyzing the functional structure of a problem at a high level of abstraction before narrowing in on more concrete details <ref> [BP87, BS91, GC88, Ras86, Ves85] </ref>. With other hierarchies, the links between levels are not necessarily related to goals.
Reference: [Cas91] <author> S.M. Casner. </author> <title> A task analytic approach to the automated design of graphic presentations. </title> <journal> ACM Transactions on Graphics, </journal> <volume> vol. 10, no. 2, </volume> <month> April </month> <year> 1991. </year>
Reference-contexts: Note that the form itself must also be considered from a psychological standpoint: The usability of the language will depend on human perceptual and cognitive strategies. For example, Fitter and Green describe the attributes of a good notation with respect to human perception and understanding [FG79]. Casner <ref> [Cas91] </ref> and others have argued that the utility of any information presentation is a function of the task that the presentation is being used to support. For example, a symbolic representation might be better than a graphic for a particular task, but worse for others.
Reference: [Che81] <author> P. Checkland. </author> <title> Systems Thinking, Systems Practice. </title> <publisher> John Wiley & Sons, </publisher> <year> 1981. </year>
Reference-contexts: Structure, * Relevant interactions between components and the means by which the system retains its integrity (the behavior of the components and their effect on the overall system state), and * Purpose or goals of the system that makes it reason able to consider it to be a coherent entity <ref> [Che81] </ref>. All of these properties need to be included in a complete system model or specification along with a description of the aspects of the environment that can affect the system state. Most of these aspects are already included in our current specification languages. <p> Checkland explains it: Any description of a control process entails an upper level imposing constraints upon the lower. The upper level is a source of an alternative (simpler) description of the lower level in terms of specific functions that are emergent as a result of the imposition of constraints <ref> [Che81, pg. 87] </ref>. Hierarchy theory deals with the fundamental differences between one level of complexity and another. Its ultimate aim is to explain the relationships between different levels: what generates the levels, what separates them, and what links them.
Reference: [CKI88] <author> B. Curtis, H. Krasner and N. Iscoe. </author> <title> A field study of the software design process for large systems. </title> <journal> Communications of the ACM, </journal> <volume> 31(2): </volume> <pages> 1268-1287, </pages> <year> 1988. </year>
Reference-contexts: This hypothesis seems particularly relevant with respect to tasks involving education and program understanding, search, design, validation, safety assurance, maintenance, and evolution. 5.1 Education and Program Under standing Curtis et.al. <ref> [CKI88] </ref> did a field study of the requirements and design process for 17 large systems. They found that substantial design effort in projects was spent coordinating a common understanding among the staff of both the application domain and of how the system should perform within it.
Reference: [DB83] <author> DeKleer J, and J.S. Brown. </author> <title> Assumptions and ambiguities in mechanistic mental models. </title> <editor> In D. Gentner and A.L. Stevens (eds.), </editor> <title> Mental Models, </title> <publisher> Lawrence Erlbaum, </publisher> <year> 1983. </year>
Reference-contexts: DeKleer and Brown found that determining the function of an electric buzzer solely from the structure and behavior of the parts requires complex reasoning <ref> [DB83] </ref>.
Reference: [DV96] <author> N. Dinadis and K.J. Vicente. </author> <title> Ecological interface design for a power plant feedwater subsystem. </title> <journal> IEEE Transactions on Nuclear Science, </journal> <note> in press. </note>
Reference-contexts: They have been developed and used successfully in cognitive engineering by Vi-cente and Rasmussen for the design of operator interfaces, a process they call ecological interface design <ref> [DV96, Vic91] </ref>. The exact number and content of the means-ends hierarchy levels may differ from domain to domain. Here I present a structure for process systems with shared software and human control.
Reference: [Dor87] <author> D. Dorner. </author> <title> On the difficulties people have in dealing with complexity. </title> <editor> In Jens Rasmussen, Keith Duncan, and Jacques Leplat, editors, </editor> <booktitle> New Technology and Human Error, </booktitle> <pages> pages 97-109, </pages> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: The information may all be included somewhere, but it may be hard to find or to determine the relationship to information specified elsewhere. Problem solving in technological systems takes place within the context of a complex causal network of relationships <ref> [Dor87, Ras86, Rea90, VR92] </ref>, and those relationships need to be reflected in the specification.
Reference: [Dun87] <author> K.D. Duncan. </author> <title> Reflections on fault diagnostic expertise. </title> <editor> In Jens Rasmussen, Keith Duncan, and Jacques Leplat, editors, </editor> <booktitle> New Technology and Human Error, </booktitle> <pages> pages 261-269, </pages> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: Each of the users of a specification may (and probably will) have different mental models of the system, depending on such factors as prior experience, the task for which the model is being used, and their role in the system <ref> [AT90, Dun87, Luc87, Rea90] </ref>. The same person may have multiple mental models of a system, and even having two contradictory models of the same system does not seem to constitute a problem for people [Luc87] Strategies also seem to be highly variable.
Reference: [FSL78] <author> B. Fischoff, P. Slovic, and S. Lichtenstein. </author> <title> Fault trees: Sensitivity of estimated failure probabilities to problem representation. </title> <journal> Journal of Experimental Psychology: Human Perception and Performance, </journal> <volume> vol. 4, </volume> <year> 1978. </year>
Reference-contexts: In experiments where some problem solvers were given incomplete representations while others were not given any representation at all, those with no representation did better <ref> [FSL78, Smi89] </ref>. An incomplete problem representation actually impaired performance because the subjects tended to rely on it as a comprehensive and truthful representation|they failed to consider important factors deliberately omitted from the representations. <p> Fischoff, who did such an experiment involving fault tree diagrams, attributed it to an "out of sight, out of mind" phenomenon <ref> [FSL78] </ref>. One place to start in deciding what should be in a system specification is with basic systems theory, which defines a system as a set of components that act together as a whole to achieve some common goal, objective, or end.
Reference: [FG79] <author> Fitter and Green. </author> <title> When do diagrams make good programming languages?. </title> <journal> Int. J. of Man-Machine Studies, </journal> <volume> 11 </volume> <pages> 235-261, </pages> <year> 1979. </year>
Reference-contexts: Note that the form itself must also be considered from a psychological standpoint: The usability of the language will depend on human perceptual and cognitive strategies. For example, Fitter and Green describe the attributes of a good notation with respect to human perception and understanding <ref> [FG79] </ref>. Casner [Cas91] and others have argued that the utility of any information presentation is a function of the task that the presentation is being used to support. For example, a symbolic representation might be better than a graphic for a particular task, but worse for others.
Reference: [GC88] <author> R. Glaser and M. T. H. Chi. </author> <title> Overview. </title> <editor> In R. Glaser, M. T. H. Chi, and M. J. Farr, editors, </editor> <booktitle> The Nature of Expertise. </booktitle> <publisher> Erlbaum, </publisher> <address> Hills-dale, New Jersey, </address> <year> 1988. </year>
Reference-contexts: Glaser and Chi suggest that experts and successful problem solvers tend to focus first on analyzing the functional structure of the problem at a high level of ab 7 straction and then narrow their search for a solution by focusing on more concrete details <ref> [GC88] </ref>. Representations that constrain search in a way that is explicitly related to the purpose or intent for which the system is designed have been shown to be more effective than those that do not because they facilitate the type of goal-directed behavior exhibited by experts [VCP95]. <p> Recent research on problem-solving behavior consistently shows that experts spend a great deal of their time analyzing the functional structure of a problem at a high level of abstraction before narrowing in on more concrete details <ref> [BP87, BS91, GC88, Ras86, Ves85] </ref>. With other hierarchies, the links between levels are not necessarily related to goals.
Reference: [GN95] <author> W. Griswold and D. Notkin. </author> <title> Architectural tradeoffs for a meaning-preserving program restructuring tool. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 21(4) </volume> <pages> 275-287, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: Tools for restructuring code have been developed to cope with this common problem of increasing complexity and decreasing coherency of maintained code <ref> [GN95] </ref>. Using intent specifications will not eliminate this need, but we hope it will be reduced by providing specifications that assist in the evolution process and, more important, assist in building software that is more easily evolved and maintained.
Reference: [Har82] <author> G. Harman. </author> <title> Logic, reasoning, and logic form. In Language, Mind, and Brain, </title> <editor> T.W. Simon and R.J. Scholes (eds.), </editor> <publisher> Lawrence Erlbaum Associates, </publisher> <year> 1982. </year>
Reference-contexts: This intentional information is critical in the design and evolution of software. As Harman has said, practical reasoning is concerned with what to intend while formal reasoning with what to believe <ref> [Har82] </ref>. "Formal logic arguments are a priori true or false with reference to an explicitly defined model, whereas functional reasoning deals with relationships between models, and truth depends on correspondence with the state of affairs in the real world" [Har82]. <p> with what to intend while formal reasoning with what to believe <ref> [Har82] </ref>. "Formal logic arguments are a priori true or false with reference to an explicitly defined model, whereas functional reasoning deals with relationships between models, and truth depends on correspondence with the state of affairs in the real world" [Har82]. In the conclusions to our paper describing our experiences specifying the requirements for TCAS II (an air craft collision avoidance system), we wrote: 4 In reverse engineering TCAS, we found it im-possible to derive the requirements specification strictly from the pseudocode and an accompanying English language description.
Reference: [JLHM91] <author> M.S. Jaffe, N.G. Leveson, M.P.E. Heim-dahl, and B.Melhart. </author> <title> Software requirements analysis for real-time process-control systems. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> SE-17(3), </volume> <month> March </month> <year> 1991. </year> <month> 25 </month>
Reference-contexts: Previously, we looked at a narrow slice of this problem|what should be contained in blackbox requirements specifications for process control software to ensure that the resulting implementations are internally complete <ref> [JLHM91, Lev95] </ref>. This paper again considers the question of specification content, but within a larger context. This question is critical because cognitive psychologists have determined that people tend to ignore information during problem solving that is not represented in the specification of the problem.
Reference: [KS90] <author> C.A. Kaplan and H.A. Simon. </author> <title> In search of insight. </title> <journal> Cognitive Psychology, </journal> <volume> vol. 22, </volume> <year> 1990. </year>
Reference-contexts: These challenges have necessitated augmenting traditional human factors approaches to consider the capabilities and limitations of the human element in complex systems. the critical attributes needed to solve the problem in a perceptually salient way <ref> [KS90] </ref>. A problem-solving strategy is an abstraction describing one consistent reasoning approach characterized by a particular mental representation and interpretation of observations [RP95]. Examples of strategies are hypothesis and test, pattern recognition, decision tree search, reasoning by analogy, and topological search. <p> Psychological experiments in problem solving find that people attend primarily to perceptually salient information <ref> [KS90] </ref>. The goal of specification language design should be to make it easy for users to extract and focus on the important information for the specific task at hand, which includes all potential tasks related to use of the specification. Cognitive engineers speak of this problem as "information pickup" [Woo95].
Reference: [KHS85] <author> K. Kotovsky, J.R. Hayes, and H.A. Simon. </author> <title> Why are some problems hard? Evidence from Tower of Hanoi. </title> <journal> Cognitive Psychology, </journal> <volume> vol. 17, </volume> <year> 1985. </year>
Reference-contexts: To provide assistance for problem solving, then, requires that we develop a theoretical basis for deciding which representations support effective problem-solving strategies. For example, problem-solving performance can be improved by providing representations that reduce the problem solver's memory load <ref> [KHS85] </ref> and that display 1 Cognitive engineering is a term that has come to denote the combination of ideas from systems engineering, cognitive psychology, and humans factors to cope with the challenges of building high-tech systems composed of humans and machines.
Reference: [Let86] <author> S. Letovsky. </author> <title> Cognitive processes in program comprehension. </title> <booktitle> In Proceedings of the First Workshop on Empirical Studies of Programmers, </booktitle> <pages> pages 58-79. </pages> <publisher> Ablex Publishing, </publisher> <address> Nor-wood, NJ, </address> <year> 1986. </year>
Reference-contexts: Examples of strategies are hypothesis and test, pattern recognition, decision tree search, reasoning by analogy, and topological search. Some computer science researchers have proposed theories about the mental models and strategies used in program understanding tasks (examples of such models are <ref> [Bro83, Let86, Pen87, SM79, SE84] </ref>). Although this approach seems useful, it may turn out to be more difficult than appears on the surface.
Reference: [Lev91] <author> N.G. Leveson. </author> <title> Software safety in embedded computer systems. </title> <journal> Communications of the ACM, </journal> <volume> vol. 34, no. 2, </volume> <month> February </month> <year> 1991. </year>
Reference-contexts: For example, in one of our empirical studies, we found that programmers had difficulty writing effective assertions for detecting errors in executing software [LCKS90]. I have suggested that using results from safety analyses might help in determining which assertions are required and where to detect the most important errors <ref> [Lev91] </ref>.
Reference: [Lev95] <author> N.G. Leveson. Safeware: </author> <title> System Safety and Computers. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1995. </year>
Reference-contexts: Previously, we looked at a narrow slice of this problem|what should be contained in blackbox requirements specifications for process control software to ensure that the resulting implementations are internally complete <ref> [JLHM91, Lev95] </ref>. This paper again considers the question of specification content, but within a larger context. This question is critical because cognitive psychologists have determined that people tend to ignore information during problem solving that is not represented in the specification of the problem. <p> Without a record of intent, important decisions can be undone during maintenance: Many serious accidents and losses can be traced to the fact that a system did not operate as intended because of changes that were not fully coordinated or fully analyzed to determine their effects <ref> [Lev95] </ref>. What is not so clear is the content and structure of the information that is needed. Simply keeping an audit trail of decisions and the reasons behind them as they are made is not practical. The number of decisions made in any large project is enormous. <p> manner assumed by the designers, (2) that the models and assumptions used during initial decision making and design were correct, and (3) that the models and assumptions are not violated by changes in the system, 12 such as workarounds or unauthorized changes in proce-dures, or by changes in the environment <ref> [Lev95] </ref>. Operational feedback on trends, incidents, and accidents should trigger reanalysis when appropriate. Linking the assumptions throughout the document with the hazard analysis (for example, to particular boxes in the system fault trees) will assist in performing safety maintenance activities. <p> software module (and back) might assist with writing effective and useful assertions to detect general violations of system goals and constraints. 5.5 Safety Assurance A complete safety analysis and methodology for building safety-critical systems requires identifying the system-level safety requirements and constraints and then tracing them down to the components <ref> [Lev95] </ref>. After the safety-critical behavior of each component has been determined (including the implications of its behavior when the components interact with each other), verification is required that the components do not violate the identified safety-related behavioral constraints.
Reference: [LCKS90] <author> N.G. Leveson, S.S. Cha, J.C. Knight, and T.J. Shimeall. </author> <title> The use of self-checks and voting in software error detection: An empirical study. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> vol. SE-16, no. 4, </volume> <month> April </month> <year> 1990. </year>
Reference-contexts: Detecting unanticipated faults during execution has turned out to be a very difficult problem. For example, in one of our empirical studies, we found that programmers had difficulty writing effective assertions for detecting errors in executing software <ref> [LCKS90] </ref>. I have suggested that using results from safety analyses might help in determining which assertions are required and where to detect the most important errors [Lev91].
Reference: [LHHR94] <author> N.G. Leveson, M. P.E. Heimdahl, H. Hil-dreth, and J.D. Reese. </author> <title> Requirements specification for process-control systems. </title> <journal> Trans. on Software Engineering, </journal> <volume> SE-20(9), </volume> <month> Septem-ber </month> <year> 1994. </year>
Reference-contexts: For the most part, only one person is able to explain why some decisions were made or why things were designed in a particular way <ref> [LHHR94] </ref>. There is widespread agreement about the need for design rationale (intent) information in order to understand complex software or to correctly and efficiently change or analyze the impact of changes to it. <p> Higher-level emergent information about purpose or intent cannot be inferred from what we normally include in such specifications. Design errors may result when we either guess incorrectly about higher-level intent or omit it from our decision-making process. For example, while specifying the system requirements for TCAS II <ref> [LHHR94] </ref>, we learned from experts that crossing maneuvers are avoided in the design for safety reasons. The analysis on which this decision is based comes partly from experience during TCAS system testing on real aircraft and partly as a result of an extensive safety analysis performed on the system. <p> Here I present a structure for process systems with shared software and human control. In order to determine the feasibility of this approach for specifying a complex system, I extended the formal TCAS II aircraft collision avoidance system requirements specification we previously wrote <ref> [LHHR94] </ref> to include intent information and other information that cannot be expressed formally but is needed in a complete system requirements specification. We are currently applying the approach to other examples, including a NASA robot and part of the U.S. Air Traffic Control System. <p> The language used to describe the components may also vary. I use a state-machine language called SpecTRM-RL (Specification Tools and Requirements Methodology-Requirements Language), which is a successor to the language (RSML) used in our official TCAS II specification <ref> [LHHR94] </ref>. Figure 6 shows part of the SpecTRM-RL description of the behavior of the CAS (collision avoidance system) subcomponent. SpecTRM-RL specifications are intended to be both easily readable with minimum instruction and formally analyzable (we have a set of analysis tools that work on these specifications).
Reference: [LPS97] <author> N.G. Leveson, L.D. Pinnel, S.D. Sandys, S. Koga, and J.D. Reese. </author> <title> Analyzing software specifications for mode confusion potential. Workshop on Human Error and System Development, </title> <address> Glascow, </address> <month> March </month> <year> 1977. </year>
Reference-contexts: One goal of intent specifications is to integrate the information needed to design "human-centered automation" into the system requirements specification. We are also working on analysis techniques to identify problematic system and software design features in order to predict where human errors are likely to occur <ref> [LPS97] </ref>. This information can be used in both the automation design and in the design of the operator procedures, tasks, interface, and training.
Reference: [Luc87] <author> D.A. Lucas. </author> <title> Mental models and new technology. </title> <editor> In Jens Rasmussen, Keith Duncan, and Jacques Leplat, editors, </editor> <booktitle> New Technology and Human Error, </booktitle> <pages> pages 321-325. </pages> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: Each of the users of a specification may (and probably will) have different mental models of the system, depending on such factors as prior experience, the task for which the model is being used, and their role in the system <ref> [AT90, Dun87, Luc87, Rea90] </ref>. The same person may have multiple mental models of a system, and even having two contradictory models of the same system does not seem to constitute a problem for people [Luc87] Strategies also seem to be highly variable. <p> The same person may have multiple mental models of a system, and even having two contradictory models of the same system does not seem to constitute a problem for people <ref> [Luc87] </ref> Strategies also seem to be highly variable.
Reference: [New66] <author> J.R. Newman. </author> <title> Extension of human capability through information processing and display systems. </title> <type> Technical Report SP-2560, </type> <institution> System Development Corporation, </institution> <year> 1966. </year>
Reference-contexts: Newman has noted, "People don't mind dealing with complexity if they have some way of controlling or handling it . . . If a person is allowed to structure a complex situation according to his perceptual and conceptual needs, sheer complexity is no bar to effective performance" <ref> [New66, Ras85] </ref>. Thus, complexity itself is not a problem if humans are presented with meaningful information in a coherent, structured context. 3.3.2 Hierarchy Theory Two ways humans cope with complexity is to use top-down reasoning and stratified hierarchies. Building systems bottom-up works for relatively simple systems.
Reference: [Nor93] <author> D.A. Norman. </author> <title> Things that Make us Smart. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1993. </year>
Reference-contexts: Specifications used in problem-solving tasks are constructed to provide assistance in this process. Cognitive psychology has firmly established that the representation of the problem provided to problem solvers can affect their performance (see Nor-man <ref> [Nor93] </ref> for a survey of this research). In fact, Woods claims that there are no neutral representations [Woo95]: The representations available to the problem solver either degrade or support performance.
Reference: [RP95] <author> J. Rasmussen and A. </author> <title> Pejtersen. Virtual ecology of work. </title> <note> In J. </note> <author> M. Flach, P. A. Hancock, K. Caird and K. J. Vicente, </author> <title> editors An Ecological Approach to Human Machine Systems I: A Global Perspective, </title> <publisher> Erlbaum, </publisher> <address> Hillsdale, New Jersey, 1995.. </address>
Reference-contexts: A problem-solving strategy is an abstraction describing one consistent reasoning approach characterized by a particular mental representation and interpretation of observations <ref> [RP95] </ref>. Examples of strategies are hypothesis and test, pattern recognition, decision tree search, reasoning by analogy, and topological search. Some computer science researchers have proposed theories about the mental models and strategies used in program understanding tasks (examples of such models are [Bro83, Let86, Pen87, SM79, SE84]).
Reference: [Pen87] <author> N. Pennington. </author> <title> Stimulus structures and mental representations in expert comprehension of computer programs. </title> <booktitle> Cognitive Psychology 19 </booktitle> <address> 295-341,1987. </address>
Reference-contexts: Examples of strategies are hypothesis and test, pattern recognition, decision tree search, reasoning by analogy, and topological search. Some computer science researchers have proposed theories about the mental models and strategies used in program understanding tasks (examples of such models are <ref> [Bro83, Let86, Pen87, SM79, SE84] </ref>). Although this approach seems useful, it may turn out to be more difficult than appears on the surface.
Reference: [Ras85] <author> J. Rasmussen. </author> <title> The Role of hierarchical knowledge representation in decision making and system management. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> vol. SMC-15, no. 2, </volume> <month> March/April </month> <year> 1985. </year>
Reference-contexts: That is, we will need to find ways to augment human ability. The situation is not hopeless. As Rasmussen observes, the complexity of a system is not an objective feature of the system <ref> [Ras85] </ref>. Observed complexity depends upon the level of resolution upon which the system is being considered. A simple object becomes complex if observed through a microscope. <p> We build such mental models and update them based on what we observe about the system, that is, by means of our interface to the system. Therefore, the apparent complexity of a system ultimately depends upon the technology of the interface system <ref> [Ras85] </ref>. The solution to the complexity problem is to take advantage of the most powerful resources people have for dealing with complexity. Newman has noted, "People don't mind dealing with complexity if they have some way of controlling or handling it . . . <p> Newman has noted, "People don't mind dealing with complexity if they have some way of controlling or handling it . . . If a person is allowed to structure a complex situation according to his perceptual and conceptual needs, sheer complexity is no bar to effective performance" <ref> [New66, Ras85] </ref>. Thus, complexity itself is not a problem if humans are presented with meaningful information in a coherent, structured context. 3.3.2 Hierarchy Theory Two ways humans cope with complexity is to use top-down reasoning and stratified hierarchies. Building systems bottom-up works for relatively simple systems. <p> Consideration of purpose or reason (top-down analysis in a means-ends hierarchy) has been shown to play a major role in understanding the operation of complex systems <ref> [Ras85] </ref>. Rubin's analysis of his attempts to understand the function of a camera's shutter (as cited in [Ras90]) provides an example of the role of intent or purpose in understanding a system.
Reference: [Ras86] <author> J. Rasmussen. </author> <title> Information Processing and Human-Machine Interaction: An Approach to Cognitive Engineering. </title> <publisher> North Holland, </publisher> <year> 1986. </year>
Reference-contexts: A study that used protocol analysis to determine the troubleshooting strategies of professional technicians working on electronic equipment found that no two sequences of actions were identical, even though the technicians were performing the same task every time (i.e., finding a faulty electronic component) <ref> [Ras86] </ref>. <p> The information may all be included somewhere, but it may be hard to find or to determine the relationship to information specified elsewhere. Problem solving in technological systems takes place within the context of a complex causal network of relationships <ref> [Dor87, Ras86, Rea90, VR92] </ref>, and those relationships need to be reflected in the specification. <p> recorded by people working on complex systems (process plant operators and computer maintainers) and found that they structured the system along two dimensions: (1) a part-whole abstraction in which the system is viewed as a group of related components at several levels of physical aggregation, and (2) a means-ends abstraction <ref> [Ras86] </ref>. 3.3.3 Means-Ends Hierarchies In a means-end abstraction, each level represents a different model of the same system. At any point in the hierarchy, the information at one level acts as the goals (the ends) with respect to the model at the next lower level (the means). <p> At any point in the hierarchy, the information at one level acts as the goals (the ends) with respect to the model at the next lower level (the means). Thus, in a means-ends abstraction, the current level specifies what, the level below how, and the level above why <ref> [Ras86] </ref>. In essence, this intent information is emergent in the sense of system theory: When moving from one level to the next higher level, the change in system properties represented is not merely removal of details of information on the physical or material properties. <p> More fundamentally, information is added on higher-level principles governing the coordination of the various functions or elements at the lower level. In man-made systems, these higher-level principles are naturally derived from the purpose of the system, i.e., from the reasons for the configurations at the level considered <ref> [Ras86] </ref> A change of level involves both a shift in concepts and in the representation structure as well as a change in the information suitable to characterize the state of the function or operation at the various levels [Ras86]. <p> system, i.e., from the reasons for the configurations at the level considered <ref> [Ras86] </ref> A change of level involves both a shift in concepts and in the representation structure as well as a change in the information suitable to characterize the state of the function or operation at the various levels [Ras86]. <p> Recent research on problem-solving behavior consistently shows that experts spend a great deal of their time analyzing the functional structure of a problem at a high level of abstraction before narrowing in on more concrete details <ref> [BP87, BS91, GC88, Ras86, Ves85] </ref>. With other hierarchies, the links between levels are not necessarily related to goals.
Reference: [Ras90] <author> J. Rasmussen. </author> <title> Mental models and the control of action in complex environments. </title> <editor> In D. Ackermann and M.J. Tauber (eds.) </editor> <title> Mental Models and Human-Computer Interaction, </title> <publisher> Elsevier (North-Holland), </publisher> <year> 1990, </year> <pages> pp. 41-69. </pages>
Reference-contexts: Consideration of purpose or reason (top-down analysis in a means-ends hierarchy) has been shown to play a major role in understanding the operation of complex systems [Ras85]. Rubin's analysis of his attempts to understand the function of a camera's shutter (as cited in <ref> [Ras90] </ref>) provides an example of the role of intent or purpose in understanding a system. <p> suggests that the resulting inference process is very artificial compared to the top-down inference process guided by functional considerations as described by Ruben. "In the DeKleer-Brown model, it will be difficult to see the woods for the trees, while Rubin's description appears to be guided by a birds-eye perspective " <ref> [Ras90] </ref>. Glaser and Chi suggest that experts and successful problem solvers tend to focus first on analyzing the functional structure of the problem at a high level of ab 7 straction and then narrow their search for a solution by focusing on more concrete details [GC88].
Reference: [Rea90] <author> J. </author> <title> Reason. Human Error. </title> <publisher> Cambridge University Press, </publisher> <year> 1990. </year>
Reference-contexts: Each of the users of a specification may (and probably will) have different mental models of the system, depending on such factors as prior experience, the task for which the model is being used, and their role in the system <ref> [AT90, Dun87, Luc87, Rea90] </ref>. The same person may have multiple mental models of a system, and even having two contradictory models of the same system does not seem to constitute a problem for people [Luc87] Strategies also seem to be highly variable. <p> The information may all be included somewhere, but it may be hard to find or to determine the relationship to information specified elsewhere. Problem solving in technological systems takes place within the context of a complex causal network of relationships <ref> [Dor87, Ras86, Rea90, VR92] </ref>, and those relationships need to be reflected in the specification.
Reference: [SWB95] <editor> N.D. Sarter, D.D. Woods, and C.E. Billings. Automation Surprises. in G. Salvendy (Ed.) </editor> <booktitle> Handbook of Human Factors/Ergonomics, 2nd Edition, </booktitle> <publisher> Wiley, </publisher> <address> New York, </address> <publisher> in press. </publisher>
Reference-contexts: For example, Weiner introduced the term clumsy automation to describe automation that places additional and unevenly distributed workload, communication, and coordination demands on pilots without adequate support [Wei89]). Sarter, Woods, and Billings <ref> [SWB95] </ref> describe additional problems associated with new attentional and knowledge demands and breakdowns in mode awareness and "automation surprises," which they attribute to technology-centered automation: Too often, the designers of the automation focus exclusively on technical aspects, such as the mapping from software inputs to outputs, on mathematical models of requirements
Reference: [SM79] <author> B. Shneiderman and R. Mayer. </author> <title> Syntactic/semantic interactions in programmer behavior: A model and experimental results. </title> <journal> Computer and Info. Sciences, </journal> <volume> 8(3) </volume> <pages> 219-238, </pages> <year> 1979. </year>
Reference-contexts: Examples of strategies are hypothesis and test, pattern recognition, decision tree search, reasoning by analogy, and topological search. Some computer science researchers have proposed theories about the mental models and strategies used in program understanding tasks (examples of such models are <ref> [Bro83, Let86, Pen87, SM79, SE84] </ref>). Although this approach seems useful, it may turn out to be more difficult than appears on the surface.
Reference: [Smi89] <author> G.F. Smith. </author> <title> Representational effects on the solving of an unstructured decision problem. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> vol. SMC-19, </volume> <year> 1989, </year> <pages> pp. 1083-1090. </pages>
Reference-contexts: In experiments where some problem solvers were given incomplete representations while others were not given any representation at all, those with no representation did better <ref> [FSL78, Smi89] </ref>. An incomplete problem representation actually impaired performance because the subjects tended to rely on it as a comprehensive and truthful representation|they failed to consider important factors deliberately omitted from the representations.
Reference: [SE84] <author> E. Soloway and K. Ehrlich. </author> <title> Empirical studies of programming knowledge. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> vol. SE-10(5):595-609, </volume> <year> 1984. </year>
Reference-contexts: Examples of strategies are hypothesis and test, pattern recognition, decision tree search, reasoning by analogy, and topological search. Some computer science researchers have proposed theories about the mental models and strategies used in program understanding tasks (examples of such models are <ref> [Bro83, Let86, Pen87, SM79, SE84] </ref>). Although this approach seems useful, it may turn out to be more difficult than appears on the surface.
Reference: [Sol88] <author> E. Soloway, J. Pinto, S. Letovsky, D. Littman, and R. Lampert. </author> <title> Designing documentation to compensate for delocalized plans. </title> <journal> Communications of the ACM, </journal> <volume> 31(2): </volume> <pages> 1259-1267, </pages> <year> 1988. </year>
Reference-contexts: The fourth level of the example TCAS intent specification simply contains the official pseudocode design specification. But this level might contain information different than we usually include in design specifications. For example, Soloway et.al. <ref> [Sol88] </ref> describe the problem of modifying code containing delocalized plans (plans or schemas with pieces spread throughout the software).
Reference: [Ves85] <author> I. Vessey. </author> <title> Expertise in debugging computer programs: A process analysis. </title> <journal> Int. J. of Man-Machine Studies, </journal> <volume> vol. 23, </volume> <year> 1985. </year>
Reference-contexts: Recent research on problem-solving behavior consistently shows that experts spend a great deal of their time analyzing the functional structure of a problem at a high level of abstraction before narrowing in on more concrete details <ref> [BP87, BS91, GC88, Ras86, Ves85] </ref>. With other hierarchies, the links between levels are not necessarily related to goals. <p> Thus they are explained bottom up. The same argument seems to apply to software debugging. There is evidence to support this hypothesis. Using protocol analysis, Vessey found that the most successful debuggers had a "system" view of the software <ref> [Ves85] </ref>. 5.2.1 Design Criteria and Evaluation An interesting implication of intent specifications is their potential effect on system and software design. Such specifications might not only be used to understand and validate designs but also to guide them.
Reference: [Vic91] <author> K.J. Vicente. </author> <title> Supporting knowledge-based behavior through ecological interface design. </title> <type> Ph.D. Dissertation, </type> <institution> University of Illinois at Urbana-Champagne, </institution> <year> 1991. </year> <month> 26 </month>
Reference-contexts: They have been developed and used successfully in cognitive engineering by Vi-cente and Rasmussen for the design of operator interfaces, a process they call ecological interface design <ref> [DV96, Vic91] </ref>. The exact number and content of the means-ends hierarchy levels may differ from domain to domain. Here I present a structure for process systems with shared software and human control.
Reference: [VCP95] <author> K.J. Vicente, K. Christoffersen and A. Perek--lit. </author> <title> Supporting operator problem solving through ecological interface design. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 25(4) </volume> <pages> 529-545, </pages> <year> 1995. </year>
Reference-contexts: Representations that constrain search in a way that is explicitly related to the purpose or intent for which the system is designed have been shown to be more effective than those that do not because they facilitate the type of goal-directed behavior exhibited by experts <ref> [VCP95] </ref>. Therefore, we should be able to improve the problem solving required in software development and evolution tasks by providing a representation (i.e., specification) of the system that facilitates goal-oriented search by making explicit the goals related to each component.
Reference: [VR90] <author> K.J. Vicente and J. Rasmussen. </author> <title> The ecology of human-machine systems II: Mediating direct perception in complex work domains. </title> <journal> Ecological Psychology, </journal> <volume> 2(3) </volume> <pages> 207-249, </pages> <year> 1990. </year>
Reference-contexts: serve as an effective medium: (1) content (what semantic information should be contained in the representation given the goals and tasks of the users, (2) structure (how to design the representation so that the user can extract the needed information), and (3) form (the notation or format of the interface) <ref> [VR90] </ref>. The next sections examine each of these four aspects of specification design in turn. 3.1 Process Any system specification method should support the systems engineering process. This process provides a logical structure for problem solving (see Figure 1).
Reference: [VR92] <author> K.J. Vicente and J. Rasmussen. </author> <title> Ecological interface design: </title> <journal> Theoretical foundations. IEEE Trans. on Systems, Man, and Cybernetics, </journal> <volume> vol 22, No. 4, </volume> <month> July/August </month> <year> 1992. </year>
Reference-contexts: Thus, being provided with an incomplete problem representation (specification) can actually lead to worse performance than having no representation at all <ref> [VR92] </ref>. One possible explanation for these results is that some problem solvers did worse because they were unaware of important omitted information. <p> The information may all be included somewhere, but it may be hard to find or to determine the relationship to information specified elsewhere. Problem solving in technological systems takes place within the context of a complex causal network of relationships <ref> [Dor87, Ras86, Rea90, VR92] </ref>, and those relationships need to be reflected in the specification. <p> Thus reasons for proper function are derived "top-down." In contrast, causes of improper function depend upon changes in the physical world (i.e., the implementation) and thus they are explained "bottom up" <ref> [VR92] </ref>. Mappings between levels are many-to-many: Components of the lower levels can serve several purposes while purposes at a higher level may be realized using several components of the lower-level model. <p> both developers and maintainers and augment the abilities of both, i.e., increase the intellectual manageability of the task. 5.2 Search Strategies Vicente and Rasmussen have noted that means-ends hierarchies constrain search in a useful way by providing traceability from the highest level goal statements down to implementations of the components <ref> [VR92] </ref>.
Reference: [Wei89] <author> E.L. Wiener. </author> <title> Human Factors of Advanced Technology ("Glass Cockpit") Transport Aircraft. </title> <type> NASA Contractor Report 177528, </type> <institution> NASA Ames Research Center, </institution> <month> June </month> <year> 1989. </year>
Reference-contexts: For example, Weiner introduced the term clumsy automation to describe automation that places additional and unevenly distributed workload, communication, and coordination demands on pilots without adequate support <ref> [Wei89] </ref>).
Reference: [Woo95] <author> D.D. Woods. </author> <title> Toward a theoretical base for representation design in the computer medium: </title> <journal> Ecological perception and aiding human cognition. </journal> <note> In J. </note> <author> M. Flach, P. A. Hancock, K. Caird and K. J. Vicente, </author> <title> editors An Ecological Approach to Human Machine Systems I: A Global Perspective, </title> <publisher> Erlbaum, </publisher> <address> Hillsdale, New Jersey, </address> <year> 1995. </year> <month> 27 </month>
Reference-contexts: Cognitive psychology has firmly established that the representation of the problem provided to problem solvers can affect their performance (see Nor-man [Nor93] for a survey of this research). In fact, Woods claims that there are no neutral representations <ref> [Woo95] </ref>: The representations available to the problem solver either degrade or support performance. To provide assistance for problem solving, then, requires that we develop a theoretical basis for deciding which representations support effective problem-solving strategies. <p> The goal of specification language design should be to make it easy for users to extract and focus on the important information for the specific task at hand, which includes all potential tasks related to use of the specification. Cognitive engineers speak of this problem as "information pickup" <ref> [Woo95] </ref>. Just because the information is in the interface does not mean that the operator can find it easily. The same is true for specifications.
References-found: 48

