URL: http://www.csc.ncsu.edu/faculty/WStewart/Publications/PS_files/Halim.ps
Refering-URL: http://www.csc.ncsu.edu/faculty/WStewart/Publications/Publications.html
Root-URL: http://www.csc.ncsu.edu
Title: A General Framework for Iterative Aggregation/Disaggregation Methods  
Author: Halim D. Kafeety, Carl D. Meyer, and William J. Stewart. N. 
Note: Research supported in part by NSF (DDM 8906248)  
Date: August 9, 1996  
Address: Raleigh, N.C. 27695-8206  
Affiliation: Carolina State University  Operations Research Program. Department of Mathematics. Department of Computer Science.  
Abstract: Aggregation/disaggregation methods are an important class of methods for computing the stationary probabilities of large-scale Markov chains. For Markov chains which are nearly uncoupled, iterative aggregation/disaggregation techniques can often result in sequences which converge at surprisingly rapid rates. But due to the variety of ways in which iterative aggregation/disaggregation methods are designed and implemented, it is generally necessary to analyze each algorithm in isolation, and it can be difficult to compare similarities and differences. The purpose of this paper is to help overcome this situation by presenting a general framework for iterative aggregation/disaggregation algorithms which can be used to analyze and compare different and rather general algorithms from the class. We will demonstrate how several of the best known IAD algorithms fit within this general framework. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Chatelin, F., and Miranker, W. L.: </author> <title> "Acceleration by Aggregation Successive Approximation Methods," </title> <journal> Linear Algebra Appl., </journal> <volume> Vol. 43, </volume> <year> 1982, </year> <pages> pp. 17-47. </pages>
Reference-contexts: A general aggregation/disaggregation theory for systems of linear equations was formulated by Chatelin & Miranker <ref> [1, 2] </ref>, and this work was placed in the context of Markov chains by Haviv [4]. However, it is less than straightforward to fit some of the newer A/D techniques into the Chatelin & Miranker or Haviv theory.
Reference: [2] <author> Chatelin, F., and Miranker, W. L.: </author> <title> "Aggregation/Disaggregation for the Eigenvalue Problem," </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> Vol. 21, </volume> <year> 1984, </year> <pages> pp. 567-582. </pages>
Reference-contexts: A general aggregation/disaggregation theory for systems of linear equations was formulated by Chatelin & Miranker <ref> [1, 2] </ref>, and this work was placed in the context of Markov chains by Haviv [4]. However, it is less than straightforward to fit some of the newer A/D techniques into the Chatelin & Miranker or Haviv theory.
Reference: [3] <author> Feinberg, B. N., and Chiu, S. S.: </author> <title> "A Method to Calculate Steady State Distributions of Large Markov Chains," </title> <journal> Operations Research, </journal> <volume> Vol. 35, No. 2, </volume> <year> 1987, </year> <pages> pp. 282-290. </pages>
Reference-contexts: The smallest such subset J fl j of J j is a recurrent set of J j . This structure includes as special cases the Single Input Superstate structure in Feinburg and Chiu <ref> [3] </ref> and the "exactly lumpable" structure in Schweitzer [8]. In the former case J fl j is the single input superstate corresponding to J j , fj fl g, while in the latter case J fl j = J j .
Reference: [4] <author> Haviv, M.: </author> <title> "Aggregation/Disaggregation Methods for Computing the Stationary Distribution of a Markov Chain." </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> Vol. 24, </volume> <year> 1987, </year> <pages> pp. 952-966. </pages>
Reference-contexts: A general aggregation/disaggregation theory for systems of linear equations was formulated by Chatelin & Miranker [1, 2], and this work was placed in the context of Markov chains by Haviv <ref> [4] </ref>. However, it is less than straightforward to fit some of the newer A/D techniques into the Chatelin & Miranker or Haviv theory.
Reference: [5] <author> Kim, D. S., and Smith, R. L.: </author> <title> "An Exact Aggregation Algorithm for a Special Class of Markov Chains," </title> <institution> Department of Industrial and Operations Engineering, University of Michigan, Ann Arbor, </institution> <type> Technical Report 89-2, </type> <year> 1989. </year>
Reference-contexts: Some important iterative A/D methods for nearly uncoupled Markov chains have been proposed by Takahashi [10], Vantilborgh [11], Koury, McAllister & Stewart [6], Kim & Smith <ref> [5] </ref>, and Sumita & Rieders [9]. <p> There are q modified GA2 aggregation steps For a fixed k, solve equation (26) for x k and then normalize x k to get OE k that satisfies OE k e = 1, i.e. x k : 5.6 Kim and Smith Algorithm J j O * I Kim and Smith's <ref> [5] </ref> algorithm is an A/D algorithm that solves a special structured Markov chain called recurrent set decomposable Markov chain.
Reference: [6] <author> Koury, J. R., McAllister, D. F., and Stewart, W. J.: </author> <title> "Iterative Methods for Computing Stationary Distributions of Nearly Completely Decomposable Markov Chains," </title> <journal> SIAM J. Alg. Disc. Meth., </journal> <volume> Vol. 5, No. 2, </volume> <year> 1984, </year> <pages> pp. 164-186. </pages>
Reference-contexts: Some important iterative A/D methods for nearly uncoupled Markov chains have been proposed by Takahashi [10], Vantilborgh [11], Koury, McAllister & Stewart <ref> [6] </ref>, Kim & Smith [5], and Sumita & Rieders [9]. <p> (I P kk ) = i (m1) D k P flk where D k = diag ( 1 (m) ; (m) 2 e k1 (m) ; (m1) k+1 e k+2 (m1) ; : : : ; q (m1) ): 5.2 Koury, McAllister, and Stewart (KMS) Algorithm Koury, McAllister and Stewart <ref> [6] </ref> introduced an iterative A/D algorithm that is similar to Takahashi algorithm, but the KMS algorithm differs from Takahashi's algorithm in step 3. In the case of KMS, step 3 is as follows. 3.
Reference: [7] <author> Meyer, C. D.: </author> <title> "Stochastic Complementation, Uncoupling Markov Chains, and the Theory of Nearly Reducible Systems," </title> <journal> SIAM Review, </journal> <volume> Vol. 31, No. 2, </volume> <pages> pp. 240-272, </pages> <year> 1989. </year>
Reference-contexts: 2 &lt; 1fi (n+q) , satisfies = (; ~ ~ A) 7 4 General Aggregation Step The concept of stochastic complementation provides a theoretical basis for reducing a single large chain into a collection of smaller independent chains whose solutions can be aggregated to solve the original chain, see Meyer <ref> [7] </ref> and the references contained therein. Definition 4.1 (Meyer [7]) The stochastic complement of P kk in P, is defined to be the matrix S [k] = P kk + P kfl (I P k ) 1 P flk where P kfl and P flk are the k th row and <p> ~ A) 7 4 General Aggregation Step The concept of stochastic complementation provides a theoretical basis for reducing a single large chain into a collection of smaller independent chains whose solutions can be aggregated to solve the original chain, see Meyer <ref> [7] </ref> and the references contained therein. Definition 4.1 (Meyer [7]) The stochastic complement of P kk in P, is defined to be the matrix S [k] = P kk + P kfl (I P k ) 1 P flk where P kfl and P flk are the k th row and k th column of blocks in P except that
Reference: [8] <author> Schweitzer, P. J.: </author> <title> "Aggregation Methods for Large Markov Chains," in Mathematical Computer Performance and Reliability, </title> <editor> G. Iazeolla, P. J. Courtois and A. Hordijk (editors), </editor> <publisher> Elsevier Science Publishers B. V. North Holland, </publisher> <address> Amsterdam, </address> <year> 1984. </year>
Reference-contexts: The smallest such subset J fl j of J j is a recurrent set of J j . This structure includes as special cases the Single Input Superstate structure in Feinburg and Chiu [3] and the "exactly lumpable" structure in Schweitzer <ref> [8] </ref>. In the former case J fl j is the single input superstate corresponding to J j , fj fl g, while in the latter case J fl j = J j .
Reference: [9] <author> Sumita, U. and Rieders, M.: </author> <title> "A New Algorithm for Computing the Ergodic Probability Vector for Large Markov Chains: Replacement Process Approach," The First International Conference on the Numerical Solution of Markov Chains, </title> <year> 1990. </year>
Reference-contexts: Some important iterative A/D methods for nearly uncoupled Markov chains have been proposed by Takahashi [10], Vantilborgh [11], Koury, McAllister & Stewart [6], Kim & Smith [5], and Sumita & Rieders <ref> [9] </ref>. <p> framework, the KMS algorithm uses ~ D in equation (25) instead of D, where ~ D k = diag ( 1 (m1) ; (m) i 2 k1 (m1) ; (m1) i k+1 k+2 (m1) ; : : : ; q (m1) ): 5.3 Sumita and Rieders Algorithm Sumita and Rieders <ref> [9] </ref> introduced an iterative A/D algorithm that is different than both Takahashi and KMS algorithms in the sense that each iteration is a sequence of aggregation and stochastic complement steps. 1. <p> Solve for the stationary probability distribution of W [k](m) , OE (m) OE k W [k](m) = OE k (m) The original Sumita and Rieders algorithm that is presented in <ref> [9] </ref> uses the latest updated OE i in step 3.1.
Reference: [10] <author> Takahashi, Y.: </author> <title> "A Lumping Method for Numerical Calculations of Stationary Distributions of Markov Chains," </title> <note> Research Report No. </note> <institution> B-18, Department of Information Sciences, Tokyo Institute of Technology, </institution> <address> Tokyo, Japan, </address> <month> June </month> <year> 1975. </year>
Reference-contexts: Some important iterative A/D methods for nearly uncoupled Markov chains have been proposed by Takahashi <ref> [10] </ref>, Vantilborgh [11], Koury, McAllister & Stewart [6], Kim & Smith [5], and Sumita & Rieders [9]. <p> C and where P ij 2 &lt; n i fin j for all i; j = 1; 2; : : : ; q, j 2 &lt; 1fin j for all j = 1; 2; : : : ; q, and n = i=1 n i . 5.1 Takahashi Algorithm Takahashi <ref> [10] </ref> introduced an iterative A/D algorithm based on a sequence of aggregation steps. His algorithm can be described as follows. 1.
Reference: [11] <author> Vantilborgh, H.: </author> <title> The Error of Aggregation, A Contribution to the Theory of Decomposable Systems and Applications. </title> <institution> These de Docteur en Sciences Appliquees, Faculte des Sciences Appliquees, Universite Catholique de Louvain-la-Neuve, Bel-gium, </institution> <year> 1981. </year> <month> 22 </month>
Reference-contexts: Some important iterative A/D methods for nearly uncoupled Markov chains have been proposed by Takahashi [10], Vantilborgh <ref> [11] </ref>, Koury, McAllister & Stewart [6], Kim & Smith [5], and Sumita & Rieders [9]. <p> In other words, step 3.1 becomes W [k](m) = P kk + P kfl A k (I D k P k A k ) 1 D k P flk where D k = diag i (m) (m) (m) (m1) (m1) q : 5.4 Vantilborgh Algorithm Vantilborgh <ref> [11] </ref> introduced an iterative A/D algorithm that is similar to Sumita and Rieders algorithm in the sense that each iteration consists of a sequence of aggregation steps and stochastic complementation steps for which each combined step solves for the probability distribution of a distinct block. <p> Solve for the stationary probability distribution of Z [k](m) , OE (m) OE k Z [k](m) = OE k ; OE k e = 1 The original Vantilborgh algorithm that is presented in <ref> [11] </ref> differs in that Z [k](m) is computed as Z [k](m) = P kk + P kfl e (m1) (m1) i fl D k P flk e The following lemma shows that both steps are identical.
References-found: 11

