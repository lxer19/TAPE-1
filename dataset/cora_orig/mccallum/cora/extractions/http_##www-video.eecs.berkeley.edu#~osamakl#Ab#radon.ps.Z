URL: http://www-video.eecs.berkeley.edu/~osamakl/Ab/radon.ps.Z
Refering-URL: http://www-video.eecs.berkeley.edu/~osamakl/Ab/
Root-URL: 
Title: Invariant Image Analysis Based on Radon Transform and SVD  
Author: Osama K. Al-Shaykh and John F. Doherty 
Date: 2, FEBRUARY 1996 1  
Note: IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS-II: ANALOG AND DIGITAL SIGNAL PROCESSING, VOL. 43, NO.  
Abstract: A Radon-based invariant image analysis method is introduced. The linearity, shift, rotation and scaling properties of the Radon transform are utilized to achieve invariant features to translation, rotation and scaling. The singular values of a matrix, constructed by row-stacking of projections, are used to construct the invariant feature vector. This feature vector will be used as input to a classifier which is here the back-propagation neural network followed by a maximum-output-selector. A performance function is introduced to evaluate the performance of the recognition system. This performance function can also be used to indicate how closely the pattern matches the decision template. The effectiveness of this method is illustrated by a simulation example and it is compared with the method of Zernike moments. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Wong and J. Steppe, </author> <title> "Invariant recognition of geometric shapes," in Methodologies of pattern recognition (S. Watanabe, </title> <publisher> ed.), </publisher> <pages> pp. 535-546, </pages> <publisher> Academic press, </publisher> <address> New York, NY, </address> <year> 1969. </year>
Reference-contexts: This function is analogous to the membership function defined in fuzzy set theory [32]. Along with some defined rules, it can be used to give a syntactic description of the output of the neural network. When this function is used as a membership function, its range will be <ref> [0; 1] </ref>. This is because Y (n) Y (k) for all k 6= n. III. RESULTS A. Introduction In this section, the proposed method is examined and its merits are illustrated. The method is then compared to the method of moments.
Reference: [2] <author> H. Al-Yousofi and S. Udpa, </author> <title> "Recognition of Arabic characters," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 14, </volume> <pages> pp. 853-857, </pages> <month> July </month> <year> 1992. </year> <title> AL-SHAYKH AND DOHERTY: INVARIANT IMAGE ANALYSIS 11 </title>
Reference: [3] <author> E. Persoon and K. Fu, </author> <title> "Shape discrimination using Fourier descriptors," </title> <journal> IEEE Transactions on Systems, Man, Cybernetics, </journal> <volume> vol. SMC-7, </volume> <pages> pp. 170-179, </pages> <month> March </month> <year> 1977. </year>
Reference: [4] <author> S. Udpa and W. Lord, </author> <title> "A Fourier descriptor classification scheme for differential probe signals," </title> <journal> Materials evaluation, </journal> <volume> vol. 42, </volume> <pages> pp. 1136-1141, </pages> <month> August </month> <year> 1984. </year>
Reference: [5] <author> A. Jain, </author> <title> Fundamentals of digital image processing. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: These properties were used to process images using one dimensional systems [18]. Let g (s; ) be the projections, Radon transform, of an image, two dimensional function, f (x; y) or f p (r; ) in the polar coordinates, then the following properties hold <ref> [5] </ref>: * Symmetry: g (s; ) = g (s; ). * Translation: Rff (x x 0 ; y y 0 )g = g (s x 0 cos y 0 sin ). * Rotation: Rff p (r; + 0 )g = g (s; + 0 ). * Scaling: Rff (ffx; ffy)g = <p> The Inverse Radon transform theory <ref> [5] </ref> states that f (r; ) = 2 2 0 s r cos ( ) s Substituting Equation (21) into Equation (23) results in f (r; ) = 2 2 0 s P r cos ( ) s f (r; ) = k 2 2 0 Z [@w k (s)=@s] ds
Reference: [6] <author> M. Hu, </author> <title> "Visual pattern recognition by moment invariants," </title> <journal> IRE Transactions on Information Theory, </journal> <volume> vol. IT-8, </volume> <pages> pp. 179-187, </pages> <month> February </month> <year> 1962. </year>
Reference: [7] <author> R. Wong and E. Hall, </author> <title> "Scene matching with invariant moments," </title> <journal> Computer Graphics and Image Processing, </journal> <volume> vol. 8, </volume> <pages> pp. 16-24, </pages> <year> 1978. </year>
Reference: [8] <author> C. Teh and R. Chin, </author> <title> "On image analysis by the methods of moments," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 10, </volume> <pages> pp. 496-512, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: Moment-based invariant image recognition compares well with the results of other popular invariant feature extraction [9]. The performance of the Zernike moments has been shown to be superior to the performance of other types of moments in terms of sensitivity to image noise and information content <ref> [8] </ref>. The same experiment conducted to study the effect of AL-SHAYKH AND DOHERTY: INVARIANT IMAGE ANALYSIS 7 Fig. 6. The shift of the first left singular vector due to the rotation of the staple remover by an angle of 0 ffi , 10 ffi , and 20 ffi respectively. <p> Hu first introduced invariant recognition using moments [33]. Based on geometric moments and algebraic invariants, he derived a set of invariant features. Afterwards, orthogonal moments have been suggested, e.g., Zernike, pseudo Zernike, and Legendre moments. Other types of moments have been used too, e.g., rotational and complex moments <ref> [8] </ref>. In the following section Zernike moments, which outperforms other types of moments, will be discussed. We compare the SVD-based recognition system to the Zernike moments method in the Section III. 10 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS-II: ANALOG AND DIGITAL SIGNAL PROCESSING, VOL. 43, NO. 2, FEBRUARY 1996 A. <p> Zernike Moments Zernike moments, A nl , of an image , f (r; ), are defined as its projections on a class of polynomials, called Zernike polynomials. These polynomials are separable in the polar coordinates and are orthogonal over the unit circle <ref> [8] </ref>, [34]. <p> This results in using 28, 34, or 40 features respectively [34]. The performance of the Zernike moments has been shown to outperform other types of moments in terms of sensitiv ity to image noise, information redundancy, and capability for image representation <ref> [8] </ref>. IV. DERIVATIONS A. Rotation Invariance The relation between singular values of the Radon trans form of the rotated image and the image itself is derived here.
Reference: [9] <author> S. Perantonis and P. </author> <title> Lisboa, "Translation, rotation, and scale invariant pattern recognition by higher-order neural networks and moment classifiers," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 3, </volume> <pages> pp. 241-251, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: E. Comparison with the Method of Moments This experiment is conducted to compare the performance of the Radon-based invariant image recognition method with the method of Zernike moments. Moment-based invariant image recognition compares well with the results of other popular invariant feature extraction <ref> [9] </ref>. The performance of the Zernike moments has been shown to be superior to the performance of other types of moments in terms of sensitivity to image noise and information content [8]. The same experiment conducted to study the effect of AL-SHAYKH AND DOHERTY: INVARIANT IMAGE ANALYSIS 7 Fig. 6. <p> be reconstructed from the Zernike moments by 1 X 1 X njlj=even jljn A nl V nl (r; ) (19) It can be approximated by f (r; ) = n=0 l njlj=even jljn A nl V nl (r; ) In pattern recognition applications, n usually equals 9, 10, or 12 <ref> [9] </ref>. This results in using 28, 34, or 40 features respectively [34]. The performance of the Zernike moments has been shown to outperform other types of moments in terms of sensitiv ity to image noise, information redundancy, and capability for image representation [8]. IV. DERIVATIONS A.
Reference: [10] <author> W. Pitts and W. McCulloch, </author> <title> "How we perceive universals: The perception of auditory and visual forms," </title> <journal> Bulletin of Mathematical Biophysics, </journal> <volume> vol. 9, </volume> <pages> pp. 127-147, </pages> <year> 1947. </year>
Reference: [11] <author> E. Barnard and D. Casasent, </author> <title> "Invariance and neural nets," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 2, </volume> <pages> pp. 498-508, </pages> <month> September </month> <year> 1991. </year>
Reference: [12] <author> R. Hecht-Nielsen, </author> <title> Neurocomputing. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1990. </year>
Reference: [13] <author> K. Fukushima and N. </author> <title> Wake, "Handwritten alphanumeric character recognition by the neucognitron," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 2, </volume> <pages> pp. 355-365, </pages> <month> May </month> <year> 1991. </year>
Reference: [14] <author> T. Maxwell and C. Giles, </author> <title> "Transformation invariance using high order correlation in neural net architectures," </title> <booktitle> in Proc. IEEE International Conference on Systems, Man and Cybernetics, Atlanta, Georgia, </booktitle> <pages> pp. 627-632, </pages> <month> October </month> <year> 1986. </year>
Reference: [15] <author> S. Kollias, A. Stafylopatis, and A. Tirakis, </author> <title> "Performance of higher order neural networks in invariant recognition," in Neural Networks: Advances and Applications (E. Gelenbe, </title> <publisher> ed.), </publisher> <pages> pp. 79-108, </pages> <publisher> Elsevier Science Publisher B.V (North-Holland), </publisher> <year> 1991. </year>
Reference: [16] <author> B. Widrow, R. Winter, and R. Baxter, </author> <title> "Layered neural nets for pattern recognition," </title> <journal> IEEE Transactions on Acoustics, Speech, Signal Processing, </journal> <volume> vol. 36, no. 7, </volume> <pages> pp. 1109-1118, </pages> <year> 1988. </year>
Reference: [17] <author> M. Fukumi, S. Omatu, F. Takeda, and T. Kosaka, </author> <title> "Rotation-invariant neural pattern recognition system with application to coin recognition," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 3, </volume> <pages> pp. 272-279, </pages> <month> March </month> <year> 1992. </year>
Reference: [18] <author> J. Sanz, E. Hinkle, and A. Jain, </author> <title> Radon and projection transform-based computer vision. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1988. </year>
Reference-contexts: Moreover, since M t N most of the computation burden is in computing the projections. Thus, according to our implementation, the Radon-based features are less expensive to compute. Moreover, fast Radon transform methods can be used to compute the projections <ref> [18] </ref>, [29]. IV. CONCLUSIONS A discrete Radon transform method for invariant image analysis using artificial neural networks was developed. This method is based on utilizing the projections of the image to achieve invariant features. <p> These properties can be utilized to achieve invariant features of the image in the Radon transform domain. These properties were used to process images using one dimensional systems <ref> [18] </ref>.
Reference: [19] <author> J. </author> <title> Radon, "Translation of Radon's 1917 paper, (translated by R. </title> <type> Lohner, </type> <institution> school of mathematics, Georgia institute of technology)," </institution> <note> in The Radon transform and some of its applications (S. </note> <editor> R. Deans, ed.), </editor> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: Appendix I. The Radon transform In 1917, Radon presented a paper on the determination of functions from their integrals along certain manifolds. He derived an explicit formula for the reconstruction of a function on the plane given its integral over all lines <ref> [19] </ref>. The Radon transform, ray sum, shadowgram, or projections of a function, denoted as g (s; ) is defined as its line integral along a line inclined at an angle from the y axis at a distance s from the origin.
Reference: [20] <author> S. Deans, </author> <title> The Radon transform and some of its applications. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1983. </year>
Reference: [21] <author> I. Gelfand and S. Gindikin, </author> <title> "Mathematical problems of tomography," Translations of mathematical monographs, </title> <journal> American mathematical society, </journal> <volume> vol. 81, </volume> <year> 1990. </year>
Reference: [22] <author> G. Henkin and A. Shananin, </author> <title> Bernstein theorems and the Radon transform. Application to the theory of production functions, </title> <journal> vol. </journal> <volume> 81. </volume> <publisher> American mathematical society, </publisher> <year> 1990. </year>
Reference: [23] <author> A. Kak and M. Slaney, </author> <title> Principles of computerized tomographic imaging. </title> <publisher> IEEE Press, </publisher> <address> New York, </address> <year> 1988. </year>
Reference: [24] <author> Y. U and G. Flachs, </author> <title> "Structural extraction by projections," </title> <booktitle> in Region V IEEE Conference digest on electrical engineers for this decade, </booktitle> <address> Austin, Texas, </address> <pages> pp. 15-19, </pages> <month> April </month> <year> 1976. </year>
Reference: [25] <author> O. Mitchell and S. Lutton, </author> <title> "Segmentation and classification of targets in flir imagery," </title> <booktitle> SPIE, Image Understanding Systems & Industrial Applications, </booktitle> <volume> vol. 155, </volume> <pages> pp. 83-90, </pages> <year> 1978. </year>
Reference: [26] <author> A. Reeves and A. Rostampour, </author> <title> "Shape analysis of segmented objects using moments," </title> <booktitle> in Proceedings of the IEEE Computer Society Conference on Pattern Recognition and Image Processing, </booktitle> <pages> pp. 171-174, </pages> <month> August </month> <year> 1981. </year>
Reference: [27] <author> P. Madhvapathy, </author> <title> "Pattern recognition using simple measures of projections," </title> <type> Master's thesis, </type> <institution> Colorado State University, </institution> <year> 1986. </year>
Reference: [28] <author> W. Pratt, </author> <title> Digital image processing. </title> <publisher> Wiley-Interscience, </publisher> <address> New York, </address> <year> 1991. </year>
Reference: [29] <author> B. Kelley and V. Madisetti, </author> <title> "The fast discrete Radon transform," </title> <booktitle> in Proceedings of the International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <year> 1992. </year>
Reference-contexts: Moreover, since M t N most of the computation burden is in computing the projections. Thus, according to our implementation, the Radon-based features are less expensive to compute. Moreover, fast Radon transform methods can be used to compute the projections [18], <ref> [29] </ref>. IV. CONCLUSIONS A discrete Radon transform method for invariant image analysis using artificial neural networks was developed. This method is based on utilizing the projections of the image to achieve invariant features.
Reference: [30] <author> P. Maass, </author> <title> "Singular value decomposition for Radon transforms," in Mathematical methods in tomography, </title> <booktitle> Lecture notes in mathematics 1497 (G. </booktitle> <editor> Herman, A. Louis, and F. Netterer, </editor> <booktitle> eds.), </booktitle> <pages> pp. 6-14, </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: Au n = n v n and A fl v n = n u n A fl is the adjoint of A The singular values , n , are usually ordered such that 1 2 n &gt; 0 <ref> [30] </ref>. The singular functions fu n g are sometimes called the generalized eigenfunctions, since (A fl A)u n = 2 A can be constructed from the triple by A = n v n u fl III.
Reference: [31] <author> B. Widrow and M. Lehr, </author> <title> "30 years of adaptive neural networks: perceptron, madaline, and back-propagation," </title> <journal> IEEE Proceedings, </journal> <volume> vol. 78, </volume> <pages> pp. 1415-1442, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: The classifier used here is a back-propagation neural network followed by a maximum-output-selector. The adaptive nature of neural networks as well as their ability to generalize from training data justifies their use for classification in our system <ref> [31] </ref>. They are expected to overcome the rotation resolution error problem. This will be illustrated by the results obtained for worst case resolution, i.e., when the angle of rotation taken is in the middle of the angular sampling period.
Reference: [32] <author> L. Zadeh, </author> <title> "Fuzzy sets," </title> <journal> Information and Control, </journal> <volume> vol. 8, </volume> <pages> pp. 338-353, </pages> <year> 1965. </year>
Reference-contexts: This function is used to give an indication of how closely a pattern matches the template the system classified. This function is analogous to the membership function defined in fuzzy set theory <ref> [32] </ref>. Along with some defined rules, it can be used to give a syntactic description of the output of the neural network. When this function is used as a membership function, its range will be [0; 1]. This is because Y (n) Y (k) for all k 6= n. III.
Reference: [33] <author> M. Hu, </author> <title> "Visual pattern recognition by moment invariants," </title> <journal> IRE Transactions on Information Theory, </journal> <volume> vol. IT-8, </volume> <pages> pp. 179-187, </pages> <month> February </month> <year> 1962. </year>
Reference-contexts: Methods of moments Methods of moments have been used in several forms to extract invariant features from images. Hu first introduced invariant recognition using moments <ref> [33] </ref>. Based on geometric moments and algebraic invariants, he derived a set of invariant features. Afterwards, orthogonal moments have been suggested, e.g., Zernike, pseudo Zernike, and Legendre moments. Other types of moments have been used too, e.g., rotational and complex moments [8].
Reference: [34] <author> A. Khotanzad and Y. H. Hong, </author> <title> "Invariant image recognition by Zernike moments," </title> <journal> Transactions on Pattern Recognition and Machine Intelligence, </journal> <volume> vol. 12, </volume> <pages> pp. 489-497, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Zernike Moments Zernike moments, A nl , of an image , f (r; ), are defined as its projections on a class of polynomials, called Zernike polynomials. These polynomials are separable in the polar coordinates and are orthogonal over the unit circle [8], <ref> [34] </ref>. <p> This results in using 28, 34, or 40 features respectively <ref> [34] </ref>. The performance of the Zernike moments has been shown to outperform other types of moments in terms of sensitiv ity to image noise, information redundancy, and capability for image representation [8]. IV. DERIVATIONS A.
Reference: [35] <author> W. Press, B. Flannery, S. Teukolsky, and W. Vetterling, </author> <title> Numerical recipes in C, </title> <booktitle> the art of scientific computing. </booktitle> <publisher> Cambridge University Press, </publisher> <year> 1988. </year>
Reference-contexts: The hidden layer consists of 14 nodes. The output layer consists of 5 nodes corresponding to the five output classes. The singular values are calculated using the subroutine provided by Press et al. <ref> [35] </ref>. The remainder of this section is organized as follows: Section III-B discusses the effect of the number of projections used to construct the projection matrix on recognition. Section III-C illustrates the robustness of the extracted features when the object is rotated or scaled. <p> Thus, approximately M N 2 additions are required to compute the projections. The singular values of the M fi N projection matrix are calculated using the subroutine provided by Press et al. <ref> [35] </ref>. The subroutine involves Householder reduction to a bidiagonal form which requires approximately 2M 2 =3 multiplications and 2M 2 =3 AL-SHAYKH AND DOHERTY: INVARIANT IMAGE ANALYSIS 9 additions [36]. Afterwards, implicit QR algorithm is used to diagonalize the bidiagonal matrix.

References-found: 35

