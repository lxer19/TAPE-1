URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/BEFORENYU/Hertz-Krogh-Palmer.AIJ-Review.ps.Z
Refering-URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/BEFORENYU/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Book Review  Introduction to the Theory of Neural Computation Reviewed by: 2  
Author: John A. Hertz, Anders S. Krogh, and Richard G. Palmer, Andreas S. Weigend 
Date: (Received June 1992; Revised October 1992)  June 1993:  
Address: 3333 Coyote Hill Road Palo Alto, CA 94304  Boulder, CO 80309-0430, USA.  
Affiliation: Xerox PARC  Department of Computer Science and Institute of Cognitive Science, University of Colorado,  
Note: To appear in ARTIFICIAL INTELLIGENCE (1993) (Elsevier Science Publishers)  Lecture Notes Volume I in the Santa Fe Institute Studies in the Sciences of Complexity. 352 pages. US$ 30.25 (paperback). ISBN 0-201-51560-1. (Library of Congress: QA76.5.H475) 2 Electronic mail: weigend@cs.colorado.edu Address after  
Abstract: Neural computation, also called connectionism, parallel distributed processing, neural network modeling or brain-style computation, has grown rapidly in the last decade. Despite this explosion, and ultimately because of impressive applications, there has been a dire need for a concise introduction from a theoretical perspective, analyzing the strengths and weaknesses of connectionist approaches and establishing links to other disciplines, such as statistics or control theory. The Introduction to the Theory of Neural Computation by Hertz, Krogh and Palmer (subsequently referred to as HKP) is written from the perspective of physics, the home discipline of the authors. The book fulfills its mission as an introduction for neural network novices, provided that they have some background in calculus, linear algebra, and statistics. It covers a number of models that are often viewed as disjoint. Critical analyses and fruitful comparisons between these models 
Abstract-found: 1
Intro-found: 1
Reference: <author> And, </author> <title> last but not least, I thank John Hertz, Anders Krogh and Richard Palmer for this great book. References AHMAD, </title> <booktitle> Subutai Book review of the Proceedings of the 1990 Connectionist Models Summer School. Artificial Intelligence, this volume (1993). </booktitle>
Reference: <author> BECKER, Suzanna and HINTON, Geoffrey E. </author> <title> A self-organizing neural network that discovers surfaces in random-dot stereograms. </title> <journal> Nature, </journal> <volume> 355 </volume> <month> 161-163 </month> <year> (1992). </year>
Reference: <author> BRIDLE, John S. </author> <title> Probabilistic interpretation of feedforward classification network outputs. </title>
Reference: <editor> In Fogelman-Soulie, F. and Herault, J., eds. Neurocomputing: </editor> <booktitle> Algorithms, Architectures and Applications, p. </booktitle> <pages> 223-236. </pages> <note> Springer (1990). page 16 References BUNTINE, </note> <author> Wray L. and WEIGEND, Andreas S. </author> <title> Bayesian backpropagation. </title> <journal> Complex Systems, </journal> <volume> 5 </volume> <month> 603-643 </month> <year> (1991). </year>
Reference: <editor> CHAUVIN, Yves and RUMELHART, David E., eds. </editor> <title> Backpropagation: Theory, Architectures and Applications. </title> <publisher> Lawrence Erlbaum (1993). </publisher>
Reference-contexts: in their wide-ranging and balanced perspective on the state of the art are the proceedings of the Connectionist Models Summer Schools (volume 2 is edited by Touretzky et al., 1990, and reviewed by Ahmad, 1993). 15 The principles and theory behind backpropagation are analyzed in Backpropagation: Theory, Architectures and Applications <ref> (edited by Chauvin and Rumelhart, 1993) </ref>. The book offers a number of perspectives including statistics, machine learning and dynamical systems, and presents applications to several fields related to the cognitive sciences, such as control, speech recognition, robotics and image processing.
Reference: <author> DIRST, Matthew and WEIGEND, Andreas S. </author> <title> On the completion of J. </title> <editor> S. Bach's last fugue. In Weigend and Gershenfeld, eds. </editor> <year> (1993). </year>
Reference: <author> HINTON, Geoffrey E. and SEJNOWSKI, Terrence J. </author> <title> Learning and Relearning in Boltzmann Machines. </title> <editor> In Rumelhart and McClelland, eds., </editor> <volume> vol. 1, </volume> <editor> p. </editor> <month> 282-317 </month> <year> (1986). </year>
Reference: <author> JORDAN, Michael I. and RUMELHART, David E. </author> <title> Forward models: Supervised learning with a distal teacher. </title> <journal> Cognitive Science, </journal> <volume> 16 </volume> <month> 315-355 </month> <year> (1992). </year>
Reference: <editor> MOODY, John E., HANSON, Steven J., and LIPPMAN, Richard P., eds. </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> vol. 4 (NIPS*4). </volume> <publisher> Morgan Kaufmann (1992). </publisher>
Reference-contexts: For further reading, I would like to point out two proceedings series and two edited books. Many of the results given in HKP were presented at the annual conferences Neural Information Processing Systems in Denver, Colorado. The proceedings of these conferences <ref> (volume 4 is edited by Moody et al., 1992) </ref> are important collections of excellent, rigorously refereed papers of high quality.
Reference: <author> MOZER, Michael C. </author> <title> The induction of multiscale temporal structure. </title> <editor> In Moody et al., eds., p. </editor> <month> 275-282 </month> <year> (1992). </year>
Reference: <author> MOZER, Michael C. </author> <title> Neural net architectures for temporal sequence processing. </title> <editor> In Weigend and Gershenfeld, eds. </editor> <year> (1993). </year>
Reference: <author> MCCLELLAND, James L. and RUMELHART, David E. </author> <title> Explorations in Parallel Distributed Processing. </title> <publisher> MIT Press (1988). </publisher>
Reference: <author> RUMELHART, David E., DURBIN, Richard M., GOLDEN, Richard M., and CHAUVIN, Yves. </author> <title> Backpropagation: Theoretical foundations. </title> <editor> In Chauvin and Rumelhart, eds. </editor> <year> (1993). </year>
Reference-contexts: in their wide-ranging and balanced perspective on the state of the art are the proceedings of the Connectionist Models Summer Schools (volume 2 is edited by Touretzky et al., 1990, and reviewed by Ahmad, 1993). 15 The principles and theory behind backpropagation are analyzed in Backpropagation: Theory, Architectures and Applications <ref> (edited by Chauvin and Rumelhart, 1993) </ref>. The book offers a number of perspectives including statistics, machine learning and dynamical systems, and presents applications to several fields related to the cognitive sciences, such as control, speech recognition, robotics and image processing.
Reference: <author> RUMELHART, David E., MCCLELLAND, James L., </author> <title> and the PDP Research Group, </title> <editor> eds. </editor> <booktitle> Parallel Distributed Processing. </booktitle> <publisher> MIT Press (1986). </publisher>
Reference: <author> SKRZYPEK, Josef, ed. </author> <title> Neural Networks Simulation Environments. </title> <publisher> Kluwer (1993). </publisher>
Reference: <author> SMOLENSKY, Paul, MOZER, Michael C., and RUMELHART, David E., eds. </author> <title> Mathematical Perspectives on Neural Networks. </title> <publisher> Lawrence Erlbaum (1993). </publisher> <editor> TESAURO, </editor> <title> Gerald Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <month> 257-277 </month> <year> (1992). </year>
Reference-contexts: The increasingly major role that mathematical analysis plays as the field matures in the 1990s is recognized in Mathematical Perspectives on Neural Networks <ref> (edited by Smolensky et al., 1993) </ref>. That book provides clear introductions to the many relevant branches of mathematics, accounts of the advances that they have brought in the understanding of neural networks, and discussions of major open problems.
Reference: <editor> TOURETZKY, David S., ELMAN, Jeffrey L., SEJNOWSKI, Terrence J., and HINTON, Geoffrey E., eds. </editor> <booktitle> Proceedings of the 1990 Connectionist Models Summer School. </booktitle> <publisher> Morgan Kaufmann (1990). </publisher>
Reference: <author> WATKINS, Christopher J.C.H. and DAYAN, Peter Q-learning. </author> <title> Machine Learning, </title> <type> 8 </type> <month> 279-292 </month> <year> (1992). </year>
Reference: <author> WEIGEND, Andreas S. and GERSHENFELD, Neil A., eds. </author> <title> Predicting the Future and Understanding the Past: a Comparison of Approaches. </title> <booktitle> Santa Fe Institute Studies in the Sciences of Complexity, Proc. </booktitle> <volume> Vol. XVII. </volume> <publisher> Addison-Wesley (1993). </publisher>
Reference-contexts: The task was to predict their continuations. (The true continuations were kept secret until the competition deadline at the beginning of 1992.) Comparing different techniques applied to the same data sets had a surprising result: connectionist networks outperformed all other methods submitted <ref> (Weigend and Gershenfeld, 1993) </ref>. page 8 2 Learning and Generalization 2.2 Reinforcement Learning connections grows with window size, requiring more and more training samples for larger input windows. 10 Recurrent networks avoid some of these problems but are harder to train.
Reference: <author> WEIGEND, Andreas S. and RUMELHART, David E. </author> <title> Generalization through minimal networks with application to forecasting. </title> <editor> In Keramidas, E., ed. </editor> <booktitle> INTERFACE'91-23rd Symposium on the Interface: Computing Science and Statistics, p. </booktitle> <pages> 362-370. </pages> <booktitle> Interface Foundation (1991). </booktitle> <pages> page 17 </pages>
References-found: 20

