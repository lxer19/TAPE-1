URL: http://polaris.cs.uiuc.edu/reports/1435.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: INTERCONNECTION NETWORKS AND DATA PREFETCHING FOR LARGE-SCALE MULTIPROCESSORS: DESIGN AND PERFORMANCE  
Author: BY SUNIL KIM 
Degree: 1985 M.S., Seoul National University, 1987 THESIS Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy in Computer Science in the Graduate College of the  
Address: 1995 Urbana, Illinois  
Affiliation: B.S., Seoul National University,  University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> J. Chang, P. S. Song, and et al., </author> <title> "The PowerPC 604 RISC microprocessor," </title> <journal> IEEE Micro, </journal> <volume> vol. 14, </volume> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: Introduction Advances in VLSI technology have dramatically increased the speed of processors and memory as well as overall system performance in the last decade. In particular, recent advances in processor architectures <ref> [1, 2, 3, 4] </ref> and compiler technologies [5, 6, 7] have achieved tremendous computing power, and this trend should continue for the foreseeable future. Increasing computing power demands higher memory performance than ever before, making high-performance memory systems crucial for high-performance computing.
Reference: [2] <author> L. Gwennap, </author> <title> "Digital leads the pack with 21164: First of next-generation RISCs extends Alpha's performance lead," Microprocessor Report, </title> <journal> vol. </journal> <volume> 8, </volume> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: Introduction Advances in VLSI technology have dramatically increased the speed of processors and memory as well as overall system performance in the last decade. In particular, recent advances in processor architectures <ref> [1, 2, 3, 4] </ref> and compiler technologies [5, 6, 7] have achieved tremendous computing power, and this trend should continue for the foreseeable future. Increasing computing power demands higher memory performance than ever before, making high-performance memory systems crucial for high-performance computing.
Reference: [3] <author> L. Gwennap, </author> <title> "620 fills out PowerPC production line," Microprocessor Report, </title> <journal> vol. </journal> <volume> 8, </volume> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: Introduction Advances in VLSI technology have dramatically increased the speed of processors and memory as well as overall system performance in the last decade. In particular, recent advances in processor architectures <ref> [1, 2, 3, 4] </ref> and compiler technologies [5, 6, 7] have achieved tremendous computing power, and this trend should continue for the foreseeable future. Increasing computing power demands higher memory performance than ever before, making high-performance memory systems crucial for high-performance computing.
Reference: [4] <author> L. Gwennap, </author> <title> "MIPS R10000 uses decoupled architecture," Microprocessor Report, </title> <journal> vol. </journal> <volume> 8, </volume> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: Introduction Advances in VLSI technology have dramatically increased the speed of processors and memory as well as overall system performance in the last decade. In particular, recent advances in processor architectures <ref> [1, 2, 3, 4] </ref> and compiler technologies [5, 6, 7] have achieved tremendous computing power, and this trend should continue for the foreseeable future. Increasing computing power demands higher memory performance than ever before, making high-performance memory systems crucial for high-performance computing.
Reference: [5] <author> G. P. Lowney, S. M. Freudenberger, and et al., </author> <title> "The multiflow trace scheduling compiler," </title> <journal> The Journal of Supercomputing, </journal> <volume> vol. 7, </volume> <pages> pp. 51-142, </pages> <month> may </month> <year> 1993. </year>
Reference-contexts: Introduction Advances in VLSI technology have dramatically increased the speed of processors and memory as well as overall system performance in the last decade. In particular, recent advances in processor architectures [1, 2, 3, 4] and compiler technologies <ref> [5, 6, 7] </ref> have achieved tremendous computing power, and this trend should continue for the foreseeable future. Increasing computing power demands higher memory performance than ever before, making high-performance memory systems crucial for high-performance computing.
Reference: [6] <author> W.-M. W. Hwu, S. A. Mahlke, and et al., </author> <title> "The superblock: An effective technique for VLIW and superscalar compilation," </title> <journal> The Journal of Supercomputing, </journal> <volume> vol. 7, </volume> <pages> pp. 229-248, </pages> <year> 1993. </year>
Reference-contexts: Introduction Advances in VLSI technology have dramatically increased the speed of processors and memory as well as overall system performance in the last decade. In particular, recent advances in processor architectures [1, 2, 3, 4] and compiler technologies <ref> [5, 6, 7] </ref> have achieved tremendous computing power, and this trend should continue for the foreseeable future. Increasing computing power demands higher memory performance than ever before, making high-performance memory systems crucial for high-performance computing.
Reference: [7] <author> D. F. Bacon, G. L. Susan, and O. J. Sharp, </author> <title> "Compiler transformations for high-performance computing," </title> <journal> ACM Computing Surveys, </journal> <volume> vol. 26, </volume> <month> Dec. </month> <year> 1994. </year>
Reference-contexts: Introduction Advances in VLSI technology have dramatically increased the speed of processors and memory as well as overall system performance in the last decade. In particular, recent advances in processor architectures [1, 2, 3, 4] and compiler technologies <ref> [5, 6, 7] </ref> have achieved tremendous computing power, and this trend should continue for the foreseeable future. Increasing computing power demands higher memory performance than ever before, making high-performance memory systems crucial for high-performance computing.
Reference: [8] <author> C. B. Stunkel, D. G. Shea, D. G. Grice, P. H. Hochschild, and M. Tsao, </author> <title> "The SP1 high-performance switch," </title> <booktitle> in Proceedings of the Scalable High Performance Computing Conference, </booktitle> <pages> pp. 150-157, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: However as the system size grows, not only physical limitations but also costs make such a network infeasible for use in large-scale multiprocessor systems. Many different network architectures have been employed in recent commercial multiprocessor systems. For example, the IBM SP-2 uses a bidirectional multistage shu*e-exchange network <ref> [8, 9] </ref>, the Cray T3D uses a 3-D bidirectional torus network [10], the TMC CM-5 uses a fat-tree network [11], and the Intel Paragon uses a 2-D mesh network [12]. As yet there is no consensus on the best network organization for large-scale multiprocessor systems. <p> However, for a large-scale multiprocessor system, there is no consensus on the best network organization. Consequently, many different network architectures have been employed. For example, the IBM SP-2 uses a bidirectional multistage shu*e-exchange network <ref> [8, 9] </ref>, the Cray T3D uses a 3-D bidirectional torus network [10], the TMC CM-5 uses a fat-tree network [11] and the Intel Paragon uses a 2-D mesh network [12].
Reference: [9] <author> C. B. Stunkel, D. G. Shea, B. Abali, M. M. Denneau, P. H. Hochschild, D. J. Joseph, B. J. Nathanson, M. Tsao, and P. R. Varker, </author> <booktitle> "Architecture and implementation of Vulcan," in Proceeding of the 8th International Parallel Processing Symposium, </booktitle> <pages> pp. 268-274, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: However as the system size grows, not only physical limitations but also costs make such a network infeasible for use in large-scale multiprocessor systems. Many different network architectures have been employed in recent commercial multiprocessor systems. For example, the IBM SP-2 uses a bidirectional multistage shu*e-exchange network <ref> [8, 9] </ref>, the Cray T3D uses a 3-D bidirectional torus network [10], the TMC CM-5 uses a fat-tree network [11], and the Intel Paragon uses a 2-D mesh network [12]. As yet there is no consensus on the best network organization for large-scale multiprocessor systems. <p> However, for a large-scale multiprocessor system, there is no consensus on the best network organization. Consequently, many different network architectures have been employed. For example, the IBM SP-2 uses a bidirectional multistage shu*e-exchange network <ref> [8, 9] </ref>, the Cray T3D uses a 3-D bidirectional torus network [10], the TMC CM-5 uses a fat-tree network [11] and the Intel Paragon uses a 2-D mesh network [12].
Reference: [10] <author> R. K. Koeninger, M. Furtney, and M. Walker, </author> <title> "A shared memory MPP from Cray research," </title> <journal> Digital Technical Journal, </journal> <volume> vol. 6, </volume> <pages> pp. 8-21, </pages> <month> Spring </month> <year> 1994. </year>
Reference-contexts: Many different network architectures have been employed in recent commercial multiprocessor systems. For example, the IBM SP-2 uses a bidirectional multistage shu*e-exchange network [8, 9], the Cray T3D uses a 3-D bidirectional torus network <ref> [10] </ref>, the TMC CM-5 uses a fat-tree network [11], and the Intel Paragon uses a 2-D mesh network [12]. As yet there is no consensus on the best network organization for large-scale multiprocessor systems. <p> However, for a large-scale multiprocessor system, there is no consensus on the best network organization. Consequently, many different network architectures have been employed. For example, the IBM SP-2 uses a bidirectional multistage shu*e-exchange network [8, 9], the Cray T3D uses a 3-D bidirectional torus network <ref> [10] </ref>, the TMC CM-5 uses a fat-tree network [11] and the Intel Paragon uses a 2-D mesh network [12]. An appropriate network selection for a given system requires an in-depth study of various aspects of network design and trade-offs, not only for performance but also for cost.
Reference: [11] <author> C. Leiserson and et al., </author> <title> "The network architecture of the connection machine CM-5," </title> <booktitle> in Symposium on Parallel Algorithms and Architectures92, </booktitle> <pages> pp. 272-285, </pages> <month> June </month> <year> 1992. </year> <month> 156 </month>
Reference-contexts: Many different network architectures have been employed in recent commercial multiprocessor systems. For example, the IBM SP-2 uses a bidirectional multistage shu*e-exchange network [8, 9], the Cray T3D uses a 3-D bidirectional torus network [10], the TMC CM-5 uses a fat-tree network <ref> [11] </ref>, and the Intel Paragon uses a 2-D mesh network [12]. As yet there is no consensus on the best network organization for large-scale multiprocessor systems. This lack of agreement shows the need for an extensive comparative study of various interconnection network organizations in a given target system. <p> Consequently, many different network architectures have been employed. For example, the IBM SP-2 uses a bidirectional multistage shu*e-exchange network [8, 9], the Cray T3D uses a 3-D bidirectional torus network [10], the TMC CM-5 uses a fat-tree network <ref> [11] </ref> and the Intel Paragon uses a 2-D mesh network [12]. An appropriate network selection for a given system requires an in-depth study of various aspects of network design and trade-offs, not only for performance but also for cost.
Reference: [12] <author> R. Berrendorf, H. C. Burg, U. Detert, R. Esser, M. Gerndt, and R. Knecht, </author> <title> "Intel Paragon XP/S architecture, software environment, and performance," </title> <type> Tech. Rep. </type> <institution> KFA-ZAM-IB-9409, KFA Research Centre, Juelich, </institution> <year> 1994. </year>
Reference-contexts: For example, the IBM SP-2 uses a bidirectional multistage shu*e-exchange network [8, 9], the Cray T3D uses a 3-D bidirectional torus network [10], the TMC CM-5 uses a fat-tree network [11], and the Intel Paragon uses a 2-D mesh network <ref> [12] </ref>. As yet there is no consensus on the best network organization for large-scale multiprocessor systems. This lack of agreement shows the need for an extensive comparative study of various interconnection network organizations in a given target system. <p> Consequently, many different network architectures have been employed. For example, the IBM SP-2 uses a bidirectional multistage shu*e-exchange network [8, 9], the Cray T3D uses a 3-D bidirectional torus network [10], the TMC CM-5 uses a fat-tree network [11] and the Intel Paragon uses a 2-D mesh network <ref> [12] </ref>. An appropriate network selection for a given system requires an in-depth study of various aspects of network design and trade-offs, not only for performance but also for cost. However, comprehensive and detailed studies addressing both the performance and network costs of a given system are virtually non-existent.
Reference: [13] <author> A. Gupta, J. Hennessy, K. Gharachorloo, T. Mowry, and W.-D. Weber, </author> <title> "Comparative evaluation of latency reducing and tolerating techniques," </title> <booktitle> in International Symposium on Computer Architecture, </booktitle> <pages> pp. 254-263, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: This lack of agreement shows the need for an extensive comparative study of various interconnection network organizations in a given target system. Different approaches can be taken in order to reduce the effect of large memory access latency. These include caches, multithreading, and prefetching <ref> [13] </ref>. Caches have been very effective in certain application domains; however, programs accessing large data sets with little data reuse often show poor cache performance.
Reference: [14] <author> C. P. Kruskal and M. Snir, </author> <title> "The performance of multistage interconnection networks for multiprocessors," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-32, </volume> <pages> pp. 1091-1098, </pages> <month> Dec. </month> <year> 1983. </year>
Reference-contexts: However, very few comprehensive and detailed studies address both the performance and cost of a network for a given system. Moreover, most network performance studies <ref> [14, 15, 16, 17, 18] </ref> are based on open-loop systems [19] in which feedback effects [20] are completely ignored. As pointed out in [20, 21] feedback effects play an important role in determining achievable network performance.
Reference: [15] <author> L. N. Bhuyan, Q. Yang, and D. P. Agrawal, </author> <title> "Performance of multiprocessor interconnection networks," </title> <journal> IEEE Computer, </journal> <volume> vol. 22, </volume> <pages> pp. 25-37, </pages> <month> Feb. </month> <year> 1989. </year>
Reference-contexts: However, very few comprehensive and detailed studies address both the performance and cost of a network for a given system. Moreover, most network performance studies <ref> [14, 15, 16, 17, 18] </ref> are based on open-loop systems [19] in which feedback effects [20] are completely ignored. As pointed out in [20, 21] feedback effects play an important role in determining achievable network performance.
Reference: [16] <author> J. H. Patel, </author> <title> "Performance of processor-memory interconnections for multiprocessors," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-30, </volume> <pages> pp. 301-310, </pages> <month> Oct. </month> <year> 1981. </year>
Reference-contexts: However, very few comprehensive and detailed studies address both the performance and cost of a network for a given system. Moreover, most network performance studies <ref> [14, 15, 16, 17, 18] </ref> are based on open-loop systems [19] in which feedback effects [20] are completely ignored. As pointed out in [20, 21] feedback effects play an important role in determining achievable network performance.
Reference: [17] <author> D. M. Dias and J. R. </author> <title> Jump, "Analysis and simulation of buffered delta networks," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-30, </volume> <pages> pp. 331-340, </pages> <month> Apr. </month> <year> 1981. </year>
Reference-contexts: However, very few comprehensive and detailed studies address both the performance and cost of a network for a given system. Moreover, most network performance studies <ref> [14, 15, 16, 17, 18] </ref> are based on open-loop systems [19] in which feedback effects [20] are completely ignored. As pointed out in [20, 21] feedback effects play an important role in determining achievable network performance. <p> However, the performance of networks measured should not be significantly different from that of networks using a deadlock-free routing technique, such as structured buffer pools or virtual channels, as long as a large queue is used with such methods <ref> [61, 17] </ref>. 2.2 Data Prefetching In data prefetching, future memory accesses are predicted and data are moved to upper levels of a memory hierarchy. Data prefetching can be broadly classified into three groups: hardware data prefetching, software data prefetching, and hybrid data prefetching. <p> This gives us an upper bound on network performance. Other network studies have found that there is not much difference in the performance of networks with unlimited queue size and with a reasonable queue size <ref> [61, 17] </ref>. A snapshot mechanism and fixed priority arbitration are used to select a message among several messages destined to the same output port.
Reference: [18] <author> S. Abraham and K. Padmanabhan, </author> <title> "Performance of the direct binary n-cube network for multiprocessor," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 38, </volume> <pages> pp. 1000-1011, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: However, very few comprehensive and detailed studies address both the performance and cost of a network for a given system. Moreover, most network performance studies <ref> [14, 15, 16, 17, 18] </ref> are based on open-loop systems [19] in which feedback effects [20] are completely ignored. As pointed out in [20, 21] feedback effects play an important role in determining achievable network performance.
Reference: [19] <author> G. Lee, </author> <title> "A performance bound of mulitstage combining networks," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-32, no. 12, </volume> <pages> pp. 1091-1098, </pages> <year> 1989. </year>
Reference-contexts: However, very few comprehensive and detailed studies address both the performance and cost of a network for a given system. Moreover, most network performance studies [14, 15, 16, 17, 18] are based on open-loop systems <ref> [19] </ref> in which feedback effects [20] are completely ignored. As pointed out in [20, 21] feedback effects play an important role in determining achievable network performance.
Reference: [20] <author> J. E. Smith and W. R. Taylor, </author> <title> "Accurate modeling of interconnection networks in vector supercomputers," </title> <booktitle> in International Conference on Supercomputing, </booktitle> <pages> pp. 264-273, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: However, very few comprehensive and detailed studies address both the performance and cost of a network for a given system. Moreover, most network performance studies [14, 15, 16, 17, 18] are based on open-loop systems [19] in which feedback effects <ref> [20] </ref> are completely ignored. As pointed out in [20, 21] feedback effects play an important role in determining achievable network performance. <p> However, very few comprehensive and detailed studies address both the performance and cost of a network for a given system. Moreover, most network performance studies [14, 15, 16, 17, 18] are based on open-loop systems [19] in which feedback effects [20] are completely ignored. As pointed out in <ref> [20, 21] </ref> feedback effects play an important role in determining achievable network performance. We need to evaluate and compare interconnection networks in terms of performance and cost, and such a study should be based on the actual network performance achievable in a system rather than the maximum possible network performance. <p> This chapter performs such a detailed study of network behavior and design trade-offs with respect to performance and cost. The only previous work, to our knowledge, in which such a detailed study has been 54 attempted is <ref> [20, 21, 85] </ref>, yet network cost was not taken into account in these studies. As pointed out in [20], most previous network studies have relied on analytical modeling and/or simulations which are based on simplified assumptions. <p> The only previous work, to our knowledge, in which such a detailed study has been 54 attempted is [20, 21, 85], yet network cost was not taken into account in these studies. As pointed out in <ref> [20] </ref>, most previous network studies have relied on analytical modeling and/or simulations which are based on simplified assumptions. These approaches are sufficient for studying first-order effects, but not adequate for understanding the behavior and design trade-offs in detail. <p> In order to overcome this limitation, interconnection networks are studied in the context of the entire system via very detailed system simulations. The simulation of the entire system makes it possible to incorporate the feedback effect <ref> [21, 20] </ref> and the pipeline effect into a network study. The feedback effect is the dependence of message injection rates on the arrival rates of replies which, in turn, is affected by network and memory contention. The feedback effect bounds the message injection rate within a certain range.
Reference: [21] <author> S. Turner and A. Veidenbaum, </author> <title> "Performance of a shared-memory system in a vector multiprocessor," </title> <booktitle> in International Conference on Supercomputing, </booktitle> <pages> pp. 315-325, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: However, very few comprehensive and detailed studies address both the performance and cost of a network for a given system. Moreover, most network performance studies [14, 15, 16, 17, 18] are based on open-loop systems [19] in which feedback effects [20] are completely ignored. As pointed out in <ref> [20, 21] </ref> feedback effects play an important role in determining achievable network performance. We need to evaluate and compare interconnection networks in terms of performance and cost, and such a study should be based on the actual network performance achievable in a system rather than the maximum possible network performance. <p> This chapter performs such a detailed study of network behavior and design trade-offs with respect to performance and cost. The only previous work, to our knowledge, in which such a detailed study has been 54 attempted is <ref> [20, 21, 85] </ref>, yet network cost was not taken into account in these studies. As pointed out in [20], most previous network studies have relied on analytical modeling and/or simulations which are based on simplified assumptions. <p> In order to overcome this limitation, interconnection networks are studied in the context of the entire system via very detailed system simulations. The simulation of the entire system makes it possible to incorporate the feedback effect <ref> [21, 20] </ref> and the pipeline effect into a network study. The feedback effect is the dependence of message injection rates on the arrival rates of replies which, in turn, is affected by network and memory contention. The feedback effect bounds the message injection rate within a certain range.
Reference: [22] <author> X.-N. Tan and K. C. Sevcik, </author> <title> "Reduced distance routing in single-stage shu*e-exchange interconnection networks," </title> <type> tech. rep., </type> <institution> University of Toronto, </institution> <month> Sept. </month> <year> 1986. </year>
Reference-contexts: In single stage shu*e-exchange networks, a shortest path exists between two nodes. Although the same routing scheme used in multistage shu*e-exchange networks can be used in single stage shu*e-exchange networks, this scheme cannot take advantage of the shortest path. Previous work <ref> [22, 23, 24] </ref> presented a routing algorithm called fast-finishing, which provides a 3 shorter path than the routing scheme used in multistage shu*e-exchange networks. How--ever, this algorithm requires the network size be a power of the switch size, which make incremental scaling of the network size difficult. <p> In this chapter, we develop an algorithm that finds a routing tag for the shortest path in single stage networks. Although a fast-finishing routing algorithm was proposed in <ref> [22, 23, 24] </ref> to find shorter routes in single stage networks, it is limited to network sizes which are a power of K. In addition it has not been shown whether such a method guarantees the shortest path.
Reference: [23] <author> P.-C. Yew, </author> <title> On The Design of Interconnection Networks For Parallel and Multiprocessor Systems. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1981. </year>
Reference-contexts: In single stage shu*e-exchange networks, a shortest path exists between two nodes. Although the same routing scheme used in multistage shu*e-exchange networks can be used in single stage shu*e-exchange networks, this scheme cannot take advantage of the shortest path. Previous work <ref> [22, 23, 24] </ref> presented a routing algorithm called fast-finishing, which provides a 3 shorter path than the routing scheme used in multistage shu*e-exchange networks. How--ever, this algorithm requires the network size be a power of the switch size, which make incremental scaling of the network size difficult. <p> In this chapter, we develop an algorithm that finds a routing tag for the shortest path in single stage networks. Although a fast-finishing routing algorithm was proposed in <ref> [22, 23, 24] </ref> to find shorter routes in single stage networks, it is limited to network sizes which are a power of K. In addition it has not been shown whether such a method guarantees the shortest path.
Reference: [24] <author> P.-C. Yew, D. A. Padua, and D. H. Lawrie, </author> <title> "Stochastic properties of a multiple-layer single-stage shu*e-exchange network in a message switching environment," </title> <journal> Journal of Digital Systems, </journal> <volume> vol. VI, no. 4, </volume> <pages> pp. 387-410, </pages> <year> 1980. </year> <month> 157 </month>
Reference-contexts: In single stage shu*e-exchange networks, a shortest path exists between two nodes. Although the same routing scheme used in multistage shu*e-exchange networks can be used in single stage shu*e-exchange networks, this scheme cannot take advantage of the shortest path. Previous work <ref> [22, 23, 24] </ref> presented a routing algorithm called fast-finishing, which provides a 3 shorter path than the routing scheme used in multistage shu*e-exchange networks. How--ever, this algorithm requires the network size be a power of the switch size, which make incremental scaling of the network size difficult. <p> In this chapter, we develop an algorithm that finds a routing tag for the shortest path in single stage networks. Although a fast-finishing routing algorithm was proposed in <ref> [22, 23, 24] </ref> to find shorter routes in single stage networks, it is limited to network sizes which are a power of K. In addition it has not been shown whether such a method guarantees the shortest path.
Reference: [25] <author> D. M. Tullsen and S. J. Eggers, </author> <title> "Limitations of cache prefetching on a bus-based multiprocessor," </title> <booktitle> in International Symposium on Computer Architecture, </booktitle> <pages> pp. 278-288, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: However, prefetching has fundamental limitations because of high memory bandwidth demand and inaccurate prefetch data prediction. As pointed out in <ref> [25] </ref>, data prefetching is not effective if cache hit rate is already high without prefetching or if network and memory bandwidth is not sufficient to support it. Data prefetching tends to demand more memory bandwidth. Effective prefetching results in a faster memory access rate by reducing memory access stalls. <p> However, prefetching has fundamental lim 85 itations because of its high memory bandwidth demand and inaccurate predictions of future data accesses. Such limitations should be evaluated quantitatively to maximize system performance with prefetching. In <ref> [25] </ref>, prefetching was evaluated under limited bus bandwidth. The results showed that although an idealized "oracle" prefetching approach was used, prefetching only slightly improved system performance, and sometimes prefetching degraded performance. In the oracle approach, all cache misses are identified and prefetching instructions are inserted for such cache misses. <p> The results showed that although an idealized "oracle" prefetching approach was used, prefetching only slightly improved system performance, and sometimes prefetching degraded performance. In the oracle approach, all cache misses are identified and prefetching instructions are inserted for such cache misses. As observed in <ref> [25] </ref>, if the cache hit rate is already very high without prefetching, and the network bandwidth is not sufficient, it is hard to achieve better performance with prefetching.
Reference: [26] <author> J. W. Fu and J. H. Patel, </author> <title> "Data prefetching in multiprocessor vector cache memories," </title> <booktitle> in International Symposium on Computer Architecture, </booktitle> <pages> pp. 54-63, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Such high memory bandwidth demand of prefetching affects the relative performance of networks by favoring networks with higher bandwidth. Although there are several studies on prefetching in multiprocessor systems <ref> [26, 27, 28, 29, 30, 31] </ref>, there is no detailed quantitative study on prefetching, various network organizations, and their mutual effects. This thesis evaluates quantitatively whether a certain network can provide sufficient bandwidth for effective prefetching and studies different interconnection networks under the heavy traffic caused by prefetching. <p> In this chapter, we reevaluate and compare TORUS, MSX and SSX networks using a prefetching method; we also address prefetching and network bandwidth issues. Although 86 there are several studies on prefetching in multiprocessor systems <ref> [26, 27, 28, 29, 30, 31] </ref>, there is no detailed quantitative study on the prefetching and various network organizations and their mutual effects. This study investigates if these three different networks can provide sufficient bandwidth for prefetching and how the channel bandwidth affects the network performance with prefetching.
Reference: [27] <author> T. Mowry and A. Gupta, </author> <title> "Tolerating latency through software-controlled prefetching in shared-memory multiprocessors," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <pages> pp. 87-106, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Such high memory bandwidth demand of prefetching affects the relative performance of networks by favoring networks with higher bandwidth. Although there are several studies on prefetching in multiprocessor systems <ref> [26, 27, 28, 29, 30, 31] </ref>, there is no detailed quantitative study on prefetching, various network organizations, and their mutual effects. This thesis evaluates quantitatively whether a certain network can provide sufficient bandwidth for effective prefetching and studies different interconnection networks under the heavy traffic caused by prefetching. <p> In this chapter, we reevaluate and compare TORUS, MSX and SSX networks using a prefetching method; we also address prefetching and network bandwidth issues. Although 86 there are several studies on prefetching in multiprocessor systems <ref> [26, 27, 28, 29, 30, 31] </ref>, there is no detailed quantitative study on the prefetching and various network organizations and their mutual effects. This study investigates if these three different networks can provide sufficient bandwidth for prefetching and how the channel bandwidth affects the network performance with prefetching.
Reference: [28] <author> F. Dahlgren, M. Dubois, and P. Stenstrom, </author> <title> "Fixed and adaptive sequential prefetch-ing in shared memory multiprocessors," </title> <booktitle> in International Conference on Parallel Processing, </booktitle> <pages> pp. 56-63, </pages> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: Such high memory bandwidth demand of prefetching affects the relative performance of networks by favoring networks with higher bandwidth. Although there are several studies on prefetching in multiprocessor systems <ref> [26, 27, 28, 29, 30, 31] </ref>, there is no detailed quantitative study on prefetching, various network organizations, and their mutual effects. This thesis evaluates quantitatively whether a certain network can provide sufficient bandwidth for effective prefetching and studies different interconnection networks under the heavy traffic caused by prefetching. <p> In this chapter, we reevaluate and compare TORUS, MSX and SSX networks using a prefetching method; we also address prefetching and network bandwidth issues. Although 86 there are several studies on prefetching in multiprocessor systems <ref> [26, 27, 28, 29, 30, 31] </ref>, there is no detailed quantitative study on the prefetching and various network organizations and their mutual effects. This study investigates if these three different networks can provide sufficient bandwidth for prefetching and how the channel bandwidth affects the network performance with prefetching.
Reference: [29] <author> E. H. Gornish, E. D. Granston, and A. V. Veidenbaum, </author> <title> "Compiler-directed data prefetching in multiprocessors with memory hierarchies," </title> <booktitle> in International Conference on Supercomputing, </booktitle> <pages> pp. 354-368, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Such high memory bandwidth demand of prefetching affects the relative performance of networks by favoring networks with higher bandwidth. Although there are several studies on prefetching in multiprocessor systems <ref> [26, 27, 28, 29, 30, 31] </ref>, there is no detailed quantitative study on prefetching, various network organizations, and their mutual effects. This thesis evaluates quantitatively whether a certain network can provide sufficient bandwidth for effective prefetching and studies different interconnection networks under the heavy traffic caused by prefetching. <p> This method uses a non-unit stride filter that attempts to detect a memory reference stream and a stride. The filter uses the misses that fall into the same address space partition. The partition size is determined by programmers or compilers. 2.2.2 Software Data Prefetching In software data prefetching <ref> [63, 29, 64, 65] </ref>, prefetching instructions are explicitly inserted in programs. If data access patterns can be detected at compile time, this scheme can prefetch data for any memory access pattern. Example 2.2 shows how prefetch instructions are added to the original code. <p> In this chapter, we reevaluate and compare TORUS, MSX and SSX networks using a prefetching method; we also address prefetching and network bandwidth issues. Although 86 there are several studies on prefetching in multiprocessor systems <ref> [26, 27, 28, 29, 30, 31] </ref>, there is no detailed quantitative study on the prefetching and various network organizations and their mutual effects. This study investigates if these three different networks can provide sufficient bandwidth for prefetching and how the channel bandwidth affects the network performance with prefetching.
Reference: [30] <author> T.-F. Chen, </author> <title> Data Prefetching for High-Performance Processors. </title> <type> PhD thesis, </type> <institution> University of Washington, </institution> <year> 1993. </year>
Reference-contexts: Such high memory bandwidth demand of prefetching affects the relative performance of networks by favoring networks with higher bandwidth. Although there are several studies on prefetching in multiprocessor systems <ref> [26, 27, 28, 29, 30, 31] </ref>, there is no detailed quantitative study on prefetching, various network organizations, and their mutual effects. This thesis evaluates quantitatively whether a certain network can provide sufficient bandwidth for effective prefetching and studies different interconnection networks under the heavy traffic caused by prefetching. <p> In this chapter, we reevaluate and compare TORUS, MSX and SSX networks using a prefetching method; we also address prefetching and network bandwidth issues. Although 86 there are several studies on prefetching in multiprocessor systems <ref> [26, 27, 28, 29, 30, 31] </ref>, there is no detailed quantitative study on the prefetching and various network organizations and their mutual effects. This study investigates if these three different networks can provide sufficient bandwidth for prefetching and how the channel bandwidth affects the network performance with prefetching.
Reference: [31] <author> E. H. Gornish, </author> <title> Adaptive and Integrated Data Prefetching for Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, Center for Supercomputing Research and Development, </institution> <year> 1995. </year>
Reference-contexts: Such high memory bandwidth demand of prefetching affects the relative performance of networks by favoring networks with higher bandwidth. Although there are several studies on prefetching in multiprocessor systems <ref> [26, 27, 28, 29, 30, 31] </ref>, there is no detailed quantitative study on prefetching, various network organizations, and their mutual effects. This thesis evaluates quantitatively whether a certain network can provide sufficient bandwidth for effective prefetching and studies different interconnection networks under the heavy traffic caused by prefetching. <p> However, for the accesses to array B, tagged prefetching generates only unnecessary prefetches. Tagged prefetching is not effective for accesses whose stride is larger than twice the cache line size. 15 Several methods have been proposed <ref> [37, 31, 62] </ref> to reduce the number of unnecessary prefetches in tagged prefetching or stream buffers. <p> When the loop is executed, the table is indexed with the instruction addresses of the memory references and prefetch requests are generated accordingly. In a similar approach proposed in [69], prefetching hardware performs data prefetches autonomously by using additional loop bound information. Gornish <ref> [31] </ref> proposes another hybrid method in which a compiler generates information on prefetch degree for each memory reference. The prefetch degree is used to prefetch data several iterations ahead in the execution of a loop. <p> In this chapter, we reevaluate and compare TORUS, MSX and SSX networks using a prefetching method; we also address prefetching and network bandwidth issues. Although 86 there are several studies on prefetching in multiprocessor systems <ref> [26, 27, 28, 29, 30, 31] </ref>, there is no detailed quantitative study on the prefetching and various network organizations and their mutual effects. This study investigates if these three different networks can provide sufficient bandwidth for prefetching and how the channel bandwidth affects the network performance with prefetching.
Reference: [32] <author> J.-L. Baer and T.-F. Chen, </author> <title> "An effective on-chip preloading scheme to reduce data access penalty," </title> <booktitle> in Supercomputing, </booktitle> <pages> pp. 176-186, </pages> <publisher> IEEE, </publisher> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: Data prefetching can be used at both levels of a memory hierarchy to hide memory access latency for accessing secondary caches or main memory. Most data prefetching studies <ref> [32, 33, 34, 35, 36] </ref> focus on first-level, on-chip prefetching. Although some of prefetch-ing schemes can work without any problems at the second-level and off-chip, others such as stride-directed prefetching [32, 33, 34] may not work effectively. <p> Most data prefetching studies [32, 33, 34, 35, 36] focus on first-level, on-chip prefetching. Although some of prefetch-ing schemes can work without any problems at the second-level and off-chip, others such as stride-directed prefetching <ref> [32, 33, 34] </ref> may not work effectively. Stride-direct prefetch-ing is necessary to prefetch data for long stride memory accesses and is effective at the first-level and on-chip. However, at the second-level of a memory hierarchy and off-chip, instruction addresses are not easily available and only first-level cache misses are visible. <p> In these methods, the next line is not prefetched until a specific memory access pattern, such as accesses to consecutive lines, is detected. 2.2.1.2 Stride-Directed Prefetching Stride-directed prefetching has been proposed in several previous papers <ref> [32, 33, 34] </ref>. In this scheme, the stride of memory accesses is detected and used for data prefetching. If memory is accessed with a constant stride, stride-directed prefetching can successfully prefetch data for such memory accesses. In this scheme, first a memory reference stream is identified. <p> For example, B (4) and B (8) are compared and the stride information is obtained. The stride, 4, is added to the address of B (8) and used to prefetch B (12). 16 Most previously proposed methods <ref> [32, 33, 34] </ref> use instruction addresses to identify memory reference streams. These methods use a table called the Reference Prediction Table (RPT). Memory reference instructions are mapped into entries of the table. The table keeps an instruction address, a data address, and a stride in each of its entries. <p> Second-level, off-chip (L2) prefetching is different from L1 prefetching because only L1 cache miss addresses are visible, and instruction addresses may not be readily available. Such differences require a different approach to L2 prefetch-ing. Although various hardware data prefetching methods have been extensively studied in the literature <ref> [32, 33, 34, 35, 37, 36] </ref>, most of them focus on first-level, on-chip (L1) prefetching. Simple hardware prefetching schemes such as tagged prefetching [36] and stream buffers [35] can be used for L2 prefetching. <p> These methods are attractive for L2 prefetch-ing because only L1 cache miss addresses are necessary to determine the data to be 118 prefetched. However, these prefetching methods are effective only for memory accesses with good spatial locality or with short strides. Another prefetching scheme is stride-directed prefetching <ref> [32, 33, 34, 37] </ref>. In stride-directed prefetching, a memory reference stream, which is a sequence of data addresses generated by a specific memory reference instruction, is detected and used to calculate the stride associated with the memory reference stream. The calculated stride is used to predict future memory accesses. <p> The calculated stride is used to predict future memory accesses. Stride-directed prefetching is effective for memory references with any constant stride. In most previous studies <ref> [32, 33, 34] </ref>, a memory reference stream is detected by using the instruction address of the memory reference. This approach, which we refer to as instruction-address-based prefetching, is quite successful in L1 prefetching. <p> In such a case, after the last entry of the RPT is mapped, subsequent load executions are ignored until the iteration start signal is received again. Other operations, such as updating memory access history and obtaining strides, can be implemented similarly to the methods proposed in <ref> [32, 33, 34] </ref>. This method can be extended to handle multiple nested loops and control flow changes inside loops. <p> Each RPT entry keeps a data address (DA l ) and a stride (St l ). It is similar to other stream detection units <ref> [32, 33, 34] </ref>, but uses loop information to identify memory reference streams. The stream detection unit is accessed with the data addresses (M a ) that miss in both the stream buffers and the miss table unit. <p> This state transition is required to obtain a stable stride. In the two states, the prefetching unit reacts differently to an incoming L1 cache miss address (M a ). These operations and state transitions are similar to those of IAP methods proposed in <ref> [32, 33, 34] </ref>. The following algorithm describes the operation of the prefetching unit in each state. Note that L (M a ) is the line address of the data address M a .
Reference: [33] <author> J. W. Fu, J. H. Patel, and B. L. Janssens, </author> <title> "Stride directed prefetching in scalar processors," </title> <booktitle> in International Symposium on Microarchitecture, </booktitle> <pages> pp. 102-110, </pages> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: Data prefetching can be used at both levels of a memory hierarchy to hide memory access latency for accessing secondary caches or main memory. Most data prefetching studies <ref> [32, 33, 34, 35, 36] </ref> focus on first-level, on-chip prefetching. Although some of prefetch-ing schemes can work without any problems at the second-level and off-chip, others such as stride-directed prefetching [32, 33, 34] may not work effectively. <p> Most data prefetching studies [32, 33, 34, 35, 36] focus on first-level, on-chip prefetching. Although some of prefetch-ing schemes can work without any problems at the second-level and off-chip, others such as stride-directed prefetching <ref> [32, 33, 34] </ref> may not work effectively. Stride-direct prefetch-ing is necessary to prefetch data for long stride memory accesses and is effective at the first-level and on-chip. However, at the second-level of a memory hierarchy and off-chip, instruction addresses are not easily available and only first-level cache misses are visible. <p> In these methods, the next line is not prefetched until a specific memory access pattern, such as accesses to consecutive lines, is detected. 2.2.1.2 Stride-Directed Prefetching Stride-directed prefetching has been proposed in several previous papers <ref> [32, 33, 34] </ref>. In this scheme, the stride of memory accesses is detected and used for data prefetching. If memory is accessed with a constant stride, stride-directed prefetching can successfully prefetch data for such memory accesses. In this scheme, first a memory reference stream is identified. <p> For example, B (4) and B (8) are compared and the stride information is obtained. The stride, 4, is added to the address of B (8) and used to prefetch B (12). 16 Most previously proposed methods <ref> [32, 33, 34] </ref> use instruction addresses to identify memory reference streams. These methods use a table called the Reference Prediction Table (RPT). Memory reference instructions are mapped into entries of the table. The table keeps an instruction address, a data address, and a stride in each of its entries. <p> Second-level, off-chip (L2) prefetching is different from L1 prefetching because only L1 cache miss addresses are visible, and instruction addresses may not be readily available. Such differences require a different approach to L2 prefetch-ing. Although various hardware data prefetching methods have been extensively studied in the literature <ref> [32, 33, 34, 35, 37, 36] </ref>, most of them focus on first-level, on-chip (L1) prefetching. Simple hardware prefetching schemes such as tagged prefetching [36] and stream buffers [35] can be used for L2 prefetching. <p> These methods are attractive for L2 prefetch-ing because only L1 cache miss addresses are necessary to determine the data to be 118 prefetched. However, these prefetching methods are effective only for memory accesses with good spatial locality or with short strides. Another prefetching scheme is stride-directed prefetching <ref> [32, 33, 34, 37] </ref>. In stride-directed prefetching, a memory reference stream, which is a sequence of data addresses generated by a specific memory reference instruction, is detected and used to calculate the stride associated with the memory reference stream. The calculated stride is used to predict future memory accesses. <p> The calculated stride is used to predict future memory accesses. Stride-directed prefetching is effective for memory references with any constant stride. In most previous studies <ref> [32, 33, 34] </ref>, a memory reference stream is detected by using the instruction address of the memory reference. This approach, which we refer to as instruction-address-based prefetching, is quite successful in L1 prefetching. <p> In such a case, after the last entry of the RPT is mapped, subsequent load executions are ignored until the iteration start signal is received again. Other operations, such as updating memory access history and obtaining strides, can be implemented similarly to the methods proposed in <ref> [32, 33, 34] </ref>. This method can be extended to handle multiple nested loops and control flow changes inside loops. <p> Each RPT entry keeps a data address (DA l ) and a stride (St l ). It is similar to other stream detection units <ref> [32, 33, 34] </ref>, but uses loop information to identify memory reference streams. The stream detection unit is accessed with the data addresses (M a ) that miss in both the stream buffers and the miss table unit. <p> The instruction-address-based prefetching architectures we simulate are similar to those proposed in <ref> [33, 34] </ref>. The IAP architecture consists of an RPT, a comparator, an address adder and a stride calculator, as show in Figure 6.6. <p> This state transition is required to obtain a stable stride. In the two states, the prefetching unit reacts differently to an incoming L1 cache miss address (M a ). These operations and state transitions are similar to those of IAP methods proposed in <ref> [32, 33, 34] </ref>. The following algorithm describes the operation of the prefetching unit in each state. Note that L (M a ) is the line address of the data address M a .
Reference: [34] <author> Y. Jegou and O. Temam, </author> <title> "Speculative prefetching," </title> <booktitle> in Supercomputing, </booktitle> <pages> pp. 57 - 66, </pages> <year> 1993. </year>
Reference-contexts: Data prefetching can be used at both levels of a memory hierarchy to hide memory access latency for accessing secondary caches or main memory. Most data prefetching studies <ref> [32, 33, 34, 35, 36] </ref> focus on first-level, on-chip prefetching. Although some of prefetch-ing schemes can work without any problems at the second-level and off-chip, others such as stride-directed prefetching [32, 33, 34] may not work effectively. <p> Most data prefetching studies [32, 33, 34, 35, 36] focus on first-level, on-chip prefetching. Although some of prefetch-ing schemes can work without any problems at the second-level and off-chip, others such as stride-directed prefetching <ref> [32, 33, 34] </ref> may not work effectively. Stride-direct prefetch-ing is necessary to prefetch data for long stride memory accesses and is effective at the first-level and on-chip. However, at the second-level of a memory hierarchy and off-chip, instruction addresses are not easily available and only first-level cache misses are visible. <p> In these methods, the next line is not prefetched until a specific memory access pattern, such as accesses to consecutive lines, is detected. 2.2.1.2 Stride-Directed Prefetching Stride-directed prefetching has been proposed in several previous papers <ref> [32, 33, 34] </ref>. In this scheme, the stride of memory accesses is detected and used for data prefetching. If memory is accessed with a constant stride, stride-directed prefetching can successfully prefetch data for such memory accesses. In this scheme, first a memory reference stream is identified. <p> For example, B (4) and B (8) are compared and the stride information is obtained. The stride, 4, is added to the address of B (8) and used to prefetch B (12). 16 Most previously proposed methods <ref> [32, 33, 34] </ref> use instruction addresses to identify memory reference streams. These methods use a table called the Reference Prediction Table (RPT). Memory reference instructions are mapped into entries of the table. The table keeps an instruction address, a data address, and a stride in each of its entries. <p> Second-level, off-chip (L2) prefetching is different from L1 prefetching because only L1 cache miss addresses are visible, and instruction addresses may not be readily available. Such differences require a different approach to L2 prefetch-ing. Although various hardware data prefetching methods have been extensively studied in the literature <ref> [32, 33, 34, 35, 37, 36] </ref>, most of them focus on first-level, on-chip (L1) prefetching. Simple hardware prefetching schemes such as tagged prefetching [36] and stream buffers [35] can be used for L2 prefetching. <p> These methods are attractive for L2 prefetch-ing because only L1 cache miss addresses are necessary to determine the data to be 118 prefetched. However, these prefetching methods are effective only for memory accesses with good spatial locality or with short strides. Another prefetching scheme is stride-directed prefetching <ref> [32, 33, 34, 37] </ref>. In stride-directed prefetching, a memory reference stream, which is a sequence of data addresses generated by a specific memory reference instruction, is detected and used to calculate the stride associated with the memory reference stream. The calculated stride is used to predict future memory accesses. <p> The calculated stride is used to predict future memory accesses. Stride-directed prefetching is effective for memory references with any constant stride. In most previous studies <ref> [32, 33, 34] </ref>, a memory reference stream is detected by using the instruction address of the memory reference. This approach, which we refer to as instruction-address-based prefetching, is quite successful in L1 prefetching. <p> In such a case, after the last entry of the RPT is mapped, subsequent load executions are ignored until the iteration start signal is received again. Other operations, such as updating memory access history and obtaining strides, can be implemented similarly to the methods proposed in <ref> [32, 33, 34] </ref>. This method can be extended to handle multiple nested loops and control flow changes inside loops. <p> Each RPT entry keeps a data address (DA l ) and a stride (St l ). It is similar to other stream detection units <ref> [32, 33, 34] </ref>, but uses loop information to identify memory reference streams. The stream detection unit is accessed with the data addresses (M a ) that miss in both the stream buffers and the miss table unit. <p> The instruction-address-based prefetching architectures we simulate are similar to those proposed in <ref> [33, 34] </ref>. The IAP architecture consists of an RPT, a comparator, an address adder and a stride calculator, as show in Figure 6.6. <p> This state transition is required to obtain a stable stride. In the two states, the prefetching unit reacts differently to an incoming L1 cache miss address (M a ). These operations and state transitions are similar to those of IAP methods proposed in <ref> [32, 33, 34] </ref>. The following algorithm describes the operation of the prefetching unit in each state. Note that L (M a ) is the line address of the data address M a .
Reference: [35] <author> N. P. Jouppi, </author> <title> "Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers," </title> <booktitle> in International Symposium on Computer Architecture, </booktitle> <pages> pp. 364-373, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Data prefetching can be used at both levels of a memory hierarchy to hide memory access latency for accessing secondary caches or main memory. Most data prefetching studies <ref> [32, 33, 34, 35, 36] </ref> focus on first-level, on-chip prefetching. Although some of prefetch-ing schemes can work without any problems at the second-level and off-chip, others such as stride-directed prefetching [32, 33, 34] may not work effectively. <p> In tagged prefetching, whenever a cache line is accessed for the first time, the next line is prefetched. Therefore, either a cache miss or the first access to a prefetched line generates a prefetch for the next line. Another similar approach is stream buffers <ref> [35] </ref>, in which a stream buffer is allocated when there is a cache miss. When a stream buffer is allocated, the line following the line containing the cache miss address is prefetched. <p> Second-level, off-chip (L2) prefetching is different from L1 prefetching because only L1 cache miss addresses are visible, and instruction addresses may not be readily available. Such differences require a different approach to L2 prefetch-ing. Although various hardware data prefetching methods have been extensively studied in the literature <ref> [32, 33, 34, 35, 37, 36] </ref>, most of them focus on first-level, on-chip (L1) prefetching. Simple hardware prefetching schemes such as tagged prefetching [36] and stream buffers [35] can be used for L2 prefetching. <p> Such differences require a different approach to L2 prefetch-ing. Although various hardware data prefetching methods have been extensively studied in the literature [32, 33, 34, 35, 37, 36], most of them focus on first-level, on-chip (L1) prefetching. Simple hardware prefetching schemes such as tagged prefetching [36] and stream buffers <ref> [35] </ref> can be used for L2 prefetching. These methods are attractive for L2 prefetch-ing because only L1 cache miss addresses are necessary to determine the data to be 118 prefetched. However, these prefetching methods are effective only for memory accesses with good spatial locality or with short strides. <p> The instruction-address-based prefetching architecture is described in the fourth section. 6.2.1 Stream Buffers Stream buffers were initially proposed in <ref> [35] </ref> and were also used in [37]. This study employs a similar stream buffer architecture with slight modifications. The stream buffers keep and update memory access history, and prefetch data according to this information.
Reference: [36] <author> A. J. Smith, </author> <title> "Cache memories," </title> <journal> Computing Surveys, </journal> <volume> vol. 14, </volume> <pages> pp. 473-530, </pages> <month> Sept. </month> <year> 1982. </year>
Reference-contexts: Data prefetching can be used at both levels of a memory hierarchy to hide memory access latency for accessing secondary caches or main memory. Most data prefetching studies <ref> [32, 33, 34, 35, 36] </ref> focus on first-level, on-chip prefetching. Although some of prefetch-ing schemes can work without any problems at the second-level and off-chip, others such as stride-directed prefetching [32, 33, 34] may not work effectively. <p> Depending on the complexity and capability of the prefetching hardware to predict future data accesses, hardware prefetching schemes fall into two groups. 14 2.2.1.1 Tagged Prefetching Tagged prefetching <ref> [36] </ref> is simple and effective, but limited in predicting future data accesses. This scheme depends on the spatial locality of memory accesses. In tagged prefetching, whenever a cache line is accessed for the first time, the next line is prefetched. <p> The only difference is the change in the cache architecture and network interface of the prefetching cache. This section describes the architectural model in detail. 87 5.2.1 Tagged Cache and Network Interface A tagged cache model <ref> [36] </ref> replaces the cache model used in the previous chapters. Tagged cache is very effective for short stride accesses and tend to generate intense memory traffic. Each cache line has one additional flag, called the prefetch bit, to mark whether a read has accessed the line or not. <p> Second-level, off-chip (L2) prefetching is different from L1 prefetching because only L1 cache miss addresses are visible, and instruction addresses may not be readily available. Such differences require a different approach to L2 prefetch-ing. Although various hardware data prefetching methods have been extensively studied in the literature <ref> [32, 33, 34, 35, 37, 36] </ref>, most of them focus on first-level, on-chip (L1) prefetching. Simple hardware prefetching schemes such as tagged prefetching [36] and stream buffers [35] can be used for L2 prefetching. <p> Such differences require a different approach to L2 prefetch-ing. Although various hardware data prefetching methods have been extensively studied in the literature [32, 33, 34, 35, 37, 36], most of them focus on first-level, on-chip (L1) prefetching. Simple hardware prefetching schemes such as tagged prefetching <ref> [36] </ref> and stream buffers [35] can be used for L2 prefetching. These methods are attractive for L2 prefetch-ing because only L1 cache miss addresses are necessary to determine the data to be 118 prefetched.
Reference: [37] <author> S. Palacharla and R. E. Kessler, </author> <title> "Evaluating stream buffers as a secondary cache replacement," </title> <booktitle> in International Symposium on Computer Architecture, </booktitle> <pages> pp. 24-33, </pages> <month> May </month> <year> 1994. </year> <month> 158 </month>
Reference-contexts: These make stride-directed prefetching difficult. This thesis develops and measures the performance of prefetching schemes that effectively perform stride-direct prefetching at the second-level and off-chip. Prefetching may replace large second-level caches as studied in <ref> [37] </ref> or prefetching can be used with second-level caches as in [38]. Though at least these two different second-level memory organizations were proposed for prefetching, no clear conclusion can be made as to what is the best second-level memory organization to use with prefetching. <p> However, for the accesses to array B, tagged prefetching generates only unnecessary prefetches. Tagged prefetching is not effective for accesses whose stride is larger than twice the cache line size. 15 Several methods have been proposed <ref> [37, 31, 62] </ref> to reduce the number of unnecessary prefetches in tagged prefetching or stream buffers. <p> Every time a memory access is generated by a memory reference instruction, the instruction address is used to index the table and the data address to obtain a stride. In another approach, proposed in <ref> [37] </ref>, memory reference stream detection is attempted without instruction addresses for off-chip prefetching implementations. This method uses a non-unit stride filter that attempts to detect a memory reference stream and a stride. The filter uses the misses that fall into the same address space partition. <p> For such applications, data prefetching at the second-level of a memory hierarchy is an attractive solution to hide a large main memory access latency. However, only a few studies have been conducted on data prefetching at the second-level of a memory hierarchy. A previous L2 prefetching study <ref> [37] </ref> showed that prefetching alone can replace L2 caches. On the other hand, another L2 prefetch-ing method proposed in [38] used a small L2 cache in addition to prefetching hardware. <p> Second-level, off-chip (L2) prefetching is different from L1 prefetching because only L1 cache miss addresses are visible, and instruction addresses may not be readily available. Such differences require a different approach to L2 prefetch-ing. Although various hardware data prefetching methods have been extensively studied in the literature <ref> [32, 33, 34, 35, 37, 36] </ref>, most of them focus on first-level, on-chip (L1) prefetching. Simple hardware prefetching schemes such as tagged prefetching [36] and stream buffers [35] can be used for L2 prefetching. <p> These methods are attractive for L2 prefetch-ing because only L1 cache miss addresses are necessary to determine the data to be 118 prefetched. However, these prefetching methods are effective only for memory accesses with good spatial locality or with short strides. Another prefetching scheme is stride-directed prefetching <ref> [32, 33, 34, 37] </ref>. In stride-directed prefetching, a memory reference stream, which is a sequence of data addresses generated by a specific memory reference instruction, is detected and used to calculate the stride associated with the memory reference stream. The calculated stride is used to predict future memory accesses. <p> These characteristics make stride-directed prefetching more difficult at L2 than at L1, and it is not clear that L2 stride-directed prefetching alone can be effective. However, the issue of L2 stride-directed prefetching is not well studied in other literature. L2 stride-directed prefetching was studied in <ref> [37] </ref>. The authors used stream buffers and a stride detection method. In their method, the address space is partitioned in order to separate references in different streams and to detect strides for references accessing in the same partition and thus in the same stream. <p> A general organization of the system architectures, which includes a CPU, L1 and L2 caches, and an L2 prefetching unit, is shown in Figure 6.2. We use stream buffers and a miss table unit, which is similar to the unit-stride filter <ref> [37] </ref>, as a baseline prefetching architecture. This baseline architecture is chosen because it does not rely on memory reference stream and stride detection, and has been shown effective in L2 prefetching in [37]. <p> We use stream buffers and a miss table unit, which is similar to the unit-stride filter <ref> [37] </ref>, as a baseline prefetching architecture. This baseline architecture is chosen because it does not rely on memory reference stream and stride detection, and has been shown effective in L2 prefetching in [37]. A loop-based stream detection unit, which detects memory reference streams based on loop-based prefetching, is added to the baseline 121 architecture. This architecture is referred to as loop-based prefetching architecture. The performance of the baseline architecture (also called miss table prefetching) and the loop-based prefetching architecture is evaluated. <p> The instruction-address-based prefetching architecture is described in the fourth section. 6.2.1 Stream Buffers Stream buffers were initially proposed in [35] and were also used in <ref> [37] </ref>. This study employs a similar stream buffer architecture with slight modifications. The stream buffers keep and update memory access history, and prefetch data according to this information. <p> Stream buffers are replaced based on the LRU replacement policy. 6.2.2 Miss Table Prefetching (MTP) Architecture Similar to the filter-based unit stride stream method proposed in <ref> [37] </ref>, this architecture uses miss tables to detect memory accesses with short strides or with good spatial locality and allocates stream buffers for such accesses. Note that it needs only L1 cache miss addresses to detect such memory access patterns. A miss tables stores line addresses. <p> This system organization is similar to the prefetching system studied in <ref> [37] </ref>, in that prefetched data are stored in stream buffers. For the IAP method studied, the prefetched data are assumed to be stored in the RPT, which has a storage associated with each entry. <p> Although the best possible performance of the LBP system is compared with that of L2 caches, the large cache size difference strongly implies that LBP systems can be more efficient than L2 caches when sufficient memory bandwidth is available. In addition, these results are consistent with <ref> [37] </ref> which already showed that prefetching alone can perform better than a large L2 cache for scientific codes with large data sets. <p> Three different second-level memory organizations were compared: a traditional cache (cache-only), prefetching without a cache (prefetching-only), and prefetching with a small cache. Prefetching-only systems had been shown to achieve comparable hit rates to large L2 caches <ref> [37] </ref>. Adding a small L2 cache to the prefetching systems boosted the performance significantly, because the cache took advantage of spatial and temporal locality among prefetched data. It also allowed the prefetching systems to work reasonably well with a small number of stream buffers.
Reference: [38] <author> K. Krishnamohan, </author> <title> "Applying Rambus technology to desktop computer main mem-ory subsystems," </title> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: These make stride-directed prefetching difficult. This thesis develops and measures the performance of prefetching schemes that effectively perform stride-direct prefetching at the second-level and off-chip. Prefetching may replace large second-level caches as studied in [37] or prefetching can be used with second-level caches as in <ref> [38] </ref>. Though at least these two different second-level memory organizations were proposed for prefetching, no clear conclusion can be made as to what is the best second-level memory organization to use with prefetching. <p> However, only a few studies have been conducted on data prefetching at the second-level of a memory hierarchy. A previous L2 prefetching study [37] showed that prefetching alone can replace L2 caches. On the other hand, another L2 prefetch-ing method proposed in <ref> [38] </ref> used a small L2 cache in addition to prefetching hardware. Although at least these two different L2 memory organizations have been proposed for prefetching, no clear conclusion can be made as to what is the best L2 memory organization with prefetching.
Reference: [39] <author> W. J. Dally, </author> <title> "Performance analysis of k-ary n-cube interconnection networks," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 39, </volume> <pages> pp. 775-784, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Limits on the number of pins and 8 wiring area restrict the size of a switch used as a network building block and network channel width. Switch size K (or KxK) means a switch has K input ports and K output ports. Recent network studies <ref> [39, 40, 41, 42, 43, 44, 45] </ref> have incorporated such design constraints into their network evaluation. A message reaches its destination by traversing several switches. One hop is a traversal of one switch by a message. <p> Switch size and channel width consume the same resources: pins and wiring area. Given a fixed number of pins and a constant wiring area, increasing the switch size results in a narrower channel width. Therefore, a design trade-off exists between switch size and channel width. In <ref> [39] </ref>, wiring area limitation is used to evaluate this trade-off. Constant bisection width is used in this evaluation. Bisection width [39, 47] is defined as the minimum number of wires cut when a network is divided into two equal halves. Constant bisection width represents wiring area constraints. <p> Therefore, a design trade-off exists between switch size and channel width. In [39], wiring area limitation is used to evaluate this trade-off. Constant bisection width is used in this evaluation. Bisection width <ref> [39, 47] </ref> is defined as the minimum number of wires cut when a network is divided into two equal halves. Constant bisection width represents wiring area constraints. When an interconnection network is implemented in a single VLSI chip, both pins and wiring area are limited. <p> The network cost is based on pin constraint models. In next two sections, we briefly describe the three network topologies. 2.1.1 Multidimensional Torus Networks The topology of a multidimensional torus network is a k-ary n-cube <ref> [39] </ref> with wraparound connections. In an n-dimensional torus, each node can be expressed as a n-tuple, (i 0 ; i 1 ; : : : ; i n1 ), where 0 i k &lt; n k and n k is the number of nodes in the kth dimension. <p> In order to obtain the actual network performance, we use trace-driven simulations with traces gathered by executing real benchmarks. In recent network studies <ref> [42, 41, 40, 39, 43] </ref> physical implementation constraints, such as a constant number of available pins and wires, have been incorporated into network performance evaluation. Under such constraints, the switch size and channel width are limited. Increasing the switch size and channel width requires the same resources: pins and wires. <p> Increasing the switch size and channel width requires the same resources: pins and wires. Hence, when such resources are limited, trade-offs exist between the switch size and channel width: increasing the channel width results in a smaller switch size and vice versa. In previous studies <ref> [42, 41, 40, 39] </ref>, the effects of such trade-offs are evaluated for a given network topology. In these analyses, a fixed number of pins or wires is used to compute the switch size and channel width for a network.
Reference: [40] <author> A. Agarwal, </author> <title> "Limits on interconnection network performance," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 2, </volume> <pages> pp. 398-412, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: Limits on the number of pins and 8 wiring area restrict the size of a switch used as a network building block and network channel width. Switch size K (or KxK) means a switch has K input ports and K output ports. Recent network studies <ref> [39, 40, 41, 42, 43, 44, 45] </ref> have incorporated such design constraints into their network evaluation. A message reaches its destination by traversing several switches. One hop is a traversal of one switch by a message. <p> Constant bisection width represents wiring area constraints. When an interconnection network is implemented in a single VLSI chip, both pins and wiring area are limited. However, when several VLSI chips and boards are used to construct an interconnection network, pins are a more constrained resource due to packaging limitations <ref> [40, 48, 49] </ref>. Agarwal [40] evaluated design trade-offs based on such pin constraints. Other studies [41, 42, 43, 44, 45] also addressed such design trade-offs in pin, wiring constraints, or both. <p> When an interconnection network is implemented in a single VLSI chip, both pins and wiring area are limited. However, when several VLSI chips and boards are used to construct an interconnection network, pins are a more constrained resource due to packaging limitations [40, 48, 49]. Agarwal <ref> [40] </ref> evaluated design trade-offs based on such pin constraints. Other studies [41, 42, 43, 44, 45] also addressed such design trade-offs in pin, wiring constraints, or both. <p> In order to obtain the actual network performance, we use trace-driven simulations with traces gathered by executing real benchmarks. In recent network studies <ref> [42, 41, 40, 39, 43] </ref> physical implementation constraints, such as a constant number of available pins and wires, have been incorporated into network performance evaluation. Under such constraints, the switch size and channel width are limited. Increasing the switch size and channel width requires the same resources: pins and wires. <p> Increasing the switch size and channel width requires the same resources: pins and wires. Hence, when such resources are limited, trade-offs exist between the switch size and channel width: increasing the channel width results in a smaller switch size and vice versa. In previous studies <ref> [42, 41, 40, 39] </ref>, the effects of such trade-offs are evaluated for a given network topology. In these analyses, a fixed number of pins or wires is used to compute the switch size and channel width for a network. <p> This is justified by the fact that within the system sizes studied, a network design is not re 56 stricted by wiring area. Instead, the pin constraint is often a more serious limitation than wiring area <ref> [40, 48, 49] </ref>. We evaluate three network topologies, namely multidimensional torus (TORUS) networks, multistage shu*e-exchange (MSX) networks, and single stage shu*e-exchange (SSX) networks, with respect to memory latency under these three constraint models. <p> In practical network design where an interconnection network is composed of several VLSI components, the number of pins available in a chip is often considered a more serious limitation than wiring area <ref> [40, 48, 49] </ref>. Also, the packaging cost for a large number of pins tends to increase exponentially and becomes a dominant cost factor. Moreover, it is impossible to find a package accommodating more than a certain number of pins. <p> Moreover, it is impossible to find a package accommodating more than a certain number of pins. As a result, the pin requirement of an interconnection network is a very important measure of network cost. In several papers <ref> [42, 40, 45] </ref>, a constant pin limitation is imposed on a node, and network performance is evaluated by varying node degree and channel width under the pin constraint.
Reference: [41] <author> S. Abraham and K. Padmanabhan, </author> <title> "Constraint based evaluation of multicomputer networks," </title> <booktitle> in International Conference on Parallel Processing, </booktitle> <volume> vol. I, </volume> <pages> pp. 521-525, </pages> <year> 1990. </year>
Reference-contexts: Limits on the number of pins and 8 wiring area restrict the size of a switch used as a network building block and network channel width. Switch size K (or KxK) means a switch has K input ports and K output ports. Recent network studies <ref> [39, 40, 41, 42, 43, 44, 45] </ref> have incorporated such design constraints into their network evaluation. A message reaches its destination by traversing several switches. One hop is a traversal of one switch by a message. <p> However, when several VLSI chips and boards are used to construct an interconnection network, pins are a more constrained resource due to packaging limitations [40, 48, 49]. Agarwal [40] evaluated design trade-offs based on such pin constraints. Other studies <ref> [41, 42, 43, 44, 45] </ref> also addressed such design trade-offs in pin, wiring constraints, or both. In this dissertation, we evaluate three different interconnection networks: multidimensional torus (TORUS) networks, multistage stage shu*e-exchange (MSX) networks and single stage shu*e-exchange (SSX) networks. We assume a system size from 64 to 256 processors. <p> In order to obtain the actual network performance, we use trace-driven simulations with traces gathered by executing real benchmarks. In recent network studies <ref> [42, 41, 40, 39, 43] </ref> physical implementation constraints, such as a constant number of available pins and wires, have been incorporated into network performance evaluation. Under such constraints, the switch size and channel width are limited. Increasing the switch size and channel width requires the same resources: pins and wires. <p> Increasing the switch size and channel width requires the same resources: pins and wires. Hence, when such resources are limited, trade-offs exist between the switch size and channel width: increasing the channel width results in a smaller switch size and vice versa. In previous studies <ref> [42, 41, 40, 39] </ref>, the effects of such trade-offs are evaluated for a given network topology. In these analyses, a fixed number of pins or wires is used to compute the switch size and channel width for a network.
Reference: [42] <author> S. Abraham, </author> <title> Issues in the Architecture of Direct Interconnection Schemes for Multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> march </month> <year> 1990. </year>
Reference-contexts: Limits on the number of pins and 8 wiring area restrict the size of a switch used as a network building block and network channel width. Switch size K (or KxK) means a switch has K input ports and K output ports. Recent network studies <ref> [39, 40, 41, 42, 43, 44, 45] </ref> have incorporated such design constraints into their network evaluation. A message reaches its destination by traversing several switches. One hop is a traversal of one switch by a message. <p> However, when several VLSI chips and boards are used to construct an interconnection network, pins are a more constrained resource due to packaging limitations [40, 48, 49]. Agarwal [40] evaluated design trade-offs based on such pin constraints. Other studies <ref> [41, 42, 43, 44, 45] </ref> also addressed such design trade-offs in pin, wiring constraints, or both. In this dissertation, we evaluate three different interconnection networks: multidimensional torus (TORUS) networks, multistage stage shu*e-exchange (MSX) networks and single stage shu*e-exchange (SSX) networks. We assume a system size from 64 to 256 processors. <p> In order to obtain the actual network performance, we use trace-driven simulations with traces gathered by executing real benchmarks. In recent network studies <ref> [42, 41, 40, 39, 43] </ref> physical implementation constraints, such as a constant number of available pins and wires, have been incorporated into network performance evaluation. Under such constraints, the switch size and channel width are limited. Increasing the switch size and channel width requires the same resources: pins and wires. <p> Increasing the switch size and channel width requires the same resources: pins and wires. Hence, when such resources are limited, trade-offs exist between the switch size and channel width: increasing the channel width results in a smaller switch size and vice versa. In previous studies <ref> [42, 41, 40, 39] </ref>, the effects of such trade-offs are evaluated for a given network topology. In these analyses, a fixed number of pins or wires is used to compute the switch size and channel width for a network. <p> Moreover, it is impossible to find a package accommodating more than a certain number of pins. As a result, the pin requirement of an interconnection network is a very important measure of network cost. In several papers <ref> [42, 40, 45] </ref>, a constant pin limitation is imposed on a node, and network performance is evaluated by varying node degree and channel width under the pin constraint.
Reference: [43] <author> S. Konstantinidou, </author> <title> Deterministic and Chaotic Adaptive Routing in Multicomputers. </title> <type> PhD thesis, </type> <institution> University of Washington, </institution> <month> may </month> <year> 1991. </year>
Reference-contexts: Limits on the number of pins and 8 wiring area restrict the size of a switch used as a network building block and network channel width. Switch size K (or KxK) means a switch has K input ports and K output ports. Recent network studies <ref> [39, 40, 41, 42, 43, 44, 45] </ref> have incorporated such design constraints into their network evaluation. A message reaches its destination by traversing several switches. One hop is a traversal of one switch by a message. <p> However, when several VLSI chips and boards are used to construct an interconnection network, pins are a more constrained resource due to packaging limitations [40, 48, 49]. Agarwal [40] evaluated design trade-offs based on such pin constraints. Other studies <ref> [41, 42, 43, 44, 45] </ref> also addressed such design trade-offs in pin, wiring constraints, or both. In this dissertation, we evaluate three different interconnection networks: multidimensional torus (TORUS) networks, multistage stage shu*e-exchange (MSX) networks and single stage shu*e-exchange (SSX) networks. We assume a system size from 64 to 256 processors. <p> In order to obtain the actual network performance, we use trace-driven simulations with traces gathered by executing real benchmarks. In recent network studies <ref> [42, 41, 40, 39, 43] </ref> physical implementation constraints, such as a constant number of available pins and wires, have been incorporated into network performance evaluation. Under such constraints, the switch size and channel width are limited. Increasing the switch size and channel width requires the same resources: pins and wires.
Reference: [44] <author> K. Bolding and S. Konstantinidou, </author> <title> "On the comparison of hypercube and torus networks," </title> <booktitle> in International Conference on Parallel Processing, </booktitle> <volume> vol. I, </volume> <pages> pp. 62-66, </pages> <year> 1992. </year>
Reference-contexts: Limits on the number of pins and 8 wiring area restrict the size of a switch used as a network building block and network channel width. Switch size K (or KxK) means a switch has K input ports and K output ports. Recent network studies <ref> [39, 40, 41, 42, 43, 44, 45] </ref> have incorporated such design constraints into their network evaluation. A message reaches its destination by traversing several switches. One hop is a traversal of one switch by a message. <p> However, when several VLSI chips and boards are used to construct an interconnection network, pins are a more constrained resource due to packaging limitations [40, 48, 49]. Agarwal [40] evaluated design trade-offs based on such pin constraints. Other studies <ref> [41, 42, 43, 44, 45] </ref> also addressed such design trade-offs in pin, wiring constraints, or both. In this dissertation, we evaluate three different interconnection networks: multidimensional torus (TORUS) networks, multistage stage shu*e-exchange (MSX) networks and single stage shu*e-exchange (SSX) networks. We assume a system size from 64 to 256 processors.
Reference: [45] <author> K. Padmanabhan, </author> <title> "On the tradeoff between node degree and communication channel width in shu*e-exchange networks," </title> <booktitle> in Proceedings, IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pp. 120-127, </pages> <publisher> IEEE, </publisher> <month> Dec. </month> <year> 1991. </year>
Reference-contexts: Limits on the number of pins and 8 wiring area restrict the size of a switch used as a network building block and network channel width. Switch size K (or KxK) means a switch has K input ports and K output ports. Recent network studies <ref> [39, 40, 41, 42, 43, 44, 45] </ref> have incorporated such design constraints into their network evaluation. A message reaches its destination by traversing several switches. One hop is a traversal of one switch by a message. <p> However, when several VLSI chips and boards are used to construct an interconnection network, pins are a more constrained resource due to packaging limitations [40, 48, 49]. Agarwal [40] evaluated design trade-offs based on such pin constraints. Other studies <ref> [41, 42, 43, 44, 45] </ref> also addressed such design trade-offs in pin, wiring constraints, or both. In this dissertation, we evaluate three different interconnection networks: multidimensional torus (TORUS) networks, multistage stage shu*e-exchange (MSX) networks and single stage shu*e-exchange (SSX) networks. We assume a system size from 64 to 256 processors. <p> Still, the increased network size needs to be a multiple of the previous network size. Padmanabhan lifted the limitation of network size from shu*e-exchange network routing, allowing a more flexible increase in network size. In <ref> [45, 53] </ref>, he laid out a theoretical framework for a routing scheme requiring only that the network size be a multiple of K and investigated the properties of the scheme. <p> The output terminals can be reached in dlog K N e hops or less because the input and output terminals are directly connected through switches. 33 First, we define the shu*e connection in a stage using a definition from Padmanab--han <ref> [45, 53] </ref>. Definition 1 (Shu*e Connection) Let N be a multiple of K. Then, the K-ary shu*e of N terminals is given by (i) = (Ki + bKi=N c) mod N; 0 i &lt; N: where terminal i maps to terminal (i). <p> This T n is a shortest path routing tag. 2 3.2.3.1 Direct Connected Single Stage Shu*e-Exchange Network Single shu*e-exchange stage networks can take a different structure described in <ref> [45] </ref> as a direct connected single stage network (Direct SSX). So far we assumed that in a single stage network, a node injecting a message is connected to an input terminal of the stage. <p> Moreover, it is impossible to find a package accommodating more than a certain number of pins. As a result, the pin requirement of an interconnection network is a very important measure of network cost. In several papers <ref> [42, 40, 45] </ref>, a constant pin limitation is imposed on a node, and network performance is evaluated by varying node degree and channel width under the pin constraint.
Reference: [46] <author> D. A. Reed and R. M. Fujimoto, </author> <title> Multicomputer Networks: Message-Based Parallel Processing. </title> <publisher> The MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: A message reaches its destination by traversing several switches. One hop is a traversal of one switch by a message. Average internode distance is the number of hops a message travels, on average, in uniform message routing <ref> [46] </ref>. In uniform message routing, the probability of source node i sending a message to destination node j is the same for all i and j, i 6= j. Every time a message travels through a switch, the message experiences a delay, which we refer to as switch latency.
Reference: [47] <author> C. D. Thompson, </author> <title> "Area-time complexity for VLSI," </title> <booktitle> in Caltech Conference on VLSI, </booktitle> <pages> pp. 495-508, </pages> <month> Jan. </month> <year> 1979. </year>
Reference-contexts: Therefore, a design trade-off exists between switch size and channel width. In [39], wiring area limitation is used to evaluate this trade-off. Constant bisection width is used in this evaluation. Bisection width <ref> [39, 47] </ref> is defined as the minimum number of wires cut when a network is divided into two equal halves. Constant bisection width represents wiring area constraints. When an interconnection network is implemented in a single VLSI chip, both pins and wiring area are limited.
Reference: [48] <author> R. Cyper, </author> <title> "Theoretical aspects of VLSI pin limitation," </title> <journal> SIAM Journal on Computing, </journal> <volume> vol. 22, </volume> <pages> pp. 356-379, </pages> <month> Apr. </month> <year> 1993. </year>
Reference-contexts: Constant bisection width represents wiring area constraints. When an interconnection network is implemented in a single VLSI chip, both pins and wiring area are limited. However, when several VLSI chips and boards are used to construct an interconnection network, pins are a more constrained resource due to packaging limitations <ref> [40, 48, 49] </ref>. Agarwal [40] evaluated design trade-offs based on such pin constraints. Other studies [41, 42, 43, 44, 45] also addressed such design trade-offs in pin, wiring constraints, or both. <p> This is justified by the fact that within the system sizes studied, a network design is not re 56 stricted by wiring area. Instead, the pin constraint is often a more serious limitation than wiring area <ref> [40, 48, 49] </ref>. We evaluate three network topologies, namely multidimensional torus (TORUS) networks, multistage shu*e-exchange (MSX) networks, and single stage shu*e-exchange (SSX) networks, with respect to memory latency under these three constraint models. <p> In practical network design where an interconnection network is composed of several VLSI components, the number of pins available in a chip is often considered a more serious limitation than wiring area <ref> [40, 48, 49] </ref>. Also, the packaging cost for a large number of pins tends to increase exponentially and becomes a dominant cost factor. Moreover, it is impossible to find a package accommodating more than a certain number of pins.
Reference: [49] <author> M. A. Franklin, D. F. Wann, and W. J. Thomas, </author> <title> "Pin limitations and partitioning of VLSI interconnection networks," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-31, </volume> <pages> pp. 1109-1116, </pages> <month> Nov. </month> <year> 1982. </year>
Reference-contexts: Constant bisection width represents wiring area constraints. When an interconnection network is implemented in a single VLSI chip, both pins and wiring area are limited. However, when several VLSI chips and boards are used to construct an interconnection network, pins are a more constrained resource due to packaging limitations <ref> [40, 48, 49] </ref>. Agarwal [40] evaluated design trade-offs based on such pin constraints. Other studies [41, 42, 43, 44, 45] also addressed such design trade-offs in pin, wiring constraints, or both. <p> This is justified by the fact that within the system sizes studied, a network design is not re 56 stricted by wiring area. Instead, the pin constraint is often a more serious limitation than wiring area <ref> [40, 48, 49] </ref>. We evaluate three network topologies, namely multidimensional torus (TORUS) networks, multistage shu*e-exchange (MSX) networks, and single stage shu*e-exchange (SSX) networks, with respect to memory latency under these three constraint models. <p> In practical network design where an interconnection network is composed of several VLSI components, the number of pins available in a chip is often considered a more serious limitation than wiring area <ref> [40, 48, 49] </ref>. Also, the packaging cost for a large number of pins tends to increase exponentially and becomes a dominant cost factor. Moreover, it is impossible to find a package accommodating more than a certain number of pins.
Reference: [50] <author> L. M. Ni and P. K. McKinley, </author> <title> "A survey of wormhole routing techniques in direct networks," </title> <journal> IEEE Computer, </journal> <volume> vol. 26, </volume> <pages> pp. 62-76, </pages> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: Figure 2.1 shows a 12-node 3-D torus, which has 2, 2, and 3 nodes in each dimension. Messages can be routed to traverse each dimension in order. In one of the simplest routing methods, called dimension-order routing <ref> [50] </ref>, a message is routed first in the highest dimension and then in the next highest dimension and so on. Within each dimension k, a message travels in one direction until it reaches the node whose address digit i k is the same as that of the destination node.
Reference: [51] <author> C. P. Kruskal and M. Snir, </author> <title> "A unified theory of interconnection network structure," </title> <journal> Theoretical Computer Science, </journal> <volume> vol. 48, </volume> <pages> pp. 75-94, </pages> <month> Aug. </month> <year> 1986. </year>
Reference-contexts: After that, the next dimension to travel is selected in a fixed order. If links are bidirectional, the direction which leads to the shortest path is selected. 2.1.2 Multistage and Single Stage Shu*e-Exchange Networks A K-ary multistage shu*e-exchange network <ref> [51, 52, 53] </ref> consists of dlog K N e shu*e-exchange stages. Each stage has N input and N output terminals that are connected to N output terminals of the previous stage and N input terminals of the next stage, respectively, as shown in Figure 3.1. <p> The DMUX has infinite queues. Although the infinite queues shift a part of switch queueing delay to the DMUX queueing delay, this does not affect our conclusions. 28 Chapter 3 Shortest Path Routing in Single Stage Shu*e-Exchange Networks 3.1 Introduction Among various interconnection network architectures, shu*e-exchange networks <ref> [83, 51, 52, 53] </ref> have been one of the most popular architectures. These networks have distributed self-routing capabilities, and when network size is increased, the internode distance and the number of switch components are increased logarithmically. These features make shu*e-exchange networks suitable for large-scale multiprocessor systems.
Reference: [52] <author> D. H. Lawrie, </author> <title> "Access and alignment of data in an array processor," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-24, </volume> <pages> pp. 173-182, </pages> <month> Dec. </month> <year> 1975. </year> <month> 159 </month>
Reference-contexts: After that, the next dimension to travel is selected in a fixed order. If links are bidirectional, the direction which leads to the shortest path is selected. 2.1.2 Multistage and Single Stage Shu*e-Exchange Networks A K-ary multistage shu*e-exchange network <ref> [51, 52, 53] </ref> consists of dlog K N e shu*e-exchange stages. Each stage has N input and N output terminals that are connected to N output terminals of the previous stage and N input terminals of the next stage, respectively, as shown in Figure 3.1. <p> The DMUX has infinite queues. Although the infinite queues shift a part of switch queueing delay to the DMUX queueing delay, this does not affect our conclusions. 28 Chapter 3 Shortest Path Routing in Single Stage Shu*e-Exchange Networks 3.1 Introduction Among various interconnection network architectures, shu*e-exchange networks <ref> [83, 51, 52, 53] </ref> have been one of the most popular architectures. These networks have distributed self-routing capabilities, and when network size is increased, the internode distance and the number of switch components are increased logarithmically. These features make shu*e-exchange networks suitable for large-scale multiprocessor systems.
Reference: [53] <author> K. Padmanabhan, </author> <title> "Design and analysis of even-sized binary shu*e-exchange net-works for multiprocessors," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 2, </volume> <pages> pp. 385-397, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: After that, the next dimension to travel is selected in a fixed order. If links are bidirectional, the direction which leads to the shortest path is selected. 2.1.2 Multistage and Single Stage Shu*e-Exchange Networks A K-ary multistage shu*e-exchange network <ref> [51, 52, 53] </ref> consists of dlog K N e shu*e-exchange stages. Each stage has N input and N output terminals that are connected to N output terminals of the previous stage and N input terminals of the next stage, respectively, as shown in Figure 3.1. <p> The DMUX has infinite queues. Although the infinite queues shift a part of switch queueing delay to the DMUX queueing delay, this does not affect our conclusions. 28 Chapter 3 Shortest Path Routing in Single Stage Shu*e-Exchange Networks 3.1 Introduction Among various interconnection network architectures, shu*e-exchange networks <ref> [83, 51, 52, 53] </ref> have been one of the most popular architectures. These networks have distributed self-routing capabilities, and when network size is increased, the internode distance and the number of switch components are increased logarithmically. These features make shu*e-exchange networks suitable for large-scale multiprocessor systems. <p> Still, the increased network size needs to be a multiple of the previous network size. Padmanabhan lifted the limitation of network size from shu*e-exchange network routing, allowing a more flexible increase in network size. In <ref> [45, 53] </ref>, he laid out a theoretical framework for a routing scheme requiring only that the network size be a multiple of K and investigated the properties of the scheme. <p> The output terminals can be reached in dlog K N e hops or less because the input and output terminals are directly connected through switches. 33 First, we define the shu*e connection in a stage using a definition from Padmanab--han <ref> [45, 53] </ref>. Definition 1 (Shu*e Connection) Let N be a multiple of K. Then, the K-ary shu*e of N terminals is given by (i) = (Ki + bKi=N c) mod N; 0 i &lt; N: where terminal i maps to terminal (i).
Reference: [54] <author> D. Gelernter, </author> <title> "A DAG-based algorithm for prevention of store-and-forward deadlock in packet networks," </title> <journal> IEEE Transaction on Computers, </journal> <volume> vol. C-30, </volume> <pages> pp. 709-715, </pages> <month> Oct. </month> <year> 1981. </year>
Reference-contexts: A related theorem and algorithm are described in detail in Chapter 3. 2.1.3 Flow Control and Deadlock When switches have a limited amount of queues, the message flow should be controlled so as not to overflow a switch. In store-and-forward routing <ref> [54] </ref>, a message is completely stored before it is sent to the next switch. When a message size is large, this scheme increases routing time significantly. Other approaches are wormhole routing [55] and virtual cut-through [56] routing. <p> Virtual cut-through routing is used throughout this thesis. In multidimensional torus networks and shu*e-exchange networks, deadlock situations may develop. Deadlock can be avoided by using various approaches proposed previously. These are structured buffer pools <ref> [54] </ref>, virtual channels [55], deflection routing [57], Chaos routing [58], and abort-and-retry [59, 60]. The deadlock problem and the effect of deadlock-free routing on network performance is beyond the scope of this thesis. In our evaluation, we use infinite queues to avoid deadlock problems.
Reference: [55] <author> W. J. Dally and C. L. Seitz, </author> <title> "Deadlock-free message routing in multiprocessor interconnection networks," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-36, </volume> <pages> pp. 547-553, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: In store-and-forward routing [54], a message is completely stored before it is sent to the next switch. When a message size is large, this scheme increases routing time significantly. Other approaches are wormhole routing <ref> [55] </ref> and virtual cut-through [56] routing. In both schemes, a message advances to the next switch without waiting for the entire message to be stored if a corresponding channel is not busy. <p> Virtual cut-through routing is used throughout this thesis. In multidimensional torus networks and shu*e-exchange networks, deadlock situations may develop. Deadlock can be avoided by using various approaches proposed previously. These are structured buffer pools [54], virtual channels <ref> [55] </ref>, deflection routing [57], Chaos routing [58], and abort-and-retry [59, 60]. The deadlock problem and the effect of deadlock-free routing on network performance is beyond the scope of this thesis. In our evaluation, we use infinite queues to avoid deadlock problems. <p> A write reply message (8 bytes) from a memory has only a message header, and a read reply message (40 bytes) has a message header and a line that contains 4 words, starting with the word requested by the processor. A request message is disassembled into flits <ref> [55] </ref> and sent out to the network. The size of a flit is determined by network channel width. A message is assembled when it arrives from memory before it is forwarded to a cache.
Reference: [56] <author> P. Kermani and L. Kleinrock, </author> <title> "Virtual cut-through: A new computer communication switching technique," </title> <journal> Computer Networks, </journal> <volume> vol. 3, </volume> <pages> pp. 267-286, </pages> <year> 1979. </year>
Reference-contexts: In store-and-forward routing [54], a message is completely stored before it is sent to the next switch. When a message size is large, this scheme increases routing time significantly. Other approaches are wormhole routing [55] and virtual cut-through <ref> [56] </ref> routing. In both schemes, a message advances to the next switch without waiting for the entire message to be stored if a corresponding channel is not busy. <p> Since only messages destined to the same output port are stored in the same queue, there is no head-of-line (HOL) blocking. The switch model supports the virtual cut-through flow control scheme <ref> [56] </ref>. In this scheme, a switch does not need to wait for a whole message to be received before it can start to send the message out. Thus, if there is no contention and no previous messages in the queue, the message experiences only a minimum switch latency.
Reference: [57] <author> N. F. Maxemchuk, </author> <title> "Comparison of deflection and store-and-forward techniques in the manhattan street and shu*e-exchange networks," </title> <booktitle> in Proceedings IEEE INFO-COM 89 : The Conference on Computer Communication, </booktitle> <pages> pp. 800-809, </pages> <year> 1989. </year>
Reference-contexts: Virtual cut-through routing is used throughout this thesis. In multidimensional torus networks and shu*e-exchange networks, deadlock situations may develop. Deadlock can be avoided by using various approaches proposed previously. These are structured buffer pools [54], virtual channels [55], deflection routing <ref> [57] </ref>, Chaos routing [58], and abort-and-retry [59, 60]. The deadlock problem and the effect of deadlock-free routing on network performance is beyond the scope of this thesis. In our evaluation, we use infinite queues to avoid deadlock problems.
Reference: [58] <author> S. Konstantinidou and L. Snyder, </author> <title> "Chaos router : architecture and performance," </title> <booktitle> in International Symposium on Computer Architecture, </booktitle> <pages> pp. 212-221, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Virtual cut-through routing is used throughout this thesis. In multidimensional torus networks and shu*e-exchange networks, deadlock situations may develop. Deadlock can be avoided by using various approaches proposed previously. These are structured buffer pools [54], virtual channels [55], deflection routing [57], Chaos routing <ref> [58] </ref>, and abort-and-retry [59, 60]. The deadlock problem and the effect of deadlock-free routing on network performance is beyond the scope of this thesis. In our evaluation, we use infinite queues to avoid deadlock problems.
Reference: [59] <author> D. S. Reeves, D. F. Gehringer, and A. Chandiramani, </author> <title> "Adaptive routing and deadlock recovery: A simulation study," </title> <booktitle> in The 4th Conference on Hypercubes, Concurrent Computers and Applications, </booktitle> <pages> pp. 331-337, </pages> <month> Mar. </month> <year> 1989. </year>
Reference-contexts: Virtual cut-through routing is used throughout this thesis. In multidimensional torus networks and shu*e-exchange networks, deadlock situations may develop. Deadlock can be avoided by using various approaches proposed previously. These are structured buffer pools [54], virtual channels [55], deflection routing [57], Chaos routing [58], and abort-and-retry <ref> [59, 60] </ref>. The deadlock problem and the effect of deadlock-free routing on network performance is beyond the scope of this thesis. In our evaluation, we use infinite queues to avoid deadlock problems.
Reference: [60] <author> J. H. Kim, Z. Liu, and A. A. Chien, </author> <title> "Compressionless routing," </title> <booktitle> in International Symposium on Computer Architecture 94, </booktitle> <year> 1994. </year>
Reference-contexts: Virtual cut-through routing is used throughout this thesis. In multidimensional torus networks and shu*e-exchange networks, deadlock situations may develop. Deadlock can be avoided by using various approaches proposed previously. These are structured buffer pools [54], virtual channels [55], deflection routing [57], Chaos routing [58], and abort-and-retry <ref> [59, 60] </ref>. The deadlock problem and the effect of deadlock-free routing on network performance is beyond the scope of this thesis. In our evaluation, we use infinite queues to avoid deadlock problems.
Reference: [61] <author> P.-Y. Chen, D. H. Lawrie, P.-C. Yew, and D. A. Padua, </author> <title> "Interconnection networks using shu*es," </title> <journal> IEEE Computers, </journal> <pages> pp. 55-64, </pages> <year> 1981. </year>
Reference-contexts: However, the performance of networks measured should not be significantly different from that of networks using a deadlock-free routing technique, such as structured buffer pools or virtual channels, as long as a large queue is used with such methods <ref> [61, 17] </ref>. 2.2 Data Prefetching In data prefetching, future memory accesses are predicted and data are moved to upper levels of a memory hierarchy. Data prefetching can be broadly classified into three groups: hardware data prefetching, software data prefetching, and hybrid data prefetching. <p> This gives us an upper bound on network performance. Other network studies have found that there is not much difference in the performance of networks with unlimited queue size and with a reasonable queue size <ref> [61, 17] </ref>. A snapshot mechanism and fixed priority arbitration are used to select a message among several messages destined to the same output port.
Reference: [62] <author> A. Varma and G. Sinha, </author> <title> "A class of prefetch schemes for on-chip data caches," </title> <booktitle> in International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: However, for the accesses to array B, tagged prefetching generates only unnecessary prefetches. Tagged prefetching is not effective for accesses whose stride is larger than twice the cache line size. 15 Several methods have been proposed <ref> [37, 31, 62] </ref> to reduce the number of unnecessary prefetches in tagged prefetching or stream buffers.
Reference: [63] <author> D. Callahan, K. Kennedy, and A. Porterfield, </author> <title> "Software prefetching," </title> <booktitle> in Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 40-52, </pages> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: This method uses a non-unit stride filter that attempts to detect a memory reference stream and a stride. The filter uses the misses that fall into the same address space partition. The partition size is determined by programmers or compilers. 2.2.2 Software Data Prefetching In software data prefetching <ref> [63, 29, 64, 65] </ref>, prefetching instructions are explicitly inserted in programs. If data access patterns can be detected at compile time, this scheme can prefetch data for any memory access pattern. Example 2.2 shows how prefetch instructions are added to the original code.
Reference: [64] <author> A. C. Klaiber and H. M. Levy, </author> <title> "An architecture for software-controlled data prefetch-ing," </title> <booktitle> in International Symposium on Computer Architecture, </booktitle> <pages> pp. 43-53, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: This method uses a non-unit stride filter that attempts to detect a memory reference stream and a stride. The filter uses the misses that fall into the same address space partition. The partition size is determined by programmers or compilers. 2.2.2 Software Data Prefetching In software data prefetching <ref> [63, 29, 64, 65] </ref>, prefetching instructions are explicitly inserted in programs. If data access patterns can be detected at compile time, this scheme can prefetch data for any memory access pattern. Example 2.2 shows how prefetch instructions are added to the original code.
Reference: [65] <author> T. C. Mowry, M. S. Lam, and A. Gupta, </author> <title> "Design and evaluation of a compiler algorithm for prefetching," </title> <booktitle> in Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 62-73, </pages> <year> 1992. </year>
Reference-contexts: This method uses a non-unit stride filter that attempts to detect a memory reference stream and a stride. The filter uses the misses that fall into the same address space partition. The partition size is determined by programmers or compilers. 2.2.2 Software Data Prefetching In software data prefetching <ref> [63, 29, 64, 65] </ref>, prefetching instructions are explicitly inserted in programs. If data access patterns can be detected at compile time, this scheme can prefetch data for any memory access pattern. Example 2.2 shows how prefetch instructions are added to the original code.
Reference: [66] <author> T. C. Mowry, </author> <title> Tolerating Latency Through Software-Controlled Data Prefetching. </title> <type> PhD thesis, </type> <institution> Stanford University, Computer Systems Laboratory, </institution> <month> Mar. </month> <year> 1994. </year> <month> 160 </month>
Reference-contexts: is 4 words, this approach reduces the overhead of the prefetch instruction execution to one quarter of that in the previous example. prefetch (A (1)) DO I = 1, N, 4 prefetch (A (I+4)) = A (I) = A (I+2) ENDDO Example 2.3: Loop Transformed to Reduce Unnecessary Prefetching Mowry <ref> [66] </ref> developed a compiler algorithm to reduce the overhead of prefetch instruction execution. His scheme is based on data reuse and locality analysis so that prefetch instructions are added for the memory accesses that are very likely to generate cache misses.
Reference: [67] <author> R. Bianchini and T. J. LeBlanc, </author> <title> "A preliminary evaluation of cache-miss-initiated prefetching techniques in scalable multiprocessors," </title> <type> tech. rep., </type> <institution> University of Rochester, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: In addition to the overhead of the prefetch instruction execution, 18 prefetches tend to compete with loads or stores for access to load/store units, which are a relatively scarce resource in most processor architectures. 2.2.3 Hybrid Data Prefetching In hybrid prefetching <ref> [67, 68] </ref>, code is produced at compile-time to generate information on memory access patterns. The information is stored in a separate table and is later used by hardware to prefetch data. Such code is inserted outside a loop and is executed once for the entire loop.
Reference: [68] <author> C.-H. Chi, </author> <title> "Compiler optimization technique for data cache prefetching using a small CAM array," </title> <booktitle> in International Conference on Parallel Processing, </booktitle> <volume> vol. I, </volume> <pages> pp. 263-266, </pages> <year> 1994. </year>
Reference-contexts: In addition to the overhead of the prefetch instruction execution, 18 prefetches tend to compete with loads or stores for access to load/store units, which are a relatively scarce resource in most processor architectures. 2.2.3 Hybrid Data Prefetching In hybrid prefetching <ref> [67, 68] </ref>, code is produced at compile-time to generate information on memory access patterns. The information is stored in a separate table and is later used by hardware to prefetch data. Such code is inserted outside a loop and is executed once for the entire loop.
Reference: [69] <author> T. Chiueh, </author> <title> "Sunder: A programmable hardware prefetch architecture for numerical loops," </title> <booktitle> in Supercomputing, </booktitle> <pages> pp. 488-497, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: Such code is inserted outside a loop and is executed once for the entire loop. When the loop is executed, the table is indexed with the instruction addresses of the memory references and prefetch requests are generated accordingly. In a similar approach proposed in <ref> [69] </ref>, prefetching hardware performs data prefetches autonomously by using additional loop bound information. Gornish [31] proposes another hybrid method in which a compiler generates information on prefetch degree for each memory reference. The prefetch degree is used to prefetch data several iterations ahead in the execution of a loop.
Reference: [70] <author> Y.-C. Chen, </author> <title> Cache Design and Performance in a Large-Scale Shared-Memory Multiprocessor System. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1993. </year>
Reference-contexts: In Chapter 6, different simulation models and benchmarks are used, which are described in that chapter. 2.3.1 Trace Generation Traces are obtained via instrumentation and execution of benchmark programs. Programs are instrumented by the MIPS tracing tool <ref> [70] </ref>, compiled with a MIPS optimizing Fortran compiler and executed on a MIPS M/120 processor system. When executed, the code instrumented by the MIPS tracing tool generates information on memory references and the elapsed time between such references, as well as information about parallel loop execution. <p> All sequential execution is performed by processor 0. 2.3.2 Benchmarks Our application domain is computationally intensive numerical applications under a DOALL computational model <ref> [70] </ref>. These applications usually have large data set sizes and large amounts of data communication between computations. Most of them can be parallelized using DOALL parallel loops. We select our benchmarks from PERFECT [71] and NAS benchmarks [72]. These are ARC3D, FLO52, TRFD, MDG and MG3P. <p> The traces of memory addresses referenced by loads and timing information are obtained via instrumentation and execution of programs. Programs are instrumented by the MIPS tracing tool <ref> [70] </ref>, compiled with a MIPS optimizing Fortran compiler, and executed on a MIPS M120 system that has a R3000/3010 processor. The address traces are supplied to prefetching architecture simulators developed for this study.
Reference: [71] <author> G. Cybenko, L. Kipp, L. Pointer, and D. Kuck, </author> <title> "Supercomputer performance evaluation and the perfect benchmarks," </title> <booktitle> in International Conference on Supercomputing, </booktitle> <year> 1990. </year>
Reference-contexts: These applications usually have large data set sizes and large amounts of data communication between computations. Most of them can be parallelized using DOALL parallel loops. We select our benchmarks from PERFECT <ref> [71] </ref> and NAS benchmarks [72]. These are ARC3D, FLO52, TRFD, MDG and MG3P. ARC3D and FLO52 are used to solve 3-D and 2-D CFD problems, respectively, and TRFD and MDG are used for two-electron integral transformations and molecular dynamics problems, respectively. MG3P implements a 3-D multigrid algorithm. <p> All are computationally intensive numerical applications. These applications usually exhibit large data set sizes. Except for ARC3D, they are selected from among the PERFECT <ref> [71] </ref> and NAS bench 132 Benchmark Description Read Write Total Memory Access (M) Access (M) Access (M) APPBT BT simulated CFD 17.47 6.51 23.98 APPSP SP simulated CFD 16.49 5.31 21.8 ARC3D 3-D CFD 20.33 9.61 29.95 CG Conjugate gradient 14.4 7.33 21.73 DYFESM Structural dynamics 18.08 6.75 24.84 FLO52 2-D
Reference: [72] <author> D. Bailey, H. Simon, J. Barton, and T.Lasinski, </author> <title> "The NAS parallel benchmarks," </title> <type> Tech. Rep. </type> <institution> RNR-91-02, NASA Ames Research Center, </institution> <year> 1991. </year>
Reference-contexts: These applications usually have large data set sizes and large amounts of data communication between computations. Most of them can be parallelized using DOALL parallel loops. We select our benchmarks from PERFECT [71] and NAS benchmarks <ref> [72] </ref>. These are ARC3D, FLO52, TRFD, MDG and MG3P. ARC3D and FLO52 are used to solve 3-D and 2-D CFD problems, respectively, and TRFD and MDG are used for two-electron integral transformations and molecular dynamics problems, respectively. MG3P implements a 3-D multigrid algorithm. <p> 29.95 CG Conjugate gradient 14.4 7.33 21.73 DYFESM Structural dynamics 18.08 6.75 24.84 FLO52 2-D fluid dynamics 15.03 6.79 21.83 MDG Molecular dynamics 20.70 8.96 29.66 MGRID Multigrid kernel 14.53 8.6 15.39 OCEAN 2-D fluid dynamics 13.21 11.69 24.91 TRFD Molecular dynamics 15.39 7.98 23.38 Table 6.1: Benchmark Characteristics marks <ref> [72] </ref>: DYFESM, FLO52, MDG, OCEAN and TRFD are PERFECT benchmarks, and APPBT, APPSP, CG, and MGRID are NAS benchmarks. Table 6.2 shows the read characteristics of each benchmark. The third column of the table shows the L1 cache hit ratio of each benchmark.
Reference: [73] <author> D. Gajski, D. Kuck, D. Lawrie, and A. Sameh, </author> <title> "Cedar a large scale multiprocessor," </title> <booktitle> in International Conference on Parallel Processing, </booktitle> <pages> pp. 524-529, </pages> <year> 1983. </year>
Reference-contexts: MG3P implements a 3-D multigrid algorithm. Selected subroutines, which account for a large fraction of program execution time, are traced. Initially, parallel versions of the programs that ran on the CEDAR <ref> [73] </ref> multiprocessor were selected. These benchmarks were further modified to have better parallelism and cache performance through the introduction of local variables, loop interchange and unrolling. All parallelism is expressed in terms of DOALL loops. Table 2.1 shows the characteristics of each benchmark.
Reference: [74] <author> M. Dubois, S. C, and F. Briggs, </author> <title> "Memory access buffering in multiprocessors," </title> <booktitle> in International Symposium on Computer Architecture, </booktitle> <pages> pp. 434-442, </pages> <year> 1986. </year>
Reference-contexts: Weak ordering is used, which allows memory requests to be issued and completed out of order <ref> [74] </ref>. Every write is acknowledged to achieve weak ordering. The memory system is composed of interleaved and pipelined high-speed memory modules that store shared data.
Reference: [75] <author> A. Veidenbaum, </author> <title> "A compiler-assisted cache coherence solution for multiprocessors," </title> <booktitle> in International Conference on Parallel Processing, </booktitle> <pages> pp. 1029-1036, </pages> <year> 1986. </year>
Reference-contexts: We use a direct-mapped, write-though, no-write-allocate cache. The cache size is 8 Kbytes, the line size is 4 words, and the word size is 8 bytes. The cache takes one cycle to service a cache hit request from a processor. We use a simple cache invalidation method proposed in <ref> [75] </ref> to solve cache coherence problems. In this method, all caches are flushed at synchronization. A 64-entry write buffer is used to minimize processor's stall on a write. When a write buffer is full, a processor stalls on both writes and reads.
Reference: [76] <author> M. J. Karol, M. G. Hluchyj, and S. P. Morgan, </author> <title> "Input versus output queueing on a space-division packet switch," </title> <journal> IEEE Transactions on Communications, </journal> <volume> vol. COM-35, </volume> <pages> pp. 1347-1356, </pages> <month> Dec. </month> <year> 1987. </year>
Reference-contexts: One difference is that the word size, the address bus and the data bus are assumed to be 64 bits instead of 32 bits. These address and data buses are directly connected to a cache. 2.3.3.3 Switch A non-blocking switch model is used <ref> [76, 77, 78] </ref> (see Figure 2.3). In this switch, a number of queues equal to the number of output ports are associated with each input port. On message arrival, an output port is selected by its destination tag, and the message is stored in the corresponding input queue.
Reference: [77] <author> M. Kumar and J. R. </author> <title> Jump, "Performance enhancement in buffered delta networks using crossbar switches and multiple links," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> no. 1, </volume> <pages> pp. 81-103, </pages> <year> 1984. </year>
Reference-contexts: One difference is that the word size, the address bus and the data bus are assumed to be 64 bits instead of 32 bits. These address and data buses are directly connected to a cache. 2.3.3.3 Switch A non-blocking switch model is used <ref> [76, 77, 78] </ref> (see Figure 2.3). In this switch, a number of queues equal to the number of output ports are associated with each input port. On message arrival, an output port is selected by its destination tag, and the message is stored in the corresponding input queue.
Reference: [78] <author> O. E. Percus and S. R. Dickey, </author> <title> "Performance analysis of clocked-regulated queues with output multiplexing in three different 2x2 crossbar switch architectures," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> no. 16, </volume> <pages> pp. 27-40, </pages> <year> 1992. </year>
Reference-contexts: One difference is that the word size, the address bus and the data bus are assumed to be 64 bits instead of 32 bits. These address and data buses are directly connected to a cache. 2.3.3.3 Switch A non-blocking switch model is used <ref> [76, 77, 78] </ref> (see Figure 2.3). In this switch, a number of queues equal to the number of output ports are associated with each input port. On message arrival, an output port is selected by its destination tag, and the message is stored in the corresponding input queue.
Reference: [79] <author> K. Padmanabhan, </author> <title> "Design and analysis of even-sized binary shu*e-exchange networks for multiprocessor," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 2, </volume> <pages> pp. 385-397, </pages> <month> Oct. </month> <year> 1991. </year> <month> 161 </month>
Reference-contexts: Thus, a processor and a memory share links to the network through the DMUX that is connected to the first and last stage of the network. We use a tag-based distributed routing algorithm proposed in <ref> [79] </ref>. The single stage shu*e-exchange network (Figure 2.6) consists of only one shu*e-exchange stage. A processor and a memory module is attached to a link of the switch through a multiplexor and demultiplexor (DMUX). This construction is used in many studies of single stage shu*e-exchange networks [80, 81].
Reference: [80] <author> P.-Y. Chen, P.-C. Yew, and D. Lawrie, </author> <title> "Performance of packet switching in buffered single-stage shu*e-exchange networks," </title> <booktitle> in International Conference on Distributed Computing Systems, </booktitle> <pages> pp. 622-627, </pages> <year> 1982. </year>
Reference-contexts: The single stage shu*e-exchange network (Figure 2.6) consists of only one shu*e-exchange stage. A processor and a memory module is attached to a link of the switch through a multiplexor and demultiplexor (DMUX). This construction is used in many studies of single stage shu*e-exchange networks <ref> [80, 81] </ref>. A shortest path routing algorithm proposed in [82] is used. Wires between switches incur delays. We use one cycle for the wire delay. When a DMUX is used for a network, the DMUX causes an additional delay.
Reference: [81] <author> D. H. Lawrie and D. A. Padua, </author> <title> "Analysis of message switching with shu*e-exchanges in multiprocessors," </title> <booktitle> in The Proceedings of the Workshop on Interconnection Networks for Parallel and Distributed Processing, </booktitle> <pages> pp. 116-123, </pages> <year> 1980. </year>
Reference-contexts: The single stage shu*e-exchange network (Figure 2.6) consists of only one shu*e-exchange stage. A processor and a memory module is attached to a link of the switch through a multiplexor and demultiplexor (DMUX). This construction is used in many studies of single stage shu*e-exchange networks <ref> [80, 81] </ref>. A shortest path routing algorithm proposed in [82] is used. Wires between switches incur delays. We use one cycle for the wire delay. When a DMUX is used for a network, the DMUX causes an additional delay.
Reference: [82] <author> S. Kim and A. Veidenbaum, </author> <title> "On shortest path routing in single stage shu*e-exchange networks," </title> <booktitle> in Seventh Annual ACM Symposium on Parallel Algorithms and Architectures 95, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: A processor and a memory module is attached to a link of the switch through a multiplexor and demultiplexor (DMUX). This construction is used in many studies of single stage shu*e-exchange networks [80, 81]. A shortest path routing algorithm proposed in <ref> [82] </ref> is used. Wires between switches incur delays. We use one cycle for the wire delay. When a DMUX is used for a network, the DMUX causes an additional delay. Since the DMUX 27 is simpler than a switch, we assume one cycle for the minimum DMUX latency.
Reference: [83] <author> L. N. Bhuyan and D. P. Agrawal, </author> <title> "Design and performance of generalized interconnection networks," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-32, </volume> <pages> pp. 1081-1090, </pages> <month> Dec. </month> <year> 1983. </year>
Reference-contexts: The DMUX has infinite queues. Although the infinite queues shift a part of switch queueing delay to the DMUX queueing delay, this does not affect our conclusions. 28 Chapter 3 Shortest Path Routing in Single Stage Shu*e-Exchange Networks 3.1 Introduction Among various interconnection network architectures, shu*e-exchange networks <ref> [83, 51, 52, 53] </ref> have been one of the most popular architectures. These networks have distributed self-routing capabilities, and when network size is increased, the internode distance and the number of switch components are increased logarithmically. These features make shu*e-exchange networks suitable for large-scale multiprocessor systems. <p> Although this simple routing tag generation is an attractive feature of shu*e-exchange networks, the network size restriction it imposes is a serious limitation on incremental network scalability. When we want to increase the size of a network, the new size has to be K times larger. In <ref> [83] </ref>, a new approach is taken to allow a more flexible network size which can be a composite number instead of a power of K. In this approach, network size needs to be factored into r numbers so that each network terminal number can be expressed as an r-tuple.
Reference: [84] <author> D. Z. Du and F. K. Hwang, </author> <title> "Generalized de Bruijn Diagraphs," </title> <journal> Networks, </journal> <volume> vol. 18, </volume> <pages> pp. 27-38, </pages> <year> 1988. </year>
Reference-contexts: Based on this theorem, we develop an algorithm to find a routing tag for one of several shortest paths in single stage networks. Later, we extend the theorem for the shortest path routing in generalized de Bruijn Digraphs <ref> [84] </ref>, which is another variant of single stage network architectures. 3.2.1 Network Structure A K-ary shu*e-exchange multistage network provides connections between N input terminals and N output terminals. The network consists of several shu*e-exchange stages. <p> In this structure, when a message is initially routed from a node, the exchange function of the switch is performed before the shu*e function is executed through the shu*e connection between switches. Logically, it can be represented by a generalized de Bruijn Digraph defined in <ref> [84] </ref>. Since the application order of the shu*e and exchange function is reversed, the routing tag generation for the direct SSX networks may require a different algorithm. However, as the next corollary proves, the relation between routing tags and other parameters remains 42 the same.
Reference: [85] <author> J. Torrellas and Z. Zhang, </author> <title> "The performance of the Cedar multistage switching network," </title> <booktitle> in Supercomputing, </booktitle> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: This chapter performs such a detailed study of network behavior and design trade-offs with respect to performance and cost. The only previous work, to our knowledge, in which such a detailed study has been 54 attempted is <ref> [20, 21, 85] </ref>, yet network cost was not taken into account in these studies. As pointed out in [20], most previous network studies have relied on analytical modeling and/or simulations which are based on simplified assumptions.
Reference: [86] <author> L. Gwennap, </author> <title> "Alternative packages emerge for processors." </title> <type> Microprocessor Report, </type> <month> Oct. </month> <year> 1993. </year>
Reference-contexts: In many cases, the number of I/O pads completely determines die size. For a certain range of the number of switch pins, the packaging cost tends to increase linearly in proportion to the number of pins <ref> [86] </ref>. In addition, a switch with wider channels can be constructed from switches with smaller channels by using a bit-slicing technique.
Reference: [87] <author> S. W. Turner and A. V. Veidenbaum, </author> <title> "Scalability of the Cedar system," </title> <booktitle> in Supercomputing, </booktitle> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: Another important issue addressed in this chapter is how to optimally use a given memory and network bandwidth. This issue becomes more important when prefetching is introduced, because prefetching tends to demand more network and memory bandwidth. Turner and Veidenbaum <ref> [87] </ref> showed that limiting the number of outstanding requests can sometimes help to minimize network latency. However, their study was done with vector multiprocessor systems, which tend to demand much higher memory and network bandwidth.
Reference: [88] <author> S. A. Przybylski, </author> <title> Cache and Memory Hierarchy Design. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1990. </year> <month> 162 </month>
Reference-contexts: Hence, achieving a low L2 cache miss rate is very important for high-performance memory systems. Secondary caches tend to have a lower hit rate than the primary cache <ref> [88] </ref>. For applications accessing large data sets, even large L2 caches cannot provide enough space and cause a lot of main memory accesses. For such applications, data prefetching at the second-level of a memory hierarchy is an attractive solution to hide a large main memory access latency.
References-found: 88

