URL: ftp://flint.cs.yale.edu/pub/flint/publications/callee.ps.gz
Refering-URL: http://daffy.cs.yale.edu/users/shao-zhong/papers.html
Root-URL: http://www.cs.yale.edu
Title: Callee-save Registers in Continuation-passing Style  
Author: ANDREW W. APPEL ZHONG SHAO 
Keyword: Register Allocation, Continuation-passing Style, Procedure Call  
Address: Princeton, NJ 08544-2087  
Affiliation: Department of Computer Science, Princeton University,  
Note: LISP AND SYMBOLIC COMPUTATION: An International Journal,  c 1992 Kluwer Academic Publishers Manufactured in The Netherlands  
Email: (appel@princeton.edu)  (zsh@princeton.edu)  
Date: 5, 191-221, 1992  
Abstract: Continuation-passing style (CPS) is a good abstract representation to use for compilation and optimization: it has a clean semantics and is easily manipulated. We examine how CPS expresses the saving and restoring of registers in source-language procedure calls. In most CPS-based compilers, the context of the calling procedure is saved in a "continuation closure"|a single variable that is passed as an argument to the function being called. This closure is a record containing bindings of all the free variables of the continuation; that is, registers that hold values needed by the caller "after the call" are written to memory in the closure, and fetched back after the call. Consider the procedure-call mechanisms used by conventional compilers. In particular, registers holding values needed after the call must be saved and later restored. The responsibility for saving registers can lie with the caller (a "caller-saves" convention) or with the called function ("callee-saves"). In practice, to optimize memory traffic, compilers find it useful to have some caller-saves registers and some callee-saves. "Conventional" CPS-based compilers that pass a pointer to a record containing all the variables needed after the call (i.e., the continuation closure), are using a caller-saves convention. We explain how to express callee-save registers in Continuation-Passing Style, and give measurements showing the resulting improvement in execution time. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Appel, Andrew W. </author> <title> Garbage collection can be faster than stack allocation. </title> <journal> Information Processing Letters, </journal> <volume> 25, </volume> <month> 4 </month> <year> (1987) </year> <month> 275-79. </month>
Reference-contexts: The idea is that the explicit deallocation (popping) of these closures is cheaper than garbage-collecting them. While we have argued that there is no inherent lower bound on the cost of garbage collection <ref> [1] </ref>, clearly in real systems there is still some overhead (as figures 10-14 show). On the other hand, the use of a runtime stack has some serious disadvantages, adding to the complexity of the runtime system|especially when an efficient call-with-current-continuation is needed [13].
Reference: 2. <author> Appel, Andrew W. </author> <title> Simple generational garbage collection and fast allocation. </title> <journal> Software|Practice and Experience, </journal> <volume> 19, </volume> <month> 2 </month> <year> (1989) </year> <month> 171-83. </month>
Reference-contexts: For each benchmark, we used a heap of approximately 5 times the amount of live data when compiled by the sml0 compiler; then the same heap size (regardless of the amount of live data) for the callee-save versions. We use an efficient two-generation collector <ref> [2] </ref>. CALLEE-SAVE REGISTERS IN CONTINUATION-PASSING STYLE 215 9. Conclusions Most continuation-based compilers [20, 17, 15] perform escape-analysis on closures to see which ones can be allocated on a stack. The idea is that the explicit deallocation (popping) of these closures is cheaper than garbage-collecting them.
Reference: 3. <author> Appel, Andrew W. </author> <title> Compiling with Continuations. </title> <publisher> Cambridge University Press (1992). </publisher>
Reference-contexts: The transformational compiler of Kelsey and Hudak [15], and the Standard ML of New Jersey compiler <ref> [4, 3] </ref>, refine this notion further so that the "abstract" optimizer can control representation decisions while still staying within the original, semantically clean and powerful Continuation Passing Style. <p> CALLEE-SAVE REGISTERS IN CONTINUATION-PASSING STYLE 197 we have the code in figure 3 (the detailed translation rules are described in Steele [20] or Appel <ref> [3] </ref>). There are two kinds of functions in our CPS code. Continuation functions are introduced in the CPS conversion phase, e.g. j, k, s. We call all non-continuation functions user functions. <p> arguments: one is the standard argument (note that an n-tuple is considered to be one argument, as in the call to f) and the another is the continuation argument. * All escaping continuation functions have one argument. * Known functions may have an arbitrary number of arguments after calling-sequence specialization <ref> [4, 3] </ref>. 4. Closure representations Continuation-passing style is meant to approximate the operation of a von Neumann computer; each operator of the former corresponds to one (or at most a few) instructions of the latter. <p> But in our initial implementation this did not seem to be the case. The problem turned out to lie in the algorithm for assigning CPS variables to registers of the target machine. After CPS-conversion, closure-conversion, and spill transformation <ref> [4, 3] </ref>, no subexpression of the CPS representation of the program can have more than m free variables, where m is the number of registers (callee+caller save) of the target machine. To do register assignment, we can traverse the CPS expression top-down, choosing a register for each variable-binding in turn. <p> We would like to avoid such moves wherever possible. Before implementing the callee-save transformation described in this paper, we had 210 APPEL AND SHAO three useful heuristics <ref> [3] </ref>: 1. Suppose there is a known function f (~x). We can choose registers in which the x i are to be passed. <p> If we change the location of the compiled code and the layout of the data space, the running time of the program can vary by 5% to 20% even though the same number of instructions is executed <ref> [3] </ref>. So we also measured instruction counts on the MIPS. We found that two callee-save registers worked best on the MIPS, and three on the SPARC.
Reference: 4. <author> Appel, Andrew W. and Jim, Trevor. </author> <title> Continuation-passing, closure-passing style. </title> <booktitle> In Sixteenth ACM Symp. on Principles of Programming Languages, </booktitle> <publisher> ACM Press, </publisher> <address> New York (1989) 293-302. </address>
Reference-contexts: The transformational compiler of Kelsey and Hudak [15], and the Standard ML of New Jersey compiler <ref> [4, 3] </ref>, refine this notion further so that the "abstract" optimizer can control representation decisions while still staying within the original, semantically clean and powerful Continuation Passing Style. <p> Continuation-passing style has several advantages as an intermediate representation for an optimizing compiler <ref> [20, 16, 4] </ref>. Because it has simple static scope, in-line expansion of functions (fi-reduction) is very simple to express as are constant-folding and other partial evaluations, common-subexpression-elimination, dead variable elimination, loop optimizations, and function calling-sequence specialization. <p> arguments: one is the standard argument (note that an n-tuple is considered to be one argument, as in the call to f) and the another is the continuation argument. * All escaping continuation functions have one argument. * Known functions may have an arbitrary number of arguments after calling-sequence specialization <ref> [4, 3] </ref>. 4. Closure representations Continuation-passing style is meant to approximate the operation of a von Neumann computer; each operator of the former corresponds to one (or at most a few) instructions of the latter. <p> The machine-code implementation of the function knows to find the values of free variables in the closure data structure. Conventional compilers use an activation record to hold the values of variables accessible by the function body. In most CPS-based compilers <ref> [17, 15, 4] </ref>, function parameters and local variables are held in registers, and free variables are accessed from the closure, so that the closure and registers together serve in place of the activation record. <p> This has been done in compilers based on ordinary -calculus [14, 10] and on continuation-passing style <ref> [15, 4] </ref>. We will represent closure creation and use in the continuation-passing style itself; each closure will be an explicit record of the CPS. <p> But in our initial implementation this did not seem to be the case. The problem turned out to lie in the algorithm for assigning CPS variables to registers of the target machine. After CPS-conversion, closure-conversion, and spill transformation <ref> [4, 3] </ref>, no subexpression of the CPS representation of the program can have more than m free variables, where m is the number of registers (callee+caller save) of the target machine. To do register assignment, we can traverse the CPS expression top-down, choosing a register for each variable-binding in turn.
Reference: 5. <author> Appel, Andrew W. and MacQueen, David B. </author> <title> A Standard ML compiler. </title> <editor> In Kahn, Gilles, editor, </editor> <booktitle> Functional Programming Languages and Computer Architecture (LNCS 274), </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York (1987) 301-24. </address>
Reference-contexts: A typical representation puts the code-pointer at field 0 of the closure-record, and the free variables at other offsets from the closure pointer, perhaps in a linked list of closures. One interesting trick <ref> [17, 5] </ref> is to let several functions share a single closure. The functions m and s might ordinarily be represented like this: The closure for m has the value for the free variable t; the closure for s has free variables m and e.
Reference: 6. <author> Appel, Andrew W., Ellis, John R., and Li, Kai. </author> <title> Real-time concurrent collection on stock multiprocessors. </title> <booktitle> SIGPLAN Notices (Proc. SIG-PLAN '88 Conf. on Prog. Lang. Design and Implementation), </booktitle> <volume> 23, </volume> <month> 7 </month> <year> (1988) </year> <month> 11-20. </month>
Reference-contexts: Most CPS-based compilers [20, 17] allocate certain closures on a stack if their pattern of usage can be sufficiently analyzed at compile time, so that they can be efficiently and promptly deallocated. However, using a stack complicates the garbage-collector interface <ref> [6] </ref>, and greatly complicates the efficient implementation of call-with-current-continuation [13]. The techniques we present here neither require nor preclude the use of a stack. We will avoid any technique that calls for side effects to existing closures.
Reference: 7. <author> Appel, Andrew W., Mattson, James S., and Tarditi, David R. </author> <title> A lexical analyzer generator for Standard ML. </title> <month> (December </month> <year> 1989). </year> <title> Distributed with Standard ML of New Jersey. </title>
Reference-contexts: Tarditi [21], processing the grammer of Standard ML. Lexgen A lexical-analyzer generator, implemented by James S. Mattson and David R. Tarditi <ref> [7] </ref>, processing the lexical description of Stan dard ML. Knuth-B An implementation of the Knuth-Bendix completion algorithm, implemented by Gerard Huet, processing some axioms of geometry.
Reference: 8. <author> Cardelli, Luca. </author> <title> Compiling a functional language. </title> <booktitle> In 1984 Symp. on LISP and Functional Programming, </booktitle> <publisher> ACM Press, </publisher> <address> New York (1984) 208-17. </address>
Reference-contexts: In general, escaping functions must be compiled using a standard closure mechanism; known functions can use cheaper, more specialized representations [20], typically with free variables passed as arguments. 5. Closure-passing style Some compilers <ref> [20, 8, 17] </ref> perform these closure analyses as part of their translation from lambda calculus or continuation-passing style into machine code.
Reference: 9. <author> Chow, Fred C. </author> <title> Minimizing register usage penalty at procedure calls. </title> <booktitle> In Proc. SIGPLAN '88 Conf. on Prog. Lang. Design and Implementation, </booktitle> <publisher> ACM Press, </publisher> <address> New York (June 1988) 85-94. </address>
Reference-contexts: Though the use of caller- and callee-save registers seems to be a very old part of lore of compilers, it appears that only recently have compilers attempted to optimize the placement of variables into caller- and callee-save registers <ref> [9] </ref>. 3. Continuation-passing style Continuation-passing style (CPS) is a language similar to -calculus, but which closely reflects the control-flow and data-flow operations of a von Neumann machine.
Reference: 10. <author> Cousineau, G., Curien, P. L., and Mauny, M. </author> <title> The categorical abstract machine. </title> <editor> In Jouannaud, J. P., editor, </editor> <booktitle> Functional Programming Languages and Computer Architecture, </booktitle> <volume> LNCS Vol 201, </volume> <publisher> Springer-Verlag, </publisher> <address> New York (1985) 50-64. </address>
Reference-contexts: But it is useful to separate the closure-introduction from machine-code generation so that the compiler is more modular; we can rewrite the program into an equivalent one in which no function has free variables. This has been done in compilers based on ordinary -calculus <ref> [14, 10] </ref> and on continuation-passing style [15, 4]. We will represent closure creation and use in the continuation-passing style itself; each closure will be an explicit record of the CPS.
Reference: 11. <author> Crowley, W. P., Hendrickson, C. P., and Rudy, T. E. </author> <title> The SIMPLE Code. </title> <type> Technical Report UCID 17715, </type> <institution> Lawrence Livermore Laboratory, Livermore, </institution> <note> CA (February 1978). CALLEE-SAVE REGISTERS IN CONTINUATION-PASSING STYLE 217 </note>
Reference-contexts: Mattson and David R. Tarditi [7], processing the lexical description of Stan dard ML. Knuth-B An implementation of the Knuth-Bendix completion algorithm, implemented by Gerard Huet, processing some axioms of geometry. Simple A spherical fluid-dynamics program, developed as a "realistic" FORTRAN benchmark <ref> [11] </ref>, translated into ID [12], and then trans lated into Standard ML by Lal George. VLIW A Very-Long-Instruction-Word instruction scheduler written by John Danskin. Measurements on the machine such as MIPS 3230 became extremely inaccurate because of cache effects.
Reference: 12. <author> Ekanadham, K. and Arvind. </author> <title> SIMPLE: An Exercise in Future Scientific Programming. Technical Report Computation Structures Group Memo 273, </title> <publisher> MIT, </publisher> <address> Cambridge, MA (July 1987). </address> <note> Simultaneously published as IBM/T.J. </note> <institution> Watson Research Center Research Report 12686, Yorktown Heights, NY. </institution>
Reference-contexts: Mattson and David R. Tarditi [7], processing the lexical description of Stan dard ML. Knuth-B An implementation of the Knuth-Bendix completion algorithm, implemented by Gerard Huet, processing some axioms of geometry. Simple A spherical fluid-dynamics program, developed as a "realistic" FORTRAN benchmark [11], translated into ID <ref> [12] </ref>, and then trans lated into Standard ML by Lal George. VLIW A Very-Long-Instruction-Word instruction scheduler written by John Danskin. Measurements on the machine such as MIPS 3230 became extremely inaccurate because of cache effects.
Reference: 13. <author> Hieb, Robert, Dybvig, R. Kent, and Bruggeman, Carl. </author> <title> Representing control in the presence of first-class continuations. </title> <booktitle> In Proc. ACM SIG-PLAN '90 Conf. on Prog. Lang. Design and Implementation, </booktitle> <publisher> ACM Press, </publisher> <address> New York (1990) 66-77. </address>
Reference-contexts: Most CPS-based compilers [20, 17] allocate certain closures on a stack if their pattern of usage can be sufficiently analyzed at compile time, so that they can be efficiently and promptly deallocated. However, using a stack complicates the garbage-collector interface [6], and greatly complicates the efficient implementation of call-with-current-continuation <ref> [13] </ref>. The techniques we present here neither require nor preclude the use of a stack. We will avoid any technique that calls for side effects to existing closures. In the first place, this corrupts the notion of CPS as a functional, not imperative, notation for program optimization. <p> On the other hand, the use of a runtime stack has some serious disadvantages, adding to the complexity of the runtime system|especially when an efficient call-with-current-continuation is needed <ref> [13] </ref>. Our new callee-save technique will work well with or without stack allocation. The closures merged together by our method are a large proportion of the ones that would be stack allocated in the "traditional" method.
Reference: 14. <author> Johnsson, Thomas. </author> <title> Lambda lifting: Transforming programs to recursive equations. </title> <booktitle> In The Second International Conference on Functional Programming Languag es and Computer Architecture, </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York (September 1985) 190-203. </address>
Reference-contexts: But it is useful to separate the closure-introduction from machine-code generation so that the compiler is more modular; we can rewrite the program into an equivalent one in which no function has free variables. This has been done in compilers based on ordinary -calculus <ref> [14, 10] </ref> and on continuation-passing style [15, 4]. We will represent closure creation and use in the continuation-passing style itself; each closure will be an explicit record of the CPS.
Reference: 15. <author> Kelsey, Richard and Hudak, Paul. </author> <title> Realistic compilation by program transformation. </title> <booktitle> In Sixteenth ACM Symp. on Principles of Programming Languages, </booktitle> <publisher> ACM Press, </publisher> <address> New York (1989) 281-92. </address>
Reference-contexts: The transformational compiler of Kelsey and Hudak <ref> [15] </ref>, and the Standard ML of New Jersey compiler [4, 3], refine this notion further so that the "abstract" optimizer can control representation decisions while still staying within the original, semantically clean and powerful Continuation Passing Style. <p> The machine-code implementation of the function knows to find the values of free variables in the closure data structure. Conventional compilers use an activation record to hold the values of variables accessible by the function body. In most CPS-based compilers <ref> [17, 15, 4] </ref>, function parameters and local variables are held in registers, and free variables are accessed from the closure, so that the closure and registers together serve in place of the activation record. <p> This has been done in compilers based on ordinary -calculus [14, 10] and on continuation-passing style <ref> [15, 4] </ref>. We will represent closure creation and use in the continuation-passing style itself; each closure will be an explicit record of the CPS. <p> We use an efficient two-generation collector [2]. CALLEE-SAVE REGISTERS IN CONTINUATION-PASSING STYLE 215 9. Conclusions Most continuation-based compilers <ref> [20, 17, 15] </ref> perform escape-analysis on closures to see which ones can be allocated on a stack. The idea is that the explicit deallocation (popping) of these closures is cheaper than garbage-collecting them.
Reference: 16. <author> Kranz, David. </author> <title> ORBIT: An optimizing compiler for Scheme. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <address> New Haven, CT (1987). </address>
Reference-contexts: Continuation-passing style has several advantages as an intermediate representation for an optimizing compiler <ref> [20, 16, 4] </ref>. Because it has simple static scope, in-line expansion of functions (fi-reduction) is very simple to express as are constant-folding and other partial evaluations, common-subexpression-elimination, dead variable elimination, loop optimizations, and function calling-sequence specialization. <p> On at least one of the calls to f , therefore, we won't need any move instructions at all: we'll just choose the same registers in which the actual parameters are located. This technique was used in Kranz's orbit compiler <ref> [16] </ref>. We can't do this for escaping functions, which must have standardized calling conventions. 2. Suppose the function call f (~x) within the subexpression is an escaping function, or a known function for which we have already determined the assignment of formal parameters.
Reference: 17. <author> Kranz, D., Kelsey, R., Rees, J., Hudak, P., Philbin, J., and Adams, N. </author> <title> ORBIT: An optimizing compiler for Scheme. </title> <booktitle> SIGPLAN Notices (Proc. Sigplan '86 Symp. on Compiler Construction), </booktitle> <volume> 21, </volume> <month> 7 (July </month> <year> 1986) </year> <month> 219-33. </month>
Reference-contexts: As first used in compiling by Steele [20], CPS was largely divorced from low-level representations: its target machine was a Lisp compiler. The ORBIT compiler of Kranz et al <ref> [17] </ref> is organized so that the variables of the CPS language correspond directly to registers or to memory locations (fields of closures), and the optimizer can tell which is which, enabling more intelligent decisions about program transformations. <p> The machine-code implementation of the function knows to find the values of free variables in the closure data structure. Conventional compilers use an activation record to hold the values of variables accessible by the function body. In most CPS-based compilers <ref> [17, 15, 4] </ref>, function parameters and local variables are held in registers, and free variables are accessed from the closure, so that the closure and registers together serve in place of the activation record. <p> In most CPS-based compilers [17, 15, 4], function parameters and local variables are held in registers, and free variables are accessed from the closure, so that the closure and registers together serve in place of the activation record. Most CPS-based compilers <ref> [20, 17] </ref> allocate certain closures on a stack if their pattern of usage can be sufficiently analyzed at compile time, so that they can be efficiently and promptly deallocated. However, using a stack complicates the garbage-collector interface [6], and greatly complicates the efficient implementation of call-with-current-continuation [13]. <p> A typical representation puts the code-pointer at field 0 of the closure-record, and the free variables at other offsets from the closure pointer, perhaps in a linked list of closures. One interesting trick <ref> [17, 5] </ref> is to let several functions share a single closure. The functions m and s might ordinarily be represented like this: The closure for m has the value for the free variable t; the closure for s has free variables m and e. <p> In general, escaping functions must be compiled using a standard closure mechanism; known functions can use cheaper, more specialized representations [20], typically with free variables passed as arguments. 5. Closure-passing style Some compilers <ref> [20, 8, 17] </ref> perform these closure analyses as part of their translation from lambda calculus or continuation-passing style into machine code. <p> We use an efficient two-generation collector [2]. CALLEE-SAVE REGISTERS IN CONTINUATION-PASSING STYLE 215 9. Conclusions Most continuation-based compilers <ref> [20, 17, 15] </ref> perform escape-analysis on closures to see which ones can be allocated on a stack. The idea is that the explicit deallocation (popping) of these closures is cheaper than garbage-collecting them.
Reference: 18. <author> Landin, P. J. </author> <title> The mechanical evaluation of expressions. </title> <journal> Computer J., </journal> <volume> 6, </volume> <month> 4 </month> <year> (1964) </year> <month> 308-20. </month>
Reference-contexts: We say that these are free variables of f . The notion of a function as a machine-code address does not provide for free variables. The usual solution to this problem is to represent functions as closures <ref> [18] </ref>. A function with free variables is said to be open; a closure is a data structure containing both the machine code address of an open function, and bindings for all the free variables of that function.
Reference: 19. <author> Reade, Chris. </author> <title> Elements of Functional Programming. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA (1989). </address>
Reference-contexts: The benchmark programs were: Life The game of Life, written by Chris Reade and described in his book <ref> [19] </ref>, running 50 generations of a glider gun. 212 APPEL AND SHAO sml0 smlt0 time time speedup Life 27.72 27.29 1.55% Yacc 9.22 8.88 3.69% Lexgen 20.72 20.78 -0.29% Knuth-B 23.66 24.30 -2.70% Simple 70.77 71.93 -1.64% VLIW 51.88 44.92 13.4 % sml3 smlt3 time speedup time speedup Life 26.87 3.1%
Reference: 20. <author> Steele, Guy L. Rabbit: </author> <title> a compiler for Scheme. </title> <type> Technical Report AI-TR-474, </type> <institution> MIT, </institution> <address> Cambridge, MA (1978). </address>
Reference-contexts: A good example of a semantically clean abstract language that makes many kinds of high-level optimizations easy is Continuation-Passing Style (CPS). As first used in compiling by Steele <ref> [20] </ref>, CPS was largely divorced from low-level representations: its target machine was a Lisp compiler. <p> Continuation-passing style has several advantages as an intermediate representation for an optimizing compiler <ref> [20, 16, 4] </ref>. Because it has simple static scope, in-line expansion of functions (fi-reduction) is very simple to express as are constant-folding and other partial evaluations, common-subexpression-elimination, dead variable elimination, loop optimizations, and function calling-sequence specialization. <p> CALLEE-SAVE REGISTERS IN CONTINUATION-PASSING STYLE 197 we have the code in figure 3 (the detailed translation rules are described in Steele <ref> [20] </ref> or Appel [3]). There are two kinds of functions in our CPS code. Continuation functions are introduced in the CPS conversion phase, e.g. j, k, s. We call all non-continuation functions user functions. <p> In most CPS-based compilers [17, 15, 4], function parameters and local variables are held in registers, and free variables are accessed from the closure, so that the closure and registers together serve in place of the activation record. Most CPS-based compilers <ref> [20, 17] </ref> allocate certain closures on a stack if their pattern of usage can be sufficiently analyzed at compile time, so that they can be efficiently and promptly deallocated. However, using a stack complicates the garbage-collector interface [6], and greatly complicates the efficient implementation of call-with-current-continuation [13]. <p> In general, escaping functions must be compiled using a standard closure mechanism; known functions can use cheaper, more specialized representations <ref> [20] </ref>, typically with free variables passed as arguments. 5. Closure-passing style Some compilers [20, 8, 17] perform these closure analyses as part of their translation from lambda calculus or continuation-passing style into machine code. <p> In general, escaping functions must be compiled using a standard closure mechanism; known functions can use cheaper, more specialized representations [20], typically with free variables passed as arguments. 5. Closure-passing style Some compilers <ref> [20, 8, 17] </ref> perform these closure analyses as part of their translation from lambda calculus or continuation-passing style into machine code. <p> We use an efficient two-generation collector [2]. CALLEE-SAVE REGISTERS IN CONTINUATION-PASSING STYLE 215 9. Conclusions Most continuation-based compilers <ref> [20, 17, 15] </ref> perform escape-analysis on closures to see which ones can be allocated on a stack. The idea is that the explicit deallocation (popping) of these closures is cheaper than garbage-collecting them.

References-found: 20

