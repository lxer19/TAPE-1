URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR94494-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Compiler Support for Machine-Independent Parallelization of Irregular Problems  
Author: Reinhard von Hanxleden 
Address: P.O. Box 1892 Houston, TX 77251-1892  
Affiliation: Rice University  
Note: Center for Research on Parallel Computation  
Date: December 1994  
Pubnum: CRPC-TR94494-S  
Abstract-found: 0
Intro-found: 1
Reference: [AL93] <author> S. P. Amarasinghe and M. S. Lam. </author> <title> Communication optimization and code generation for distributed memory machines. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 28(6) </volume> <pages> 126-138, </pages> <month> June </month> <year> 1993. </year> <booktitle> Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation. </booktitle>
Reference-contexts: Several researchers have addressed the communication generation problem, although often restricted to relatively simple array reference patterns. Amarasinghe and Lam optimize communication generation using Last Write Trees <ref> [AL93] </ref>. They assume affine loop bounds and array indices, they do not allow loops within conditionals as shown in Figure 3.1. Gupta and Schonberg use Available Section Descriptors, computed by interval based data-flow analysis, to determine the availability of data on a virtual processor grid [GS93].
Reference: [All70] <author> F. E. Allen. </author> <title> Control flow analysis. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 5(7) </volume> <pages> 1-19, </pages> <year> 1970. </year>
Reference-contexts: The W2 compiler [GS90] for the Warp multiprocessor gathers information such as the set of definitions reaching a basic block to exploit the fine-grain parallelism offered by the highly pipelined functional units. It is based on interval analysis <ref> [All70, Coc70] </ref> and computes information with array region granularity. Granston and Veidenbaum combine flow and dependence analysis to detect redundant global memory accesses in parallelized and vectorized codes [GV91]. They assume that the program is already annotated with Read/Write operations. <p> These data are indicated in the framework by GIVE (l) GIVE init (l). 3.3.4 The Interval-Flow Graph A general data-flow analysis algorithm that considers loop nesting hierarchies is interval analysis. It can be used for forward problems, such as available expressions <ref> [All70, Coc70] </ref>, and backward problems, such as live variables) [Ken71], and it has also been used for code motion [DP93] and incremental analysis [Bur90]. We are using a variant of interval analysis that is based on Tarjan intervals [Tar74].
Reference: [APT90] <author> F. Andre, J. Pazat, and H. Thomas. </author> <title> Pandore: A system to manage data distribution. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Several compilation systems for exploiting fine-grained parallelism have been and are being built, which include Al [Tse90], Aspar [IFKF90], C*/Dataparallel C [HQL + 91, RS87], Crystal [LC91], Dino [RSW91], Id Nouveau [RP89], Mimd-izer [SWW92], Oxygen [RA90], P 3 C [GAY91], Pandore <ref> [APT90] </ref>, Parafrase-2 [GB92], Paragon [CCRS91], Spot [SS90, Soc90], Superb [ZBG88], and Vienna Fortran [BCZ92].
Reference: [Bad87] <author> S. B. Baden. </author> <title> Run-Time Partitioning of Scientific Continuum Calculations Running on Multiprocessors. </title> <type> PhD thesis, </type> <institution> Lawrence Berkeley Laboratory, University of California, </institution> <year> 1987. </year>
Reference-contexts: The second group comes into play after the data structures have been laid out; these tools try to free the user from dealing with the access properties of the parallel program; i.e., they examine data locality. 7.1.1 Tools based on spatial decomposition The Generic Multiprocessor The Generic Multiprocessor (GenMP) <ref> [Bad87, Bad91] </ref> aims at providing a machine-independent programming environment for a certain class of problems, namely scientific calculations that are spatially localized on a mesh.
Reference: [Bad91] <author> S. B. Baden. </author> <title> Programming abstractions for dynamically partitioning and coordinating localized scientific calculations running on multiprocessors. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 12(1) </volume> <pages> 145-157, </pages> <year> 1991. </year>
Reference-contexts: The second group comes into play after the data structures have been laid out; these tools try to free the user from dealing with the access properties of the parallel program; i.e., they examine data locality. 7.1.1 Tools based on spatial decomposition The Generic Multiprocessor The Generic Multiprocessor (GenMP) <ref> [Bad87, Bad91] </ref> aims at providing a machine-independent programming environment for a certain class of problems, namely scientific calculations that are spatially localized on a mesh.
Reference: [Bad92] <author> S. B. Baden. </author> <title> Lattice parallelism: A parallel programming model for manipulating localized non-uniform scientific data structures. In Intel Supercomputer University Partners Conference, </title> <address> Timberline Lodge, Mt. Hood, OR, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: The limitation of this approach lies in its specificity towards applications that already use data structures reflecting locality characteristics; this we try to overcome by using general value-based decompositions as introduced in Chapter 2. Lattice Parallelism Lattice Parallelism (LPar) <ref> [Bad92, BK93, BKF94, FB95] </ref> is an SPMD programming model that supports coarse-grained parallelism based on the Fidil language [HC88] and the owner computes rule. It is intended for non-uniform computations that involve partial differential equations and have local structure. <p> It enables very elegant formulations of a limited class of problems and can be seen as a potential user of an implementation of the value-based decompositions proposed in Chapter 2. Citing Baden <ref> [Bad92] </ref>: It is not an implementation-level system, and relies on application libraries or other run time systems to handle data partitioning or to handle machine-level optimizations, that could be provided for example by Dino or by Fortran D. 126 7.1.2 Tools based on access patterns Chaos The Chaos primitives, which succeed
Reference: [Bal90] <author> V. Balasundaram. </author> <title> A mechanism for keeping useful internal information in parallel programming tools: The data access descriptor. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 9(2) </volume> <pages> 154-170, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The bulk of the work in this field has treated all variables as scalars, resulting in a very conservative analysis for array variables. More precise methods are based on representations of array subsets, such as data access descriptors <ref> [Bal90] </ref> or regular sections [HK91]. The W2 compiler [GS90] for the Warp multiprocessor gathers information such as the set of definitions reaching a basic block to exploit the fine-grain parallelism offered by the highly pipelined functional units.
Reference: [BBLS91] <author> D. Bailey, J. Barton, T. Lasinski, and H. Simon. </author> <title> The NAS parallel benchmarks. </title> <journal> International Journal of Supercomputing Applications, </journal> <volume> 5(3) </volume> <pages> 63-73, </pages> <month> Fall </month> <year> 1991. </year>
Reference-contexts: Speedups the index-based mapping does not result in any speedup due to the bad locality, the value-based distribution provides about 40% efficiency for 16 processors. 6.1.3 A sparse matrix computation The NAS CGM benchmark solves an unstructured sparse linear system by the conjugate gradient method <ref> [BBLS91] </ref>. The bulk of the computational time is spent in matvec, which multiplies a sparse matrix a with a vector x, as shown in Figure 6.7.
Reference: [BBO + 83] <author> B. R. Brooks, R. E. Bruccoleri, B. D. Olafson, D. J. States, S. Swaminathan, and M. Karplus. CHARMM: </author> <title> A program for macromolecular energy, mini 135 mization and dynamics calculations. </title> <journal> Journal of Computational Chemistry, </journal> <volume> 4(2) </volume> <pages> 187-217, </pages> <year> 1983. </year>
Reference-contexts: (j) are likely to be mapped to the same processor, no matter what relationship i and j may have to each other. 2.1.1 Molecular dynamics An example for value-based distributions Examples where data are not related by indices, but by values, are molecular dynamics programs such as Gromos [GB88], CHARMM <ref> [BBO + 83] </ref>, or ARGOS [SM90] that are used to simulate biomolecular systems. One important routine common to these programs is the non-bonded force (NBF) routine, which typically accounts for the bulk of the computational work (around 90%). Figure 2.1 shows an abstracted version of a sequential NBF calculation.
Reference: [BCZ92] <author> S. Benkner, B. Chapman, and H. Zima. </author> <title> Vienna Fortran 90. </title> <booktitle> In Scalable High Performance Computing Conference, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: HPF derives many of its underlying concepts from Fortran D, which is the base language chosen for the extensions proposed here, and other languages, such as Vienna Fortran <ref> [BCZ92] </ref>. The body of work that focuses on irregular applications is much smaller. <p> exploiting fine-grained parallelism have been and are being built, which include Al [Tse90], Aspar [IFKF90], C*/Dataparallel C [HQL + 91, RS87], Crystal [LC91], Dino [RSW91], Id Nouveau [RP89], Mimd-izer [SWW92], Oxygen [RA90], P 3 C [GAY91], Pandore [APT90], Parafrase-2 [GB92], Paragon [CCRS91], Spot [SS90, Soc90], Superb [ZBG88], and Vienna Fortran <ref> [BCZ92] </ref>.
Reference: [BHMS91] <author> M. Bromley, S. Heller, T. McNerney, and G. Steele, Jr. </author> <title> Fortran at ten gi-gaflops: The Connection Machine convolution compiler. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on Program Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: The virtual-machine model used by CM Fortran <ref> [BHMS91] </ref> can be seen as a typical example of the latter [Chr91], as explained in more detail in Chapter 4. A programmer should not have to make this tradeoff when choosing a compiler, especially in a performance-oriented field such as scientific parallel computing.
Reference: [Bia91] <author> E. S. Biagioni. </author> <title> Scan Directed Load Balancing. </title> <type> PhD thesis, </type> <institution> University of North Carolina at Chapel Hill, </institution> <year> 1991. </year>
Reference-contexts: If an application has good index-based locality, then an irregular distribution may be used to improve load balance, but some or all of the mapping computation can still be done on the fly <ref> [BK93, Bia91, CHMS94] </ref>. Given a value-based distribution (i.e., assuming no index-based locality), which for example distributes x according to its values, one could envision a scheme that also had very little state specifically devoted to representing the distribution. <p> The restricted control flow of pure SIMD programming has been addressed by several researchers. General simulators of MIMD semantics on SIMD machines have been implemented by Kuszmaul [Kus86] and Hudak et al. [HM88] on the Connection Machine and by Biagioni <ref> [Bia91] </ref> and Dietz et al. [DC92] on the MasPar. These simulations are generally based on graph reduction interpreters for functional languages. Their performance tends to be scalable, but in absolute measures still below the speed of sequential work stations. <p> Scan operations are one efficient way for determining the total workload and its distribution <ref> [Bia91, Ble90] </ref>.
Reference: [BK93] <author> S. B. Baden and S. R. Kohn. </author> <title> Portable parallel programming of numerical problems under the LPAR system. </title> <type> Technical Report CS93-330, </type> <institution> Department of Computer Science and Engineering, University of California, </institution> <address> San Diego, </address> <year> 1993. </year>
Reference-contexts: If an application has good index-based locality, then an irregular distribution may be used to improve load balance, but some or all of the mapping computation can still be done on the fly <ref> [BK93, Bia91, CHMS94] </ref>. Given a value-based distribution (i.e., assuming no index-based locality), which for example distributes x according to its values, one could envision a scheme that also had very little state specifically devoted to representing the distribution. <p> The limitation of this approach lies in its specificity towards applications that already use data structures reflecting locality characteristics; this we try to overcome by using general value-based decompositions as introduced in Chapter 2. Lattice Parallelism Lattice Parallelism (LPar) <ref> [Bad92, BK93, BKF94, FB95] </ref> is an SPMD programming model that supports coarse-grained parallelism based on the Fidil language [HC88] and the owner computes rule. It is intended for non-uniform computations that involve partial differential equations and have local structure.
Reference: [BKF94] <author> S. B. Baden, S. R. Kohn, and S. J. Fink. </author> <title> Programming with LPARX. In Intel Supercomputer User's Group Meeting, </title> <month> June </month> <year> 1994. </year> <note> Also available via anonymous ftp from cs.ucsd.edu as pub/baden/tr/cs94-377.ps. </note>
Reference-contexts: The limitation of this approach lies in its specificity towards applications that already use data structures reflecting locality characteristics; this we try to overcome by using general value-based decompositions as introduced in Chapter 2. Lattice Parallelism Lattice Parallelism (LPar) <ref> [Bad92, BK93, BKF94, FB95] </ref> is an SPMD programming model that supports coarse-grained parallelism based on the Fidil language [HC88] and the owner computes rule. It is intended for non-uniform computations that involve partial differential equations and have local structure.
Reference: [BKP93] <author> F. Bodin, L. Kervella, and T. Priol. Fortran-S: </author> <title> A Fortran interface for shared virtual memory architectures. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: It supports BLOCK, CYCLIC, and user-defined irregular decompositions. The goal of Arf is to demonstrate that inspector/executors based on Parti primitives can be automatically generated by the compiler. Fortran S Fortran S <ref> [BKP93] </ref> is a variation on Fortran 77 that contains directives for explicit parallelism.
Reference: [Ble90] <author> G. E. Blelloch. </author> <title> Vector Models for Data-Parallel Computing. </title> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Loop flattening can also be used to process multiple array segments of different lengths per processor, as introduced in Blelloch's V-RAM model <ref> [Ble90] </ref>. Thus it can be viewed as a generalization of substituting direct addressing with indirect addressing as Tomboulian and Pappas did for computing the Mandelbrot set [TP90]. 7.4.4 Fast scan operations The inhomogeneous workload across processors generally associated with irregular problems calls for load balancing. <p> Scan operations are one efficient way for determining the total workload and its distribution <ref> [Bia91, Ble90] </ref>.
Reference: [BP90] <author> K. P. Belkhale and P. Prithviraj. </author> <title> Recursive partitions on multiprocessors. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <pages> pages 930-938, </pages> <year> 1990. </year>
Reference-contexts: Note also that the user may not select a specific strategy for distributing data explicitly, as shown in Figure 2.3. In this case the compiler chooses a default strategy, such as recursive bisection <ref> [BP90] </ref> or spacefilling curves [PB94].
Reference: [BS90] <author> H. Berryman and J. Saltz. </author> <title> A manual for PARTI runtime primitives. </title> <type> ICASE Interim Report 13, </type> <institution> Institute for Computer Application in Science and Engineering, Hampton, VA, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: example by Dino or by Fortran D. 126 7.1.2 Tools based on access patterns Chaos The Chaos primitives, which succeed Parti (Parallel Automated Runtime Toolkit at ICASE), are a set of high level communication routines that provide convenient access to off-processor elements of arrays that are accessed (and distributed) irregularly <ref> [BS90, SBW90, DMS + 92, DHU + 93] </ref>. The authors of Parti were the first to propose and implement user-defined irregular distributions [MSS + 88] and a hashed cache for nonlocal values [MSMB90]. They build on the inspector-executor paradigm described above; they 1. Coordinate interprocessor data movement, 2.
Reference: [BSGM90] <author> H. Berryman, J. Saltz, W. Gropp, and R. Mirchandaney. </author> <title> Krylov methods preconditioned with incompletely factored matrices on the CM-2. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8 </volume> <pages> 186-190, </pages> <year> 1990. </year> <month> 136 </month>
Reference-contexts: This seems to be a situation potentially occurring in many scientific programs solving irregular problems <ref> [BSGM90, SPBR91, TP90, WLR90] </ref>. 6.2.1 The application One example of a loop nest with varying inner bounds is the NBF calculation as shown in Figure 2.1. However, this kernel takes advantage of Newton's Third Law and performs an indirect lhs assignment to f (j), which results in irregular communication patterns.
Reference: [BT88] <author> Henri E. Bal and Andrew S. Tanenbaum. </author> <title> Distributed programming with shared data. </title> <booktitle> In Proceedings of the IEEE CS 1988 International Conference on Computer Languages, </booktitle> <pages> pages 82-91, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: Examples of these systems are Amber [CAL + 89], Clouds [RAK88], Dash [LLG + 90], Ivy [LH89], Midway [BZ91], Munin [CBZ91, KCZ92], Orca <ref> [BT88] </ref>, and Platinum [CF89]. They preserve sequential semantics by enforcing a consistency protocol, which can be lazy or eager, based on invalidations or updates. Munin supports several such protocols, the choice between them for each individual shared variable is guided by access pattern annotations provided by the user.
Reference: [Bur90] <author> M. Burke. </author> <title> An interval-based approach to exhaustive and incremental inter-procedural data-flow analysis. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12(3) </volume> <pages> 341-395, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: It can be used for forward problems, such as available expressions [All70, Coc70], and backward problems, such as live variables) [Ken71], and it has also been used for code motion [DP93] and incremental analysis <ref> [Bur90] </ref>. We are using a variant of interval analysis that is based on Tarjan intervals [Tar74].
Reference: [BZ91] <author> Brian N. Bershad and Matthew J. Zekauskas. </author> <title> Shared memory parallel programming with entry consistency for distributed memory multiprocessors. </title> <type> Technical Report CMU-CS-91-170, </type> <institution> Carnegie-Mellon University, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: Examples of these systems are Amber [CAL + 89], Clouds [RAK88], Dash [LLG + 90], Ivy [LH89], Midway <ref> [BZ91] </ref>, Munin [CBZ91, KCZ92], Orca [BT88], and Platinum [CF89]. They preserve sequential semantics by enforcing a consistency protocol, which can be lazy or eager, based on invalidations or updates.
Reference: [CAL + 89] <author> J. Chase, F. Amador, E. Lazowska, H. Levy, and R. Littlefield. </author> <title> The Amber system: Parallel programming on a network of multiprocessors. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 147-158, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Each dimension of the decomposition is distributed in a BLOCK, CYCLIC, or BLOCK CYCLIC manner or replicated. 7.3 The Operating System Virtual or hardware supported single-address space systems can ease the task of parallel programming by eliminating separate address spaces and explicit communications. Examples of these systems are Amber <ref> [CAL + 89] </ref>, Clouds [RAK88], Dash [LLG + 90], Ivy [LH89], Midway [BZ91], Munin [CBZ91, KCZ92], Orca [BT88], and Platinum [CF89]. They preserve sequential semantics by enforcing a consistency protocol, which can be lazy or eager, based on invalidations or updates.
Reference: [CBZ91] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Examples of these systems are Amber [CAL + 89], Clouds [RAK88], Dash [LLG + 90], Ivy [LH89], Midway [BZ91], Munin <ref> [CBZ91, KCZ92] </ref>, Orca [BT88], and Platinum [CF89]. They preserve sequential semantics by enforcing a consistency protocol, which can be lazy or eager, based on invalidations or updates.
Reference: [CCRS91] <author> C. Chase, A. Cheung, A. Reeves, and M. Smith. </author> <title> Paragon: A parallel programming environment for scientific applications using communication structures. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Several compilation systems for exploiting fine-grained parallelism have been and are being built, which include Al [Tse90], Aspar [IFKF90], C*/Dataparallel C [HQL + 91, RS87], Crystal [LC91], Dino [RSW91], Id Nouveau [RP89], Mimd-izer [SWW92], Oxygen [RA90], P 3 C [GAY91], Pandore [APT90], Parafrase-2 [GB92], Paragon <ref> [CCRS91] </ref>, Spot [SS90, Soc90], Superb [ZBG88], and Vienna Fortran [BCZ92].
Reference: [CF89] <author> A. Cox and R. Fowler. </author> <title> The implementation of a coherent memory abstraction on a NUMA multiprocessor: Experiences with Platinum. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1989. </year>
Reference-contexts: Examples of these systems are Amber [CAL + 89], Clouds [RAK88], Dash [LLG + 90], Ivy [LH89], Midway [BZ91], Munin [CBZ91, KCZ92], Orca [BT88], and Platinum <ref> [CF89] </ref>. They preserve sequential semantics by enforcing a consistency protocol, which can be lazy or eager, based on invalidations or updates. Munin supports several such protocols, the choice between them for each individual shared variable is guided by access pattern annotations provided by the user.
Reference: [CG89] <author> N. Carriero and D. Gelernter. </author> <title> Linda in context. </title> <journal> Communications of the ACM, </journal> <volume> 32(4) </volume> <pages> 444-458, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Early work in the field of compiling for distributed-memory machines focussed on defining frameworks for nonlocal memory accesses [CK88] and data distributions [GB91, HA90, RS89]. For exploiting coarse-grained functional parallelism, high-level parallel languages such as Linda <ref> [CG89] </ref>, Strand [FT90, FO90], and Delirium [LS91] have been defined.
Reference: [CHK94] <author> T. W. Clark, R. v. Hanxleden, and K. Kennedy. </author> <title> Experiences on data-parallel programming. </title> <type> Technical Report CRPC-TR94495-S, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <note> Decem-ber 1994. Available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR94495-S. 137 </note>
Reference-contexts: There were a number of difficulties that are probably neither limited to Gromos nor do they seem likely to be addressed by improved compiler technology in the near future. The following lists a few of them, a more detailed discussion can be found elsewhere <ref> [CHK94] </ref>. * Multi-dimensional arrays that we would like to distribute only along a certain dimension are linearized. 110 Fortran D for the NAS CGM benchmark. Gromos in Fortran D.
Reference: [CHMS94] <author> T. W. Clark, R. v. Hanxleden, J. A. McCammon, and L. R. Scott. </author> <title> Parallelization using spatial decomposition for molecular dynamics. </title> <booktitle> In Scalable High Performance Computing Conference, </booktitle> <address> Knoxville, TN, </address> <month> May </month> <year> 1994. </year> <note> Available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR93356-S. </note>
Reference-contexts: However, one of the main advantages of distributed-memory machines, scalability to large problem and machine sizes, has to be compromised under this approach. Alternatively, one may distribute the data in a way that considers the actual data dependences specific to their application <ref> [CHMS94] </ref>. These "irregular" mappings are typically harder to debug and manage than regular mappings and present an additional level of complexity beyond general message-passing style programming. 6 1.2.4 Value-based mappings Value-based distributions were initially proposed as an enhancement to Fortran 77D [Han92]. <p> If an application has good index-based locality, then an irregular distribution may be used to improve load balance, but some or all of the mapping computation can still be done on the fly <ref> [BK93, Bia91, CHMS94] </ref>. Given a value-based distribution (i.e., assuming no index-based locality), which for example distributes x according to its values, one could envision a scheme that also had very little state specifically devoted to representing the distribution.
Reference: [Chr91] <author> P. Christy. </author> <title> Virtual processors considered harmful. </title> <booktitle> In Proceedings of the 6th Distributed Memory Computing Conference, </booktitle> <address> Portland, OR, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: The virtual-machine model used by CM Fortran [BHMS91] can be seen as a typical example of the latter <ref> [Chr91] </ref>, as explained in more detail in Chapter 4. A programmer should not have to make this tradeoff when choosing a compiler, especially in a performance-oriented field such as scientific parallel computing. <p> Therefore, using the L 1 u loops does not automatically result in savings by reducing the number of processed layers; however, we have to pay the additional overhead of checking on each layer whether it is active <ref> [Chr91] </ref>.
Reference: [CK88] <author> D. Callahan and K. Kennedy. </author> <title> Compiling programs for distributed-memory multiprocessors. </title> <journal> Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 151-169, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: If we do not use a strict owner-computes rule <ref> [CK88] </ref>, then non-owned data may not only be locally referenced, but also locally defined. <p> Early work in the field of compiling for distributed-memory machines focussed on defining frameworks for nonlocal memory accesses <ref> [CK88] </ref> and data distributions [GB91, HA90, RS89]. For exploiting coarse-grained functional parallelism, high-level parallel languages such as Linda [CG89], Strand [FT90, FO90], and Delirium [LS91] have been defined.
Reference: [CK92] <author> S. Carr and K. Kennedy. </author> <title> Scalar replacement in the presence of conditional control flow. </title> <type> Technical Report TR92283, </type> <institution> Rice University, CRPC, </institution> <month> November </month> <year> 1992. </year> <note> To appear in Software Practice& Experience. </note>
Reference-contexts: Therefore, combinations of dependence analysis and PRE have been used, for example for determining reaching definitions [GS90] or performing scalar replacement <ref> [CK92] </ref>. For example, Duesterwald et al. incorporate iteration distance vectors (assuming regular array references) into an array-reference data-flow framework, which is then applied to memory optimizations and controlled loop unrolling [DGS93]. Several researchers have addressed the communication generation problem, although often restricted to relatively simple array reference patterns.
Reference: [CLR90] <author> T.H. Cormen, C.E. Leiserson, and R.L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: However, we divide his forward edges into Flow and Entry edges depending on whether they enter an interval or not (others divide them into forward and tree edges depending on whether they are part of an embedded tree or not <ref> [CLR90] </ref>). Note also that for each Jump edge (m; n), G contains Level (m) Level (n) Synthetic edges, one from the header of each interval jumped out of. Give-N-Take requires G to have the following properties: * G is reducible; i.e., each loop has a unique header node.
Reference: [CM69] <author> J. Cocke and R. Miller. </author> <title> Some analysis techniques for optimizing computer programs. </title> <booktitle> In Proceedings of the 2nd Annual Hawaii International Conference on System Sciences, </booktitle> <pages> pages 143-146, </pages> <year> 1969. </year>
Reference-contexts: Give-N-Take requires G to have the following properties: * G is reducible; i.e., each loop has a unique header node. This can be achieved, for example, by node splitting <ref> [CM69] </ref>. * For each non-empty interval T (h), there exists a unique n 2 T (h) such that (n; h) 2 E; i.e., there is only one Cycle edge out of T (h).
Reference: [CM90] <author> T. W. Clark and J. A. McCammon. </author> <title> Parallelization of a molecular dynamics non-bonded force algorithm for MIMD architectures. </title> <journal> Computers & Chemistry, </journal> <volume> 14(3) </volume> <pages> 219-224, </pages> <year> 1990. </year>
Reference-contexts: Another alternative is to replicate the data and distribute just the computation itself and combine results at the end. This approach has the further advantage of simplicity and robustness, and for relatively small problem sizes and numbers of processors it may actually result in satisfactory performance <ref> [CM90] </ref>. However, one of the main advantages of distributed-memory machines, scalability to large problem and machine sizes, has to be compromised under this approach. Alternatively, one may distribute the data in a way that considers the actual data dependences specific to their application [CHMS94].
Reference: [CMZ92] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Programming in Vienna Fortran. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 31-50, </pages> <month> Fall </month> <year> 1992. </year>
Reference-contexts: memory machines by first inspecting each iteration of a loop on whether a processor owns the page of data associated with it, and then looping explicitly over these iterations. 1.2.3 Mapping arrays and mapping functions To specify irregular mappings in a data-parallel context, index-based mapping arrays [WSBH91] or mapping functions <ref> [CMZ92] </ref> have been proposed. However, such arrays or functions that explicitly map indices to processors have to be provided by the programmer, even though she or he may not be interested in what exactly these mappings look like.
Reference: [Coc70] <author> J. Cocke. </author> <title> Global common subexpression elimination. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 5(7) </volume> <pages> 20-24, </pages> <year> 1970. </year>
Reference-contexts: The W2 compiler [GS90] for the Warp multiprocessor gathers information such as the set of definitions reaching a basic block to exploit the fine-grain parallelism offered by the highly pipelined functional units. It is based on interval analysis <ref> [All70, Coc70] </ref> and computes information with array region granularity. Granston and Veidenbaum combine flow and dependence analysis to detect redundant global memory accesses in parallelized and vectorized codes [GV91]. They assume that the program is already annotated with Read/Write operations. <p> These data are indicated in the framework by GIVE (l) GIVE init (l). 3.3.4 The Interval-Flow Graph A general data-flow analysis algorithm that considers loop nesting hierarchies is interval analysis. It can be used for forward problems, such as available expressions <ref> [All70, Coc70] </ref>, and backward problems, such as live variables) [Ken71], and it has also been used for code motion [DP93] and incremental analysis [Bur90]. We are using a variant of interval analysis that is based on Tarjan intervals [Tar74].
Reference: [Dah90] <author> D. Dahl. </author> <title> Mapping and compiled communication on the connection machine system. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: The data-flow framework presented as part of this thesis in Chapter 3 is designed for attacking the second part of the problem, namely enabling the compiler to make good use of these primitives without further advice by the user. The Communication Compiler The Communication Compiler <ref> [Dah90] </ref> is a software facility for scheduling general communications on the Connection Machine. It employs simulated annealing to find 127 a data mapping with as low communication requirements as possible. It uses a recursive routing algorithm to determine an actual communication schedule.
Reference: [Das94] <author> R. Das. </author> <title> Compilation Techniques for Irregular Problems on Parallel Machines. </title> <type> PhD thesis, </type> <institution> The College of William and Mary in Virginia, </institution> <year> 1994. </year>
Reference-contexts: A scatter writes all off-processor data that have been defined in the loop back to their owners. The inspector-executor paradigm has been shown to be very effective under certain circumstances and recently has been extended to general patterns of control flow <ref> [DSvH93, Das94] </ref>. 1.2.2 Compilation systems for irregular problems Projects that have aimed at least to some degree towards compiler support for par-allelizing irregular problems are the following. Kali Kali [KMV90, MV90, KM91] is the first compiler system that supports both regular and irregular computations on MIMD distributed-memory machines. <p> Let INSP := f (n; Insp (n)) j Insp (n) 6= ;g; in nbf, it is INSP = f (3; fhSchedule for fjgig)g. Note that this strategy imposes some restrictions that might result in unnecessary re-inspections. For a more general approach, see Das <ref> [Das94] </ref>. 5.2.6 Executors Executors are slightly modified versions of regions in P that contain irregular references. Part of the modifications is to replace irregular subscripts by references to trace arrays (see Section 5.3.3). The use of trace arrays requires insertion of counters for indexing.
Reference: [DC92] <author> H. Dietz and W. Cohen. </author> <title> A control-parallel programming model implemented on SIMD hardware. </title> <booktitle> In Proceedings of the Fifth Workshop on Languages and 138 Compilers for Parallel Computing, </booktitle> <pages> pages 311-325, </pages> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: The restricted control flow of pure SIMD programming has been addressed by several researchers. General simulators of MIMD semantics on SIMD machines have been implemented by Kuszmaul [Kus86] and Hudak et al. [HM88] on the Connection Machine and by Biagioni [Bia91] and Dietz et al. <ref> [DC92] </ref> on the MasPar. These simulations are generally based on graph reduction interpreters for functional languages. Their performance tends to be scalable, but in absolute measures still below the speed of sequential work stations.
Reference: [DGS93] <author> E. Duesterwald, R. Gupta, and M. L. Soffa. </author> <title> A practical data flow framework for array reference analysis and its use in optimizations. </title> <journal> ACM SIG-PLAN Notices, </journal> <volume> 28(6) </volume> <pages> 68-77, </pages> <month> June </month> <year> 1993. </year> <booktitle> Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation. </booktitle>
Reference-contexts: For example, Duesterwald et al. incorporate iteration distance vectors (assuming regular array references) into an array-reference data-flow framework, which is then applied to memory optimizations and controlled loop unrolling <ref> [DGS93] </ref>. Several researchers have addressed the communication generation problem, although often restricted to relatively simple array reference patterns. Amarasinghe and Lam optimize communication generation using Last Write Trees [AL93]. They assume affine loop bounds and array indices, they do not allow loops within conditionals as shown in Figure 3.1.
Reference: [Dha88a] <author> D.M. Dhamdhere. </author> <title> A fast algorithm for code movement optimization. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 23(10) </volume> <pages> 172-180, </pages> <year> 1988. </year>
Reference-contexts: This can either be done at code generation time, or by post-processing the results of Give-N-Take, in a way that is similar to 56 the synthetic (dashed) edge between nodes 2 and 3. a mechanism employed in edge placement <ref> [Dha88a] </ref> for avoiding code proliferation. Our implementation took the latter route, by running a backward pass on G which checks whether these movements can be done without conflicts. 3.6 Summary This chapter has outlined a general code generation framework, based on Tarjan intervals, that handles several different classes of problems.
Reference: [Dha88b] <author> D.M. Dhamdhere. </author> <title> Register assignment using code placement techniques. </title> <journal> Computer Languages, </journal> <volume> 13(2) </volume> <pages> 75-93, </pages> <year> 1988. </year>
Reference-contexts: For example, when placing register loads and stores, certain loads may become redundant with previous definitions. This is generally treated as a special case, for example by developing different, but interdependent sets of equations for loads and stores <ref> [Dha88b] </ref>. Pessimistic loop handling: One difficulty with flow analysis has traditionally been the treatment of loop constructs that allow zero-trip instances, such as a Fortran do loop. Hoisting code out of such loops is generally considered unsafe, as it may introduce statements on paths where they have not existed before. <p> Classical PRE, for example, can be classified as a Lazy-Before problem. This means that the same framework can be used for different flavors of problems; there are no separate sets of equations for loads and stores <ref> [Dha88b] </ref>, or for Reads and Writes [GV91]. 3.2 A Code Placement Example Problem: Communication Generation An example of code placement is the generation of communication statements when compiling data-parallel languages, such as High Performance Fortran [KLS + 94] or Fortran D [HKT92a].
Reference: [Dha91] <author> D.M. Dhamdhere. </author> <title> Practical adaptation of the global optimization algorithm of Morel and Renvoise. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(2) </volume> <pages> 291-294, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Example applications include common subexpression elimination, loop invariant code motion, and strength reduction. The original PRE framework was developed by Morel and Renvoise [MR79] and has since then experienced various refinements <ref> [JD82, DS88, 25 Dha88a, Dha91, DRZ92, KRS92] </ref>. However, the PRE frameworks developed to date still have certain limitations, which become apparent when trying to apply them to more complex code placement tasks.
Reference: [DHU + 93] <author> R. Das, Y.-S. Hwang, M. Uysal, J. Saltz, and A. Sussman. </author> <title> Applying the CHAOS/PARTI library to irregular problems in computational chemistry and computational aerodynamics. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <institution> Mississippi State University, </institution> <address> Starkville, MS. </address> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1993. </year>
Reference-contexts: Therefore, not only storing but also accessing the information adds complexity to the program. As also described in Section 7.1.2, run-time libraries such as Chaos can take most of the complexity of this task from the programmer <ref> [DHU + 93] </ref>, but their use still requires explicit managing of the data structures associated with irregular distributions and communications. 2.2.2 Storing value-based distributed data Again assuming that x is distributed according to its value, the number of elements of x assigned to each processor typically varies and is not known <p> example by Dino or by Fortran D. 126 7.1.2 Tools based on access patterns Chaos The Chaos primitives, which succeed Parti (Parallel Automated Runtime Toolkit at ICASE), are a set of high level communication routines that provide convenient access to off-processor elements of arrays that are accessed (and distributed) irregularly <ref> [BS90, SBW90, DMS + 92, DHU + 93] </ref>. The authors of Parti were the first to propose and implement user-defined irregular distributions [MSS + 88] and a hashed cache for nonlocal values [MSMB90]. They build on the inspector-executor paradigm described above; they 1. Coordinate interprocessor data movement, 2.
Reference: [DK83] <author> D.M. Dhamdhere and J.S. Keith. </author> <title> Characterization of program loops in code optimization. </title> <journal> Computer Languages, </journal> <volume> 8 </volume> <pages> 69-76, </pages> <year> 1983. </year>
Reference-contexts: Several techniques exist to circumvent this difficulty, for example adding an extra guard and a preheader node to each loop [Sor89], explicitly introducing zero-trip paths <ref> [DK83] </ref>, or collapsing innermost loops [HKK + 92]. These strategies, however, result in some loss of information, and they do not fully apply to nested loops. Therefore, the GiveN-Take framework generally treats a loop as if it will be executed at least once.
Reference: [DK93] <author> D. M. Dhamdhere and U. P. Khedker. </author> <title> Complexity of bidirectional data flow analysis. </title> <booktitle> In Conference Record of the Twentieth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 397-408, </pages> <address> Charleston, South Carolina, </address> <month> January </month> <year> 1993. </year>
Reference: [DMS + 92] <author> R. Das, D. Mavriplis, J. Saltz, S. Gupta, and R. Ponnusamy. </author> <title> The design and implementation of a parallel unstructured Euler solver using software primitives, </title> <booktitle> AIAA-92-0562. In Proceedings of the 30th Aerospace Sciences Meeting. AIAA, </booktitle> <month> January </month> <year> 1992. </year>
Reference-contexts: example by Dino or by Fortran D. 126 7.1.2 Tools based on access patterns Chaos The Chaos primitives, which succeed Parti (Parallel Automated Runtime Toolkit at ICASE), are a set of high level communication routines that provide convenient access to off-processor elements of arrays that are accessed (and distributed) irregularly <ref> [BS90, SBW90, DMS + 92, DHU + 93] </ref>. The authors of Parti were the first to propose and implement user-defined irregular distributions [MSS + 88] and a hashed cache for nonlocal values [MSMB90]. They build on the inspector-executor paradigm described above; they 1. Coordinate interprocessor data movement, 2.
Reference: [DP93] <author> D.M. Dhamdhere and H. Patil. </author> <title> An elimination algorithm for bidirectional data flow problems using edge placement. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 15(2) </volume> <pages> 312-336, </pages> <month> April </month> <year> 1993. </year> <month> 139 </month>
Reference-contexts: It can be used for forward problems, such as available expressions [All70, Coc70], and backward problems, such as live variables) [Ken71], and it has also been used for code motion <ref> [DP93] </ref> and incremental analysis [Bur90]. We are using a variant of interval analysis that is based on Tarjan intervals [Tar74].
Reference: [DPSM91] <author> R. Das, R. Ponnusamy, J. Saltz, and D. Mavriplis. </author> <title> Distributed memory compiler methods for irregular problems | Data copy reuse and runtime partitioning. </title> <type> ICASE Report 91-73, </type> <institution> Institute for Computer Application in Science and Engineering, Hampton, VA, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: An optimization that is not implemented yet but at least conceptionally fairly straightforward is to use incremental schedules for pruning messages in case at least some of the data covered by a reference are already locally available <ref> [DPSM91, HKK + 92] </ref>.
Reference: [DRZ92] <author> D.M. Dhamdhere, B.K. Rosen, and F.K. Zadeck. </author> <title> How to analyze large programs efficiently and informatively. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Program Language Design and Implementation, </booktitle> <pages> pages 212-223, </pages> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Example applications include common subexpression elimination, loop invariant code motion, and strength reduction. The original PRE framework was developed by Morel and Renvoise [MR79] and has since then experienced various refinements <ref> [JD82, DS88, 25 Dha88a, Dha91, DRZ92, KRS92] </ref>. However, the PRE frameworks developed to date still have certain limitations, which become apparent when trying to apply them to more complex code placement tasks.
Reference: [DS88] <author> K. Drechsler and M. Stadel. </author> <title> A solution to a problem with Morel and Ren-voise's "Global optimization by suppression of partial redundancies". </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(4) </volume> <pages> 635-640, </pages> <month> Octo-ber </month> <year> 1988. </year>
Reference-contexts: Example applications include common subexpression elimination, loop invariant code motion, and strength reduction. The original PRE framework was developed by Morel and Renvoise [MR79] and has since then experienced various refinements <ref> [JD82, DS88, 25 Dha88a, Dha91, DRZ92, KRS92] </ref>. However, the PRE frameworks developed to date still have certain limitations, which become apparent when trying to apply them to more complex code placement tasks.
Reference: [DSvH93] <author> R. Das, J. Saltz, and R. v. Hanxleden. </author> <title> Slicing analysis and indirect accesses to distributed arrays. </title> <editor> In U. Banerjee et al., editor, </editor> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> volume 769, </volume> <pages> pages 152-168. </pages> <publisher> Springer, </publisher> <address> Berlin, </address> <month> August </month> <year> 1993. </year> <booktitle> From the Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR. </address> <note> Available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR93319-S. </note>
Reference-contexts: A scatter writes all off-processor data that have been defined in the loop back to their owners. The inspector-executor paradigm has been shown to be very effective under certain circumstances and recently has been extended to general patterns of control flow <ref> [DSvH93, Das94] </ref>. 1.2.2 Compilation systems for irregular problems Projects that have aimed at least to some degree towards compiler support for par-allelizing irregular problems are the following. Kali Kali [KMV90, MV90, KM91] is the first compiler system that supports both regular and irregular computations on MIMD distributed-memory machines. <p> Here the subscript dependence set refers to the data that a subscript depends on. This can be thought of as the set of data that are referenced when inspecting for a subscript; a more detailed discussion can be found elsewhere <ref> [DSvH93] </ref>. For example, a subscript that is itself an indirection-array lookup depends on the indirection array. <p> In the example, we do not need to communicate f since forces have not been computed yet. 5.3.3 Trace arrays The current strategy for collecting off-processor references is to record each individual subscript in a trace array <ref> [DSvH93] </ref>. These arrays contain traces of the subscripts encountered during inspection and will be localized from global to local name space. In our example, a trace array j$glob is created for subscript j (lines 41-50 in the loop nest in lines 30-37 of the original, sequential program in Figure 2.3. <p> However, the traces may still become too space consuming, in which case more space saving alternatives, such as a hash table combined with name-space translations on the fly, may be used <ref> [DSvH93] </ref>. 5.3.4 Inspectors The inspector code has to perform the following tasks: 1. If the size of a subscript trace is not known at compile time, then a counting slice has to be generated and the trace array has to be allocated.
Reference: [ELZ86] <author> D. Eager, E. D. Lazowska, and J. Zahorjan. </author> <title> A comparison of receiver-initiated and sender-initiated adaptive load sharing. Performance Evaluation, </title> <booktitle> 6 </booktitle> <pages> 53-68, </pages> <year> 1986. </year>
Reference-contexts: Here information about the utilization of different processors can be helpful. However, the work done in this area has focussed on thread-based parallelism <ref> [ELZ86, Luc88] </ref>, typically even associated with distinct processes, instead of data parallelism.
Reference: [FB95] <author> S. J. Fink and S. B. Baden. </author> <title> Run-time data distribution for block-structured applications on distributed memory computers. </title> <booktitle> In Seventh SIAM Conf. on Parallel Proc. for Scientific Computing, </booktitle> <month> February </month> <year> 1995. </year> <note> Also available via anonymous ftp from cs.ucsd.edu as pub/baden/tr/cs94-386.ps. </note>
Reference-contexts: The limitation of this approach lies in its specificity towards applications that already use data structures reflecting locality characteristics; this we try to overcome by using general value-based decompositions as introduced in Chapter 2. Lattice Parallelism Lattice Parallelism (LPar) <ref> [Bad92, BK93, BKF94, FB95] </ref> is an SPMD programming model that supports coarse-grained parallelism based on the Fidil language [HC88] and the owner computes rule. It is intended for non-uniform computations that involve partial differential equations and have local structure.
Reference: [FHK + 90] <author> G. C. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report TR90-141, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year> <note> Revised April, </note> <year> 1991. </year>
Reference-contexts: Introduction Data-parallel languages, such as High Performance Fortran (HPF) [KLS + 94] and Fortran D <ref> [FHK + 90] </ref>, allow for a "machine independent parallel programming style," in which the applications programmer uses a dialect of a sequential language and annotates it with high-level data distribution information. From this annotated program, data-parallel compilers will generate codes in different native Fortran dialects for different parallel architectures. <p> We will give program examples in different variants of pseudo-Fortran: F77 Strictly sequential Fortran 77 (possibly a "dusty deck" program). F77D - F77 enhanced with decomposition statements as proposed in Fortran D <ref> [FHK + 90] </ref>. An important goal of F77D is to provide a basis for efficient compilation towards both MIMD and SIMD distributed-memory machines, so it does not contain any constructs that are specific to either architecture. <p> The following contains a very brief summary of its basic concepts, the complete language is described in detail elsewhere <ref> [FHK + 90] </ref>. Citing Hiranandani et al. [HKT92b]: Fortran D is the first language to provide users with explicit control over data partitioning with both data alignment and distribution specifications. The DECOMPOSITION statement specifies an abstract problem or 128 index domain.
Reference: [FO90] <author> I. Foster and R. Overbeek. </author> <title> Bilingual parallel programming. </title> <booktitle> In Advances in Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, August 1990. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: Early work in the field of compiling for distributed-memory machines focussed on defining frameworks for nonlocal memory accesses [CK88] and data distributions [GB91, HA90, RS89]. For exploiting coarse-grained functional parallelism, high-level parallel languages such as Linda [CG89], Strand <ref> [FT90, FO90] </ref>, and Delirium [LS91] have been defined.
Reference: [FT90] <author> I. Foster and S. Taylor. Strand: </author> <title> New Concepts in Parallel Programming. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1990. </year> <month> 140 </month>
Reference-contexts: Early work in the field of compiling for distributed-memory machines focussed on defining frameworks for nonlocal memory accesses [CK88] and data distributions [GB91, HA90, RS89]. For exploiting coarse-grained functional parallelism, high-level parallel languages such as Linda [CG89], Strand <ref> [FT90, FO90] </ref>, and Delirium [LS91] have been defined.
Reference: [GAY91] <author> E. Gabber, A. Averbuch, and A. Yehudai. </author> <title> Experience with a portable paral-lelizing Pascal compiler. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Several compilation systems for exploiting fine-grained parallelism have been and are being built, which include Al [Tse90], Aspar [IFKF90], C*/Dataparallel C [HQL + 91, RS87], Crystal [LC91], Dino [RSW91], Id Nouveau [RP89], Mimd-izer [SWW92], Oxygen [RA90], P 3 C <ref> [GAY91] </ref>, Pandore [APT90], Parafrase-2 [GB92], Paragon [CCRS91], Spot [SS90, Soc90], Superb [ZBG88], and Vienna Fortran [BCZ92].
Reference: [GB88] <author> W. F. van Gunsteren and H. J. C. Berendsen. GROMOS: </author> <title> GROningen MOlecular Simulation software. </title> <type> Technical report, </type> <institution> Laboratory of Physical Chemistry, University of Groningen, </institution> <address> Nijenborgh, The Netherlands, </address> <year> 1988. </year>
Reference-contexts: and x (j) are likely to be mapped to the same processor, no matter what relationship i and j may have to each other. 2.1.1 Molecular dynamics An example for value-based distributions Examples where data are not related by indices, but by values, are molecular dynamics programs such as Gromos <ref> [GB88] </ref>, CHARMM [BBO + 83], or ARGOS [SM90] that are used to simulate biomolecular systems. One important routine common to these programs is the non-bonded force (NBF) routine, which typically accounts for the bulk of the computational work (around 90%).
Reference: [GB91] <author> M. Gupta and P. Banerjee. </author> <title> Automatic data partitioning on distributed memory multiprocessors. </title> <booktitle> In Proceedings of the 6th Distributed Memory Computing Conference, </booktitle> <address> Portland, OR, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Early work in the field of compiling for distributed-memory machines focussed on defining frameworks for nonlocal memory accesses [CK88] and data distributions <ref> [GB91, HA90, RS89] </ref>. For exploiting coarse-grained functional parallelism, high-level parallel languages such as Linda [CG89], Strand [FT90, FO90], and Delirium [LS91] have been defined.
Reference: [GB92] <author> M. Gupta and P. Banerjee. </author> <title> Compile-time estimation of communication costs on multicomputers. </title> <booktitle> In Proceedings of the 6th International Parallel Processing Symposium, </booktitle> <address> Beverly Hills, CA, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: Several compilation systems for exploiting fine-grained parallelism have been and are being built, which include Al [Tse90], Aspar [IFKF90], C*/Dataparallel C [HQL + 91, RS87], Crystal [LC91], Dino [RSW91], Id Nouveau [RP89], Mimd-izer [SWW92], Oxygen [RA90], P 3 C [GAY91], Pandore [APT90], Parafrase-2 <ref> [GB92] </ref>, Paragon [CCRS91], Spot [SS90, Soc90], Superb [ZBG88], and Vienna Fortran [BCZ92].
Reference: [Ger90] <author> M. Gerndt. </author> <title> Updating distributed variables in local computations. </title> <journal> Concur-rency: Practice and Experience, </journal> <volume> 2(3) </volume> <pages> 171-193, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Parallelism is expressed by mapping a logical processing Domain onto a spatial processing Domain. LPar supports load balancing and ghost regions, in which each processor stores data within a certain proximity to its own data, similarly to overlap regions <ref> [Ger90] </ref>. It is currently implemented in C++ for the iPSC/860. LPar treats parallelism at a very high level, it manipulates the structure of the data, rather than the data itself.
Reference: [GS90] <author> T. Gross and P. Steenkiste. </author> <title> Structured dataflow analysis for arrays and its use in an optimizing compiler. </title> <journal> Software|Practice and Experience, </journal> <volume> 20(2) </volume> <pages> 133-155, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: The bulk of the work in this field has treated all variables as scalars, resulting in a very conservative analysis for array variables. More precise methods are based on representations of array subsets, such as data access descriptors [Bal90] or regular sections [HK91]. The W2 compiler <ref> [GS90] </ref> for the Warp multiprocessor gathers information such as the set of definitions reaching a basic block to exploit the fine-grain parallelism offered by the highly pipelined functional units. It is based on interval analysis [All70, Coc70] and computes information with array region granularity. <p> Therefore, combinations of dependence analysis and PRE have been used, for example for determining reaching definitions <ref> [GS90] </ref> or performing scalar replacement [CK92]. For example, Duesterwald et al. incorporate iteration distance vectors (assuming regular array references) into an array-reference data-flow framework, which is then applied to memory optimizations and controlled loop unrolling [DGS93].
Reference: [GS93] <author> M. Gupta and E. Schonberg. </author> <title> A framework for exploiting data availability to optimize communication. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: We assume that these data have to be written back to their owners before they can be used by other processors, as shown in Figure 3.4. (An alternative would be the direct exchange between a non-owner that writes data and another non-owner that reads them <ref> [Gup92, GS93] </ref>. <p> They assume affine loop bounds and array indices, they do not allow loops within conditionals as shown in Figure 3.1. Gupta and Schonberg use Available Section Descriptors, computed by interval based data-flow analysis, to determine the availability of data on a virtual processor grid <ref> [GS93] </ref>. They apply (regular) mapping functions to map this information to individual processors and list redundant communication elimination and communication generation as possible applications. <p> An option not considered here is that p might receive d from any processor that has a valid copy of d <ref> [GS93] </ref>. Since the owner-computes rule is not strictly applied, any processor p may define d. If p 6= owner (d) and d will be referenced by some processor q, q 6= p, then p must send d to owner (d) after defining d.
Reference: [Gup92] <author> M. Gupta. </author> <title> Automatic Data Partitioning on Distributed Memory Multicom-puters. </title> <type> PhD thesis, </type> <institution> College of Engineering, University of Illinois at Urbana-Champaign, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: We assume that these data have to be written back to their owners before they can be used by other processors, as shown in Figure 3.4. (An alternative would be the direct exchange between a non-owner that writes data and another non-owner that reads them <ref> [Gup92, GS93] </ref>.
Reference: [GV91] <author> E. Granston and A. Veidenbaum. </author> <title> Detecting redundant accesses to array data. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: It is based on interval analysis [All70, Coc70] and computes information with array region granularity. Granston and Veidenbaum combine flow and dependence analysis to detect redundant global memory accesses in parallelized and vectorized codes <ref> [GV91] </ref>. They assume that the program is already annotated with Read/Write operations. Their technique tries to eliminate these operations where possible, also across loop nests and in the presence of conditionals. <p> Classical PRE, for example, can be classified as a Lazy-Before problem. This means that the same framework can be used for different flavors of problems; there are no separate sets of equations for loads and stores [Dha88b], or for Reads and Writes <ref> [GV91] </ref>. 3.2 A Code Placement Example Problem: Communication Generation An example of code placement is the generation of communication statements when compiling data-parallel languages, such as High Performance Fortran [KLS + 94] or Fortran D [HKT92a]. <p> N + 5)g do j = 1; N enddo else Read Send fx (6 : N + 5)g Read Recv fx (6 : N + 5)g endif do k = 1; N enddo non-owned data (left), and a corresponding placement of global Writes (right). 33 in parallelized and vectorized codes <ref> [GV91] </ref>. Their technique tries to eliminate these operations where possible, also across loop nests and in the presence of conditionals, and they eliminate reads of non-owned variables if these variables have already been read or written locally.
Reference: [GW76] <author> S. Graham and M. Wegman. </author> <title> A fast and usually linear algorithm for global data flow analysis. </title> <journal> Journal of the ACM, </journal> <volume> 23(1) </volume> <pages> 172-202, </pages> <month> January </month> <year> 1976. </year>
Reference-contexts: Therefore, GiveNTake has to evaluate each equation only once for each node, which implies guaranteed termination and low computational complexity; it also implies fastness <ref> [GW76] </ref>. However, since the direction of the flow of information varies across the equations, we still need multiple passes over the control flow graph, solving a different set of equations during each pass.
Reference: [HA90] <author> D. Hudak and S. Abraham. </author> <title> Compiler techniques for data partitioning of sequentially iterated parallel loops. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year> <month> 141 </month>
Reference-contexts: Early work in the field of compiling for distributed-memory machines focussed on defining frameworks for nonlocal memory accesses [CK88] and data distributions <ref> [GB91, HA90, RS89] </ref>. For exploiting coarse-grained functional parallelism, high-level parallel languages such as Linda [CG89], Strand [FT90, FO90], and Delirium [LS91] have been defined.
Reference: [Han89] <author> R. v. Hanxleden. </author> <title> Parallelizing dynamic processes. </title> <type> Master's thesis, </type> <institution> Dept. ofhpcnComputer Science, The Pennsylvania State University, </institution> <month> August </month> <year> 1989. </year>
Reference-contexts: In fact, the strategy specified by the user might be passed through verbatim to the run-time library. However, one might still require a certain minimal set of strategies to be always available <ref> [Han89, PSC93a] </ref>. Note also that the user may not select a specific strategy for distributing data explicitly, as shown in Figure 2.3. In this case the compiler chooses a default strategy, such as recursive bisection [BP90] or spacefilling curves [PB94].
Reference: [Han92] <author> R. v. Hanxleden. </author> <title> Compiler support for machine independent parallelization of irregular problems. </title> <type> Technical Report CRPC-TR92301-S, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> November </month> <year> 1992. </year> <type> Ph.D. Thesis Proposal. </type>
Reference-contexts: These "irregular" mappings are typically harder to debug and manage than regular mappings and present an additional level of complexity beyond general message-passing style programming. 6 1.2.4 Value-based mappings Value-based distributions were initially proposed as an enhancement to Fortran 77D <ref> [Han92] </ref>. A variant of it, based on a GeoCoL (Geometrical, Connectivity and/or Load) data structure, has since then been implemented in a Fortran 90D prototype compiler by Ponnusamy et al. [PSC93a, PSC + 93b].
Reference: [Han93] <author> R. v. Hanxleden. </author> <title> Handling irregular problems with Fortran D | A preliminary report. </title> <booktitle> In Proceedings of the Fourth Workshop on Compilers for Parallel Computers, </booktitle> <pages> pages 353-364, </pages> <address> Delft, The Netherlands, </address> <month> December </month> <year> 1993. </year> <note> D Newsletter #9, available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR93339-S. </note>
Reference-contexts: Note that the same problem occurs when regularly distributed arrays are accessed irregularly and we wish to append buffer space for off-processor data at the end of the array <ref> [Han93] </ref>. 2.2.3 Translating name spaces Translating between the global name space of a Fortran D program and the local name space of the node program is an important component of the parallelization process.
Reference: [Hav93] <author> P. Havlak. </author> <title> Construction of thinned gate single-assignment form. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Value numbers are computed for both constant and non-constant expressions and are closely related to the SSA information <ref> [Hav93, Hav94] </ref>. They provide for example information about whether an expression is an immediate or auxiliary induction variable or a linear combination thereof.
Reference: [Hav94] <author> P. Havlak. </author> <title> Interprocedural Symbolic Analysis. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> May </month> <year> 1994. </year> <note> Available as Technical Report CRPC-TR94451-S. </note>
Reference-contexts: Value numbers are computed for both constant and non-constant expressions and are closely related to the SSA information <ref> [Hav93, Hav94] </ref>. They provide for example information about whether an expression is an immediate or auxiliary induction variable or a linear combination thereof. <p> For example, it assumes that all actual arguments may be used and defined. However, the underlying symbolic analysis does already provide some of the information that would be necessary for interprocedural placement <ref> [Hav94] </ref>. 5.4 An Object-Oriented Design An important aspect of the implementation is its object-oriented design, reflected in the encapsulation of most concepts and algorithms into separate classes.
Reference: [HC88] <author> P. N. Hilfinger and P. Colella. FIDIL: </author> <title> A language for scientific programming. </title> <type> Technical Report UCRL-98057, </type> <institution> Lawrence Livermore National Laboratory, </institution> <month> January </month> <year> 1988. </year>
Reference-contexts: Lattice Parallelism Lattice Parallelism (LPar) [Bad92, BK93, BKF94, FB95] is an SPMD programming model that supports coarse-grained parallelism based on the Fidil language <ref> [HC88] </ref> and the owner computes rule. It is intended for non-uniform computations that involve partial differential equations and have local structure. It explicitly excludes unstructured calculations such as sparse matrix linear algebra and finite element problems.
Reference: [HHKT92] <author> M. W. Hall, S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Interprocedural compilation of Fortran D for MIMD distributed-memory machines. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <address> Minneapolis, MN, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: Section 7.2.2 briefly describes the main concepts of the Fortran D language. A prototype Fortran D compiler targeting distributed-memory architectures has had considerable success with regular problems <ref> [HHKT92, HKK + 91, HKT91, HKT92b, Tse93] </ref>. <p> An important class of interprocedural information provided by the regular compiler is reaching decomposition analysis, which propagates Fortran D specific decomposition information from callers to callees <ref> [HHKT92] </ref>. 5.2.3 The data-flow universe for communication analysis In preparation for analyzing the communication requirements of P , the irregular compiler first determines IREFS, the set of both regular and irregular references to arrays that are accessed irregularly somewhere, and then computes KEYS, the data-flow universe.
Reference: [Hig93] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification, version 1.0. </title> <type> Technical Report CRPC-TR92225, </type> <institution> Center for Research on Parallel Computation, Rice University, Houston, TX, </institution> <year> 1992 </year> <month> (revised Jan. </month> <year> 1993). </year> <note> To appear in Scientific Programming, </note> <month> July </month> <year> 1993. </year>
Reference-contexts: For enhancing readability of F90 SIMD examples, we extend the language constructs that are typically implemented by vendors in several ways: * The forall construct can be applied not only to single statements, but also to blocks, as is the case in HPF <ref> [Hig93] </ref>.
Reference: [HK91] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: The bulk of the work in this field has treated all variables as scalars, resulting in a very conservative analysis for array variables. More precise methods are based on representations of array subsets, such as data access descriptors [Bal90] or regular sections <ref> [HK91] </ref>. The W2 compiler [GS90] for the Warp multiprocessor gathers information such as the set of definitions reaching a basic block to exploit the fine-grain parallelism offered by the highly pipelined functional units. It is based on interval analysis [All70, Coc70] and computes information with array region granularity.
Reference: [HK92] <author> R. v. Hanxleden and K. Kennedy. </author> <title> Relaxing SIMD control flow constraints using loop transformations. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Program Language Design and Implementation, </booktitle> <pages> pages 188-199, </pages> <address> 142 San Francisco, CA, </address> <month> June </month> <year> 1992. </year> <note> ACM Press. Available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR92207-S. </note>
Reference-contexts: This can either be emulated using stacks of MASK bits, or it can be implemented directly in an MSIMD machine which contains multiple program counters. In either case, their proposal is mainly concerned with enabling the concurrent execution of both branches in if-then-else constructs. 130 Loop flattening <ref> [HK92] </ref> is one technique to overcome this limitation for loop nests with varying loop bounds, as proposed in Chapter 4. Loop flattening can also be used to process multiple array segments of different lengths per processor, as introduced in Blelloch's V-RAM model [Ble90]. <p> Loop flattening was designed to ease some particular SIMD restrictions without introducing any overhead; however, it supports a programming style that seems to be preferable on current SIMD machines even when running regular applications <ref> [HK92] </ref>. This rather surprising result suggests that flattened loops make it easier for compilers to derive the information they need for performing certain optimizations, such as pruning out virtual processor layers for individual statements whenever possible.
Reference: [HK93] <author> R. v. Hanxleden and K. Kennedy. </author> <title> A code placement framework and its application to communication generation. </title> <type> Technical Report CRPC-TR93337-S, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> October </month> <year> 1993. </year> <note> Available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR93337-S. </note>
Reference-contexts: In our communication problem, this includes an array portion p if either the contents of this portion get partly modified at n, or if p itself gets changed, for example if p is an indirect array reference and n modifies the indirection array <ref> [HK93] </ref>. 46 STEAL (n) = STEAL init (n) [ STEAL loc (LastChild (n)) (3.1) GIVE (n) = GIVE init (n) [ GIVE loc (LastChild (n)) (3.2) BLOCK (n) = STEAL (n) [ GIVE (n) [ [ s2Succs E (n) BLOCK loc (s) (3.3) TAKEN out (n) = " s2Succs FJS (n)
Reference: [HKK + 91] <author> S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, and C. Tseng. </author> <title> An overview of the Fortran D programming system. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Section 7.2.2 briefly describes the main concepts of the Fortran D language. A prototype Fortran D compiler targeting distributed-memory architectures has had considerable success with regular problems <ref> [HHKT92, HKK + 91, HKT91, HKT92b, Tse93] </ref>.
Reference: [HKK + 92] <author> R. v. Hanxleden, K. Kennedy, C. Koelbel, R. Das, and J. Saltz. </author> <title> Compiler analysis for irregular problems in Fortran D. </title> <editor> In U. Banerjee et al., editor, </editor> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> volume 757, </volume> <pages> pages 97-111. </pages> <publisher> Springer, </publisher> <address> Berlin, </address> <month> August </month> <year> 1992. </year> <booktitle> From the Proceedings of the Fifth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT. </address> <note> Available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR92287-S. </note>
Reference-contexts: The communication statements are in a high level format that does not include any processor ids, schedule parameters, message tags, and so on. Communication schedule generation, which is a non-trivial problem in itself <ref> [HKK + 92] </ref>, and the conversion from global to local name space are also excluded. <p> Several techniques exist to circumvent this difficulty, for example adding an extra guard and a preheader node to each loop [Sor89], explicitly introducing zero-trip paths [DK83], or collapsing innermost loops <ref> [HKK + 92] </ref>. These strategies, however, result in some loss of information, and they do not fully apply to nested loops. Therefore, the GiveN-Take framework generally treats a loop as if it will be executed at least once. <p> Here DEF (n) ffi and DEF (n) " are the data that are either "touched" or "contained" (i.e., partially or fully enclosed) by references in DEF (n) <ref> [HKK + 92] </ref>. For example, since i and j cannot be proven to be disjoint subscript ranges in nbf, it is fx (i)g ffi = fx (i); x (j)g. <p> An optimization that is not implemented yet but at least conceptionally fairly straightforward is to use incremental schedules for pruning messages in case at least some of the data covered by a reference are already locally available <ref> [DPSM91, HKK + 92] </ref>.
Reference: [HKT91] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: Section 7.2.2 briefly describes the main concepts of the Fortran D language. A prototype Fortran D compiler targeting distributed-memory architectures has had considerable success with regular problems <ref> [HHKT92, HKK + 91, HKT91, HKT92b, Tse93] </ref>.
Reference: [HKT92a] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler support for machine-independent parallel programming in Fortran D. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Languages, Compilers, and Run-Time Environments for Distributed Memory Machines. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1992. </year>
Reference-contexts: no separate sets of equations for loads and stores [Dha88b], or for Reads and Writes [GV91]. 3.2 A Code Placement Example Problem: Communication Generation An example of code placement is the generation of communication statements when compiling data-parallel languages, such as High Performance Fortran [KLS + 94] or Fortran D <ref> [HKT92a] </ref>. For example, a processor of a distributed-memory machine may reference owned data, which by default reside on the processor, as well as non-owned data, which reside on other processors. <p> A sufficient condition is that the loop into which we lift an inner loop body can be parallelized, which might be hard to detect, especially if indirect addressing occurs. However, this is already a necessary condition for parallelizing loops in general, and therewith a standard problem for parallelizing compilers <ref> [HKT92a] </ref>. The same technology developed there can be applied here.
Reference: [HKT92b] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Evaluation of compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Section 7.2.2 briefly describes the main concepts of the Fortran D language. A prototype Fortran D compiler targeting distributed-memory architectures has had considerable success with regular problems <ref> [HHKT92, HKK + 91, HKT91, HKT92b, Tse93] </ref>. <p> Figure 3.1 shows another, simple example node code containing references to distributed data. Since generating an individual message for each datum to be exchanged would be prohibitively expensive on most architectures, optimizations such as message vec-torization, latency hiding, and avoiding redundant communication are crucial for achieving acceptable performance <ref> [HKT92b] </ref>. <p> The following contains a very brief summary of its basic concepts, the complete language is described in detail elsewhere [FHK + 90]. Citing Hiranandani et al. <ref> [HKT92b] </ref>: Fortran D is the first language to provide users with explicit control over data partitioning with both data alignment and distribution specifications. The DECOMPOSITION statement specifies an abstract problem or 128 index domain.
Reference: [HM88] <author> P. Hudak and E. Mohr. </author> <title> Graphinators and the duality of SIMD and MIMD. </title> <booktitle> In Proceedings of the 1988 ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pages 224-234, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: This may cause additional idling when running irregular problems on SIMD machines instead of MIMD machines. The restricted control flow of pure SIMD programming has been addressed by several researchers. General simulators of MIMD semantics on SIMD machines have been implemented by Kuszmaul [Kus86] and Hudak et al. <ref> [HM88] </ref> on the Connection Machine and by Biagioni [Bia91] and Dietz et al. [DC92] on the MasPar. These simulations are generally based on graph reduction interpreters for functional languages. Their performance tends to be scalable, but in absolute measures still below the speed of sequential work stations.
Reference: [HPE94] <author> M. Hahad, T. Priol, and J. Erhel. </author> <title> Irregular loop patterns compilation on distributed shared memory multiprocessors. </title> <note> Publication Interne 862, </note> <institution> 143 IRISA, Rennes, France, </institution> <month> September </month> <year> 1994. </year> <note> Available via anonymous ftp from ftp.irisa.fr as techreports/1994/PI-862.ps.Z. </note>
Reference-contexts: It supports BLOCK, CYCLIC, and user-defined irregular decompositions. The goal of Arf is to demonstrate that inspector/executors based on Parti primitives can be automatically generated by the compiler. Fortran S Fortran S [BKP93] is a variation on Fortran 77 that contains directives for explicit parallelism. Conditioned Iterations Loops <ref> [HPE94] </ref> amortize the cost of irregular data accesses on distributed shared memory machines by first inspecting each iteration of a loop on whether a processor owns the page of data associated with it, and then looping explicitly over these iterations. 1.2.3 Mapping arrays and mapping functions To specify irregular mappings in
Reference: [HQL + 91] <author> P. Hatcher, M. Quinn, A. Lapadula, B. Seevers, R. Anderson, and R. Jones. </author> <title> Data-parallel programming on MIMD computers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 377-383, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: For exploiting coarse-grained functional parallelism, high-level parallel languages such as Linda [CG89], Strand [FT90, FO90], and Delirium [LS91] have been defined. Several compilation systems for exploiting fine-grained parallelism have been and are being built, which include Al [Tse90], Aspar [IFKF90], C*/Dataparallel C <ref> [HQL + 91, RS87] </ref>, Crystal [LC91], Dino [RSW91], Id Nouveau [RP89], Mimd-izer [SWW92], Oxygen [RA90], P 3 C [GAY91], Pandore [APT90], Parafrase-2 [GB92], Paragon [CCRS91], Spot [SS90, Soc90], Superb [ZBG88], and Vienna Fortran [BCZ92].
Reference: [HS91] <author> R. v. Hanxleden and L. R. Scott. </author> <title> Parallelizing dynamic processes on message passing architectures. </title> <editor> In J. Dongorra et al., editor, </editor> <booktitle> Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 451-455, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: its contributions in perspective by examining related work also at higher and lower levels and in the domain of regular applications. 7.1 Tools Tools can assist in load balancing and in communication, both of which can be particularly tedious and error prone when trying to parallelize an irregular problem efficiently <ref> [HS91] </ref>. The tools for parallelizing irregular problems can roughly be divided into two groups. The first group of tools provides an easier grip on the physical properties of the problem; i.e., it takes advantage of spatial locality.
Reference: [HtEBBW] <author> Little Red Riding Hood and the Eight Big Bad Wolves. </author> <title> Evaluating the performance of the GSA system under BB race conditions. </title> <note> Valhalla Press. In preparation. </note>
Reference-contexts: However, the work done in this area has focussed on thread-based parallelism [ELZ86, Luc88], typically even associated with distinct processes, instead of data parallelism. One also has to keep in mind that irregularities in the presence of race conditions might lead to system crashes <ref> [HtEBBW] </ref>. 129 7.4 The Hardware Some hardware facilities that can be particularly useful for irregular applications are the following. 7.4.1 Low latency Due to the typically very irregular access patterns, message blocking becomes more complicated than for regular applications [SHG92].
Reference: [IFKF90] <author> K. Ikudome, G. Fox, A. Kolawa, and J. Flower. </author> <title> An automatic and symbolic parallelization system for distributed memory parallel computers. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: For exploiting coarse-grained functional parallelism, high-level parallel languages such as Linda [CG89], Strand [FT90, FO90], and Delirium [LS91] have been defined. Several compilation systems for exploiting fine-grained parallelism have been and are being built, which include Al [Tse90], Aspar <ref> [IFKF90] </ref>, C*/Dataparallel C [HQL + 91, RS87], Crystal [LC91], Dino [RSW91], Id Nouveau [RP89], Mimd-izer [SWW92], Oxygen [RA90], P 3 C [GAY91], Pandore [APT90], Parafrase-2 [GB92], Paragon [CCRS91], Spot [SS90, Soc90], Superb [ZBG88], and Vienna Fortran [BCZ92].
Reference: [JD82] <author> S.M. Joshi and D.M. Dhamdhere. </author> <title> A composite hoisting-strength reduction transformation for global program optimization, parts I & II. </title> <journal> International Journal of Computer Mathematics, </journal> <volume> 11 </volume> <pages> 21-41, 111-126, </pages> <year> 1982. </year>
Reference-contexts: Example applications include common subexpression elimination, loop invariant code motion, and strength reduction. The original PRE framework was developed by Morel and Renvoise [MR79] and has since then experienced various refinements <ref> [JD82, DS88, 25 Dha88a, Dha91, DRZ92, KRS92] </ref>. However, the PRE frameworks developed to date still have certain limitations, which become apparent when trying to apply them to more complex code placement tasks.
Reference: [KCZ92] <author> P. Keleher, A. Cox, and W. Zwaenepoel. </author> <title> Lazy consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Examples of these systems are Amber [CAL + 89], Clouds [RAK88], Dash [LLG + 90], Ivy [LH89], Midway [BZ91], Munin <ref> [CBZ91, KCZ92] </ref>, Orca [BT88], and Platinum [CF89]. They preserve sequential semantics by enforcing a consistency protocol, which can be lazy or eager, based on invalidations or updates.
Reference: [Ken71] <author> K. Kennedy. </author> <title> A global flow analysis algorithm. </title> <journal> International Journal of Computer Mathematics, </journal> <volume> 3 </volume> <pages> 5-15, </pages> <year> 1971. </year>
Reference-contexts: It can be used for forward problems, such as available expressions [All70, Coc70], and backward problems, such as live variables) <ref> [Ken71] </ref>, and it has also been used for code motion [DP93] and incremental analysis [Bur90]. We are using a variant of interval analysis that is based on Tarjan intervals [Tar74].
Reference: [KLS90] <author> K. Knobe, J. Lukas, and G. Steele, Jr. </author> <title> Data optimization: Allocation of arrays to reduce communication on SIMD machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(2) </volume> <pages> 102-118, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: Furthermore, this potential waste of processing time can not only occur for small arrays, but it is encountered whenever array sizes are not exact multiples of Gran <ref> [KLS90] </ref>. On the CM-2, using the slicewise compiler results in Gran = P fl 4=32 = P=8 (32 processors per FPA, vector length 4); i.e., we can economically use arrays whose total sizes are arbitrary multiples of P=8.
Reference: [KLS + 94] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: Introduction Data-parallel languages, such as High Performance Fortran (HPF) <ref> [KLS + 94] </ref> and Fortran D [FHK + 90], allow for a "machine independent parallel programming style," in which the applications programmer uses a dialect of a sequential language and annotates it with high-level data distribution information. <p> These extensions could also be applied directly to the HPF standard <ref> [KLS + 94] </ref>. We were able to limit ourselves to a simple extension of the already existing DISTRIBUTE and ALIGN directives, as was also illustrated by the code in Figure 2.3. <p> different flavors of problems; there are no separate sets of equations for loads and stores [Dha88b], or for Reads and Writes [GV91]. 3.2 A Code Placement Example Problem: Communication Generation An example of code placement is the generation of communication statements when compiling data-parallel languages, such as High Performance Fortran <ref> [KLS + 94] </ref> or Fortran D [HKT92a]. For example, a processor of a distributed-memory machine may reference owned data, which by default reside on the processor, as well as non-owned data, which reside on other processors.
Reference: [KM91] <author> C. Koelbel and P. Mehrotra. </author> <title> Programming data parallel algorithms on distributed memory machines using Kali. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year> <month> 144 </month>
Reference-contexts: Kali Kali <ref> [KMV90, MV90, KM91] </ref> is the first compiler system that supports both regular and irregular computations on MIMD distributed-memory machines. Programs written for Kali must specify a virtual processor array and assign distributed arrays to BLOCK, CYCLIC, or user-specified decompositions.
Reference: [KMCKC93] <author> U. Kremer, J. Mellor-Crummey, K. Kennedy, and A. Carle. </author> <title> Automatic data layout for distributed-memory machines in the D programming environment. </title> <editor> In Christoph W. Kessler, editor, </editor> <title> Automatic Parallelization | New Approaches to Code Generation, Data Distribution, </title> <booktitle> and Performance Prediction, </booktitle> <pages> pages 136-152. </pages> <booktitle> Vieweg Advanced Studies in Computer Science, </booktitle> <publisher> Verlag Vieweg, Wiesbaden, </publisher> <address> Germany, </address> <year> 1993. </year> <note> Also available as technical report CRPC-TR93-298-S, </note> <institution> Rice University. </institution>
Reference-contexts: We will refer to this characteristic as index-based locality. Exactly how arrays should be mapped in the presence of index-based locality is by no means trivial and still an active field of research <ref> [KMCKC93] </ref>. However, one can generally assume that only regular mappings (such as BLOCK, CYCLIC, or BLOCK CYCLIC) and remappings need to be considered. For irregular problems, this assumption cannot be made.
Reference: [KMSB90] <author> C. Koelbel, P. Mehrotra, J. Saltz, and S. Berryman. </author> <title> Parallel loops on distributed machines. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Developing annotations for conveying information to the compiler at a high level is a part of this dissertation. 1.2.1 The inspector-executor paradigm An important concept associated with communication optimization for applications using irregular array subscripts is the inspector-executor paradigm <ref> [MSS + 88, KMV90, KMSB90, WSBH91] </ref>. A loop that contains indirect accesses to a distributed array is processed in four steps: 4 1. The inspector runs through the loop and only records which array elements are accessed, without doing the actual computation. <p> Communication is then generated automatically based on the ON clause and data decompositions. An inspector/executor strategy as described in Section 1.2.1 is used for run-time preprocessing of communication for irregularly distributed arrays <ref> [KMSB90] </ref>. Major differences between Kali and Fortran D include Kali's mandatory ON clauses for parallel loops and Fortran D's support for alignment, collective communication, and dynamic decomposition. 5 Arf is another compiler based on the inspector-executor paradigm.
Reference: [KMT91] <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Analysis and transformation in the ParaScope Editor. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: This approach is similar to the power steering paradigm <ref> [KMT91] </ref> used for loop transformations, where the compiler cannot always pick the best transformation, but it assists the user by (conservatively) testing correctness and performing the actual rewriting work. <p> abstracted version of the Fortran D output in Figures 5.1 and 5.2, which we will be mostly referring to. 5.2 The Analysis Phase 5.2.1 Symbolic analysis Even though the Fortran D compiler can be used in batch mode as a stand-alone tool, it is part of the ParaScope programming environment <ref> [KMT91] </ref>. Therefore the compiler not only can be used within ParaScope, but it also takes advantage of the information and utilities provided by this environment.
Reference: [KMV90] <author> C. Koelbel, P. Mehrotra, and J. Van Rosendale. </author> <title> Supporting shared data structures on distributed memory machines. </title> <booktitle> In Proceedings of the Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Seattle, WA, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: Developing annotations for conveying information to the compiler at a high level is a part of this dissertation. 1.2.1 The inspector-executor paradigm An important concept associated with communication optimization for applications using irregular array subscripts is the inspector-executor paradigm <ref> [MSS + 88, KMV90, KMSB90, WSBH91] </ref>. A loop that contains indirect accesses to a distributed array is processed in four steps: 4 1. The inspector runs through the loop and only records which array elements are accessed, without doing the actual computation. <p> Kali Kali <ref> [KMV90, MV90, KM91] </ref> is the first compiler system that supports both regular and irregular computations on MIMD distributed-memory machines. Programs written for Kali must specify a virtual processor array and assign distributed arrays to BLOCK, CYCLIC, or user-specified decompositions.
Reference: [KRS92] <author> J. Knoop, O. Ruthing, and B. Steffen. </author> <title> Lazy code motion. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Program Language Design and Implementation, </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Example applications include common subexpression elimination, loop invariant code motion, and strength reduction. The original PRE framework was developed by Morel and Renvoise [MR79] and has since then experienced various refinements <ref> [JD82, DS88, 25 Dha88a, Dha91, DRZ92, KRS92] </ref>. However, the PRE frameworks developed to date still have certain limitations, which become apparent when trying to apply them to more complex code placement tasks. <p> We will refer to node n as LastChild (h). * There are no critical edges, which connect a node with multiple outgoing edges to a node with multiple incoming edges. This can be achieved, for example, by inserting synthetic nodes <ref> [KRS92] </ref>.
Reference: [Kus86] <author> B. C. Kuszmaul. </author> <title> Simulating applicative architectures on the Connection Machine. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1986. </year>
Reference-contexts: This may cause additional idling when running irregular problems on SIMD machines instead of MIMD machines. The restricted control flow of pure SIMD programming has been addressed by several researchers. General simulators of MIMD semantics on SIMD machines have been implemented by Kuszmaul <ref> [Kus86] </ref> and Hudak et al. [HM88] on the Connection Machine and by Biagioni [Bia91] and Dietz et al. [DC92] on the MasPar. These simulations are generally based on graph reduction interpreters for functional languages.
Reference: [LC91] <author> J. Li and M. Chen. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 361-376, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: For exploiting coarse-grained functional parallelism, high-level parallel languages such as Linda [CG89], Strand [FT90, FO90], and Delirium [LS91] have been defined. Several compilation systems for exploiting fine-grained parallelism have been and are being built, which include Al [Tse90], Aspar [IFKF90], C*/Dataparallel C [HQL + 91, RS87], Crystal <ref> [LC91] </ref>, Dino [RSW91], Id Nouveau [RP89], Mimd-izer [SWW92], Oxygen [RA90], P 3 C [GAY91], Pandore [APT90], Parafrase-2 [GB92], Paragon [CCRS91], Spot [SS90, Soc90], Superb [ZBG88], and Vienna Fortran [BCZ92].
Reference: [LH89] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> IEEE Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Examples of these systems are Amber [CAL + 89], Clouds [RAK88], Dash [LLG + 90], Ivy <ref> [LH89] </ref>, Midway [BZ91], Munin [CBZ91, KCZ92], Orca [BT88], and Platinum [CF89]. They preserve sequential semantics by enforcing a consistency protocol, which can be lazy or eager, based on invalidations or updates.
Reference: [LLG + 90] <author> D. Lenoski, J. Laudon, K Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990. </year> <month> 145 </month>
Reference-contexts: Examples of these systems are Amber [CAL + 89], Clouds [RAK88], Dash <ref> [LLG + 90] </ref>, Ivy [LH89], Midway [BZ91], Munin [CBZ91, KCZ92], Orca [BT88], and Platinum [CF89]. They preserve sequential semantics by enforcing a consistency protocol, which can be lazy or eager, based on invalidations or updates.
Reference: [LS91] <author> S. Lucco and O. Sharp. </author> <title> Parallel programming with coordination structures. </title> <booktitle> In Conference Record of the Eighteenth ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Orlando, FL, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: Early work in the field of compiling for distributed-memory machines focussed on defining frameworks for nonlocal memory accesses [CK88] and data distributions [GB91, HA90, RS89]. For exploiting coarse-grained functional parallelism, high-level parallel languages such as Linda [CG89], Strand [FT90, FO90], and Delirium <ref> [LS91] </ref> have been defined.
Reference: [Luc88] <author> B. J. Lucier. </author> <title> Performance evaluation for multiprocessors programmed using monitors. </title> <booktitle> In Proceedings of the 1988 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, volume 16 of SIGMET-RICS Performance Evaluation Review, </booktitle> <year> 1988. </year>
Reference-contexts: Here information about the utilization of different processors can be helpful. However, the work done in this area has focussed on thread-based parallelism <ref> [ELZ86, Luc88] </ref>, typically even associated with distinct processes, instead of data parallelism.
Reference: [Mas91] <institution> MasPar Computer Corporation, Sunnyvale, CA. MasPar Fortran Reference Manual, </institution> <year> 1991. </year>
Reference-contexts: F77 MIMD Aversion of Fortran 77 designed to run on a MIMD machine, which assumes a separate name space for each processor. F90 SIMD Aversion of Fortran 90 designed to run on a SIMD machine, similar to Connection Machine Fortran [Thi91] or MasPar Fortran <ref> [Mas91] </ref>.
Reference: [Mav91] <author> D. Mavriplis. </author> <title> Three dimensional unstructured multigrid for the euler equations. </title> <type> Technical Report 91-41, </type> <institution> Institute for Computer Application in Science and Engineering, Hampton, VA, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: results in assigning each processor very irregular subdomains, with accordingly poor locality and load balance. 2.1.2 Unstructured meshes An example for value-based alignments An example of a value-based alignment is shown in Figure 2.6, which shows a Fortran D version of a sweep over the edges of an unstructured mesh <ref> [Mav91] </ref>. There are two decompositions, nodeD for the node data and edgeD for the edge data.
Reference: [MR79] <author> E. Morel and C. </author> <title> Renvoise. Global optimization by suppression of partial redundancies. </title> <journal> Communications of the ACM, </journal> <volume> 22(2) </volume> <pages> 96-103, </pages> <month> February </month> <year> 1979. </year>
Reference-contexts: Example applications include common subexpression elimination, loop invariant code motion, and strength reduction. The original PRE framework was developed by Morel and Renvoise <ref> [MR79] </ref> and has since then experienced various refinements [JD82, DS88, 25 Dha88a, Dha91, DRZ92, KRS92]. However, the PRE frameworks developed to date still have certain limitations, which become apparent when trying to apply them to more complex code placement tasks.
Reference: [MR90] <author> T. Marlowe and B. Ryder. </author> <title> Properties of data flow frameworks. </title> <journal> Acta Infor-matica, </journal> <volume> 28 </volume> <pages> 121-163, </pages> <year> 1990. </year>
Reference-contexts: Furthermore, whatever has been produced can be consumed arbitrarily often, until it gets destroyed. Data-flow frameworks are commonly characterized by a pair hL; F i, where L is a meet semilattice and F is a class of functions; see Marlowe and Ryder <ref> [MR90] </ref> for a discussion of these and other general aspects of data-flow frameworks. Roughly speaking, L characterizes the solution space (or universe) of the framework, such as the set of common subexpressions or available constants, and their interrelationships. <p> However, it has been noted by several researchers that for typical programs, both the average out-degree of flow graph nodes and the maximal loop nesting depth can be assumed to be bounded by small constant independent of the size of the program <ref> [MR90] </ref>. Therefore, the increase of G should be fairly small for well structured programs. for example, corresponds to the interval T (2) formed by nodes 3, 4, 5, with header 2; again, remember that the header itself is not part of the interval.
Reference: [MSMB90] <author> S. Mirchandaney, J. Saltz, P. Mehrotra, and H. Berryman. </author> <title> A scheme for supporting automatic data migration on multicomputers. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: The authors of Parti were the first to propose and implement user-defined irregular distributions [MSS + 88] and a hashed cache for nonlocal values <ref> [MSMB90] </ref>. They build on the inspector-executor paradigm described above; they 1. Coordinate interprocessor data movement, 2. Manage the storage of and access to copies of off-processor data, and 3.
Reference: [MSS + 88] <author> R. Mirchandaney, J. Saltz, R. Smith, D. Nicol, and K. Crowley. </author> <title> Principles of runtime support for parallel processors. </title> <booktitle> In Proceedings of the Second International Conference on Supercomputing, </booktitle> <pages> pages 140-152, </pages> <address> St. Malo, France, July 1988. </address> <publisher> ACM Press. </publisher>
Reference-contexts: Developing annotations for conveying information to the compiler at a high level is a part of this dissertation. 1.2.1 The inspector-executor paradigm An important concept associated with communication optimization for applications using irregular array subscripts is the inspector-executor paradigm <ref> [MSS + 88, KMV90, KMSB90, WSBH91] </ref>. A loop that contains indirect accesses to a distributed array is processed in four steps: 4 1. The inspector runs through the loop and only records which array elements are accessed, without doing the actual computation. <p> The authors of Parti were the first to propose and implement user-defined irregular distributions <ref> [MSS + 88] </ref> and a hashed cache for nonlocal values [MSMB90]. They build on the inspector-executor paradigm described above; they 1. Coordinate interprocessor data movement, 2. Manage the storage of and access to copies of off-processor data, and 3.
Reference: [MV90] <author> P. Mehrotra and J. Van Rosendale. </author> <title> Programming distributed memory architectures using Kali. </title> <booktitle> In Advances in Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, August 1990. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: Kali Kali <ref> [KMV90, MV90, KM91] </ref> is the first compiler system that supports both regular and irregular computations on MIMD distributed-memory machines. Programs written for Kali must specify a virtual processor array and assign distributed arrays to BLOCK, CYCLIC, or user-specified decompositions.
Reference: [PB94] <author> J. R. Pilkington and S. B. Baden. </author> <title> Partitioning with spacefilling curves. </title> <type> Technical Report CS94-349, </type> <institution> Department of Computer Science and Engineering, University of California, </institution> <address> San Diego, </address> <month> March </month> <year> 1994. </year>
Reference-contexts: Note also that the user may not select a specific strategy for distributing data explicitly, as shown in Figure 2.3. In this case the compiler chooses a default strategy, such as recursive bisection [BP90] or spacefilling curves <ref> [PB94] </ref>.
Reference: [PSC93a] <author> R. Ponnusamy, J. Saltz, and A. Choudhary. </author> <title> Runtime compilation techniques for data partitioning and communication schedule reuse. </title> <booktitle> In Supercomputing 146 '93, </booktitle> <pages> pages 361-370. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year> <note> Technical Report CS-TR-3055 and UMIACS-TR-93-32, University of Maryland, April `93. Available via anonymous ftp from hyena.cs.umd.edu. </note>
Reference-contexts: A variant of it, based on a GeoCoL (Geometrical, Connectivity and/or Load) data structure, has since then been implemented in a Fortran 90D prototype compiler by Ponnusamy et al. <ref> [PSC93a, PSC + 93b] </ref>. However, the GeoCoL structure still has to be managed explicitly by the programmer. 1.2.5 Communication analysis Determining communication requirements and satisfying them efficiently is critical for any parallel program running on a distributed-memory machine. <p> (Section 1.2.1) allows us to message-vectorize low-locality data accesses, even in the absence of compile-time knowledge. 2.2.5 A bootstrapping problem One characteristic of value-based mappings is that they may pose a certain bootstrapping problem to both the user and the compiler, as has also been identified by Ponnussamy et al. <ref> [PSC93a] </ref>. This problem occurs when an array is distributed based on its own values, which is considered perfectly legal, as is the case in the code shown in Figure 2.3. <p> In fact, the strategy specified by the user might be passed through verbatim to the run-time library. However, one might still require a certain minimal set of strategies to be always available <ref> [Han89, PSC93a] </ref>. Note also that the user may not select a specific strategy for distributing data explicitly, as shown in Figure 2.3. In this case the compiler chooses a default strategy, such as recursive bisection [BP90] or spacefilling curves [PB94].
Reference: [PSC + 93b] <author> R. Ponnusamy, J. Saltz, A. Choudhary, Y.-S. Hwang, and G. Fox. </author> <title> Runtime support and compilation methods for user-specified data distributions. </title> <institution> Technical Report CS-TR-3194 and UMIACS-TR-93-135, University of Maryland, </institution> <month> November </month> <year> 1993. </year> <note> Available via anonymous ftp from hyena.cs.umd.edu. </note>
Reference-contexts: A variant of it, based on a GeoCoL (Geometrical, Connectivity and/or Load) data structure, has since then been implemented in a Fortran 90D prototype compiler by Ponnusamy et al. <ref> [PSC93a, PSC + 93b] </ref>. However, the GeoCoL structure still has to be managed explicitly by the programmer. 1.2.5 Communication analysis Determining communication requirements and satisfying them efficiently is critical for any parallel program running on a distributed-memory machine.
Reference: [PT91] <author> M. Philippsen and W. F. Tichy. </author> <title> Modula-2 fl and its compilation. </title> <booktitle> In First International Conference of the Austrian Center for Parallel Computation, </booktitle> <address> Salzburg, Austria, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: These simulations are generally based on graph reduction interpreters for functional languages. Their performance tends to be scalable, but in absolute measures still below the speed of sequential work stations. Philippsen et al. introduce two variants of a forall statement, a synchronous version and an asynchronous one <ref> [PT91] </ref>. The asynchronous forall enables multiple threads of control to coexist. This can either be emulated using stacks of MASK bits, or it can be implemented directly in an MSIMD machine which contains multiple program counters.
Reference: [RA90] <author> R. Ruhl and M. Annaratone. </author> <title> Parallelization of fortran code on distributed-memory parallel processors. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Several compilation systems for exploiting fine-grained parallelism have been and are being built, which include Al [Tse90], Aspar [IFKF90], C*/Dataparallel C [HQL + 91, RS87], Crystal [LC91], Dino [RSW91], Id Nouveau [RP89], Mimd-izer [SWW92], Oxygen <ref> [RA90] </ref>, P 3 C [GAY91], Pandore [APT90], Parafrase-2 [GB92], Paragon [CCRS91], Spot [SS90, Soc90], Superb [ZBG88], and Vienna Fortran [BCZ92].
Reference: [RAK88] <author> U. Ramachandran, M. Ahamad, and Y. Khalidi. </author> <title> Unifying synchronization and data transfer in maintaining coherence of distributed shared memory. </title> <type> Technical Report GIT-CS-88/23, </type> <institution> Georgia Institute of Technology, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: Examples of these systems are Amber [CAL + 89], Clouds <ref> [RAK88] </ref>, Dash [LLG + 90], Ivy [LH89], Midway [BZ91], Munin [CBZ91, KCZ92], Orca [BT88], and Platinum [CF89]. They preserve sequential semantics by enforcing a consistency protocol, which can be lazy or eager, based on invalidations or updates.
Reference: [RP86] <author> B.G. Ryder and M.C. Paull. </author> <title> Elimination algorithms for data flow analysis. </title> <journal> ACM Computing Surveys, </journal> <volume> 18 </volume> <pages> 77-316, </pages> <year> 1986. </year>
Reference-contexts: Allen-Cocke intervals include in addition all nodes whose predecessors are all in T (h); i.e., they might include an acyclic structure dangling off the loop. In that sense, Tarjan intervals reflect the loop structure more closely than Allen-Cocke intervals <ref> [RP86] </ref>. Note that a node nested in multiple loops is a member of the Tarjan interval of the header of each enclosing loop. Unlike in classical interval analysis, we do not explicitly construct a sequence of graphs in which intervals are recursively collapsed into single nodes.
Reference: [RP89] <author> A. Rogers and K. Pingali. </author> <title> Process decomposition through locality of reference. </title> <booktitle> In Proceedings of the ACM SIGPLAN '89 Conference on Program Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Several compilation systems for exploiting fine-grained parallelism have been and are being built, which include Al [Tse90], Aspar [IFKF90], C*/Dataparallel C [HQL + 91, RS87], Crystal [LC91], Dino [RSW91], Id Nouveau <ref> [RP89] </ref>, Mimd-izer [SWW92], Oxygen [RA90], P 3 C [GAY91], Pandore [APT90], Parafrase-2 [GB92], Paragon [CCRS91], Spot [SS90, Soc90], Superb [ZBG88], and Vienna Fortran [BCZ92].
Reference: [RS87] <author> J. Rose and G. Steele, Jr. </author> <title> C fl : An extended C language for data parallel programming. </title> <editor> In L. Kartashev and S. Kartashev, editors, </editor> <booktitle> Proceedings of the Second International Conference on Supercomputing, </booktitle> <address> Santa Clara, CA, </address> <month> May </month> <year> 1987. </year>
Reference-contexts: For exploiting coarse-grained functional parallelism, high-level parallel languages such as Linda [CG89], Strand [FT90, FO90], and Delirium [LS91] have been defined. Several compilation systems for exploiting fine-grained parallelism have been and are being built, which include Al [Tse90], Aspar [IFKF90], C*/Dataparallel C <ref> [HQL + 91, RS87] </ref>, Crystal [LC91], Dino [RSW91], Id Nouveau [RP89], Mimd-izer [SWW92], Oxygen [RA90], P 3 C [GAY91], Pandore [APT90], Parafrase-2 [GB92], Paragon [CCRS91], Spot [SS90, Soc90], Superb [ZBG88], and Vienna Fortran [BCZ92].
Reference: [RS89] <author> J. Ramanujam and P. Sadayappan. </author> <title> A methodology for parallelizing programs for multicomputers and complex memory multiprocessors. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <address> Reno, NV, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: Early work in the field of compiling for distributed-memory machines focussed on defining frameworks for nonlocal memory accesses [CK88] and data distributions <ref> [GB91, HA90, RS89] </ref>. For exploiting coarse-grained functional parallelism, high-level parallel languages such as Linda [CG89], Strand [FT90, FO90], and Delirium [LS91] have been defined.
Reference: [RSW91] <author> M. Rosing, R. Schnabel, and R. Weaver. </author> <title> The DINO parallel programming language. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(1) </volume> <pages> 30-42, </pages> <month> Septem-ber </month> <year> 1991. </year> <month> 147 </month>
Reference-contexts: For exploiting coarse-grained functional parallelism, high-level parallel languages such as Linda [CG89], Strand [FT90, FO90], and Delirium [LS91] have been defined. Several compilation systems for exploiting fine-grained parallelism have been and are being built, which include Al [Tse90], Aspar [IFKF90], C*/Dataparallel C [HQL + 91, RS87], Crystal [LC91], Dino <ref> [RSW91] </ref>, Id Nouveau [RP89], Mimd-izer [SWW92], Oxygen [RA90], P 3 C [GAY91], Pandore [APT90], Parafrase-2 [GB92], Paragon [CCRS91], Spot [SS90, Soc90], Superb [ZBG88], and Vienna Fortran [BCZ92].
Reference: [SBW90] <author> J. Saltz, H. Berryman, and J. Wu. </author> <title> Multiprocessors and runtime compilation. </title> <type> ICASE Report 90-59, </type> <institution> Institute for Computer Application in Science and Engineering, Hampton, VA, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: example by Dino or by Fortran D. 126 7.1.2 Tools based on access patterns Chaos The Chaos primitives, which succeed Parti (Parallel Automated Runtime Toolkit at ICASE), are a set of high level communication routines that provide convenient access to off-processor elements of arrays that are accessed (and distributed) irregularly <ref> [BS90, SBW90, DMS + 92, DHU + 93] </ref>. The authors of Parti were the first to propose and implement user-defined irregular distributions [MSS + 88] and a hashed cache for nonlocal values [MSMB90]. They build on the inspector-executor paradigm described above; they 1. Coordinate interprocessor data movement, 2.
Reference: [SCMB90] <author> J. Saltz, K. Crowley, R. Mirchandaney, and H. Berryman. </author> <title> Run-time scheduling and execution of loops on message passing machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(2) </volume> <pages> 303-312, </pages> <year> 1990. </year>
Reference-contexts: They build on the inspector-executor paradigm described above; they 1. Coordinate interprocessor data movement, 2. Manage the storage of and access to copies of off-processor data, and 3. Support a shared name space, using a distributed translation table <ref> [SCMB90] </ref> to store the local address and processor number for each distributed array element. High-level library routines, such as Chaos, can assist in tasks such as global-to-local name space mappings, communication schedule generation, and schedule based communication.
Reference: [SH91] <author> J. P. Singh and J. L. Hennessy. </author> <title> An empirical investigation of the effectiveness and limitations of automatic parallelization. </title> <booktitle> In Proceedings of the International Symposium on Shared Memory Multiprocessing, </booktitle> <address> Tokyo, Japan, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Citing from a study about parallelizing different applications (including a molecular dynamics simulation) using the FX/Fortran parallelizing compiler <ref> [SH91] </ref>: It is worth noting that the available directives were sometimes found to be restrictive or incapable of expressing the exact information we wished to convey to the compiler.
Reference: [SHG92] <author> J. P. Singh, J. L. Hennessy, and A. Gupta. </author> <title> Implications of hierarchical N-body methods for multiprocessor architecture. </title> <type> Technical Report CSL-TR-92-506, </type> <institution> Stanford University, </institution> <year> 1992. </year>
Reference-contexts: in the presence of race conditions might lead to system crashes [HtEBBW]. 129 7.4 The Hardware Some hardware facilities that can be particularly useful for irregular applications are the following. 7.4.1 Low latency Due to the typically very irregular access patterns, message blocking becomes more complicated than for regular applications <ref> [SHG92] </ref>.
Reference: [SLY90] <author> Z. Shen, Z. Li, and P. Yew. </author> <title> An empirical study of Fortran programs for par-allelizing compilers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(3) </volume> <pages> 356-364, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: Simple arrays, accessed directly, are typical for regular problems, whereas irregular applications may employ arrays with indirection vectors, pointers, linked lists, or quad trees, for example <ref> [SLY90] </ref>. We consider a problem to be irregular if its data access patterns are hard to analyze at compile time; i.e., there is no obvious, simple parallelization and data distribution that gives good speedups and makes efficient use of processing power and the memory hierarchy.
Reference: [SM90] <author> T. P. Straatsma and J. Andrew McCammon. ARGOS, </author> <title> a vectorized general molecular dynamics program. </title> <journal> Journal of Computational Chemistry, </journal> <volume> II(8):943-951, </volume> <year> 1990. </year>
Reference-contexts: mapped to the same processor, no matter what relationship i and j may have to each other. 2.1.1 Molecular dynamics An example for value-based distributions Examples where data are not related by indices, but by values, are molecular dynamics programs such as Gromos [GB88], CHARMM [BBO + 83], or ARGOS <ref> [SM90] </ref> that are used to simulate biomolecular systems. One important routine common to these programs is the non-bonded force (NBF) routine, which typically accounts for the bulk of the computational work (around 90%). Figure 2.1 shows an abstracted version of a sequential NBF calculation.
Reference: [SM91] <author> J. Shen and J. A. McCammon. </author> <title> Molecular dynamics simulation of Superoxide interacting with Superoxide Dismutase. </title> <journal> Chemical Physics, </journal> <volume> 158 </volume> <pages> 191-198, </pages> <year> 1991. </year>
Reference-contexts: SOD is a catalytic enzyme composed of two identical subunits, each with 151 amino-acid residues and two metal atoms <ref> [SM91] </ref>. and pCnt ave , which indicate the computational workloads for different cutoff radii. As expected, both values increase cubicly with the cutoff radius.
Reference: [Soc90] <author> D. Socha. </author> <title> Compiling single-point iterative programs for distributed memory computers. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Several compilation systems for exploiting fine-grained parallelism have been and are being built, which include Al [Tse90], Aspar [IFKF90], C*/Dataparallel C [HQL + 91, RS87], Crystal [LC91], Dino [RSW91], Id Nouveau [RP89], Mimd-izer [SWW92], Oxygen [RA90], P 3 C [GAY91], Pandore [APT90], Parafrase-2 [GB92], Paragon [CCRS91], Spot <ref> [SS90, Soc90] </ref>, Superb [ZBG88], and Vienna Fortran [BCZ92].
Reference: [Sor89] <author> A. Sorkin. </author> <title> Some comments on "A solution to a problem with Morel and Ren-voise's `Global optimization by suppression of partial redundancies' ". ACM Transactions on Programming Languages and Systems, </title> <booktitle> 11(4) </booktitle> <pages> 666-668, </pages> <month> Octo-ber </month> <year> 1989. </year>
Reference-contexts: Several techniques exist to circumvent this difficulty, for example adding an extra guard and a preheader node to each loop <ref> [Sor89] </ref>, explicitly introducing zero-trip paths [DK83], or collapsing innermost loops [HKK + 92]. These strategies, however, result in some loss of information, and they do not fully apply to nested loops. Therefore, the GiveN-Take framework generally treats a loop as if it will be executed at least once.
Reference: [SPBR91] <author> J. Saltz, S. Petiton, H. Berryman, and A. Rifkin. </author> <title> Performance effects of irregular communication patterns on massively parallel multicomputers. </title> <type> ICASE Report 91-12, </type> <institution> Institute for Computer Application in Science and Engineering, Hampton, VA, </institution> <month> January </month> <year> 1991. </year> <month> 148 </month>
Reference-contexts: This seems to be a situation potentially occurring in many scientific programs solving irregular problems <ref> [BSGM90, SPBR91, TP90, WLR90] </ref>. 6.2.1 The application One example of a loop nest with varying inner bounds is the NBF calculation as shown in Figure 2.1. However, this kernel takes advantage of Newton's Third Law and performs an indirect lhs assignment to f (j), which results in irregular communication patterns.
Reference: [SS90] <author> L. Snyder and D. Socha. </author> <title> An algorithm producing balanced partitionings of data arrays. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Several compilation systems for exploiting fine-grained parallelism have been and are being built, which include Al [Tse90], Aspar [IFKF90], C*/Dataparallel C [HQL + 91, RS87], Crystal [LC91], Dino [RSW91], Id Nouveau [RP89], Mimd-izer [SWW92], Oxygen [RA90], P 3 C [GAY91], Pandore [APT90], Parafrase-2 [GB92], Paragon [CCRS91], Spot <ref> [SS90, Soc90] </ref>, Superb [ZBG88], and Vienna Fortran [BCZ92].
Reference: [SWW92] <author> R. Sawdayi, G. Wagenbreth, and J. Williamson. MIMDizer: </author> <title> Functional and data decomposition. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Languages, Compilers, and Run-Time Environments for Distributed Memory Machines. </title> <publisher> Elsevier, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1992. </year>
Reference-contexts: Several compilation systems for exploiting fine-grained parallelism have been and are being built, which include Al [Tse90], Aspar [IFKF90], C*/Dataparallel C [HQL + 91, RS87], Crystal [LC91], Dino [RSW91], Id Nouveau [RP89], Mimd-izer <ref> [SWW92] </ref>, Oxygen [RA90], P 3 C [GAY91], Pandore [APT90], Parafrase-2 [GB92], Paragon [CCRS91], Spot [SS90, Soc90], Superb [ZBG88], and Vienna Fortran [BCZ92].
Reference: [Tar74] <author> R. E. Tarjan. </author> <title> Testing flow graph reducibility. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 9 </volume> <pages> 355-365, </pages> <year> 1974. </year>
Reference-contexts: It can be used for forward problems, such as available expressions [All70, Coc70], and backward problems, such as live variables) [Ken71], and it has also been used for code motion [DP93] and incremental analysis [Bur90]. We are using a variant of interval analysis that is based on Tarjan intervals <ref> [Tar74] </ref>. Like Allen-Cocke intervals, a Tarjan interval T (h) is a set of control-flow nodes that corresponds to a loop in the program text, entered through a unique header node h, where h 62 T (h). <p> We also define Header (n) = m if n is the sink of an Entry edge originating in m; otherwise, Header (n) = ;. Note that Cycle and Jump edges correspond to Tarjan's cycle and cross edges, respectively <ref> [Tar74] </ref>. However, we divide his forward edges into Flow and Entry edges depending on whether they enter an interval or not (others divide them into forward and tree edges depending on whether they are part of an embedded tree or not [CLR90]).
Reference: [Thi91] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA. </address> <note> CM Fortran Reference Manual, </note> <year> 1991. </year>
Reference-contexts: F77 MIMD Aversion of Fortran 77 designed to run on a MIMD machine, which assumes a separate name space for each processor. F90 SIMD Aversion of Fortran 90 designed to run on a SIMD machine, similar to Connection Machine Fortran <ref> [Thi91] </ref> or MasPar Fortran [Mas91].
Reference: [TP90] <author> S. Tomboulian and M. Pappas. </author> <title> Indirect addressing and load balancing for faster solutions to the Mandelbrot set on SIMD architectures. </title> <booktitle> In Frontiers90: The 3rd Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 443-450, </pages> <address> College Park, MD, </address> <month> October </month> <year> 1990. </year>
Reference-contexts: This seems to be a situation potentially occurring in many scientific programs solving irregular problems <ref> [BSGM90, SPBR91, TP90, WLR90] </ref>. 6.2.1 The application One example of a loop nest with varying inner bounds is the NBF calculation as shown in Figure 2.1. However, this kernel takes advantage of Newton's Third Law and performs an indirect lhs assignment to f (j), which results in irregular communication patterns. <p> Loop flattening can also be used to process multiple array segments of different lengths per processor, as introduced in Blelloch's V-RAM model [Ble90]. Thus it can be viewed as a generalization of substituting direct addressing with indirect addressing as Tomboulian and Pappas did for computing the Mandelbrot set <ref> [TP90] </ref>. 7.4.4 Fast scan operations The inhomogeneous workload across processors generally associated with irregular problems calls for load balancing. Scan operations are one efficient way for determining the total workload and its distribution [Bia91, Ble90].
Reference: [Tse90] <author> P.-S. Tseng. </author> <title> A parallelizing compiler for distributed memory parallel computers. </title> <booktitle> In Proceedings of the ACM SIGPLAN '90 Conference on Program Language Design and Implementation, </booktitle> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: For exploiting coarse-grained functional parallelism, high-level parallel languages such as Linda [CG89], Strand [FT90, FO90], and Delirium [LS91] have been defined. Several compilation systems for exploiting fine-grained parallelism have been and are being built, which include Al <ref> [Tse90] </ref>, Aspar [IFKF90], C*/Dataparallel C [HQL + 91, RS87], Crystal [LC91], Dino [RSW91], Id Nouveau [RP89], Mimd-izer [SWW92], Oxygen [RA90], P 3 C [GAY91], Pandore [APT90], Parafrase-2 [GB92], Paragon [CCRS91], Spot [SS90, Soc90], Superb [ZBG88], and Vienna Fortran [BCZ92].
Reference: [Tse93] <author> C. Tseng. </author> <title> An Optimizing Fortran D Compiler for MIMD Distributed-Memory Machines. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Section 7.2.2 briefly describes the main concepts of the Fortran D language. A prototype Fortran D compiler targeting distributed-memory architectures has had considerable success with regular problems <ref> [HHKT92, HKK + 91, HKT91, HKT92b, Tse93] </ref>. <p> This chapter describes an extension of the Fortran D compiler prototype <ref> [Tse93] </ref> to handle irregular problems as part of the overall validation. Other validation components are experiments (Chapter 6) and additional theoretical proofs (Appendix A). The rest of this chapter is organized as follows. Section 5.1 gives a general overview of the compiler. <p> If no value is specified for n$proc, then four processors are used by default. Note how the use of induction variables outside of subscripts in reduced loops may necessitate the need for adding a processor dependent offset <ref> [Tse93] </ref>. An additional task currently performed by the regular compiler is the communication generation for regular subscripts (none such communication is needed in nbf). This process does not make use of the Give-N-Take analysis yet, one of the reasons being that Give-N-Take so far does not include dependence analysis.
Reference: [WCSM93] <author> Y.-T. Wong, T. W. Clark, J. Shen, and J. A. McCammon. </author> <title> Molecular dynamics simulation of substrate-enzyme interactions in the active site channel of superoxide dismutase. </title> <journal> Journal of Molecular Simulation, </journal> <volume> 10(2-6):277-289, </volume> <year> 1993. </year>
Reference-contexts: Figures 2.4 and 2.5 show the SOD mappings resulting from one- and three-dimensional value-based mappings, respectively. fl SOD (superoxide dismutase) is a catalytic enzyme that converts the toxic free-radical, O 4 2 , a byproduct of aerobic respiration, to the neutral molecules O 2 and H 2 O 2 <ref> [WCSM93] </ref>. 13 to the processor they are mapped to, for an eight-processor configuration.
Reference: [WLR90] <author> M. Willebeek-LeMair and A. P. Reeves. </author> <title> Solving nonuniform problems on SIMD computers: Case study on region growing. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8 </volume> <pages> 135-149, </pages> <year> 1990. </year>
Reference-contexts: This seems to be a situation potentially occurring in many scientific programs solving irregular problems <ref> [BSGM90, SPBR91, TP90, WLR90] </ref>. 6.2.1 The application One example of a loop nest with varying inner bounds is the NBF calculation as shown in Figure 2.1. However, this kernel takes advantage of Newton's Third Law and performs an indirect lhs assignment to f (j), which results in irregular communication patterns.
Reference: [WSBH91] <author> J. Wu, J. Saltz, H. Berryman, and S. Hiranandani. </author> <title> Distributed memory compiler design for sparse problems. </title> <type> ICASE Report 91-13, </type> <institution> Institute for Computer Application in Science and Engineering, Hampton, VA, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: Developing annotations for conveying information to the compiler at a high level is a part of this dissertation. 1.2.1 The inspector-executor paradigm An important concept associated with communication optimization for applications using irregular array subscripts is the inspector-executor paradigm <ref> [MSS + 88, KMV90, KMSB90, WSBH91] </ref>. A loop that contains indirect accesses to a distributed array is processed in four steps: 4 1. The inspector runs through the loop and only records which array elements are accessed, without doing the actual computation. <p> accesses on distributed shared memory machines by first inspecting each iteration of a loop on whether a processor owns the page of data associated with it, and then looping explicitly over these iterations. 1.2.3 Mapping arrays and mapping functions To specify irregular mappings in a data-parallel context, index-based mapping arrays <ref> [WSBH91] </ref> or mapping functions [CMZ92] have been proposed. However, such arrays or functions that explicitly map indices to processors have to be provided by the programmer, even though she or he may not be interested in what exactly these mappings look like. <p> However, representing a value-based distribution explicitly requires a large amount of state. A translation table maps global indices i glob into pairs (i loc ; p) of local indices and processor numbers. Often the translation table itself is too large to be fully replicated and is distributed instead <ref> [WSBH91] </ref>. Therefore, not only storing but also accessing the information adds complexity to the program.
Reference: [WSHB91] <author> J. Wu, J. Saltz, S. Hiranandani, and H. Berryman. </author> <title> Runtime compilation methods for multicomputers. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year> <month> 149 </month>
Reference-contexts: Arf is designed to interface Fortran application programs with the Parti run-time routines described in Section 7.1.2 <ref> [WSHB91] </ref>. It supports BLOCK, CYCLIC, and user-defined irregular decompositions. The goal of Arf is to demonstrate that inspector/executors based on Parti primitives can be automatically generated by the compiler. Fortran S Fortran S [BKP93] is a variation on Fortran 77 that contains directives for explicit parallelism.
Reference: [ZBG88] <author> H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> SUPERB: A tool for semi-automatic MIMD/SIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year> <month> 150 </month>
Reference-contexts: Several compilation systems for exploiting fine-grained parallelism have been and are being built, which include Al [Tse90], Aspar [IFKF90], C*/Dataparallel C [HQL + 91, RS87], Crystal [LC91], Dino [RSW91], Id Nouveau [RP89], Mimd-izer [SWW92], Oxygen [RA90], P 3 C [GAY91], Pandore [APT90], Parafrase-2 [GB92], Paragon [CCRS91], Spot [SS90, Soc90], Superb <ref> [ZBG88] </ref>, and Vienna Fortran [BCZ92].
References-found: 148

