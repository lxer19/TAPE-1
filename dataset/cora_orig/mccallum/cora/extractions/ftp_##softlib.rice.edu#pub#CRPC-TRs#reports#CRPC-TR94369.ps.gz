URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR94369.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Email: mirchand@cs.rice.edu seema@cs.rice.edu sethi@cs.rice.edu  
Title: Improving the Performance of DSM Systems via Compiler Involvement  
Author: Ravi Mirchandaney* Seema Hiranandani Ajay Sethi 
Address: Houston, TX 77025  Houston, TX 77251-1892  
Affiliation: Center,  Computation, Rice University,  
Note: *Shell Oil Company, Bellaire Research  Center for Research on Parallel  
Abstract: Distributed shared memory (DSM) systems provide an illusion of shared memory on distributed memory systems such as workstation networks and some parallel computers such as the Cray T3D and Convex SPP-1. This illusion is provided either by enhancements to hardware, software, or a combination thereof. On these systems, users can write programs using a shared memory style of programming instead of message passing which is tedious and error prone. Our experience with one such system, TreadMarks, has shown that a large class of applications do not perform well on these systems. TreadMarks is a software distributed shared memory system designed by Rice University researchers to run on networks of workstations and massively parallel computers. Due to the distributed nature of the memory system, shared memory synchronization primitives such as locks and barriers often cause significant amounts of communication. We have provided a set of powerful primitives that will alleviate the problems with locks and barriers on such systems. We have designed two sets of primitives, the first set maintains coherence and is easy to use by a programmer, the second set does not maintain coherence and is best used by a compiler. These primitives require that the underlying DSM be enhanced. We have implemented some of our primitives on the TreadMarks DSM system and obtained reasonable performance improvements on application kernels from molecular dynamics and numerical analysis. Furthermore, we have identified key compiler optimizations that use the non-coherent primitives to reduce synchronization overhead and improve the performance of DSM systems.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. V. Adve and M. D. Hill. </author> <title> A unified formalization of four shared-memory models. </title> <type> Technical Report CS-1051, </type> <institution> Univerity of Wisconsin, Madison, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: Note that the actual pages are not sent at this time, only notices that specific pages have been modified. The happened-before-1 partial order <ref> [1] </ref> is used to compute which changes need to be propagated to the acquirer. The happened-before-1 partial order can be represented by means of vector timestamps on write notices.
Reference: [2] <author> J. R. Allen, D. Callahan, and K. Kennedy. </author> <title> Automatic decomposition of scientific programs for parallel execution. </title> <booktitle> In Proceedings of the Fourteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Munich, Germany, </address> <month> January </month> <year> 1987. </year>
Reference-contexts: The input program may be a shared memory program, however in that case, the user may choose to manually perform the relevant optimizations. It should be noted that we do not propose to reinvent all the shared memory compiler optimizations, some of which are described in [27] <ref> [2] </ref> [3] [4] [5] [30] and possibly performed by a compiler such as KAP [26]. Rather, a shared memory compiler or an interactive tool will form the basis on which our enhancements are built.
Reference: [3] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic loop interchange. </title> <booktitle> In Proceedings of the SIGPLAN '84 Symposium on Compiler Construction, </booktitle> <address> Montreal, Canada, </address> <month> June </month> <year> 1984. </year>
Reference-contexts: The input program may be a shared memory program, however in that case, the user may choose to manually perform the relevant optimizations. It should be noted that we do not propose to reinvent all the shared memory compiler optimizations, some of which are described in [27] [2] <ref> [3] </ref> [4] [5] [30] and possibly performed by a compiler such as KAP [26]. Rather, a shared memory compiler or an interactive tool will form the basis on which our enhancements are built.
Reference: [4] <author> J. Anderson and M. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Program Language Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: The input program may be a shared memory program, however in that case, the user may choose to manually perform the relevant optimizations. It should be noted that we do not propose to reinvent all the shared memory compiler optimizations, some of which are described in [27] [2] [3] <ref> [4] </ref> [5] [30] and possibly performed by a compiler such as KAP [26]. Rather, a shared memory compiler or an interactive tool will form the basis on which our enhancements are built.
Reference: [5] <author> B. Appelbe and B. Lakshmanan. </author> <title> Program transformations for locality using affinity regions. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: It should be noted that we do not propose to reinvent all the shared memory compiler optimizations, some of which are described in [27] [2] [3] [4] <ref> [5] </ref> [30] and possibly performed by a compiler such as KAP [26]. Rather, a shared memory compiler or an interactive tool will form the basis on which our enhancements are built.
Reference: [6] <author> V. Balasundaram and K. Kennedy. </author> <title> A technique for summarizing data access and its use in parallelism enhancing transformations. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Program Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: The cost associated with this primitive is lower than SL because the compiler can dispense with unnecessary consistency related overhead normally incurred by the DSM system. Compiler Support The Fortran D compiler uses Regular Section Descriptors (RSDs) [12] <ref> [6] </ref> to summarize the data region accessed in a loop nest. The RSDs are used to generate appropriate send and receive communication calls. For more details refer to Hiranandani et.al [22] [21]. We propose to use similar information to determine the data that must be communicated during a DSR operation.
Reference: [7] <author> J. Bennett, J. Carter, and W. Zwaenepoel. Munin: </author> <title> Distributed shared memory based on type-specific memory coherence. </title> <booktitle> In Proceedings of the Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Seattle, WA, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: In that sense, it implements a lazy RC because changes to shared data are not propagated after a release. Modifications are only made visible to a processor after it performs an acquire. By comparison, Munin <ref> [7] </ref> implements an eager RC algorithm because modifications are made visible after a release is done by a processor. Both Munin and TreadMarks are able to delay modifications to pages by allowing multiple writers to modify disjoint portions of a shared page.
Reference: [8] <author> B. N. Bershad and M. J. Zekauskas. Midway: </author> <title> Shared memory parallel programming with entry consistency and distributed memory multiprocessors. </title> <type> Technical Report CMU-CS-91-170, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: Consequently, we have seen an increased interest in the idea of using DSMs to program distributed memory systems. DSM systems allow users to write programs using a shared-memory style of programming. A DSM may be implemented in software such Midway <ref> [8] </ref> [11], Mirage [18], Munin [14] and TreadMarks [24] or in hardware such as the Cray T3D or a combination of hardware and software as in Stanford's Flash Multiprocessor. The programming model provided by DSM systems is desirable.
Reference: [9] <author> M. Beternitz, M. Lai, V. Sarkar, and B. Simon. </author> <title> Compiler solution for the stale data and false sharing problem. </title> <type> Technical Report TR03.466, </type> <institution> IBM, Santa Teresa Laboratory, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: No compiler support was available for iteration space tiling or array layouts. Projects such as KOAN [28] and others <ref> [9] </ref> perform optimizations to reduce the effect of false sharing on software DSM systems. One of the problems that some DSM systems face is the ping-pong problem, where a page that is concurrently written by multiple writers moves rapidly between the processors.
Reference: [10] <author> W. Bolosky, R. Fitzgerald, and M. Scott. </author> <title> Simple but effective techniques for NUMA memory management. </title> <booktitle> In Proceedings of the Twelfth Symposium on Operating Systems Principles, </booktitle> <address> Litchfield Park, AZ, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: This approach can be expensive in a software DSM. We propose to address the issue of changing access patterns using our DSM send and DSM recv primitives. An example of its use is provided in Section 4.1. Array padding <ref> [10] </ref> can be used to ensure that no two rows of an array are mapped to a single page. However, if the rows are much smaller than pages, this strategy can result in severe fragmentation problems. <p> Figure 14 shows the performance improvements we achieved using the scatter add routine for the MD kernel. 6 Related Work Methods for reducing the performance degradation due to false sharing of pages in shared memory programs have been studied by several researchers. Bolosky et. al. <ref> [10] </ref> modified a Mach kernel running on the IBM ACE multiprocessor (a NUMA machine) to allow the notion of replicated, private and global pages. Replicated pages were either read-only pages or writeable pages that were not written.
Reference: [11] <author> R. Bryant, P. Carini, H-Y Chang, and B. Rosenburg. </author> <title> Supporting structured shared virtual memory under mach. </title> <booktitle> In Proceedings of the 2nd Mach Usenix Symposium, </booktitle> <month> November </month> <year> 1991. </year>
Reference-contexts: Consequently, we have seen an increased interest in the idea of using DSMs to program distributed memory systems. DSM systems allow users to write programs using a shared-memory style of programming. A DSM may be implemented in software such Midway [8] <ref> [11] </ref>, Mirage [18], Munin [14] and TreadMarks [24] or in hardware such as the Cray T3D or a combination of hardware and software as in Stanford's Flash Multiprocessor. The programming model provided by DSM systems is desirable.
Reference: [12] <author> D. Callahan. </author> <title> A Global Approach to Detection of Parallelism. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> March </month> <year> 1987. </year> <month> 23 </month>
Reference-contexts: The cost associated with this primitive is lower than SL because the compiler can dispense with unnecessary consistency related overhead normally incurred by the DSM system. Compiler Support The Fortran D compiler uses Regular Section Descriptors (RSDs) <ref> [12] </ref> [6] to summarize the data region accessed in a loop nest. The RSDs are used to generate appropriate send and receive communication calls. For more details refer to Hiranandani et.al [22] [21].
Reference: [13] <author> A. Carle, K. Kennedy, U. Kremer, and J. Mellor-Crummey. </author> <title> Automatic data layout for distributed-memory machines in the D programming environment. In Proceedings of AP'93 International Workshop on Automatic Distributed Memory Parallelization, Automatic Data Distribution and Automatic Parallel Performance Prediction, </title> <booktitle> Saarbrucken, </booktitle> <address> Germany, </address> <month> March </month> <year> 1993. </year>
Reference-contexts: However, if the rows are much smaller than pages, this strategy can result in severe fragmentation problems. We investigate the effects of data layout based upon the tiling method for Cholesky. 3.6 Iteration Space Tiling There is on going research at several institutions, including Rice University <ref> [13] </ref>, to build a tool that automatically inserts data mapping directives in user programs. Directives like align and distribute are used by the compiler to tile the iteration space as well as to improve data locality by appropriately distributing the data among the processors.
Reference: [14] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: Consequently, we have seen an increased interest in the idea of using DSMs to program distributed memory systems. DSM systems allow users to write programs using a shared-memory style of programming. A DSM may be implemented in software such Midway [8] [11], Mirage [18], Munin <ref> [14] </ref> and TreadMarks [24] or in hardware such as the Cray T3D or a combination of hardware and software as in Stanford's Flash Multiprocessor. The programming model provided by DSM systems is desirable. However, due to the distributed nature of the memory system, shared memory synchronization primitives are expensive.
Reference: [15] <author> R. Das and J. Saltz. </author> <title> Parallelizing molecular dynamics codes using parti software primitives. </title> <booktitle> In Parallel Processing for Scientific Computation, Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> Norfolk VA, </address> <month> March </month> <year> 1993, 1993. </year>
Reference-contexts: We will incorporate a framework similar to the inspector-executor in our compiler. In certain computations an incomplete reduction, where not all processors concurrently update a variable, is performed. Such reductions can be parallelized by using scatter routine provided by the PARTI framework <ref> [15] </ref>. The scatter routine implemented for the DSM systems allows the compiler to perform a restricted reduction on array elements as well as to eliminate certain synchronizations. We now describe the compiler and DSM enhancements needed to permit concurrent updates to shared variables using the scatter optimization.
Reference: [16] <author> J. Dongarra. </author> <title> Performance of various computers using standard linear equations. </title> <type> Technical Report CS-89-93, </type> <institution> University of Tennessee, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: All our tests were conducted using TreadMarks on an ATM network of DECstations and the Intel Paragon at Rice University. 5.1 Experimental Environment The ATM Network Our ATM network consists of 8 DECstation-5000/240s running Ultrix V4.3. These processors have a 100fi100 Linpack performance of 5.3 Mflops <ref> [16] </ref>. Each of these machines has a Fore Systems ATM board, connected to an ATM switch. The interface boards have a capacity of 100 Mb/sec and the switch has an aggregate capacity of 1.2 Gb/sec. The Fore Systems ATM boards perform programmed I/O into transmit and receive FIFOs.
Reference: [17] <author> S. Dwarkadas, P. Keleher, A. Cox, and W. Zwaenepoel. </author> <title> Evaluation of release consistent software distributed shared memory on emerging network technology. </title> <booktitle> In Proceedings of the 20th International Symposium on Computer Architecture, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: We applied our extensions to TreadMarks and obtained reasonable reduction in communication overhead. TreadMarks is a software DSM system designed by Rice University researchers to run on networks of workstations and MPPs. The evaluation of TreadMarks on workstations connected by ethernet and ATM networks is provided Dwarkadas et.al <ref> [17] </ref> and Keleher et.al [24]. The results of various applications and kernels running on TreadMarks can be summarized by saying that moderate to coarse grained computations such as Jacobi iteration and Traveling Salesman Problem (TSP), show fairly good speedups.
Reference: [18] <author> B. Fleisch and G. Popek. </author> <title> Mirage: A coherent distributed shared memory design. </title> <booktitle> In Proceedings of the Twelfth Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1989. </year>
Reference-contexts: Consequently, we have seen an increased interest in the idea of using DSMs to program distributed memory systems. DSM systems allow users to write programs using a shared-memory style of programming. A DSM may be implemented in software such Midway [8] [11], Mirage <ref> [18] </ref>, Munin [14] and TreadMarks [24] or in hardware such as the Cray T3D or a combination of hardware and software as in Stanford's Flash Multiprocessor. The programming model provided by DSM systems is desirable.
Reference: [19] <author> E. Granston and H. Wishoff. </author> <title> Managing pages in shared virtual memory systems: Getting the compiler into the game. </title> <booktitle> In Proceedings of the 1993 ACM International Conference on Supercomputing, </booktitle> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: However, data can be laid out in shared memory to increase the locality of accesses. Granston and Wijshoff <ref> [19] </ref> discuss several methods to improve data layout in DSM systems so that false sharing is minimized. In the best case, each data item is assigned to its own page (s). Unfortunately, this is impractical because fragmentation can be very high. <p> In DSM systems, physical distribution of data among the processors has no relevance. What is relevant however, is that the iteration space be tiled in such a way that a processor accesses elements on the same page (s). Granston and Wijshoff <ref> [19] </ref> discuss tiling the iteration space using a page sensitive algorithm. They assume that the underlying system does not permit multiple writers, or if so, it does in a very limited set of circumstances. The example problem discussed in the paper is shown below.
Reference: [20] <author> R. v. Hanxleden, K. Kennedy, C. Koelbel, R. Das, and J. Saltz. </author> <title> Compiler analysis for irregular problems in Fortran D. </title> <booktitle> In Proceedings of the Fifth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: Each processor writes to the local variable and at the end of reduction, a global accumulation is performed to collect the partial results. 3.4 Sparse Matrix Computations Loop nests which possess dependencies that cannot be characterized during compilation can be handled using the idea of inspector-executor [32] <ref> [20] </ref> [33] developed in the PARTI system. At compile time, the loop nest is transformed into two loops: an inspector and an executor. <p> In order to guarantee that all processors have a consistent version of the page after the reduction has occurred, we perform a global accumulate to collect the partial sums. 16 4.4 Molecular Dynamics Kernel A simplified version of an MD kernel <ref> [20] </ref> is depicted in Figure 12. The x and f (representing the distance and force components respectively) arrays are laid out serially in shared memory, ordered by atom number. We refer to this type of data layout as the canonical layout.
Reference: [21] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: Compiler Support The Fortran D compiler uses Regular Section Descriptors (RSDs) [12] [6] to summarize the data region accessed in a loop nest. The RSDs are used to generate appropriate send and receive communication calls. For more details refer to Hiranandani et.al [22] <ref> [21] </ref>. We propose to use similar information to determine the data that must be communicated during a DSR operation.
Reference: [22] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Compiler Support The Fortran D compiler uses Regular Section Descriptors (RSDs) [12] [6] to summarize the data region accessed in a loop nest. The RSDs are used to generate appropriate send and receive communication calls. For more details refer to Hiranandani et.al <ref> [22] </ref> [21]. We propose to use similar information to determine the data that must be communicated during a DSR operation. <p> Compiler Support Compilation techniques for distributed memory machines have been successful in recognizing and parallelizing reductions by allowing processors to compute partial values locally. At the end of the loop, all processors perform a global accumulation operation that updates their copy of the variable <ref> [22] </ref>. We will use the same compilation technology for parallelizing reductions. The compiler will insert a global accumulation call at the end of the reduction in order to collect the partial values computed on each processor. The underlying DSM must be extended to handle such global accumulations. <p> In both cases communication overhead is very high. We intend to reduce the communication by sacrificing some amount of parallelism and using DSM Send Recv to minimize unnecessary communication traffic. The resulting loop nest exhibits pipeline parallelism and has high spatial locality <ref> [22] </ref>. The loop nest with section locks and pipeline parallelism is shown in Figure 10. Experiments conducted on the iPSC/860 at Rice University have shown that pipelining 13 the computation achieves good speedups [23]. 4.2 Cholesky Factorization In Figure 11, we depict cholesky factorization with a block-cyclic tiling.
Reference: [23] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Preliminary experiences with the Fortran D compiler. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: The resulting loop nest exhibits pipeline parallelism and has high spatial locality [22]. The loop nest with section locks and pipeline parallelism is shown in Figure 10. Experiments conducted on the iPSC/860 at Rice University have shown that pipelining 13 the computation achieves good speedups <ref> [23] </ref>. 4.2 Cholesky Factorization In Figure 11, we depict cholesky factorization with a block-cyclic tiling. The b loop iterates over each block for every processor, and the i loop iterates over the blocks for each processor. The function lower block cyclic returns the lower bound for the i loop.
Reference: [24] <author> P. Keleher, A. Cox, and W. Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Consequently, we have seen an increased interest in the idea of using DSMs to program distributed memory systems. DSM systems allow users to write programs using a shared-memory style of programming. A DSM may be implemented in software such Midway [8] [11], Mirage [18], Munin [14] and TreadMarks <ref> [24] </ref> or in hardware such as the Cray T3D or a combination of hardware and software as in Stanford's Flash Multiprocessor. The programming model provided by DSM systems is desirable. However, due to the distributed nature of the memory system, shared memory synchronization primitives are expensive. <p> TreadMarks is a software DSM system designed by Rice University researchers to run on networks of workstations and MPPs. The evaluation of TreadMarks on workstations connected by ethernet and ATM networks is provided Dwarkadas et.al [17] and Keleher et.al <ref> [24] </ref>. The results of various applications and kernels running on TreadMarks can be summarized by saying that moderate to coarse grained computations such as Jacobi iteration and Traveling Salesman Problem (TSP), show fairly good speedups. <p> Memory accesses can be described as being ordinary or synchronization, with the latter category further divided into acquire and release accesses. RC requires that ordinary accesses to shared memory be performed only when a subsequent release by the same processor is performed <ref> [24] </ref>. Acquires and releases may be thought of as being similar to synchronization operations performed on locks. TreadMarks implements RC as well as mechanisms to minimize the number of messages that are sent.
Reference: [25] <institution> Kendall Square Research, </institution> <address> Waltham, MA. </address> <note> KSR1 Principles of Operation, revision 6.0 edition, </note> <month> October </month> <year> 1992. </year>
Reference-contexts: The compiler may select a particular tiling for a loop nest. This tiling may or may not be the one that is optimal for the problem. Several compilers permit the user to provide an optional directive that forces a particular tiling strategy <ref> [25] </ref>. Because our approach is to build on current compiler technology, we will permit the user to provide such a directive. 4 Representative Kernels In this section, we present four important kernels from the fields of numerical analysis and molecular dynamics.
Reference: [26] <author> Kuck & Associates, Inc. </author> <title> KAP User's Guide. </title> <address> Champaign, IL 61820, </address> <year> 1988. </year>
Reference-contexts: It should be noted that we do not propose to reinvent all the shared memory compiler optimizations, some of which are described in [27] [2] [3] [4] [5] [30] and possibly performed by a compiler such as KAP <ref> [26] </ref>. Rather, a shared memory compiler or an interactive tool will form the basis on which our enhancements are built.
Reference: [27] <author> D. Kuck, R. Kuhn, D. Padua, B. Leasure, and M. J. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Conference Record of the Eighth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Williamsburg, VA, </address> <month> January </month> <year> 1981. </year>
Reference-contexts: The input program may be a shared memory program, however in that case, the user may choose to manually perform the relevant optimizations. It should be noted that we do not propose to reinvent all the shared memory compiler optimizations, some of which are described in <ref> [27] </ref> [2] [3] [4] [5] [30] and possibly performed by a compiler such as KAP [26]. Rather, a shared memory compiler or an interactive tool will form the basis on which our enhancements are built.
Reference: [28] <author> Z. Lajormi and E. Priol. Koan: </author> <title> Shared memory for the ipsc/2 hypercube. </title> <booktitle> In CONPAR-VAPP 92. </booktitle> <address> Springler-Verlig, </address> <month> September </month> <year> 1992. </year>
Reference-contexts: The actual packing of structures to fit in a page (or a set of pages) so as to reduce false sharing was done manually through modifications to the program's data structures. No compiler support was available for iteration space tiling or array layouts. Projects such as KOAN <ref> [28] </ref> and others [9] perform optimizations to reduce the effect of false sharing on software DSM systems. One of the problems that some DSM systems face is the ping-pong problem, where a page that is concurrently written by multiple writers moves rapidly between the processors.
Reference: [29] <author> D. Lenoski, J. Laudon, T. Joe, D. Nakahira, L. Stevens, A. Gupta, and J. Hennessy. </author> <title> The DASH prototype: Implementation and performance. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Section 4 illustrates the use of compiler optimizations on kernels from NA and MD. Section 5 provides performance results for DSM programs that incorporate these optimizations. We briefly survey related work in Section 6 and conclude in Section 7. 2 2 Description of TreadMarks Release consistency (RC) <ref> [29] </ref> is a relaxed model of memory consistency that permits changes to shared memory to be delayed until certain synchronization actions are performed. Memory accesses can be described as being ordinary or synchronization, with the latter category further divided into acquire and release accesses.
Reference: [30] <author> K. S. McKinley. </author> <title> Automatic and Interactive Parallelization. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: It should be noted that we do not propose to reinvent all the shared memory compiler optimizations, some of which are described in [27] [2] [3] [4] [5] <ref> [30] </ref> and possibly performed by a compiler such as KAP [26]. Rather, a shared memory compiler or an interactive tool will form the basis on which our enhancements are built.
Reference: [31] <author> S. K. Reinhardt, J. L. Larus, and D. A. Wood. </author> <title> Tempest and typhoon : User-level shared memory. </title> <booktitle> In Proceedings of the 21th International Symposium on Computer Architecture, </booktitle> <year> 1994. </year> <month> 24 </month>
Reference-contexts: Using this approach, the "inspector" iterations of the loop will record the non-local accesses (and other relevant information like the owner of the data, etc.) while the other iterations use this information. The Tempest/Typhoon system being developed by Reinhardt et.al <ref> [31] </ref> will allow certain iterations to be "inspector" iterations. Using the information collected by the inspector, the routines like scatter are implemented as before. DSM Support Each processor uses the lists of elements to be received from remote processors to post receives.
Reference: [32] <author> J. Saltz, H. Berryman, and J. Wu. </author> <title> Multiprocessors and run-time compilation. </title> <journal> Concurrency: Practice & Expe--rience, </journal> <volume> 3(6) </volume> <pages> 573-592, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: Each processor writes to the local variable and at the end of reduction, a global accumulation is performed to collect the partial results. 3.4 Sparse Matrix Computations Loop nests which possess dependencies that cannot be characterized during compilation can be handled using the idea of inspector-executor <ref> [32] </ref> [20] [33] developed in the PARTI system. At compile time, the loop nest is transformed into two loops: an inspector and an executor.
Reference: [33] <author> J. Saltz, K. Crowley, R. Mirchandaney, and H. Berryman. </author> <title> Run-time scheduling and execution of loops on message passing machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(4) </volume> <pages> 303-312, </pages> <month> April </month> <year> 1990. </year> <month> 25 </month>
Reference-contexts: Each processor writes to the local variable and at the end of reduction, a global accumulation is performed to collect the partial results. 3.4 Sparse Matrix Computations Loop nests which possess dependencies that cannot be characterized during compilation can be handled using the idea of inspector-executor [32] [20] <ref> [33] </ref> developed in the PARTI system. At compile time, the loop nest is transformed into two loops: an inspector and an executor.
References-found: 33

