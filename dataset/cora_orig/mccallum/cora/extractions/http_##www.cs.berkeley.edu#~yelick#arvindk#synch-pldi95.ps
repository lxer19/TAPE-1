URL: http://www.cs.berkeley.edu/~yelick/arvindk/synch-pldi95.ps
Refering-URL: http://www.cs.berkeley.edu/~yelick/papers.html
Root-URL: 
Title: Optimizing Parallel Programs with Explicit Synchronization  
Author: Arvind Krishnamurthy and Katherine Yelick 
Address: Berkeley  
Affiliation: Computer Science Division University of California,  
Abstract: We present compiler analyses and optimizations for explicitly parallel programs that communicate through a shared address space. Any type of code motion on explicitly parallel programs requires a new kind of analysis to ensure that operations reordered on one processor cannot be observed by another. The analysis, based on work by Shasha and Snir, checks for cycles among interfering accesses. We improve the accuracy of their analysis by using additional information from post-wait synchronization, barriers, and locks. We demonstrate the use of this analysis by optimizing remote access on distributed memory machines. The optimizations include message pipelining, to allow multiple outstanding remote memory operations, conversion of two-way to one-way communication, and elimination of communication through data re-use. The performance improvements are as high as 20-35% for programs running on a CM-5 multiprocessor using the Split-C language as a global address layer. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. V. Adve and M. D. Hill. </author> <title> Weak Ordering-A New Definition. </title> <booktitle> In 17th International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1990. </year>
Reference-contexts: Our algorithm for analyzing post-wait synchronization is similar in spirit; however, we can also exploit mutual-exclusion information on accesses. Also related to our work is the re-search that proposes weaker memory models <ref> [1, 7] </ref>. Those approaches change the programmer's model by giving programming conventions under which sequential consistency is ensured. Our work shifts this burden from the programmer to the compiler.
Reference: [2] <author> R. Arpaci, D. Culler, A. Krishnamurthy, S. Steinberg, and K. Yelick. </author> <title> Empirical Evaluation of the CRAY-T3D: A Compiler Perspective. </title> <booktitle> In International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference: [3] <author> H. Berryman, J. Saltz, and J. Scroggs. </author> <title> Execution Time Support for Adaptive Scientific Algorithms on Distributed Memory Multiprocessors. </title> <journal> Concurrenty: Practice and Experience, </journal> <month> June </month> <year> 1991. </year>
Reference-contexts: Compilers and runtime systems for data parallel languages like HPF and Fortran-D [9] implement message pipelin-ing optimizations and data re-use. The Parti runtime system and associated HPF compiler uses a combination of compiler and runtime analysis to optimize communication <ref> [3] </ref>, and these optimizations have also been studied in the context of parallelizing compilers [17]. However, as discussed earlier, compiling data parallel programs is fundamentally different from compiling explicitly parallel programs.
Reference: [4] <author> D. Callahan and J. Subhlok. </author> <title> Static Analysis of Low-level Synchronization. </title> <booktitle> In ACM SIGPLAN and SIGOPS Workshop on Parallel and Distributed Debugging, </booktitle> <month> May </month> <year> 1988. </year>
Reference: [5] <author> W. W. Carlson and J. M. Draper. </author> <title> Distributed Data Access in AC. </title> <booktitle> In ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Analysis for these programs is based on the pioneering work by Shasha and Snir [18], which was later extended by Midkiff et al [16] to handle array based accesses. We analyze the synchronization <ref> [5] </ref> accesses in the program and obtain precedence and mutual exclusion information regarding remote accesses. Others have proposed algorithms for analyzing synchronization constructs in the context of framing data-flow equations for parallel programs, where strict precedence information is necessary [4][8].
Reference: [6] <author> D. E. Culler, A. Dusseau, S. C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Supercomputing '93, </booktitle> <address> Portland, Oregon, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Three important optimizations for these multiprocessors are overlapping communication, eliminating round-trip message traffic, and avoiding communication altogether. The first optimization, message pipelining, changes remote read and write operations into their split-phase analogs, get and put. In a split-phase operation, the initiation of an access is separated from its completion <ref> [6] </ref>. The operation to force completion of outstanding split-phase operations comes in many forms, the simplest of which (called sync or fence) blocks until all outstanding accesses are complete. To improve communication overlap, puts and gets are moved backwards in the program execution and syncs are moved forward. <p> Related work is surveyed in section 9 and conclusions drawn in section 10. 2 Programming Language Our analyses are designed for explicitly parallel shared memory programs. We have implemented them in a source-to-source transformer for a subset of Split-C <ref> [6] </ref>. Split-C is an explicitly parallel SPMD language for programming distributed memory machines using a global address space abstraction. The parallel threads interact through reads and writes on a shared address space that contains distributed arrays and shared objects accessible through wide pointers.
Reference: [7] <author> K. Gharachorloo, D. Lenoski, J. Laudon, A. Gupta, and J. Hennessy. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors. </title> <booktitle> In 17th International Symposium on Computer Architecture, </booktitle> <year> 1990. </year>
Reference-contexts: Our algorithm for analyzing post-wait synchronization is similar in spirit; however, we can also exploit mutual-exclusion information on accesses. Also related to our work is the re-search that proposes weaker memory models <ref> [1, 7] </ref>. Those approaches change the programmer's model by giving programming conventions under which sequential consistency is ensured. Our work shifts this burden from the programmer to the compiler.
Reference: [8] <author> D. Grunwald and H. Srinivasan. </author> <title> Data flow equations for Explicitly Parallel Programs. </title> <booktitle> In ACM Symposium on Principles and Practices of Parallel Programming, </booktitle> <month> June </month> <year> 1993. </year>
Reference: [9] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiler Optimziations for Fortran D on MIMD Distributed-Memory Machines. </title> <booktitle> In Proceedings of the 1991 International Conference on Supercomputing, </booktitle> <year> 1991. </year>
Reference-contexts: Our analysis could also be used for compiling weak memory programs since it can determine when code motion is legal, which is critical for generating prefetch instructions. Compilers and runtime systems for data parallel languages like HPF and Fortran-D <ref> [9] </ref> implement message pipelin-ing optimizations and data re-use. The Parti runtime system and associated HPF compiler uses a combination of compiler and runtime analysis to optimize communication [3], and these optimizations have also been studied in the context of parallelizing compilers [17].
Reference: [10] <author> T. E. Jeremiassen and S. J. Eggers. </author> <title> Reducing False Sharing on Shared Memory Multiprocessors through Compile Time Data Transformations. </title> <booktitle> In ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Rather than adding sophisticated analysis to line up barriers <ref> [10] </ref>, we use a simple run-time solution that works well for many real programs. We add a run-time check to each barrier to determine whether these are the ones lined up during compilation.
Reference: [11] <author> A. Krishnamurthy and K. Yelick. </author> <title> Optimizing Parallel SPMD Programs. </title> <booktitle> In Languages and Compilers for Parallel Computing, </booktitle> <year> 1994. </year>
Reference-contexts: Cycle detection was first described by Shasha and Snir [18] and later extended by Midkiff, Padua, and Cytron to handle array indices [16]. In previous work, we showed that by restricting attention to Single Program Multiple Data (SPMD) programs, one could significantly reduce the complexity of cycle detection <ref> [11] </ref>. The primary contribution of this paper is improved cycle detection that makes use of synchronization information in the program. Shasha and Snir's analysis, when applied to real applications, discovers a huge number of spurious cycles, because cycles are detected between accesses that will never execute concurrently due to synchronization.
Reference: [12] <author> L. Lamport. </author> <title> How to Make a Multiprocessor Computer that Correctly Executes Multiprocess Programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9), </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: Intuitively, the parallel programmer relies on the notion of sequential consistency: the parallel execution must behave as if it were an interleaving of the sequences of memory operations from each of the processors <ref> [12] </ref>. If only the local dependencies within a processor are observed, the program execution might not be sequentially consistent [15]. <p> An execution E is sequentially consistent if there exists a total order S of the operations in E, i.e., E S, such that S is a correct sequential execution where the reads must return the value of the most recent preceding write <ref> [12] </ref>.
Reference: [13] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In 17th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: Even if packets do not get reordered, two access sent to two different processors may be handled out of order, since latencies may vary. Also, on a machine like DASH <ref> [13] </ref>, with hardware caching, writes do not wait for all invalidations to complete, so remote accesses might appear to execute in reverse-order.
Reference: [14] <author> N. K. Madsen. </author> <title> Divergence Preserving Discrete Surface Integral Methods for Maxwell's Curl Equations Using Non-Orthogonal Unstructured Grids. </title> <type> Technical Report 92.04, </type> <institution> RIACS, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: The primary data structure is a grid, and the core of this application is a stencil-like computation. EM3D: Em3d models the propagation of electromagnetic waves through objects in three dimensions <ref> [14] </ref>. The computation consists of a series of "leapfrog" integration steps: on alternate half time steps, changes in the electric field are calculated as a linear function of the neighboring magnetic field values and vice versa.
Reference: [15] <author> S. Midkiff and D. Padua. </author> <title> Issues in the Optimization of Parallel Programs. </title> <booktitle> In International Conference on Parallel Processing - Vol II, </booktitle> <year> 1990. </year>
Reference-contexts: If only the local dependencies within a processor are observed, the program execution might not be sequentially consistent <ref> [15] </ref>. <p> As a result, they either generate incorrect code or miss opportunities for optimizing communication and synchronization, and the quality of the scalar code is limited by the inability to move code around parallelism primitives <ref> [15] </ref>. We present optimizations for multiprocessors with physically distributed memory and hardware or software support for a global address space. As shown in table 1, a remote reference on such a machine has a long latency [2][21][13]. <p> As expected, the optimized versions scale better with processors 9 Related Work Most of the research in optimizing parallel programs has been for data parallel programs. In the more general control parallel setting, Midkiff and Padua <ref> [15] </ref> describe eleven different instances where standard optimizations (like code motion and dead code elimination) cannot be directly applied. Analysis for these programs is based on the pioneering work by Shasha and Snir [18], which was later extended by Midkiff et al [16] to handle array based accesses.
Reference: [16] <author> S. P. Midkiff, D. Padua, and R. G. Cytron. </author> <title> Compiling Programs with User Parallelism. </title> <booktitle> In Languages and Compilers for Parallel Computing, </booktitle> <year> 1990. </year>
Reference-contexts: Cycle detection was first described by Shasha and Snir [18] and later extended by Midkiff, Padua, and Cytron to handle array indices <ref> [16] </ref>. In previous work, we showed that by restricting attention to Single Program Multiple Data (SPMD) programs, one could significantly reduce the complexity of cycle detection [11]. The primary contribution of this paper is improved cycle detection that makes use of synchronization information in the program. <p> Analysis for these programs is based on the pioneering work by Shasha and Snir [18], which was later extended by Midkiff et al <ref> [16] </ref> to handle array based accesses. We analyze the synchronization [5] accesses in the program and obtain precedence and mutual exclusion information regarding remote accesses. Others have proposed algorithms for analyzing synchronization constructs in the context of framing data-flow equations for parallel programs, where strict precedence information is necessary [4][8].
Reference: [17] <author> A. Rogers and K. Pingali. </author> <title> Compiling for distributed memory architectures. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <month> March </month> <year> 1994. </year>
Reference-contexts: The Parti runtime system and associated HPF compiler uses a combination of compiler and runtime analysis to optimize communication [3], and these optimizations have also been studied in the context of parallelizing compilers <ref> [17] </ref>. However, as discussed earlier, compiling data parallel programs is fundamentally different from compiling explicitly parallel programs.
Reference: [18] <author> D. Shasha and M. Snir. </author> <title> Efficient and Correct Execution of Parallel Programs that Share Memory. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(2), </volume> <month> April </month> <year> 1988. </year>
Reference-contexts: If only the local dependencies within a processor are observed, the program execution might not be sequentially consistent [15]. To guarantee sequential consistency under reordering transformations, a new type of analysis called cycle detection is required <ref> [18] </ref>. fl This work was supported in part by the Advanced Research Projects Agency of the Department of Defense monitored by the Office of Naval Research under contract DABT63-92-C-0026, by the Department of Energy under contract DE-FG03-94ER25206, and by the National Science Foundation. <p> A final optimization is the elimination of remote accesses by either re-using values of previous accesses or updating a remote value locally multiple times before issuing a write operation on the final value. Cycle detection was first described by Shasha and Snir <ref> [18] </ref> and later extended by Midkiff, Padua, and Cytron to handle array indices [16]. In previous work, we showed that by restricting attention to Single Program Multiple Data (SPMD) programs, one could significantly reduce the complexity of cycle detection [11]. <p> In order to extend the system contract for programs with weak memory accesses, rather than relying on a particular instruction set with non-blocking memory operations and synchronizing accesses, we use a more general framework proposed by Shasha and Snir <ref> [18] </ref>. A delay set D specifies some pairs of memory accesses as being ordered, which says that the second operation must be delayed until the first one is complete. <p> Definition 3 D S&S = f [a i ; a j ] 2 P j [a i ; a j ] has a back-path in P [ Cg. Theorem 1 <ref> [18] </ref> D S&S is sufficient. <p> In the more general control parallel setting, Midkiff and Padua [15] describe eleven different instances where standard optimizations (like code motion and dead code elimination) cannot be directly applied. Analysis for these programs is based on the pioneering work by Shasha and Snir <ref> [18] </ref>, which was later extended by Midkiff et al [16] to handle array based accesses. We analyze the synchronization [5] accesses in the program and obtain precedence and mutual exclusion information regarding remote accesses.
Reference: [19] <author> J. P. Singh, W. D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared memory. Computer Architecture News, </title> <month> March </month> <year> 1992. </year>
Reference-contexts: A brief description of the applications is given below: Ocean: This benchmark is from the Splash benchmark suite <ref> [19] </ref>, and studies the role of eddy and boundary currents in large-scale ocean movements. The primary data structure is a grid, and the core of this application is a stencil-like computation. EM3D: Em3d models the propagation of electromagnetic waves through objects in three dimensions [14].
Reference: [20] <institution> The SPARC Architecture Manual: </institution> <note> Version 8. Sparc International, </note> <institution> Inc., </institution> <year> 1992. </year>
Reference-contexts: Most processors have write buffers, which allow read operations to overtake write operations that have been issued earlier. In fact, on the Su-perSparcs <ref> [20] </ref> the write-buffer itself is not guaranteed to be FIFO. Reordering may also take place at the network level in distributed memory multiprocessors, because some networks adaptively route packets to avoid congestion.
Reference: [21] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
References-found: 21

