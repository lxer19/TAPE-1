URL: http://www.cs.nmsu.edu/lldap/download/ieee.ps.Z
Refering-URL: http://www.cs.nmsu.edu/lldap/pub_lp/ieee.html
Root-URL: http://www.cs.nmsu.edu
Title: Data Parallel Logic Programming in &ACE  
Author: Enrico Pontelli Gopal Gupta 
Address: Las Cruces, NM 88003-0001  
Affiliation: Laboratory for Logic, Databases, and Advanced Programming Department of Computer Science New Mexico State University  
Abstract: ACE is a high performance Parallel Prolog System developed at the Laboratory for Logic, Databases, and Advanced Programming that exploits and-parallelism from Prolog programs. &ACE was developed to exploit MIMD parallelism. However, SPMD parallelism also arises naturally in many Pro-log programs. In this paper we develop runtime techniques that allow systems that have primarily been designed to exploit MIMD parallelism (such as &ACE) to also efficiently exploit SPMD parallelism. These runtime techniques have been incorporated in the &ACE system. Performance of &ACE augmented with these techniques on programs containing SPMD parallelism is presented. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Carro and M. Hermenegildo. </author> <title> A Note on Data-Parallelism and (And-Parallel) Prolog. </title> <booktitle> In ICLP94 Workshop on Parallel and Data Parallel Execution of Logic Programs, </booktitle> <year> 1994. </year>
Reference-contexts: Approach 5 It should be noted that does not lead to any loss of generality since in most cases or-parallelism and and-parallelism are interconvertible [4]. 4 proc_elem (X1) proc_list (Y1) proc_elem (X2) proc_list (Y2) proc_elem (X3) proc_list (Y3) proc_elem (X4) proc_list (Y4) proc_elem (X1)proc_elem (X2)proc_elem (X3)proc_elem (X4) (i) (ii) a <ref> [1] </ref> a [3] a [1..n] The first approach, taken by Hermenegildo's group [1], focuses on the transformation of list processing programs. Hermenegildo's group has proposed two techniques. <p> loss of generality since in most cases or-parallelism and and-parallelism are interconvertible [4]. 4 proc_elem (X1) proc_list (Y1) proc_elem (X2) proc_list (Y2) proc_elem (X3) proc_list (Y3) proc_elem (X4) proc_list (Y4) proc_elem (X1)proc_elem (X2)proc_elem (X3)proc_elem (X4) (i) (ii) a <ref> [1] </ref> a [3] a [1..n] The first approach, taken by Hermenegildo's group [1], focuses on the transformation of list processing programs. Hermenegildo's group has proposed two techniques. The first is targeted to the generation of code which unrolls recursion over a list by N elements at the time, where N is a fixed constant.
Reference: [2] <author> S. Debray and M. Jain. </author> <title> A Simple Program Transformation for Parallelism. </title> <booktitle> In Proc. of the 1994 Symposium on Logic Programming. </booktitle> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Debray's Approach A similar approach has been recently proposed by Debray <ref> [2] </ref> and it applies to recursions where: 1. the depth is known; 2. the outcome of the recursion is obtained by applying an associative operator to combine the out comes produced by each iteration.
Reference: [3] <author> D. </author> <title> DeGroot. Restricted AND-Parallelism. </title> <booktitle> In Int'l Conf. on 5th Generation Computer Systems, </booktitle> <pages> pages 471-478. </pages> <address> Tokyo, </address> <month> Nov. </month> <year> 1984. </year>
Reference-contexts: It should be noted that does not lead to any loss of generality since in most cases or-parallelism and and-parallelism are interconvertible [4]. 4 proc_elem (X1) proc_list (Y1) proc_elem (X2) proc_list (Y2) proc_elem (X3) proc_list (Y3) proc_elem (X4) proc_list (Y4) proc_elem (X1)proc_elem (X2)proc_elem (X3)proc_elem (X4) (i) (ii) a [1] a <ref> [3] </ref> a [1..n] The first approach, taken by Hermenegildo's group [1], focuses on the transformation of list processing programs. Hermenegildo's group has proposed two techniques. <p> Foundations of much of the work described in this section were laid down in <ref> [3, 8] </ref>, however, the specific implementation described is that of the &ACE system. Conventionally, an and-parallel Prolog system works by executing a program that has been annotated with parallel conjunctions. These parallel conjunction annotations are either inserted by a parallelizing compiler [7] or hand-coded by the programmer. <p> upon the SICStus (sequential) Prolog Engine which ensures good performance in the single processor case. &ACE implementation itself is inspired by the RAPWAM [7] 7 . &ACE is an extension to the sequential WAM (Warren Abstract Machine [20]) for and-parallel execution of Prolog programs with and-parallel annotation (such as CGEs <ref> [3] </ref>). The &ACE system has shown remarkable results on a variety of benchmarks. On most of the commonly used benchmarks the system has shown excellent execution times and speedups|figure 4 shows the speedups obtained on some of these benchmarks 8 .
Reference: [4] <author> G.Gupta E. Pontelli. </author> <title> On the Duality Between Or- and And-Parallelism in Logic Programming. </title> <type> Technical report, </type> <institution> New Mexico State University, </institution> <year> 1995. </year>
Reference-contexts: Two proposals in this area have been made that are extremely similar in nature. Hermenegildo's Approach 5 It should be noted that does not lead to any loss of generality since in most cases or-parallelism and and-parallelism are interconvertible <ref> [4] </ref>. 4 proc_elem (X1) proc_list (Y1) proc_elem (X2) proc_list (Y2) proc_elem (X3) proc_list (Y3) proc_elem (X4) proc_list (Y4) proc_elem (X1)proc_elem (X2)proc_elem (X3)proc_elem (X4) (i) (ii) a [1] a [3] a [1..n] The first approach, taken by Hermenegildo's group [1], focuses on the transformation of list processing programs.
Reference: [5] <author> G. Gupta, M. Hermenegildo, and E. Pontelli. </author> <title> &ACE: A High-performance Parallel Prolog System. </title> <booktitle> In IPPS 95. IEEE Computer Society, </booktitle> <address> Santa Barbara, CA, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: However, the same is not true of parallel processing of Symbolic and Artificial Intelligence (AI) applications. There has been some work [11, 19], but there are still no systems that are general enough and that programmers can use without being experts on parallel processing. &ACE <ref> [5] </ref> is a a system that is a step towards the goal of building a general automatic tool for parallelizing symbolic and AI applications. &ACE is based on the logic programming paradigm and the Prolog technology. <p> It also presents a critical survey of earlier efforts for exploiting SPMD parallelism in Prolog. Section 3 describes the parallel Prolog system called &ACE <ref> [5] </ref> that is designed to exploit MIMD parallelism from Prolog programs. <p> In order to support this pattern of execution we require a minimal support for control and-parallelism. In particular, independent and-parallelism (like the one supported by systems like &ACE <ref> [5] </ref> and &-Prolog [7] and ) is sufficient if the recursive call is independent from the outcome of the processing of one element of the recursive structure, otherwise a form of dependent and-parallelism is required (like DDAS [17]). <p> Other implementations based on the principles of RAP-WAM have also been proposed in the past, like &-Prolog [7] and DDAS [17]. 8 Data produce by running the system on a Sequent Symme try Multiprocessors. 7 further details regarding the structure of &ACE and its performance the reader is referred to <ref> [6, 5] </ref>. Speedups Takeuchi Compiler Hanoi Quicksort Speedup No. of Agents 1.00 3.00 5.00 7.00 9.00 4 Last Parallel Call Optimization In this section we present the last parallel call optimization that allows us to execute data-parallel programs efficiently in the &ACE system.
Reference: [6] <author> G. Gupta, M. Hermenegildo, E. Pontelli, and V. Santos Costa. </author> <title> ACE: And/Or-parallel Copying-based Execution of Logic Programs. </title> <booktitle> In Proc. ICLP'94, </booktitle> <pages> pages 93-109. </pages> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Other implementations based on the principles of RAP-WAM have also been proposed in the past, like &-Prolog [7] and DDAS [17]. 8 Data produce by running the system on a Sequent Symme try Multiprocessors. 7 further details regarding the structure of &ACE and its performance the reader is referred to <ref> [6, 5] </ref>. Speedups Takeuchi Compiler Hanoi Quicksort Speedup No. of Agents 1.00 3.00 5.00 7.00 9.00 4 Last Parallel Call Optimization In this section we present the last parallel call optimization that allows us to execute data-parallel programs efficiently in the &ACE system.
Reference: [7] <author> M. Hermenegildo and K. Greene. </author> <title> &-Prolog and its Performance: Exploiting Independent And-Parallelism. </title> <booktitle> In 1990 Int'l Conf. on Logic Prog., </booktitle> <pages> pages 253-268. </pages> <publisher> MIT Press, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: In order to support this pattern of execution we require a minimal support for control and-parallelism. In particular, independent and-parallelism (like the one supported by systems like &ACE [5] and &-Prolog <ref> [7] </ref> and ) is sufficient if the recursive call is independent from the outcome of the processing of one element of the recursive structure, otherwise a form of dependent and-parallelism is required (like DDAS [17]). <p> Conventionally, an and-parallel Prolog system works by executing a program that has been annotated with parallel conjunctions. These parallel conjunction annotations are either inserted by a parallelizing compiler <ref> [7] </ref> or hand-coded by the programmer. Execution of all goals in a parallel conjunction is started in parallel when control reaches that parallel conjunction. Whenever a parallel conjunction is encountered during execution, a data structure|the parcall frame| describing the parallel conjunction is allocated on the (control) stack. <p> In an and-parallel system we must ensure that the backtracking semantics is such that all solutions are reported. One such backtracking semantics has been proposed by Hermenegildo and Nasr <ref> [7] </ref>: consider the subgoals shown below, where `,' is used between sequential subgoals (because of data-dependencies) and `&' for parallel subgoals (no data-dependencies): a, b, (c & d & e), g, h. <p> Independent and-parallelism with the backtracking semantics described above has been implemented quite efficiently by the authors in the &ACE system. The &ACE system is built upon the SICStus (sequential) Prolog Engine which ensures good performance in the single processor case. &ACE implementation itself is inspired by the RAPWAM <ref> [7] </ref> 7 . &ACE is an extension to the sequential WAM (Warren Abstract Machine [20]) for and-parallel execution of Prolog programs with and-parallel annotation (such as CGEs [3]). The &ACE system has shown remarkable results on a variety of benchmarks. <p> On most of the commonly used benchmarks the system has shown excellent execution times and speedups|figure 4 shows the speedups obtained on some of these benchmarks 8 . For 7 Other implementations based on the principles of RAP-WAM have also been proposed in the past, like &-Prolog <ref> [7] </ref> and DDAS [17]. 8 Data produce by running the system on a Sequent Symme try Multiprocessors. 7 further details regarding the structure of &ACE and its performance the reader is referred to [6, 5].
Reference: [8] <author> M. V. Hermenegildo. </author> <title> An Abstract Machine for Restricted AND-parallel Execution of Logic Programs. </title> <booktitle> In Proc. 3rd ICLP, </booktitle> <volume> LNCS 225, </volume> <pages> pages 25-40. </pages> <publisher> Springer-Verlag, </publisher> <year> 1986. </year>
Reference-contexts: Foundations of much of the work described in this section were laid down in <ref> [3, 8] </ref>, however, the specific implementation described is that of the &ACE system. Conventionally, an and-parallel Prolog system works by executing a program that has been annotated with parallel conjunctions. These parallel conjunction annotations are either inserted by a parallelizing compiler [7] or hand-coded by the programmer.
Reference: [9] <author> H. Millroth J. Barklund. </author> <title> Providing Iteration and Concurrency in Logic Program through Bounded Quantifications. </title> <booktitle> In Proc. of the Conf. on Fifth Generation Computer Systems, </booktitle> <year> 1992. </year>
Reference-contexts: Furthermore, at least in theory, this approach is completely automatic and transparent to the user. The most significant proposals in this class are represented by the Bounded Quantification approach <ref> [9] </ref> and the Reform Prolog system [10]. The Bounded Quantifications approach is based on the idea of supplying to the user a way of directly expressing iterations through the use of bounded quantifications (i.e., quantifications over a specified finite range).
Reference: [10] <author> H. Millroth J. Bevemyr, T. Lindgren. </author> <title> Reform Prolog: the Language and its Implementation. </title> <booktitle> In Proc. of the 10th Int'l Conference on Logic Programming. </booktitle> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: They are analogs of the last call optimization found in modern sequential Prolog systems. LPCO and NPCO allow &ACE to execute SPMD programs with the efficiency of dedicated SPMD parallel logic programming systems such as Reform Prolog <ref> [10] </ref>. The efficiency of &ACE (augmented with LPCO and NPCO) for programs containing SPMD parallelism is demonstrated by presented performance figures. Thus, our research shows that there is no need of an specialized parallel implementations to take the advantage of data-parallelism present in Prolog applications. <p> Section 4 presents runtime implementation techniques, called Last Parallel Call Optimization (LPCO) and Nested Parallel Call Optimization (NPCO), that allow &ACE to execute SPMD programs with the efficiency of dedicated SPMD parallel logic programming systems such as Reform Pro-log <ref> [10] </ref>. <p> The exploitation of this form of parallelism, where all the iterations of the recursion are executed by different parallel threads, has also been named Recursion Parallelism in the literature <ref> [10] </ref>. Two main cases can be identified regarding direct exploitation of data-parallelism from recursive programs: 1. Deterministic Programs: if the iterations of the recursion are deterministic, i.e. they do offer at most one solution, then the problem is relatively simpler. <p> Furthermore, at least in theory, this approach is completely automatic and transparent to the user. The most significant proposals in this class are represented by the Bounded Quantification approach [9] and the Reform Prolog system <ref> [10] </ref>. The Bounded Quantifications approach is based on the idea of supplying to the user a way of directly expressing iterations through the use of bounded quantifications (i.e., quantifications over a specified finite range).
Reference: [11] <editor> L. Kanal and C.B. Suttner, editors. </editor> <booktitle> Proceedings of the Workshop on Parallel Processing for AI. </booktitle> <institution> Technische Universitat Munchen, Institut fur In-formatik, </institution> <year> 1992. </year>
Reference-contexts: There are parallelizing compilers and other tools available for parallel numerical processing that programmers can use without being experts on parallel processing. However, the same is not true of parallel processing of Symbolic and Artificial Intelligence (AI) applications. There has been some work <ref> [11, 19] </ref>, but there are still no systems that are general enough and that programmers can use without being experts on parallel processing. &ACE [5] is a a system that is a step towards the goal of building a general automatic tool for parallelizing symbolic and AI applications. &ACE is based
Reference: [12] <author> L. Lamport. </author> <title> The Parallel Execution of DO-loops. </title> <journal> Communications of the ACM, </journal> <volume> 17 </volume> <pages> 83-93, </pages> <year> 1974. </year>
Reference-contexts: 1 Introduction Parallelism and Logic Programming Technology Parallel processing of numerical problems has been an active area of research for almost two decades now <ref> [12, 22, 23] </ref>. There are parallelizing compilers and other tools available for parallel numerical processing that programmers can use without being experts on parallel processing. However, the same is not true of parallel processing of Symbolic and Artificial Intelligence (AI) applications.
Reference: [13] <author> E. Pontelli and G. Gupta. </author> <title> Nested Parallel Call Optimization. </title> <type> Technical report, </type> <institution> New Mexico State University, </institution> <year> 1994. </year>
Reference-contexts: The introduction of these extensions allows to extend the applicability of this model to a considerable larger set of programs. This extended form of Data-parallelism management has been named Nested Parallel Call Optimization (NPCO) and it has been already partially implemented in &ACE <ref> [13] </ref>. 5 Related Work and Conclusions Ramkumar's distributed last call optimization and adaptations of last call optimization to committed choice languages are two efforts in a direction similar to LPCO.
Reference: [14] <author> E. Pontelli, G. Gupta, and D. Tang. </author> <title> Determi-nacy Driven Optimizations of Parallel Prolog Implementations. </title> <booktitle> In Proc. of the Int'l Conference on Logic Programming 95. </booktitle> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: The LPCO illustrates an important principle of optimization of parallel systems, termed the reduced nesting principle <ref> [14] </ref> that states: The level of nesting of control structures in a computation should be reduced whenever possible. 4.2 Implementation of LPCO To implement LPCO, the compiler will generate a different instruction when it sees a parallel conjunct at the end of a clause.
Reference: [15] <author> B. Ramkumar. </author> <title> Distributed Last Call Optimization for Portable Parallel Prolog Systems. </title> <journal> ACM Letters on Prog. Languages and Systems, </journal> <volume> 3(1), </volume> <year> 1992. </year>
Reference-contexts: Neither one is motivated by a desire to exploit data-parallelism efficiently. * Ramkumar's distributed last call optimization: The only work that one could think of as coming close is that of Ramkumar <ref> [15] </ref>. In it he describes what he calls distributed last call optimization, designed for his and Kale's ROPM system [16]. This optimization is specific to process based systems (like ROPM) and its main objective is to reduce the message flow between goals during parallel executions.
Reference: [16] <author> B. Ramkumar and L. V. Kale. </author> <title> Compiled Execution of the Reduce-OR Process Model on Multiprocessors. </title> <booktitle> In Proc. NACLP'89, </booktitle> <pages> pages 313-331. </pages> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: In it he describes what he calls distributed last call optimization, designed for his and Kale's ROPM system <ref> [16] </ref>. This optimization is specific to process based systems (like ROPM) and its main objective is to reduce the message flow between goals during parallel executions.
Reference: [17] <author> K. Shen. </author> <title> Studies in And/Or Parallelism in Pro-log. </title> <type> PhD thesis, </type> <institution> U. of Cambridge, </institution> <year> 1992. </year>
Reference-contexts: In particular, independent and-parallelism (like the one supported by systems like &ACE [5] and &-Prolog [7] and ) is sufficient if the recursive call is independent from the outcome of the processing of one element of the recursive structure, otherwise a form of dependent and-parallelism is required (like DDAS <ref> [17] </ref>). Nevertheless, the use of pure control parallelism to exploit data-parallelism leads to some undesired side-effects, in terms of space and time overheads. <p> For 7 Other implementations based on the principles of RAP-WAM have also been proposed in the past, like &-Prolog [7] and DDAS <ref> [17] </ref>. 8 Data produce by running the system on a Sequent Symme try Multiprocessors. 7 further details regarding the structure of &ACE and its performance the reader is referred to [6, 5].
Reference: [18] <author> K. Shen and M. Hermenegildo. </author> <title> A Flexible Stack Memory Management Scheme for NonDeterministic, And-parallel Execution of Logic Programs. </title> <type> Technical report, </type> <institution> Facultad de Infor-matica, </institution> <address> U. P. Madrid, 28660 Boadilla del Monte, Madrid, Spain, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: The experimental tests that we have performed consist of running various benchmarks, measuring the execution time and the corresponding memory consumption (a factor that has been proven to be of very im 11 portant for parallel logic programming systems <ref> [18] </ref>). In particular we selected the benchmarks in order to separately study the effects of the LPCO on programs whose execution is pure forward execution (i.e. no backtracking); mainly backward execution. The benchmarks adopted are mainly classical benchmarks commonly used in parallel logic programming.
Reference: [19] <editor> L.N. Kanal V. Kumar, P.S. Gopalakrishnan, editor. </editor> <booktitle> Parallel Algorithms for Machine Intelligence and Vision. </booktitle> <publisher> Springer Verlag, </publisher> <year> 1990. </year>
Reference-contexts: There are parallelizing compilers and other tools available for parallel numerical processing that programmers can use without being experts on parallel processing. However, the same is not true of parallel processing of Symbolic and Artificial Intelligence (AI) applications. There has been some work <ref> [11, 19] </ref>, but there are still no systems that are general enough and that programmers can use without being experts on parallel processing. &ACE [5] is a a system that is a step towards the goal of building a general automatic tool for parallelizing symbolic and AI applications. &ACE is based
Reference: [20] <author> D. H. D. Warren. </author> <title> An Abstract Prolog Instruction Set. </title> <type> Technical Report 309, </type> <institution> Artificial Intelligence Center, SRI International, </institution> <address> 333 Ravenswood Ave, Menlo Park CA 94025, </address> <year> 1983. </year>
Reference-contexts: The &ACE system is built upon the SICStus (sequential) Prolog Engine which ensures good performance in the single processor case. &ACE implementation itself is inspired by the RAPWAM [7] 7 . &ACE is an extension to the sequential WAM (Warren Abstract Machine <ref> [20] </ref>) for and-parallel execution of Prolog programs with and-parallel annotation (such as CGEs [3]). The &ACE system has shown remarkable results on a variety of benchmarks.
Reference: [21] <author> D.H.D. Warren. </author> <title> An Improved Prolog Implementation Which Optimises Tail Recursion. </title> <booktitle> In Proc. of the 2nd ICLP. </booktitle> <publisher> Academic Press, </publisher> <year> 1984. </year>
Reference-contexts: The advantages of LPCO are very similar to those for last call optimization <ref> [21] </ref> in the WAM. The conditions under which the LPCO applies are also very similar to those under which last call optimization is applicable in sequential systems.
Reference: [22] <author> M. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: 1 Introduction Parallelism and Logic Programming Technology Parallel processing of numerical problems has been an active area of research for almost two decades now <ref> [12, 22, 23] </ref>. There are parallelizing compilers and other tools available for parallel numerical processing that programmers can use without being experts on parallel processing. However, the same is not true of parallel processing of Symbolic and Artificial Intelligence (AI) applications.
Reference: [23] <author> H. Zima and B. Chapman. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> ACM Press, </publisher> <year> 1991. </year>
Reference-contexts: 1 Introduction Parallelism and Logic Programming Technology Parallel processing of numerical problems has been an active area of research for almost two decades now <ref> [12, 22, 23] </ref>. There are parallelizing compilers and other tools available for parallel numerical processing that programmers can use without being experts on parallel processing. However, the same is not true of parallel processing of Symbolic and Artificial Intelligence (AI) applications. <p> Deterministic Programs: if the iterations of the recursion are deterministic, i.e. they do offer at most one solution, then the problem is relatively simpler. It can be seen as analogous to the problem of automatically parallelizing loops in imperative programming languages, like FORTRAN <ref> [23] </ref>. 2.
References-found: 23

