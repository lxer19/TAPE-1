URL: http://www.cs.colostate.edu/~ftppub/TechReports/1996/tr96-105.ps.Z
Refering-URL: http://www.cs.colostate.edu/~vision/html/publications.html
Root-URL: 
Phone: Phone: (970) 491-5792 Fax: (970) 491-2466  
Title: Toward Target Verification Through 3-D Model-Based Sensor Fusion  
Author: J. Ross Beveridge, Mark R. Stevens Anthony N. A. Schwickerath 
Note: This work was sponsored by the Advanced Research Projects Agency (ARPA) under grants DAAH04-93-G-422 and DAAH04-95-1-0447, monitored by the U. S. Army Research Office.  
Web: WWW: http://www.cs.colostate.edu  
Address: Fort Collins, CO 80523-1873  
Affiliation: Computer Science Department Colorado State University  
Date: February 2, 1996  
Pubnum: Technical Report CS-96-105  
Abstract: Computer Science Technical Report 
Abstract-found: 1
Intro-found: 1
Reference: [ Agg90 ] <author> J. K. Aggarwal. </author> <title> Multisensor Fusion for Automatic Scene Interpretation. </title> <editor> In Ramesh C. Jain and Anil K. Jain, editors, </editor> <title> Analysis and Interpretation of Range Images, chapter 8. </title> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: In future work, we will test the relative merits of our approach compared to the more general formulation of Hel-Or and Werman as well as more traditional photogrammetric approaches. Aggarwal <ref> [ Agg90 ] </ref> summarizes past sensor fusion work and makes two points particularly relevant to this paper. Aggarwal notes that past work on sensor fusion emphasized single modality sensors, with comparatively little work on different sensor modalities. The implied explanation is that relating data from different modalities is more difficult.
Reference: [ BC82 ] <author> R. C. Bolles and R. A. Cain. </author> <title> Recognizing and Locating Partially Visible Objects: The Local-Feature-Focus Method. </title> <journal> International Journal of Robotics Research, </journal> <volume> 1(3):57 - 82, </volume> <year> 1982. </year>
Reference: [ BDHR94 ] <author> Shashi Buluswar, Bruce A. Draper, Allen Hanson, and Edward Riseman. </author> <title> Non-parametric Classification of Pixels Under Varying Outdoor Illumination. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 1619-1626, </pages> <address> Los Altos, CA, </address> <month> November </month> <year> 1994. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This more geometrically precise 3D framework for ATR promises more accurate match measures, and consequently, more robust target identification. Since the associated computational demands are considerable, less expensive detection <ref> [ BDHR94 ] </ref> and hypothesis generation [ Bev92b ] algorithms are being used to provide focus of attention. Thus, it is assumed that the coregistration algorithms are being asked to rank and resolve quite specific hypotheses generated by up-stream processing.
Reference: [ Bev92a ] <author> J. Ross Beveridge. </author> <title> Comparing Subset-convergent and Variable-depth Local Search on Perspective Sensitive Landmark Recognition Problems. </title> <booktitle> In Proceedings: SPIE Intelligent Robots and Computer Vision XI: Algorithms, Techniques, and Active Vision, </booktitle> <volume> volume 1825, </volume> <pages> pages 168 - 179. SPIE, </pages> <month> November </month> <year> 1992. </year>
Reference: [ Bev92b ] <author> James E. Bevington. </author> <title> Laser Radar ATR Algorithms: Phase III Final Report. </title> <type> Technical report, </type> <institution> Alliant Techsystems, Inc., </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: This more geometrically precise 3D framework for ATR promises more accurate match measures, and consequently, more robust target identification. Since the associated computational demands are considerable, less expensive detection [ BDHR94 ] and hypothesis generation <ref> [ Bev92b ] </ref> algorithms are being used to provide focus of attention. Thus, it is assumed that the coregistration algorithms are being asked to rank and resolve quite specific hypotheses generated by up-stream processing. <p> The overall goal is to utilize these three sensor modalities to improve the reliability of target identification algorithms. Most of the prior work on target identification uses a fixed set of image-based templates or probes <ref> [ Bev92b; VDL95; DVD93 ] </ref> . These techniques have their place, and we are using the Alliant Techsystems probing algorithms [ Bev92b ] to provide initial object-and-pose hypotheses. <p> Most of the prior work on target identification uses a fixed set of image-based templates or probes [ Bev92b; VDL95; DVD93 ] . These techniques have their place, and we are using the Alliant Techsystems probing algorithms <ref> [ Bev92b ] </ref> to provide initial object-and-pose hypotheses. However, as the final step in verification, unless precise 3D coregistration is performed 1 , it is neither possible to perform final cross-sensor model alignment nor is it possible to exploit precise 3D geometric relationships.
Reference: [ Bev93 ] <author> J. Ross Beveridge. </author> <title> Local Search Algorithms for Geometric Obejct Recognition: Optimal Correspondence and Pose. </title> <type> PhD thesis, </type> <institution> University of Massachuesetts at Amherst, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: Huttenlocher [ HU90 ] demonstrated alignment-based recognition under orthographic projection. Grimson's work [ Gri90 ] on constraint base matching has emphasized local feature compatibility to prune tree search and thus minimize tests of global alignment. Our past work <ref> [ BWR90; BWR91; Bev93; BR95 ] </ref> emphasized global alignment as a basis for match ranking and optimal matching to CCD sensor data. Local search through the match space constructs a globally consistent match by finding a sequence of successively better match hypotheses until one which is locally optimal is found. <p> For a given match hypothesis, the fit error is based upon alignment using Kumar's [ Kum89; KH94 ] pose algorithm. Random trials of steepest-descent local search on CCD matching problems demonstrate that the technique finds, with high probability, matches which are both globally optimal and globally consistent <ref> [ Bev93; BR95 ] </ref> . To extend this type of globally consistent matching for multisensor recognition, a global alignment or coregistration algorithm is needed. As an initial step, we investigate a combination of proven single sensor error formulations. <p> These bottom-up feature extraction algorithms are prone to error [ Cla89; BGK + 89 ] , and often produce extraneous line segments, 9 fragmented segments, and sometimes over-grouped segments. Our past work in other problem domains <ref> [ Bev93 ] </ref> demonstrated local search, coupled with sound and efficient tests of global alignment, could overcome significant amounts of fragmentation, over-grouping and clutter. <p> E match;sensor (c) = E fit;sensor (c) + E om;sensor (c) (27) 25 We have made extensive use of this fit-plus-omission formulation in prior work on matching to optical image features <ref> [ Bev93 ] </ref> . The term E fit;sensor measures how well the features fit. Here it is defined to be a function of the residual error from the coregistration process. <p> Search stops when no further progress can be made. This constitutes a local search algorithm in correspondence space. For matching problems using line segments derived from models and optical images, we've shown this to be a general and effective technique <ref> [ Bev93 ] </ref> . A first pass algorithm for minimizing the error defined in equation 27 has been implemented. This algorithm is highly restricted: pairs of features may be removed but not added. Not surprisingly, this restriction limits the algorithms usefulness.
Reference: [ BGK + 89 ] <author> J. Ross Beveridge, Joey Griffith, Ralf R. Kohler, Allen R. Hanson, and Edward M. Riseman. </author> <title> Segmenting images using localized histograms and region merging. </title> <journal> International Journal of Computer Vision, </journal> <volume> 2(3):311 - 347, </volume> <month> January </month> <year> 1989. </year>
Reference-contexts: These linear features are, in turn, matched to linear features of stored object models [ Low85; Low91; HU87; HU88; HU90; GH91; BWR89; BWR90; BWR91; BR92a; BR92b; Bev92a; Bev93; BR94; BHP94b; BR95 ] . These bottom-up feature extraction algorithms are prone to error <ref> [ Cla89; BGK + 89 ] </ref> , and often produce extraneous line segments, 9 fragmented segments, and sometimes over-grouped segments.
Reference: [ BHP94a ] <author> J. Ross Beveridge, Allen Hanson, and Durga Panda. </author> <title> Integrated color ccd, flir & ladar based object modeling and recognition. </title> <type> Technical report, </type> <institution> Colorado State University and Alliant Techsystems and University of Massachusetts, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: Projection is possible because both the intrinsic sensor parameters and the approximate pose of the target are known. The parameters for the color sensor have been determined off-line using calibration targets <ref> [ BHP94a ] </ref> . Thus, for each silhouette feature generated from the target model, a search is initiated in the image for the best corresponding line segment. The first step in this search is to construct a gradient mask tuned to the particular expected orientation of each silhouette edge.
Reference: [ BHP94b ] <author> J. Ross Beveridge, Allen Hanson, and Durga Panda. </author> <title> RSTA Research of the Colorado State, University of Massachusetts and Alliant Techsystems Team. In Image Understanding Workshop (separate addendum), </title> <note> page (to appear). ARPA, </note> <month> November </month> <year> 1994. </year>
Reference: [ BHR86 ] <author> J. Brian Burns, Allen R. Hanson, and Edward M. Riseman. </author> <title> Extracting Straight Lines. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-8(4):425-455, </volume> <month> July </month> <year> 1986. </year>
Reference-contexts: dealt with later when matching the model features to the sensor features. 4.2 Model-Directed Linear Feature Detection Traditional methods for locating objects in optical imagery typically use edge detection algorithms. [ MH80; Hil83 ] locate local edges which may then be grouped into larger features such as straight line segments <ref> [ BHR86; LB83 ] </ref> . These linear features are, in turn, matched to linear features of stored object models [ Low85; Low91; HU87; HU88; HU90; GH91; BWR89; BWR90; BWR91; BR92a; BR92b; Bev92a; Bev93; BR94; BHP94b; BR95 ] . <p> Our past work in other problem domains [ Bev93 ] demonstrated local search, coupled with sound and efficient tests of global alignment, could overcome significant amounts of fragmentation, over-grouping and clutter. However, the features produced by the Burns algorithm <ref> [ BHR86 ] </ref> on the Fort Carson imagery are of such poor quality that we have adopted a top-down rather than a bottom-up approach in the work developed here.
Reference: [ BJLP92 ] <author> James Bevington, Randy Johnston, Joel Lee, and Richard Peters. </author> <title> A modular target recognition algorithm for ladar. </title> <booktitle> In Proc of the 2nd Automatic Target Recognizer Systems and Technology Conference, </booktitle> <pages> pages 91 - 104, </pages> <address> Fort Belvoir, VA, </address> <month> mar </month> <year> 1992. </year>
Reference-contexts: While model-based approaches to Automatic Target Recognition have become much more common [ DVD93; GJSL90 ] , direct incorporation of alignment into the recognition process is rare <ref> [ BJLP92 ] </ref> . This paper presents algorithms and results from a project at Colorado State University which is developing a new family of Target Verification algorithms based upon alignment. Recognition problems in ATR typically involve complex objects viewed at great distances.
Reference: [ BPY94 ] <author> J. Ross Beveridge, Durga P. Panda, and Theodore Yachik. </author> <title> November 1993 Fort Carson RSTA Data Collection Final Report. </title> <type> Technical Report CSS-94-118, </type> <institution> Colorado State University, </institution> <address> Fort Collins, CO, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: To fill in while no LADAR sensor is on the vehicle, we went to Fort Carson with Lockheed Martin in the Fall of 1993 and collected a large set of range, color and IR imagery <ref> [ BPY94 ] </ref> , and it is this data which is considered here. The overall goal is to utilize these three sensor modalities to improve the reliability of target identification algorithms. <p> In addition, the objects being recognized are generally at a significant distance, and hence do not cover much of the image. This results in lower quality and less data. From the set of hundreds of images collected at Fort Carson <ref> [ BPY94 ] </ref> , we selected three pairs of range and color. These were selected to represent some different qualities of the ATR domain. Shots 18 and 20 represent relatively close examples (approximately 50 meters) of two different vehicles, the M60 and M113, respectively.
Reference: [ BR92a ] <author> J. Ross Beveridge and Edward M. Riseman. </author> <title> Can Too Much Perspective Spoil the View? A Case Study in 2D Affine Versus 3D Perspective Model Matching. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 665 - 663, </pages> <address> San Mateo, CA, January 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ BR92b ] <author> J. Ross Beveridge and Edward M. Riseman. </author> <title> Hybrid Weak-Perspective and Full-Perspective Matching. </title> <booktitle> In Proceedings: IEEE 1992 Computer Society Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 432 - 438. </pages> <publisher> IEEE Computer Society, </publisher> <month> June </month> <year> 1992. </year> <month> 37 </month>
Reference-contexts: Unsuccessful termination occurs if the total number of iterations exceeds a maximum number of iterations. The Levenberg-Marquardt [ PFTV88 ] method has been found to be robust in our past single sensor pose work <ref> [ BR92b; BR94 ] </ref> , and it is used here to find the optimal coregistration parameters. 16 @E fit P n o P 2 mij fi ^ N oi ffi~! mo + ^ N oi ~ T mo ~ P e P n r moi ffi~! mo + M moi ~
Reference: [ BR94 ] <author> J. Ross Beveridge and Edward M. Riseman. </author> <title> Optimal Geometric Model Matching Under Full 3D Perspective. </title> <booktitle> In Second CAD-Based Vision Workshop, </booktitle> <pages> pages 54 - 63. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> February </month> <year> 1994. </year> <note> (Submitted to CVGIP-IU). </note>
Reference-contexts: Unsuccessful termination occurs if the total number of iterations exceeds a maximum number of iterations. The Levenberg-Marquardt [ PFTV88 ] method has been found to be robust in our past single sensor pose work <ref> [ BR92b; BR94 ] </ref> , and it is used here to find the optimal coregistration parameters. 16 @E fit P n o P 2 mij fi ^ N oi ffi~! mo + ^ N oi ~ T mo ~ P e P n r moi ffi~! mo + M moi ~
Reference: [ BR95 ] <author> J. Ross Beveridge and Edward M. Riseman. </author> <title> Optimal Geometric Model Matching Under Full 3D Perspective. Computer Vision and Image Understanding, </title> <note> 61(3):351 - 364, 1995. (short version in IEEE Second CAD-Based Vision Workshop). </note>
Reference-contexts: Huttenlocher [ HU90 ] demonstrated alignment-based recognition under orthographic projection. Grimson's work [ Gri90 ] on constraint base matching has emphasized local feature compatibility to prune tree search and thus minimize tests of global alignment. Our past work <ref> [ BWR90; BWR91; Bev93; BR95 ] </ref> emphasized global alignment as a basis for match ranking and optimal matching to CCD sensor data. Local search through the match space constructs a globally consistent match by finding a sequence of successively better match hypotheses until one which is locally optimal is found. <p> For a given match hypothesis, the fit error is based upon alignment using Kumar's [ Kum89; KH94 ] pose algorithm. Random trials of steepest-descent local search on CCD matching problems demonstrate that the technique finds, with high probability, matches which are both globally optimal and globally consistent <ref> [ Bev93; BR95 ] </ref> . To extend this type of globally consistent matching for multisensor recognition, a global alignment or coregistration algorithm is needed. As an initial step, we investigate a combination of proven single sensor error formulations.
Reference: [ BWR89 ] <author> J. Ross Beveridge, Rich Weiss, and Edward M. Riseman. </author> <title> Optimization of 2-dimensional model matching. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 815 - 830, </pages> <address> Los Altos, CA, </address> <month> June </month> <year> 1989. </year> <title> DARPA, </title> <publisher> Morgan Kaufmann Publishers, Inc (Also a Tech. Report). </publisher>
Reference-contexts: Kumar subsequently showed this measure to be sensitive to fragmentation noise in the image segments. To correct this weakness, Kumar subsequently developed a measure using 3D model features to define planes. We earlier observed an analogous result for 2D line fitting <ref> [ BWR89 ] </ref> . While Kumar's later formulation is better when fragmentation occurs, we avoided it for two reasons. First, to use the more recent measure requires an additional renormalization of terms after each iteration.
Reference: [ BWR90 ] <author> J. Ross Beveridge, Rich Weiss, and Edward M. Riseman. </author> <title> Combinatorial Optimization Applied to Variable Scale 2D Model Matching. </title> <booktitle> In Proceedings of the IEEE International Conference on Pattern Recognition 1990, Atlantic City, </booktitle> <pages> pages 18 - 23. </pages> <publisher> IEEE, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: Huttenlocher [ HU90 ] demonstrated alignment-based recognition under orthographic projection. Grimson's work [ Gri90 ] on constraint base matching has emphasized local feature compatibility to prune tree search and thus minimize tests of global alignment. Our past work <ref> [ BWR90; BWR91; Bev93; BR95 ] </ref> emphasized global alignment as a basis for match ranking and optimal matching to CCD sensor data. Local search through the match space constructs a globally consistent match by finding a sequence of successively better match hypotheses until one which is locally optimal is found.
Reference: [ BWR91 ] <author> J. Ross Beveridge, Rich Weiss, and Edward M. Riseman. </author> <title> Optimization of 2-Dimensional Model Matching. </title> <editor> In Hatem Nasr, editor, </editor> <booktitle> Selected Papers on Automatic Object Recognition (originally appeared in DARPA Image Understanding Workshop, 1989), SPIE Milestone Series. SPIE, </booktitle> <address> Bellingham, WA, </address> <year> 1991. </year>
Reference-contexts: Huttenlocher [ HU90 ] demonstrated alignment-based recognition under orthographic projection. Grimson's work [ Gri90 ] on constraint base matching has emphasized local feature compatibility to prune tree search and thus minimize tests of global alignment. Our past work <ref> [ BWR90; BWR91; Bev93; BR95 ] </ref> emphasized global alignment as a basis for match ranking and optimal matching to CCD sensor data. Local search through the match space constructs a globally consistent match by finding a sequence of successively better match hypotheses until one which is locally optimal is found.
Reference: [ Can86 ] <author> John Canny. </author> <title> A computational approach to edge detection. </title> <journal> PAMI, </journal> <volume> 8(6) </volume> <pages> 679-698, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: To realize a robust, model-driven, edge detection capability, two ideas from the literature have been combined: model-driven edge detection [ FL87; FL88 ] and directionally tuned gradient filters <ref> [ Can86 ] </ref> . The quality of a straight line segment denoting an extended edge is defined to be a function of the gradient magnitude under that edge. A gradient mask tuned to the specific expected orientation of the segment is used. <p> This mask is formed by rotating the first derivative of a Gaussian to match the orientation of the current silhouette edge. There are many precedents for tuned edge masks including Canny <ref> [ Can86 ] </ref> and Torres [ TP86 ] . Others to develop and use such masks for bottom-up edge detection include [ Shu94; FA91 ] . An example of such a filter, displayed as an image, is shown in Figure 4c.
Reference: [ Cla89 ] <author> James J. Clark. </author> <title> Authenticating Edges Produced by Zero-Crossing Algorithms. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-11(1):43-57, </volume> <month> January </month> <year> 1989. </year>
Reference-contexts: These linear features are, in turn, matched to linear features of stored object models [ Low85; Low91; HU87; HU88; HU90; GH91; BWR89; BWR90; BWR91; BR92a; BR92b; Bev92a; Bev93; BR94; BHP94b; BR95 ] . These bottom-up feature extraction algorithms are prone to error <ref> [ Cla89; BGK + 89 ] </ref> , and often produce extraneous line segments, 9 fragmented segments, and sometimes over-grouped segments.
Reference: [ DVD93 ] <author> Richard L. Delanoy, Jacques G. Verly, and Dan E. Dudgeon. </author> <title> Machine Intelligent Automatic Recognition of Critical Mobile Targets in Laser Radar Imagery. </title> <journal> The Lincoln Laboratory Journal, </journal> <volume> 6(1) </volume> <pages> 161-186, </pages> <month> Spring </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Model-based object recognition work has long emphasized the importance of aligning 3D object models to features extracted from sensed imagery [ BC82; Low91; GH91; HU90; HU88; BR92a; BR92b; Bev92a; Bev93; BR94; BHP94b; BR95 ] . While model-based approaches to Automatic Target Recognition have become much more common <ref> [ DVD93; GJSL90 ] </ref> , direct incorporation of alignment into the recognition process is rare [ BJLP92 ] . This paper presents algorithms and results from a project at Colorado State University which is developing a new family of Target Verification algorithms based upon alignment. <p> The overall goal is to utilize these three sensor modalities to improve the reliability of target identification algorithms. Most of the prior work on target identification uses a fixed set of image-based templates or probes <ref> [ Bev92b; VDL95; DVD93 ] </ref> . These techniques have their place, and we are using the Alliant Techsystems probing algorithms [ Bev92b ] to provide initial object-and-pose hypotheses.
Reference: [ EG92 ] <author> R. O. Eason and R. C. Gonzalez. </author> <title> Least-Squares Fusion of Multisensory Data. </title> <editor> In Mongi A. Abidi and Rafael C. Gonzalez, editors, </editor> <booktitle> Data Fusion in Robotics and Machine Intelligence, chapter 9. </booktitle> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: Herbert [ HKK90 ] presents least-squares mechanism for computing the rigid transform between range and color CCD sensors based upon corresponding image points in the two sensor images. However, this work does not explicitly recover the associated pose of the 3D points relative to the sensors. Both Eason <ref> [ EG92 ] </ref> and Hel-Or [ HOW93 ] developed least-squares multisensor pose algorithms. However, these algorithms only solve for the six-degrees-of-freedom pose estimate. 3 They do not support simultaneous adjustment of the sensor registration parameters. In terms of constraints, all of these methods assume a known sensor-to-sensor registration.
Reference: [ FA91 ] <author> William T. Freeman and Edward H. Adelson. </author> <title> The Design and Use of Steerable Filters. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-13(9):891-906, </volume> <month> September </month> <year> 1991. </year>
Reference-contexts: There are many precedents for tuned edge masks including Canny [ Can86 ] and Torres [ TP86 ] . Others to develop and use such masks for bottom-up edge detection include <ref> [ Shu94; FA91 ] </ref> . An example of such a filter, displayed as an image, is shown in Figure 4c. The second step is to measure the overall gradient response under the segment.
Reference: [ FL87 ] <author> Pascal Fua and Yvan G. Leclerc. </author> <title> Finding Object Boundaries Using Guided Gradient Ascent. </title> <booktitle> In Proceedings: Image Understanding Workshop - 1987, </booktitle> <pages> pages 888-891. </pages> <publisher> DARPA, Morgan Kaufmann, </publisher> <month> February </month> <year> 1987. </year>
Reference-contexts: To realize a robust, model-driven, edge detection capability, two ideas from the literature have been combined: model-driven edge detection <ref> [ FL87; FL88 ] </ref> and directionally tuned gradient filters [ Can86 ] . The quality of a straight line segment denoting an extended edge is defined to be a function of the gradient magnitude under that edge.
Reference: [ FL88 ] <author> Pascal Fua and Yvan G. Leclerc. </author> <title> Model Driven Edge Detection. </title> <booktitle> In Proceedings: Image Understanding Workshop - 1988, </booktitle> <pages> pages 1016-1020. </pages> <publisher> DARPA, Morgan Kaufmann, </publisher> <month> April </month> <year> 1988. </year>
Reference-contexts: To realize a robust, model-driven, edge detection capability, two ideas from the literature have been combined: model-driven edge detection <ref> [ FL87; FL88 ] </ref> and directionally tuned gradient filters [ Can86 ] . The quality of a straight line segment denoting an extended edge is defined to be a function of the gradient magnitude under that edge.
Reference: [ GBSF94 ] <author> Michael E. Goss, J. Ross Beveridge, Mark R. Stevens, and Aaron Fuegi. </author> <title> Visualization and Verification of Automatic Target Recognition Results Using Combined Range and Optical Imagery. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 491-494. ARPA, </pages> <month> nov </month> <year> 1994. </year>
Reference-contexts: The LADAR imagery is drawn as a set of 3D polygons at range sample depth. The visualization algorithm allows the 3D data to be rendered interactively from a different viewpoints, and provides a better feel for the characteristics of the data than the typical 2D range intensity image <ref> [ GBSF94; GBSF95 ] </ref> . 4 Deriving Target Model and Sensor Features 4.1 Dynamic Model Feature Generation Highly detailed models of the vehicles in our Fort Carson dataset exist in the CAD model format known as BRL/CAD [ U. 91 ] .
Reference: [ GBSF95 ] <author> Michael E. Goss, J. Ross Beveridge, Mark R. Stevens, and Aaron D. Fuegi. </author> <title> Three-dimensional visualization environment for multi-sensor data analysis, interpretation, and model-based object recognition. </title> <editor> In Georges G. Grinstein and Robert F. Erbacher, editors, </editor> <booktitle> Proceedings: Visual Data Exploration and Analysis II, </booktitle> <pages> pages 283-291. </pages> <booktitle> SPIE Vol. </booktitle> <volume> 2410, </volume> <month> feb </month> <year> 1995. </year>
Reference-contexts: The LADAR imagery is drawn as a set of 3D polygons at range sample depth. The visualization algorithm allows the 3D data to be rendered interactively from a different viewpoints, and provides a better feel for the characteristics of the data than the typical 2D range intensity image <ref> [ GBSF94; GBSF95 ] </ref> . 4 Deriving Target Model and Sensor Features 4.1 Dynamic Model Feature Generation Highly detailed models of the vehicles in our Fort Carson dataset exist in the CAD model format known as BRL/CAD [ U. 91 ] .
Reference: [ GH91 ] <author> W. Eric L. Grimson and Daniel P. Huttenlocher. </author> <title> On the Verification of Hypothesized Matches in Model-Based Recognition. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(12):1201 - 1213, </volume> <month> December </month> <year> 1991. </year>
Reference: [ GJSL90 ] <author> Jeffrey Gillberg, Randy Johnston, Kris Siejko, and Joel Lee. </author> <title> Laser Radar ATR Algorithms. </title> <type> Technical Report DAABO7-87-C-F109, </type> <institution> Honeywell Systems and Research Center, Minnesota, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Model-based object recognition work has long emphasized the importance of aligning 3D object models to features extracted from sensed imagery [ BC82; Low91; GH91; HU90; HU88; BR92a; BR92b; Bev92a; Bev93; BR94; BHP94b; BR95 ] . While model-based approaches to Automatic Target Recognition have become much more common <ref> [ DVD93; GJSL90 ] </ref> , direct incorporation of alignment into the recognition process is rare [ BJLP92 ] . This paper presents algorithms and results from a project at Colorado State University which is developing a new family of Target Verification algorithms based upon alignment.
Reference: [ Gri90 ] <author> W. Eric L. </author> <title> Grimson. Object Recognition by Computer: The Role of Geometric Constraints. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: For example, Lowe utilized this approach subject to orthographic projection [ LB85 ] and later demonstrated object tracking under perspective projection [ Low91 ] . Huttenlocher [ HU90 ] demonstrated alignment-based recognition under orthographic projection. Grimson's work <ref> [ Gri90 ] </ref> on constraint base matching has emphasized local feature compatibility to prune tree search and thus minimize tests of global alignment. Our past work [ BWR90; BWR91; Bev93; BR95 ] emphasized global alignment as a basis for match ranking and optimal matching to CCD sensor data.
Reference: [ Hil83 ] <author> Ellen C. Hildreth. </author> <title> The Detection of Intensity Changes by Computer and Biological Vision Systems. Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> 22 </volume> <pages> 1-27, </pages> <year> 1983. </year>
Reference-contexts: Noise is dealt with later when matching the model features to the sensor features. 4.2 Model-Directed Linear Feature Detection Traditional methods for locating objects in optical imagery typically use edge detection algorithms. <ref> [ MH80; Hil83 ] </ref> locate local edges which may then be grouped into larger features such as straight line segments [ BHR86; LB83 ] .
Reference: [ HKK90 ] <author> Martial Hebert, Takeo Kanade, and InSo Kweon. </author> <title> 3-D Vision Techniques for Autonomous Vehicles. </title> <editor> In Ramesh C. Jain and Anil K. Jain, editors, </editor> <title> Analysis and Interpretation of Range Images, chapter 7. </title> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: Each of these is a least-squares algorithm based upon known correspondence mappings between features. This paper describes a joining of these two least-squares problems into a single coupled least-squares optimization problem. Others have worked on problems similar to the coregistration problem discussed in this paper. Herbert <ref> [ HKK90 ] </ref> presents least-squares mechanism for computing the rigid transform between range and color CCD sensors based upon corresponding image points in the two sensor images. However, this work does not explicitly recover the associated pose of the 3D points relative to the sensors.
Reference: [ Ho93 ] <author> Y. Hel-or. </author> <title> Model Based Pose Estimation from Uncertain Data. </title> <type> PhD thesis, </type> <institution> Hebrew Univeristy in Jerusalem, </institution> <year> 1993. </year>
Reference-contexts: However, these algorithms only solve for the six-degrees-of-freedom pose estimate. 3 They do not support simultaneous adjustment of the sensor registration parameters. In terms of constraints, all of these methods assume a known sensor-to-sensor registration. More recent work by Hel-Or and Werman <ref> [ Y. 94; Ho93 ] </ref> adds degrees of freedom to account for articulated objects and nicely handles variable constraints in a single extended Kalman filter formulation. Their general Kalman filter approach could be applied to the coregistration problem formulated here.
Reference: [ Hor86 ] <author> B. K. P. Horn. </author> <title> Robot Vision. </title> <publisher> The MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Section 6.1.1 gives a more complete description of Kumar's formulation and explains our decision to base our measure on Kumar's earlier, rather than later, error formulation. Horn <ref> [ Hor86 ] </ref> details a technique for aligning two sets of 3D points. Each of these is a least-squares algorithm based upon known correspondence mappings between features. This paper describes a joining of these two least-squares problems into a single coupled least-squares optimization problem. <p> The lack of a rotation parameter between mc mapped points and the range sensor coordinate system constrains the sensor-to-sensor orientation. Notice that these constraints retain the same degree of nonlinearity (degree 2) found in the original Kumar [ Kum89; KH94 ] and Horn <ref> [ Hor86 ] </ref> equations. This is desirable, since increasing the nonlinearity of a system of equations tends to increase the instability of the solution. Again, we use a weighted form based on a threshold t mr , which keeps us generally in the range [0; 1].
Reference: [ HOW93 ] <author> Y. Hel-Or and M. Werman. </author> <title> Absolute Orientation from Uncertain Data: A Unified Approach. </title> <booktitle> In Proceedings: Computer Vision and Pattern Recognition, </booktitle> <pages> pages 77 - 82. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> June </month> <year> 1993. </year> <month> 38 </month>
Reference-contexts: However, this work does not explicitly recover the associated pose of the 3D points relative to the sensors. Both Eason [ EG92 ] and Hel-Or <ref> [ HOW93 ] </ref> developed least-squares multisensor pose algorithms. However, these algorithms only solve for the six-degrees-of-freedom pose estimate. 3 They do not support simultaneous adjustment of the sensor registration parameters. In terms of constraints, all of these methods assume a known sensor-to-sensor registration.
Reference: [ HU87 ] <author> Daniel P. Huttenlocher and Shimon Ullman. </author> <title> Object Recognition Using Alignement. </title> <booktitle> In Proc. First International Conference on Computer Vision, </booktitle> <pages> pages 102-111, </pages> <address> London, England, June 1987. </address> <publisher> Computer Society Press of the IEEE. </publisher>
Reference: [ HU88 ] <author> Daniel P. Huttenlocher and Shimon Ullman. </author> <title> Recognizing Solid Objects by Alignment. </title> <booktitle> In Proc. of the DARPA Image Understanding Workshop, </booktitle> <pages> pages 1114 - 1124, </pages> <address> Cambridge, April 1988. </address> <publisher> Morgan Kaufman Publishers, Inc., </publisher> <address> New York. </address>
Reference: [ HU90 ] <author> Daniel P. Huttenlocher and Shimon Ullman. </author> <title> Recognizing Solid Objects by Alignment with an Image. </title> <journal> International Journal of Computer Vision, </journal> <volume> 5(2):195 - 212, </volume> <month> November </month> <year> 1990. </year>
Reference-contexts: For example, Lowe utilized this approach subject to orthographic projection [ LB85 ] and later demonstrated object tracking under perspective projection [ Low91 ] . Huttenlocher <ref> [ HU90 ] </ref> demonstrated alignment-based recognition under orthographic projection. Grimson's work [ Gri90 ] on constraint base matching has emphasized local feature compatibility to prune tree search and thus minimize tests of global alignment.
Reference: [ KD87 ] <author> Matthew R. Korn and Charles R. Dyer. </author> <title> 3D Multiview Object Representations for Model-Based Object Recognition. </title> <journal> Pattern Recognition, </journal> <volume> 20(1) </volume> <pages> 91-103, </pages> <year> 1987. </year>
Reference-contexts: Common approaches to model feature generation have centered around an off-line model analysis in which visible features are determined for all viewpoints [ PD87; Pla88 ] . The results are then grouped into regions of constant topology <ref> [ KD87 ] </ref> and stored in an aspect graph representation [ KvD76; KvD79 ] . The aspect graph is used at runtime to obtain the list of visible features for a given pose [ SD92 ] .
Reference: [ KH94 ] <author> Rakesh Kumar and Allen R. Hanson. </author> <title> Robust methods for estimating pose and a sensitivity analysis. </title> <booktitle> CVGIP:Image Understanding, </booktitle> <volume> 11, </volume> <year> 1994. </year>
Reference-contexts: Local search through the match space constructs a globally consistent match by finding a sequence of successively better match hypotheses until one which is locally optimal is found. For a given match hypothesis, the fit error is based upon alignment using Kumar's <ref> [ Kum89; KH94 ] </ref> pose algorithm. Random trials of steepest-descent local search on CCD matching problems demonstrate that the technique finds, with high probability, matches which are both globally optimal and globally consistent [ Bev93; BR95 ] . <p> To extend this type of globally consistent matching for multisensor recognition, a global alignment or coregistration algorithm is needed. As an initial step, we investigate a combination of proven single sensor error formulations. Kumar <ref> [ Kum89; KH94 ] </ref> has developed a succession of single sensor algorithms for a typical `pin-hole' camera model which takes perspective into account. Section 6.1.1 gives a more complete description of Kumar's formulation and explains our decision to base our measure on Kumar's earlier, rather than later, error formulation. <p> The range-to-model error is simply the squared Euclidean distance between corresponding features. The optical-to-model error is measured using a squared error between planes defined by image line segments and points on the 3D object model. This measure of optical-to-model error is the first measure developed by Kumar <ref> [ Kum89; KH94 ] </ref> . Kumar subsequently showed this measure to be sensitive to fragmentation noise in the image segments. To correct this weakness, Kumar subsequently developed a measure using 3D model features to define planes. We earlier observed an analogous result for 2D line fitting [ BWR89 ] . <p> The first term, E fit;o , measures distance between corresponding optical and model features. This term is precisely the point-to-plane error 13 criterion defined by Kumar <ref> [ Kum89; KH94 ] </ref> for computing camera-to-model pose. The second, E fit;r , is simply the sum-of-squared Euclidean distances between corresponding model and range points. The coregistration parameters themselves are denoted by F . <p> The weighting term oi is typically 1, but can be used 14 to bias some features over others. For example, oi can be used to weight lines based on inverse distance to perform a normalization similar to that considered in Kumar's later measures <ref> [ Kum89; KH94 ] </ref> . The weighting term 1 2n o t 2 mo i oi normalizes the E fit;o measure based upon the assumption that the image line endpoints fall within t mo of the associated model line. <p> The lack of a rotation parameter between mc mapped points and the range sensor coordinate system constrains the sensor-to-sensor orientation. Notice that these constraints retain the same degree of nonlinearity (degree 2) found in the original Kumar <ref> [ Kum89; KH94 ] </ref> and Horn [ Hor86 ] equations. This is desirable, since increasing the nonlinearity of a system of equations tends to increase the instability of the solution. <p> While the matrix terms (r mc 1;1 . . . r mc 3;3 ) could be constructed in such a way as to allow only rigid rotations, this would increase the degree of nonlinearity in the equation. Kumar <ref> [ Kum89; KH94 ] </ref> suggests a better approach: Rodriguez's formula, which is an approximation appropriate for small rotations. To rotate a point ~ P mi by an amount R mo , we decompose R mo into an estimate R e mo and a small update, ffi~! mo .
Reference: [ Kum89 ] <author> Rakesh Kumar. </author> <title> Determination of Camera Location and Orientation. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 870 - 881, </pages> <address> Los Altos, CA, </address> <month> June </month> <year> 1989. </year> <title> DARPA, </title> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: Local search through the match space constructs a globally consistent match by finding a sequence of successively better match hypotheses until one which is locally optimal is found. For a given match hypothesis, the fit error is based upon alignment using Kumar's <ref> [ Kum89; KH94 ] </ref> pose algorithm. Random trials of steepest-descent local search on CCD matching problems demonstrate that the technique finds, with high probability, matches which are both globally optimal and globally consistent [ Bev93; BR95 ] . <p> To extend this type of globally consistent matching for multisensor recognition, a global alignment or coregistration algorithm is needed. As an initial step, we investigate a combination of proven single sensor error formulations. Kumar <ref> [ Kum89; KH94 ] </ref> has developed a succession of single sensor algorithms for a typical `pin-hole' camera model which takes perspective into account. Section 6.1.1 gives a more complete description of Kumar's formulation and explains our decision to base our measure on Kumar's earlier, rather than later, error formulation. <p> The range-to-model error is simply the squared Euclidean distance between corresponding features. The optical-to-model error is measured using a squared error between planes defined by image line segments and points on the 3D object model. This measure of optical-to-model error is the first measure developed by Kumar <ref> [ Kum89; KH94 ] </ref> . Kumar subsequently showed this measure to be sensitive to fragmentation noise in the image segments. To correct this weakness, Kumar subsequently developed a measure using 3D model features to define planes. We earlier observed an analogous result for 2D line fitting [ BWR89 ] . <p> The first term, E fit;o , measures distance between corresponding optical and model features. This term is precisely the point-to-plane error 13 criterion defined by Kumar <ref> [ Kum89; KH94 ] </ref> for computing camera-to-model pose. The second, E fit;r , is simply the sum-of-squared Euclidean distances between corresponding model and range points. The coregistration parameters themselves are denoted by F . <p> The weighting term oi is typically 1, but can be used 14 to bias some features over others. For example, oi can be used to weight lines based on inverse distance to perform a normalization similar to that considered in Kumar's later measures <ref> [ Kum89; KH94 ] </ref> . The weighting term 1 2n o t 2 mo i oi normalizes the E fit;o measure based upon the assumption that the image line endpoints fall within t mo of the associated model line. <p> The lack of a rotation parameter between mc mapped points and the range sensor coordinate system constrains the sensor-to-sensor orientation. Notice that these constraints retain the same degree of nonlinearity (degree 2) found in the original Kumar <ref> [ Kum89; KH94 ] </ref> and Horn [ Hor86 ] equations. This is desirable, since increasing the nonlinearity of a system of equations tends to increase the instability of the solution. <p> While the matrix terms (r mc 1;1 . . . r mc 3;3 ) could be constructed in such a way as to allow only rigid rotations, this would increase the degree of nonlinearity in the equation. Kumar <ref> [ Kum89; KH94 ] </ref> suggests a better approach: Rodriguez's formula, which is an approximation appropriate for small rotations. To rotate a point ~ P mi by an amount R mo , we decompose R mo into an estimate R e mo and a small update, ffi~! mo .
Reference: [ Kum92 ] <author> Rakesh Kumar. </author> <title> Model Dependent Inference of 3D Information From a Sequence of 2D Images. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, Amherst, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: The subsets need to be at least large enough to cover the degrees of freedom, so we would need to select at least 3 optical lines and 1 range point. However, Kumar <ref> [ Kum92 ] </ref> found that selecting a minimal number of features caused the solution to be sensitive to the Gaussian noise that we assume is overlaid onto the true data. As a consequence, it is better to select a larger subset to stabilize the optimal pose against noise.
Reference: [ KvD76 ] <author> J. J. Koenderink and A. J. van Doorn. </author> <title> The Singularities of Visual Mapping. </title> <journal> Biological Cybernetics, </journal> <volume> 24 </volume> <pages> 51-59, </pages> <year> 1976. </year>
Reference-contexts: Common approaches to model feature generation have centered around an off-line model analysis in which visible features are determined for all viewpoints [ PD87; Pla88 ] . The results are then grouped into regions of constant topology [ KD87 ] and stored in an aspect graph representation <ref> [ KvD76; KvD79 ] </ref> . The aspect graph is used at runtime to obtain the list of visible features for a given pose [ SD92 ] .
Reference: [ KvD79 ] <author> J. J. Koenderink and A. J. van Doorn. </author> <title> The Internal Representation of Shape with Respect to Vision. </title> <journal> Biological Cybernetics, </journal> <volume> 32 </volume> <pages> 211-216, </pages> <year> 1979. </year>
Reference-contexts: Common approaches to model feature generation have centered around an off-line model analysis in which visible features are determined for all viewpoints [ PD87; Pla88 ] . The results are then grouped into regions of constant topology [ KD87 ] and stored in an aspect graph representation <ref> [ KvD76; KvD79 ] </ref> . The aspect graph is used at runtime to obtain the list of visible features for a given pose [ SD92 ] .
Reference: [ LB83 ] <author> David G. Lowe and T. O. Binford. </author> <title> The Perceptual Organization of Visual Images: Segmentation as a Basis for Recognition. </title> <booktitle> In Proceedings Image Understanding Workshop, Stanford, </booktitle> <pages> pages 203 - 209, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: dealt with later when matching the model features to the sensor features. 4.2 Model-Directed Linear Feature Detection Traditional methods for locating objects in optical imagery typically use edge detection algorithms. [ MH80; Hil83 ] locate local edges which may then be grouped into larger features such as straight line segments <ref> [ BHR86; LB83 ] </ref> . These linear features are, in turn, matched to linear features of stored object models [ Low85; Low91; HU87; HU88; HU90; GH91; BWR89; BWR90; BWR91; BR92a; BR92b; Bev92a; Bev93; BR94; BHP94b; BR95 ] .
Reference: [ LB85 ] <author> David G. Lowe and Thomas O. Binford. </author> <title> The Recovery of Three-Dimensional Structure from Image Curves. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 7(3):320 - 325, </volume> <year> 1985. </year>
Reference-contexts: For example, Lowe utilized this approach subject to orthographic projection <ref> [ LB85 ] </ref> and later demonstrated object tracking under perspective projection [ Low91 ] . Huttenlocher [ HU90 ] demonstrated alignment-based recognition under orthographic projection. Grimson's work [ Gri90 ] on constraint base matching has emphasized local feature compatibility to prune tree search and thus minimize tests of global alignment.
Reference: [ Low85 ] <author> David G. Lowe. </author> <title> Perceptual Organization and Visual Recognition. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1985. </year>
Reference: [ Low91 ] <author> David G. Lowe. </author> <title> Fitting Parameterized Three-Dimensional Models to Images. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(5):441 - 450, </volume> <month> May </month> <year> 1991. </year>
Reference-contexts: For example, Lowe utilized this approach subject to orthographic projection [ LB85 ] and later demonstrated object tracking under perspective projection <ref> [ Low91 ] </ref> . Huttenlocher [ HU90 ] demonstrated alignment-based recognition under orthographic projection. Grimson's work [ Gri90 ] on constraint base matching has emphasized local feature compatibility to prune tree search and thus minimize tests of global alignment.
Reference: [ MBCA85 ] <author> M. J. Magee, B. A. Boyter, C. H. Chien, and J. K. Aggarwal. </author> <title> Experiments in Intensity Guided Range Sensing Recognition of Three-Dimensional Objects. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 7(6):629 - 637, </volume> <month> November </month> <year> 1985. </year>
Reference-contexts: Aggarwal notes that past work on sensor fusion emphasized single modality sensors, with comparatively little work on different sensor modalities. The implied explanation is that relating data from different modalities is more difficult. While Aggarwal <ref> [ MBCA85 ] </ref> and others [ SG87 ] have examples of successful mixed-modality fusion, this is still a young research area. Aggarwal also notes that to properly perform mixed-modality sensor fusion, coordinate transformations between images need to be adaptively determined.
Reference: [ MH80 ] <author> David Marr and Ellen C. Hildreth. </author> <title> Theory of Edge Detection. </title> <journal> Proceedings of the Royal Society of London, </journal> <volume> B207:187-217, </volume> <year> 1980. </year>
Reference-contexts: Noise is dealt with later when matching the model features to the sensor features. 4.2 Model-Directed Linear Feature Detection Traditional methods for locating objects in optical imagery typically use edge detection algorithms. <ref> [ MH80; Hil83 ] </ref> locate local edges which may then be grouped into larger features such as straight line segments [ BHR86; LB83 ] .
Reference: [ PD87 ] <author> Harry Platinga and Charles Dyer. </author> <title> Visibility, Occlusion, and the Aspect Graph. </title> <type> Technical Report 736, </type> <institution> University of Wisconsin - Madison, </institution> <month> December </month> <year> 1987. </year>
Reference-contexts: From these simpler models, features to be used in the matching process are then obtained. Common approaches to model feature generation have centered around an off-line model analysis in which visible features are determined for all viewpoints <ref> [ PD87; Pla88 ] </ref> . The results are then grouped into regions of constant topology [ KD87 ] and stored in an aspect graph representation [ KvD76; KvD79 ] .
Reference: [ PFTV88 ] <author> William H. Press, Brian P. Flannery, Saul A. Teukolsky, and William T. Vetterling. </author> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1988. </year>
Reference-contexts: The constants in Table 2 are recomputed each time through the loop. The algorithm converges when the amount by which E fit drops between successive iterations falls below a preset threshold. Unsuccessful termination occurs if the total number of iterations exceeds a maximum number of iterations. The Levenberg-Marquardt <ref> [ PFTV88 ] </ref> method has been found to be robust in our past single sensor pose work [ BR92b; BR94 ] , and it is used here to find the optimal coregistration parameters. 16 @E fit P n o P 2 mij fi ^ N oi ffi~! mo + ^ N
Reference: [ Pin88 ] <author> Juan Pineda. </author> <title> A Parallel Algorithm for Polygon Rasterization. </title> <booktitle> In Proceedings of Siggraph '88, </booktitle> <pages> pages 17-20, </pages> <year> 1988. </year>
Reference-contexts: An example of such a filter, displayed as an image, is shown in Figure 4c. The second step is to measure the overall gradient response under the segment. A commonly used graphics anti-aliasing technique known as Pineda Arithmetic <ref> [ Pin88 ] </ref> , is used to determine with subpixel accuracy where the projected line crosses the sensor image. A weighting term is created (see Figure 4f) to scale the gradient at each pixel, and obtain the gradient for a single line.
Reference: [ Pla88 ] <author> William Harry Plantinga. </author> <title> The ASP: A Continuous, Viewer-Centered Object Representation for Computer Vision. </title> <type> PhD thesis, </type> <institution> University of Wisconsin at Madison, </institution> <year> 1988. </year>
Reference-contexts: From these simpler models, features to be used in the matching process are then obtained. Common approaches to model feature generation have centered around an off-line model analysis in which visible features are determined for all viewpoints <ref> [ PD87; Pla88 ] </ref> . The results are then grouped into regions of constant topology [ KD87 ] and stored in an aspect graph representation [ KvD76; KvD79 ] .
Reference: [ RL87 ] <author> Peter J. Rousseeuw and Annick M. Leroy. </author> <title> Robust Regression and Outlier Detection. </title> <publisher> Wiley, </publisher> <year> 1987. </year>
Reference-contexts: If, however, the correspondence data contains outliers, our method will behave unstably. Median filtering is a robust statistic for detecting and removing outliers <ref> [ RL87 ] </ref> . Median filtering handles outliers by fitting to the subset of the data which minimizes the ensemble median error value. It is a robust statistic when there are less than 50% outliers.
Reference: [ SB94 ] <author> Anthony N. A. Schwickerath and J. Ross Beveridge. </author> <title> Model to Multisensor Coregistration with Eight Degrees of Freedom. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 481 - 490, </pages> <address> Los Altos, CA, </address> <month> November </month> <year> 1994. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Local search, as developed in section 6.2.3,is more general in two ways. First, a match error is developed which evaluates coverage of the model as well as quality of fit. 2 The weights are the combined threshold and ff fit term described in <ref> [ SB94 ] </ref> 3 A more detailed analysis of the results from test I and II is available in [ SB94 ] . 18 Second, the definition of a discrete neighborhood of alternate solutions guides the search to a local optimum, increasing the chance of finding a good solution in a <p> First, a match error is developed which evaluates coverage of the model as well as quality of fit. 2 The weights are the combined threshold and ff fit term described in <ref> [ SB94 ] </ref> 3 A more detailed analysis of the results from test I and II is available in [ SB94 ] . 18 Second, the definition of a discrete neighborhood of alternate solutions guides the search to a local optimum, increasing the chance of finding a good solution in a large combinatorial space over unguided random sampling. 6.2.1 Building Matches: Median Filtering Least-squares methods, such as our E fit
Reference: [ SBG95 ] <author> Mark R. Stevens, J. Ross Beveridge, and Michael E. Goss. </author> <title> Reduction of BRL/CAD Models and Their Use in Automatic Target Recognition Algorithms. </title> <booktitle> In BRL/CAD Symposium 95, </booktitle> <year> 1995. </year>
Reference-contexts: Algorithms to reduce the model complexity to a level more closely related to the sensor granularity have already been developed <ref> [ SBG95; Ste95 ] </ref> . From these simpler models, features to be used in the matching process are then obtained. Common approaches to model feature generation have centered around an off-line model analysis in which visible features are determined for all viewpoints [ PD87; Pla88 ] .
Reference: [ SD92 ] <author> W. Brent Seales and Charles R. Dyer. </author> <title> Modeling the Rim Appearance. </title> <booktitle> In Proceedings of the 3rd International Conference on Computer Vision, </booktitle> <pages> pages 698-701, </pages> <year> 1992. </year> <month> 39 </month>
Reference-contexts: The results are then grouped into regions of constant topology [ KD87 ] and stored in an aspect graph representation [ KvD76; KvD79 ] . The aspect graph is used at runtime to obtain the list of visible features for a given pose <ref> [ SD92 ] </ref> . As an alternative to off-line compilation of the stored graph representation, our approach instead exploits the power of modern graphics hardware to achieve real-time generation of relevant model features [ Ste95 ] . <p> Thus, if the background color appears in a pixel's eight-connected neighborhood, the associated face lies on the silhouette. Further search determines which specific face boundaries (edges) generate the silhouette. An edge is a possible silhouette edge if only one of the two bounding faces is visible <ref> [ SD92 ] </ref> . This step may leave some edges which are actually internal as hypothesized silhouette edges. It also does not deal with self-occlusion. A clipping algorithm is then used to discover and discard those edges, and portions of edges, which are not part of the silhouette.
Reference: [ SG87 ] <author> A. Stentz and Y. </author> <title> Goto. </title> <booktitle> The CMU Navigational Architecture. In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 440-446, </pages> <address> Los Angeles, CA, </address> <month> February </month> <year> 1987. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Aggarwal notes that past work on sensor fusion emphasized single modality sensors, with comparatively little work on different sensor modalities. The implied explanation is that relating data from different modalities is more difficult. While Aggarwal [ MBCA85 ] and others <ref> [ SG87 ] </ref> have examples of successful mixed-modality fusion, this is still a young research area. Aggarwal also notes that to properly perform mixed-modality sensor fusion, coordinate transformations between images need to be adaptively determined.
Reference: [ Shu94 ] <author> Alexander Shustorovich. </author> <title> Scale Specific and Robust Edge/Line Encoding with Linear Combinations of Gabor Wavelets. </title> <journal> Pattern Recognition, </journal> <volume> 27(5) </volume> <pages> 713-725, </pages> <year> 1994. </year>
Reference-contexts: There are many precedents for tuned edge masks including Canny [ Can86 ] and Torres [ TP86 ] . Others to develop and use such masks for bottom-up edge detection include <ref> [ Shu94; FA91 ] </ref> . An example of such a filter, displayed as an image, is shown in Figure 4c. The second step is to measure the overall gradient response under the segment.
Reference: [ Ste95 ] <author> Mark R. Stevens. </author> <title> Obtaining 3D Silhouettes and Sampled Surfaces from Solid Models for use in Computer Vision. </title> <type> Master's thesis, </type> <institution> Colorado State University, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: Algorithms to reduce the model complexity to a level more closely related to the sensor granularity have already been developed <ref> [ SBG95; Ste95 ] </ref> . From these simpler models, features to be used in the matching process are then obtained. Common approaches to model feature generation have centered around an off-line model analysis in which visible features are determined for all viewpoints [ PD87; Pla88 ] . <p> As an alternative to off-line compilation of the stored graph representation, our approach instead exploits the power of modern graphics hardware to achieve real-time generation of relevant model features <ref> [ Ste95 ] </ref> . Two sets of model features are generated based upon an estimate of the target's position and orientation. The first set represents the model silhouette lines appropriate for matching to optical imagery. The second set represents the 3D sampled surface information for matching to the range data.
Reference: [ SWF95 ] <author> G.D Sullivan, A.D. Worrall, and J.M Ferryman. </author> <title> Visual Object Recognition Using Deformable Models of Vehicles. </title> <booktitle> In Workshop on Context-Based Vision, </booktitle> <pages> pages 75-86, </pages> <month> june </month> <year> 1995. </year>
Reference-contexts: Coregistration parameters F are perturbed about a current estimate in order to test for better estimates. The process repeats until no improvement is possible. This general approach has been used by others, including some recent work by Sullivan <ref> [ SWF95 ] </ref> in which vehicles are tracked in video imagery. Since the space of coregistration parameters is continuous, gradient methods suggest themselves. Sampling about the current estimate might be likened to sampling the gradient, but this interpretation is problematic.
Reference: [ TP86 ] <author> Vincent Torre and Tomaso A. Poggio. </author> <title> On Edge Detection. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-8(2):147-164, </volume> <month> March </month> <year> 1986. </year>
Reference-contexts: This mask is formed by rotating the first derivative of a Gaussian to match the orientation of the current silhouette edge. There are many precedents for tuned edge masks including Canny [ Can86 ] and Torres <ref> [ TP86 ] </ref> . Others to develop and use such masks for bottom-up edge detection include [ Shu94; FA91 ] . An example of such a filter, displayed as an image, is shown in Figure 4c. The second step is to measure the overall gradient response under the segment.
Reference: [ U. 91 ] <author> U. S. </author> <note> Army Ballistic Research Laboratory. BRL-CAD User's Manual, release 4.0 edition, </note> <month> December </month> <year> 1991. </year>
Reference-contexts: for the characteristics of the data than the typical 2D range intensity image [ GBSF94; GBSF95 ] . 4 Deriving Target Model and Sensor Features 4.1 Dynamic Model Feature Generation Highly detailed models of the vehicles in our Fort Carson dataset exist in the CAD model format known as BRL/CAD <ref> [ U. 91 ] </ref> . Algorithms to reduce the model complexity to a level more closely related to the sensor granularity have already been developed [ SBG95; Ste95 ] . From these simpler models, features to be used in the matching process are then obtained.
Reference: [ VDL95 ] <author> J. G. Verly, D. E. Dudgeon, and R. T. Lacoss. </author> <title> Progress Report on the Development of the Automatic Target Recognition System for the UGV/RSTA LADAR. </title> <type> Technical Report 1006, </type> <institution> Massachusetts Institute of Technology, Lincoln Laboratory, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: The overall goal is to utilize these three sensor modalities to improve the reliability of target identification algorithms. Most of the prior work on target identification uses a fixed set of image-based templates or probes <ref> [ Bev92b; VDL95; DVD93 ] </ref> . These techniques have their place, and we are using the Alliant Techsystems probing algorithms [ Bev92b ] to provide initial object-and-pose hypotheses.
Reference: [ Y. 94 ] <author> Y. Hel-Or and M. Werman. </author> <title> Constraint-Fusion for Interpretation of Articulated Objects. </title> <booktitle> In Proceedings: Computer Vision and Pattern Recognition, </booktitle> <pages> pages 39 - 45. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> June </month> <year> 1994. </year> <month> 40 </month>
Reference-contexts: However, these algorithms only solve for the six-degrees-of-freedom pose estimate. 3 They do not support simultaneous adjustment of the sensor registration parameters. In terms of constraints, all of these methods assume a known sensor-to-sensor registration. More recent work by Hel-Or and Werman <ref> [ Y. 94; Ho93 ] </ref> adds degrees of freedom to account for articulated objects and nicely handles variable constraints in a single extended Kalman filter formulation. Their general Kalman filter approach could be applied to the coregistration problem formulated here.
References-found: 67

