URL: http://www.cs.rutgers.edu/~pstocks/doc/thesis.ps.Z
Refering-URL: http://www.cs.rutgers.edu/~pstocks/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Applying formal methods to software testing  
Author: Philip Alan Stocks BSc (Honours) 
Degree: A thesis submitted to THE DEPARTMENT OF COMPUTER SCIENCE THE UNIVERSITY OF QUEENSLAND for the degree of DOCTOR OF PHILOSOPHY  
Date: December 1993  
Abstract-found: 0
Intro-found: 1
Reference: [AA92] <author> N. Amla and P. Ammann. </author> <title> Using Z specifications in category partition testing. </title> <booktitle> In Proceedings of COMPASS 1992, the Seventh Annual Conference on Computer Assurance, </booktitle> <pages> pages 3-10, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: In terms of adapting existing testing techniques to rely on model-based specifications, category partitioning [OB88] has received some attention <ref> [AA92, Lay92] </ref>. However, the only connection between using category partitioning with formal specifications and improved testing these papers draw is that a model-based specification clearly and unambiguously states information relevant to category partitioning, which is somewhat self-evident.
Reference: [AHKN90] <author> J. Arkko, V. Hirvisalo, J. Kuusela, and E. Nuutila. </author> <title> Supporting testing of specifications and implementations. </title> <journal> EUROMICRO Journal, </journal> <volume> 30(1-5):297-302, </volume> <month> August </month> <year> 1990. </year> <month> EUROMICRO'90. </month>
Reference-contexts: In other work on testing based on algebraic specifications, Arkko et al. describe a tool which derives tests from algebraic specifications using methods similar to those outlined above, and assists in transforming them into an appropriate form for testing the implementation <ref> [AHKN90] </ref>. Gerrard et al. expound the benefits of using these methods in design time testing [GCG90]. Model-based specifications In contrast to process algebras, the body of literature on deriving tests from model-based specifications is Lilliputian.
Reference: [BB89] <author> T. Bolognesi and E. Brinksma. </author> <title> Introduction to the ISO specification language LOTOS. </title> <editor> In P. H. J. van Eijk, C. A. Vissers, and M. Diaz, editors, </editor> <booktitle> The Formal Description Technique LOTOS, </booktitle> <pages> pages 23-76. </pages> <publisher> North Holland, </publisher> <year> 1989. </year>
Reference-contexts: There are three general styles of specification: Process algebras Process algebras describe systems in terms of behaviour and interaction of active agents. Examples of process algebra formal methods are CSP [Hoa85] and LOTOS <ref> [BB89] </ref>. Algebraic specifications Algebraic specifications describe systems constructively in terms of applications of system operations. States are defined by construction from some defined basic structure, and passed as parameters. Examples of algebraic specification notations are Larch [GHW85], OBJ [Gog84], and RSL [RAI92]. CHAPTER 1. <p> Trying to give pointers into this immense body, on a topic not central to this thesis, is futile. The key concept in test derivation from process algebras is trace analysis. Process algebra notations (e.g., Petri Nets [Pet81], CSP [Hoa85], LOTOS <ref> [BB89] </ref>) define labelled transition systems, showing how a system moves from one state to the next, without concentrating on the details of states. The simple nature of these systems, CHAPTER 2.
Reference: [BCFG86] <author> L. Bouge, N. Choquet, L. Fribourg, and M.-C. </author> <title> Gaudel. Test sets generation from algebraic specifications using logic programming. </title> <journal> Journal of Systems and Software, </journal> <volume> 6(4) </volume> <pages> 343-360, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: Bouge [Bou85] introduces the concepts of projective reliability and asymptotic validity, which are collectively renamed the regularity hypothesis in later work <ref> [BCFG86] </ref>. The essence of the hypothesis is that algebraic data types can be tested by constructing all their instances (based on operation combinations) up to a certain complexity level. <p> As this complexity level approaches infinity, the test set approaches the exhaustive test set, so theoretically more effective test sets can be constructed by increasing the complexity level used. Bouge et al. <ref> [BCFG86] </ref> also introduce the uniformity hypothesis, which is a statement of the well-known technique of inferring a program's correctness for a subdomain of input by testing its correctness on one element of the subdomain. <p> Similar to reliability and validity, there is no general or algorithmic way to partition an implementation's input into revealing subdomains. In their work on deriving tests from algebraic specifications, Bouge et al. also developed hypotheses for test suites. They introduce the regularity hypothesis and uniformity hypothesis for algebraic specifications <ref> [Bou85, BCFG86] </ref>. The regularity hypothesis was developed from the notions of projective reliability and asymptotic validity, which correspond to Goodenough's and Gerhart's reliability and validity. Algebraic data types are built up from applications of operations on the data type.
Reference: [Bei90] <author> B. Beizer. </author> <title> Software Testing Techniques. </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, </address> <note> second edition, </note> <year> 1990. </year>
Reference-contexts: Nevertheless, our main usage of the term corresponds to the standard usage, and it would be unnecessarily confusing to try to introduce new terminology. 1.3.2 Software testing We assume understanding of the general principles of software testing (e.g., <ref> [Bei90, Mye79] </ref>). For the sake of clarity, the following explains how we will use some of the less strict terminology throughout this thesis. Functional unit Functional units are the elements of specification or code defining distinct pieces of system functionality. <p> Random testing is a useful benchmark when considering other testing strategies (e.g., [DN84, HT88]). Random tests can be generated from a specification of the structure of the input. Partitioning CHAPTER 1. INTRODUCTION 6 There are many input partitioning approaches to software testing (e.g., <ref> [Bei90, OB88, RC85, WO80] </ref>). The common approach of each is to divide the input domain into sub-domains for which each element has the same error-detecting ability as any other.
Reference: [BGM91] <author> G. Bernot, M.-C. Gaudel, and B. Marre. </author> <title> Software testing based on formal specifications: A theory and a tool. </title> <journal> Software Engineering Journal, </journal> <volume> 6(6) </volume> <pages> 387-405, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Bouge et al. [BCFG86] also introduce the uniformity hypothesis, which is a statement of the well-known technique of inferring a program's correctness for a subdomain of input by testing its correctness on one element of the subdomain. An extension to the underlying theory is reported by Bernot et al. <ref> [BGM91] </ref>, where the projective, asymptotic testing approach is replaced by the approach of starting with an ideal exhaustive test set, derived from the notion of specification satisfaction, and using hypotheses to reduce the size of the test set to practical levels, while maintaining as much of the error-revealing power as possible.
Reference: [BHO89] <author> M. J. Balcer, W. M. Hasling, and T. J. </author> <title> Ostrand. Automatic generation of test scripts from formal test specifications. </title> <booktitle> Software Engineering 143 BIBLIOGRAPHY 144 Notes, </booktitle> <volume> 14(8) </volume> <pages> 210-218, </pages> <month> December </month> <year> 1989. </year> <booktitle> Proceedings of the ACM SIG-SOFT '89 Third Symposium on Software Testing, Analysis, and Verification (TAV3). </booktitle>
Reference-contexts: The major functions of the tool are to annotate parts of the specification for record keeping purposes (for example, highlighting functional units), and maintain relationships between parts of the specification and any test information derived from them. Category partitioning <ref> [OB88, BHO89] </ref> is a more advanced method for natural language specification-based testing. Specifications are analysed to determine the various functional units. For each functional unit, the relevant characteristics of the parameters and environment objects are identified and classified in categories. <p> The functional unit under test, test oracle, and test purpose are all examples of additional considerations. Hence the need for some method of relating this information and defining tests. TSL, Test Specification Language, is a notation used in defining tests derived using category partitioning <ref> [OB88, BHO89] </ref>. Test scripts are derived from (informal) specifications indicating relevant inputs to operations and possible choices of values for these parameters. A test frame is one combination of choices for categories. Test frames are automatically generated by essentially calculating the cross product of the choices in the categories.
Reference: [BN92] <author> S. M. Brien and J. E. </author> <title> Nicholls. Z base standard version 1.0. </title> <type> Technical report, </type> <institution> Programming Research Group, Oxford University Computing Laboratory, Oxford University, </institution> <year> 1992. </year>
Reference-contexts: CHAPTER 1. INTRODUCTION 9 Model-based specifications Model-based specifications construct explicit models of the system state and show how the state can be changed by various operations. Examples of model-based specification notations are RSL [RAI92], VDM [Jon90], and Z <ref> [Spi92, BN92] </ref>. In this thesis, we use the Z notation [Spi92, BN92]. Z is a model-based specification notation, which was developed by J.-R. Abrial and the Programming Research Group at Oxford University. <p> CHAPTER 1. INTRODUCTION 9 Model-based specifications Model-based specifications construct explicit models of the system state and show how the state can be changed by various operations. Examples of model-based specification notations are RSL [RAI92], VDM [Jon90], and Z <ref> [Spi92, BN92] </ref>. In this thesis, we use the Z notation [Spi92, BN92]. Z is a model-based specification notation, which was developed by J.-R. Abrial and the Programming Research Group at Oxford University. Z is based on set theory and predicate calculus, and uses a schema calculus for defining states and operations.
Reference: [Bou85] <author> L. Bouge. </author> <title> A contribution to the theory of program testing. </title> <journal> Theoretical Computer Science, </journal> <volume> 37 </volume> <pages> 151-181, </pages> <year> 1985. </year>
Reference-contexts: Research on deriving tests from algebraic specifications is quite advanced. The basis of most work on testing using algebraic specifications is an ongoing effort involving the Universite de Paris and the Ecole Normale Superieure, which has produced appealing results in testing theory and test derivation from algebraic specifications. Bouge <ref> [Bou85] </ref> introduces the concepts of projective reliability and asymptotic validity, which are collectively renamed the regularity hypothesis in later work [BCFG86]. The essence of the hypothesis is that algebraic data types can be tested by constructing all their instances (based on operation combinations) up to a certain complexity level. <p> Similar to reliability and validity, there is no general or algorithmic way to partition an implementation's input into revealing subdomains. In their work on deriving tests from algebraic specifications, Bouge et al. also developed hypotheses for test suites. They introduce the regularity hypothesis and uniformity hypothesis for algebraic specifications <ref> [Bou85, BCFG86] </ref>. The regularity hypothesis was developed from the notions of projective reliability and asymptotic validity, which correspond to Goodenough's and Gerhart's reliability and validity. Algebraic data types are built up from applications of operations on the data type.
Reference: [Bud81] <author> T. A. Budd. </author> <title> Mutation analysis: Ideas, examples, problems, and prospects. </title> <editor> In B. Chandrasekaran and S. Radicchi, editors, </editor> <title> Computer Program Testing. </title> <publisher> North-Holland, </publisher> <year> 1981. </year>
Reference-contexts: Mutation testing, which is described below, is an example of a fault-based testing strategy. Mutation testing CHAPTER 1. INTRODUCTION 7 Mutation testing and analysis (e.g., <ref> [Bud81, DLS78, Ham77] </ref>) is based on the `competent programmer hypothesis' [DLS78], which states that a competent programmer produces programs which are `nearly' correct. By corollary, the proposal of mutation techniques is that actual programs differ from correct programs by small, and perhaps syntactic, amounts. <p> Clearly, the observations described in this work should be used to make sure any test set derived using a partition strategy is superior to a random test set. CHAPTER 2. SPECIFICATION-BASED TESTING ISSUES 23 Mutation analysis <ref> [Ham77, DLS78, Bud81] </ref> is an evaluation method with a different approach. The properties in the previous section aim at demonstrating that the implementation is correct for certain input and inferring it is correct for all input. Conversely, mutation testing demonstrates that certain errors are not in the implementation. <p> The flexibility should be preserved to change propagation domains in the library, use different propagations as the situation demands, or even assign priority to propagations should the need arise. 5.2.2 Specification mutation Mutation analysis <ref> [Ham77, DLS78, Bud81, How82] </ref> is primarily a means of assessing test suites. When a program passes all tests in a suite, mutant programs are generated and the suite is assessed in terms of how many mutants it distinguishes from the original program.
Reference: [CDHW93] <author> D. Carrington, D. Duke, I. Hayes, and J. Welsh. </author> <title> Deriving modular designs from formal specifications. </title> <journal> Software Engineering Notes, </journal> <volume> 18(5) </volume> <pages> 89-98, </pages> <month> December </month> <year> 1993. </year> <booktitle> Proceedings of the First ACM SIGSOFT Symposium on the Foundations of Software Engineering. </booktitle>
Reference-contexts: Specification-derived tests may be of some assistance in debugging because the relationship between the test and the specification makes it clear which parts of the specification aren't implemented correctly. There can be a simple mapping from specification structure to implementation structure, but this need not necessarily be true <ref> [CDHW93] </ref>. If the implementation structure does correspond closely to the specification structure, it will be easier to find where the fault lies.
Reference: [CFR90] <author> J. Collofello, T. Fisher, and M. Rees. </author> <title> A testing methodology framework. </title> <editor> In G. J. Knafl, editor, </editor> <booktitle> Proceedings of the 15th Annual International Computer Software and Applications Conference. IEEE Computer Society, </booktitle> <year> 1990. </year>
Reference-contexts: The absence of a universally effective testing strategy means multiple strategies should be used in conjunction, but without guidelines, this relies heavily on judgement and experience. Collofello et al. <ref> [CFR90] </ref> describe a framework for choosing and applying testing strategies. Their approach classifies four levels of testing (structural, functional, multifunctional, and system) and provides reasonable entry and exit criteria for advancing from one stage to the next.
Reference: [CHR82] <author> L. A. Clarke, J. Hassell, and D. J. Richardson. </author> <title> A close look at domain testing. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 8(4) </volume> <pages> 380-390, </pages> <month> July </month> <year> 1982. </year>
Reference-contexts: Domain testing Domain testing is a very successful technique for testing systems with linear input spaces <ref> [WC80, CHR82] </ref>. A program's control flow is analysed to partition the input into subdomains with linear domain boundaries. Errors in the control flow of the program will cause these boundaries to shift from their correct positions. <p> The domain propagation templates do not force this depth of transitivity. Naturally, we should include a special test to distinguish any mutants not detected by other tests. 7.2.2 Strategies The test template framework provides common ground for comparing and contrasting testing strategies. For example, an aim of domain testing <ref> [WC80, CHR82] </ref> is to derive the fewest test points possible to test each boundary. We saw in the fileread example from chapter 6 that domain testing did, indeed, derive significantly fewer CHAPTER 7.
Reference: [Cox90] <author> B. J. Cox. </author> <title> Planning the software industrial revolution. </title> <journal> IEEE Software, </journal> <volume> 7(6) </volume> <pages> 25-33, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: Software design: Components Cox describes a software engineering ideal where software development involves constructing programs from pre-defined components, similar to conventional engineering's use of nuts and bolts <ref> [Cox90] </ref>. A software engineering revolution similar to the industrial revolution is discussed which will move software development from a `build everything from scratch' approach to a `build from re-usable components where possible' approach.
Reference: [CS94] <author> D. Carrington and P. Stocks. </author> <title> A tale of two paradigms: Formal methods and software testing. </title> <booktitle> In Proceedings of the 8th Z User Meeting. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1994. </year> <note> BIBLIOGRAPHY 145 </note>
Reference-contexts: This case study was used to demonstrate applications of formal methods in software testing in <ref> [CS94] </ref>; the specification and testing below are drawn from this work. 6.1.1 Specification We define the set of all valid integer representations of triangles: ValidTriangle : ff (fl fi fl fi fl) 8 x ; y; z : fl * (x ; y; z ) 2 ValidTriangle , (x &lt; y
Reference: [CW93] <author> E. Cusack and C. Wezeman. </author> <title> Deriving tests for objects specified in Z. </title> <editor> In J. P. Bowen and J. E. Nicholls, editors, </editor> <booktitle> Z User Workshop. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: It would be interesting to see how knowledge of model-based language constructs could be used to assist in choosing more informative tests using category partitioning. Cusack and Wezeman <ref> [CW93] </ref> derive labelled transition systems from Object-Z 1 specifications. The labelled transition systems are concerned with the external behaviour of the specification and the CO-OP method [Wez90] is used to derive canonical testers from the transition systems which test conformance of the external behaviours of the implementations to the specifications. <p> The before and after state expressions for all the specified operations are disjoined and reduced to DNF. From the resulting expression, a finite state automaton representing the behaviour of the system can be derived and the relevant paths showing how to construct the tests are shown. Cusack and Wezeman <ref> [CW93] </ref> also take a state automata approach, deriving labelled transition systems from the specification. 2.2.5 Reification The abstractness of the (formal) specification has two ramifications on specification-based testing. Firstly, only black-box testing can be achieved.
Reference: [DDH72] <author> O.-J. Dahl, E. W. Dijkstra, and C. A. R. Hoare. </author> <title> Structured programming. APIC Studies in Data Processing, number 8. </title> <publisher> Academic Press, </publisher> <year> 1972. </year>
Reference-contexts: The only explanation we can offer is that informal specifications have limited usefulness (but are still required) in testing, and that the real benefits are to be gained from formal specifications, which are now reaching a 1 Dijkstra, of course, paraphrased from <ref> [DDH72] </ref>. 1 CHAPTER 1. INTRODUCTION 2 level of maturity and stability. This thesis examines applications of formal methods to software testing, which offers many advantages for testing. The formal specification of a software product can be used as a guide for designing functional tests for the product.
Reference: [DF92] <author> J. Dick and A. Faivre. </author> <title> Automatic partition analysis of VDM specifications. </title> <type> Technical Report RAD/DMA/92027, </type> <institution> Research and Advanced Development, Bull Systems Products, BULL S.A., </institution> <address> Rue Jean Jaures, 78340 Les Clayes-sous-Bois, France, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: The work of Dick and Faivre is a major contribution to the use of model-based 1 Object-Z is an object-oriented extension to Z developed at the University of Queensland [DKRS91]. CHAPTER 2. SPECIFICATION-BASED TESTING ISSUES 16 specifications in software testing <ref> [DF92, DF93] </ref>. A standard heuristic in testing is partition testing, where the input space is partitioned into sub-domains, and one test is drawn from each sub-domain. <p> SPECIFICATION-BASED TESTING ISSUES 19 in the specification. For model-based specifications, approaches taken to determine sequencing information for testing involve extracting a finite state automaton for the system from the information embodied in the specification. Dick and Faivre <ref> [DF92, DF93] </ref> use reduction to DNF, similar to their approach to deriving tests. The before and after state expressions for all the specified operations are disjoined and reduced to DNF.
Reference: [DF93] <author> J. Dick and A. Faivre. </author> <title> Automating the generation and sequencing of test cases from model-based specifications. </title> <editor> In J. C. P. Woodcock and P. G. Larsen, editors, </editor> <title> FME'93 Industrial Strength Formal Methods, </title> <booktitle> Lecture Notes in Computer Science 670, </booktitle> <pages> pages 268-284. </pages> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: The work of Dick and Faivre is a major contribution to the use of model-based 1 Object-Z is an object-oriented extension to Z developed at the University of Queensland [DKRS91]. CHAPTER 2. SPECIFICATION-BASED TESTING ISSUES 16 specifications in software testing <ref> [DF92, DF93] </ref>. A standard heuristic in testing is partition testing, where the input space is partitioned into sub-domains, and one test is drawn from each sub-domain. <p> SPECIFICATION-BASED TESTING ISSUES 19 in the specification. For model-based specifications, approaches taken to determine sequencing information for testing involve extracting a finite state automaton for the system from the information embodied in the specification. Dick and Faivre <ref> [DF92, DF93] </ref> use reduction to DNF, similar to their approach to deriving tests. The before and after state expressions for all the specified operations are disjoined and reduced to DNF. <p> Our previous work [SC91, SC93b] recognises the potential of rigorous reification methods for steadily increasing the white-box aspects of the test suite, and for ensuring accurate implementation-level representations of the tests. Discussion of these concepts is deferred to chapter 7. Dick and Faivre <ref> [DF93] </ref> also recognise the possibility of using refinements of specifications to introduce white-box tests into the test suite. 2.3 Supporting testing Testing support concerns practices designed to improve and judge the overall quality of test suites. Without adequate support practices, the value of testing will always be dubious. CHAPTER 2. <p> define the derivation relationship between these templates and the valid input space in the template hierarchy. fCE equ ; CE iso ; CE sca ; CE inv g = TTH Classify (VIS Classify ; cause effect ) DNF partitioning Our next strategy uses the partition analysis of Dick and Faivre <ref> [DF93] </ref>. This reduces mathematical expressions representing operations or existing test templates to Disjunctive Normal Form (DNF). From the DNF expression, disjoint partitions can be extracted easily. DNF partition : STRATEGY We apply this strategy to sub-divide the cause-effect templates derived above.
Reference: [DKRS91] <author> R. Duke, P. King, G. Rose, and G. Smith. </author> <title> The Object-Z specification language version 1. </title> <type> Technical Report 91-1, </type> <institution> Software Verification Research Centre, The University of Queensland, Queensland 4072, Aus-tralia, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: This is similar to methods used in conformance testing based on process algebras. The work of Dick and Faivre is a major contribution to the use of model-based 1 Object-Z is an object-oriented extension to Z developed at the University of Queensland <ref> [DKRS91] </ref>. CHAPTER 2. SPECIFICATION-BASED TESTING ISSUES 16 specifications in software testing [DF92, DF93]. A standard heuristic in testing is partition testing, where the input space is partitioned into sub-domains, and one test is drawn from each sub-domain.
Reference: [DLS78] <author> R. A. DeMillo, R. J. Lipton, and F. G. Sayward. </author> <title> Hints on test data selection: Help for the practicing programmer. </title> <journal> Computer, </journal> <volume> 11(4) </volume> <pages> 34-41, </pages> <month> April </month> <year> 1978. </year>
Reference-contexts: Mutation testing, which is described below, is an example of a fault-based testing strategy. Mutation testing CHAPTER 1. INTRODUCTION 7 Mutation testing and analysis (e.g., <ref> [Bud81, DLS78, Ham77] </ref>) is based on the `competent programmer hypothesis' [DLS78], which states that a competent programmer produces programs which are `nearly' correct. By corollary, the proposal of mutation techniques is that actual programs differ from correct programs by small, and perhaps syntactic, amounts. <p> Mutation testing, which is described below, is an example of a fault-based testing strategy. Mutation testing CHAPTER 1. INTRODUCTION 7 Mutation testing and analysis (e.g., [Bud81, DLS78, Ham77]) is based on the `competent programmer hypothesis' <ref> [DLS78] </ref>, which states that a competent programmer produces programs which are `nearly' correct. By corollary, the proposal of mutation techniques is that actual programs differ from correct programs by small, and perhaps syntactic, amounts. <p> Clearly, the observations described in this work should be used to make sure any test set derived using a partition strategy is superior to a random test set. CHAPTER 2. SPECIFICATION-BASED TESTING ISSUES 23 Mutation analysis <ref> [Ham77, DLS78, Bud81] </ref> is an evaluation method with a different approach. The properties in the previous section aim at demonstrating that the implementation is correct for certain input and inferring it is correct for all input. Conversely, mutation testing demonstrates that certain errors are not in the implementation. <p> The flexibility should be preserved to change propagation domains in the library, use different propagations as the situation demands, or even assign priority to propagations should the need arise. 5.2.2 Specification mutation Mutation analysis <ref> [Ham77, DLS78, Bud81, How82] </ref> is primarily a means of assessing test suites. When a program passes all tests in a suite, mutant programs are generated and the suite is assessed in terms of how many mutants it distinguishes from the original program.
Reference: [DM91] <author> P. Dauchy and B. Marre. </author> <title> Test data selection from algebraic specifications: Application to an automatic subway module. </title> <editor> In A. van Lam-sweerde and A. Fugetta, editors, </editor> <booktitle> Lecture Notes in Computer Science 550. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1991. </year> <booktitle> 3rd European Software Engineering Conference, ESEC '91. </booktitle>
Reference-contexts: In the case of conditional expressions, case analysis is used to partition the input domains which are assumed to behave according to the uniformity hypothesis. The method and tool have been used successfully on a substantial case study <ref> [DM91] </ref>. In other work on testing based on algebraic specifications, Arkko et al. describe a tool which derives tests from algebraic specifications using methods similar to those outlined above, and assists in transforming them into an appropriate form for testing the implementation [AHKN90].
Reference: [DN84] <author> J. W. Duran and S. C. Ntafos. </author> <title> An evaluation of random testing. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 10(4) </volume> <pages> 438-444, </pages> <month> July </month> <year> 1984. </year> <note> BIBLIOGRAPHY 146 </note>
Reference-contexts: We offer some references to the literature for guidance, but they are not intended to be complete. Random testing Tests are chosen randomly from the space of possible inputs. No sub-division of input domains is undertaken. Random testing is a useful benchmark when considering other testing strategies (e.g., <ref> [DN84, HT88] </ref>). Random tests can be generated from a specification of the structure of the input. Partitioning CHAPTER 1. INTRODUCTION 6 There are many input partitioning approaches to software testing (e.g., [Bei90, OB88, RC85, WO80]). <p> The benchmark of any test selection criteria not meeting such standards as reliability and validity is to be more effective than random testing. The intuition that any structured approach to test selection must be better than random sampling of the input has come under attack <ref> [DN84] </ref>, even to the point of random testing being superior to partition testing approaches in some cases [Ham87]. This calls into question any confidence gained in an implementation through partition testing strategies [HT88].
Reference: [Fre91] <author> R. S. Freedman. </author> <title> Testability of software components. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(6) </volume> <pages> 553-564, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: In summary, specification mutation defines validation obligations for the specifier, i.e., situations for which the specifier is obliged to show that the original specification really specifies what is intended. 7.3.2 Testability and design Freedman defines properties of software procedures that determine their `testability' <ref> [Fre91] </ref>. These ideas, controllability and observability, are borrowed from established hardware verification processes. A controllable and observable procedure exhibits CHAPTER 7. THE FRAMEWORK IN THE LARGER PICTURE 131 high testability.
Reference: [GCG90] <author> C. P. Gerrard, D. Coleman, and R. M. Gallimore. </author> <title> Formal specification and design time testing. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 16(1) </volume> <pages> 1-12, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Gerrard et al. expound the benefits of using these methods in design time testing <ref> [GCG90] </ref>. Model-based specifications In contrast to process algebras, the body of literature on deriving tests from model-based specifications is Lilliputian. In regard to test derivation, an advantage of model-based specifications is that the data involved and the relationships amongst them are clearly defined.
Reference: [GG75] <author> J. B. Goodenough and S. L. Gerhart. </author> <title> Toward a theory of test data selection. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 1(2) </volume> <pages> 156-173, </pages> <month> June </month> <year> 1975. </year>
Reference-contexts: However, formal specifications can play an important role in software testing. Of course, it is not surprising that specifications are important to software testing; it is impossible to test software without specifications of some kind. As Goodenough and Gerhart note, testing based only on program implementation is fundamentally flawed <ref> [GG75] </ref>. Despite this, only a small portion of the testing literature deals with specification-based testing issues. <p> The basis of these inductive approaches is defining properties of test suites selected according to test selection criteria. Such properties enable statements to be made about implementations tested on test data derived using the test criterion. The pioneering work of Goodenough and Gerhart on testing theory <ref> [GG75] </ref> for-malised this issue, and prompted much additional work in this area. Goodenough and Gerhart introduce the reliability and validity hypotheses [GG75]. A test suite is defined to be successful if every test is passed. <p> Such properties enable statements to be made about implementations tested on test data derived using the test criterion. The pioneering work of Goodenough and Gerhart on testing theory <ref> [GG75] </ref> for-malised this issue, and prompted much additional work in this area. Goodenough and Gerhart introduce the reliability and validity hypotheses [GG75]. A test suite is defined to be successful if every test is passed. A test criterion is reliable if every derived test suite is successful or every derived test suite is unsuccessful. <p> Many criteria make statements about tests in terms of the results of executing them; we cannot define such criteria in the formalism of the framework. For example, validity and reliability as defined by Goodenough and Gerhart <ref> [GG75] </ref>, depend on analysing the success of a test suite, where a test suite is defined to be successful if every test in the suite is passed. Because we cannot describe such phenomena of our abstract test suites using Z, we cannot define these criteria using the framework.
Reference: [GHW85] <author> J. V. Guttag, J. J. Horning, and J. M. Wing. </author> <title> Larch in five easy pieces. </title> <type> Technical Report 5, </type> <institution> Digital Equipment Corporation Systems Research Center, Palo Alto, California, USA, </institution> <year> 1985. </year>
Reference-contexts: Examples of process algebra formal methods are CSP [Hoa85] and LOTOS [BB89]. Algebraic specifications Algebraic specifications describe systems constructively in terms of applications of system operations. States are defined by construction from some defined basic structure, and passed as parameters. Examples of algebraic specification notations are Larch <ref> [GHW85] </ref>, OBJ [Gog84], and RSL [RAI92]. CHAPTER 1. INTRODUCTION 9 Model-based specifications Model-based specifications construct explicit models of the system state and show how the state can be changed by various operations. Examples of model-based specification notations are RSL [RAI92], VDM [Jon90], and Z [Spi92, BN92].
Reference: [GMH81] <author> J. Gannon, P. McMullin, and R. Hamlet. </author> <title> Data-Abstraction implementation, specification, and testing. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 3(2) </volume> <pages> 211-223, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: In fact, the earliest uses of (formal) specifications in testing have been as sources of test oracles. DAISTS (Data Abstraction, Implementation, Specification and Testing System) is a system which focuses on using specifications as oracles <ref> [GMH81, MG83] </ref>. The DAISTS approach is to annotate program code with algebraic specifications of data types and tests. The specification axioms are translated into code segments which call procedures in the implementation. The tests specify which axiom they are testing and provide instantiations for the free variables in the axiom. <p> Our model of oracles is simply to specify expected results; checking that the actual results match the expected results is not addressed. It could be done manually, of course, but there is room for applying and experimenting with other techniques, such as those discussed in <ref> [GMH81, Hay86, RAO92] </ref>. The reification model also requires extension. At present, it only deals with data reification, and is incomplete. Procedural reification also needs to be considered, especially the effects of procedural reification on specification/implementation structure and white-box tests.
Reference: [Gog84] <author> J. A. Goguen. </author> <title> Parameterized programming. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 10(5) </volume> <pages> 528-544, </pages> <year> 1984. </year>
Reference-contexts: Examples of process algebra formal methods are CSP [Hoa85] and LOTOS [BB89]. Algebraic specifications Algebraic specifications describe systems constructively in terms of applications of system operations. States are defined by construction from some defined basic structure, and passed as parameters. Examples of algebraic specification notations are Larch [GHW85], OBJ <ref> [Gog84] </ref>, and RSL [RAI92]. CHAPTER 1. INTRODUCTION 9 Model-based specifications Model-based specifications construct explicit models of the system state and show how the state can be changed by various operations. Examples of model-based specification notations are RSL [RAI92], VDM [Jon90], and Z [Spi92, BN92].
Reference: [Gou83] <author> John S. Gourlay. </author> <title> A mathematical framework for the investigation of testing. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 9(6) </volume> <pages> 686-709, </pages> <month> November </month> <year> 1983. </year>
Reference-contexts: Fault-based strategies have been shown to be at least as powerful as code coverage strategies such as executing all branches in a program <ref> [Gou83, How86b] </ref>. Mutation testing, which is described below, is an example of a fault-based testing strategy. Mutation testing CHAPTER 1. INTRODUCTION 7 Mutation testing and analysis (e.g., [Bud81, DLS78, Ham77]) is based on the `competent programmer hypothesis' [DLS78], which states that a competent programmer produces programs which are `nearly' correct.
Reference: [Hal88] <author> P. A. V. Hall. </author> <title> Towards testing with respect to formal specifications. </title> <booktitle> In Second IEE/BCS Conference on Software Engineering 88, </booktitle> <pages> pages 159-163. </pages> <publisher> IEE, </publisher> <month> July </month> <year> 1988. </year>
Reference-contexts: One explanation for this is that model-based specifications are not executable and not necessarily constructive, which limits our ability to automatically generate meaningful tests. However, there are still some very good results in specification-based testing using model-based notations. Hall uses a general approach to derive tests from Z specifications <ref> [Hal88] </ref>. Simple partitions of the input space are constructed by examining the obvious divisions of input defined in the predicates of operations. This case analysis is highly structured, but not rigorous. Scullard uses a reduction technique to derive tests from VDM specifications [Scu88]. <p> Hall, considering how to construct certain system states from operations in a Z specification, notes, `All this leads us towards a far more algebraic interest in the specification, and the need to know the result of sequences of operations in terms of their visible effects for given input sequences.' <ref> [Hal88] </ref>. For example, for algebraic specifications, a derived test is defined in terms of the constructor operations required to build it from the basic type components; this embodies a direct mapping to the sequence of operations in the implementation required to construct this test.
Reference: [Hal91] <author> P. A. V. Hall. </author> <title> Relationship between specifications and testing. </title> <journal> Information and Software Technology, </journal> <volume> 33(1) </volume> <pages> 47-52, </pages> <year> 1991. </year>
Reference-contexts: A successful test indicates the implementation works correctly for that case. The underlying theory of testing is based on inducing the correctness of the implementation on all input based on a sample of input. Hall analyses the relationship between specifications and testing, making this point <ref> [Hal91] </ref>. Tests represent the `base case' of the inductive proof. The inductive step relies on generalising the results of one input to many. Mostly, this is done informally.
Reference: [Ham77] <author> R. G. Hamlet. </author> <title> Testing programs with the aid of a compiler. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 3(4) </volume> <pages> 279-290, </pages> <month> July </month> <year> 1977. </year> <note> BIBLIOGRAPHY 147 </note>
Reference-contexts: Mutation testing, which is described below, is an example of a fault-based testing strategy. Mutation testing CHAPTER 1. INTRODUCTION 7 Mutation testing and analysis (e.g., <ref> [Bud81, DLS78, Ham77] </ref>) is based on the `competent programmer hypothesis' [DLS78], which states that a competent programmer produces programs which are `nearly' correct. By corollary, the proposal of mutation techniques is that actual programs differ from correct programs by small, and perhaps syntactic, amounts. <p> Clearly, the observations described in this work should be used to make sure any test set derived using a partition strategy is superior to a random test set. CHAPTER 2. SPECIFICATION-BASED TESTING ISSUES 23 Mutation analysis <ref> [Ham77, DLS78, Bud81] </ref> is an evaluation method with a different approach. The properties in the previous section aim at demonstrating that the implementation is correct for certain input and inferring it is correct for all input. Conversely, mutation testing demonstrates that certain errors are not in the implementation. <p> The flexibility should be preserved to change propagation domains in the library, use different propagations as the situation demands, or even assign priority to propagations should the need arise. 5.2.2 Specification mutation Mutation analysis <ref> [Ham77, DLS78, Bud81, How82] </ref> is primarily a means of assessing test suites. When a program passes all tests in a suite, mutant programs are generated and the suite is assessed in terms of how many mutants it distinguishes from the original program.
Reference: [Ham87] <author> R. Hamlet. </author> <title> Probable correctness theory. </title> <journal> Information Processing Letters, </journal> <volume> 25 </volume> <pages> 17-25, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: The intuition that any structured approach to test selection must be better than random sampling of the input has come under attack [DN84], even to the point of random testing being superior to partition testing approaches in some cases <ref> [Ham87] </ref>. This calls into question any confidence gained in an implementation through partition testing strategies [HT88]. Weyuker and Jeng compared partition testing and random testing on an analytical basis and determined circumstances in which each strategy excels [JW89, WJ91].
Reference: [Ham89] <author> R. Hamlet. </author> <title> Theoretical comparison of testing methods. </title> <journal> Software Engineering Notes, </journal> <volume> 14(8) </volume> <pages> 28-37, </pages> <month> December </month> <year> 1989. </year> <booktitle> Proceedings of the ACM SIGSOFT '89 Third Symposium on Software Testing, Analysis, and Verification (TAV3). </booktitle>
Reference-contexts: The tool support should take care of the mundane tasks and record-keeping. Also very important is empirical analysis of the effectiveness of testing strategies. Assessing testing strategies is a very difficult area and it is unclear what the best methods for assessing strategies are <ref> [Ham89] </ref>, though the arguments for a statistical approach to reliability measurement presented in, for example, [Ham89, HV93] are compelling. We have proposed two new strategies with great intuitive appeal and some success in detecting errors undetected by other approaches. <p> Also very important is empirical analysis of the effectiveness of testing strategies. Assessing testing strategies is a very difficult area and it is unclear what the best methods for assessing strategies are [Ham89], though the arguments for a statistical approach to reliability measurement presented in, for example, <ref> [Ham89, HV93] </ref> are compelling. We have proposed two new strategies with great intuitive appeal and some success in detecting errors undetected by other approaches. However, some form of empirical analysis is in order to justify our intuitions. Finally, another aspect for consideration is applying the framework beyond unit testing.
Reference: [Hay86] <author> I. J. Hayes. </author> <title> Specification directed module testing. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 12(1) </volume> <pages> 124-133, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: DAISTS checks that the program implements the specification for the cases defined in the tests section by constructing implementation drivers from the specification and using the tests as input. This notion of the specification driving the implementation is extended by Hayes <ref> [Hay86] </ref>, who considers oracle issues for model-based specifications of abstract data types. Hayes shows how oracle procedures can be derived from Z specifications of abstract data types to check invariants, pre-conditions, and the input-output relationship. <p> Our model of oracles is simply to specify expected results; checking that the actual results match the expected results is not addressed. It could be done manually, of course, but there is room for applying and experimenting with other techniques, such as those discussed in <ref> [GMH81, Hay86, RAO92] </ref>. The reification model also requires extension. At present, it only deals with data reification, and is incomplete. Procedural reification also needs to be considered, especially the effects of procedural reification on specification/implementation structure and white-box tests.
Reference: [Hay87] <author> I. Hayes, </author> <title> editor. Specification Case Studies. </title> <booktitle> Series in Computer Science. </booktitle> <publisher> Prentice Hall International, </publisher> <year> 1987. </year> <note> Later edition available. </note>
Reference-contexts: EXAMPLES AND CASE STUDIES 76 6.2 File read The second example introduces a little more complexity of specification. The specification is of a simplified read operation on files, and is based on the specification of the UNIX read operation in <ref> [Hay87] </ref>. This case study was used as a primary example of the framework in [SC93c]; the specification and testing below are drawn from this work. 6.2.1 Specification Files are modelled as bounded sequences of bytes.
Reference: [Hay93] <author> I. Hayes, </author> <title> editor. Specification Case Studies. </title> <booktitle> Series in Computer Science. </booktitle> <publisher> Prentice Hall International, </publisher> <address> second edition, </address> <year> 1993. </year>
Reference-contexts: Z is based on set theory and predicate calculus, and uses a schema calculus for defining states and operations. We assume the reader is familiar with such concepts as sets, relations, and predicate calculus. Appendix A presents excerpts from the Z glossary in <ref> [Hay93] </ref>, explaining the Z notation we use that may not be familiar. Chapter 2 Specification-based testing issues To discuss work related to this thesis and to define the contribution of this thesis, we present a classification of the elements of specification-based software testing.
Reference: [Het88] <author> B. Hetzel. </author> <title> The Complete Guide to Software Testing. </title> <institution> QED Information Sciences, Wellesley, Massachusetts, </institution> <note> second edition, </note> <year> 1988. </year>
Reference-contexts: This latter is also useful for breaking `code now/test later' practices in software engineering, and helping develop a parallel testing activity for all software life-cycle phases as advocated in <ref> [Het88] </ref>. Our rather general interest in using formal methods to assist software testing leads us towards developing a framework in which to conduct specification-based testing, which includes a formal model of test suites.
Reference: [Hoa85] <author> C. A. R. Hoare. </author> <title> Communicating Sequential Processes. </title> <booktitle> Series in Computer Science. Prentice-Hall International, </booktitle> <year> 1985. </year>
Reference-contexts: There are three general styles of specification: Process algebras Process algebras describe systems in terms of behaviour and interaction of active agents. Examples of process algebra formal methods are CSP <ref> [Hoa85] </ref> and LOTOS [BB89]. Algebraic specifications Algebraic specifications describe systems constructively in terms of applications of system operations. States are defined by construction from some defined basic structure, and passed as parameters. Examples of algebraic specification notations are Larch [GHW85], OBJ [Gog84], and RSL [RAI92]. CHAPTER 1. <p> Trying to give pointers into this immense body, on a topic not central to this thesis, is futile. The key concept in test derivation from process algebras is trace analysis. Process algebra notations (e.g., Petri Nets [Pet81], CSP <ref> [Hoa85] </ref>, LOTOS [BB89]) define labelled transition systems, showing how a system moves from one state to the next, without concentrating on the details of states. The simple nature of these systems, CHAPTER 2.
Reference: [How82] <author> W. E. Howden. </author> <title> Weak mutation testing and completeness of program test sets. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-8, </volume> <year> 1982. </year>
Reference-contexts: The flexibility should be preserved to change propagation domains in the library, use different propagations as the situation demands, or even assign priority to propagations should the need arise. 5.2.2 Specification mutation Mutation analysis <ref> [Ham77, DLS78, Bud81, How82] </ref> is primarily a means of assessing test suites. When a program passes all tests in a suite, mutant programs are generated and the suite is assessed in terms of how many mutants it distinguishes from the original program.
Reference: [How86a] <author> W. E. Howden. </author> <title> A functional approach to program testing and analysis. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 12(10) </volume> <pages> 997-1005, </pages> <month> October </month> <year> 1986. </year>
Reference-contexts: In analogue to applying fault-based techniques for unit testing at the specification level (mutation testing), it would be interesting to bring fault-based techniques for interface testing to the specification-level, such as Howden's work on functional testing, which discusses fault models of interfaces <ref> [How86a, How86b] </ref>. 8.2.1 Further applications We saw in chapter 7 that the framework has many applications beyond software test definition and derivation. Here, we outline some ideas for some interesting CHAPTER 8. DISCUSSION 140 applications and extensions that we have not examined in detail.
Reference: [How86b] <author> W. E. Howden. </author> <title> Functional Program Testing and Analysis. </title> <publisher> McGraw Hill, </publisher> <year> 1986. </year>
Reference-contexts: Errors in the control flow of the program will cause these boundaries to shift from their correct positions. With domain testing, the minimum number of points is selected to ensure detection of boundary shifts. Fault-based testing Fault-based testing (e.g., <ref> [How86b, Mor90a] </ref>) is not a single strategy but a broad categorisation of strategies. The common theme of fault-based strategies is demonstrating that known faults do not exist in programs. Known faults range from using relational operators incorrectly (like using where &lt; was intended), to using variables before they are declared. <p> Fault-based strategies have been shown to be at least as powerful as code coverage strategies such as executing all branches in a program <ref> [Gou83, How86b] </ref>. Mutation testing, which is described below, is an example of a fault-based testing strategy. Mutation testing CHAPTER 1. INTRODUCTION 7 Mutation testing and analysis (e.g., [Bud81, DLS78, Ham77]) is based on the `competent programmer hypothesis' [DLS78], which states that a competent programmer produces programs which are `nearly' correct. <p> In analogue to applying fault-based techniques for unit testing at the specification level (mutation testing), it would be interesting to bring fault-based techniques for interface testing to the specification-level, such as Howden's work on functional testing, which discusses fault models of interfaces <ref> [How86a, How86b] </ref>. 8.2.1 Further applications We saw in chapter 7 that the framework has many applications beyond software test definition and derivation. Here, we outline some ideas for some interesting CHAPTER 8. DISCUSSION 140 applications and extensions that we have not examined in detail.
Reference: [HT88] <author> R. Hamlet and R. Taylor. </author> <title> Partition testing does not inspire confidence. </title> <booktitle> In Proceedings of the Second Workshop on Software Testing, Verification, and Analysis, </booktitle> <pages> pages 206-215, </pages> <address> Banff, Canada, </address> <month> July </month> <year> 1988. </year> <note> BIBLIOGRAPHY 148 </note>
Reference-contexts: We offer some references to the literature for guidance, but they are not intended to be complete. Random testing Tests are chosen randomly from the space of possible inputs. No sub-division of input domains is undertaken. Random testing is a useful benchmark when considering other testing strategies (e.g., <ref> [DN84, HT88] </ref>). Random tests can be generated from a specification of the structure of the input. Partitioning CHAPTER 1. INTRODUCTION 6 There are many input partitioning approaches to software testing (e.g., [Bei90, OB88, RC85, WO80]). <p> This calls into question any confidence gained in an implementation through partition testing strategies <ref> [HT88] </ref>. Weyuker and Jeng compared partition testing and random testing on an analytical basis and determined circumstances in which each strategy excels [JW89, WJ91].
Reference: [HV93] <author> R. Hamlet and J. Voas. </author> <title> Faults on its sleeve: Amplifying software reliability testing. </title> <booktitle> In Proceedings of the International Symposium on Software Testing and Analysis (ISSTA'93), </booktitle> <pages> pages 89-98, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Also very important is empirical analysis of the effectiveness of testing strategies. Assessing testing strategies is a very difficult area and it is unclear what the best methods for assessing strategies are [Ham89], though the arguments for a statistical approach to reliability measurement presented in, for example, <ref> [Ham89, HV93] </ref> are compelling. We have proposed two new strategies with great intuitive appeal and some success in detecting errors undetected by other approaches. However, some form of empirical analysis is in order to justify our intuitions. Finally, another aspect for consideration is applying the framework beyond unit testing.
Reference: [Jon90] <author> C. B. Jones. </author> <title> Systematic Software Development Using VDM. </title> <booktitle> Series in Computer Science. </booktitle> <publisher> Prentice Hall International, </publisher> <year> 1990. </year> <note> Second Edition. </note>
Reference-contexts: Examples of algebraic specification notations are Larch [GHW85], OBJ [Gog84], and RSL [RAI92]. CHAPTER 1. INTRODUCTION 9 Model-based specifications Model-based specifications construct explicit models of the system state and show how the state can be changed by various operations. Examples of model-based specification notations are RSL [RAI92], VDM <ref> [Jon90] </ref>, and Z [Spi92, BN92]. In this thesis, we use the Z notation [Spi92, BN92]. Z is a model-based specification notation, which was developed by J.-R. Abrial and the Programming Research Group at Oxford University. <p> Firstly, only black-box testing can be achieved. Secondly, in their pure form, the derived tests are as abstract as the specification, and some, perhaps instinctive, transformation from abstract to concrete takes place in producing the final test data. Specification reification (e.g., <ref> [Jon90, Mor90b] </ref>) has interesting applications to specification-based testing. Our previous work [SC91, SC93b] recognises the potential of rigorous reification methods for steadily increasing the white-box aspects of the test suite, and for ensuring accurate implementation-level representations of the tests. Discussion of these concepts is deferred to chapter 7.
Reference: [JW89] <author> B. Jeng and E. J. Weyuker. </author> <title> Some observations on partition testing. </title> <journal> Software Engineering Notes, </journal> <volume> 14(8) </volume> <pages> 38-47, </pages> <month> December </month> <year> 1989. </year> <booktitle> Proceedings of the ACM SIGSOFT '89 Third Symposium on Software Testing, Analysis, and Verification (TAV3). </booktitle>
Reference-contexts: This calls into question any confidence gained in an implementation through partition testing strategies [HT88]. Weyuker and Jeng compared partition testing and random testing on an analytical basis and determined circumstances in which each strategy excels <ref> [JW89, WJ91] </ref>. For example, their fourth observation states that if all partitions are of the same size and the same number of tests for each partition is chosen, then the test set is at least as good as a random test set.
Reference: [Kem85] <author> R. A. Kemmerer. </author> <title> Testing formal specifications to detect design errors. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 11(1) </volume> <pages> 32-43, </pages> <month> January </month> <year> 1985. </year>
Reference-contexts: Unfortunately, the specification can only be compared against the original requirements, which are not formal, so it is not possible to prove that the specification is correct. However, steps can be taken to improve our confidence in the specification. Kemmerer <ref> [Kem85] </ref> discusses an approach whereby specifications are made executable, and run on some test cases. Kneuper [Kne89] presents specification animation using symbolic execution as a validation method. Both of these approaches are prototyping the product by making the specification live in some way.
Reference: [Kne89] <author> Ralf Kneuper. </author> <title> Symbolic execution as a tool for validation of specifications. </title> <type> Technical Report UMCS-89-7-1, </type> <institution> Department of Computer Science, University of Manchester, </institution> <year> 1989. </year> <type> Ph.D. dissertation. </type>
Reference-contexts: However, steps can be taken to improve our confidence in the specification. Kemmerer [Kem85] discusses an approach whereby specifications are made executable, and run on some test cases. Kneuper <ref> [Kne89] </ref> presents specification animation using symbolic execution as a validation method. Both of these approaches are prototyping the product by making the specification live in some way. There are two major drawbacks of the prototyping approach.
Reference: [Lay92] <author> G. Laycock. </author> <title> Formal specification and testing: A case study. </title> <journal> Journal of Software Testing, Verification and Reliability, </journal> <volume> 2(1) </volume> <pages> 7-23, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: In terms of adapting existing testing techniques to rely on model-based specifications, category partitioning [OB88] has received some attention <ref> [AA92, Lay92] </ref>. However, the only connection between using category partitioning with formal specifications and improved testing these papers draw is that a model-based specification clearly and unambiguously states information relevant to category partitioning, which is somewhat self-evident.
Reference: [Lin92] <author> P. A. Lindsay. </author> <title> The ISDM case study: A dependency management system (inception paper). </title> <note> SVRC working paper. </note> <institution> Software Verification Research Centre, The University of Queensland, </institution> <year> 1992. </year>
Reference-contexts: EXAMPLES AND CASE STUDIES 90 6.3 Dependency management system The Dependency Management System (DMS) case study was a testbed in part of a larger project. This larger project was exploring software development methodologies, and the DMS was used as a common case study <ref> [Lin92] </ref>. The DMS is a critical component of a theorem proving tool, whose role is to keep track of dependencies between theorems and assertions in a proof, thus preventing circular reasoning.
Reference: [MG83] <author> Paul R. McMullin and John D. Gannon. </author> <title> Combining testing with formal specification: A case study. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 9(3) </volume> <pages> 328-335, </pages> <month> May </month> <year> 1983. </year>
Reference-contexts: In fact, the earliest uses of (formal) specifications in testing have been as sources of test oracles. DAISTS (Data Abstraction, Implementation, Specification and Testing System) is a system which focuses on using specifications as oracles <ref> [GMH81, MG83] </ref>. The DAISTS approach is to annotate program code with algebraic specifications of data types and tests. The specification axioms are translated into code segments which call procedures in the implementation. The tests specify which axiom they are testing and provide instantiations for the free variables in the axiom.
Reference: [Mor90a] <author> Larry J. Morell. </author> <title> A theory of fault-based testing. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 16(8) </volume> <pages> 844-857, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Errors in the control flow of the program will cause these boundaries to shift from their correct positions. With domain testing, the minimum number of points is selected to ensure detection of boundary shifts. Fault-based testing Fault-based testing (e.g., <ref> [How86b, Mor90a] </ref>) is not a single strategy but a broad categorisation of strategies. The common theme of fault-based strategies is demonstrating that known faults do not exist in programs. Known faults range from using relational operators incorrectly (like using where &lt; was intended), to using variables before they are declared.
Reference: [Mor90b] <author> C. Morgan. </author> <title> Programming from Specifications. </title> <booktitle> Series in Computer Science. Prentice-Hall International, </booktitle> <year> 1990. </year> <note> BIBLIOGRAPHY 149 </note>
Reference-contexts: Firstly, only black-box testing can be achieved. Secondly, in their pure form, the derived tests are as abstract as the specification, and some, perhaps instinctive, transformation from abstract to concrete takes place in producing the final test data. Specification reification (e.g., <ref> [Jon90, Mor90b] </ref>) has interesting applications to specification-based testing. Our previous work [SC91, SC93b] recognises the potential of rigorous reification methods for steadily increasing the white-box aspects of the test suite, and for ensuring accurate implementation-level representations of the tests. Discussion of these concepts is deferred to chapter 7.
Reference: [Mye79] <author> G. J. Myers. </author> <title> The Art of Software Testing. Business data processing. </title> <publisher> Wiley-Interscience, </publisher> <year> 1979. </year>
Reference-contexts: Nevertheless, our main usage of the term corresponds to the standard usage, and it would be unnecessarily confusing to try to introduce new terminology. 1.3.2 Software testing We assume understanding of the general principles of software testing (e.g., <ref> [Bei90, Mye79] </ref>). For the sake of clarity, the following explains how we will use some of the less strict terminology throughout this thesis. Functional unit Functional units are the elements of specification or code defining distinct pieces of system functionality. <p> EXAMPLES AND CASE STUDIES 66 and so it is not included. The specifications are all presented in Z. 6.1 Triangle Our first example is the very familiar triangle problem, with which we demonstrate some familiar testing strategies. A statement of the problem is drawn from <ref> [Mye79] </ref>: The required program is to input three natural numbers and determine whether these values can be the sides of a triangle, and if so, what type of triangle (equilateral, isosceles or scalene). <p> One option would be to generate only the four test templates corresponding to the cause-effect strategy. Instantiating these templates would produce test-cases that would check that all four outcomes are able to be generated by the software under test. Myers <ref> [Mye79] </ref> uses this example to demonstrate the complexities of software testing, indicating a wide range of cases that should be tested, and expecting readers not to guess many of them. This systematic testing using the framework covers all these cases and more.
Reference: [OB88] <author> T. J. Ostrand and M. J. Balcer. </author> <title> The category-partition method for specifying and generating functional tests. </title> <journal> Communications of the ACM, </journal> <volume> 31(6) </volume> <pages> 676-686, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Random testing is a useful benchmark when considering other testing strategies (e.g., [DN84, HT88]). Random tests can be generated from a specification of the structure of the input. Partitioning CHAPTER 1. INTRODUCTION 6 There are many input partitioning approaches to software testing (e.g., <ref> [Bei90, OB88, RC85, WO80] </ref>). The common approach of each is to divide the input domain into sub-domains for which each element has the same error-detecting ability as any other. <p> The major functions of the tool are to annotate parts of the specification for record keeping purposes (for example, highlighting functional units), and maintain relationships between parts of the specification and any test information derived from them. Category partitioning <ref> [OB88, BHO89] </ref> is a more advanced method for natural language specification-based testing. Specifications are analysed to determine the various functional units. For each functional unit, the relevant characteristics of the parameters and environment objects are identified and classified in categories. <p> Function signatures are reduced to their basic types, and the input and output spaces are analysed to extract sets of typical values according to various domain CHAPTER 2. SPECIFICATION-BASED TESTING ISSUES 15 partitions. This is essentially category partitioning <ref> [OB88] </ref> of both the input and output spaces. Richardson et al. [ROT89] examine strategies for selecting tests by extending implementation-based testing techniques to apply to formal specifications. Testing strategies are classified as error-based or fault-based. <p> This work does not define new ways to select tests; it defines, in general terms for each of these broad classes of testing approaches, the elements of the input and acceptance criteria. In terms of adapting existing testing techniques to rely on model-based specifications, category partitioning <ref> [OB88] </ref> has received some attention [AA92, Lay92]. However, the only connection between using category partitioning with formal specifications and improved testing these papers draw is that a model-based specification clearly and unambiguously states information relevant to category partitioning, which is somewhat self-evident. <p> The functional unit under test, test oracle, and test purpose are all examples of additional considerations. Hence the need for some method of relating this information and defining tests. TSL, Test Specification Language, is a notation used in defining tests derived using category partitioning <ref> [OB88, BHO89] </ref>. Test scripts are derived from (informal) specifications indicating relevant inputs to operations and possible choices of values for these parameters. A test frame is one combination of choices for categories. Test frames are automatically generated by essentially calculating the cross product of the choices in the categories.
Reference: [Off92] <author> A. J. Offutt. </author> <title> Investigations of the software testing coupling effect. </title> <journal> ACM Transactions on Software Engineering and Methodology, </journal> <volume> 1(1) </volume> <pages> 5-20, </pages> <month> Jan-uary </month> <year> 1992. </year>
Reference-contexts: Practically, the chance of spotting extra errors with this approach does not warrant the costs of constructing a complete suite of second-order mutants. Research into the so-called `coupling-effect' suggests that tests exposing simple errors also expose complex errors, and that second-order mutation is thus not required <ref> [Off92] </ref>. We cannot advocate either of these hypotheses, but on the whole we do not have enough evidence to suggest that second-order mutation is worthwhile. Our specification mutation strategy deals only with single mutations.
Reference: [OSW86] <author> T. J. Ostrand, R. Sigal, and E. J. Weyuker. </author> <title> Design for a tool to manage specification-based testing. </title> <booktitle> In Workshop on Software Testing, </booktitle> <pages> pages 41-50, </pages> <address> Banff, Canada, July 1986. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: These methods focus on identifying key elements in the specification. The first realisation when considering deriving tests from informal specifications is the size and impreciseness of such specifications, along with (usually) poor ability to relate components of the specification. Clearly, tool support is a major consideration. Ostrand et al. <ref> [OSW86] </ref> describe a tool for managing specification-based testing from informal specifications. The major functions of the tool are to annotate parts of the specification for record keeping purposes (for example, highlighting functional units), and maintain relationships between parts of the specification and any test information derived from them. <p> An extremely undervalued area of research, this problem is addressed by Ostrand et al. <ref> [OSW86] </ref>. Their tool for specification-based testing, SPECMAN, assists in structuring tests, by acting as record keeping support for constructing two tables, called the Functional Test Table and the Test Case Table.
Reference: [Pac90] <author> J. Pachl. </author> <title> A notation for specifying test selection criteria. </title> <editor> In L. Logrippo, R. L. Probert, and H. Ural, editors, </editor> <title> Protocol Specification, Testing, and Verification, X. </title> <publisher> North-Holland, </publisher> <year> 1990. </year>
Reference-contexts: A contrasting approach to test specification is taken by Pachl <ref> [Pac90] </ref>, who introduces notation for defining test criteria. Tests are defined by the criteria they must satisfy. Basic criteria such as being exhaustive and various orderings are defined along with two operators for constructing the union and intersection of test criteria.
Reference: [Pet81] <author> J. L. Peterson. </author> <title> Petri Net Theory and the Modeling of Systems. </title> <publisher> Prentice-Hall, </publisher> <year> 1981. </year>
Reference-contexts: Trying to give pointers into this immense body, on a topic not central to this thesis, is futile. The key concept in test derivation from process algebras is trace analysis. Process algebra notations (e.g., Petri Nets <ref> [Pet81] </ref>, CSP [Hoa85], LOTOS [BB89]) define labelled transition systems, showing how a system moves from one state to the next, without concentrating on the details of states. The simple nature of these systems, CHAPTER 2.
Reference: [PZ91] <author> A. Parrish and S. H. </author> <title> Zweben. Analysis and refinement of software test data adequacy properties. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(6) </volume> <pages> 565-581, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: These properties are usually called adequacy criteria. These simpler properties assist in deciding when an implementation has received sufficient testing. Parrish and Zweben analysed the eleven adequacy criteria for test sets selected using program structure <ref> [PZ91] </ref>. They demonstrated that these properties were interdependent and inconsistent in some cases, and refined them to a collection of seven independent and consistent criteria: 1. Applicability For every program, there is an adequate test set for criteria derived from the program. 2. Nonexhaustive applicability. CHAPTER 2.
Reference: [RAI92] <institution> The RAISE Language Group. The RAISE Specification Language, </institution> <year> 1992. </year>
Reference-contexts: Algebraic specifications Algebraic specifications describe systems constructively in terms of applications of system operations. States are defined by construction from some defined basic structure, and passed as parameters. Examples of algebraic specification notations are Larch [GHW85], OBJ [Gog84], and RSL <ref> [RAI92] </ref>. CHAPTER 1. INTRODUCTION 9 Model-based specifications Model-based specifications construct explicit models of the system state and show how the state can be changed by various operations. Examples of model-based specification notations are RSL [RAI92], VDM [Jon90], and Z [Spi92, BN92]. <p> Examples of algebraic specification notations are Larch [GHW85], OBJ [Gog84], and RSL <ref> [RAI92] </ref>. CHAPTER 1. INTRODUCTION 9 Model-based specifications Model-based specifications construct explicit models of the system state and show how the state can be changed by various operations. Examples of model-based specification notations are RSL [RAI92], VDM [Jon90], and Z [Spi92, BN92]. In this thesis, we use the Z notation [Spi92, BN92]. Z is a model-based specification notation, which was developed by J.-R. Abrial and the Programming Research Group at Oxford University.
Reference: [RAO92] <author> D. J. Richardson, S. L. Aha, and T. O. O'Malley. </author> <title> Specification-based test oracles for reactive systems. </title> <booktitle> In Proceedings of the 14th International Conference on Software Engineering, </booktitle> <pages> pages 105-118, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: This is preliminary work on data refinement and is concerned with demonstrating that the more concrete specification of the data type implements the abstract specification of the data type. The importance of oracles is being taken more and more seriously, as is demonstrated by Richardson et al. <ref> [RAO92] </ref>, who argue `test oracles should be derived from specifications in conjunction with testing criteria, represented in a common form, and their use made integral to the testing process'. The underlying approach is not limited to any particular specification language, though [RAO92] presents examples using the RTL and Z notations. <p> more seriously, as is demonstrated by Richardson et al. <ref> [RAO92] </ref>, who argue `test oracles should be derived from specifications in conjunction with testing criteria, represented in a common form, and their use made integral to the testing process'. The underlying approach is not limited to any particular specification language, though [RAO92] presents examples using the RTL and Z notations. The approach is to construct mappings CHAPTER 2. SPECIFICATION-BASED TESTING ISSUES 18 from the name spaces of the specification and implementation to the name space of the oracle. Usually, the oracle name space is the same as the specification name space. <p> Our model of oracles is simply to specify expected results; checking that the actual results match the expected results is not addressed. It could be done manually, of course, but there is room for applying and experimenting with other techniques, such as those discussed in <ref> [GMH81, Hay86, RAO92] </ref>. The reification model also requires extension. At present, it only deals with data reification, and is incomplete. Procedural reification also needs to be considered, especially the effects of procedural reification on specification/implementation structure and white-box tests.
Reference: [RC85] <author> D. J. Richardson and L. A. Clarke. </author> <title> Partition analysis: A method combining testing and verification. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 11(12) </volume> <pages> 1447-1490, </pages> <month> December </month> <year> 1985. </year> <note> BIBLIOGRAPHY 150 </note>
Reference-contexts: Random testing is a useful benchmark when considering other testing strategies (e.g., [DN84, HT88]). Random tests can be generated from a specification of the structure of the input. Partitioning CHAPTER 1. INTRODUCTION 6 There are many input partitioning approaches to software testing (e.g., <ref> [Bei90, OB88, RC85, WO80] </ref>). The common approach of each is to divide the input domain into sub-domains for which each element has the same error-detecting ability as any other.
Reference: [Ros92] <author> G. A. Rose. </author> <title> The ISDM case study: A dependency management system (Z and Object-Z specification). </title> <note> SVRC working paper. </note> <institution> Software Verification Research Centre, The University of Queensland, </institution> <year> 1992. </year>
Reference-contexts: An operational profile of the DMS shows that CanAdd is the most used operation, and it also has the most interesting specification. 6.3.1 Specification The specifications of the DMS and CanAdd are drawn from <ref> [Ros92] </ref>. The dependency management system keeps track of dependencies between nodes. In the context of the surrounding proof system, nodes will represent axioms or theories. The basic DMS maintains a set of nodes, direct dependencies between nodes, and inferred transitive dependencies between nodes. CHAPTER 6.
Reference: [ROT89] <author> D. J. Richardson, O. O'Malley, and C. Tittle. </author> <title> Approaches to specification-based testing. </title> <journal> Software Engineering Notes, </journal> <volume> 14(8) </volume> <pages> 86-96, </pages> <month> December </month> <year> 1989. </year> <booktitle> Proceedings of the ACM SIGSOFT '89 Third Symposium on Software Testing, Analysis, and Verification (TAV3). </booktitle>
Reference-contexts: Function signatures are reduced to their basic types, and the input and output spaces are analysed to extract sets of typical values according to various domain CHAPTER 2. SPECIFICATION-BASED TESTING ISSUES 15 partitions. This is essentially category partitioning [OB88] of both the input and output spaces. Richardson et al. <ref> [ROT89] </ref> examine strategies for selecting tests by extending implementation-based testing techniques to apply to formal specifications. Testing strategies are classified as error-based or fault-based.
Reference: [SC91] <author> P. Stocks and D. A. Carrington. </author> <title> Deriving software test cases from formal specifications. </title> <booktitle> In 6th Australian Software Engineering Conference, </booktitle> <pages> pages 327-340, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Secondly, in their pure form, the derived tests are as abstract as the specification, and some, perhaps instinctive, transformation from abstract to concrete takes place in producing the final test data. Specification reification (e.g., [Jon90, Mor90b]) has interesting applications to specification-based testing. Our previous work <ref> [SC91, SC93b] </ref> recognises the potential of rigorous reification methods for steadily increasing the white-box aspects of the test suite, and for ensuring accurate implementation-level representations of the tests. Discussion of these concepts is deferred to chapter 7. <p> This is especially effective if the specifier attempts to construct instances of invalid cases. Simple and common mistakes like off-by-one errors can remain hidden in large specifications that aren't well tested, and still be detected this way <ref> [SC91] </ref>. Part of the reason this is useful is that the invalid spaces present a different perspective on the specification and can often help the specifier overcome preconceptions and misconceptions if done carefully. Essentially this examination of input and output spaces is a focussed form of review.
Reference: [SC92] <author> P. Stocks and D. A. Carrington. </author> <title> The ISDM case study: A dependency management system (specification-based testing). </title> <note> SVRC working paper. </note> <institution> Software Verification Research Centre, The University of Queensland, </institution> <year> 1992. </year>
Reference-contexts: However, the purpose of this example is to demonstrate the new strategies introduced in the previous chapter, so we do not present this aspect of testing the DMS. This testing can be found in <ref> [SC92] </ref>. Specification mutation We now apply the specification mutation strategy. spec mut : STRATEGY The mutants are constructed by substituting or swapping types and variables, and applying the various mutations shown in appendix D to the specification. We work from this fully expanded specification of CanAdd CHAPTER 6.
Reference: [SC93a] <author> P. Stocks and D. A. Carrington. </author> <title> Test template framework: A specification-based testing case study. </title> <booktitle> In Proceedings of the International Symposium on Software Testing and Analysis (ISSTA'93), </booktitle> <pages> pages 11-18, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: The simpler strategies used in the first two studies proved insufficient for this case study, which prompted the development of the two new strategies introduced in chapter 5. This example shows these new strategies in action. A further case study demonstrating the new strategies can be found in <ref> [SC93a] </ref>. While this case study also highlights the need for these new strategies, we feel that it does not add much to the discussion here, 65 CHAPTER 6. EXAMPLES AND CASE STUDIES 66 and so it is not included.
Reference: [SC93b] <author> P. Stocks and D. A. Carrington. </author> <title> Test templates: A specification-based testing framework. </title> <booktitle> In Proceedings of the 15th International Conference on Software Engineering, </booktitle> <pages> pages 405-414, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Secondly, in their pure form, the derived tests are as abstract as the specification, and some, perhaps instinctive, transformation from abstract to concrete takes place in producing the final test data. Specification reification (e.g., [Jon90, Mor90b]) has interesting applications to specification-based testing. Our previous work <ref> [SC91, SC93b] </ref> recognises the potential of rigorous reification methods for steadily increasing the white-box aspects of the test suite, and for ensuring accurate implementation-level representations of the tests. Discussion of these concepts is deferred to chapter 7.
Reference: [SC93c] <author> P. Stocks and D. A. Carrington. </author> <title> Test templates: A specification-based testing framework. </title> <type> Technical Report 243, </type> <institution> Key Centre for Software Technology, Department of Computer Science, The University of Queens-land, </institution> <year> 1993. </year>
Reference-contexts: The specification is of a simplified read operation on files, and is based on the specification of the UNIX read operation in [Hay87]. This case study was used as a primary example of the framework in <ref> [SC93c] </ref>; the specification and testing below are drawn from this work. 6.2.1 Specification Files are modelled as bounded sequences of bytes.
Reference: [Sco88] <author> L. T. Scott. </author> <title> On the problem of software testing and the generation of test data. </title> <type> Master's thesis, </type> <institution> The University of Queensland, Queensland 4072, Australia, </institution> <year> 1988. </year> <note> BIBLIOGRAPHY 151 </note>
Reference-contexts: this example the minimum value for * is 1. * : fl We introduce the domain testing strategy. domain testing : STRATEGY Domain boundary DB 1 requires four test points: 2 ON and 2 OFF. [WC80] uses only 1 OFF point for this type of inequality, but, as noted in <ref> [Sco88] </ref>, another OFF point is required to distinguish this inequality from an equality.
Reference: [Scu88] <author> G. T. Scullard. </author> <title> Test case selection using VDM. </title> <editor> In R. Bloomfield, L. Marshall, and R. Jones, editors, </editor> <title> VDM '88 VDM- The Way Ahead, </title> <booktitle> Lecture Notes in Computer Science 328, </booktitle> <pages> pages 178-186. </pages> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: Simple partitions of the input space are constructed by examining the obvious divisions of input defined in the predicates of operations. This case analysis is highly structured, but not rigorous. Scullard uses a reduction technique to derive tests from VDM specifications <ref> [Scu88] </ref>. Function signatures are reduced to their basic types, and the input and output spaces are analysed to extract sets of typical values according to various domain CHAPTER 2. SPECIFICATION-BASED TESTING ISSUES 15 partitions. This is essentially category partitioning [OB88] of both the input and output spaces.
Reference: [Spi89] <author> J. M. Spivey. </author> <title> The Z Notation: A Reference Manual. </title> <booktitle> Series in Computer Science. </booktitle> <publisher> Prentice Hall International, </publisher> <year> 1989. </year> <note> Later edition available. </note>
Reference: [Spi92] <author> J. M. Spivey. </author> <title> The Z Notation: A Reference Manual. </title> <booktitle> Series in Computer Science. </booktitle> <publisher> Prentice Hall International, </publisher> <address> second edition, </address> <year> 1992. </year>
Reference-contexts: CHAPTER 1. INTRODUCTION 9 Model-based specifications Model-based specifications construct explicit models of the system state and show how the state can be changed by various operations. Examples of model-based specification notations are RSL [RAI92], VDM [Jon90], and Z <ref> [Spi92, BN92] </ref>. In this thesis, we use the Z notation [Spi92, BN92]. Z is a model-based specification notation, which was developed by J.-R. Abrial and the Programming Research Group at Oxford University. <p> CHAPTER 1. INTRODUCTION 9 Model-based specifications Model-based specifications construct explicit models of the system state and show how the state can be changed by various operations. Examples of model-based specification notations are RSL [RAI92], VDM [Jon90], and Z <ref> [Spi92, BN92] </ref>. In this thesis, we use the Z notation [Spi92, BN92]. Z is a model-based specification notation, which was developed by J.-R. Abrial and the Programming Research Group at Oxford University. Z is based on set theory and predicate calculus, and uses a schema calculus for defining states and operations. <p> The difference between schema types and schemas There is a subtle difference between schemas and schema types in Z, best illustrated with an example. Consider the following definitions with the type of the defined entity shown at its side. Bindings are described using the : : notation from <ref> [Spi92] </ref>. CHAPTER 4. <p> This section is intended to introduce the concept and some of the basic formalisms involved, and mainly to CHAPTER 7. THE FRAMEWORK IN THE LARGER PICTURE 113 whet our appetites for future research in this area. As such, we use the fundamental model of reification used in <ref> [Spi92] </ref>: between a specification (at the abstract level) and a reification of the specification (at the concrete level, though not necessarily the final implementation) there exists an abstraction relation, say Abs.
Reference: [Tan76] <author> A. S. Tanenbaum. </author> <title> In defense of program testing or correctness proofs considered harmful. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 11(5) </volume> <pages> 64-68, </pages> <month> May </month> <year> 1976. </year>
Reference-contexts: Despite the major limitation of testing that it can only show the presence of errors and never their absence 1 , it will always be a necessary verification technique. Lucid arguments to this effect can be found in <ref> [Tan76] </ref>. The community is also aware of the usefulness of formal methods for specifying and designing software. The accepted role of formal specifications in program verification is as the basis for proofs of correctness and rigorous transformation methodologies. However, formal specifications can play an important role in software testing.
Reference: [WC80] <author> L. J. White and E. I. Cohen. </author> <title> A domain strategy for computer program testing. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 6(3) </volume> <pages> 247-257, </pages> <month> May </month> <year> 1980. </year>
Reference-contexts: Domain testing Domain testing is a very successful technique for testing systems with linear input spaces <ref> [WC80, CHR82] </ref>. A program's control flow is analysed to partition the input into subdomains with linear domain boundaries. Errors in the control flow of the program will cause these boundaries to shift from their correct positions. <p> STRATEGIES 49 determined, and then the input partition is based on the input domains that map to the identified output domains. This output partition is determined by reducing the output expression to disjunctive normal form. Domain testing Domain testing <ref> [WC80] </ref> uses the control flow of a program to partition its input space. The path predicates form boundaries of the various input domains in the program's input space. The strategy tests for domain errors by checking whether the domain borders are in the correct position. <p> DNF partitioning Determine input domains making a disjunctive normal form partition of the input space. Boundary analysis Test the values occurring at the boundaries of ranges, particularly in bounded data structures and conditional expressions. Domain testing White's and Cohen's domain testing <ref> [WC80] </ref>. Construct linear representation for input domains, and test particular points on and near the domain boundaries. Of the restrictions to the applicability of domain testing, the only one CHAPTER 6. <p> The domain boundaries are determined by the simple predicates over the valid input space. Points to verify the boundaries are chosen in accordance with the strategy presented in <ref> [WC80] </ref>. These tests guarantee that the boundaries are correct (up to a small error margin), i.e., that the control flow is correct. <p> In the discrete space of this example the minimum value for * is 1. * : fl We introduce the domain testing strategy. domain testing : STRATEGY Domain boundary DB 1 requires four test points: 2 ON and 2 OFF. <ref> [WC80] </ref> uses only 1 OFF point for this type of inequality, but, as noted in [Sco88], another OFF point is required to distinguish this inequality from an equality. <p> The domain propagation templates do not force this depth of transitivity. Naturally, we should include a special test to distinguish any mutants not detected by other tests. 7.2.2 Strategies The test template framework provides common ground for comparing and contrasting testing strategies. For example, an aim of domain testing <ref> [WC80, CHR82] </ref> is to derive the fewest test points possible to test each boundary. We saw in the fileread example from chapter 6 that domain testing did, indeed, derive significantly fewer CHAPTER 7.
Reference: [Wez90] <author> C. D. Wezeman. </author> <title> The CO-OP method for compositional derivation of canonical testers. </title> <editor> In E. Brinksma, G. Scollo, and C. A. Vissers, editors, </editor> <title> Protocol Specification, Testing and Verification IX. </title> <publisher> North Holland, </publisher> <year> 1990. </year>
Reference-contexts: Cusack and Wezeman [CW93] derive labelled transition systems from Object-Z 1 specifications. The labelled transition systems are concerned with the external behaviour of the specification and the CO-OP method <ref> [Wez90] </ref> is used to derive canonical testers from the transition systems which test conformance of the external behaviours of the implementations to the specifications. This is similar to methods used in conformance testing based on process algebras.
Reference: [WJ91] <author> E. J. Weyuker and B. Jeng. </author> <title> Analyzing partition testing strategies. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(7) </volume> <pages> 703-711, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: This calls into question any confidence gained in an implementation through partition testing strategies [HT88]. Weyuker and Jeng compared partition testing and random testing on an analytical basis and determined circumstances in which each strategy excels <ref> [JW89, WJ91] </ref>. For example, their fourth observation states that if all partitions are of the same size and the same number of tests for each partition is chosen, then the test set is at least as good as a random test set. <p> This is CHAPTER 7. THE FRAMEWORK IN THE LARGER PICTURE 123 not to say that these criteria cannot be checked for tests derived using the framework, we just cannot make a formal statement of the criteria. However, some criteria are expressible. For example, <ref> [WJ91] </ref> lists a number of properties of partition testing (using random selection of tests within partitions) useful for comparing the test set with a collection of random tests.
Reference: [WO80] <author> E. J. Weyuker and T. J. </author> <title> Ostrand. Theories of program testing and the application of revealing subdomains. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 6(3) </volume> <pages> 236-246, </pages> <month> May </month> <year> 1980. </year>
Reference-contexts: Random testing is a useful benchmark when considering other testing strategies (e.g., [DN84, HT88]). Random tests can be generated from a specification of the structure of the input. Partitioning CHAPTER 1. INTRODUCTION 6 There are many input partitioning approaches to software testing (e.g., <ref> [Bei90, OB88, RC85, WO80] </ref>). The common approach of each is to divide the input domain into sub-domains for which each element has the same error-detecting ability as any other. <p> The more general notion of generalising the results of a test is the basis of revealing subdomains, introduced by Weyuker and Ostrand <ref> [WO80] </ref>. A subdomain of the input is revealing if the implementation passes every test input in the subdomain, or the implementation fails every test input in the subdomain. Revealing subdomains CHAPTER 2. SPECIFICATION-BASED TESTING ISSUES 21 exhibit uniform testing behaviour. <p> The problem of shallow partitioning is greater in specification-based testing, because everything is expressed at such a high level. The domain propagation strategy 2 In the sense of Weyuker and Ostrand <ref> [WO80] </ref>. CHAPTER 5. STRATEGIES 51 addresses this shallow partitioning, providing a simple, yet rigorous, technique for partitioning the input space in a more realistic way. The domains of the input partition cannot necessarily be shown to be revealing, but they are a more accurate representation of the various input classes. <p> Not all domain subdivisions will enforce the entire input domain to be covered. For example, with an ideal, revealing domain-partitioning <ref> [WO80] </ref>, where the input is partitioned into the set of all error-causing inputs and the set of all correct inputs, one need only consider the error-causing domains. Checking these properties can be useful in detecting incorrect use of strategies when defining templates.
References-found: 80

