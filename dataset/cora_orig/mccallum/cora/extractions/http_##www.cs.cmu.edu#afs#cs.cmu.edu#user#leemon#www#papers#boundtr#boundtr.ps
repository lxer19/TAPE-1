URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/leemon/www/papers/boundtr/boundtr.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/leemon/www/papers/index.html
Root-URL: 
Email: rjw@ccs.neu.edu  bairdlc@wL.wpafb.af.mil  
Title: Tight Performance Bounds on Greedy Policies Based on Imperfect Value Functions  
Author: Ronald J. Williams Leemon C. Baird, III 
Date: 24, 1993  
Note: Wright-Patterson Air Force Base,  November  This work was supported by Grant IRI-8921275 from the National Science Foundation and by the U. S. Air Force.  
Address: Boston, MA 02115  OH 45433-6543  
Affiliation: College of Computer Science Northeastern University  Wright Laboratory  
Abstract: Northeastern University College of Computer Science Technical Report NU-CCS-93-14 Abstract Consider a given value function on states of a Markov decision problem, as might result from applying a reinforcement learning algorithm. Unless this value function equals the corresponding optimal value function, at some states there will be a discrepancy, which is natural to call the Bellman residual, between what the value function specifies at that state and what is obtained by a one-step lookahead along the seemingly best action at that state using the given value function to evaluate all succeeding states. This paper derives a tight bound on how far from optimal the discounted return for a greedy policy based on the given value function will be as a function of the maximum norm magnitude of this Bellman residual. A corresponding result is also obtained for value functions defined on state-action pairs, as are used in Q-learning. One significant application of these results is to problems where a function approximator is used to learn a value function, with training of the approximator based on trying to minimize the Bellman residual across states or state-action pairs. When 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bertsekas, D. P. </author> <year> (1987). </year> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <address> Engle-wood Cliffs, NJ: </address> <publisher> Prentice Hall. </publisher>
Reference: <author> Bradtke, S. J. </author> <year> (1993). </year> <title> Reinforcement learing applied to linear quadratic regulation. </title> <editor> In: S. </editor> <publisher> J. </publisher>
Reference: <editor> Hanson, J. D. Cowan, & C. L. Giles (Eds.) </editor> <booktitle> Advances in Neural Information Processing Systems 5. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Moore, A. W. & Atkeson, C. G. </author> <year> (1993). </year> <title> Memory-based reinforcement learning: Converging with less data and less real time. </title> <editor> In: S. J. Hanson, J. D. Cowan, & C. L. Giles (Eds.) </editor> <booktitle> Advances in Neural Information Processing Systems 5. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Peng, J. </author> <year> (1993). </year> <title> Efficient Dynamic Programming-Based Learning for Control. </title> <type> Ph.D. Dissertation, </type> <institution> College of Computer Science, Northeastern University, </institution> <address> Boston, MA. </address>
Reference: <author> Peng, J. & Williams, R. J. </author> <year> (1993). </year> <title> Efficient learning and planning within the Dyna framework, </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 2, </volume> <pages> 437-454. </pages>
Reference: <author> Singh, S. P. </author> <year> (1993). </year> <title> Learning Control in Dynamic Environments. </title> <type> Ph.D. Dissertation, </type> <institution> Department of Computer Science, University of Massachusetts, </institution> <address> Amherst, MA. </address>
Reference: <author> Singh, S. P. & Yee, R. C. </author> <title> (To appear). An upper bound on the loss from approximate optimal-value functions. </title> <booktitle> Machine Learning. </booktitle>
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference-contexts: And most importantly for applications to learning, when training a function approximator to represent a value function on states (or state-action pairs, as considered below), the approach universally used is based on trying to minimize the individual temporal difference (TD) errors <ref> (Sutton, 1988) </ref>, which are closely related to the Bellman residual.
Reference: <author> Sutton, R. S. </author> <year> (1990). </year> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> Proceedings of the Seventh International Conference in Machine Learning, </booktitle> <pages> 216-224. </pages>
Reference: <author> Sutton, R. S. </author> <year> (1991). </year> <title> Planning by incremental dynamic programming. </title> <booktitle> Proceedings of the 8th International Machine Learning Workshop. </booktitle>
Reference: <author> Thrun, S. & Schwartz, A. </author> <title> (1993) Issues in using function approximation for reinforcement learning. </title> <booktitle> Proceedings of the Fourth Connectionist Models Summer School. </booktitle> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning from delayed rewards. </title> <type> Ph.D. Dissertation, </type> <institution> Cambridge University, </institution> <address> Cambridge, England. </address> <note> 19 Watkins, </note> <author> C. J. C. H. & Dayan, P. </author> <year> (1992). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 279-292. </pages>
Reference: <author> Werbos, P. J. </author> <year> (1990). </year> <title> Consistency of HDP applied to a simple reinforcement learning problem. </title> <booktitle> Neural Networks, </booktitle> <volume> 3, </volume> <pages> 179-189. 20 </pages>
References-found: 14

