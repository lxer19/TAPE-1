URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1993/tr-93-065.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1993.html
Root-URL: http://www.icsi.berkeley.edu
Title: An Efficient Probabilistic Context-Free Parsing Algorithm that Computes Prefix Probabilities  
Author: Andreas Stolcke 
Note: Abridged version to appear in Computational Linguistics 21(2), 1995. Present address: Speech  
Address: I 1947 Center St. Suite 600 Berkeley, California 94704-1198  333 Ravenswood Ave., Menlo Park, CA 94025,  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  Technology and Research Laboratory, SRI International,  
Pubnum: TR-93-065  
Email: e-mail stolcke@speech.sri.com.  
Phone: (510) 643-9153 FAX (510) 643-7684  
Date: November 1993 (Revised April 1995)  
Abstract: We describe an extension of Earley's parser for stochastic context-free grammars that computes the following quantities given a stochastic context-free grammar and an input string: a) probabilities of successive prefixes being generated by the grammar; b) probabilities of substrings being generated by the nonterminals, including the entire string being generated by the grammar; c) most likely (Viterbi) parse of the string; d) posterior expected number of applications of each grammar production, as required for reestimating rule probabilities. (a) and (b) are computed incrementally in a single left-to-right pass over the input. Our algorithm compares favorably to standard bottom-up parsing methods for SCFGs in that it works efficiently on sparse grammars by making use of Earley's top-down control structure. It can process any context-free rule format without conversion to some normal form, and combines computations for (a) through (d) in a single algorithm. Finally, the algorithm has simple extensions for processing partially bracketed inputs, and for finding partial parses and their likelihoods on ungrammatical inputs. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aho, Alfred V., & Jeffrey D. Ullman. </author> <year> 1972. </year> <title> The Theory of Parsing, Translation, and Compiling. Volume1: Parsing. </title> <address> Englewood Cliffs, N.J.: </address> <publisher> Prentice-Hall. </publisher>
Reference-contexts: This approach is a subject of ongoing work, in the context of tight-coupling SCFGs with speech decoders (Jurafsky et al. 1995). 6.3 Relation to probabilistic LR parsing One of the major alternative context-free parsing paradigms besides Earley's algorithm is LR parsing <ref> (Aho & Ullman 1972) </ref>. A comparison of the two approaches, both in their probabilistic and non-probabilistic aspects, is interesting and provides useful insights. The following remarks assume familiarity with both approaches.
Reference: <author> Bahl, Lalit R., Frederick Jelinek, & Robert L. Mercer. </author> <year> 1983. </year> <title> A maximum likelihood approach to continuous speech recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 5.179-190. </journal>
Reference-contexts: These conditional probabilities can then be used as word transition probabilities in a Viterbi-style decoder or to incrementally compute the cost function for a stack decoder <ref> (Bahl et al. 1983) </ref>. 1 Their paper phrases these problem in terms of context-free probabilistic grammars, but they generalize in obvious ways to other classes of models. 1 Another application where prefix probabilities play a central role is the extraction of n-gram probabilities from SCFGs (Stolcke & Segal 1994).
Reference: <author> Baker, James K. </author> <year> 1979. </year> <title> Trainable grammars for speech recognition. </title> <booktitle> In Speech Communication Papers for the 97th Meeting of the Acoustical Society of America, </booktitle> <editor> ed. by Jared J. Wolf & Dennis H. Klatt, </editor> <address> 547-550, </address> <publisher> MIT, </publisher> <address> Cambridge, Mass. </address>
Reference-contexts: Partial parses are assembled just as in non-probabilistic parsing (modulo possible pruning based on probabilities), while substring probabilities (also known as "inside" probabilities) can be computed in a straightforward way. Thus, the CYK chart parser underlies the standard solutions to problems (1) and (4) <ref> (Baker 1979) </ref>, as well as (2) (Jelinek 1985). While the Jelinek & Lafferty (1991) solution to problem (3) is not a direct extension of CYK parsing they nevertheless present their algorithm in terms of its similarities to the computation of inside probabilities. <p> This constitutes the main distinguishing feature of Earley parsing compared to the strict bottom-up computation used in the standard inside probability computation <ref> (Baker 1979) </ref>. There, inside probabilities for all positions and nonterminals are computed, regardless of possible prefixes. 4.4 Computing forward and inner probabilities Forward and inner probabilities not only subsume the prefix and string probabilities, they are also straightforward to compute during a run of Earley's algorithm. <p> All in all, we get the same time O (l 3 ), space O (l 2 ) bounds as in the Inside/Outside <ref> (Baker 1979) </ref> and LRI (Jelinek & Lafferty 1991) algorithms, with the advantage of better results on known grammar classes. 4.8.2 Scaling with grammar size We will not try to give a precise characterization in the case of sparse grammars (Appendix B.3 gives some hints on how to implement the algorithm efficiently <p> We chose the symbol R L in this paper to point to this difference. 21 Of course a CYK-style parser can operate left-to-right, right-to-left, or otherwise by reordering the computation of chart entries. 31 Full CNF Sparse CFG Bottom-up Inside/outside Stochastic RTNs <ref> (Baker 1979) </ref> (Kupiec 1992) Left-to-right LRI Probabilistic (Jelinek & Lafferty 1991) Earley Table 4: Tentative typology of SCFG algorithms according to prevailing directionality and sparseness of the CFG. to the arbitrary length of the right-hand sides of productions.
Reference: <author> Baum, Leonard E., Ted Petrie, George Soules, & Norman Weiss. </author> <year> 1970. </year> <title> A maximization technique occuring in the statistical analysis of probabilistic functions in Markov chains. </title> <journal> The Annals of Mathematical Statistics 41.164-171. </journal>
Reference-contexts: EM is a generalization of the well-known Baum-Welch algorithm for HMM estimation <ref> (Baum et al. 1970) </ref>; the original formulation for the case of SCFGs is due to Baker (1979). For SCFGs, the E-step involves computing the expected number of times each production is applied in generating the training corpus.
Reference: <author> Booth, Taylor L., & Richard A. Thompson. </author> <year> 1973. </year> <title> Applying probability measures to abstract languages. </title> <journal> IEEE Transactions on Computers C-22.442-450. </journal>
Reference: <author> Briscoe, Ted, & John Carroll. </author> <year> 1993. </year> <title> Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars. </title> <note> Computational Linguistics 19.25-59. </note>
Reference: <author> Casacuberta, F., & E. Vidal. </author> <year> 1988. </year> <title> A parsing algorithm for weighted grammars and substring recognition. In Syntactic and Structural Pattern Recognition, </title> <editor> ed. by Gabriel Ferrate, Theo Pavlidis, Alberto Sanfeliu, & Horst Bunke, </editor> <booktitle> volume F45 of NATO ASI Series, </booktitle> <pages> 51-67. </pages> <address> Berlin: </address> <publisher> Springer Verlag. </publisher>
Reference: <author> Corazza, Anna, Renato De Mori, Roberto Gretter, & Giorgio Satta. </author> <year> 1991. </year> <title> Computation of probabilities for an island-driven parser. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 13.936-950. </journal>
Reference-contexts: which defines a language as a probability distribution over strings, they have been used in a variety of applications: for the selection of parses for ambiguous inputs (Fujisaki et al. 1991); to guide the rule choice efficiently during parsing (Jones & Eisner 1992); to compute island probabilities for non-linear parsing <ref> (Corazza et al. 1991) </ref>. In speech recognition, probabilistic context-free grammars play a central role in integrating low-level word models with higher-level language models (Ney 1992), as well as in non-finite state acoustic and phonotactic modeling (Lari & Young 1991).
Reference: <author> Dempster, A. P., N. M. Laird, & D. B. Rubin. </author> <year> 1977. </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, Series B 34.1-38. </journal>
Reference-contexts: :Y ) as well as T 0 = Viterbi-parse (i : j Y ! -:) Adjoin T 0 to T as the right-most child at the root, and return T . 5.2 Rule probability estimation The rule probabilities in a SCFG can be iteratively estimated using the EM (Expectation-Maximization) algorithm <ref> (Dempster et al. 1977) </ref>.
Reference: <author> Earley, Jay. </author> <year> 1970. </year> <title> An efficient context-free parsing algorithm. </title> <journal> Communications of the ACM 6.451-455. </journal>
Reference: <author> Fujisaki, T., F. Jelinek, J. Cocke, E. Black, & T. Nishino. </author> <year> 1991. </year> <title> A probabilistic parsing method for sentence disambiguation. In Current Issues in Parsing Technology, ed. by Masaru Tomita, </title> <booktitle> chapter 10, </booktitle> <pages> 139-152. </pages> <address> Boston: </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: 1 Introduction Context-free grammars are widely used as models of natural language syntax. In their probabilistic version, which defines a language as a probability distribution over strings, they have been used in a variety of applications: for the selection of parses for ambiguous inputs <ref> (Fujisaki et al. 1991) </ref>; to guide the rule choice efficiently during parsing (Jones & Eisner 1992); to compute island probabilities for non-linear parsing (Corazza et al. 1991).
Reference: <author> Graham, Susan L., Michael A. Harrison, & Walter L. Ruzzo. </author> <year> 1980. </year> <title> An improved context-free recognizer. </title> <journal> ACM Transactions on Programming Languages and Systems 2.415-462. </journal>

Reference: <author> Jurafsky, Daniel, Chuck Wooters, Jonathan Segal, Andreas Stolcke, Eric Fosler, Gary Tajchman, & Nelson Morgan. </author> <year> 1995. </year> <title> Using a stochastic context-free grammar as a language model for speech recognition. </title> <booktitle> In Proceedings IEEE Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> 189-192. </pages> ||, <note> Chuck Wooters, </note> <author> Gary Tajchman, Jonathan Segal, Andreas Stolcke, Eric Fosler, & Nelson Morgan. </author> <year> 1994. </year> <title> The Berkeley Restaurant Project. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing , volume 4, </booktitle> <pages> 2139-2142, </pages> <address> Yokohama. </address>
Reference-contexts: On the contrary, by using Earley-style parsing with a set of carefully designed and estimated "fault tolerant" top-level productions, it should be possible to use probabilities to better advantage in robust parsing. This approach is a subject of ongoing work, in the context of tight-coupling SCFGs with speech decoders <ref> (Jurafsky et al. 1995) </ref>. 6.3 Relation to probabilistic LR parsing One of the major alternative context-free parsing paradigms besides Earley's algorithm is LR parsing (Aho & Ullman 1972). A comparison of the two approaches, both in their probabilistic and non-probabilistic aspects, is interesting and provides useful insights. <p> The parser now uses the method described here to provide exact SCFG prefix and next-word probabilities to a tightly-coupled speech decoder <ref> (Jurafsky et al. 1995) </ref>. An essential idea in the probabilistic formulation of Earley's algorithm is the collapsing of recursive predictions and unit completion chains, replacing both with lookups in precomputed matrices. This idea arises in our formulation out of the need to compute probability sums given as infinite series.
Reference: <author> Kupiec, Julian. </author> <year> 1992. </year> <title> Hidden Markov estimation for unrestricted stochastic context-free grammars. </title> <booktitle> In Proceedings IEEE Conference on Acoustics, Speech and Signal Processing , volume 1, </booktitle> <pages> 177-180, </pages> <address> San Francisco. </address>
Reference-contexts: We chose the symbol R L in this paper to point to this difference. 21 Of course a CYK-style parser can operate left-to-right, right-to-left, or otherwise by reordering the computation of chart entries. 31 Full CNF Sparse CFG Bottom-up Inside/outside Stochastic RTNs (Baker 1979) <ref> (Kupiec 1992) </ref> Left-to-right LRI Probabilistic (Jelinek & Lafferty 1991) Earley Table 4: Tentative typology of SCFG algorithms according to prevailing directionality and sparseness of the CFG. to the arbitrary length of the right-hand sides of productions.
Reference: <author> Lari, K., & S. J. Young. </author> <year> 1990. </year> <title> The estimation of stochastic context-free grammars using the Inside-Outside algorithm. </title> <booktitle> Computer Speech and Language 4.35-56. </booktitle> ||, & ||. <year> 1991. </year> <title> Applications of stochastic context-free grammars using the Inside-Outside algorithm. </title> <booktitle> Computer Speech and Language 5.237-257. </booktitle>
Reference-contexts: Unfortunately, it seems that in the case of unconstrained SCFG estimation local maxima present a very real problem, and make success dependent on chance 23 and initial conditions <ref> (Lari & Young 1990) </ref>. Pereira & Schabes (1992) showed that partially bracketed input samples can alleviate the problem in certain cases. The bracketing information constrains the parse of the inputs, and therefore the parameter estimates, steering it clear from some of the suboptimal solutions that could otherwise be found.
Reference: <author> Magerman, David M., & Mitchell P. Marcus. </author> <year> 1991. </year> <title> Pearl: A probabilistic chart parser. </title> <booktitle> In Proceedings of the 2nd International Workshop on Parsing Technologies, </booktitle> <pages> 193-199, </pages> <address> Cancun, Mexico. ||, & Carl Weir. </address> <year> 1992. </year> <title> Efficiency, robustness and accuracy in Picky chart parsing. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> 40-47, </pages> <institution> University of Delaware, Newark, Delaware. </institution>
Reference: <author> Nakagawa, Sei-ichi. </author> <year> 1987. </year> <title> Spoken sentence recognition by time-synchronous parsing algorithm of context-free grammar. </title> <booktitle> In Proceedings IEEE Conference on Acoustics, Speech and Signal Processing , volume 2, </booktitle> <pages> 829-832, </pages> <address> Dallas, Texas. </address>
Reference-contexts: In some work, context-free grammars are combined with scoring functions that are not strictly probabilistic <ref> (Nakagawa 1987) </ref>, or they are used with context-sensitive and/or semantic probabilities (Magerman & Marcus 1991; Magerman & Weir 1992; Jones & Eisner 1992; Briscoe & Carroll 1993).
Reference: <author> Ney, Hermann. </author> <year> 1992. </year> <title> Stochastic grammars and pattern recognition. In Speech Recognition and Understanding. Recent Advances, Trends, and Applications, ed. by Pietro Laface & Renato De Mori, </title> <booktitle> volume F75 of NATO ASI Series, </booktitle> <pages> 319-344. </pages> <address> Berlin: Springer Verlag. </address> <booktitle> Proceedings of the NATO Advanced Study Institute, </booktitle> <address> Cetraro, Italy, </address> <month> July </month> <year> 1990. </year> <title> P aseler, </title> <address> Annedore. </address> <year> 1988. </year> <title> Modification of Earley's algorithm for speech recognition. In Recent Advances in Speech Understanding and Dialog Systems, </title> <editor> ed. by H. Niemann, M. Lang, & G. Sagerer, </editor> <booktitle> volume F46 of NATO ASI Series, </booktitle> <pages> 466-472. </pages> <address> Berlin: Springer Verlag. </address> <booktitle> Proceedings of the NATO Advanced Study Institute, </booktitle> <address> Bad Windsheim, Germany, </address> <month> July </month> <year> 1987. </year>
Reference-contexts: In speech recognition, probabilistic context-free grammars play a central role in integrating low-level word models with higher-level language models <ref> (Ney 1992) </ref>, as well as in non-finite state acoustic and phonotactic modeling (Lari & Young 1991).
Reference: <author> Pereira, Fernando, & Yves Schabes. </author> <year> 1992. </year> <title> Inside-outside reestimation from partially bracketed corpora. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> 128-135, </pages> <institution> University of Delaware, Newark, Delaware. </institution>
Reference: <author> Pereira, Fernando C. N., & Stuart M. Shieber. </author> <year> 1987. </year> <title> Prolog and Natural-Language Analysis. </title> <booktitle> Number 10 in CSLI Lecture Notes Series. Stanford, CA: Center for the Study of Language and Information. </booktitle>
Reference: <author> Rabiner, L. R., & B. H. Juang. </author> <year> 1986. </year> <title> An introduction to hidden Markov models. </title> <journal> IEEE ASSP Magazine 3.4-16. </journal> <volume> 34 Schabes, </volume> <pages> Yves, </pages> <year> 1991. </year> <title> An inside-outside algorithm for estimating the parameters of a hidden stochastic context-free grammar based on Earley's algorithm. Unpublished mss. </title> <booktitle> Presented at the Second Workshop on Mathematics of Language, </booktitle> <address> Tarritown, N.Y., </address> <month> May </month> <year> 1991. </year>
Reference-contexts: This can be accomplished by attaching two probabilistic quantities to each Earley state, as follows. The terminology is derived from analogous or similar quantities commonly used in the literature on Hidden Markov Models (HMMs) <ref> (Rabiner & Juang 1986) </ref> and in Baker (1979). 7 Definition 4 The following definitions are relative to an implied input string x. a) The forward probability ff i ( k X ! :) is the sum of the probabilities of all constrained paths of length i that end in state k <p> Both the definition of Viterbi parse, and its computation are straightforward generalizations of the corresponding notion for Hidden Markov Models <ref> (Rabiner & Juang 1986) </ref>, where one computes the Viterbi path (state sequence) through an HMM. Precisely the same approach can be used in the Earley parser, using the fact that each derivation corresponds to a path. The standard computational technique for Viterbi parses is applicable here. <p> obtained from the chart will then contain precisely the maximal ones. 6 Discussion 6.1 Relation to finite-state models Throughout the exposition of the Earley algorithm and its probabilistic extension we have been alluding, in concepts and terminology, to the algorithms used with probabilistic finite-state models, in particular Hidden Markov Models <ref> (Rabiner & Juang 1986) </ref>. Many concepts carry over, if suitably generalized, most notably that of forward probabilities.
Reference: <author> Stolcke, Andreas, & Jonathan Segal. </author> <year> 1994. </year> <title> Precise n-gram probabilities from stochastic context-free grammars. </title> <booktitle> In Proceedings of the 31th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> 74-79, </pages> <address> New Mexico State University, Las Cruces, NM. </address>
Reference-contexts: function for a stack decoder (Bahl et al. 1983). 1 Their paper phrases these problem in terms of context-free probabilistic grammars, but they generalize in obvious ways to other classes of models. 1 Another application where prefix probabilities play a central role is the extraction of n-gram probabilities from SCFGs <ref> (Stolcke & Segal 1994) </ref>. Here, too, efficient incremental computation saves time since the work for common prefix strings can be shared. The key to most of the features of our algorithm is that it is based on the top-down parsing method for non-probabilistic CFGs developed by Earley (1970).
Reference: <author> Tomita, Masaru. </author> <year> 1986. </year> <title> Efficient Parsing for Natural Language. </title> <address> Boston: </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: Generalized LR parsing is an extension that allows parallel tracking of multiple state transitions and stack actions by using a graph-structured stack <ref> (Tomita 1986) </ref>. Probabilistic LR parsing (Wright 1990) is based on LR items augmented with certain conditional probabilities. <p> However, the size of LR parser tables can be exponential in the size of the grammar (due to the number of potential item subsets). Furthermore, if the generalized LR method is used for dealing with non-deterministic grammars <ref> (Tomita 1986) </ref> the runtime on arbitrary inputs may also grow exponentially. The bottom line is that each application's needs have to be evaluated against the pros and cons of both approaches to find the best solution.
Reference: <author> Wright, J. H. </author> <year> 1990. </year> <title> LR parsing of probabilistic grammars with input uncertainty for speech recognition. </title> <booktitle> Computer Speech and Language 4.297-323. </booktitle> <pages> 35 </pages>
Reference-contexts: Generalized LR parsing is an extension that allows parallel tracking of multiple state transitions and stack actions by using a graph-structured stack (Tomita 1986). Probabilistic LR parsing <ref> (Wright 1990) </ref> is based on LR items augmented with certain conditional probabilities.
References-found: 24

