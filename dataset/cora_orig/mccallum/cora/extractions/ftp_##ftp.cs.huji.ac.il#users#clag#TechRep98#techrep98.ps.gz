URL: ftp://ftp.cs.huji.ac.il/users/clag/TechRep98/techrep98.ps.gz
Refering-URL: http://www.cs.huji.ac.il/~clag/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: clag@cs.huji.ac.il, jeff@cs.huji.ac.il  
Title: Mutual Adaptation Enhanced by Social Laws  
Author: Claudia V. Goldman Jeffrey S. Rosenschein 
Keyword: Multiagent Learning, Social Laws.  
Note: Supported by the Eshkol Fellowship, Israeli Ministry of Science  
Address: Givat Ram, Jerusalem, Israel  
Affiliation: Institute of Computer Science The Hebrew University  
Abstract: We investigate the issue of multiagent learning with focus on the interactions among the learning agents. Each agent learns directly from the other agents that operate in the same environment. Each agent computes a function that expresses its satisfaction level after it performs an action, and after the other agents have expressed their degree of approval of the agent's behavior. Each agent adjusts its behavior based on an adaptation criterion applied to its satisfaction level. Our aim is that the agents learn to adapt their behaviors. In this work, we impose a social law on the agents' behavior while they learn to adapt to the other agents in the environment. Each agent has attached a level of cooperation that induces it to follow the rule to different degrees. We show that the level of mutual adaptation of the agents can increase when they follow a social law. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Minoru Asada, Eiji Uchibe, and Koh Hosoda. </author> <title> Agents that learn from other competitive agents. </title> <booktitle> In Machine Learning: Proceedings of the Twelfth international conference,Workshop on Agents that Learn from Other Agents, </booktitle> <year> 1995. </year> <month> 12 </month>
Reference-contexts: In addition, when agents build models of the other agents, there is always a level of uncertainty about the agents' next actions. The learning approach in multiagent systems has focussed mainly on agents that learn from their environment <ref> [5, 8, 15, 14, 10, 11, 9, 1] </ref>. Thus personal influences of specific agents on other specific agents cannot be taken into consideration.
Reference: [2] <author> Claudia V. Goldman. </author> <title> Emergent coordination through the use of cooperative state--changing rules. </title> <type> Master's thesis, </type> <institution> Hebrew University, </institution> <year> 1993. </year>
Reference-contexts: Otherwise, if the agents coordinate K &lt; * times, we can claim that the agents in this system are * adaptive. Second, we need to define the level of cooperation of an agent <ref> [3, 2] </ref>. This is a parameter that the agent's designer decides on and attaches to its agent. The problem of determining the level of cooperation for an agent is domain dependent.
Reference: [3] <author> Claudia V. Goldman and Jeffrey S. Rosenschein. </author> <title> Emergent coordination through the use of cooperative state-changing rules. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 408-413, </pages> <address> Seattle, Washington, </address> <year> 1994. </year>
Reference-contexts: We have combined the idea of state changing rules <ref> [3] </ref> or social laws [12, 13] with the adaptation algorithm. <p> In this work we will concentrate on cooperative agents, which means that each agent weighs equally its own gain and the feedback of others. In addition, we here combine the mutual adaptation approach together with the social laws paradigm <ref> [3, 12, 13] </ref>. The main question we want to investigate is whether a social law imposed on a multiagent system can enhance the level of mutual adaptation of the agents. <p> Otherwise, if the agents coordinate K &lt; * times, we can claim that the agents in this system are * adaptive. Second, we need to define the level of cooperation of an agent <ref> [3, 2] </ref>. This is a parameter that the agent's designer decides on and attaches to its agent. The problem of determining the level of cooperation for an agent is domain dependent.
Reference: [4] <author> Claudia V. Goldman and Jeffrey S. Rosenschein. </author> <title> Societies of intelligent software agents. to be submitted. Adaptive Behavior Journal, Special Issue on Simulation Models of Social Agents, </title> <year> 1998. </year>
Reference-contexts: 1 Introduction The Satisfaction Level approach <ref> [4] </ref> is a new way to look at the process of adaptation in a multiagent system. The problem we face is that of a society of agents (not necessarily cooperative nor homogeneous) that act in the same environment. <p> The aim of the algorithm is that the agents change their actions so that they will increasingly adapt to each other during the time they interact. Experiments showing the emergent behaviors we observed were described in <ref> [4] </ref>. We have also tested whether the agents' characteristics can change the influence of the algorithm in such a way that the agent will be in a better situation relative to its goal. This can be expressed by the agent's type. <p> multiagent system where the agents adapt to each other dynamically. 1 After the simulations we have run we can suggest a pair of levels of cooperation, that are likely to improve the level of mutual adaptation of the system. 2 A Multi-Adaptive Agent System We proposed an adaptation algorithm in <ref> [4] </ref>, where we looked at a multiagent system as composed of teachers and learners. Each agent plays at each time both roles. Each agent chooses an action to perform, and then it computes its level of satisfaction. This function is composed of two terms. <p> This shows that although our approach is to have agents that learn to adapt to each other, we also have to consider the self interests of each of the agents during the process of mutual adaptation. In the work presented in <ref> [4] </ref> we investigate the influences that agents with different types might have on the level of adaptation of the system. We handled nine situations that combine agents of the following types: selfish, altruistic, cooperative. <p> To learn about adaptive strategies that can be developed by agents dynamically, we have been experimenting with a single traffic intersection <ref> [4] </ref>. There are two agents; i.e., two traffic signal controllers. A 1 is responsible for the traffic flow in the vertical direction, while A 2 is in charge of the horizontal road. Time is considered to be discrete. The traffic at each road is represented by a simple queue.
Reference: [5] <editor> John J. Grefenstette. </editor> <booktitle> Lamarckian learning in multi-agent environments. In Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <pages> pages 303|310, </pages> <year> 1991. </year>
Reference-contexts: In addition, when agents build models of the other agents, there is always a level of uncertainty about the agents' next actions. The learning approach in multiagent systems has focussed mainly on agents that learn from their environment <ref> [5, 8, 15, 14, 10, 11, 9, 1] </ref>. Thus personal influences of specific agents on other specific agents cannot be taken into consideration.
Reference: [6] <author> Michael L. Littman. </author> <title> Markov games as a framework for multi-agent reinforcement learning. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh international conference, </booktitle> <pages> pages 157|163, </pages> <year> 1994. </year>
Reference-contexts: Another problem is there might not exist a central agent or program that can follow everything that is happening in a specific environment. Littman also looked at Markov games that deal with the specific actions taken by the agents and how they pay back other agents. In his paper <ref> [6] </ref> he mentioned that this is a simple model that cannot deal with agents that learn to cooperate. Mataric [7] also mentions a function that expresses three types of reinforcement including the direct reinforcement from the agents acting in the same world. In this case the agents are homogeneous.
Reference: [7] <editor> Maja J. Mataric. </editor> <title> Learning to behave socially. </title> <booktitle> In Proceedings From Animals to Animats 3, Third International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 157|163, </pages> <year> 1994. </year>
Reference-contexts: Littman also looked at Markov games that deal with the specific actions taken by the agents and how they pay back other agents. In his paper [6] he mentioned that this is a simple model that cannot deal with agents that learn to cooperate. Mataric <ref> [7] </ref> also mentions a function that expresses three types of reinforcement including the direct reinforcement from the agents acting in the same world. In this case the agents are homogeneous.
Reference: [8] <editor> Maja J. Mataric. </editor> <title> Reward functions for accelerated learning. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh international conference, </booktitle> <pages> pages 157|163, </pages> <year> 1994. </year>
Reference-contexts: In addition, when agents build models of the other agents, there is always a level of uncertainty about the agents' next actions. The learning approach in multiagent systems has focussed mainly on agents that learn from their environment <ref> [5, 8, 15, 14, 10, 11, 9, 1] </ref>. Thus personal influences of specific agents on other specific agents cannot be taken into consideration.
Reference: [9] <author> Tuomas W. Sandholm and Robert H. Crites. </author> <title> Multiagent reinforcement learning in the iterated prisoner's dilemma. </title> <journal> Biosystems Journal Special Issue on the Prisoner's Dilemma, </journal> <note> submitted. </note>
Reference-contexts: In addition, when agents build models of the other agents, there is always a level of uncertainty about the agents' next actions. The learning approach in multiagent systems has focussed mainly on agents that learn from their environment <ref> [5, 8, 15, 14, 10, 11, 9, 1] </ref>. Thus personal influences of specific agents on other specific agents cannot be taken into consideration.
Reference: [10] <author> S. Sen, M. Sekaran, and J. Hale. </author> <title> Learning to coordinate without sharing information. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 426-431, </pages> <year> 1994. </year>
Reference-contexts: In addition, when agents build models of the other agents, there is always a level of uncertainty about the agents' next actions. The learning approach in multiagent systems has focussed mainly on agents that learn from their environment <ref> [5, 8, 15, 14, 10, 11, 9, 1] </ref>. Thus personal influences of specific agents on other specific agents cannot be taken into consideration.
Reference: [11] <author> Sandip Sen and Mahendra Sekaran. </author> <title> Multiagent coordination with learning classifier systems. </title> <booktitle> In Workshop on Adaptation and Learningin Multiagent Systems at the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <address> Montreal, Quebec, </address> <year> 1995. </year>
Reference-contexts: In addition, when agents build models of the other agents, there is always a level of uncertainty about the agents' next actions. The learning approach in multiagent systems has focussed mainly on agents that learn from their environment <ref> [5, 8, 15, 14, 10, 11, 9, 1] </ref>. Thus personal influences of specific agents on other specific agents cannot be taken into consideration.
Reference: [12] <author> Yoav Shoham and Moshe Tennenholtz. </author> <title> On the synthesis of useful social laws for artificial agent societies (preliminary report). </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <address> San Jose, California, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: We have combined the idea of state changing rules [3] or social laws <ref> [12, 13] </ref> with the adaptation algorithm. <p> In this work we will concentrate on cooperative agents, which means that each agent weighs equally its own gain and the feedback of others. In addition, we here combine the mutual adaptation approach together with the social laws paradigm <ref> [3, 12, 13] </ref>. The main question we want to investigate is whether a social law imposed on a multiagent system can enhance the level of mutual adaptation of the agents.
Reference: [13] <author> Yoav Shoham and Moshe Tennenholtz. </author> <title> On social laws for artificial agent societies: Off-line design. </title> <journal> Artificial Intelligence, </journal> <note> page to appear, 1993. 13 </note>
Reference-contexts: We have combined the idea of state changing rules [3] or social laws <ref> [12, 13] </ref> with the adaptation algorithm. <p> In this work we will concentrate on cooperative agents, which means that each agent weighs equally its own gain and the feedback of others. In addition, we here combine the mutual adaptation approach together with the social laws paradigm <ref> [3, 12, 13] </ref>. The main question we want to investigate is whether a social law imposed on a multiagent system can enhance the level of mutual adaptation of the agents.
Reference: [14] <author> M. Tan. </author> <title> Multi-agent reinforcement learning: Independent vs. </title> <booktitle> cooperative agents. In Machine Learning: Proceedings of the Tenth international conference, </booktitle> <pages> pages 330|337, </pages> <year> 1993. </year>
Reference-contexts: In addition, when agents build models of the other agents, there is always a level of uncertainty about the agents' next actions. The learning approach in multiagent systems has focussed mainly on agents that learn from their environment <ref> [5, 8, 15, 14, 10, 11, 9, 1] </ref>. Thus personal influences of specific agents on other specific agents cannot be taken into consideration.
Reference: [15] <author> Christopher J.C.H. Watkins and Peter Dayan. </author> <note> Technical note Q-learning. Machine Learning, 8:279|292, 1992. 14 </note>
Reference-contexts: In addition, when agents build models of the other agents, there is always a level of uncertainty about the agents' next actions. The learning approach in multiagent systems has focussed mainly on agents that learn from their environment <ref> [5, 8, 15, 14, 10, 11, 9, 1] </ref>. Thus personal influences of specific agents on other specific agents cannot be taken into consideration.
References-found: 15

