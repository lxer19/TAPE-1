URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-93-1141/CS-TR-93-1141.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-93-1141/
Root-URL: http://www.cs.wisc.edu
Email: kyros@cs.wisc.edu  dyer@cs.wisc.edu  
Title: Global Surface Reconstruction By Purposive Control of Observer Motion  
Author: Kiriakos N. Kutulakos Charles R. Dyer 
Note: The support of the National Science Foundation under Grant Nos. IRI-9022608 and IRI-9220782 is gratefully acknowledged.  
Address: Madison, Wisconsin 53706 USA  
Affiliation: Computer Sciences Department University of Wisconsin  
Abstract: Technical Report #1141 April 1993 Abstract What real-time, qualitative viewpoint-control behaviors are important for performing global visual exploration tasks such as searching for specific surface markings, building a global model of an arbitrary object, or recognizing an object? In this paper we consider the task of purposefully controlling the motion of an active, monocular observer in order to recover a global description of a smooth, arbitrarily-shaped object. We formulate global surface reconstruction as the qualitative task of controlling the motion of the observer so that the visible rim slides over the maximal, connected, reconstructible surface regions intersecting the visible rim at the initial viewpoint. We show that these regions are bounded by a subset of the visual event curves defined on the surface. By studying the epipolar parameterization, we develop four basic behaviors that allow reconstruction of a surface patch around any point in a reconstructible surface region. These behaviors control viewpoint to achieve and maintain a well-defined geometric relationship with the object's surface, rely only on information extracted directly from images (e.g., tangents to the occluding contour), and are simple enough to be executed in real time. We then show how global surface reconstruction can be provably achieved by (1) appropriately integrating these behaviors to iteratively "grow" the reconstructed regions, and (2) obeying four simple rules. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. J. Gibson, </author> <title> The Ecological Approach to Visual Perception. </title> <publisher> Houghton Mi*in, </publisher> <year> 1979. </year>
Reference: [2] <author> E. J. Gibson, </author> <title> "Exploratory behavior in the development of perceiving, acting, and the acquiring of knowledge," </title> <journal> Ann. Rev. Psychol, </journal> <volume> vol. 39, </volume> <pages> pp. 1-41, </pages> <year> 1988. </year>
Reference-contexts: To date, a behavior-based approach for visually exploring an unknown, arbitrarily-shaped object has not been reported, although the significance of such an ability is clear <ref> [2, 6] </ref>. In this paper we present an approach that fills this gap.
Reference: [3] <author> A. L. Yarbus, </author> <title> Eye Movements and Vision. </title> <publisher> Plenum Press, </publisher> <year> 1967. </year>
Reference: [4] <author> R. L. Klatzky and S. J. Lederman, </author> <title> "The intelligent hand," </title> <journal> The Psychology of Learning and Motivation, </journal> <volume> vol. 21, </volume> <pages> pp. 121-151, </pages> <year> 1987. </year>
Reference-contexts: Behaviors implementing viewpoint control will correspond to object manipulations <ref> [4, 17, 18] </ref>, where an observer purposefully rotates a handheld object based on its appearance, or to observer motions, where the observer moves around an object.
Reference: [5] <author> J. J. Gibson, </author> <title> The Senses Considered as Perceptual Systems. </title> <publisher> Houghton Mi*in Company, </publisher> <year> 1966. </year>
Reference-contexts: It further suggests that visual information should be actively obtained rather than imposed <ref> [5] </ref>, e.g., through an a priori determined collection of images. Computer vision research has only recently started to investigate this approach to visual processing.
Reference: [6] <author> R. Bajcsy and M. Campos, </author> <title> "Active and exploratory perception," Computer Vision, Graphics, </title> <booktitle> and Image Processing: Image Understanding, </booktitle> <volume> vol. 56, no. 1, </volume> <pages> pp. 31-40, </pages> <year> 1992. </year>
Reference-contexts: It further suggests that visual information should be actively obtained rather than imposed [5], e.g., through an a priori determined collection of images. Computer vision research has only recently started to investigate this approach to visual processing. However, it is becoming increasingly evident that a behavior-based <ref> [6] </ref>, animate [7], and purposive [8] approach to visual processing holds great promise for developing abilities comparable to those of biological organisms; numerous recent results have shown that by using and combining simple behaviors that control the camera parameters (e.g., the direction of gaze), real-time and robust solutions to navigation, reconstruction <p> To date, a behavior-based approach for visually exploring an unknown, arbitrarily-shaped object has not been reported, although the significance of such an ability is clear <ref> [2, 6] </ref>. In this paper we present an approach that fills this gap.
Reference: [7] <author> D. H. Ballard and C. M. Brown, </author> <booktitle> "Principles of animate vision," Computer Vision, Graphics, and Image Processing: Image Understanding, </booktitle> <volume> vol. 56, no. 1, </volume> <pages> pp. 3-21, </pages> <year> 1992. </year> <note> Special Issue on Purposive, Qualitative, Active Vision. </note>
Reference-contexts: It further suggests that visual information should be actively obtained rather than imposed [5], e.g., through an a priori determined collection of images. Computer vision research has only recently started to investigate this approach to visual processing. However, it is becoming increasingly evident that a behavior-based [6], animate <ref> [7] </ref>, and purposive [8] approach to visual processing holds great promise for developing abilities comparable to those of biological organisms; numerous recent results have shown that by using and combining simple behaviors that control the camera parameters (e.g., the direction of gaze), real-time and robust solutions to navigation, reconstruction and tracking
Reference: [8] <author> Y. Aloimonos, </author> <title> "Purposive and qualitative active vision," </title> <booktitle> in Proc. Int. Conf. on Pattern Recognition, </booktitle> <pages> pp. 346-360, </pages> <year> 1990. </year>
Reference-contexts: Computer vision research has only recently started to investigate this approach to visual processing. However, it is becoming increasingly evident that a behavior-based [6], animate [7], and purposive <ref> [8] </ref> approach to visual processing holds great promise for developing abilities comparable to those of biological organisms; numerous recent results have shown that by using and combining simple behaviors that control the camera parameters (e.g., the direction of gaze), real-time and robust solutions to navigation, reconstruction and tracking tasks, for example, <p> There are two issues here: Efficiency, i.e., how efficiently the viewing parameters can be controlled to solve the task, and correctness, i.e., whether the task can always be accomplished by an appropriate combination of behaviors. Like many others (e.g., <ref> [8] </ref>), we are only interested in behaviors that can be executed in real time and that tightly couple sensing and action.
Reference: [9] <author> A. L. Abbott, </author> <title> "Selective fixation control for machine vision: A survey," </title> <booktitle> in Proc. IEEE Conf. Syst. Man Cybern., </booktitle> <year> 1991. </year>
Reference: [10] <author> D. H. Ballard and A. Ozcandarli, </author> <title> "Eye fixation and early vision: Kinetic depth," </title> <booktitle> in Proc. 2nd Int. Conf. on Computer Vision, </booktitle> <pages> pp. 524-531, </pages> <year> 1988. </year>
Reference: [11] <author> D. H. Ballard, </author> <title> "Reference frames for animate vision," </title> <booktitle> in Proc. Int. Joint Conf. Artificial Intelligence, </booktitle> <pages> pp. 1635-1641, </pages> <year> 1989. </year>
Reference: [12] <author> R. C. Nelson and J. Aloimonos, </author> <title> "Obstacle avoidance using flow field divergence," </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <volume> vol. 11, no. 10, </volume> <pages> pp. 1102-1106, </pages> <year> 1989. </year>
Reference: [13] <author> T. J. Olson and D. J. Coombs, </author> <title> "Real-time vergence control for binocular robots," </title> <journal> Int. J. Computer Vision, </journal> <volume> vol. 7, no. 1, </volume> <pages> pp. 67-89, </pages> <year> 1991. </year>
Reference: [14] <author> K. Pahlavan and J.-O. Eklundh, </author> <title> "A head-eye system | analysis and design," Computer Vision, Graphics, </title> <booktitle> and Image Processing: Image Understanding, </booktitle> <volume> vol. 56, no. 1, </volume> <pages> pp. 41-56, </pages> <year> 1992. </year> <note> Special Issue on Purposive, Qualitative, Active Vision. </note>
Reference-contexts: So how can one guarantee that a collection of behaviors will always perform a given task? The answer is to first isolate each behavior and study its effects (e.g., how each behavior controls a viewing parameter) and then, even more importantly, to study their interactions <ref> [14, 21-25] </ref>. We believe that it is in this long-range interaction and cooperation between the component behaviors that the solution to global geometric tasks lies.
Reference: [15] <author> E. Krotkov, </author> <title> "Focusing," </title> <journal> Int. J. Computer Vision, </journal> <volume> vol. 1, </volume> <pages> pp. 223-237, </pages> <year> 1987. </year>
Reference: [16] <author> R. Sharma and Y. Aloimonos, </author> <title> "Visual motion analysis under interceptive behavior," </title> <booktitle> in Proc. Computer Vision and Pattern Recognition, </booktitle> <pages> pp. 148-153, </pages> <year> 1992. </year> <month> 53 </month>
Reference: [17] <editor> R. Bajcsy and C. Tsikos, </editor> <title> "Perception via manipulation," </title> <booktitle> in Robotics Research 4 (R. </booktitle> <editor> C. Bolles and B. Roth, </editor> <booktitle> eds.), </booktitle> <pages> pp. 237-244, </pages> <publisher> MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: Behaviors implementing viewpoint control will correspond to object manipulations <ref> [4, 17, 18] </ref>, where an observer purposefully rotates a handheld object based on its appearance, or to observer motions, where the observer moves around an object.
Reference: [18] <author> C. J. Tsikos and R. K. </author> <title> Bajcsy, "Segmentation via manipulation," </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <volume> vol. 7, no. 3, </volume> <pages> pp. 306-319, </pages> <year> 1991. </year>
Reference-contexts: Behaviors implementing viewpoint control will correspond to object manipulations <ref> [4, 17, 18] </ref>, where an observer purposefully rotates a handheld object based on its appearance, or to observer motions, where the observer moves around an object.
Reference: [19] <author> D. H. Ballard, </author> <title> "Animate vision," </title> <journal> Artificial Intelligence, </journal> <volume> vol. 48, </volume> <pages> pp. 57-86, </pages> <year> 1991. </year>
Reference-contexts: This is because our purpose is to develop behaviors that exploit the ability to quickly interact with the environment to achieve simple and real-time solutions for a given task, rather than rely on the availability of large amounts of computational power <ref> [19] </ref>. Unfortunately, the correctness of a behavior or a collection of behaviors for performing a given task is much harder to evaluate. One way is to conduct an experimental validation, i.e., implement them and see how they perform under various conditions [20].
Reference: [20] <author> L. E. Wixson, </author> <title> "Exploiting world structure to efficiently search for objects," </title> <type> Tech. Rep. 434, </type> <institution> University of Rochester, </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: Unfortunately, the correctness of a behavior or a collection of behaviors for performing a given task is much harder to evaluate. One way is to conduct an experimental validation, i.e., implement them and see how they perform under various conditions <ref> [20] </ref>. Instead, before building and testing an experimental system, we take a theoretical approach to this problem. We prove that the behaviors used will always perform the task.
Reference: [21] <author> C. Brown, </author> <title> "Prediction and cooperation in gaze control," </title> <journal> Biological Cybernetics, </journal> <volume> vol. 63, </volume> <pages> pp. 61-70, </pages> <year> 1990. </year>
Reference: [22] <author> A. L. Abbott and N. Ahuja, </author> <title> "Active surface reconstruction by integrating focus, vergence, stereo, and camera calibration," </title> <booktitle> in Proc. 3rd Int. Conf. on Computer Vision, </booktitle> <pages> pp. 489-492, </pages> <year> 1990. </year>
Reference-contexts: It is well known that the occluding contour is a rich source of shape information [37-46]. Furthermore, contrary to approaches that use sophisticated sensing mechanisms to reconstruct the scene from a single viewpoint or a small number of viewpoints <ref> [22, 33, 34] </ref>, recent results demonstrate that 4 the occluding contour can be reliably detected in edge images [27, 47-49], and that quantitative shape information (e.g., curvature) can be efficiently and accurately recovered from the occluding contour [27, 31].
Reference: [23] <author> E. Krotkov, </author> <title> Active Computer Vision by Cooperative Focus and Stereo. </title> <publisher> Springer-Verlag, </publisher> <year> 1989. </year>
Reference: [24] <author> R. D. Rimey and C. M. Brown, </author> <title> "Task-oriented vision with multiple bayes nets," in Active Vision (A. </title> <editor> Blake and A. Yuille, </editor> <booktitle> eds.), </booktitle> <pages> pp. 217-236, </pages> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference: [25] <author> R. A. Brooks, </author> <title> "A robot that walks: Emergent behaviors from a carefully evolved network," </title> <booktitle> in Proc. IEEE Robotics Automat. Conf., </booktitle> <pages> pp. 692-696, </pages> <year> 1989. </year>
Reference: [26] <author> Y. Aloimonos, I. Weiss, and A. Bandyopadhyay, </author> <title> "Active vision," </title> <booktitle> in Proc. 1st Int. Conf. on Computer Vision, </booktitle> <pages> pp. 35-54, </pages> <year> 1987. </year>
Reference-contexts: We use a shape-from-motion module for extracting surface shape information <ref> [26, 27] </ref>. We consider this task for smooth surfaces of arbitrary shape; the viewed object is unknown, can be non-convex, and can self-occlude. <p> Moreover, in the context of animate and purposive vision, their major drawback is their inability to exploit the real-time control of the observer's viewpoint to simplify the procedure for solving the task (e.g., by utilizing shape-from-motion approaches, which have been shown to considerably simplify shape recovery computations <ref> [26, 27, 36] </ref>), as well as to simplify the viewpoint control process itself. 1.1 Global Reconstruction from the Occluding Contour The behaviors we develop in this paper control the observer's motion so that surface shape information can be recovered from a dense sequence of images using a shape-from-motion module.
Reference: [27] <author> R. Cipolla and A. Blake, </author> <title> "Surface shape from the deformation of apparent contours," </title> <journal> Int. J. Computer Vision, </journal> <volume> vol. 9, no. 2, </volume> <pages> pp. 83-112, </pages> <year> 1992. </year>
Reference-contexts: We use a shape-from-motion module for extracting surface shape information <ref> [26, 27] </ref>. We consider this task for smooth surfaces of arbitrary shape; the viewed object is unknown, can be non-convex, and can self-occlude. <p> Moreover, in the context of animate and purposive vision, their major drawback is their inability to exploit the real-time control of the observer's viewpoint to simplify the procedure for solving the task (e.g., by utilizing shape-from-motion approaches, which have been shown to considerably simplify shape recovery computations <ref> [26, 27, 36] </ref>), as well as to simplify the viewpoint control process itself. 1.1 Global Reconstruction from the Occluding Contour The behaviors we develop in this paper control the observer's motion so that surface shape information can be recovered from a dense sequence of images using a shape-from-motion module. <p> Furthermore, contrary to approaches that use sophisticated sensing mechanisms to reconstruct the scene from a single viewpoint or a small number of viewpoints [22, 33, 34], recent results demonstrate that 4 the occluding contour can be reliably detected in edge images <ref> [27, 47-49] </ref>, and that quantitative shape information (e.g., curvature) can be efficiently and accurately recovered from the occluding contour [27, 31]. <p> reconstruct the scene from a single viewpoint or a small number of viewpoints [22, 33, 34], recent results demonstrate that 4 the occluding contour can be reliably detected in edge images [27, 47-49], and that quantitative shape information (e.g., curvature) can be efficiently and accurately recovered from the occluding contour <ref> [27, 31] </ref>. <p> The deformations of the occluding contour have been studied by Giblin and Weiss [50], Cipolla and Blake <ref> [27] </ref>, Vaillant and Faugeras [47], and Koenderink [51, 52], while the contour's topological changes have been an active research topic for more than a decade (e.g., [52-54]). Much of the analysis we present in this paper builds on this work. <p> The occluding contour consists of a single curve whose endpoints are a T-junction and a cusp. For simplicity, we show the visible rim projected to a planar image perpendicular to p occ . Giblin [50] and Cipolla <ref> [27] </ref> showed that under continuous observer motion along a curve c (t), the changes in the geometry of the occluding contour can be used to completely describe the local shape of the surface (i.e., curvature) at points on the visible rim. <p> This allows the recovery of the local shape of S for a whole patch of points containing p. A suitable parameterization that relates the deformation of the occluding contour with the local shape of the surface at p is the epipolar parameterization <ref> [27, 50, 57] </ref>, discussed below. 13 2.1 The Epipolar Parameterization Intuitively, the epipolar parameterization captures the idea that under continuous motion of the observer (and when the topology of the occluding contour does not change), the set of points comprising the visible rim consists of smooth curves that "slide" over the <p> This curve defines a correspondence between a visible rim point at c (t) and at c (t + t). See <ref> [27, 50] </ref> for details on how the epipolar parameterization can be used to recover the first and second fundamental forms of the surface for all points in .
Reference: [28] <author> K. N. Kutulakos and C. R. Dyer, </author> <title> "Recovering shape by purposive viewpoint adjustment," </title> <booktitle> in Proc. Computer Vision and Pattern Recognition, </booktitle> <pages> pp. 16-22, </pages> <year> 1992. </year>
Reference-contexts: Very little work has been published on the use of real-time viewpoint-control behaviors for reconstruction, exploration or recognition tasks. However, the few recent approaches taking advantage of viewpoint-control behaviors demonstrate their importance and generality. We showed in an earlier paper <ref> [28] </ref> that the shape recovery problem for smooth surfaces becomes considerably simplified if the observer uses a simple viewpoint-control behavior to move to a special viewpoint, which for the case of surfaces of revolution corresponds to their side view. <p> This is because in that case x t = 0 (i.e., point p remains on the visible rim under an infinitesimal viewpoint change <ref> [28, 57] </ref>) and consequently x s ^ x t does not define the tangent plane of S at p. <p> The main idea is similar to the one used in <ref> [28] </ref>: If the observer moves along specific directions on the tangent plane of the selected point p, p will remain on the visible rim but will cease to be the endpoint of the visible rim curve containing it.
Reference: [29] <author> D. Wilkes and J. K. Tsotsos, </author> <title> "Active object recognition," </title> <booktitle> in Proc. Computer Vision and Pattern Recognition, </booktitle> <pages> pp. 136-141, </pages> <year> 1992. </year>
Reference-contexts: The work of Wilkes and Tsotsos <ref> [29] </ref> illustrates how the ability to purposefully change viewpoint can simplify 3 the task of object recognition in a simple world of Origami objects. Grosso and Ballard [30] are currently designing a head-eye system for implementing viewpoint-control behaviors.
Reference: [30] <author> E. Grosso and D. H. Ballard, </author> <title> "Head-centered orientation strategies in animate vision," </title> <type> Tech. Rep. 442, </type> <institution> Univeristy of Rochester, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: The work of Wilkes and Tsotsos [29] illustrates how the ability to purposefully change viewpoint can simplify 3 the task of object recognition in a simple world of Origami objects. Grosso and Ballard <ref> [30] </ref> are currently designing a head-eye system for implementing viewpoint-control behaviors. Recent work by Blake et al. [31] showed that shorter paths can be achieved in robotic navigation tasks if the shape of the obstacles is taken into account during navigation.
Reference: [31] <author> A. Blake, A. Zisserman, and R. Cipolla, </author> <title> "Visual exploration of free-space," in Active Vision (A. </title> <editor> Blake and A. Yuille, </editor> <booktitle> eds.), </booktitle> <pages> pp. 175-188, </pages> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: Grosso and Ballard [30] are currently designing a head-eye system for implementing viewpoint-control behaviors. Recent work by Blake et al. <ref> [31] </ref> showed that shorter paths can be achieved in robotic navigation tasks if the shape of the obstacles is taken into account during navigation. <p> reconstruct the scene from a single viewpoint or a small number of viewpoints [22, 33, 34], recent results demonstrate that 4 the occluding contour can be reliably detected in edge images [27, 47-49], and that quantitative shape information (e.g., curvature) can be efficiently and accurately recovered from the occluding contour <ref> [27, 31] </ref>.
Reference: [32] <author> S. A. Hutchinson and A. Kak, </author> <title> "Planning sensing strategies in a robot work cell with multi-sensor capabilities," </title> <journal> IEEE Trans. Robotics Automat., </journal> <volume> vol. 5, no. 6, </volume> <pages> pp. 765-783, </pages> <year> 1989. </year> <month> 54 </month>
Reference-contexts: Apart from the above approaches, viewpoint control for performing various tasks has been treated as a complex and computationally-intensive optimization problem (i.e., "where to look next"), where the best next viewpoint is searched for within the space of all possible viewpoints <ref> [32] </ref>. In tasks involving surface reconstruction, the viewpoint-control mechanisms assumed that a three-dimensional representation of the visible surfaces can be recovered independently from each viewpoint [33-35], ruling out the applicability of these mechanisms in more qualitative visual exploration tasks.
Reference: [33] <author> P. Whaite and F. P. Ferrie, </author> <title> "From uncertainty to visual exploration," </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <volume> vol. 13, no. 10, </volume> <pages> pp. 1038-1049, </pages> <year> 1991. </year>
Reference-contexts: It is well known that the occluding contour is a rich source of shape information [37-46]. Furthermore, contrary to approaches that use sophisticated sensing mechanisms to reconstruct the scene from a single viewpoint or a small number of viewpoints <ref> [22, 33, 34] </ref>, recent results demonstrate that 4 the occluding contour can be reliably detected in edge images [27, 47-49], and that quantitative shape information (e.g., curvature) can be efficiently and accurately recovered from the occluding contour [27, 31].
Reference: [34] <author> J. Maver and R. </author> <title> Bajcsy, "Occlusions and the next view planning," </title> <booktitle> in Proc. IEEE Robotics Automat. Conf., </booktitle> <pages> pp. 1806-1811, </pages> <year> 1992. </year>
Reference-contexts: It is well known that the occluding contour is a rich source of shape information [37-46]. Furthermore, contrary to approaches that use sophisticated sensing mechanisms to reconstruct the scene from a single viewpoint or a small number of viewpoints <ref> [22, 33, 34] </ref>, recent results demonstrate that 4 the occluding contour can be reliably detected in edge images [27, 47-49], and that quantitative shape information (e.g., curvature) can be efficiently and accurately recovered from the occluding contour [27, 31].
Reference: [35] <author> C. I. Connoly, </author> <title> "The determination of next best views," </title> <booktitle> in Proc. IEEE Robotics Automat. Conf., </booktitle> <pages> pp. 432-435, </pages> <year> 1985. </year>
Reference: [36] <author> C. Tomasi and T. Kanade, </author> <title> "Shape and motion from image streams under orthography: A factorization method," </title> <journal> Int. J. Computer Vision, </journal> <volume> vol. 9, no. 2, </volume> <pages> pp. 137-154, </pages> <year> 1992. </year>
Reference-contexts: Moreover, in the context of animate and purposive vision, their major drawback is their inability to exploit the real-time control of the observer's viewpoint to simplify the procedure for solving the task (e.g., by utilizing shape-from-motion approaches, which have been shown to considerably simplify shape recovery computations <ref> [26, 27, 36] </ref>), as well as to simplify the viewpoint control process itself. 1.1 Global Reconstruction from the Occluding Contour The behaviors we develop in this paper control the observer's motion so that surface shape information can be recovered from a dense sequence of images using a shape-from-motion module.
Reference: [37] <author> H. G. Barrow and J. M. Tenenbaum, </author> <title> "Interpreting line drawings as three-dimensional images," </title> <journal> Artificial Intelligence, </journal> <volume> vol. 17, </volume> <pages> pp. 75-116, </pages> <year> 1981. </year>
Reference: [38] <author> M. Brady, J. Ponce, A. Yuille, and H. Asada, </author> <title> "Describing surfaces," </title> <journal> Computer Graphics and Image Processing, </journal> <volume> vol. 32, </volume> <pages> pp. 1-28, </pages> <year> 1985. </year>
Reference: [39] <author> D. J. Kriegman and J. Ponce, </author> <title> "On recognizing and positioning curved 3-d objects from image contours," </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <volume> vol. 12, no. 12, </volume> <pages> pp. 1127-1137, </pages> <year> 1990. </year>
Reference: [40] <author> D. Marr and H. K. Nishihara, </author> <title> "Visual information processing: </title> <journal> Artificial intelligence and the sensorium of sight," Technology Review, </journal> <volume> vol. 81, </volume> <pages> pp. 2-23, </pages> <year> 1978. </year>
Reference: [41] <author> J. Malik, </author> <title> "Interpreting line drawings of curved objects," </title> <journal> Int. J. Computer Vision, </journal> <volume> vol. 1, </volume> <pages> pp. 73-103, </pages> <year> 1987. </year>
Reference: [42] <author> J. Ponce, D. Chelberg, and W. B. Mann, </author> <title> "Invariant properties of straight homogeneous generalized cylinders and their contours," </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <volume> vol. 11, no. 9, </volume> <pages> pp. 951-966, </pages> <year> 1990. </year>
Reference: [43] <author> W. Richards, J. J. Koenderink, and D. D. Hoffman, </author> <title> "Inferring 3d shapes from 2d silhouettes," in Natural Computation (W. Richards, </title> <publisher> ed.), </publisher> <pages> pp. 125-136, </pages> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1988. </year>
Reference: [44] <author> W. Richards, B. Dawson, and D. Whittington, </author> <title> "Encoding contour shape by curvature extrema," in Natural Computation (W. Richards, </title> <publisher> ed.), </publisher> <pages> pp. 83-98, </pages> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1988. </year>
Reference: [45] <author> W. B. Seales and C. R. Dyer, </author> <title> "Viewpoint from occluding contour," Computer Vision, Graphics, </title> <booktitle> and Image Processing: Image Understanding, </booktitle> <volume> vol. 55, no. 2, </volume> <pages> pp. 198-211, </pages> <year> 1992. </year>
Reference: [46] <author> F. Ulupinar and R. Nevatia, </author> <title> "Using symmetries for analysis of shape from contour," </title> <booktitle> in Proc. 2nd Int. Conf. on Computer Vision, </booktitle> <pages> pp. 414-426, </pages> <year> 1988. </year>
Reference: [47] <author> R. Vaillant and O. D. Faugeras, </author> <title> "Using extremal boundaries for 3-d object modeling," </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <volume> vol. 14, no. 2, </volume> <pages> pp. 157-173, </pages> <year> 1992. </year> <month> 55 </month>
Reference-contexts: The deformations of the occluding contour have been studied by Giblin and Weiss [50], Cipolla and Blake [27], Vaillant and Faugeras <ref> [47] </ref>, and Koenderink [51, 52], while the contour's topological changes have been an active research topic for more than a decade (e.g., [52-54]). Much of the analysis we present in this paper builds on this work.
Reference: [48] <author> W. B. Thompson, </author> <title> "Structure-from-motion by tracking occlusion boundaries," </title> <booktitle> in Proc. Work--shop on Visual Motion, </booktitle> <pages> pp. 201-203, </pages> <year> 1989. </year>
Reference: [49] <author> W. B. Thompson and J. S. Painter, </author> <title> "Qualitative constraints for structure-from-motion," Computer Vision, Graphics, </title> <booktitle> and Image Processing: Image Understanding, </booktitle> <volume> vol. 56, no. 1, </volume> <pages> pp. 69-77, </pages> <year> 1992. </year> <note> Special Issue on Purposive, Qualitative, Active Vision. </note>
Reference: [50] <author> P. Giblin and R. Weiss, </author> <title> "Reconstruction of surfaces from profiles," </title> <booktitle> in Proc. 1st Int. Conf. on Computer Vision, </booktitle> <pages> pp. 136-144, </pages> <year> 1987. </year>
Reference-contexts: The deformations of the occluding contour have been studied by Giblin and Weiss <ref> [50] </ref>, Cipolla and Blake [27], Vaillant and Faugeras [47], and Koenderink [51, 52], while the contour's topological changes have been an active research topic for more than a decade (e.g., [52-54]). Much of the analysis we present in this paper builds on this work. <p> The visible rim and the occluding contour corresponding to the projection of a bean-shaped surface are shown (adapted from <ref> [50] </ref>). The occluding contour consists of a single curve whose endpoints are a T-junction and a cusp. For simplicity, we show the visible rim projected to a planar image perpendicular to p occ . Giblin [50] and Cipolla [27] showed that under continuous observer motion along a curve c (t), the <p> the occluding contour corresponding to the projection of a bean-shaped surface are shown (adapted from <ref> [50] </ref>). The occluding contour consists of a single curve whose endpoints are a T-junction and a cusp. For simplicity, we show the visible rim projected to a planar image perpendicular to p occ . Giblin [50] and Cipolla [27] showed that under continuous observer motion along a curve c (t), the changes in the geometry of the occluding contour can be used to completely describe the local shape of the surface (i.e., curvature) at points on the visible rim. <p> This allows the recovery of the local shape of S for a whole patch of points containing p. A suitable parameterization that relates the deformation of the occluding contour with the local shape of the surface at p is the epipolar parameterization <ref> [27, 50, 57] </ref>, discussed below. 13 2.1 The Epipolar Parameterization Intuitively, the epipolar parameterization captures the idea that under continuous motion of the observer (and when the topology of the occluding contour does not change), the set of points comprising the visible rim consists of smooth curves that "slide" over the <p> This curve defines a correspondence between a visible rim point at c (t) and at c (t + t). See <ref> [27, 50] </ref> for details on how the epipolar parameterization can be used to recover the first and second fundamental forms of the surface for all points in .
Reference: [51] <author> J. J. Koenderink, </author> <title> "An internal representation for solid shape based on the topological properties of the apparent contour," in Image Understanding 1985-86 (W. </title> <editor> Richards and S. Ullman, </editor> <booktitle> eds.), </booktitle> <pages> pp. 257-285, </pages> <address> Norwood, NJ: </address> <publisher> Ablex Publishing Co., </publisher> <year> 1987. </year>
Reference-contexts: The deformations of the occluding contour have been studied by Giblin and Weiss [50], Cipolla and Blake [27], Vaillant and Faugeras [47], and Koenderink <ref> [51, 52] </ref>, while the contour's topological changes have been an active research topic for more than a decade (e.g., [52-54]). Much of the analysis we present in this paper builds on this work. <p> The occluding contour of S is the projection of the visible rim on the image (Figure 3). The occluding contour is a collection of open and closed smooth curves for almost all positions of the observer. The endpoints of open occluding contour curves are either cusps or T-junctions <ref> [51] </ref>. <p> touch the surface 4 A line is said to have n-th order contact with a surface at a point p when all directional derivatives at p along the line up to (but not including) order n are zero. 17 at points on certain characteristic curves associated with the visual event <ref> [51, 53] </ref>. <p> These events occur when there is a visible rim point p for which the line l connecting it to the observer's viewpoint has fourth order contact with the surface. The swallowtail event occurs when p is on a flecnodal curve <ref> [51] </ref>, while the lip and the beak-to-beak events occur when p is on a parabolic curve. The multilocal events of codimension one are the triple-point, tangent-crossing and cusp-crossing events [53] (Figure 5 (b)). <p> Point p occ must not be the endpoint of an occluding contour curve. Step 2: Compute the surface normal at p. The normal is given by T ^ p occ , where T is the tangent to the occluding contour at p occ <ref> [51] </ref>. Step 3: (Reconstructing the occluded points near p.) Select a direction v 1 for moving on the motion sphere that satisfies the inequality n (p) v 1 &gt; 0.
Reference: [52] <author> J. J. Koenderink, </author> <title> Solid Shape. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: The deformations of the occluding contour have been studied by Giblin and Weiss [50], Cipolla and Blake [27], Vaillant and Faugeras [47], and Koenderink <ref> [51, 52] </ref>, while the contour's topological changes have been an active research topic for more than a decade (e.g., [52-54]). Much of the analysis we present in this paper builds on this work. <p> The local shape of the surface at p can be described by finding a suitable parameterization x for the surface in a neighborhood of p and calculating the first and second fundamental forms of S with respect to x <ref> [52] </ref>. Furthermore, if such a parameterization can be found, the first and second fundamental forms for all points in can be computed. This allows the recovery of the local shape of S for a whole patch of points containing p. <p> That is, for almost all viewpoints, the contour's topology does not change when these viewpoints are infinitesimally perturbed (e.g., see <ref> [52, 58] </ref>). Results from singularity theory show that the space of viewpoints can be partitioned into a collection of maximal connected cells within which the topology of the occluding contour remains constant. Visual events occur when the observer's viewpoint belongs to the boundaries of these maximal cells. <p> The visual event curves listed in Theorem 4 are therefore potential boundaries of a reconstructible surface region. They bound such regions only if they contain points with no visibility arcs. Figure 13 shows the reconstructible regions for two objects. 35 (a) (b) (c) <ref> [52] </ref>. The figures are taken from [53]. The squash-shaped surface in (a) and (b) is completely reconstructible: No cusp-crossing or triple-point visual events can occur, and the surfaces do not have concavities.
Reference: [53] <author> S. Petitjean, J. Ponce, and D. J. Kriegman, </author> <title> "Computing exact aspect graphs of curved objects: Algebraic surfaces," </title> <journal> Int. J. Computer Vision, </journal> <volume> vol. 9, no. 3, </volume> <pages> pp. 231-255, </pages> <year> 1992. </year>
Reference-contexts: Examples of such regions are surface concavities, which are bounded by parabolic curves on the surface. These curves belong to a special class of surface curves, the visual event curves <ref> [53] </ref>, which we discuss in detail later. <p> touch the surface 4 A line is said to have n-th order contact with a surface at a point p when all directional derivatives at p along the line up to (but not including) order n are zero. 17 at points on certain characteristic curves associated with the visual event <ref> [51, 53] </ref>. <p> The swallowtail event occurs when p is on a flecnodal curve [51], while the lip and the beak-to-beak events occur when p is on a parabolic curve. The multilocal events of codimension one are the triple-point, tangent-crossing and cusp-crossing events <ref> [53] </ref> (Figure 5 (b)). Triple-point events occur at viewpoints where there is a triplet of collinear rim points whose supporting line l passes through the observer's viewpoint. These three points project to a single point on the occluding contour. <p> The visual event curves listed in Theorem 4 are therefore potential boundaries of a reconstructible surface region. They bound such regions only if they contain points with no visibility arcs. Figure 13 shows the reconstructible regions for two objects. 35 (a) (b) (c) [52]. The figures are taken from <ref> [53] </ref>. The squash-shaped surface in (a) and (b) is completely reconstructible: No cusp-crossing or triple-point visual events can occur, and the surfaces do not have concavities.
Reference: [54] <author> J. H. Rieger, </author> <title> "The geometry of view space of opaque objects bounded by smooth surfaces," </title> <journal> Artificial Intelligence, </journal> <volume> vol. 44, </volume> <pages> pp. 1-40, </pages> <year> 1990. </year>
Reference-contexts: Higher codimension visual events occur at the intersections of surfaces associated with events of codimension one. In the following we only use results from the analysis of codimension one events for transparent generic surfaces; see <ref> [54] </ref> for a detailed discussion of visual events of higher codimension and the geometry of viewpoint space partitioning in the neighborhood of such viewpoints both for transparent and opaque surfaces.
Reference: [55] <author> M. P. D. Carmo, </author> <title> Differential Geometry of Curves and Surfaces. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall Inc., </publisher> <year> 1976. </year>
Reference-contexts: This segment has already disappeared when the observer reaches the viewpoint corresponding to (f); further downward motion cannot be used to reconstruct the surface in the vicinity of the dark line drawn on the surface (the dark line corresponds to a parallel <ref> [55] </ref> of the torus). 1.2 Paper Overview We study global surface reconstruction by first developing a collection of behaviors that perform simpler tasks, and then generalizing them to address the complete task (Table 1).
Reference: [56] <author> Y. L. Kergosien, </author> <title> "La famille des projections orthogonales d'une surface et ses singularites," </title> <editor> C. R. </editor> <booktitle> Acad. Sc. Paris, </booktitle> <volume> vol. 292, </volume> <pages> pp. 929-932, </pages> <year> 1981. </year>
Reference-contexts: Informally, generic surfaces exemplify the notion of non-degeneracy: They are surfaces whose topological and geometrical characteristics (e.g., parabolic curves, the Gauss map) are not affected by infinitesimal perturbations of the surface. Generic surfaces have been the subject of active research in singularity theory <ref> [56] </ref>; their precise definition is rather technical and beyond the scope of this paper. Although in this paper we focus on the global reconstruction of generic surfaces, our results generalize directly to non-generic surfaces. <p> Visual events occur when the observer's viewpoint belongs to the boundaries of these maximal cells. An infinitesimal perturbation of such a viewpoint results in changes in the topology of the occluding contour. A catalog of the visual events was given by Kergosien <ref> [56] </ref> for the case where a surface is smooth, generic and transparent (i.e., the occluding contour is considered to be the projection of all rim points, not just the points on the visible rim), and is observed under orthographic projection.
Reference: [57] <author> P. Giblin and M. G. Soares, </author> <title> "On the geometry of a surface and its singular profiles," </title> <journal> Image and Vision Computing, </journal> <volume> vol. 6, no. 4, </volume> <pages> pp. 225-234, </pages> <year> 1988. </year>
Reference-contexts: This allows the recovery of the local shape of S for a whole patch of points containing p. A suitable parameterization that relates the deformation of the occluding contour with the local shape of the surface at p is the epipolar parameterization <ref> [27, 50, 57] </ref>, discussed below. 13 2.1 The Epipolar Parameterization Intuitively, the epipolar parameterization captures the idea that under continuous motion of the observer (and when the topology of the occluding contour does not change), the set of points comprising the visible rim consists of smooth curves that "slide" over the <p> This is because in that case x t = 0 (i.e., point p remains on the visible rim under an infinitesimal viewpoint change <ref> [28, 57] </ref>) and consequently x s ^ x t does not define the tangent plane of S at p.
Reference: [58] <author> J. J. Koenderink and A. J. van Doorn, </author> <title> "The singularities of the visual mapping," </title> <journal> Biological Cybernetics, </journal> <volume> vol. 24, </volume> <pages> pp. 51-59, </pages> <year> 1976. </year>
Reference-contexts: That is, for almost all viewpoints, the contour's topology does not change when these viewpoints are infinitesimally perturbed (e.g., see <ref> [52, 58] </ref>). Results from singularity theory show that the space of viewpoints can be partitioned into a collection of maximal connected cells within which the topology of the occluding contour remains constant. Visual events occur when the observer's viewpoint belongs to the boundaries of these maximal cells.
Reference: [59] <author> J. Ponce, S. Petitjean, and D. Kriegman, </author> <title> "Computing exact aspect graphs of curved objects: Algebraic surfaces," </title> <booktitle> in Proc. Second European Conference on Computer Vision, </booktitle> <year> 1992. </year>
Reference-contexts: Consequently, the same catalog of visual events is still valid. However, degeneracies may also occur: The visual event "curves" defined above may in fact become two-dimensional regions on the surface (e.g., all points on a cylinder are parabolic). See <ref> [59] </ref> for a discussion of this issue in the case of algebraic surfaces (which are not generic), and the computation of visual event curves for these surfaces.
Reference: [60] <author> C. K. Cowan and P. D. Kovesi, </author> <title> "Automatic sensor placement from vision task requirements," </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <volume> vol. 10, no. 3, </volume> <pages> pp. 407-416, </pages> <year> 1988. </year>
Reference-contexts: Furthermore, no precise stopping condition is provided for terminating the observer's motion in these two steps. This leaves considerable freedom for the observer to make a choice that satisfies additional requirements <ref> [60, 61] </ref> (e.g., distance to the surface).
Reference: [61] <author> K. Tarabanis and R. Y. Tsai, </author> <title> "Computing viewpoints that satisfy optical constraints," </title> <booktitle> in Proc. Computer Vision and Pattern Recognition, </booktitle> <pages> pp. 152-158, </pages> <year> 1991. </year>
Reference-contexts: Furthermore, no precise stopping condition is provided for terminating the observer's motion in these two steps. This leaves considerable freedom for the observer to make a choice that satisfies additional requirements <ref> [60, 61] </ref> (e.g., distance to the surface).
Reference: [62] <author> M. J. Swain and D. H. Ballard, </author> <title> "Color indexing," </title> <journal> Int. J. Computer Vision, </journal> <volume> vol. 7, no. 1, </volume> <pages> pp. 11-32, </pages> <year> 1991. </year>
Reference-contexts: How can the behaviors we developed generalize to perform more qualitative tasks? We are currently investigating the task of visually exploring the surface of an object in order to find a specific surface feature (e.g., your cup does not have a blue handle but mine does <ref> [62] </ref>). This task is not as constrained as the surface reconstruction task; what is needed is a way to control the motion of the occlusion boundary (i.e., the boundary of the visible points on the surface), which is a superset of the visible rim.
Reference: [63] <author> E. Grosso, M. Tistarelli, and G. </author> <title> Sandini, "Active/dynamic stereo for navigation," </title> <booktitle> in Proc. Second European Conference on Computer Vision, </booktitle> <pages> pp. 516-525, </pages> <year> 1992. </year> <month> 56 </month>
Reference: [64] <author> J. J. Koenderink and A. J. van Doorn, </author> <title> "The shape of smooth objects and the way contours end," in Natural Computation (W. Richards, </title> <publisher> ed.), </publisher> <pages> pp. 115-124, </pages> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1988. </year>
Reference: [65] <author> J. J. Koenderink and A. J. van Doorn, </author> <title> "A description of the structure of visual images in terms of an ordered hierarchy of light and dark blobs," </title> <booktitle> in Proc. Second Int. Visual Psychophysics and Medical Imaging Conf., </booktitle> <pages> pp. 173-176, </pages> <year> 1981. </year> <month> 57 </month>
References-found: 65

