URL: ftp://ftp.cis.ufl.edu/cis/tech-reports/tr95/tr95-024.ps
Refering-URL: http://www.cis.ufl.edu/tech-reports/tech-reports/tr95-abstracts.html
Root-URL: http://www.cis.ufl.edu
Title: INCOMPLETE LU FACTORIZATION: A MULTIFRONTAL APPROACH  
Author: YOGIN E. CAMPBELL AND TIMOTHY A. DAVIS 
Abstract: Technical Report TR-95-024, Computer and Information Sciences Department, University of Florida, Gainesville, FL, 32611 USA. October, 1995. Key words. Unsymmetric-pattern multifrontal method, sparse matrices, incomplete factorization, iterative methods. AMS (MOS) subject classifications. 05C50, 65F50, 65F05. Abstract. We present a new class of preconditioners based on an unsymmetric multifrontal incomplete LU factorization algorithm. In this algorithm entire update rows and columns of the frontal matrices are dropped based on a level drop strategy. We discuss some of the successes and difficulties inherent in applying this drop strategy. In general, we found the memory usage of this incomplete multifrontal factorization algorithm to be smaller than its complete multifrontal counterpart. We present numerical results to show the quality of the factors when used as preconditioners with the conjugate gradient squared iterative method. 
Abstract-found: 1
Intro-found: 1
Reference: <author> 18 Fig. 5.5. </author> <title> Sherman3 matrix: effect of g on memory </title>
Reference-contexts: Much research has been done on the effectiveness of the preconditioners constructed using the incomplete LU or Cholesky factorization approach <ref> [2, 3, 4, 14, 13, 15, 16, 18] </ref>. The focus in constructing effective preconditioners based on the incomplete factorization approach has been to determine the best (or most useful) set of elements to drop (or to keep) as the factorization algorithm progresses.
Reference: [1] <author> P. R. Amestoy, T. A. Davis, and I. S. Duff. </author> <title> An approximateminimum degree ordering algorithm. SIAM J. Matrix Analysis and Application, </title> <note> (to appear). Also CISE Technical Report TR-94-039. </note>
Reference-contexts: This relaxed amalgamation allows the use of dense matrix kernels in the innermost loops (level 3 BLAS [10]). Fifth, pivot selection and degree update are based on an unsymmetric analogue of the (symmetric) approximate minimum degree ordering algorithm <ref> [1] </ref>. Finally, aggressive assembly is done: as many rows and columns of entries as possible from the original matrix and unassembled contribution blocks are assembled into the current frontal matrix. An outline of the algorithm is shown in Figure 3.1. 3.1. Step 1a: selecting a seed pivot. <p> This choice of seed pivot is based on two criteria: fill-in control and numerical stability. To help in selecting a pivot with low fill-in, the upper bound degrees of the remaining rows and columns in the active submatrix are maintained (see <ref> [1] </ref> for a discussion of how these upper bound degrees are efficiently computed). Let d x and d x be the true and approximate degree of the row or column x, respectively, where d x d x . <p> Finding a tight upper bound for the degrees of the rows and columns in the contribution matrix involves the use of an efficient scheme described in <ref> [1] </ref>. As shown in [1], the tight upper bounds on the degrees usually result in low fill-in. 5 Fig. 3.3. Partitioning of real memory 3.5. Step 2c: numerical update using the level 3 BLAS. Forming the Schur complement to update the contribution block is done in step 2c. <p> Finding a tight upper bound for the degrees of the rows and columns in the contribution matrix involves the use of an efficient scheme described in <ref> [1] </ref>. As shown in [1], the tight upper bounds on the degrees usually result in low fill-in. 5 Fig. 3.3. Partitioning of real memory 3.5. Step 2c: numerical update using the level 3 BLAS. Forming the Schur complement to update the contribution block is done in step 2c. <p> completed, C is then put onto a heap for later assembly, the LU factors computed within the current frontal matrix are stored, and, finally, information on the row and column structure of C are saved to allow the easy assembly of C into subsequent frontal matrices (the quotient graph information <ref> [1] </ref>). 3.7.1. Data structures. The real memory used by the unsymmetric-pattern multifrontal algorithm is partitioned as shown in Figure 3.3. (This memory layout is actually for an early version of the algorithm embodied in MA38/UMFPACK2.0.) A similar layout is used for the integer memory used by the algorithm.
Reference: [2] <author> O. Axelsson. </author> <title> A survey of preconditioned iterative methods for linear systems of algebraic equations. </title> <journal> Bit, </journal> <volume> 25 </volume> <pages> 166-187, </pages> <year> 1985. </year>
Reference-contexts: Much research has been done on the effectiveness of the preconditioners constructed using the incomplete LU or Cholesky factorization approach <ref> [2, 3, 4, 14, 13, 15, 16, 18] </ref>. The focus in constructing effective preconditioners based on the incomplete factorization approach has been to determine the best (or most useful) set of elements to drop (or to keep) as the factorization algorithm progresses.
Reference: [3] <author> O. Axelsson and N. Munksgaard. </author> <title> A class of preconditioned conjugate gardients methods for the solution of a mixed finite-element discretization of the biharmonic operator. </title> <journal> Int. J. Numer. Math. Eng., </journal> <volume> 14 </volume> <pages> 1001-1019, </pages> <year> 1978. </year>
Reference-contexts: Much research has been done on the effectiveness of the preconditioners constructed using the incomplete LU or Cholesky factorization approach <ref> [2, 3, 4, 14, 13, 15, 16, 18] </ref>. The focus in constructing effective preconditioners based on the incomplete factorization approach has been to determine the best (or most useful) set of elements to drop (or to keep) as the factorization algorithm progresses.
Reference: [4] <author> O. Axelsson and N. Munksgaard. </author> <title> Analysis of incomplete factorizations with fixed storage allocation. </title> <editor> In D. J. Evans, editor, </editor> <title> Preconditioning Methods: </title> <journal> Analysis and Applications, </journal> <pages> pages 219-241. </pages> <publisher> Gordon and Breach, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: Much research has been done on the effectiveness of the preconditioners constructed using the incomplete LU or Cholesky factorization approach <ref> [2, 3, 4, 14, 13, 15, 16, 18] </ref>. The focus in constructing effective preconditioners based on the incomplete factorization approach has been to determine the best (or most useful) set of elements to drop (or to keep) as the factorization algorithm progresses.
Reference: [5] <author> Y. E. Campbell. </author> <title> Multifrontal algorithms for sparse inverse subsets and incomplete LU factorization. </title> <type> PhD thesis, </type> <institution> Computer and Information Science and Engineering Department, Univ. of Florida, </institution> <address> Gainesville, FL, </address> <month> November </month> <year> 1995. </year> <note> Also CISE Technical Report TR-95-025. </note>
Reference-contexts: These heuristics include dropping by value, dropping by position, dropping based on storage constraints, or some hybrid form of the previous three strategies. In this paper we discuss a new class of preconditioners based on the incomplete LU factorization of general unsymmetric matrices <ref> [5] </ref>. Our incomplete factorization algorithm is derived from the unsymmetric-pattern multifrontal algorithm of Davis and Duff [9, 7]. The dropping strategy based on fill levels.
Reference: [6] <author> T. A. Davis. </author> <title> A combined unifrontal/multifrontal method for unsymmetric sparse matrices. </title> <type> Technical Report TR-94-005, </type> <institution> University of Florida, </institution> <address> Gainesville, FL, </address> <year> 1994. </year>
Reference-contexts: This data structure allows both the contribution matrix area and the LU factors areas to grow and shrink easily within the frontal working array. Davis first presented this arrangement in <ref> [6, 8] </ref>. 4. The unsymmetric multifrontal incomplete LU algorithm.
Reference: [7] <author> T. A. Davis and I. S. Duff. </author> <title> A combined unifrontal/multifrontal method for unsymmetric sparse matrices. </title> <type> Technical Report TR-95-020, </type> <institution> Computer and Information Science and Engineering Department, Univ. of Florida, </institution> <year> 1995. </year>
Reference-contexts: In this paper we discuss a new class of preconditioners based on the incomplete LU factorization of general unsymmetric matrices [5]. Our incomplete factorization algorithm is derived from the unsymmetric-pattern multifrontal algorithm of Davis and Duff <ref> [9, 7] </ref>. The dropping strategy based on fill levels. The basic idea is to associate a fill level with each entry in the coefficient matrix and a prescribed way of updating those fill levels as the incomplete factorization progresses. <p> Section 3 gives a step by step discussion of the unsymmetric-pattern multifrontal (complete) LU factorization algorithm. For more details on the unsymmetric-pattern multifrontal algorithm we refer the reader to <ref> [9, 7] </ref>. In Section 4 we develop the unsymmetric multifrontal incomplete algorithm. This is followed in Section 5 by the results and discussion of our numerical experiments. Here we focus on the quality of the incomplete factors used as preconditioners in the conjugate gradient squared iterative method. <p> The unsymmetric multifrontal (complete) LU algorithm. Our unsymmetric multifrontal incomplete LU factorization algorithm is derived from the unsymmetric-pattern multifrontal factorization algorithm of Davis and Duff (Harwell-Boeing subroutine MA38 or UMFPACK Version 2.0) <ref> [9, 7] </ref>. Some of the main features of the Davis/Duff algorithm are summarized below. First, this algorithm has no separate analyze and factorize phases. Pivot selection and elimination are both done in a single pass since it is assumed from the outset that pivoting for numerical stability is necessary.
Reference: [8] <author> T. A. Davis and I. S. Duff. </author> <title> A combined unifrontal/multifrontal method for unsymmetric sparse matrices. </title> <type> Technical Report TR-95-020, </type> <institution> University of Florida, </institution> <address> Gainesville, FL, </address> <year> 1995. </year>
Reference-contexts: This data structure allows both the contribution matrix area and the LU factors areas to grow and shrink easily within the frontal working array. Davis first presented this arrangement in <ref> [6, 8] </ref>. 4. The unsymmetric multifrontal incomplete LU algorithm.
Reference: [9] <author> T. A. Davis and I. S. Duff. </author> <title> An unsymmetric-pattern multifrontal method for sparse LU factorization. SIAM J. Matrix Analysis and Application, </title> <note> (to appear). Also CISE Technical Report TR-94-038. </note>
Reference-contexts: In this paper we discuss a new class of preconditioners based on the incomplete LU factorization of general unsymmetric matrices [5]. Our incomplete factorization algorithm is derived from the unsymmetric-pattern multifrontal algorithm of Davis and Duff <ref> [9, 7] </ref>. The dropping strategy based on fill levels. The basic idea is to associate a fill level with each entry in the coefficient matrix and a prescribed way of updating those fill levels as the incomplete factorization progresses. <p> Section 3 gives a step by step discussion of the unsymmetric-pattern multifrontal (complete) LU factorization algorithm. For more details on the unsymmetric-pattern multifrontal algorithm we refer the reader to <ref> [9, 7] </ref>. In Section 4 we develop the unsymmetric multifrontal incomplete algorithm. This is followed in Section 5 by the results and discussion of our numerical experiments. Here we focus on the quality of the incomplete factors used as preconditioners in the conjugate gradient squared iterative method. <p> The unsymmetric multifrontal (complete) LU algorithm. Our unsymmetric multifrontal incomplete LU factorization algorithm is derived from the unsymmetric-pattern multifrontal factorization algorithm of Davis and Duff (Harwell-Boeing subroutine MA38 or UMFPACK Version 2.0) <ref> [9, 7] </ref>. Some of the main features of the Davis/Duff algorithm are summarized below. First, this algorithm has no separate analyze and factorize phases. Pivot selection and elimination are both done in a single pass since it is assumed from the outset that pivoting for numerical stability is necessary.
Reference: [10] <author> J. Dongarra, J. Du Croz, S. Hammarling, and I. Duff. </author> <title> A set of Level 3 Basic Linear Algebra Subprograms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 16(1) </volume> <pages> 1-17, </pages> <year> 1990. </year>
Reference-contexts: Fourth, advantage is taken of repeated structures in the matrix by factorizing more than one pivot in each frontal matrix: relaxed amalgamation. This relaxed amalgamation allows the use of dense matrix kernels in the innermost loops (level 3 BLAS <ref> [10] </ref>). Fifth, pivot selection and degree update are based on an unsymmetric analogue of the (symmetric) approximate minimum degree ordering algorithm [1].
Reference: [11] <author> I.S. Duff and G. A. Meurant. </author> <title> The effect of ordering on preconditioned conjugate gradients. </title> <journal> BIT, </journal> <volume> 29 </volume> <pages> 635-657, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Note that for the three smaller matrices (Tables 5.2 5.4), both the condition number and Frobenius norm of the remainder matrix (jjRjj F ) also decrease rather rapidly with increasing level tolerance. The decrease of jjRjj F with increasing level tolerance supports the observation made in <ref> [11] </ref> that entries with higher levels tend to have smaller numerical values. 5.4. The eigenvalue spectrum. Figures 5.1, 5.2 and 5.3 show the eigenvalue spectrum for the unpreconditioned matrix and for three level tolerances for the three smaller matrices.
Reference: [12] <author> A. Jennings and G. A. Malik. </author> <title> Partial elimination. </title> <journal> J. Inst. Math. Applics., </journal> <volume> 20 </volume> <pages> 307-316, </pages> <year> 1977. </year>
Reference-contexts: The iterative method typically has a faster convergence rate on the transformed system, M 1 Ax = M 1 b, if the eigenvalue spectrum of the preconditioned coefficient matrix, M 1 A, is more densely clustered than the original coefficient matrix A <ref> [12] </ref>. Much research has been done on the effectiveness of the preconditioners constructed using the incomplete LU or Cholesky factorization approach [2, 3, 4, 14, 13, 15, 16, 18].
Reference: [13] <author> M. T. Jones and P. E. Plassman. </author> <title> An improved Cholesky factorization. </title> <type> Technical Report Preprint MCS-P206-0191, </type> <institution> Argonne National Laboratory, Argonne, Illinois, </institution> <year> 1992. </year>
Reference-contexts: Much research has been done on the effectiveness of the preconditioners constructed using the incomplete LU or Cholesky factorization approach <ref> [2, 3, 4, 14, 13, 15, 16, 18] </ref>. The focus in constructing effective preconditioners based on the incomplete factorization approach has been to determine the best (or most useful) set of elements to drop (or to keep) as the factorization algorithm progresses.
Reference: [14] <author> D. S. Kershaw. </author> <title> The incomplete Cholesky-conjugate gradient method for the iterative solution of systems of linear equations. </title> <journal> J. Comput. Phys., </journal> <volume> 26 </volume> <pages> 43-65, </pages> <year> 1978. </year> <month> 19 </month>
Reference-contexts: Much research has been done on the effectiveness of the preconditioners constructed using the incomplete LU or Cholesky factorization approach <ref> [2, 3, 4, 14, 13, 15, 16, 18] </ref>. The focus in constructing effective preconditioners based on the incomplete factorization approach has been to determine the best (or most useful) set of elements to drop (or to keep) as the factorization algorithm progresses. <p> That is, rather than allowing a local pivot failure due to the size of the pivot value being too small, we replace the pivot value by some specified value (typically 1.0). Kershaw in <ref> [14] </ref> used this strategy to avoid pivot failures and/or the loss of positive-definiteness of the incomplete Cholesky factors.
Reference: [15] <author> J. Meijerink and A. Van Der Vorst. </author> <title> An iterative solution method for linear systems of which the coefficient matrix is a symmetric M-matrix. </title> <journal> Mathematics of Computation, </journal> <volume> 31 </volume> <pages> 134-155, </pages> <year> 1977. </year>
Reference-contexts: Much research has been done on the effectiveness of the preconditioners constructed using the incomplete LU or Cholesky factorization approach <ref> [2, 3, 4, 14, 13, 15, 16, 18] </ref>. The focus in constructing effective preconditioners based on the incomplete factorization approach has been to determine the best (or most useful) set of elements to drop (or to keep) as the factorization algorithm progresses.
Reference: [16] <author> N. Munksgaard. </author> <title> Solving sparse symmetric sets of linear equations by preconditioned conjugate gardients. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 6 </volume> <pages> 206-219, </pages> <year> 1980. </year>
Reference-contexts: Much research has been done on the effectiveness of the preconditioners constructed using the incomplete LU or Cholesky factorization approach <ref> [2, 3, 4, 14, 13, 15, 16, 18] </ref>. The focus in constructing effective preconditioners based on the incomplete factorization approach has been to determine the best (or most useful) set of elements to drop (or to keep) as the factorization algorithm progresses.
Reference: [17] <author> P. Sonnveld. </author> <title> CGS, a fast Lanczos-type solver for nonsymmetric linear systems. </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 10 </volume> <pages> 36-52, </pages> <year> 1989. </year>
Reference-contexts: Table 5.1 gives the set of small and medium-sized matrices on which we report results. 5.2. Numerical experiments information. We used a Sun SPARC station 10 for all experiments. The frontal growth-factor, g, was set to two. We used the preconditioned conjugate gradient square iterative method (CGS) <ref> [17] </ref> with a maximum of 250 iterations allowed for each run with a preconditioner. Convergence is achieved if the relative 2-norm of the residual is less than 10 5 and the relative error in the solution is less than 10 3 .
Reference: [18] <author> R. S. Varga. </author> <title> Factorizations and normalized iterative methods, in Boundary Problems in Differential Equations (edited by R.E. </title> <type> Langer). </type> <institution> The University of Wisconsin Press, Madison, Wisconsin, </institution> <year> 1960. </year> <note> Note: all University of Florida technical reports in this list of references are available in postscript form via anonymous ftp to ftp.cis.ufl.edu in the directory cis/tech-reports, or via the World Wide Web at http://www.cis.ufl.edu/~davis. </note>
Reference-contexts: Much research has been done on the effectiveness of the preconditioners constructed using the incomplete LU or Cholesky factorization approach <ref> [2, 3, 4, 14, 13, 15, 16, 18] </ref>. The focus in constructing effective preconditioners based on the incomplete factorization approach has been to determine the best (or most useful) set of elements to drop (or to keep) as the factorization algorithm progresses.
References-found: 19

