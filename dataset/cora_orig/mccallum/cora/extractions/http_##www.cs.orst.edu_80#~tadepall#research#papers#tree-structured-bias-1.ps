URL: http://www.cs.orst.edu:80/~tadepall/research/papers/tree-structured-bias-1.ps
Refering-URL: http://www.cs.orst.edu:80/~tadepall/research/publications.html
Root-URL: 
Email: (tadepalli@cs.orst.edu)  
Title: Learning from Queries and Examples with Tree-structured Bias  
Author: Prasad Tadepalli 
Address: Corvallis, OR 97331  
Affiliation: Department of Computer Science Oregon State University  
Abstract: Incorporating declarative bias or prior knowledge into learning is an active research topic in machine learning. Tree-structured bias specifies the prior knowledge as a tree of "relevance" relationships between attributes. This paper presents a learning algorithm that implements tree-structured bias, i.e., learns any target function probably approximately correctly from random examples and membership queries if it obeys a given tree-structured bias. The theoretical predictions of the paper are em pirically validated.
Abstract-found: 1
Intro-found: 1
Reference: <author> Angluin, D., </author> <title> Queries and concept learning, </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <year> 1988. </year>
Reference-contexts: This paper presents a polynomial-time learning algorithm that efficiently learns functions which obey a given tree-structured bias. The teacher provides random examples, and also gives the outputs for any inputs queried by the learner, i.e., answers "membership queries" <ref> (Angluin, 1988) </ref>. The algorithm is proved correct using the PAC learning framework, and the time and sample complexities are estimated. The paper also presents empirical results that validate the theoretical predictions, illustrating the power of declarative bias and special-purpose learning algorithms designed to exploit it. <p> Hence, we assume that each internal node in the determination tree has 2 to k children. 3 The PAC Learning Framework In this paper, we will be using a version of Probably Approximately Correct (PAC) learning, introduced by Valiant, and extended by Angluin (Valiant, 1984), <ref> (Angluin, 1988) </ref>. For a broad introduction, see (Natarajan, 1991). We restrict ourselves here to learning boolean functions. An assignment is a mapping of attributes B 1 ; : : :; B n to their values in = f0; 1g. n denotes the set of binary strings of length n. <p> The algorithms for learning read-once formulas first learn the tree structure, and then adopt methods similar to Learn-Tree to learn the functions at the nodes in the tree. Angluin studied the problem of exact identification of a target concept from membership and equivalence queries <ref> (Angluin, 1988) </ref>. Equivalence queries are in the form of "is the target function the same as G?" If G is the target function, the teacher answers `yes'; otherwise the teacher responds with a counter-example, i.e., an example where G and the target function disagree.
Reference: <author> Blumer, A. Ehrenfeucht, A., Haussler, D., and Warmuth, M. K., </author> <title> "Learnability and the Vapnik-Chervonenkis Dimension," </title> <journal> Journal of the ACM, </journal> <volume> 36:4, </volume> <year> 1989. </year>
Reference: <author> Cohen, W. W., </author> <title> Compiling Prior Knowledge Into an Explicit Bias, </title> <booktitle> In Proceedings of Machine Learning Conference, </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Incorporating declarative bias into learning is an active research topic in machine learning, and has recently given rise to many useful algorithms, e.g., <ref> (Cohen, 1992) </ref>, (Pazzani and Kibler, 1992), and (Shavlik and Towell, 1989). However, most of these algorithms are "general-purpose" or "weak" in that they are not designed to take advantage of the structure of the prior knowledge, and hence, do not always give good results.
Reference: <author> Getoor, L., </author> <title> The instance description: How it can be derived and the use of its derivation. </title> <type> MS report, </type> <institution> Computer Science Division, University of California, Berkeley, </institution> <address> CA, </address> <year> 1989. </year>
Reference-contexts: Only the leaves and the root of the tree are observable during training, which makes the learning problem difficult. Previous attempts to implement tree-structured bias had only limited success and restricted scope <ref> (Getoor, 1989) </ref>. This paper presents a polynomial-time learning algorithm that efficiently learns functions which obey a given tree-structured bias. The teacher provides random examples, and also gives the outputs for any inputs queried by the learner, i.e., answers "membership queries" (Angluin, 1988). <p> Since this is also the most expensive part of the algorithm, its complexity is polynomial in n, 1 ffi , satisfying all our requirements. * 5 Experimental Results The above algorithm was implemented as a program called TSB. There were two previous implementations of tree-structured bias <ref> (Getoor, 1989) </ref>. However, both of them are inadequate to compare with TSB. One of them assumes that the boolean func tions at each node are monotone.
Reference: <author> Hancock, T. and Hellerstein, L., </author> <title> Learning Read-Once Formulas over Fields and Extended Bases, </title> <booktitle> In Proceedings of the Fourth Annual Workshop on Computational Learning Theory, </booktitle> <publisher> Morgan Kauf-mann, </publisher> <year> 1991. </year>
Reference-contexts: In the worst case, this involves trying each possible input, which is not allowed. Our algorithm shares some features with the read-once formulas studied in the Computational Learning Theory literature, e.g., <ref> (Hancock and Heller-stein, 1991) </ref>. Read-once formulas are boolean formulas where each variable appears exactly once.
Reference: <author> Kearns, M. and Valiant, L. G., </author> <title> Cryptographic Limitations on Learning Boolean Formulae and Finite Automata, </title> <booktitle> In Proceedings of 21 st Annual ACM Symposium on Theory of Computing, </booktitle> <publisher> Assoc. Com-put. Mach., </publisher> <year> 1989. </year>
Reference-contexts: Learning with tree-structured bias from classified random examples is reducible to learning arbitrary boolean formulas (Pitt and Warmuth, 1990), which in turn is reducible to some apparently hard cryptographic problems <ref> (Kearns and Valiant, 1989) </ref>. To see that tree-structured bias cannot be implemented with membership queries alone, consider the class of boolean functions which output 1 on exactly one input and output 0 on all others.
Reference: <author> Mahadevan, S. and Tadepalli, P., </author> <title> On The Tractability of Learning from Incomplete Theories, </title> <booktitle> In Proceedings of the Fifth International Machine Learning Conference, </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: The property of determination knowledge that is important to us here is that by identifying the relevant features for a target function, it reduces the set of functions that the learner has to consider, which in turn reduces the number of examples sufficient for learning <ref> (Mahadevan and Tadepalli, 1988) </ref>. Russell showed that the power of determinations in reducing the number of training examples is even greater when the learner has a set of determinations structured as a tree (Russell, 1989).
Reference: <author> Mitchell, T. </author> <title> The Need for biases in learning generalizations. </title> <note> In Readings in Machine Learning (Ed) Shavlik, </note> <author> J. and Dietterich, T. G., Morgan Kauf-mann, </author> <year> 1990. </year>
Reference: <author> Natarajan, B., </author> <title> Machine Learning: A Theoretical Approach, </title> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: For a broad introduction, see <ref> (Natarajan, 1991) </ref>. We restrict ourselves here to learning boolean functions. An assignment is a mapping of attributes B 1 ; : : :; B n to their values in = f0; 1g. n denotes the set of binary strings of length n. <p> The results in PAC learning framework suggest that the sample size sufficient to learn a given space of functions using only random examples varies logarithmically with the number of functions in the function space (Blumer et al., 1987), <ref> (Natarajan, 1991) </ref>. Russell used these results to derive an upper bound on the number of random examples sufficient to learn a function consistent with a given tree-structured bias (Russell, 1989).
Reference: <author> Pazzani, M. and Kibler, D., </author> <title> The Utility of Knowledge in Inductive Learning, Machine Learning, </title> <publisher> Kluwer Academic, </publisher> <address> 9:1, </address> <year> 1992. </year>
Reference-contexts: Incorporating declarative bias into learning is an active research topic in machine learning, and has recently given rise to many useful algorithms, e.g., (Cohen, 1992), <ref> (Pazzani and Kibler, 1992) </ref>, and (Shavlik and Towell, 1989). However, most of these algorithms are "general-purpose" or "weak" in that they are not designed to take advantage of the structure of the prior knowledge, and hence, do not always give good results.
Reference: <author> Pitt, L. and Warmuth, M. K., </author> <title> Prediction Preserving Reducibility, Journal of Computer and System Sciences, </title> <publisher> Academic Press, </publisher> <address> 41:3, </address> <year> 1990. </year>
Reference-contexts: Learning with tree-structured bias from classified random examples is reducible to learning arbitrary boolean formulas <ref> (Pitt and Warmuth, 1990) </ref>, which in turn is reducible to some apparently hard cryptographic problems (Kearns and Valiant, 1989). To see that tree-structured bias cannot be implemented with membership queries alone, consider the class of boolean functions which output 1 on exactly one input and output 0 on all others.
Reference: <author> Quinlan, R., </author> <title> Induction of Decision Trees, Machine Learning, </title> <publisher> Kluwer Academic, </publisher> <address> 1:1, </address> <year> 1986. </year>
Reference-contexts: The other is a neural network implementation which uses Backprop for learning and is too slow to be used on trees of a few dozen nodes. Hence TSB is compared with with ID3, which is knowledge-free and hence cannot be expected to perform as well <ref> (Quinlan, 1986) </ref>. However, the comparison is useful to demonstrate the power of tree-structured bias and special-purpose learning algorithms designed to exploit it. TSB was tested on a determination tree with k = 3.
Reference: <author> Russell, S., </author> <title> The Use of Knowledge in Analogy and Induction. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1989. </year>
Reference-contexts: The determination knowledge identifies all the attributes relevant to making predictions about a target attribute <ref> (Russell, 1989) </ref>. <p> Russell showed that the power of determinations in reducing the number of training examples is even greater when the learner has a set of determinations structured as a tree <ref> (Russell, 1989) </ref>. Tree-structured bias consists of a tree of attributes such that each attribute at a node is determined by a small number of attributes represented as its children. <p> We close with a discussion of related work and future research. 2 Tree-structured Bias Russell identified a useful notion of bias called "determinations" which specifies the set of all relevant attributes to make predictions about a target attribute <ref> (Russell, 1989) </ref>. Let an object x be described by a set of input attributes P 1 ; : : : ; P n , whose values are denoted by P 1 (x); : : : ; P n (x). <p> Since learning occurs by filtering out functions which are not consistent with the training examples, the more constraining the determination, the fewer the examples needed to filter the remaining functions. Russell introduced "tree-structured bias" to further reduce the number of examples needed for learning <ref> (Russell, 1989) </ref>. Tree-structured bias consists of a tree of attributes such that each attribute at a node is determined by at most a small constant number (k) of other attributes, which are represented as its children. <p> Russell used these results to derive an upper bound on the number of random examples sufficient to learn a function consistent with a given tree-structured bias <ref> (Russell, 1989) </ref>.
Reference: <author> Shavlik, J. W. and Towell, G. G., </author> <title> An Approach to Combining Explanation-Based and Neural Learning Algorithms, </title> <journal> Connection Sciences, </journal> <volume> 1:3, </volume> <year> 1989. </year>
Reference-contexts: Incorporating declarative bias into learning is an active research topic in machine learning, and has recently given rise to many useful algorithms, e.g., (Cohen, 1992), (Pazzani and Kibler, 1992), and <ref> (Shavlik and Towell, 1989) </ref>. However, most of these algorithms are "general-purpose" or "weak" in that they are not designed to take advantage of the structure of the prior knowledge, and hence, do not always give good results.
Reference: <author> Valiant, L. G., </author> <title> A Theory of the Learnable, </title> <journal> Communications of the ACM, </journal> <volume> 27:11, </volume> <month> August </month> <year> 1984. </year>
Reference-contexts: Hence, we assume that each internal node in the determination tree has 2 to k children. 3 The PAC Learning Framework In this paper, we will be using a version of Probably Approximately Correct (PAC) learning, introduced by Valiant, and extended by Angluin <ref> (Valiant, 1984) </ref>, (Angluin, 1988). For a broad introduction, see (Natarajan, 1991). We restrict ourselves here to learning boolean functions.
References-found: 15

