URL: ftp://ftp.cs.colorado.edu/users/baveja/Papers/ML91.ps.gz
Refering-URL: http://www.cs.colorado.edu/~baveja/papers.html
Root-URL: 
Email: satinder@cs.umass.edu  
Title: Transfer of Learning Across Compositions of Sequential Tasks  
Author: Satinder P. Singh 
Address: Amherst,MA 01003  
Affiliation: Computer and Information Science Dept. University of Massachusetts,  
Abstract: Most "weak" learning algorithms, including reinforcement learning methods, have been applied on tasks with single goals. The effort to build more sophisticated learning systems that operate in complex environments will require the ability to handle multiple goals. Methods that allow transfer of learning will play a crucial role in learning systems that support multiple goals. In this paper I describe a class of multiple tasks that represents a subset of routine animal activity. I present a new learning algorithm and an architecture that allows transfer of learning by the "sharing" of solutions to the common parts of multiple tasks. A proof of the algorithm is also provided.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. G. Barto and S. P. Singh. </author> <title> Reinforcement learning and dynamic programming. </title> <booktitle> In Proc. of the Sixth Yale Workshop on Adaptive and Learning Systems, </booktitle> <address> New Haven, CT, </address> <month> Aug </month> <year> 1990. </year>
Reference-contexts: I will adopt the latter, more economical, formulation of multiple SDTs. 3 COMPOSITIONAL Q-LEARNING If an exact model of the dynamics of the world and the payoff structure is available, dynamic programming based computational procedures can be used to solve for the optimal policy (see Barto and Singh <ref> [1] </ref>, 1990).
Reference: [2] <author> A. G. Barto, R. S. Sutton, and C. W. Anderson. </author> <title> Neuronlike elements that can solve difficult learning control problems. </title> <journal> IEEE SMC, </journal> <volume> 13 </volume> <pages> 835-846, </pages> <year> 1983. </year>
Reference-contexts: In the absence of a world-model and the payoff structure, reinforcement learning methods that approximate dynamic programming techniques, such as temporal difference (TD) procedures (Sutton [8], 1988; Barto et al. <ref> [2] </ref>, 1983) and Q-learning methods (Watkins [10], 1989; Barto and Singh [4], 1990), can be used to directly estimate an optimal policy without building an explicit model of the world-dynamics.
Reference: [3] <author> A. G. Barto, R. S. Sutton, and C. Watkins. </author> <title> Sequential decision problems and neural networks. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> NIPS 2, </booktitle> <pages> pages 686-693, </pages> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [4] <author> A.G. Barto and S.P. Singh. </author> <title> On the computational economics of reinforcement learning. </title> <booktitle> In Proc. of the 1990 Connectionist Models Summer School, </booktitle> <address> San Mateo, CA, Nov. 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In the absence of a world-model and the payoff structure, reinforcement learning methods that approximate dynamic programming techniques, such as temporal difference (TD) procedures (Sutton [8], 1988; Barto et al. [2], 1983) and Q-learning methods (Watkins [10], 1989; Barto and Singh <ref> [4] </ref>, 1990), can be used to directly estimate an optimal policy without building an explicit model of the world-dynamics.
Reference: [5] <author> R. A. Jacobs. </author> <title> Task decomposition through competition in a modular connectionist architecture. </title> <type> PhD thesis, </type> <institution> COINS dept Univ. of Massachusetts, </institution> <address> Amherst, Mass. U.S.A., </address> <year> 1990. </year>
Reference-contexts: Note that CQ-learning does not figure out the decomposition of a composite task. 4 SCHEDULING ARCHITECTURE The scheduling architecture is adapted from the modular gating architecture developed by Jacobs <ref> [5] </ref> (1990), which has only been used to learn multiple non-sequential tasks within the supervised learning paradigm. The scheduling architecture combines CQ-learning with Nowlan's [6] (1990) competitive experts algorithm in order to learn both the decomposition for a composite task and its Q-value function.
Reference: [6] <author> S. J. Nowlan. </author> <title> Competing experts: An experimental investigation of associative mixture models. </title> <type> Technical Report CRG-TR-90-5, </type> <institution> Department of Computer Sc., Univ. of Toronto, Toronto, Canada, </institution> <year> 1990. </year>
Reference-contexts: The scheduling architecture combines CQ-learning with Nowlan's <ref> [6] </ref> (1990) competitive experts algorithm in order to learn both the decomposition for a composite task and its Q-value function. A very brief description of the scheduling architecture is presented here (See Nowlan [6], (1990); Singh [7], (1991) for details.). <p> The scheduling architecture combines CQ-learning with Nowlan's <ref> [6] </ref> (1990) competitive experts algorithm in order to learn both the decomposition for a composite task and its Q-value function. A very brief description of the scheduling architecture is presented here (See Nowlan [6], (1990); Singh [7], (1991) for details.). <p> Scheduling Module Stochastic Switch Action Action Action Module 1 Module 2 Module Q Q Q o m l s r White Noise N (0, s) + + + Bias Module Goal b g 1 2 State State Q n nn . . . s State Reduced State Reduced State Nowlan <ref> [6] </ref> (1990)). See text for details. The scheduling architecture (see Figure 1) consists of a number of Q-modules that compete to learn the Q-value functions for the elemental tasks.
Reference: [7] <author> S. P. Singh. </author> <title> Transfer of learning by composing elemental sequential tasks, </title> <note> 1991. submitted to Machine Learning Special Issue on Reinforcement Learning. </note>
Reference-contexts: The scheduling architecture combines CQ-learning with Nowlan's [6] (1990) competitive experts algorithm in order to learn both the decomposition for a composite task and its Q-value function. A very brief description of the scheduling architecture is presented here (See Nowlan [6], (1990); Singh <ref> [7] </ref>, (1991) for details.). <p> Please note that due to lack of space, details of the scheduling architecture have not been provided. See Singh <ref> [7] </ref>, (1991) for details. 7 DISCUSSION This paper presented an economical formulation for multiple, compositionally-structured tasks within the sequential decision-making framework.
Reference: [8] <author> R. S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: In the absence of a world-model and the payoff structure, reinforcement learning methods that approximate dynamic programming techniques, such as temporal difference (TD) procedures (Sutton <ref> [8] </ref>, 1988; Barto et al. [2], 1983) and Q-learning methods (Watkins [10], 1989; Barto and Singh [4], 1990), can be used to directly estimate an optimal policy without building an explicit model of the world-dynamics.
Reference: [9] <author> R. S. Sutton. </author> <title> Integrating architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proc. of the Seventh International Conf. on Machine Learning, </booktitle> <pages> pages 216-224, </pages> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: An optimal control policy is a mapping from states to actions that achieves the agent's objective. Control architectures based on reinforcement learning methods are increasingly being used for learning situation-action rules, or reactions, that can then be used for real-time decision making (Sutton <ref> [9] </ref>, 1990; Whitehead and Ballard [11],1990). To date, most applications have dealt with learning to solve single tasks, i.e., learning the situation-action rules for satisfying a single goal. One way to solve multiple SDTs is to learn the optimal policy for each SDT independently.
Reference: [10] <author> C. J. C. H. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge Univ., </institution> <address> Cambridge, England, </address> <year> 1989. </year>
Reference-contexts: In the absence of a world-model and the payoff structure, reinforcement learning methods that approximate dynamic programming techniques, such as temporal difference (TD) procedures (Sutton [8], 1988; Barto et al. [2], 1983) and Q-learning methods (Watkins <ref> [10] </ref>, 1989; Barto and Singh [4], 1990), can be used to directly estimate an optimal policy without building an explicit model of the world-dynamics.
Reference: [11] <author> S. D. Whitehead and D. H. Ballard. </author> <title> Active perception and reinforcement learning. </title> <booktitle> In Proc. of the Seventh International Conf. on Machine Learning, </booktitle> <address> Austin, TX, </address> <month> June </month> <year> 1990. </year>
References-found: 11

