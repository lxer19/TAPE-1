URL: http://www.ics.uci.edu/~pazzani/Publications/Pazzani-aistats96.ps
Refering-URL: http://www.ics.uci.edu/~mlearn/MLPapers.html
Root-URL: 
Email: pazzani@ics.uci.edu  
Phone: phone: (714) 824-5888 fax (714) 824-4056  
Title: Searching for dependencies in Bayesian classifiers j A n V n j If the attributes
Author: Michael J. Pazzani 
Note: 1.1 Introduction P (C i jA 1 V 1  
Address: Irvine, CA 92717  
Affiliation: Department of Information and Computer Science University of California, Irvine  
Abstract: Naive Bayesian classifiers which make independence assumptions perform remarkably well on some data sets but poorly on others. We explore ways to improve the Bayesian classifier by searching for dependencies among attributes. We propose and evaluate two algorithms for detecting dependencies among attributes and show that the backward sequential elimination and joining algorithm provides the most improvement over the naive Bayesian classifier. The domains on which the most improvement occurs are those domains on which the naive Bayesian classifier is significantly less accurate than a decision tree learner. This suggests that the attributes used in some common databases are not independent conditioned on the class and that the violations of the independence assumption that affect the accuracy of the classifier The Bayesian classifier (Duda & Hart, 1973) is a probabilistic method for classification. It can be used to determine the probability that an example j belongs to class C i given values of attributes of an example represented as a set of n nominally-valued attribute-value pairs of the form A 1 = V 1 j : ^ P (A k = V k j jC i ) may be estimated from the training data. To determine the most likely class of a test example, the probability of each class is computed with Equation 1. A classifier created in this manner is sometimes called a simple (Langley, 1993) or naive (Kononenko, 1990) Bayesian classifier. One important evaluation metric for machine learning methods is the predictive accuracy on unseen examples. This is measured by randomly selecting a subset of the examples in a database to use as training examples and reserving the remainder to be used as test examples. In the case of the simple Bayesian classifier, the training examples are used to estimate probabilities and Equation 1.1 is then used can be detected from training data.
Abstract-found: 1
Intro-found: 1
Reference: <author> Almuallim, H., and Dietterich, T. G. </author> <year> (1991). </year> <title> Learning with many irrelevant features. </title> <booktitle> In Ninth National Conference on Artificial Intelligence, </booktitle> <pages> 547-552. </pages> <publisher> MIT Press. </publisher>
Reference: <author> Caruana, R., & Freitag, D. </author> <year> (1994). </year> <title> Greedy attribute selection. </title> <editor> In Cohen, W., and Hirsh, H., eds., </editor> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference. </booktitle> <publisher> Morgan Kaufmann Cooper, </publisher> <editor> G. & Herskovits, E. </editor> <year> (1992). </year> <title> A Bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9, </volume> <pages> 309-347. </pages>
Reference: <author> Danyluk, A. & Provost, F. </author> <year> (1993). </year> <title> Small disjuncts in action: Learning to diagnose errors in the telephone network local loop. </title> <booktitle> Machine Learning Conference, </booktitle> <pages> pp 81-88. </pages>
Reference: <author> Duda, R. & Hart, P. </author> <year> (1973). </year> <title> Pattern classification and scene analysis. </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: 1.1 Introduction The Bayesian classifier <ref> (Duda & Hart, 1973) </ref> is a probabilistic method for classification.
Reference: <author> John, G. Kohavi, R., & Pfleger, K. </author> <year> (1994). </year> <booktitle> Irrelevant Features and the subset selection problem Proceedings of the Eleventh International Conference on Machine Learning. </booktitle> <address> New Brunswick, NJ. </address>
Reference: <author> Kittler, J. </author> <year> (1986). </year> <title> Feature selection and extraction. In Young & Fu, </title> <editor> (eds.), </editor> <booktitle> Handbook of pattern recognition and image processing. </booktitle> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference: <author> Kononenko, I. </author> <year> (1990). </year> <title> Comparison of inductive and naive Bayesian learning approaches to automatic knowledge acquisition. </title> <editor> In B. Wielinga (Eds..), </editor> <booktitle> Current trends in knowledge acquisition. </booktitle> <address> Amsterdam: </address> <publisher> IOS Press. </publisher>
Reference-contexts: To determine the most likely class of a test example, the probability of each class is computed with Equation 1. A classifier created in this manner is sometimes called a simple (Langley, 1993) or naive <ref> (Kononenko, 1990) </ref> Bayesian classifier. One important evaluation metric for machine learning methods is the predictive accuracy on unseen examples. This is measured by randomly selecting a subset of the examples in a database to use as training examples and reserving the remainder to be used as test examples.
Reference: <author> Kononenko, I. </author> <year> (1991). </year> <title> Semi-naive Bayesian classifier. </title> <booktitle> Proceedings of the Sixth European Working Session on Learning. </booktitle> <pages> (pp. 206-219). </pages> <address> Porto, Portugal: </address> <publisher> Pittman. </publisher>
Reference-contexts: In practice, when these probabilities are estimated from training data, these quantities will rarely be equal. One approach to deal with this problem is to assume that attributes are conditionally independent unless there is a significant difference according to some statistical criteria <ref> (Kononenko, 1991) </ref>. Here, we consider an alternative approach that does not directly address the question of conditional independence. Instead, we ask, for the purposes of maximizing predictive accuracy, whether it is better to join two attributes.
Reference: <author> Langley, P. </author> <year> (1993). </year> <title> Induction of recursive Bayesian classifiers. </title> <booktitle> Proceedings of the 1993 European Conference on Machine Learning. </booktitle> <pages> (pp. 153-164). </pages> <address> Vienna: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: To determine the most likely class of a test example, the probability of each class is computed with Equation 1. A classifier created in this manner is sometimes called a simple <ref> (Langley, 1993) </ref> or naive (Kononenko, 1990) Bayesian classifier. One important evaluation metric for machine learning methods is the predictive accuracy on unseen examples. <p> Leave-one-out evaluation is used because it allows a single Bayesian classifier to be constructed on the entire training set. To classify a training example, the contribution of that example to the probability estimates is subtracted out <ref> (Langley, 1993) </ref>. Each algorithm considers a set of possible operations (such as joining two attributes) and selects the operation that most improves the accuracy of the classifier as measured by leave-one-out cross validation.
Reference: <author> Langley, P. & Sage, S. </author> <year> (1994). </year> <title> Induction of selective Bayesian classifiers. </title> <booktitle> Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence. </booktitle> <address> Seattle, </address> <note> WA Moore, </note> <author> A. W., and Lee, M. S. </author> <year> 1994. </year> <title> Efficient algorithms for minimizing cross validation error. </title> <editor> In Cohen, W. W., and Hirsh, H., eds., </editor> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Murphy, P. M. , & Aha, D. W. </author> <year> (1995). </year> <title> UCI Repository of machine learning databases. </title> <institution> Irvine: University of California, Department of Information & Computer Science.[Machine-readable data repository ftp://ics.uci.edu:/pub/machine-learning-databases/] Pazzani, </institution> <note> M., </note> <author> Merz, C., Murphy, P., Ali, K., Hume, T., and Brunk, C. </author> <year> (1994). </year> <title> Reducing Misclassifi-cation Costs. </title> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning. </booktitle> <address> New Brunswick, NJ. </address>
Reference-contexts: For example, Table 1.1 compares the naive Bayesian classifier to ID3, a simple decision tree algorithm (Quinlan, 1986) on several problems from the UCI archive of machine learning databases <ref> (Murphy & Aha, 1995) </ref>. On each problem, both algorithms were run 24 times on the same training sets and tested on the same disjoint test sets consisting of all examples not in the training set.
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic reasoning in intelligent systems: Networks of plausible inference. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Quinlan, J.R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <month> 81-106. </month> <title> Searching for dependencies in Bayesian classifiers 10 Rachlin, </title> <type> Kasif, Salzberg & Aha, </type> <year> (1994). </year> <title> Towards a better understanding of memory-based reasoning systems. </title> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning. </booktitle> <address> New Brunswick, NJ. </address>
Reference-contexts: On many problems, the accuracy of the naive Bayesian classifier is equal to or greater than that of more sophisticated machine learning algorithms. For example, Table 1.1 compares the naive Bayesian classifier to ID3, a simple decision tree algorithm <ref> (Quinlan, 1986) </ref> on several problems from the UCI archive of machine learning databases (Murphy & Aha, 1995). On each problem, both algorithms were run 24 times on the same training sets and tested on the same disjoint test sets consisting of all examples not in the training set.
Reference: <author> Ragavan, H. & Rendell, L. </author> <year> (1993). </year> <title> Lookahead feature construction for learning hard concepts. </title> <booktitle> Machine Learning: Proceedings of the Tenth International Conference. </booktitle> <publisher> Morgan Kaufmann Schlimmer, </publisher> <editor> J. </editor> <year> (1987). </year> <title> Incremental adjustment of representations for learning. </title> <booktitle> Machine Learning: Proceedings of the Fourth International Workshop. </booktitle> <publisher> Morgan Kaufmann Schaffer, </publisher> <address> C. </address> <year> (1994). </year> <booktitle> A conservation law of generalization performance Proceedings of the Eleventh International Conference on Machine Learning. </booktitle> <address> New Brunswick, NJ. </address>
References-found: 14

