URL: http://www-anw.cs.umass.edu/People/singh/Papers/singh-MLJ-1.ps.Z
Refering-URL: http://www-anw.cs.umass.edu/People/singh/Papers/
Root-URL: 
Email: SATINDER@cs.umass.edu  
Title: Transfer of Learning by Composing Solutions of Elemental Sequential Tasks  
Author: Satinder Pal Singh 
Keyword: Reinforcement Learning, Compositional Learning, Modular Architecture, Transfer of Learning. Running Head: Transfer of Learning across Sequential Tasks  
Date: December 1, 1991  
Address: Amherst, MA 01003  
Affiliation: Department of Computer Science University of Massachusetts  
Abstract: Although building sophisticated learning agents that operate in complex environments will require learning to perform multiple tasks, most applications of reinforcement learning have focussed on single tasks. In this paper I consider a class of sequential decision tasks (SDTs), called composite sequential decision tasks, formed by temporally concatenating a number of elemental sequential decision tasks. Elemental SDTs cannot be decomposed into simpler SDTs. I consider a learning agent that has to learn to solve a set of elemental and composite SDTs. I assume that the structure of the composite tasks is unknown to the learning agent. The straightforward application of reinforcement learning to multiple tasks requires learning the tasks separately, which can waste computational resources, both memory and time. I present a new learning algorithm and a modular architecture that learns the decomposition of composite SDTs, and achieves transfer of learning by sharing the solutions of elemental SDTs across multiple composite SDTs. The solution of a composite SDT is constructed by computationally inexpensive modifications of the solutions of its constituent elemental SDTs. I provide a proof of one aspect of the learning algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A. G., Bradtke, S. J., & Singh, S. P. </author> <year> (1991). </year> <title> Real-time learning and control using asynchronous dynamic programming. </title> <type> Technical Report 91-57, </type> <institution> COINS dept., University of Mas-sachusetts, </institution> <address> Amherst, MA. </address>
Reference: <author> Barto, A. G. & Singh, S. P. </author> <year> (1990). </year> <title> On the computational economics of reinforcement learning. </title> <booktitle> In Proc. of the 1990 Connectionist Models Summer School, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Barto, A. G., Sutton, R. S., & Anderson, C. W. </author> <year> (1983). </year> <title> Neuronlike elements that can solve difficult learning control problems. </title> <journal> IEEE SMC, </journal> <volume> 13, </volume> <pages> 835-846. </pages>
Reference: <author> Barto, A. G., Sutton, R. S., & Watkins, C. </author> <year> (1990). </year> <title> Sequential decision problems and neural networks. </title> <editor> In Touretzky, D. S. (Ed.), </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 686-693, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Bertsekas, D. P. </author> <year> (1987). </year> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference-contexts: Thus, decisions cannot be viewed in isolation because the agent must account for both the short-term and the long-term consequences of its actions. The framework of sequential decision-making is quite general and can be used to pose a large number of tasks from diverse fields <ref> (e.g., Bertsekas, 1987) </ref>. Much of everyday human activity involves sequential tasks that have compositional structure, i.e., complex tasks built up in a systematic way from simpler tasks. <p> If the environment dynamics and the payoff function are known, computational procedures based on dynamic programming can be used to find an optimal policy <ref> (Bertsekas, 1987) </ref>. In particular, DP methods based on the value iteration algorithm (e.g. Ross, 1983) compute an optimal value function that assigns to states their scalar expected returns under an optimal policy.
Reference: <author> Brooks, R. </author> <year> (1989). </year> <title> A robot that walks: Emergent behaviors from a carefully evolved network. </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <pages> 23-262. </pages>
Reference-contexts: CQ-L's use of acquired subsequences to solve composite tasks is similar to the use of macro-operators (Korf, 1985) in macro-learning systems (Iba, 1989). Another architecture similar to CQ-L is the subsumption architecture for autonomous intelligent agents <ref> (Brooks, 1989) </ref>, which is composed of several task-achieving modules along with precompiled switching circuitry that controls which module should be active at any time. Maes and Brooks (1990) showed how reinforcement learning can be used to learn the switching circuitry for a robot with hardwired task modules.
Reference: <author> Duda, R. O. & Hart, P. E. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: Learning is achieved by gradient descent in the log likelihood of generating the desired target patterns. The gating architecture has been used to learn only multiple non-sequential tasks within the supervised learning paradigm <ref> (Duda & Hart, 1973) </ref>. I extend the gating architecture to a CQ-Learning architecture (Figure 1), called CQ-L, that can learn multiple compositionally-structured sequential tasks even when training information required for supervised learning is not available.
Reference: <author> Iba, G. A. </author> <year> (1989). </year> <title> A heuristic approach to the discovery of macro-operators. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 285-317. </pages>
Reference-contexts: CQ-L's use of acquired subsequences to solve composite tasks is similar to the use of macro-operators (Korf, 1985) in macro-learning systems <ref> (Iba, 1989) </ref>. Another architecture similar to CQ-L is the subsumption architecture for autonomous intelligent agents (Brooks, 1989), which is composed of several task-achieving modules along with precompiled switching circuitry that controls which module should be active at any time.
Reference: <author> Jacobs, R. A. </author> <year> (1990). </year> <title> Task decomposition through competition in a modular connectionist architecture. </title> <type> PhD thesis, </type> <institution> COINS dept., Univ. of Massachusetts, </institution> <address> Amherst, Mass. U.S.A. </address>
Reference: <author> Jacobs, R. A. & Jordan, M. I. </author> <year> (1991). </year> <title> A competitive modular connectionist architecture. </title> <booktitle> In Advances is Neural Information Processing Systems 3. </booktitle>
Reference: <author> Jacobs, R. A., Jordan, M. I., Nowlan, S. J., & Hinton, G. E. </author> <year> (1991). </year> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3 (1). </volume>
Reference: <author> Kaelbling, L. P. </author> <year> (1990). </year> <title> Learning in Embedded Systems. </title> <type> PhD thesis, </type> <institution> Stanford University, Department of Computer Science, Stanford, CA. </institution> <note> Technical Report TR-90-04. </note>
Reference: <author> Korf, R. E. </author> <year> (1985). </year> <title> Macro-operators: A weak method for learning. </title> <journal> Artificial Intelligence, </journal> <volume> 26, </volume> <pages> 35-77. </pages>
Reference-contexts: CQ-L's use of acquired subsequences to solve composite tasks is similar to the use of macro-operators <ref> (Korf, 1985) </ref> in macro-learning systems (Iba, 1989). Another architecture similar to CQ-L is the subsumption architecture for autonomous intelligent agents (Brooks, 1989), which is composed of several task-achieving modules along with precompiled switching circuitry that controls which module should be active at any time.
Reference: <author> Maes, P. & Brooks, R. </author> <year> (1990). </year> <title> Learning to coordinate behaviours. </title> <booktitle> In Proceedings of the Eighth AAAI, </booktitle> <pages> pages 796-802. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Mahadevan, S. & Connell, J. </author> <year> (1990). </year> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <type> Technical report, </type> <institution> IBM Research Division, T.J.Watson Research Center, Yorktown Heights, NY. </institution> <note> 19 Nowlan, </note> <author> S. J. </author> <year> (1990). </year> <title> Competing experts: An experimental investigation of associative mixture models. </title> <type> Technical Report CRG-TR-90-5, </type> <institution> Department of Computer Sc., Univ. of Toronto, Toronto, Canada. </institution>
Reference: <author> Ross, S. </author> <year> (1983). </year> <title> Introduction to Stochastic Dynamic Programming. </title> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference-contexts: Any control policy that achieves the agent's goal (and there may be more than one) is called an optimal policy. The optimal policies for the class of infinite horizon MDTs defined above are stationary <ref> (Ross, 1983) </ref>, i.e., are not functions of time, and therefore, I restrict attention to stationary policies. Clearly, both the environment dynamics and the payoff function play a role in determining the set of optimal policies. <p> If the environment dynamics and the payoff function are known, computational procedures based on dynamic programming can be used to find an optimal policy (Bertsekas, 1987). In particular, DP methods based on the value iteration algorithm <ref> (e.g. Ross, 1983) </ref> compute an optimal value function that assigns to states their scalar expected returns under an optimal policy.
Reference: <author> Singh, S. P. </author> <year> (1992a). </year> <title> On the efficient learning of multiple sequential tasks. </title> <editor> In Lippman, R., Moody, J., & Touretzky, D. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <address> San Mteo, CA. </address> <publisher> Morgan Kauffman. </publisher>
Reference-contexts: In this paper I propose a compositional learning scheme in which separate modules learn to solve the elemental tasks, and a task-sensitive gating module solves composite tasks by learning to compose the appropriate elemental modules over time <ref> (also see Singh, 1992a) </ref>.
Reference: <author> Singh, S. P. </author> <year> (1992b). </year> <title> Solving multiple sequential tasks using a hierarchy of variable temporal resolution models. </title> <note> Submitted to Machine Learning Conference, </note> <year> 1992. </year>
Reference: <author> Skinner, B. F. </author> <year> (1938). </year> <title> The Behavior of Organisms: An experimental analysis. </title> <address> New York: D. </address> <publisher> Appleton Century. </publisher>
Reference-contexts: Another approach is to train the robot on a succession of tasks, where each succeeding task requires some subset of the already learned elemental tasks, plus a new elemental task. This roughly corresponds to the "shaping" procedures <ref> (Skinner, 1938) </ref> used by psychologists to train animals to do complex motor tasks. A simple simulation to illustrate shaping was constructed by training CQ-L with one Q-module on one elemental task, T 3 , for 1; 000 trials and then training on the composite task C 2 .
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference: <author> Sutton, R. S. </author> <year> (1990). </year> <title> Integrating architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proc. of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 216-224, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge Univ., </institution> <address> Cambridge, England. </address>
Reference: <author> Watkins, C. J. C. H. & Dayan, P. </author> <year> (1992). </year> <note> Q-learning. Machine Learning. to appear. </note>
Reference: <author> Whitehead, S. D. & Ballard, D. H. </author> <year> (1990). </year> <title> Active perception and reinforcement learning. </title> <booktitle> In Proc. of the Seventh International Conference on Machine Learning, </booktitle> <address> Austin, TX. </address> <note> M. 20 </note>
References-found: 24

