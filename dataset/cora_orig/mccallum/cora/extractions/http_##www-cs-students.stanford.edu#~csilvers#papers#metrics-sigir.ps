URL: http://www-cs-students.stanford.edu/~csilvers/papers/metrics-sigir.ps
Refering-URL: http://www-cs-students.stanford.edu/~csilvers/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: email: schuetze@parc.xerox.com, csilvers@cs.stanford.edu  
Title: Projections for Efficient Document Clustering  
Author: Hinrich Schutze, Craig Silverstein 
Web: URL: ftp://parcftp.xerox.com/pub/qca/papers  
Address: 3333 Coyote Hill Road Palo Alto, CA 94304  
Affiliation: Xerox Palo Alto Research Center  
Abstract: Clustering is increasing in importance, but linear- and even constant-time clustering algorithms are often too slow for real-time applications. A simple way to speed up clustering is to speed up the distance calculations at the heart of clustering routines. We study two techniques for improving the cost of distance calculations, LSI and truncation, and determine both how much these techniques speed up clustering and how much they affect the quality of the resulting clusters. We find that the speed increase is significant while | surprisingly | the quality of clustering is not adversely affected. We conclude that truncation yields clusters as good as those produced by full-profile clustering while offering a significant speed advantage. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Michael W. Berry. </author> <title> Large-scale sparse singular value computations. </title> <journal> The International Journal of Supercomputer Applications, </journal> <volume> 6(1) </volume> <pages> 13-49, </pages> <year> 1992. </year>
Reference-contexts: In this sense, LSI finds an optimal solution to dimensionality reduction. See [9] for a further discussion of LSI and <ref> [1] </ref> for a description of SVD and the algorithms we use to compute it.
Reference: [2] <author> Chris Buckley, Amit Singhal, Mandar Mitra, and Ger-ard Salton. </author> <title> New retrieval approaches using SMART: </title> <booktitle> TREC 4. </booktitle> <pages> pages 25-48, </pages> <year> 1996. </year> <note> In [15]. </note>
Reference-contexts: Truncation works as follows: consider, for each vector d, the c 2 An exception is the very long queries that may be generated via relevance feedback and pseudo-feedback. In this case, truncation is not just an efficiency tool but, as discussed in <ref> [2] </ref>, is also probably necessary to maintain the quality of the search result set. 3 We use the concepts of similarity and distance interchangeably here since Euclidean distance and correlation coefficient produce the same ranking for normalized vectors: fi fi 2 X X i 2 a i b i + b <p> This type of strategy has been used by [22, 4, 29, 13] among others. In feedback selection, we modify the query with pseudo-feedback before selecting the cluster closest to it. We use the method suggested in <ref> [2] </ref>: We expand the query with the 20 documents that are ranked at the top in an initial retrieval with the unmodified query, and then we delete all but the 50 highest weighted terms of the expanded query.
Reference: [3] <author> Edith Cohen and David D. Lewis. </author> <title> Approximating matrix multiplication for pattern recognition tasks. </title> <booktitle> In Pro ceedings of the Eight Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 682-691, </pages> <year> 1997. </year>
Reference-contexts: Projecting documents is the simplest way to speed up the distance calculation, but it is not the only one. Cohen and Lewis have looked at using a modified matrix multiplication routine for calculating approximate Euclidean distance <ref> [3] </ref>. Such an approach can be used in conjunction with document projection but may be more costly. Regardless, the fact that the algorithms community is expressing interest in this problem is an indication of its increasing importance. In Section 2 we describe the projection techniques evaluated in this paper.
Reference: [4] <author> W. B. Croft. </author> <title> A model of cluser searching based on classification. </title> <journal> Information Systems, </journal> <volume> 5 </volume> <pages> 189-196, </pages> <year> 1980. </year>
Reference-contexts: There are three levels for this factor. In the first, closest cluster selection, we select the cluster that is closest to the query, or more exactly, the cluster whose centroid is closest to the query. This type of strategy has been used by <ref> [22, 4, 29, 13] </ref> among others. In feedback selection, we modify the query with pseudo-feedback before selecting the cluster closest to it.
Reference: [5] <author> C. J. Crouch. </author> <title> An approach to the automatic construction of global thesauri. </title> <booktitle> Information Processing & Management, </booktitle> <volume> 26(5) </volume> <pages> 629-640, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction Clustering is becoming increasingly widespread: It is finding applications in browsing [8, 7], in improving the performance of similarity search tools [16, 19], and in automatically generating thesauri <ref> [5, 6] </ref>. In query analysis, clustering has been used for transforming a free-text query into a fuzzy Boolean constraint [25]. The popularity of Yahoo! demonstrates the potential of categorization for presenting information on the World Wide Web.
Reference: [6] <author> Carolyn J. Crouch and Bokyung Yang. </author> <title> Experiments in automatic statistical thesaurus construction. </title> <booktitle> In Proceedings of SIGIR, </booktitle> <pages> pages 77-88, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction Clustering is becoming increasingly widespread: It is finding applications in browsing [8, 7], in improving the performance of similarity search tools [16, 19], and in automatically generating thesauri <ref> [5, 6] </ref>. In query analysis, clustering has been used for transforming a free-text query into a fuzzy Boolean constraint [25]. The popularity of Yahoo! demonstrates the potential of categorization for presenting information on the World Wide Web.
Reference: [7] <author> Douglas R. Cutting, David R. Karger, and Jan O. Ped-ersen. </author> <title> Constant interaction-time scatter/gather browsing of very large document collections. </title> <booktitle> In Proceedings of SIGIR'93, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Clustering is becoming increasingly widespread: It is finding applications in browsing <ref> [8, 7] </ref>, in improving the performance of similarity search tools [16, 19], and in automatically generating thesauri [5, 6]. In query analysis, clustering has been used for transforming a free-text query into a fuzzy Boolean constraint [25]. <p> Many of these clustering applications demand rapid response times while utilizing data sets too large for linear-time clustering algorithms. Even constant-time clustering algorithms such as constant-time Scatter/Gather <ref> [7] </ref> can, because of large constants, be too slow for very large data sets. It is possible to decrease the constants used in clustering routines. We concentrate on doing so in the context of clustering text documents, which we consider as vectors of terms. <p> IDF weighting down-weights frequent words and upweights rare words, which may be useful for similarity search but complicates clustering since clusters tend to be formed based on patterns of frequent words. We test two instantiations of TF-c, TF-50 and TF-20. TF-50 is similar to the truncation method recommended in <ref> [7] </ref>. (Cutting et al. also truncate to 50 terms, but they use d t for term frequency weighting instead of 1 + log d t .) We test TF-20 to explore how a substantial reduction in the truncation constant affects time efficiency and clustering effectiveness.
Reference: [8] <author> Douglas R. Cutting, Jan O. Pedersen, David Karger, and John W. Tukey. Scatter/gather: </author> <title> A cluster-based approach to browsing large document collections. </title> <booktitle> In Proceedings of SIGIR '92, </booktitle> <pages> pages 318-329, </pages> <address> New York, </address> <year> 1992. </year> <institution> Association of Computing Machinery. </institution>
Reference-contexts: 1 Introduction Clustering is becoming increasingly widespread: It is finding applications in browsing <ref> [8, 7] </ref>, in improving the performance of similarity search tools [16, 19], and in automatically generating thesauri [5, 6]. In query analysis, clustering has been used for transforming a free-text query into a fuzzy Boolean constraint [25]. <p> We evaluate the performance of each projection method by embedding it in a fixed distance metric | Euclidean distance | and using this metric in a fixed clustering algorithm | Buckshot <ref> [8] </ref>. 6 Buckshot first chooses a random sample of size p n, where n is the number of vectors to be clustered, and then clusters this sample with an O (n 2 ) algorithm. Hence, overall time complexity of this step is O (n).
Reference: [9] <author> Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. </author> <title> Indexing by latent semantic analysis. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 41(6) </volume> <pages> 391-407, </pages> <year> 1990. </year>
Reference-contexts: In this sense, LSI finds an optimal solution to dimensionality reduction. See <ref> [9] </ref> for a further discussion of LSI and [1] for a description of SVD and the algorithms we use to compute it. <p> In the similarity search context, dimen-sion reduction via LSI has proven to be effective in terms of retrieval performance <ref> [9, 11] </ref>. Truncation, on the other hand, is not interesting as an optimization technique for speed in similarity search, since queries generally are shorter than 20 key words 2 and access to the inverted index of documents is independent of the number of terms per document. <p> We test three instantiations of LSI-d, LSI-150, LSI-50, and LSI-20. The constant 150 is typical for the range of truncation constants in which LSI is competitive with | or superior to | term-based similarity search <ref> [9, 11, 12] </ref>. We test the two lower truncation constants, LSI-20 and LSI-50, to explore how a substantial reduction in the constant affects time efficiency and clustering effectiveness. 3 Experimental Design We would like to compare projection techniques both according to time efficiency and according to clustering effectiveness. <p> However, it is surprising that there is no significant difference between LSI-20, LSI-50 and LSI-150: In the similarity search arena, LSI with 50 dimensions has been shown to perform worse than LSI with 150 dimensions <ref> [9] </ref>. We conclude that for clustering, as opposed to 8 We also ran an ANOVA on the 5-way table of dimensions 49 fi 2 fi 2 fi 3 fi 6 with a separate average precision result for each query (as opposed to averaging average precision over 49 queries). <p> In fact, LSI-20 performed better than LSI-50 and LSI-150 under both the goodness measures we used, though we attribute this small, insignificant difference to noise. A major motivation for using LSI in similarity search is that it addresses the vocabulary problem in term-based retrieval. An example (adapted from <ref> [9] </ref>) is given in Figure 1. The query in the figure will only retrieve documents 1 and 3 for term-based similarity search.
Reference: [10] <author> A.P. Dempster, N.M. Laird, and D.B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> J. Royal Statistical Society Series B, </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: It is easy to measure time efficiency by recording the CPU time for the various techniques when embedded in a fixed clustering algorithm. Measuring clustering effectiveness is harder. One measure, used in probabilistic clustering methods such as EM clustering <ref> [10] </ref>, is the probability that the vectors are generated by a particular cluster model. Another measure, used in group-average agglomerative clustering, is the average distance between members of a cluster.
Reference: [11] <author> S. T. Dumais. </author> <title> Improving the retrieval of information from external sources. Behavior Research Methods, Instruments, </title> <journal> and Computers, </journal> <volume> 23 </volume> <pages> 229-236, </pages> <year> 1991. </year>
Reference-contexts: In the similarity search context, dimen-sion reduction via LSI has proven to be effective in terms of retrieval performance <ref> [9, 11] </ref>. Truncation, on the other hand, is not interesting as an optimization technique for speed in similarity search, since queries generally are shorter than 20 key words 2 and access to the inverted index of documents is independent of the number of terms per document. <p> We test three instantiations of LSI-d, LSI-150, LSI-50, and LSI-20. The constant 150 is typical for the range of truncation constants in which LSI is competitive with | or superior to | term-based similarity search <ref> [9, 11, 12] </ref>. We test the two lower truncation constants, LSI-20 and LSI-50, to explore how a substantial reduction in the constant affects time efficiency and clustering effectiveness. 3 Experimental Design We would like to compare projection techniques both according to time efficiency and according to clustering effectiveness.
Reference: [12] <author> Susan T. Dumais. </author> <title> Latent semantic indexing (lsi): </title> <type> Trec-3 report. </type> <pages> pages 219-230, </pages> <year> 1995. </year> <note> In [15]. </note>
Reference-contexts: We test three instantiations of LSI-d, LSI-150, LSI-50, and LSI-20. The constant 150 is typical for the range of truncation constants in which LSI is competitive with | or superior to | term-based similarity search <ref> [9, 11, 12] </ref>. We test the two lower truncation constants, LSI-20 and LSI-50, to explore how a substantial reduction in the constant affects time efficiency and clustering effectiveness. 3 Experimental Design We would like to compare projection techniques both according to time efficiency and according to clustering effectiveness.
Reference: [13] <author> A. Griffiths, H. C. Luckhurst, and P. Willett. </author> <title> Using inter-document similarity information in document retrieval systems. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 37 </volume> <pages> 3-11, </pages> <year> 1986. </year>
Reference-contexts: There are three levels for this factor. In the first, closest cluster selection, we select the cluster that is closest to the query, or more exactly, the cluster whose centroid is closest to the query. This type of strategy has been used by <ref> [22, 4, 29, 13] </ref> among others. In feedback selection, we modify the query with pseudo-feedback before selecting the cluster closest to it.
Reference: [14] <editor> D. K. Harman, editor. </editor> <booktitle> The Fourth Text REtrieval Conference (TREC-4). </booktitle> <institution> U.S. Department of Commerce, </institution> <address> Washington DC, </address> <year> 1996. </year> <note> NIST Special Publication 500-236. </note>
Reference-contexts: In the final step, all n vectors are assigned to the cluster whose centroid they are closest to. The centroid computation and reassignment steps have time complexity O (n), so overall complexity is O (n). Our test corpus is the Wall Street Journal subpart of the TREC-4 collection <ref> [14] </ref>. The queries we use are queries 202 through 250 of that collection; these are the 49 queries used for ad-hoc evaluation in TREC-4. The corpus consists of 74520 documents.
Reference: [15] <editor> D.K. Harman, editor. </editor> <booktitle> The Second Text REtrieval Conference (TREC-2). </booktitle> <institution> U.S. Department of Commerce, </institution> <address> Washington DC, </address> <year> 1994. </year> <note> NIST Special Publication 500-215. </note>
Reference: [16] <author> Marti A. Hearst and Jan O. Pedersen. </author> <title> Reexamining the cluster hypothesis. </title> <booktitle> In Proceedings of SIGIR '96, </booktitle> <pages> pages 76-84, </pages> <address> Zurich, </address> <year> 1996. </year>
Reference-contexts: 1 Introduction Clustering is becoming increasingly widespread: It is finding applications in browsing [8, 7], in improving the performance of similarity search tools <ref> [16, 19] </ref>, and in automatically generating thesauri [5, 6]. In query analysis, clustering has been used for transforming a free-text query into a fuzzy Boolean constraint [25]. The popularity of Yahoo! demonstrates the potential of categorization for presenting information on the World Wide Web. <p> Instead of using these rather abstract measures, we base our evaluation on cluster retrieval, a measure that is closely aligned with information retrieval performance. Cluster retrieval is a retrieval strategy based on the cluster hypothesis <ref> [18, 29, 16] </ref> which states that "closely associated documents tend to be relev ant to the same requests" [28]. <p> The queries (or topics) in TREC are long, but in many applications of information retrieval users supply much shorter queries. Finally, we also vary the way the "best" cluster is selected, using both a procedure similar to the one suggested in <ref> [16] </ref> and two automatic methods. The factors are summarized in Table 1 and described in more detail below. The scope factor describes the type of clustering. Global clustering clusters the 74520 documents in the corpus into about 400 clusters. In the first phase of Buckshot, 400 cluster centers are computed. <p> Finally, in density selection, we select the cluster with the highest proportion of relevant documents (cf. <ref> [16] </ref>). Note that density selection presumes that some amount of relevance information is available to the selection procedure whereas closest and feedback do not. The final factor, projection technique, has already been discussed in Section 2.
Reference: [17] <author> David A. Hull, Jan O. Pedersen, and Hinrich Schutze. </author> <title> Method combination for document filtering. </title> <booktitle> In Proceedings of SIGIR '96, </booktitle> <pages> pages 279-287, </pages> <address> Zurich, </address> <year> 1996. </year>
Reference-contexts: For this reason, we use a second rank-based score which compares methods on a query-by-query basis <ref> [17] </ref>. For this score, the average precision results of the 72 methods are ranked for a particular query. The best method receives rank 0, the second rank 1, and so on until the worst result receives rank 71.
Reference: [18] <author> N. Jardine and C. J. van Rijsbergen. </author> <title> The use of hierarchic clustering in information retrieval. </title> <booktitle> Information Storage and Retrieval, </booktitle> <volume> 7 </volume> <pages> 217-240, </pages> <year> 1971. </year>
Reference-contexts: Instead of using these rather abstract measures, we base our evaluation on cluster retrieval, a measure that is closely aligned with information retrieval performance. Cluster retrieval is a retrieval strategy based on the cluster hypothesis <ref> [18, 29, 16] </ref> which states that "closely associated documents tend to be relev ant to the same requests" [28].
Reference: [19] <author> Allan Lu, Maen Ayoub, and Jianhua Dong. </author> <title> Ad hoc experiments using EUREKA. </title> <booktitle> In Proceedings of TREC-5, </booktitle> <address> Gaithersburg MD, 1992. </address> <publisher> NIST. </publisher>
Reference-contexts: 1 Introduction Clustering is becoming increasingly widespread: It is finding applications in browsing [8, 7], in improving the performance of similarity search tools <ref> [16, 19] </ref>, and in automatically generating thesauri [5, 6]. In query analysis, clustering has been used for transforming a free-text query into a fuzzy Boolean constraint [25]. The popularity of Yahoo! demonstrates the potential of categorization for presenting information on the World Wide Web. <p> Cluster selection. Density selection (mean average precision: 0.108) is better than closest (mean average precision: 0.042) and feedback (mean average precision: 0.043) (ff = 0:01). This result suggests that it can be quite hard to find automatic cluster selection methods (like the one in <ref> [19] </ref>) that perform as well as partially manual ones like density. <p> The number of clusters was fixed at 400 for global and 5 in local clustering, resulting in an average cluster size of about 200. However, smaller cluster sizes have been found to be effective in <ref> [19] </ref> for local clustering, and the effect of cluster size on global clustering remains to be explored. In addition, we are interested in global term truncation, in which one global set of, say, 1000 terms is chosen to be retained after truncation.
Reference: [20] <author> Lyman Ott. </author> <title> An introduction to statistical methods and data analysis. </title> <publisher> Wadsworth, </publisher> <address> Belmont CA, </address> <year> 1992. </year>
Reference-contexts: In order to test for significant differences we use Tukey's W procedure. (For a detailed description of this test, see e.g. <ref> [20] </ref>.) This test takes into consideration that when a large number of difference tests are performed independently, there is high probability of error. For example, if the probability of error of an individual test is 5% and 100 tests are performed, then on average five tests will have erroneous results.
Reference: [21] <author> Jan O. Pedersen and Craig Silverstein. </author> <title> Almost-constant-time clustering of arbitrary corpus subsets. </title> <booktitle> In these proceedings, </booktitle> <year> 1997. </year>
Reference-contexts: For a given query, global is actually much faster than local if the global clustering already exists. The best solution to this time/quality trade-off might be a hybrid scheme where a pre-processed clustering is adaptively modified based on the result set. One possibility, described in <ref> [21] </ref>, is significantly faster than a local clustering scheme but equal to it in quality. Query. Long queries (mean average precision: 0.069) are significantly better than short queries (mean average precision: 0.060) (ff = 0:001).
Reference: [22] <author> G. Salton. </author> <title> Cluster search strategies and the optimization of retrieval effectiveness. </title> <editor> In G. Salton, editor, </editor> <booktitle> The SMART Retrieval System, </booktitle> <pages> pages 223-242. </pages> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs NJ, </address> <year> 1971. </year>
Reference-contexts: There are three levels for this factor. In the first, closest cluster selection, we select the cluster that is closest to the query, or more exactly, the cluster whose centroid is closest to the query. This type of strategy has been used by <ref> [22, 4, 29, 13] </ref> among others. In feedback selection, we modify the query with pseudo-feedback before selecting the cluster closest to it.
Reference: [23] <author> Gerard Salton and Chris Buckley. </author> <title> Improving retrieval performance by relevance feedback. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 41(4) </volume> <pages> 288-297, </pages> <year> 1990. </year>
Reference-contexts: As is common, we weight the vector d before truncation. (One reason weighting is common is that it has been shown to give improved results for similarity search <ref> [23] </ref>.) The weighting technique we use is term frequency weighting, in which we replace d t by 1 + log d t . We call this technique TF weighting to c terms, or TF-c.
Reference: [24] <author> Hinrich Schutze. </author> <title> Ambiguity Resolution in Language Learning. </title> <publisher> CSLI Publications, </publisher> <address> Stanford CA, </address> <year> 1997. </year>
Reference-contexts: For another application of clustering, word sense disambiguation, it has been shown that projection onto a smaller subspace does not affect performance <ref> [24] </ref>. This is further motivation for exploring projection for clustering in information retrieval. There are two different approaches to projecting documents. One is a local method, where for each document we excise a number of "unimportant" terms.
Reference: [25] <author> Hinrich Schutze and Jan O. Pedersen. </author> <title> A cooccurrence-based thesaurus and two applications to information retrieval. </title> <booktitle> Information Processing & Management, </booktitle> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: 1 Introduction Clustering is becoming increasingly widespread: It is finding applications in browsing [8, 7], in improving the performance of similarity search tools [16, 19], and in automatically generating thesauri [5, 6]. In query analysis, clustering has been used for transforming a free-text query into a fuzzy Boolean constraint <ref> [25] </ref>. The popularity of Yahoo! demonstrates the potential of categorization for presenting information on the World Wide Web. Clustering can only approximate a manual categorization like Yahoo!'s, but in many cases such an approximation is still beneficial while at the same time being cheap to install.
Reference: [26] <author> Amit Singhal, Gerard Salton, and Chris Buckley. </author> <title> Length normalization in degraded text collections. </title> <booktitle> In Fifth Annual Symposium on Document Analysis and Information Retrieval, </booktitle> <pages> pages 149-162, </pages> <address> Las Vegas NV, </address> <year> 1996. </year>
Reference-contexts: Local clustering, on the other hand, first chooses the 1000 documents that are closest to the query according to lnc.ltc weighted search. These documents are then clustered 5 We use lnc.ltc weighting, a form of term frequency and inverse document frequency weighting, for similarity search <ref> [26] </ref>.
Reference: [27] <author> Jean Tague-Sutcliffe and James Blustein. </author> <title> A statistical analysis of TREC-3 data. </title> <editor> In D.K. Harman, editor, </editor> <booktitle> The Third Text REtrieval Conference (TREC-3), </booktitle> <pages> pages 385-398, </pages> <address> Washington DC, </address> <year> 1995. </year> <type> U.S. </type> <institution> Department of Commerce. </institution>
Reference-contexts: Tukey's W procedure guarantees that, for a 95% significance level, the probability of error for any of the differences between levels is only 5% regardless of the number of levels under consideration. Tukey's W procedure is similar to the Scheffe test used in <ref> [27] </ref>. 4 Results and Discussion 4.1 Efficiency The experiments consisted of clustering the WSJ subcollection using each of the 6 projections for global clustering.
Reference: [28] <editor> C. J. van Rijsbergen. </editor> <booktitle> Information Retrieval. </booktitle> <address> Butter-worths, London, </address> <year> 1979. </year> <note> Second Edition. </note>
Reference-contexts: Cluster retrieval is a retrieval strategy based on the cluster hypothesis [18, 29, 16] which states that "closely associated documents tend to be relev ant to the same requests" <ref> [28] </ref>.
Reference: [29] <author> Ellen M. Voorhees. </author> <title> The cluster hypothesis revisited. </title> <booktitle> In Proceedings of SIGIR '85, </booktitle> <pages> pages 188-196, </pages> <year> 1985. </year>
Reference-contexts: Instead of using these rather abstract measures, we base our evaluation on cluster retrieval, a measure that is closely aligned with information retrieval performance. Cluster retrieval is a retrieval strategy based on the cluster hypothesis <ref> [18, 29, 16] </ref> which states that "closely associated documents tend to be relev ant to the same requests" [28]. <p> There are three levels for this factor. In the first, closest cluster selection, we select the cluster that is closest to the query, or more exactly, the cluster whose centroid is closest to the query. This type of strategy has been used by <ref> [22, 4, 29, 13] </ref> among others. In feedback selection, we modify the query with pseudo-feedback before selecting the cluster closest to it.
References-found: 29

