URL: http://www.cs.umn.edu/Users/dept/users/kumar/classpar-kdd-journal.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Title: Parallel Formulations of Decision-Tree Classification Algorithms  
Author: Anurag Srivastava Eui-Hong Han Vipin Kumar Vineet Singh 
Keyword: Data mining, parallel processing, classification, scalability, decision trees  
Address: Editor:  
Affiliation: 1 Digital Impact 2 Dept. of Computer Science Engineering, University of Minnesota 3 Information Technology Lab, Hitachi America, Ltd.  
Note: 1-24 c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Abstract: Classification decision tree algorithms are used extensively for data mining in many domains such as retail target marketing, fraud detection, etc. Highly parallel algorithms for constructing classification decision trees are desirable for dealing with large data sets in reasonable amount of time. Algorithms for building classification decision trees have a natural concurrency, but are difficult to parallelize due to the inherent dynamic nature of the computation. In this paper, we present parallel formulations of classification decision tree learning algorithm based on induction. We describe two basic parallel formulations. One is based on Synchronous Tree Construction Approach and the other is based on Partitioned Tree Construction Approach. We discuss the advantages and disadvantages of using these methods and propose a hybrid method that employs the good features of these methods. We also provide the analysis of the cost of computation and communication of the proposed hybrid method. Moreover, experimental results on an IBM SP-2 demonstrate excellent speedups and scalability. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Database mining: A performance perspective. </title> <journal> IEEE Transactions on Knowledge and Data Eng., </journal> <volume> 5(6) </volume> <pages> 914-925, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: We also provide the analysis of the cost of computation and communication of the proposed hybrid method. Moreover, experimental results on an IBM SP-2 demonstrate excellent speedups and scalability. 2. Related Work 2.1. Sequential Decision-Tree Classification Algorithms Most of the existing induction-based algorithms like C4.5 [20], CDP <ref> [1] </ref>, SLIQ [18], and SPRINT [21] use Hunt's method [20] as the basic algorithm. Here is a recursive description of Hunt's method for constructing a decision tree from a set T of training cases with classes denoted fC 1 ; C 2 ; : : : ; C k g.
Reference: 2. <author> K. Alsabti, S. Ranka, and V. Singh. </author> <title> A one-pass algorithm for accurately estimating quantiles for disk-resident data. </title> <booktitle> In Proc. of the 23rd VLDB Conference, </booktitle> <year> 1997. </year>
Reference-contexts: In this case, the parallel formulations as presented in the previous subsections are directly applicable without any modification. Another approach towards discretization is to discretize at every node in the tree. There are two examples of this approach. The first example can be found in [3] where quantiles <ref> [2] </ref> are used to discretize continuous attributes. The second example of this approach to discretize at each node is SPEC [23] where a clustering technique is used.
Reference: 3. <author> K. Alsabti, S. Ranka, and V. Singh. </author> <title> CLOUDS: Classification for large or out-of-core datasets. </title> <note> http://www.cise.ufl.edu/ranka/dm.html, 1998. </note>
Reference-contexts: In this case, the parallel formulations as presented in the previous subsections are directly applicable without any modification. Another approach towards discretization is to discretize at every node in the tree. There are two examples of this approach. The first example can be found in <ref> [3] </ref> where quantiles [2] are used to discretize continuous attributes. The second example of this approach to discretize at each node is SPEC [23] where a clustering technique is used.
Reference: 4. <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, Monterrey, </publisher> <address> CA, </address> <year> 1984. </year>
Reference-contexts: Table 3 shows the class distribution information of data attribute Humidity. Once the class distribution information of all the attributes are gathered, each attribute is evaluated in terms of either entropy [20] or Gini Index <ref> [4] </ref>. The best attribute is selected as a test for the node expansion. 5 The C4.5 algorithm generates a classification-decision tree for the given training data set by recursively partitioning the data. The decision tree is grown using depth-first strategy.
Reference: 5. <author> J. Catlett. </author> <title> Megainduction: Machine Learning on Very Large Databases. </title> <type> PhD thesis. </type> <institution> University of Sydney, </institution> <year> 1991. </year>
Reference-contexts: One way to reduce the computational complexity of building a decision tree classifier using large training datasets is to use only a small sample of the 2 training data. Such methods do not yield the same classification accuracy as a decision tree classifier that uses the entire data set <ref> [24, 5, 6, 7] </ref>. In order to get reasonable accuracy in a reasonable amount of time, parallel algorithms may be required. Classification decision tree construction algorithms have natural concurrency, as once a node is generated, all of its children in the classification tree can be generated concurrently.
Reference: 6. <author> Philip K. Chan and Salvatore J. Stolfo. </author> <title> Experiments on multistrategy learning by met-alearning. </title> <booktitle> In Proc. Second Intl. Conference on Info. and Knowledge Mgmt., </booktitle> <pages> pages 314-323, </pages> <year> 1993. </year>
Reference-contexts: One way to reduce the computational complexity of building a decision tree classifier using large training datasets is to use only a small sample of the 2 training data. Such methods do not yield the same classification accuracy as a decision tree classifier that uses the entire data set <ref> [24, 5, 6, 7] </ref>. In order to get reasonable accuracy in a reasonable amount of time, parallel algorithms may be required. Classification decision tree construction algorithms have natural concurrency, as once a node is generated, all of its children in the classification tree can be generated concurrently.
Reference: 7. <author> Philip K. Chan and Salvatore J. Stolfo. </author> <title> Metalearning for multistrategy learning and parallel learning. </title> <booktitle> In Proc. Second Intl. Conference on Multistrategy Learning, </booktitle> <pages> pages 150-165, </pages> <year> 1993. </year>
Reference-contexts: One way to reduce the computational complexity of building a decision tree classifier using large training datasets is to use only a small sample of the 2 training data. Such methods do not yield the same classification accuracy as a decision tree classifier that uses the entire data set <ref> [24, 5, 6, 7] </ref>. In order to get reasonable accuracy in a reasonable amount of time, parallel algorithms may be required. Classification decision tree construction algorithms have natural concurrency, as once a node is generated, all of its children in the classification tree can be generated concurrently.
Reference: 8. <author> J. Chattratichat, J. Darlington, M. Ghanem, Y. Guo, H. Huning, M. Kohler, J. Sutiwara-phun, H.W. To, and D. Yang. </author> <title> Large scale data mining: Challenges and responses. </title> <booktitle> In Proc. of the Third Int'l Conference on Knowledge Discovery and Data Mining, </booktitle> <year> 1997. </year>
Reference-contexts: This approach is related in nature to the partitioned tree construction approach discussed in this paper. In the partitioned tree construction approach, actual data samples are partitioned (horizontal partitioning) whereas in this approach attributes are partitioned (vertical partitioning). 6 In <ref> [8] </ref>, a few general approaches for parallelizing C4.5 are discussed. In the Dynamic Task Distribution (DTD) scheme, a master processor allocates a subtree of the decision tree to an idle slave processor. This scheme does not require communication among processors, but suffers from the load imbalance. <p> The DP-att scheme distributes the attributes. This scheme has the advantages of being both load-balanced and requiring minimal communications. However, this scheme does not scale well with increasing number of processors. The results in <ref> [8] </ref> show that the effectiveness of different parallelization schemes varies significantly with data sets being used. Kufrin proposed an approach called Parallel Decision Trees (PDT) in [15]. This approach is similar to the DP-rec scheme [8] and synchronous tree construction approach discussed in this paper, as the data sets are partitioned <p> The results in <ref> [8] </ref> show that the effectiveness of different parallelization schemes varies significantly with data sets being used. Kufrin proposed an approach called Parallel Decision Trees (PDT) in [15]. This approach is similar to the DP-rec scheme [8] and synchronous tree construction approach discussed in this paper, as the data sets are partitioned among processors. The PDT approach designate one processor as the "host" processor and the remaining processors as "worker" processors.
Reference: 9. <editor> D.J. Spiegelhalter D. Michie and C.C. Taylor. </editor> <title> Machine Learning, Neural and Statistical Classification. </title> <publisher> Ellis Horwood, </publisher> <year> 1994. </year>
Reference-contexts: Application domains include retail target marketing, fraud detection, and design of telecommunication service plans. Several classification models like neural networks [17], genetic algorithms [11], and decision trees [20] have been proposed. Decision trees are probably the most popular since they obtain reasonable accuracy <ref> [9] </ref> and they are relatively inexpensive to compute. Most current classification algorithms such as C4.5 [20], and SLIQ [18] are based on the ID3 classification decision tree algorithm [20]. In the data mining domain, the data to be processed tends to be very large.
Reference: 10. <author> S. Goil, S. Aluru, and S. Ranka. </author> <title> Concatenated parallelism: A technique for efficient parallel divide and conquer. </title> <booktitle> In Proc. of the Symposium of Parallel and Distributed Computing (SPDP'96), </booktitle> <year> 1996. </year>
Reference-contexts: Goil, Aluru, and Ranka proposed the Concatenated Parallelism strategy for efficient parallel solution of divide and conquer problems <ref> [10] </ref>. In this strategy, the mix of data parallelism and task parallelism is used as a solution to the parallel divide and conquer algorithm. Data parallelism is used until there are enough subtasks are genearted, and then task parallelism is used, i.e., each processor works on independent subtasks.
Reference: 11. <author> D. E. Goldberg. </author> <title> Genetic Algorithms in Search, Optimizations and Machine Learning. </title> <address> Morgan-Kaufman, </address> <year> 1989. </year>
Reference-contexts: Application domains include retail target marketing, fraud detection, and design of telecommunication service plans. Several classification models like neural networks [17], genetic algorithms <ref> [11] </ref>, and decision trees [20] have been proposed. Decision trees are probably the most popular since they obtain reasonable accuracy [9] and they are relatively inexpensive to compute. Most current classification algorithms such as C4.5 [20], and SLIQ [18] are based on the ID3 classification decision tree algorithm [20].
Reference: 12. <author> S.J. Hong. </author> <title> Use of contextual information for feature ranking and discretization. </title> <journal> IEEE Transactions on Knowledge and Data Eng., </journal> <volume> 9(5) </volume> <pages> 718-730, </pages> <month> September/October </month> <year> 1997. </year>
Reference-contexts: Once again, communication in these formulations [21, 13] can be reduced using the hybrid scheme of Section 3.3. Another completely different way of handling continuous attributes is to discretize them once as a preprocessing step <ref> [12] </ref>. In this case, the parallel formulations as presented in the previous subsections are directly applicable without any modification. Another approach towards discretization is to discretize at every node in the tree. There are two examples of this approach.
Reference: 13. <author> M.V. Joshi, G. Karypis, and V. Kumar. ScalParC: </author> <title> A new scalable and efficient parallel classification algorithm for mining large datasets. </title> <booktitle> In Proc. of the International Parallel Processing Symposium, </booktitle> <year> 1998. </year>
Reference-contexts: The parallel implementation of SPRINT [21] and ScalParC <ref> [13] </ref> use methods for partitioning work that is identical to the one used in the synchronous tree construction approach discussed in this paper. Serial SPRINT [21] sorts the continuous attributes only once in the beginning and keeps a separate attribute list with record identifiers. <p> The reason is that each processor requires O (N ) memory to store the hash table and O (N ) communication overhead for all-to-all broadcast, where N is the number of records in the data set. The recently proposed ScalParC <ref> [13] </ref> improves upon the SPRINT by employing a dis 7 tributed hash table to efficiently implement the splitting phase of the SPRINT. <p> A more efficient way of handling continuous attributes without incurring the high cost of repeated sorting is to use the pre-sorting technique used in algorithms SLIQ [18], SPRINT [21], and ScalParC <ref> [13] </ref>. These algorithms require only one presorting step, but need to construct a hash table at each level of the classification tree. In the parallel formulations of these algorithms, the content of this hash table needs to be available globally, requiring communication among processors. 14 Table 4. <p> In the parallel formulations of these algorithms, the content of this hash table needs to be available globally, requiring communication among processors. 14 Table 4. Symbols used in the analysis. Existing parallel formulations of these schemes <ref> [21, 13] </ref> perform communication that is similar in nature to that of our synchronous tree construction approach discussed in Section 3.1. Once again, communication in these formulations [21, 13] can be reduced using the hybrid scheme of Section 3.3. <p> Symbols used in the analysis. Existing parallel formulations of these schemes <ref> [21, 13] </ref> perform communication that is similar in nature to that of our synchronous tree construction approach discussed in Section 3.1. Once again, communication in these formulations [21, 13] can be reduced using the hybrid scheme of Section 3.3. Another completely different way of handling continuous attributes is to discretize them once as a preprocessing step [12]. In this case, the parallel formulations as presented in the previous subsections are directly applicable without any modification.
Reference: 14. <author> George Karypis and Vipin Kumar. </author> <title> Unstructured tree search on simd parallel computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22(3) </volume> <pages> 379-391, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: Without assuming anything about network congestion, load balancing phase can be done using transportation primitive [22] in time 2fl N P flt w time provided N Splitting is done when the accumulated cost of communication becomes equal to the cost of moving records around in the splitting phase <ref> [14] </ref>. So splitting is done when: X (Communication Cost) Moving Cost + Load Balancing This criterion for splitting ensures that the communication cost for this scheme will be within twice the communication cost for an optimal scheme [14]. The splitting is recursive and is applied as many times as required. <p> becomes equal to the cost of moving records around in the splitting phase <ref> [14] </ref>. So splitting is done when: X (Communication Cost) Moving Cost + Load Balancing This criterion for splitting ensures that the communication cost for this scheme will be within twice the communication cost for an optimal scheme [14]. The splitting is recursive and is applied as many times as required. Once splitting is done, the above computations are applied to each partition. When a partition of processors starts to idle, then it sends a request to a busy partition about its idle state.
Reference: 15. <author> R. Kufrin. </author> <title> Decision trees on parallel processors. </title> <editor> In J. Geller, H. Kitano, and C.B. Suttner, editors, </editor> <booktitle> Parallel Processing for Artificial Intelligence 3. </booktitle> <publisher> Elsevier Science, </publisher> <year> 1997. </year>
Reference-contexts: However, this scheme does not scale well with increasing number of processors. The results in [8] show that the effectiveness of different parallelization schemes varies significantly with data sets being used. Kufrin proposed an approach called Parallel Decision Trees (PDT) in <ref> [15] </ref>. This approach is similar to the DP-rec scheme [8] and synchronous tree construction approach discussed in this paper, as the data sets are partitioned among processors. The PDT approach designate one processor as the "host" processor and the remaining processors as "worker" processors.
Reference: 16. <author> Vipin Kumar, Ananth Grama, Anshul Gupta, and George Karypis. </author> <title> Introduction to Parallel Computing: Algorithm Design and Analysis. </title> <publisher> Benjamin Cummings/ Addison Wesley, </publisher> <address> Redwod City, </address> <year> 1994. </year>
Reference-contexts: However, in order to split the attribute lists, the full size hash table is required on all the processors. In order to construct the hash table, all-to-all broadcast <ref> [16] </ref> is performed, that makes this algorithm unscalable with respect to runtime and memory requirements. <p> Depth-First or Breadth-First), and call that node as the current node. At the beginning, root node is selected as the current node. 2. For each data attribute, collect class distribution information of the local data at the current node. 3. Exchange the local class distribution information using global reduction <ref> [16] </ref> among processors. 4. Simultaneously compute the entropy gains of each attribute at each processor and select the best attribute for child node expansion. 5. <p> Here we give a detailed analysis for the case when only discrete attributes are present. The analysis for the case with continuous attributes can be found in [23]. The detailed study of the communication patterns used in this analysis can be found in <ref> [16] </ref>. Table 4 describes the symbols used in this section. 15 4.1. Assumptions * The processors are connected in a hypercube topology. Complexity measures for other topologies can be easily derived by using the communication complexity expressions for other topologies given in [16]. * The expression for communication and computation are <p> used in this analysis can be found in <ref> [16] </ref>. Table 4 describes the symbols used in this section. 15 4.1. Assumptions * The processors are connected in a hypercube topology. Complexity measures for other topologies can be easily derived by using the communication complexity expressions for other topologies given in [16]. * The expression for communication and computation are written for a full binary tree with 2 L leaves at depth L. <p> Scalability Analysis Isoefficiency metric has been found to be a very useful metric of scalability for a large number of problems on a large class of commercial parallel computers <ref> [16] </ref>. It is defined as follows. Let P be the number of processors and W the problem size (in total time taken for the best sequential algorithm). <p> Notes 1. If the message size is large, by routing message in parts, this communication step can be done in time : (t s + t w fl MesgSize) fl k 0 for a small constant k 0 . Refer to <ref> [16] </ref> section 3.7 for details.
Reference: 17. <author> R. Lippmann. </author> <title> An introduction to computing with neural nets. </title> <journal> IEEE ASSP Magazine, </journal> <volume> 4(22), </volume> <month> April </month> <year> 1987. </year>
Reference-contexts: Application domains include retail target marketing, fraud detection, and design of telecommunication service plans. Several classification models like neural networks <ref> [17] </ref>, genetic algorithms [11], and decision trees [20] have been proposed. Decision trees are probably the most popular since they obtain reasonable accuracy [9] and they are relatively inexpensive to compute.
Reference: 18. <author> M. Mehta, R. Agrawal, and J. Rissanen. SLIQ: </author> <title> A fast scalable classifier for data mining. </title> <booktitle> In Proc. of the Fifth Int'l Conference on Extending Database Technology, </booktitle> <address> Avignon, France, </address> <year> 1996. </year> <month> 24 </month>
Reference-contexts: Several classification models like neural networks [17], genetic algorithms [11], and decision trees [20] have been proposed. Decision trees are probably the most popular since they obtain reasonable accuracy [9] and they are relatively inexpensive to compute. Most current classification algorithms such as C4.5 [20], and SLIQ <ref> [18] </ref> are based on the ID3 classification decision tree algorithm [20]. In the data mining domain, the data to be processed tends to be very large. Hence, it is highly desirable to design computationally efficient as well as scalable algorithms. <p> We also provide the analysis of the cost of computation and communication of the proposed hybrid method. Moreover, experimental results on an IBM SP-2 demonstrate excellent speedups and scalability. 2. Related Work 2.1. Sequential Decision-Tree Classification Algorithms Most of the existing induction-based algorithms like C4.5 [20], CDP [1], SLIQ <ref> [18] </ref>, and SPRINT [21] use Hunt's method [20] as the basic algorithm. Here is a recursive description of Hunt's method for constructing a decision tree from a set T of training cases with classes denoted fC 1 ; C 2 ; : : : ; C k g. <p> This process is repeated for each continuous attribute. Recently proposed classification algorithms SLIQ <ref> [18] </ref> and SPRINT [21] avoid costly sorting at each node by pre-sorting continuous attributes once in the beginning. In SPRINT, each continuous attribute is maintained in a sorted attribute list. In this list, each entry contains a value of the attribute and its corresponding record id. <p> Hence even in this case, it will be useful to use a scheme similar to the hybrid approach discussed in Section 3.3. A more efficient way of handling continuous attributes without incurring the high cost of repeated sorting is to use the pre-sorting technique used in algorithms SLIQ <ref> [18] </ref>, SPRINT [21], and ScalParC [13]. These algorithms require only one presorting step, but need to construct a hash table at each level of the classification tree. <p> Experimental Results We have implemented the three parallel formulations using the MPI programming library. We use binary splitting at each decision tree node and grow the tree in breadth first manner. For generating large datasets, we have used the widely used synthetic dataset proposed in the SLIQ paper <ref> [18] </ref> for all our experiments. Ten classification functions were also proposed in [18] for these datasets. We have used the function 2 dataset for our algorithms. In this dataset, there are two class labels and each record consists of 9 attributes having 3 categoric and 6 continuous attributes. <p> We use binary splitting at each decision tree node and grow the tree in breadth first manner. For generating large datasets, we have used the widely used synthetic dataset proposed in the SLIQ paper <ref> [18] </ref> for all our experiments. Ten classification functions were also proposed in [18] for these datasets. We have used the function 2 dataset for our algorithms. In this dataset, there are two class labels and each record consists of 9 attributes having 3 categoric and 6 continuous attributes.
Reference: 19. <author> R.A. Pearson. </author> <title> A coarse grained parallel induction heuristic. </title> <editor> In H. Kitano, V. Kumar, and C.B. Suttner, editors, </editor> <booktitle> Parallel Processing for Artificial Intelligence 2, </booktitle> <pages> pages 207-226. </pages> <publisher> Elsevier Science, </publisher> <year> 1994. </year>
Reference-contexts: Therefore, in this paper, we focus on the initial tree generation only and not on the pruning part of the computation. 2.2. Parallel Decision-Tree Classification Algorithms Several parallel formulations of classification rule learning have been proposed recently. Pearson presented an approach that combines node-based decomposition and attribute-based decomposition <ref> [19] </ref>. It is shown that the node-based decomposition (task parallelism) alone has several probelms. One problem is that only a few processors are utilized in the beginning due to the small number of expanded tree nodes.
Reference: 20. <author> J. Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: Application domains include retail target marketing, fraud detection, and design of telecommunication service plans. Several classification models like neural networks [17], genetic algorithms [11], and decision trees <ref> [20] </ref> have been proposed. Decision trees are probably the most popular since they obtain reasonable accuracy [9] and they are relatively inexpensive to compute. Most current classification algorithms such as C4.5 [20], and SLIQ [18] are based on the ID3 classification decision tree algorithm [20]. <p> Several classification models like neural networks [17], genetic algorithms [11], and decision trees <ref> [20] </ref> have been proposed. Decision trees are probably the most popular since they obtain reasonable accuracy [9] and they are relatively inexpensive to compute. Most current classification algorithms such as C4.5 [20], and SLIQ [18] are based on the ID3 classification decision tree algorithm [20]. In the data mining domain, the data to be processed tends to be very large. Hence, it is highly desirable to design computationally efficient as well as scalable algorithms. <p> genetic algorithms [11], and decision trees <ref> [20] </ref> have been proposed. Decision trees are probably the most popular since they obtain reasonable accuracy [9] and they are relatively inexpensive to compute. Most current classification algorithms such as C4.5 [20], and SLIQ [18] are based on the ID3 classification decision tree algorithm [20]. In the data mining domain, the data to be processed tends to be very large. Hence, it is highly desirable to design computationally efficient as well as scalable algorithms. <p> We also provide the analysis of the cost of computation and communication of the proposed hybrid method. Moreover, experimental results on an IBM SP-2 demonstrate excellent speedups and scalability. 2. Related Work 2.1. Sequential Decision-Tree Classification Algorithms Most of the existing induction-based algorithms like C4.5 <ref> [20] </ref>, CDP [1], SLIQ [18], and SPRINT [21] use Hunt's method [20] as the basic algorithm. <p> Moreover, experimental results on an IBM SP-2 demonstrate excellent speedups and scalability. 2. Related Work 2.1. Sequential Decision-Tree Classification Algorithms Most of the existing induction-based algorithms like C4.5 <ref> [20] </ref>, CDP [1], SLIQ [18], and SPRINT [21] use Hunt's method [20] as the basic algorithm. Here is a recursive description of Hunt's method for constructing a decision tree from a set T of training cases with classes denoted fC 1 ; C 2 ; : : : ; C k g. <p> For a continuous attribute, binary tests involving all the distinct values of the attribute are considered. Table 3 shows the class distribution information of data attribute Humidity. Once the class distribution information of all the attributes are gathered, each attribute is evaluated in terms of either entropy <ref> [20] </ref> or Gini Index [4]. The best attribute is selected as a test for the node expansion. 5 The C4.5 algorithm generates a classification-decision tree for the given training data set by recursively partitioning the data. The decision tree is grown using depth-first strategy.
Reference: 21. <author> J. Shafer, R. Agrawal, and M. Mehta. SPRINT: </author> <title> A scalable parallel classifier for data mining. </title> <booktitle> In Proc. of the 22nd VLDB Conference, </booktitle> <year> 1996. </year>
Reference-contexts: Moreover, experimental results on an IBM SP-2 demonstrate excellent speedups and scalability. 2. Related Work 2.1. Sequential Decision-Tree Classification Algorithms Most of the existing induction-based algorithms like C4.5 [20], CDP [1], SLIQ [18], and SPRINT <ref> [21] </ref> use Hunt's method [20] as the basic algorithm. Here is a recursive description of Hunt's method for constructing a decision tree from a set T of training cases with classes denoted fC 1 ; C 2 ; : : : ; C k g. <p> This process is repeated for each continuous attribute. Recently proposed classification algorithms SLIQ [18] and SPRINT <ref> [21] </ref> avoid costly sorting at each node by pre-sorting continuous attributes once in the beginning. In SPRINT, each continuous attribute is maintained in a sorted attribute list. In this list, each entry contains a value of the attribute and its corresponding record id. <p> The PDT approach has an additional communication bottleneck, as every worker processor sends the collected statistics to the host processor at the roughly same time and the host processor sends out the split decision to all working processors at the same time. The parallel implementation of SPRINT <ref> [21] </ref> and ScalParC [13] use methods for partitioning work that is identical to the one used in the synchronous tree construction approach discussed in this paper. Serial SPRINT [21] sorts the continuous attributes only once in the beginning and keeps a separate attribute list with record identifiers. <p> The parallel implementation of SPRINT <ref> [21] </ref> and ScalParC [13] use methods for partitioning work that is identical to the one used in the synchronous tree construction approach discussed in this paper. Serial SPRINT [21] sorts the continuous attributes only once in the beginning and keeps a separate attribute list with record identifiers. The splitting phase of a decision tree node maintains this sorted order without requiring to sort the records again. <p> A more efficient way of handling continuous attributes without incurring the high cost of repeated sorting is to use the pre-sorting technique used in algorithms SLIQ [18], SPRINT <ref> [21] </ref>, and ScalParC [13]. These algorithms require only one presorting step, but need to construct a hash table at each level of the classification tree. In the parallel formulations of these algorithms, the content of this hash table needs to be available globally, requiring communication among processors. 14 Table 4. <p> In the parallel formulations of these algorithms, the content of this hash table needs to be available globally, requiring communication among processors. 14 Table 4. Symbols used in the analysis. Existing parallel formulations of these schemes <ref> [21, 13] </ref> perform communication that is similar in nature to that of our synchronous tree construction approach discussed in Section 3.1. Once again, communication in these formulations [21, 13] can be reduced using the hybrid scheme of Section 3.3. <p> Symbols used in the analysis. Existing parallel formulations of these schemes <ref> [21, 13] </ref> perform communication that is similar in nature to that of our synchronous tree construction approach discussed in Section 3.1. Once again, communication in these formulations [21, 13] can be reduced using the hybrid scheme of Section 3.3. Another completely different way of handling continuous attributes is to discretize them once as a preprocessing step [12]. In this case, the parallel formulations as presented in the previous subsections are directly applicable without any modification. <p> We have used the function 2 dataset for our algorithms. In this dataset, there are two class labels and each record consists of 9 attributes having 3 categoric and 6 continuous attributes. The same dataset was also used by the SPRINT algorithm <ref> [21] </ref> for evaluating its performance. Experiments were done on an IBM SP2. The results for comparing speedup of the three parallel formulations are reported for parallel runs on 1, 2, 4, 8, and 16 processors. More experiments for the hybrid approach are reported for up to 128 processors.
Reference: 22. <author> R. Shankar, K. Alsabti, and S. Ranka. </author> <title> Many-to-many communication with bounded traffic. </title> <booktitle> In Frontiers '95, the fifth symposium on advances in massively parallel computation, </booktitle> <address> McLean, VA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: Also, the cost for load balancing assumes that there is no network congestion. This is a reasonable assumption for networks that are bandwidth-rich as is the case with most commercial systems. Without assuming anything about network congestion, load balancing phase can be done using transportation primitive <ref> [22] </ref> in time 2fl N P flt w time provided N Splitting is done when the accumulated cost of communication becomes equal to the cost of moving records around in the splitting phase [14].
Reference: 23. <author> Anurag Srivastava, Vineet Singh, Eui-Hong Han, and Vipin Kumar. </author> <title> An efficient, scalable, parallel classifier for data mining. </title> <type> Technical Report TR-97-010,http://www.cs.umn.edu/kumar, </type> <institution> Department of Computer Science, University of Min-nesota, Minneapolis, </institution> <year> 1997. </year>
Reference-contexts: There are two examples of this approach. The first example can be found in [3] where quantiles [2] are used to discretize continuous attributes. The second example of this approach to discretize at each node is SPEC <ref> [23] </ref> where a clustering technique is used. SPEC has been shown to be very efficient in terms of runtime and has also been shown to perform essentially identical to several other widely used tree classifiers in terms of classification accuracy [23]. <p> of this approach to discretize at each node is SPEC <ref> [23] </ref> where a clustering technique is used. SPEC has been shown to be very efficient in terms of runtime and has also been shown to perform essentially identical to several other widely used tree classifiers in terms of classification accuracy [23]. Parallelization of the discretization at every node of the tree is similar in nature to the parallelization of the computation of entropy gain for discrete attributes, because both of these methods of discretization require some global communication among all the processors that are responsible for a node. <p> In particular, parallel formulations of the clustering step in SPEC is essentially identical to the parallel formulations for the discrete case discussed in the previous subsections <ref> [23] </ref>. 4. Analysis of the Hybrid Algorithm In this section, we provide the analysis of the hybrid algorithm proposed in Section 3.3. Here we give a detailed analysis for the case when only discrete attributes are present. The analysis for the case with continuous attributes can be found in [23]. <p> subsections <ref> [23] </ref>. 4. Analysis of the Hybrid Algorithm In this section, we provide the analysis of the hybrid algorithm proposed in Section 3.3. Here we give a detailed analysis for the case when only discrete attributes are present. The analysis for the case with continuous attributes can be found in [23]. The detailed study of the communication patterns used in this analysis can be found in [16]. Table 4 describes the symbols used in this section. 15 4.1. Assumptions * The processors are connected in a hypercube topology. <p> Each processor can send or receive a maximum of N P training data items. Assuming no congestion in the interconnection network, cost for load balancing is: Cost for load balancing phase 2 fl N fl t w (4) A detailed derivation of Equation 4 above is given in <ref> [23] </ref>. Also, the cost for load balancing assumes that there is no network congestion. This is a reasonable assumption for networks that are bandwidth-rich as is the case with most commercial systems. <p> For these experiments, we used the original data set with continuous attributes and used a clustering technique to discretize continuous attributes at each decision tree node <ref> [23] </ref>. Note that the parallel formulation gives almost identical performance as the serial algorithm in terms of accuracy and classification tree size [23]. The results in Figure 8 show the speedup of the hybrid approach. The results confirm that the hybrid approach is indeed very effective. <p> For these experiments, we used the original data set with continuous attributes and used a clustering technique to discretize continuous attributes at each decision tree node <ref> [23] </ref>. Note that the parallel formulation gives almost identical performance as the serial algorithm in terms of accuracy and classification tree size [23]. The results in Figure 8 show the speedup of the hybrid approach. The results confirm that the hybrid approach is indeed very effective. To study the scaleup behavior, we kept the dataset size at each processor constant at 50,000 examples and increased the number of processors.
Reference: 24. <author> J. Wirth and J. Catlett. </author> <title> Experiments on the costs and benefits of windowing in ID3. </title> <booktitle> In 5th Int'l Conference on Machine learning, </booktitle> <year> 1988. </year>
Reference-contexts: One way to reduce the computational complexity of building a decision tree classifier using large training datasets is to use only a small sample of the 2 training data. Such methods do not yield the same classification accuracy as a decision tree classifier that uses the entire data set <ref> [24, 5, 6, 7] </ref>. In order to get reasonable accuracy in a reasonable amount of time, parallel algorithms may be required. Classification decision tree construction algorithms have natural concurrency, as once a node is generated, all of its children in the classification tree can be generated concurrently.
References-found: 24

