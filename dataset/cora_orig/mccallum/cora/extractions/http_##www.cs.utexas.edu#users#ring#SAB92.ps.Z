URL: http://www.cs.utexas.edu/users/ring/SAB92.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/ring/
Root-URL: 
Email: (ring@cs.utexas.edu)  
Title: Two Methods for Hierarchy Learning in Reinforcement Environments  
Author: Mark Ring 
Address: Austin, TX 78712  
Affiliation: Department of Computer Sciences University of Texas at Austin  
Note: In Meyer, J.-A., Roitblat, H., and Wilson, S., editors, From Animals to Animats 2: Proceedings of the Second International Conference on Simulation of Adaptive Behavior, pages 148-155. MIT Press.  
Abstract: This paper describes two methods for hierarchically organizing temporal behaviors. The first is more intuitive: grouping together common sequences of events into single units so that they may be treated as individual behaviors. This system immediately encounters problems, however, because the units are binary, meaning the behaviors must execute completely or not at all, and this hinders the construction of good training algorithms. The system also runs into difficulty when more than one unit is (or should be) active at the same time. The second system is a hierarchy of transition values. This hierarchy dynamically modifies the values that specify the degree to which one unit should follow another. These values are continuous, allowing the use of gradient descent during learning. Furthermore, many units are active at the same time as part of the system's normal functionings.
Abstract-found: 1
Intro-found: 1
Reference: <author> Albus, J. S. </author> <year> (1979). </year> <title> Mechanisms of planning and problem solving in the brain. </title> <journal> Mathematical Biosciences, </journal> <volume> 45 </volume> <pages> 247-293. </pages>
Reference: <author> Bachrach, J. R. </author> <year> (1992). </year> <title> Connectionist Modeling and Control of Finite State Environments. </title> <type> PhD thesis, </type> <institution> Department of Computer and Information Sciences, University of Massachusetts. </institution>
Reference: <author> Barto, A. G., Bradtke, S. J., and Singh, S. P. </author> <year> (1991). </year> <title> Real-time learning and control using asynchronous dynamic programming. </title> <type> Technical Report 91-57, </type> <institution> Computer Science Department, University of Massachusetts at Amherst. </institution>
Reference: <author> Barto, A. G., Sutton, R. S., and Anderson., C. W. </author> <year> (1983). </year> <title> Neuron-like elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 13 </volume> <pages> 835-846. </pages>
Reference-contexts: The w's are therefore a trace of weight changes that decay exponentially|like the eligibility trace presented in <ref> (Barto et al., 1983) </ref>. The trace constantly accrues weight changes over time, biased toward the most recent ones, but the changes are only applied to the weights when a reward is received. 2.1 A Different Approach is Needed There are problems with this approach.
Reference: <author> Barto, A. G., Sutton, R. S., and Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning and sequential decision making. </title> <type> Technical Report COINS Technical Report 89-95, </type> <institution> University of Mas-sachusetts at Amherst, Department of Computer and Information Science. </institution>
Reference: <author> Cleeremans, A., Servan-Schreiber, D., and McClelland, J. L. </author> <year> (1989). </year> <title> Finite state automata and simple recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 1(3) </volume> <pages> 372-381. </pages>
Reference: <author> Dawkins, R. </author> <year> (1976). </year> <title> Hierarchical organisation: a candidate principle for ethology. </title> <editor> In Bateson, P. P. G. and Hinde, R. A., editors, </editor> <booktitle> Growing Points in Ethology, </booktitle> <pages> pages 7-54. </pages> <address> Cambridge: </address> <publisher> Cambridge University Press. </publisher>
Reference-contexts: In this paper, I will focus on the latter class. Specifically, I will explore two methods for learning behavior hierarchies, where a behavior is a sequence of perceptions and/or actions. The motivation for hierarchy learning is one of efficiency and increased learning potential. As Dawkins pointed out <ref> (Dawkins, 1976, p. 16) </ref>, complex organisms probably evolved from simpler organisms that embodied many of the sub-assemblies required to build the more complex organisms. <p> Neural networks, on the other hand, are typically renowned for their ability to forget when learning something new. Dawkins <ref> (Dawkins, 1976) </ref> presented a method of hierarchically reducing the description of symbol sequences without loss of information.
Reference: <author> Drescher, G. L. </author> <year> (1991). </year> <title> Made-Up Minds: A Constructivist Approach to Artificial Intelligence. </title> <address> Cambridge, Mas-sachusetts: </address> <publisher> MIT Press. </publisher>
Reference: <author> Fahlman, S. E. </author> <year> (1991). </year> <title> The recurrent cascade-correlation architecture. </title> <editor> In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 190-196. </pages> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: The Recurrent Cascade Correlation (RCC) algorithm however is not so limited. This algorithm also learns sequences by adding new units, and it has been shown capable of incremental learning <ref> (Fahlman, 1991) </ref>. It does not build explicit behavior hierarchies, however, and it cannot be used for reinforcement learning because of the intricacies of its training algorithm.
Reference: <author> Giles, C. L., Miller, C. B., Chen, D., Sun, G. Z., Chen, H. H., and Lee, Y. C. </author> <year> (1992). </year> <title> Extracting and learning an unknown grammar with recurrent neural networks. </title> <editor> In Moody, J. E., Hanson, S. J., and Lippman, R. P., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 317-324. </pages> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Mozer, M. C. </author> <year> (1992). </year> <title> Induction of multiscale temporal structure. </title> <editor> In Moody, J. E., Hanson, S. J., and Lippmann, R. P., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 275-282. </pages> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: For example, tasks of the form given in Table 1 have been given to the network to compare against the recurrent network of Mozer <ref> (Mozer, 1992) </ref>. These two sequences formed a single traning set given to the network. The network was given as sensory input each element in the sequence one at a time until the end of the sequence was reached.
Reference: <author> Pollack, J. B. </author> <year> (1991). </year> <title> The induction of dynamical recognizers. </title> <journal> Machine Learning, </journal> <volume> 7 </volume> <pages> 227-252. </pages>
Reference: <author> Ring, M. B. </author> <year> (1991). </year> <title> Incremental development of complex behaviors through automatic construction of sensory-motor hierarchies. </title> <editor> In Birnbaum, L. A. and Collins, G. C., editors, </editor> <booktitle> Machine Learning: Proceedings of the Eighth International Workshop (ML91), </booktitle> <pages> pages 343-347. </pages> <publisher> Morgan Kaufmann Publishers. 7 Ring, </publisher> <editor> M. B. </editor> <year> (1993). </year> <title> Learning sequential tasks by incremen-tally adding higher orders. </title> <editor> In Giles, C. L., Hanson, S. J., and Cowan, J. D., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 115-122. </pages> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: I can however give a general overview and describe the intended behavior of the system from a high-level perspective, depicting the units as something akin to macro-operators. The details of the implementation are somewhat cumbersome, but are given in slightly more detail in a previous paper <ref> (Ring, 1991) </ref>. completed votes for a successor with its weights: 2 a i (t + 1) = j where o j (t), the output of unit j at time t, is 1:0 if the behavior represented by unit j completed at time t, and is 0:0 otherwise; w ij is the
Reference: <editor> Roitblat, H. L. </editor> <year> (1991). </year> <title> Cognitive action theory as a control architecture. </title> <editor> In Meyer, J. A. and Wilson, S. W., editors, </editor> <booktitle> From Animals to Animats: Proceedings of the First International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 444-450. </pages> <publisher> MIT Press. </publisher>
Reference: <author> Schmidhuber, J. </author> <year> (1992). </year> <title> Learning unambiguous reduced sequence descriptions. </title> <editor> In Moody, J. E., Hanson, S. J., and Lippman, R. P., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 291-298. </pages> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Singh, S. P. </author> <year> (1992). </year> <title> Transfer of learning by composing solutions of elemental sequential tasks. </title> <journal> Machine Learning, </journal> <volume> 8(3/4). </volume>
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44. </pages>
Reference-contexts: These methods of reinforcement for both Method I and Method II are really quite old-fashioned. There are many newer and better approaches based on temporal difference (TD) methods <ref> (Sutton, 1988) </ref> and dynamic programming, for example (Barto et al., 1983; Barto et al., 1989; Sutton, 1990; Barto et al., 1991), that are much more efficient.
Reference: <author> Sutton, R. S. </author> <year> (1990). </year> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <editor> In Porter, B. W. and Mooney, R. J., editors, </editor> <booktitle> Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 216-224. </pages> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Watrous, R. L. and Kuhn, G. M. </author> <year> (1992). </year> <title> Induction of finite-state languages using second-order recurrent networks. </title> <editor> In Moody, J. E., Hanson, S. J., and Lippman, R. P., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 309-316. </pages> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Widrow, B. and Hoff, M. E. </author> <year> (1960). </year> <title> Adaptive switching circuits. </title> <booktitle> In IRE WESCON Convention Record, </booktitle> <pages> pages 96-104. </pages> <address> IRE. New York. </address>
Reference-contexts: When a unit is chosen at the next time-step, t + 1, the choice is biased by these values: though stochastic, the unit chosen is probably the one with the highest activation. The connection weights are set by use of the delta rule <ref> (Widrow and Hoff, 1960) </ref>, amplified by the reinforcement signal: w ij (t) = R (t)o j (t)(T i (t) a i (t)) (2) That is, the weight change at time t of the connection from unit j to unit i is equal to the product of the learning rate, , the
Reference: <author> Wilson, S. W. </author> <year> (1989). </year> <title> Hierarchical credit allocation in a classifier system. </title> <editor> In Elzas, M. S., Oren, T. I., and Zeigler, B. P., editors, </editor> <booktitle> Modeling and Simulation Methodology. </booktitle>
Reference: <institution> Elsevier Science Publishers B.V. </institution>
Reference: <author> Wixson, L. E. </author> <year> (1991). </year> <title> Scaling reinforcement learning techniques via modularity. </title> <editor> In Birnbaum, L. A. and Collins, G. C., editors, </editor> <booktitle> Machine Learning: Proceedings of the Eighth International Workshop (ML91), </booktitle> <pages> pages 368-372. </pages> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Wynn-Jones, M. </author> <year> (1993). </year> <title> Node splitting: A constructive algorithm for feed-forward neural networks. </title> <journal> Neural Computing and Applications, </journal> <volume> 1(1) </volume> <pages> 17-22. 8 </pages>
References-found: 24

