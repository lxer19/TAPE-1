URL: http://cobar.cs.umass.edu/info/psfiles/tepubs/te-43.ps
Refering-URL: http://cobar.cs.umass.edu/pubfiles/te.html
Root-URL: 
Title: CRYSTAL: Learning Domain-specific Text Analysis Rules  
Author: Stephen G. Soderland 
Date: November 6, 1996  
Address: Amherst  
Affiliation: Laboratory University of Massachusetts at  
Note: Natural Language Processing  
Abstract-found: 0
Intro-found: 1
Reference: [Aha et al. 1991] <author> Aha, D., Kilber, D., Albert, M. </author> <title> Instance-Based Learning Algorithms. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 37-66, </pages> <year> 1991. </year>
Reference-contexts: The class (the concept being learned) is already known and CRYSTAL applies the similarity metric only to instances of that class. Two IBL algorithms with different strategies for handling noisy training instances are David Aha's IB3 <ref> [Aha et al. 1991] </ref> and Scott Cost and Steven Salzberg's PEBLS [Cost and Salzberg 1993]. Each of these algorithms tabulates classification performance statistics on each exemplar to reduce the effect of noisy training instances.
Reference: [Breiman et al. 1984] <author> Breiman, L., Friedman, J. H., Olshen, R. A., and Stone, C. J. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth Statistic/Probability Series, </publisher> <year> 1984. </year>
Reference-contexts: Entropy = X p i log 2 (p i ) where (p 1 ; :::p k ) is a probability distribution among k classes at a given node. Another basis for a "goodness of split" metric is the Gini diversity index <ref> [Breiman et al. 1984] </ref>. Gini index = X p i p j Experiments by John Mingers suggest that the particular feature selection metric used is not critical to performance [Mingers 1989]. With its recursive partitioning, decision tree algorithms tend to fragment the instance space.
Reference: [Brill 1994] <author> Brill, E. </author> <title> Some Advances in Transformation-Based Part of Speech Tagging. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> 722-727, </pages> <year> 1994. </year>
Reference-contexts: restrictions have been left as options for the CRYSTAL system, with the default being the full representation. 66 Chapter 7 Related Work in Natural Language Processing Most research in applying machine learning to natural language processing has been primarily at the level of lexical or semantic disambiguation of individual words <ref> [Brill 1994, Cardie 1993, Yarowsky 1992, Church 1988] </ref> and in learning heuristics to guide probabilistic parsing [Charniak 1995, Magerman 1995]. Little work has been done, however, in using corpus-based techniques for a higher level of inferencing that goes beyond the meaning of individual words.
Reference: [Cardie 1993] <author> Cardie, C. </author> <title> A Case-Based Approach to Knowledge Acquisition for Domain-Specific Sentence Analysis. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> 798-803, </pages> <year> 1993. </year>
Reference-contexts: restrictions have been left as options for the CRYSTAL system, with the default being the full representation. 66 Chapter 7 Related Work in Natural Language Processing Most research in applying machine learning to natural language processing has been primarily at the level of lexical or semantic disambiguation of individual words <ref> [Brill 1994, Cardie 1993, Yarowsky 1992, Church 1988] </ref> and in learning heuristics to guide probabilistic parsing [Charniak 1995, Magerman 1995]. Little work has been done, however, in using corpus-based techniques for a higher level of inferencing that goes beyond the meaning of individual words.
Reference: [Charniak 1995] <author> Charniak, E. </author> <title> Parsing with Context-free Grammars and Word Statistics, </title> <type> Technical Report CS-95-28, </type> <institution> Department of Computer Science, Brown University, </institution> <year> 1995. </year>
Reference-contexts: representation. 66 Chapter 7 Related Work in Natural Language Processing Most research in applying machine learning to natural language processing has been primarily at the level of lexical or semantic disambiguation of individual words [Brill 1994, Cardie 1993, Yarowsky 1992, Church 1988] and in learning heuristics to guide probabilistic parsing <ref> [Charniak 1995, Magerman 1995] </ref>. Little work has been done, however, in using corpus-based techniques for a higher level of inferencing that goes beyond the meaning of individual words. The work most closely related to CRYSTAL has come from participants in recent Message Understanding Conferences [MUC-4 1992, MUC-5 1993, MUC-6 1995].
Reference: [Church 1988] <author> Church, K. </author> <title> A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text. </title> <booktitle> In Proceedings of the Second Conference on Applied Natural Language Processing, </booktitle> <pages> 136-143, </pages> <year> 1988. </year>
Reference-contexts: restrictions have been left as options for the CRYSTAL system, with the default being the full representation. 66 Chapter 7 Related Work in Natural Language Processing Most research in applying machine learning to natural language processing has been primarily at the level of lexical or semantic disambiguation of individual words <ref> [Brill 1994, Cardie 1993, Yarowsky 1992, Church 1988] </ref> and in learning heuristics to guide probabilistic parsing [Charniak 1995, Magerman 1995]. Little work has been done, however, in using corpus-based techniques for a higher level of inferencing that goes beyond the meaning of individual words.
Reference: [Church et al. 1991] <author> Church, K., Gale, W., Hanks, P., Hindle, D. </author> <title> Using Statistics in Lexical Analysis. Lexical Acquisition: Exploiting On-Line Resources to Build a Lexicon. </title> <publisher> Lawrence Erlbaum Associates, Publishers, </publisher> <pages> 115-164, </pages> <year> 1991. </year>
Reference-contexts: While this is not an insignificant investment of time, I feel that I can call this a modest amount of training. The training corpus for Hospital Discharge consisted of less than 150,000 words. Statistical corpus-based techniques typically require tens of millions of words of training <ref> [Church et al. 1991] </ref>. It is a combination of the limited domain and the use of semantic class information that allow CRYSTAL to work with such a comparatively small amount of training.
Reference: [Clark and Niblett 1989] <author> Clark, P. and Niblett, T. </author> <title> The CN2 Induction Algorithm. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 261-283, </pages> <year> 1989. </year>
Reference-contexts: covering algorithm presented is CN2, which has time complexity similar to A q and a similar top down approach. 1 Lymphomography, breast cancer, and primary tumor data sets from the University Medical Center in Ljubljana, Yugoslavia 2 The UC Irvine collection of machine learning data sets 77 8.1.2 CN2 CN2 <ref> [Clark and Niblett 1989] </ref>, developed by Peter Clark and Tim Niblett, combines aspects of A q with those of decision trees and decision lists.
Reference: [Cost and Salzberg 1993] <author> Cost, S. and Salzberg, S. </author> <title> A Weighted Nearest Neighbor Algorithm for Learning with Symbolic Features. </title> <journal> Machine Learning, </journal> <volume> 10, </volume> <pages> 57-78, </pages> <year> 1993. </year> <month> 95 </month>
Reference-contexts: The class (the concept being learned) is already known and CRYSTAL applies the similarity metric only to instances of that class. Two IBL algorithms with different strategies for handling noisy training instances are David Aha's IB3 [Aha et al. 1991] and Scott Cost and Steven Salzberg's PEBLS <ref> [Cost and Salzberg 1993] </ref>. Each of these algorithms tabulates classification performance statistics on each exemplar to reduce the effect of noisy training instances. IB3 bases its classification of new instances only on instances that pass a statistical significance test as reliable classifiers.
Reference: [Fisher and Riloff 1992] <author> Fisher, D. and Riloff, E. </author> <title> Applying Statistical Meth--ods to Small Corpora: Benefitting from a Limited Domain. </title> <booktitle> In Working Notes of 1992 AAAI Fall Symposium Series: Probabilistic Approaches to Natural Language, </booktitle> <year> 1992. </year>
Reference-contexts: David Fisher and Ellen Riloff have shown that statistically significant co-occurrence frequencies can be derived from a small corpus in a limited domain <ref> [Fisher and Riloff 1992] </ref>. The amount of training can be viewed as modest from another point of view. Developing a set of rules by hand will also require a set of annotated examples to guide development for all but the simplest of information extraction tasks.
Reference: [Fisher et al. 1995] <author> Fisher, D., Soderland, S., McCarthy, J., Feng, F., Lehn-ert, W. </author> <title> Description of the UMass System as Used for MUC-6. </title> <booktitle> In Proceedings of the Sixth Message Understanding Conference, </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <pages> 221-236, </pages> <year> 1995. </year>
Reference-contexts: The semantic hierarchy allows rules with class constraints to cover sub-classes, for example a constraint requiring the class &lt;Person&gt; covers the sub-classes &lt;Person Name&gt; and &lt;Generic Person&gt; in Management Succession instances. Training instances for these experiments were created by the BADGER sentence analyzer of the University of Massachusetts <ref> [Fisher et al. 1995] </ref>. The Hospital Discharge domain used a version of BADGER that segments each simple clause into constituents such as SUBJ, VERB, OBJ, ADV, and PP (prepositional phrase).
Reference: [Huffman 1996] <author> Huffman, S. </author> <title> Learning Information Extraction Patterns from Examples. Connectionist, Statistical, and Symbolic approaches to Learning for Natural Language Processing. </title> <publisher> Springer, </publisher> <pages> 246-260, </pages> <year> 1996. </year>
Reference-contexts: The second system is PALKA [Kim and Moldovan 1992], developed by the University of Southern California for their MUC-5 system. A third trainable system appeared in MUC-6, the HASTEN system from SRA [Krupka 1995]. The fourth system described in this chapter is LIEP <ref> [Huffman 1996] </ref> that was developed on the MUC-6 domain. By the time of the MUC-6 conference, the University of Massachusetts had moved from AutoSlog to CRYSTAL. 7.1 AutoSlog The AutoSlog dictionary construction tool was developed by Ellen Riloff at the University of Massachusetts [Riloff 1996]. <p> The Egraph anchor had a constraint on the verb root, and the irrelevant sentence element had no constraints. 7.4 LIEP The last system to be discussed in this chapter is Scott Huffman's LIEP (Learning Information Extraction Patterns) <ref> [Huffman 1996] </ref>. LIEP uses a set of heuristics to create rules, called extraction patterns, from single training instances. There is also a mechanism to generalize extraction patterns slightly. LIEP learns patterns for multi-slot concept extraction, such as Management Succession events.
Reference: [Kim and Moldovan 1992] <author> Kim, J. and Moldovan, D. PALKA: </author> <title> A System for Linguistic Knowledge Acquisition. </title> <type> Technical Report PKPL 92-8, </type> <institution> USC Department of Electrical Engineering Systems, </institution> <year> 1992. </year>
Reference-contexts: The first is 67 the AutoSlog dictionary construction tool [Riloff 1996] used by the University of Massachusetts in MUC-4 and MUC-5. AutoSlog combines machine learning with a "human in the loop" who edits the proposed rules. The second system is PALKA <ref> [Kim and Moldovan 1992] </ref>, developed by the University of Southern California for their MUC-5 system. A third trainable system appeared in MUC-6, the HASTEN system from SRA [Krupka 1995]. The fourth system described in this chapter is LIEP [Huffman 1996] that was developed on the MUC-6 domain. <p> The system presented in the next section uses a covering algorithm approach that resembles CRYSTAL more closely than AutoSlog does. 7.2 PALKA The PALKA <ref> [Kim and Moldovan 1992] </ref> system was developed by Jun-Tae Kim and Dan Moldovan for the University of Southern California MUC-5 system. PALKA (Parallel Automatic Linguistic Knowledge Acquisition) uses a method similar to the version space algorithm (See Section 8.1.3) to generate text extraction rules from training instances.
Reference: [Krupka 1995] <author> Krupka, G. </author> <title> Description of the SRA System as Used for MUC-6. </title> <booktitle> In Proceedings of the Sixth Message Understanding Conference, </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <pages> 221-236, </pages> <year> 1995. </year>
Reference-contexts: AutoSlog combines machine learning with a "human in the loop" who edits the proposed rules. The second system is PALKA [Kim and Moldovan 1992], developed by the University of Southern California for their MUC-5 system. A third trainable system appeared in MUC-6, the HASTEN system from SRA <ref> [Krupka 1995] </ref>. The fourth system described in this chapter is LIEP [Huffman 1996] that was developed on the MUC-6 domain. <p> George Krupka developed HASTEN <ref> [Krupka 1995] </ref> for the SRA Corporation's MUC-6 text analysis system. HASTEN stores each training instances as an Egraph, which associates structural elements of the sentence with semantic classes and also with case frame slots of an extracted concept.
Reference: [Lehnert et al. 1983] <author> Lehnert, W., Dyer M., Johnson P., Yang C.J., Harley S. </author> <title> BORIS An Experiment in In-Depth Understanding of Narratives. </title> <journal> Artificial Intelligence, </journal> <volume> 20, </volume> <pages> 15-62, </pages> <year> 1983. </year>
Reference-contexts: This requires a system that can reliably extract both the explicitly stated information and that which can be reasonably inferred. Unfortunately, the amount of knowledge needed for in-depth understanding is overwhelming. The BORIS system <ref> [Lehnert et al. 1983] </ref>, developed by Michael Dyer and other researchers at Yale in the 1980's gives an indication of just how much knowledge is needed.
Reference: [Lindberg et al. 1993] <author> Lindberg, D., Humphreys, B., McCray, A. </author> <title> Unified Medical Language Systems. </title> <booktitle> Methods of Information in Medicine, </booktitle> <volume> 32(4), </volume> <pages> 281-291, </pages> <year> 1993. </year>
Reference-contexts: Only the fluctuation in precision of In,Out,Org is significant according to the t-test, but even this should be ignored due to the low sample size for this concept.] 3 The Unified Medical Language System thesaurus of the National Library of Medicine <ref> [Lindberg et al. 1993] </ref>. 37 As in the Management Succession domain, recall increases with further training, while precision stays fairly flat 4 . It appears than recall for Symptom,Absent may be starting to level off at three thousand training instances and Diagnosis,Ruled Out at three hundred. <p> A generic medical thesaurus, the Unified Medical Language System (UMLS) <ref> [Lindberg et al. 1993] </ref> had been used with only minor customization. Unfortunately, class assignment based on UMLS did not correlate closely with annotations of the target concepts. In particular, the class &lt;Sign or Symptom&gt; was a poor predictor of the concept Symptom,Present.
Reference: [Magerman 1995] <author> Magerman, D. </author> <title> Statistical Decision-Tree Models for Parsing. </title> <booktitle> In Proceedings of the 33rd Annual Meeting of the ACL, </booktitle> <year> 1995. </year>
Reference-contexts: representation. 66 Chapter 7 Related Work in Natural Language Processing Most research in applying machine learning to natural language processing has been primarily at the level of lexical or semantic disambiguation of individual words [Brill 1994, Cardie 1993, Yarowsky 1992, Church 1988] and in learning heuristics to guide probabilistic parsing <ref> [Charniak 1995, Magerman 1995] </ref>. Little work has been done, however, in using corpus-based techniques for a higher level of inferencing that goes beyond the meaning of individual words. The work most closely related to CRYSTAL has come from participants in recent Message Understanding Conferences [MUC-4 1992, MUC-5 1993, MUC-6 1995].
Reference: [Michalski 1983] <author> Michalski, R. S. </author> <title> A Theory and Methodology of Inductive Learning. </title> <journal> Artificial Intelligence, </journal> <volume> 20, </volume> <pages> 111-161, </pages> <year> 1983. </year>
Reference-contexts: The covering algorithms described here will be Michalski's A q algorithm, Clark and Niblett's CN2, and finally Mitchell's candidate elimination algorithm. 8.1.1 A q Ryszard Michalski and his students have published several versions of the A q covering algorithm <ref> [Michalski 1983] </ref> and the INDUCE inductive learning program. Peter Clark and Tim Niblett [89] also offer a readable explanation of A q . The basic methodology of A q has much in common with CRYSTAL.
Reference: [Mingers 1989] <author> Mingers, J. </author> <title> An Empirical Comparison of Selection Measures for Decision-Tree Induction. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 319-342, </pages> <year> 1989. </year>
Reference-contexts: Another basis for a "goodness of split" metric is the Gini diversity index [Breiman et al. 1984]. Gini index = X p i p j Experiments by John Mingers suggest that the particular feature selection metric used is not critical to performance <ref> [Mingers 1989] </ref>. With its recursive partitioning, decision tree algorithms tend to fragment the instance space. Nodes near the leaves of the tree are often based on a small number of training examples and have low predictive accuracy.
Reference: [Mitchell 1982] <author> Mitchell, T. </author> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> 18, </volume> <pages> 203-226, </pages> <year> 1982. </year>
Reference: [MUC-3 1991] <editor> Proceedings of the Third Message Understanding Conference, </editor> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1991. </year>
Reference-contexts: One approach to reducing the knowledge acquisition to a manageable size has been that of information extraction (IE). In an IE system, the task is restricted to identifying a predefined set of concepts in a specific domain and ignoring other information. A series of Message Understanding Conferences <ref> [MUC-3 1991, MUC-4 1992, MUC-5 1993, MUC-6 1995] </ref> sponsored by ARPA has given impetus to this approach. I will present CRYSTAL [Soderland et al. 1995], a system that automatically learns domain-specific rules for information extraction.
Reference: [MUC-4 1992] <editor> Proceedings of the Fourth Message Understanding Conference, </editor> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1992. </year>
Reference-contexts: One approach to reducing the knowledge acquisition to a manageable size has been that of information extraction (IE). In an IE system, the task is restricted to identifying a predefined set of concepts in a specific domain and ignoring other information. A series of Message Understanding Conferences <ref> [MUC-3 1991, MUC-4 1992, MUC-5 1993, MUC-6 1995] </ref> sponsored by ARPA has given impetus to this approach. I will present CRYSTAL [Soderland et al. 1995], a system that automatically learns domain-specific rules for information extraction. <p> Little work has been done, however, in using corpus-based techniques for a higher level of inferencing that goes beyond the meaning of individual words. The work most closely related to CRYSTAL has come from participants in recent Message Understanding Conferences <ref> [MUC-4 1992, MUC-5 1993, MUC-6 1995] </ref>. Nearly all systems use some form of pattern matching rules or finite state automata to identify references to concepts of interest. Although most MUC participants build these rules by hand, the methodology looks uncannily like a hand-simulation of CRYSTAL.
Reference: [MUC-5 1993] <editor> Proceedings of the Fifth Message Understanding Conference, </editor> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1993. </year>
Reference-contexts: One approach to reducing the knowledge acquisition to a manageable size has been that of information extraction (IE). In an IE system, the task is restricted to identifying a predefined set of concepts in a specific domain and ignoring other information. A series of Message Understanding Conferences <ref> [MUC-3 1991, MUC-4 1992, MUC-5 1993, MUC-6 1995] </ref> sponsored by ARPA has given impetus to this approach. I will present CRYSTAL [Soderland et al. 1995], a system that automatically learns domain-specific rules for information extraction. <p> Little work has been done, however, in using corpus-based techniques for a higher level of inferencing that goes beyond the meaning of individual words. The work most closely related to CRYSTAL has come from participants in recent Message Understanding Conferences <ref> [MUC-4 1992, MUC-5 1993, MUC-6 1995] </ref>. Nearly all systems use some form of pattern matching rules or finite state automata to identify references to concepts of interest. Although most MUC participants build these rules by hand, the methodology looks uncannily like a hand-simulation of CRYSTAL.
Reference: [MUC-6 1995] <editor> Proceedings of the Sixth Message Understanding Conference, </editor> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1995. </year>
Reference-contexts: One approach to reducing the knowledge acquisition to a manageable size has been that of information extraction (IE). In an IE system, the task is restricted to identifying a predefined set of concepts in a specific domain and ignoring other information. A series of Message Understanding Conferences <ref> [MUC-3 1991, MUC-4 1992, MUC-5 1993, MUC-6 1995] </ref> sponsored by ARPA has given impetus to this approach. I will present CRYSTAL [Soderland et al. 1995], a system that automatically learns domain-specific rules for information extraction. <p> To illustrate how an IE system works, let us consider the Management 1 Succession domain, which was used in the MUC-6 performance evaluation <ref> [MUC-6 1995] </ref>. The task for this domain is to analyze news articles and identify persons moving into or out of top corporate management positions. The only information considered relevant are the persons, positions, and corporations that are directly involved in a management succession event. <p> Little work has been done, however, in using corpus-based techniques for a higher level of inferencing that goes beyond the meaning of individual words. The work most closely related to CRYSTAL has come from participants in recent Message Understanding Conferences <ref> [MUC-4 1992, MUC-5 1993, MUC-6 1995] </ref>. Nearly all systems use some form of pattern matching rules or finite state automata to identify references to concepts of interest. Although most MUC participants build these rules by hand, the methodology looks uncannily like a hand-simulation of CRYSTAL.
Reference: [Murthy et al. 1994] <author> Murthy, S.K., Kasif, S., and Salzberg, S. </author> <title> A System for Induction of Oblique Decision Trees. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2, </volume> <pages> 1-32, </pages> <year> 1994. </year>
Reference-contexts: The key step in building a decision tree is to select a feature at each node whose values best separate different classes into different partitions. Some decision tree algorithms such as OC1 <ref> [Murthy et al. 1994] </ref> also allow the test at a node to be a linear combination of features. This makes the selection of a test even more expensive. Many decision tree algorithms, including C4.5, base the feature selection metric on the information-theoretic measure, Shannon entropy.
Reference: [Pagallo and Haussler 1990] <author> Pagallo, G. and Haussler, D. </author> <title> Boolean Feature Discovery in Empirical Learning. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 71-99, </pages> <year> 1990. </year>
Reference-contexts: I will use Ross Quinlan's C4.5 [Quinlan 1993] as a representative of decision tree algorithms and use Giulia Pagallo and David Haussler's GREEDY3 algorithm <ref> [Pagallo and Haussler 1990] </ref> to represent decision lists. 80 8.3.1 C4.5 Decision tree algorithms have something in common with covering algorithms: top down processing and exhaustive consideration of attributes. Decision tree induction begins with an empty tree and recursively adds tests at each tree node to partition the instance space.
Reference: [Quinlan 1993] <author> Quinlan, J.R. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1993. </year>
Reference-contexts: IB3 discards much of the training set, but PEBLS retains all training instances, which can become expensive in terms of memory. 8.3 Decision Tree Algorithms A well known family of machine learning algorithms is decision tree induction. I will use Ross Quinlan's C4.5 <ref> [Quinlan 1993] </ref> as a representative of decision tree algorithms and use Giulia Pagallo and David Haussler's GREEDY3 algorithm [Pagallo and Haussler 1990] to represent decision lists. 80 8.3.1 C4.5 Decision tree algorithms have something in common with covering algorithms: top down processing and exhaustive consideration of attributes.
Reference: [Quinlan and Cameron-Jones 1995] <author> Quinlan, J.R. and Cameron-Jones, </author> <title> R.M. Oversearching and Layered Search in Emperical Learning. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1019-1024, </pages> <year> 1995. </year>
Reference: [Riloff 1996] <author> Riloff, E. </author> <title> Automatically Constructing a Dictionary for Information Extraction Tasks. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> 811-816, </pages> <year> 1993. </year>
Reference-contexts: Three of the MUC participants have developed systems that learn text analysis rules. The remainder of this section will compare CRYSTAL with these other systems, plus another system based on a MUC domain. The first is 67 the AutoSlog dictionary construction tool <ref> [Riloff 1996] </ref> used by the University of Massachusetts in MUC-4 and MUC-5. AutoSlog combines machine learning with a "human in the loop" who edits the proposed rules. The second system is PALKA [Kim and Moldovan 1992], developed by the University of Southern California for their MUC-5 system. <p> By the time of the MUC-6 conference, the University of Massachusetts had moved from AutoSlog to CRYSTAL. 7.1 AutoSlog The AutoSlog dictionary construction tool was developed by Ellen Riloff at the University of Massachusetts <ref> [Riloff 1996] </ref>. AutoSlog passes through the training texts a single time and proposes concept definitions from instances of the concepts to be extracted. AutoSlog uses "one shot" learning with no generalization phase and no testing of proposed rules on the training data. <p> Instead, it uses heuristics to craft the best concept definition it can from a single motivating example. An AutoSlog concept definition assigns a fixed level of semantic constraint to the extracted phrase. A more recent version of AutoSlog <ref> [Riloff 1996] </ref> has no semantic constraints at all on the extracted phrase. This version assumes that later processing in an information extraction system will filter out extraction errors by overgeneralized AutoSlog rules.
Reference: [Riloff 1996] <author> Riloff, E. </author> <title> Automatically Generating Extraction Patterns from Untagged Text. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> 1044-1049, </pages> <year> 1996. </year>
Reference-contexts: Three of the MUC participants have developed systems that learn text analysis rules. The remainder of this section will compare CRYSTAL with these other systems, plus another system based on a MUC domain. The first is 67 the AutoSlog dictionary construction tool <ref> [Riloff 1996] </ref> used by the University of Massachusetts in MUC-4 and MUC-5. AutoSlog combines machine learning with a "human in the loop" who edits the proposed rules. The second system is PALKA [Kim and Moldovan 1992], developed by the University of Southern California for their MUC-5 system. <p> By the time of the MUC-6 conference, the University of Massachusetts had moved from AutoSlog to CRYSTAL. 7.1 AutoSlog The AutoSlog dictionary construction tool was developed by Ellen Riloff at the University of Massachusetts <ref> [Riloff 1996] </ref>. AutoSlog passes through the training texts a single time and proposes concept definitions from instances of the concepts to be extracted. AutoSlog uses "one shot" learning with no generalization phase and no testing of proposed rules on the training data. <p> Instead, it uses heuristics to craft the best concept definition it can from a single motivating example. An AutoSlog concept definition assigns a fixed level of semantic constraint to the extracted phrase. A more recent version of AutoSlog <ref> [Riloff 1996] </ref> has no semantic constraints at all on the extracted phrase. This version assumes that later processing in an information extraction system will filter out extraction errors by overgeneralized AutoSlog rules.
Reference: [Rivest 1987] <author> Rivest, R. </author> <title> Learning Decision Lists. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <year> 1987. </year>
Reference-contexts: This is repeated until all instances are covered. Unlike A q and CRYSTAL, which build an unordered set of concept descriptions, CN2 builds a decision list <ref> [Rivest 1987] </ref> of rules that are applied in order. This handles exceptions naturally. An earlier rule can remove instances that would otherwise be errors to a later rule. Like A q , CN2 builds concept descriptions in a top down fashion, successively adding attributes to specialize a description.
Reference: [Soderland and Lehnert 1994] <author> Soderland, S. and Lehnert, W. Wrap-Up: </author> <title> a Trainable Discourse Module for Information Extraction. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2, </volume> <pages> 131-158, </pages> <year> 1994. </year>
Reference-contexts: Machine learning algorithms that use explicit feature vectors or have a basic step that considers all possible attributes can become impractical when the feature set becomes extremely large. I had an unsatisfactory experience using decision trees in a system called WRAP-UP <ref> [Soderland and Lehnert 1994] </ref> that learned to make inferences during discourse processing in information extraction. An excessive amount of memory and computation time was required to induce a decision tree when there were thousands of instances with thousands of features.
Reference: [Soderland et al. 1995] <author> Soderland, S., Fisher, D., Aseltine, J., Lehnert, W. </author> <title> CRYSTAL: Inducing a Conceptual Dictionary. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1314-1321, </pages> <year> 1995. </year> <month> 97 </month>
Reference-contexts: A series of Message Understanding Conferences [MUC-3 1991, MUC-4 1992, MUC-5 1993, MUC-6 1995] sponsored by ARPA has given impetus to this approach. I will present CRYSTAL <ref> [Soderland et al. 1995] </ref>, a system that automatically learns domain-specific rules for information extraction.
Reference: [Yarowsky 1992] <author> Yarowsky, D. </author> <title> Word Sense Disambiguation Using Statistical Models of Roget's Categories Trained on Large Corpora. </title> <booktitle> In Proceedings of the Fourteenth International Conference on Computational Linguistics, </booktitle> <pages> 454-460, </pages> <year> 1992. </year> <month> 98 </month>
Reference-contexts: restrictions have been left as options for the CRYSTAL system, with the default being the full representation. 66 Chapter 7 Related Work in Natural Language Processing Most research in applying machine learning to natural language processing has been primarily at the level of lexical or semantic disambiguation of individual words <ref> [Brill 1994, Cardie 1993, Yarowsky 1992, Church 1988] </ref> and in learning heuristics to guide probabilistic parsing [Charniak 1995, Magerman 1995]. Little work has been done, however, in using corpus-based techniques for a higher level of inferencing that goes beyond the meaning of individual words.
References-found: 34

