URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3682/3682.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Title: PIVOTED CAUCHY-LIKE PRECONDITIONERS FOR REGULARIZED SOLUTION OF ILL-POSED PROBLEMS  
Author: MISHA E. KILMER AND DIANNE P. O'LEARY 
Keyword: Key words. Regularization, ill-posed problems, Toeplitz, Cauchy-like, preconditioner, conjugate gradient, least squares  
Date: September 3, 1996  
Note: AMS(MOS) subject classifications. 65R20, 45L10, 94A12  
Abstract: Many ill-posed problems are solved using a discretization that results in a least squares problem or a linear system involving a Toeplitz matrix. The exact solution to such problems is often hopelessly contaminated by noise, since the discretized problem is quite ill-conditioned, and noise components in the approximate null-space dominate the solution vector. Therefore we seek an approximate solution that does not have large components in these directions. We use a preconditioned conjugate gradient algorithm to compute such a regularized solution. An orthogonal change of coordinates transforms the Toeplitz matrix to a Cauchy-like matrix, and we choose our preconditioner to be a low rank Cauchy-like matrix determined in the course of Gu's fast modified complete pivoting algorithm. We show that if the kernel of the ill-posed problem is smooth, then this preconditioner has desirable properties: the largest singular values of the preconditioned matrix are clustered around one, the smallest singular values, corresponding to the noise subspace, remain small, and the signal and noise spaces are relatively unmixed. The preconditioned algorithm costs only O(n lg n) operations per iteration for a problem with n variables. The effectiveness of the preconditioner for filtering noise is demonstrated on three examples. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Bojanczyk, </author> <year> 1996. </year> <type> Personal Communication. </type>
Reference-contexts: However, the algorithm is very fragile. It can be unstable for large values of n and, even when used on a well conditioned matrix, may require pivoting to maintain stability <ref> [18, 1] </ref>.
Reference: [2] <author> R. Chan, J. Nagy, and R. Plemmons, </author> <title> Circulant preconditioned Toeplitz least squares iterations, </title> <note> SIAM J. Matrix Anal. and Appl., 15 (1994), p. 80. </note>
Reference-contexts: Its smallest entries are attained for jk jj 0 or n, but there are very few small values. In fact, direct computation shows that for n 100, at least 95% of the entries in the first row have denominators in the range <ref> [10 1 ; 2] </ref>, and the other rows have even more in this range. Figure 1 plots values of the matrix h j! k j j k;j=1;:::;n for n = 100 above given tolerance levels. <p> as the number of operations to apply C to a vector, since C = P F T S fl 0 F fl Q and the product of a Toeplitz matrix with a vector can be computed in O (n lg n) operations by embedding the matrix in a circulant matrix <ref> [2] </ref>. Thus, each iteration of CGLS costs O (n lg n) operations. 11 Fig. 3. Fourier coefficients of the noisy data for Example 1. Fig. 4. Relative error in computed solution for m = 0, m = 27, and m = 43, Example 1. Fig. 5. <p> whose entries are defined by t i;j = 51 (ff i fi j ) if ji jj 8; 0 otherwise, where ff i = fi i = 51 and 1 p exp ( 4ffi 2 ); ffi = 0:15 : This matrix is the one used in Example 4 of <ref> [2] </ref>. The authors note that such matrices may occur in image restoration contexts as "prototype problems" and are used to model certain degradations in the recorded image. The condition number of T is approximately 2:4 fi 10 6 .
Reference: [3] <author> T. Chan, </author> <title> An optimal circulant preconditioner for Toeplitz systems, </title> <journal> SIAM J. Sci. Statist. Com-put., </journal> <volume> 9 (1988), </volume> <pages> pp. 766-771. </pages>
Reference-contexts: Toeplitz matrices have several properties convenient for iterative methods like conjugate gradients: multiplication of a Toeplitz matrix times a vector can be done in O (n lg n) operations, and circulant preconditioners can be quite efficient <ref> [25, 3] </ref>. There are some difficulties, though. The inverse of a Toeplitz matrix does not generally have Toeplitz structure, and the fast factorization algorithms for Toeplitz matrices can require as much as O (n 3 ) flops if pivoting is used to improve stability; see [27, 11, 4], for example.
Reference: [4] <author> T. F. Chan and P. C. Hansen, </author> <title> A lookahead Levinson algorithm for general Toeplitz systems, </title> <booktitle> IEEE Proc. Signal Processing, 40 (1992), </booktitle> <pages> pp. 1079-1090. </pages>
Reference-contexts: There are some difficulties, though. The inverse of a Toeplitz matrix does not generally have Toeplitz structure, and the fast factorization algorithms for Toeplitz matrices can require as much as O (n 3 ) flops if pivoting is used to improve stability; see <ref> [27, 11, 4] </ref>, for example. To overcome these difficulties, we make use of the fact that Toeplitz matrices are related to Cauchy-like matrices by fast orthogonal transformations [17, 8, 10]. Cauchy-like matrices, discussed in detail in x2, permit fast matrix-vector multiplication.
Reference: [5] <author> R. D. Fierro, G. H. Golub, P. C. Hansen, and D. P. O'Leary, </author> <title> Regularization by truncated total least squares, </title> <booktitle> in Proceedings of the Fifth SIAM Conference on Applied Linear Algebra, </booktitle> <editor> J. G. Lewis, ed., </editor> <address> Philadelphia, 1994, </address> <publisher> SIAM Press, </publisher> <pages> pp. 102-105. </pages>
Reference-contexts: Regularization can be thought of as exchanging the original, ill-posed problem for a more well-posed problem whose solution approximates the true solution. Many regularization methods, both direct and iterative, have been discussed in the literature; see, for example, <ref> [12, 15, 9, 5] </ref>. In this paper we will primarily be concerned with regularization via conjugate gradient iterations [7, 22, 29], where the regularization parameter is the number of iterations.
Reference: [6] <author> T. Finck, G. Heinig, and K. Rost, </author> <title> An inversion formula and fast algorithms for Cauchy-Vandermonde matrices, </title> <note> Linear Algebra Appl., 183 (1993), p. 179. </note>
Reference-contexts: The notation denotes the componentwise product of two vectors. Fast multiplication by the matrix C 0 requires finding the coefficients of a polyno mial whose roots are the elements of fi 1 and 1 <ref> [6] </ref>, and this process can be unstable. <p> This formulation allows C 1 1 r to be computed in O (n lg n) operations in a stable manner, using an observation of Finck, Heinig, and Rost <ref> [6] </ref> that any Cauchy-like matrix can be factored as C 0 = diag (h ( 1 ); : : : ; h ( n )) 1 V ()HV (!) T ;(17) 10 Fig. 2.
Reference: [7] <author> H. E. Fleming, </author> <title> Equivalence of regularization and truncated iteration in the solution of ill-posed problems, </title> <journal> Linear Algebra Appl., </journal> <volume> 130 (1990), </volume> <pages> pp. 133-150. </pages>
Reference-contexts: Many regularization methods, both direct and iterative, have been discussed in the literature; see, for example, [12, 15, 9, 5]. In this paper we will primarily be concerned with regularization via conjugate gradient iterations <ref> [7, 22, 29] </ref>, where the regularization parameter is the number of iterations. Toeplitz matrices have several properties convenient for iterative methods like conjugate gradients: multiplication of a Toeplitz matrix times a vector can be done in O (n lg n) operations, and circulant preconditioners can be quite efficient [25, 3]. <p> We refer to the resulting algorithm as CGLS [19]. If the discrete Picard condition holds, then CGLS acts as an iterative regularization method with the iteration index taking the role of the regularization parameter <ref> [7, 13, 15] </ref>. Convergence is governed by the spread and clustering of the singular values [28]. Therefore, preconditioning is often applied in an effort to cluster the singular values, thus speeding convergence.
Reference: [8] <author> I. Gohberg, T. Kailath, and V. Olshevsky, </author> <title> Fast Gaussian elimination with partial pivoting of matrices with displacement structure, </title> <booktitle> Mathematics of Computation, </booktitle> <year> (1995). </year>
Reference-contexts: To overcome these difficulties, we make use of the fact that Toeplitz matrices are related to Cauchy-like matrices by fast orthogonal transformations <ref> [17, 8, 10] </ref>. Cauchy-like matrices, discussed in detail in x2, permit fast matrix-vector multiplication. But, in contrast to Toeplitz matrices, the inverse of a Cauchy-like matrix is Cauchy-like, and complete pivoting can be incorporated in its LDU factorization at a total cost of O (n 2 ). <p> For the matrices and displacement equations of interest here, ` = 1 or 2 <ref> [8] </ref>. We exploit three important properties of Cauchy-like matrices. 2 Property 1. Row and column permutations of Cauchy-like matrices are Cauchy- like, as are leading principal submatrices. This property allows pivoting in fast algorithms for factoring Cauchy-like matrices [17, 8]. Property 2. <p> We exploit three important properties of Cauchy-like matrices. 2 Property 1. Row and column permutations of Cauchy-like matrices are Cauchy- like, as are leading principal submatrices. This property allows pivoting in fast algorithms for factoring Cauchy-like matrices <ref> [17, 8] </ref>. Property 2. <p> Alternatively, X and W can be determined from the relations CX = A; W T C = B T :(5) The third important property is that Toeplitz matrices also satisfy certain displacement equations <ref> [21, 8] </ref> which allow them to be transformed via fast Fourier transforms into Cauchy-like matrices [17, 8]: Property 3. <p> Alternatively, X and W can be determined from the relations CX = A; W T C = B T :(5) The third important property is that Toeplitz matrices also satisfy certain displacement equations [21, 8] which allow them to be transformed via fast Fourier transforms into Cauchy-like matrices <ref> [17, 8] </ref>: Property 3. <p> (F A)(B T S fl where S 1 = diag (1; e 2i 2i S 1 = diag (e i (2n1)i S 0 = diag (1; e i i and F is the normalized inverse discrete Fourier transform matrix defined by F = p n : Gohberg, Kailath, and Olshevsky <ref> [8] </ref> suggest a stable O (`n 2 ) partial pivoting algorithm to factor C = P LU . Sweet and Brent [26] show, however, that element growth in this algorithm depends not only on the magnitude of L and U , but on the generator for the Cauchy-like matrix.
Reference: [9] <author> W. Groetsch, </author> <title> Theory of Tikhonov Regularization for Fredholm Equations of the First Kind, </title> <publisher> Pitman Publishing Limited, </publisher> <year> 1984. </year>
Reference-contexts: Regularization can be thought of as exchanging the original, ill-posed problem for a more well-posed problem whose solution approximates the true solution. Many regularization methods, both direct and iterative, have been discussed in the literature; see, for example, <ref> [12, 15, 9, 5] </ref>. In this paper we will primarily be concerned with regularization via conjugate gradient iterations [7, 22, 29], where the regularization parameter is the number of iterations.
Reference: [10] <author> M. Gu, </author> <title> Stable and efficient algorithms for structured systems of linear equations, </title> <type> Tech. Report LBL-37690, </type> <institution> Lawrence Berkeley Laboratory, </institution> <year> 1995. </year>
Reference-contexts: To overcome these difficulties, we make use of the fact that Toeplitz matrices are related to Cauchy-like matrices by fast orthogonal transformations <ref> [17, 8, 10] </ref>. Cauchy-like matrices, discussed in detail in x2, permit fast matrix-vector multiplication. But, in contrast to Toeplitz matrices, the inverse of a Cauchy-like matrix is Cauchy-like, and complete pivoting can be incorporated in its LDU factorization at a total cost of O (n 2 ). <p> Sweet and Brent [26] show, however, that element growth in this algorithm depends not only on the magnitude of L and U , but on the generator for the Cauchy-like matrix. For our test matrices, partial pivoting alone did not provide the rank revealing information that we need. Gu <ref> [10] </ref> presents an algorithm that can perform a fast O (`n 2 ) variation of LU decomposition with complete pivoting. Recall that in complete pivoting, at every elimination step one chooses the largest element in the current submatrix as the pivot in order to reduce element growth. <p> Gu proposes instead that one find an entry sufficiently large in magnitude by considering the largest 2-norm column of A B T corresponding to the part that remains to be factored at each step. This algorithm computes the factorization C = P LU Q <ref> [10, Alg. 2] </ref> using only the readily determined generators (see x4), and Gu shows that it is efficient and numerically stable, provided that element growth in the computed factorization is not large. <p> We have observed that if Gu's algorithm is applied to a matrix with this structure, then C 1 will contain the four corner blocks. The interested reader is referred to <ref> [10] </ref> for details on the complete pivoting strategy, but the key fact is that Gu makes his pivoting decisions based on the size of elements in the generator A B T corresponding to the block that remains to be factored.
Reference: [11] <author> M. H. Gutknecht and H. M., </author> <title> Look-ahead Levinson and Schur algorithms for non-Hermitian Toeplitz systems, </title> <type> Tech. Report IPS 93-11, </type> <institution> IPS Supercomputing, ETH-Zurich, </institution> <year> 1993. </year>
Reference-contexts: There are some difficulties, though. The inverse of a Toeplitz matrix does not generally have Toeplitz structure, and the fast factorization algorithms for Toeplitz matrices can require as much as O (n 3 ) flops if pivoting is used to improve stability; see <ref> [27, 11, 4] </ref>, for example. To overcome these difficulties, we make use of the fact that Toeplitz matrices are related to Cauchy-like matrices by fast orthogonal transformations [17, 8, 10]. Cauchy-like matrices, discussed in detail in x2, permit fast matrix-vector multiplication.
Reference: [12] <author> M. Hanke, </author> <title> Regularization with differential operators, an iterative approach, Numerical Functional Analysis and Optimization, </title> <month> 13 </month> <year> (1992). </year>
Reference-contexts: Regularization can be thought of as exchanging the original, ill-posed problem for a more well-posed problem whose solution approximates the true solution. Many regularization methods, both direct and iterative, have been discussed in the literature; see, for example, <ref> [12, 15, 9, 5] </ref>. In this paper we will primarily be concerned with regularization via conjugate gradient iterations [7, 22, 29], where the regularization parameter is the number of iterations.
Reference: [13] <author> M. Hanke, J. Nagy, and R. Plemmons, </author> <title> Preconditioned iterative regularization for ill-posed problems, Numerical Linear Algebra and Sci. </title> <journal> Computing, </journal> <note> (1993). </note> <author> L. Reichel, A. Ruttan and R.S. Varga, </author> <title> editors. </title>
Reference-contexts: We refer to the resulting algorithm as CGLS [19]. If the discrete Picard condition holds, then CGLS acts as an iterative regularization method with the iteration index taking the role of the regularization parameter <ref> [7, 13, 15] </ref>. Convergence is governed by the spread and clustering of the singular values [28]. Therefore, preconditioning is often applied in an effort to cluster the singular values, thus speeding convergence.
Reference: [14] <author> P. C. Hansen, </author> <title> The discrete Picard condition for discrete ill-posed problems, </title> <journal> BIT, </journal> <volume> 30 (1990), </volume> <pages> pp. </pages> <month> 658-672. </month> <title> [15] , Rank Deficient and Discrete Ill-Posed Problems, </title> <type> PhD thesis, </type> <institution> Technical University of Denmark, Lyngby, Denmark, </institution> <month> July, </month> <year> 1995. </year> <note> UNIC Report UNIC-95-07. </note>
Reference-contexts: Three assumptions will guide our analysis: 1. The matrix T has been normalized so that its largest singular value is of order 1. 2. The uncontaminated data vector ^g satisfies the discrete Picard condition; i.e., the spectral coefficients of ^g decay in absolute value like the singular values <ref> [30, 14] </ref>. 3. The additive noise is zero-mean white Gaussian. In this case, the components of the error e are independent random variables normally distributed with mean zero and variance * 2 . We need to define the signal and noise subspaces.
Reference: [16] <author> P. C. Hansen and D. P. O'Leary, </author> <title> The use of the L-curve in the regularization of discrete ill-posed problems, </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 14 (1993), </volume> <pages> pp. 1487-1503. </pages>
Reference-contexts: The approximate solution in the original coordinate system is f = S fl When to stop the CGLS iteration in order to get the best approximate solution is a well-studied but open question (for instance, see <ref> [16] </ref> and the references therein). We do not solve this problem, but we consider the other algorithmic issues in the following subsections. 4.1. Determining the size of C 1 . The choice of the parameter m determines the number of clustered singular values in the preconditioned system.
Reference: [17] <author> G. Heinig, </author> <title> Inversion of generalized cauchy matrices and other classes of structured matrices, Linear Algebra in Signal Processing, </title> <journal> IMA Volumes in Math. and Appl., </journal> <volume> 69 (1994), </volume> <pages> pp. 95-114. </pages> <month> [18] , </month> <year> 1996. </year> <type> Personal Communication. </type>
Reference-contexts: To overcome these difficulties, we make use of the fact that Toeplitz matrices are related to Cauchy-like matrices by fast orthogonal transformations <ref> [17, 8, 10] </ref>. Cauchy-like matrices, discussed in detail in x2, permit fast matrix-vector multiplication. But, in contrast to Toeplitz matrices, the inverse of a Cauchy-like matrix is Cauchy-like, and complete pivoting can be incorporated in its LDU factorization at a total cost of O (n 2 ). <p> We exploit three important properties of Cauchy-like matrices. 2 Property 1. Row and column permutations of Cauchy-like matrices are Cauchy- like, as are leading principal submatrices. This property allows pivoting in fast algorithms for factoring Cauchy-like matrices <ref> [17, 8] </ref>. Property 2. <p> Row and column permutations of Cauchy-like matrices are Cauchy- like, as are leading principal submatrices. This property allows pivoting in fast algorithms for factoring Cauchy-like matrices [17, 8]. Property 2. The inverse of a Cauchy-like matrix is Cauchy-like: C 1 = x T i ! j 1i;jn Heinig <ref> [17] </ref> gives an O (n lg 2 n) algorithm to compute X (with rows x T i ) and W (with rows w T i ) given A, B, fi, and , and explains how, using the FFT, a system involving a Cauchy-like matrix can be solved in O (n lg <p> Alternatively, X and W can be determined from the relations CX = A; W T C = B T :(5) The third important property is that Toeplitz matrices also satisfy certain displacement equations [21, 8] which allow them to be transformed via fast Fourier transforms into Cauchy-like matrices <ref> [17, 8] </ref>: Property 3. <p> Applying the preconditioner. Let r be a vector of length m and assume that no pivoting was done when ~ C was factored. Heinig <ref> [17] </ref> states that C 1 1 r may be written as C 1 ` X (X 1 ) j (C 0 (W 1 ) j r) where (X 1 ) j is the jth column of X 1 , (W 1 ) j is the jth column of W 1 ,
Reference: [19] <author> M. R. Hestenes and E. </author> <title> Stiefel, Methods of conjugate gradients for solving linear systems, </title> <institution> J. Res. Natl. Bur. Standards, </institution> <month> 49 </month> <year> (1952), </year> <pages> pp. 409-436. </pages>
Reference-contexts: Regularization by preconditioned conjugate gradients. The standard conjugate gradient (CG) method <ref> [19] </ref> is an iterative method for solving systems 4 of linear equations for which the matrix is symmetric positive definite. If the ma-trix is not symmetric positive definite, one can use a variant of standard CG which solves the normal equations in factored form. <p> If the ma-trix is not symmetric positive definite, one can use a variant of standard CG which solves the normal equations in factored form. We refer to the resulting algorithm as CGLS <ref> [19] </ref>. If the discrete Picard condition holds, then CGLS acts as an iterative regularization method with the iteration index taking the role of the regularization parameter [7, 13, 15]. Convergence is governed by the spread and clustering of the singular values [28].
Reference: [20] <author> R. A. Horn and C. R. Johnson, </author> <title> Topics in Matrix Analysis, </title> <publisher> Cambridge University Press, </publisher> <year> 1991. </year>
Reference-contexts: If Y 1 and Y 2 are two nfin matrices and the rank of Y 2 is nm then a theorem of Weyl <ref> [20, Thm. 3.3.16] </ref> implies n (Y 1 + Y 2 ) m (Y 1 ). Now set 1 C 2 1 C 2 and notice that the eigenvalues of E 1 are the squares of the singular values of Y 1 .
Reference: [21] <author> T. Kailath, S. Kung, and M. Morf, </author> <title> Displacement ranks of matrices and linear equations, </title> <journal> Journal of Math. Anal. and Appl., </journal> <volume> 78 (1979), </volume> <pages> pp. 395-407. </pages>
Reference-contexts: Alternatively, X and W can be determined from the relations CX = A; W T C = B T :(5) The third important property is that Toeplitz matrices also satisfy certain displacement equations <ref> [21, 8] </ref> which allow them to be transformed via fast Fourier transforms into Cauchy-like matrices [17, 8]: Property 3.
Reference: [22] <author> C. </author> <title> Lanczos, Solution of systems of linear equations by minimized iterations, </title> <journal> Journal of Research of the N.B.S., </journal> <volume> 49 (1952), </volume> <pages> pp. 33-53. </pages>
Reference-contexts: Many regularization methods, both direct and iterative, have been discussed in the literature; see, for example, [12, 15, 9, 5]. In this paper we will primarily be concerned with regularization via conjugate gradient iterations <ref> [7, 22, 29] </ref>, where the regularization parameter is the number of iterations. Toeplitz matrices have several properties convenient for iterative methods like conjugate gradients: multiplication of a Toeplitz matrix times a vector can be done in O (n lg n) operations, and circulant preconditioners can be quite efficient [25, 3].
Reference: [23] <author> D. L. Phillips, </author> <title> A technique for the numerical solution of certain integral equations of the first kind, </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 9 (1962), </volume> <pages> pp. 84-97. </pages>
Reference-contexts: Phillips test problem. Next we consider the discretized version of the well-known first-kind Fredholm integral equation studied by D.L. Phillips <ref> [23] </ref>. The 1 We first determined ^ f using Matlab's square function, ^ f = square (2v fl :3) with v = [0 : :1: 9:9], then computed ^g = T ^ f. 13 Fig. 6. Uncontaminated data vector (left) and exact solution vector (right) for Example 2. Fig. 7.
Reference: [24] <author> G. W. Stewart and J. guang Sun, </author> <title> Matrix Perturbation Theory, </title> <publisher> Academic Press, </publisher> <year> 1990. </year>
Reference-contexts: By Corollary IV.4.9 <ref> [24] </ref>, we know that i (E 1 ) i ((M 1 C) fl (M 1 C))(12) We need to show that k (E 1 ) 1.
Reference: [25] <author> G. Strang, </author> <title> A proposal for Toeplitz matrix calculations, </title> <journal> Stud. Appl. Math., </journal> <volume> 74 (1986), </volume> <pages> pp. 171-176. </pages>
Reference-contexts: Toeplitz matrices have several properties convenient for iterative methods like conjugate gradients: multiplication of a Toeplitz matrix times a vector can be done in O (n lg n) operations, and circulant preconditioners can be quite efficient <ref> [25, 3] </ref>. There are some difficulties, though. The inverse of a Toeplitz matrix does not generally have Toeplitz structure, and the fast factorization algorithms for Toeplitz matrices can require as much as O (n 3 ) flops if pivoting is used to improve stability; see [27, 11, 4], for example.
Reference: [26] <author> D. Sweet and R. Brent, </author> <title> Error analysis of a fast partial pivoting method for structured matrices, </title> <booktitle> Advanced Signal Processing Algorithms, Proc. of SPIE, 2363 (1995), </booktitle> <pages> pp. 266-280. </pages>
Reference-contexts: Sweet and Brent <ref> [26] </ref> show, however, that element growth in this algorithm depends not only on the magnitude of L and U , but on the generator for the Cauchy-like matrix. For our test matrices, partial pivoting alone did not provide the rank revealing information that we need.
Reference: [27] <author> D. R. Sweet, </author> <title> The use of pivoting to improve the numerical performance of Toeplitz matrix algorithms, </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 14 (1993), </volume> <pages> pp. 468-493. </pages>
Reference-contexts: There are some difficulties, though. The inverse of a Toeplitz matrix does not generally have Toeplitz structure, and the fast factorization algorithms for Toeplitz matrices can require as much as O (n 3 ) flops if pivoting is used to improve stability; see <ref> [27, 11, 4] </ref>, for example. To overcome these difficulties, we make use of the fact that Toeplitz matrices are related to Cauchy-like matrices by fast orthogonal transformations [17, 8, 10]. Cauchy-like matrices, discussed in detail in x2, permit fast matrix-vector multiplication.
Reference: [28] <author> A. van der Sluis and H. van der Vorst, </author> <title> The rate of convergence of conjugate gradients, </title> <journal> Numer. Math, </journal> <volume> 48 (1986), </volume> <pages> pp. </pages> <month> 543-560. </month> <title> [29] , Sirt- and CG-type methods for the iterative solution of sparse linear least-squares problems, </title> <journal> Linear Algebra and Appl., </journal> <volume> 130 (1990), </volume> <pages> pp. 257-302. </pages>
Reference-contexts: We refer to the resulting algorithm as CGLS [19]. If the discrete Picard condition holds, then CGLS acts as an iterative regularization method with the iteration index taking the role of the regularization parameter [7, 13, 15]. Convergence is governed by the spread and clustering of the singular values <ref> [28] </ref>. Therefore, preconditioning is often applied in an effort to cluster the singular values, thus speeding convergence.
Reference: [30] <author> J. M. Varah, </author> <title> Pitfalls in the numerical solution of linear ill-posed problems, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 4 (1983), </volume> <pages> pp. 164-176. 20 </pages>
Reference-contexts: Three assumptions will guide our analysis: 1. The matrix T has been normalized so that its largest singular value is of order 1. 2. The uncontaminated data vector ^g satisfies the discrete Picard condition; i.e., the spectral coefficients of ^g decay in absolute value like the singular values <ref> [30, 14] </ref>. 3. The additive noise is zero-mean white Gaussian. In this case, the components of the error e are independent random variables normally distributed with mean zero and variance * 2 . We need to define the signal and noise subspaces.
References-found: 27

