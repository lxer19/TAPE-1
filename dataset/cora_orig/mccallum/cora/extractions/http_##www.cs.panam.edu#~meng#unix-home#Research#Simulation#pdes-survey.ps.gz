URL: http://www.cs.panam.edu/~meng/unix-home/Research/Simulation/pdes-survey.ps.gz
Refering-URL: http://www.cs.panam.edu/~meng/unix-home/Research/Simulation/
Root-URL: http://www.cs.panam.edu
Title: Parallel Simulation Today  
Author: David Nicol Richard Fujimoto 
Address: Williamsburg, VA 23187-8795  Atlanta, Ga 30332-0280  
Affiliation: Dept. of Computer Science College of William Mary  College of Computing Georgia Institute of Technology  
Abstract: This paper surveys topics that presently define the state of the art in parallel simulation. Included in the tutorial are discussions on new protocols, mathematical performance analysis, time parallelism, hardware support for parallel simulation, load balancing algorithms, and dynamic memory management for optimistic synchronization. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> I. F. Akyildiz, L. Chen, S. R. Das, R. M. Fujimoto, and R. Serfozo. </author> <title> Performance analysis of Time Warp with limited memory. </title> <booktitle> Proceedings of the 1992 ACM SIGMETRICS Conference on Measuring and Modeling Computer Systems, </booktitle> <volume> 20(1), </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: Even so, the transition probabilities can only be approximated, and then only in terms of multiple ( 10) model unknowns. Solution requires a fixed-point numerical procedure to solve a set of a dozen or so coupled non-linear equations. The Time Warp model above was extended in <ref> [1] </ref> to consider the effects of limited memory in a shared memory system. It is assumed that all memory is allocated from a global buffer, with capacity supporting up to M uncommitted events. <p> a tightly coupled multiprocessor, computation of GVT is straightforward because one can use a barrier synchronization to "freeze" the computation and obtain a global snapshot of the system, though care must be 23 taken or serious performance degradations may occur, particularly if the system contains a limited amount of memory <ref> [1] </ref>. However, computation of GVT is more complex in distributed and loosely coupled systems because such snapshots are not so easily obtained. <p> Of course, a Time Warp program will run very slowly if one only provides the absolute minimum amount of memory. The question of Time Warp performance as the amount of memory is varied has been studied <ref> [1] </ref>. An analytic model was developed that indicates for homogeneous workloads, Time Warp requires relatively little memory to achieve good performance, i.e., performance with unlimited memory.
Reference: [2] <author> H. Ammar and S. Deng. </author> <title> Time Warp simulation using time scale decomposition. </title> <booktitle> In Advances in Parallel and Distributed Simulation, </booktitle> <volume> volume 23, </volume> <pages> pages 11-24. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: In this case a fix-up operation must be performed. This method will work if the cost of a fix-up is much less than the cost of resimulating the interval. Variations on this idea are found in <ref> [2] </ref>, and [56]. 4.3 Future Directions Time offers another dimension in which we may seek performance gains through parallelism. However, as yet any implementation observed to actually achieve performance gains relies very heavily on the specifics of the problem being simulated.
Reference: [3] <author> R. Ayani. </author> <title> A parallel simulation scheme based on the distance between objects. </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <volume> 21(2) </volume> <pages> 113-118, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: These protocols typically compute, distribute and are controlled by global system information. In this they reflect a philosophical shift away from the roots of parallel simulation in asynchronous distributed system theory. The algorithms studied in <ref> [85, 14, 75, 3, 87, 32] </ref> all compute a minimum time defining a time beyond which a processor will not venture until the next window "phase". Typically, this calculation involves lookahead of some kind.
Reference: [4] <author> R. Baccelli and Miguel Canales. </author> <title> Parallel simulation of stochastic petri nets using recurrence equations. </title> <booktitle> In Proceedings of the 1992 SIGMETRICS Conference, </booktitle> <pages> pages 257-258, </pages> <address> Newport, Rhode Island, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: This same basic idea can be extended in a number of ways, including networks of feed-forward queues [39], and certain classes of timed Petri nets <ref> [4] </ref>. The remarkable thing about this approach is that the degree of parallelism we may exploit is limited only by the size of the parallel machine and its memory. 14 The class of recurrence equations that yield directly to this approach is actually quite constrained.
Reference: [5] <author> D. Ball and S. Hoyt. </author> <title> The adaptive Time-Warp concurrency control algorithm. </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <volume> 22(1) </volume> <pages> 174-177, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Once all processors have synchronized at time t (which is itself a non-trivial problem addressed in [68]), a new window [t + ; t + 2] is simulated. This basic proposal is found originally in [85], with variations appearing in [90] and <ref> [5] </ref>. A similar proposal to extend constrained optimistism to the Bounded-Lag protocol is found in [60]. 2.3 Protocols Based on Windows One emerging theme in protocol research is to study protocols that constrain all concurrent simulation activity to be within some window of global synchronization time.
Reference: [6] <author> S. Bellenot. </author> <title> Global virtual time algorithms. </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <volume> 22(1) </volume> <pages> 122-127, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: This approach has some similarity to ring-based algorithms for detecting deadlock [65]. Bellenot uses a statically defined tree to to initiate, compute, and disseminate GVT values <ref> [6] </ref>. Reynolds also uses a tree structure to compute GVT in his hardware synchronization network, described earlier [64]. 7.2 Incremental and Infrequent State Saving Nearly all Time Warp based memory management schemes use fossil collection to reclaim state.
Reference: [7] <author> S. Bellenot. </author> <title> State skipping performance with the Time Warp operating system. </title> <booktitle> In 6 th Workshop on Parallel and Distributed Simulation, </booktitle> <volume> volume 24, </volume> <pages> pages 53-64. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> Jan. </month> <year> 1992. </year>
Reference-contexts: Preiss, MacIntyre, and Loucks [80] and Bellenot <ref> [7] </ref> validate Lin's results experimentally. Bellenot also observes that benefits in reducing state saving frequency diminish or become liabilities as the number of processors is increased. Finally, it might be noted that infrequent state saving economizes on storage for state vectors, but at the expense of storage for event messages.
Reference: [8] <author> B. Berkman and R. Ayani. </author> <title> Parallel simulation of multistage interconnection networks on an SIMD computer. </title> <booktitle> In Advances in Parallel and Distributed Simulation, </booktitle> <volume> volume 23, </volume> <pages> pages 133-140. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: Successful window-based SIMD simulation of a switching network is reported in <ref> [8] </ref>, and of a circuit-switched communication network in [32]. 2.4 Application Specific Protocols It is frequently the case that the importance of an application justifies tailoring a protocol to its special requirements and characteristics.
Reference: [9] <author> J. V. Briner. </author> <title> Parallel mixed-level simulation of digital circuits using virtual time. </title> <type> Ph. D. Thesis, </type> <institution> Duke University, Durham, </institution> <address> N. C., </address> <year> 1990. </year>
Reference-contexts: They report performance improvements of up to 25% on eight processors over an algorithm based on selecting random partitions. One would expect larger improvements in performance with more processors because communication overheads then become more significant. 20 Davoren [19] and Briner <ref> [9] </ref> also examine static partitioning algorithms for digital logic simulation. Da--voren bases his work on the Chandy/Misra/Bryant null message algorithm. He constructs a locality tree that is based on the hierarchical design of the circuit through different levels of abstraction (transistors, gates, multiplexers, etc.). <p> Nevertheless, Briner uses incremental state saving in an implementation of Time Warp for logic simulations, and reports state saving overheads of only 20% for transistor level simulation, and 60% for gate level simulation <ref> [9] </ref>. An alternative approach is to save entire state vectors, but reduce the frequency of state saving.
Reference: [10] <author> R. E. Bryant. </author> <title> Simulation of packet communication architecture computer systems. </title> <institution> MIT-LCS-TR-188, Massachusetts Institute of Technology, </institution> <year> 1977. </year>
Reference-contexts: Over the course of 15 years a profusion of new protocols have been proposed; we cannot but touch upon a few of the new ones here. Our intention is to give examples illustrating general trends in protocol research|enhancements to classical Chandy-Misra-Byrant (CMB) style protocols <ref> [16, 10] </ref>, enhancements to Time Warp [43], and new, synchronous protocols. 1 Before discussing the new directions, let us briefly revisit the synchronization problem and the classical approaches to it. Consider the network of four queues illustrated in fig. 1 (a).
Reference: [11] <author> C. A. Buzzell, M. J. Robb, and R. M. Fujimoto. </author> <title> Modular VME rollback hardware for Time Warp. </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <volume> 22(1) </volume> <pages> 153-156, </pages> <month> January </month> <year> 1990. </year> <month> 29 </month>
Reference-contexts: This approach minimizes the amount of copying that is required. Simulations indicate that state saving overhead can be reduced to only a few percent of the computation. Special caches are used to improve performance. A simplified prototype implementation of the rollback chip has been developed in the commercial sector <ref> [11] </ref>. Also, the hardware design of the rollback chip has been 18 verified using formal techniques [38]. 5.3 Global Synchronization Networks One of the reasons protocols for parallel simulation are nontrivial is the fact that critical synchronization information is distributed across the multiprocessor system.
Reference: [12] <author> W. Cai and S. J. Turner. </author> <title> An algorithm for distributed discrete-event simulation the "carrier null mes-sage" approach. </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <volume> 22(1) </volume> <pages> 3-8, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: If it could then learn that no other queue 2 as are link times|the time-stamp on the last message to cross a communication link . will send a job prior to time 3, it can then simulate the departure. This observation is explored in <ref> [12] </ref>, the "Carrier Null Message" approach. In standard CMB algorithms null messages propagate through a system| the result of receiving a null message is usually to send a slew of others.
Reference: [13] <author> K. M. Chandy. </author> <title> A survey of analytic models of rollback and recovery strategies. </title> <journal> IEEE Computer, </journal> <volume> 8(5) </volume> <pages> 40-47, </pages> <month> May </month> <year> 1975. </year>
Reference-contexts: This tradeoff suggests that there may be an optimal state saving frequency that balances state saving overhead and recomputation costs. This question has been studied in the context of fault tolerant computation, e.g., see <ref> [13, 33] </ref>.
Reference: [14] <author> K. M. Chandy and R. Sherman. </author> <title> The conditional event approach to distributed simulation. </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <volume> 21(2) </volume> <pages> 93-99, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: These protocols typically compute, distribute and are controlled by global system information. In this they reflect a philosophical shift away from the roots of parallel simulation in asynchronous distributed system theory. The algorithms studied in <ref> [85, 14, 75, 3, 87, 32] </ref> all compute a minimum time defining a time beyond which a processor will not venture until the next window "phase". Typically, this calculation involves lookahead of some kind.
Reference: [15] <author> K. M. Chandy and R. Sherman. </author> <title> Space, time, and simulation. </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <volume> 21(2) </volume> <pages> 53-57, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: It has recently been recognized that parallelism can also be found in time|when the behavior of a single object at different points in time can be concurrently simulated. Early recognition of this fact is found in <ref> [15] </ref>, where the authors observe that simulations are fixed-point computations, and as such can be executed 13 as asynchronous-update computations.
Reference: [16] <author> K.M. Chandy and J. Misra. </author> <title> Distributed simulation: A case study in design and verification of distributed programs. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> 5(5) </volume> <pages> 440-452, </pages> <month> September </month> <year> 1979. </year>
Reference-contexts: Over the course of 15 years a profusion of new protocols have been proposed; we cannot but touch upon a few of the new ones here. Our intention is to give examples illustrating general trends in protocol research|enhancements to classical Chandy-Misra-Byrant (CMB) style protocols <ref> [16, 10] </ref>, enhancements to Time Warp [43], and new, synchronous protocols. 1 Before discussing the new directions, let us briefly revisit the synchronization problem and the classical approaches to it. Consider the network of four queues illustrated in fig. 1 (a).
Reference: [17] <author> A. I. </author> <title> Concepcion. A hierarchical computer architecture for distributed simulation. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-38(2):311-319, </volume> <month> February </month> <year> 1989. </year>
Reference-contexts: The machine architecture itself is a network of processors, with some processors dedicated to performing specific functions, e.g., coordination of process execution. Details of the hardware organization are sketchy, however. Concepcion describes an for architecture for discrete event simulation called the hierarchical multibus multiprocessor architecture (HM 2 A) <ref> [17] </ref>. This architecture is motivated by a methodology that is proposed for constructing hierarchical, modular, simulation models which are then mapped to the multiprocessor. The machine structure is a tree of clusters where each cluster includes a collection of "slave" processors (each with local memory) connected by a bus.
Reference: [18] <author> S. R. Das and R. M. Fujimoto. </author> <title> A performance study of the cancelback protocol for Time Warp. </title> <type> Technical Report GIT-CC-92/50, </type> <institution> College of Computing, Georgia Institute of Technology, Atlanta, G A, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: Warp augmented with cancelback. 27 Further, an experimental study has examined the performance/memory tradeoff using several non-homogeneous workloads, and specifically, workloads designed to have some number of overly optimistic processes that advance, more or less unthrottled, into the simulated future, constrained only by the amount of memory in the system <ref> [18] </ref>. This provides a clear stress case for any Time Warp system.
Reference: [19] <author> M. Davoren. </author> <title> A structural mapping for parallel digital logic simulation. </title> <booktitle> In Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <volume> volume 21, </volume> <pages> pages 179-182. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> March </month> <year> 1989. </year>
Reference-contexts: They report performance improvements of up to 25% on eight processors over an algorithm based on selecting random partitions. One would expect larger improvements in performance with more processors because communication overheads then become more significant. 20 Davoren <ref> [19] </ref> and Briner [9] also examine static partitioning algorithms for digital logic simulation. Da--voren bases his work on the Chandy/Misra/Bryant null message algorithm. He constructs a locality tree that is based on the hierarchical design of the circuit through different levels of abstraction (transistors, gates, multiplexers, etc.).
Reference: [20] <author> E. DeBenedictis, S. Ghosh, and M.-L. Yu. </author> <title> A novel algorithm for discrete-event simulation. </title> <journal> Computer, </journal> <volume> 24(6) </volume> <pages> 21-33, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: VLSI simulation is notorious for its computational demands, the significance of successful parallelization would be large. Standard CMB and Time Warp approaches have been attempted [86], [63], with only mixed results. Recognizing that feedback loops pose one of the hardest problems for a conservative synchronization algorithm, <ref> [20] </ref> propose an approach where the network to be simulated is transformed into another (larger) one containing no feedback loops. This algorithm is tested on a latch constructed from two cross-coupled NAND gates. Another important class of simulation models are continuous time Markov chains (CTMC).
Reference: [21] <author> P.M. Dickens. </author> <title> Performance Analysis of Parallel Simulations. </title> <type> PhD thesis, </type> <institution> University of Virginia, </institution> <month> De-cember </month> <year> 1992. </year>
Reference-contexts: Another effort analytically examines the cost of widening the conservative window defined above somewhat, thereby finding more events to execute in parallel, but also suffering the risk of being rolled back <ref> [21] </ref>. Analysis of the extension shows that the window construct prevents rollbacks from cascading very far. Furthermore if state-saving costs are not large, the benefit of extending the window exceeds the costs, and better performance than the conservative window scheme may be achieved.
Reference: [22] <author> S. Eick, A. Greenberg, B. Lubachevsky, and A. Weiss. </author> <title> Synchronous relaxation for parallel simulations with applications to circuit-switched networks. </title> <booktitle> In Advances in Parallel and Distributed Simulation, </booktitle> <volume> volume 23, </volume> <pages> pages 151-162. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: Numerical solution shows excellent agreement both with empirical results, and with the values predicted by the earlier model. Finally, an analysis of synchronous relaxation is given in <ref> [22] </ref>. Convergence rate is always (or ought to be) the primary issue with any relaxation algorithm.
Reference: [23] <author> R. Felderman and L. Kleinrock. </author> <title> An upper bound on the improvement of asynchronous versus sychronous distributed processing. </title> <booktitle> In Distributed Simulation, </booktitle> <volume> volume 22, </volume> <pages> pages 131-136. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> Jan. </month> <year> 1990. </year>
Reference-contexts: The remainder of the section examines different topical areas of recent analytic work. 3.1 Synchronous vs. Asynchronous A significant body of work is devoted to comparing different synchronization algorithms. In <ref> [23] </ref> it is shown that the average performance difference between synchronous time-stepping and an optimistic asynchronous algorithm such as Time Warp is no more than a factor of O (log P ), P being the number of processors. The derivation of this result is straightforward.
Reference: [24] <author> R. Felderman and L. Kleinrock. </author> <title> Bounds and approximations for self-initiating distributed simulation without lookhead. </title> <journal> ACM Trans. on Modeling and Computer Simulation, </journal> <volume> 1(4), </volume> <month> October </month> <year> 1991. </year>
Reference-contexts: A similar analysis gives the upper bound for the conservative method with lookahead. Without lookahead the conservative method achieves a processor utilization of 1=P |serial processing|which demonstrates its utter reliance on lookahead to achieve good performance. An interesting point of comparison is developed in <ref> [24] </ref>, where the distributional assumptions concerning simulation time advance and per-event execution time are exactly reversed|an event is assumed to require an exponential processing time (with mean 1), but advances simulation time by a deterministic one unit. <p> Furthermore, the bounds become close to observed simulated rates as k grows. The only difference between the models in [73] and <ref> [24] </ref> are distributional, and yet the results are very different. Both analyses look at how GVT advances; the difference in results derive immediately from the stochastic component of GVT advance.
Reference: [25] <author> R. Felderman and L. Kleinrock. </author> <title> Two processor Time Warp analysis: Some results on a unifying approach. </title> <booktitle> In Advances in Parallel and Distributed Simulation, </booktitle> <volume> volume 23, </volume> <pages> pages 3-10. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> Jan. </month> <year> 1991. </year> <month> 30 </month>
Reference-contexts: The associated discrete-time Markov chain has but two states. Transition probabilities follow immediately from the message probabilities. A more complex two processor model is analyzed in <ref> [25] </ref>. Here one assumes that a processor takes only integer-valued time-stamps, and that upon executing an event (assumed to require a deterministic 1 tick) it advances its local clock by a random geometrically distributed amount.
Reference: [26] <author> M. A. Franklin, D. F. Wann, and K. F. Wong. </author> <title> Parallel machines and algorithms for discrete-event simulation. </title> <booktitle> Proceedings of the 1984 International Conference on Parallel Processing, </booktitle> <pages> pages 449-458, </pages> <month> August </month> <year> 1984. </year>
Reference-contexts: Machines have been developed for simulation of logic circuits (e.g., see <ref> [26] </ref> for a survey of approaches), however these usually do not allow concurrent execution of events containing different time-stamps.
Reference: [27] <author> R. M. Fujimoto. </author> <title> Time Warp on a shared memory multiprocessor. </title> <journal> Transactions of the Society for Computer Simulation, </journal> <volume> 6(3) </volume> <pages> 211-239, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: The machine, however, is envisioned to be a general purpose parallel processor based on optimistic synchronization. 5.2 Hardware Support for State Saving in Time Warp In Time Warp, processes must periodically checkpoint their state in case a rollback later occurs. State saving overheads can incur a significant overhead <ref> [27] </ref>. One can alleviate this overhead to some extent by reducing the frequency of checkpointing, however, analytic and experimental data suggest that the optimal checkpoint interval may be frequent (e.g., every few events) [80].
Reference: [28] <author> R. M. Fujimoto. </author> <title> The virtual time machine. </title> <booktitle> International Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 199-208, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: In this way, simulation computations propagate up and down the tree, activating simulation models at different levels of hierarchy as needed. 17 A third machine proposal is the Virtual Time Machine <ref> [28, 35] </ref>. Unlike the above machine organizations, this machine is based on optimistic synchronization. The machine is a shared memory multiprocessor with a special type of memory system called space-time memory, and a hardware implemented rollback mechanism. The most interesting aspect of the machine architecture is its memory system.
Reference: [29] <author> R. M. Fujimoto. </author> <title> Parallel discrete event simulation. </title> <journal> Communications of the ACM, </journal> <volume> 33(10) </volume> <pages> 30-53, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: A drawback with this approach, however, is that rollbacks become more expensive because the state vector must be reconstructed from the incremental changes. This is problematic because as illustrated in <ref> [29] </ref>, the computation is more prone to unstable execution if rollback costs are high. Nevertheless, Briner uses incremental state saving in an implementation of Time Warp for logic simulations, and reports state saving overheads of only 20% for transistor level simulation, and 60% for gate level simulation [9].
Reference: [30] <author> R. M. Fujimoto, J. Tsai, and G. Gopalakrishnan. </author> <title> Design and evaluation of the rollback chip: Special purpose hardware for Time Warp. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(1) </volume> <pages> 68-82, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: Fujimoto, et al. propose a component called the rollback chip that provides hardware support for state saving and rollback in Time Warp <ref> [30] </ref>. This component was the forerunner to the space-time memory system described above. The rollback chip can be viewed as a special memory management unit. A process may issue a "mark" operation to indicate that the state of a data segment must be preserved in case a rollback later occurs.
Reference: [31] <author> A. Gafni. </author> <title> Rollback mechanisms for optimistic distributed simulation systems. </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <volume> 19(3) </volume> <pages> 61-67, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: The message with the largest send-time-stamp is returned. Gafni proposes a protocol that utilizes message sendback as well as other mechanisms to reclaim storage used by state vectors and messages stored in the output queue when a process finds that its local memory is exhausted <ref> [31] </ref>. More recently, Jefferson has proposed an alternative approach called cancelback [44]. While Gafni's algorithm will only discard state in the process that ran out of memory, cancelback allows state in any process to be reclaimed. Messages containing high send-time-stamps are sent back to reclaim storage allocated to messages.
Reference: [32] <author> B. Gaujal, A. Greenberg, and D. Nicol. </author> <title> A sweep algorithm for massively parallel simulation of circuit-switched networks. </title> <type> Technical Report ICASE Technical Report 92-30, </type> <institution> ICASE, </institution> <month> July </month> <year> 1992. </year> <note> To appear in Journal of Parallel and Distributed Computing. </note>
Reference-contexts: These protocols typically compute, distribute and are controlled by global system information. In this they reflect a philosophical shift away from the roots of parallel simulation in asynchronous distributed system theory. The algorithms studied in <ref> [85, 14, 75, 3, 87, 32] </ref> all compute a minimum time defining a time beyond which a processor will not venture until the next window "phase". Typically, this calculation involves lookahead of some kind. <p> Successful window-based SIMD simulation of a switching network is reported in [8], and of a circuit-switched communication network in <ref> [32] </ref>. 2.4 Application Specific Protocols It is frequently the case that the importance of an application justifies tailoring a protocol to its special requirements and characteristics. This approach often delivers performance advantages over "general" protocols, which may suffer extra overheads to support circumstances rarely encountered in the application. <p> However, even in more general cases there is often some utility in viewing the simulation as the solution of recurrence relations, because one can solve the equations iteratively. The following approach, called "sweeping" in <ref> [32] </ref> shows how. Consider a communication link that is able to carry K calls simultaneously. If a new call arrives at an instant when the trunk is saturated, the call is lost.
Reference: [33] <author> E. Gelenbe. </author> <title> On the optimum checkpoint interval. </title> <journal> Journal of the ACM, </journal> <volume> 26(4) </volume> <pages> 259-270, </pages> <month> April </month> <year> 1979. </year>
Reference-contexts: This tradeoff suggests that there may be an optimal state saving frequency that balances state saving overhead and recomputation costs. This question has been studied in the context of fault tolerant computation, e.g., see <ref> [13, 33] </ref>.
Reference: [34] <author> P. I. Georgiadis, M. P. Papazoglou, and D. G. Maritsas. </author> <title> Towards a parallel simula machine. </title> <booktitle> Proceedings of the 8th Annual Symposium on Computer Architecture, </booktitle> <volume> 9(3) </volume> <pages> 263-278, </pages> <month> May </month> <year> 1981. </year>
Reference-contexts: For example, Georgiadis et al., proposed a multiprocessor implementation for Simula programs in the early 1980's <ref> [34] </ref>. There, a special purpose parallel simulation engine was envisioned that utilizes a controller processor to manage the execution of the parallel simulator, and determine which processes are available for execution.
Reference: [35] <author> K. Ghosh and R. M. Fujimoto. </author> <title> Parallel discrete event simulation using space-time memory. </title> <booktitle> Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <volume> Vol. 3, 3 </volume> <pages> 201-208, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: In this way, simulation computations propagate up and down the tree, activating simulation models at different levels of hierarchy as needed. 17 A third machine proposal is the Virtual Time Machine <ref> [28, 35] </ref>. Unlike the above machine organizations, this machine is based on optimistic synchronization. The machine is a shared memory multiprocessor with a special type of memory system called space-time memory, and a hardware implemented rollback mechanism. The most interesting aspect of the machine architecture is its memory system.
Reference: [36] <author> D.W. Glazer. </author> <title> Load Balancing Parallel Discrete-Event Simulations. </title> <type> PhD thesis, </type> <institution> McGill University, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: This effectively treats time spent executing wrong computations as idle time. Based on this metric, they propose a strategy that migrates processes from processors with high effective utilization to those with low utilization. An algorithm that is similar in spirit is proposed in <ref> [36] </ref>. This algorithm allocates virtual time-slices to processes, based on their observed rate of advancing the local simulation clock.
Reference: [37] <author> A. Goldberg. </author> <title> Virtual time synchronization of replicated processes. </title> <booktitle> In 6 th Workshop on Parallel and Distributed Simulation, </booktitle> <volume> volume 24, </volume> <pages> pages 107-116. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> Jan. </month> <year> 1992. </year>
Reference-contexts: Reiher and Jefferson demonstrate that phase splitting and the effective utilization metric are useful to dynamically balance the load in simulations of a communication network, a system of colliding pucks, and a combat models [82]. Goldberg describes an interesting approach to the load distribution problem <ref> [37] </ref>. If a process becomes a bottleneck, it is replicated to form two or more identical copies, each able to execute concurrently with the others. Read requests are sent to one replica, while write requests are sent to all of them.
Reference: [38] <author> G. Gopalakrishnan and R. M. Fujimoto. </author> <title> Design and verification of the rollback chip using HOP: A case study of formal methods applied to hardware design. </title> <type> Technical Report UUCS-91-015, </type> <institution> Dept of Computer Science, Univ. of Utah, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: Special caches are used to improve performance. A simplified prototype implementation of the rollback chip has been developed in the commercial sector [11]. Also, the hardware design of the rollback chip has been 18 verified using formal techniques <ref> [38] </ref>. 5.3 Global Synchronization Networks One of the reasons protocols for parallel simulation are nontrivial is the fact that critical synchronization information is distributed across the multiprocessor system. For instance, in conservative protocols, information indicating which events can be safely processed may be distributed across other processors.
Reference: [39] <author> A.G. Greenberg, B.D. Lubachevsky, and I. Mitrani. </author> <title> Algorithms for unboundedly parallel simulations. </title> <journal> ACM Trans. on Comp. Systems, </journal> <volume> 9(3) </volume> <pages> 201-221, </pages> <year> 1991. </year>
Reference-contexts: Early recognition of this fact is found in [15], where the authors observe that simulations are fixed-point computations, and as such can be executed 13 as asynchronous-update computations. Practical exploitation of time parallelism was first established by work reported in <ref> [39] </ref>, where it was shown how certain queueing systems can be expressed as systems of recurrence relations (in the time domain), which can be solved using standard parallel prefix methods on massively parallel machines. <p> This same basic idea can be extended in a number of ways, including networks of feed-forward queues <ref> [39] </ref>, and certain classes of timed Petri nets [4].
Reference: [40] <author> A. Gupta, I. F. Akyildiz, and R. M. Fujimoto. </author> <title> Performance analysis of Time Warp with multiple homogenous processors. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(10) </volume> <pages> 1013-1027, </pages> <month> October </month> <year> 1991. </year> <month> 31 </month>
Reference-contexts: Solution of the chain's equilibrium probabilities is non-trivial, but can be done exactly. Markov models of Time Warp on multiple processors have also been developed. The model in <ref> [40] </ref> assumes * unlimited memory at each processor; * message processing is comprised of advancing the simulation clock by an exponential amount, and by sending one message to another processor, chosen uniformly at random; * the time required to execute an event is exponentially distributed.
Reference: [41] <author> P. Heidelberger and D. Nicol. </author> <title> Conservative parallel simuation of continuous time markov chains using uniformization. </title> <type> Technical Report IBM Research Report RC-16780, </type> <institution> IBM Research Division, </institution> <month> April </month> <year> 1991. </year> <note> To appear in IEEE Trans. on Parallel and Distributed Systems. </note>
Reference-contexts: At the completion of the holding time the CTMC makes a random transition into another state. The probability distribution of the transition also depends on s. CTMCs are very general constructs, and are often used to model complex computer systems and communication networks. In a series of papers <ref> [41, 70, 71] </ref> it is shown that the mathematical structure of CTMC models can be exploited for the purposes of synchronization. Using the notion of uniformization, it is possible to simulate a CTMC on a parallel machine in two phases.
Reference: [42] <author> P. Heidelberger and H. Stone. </author> <title> Parallel trace-driven cache simulation by time partitioning. </title> <type> Technical Report RC 15500, </type> <institution> IBM Research, </institution> <month> February </month> <year> 1990. </year>
Reference-contexts: Convergence was rapid on a 16K PE SIMD architecture; typically thousands of calls were classified using only a handful of sweep iterations. 4.2 Other Methods Time parallelism was also noticed in LRU trace-driven cache simulations <ref> [42] </ref> for MIMD architectures (each memory reference constitutes an event); this observation was extended in [69] for more general replacement policies, and SIMD machines. The latter approach also involves the parallel solution of recurrence equations, but in a less direct fashion than the methods described so far.
Reference: [43] <author> D. R. Jefferson. </author> <title> Virtual time. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(3) </volume> <pages> 404-425, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: Our intention is to give examples illustrating general trends in protocol research|enhancements to classical Chandy-Misra-Byrant (CMB) style protocols [16, 10], enhancements to Time Warp <ref> [43] </ref>, and new, synchronous protocols. 1 Before discussing the new directions, let us briefly revisit the synchronization problem and the classical approaches to it. Consider the network of four queues illustrated in fig. 1 (a). <p> In Time Warp, for instance, each process maintains past state vectors in its state queue, processed events in its input queue, and records of previously sent messages (anti-messages) in its output queue. A mechanism called fossil collection is provided to reclaim "old" history information that is no longer needed <ref> [43] </ref>. Fossil collection relies on the computation of a quantity called global virtual time (GVT), which will be defined momentarily. Storage used by message buffers and snapshots of process state that are older than (GVT) can be reclaimed and used for other purposes. <p> Let us digress for a moment to discuss the computation of GVT. GVT represents a lower bound on the time-stamp of any future rollback. In Time Warp, as originally proposed in <ref> [43] </ref>, rollbacks only arise from receiving positive or negative messages in the past. Further, a process at simulated time T might produce a new (positive) message with time-stamp equal (or only slightly larger than, in systems that do not allow zero time-stamp increments) to T . <p> Several approaches have been developed to address this concern. The basic idea behind these mechanisms is to roll back overly optimistic computations, and reclaim the memory they use for other purposes. Jefferson first proposed a mechanism called message sendback to achieve this effect <ref> [43] </ref>. In message sendback, the Time Warp executive may return a message to its original sender without ever processing it, and reclaim the memory used by the message. <p> Only messages with send-time-stamp greater than GVT can be returned, since otherwise, a rollback beyond GVT might result. Jefferson's original proposal invokes message sendback when a process receives a message, but finds that there is no memory available to store it <ref> [43] </ref>. The message with the largest send-time-stamp is returned. Gafni proposes a protocol that utilizes message sendback as well as other mechanisms to reclaim storage used by state vectors and messages stored in the output queue when a process finds that its local memory is exhausted [31].
Reference: [44] <author> D. R. Jefferson. </author> <title> Virtual time II: Storage management in distributed simulation. </title> <booktitle> Proceedings of the Ninth Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 75-89, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Gafni proposes a protocol that utilizes message sendback as well as other mechanisms to reclaim storage used by state vectors and messages stored in the output queue when a process finds that its local memory is exhausted [31]. More recently, Jefferson has proposed an alternative approach called cancelback <ref> [44] </ref>. While Gafni's algorithm will only discard state in the process that ran out of memory, cancelback allows state in any process to be reclaimed. Messages containing high send-time-stamps are sent back to reclaim storage allocated to messages. <p> The second parameter, the salvage parameter that was defined earlier, is a control for tuning performance. It is interesting to note that while Time Warp with cancelback or artificial rollback are storage optimal, certain conservative simulation protocols are not. Lin et al. [54] and Jefferson <ref> [44] </ref> show that the Chandy/Misra/Bryant algorithm may require O (nk) space for parallel simulations executing on n processors where the sequential simulation requires only O (n + k) space.
Reference: [45] <author> D. S. Johnson, C. R. Aragon, L. A. McGeoch, and C. Shevon. </author> <title> Optimization by simulated annealing: An experimental evaluation: Part I, graph partitioning. </title> <journal> Operations Research, </journal> <pages> pages 865-892, </pages> <year> 1989. </year>
Reference-contexts: Load balancing has been widely studied for general (i.e., not necessarily simulation) parallel and distributed computation. Many of the techniques that have been proposed, e.g., simulated annealing <ref> [45] </ref>, distributed drafting [67], pressure based load migration [48], among others, can be applied to parallel simulation programs. 6.2 Static Load Balancing Techniques Early work on static load balancing is found in [76, 74].
Reference: [46] <author> S. A. Kravitz and B. D. Ackland. </author> <title> Static vs. dynamic partitioning of circuits for a MOS timing simulator on a message-based multiprocessor. </title> <booktitle> In Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <volume> volume 19, </volume> <pages> pages 136-140. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> July </month> <year> 1988. </year>
Reference-contexts: The modified bisection algorithm only yields a modest improvement over the original algorithm. He reports that hand partitioning based on the hierarchical structure of the computation (such as that proposed by Davoren) yields up to three times better performance compared to the bisection algorithms. Kravitz and Ackland <ref> [46] </ref> also examine some simple static partitioning schemes for circuit simulations. Based on empirical studies, they conclude that these approaches yield reasonably good results, and the overhead for dynamic repartitioning does not justify the potential performance gain. Their work is based on time-stepped simulations.
Reference: [47] <author> D. Kumar and S. Harous. </author> <title> An approach towards distributed simulation of timed petri nets. </title> <booktitle> In Proceedings of the 1990 Winter Simulation Conference, </booktitle> <pages> pages 428-435, </pages> <address> New Orleans, LA., </address> <month> December </month> <year> 1990. </year>
Reference-contexts: A final illustration of application dependent protocols occurs considering the simulation of Timed Petri 7 Nets (TPN). The semantics of a TPN simulation do not fit easily into the CMB world-view. As a consequence, extensions to the CMB protocol have been proposed in <ref> [47] </ref> and [89]. However, it is possible to simulate a TPN using a general windowing protocol, as shown in [72]. 2.5 Future Directions Synchronization will always be an interesting area of study.
Reference: [48] <author> F. Lin and R. M. Keller. </author> <title> The gradient model load balancing method. </title> <journal> IEEE Transactions on Software Engineering, </journal> <pages> pages 32-38, </pages> <month> January </month> <year> 1987. </year>
Reference-contexts: Load balancing has been widely studied for general (i.e., not necessarily simulation) parallel and distributed computation. Many of the techniques that have been proposed, e.g., simulated annealing [45], distributed drafting [67], pressure based load migration <ref> [48] </ref>, among others, can be applied to parallel simulation programs. 6.2 Static Load Balancing Techniques Early work on static load balancing is found in [76, 74].
Reference: [49] <author> Y.-B. Lin. </author> <title> Memory management algorithms for optimistic parallel simulation. </title> <booktitle> In 6 th Workshop on Parallel and Distributed Simulation, volume 24. SCS Simulation Series, </booktitle> <month> Jan. </month> <year> 1992. </year>
Reference-contexts: For cancelback, GVT is defined as the minimum among (1) the local clocks of the processes in the simulation, and (2) the send-time-stamp of all messages in transit <ref> [49] </ref>. The artificial rollback protocol, described below, also uses this definition of GVT. Another approach, proposed by Lin, is the artificial rollback algorithm [49]. When storage is exhausted and fossil collection fails to reclaim additional memory, processes are rolled back to recover memory. <p> cancelback, GVT is defined as the minimum among (1) the local clocks of the processes in the simulation, and (2) the send-time-stamp of all messages in transit <ref> [49] </ref>. The artificial rollback protocol, described below, also uses this definition of GVT. Another approach, proposed by Lin, is the artificial rollback algorithm [49]. When storage is exhausted and fossil collection fails to reclaim additional memory, processes are rolled back to recover memory. The 26 process that is the furthest ahead is rolled back to the time of the second most advanced process.
Reference: [50] <author> Y.-B. Lin and E. Lazowska. </author> <title> Optimality considerations of "Time Warp" parallel simulation. </title> <booktitle> In Distributed Simulation, </booktitle> <volume> volume 22, </volume> <pages> pages 29-34. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> Jan. </month> <year> 1990. </year>
Reference-contexts: While simple, this model serves 8 to show that in a statistical sense, one ought to limit one's expectations of asynchronous vs. synchronous methods. 3.2 Optimality of Optimism Conditions for the optimality of Time Warp (in the absence of overhead costs) are demonstrated in <ref> [50] </ref>. At a glance, this result seems intuitive, because Time Warp need never block.
Reference: [51] <author> Y-B. Lin and E. D. Lazowska. </author> <title> Determining the global virtual time in a distributed simulation. </title> <type> Technical Report 90-01-02, </type> <institution> Dept. of Computer Science, University of Washington, </institution> <address> Seattle, Washington, </address> <year> 1989. </year>
Reference-contexts: Lin and Lazowska propose a scheme that avoids acknowledgements by having each process communicate with the other processes to which it communicates when it begins a GVT computation in order to identify any transient messages. Details of their algorithm are described in <ref> [51] </ref>. Race conditions may arise because the individual processors receive the "start GVT computation" signal at different points in time. For example, processor 1 might compute its local minima to be 100.
Reference: [52] <author> Y-B. Lin and E. D. Lazowska. </author> <title> Reducing the state saving overhead for Time Warp parallel simulation. </title> <type> Technical Report 90-02-03, </type> <institution> Dept. of Computer Science, University of Washington, </institution> <address> Seattle, Washington, </address> <month> February </month> <year> 1990. </year>
Reference-contexts: this tradeoff in the context of Time Warp programs, and show that an error in overestimating the state saving frequency is more costly than an equal magnitude error in underestimating the frequency, i.e., it is better to err on the side of less-frequent-than-optimal state saving in order to maximize performance <ref> [52] </ref>.
Reference: [53] <author> Y-B. Lin and E. D. Lazowska. </author> <title> A time-division algorithm for parallel simulation. </title> <journal> ACM Transactions on Modeling and Computer Simulation, </journal> <volume> 1(1) </volume> <pages> 73-83, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Lin et al. [54] and Jefferson [44] show that the Chandy/Misra/Bryant algorithm may require O (nk) space for parallel simulations executing on n processors where the sequential simulation requires only O (n + k) space. Further, Lin and Preiss <ref> [53] </ref> report the existence of simulations where Chandy/Misra/Bryant have exponential space complexity, and thus utilize much more storage than the sequential simulation. On the other hand, they also indicate that this algorithm may sometimes use less storage than that which is required by the sequential simulator. <p> On the other hand, they also indicate that this algorithm may sometimes use less storage than that which is required by the sequential simulator. Time Warp with cancelback or artificial rollback always requires at least this much <ref> [53] </ref>. Of course, a Time Warp program will run very slowly if one only provides the absolute minimum amount of memory. The question of Time Warp performance as the amount of memory is varied has been studied [1].
Reference: [54] <author> Y-B. Lin, E. D. Lazowska, and J-L. Baer. </author> <title> Conservative parallel simulation for systems with no lookahead prediction. </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <volume> 22(1) </volume> <pages> 144-149, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: The second parameter, the salvage parameter that was defined earlier, is a control for tuning performance. It is interesting to note that while Time Warp with cancelback or artificial rollback are storage optimal, certain conservative simulation protocols are not. Lin et al. <ref> [54] </ref> and Jefferson [44] show that the Chandy/Misra/Bryant algorithm may require O (nk) space for parallel simulations executing on n processors where the sequential simulation requires only O (n + k) space.
Reference: [55] <author> Y.-B. Lin and E.D. Lazowska. </author> <title> A study of Time Warp rollback mechanisms. </title> <journal> ACM Trans. on Modeling and Computer Simulation, </journal> <volume> 1(1) </volume> <pages> 51-72, </pages> <month> January </month> <year> 1991. </year> <month> 32 </month>
Reference-contexts: Some recent analytic work attempts to explain this behavior. Lazy and aggressive cancellation are examined in <ref> [55] </ref>. Equations for the probability of rollback are derived for some simple queueing networks, as is the probability that a rolled back message is actually correct. This latter probability assesses the utility of lazy cancellation.
Reference: [56] <author> Y.-B. Lin and E.D. Lazowska. </author> <title> A time-division algorithm for parallel simualtion. </title> <journal> ACM Trans. on Modeling and Computer Simulation, </journal> <volume> 1(1) </volume> <pages> 73-83, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: In this case a fix-up operation must be performed. This method will work if the cost of a fix-up is much less than the cost of resimulating the interval. Variations on this idea are found in [2], and <ref> [56] </ref>. 4.3 Future Directions Time offers another dimension in which we may seek performance gains through parallelism. However, as yet any implementation observed to actually achieve performance gains relies very heavily on the specifics of the problem being simulated.
Reference: [57] <author> R. Lipton and D. Mizell. </author> <title> Time Warp vs. Chandy-Misra: A worst-case comparison. </title> <booktitle> In Distributed Simulation, </booktitle> <volume> volume 22, </volume> <pages> pages 137-143. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> Jan. </month> <year> 1990. </year>
Reference-contexts: This causes the effective critical path to lengthen. Again, even though the model is simple and the assumption of zero-cost overhead is unrealistic, some insight is gained into the behavior of the protocols studied. In a similar vein an interesting asymmetry is demonstrated in <ref> [57] </ref>, with examples showing that Time Warp is capable of arbitrarily better performance than most conservative methods and a proof that the converse is not true.
Reference: [58] <author> B. Lubachevsky, A. Weiss, and A. Shwartz. </author> <title> An analysis of rollback-based simulation. </title> <journal> ACM Trans. on Modeling and Computer Simulation, </journal> <volume> 1(2) </volume> <pages> 154-192, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: This latter probability assesses the utility of lazy cancellation. A sophisticated model of rollback behavior based in the theory of branching processes is developed in <ref> [58] </ref>. The model assumes that the effect of processing an event is to generate a random number b of other events. This assumption essentially defines a branching process of event causality. One can view the progress of a simulation in terms of the growth of this tree.
Reference: [59] <author> B. D. Lubachevsky. </author> <title> Scalability of the bounded lag distributed discrete event simulation. </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <volume> 21(2) </volume> <pages> 100-107, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: The natural question to ask of such algorithms is whether windows tend to admit enough parallel events to be effective. This issue has been addressed for the very algorithm above, as well as for the Bounded Lag algorithm <ref> [59] </ref>. Both algorithms are scalable, which means that their performance characteristics do not degrade as the size of the problem and architecture simultaneously increase. Some insight into this phenomenon is gained if we suppose that a job's service time is always at least c &gt; 0.
Reference: [60] <author> B. D. Lubachevsky, A. Shwartz, and A. Weiss. </author> <title> Rollback sometimes works ... if filtered. </title> <booktitle> 1989 Winter Simulation Conference Proceedings, </booktitle> <pages> pages 630-639, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: This basic proposal is found originally in [85], with variations appearing in [90] and [5]. A similar proposal to extend constrained optimistism to the Bounded-Lag protocol is found in <ref> [60] </ref>. 2.3 Protocols Based on Windows One emerging theme in protocol research is to study protocols that constrain all concurrent simulation activity to be within some window of global synchronization time. These protocols typically compute, distribute and are controlled by global system information.
Reference: [61] <author> V. Madisetti, D. Hardaker, and R. Fujimoto. </author> <title> The mimdix operating system for parallel simulation. </title> <booktitle> In 6 th Workshop on Parallel and Distributed Simulation, </booktitle> <volume> volume 24, </volume> <pages> pages 65-74. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> Jan. </month> <year> 1992. </year>
Reference-contexts: One way to view this is as the parallelization of rollbacks that would otherwise occur serially. This idea finds expression in [62]. Another way of implementing this same basic idea is to build periodic|or random|preemptive rollbacks that occur independently of any activity in the simulation model <ref> [61] </ref>. The idea is to ensure that all processors are more-or-less synchronized in the same region of simulation time, with the hope that rollback cascades are less likely as a result. A related line of thought is to simply constrain Time Warp's optimism.
Reference: [62] <author> V. Madisetti, J. Walrand, and D. Messerschmitt. Wolf: </author> <title> A rollback algorithm for optimistic distributed simulation systems. </title> <booktitle> 1988 Winter Simulation Conference Proceedings, </booktitle> <pages> pages 296-305, </pages> <month> December </month> <year> 1988. </year>
Reference-contexts: For example, when one processor needs to rollback it may immediately issue rollback instructions to other processors who will likely have to roll back anyway as a result. One way to view this is as the parallelization of rollbacks that would otherwise occur serially. This idea finds expression in <ref> [62] </ref>. Another way of implementing this same basic idea is to build periodic|or random|preemptive rollbacks that occur independently of any activity in the simulation model [61].
Reference: [63] <author> J. Briner, Jr. </author> <title> Fast parallel simulation of digital systems. </title> <booktitle> In Advances in Parallel and Distributed Simulation, </booktitle> <volume> volume 23, </volume> <pages> pages 71-77. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: One such example is the simulation of digital logic networks. VLSI simulation is notorious for its computational demands, the significance of successful parallelization would be large. Standard CMB and Time Warp approaches have been attempted [86], <ref> [63] </ref>, with only mixed results. Recognizing that feedback loops pose one of the hardest problems for a conservative synchronization algorithm, [20] propose an approach where the network to be simulated is transformed into another (larger) one containing no feedback loops.
Reference: [64] <author> P. Reynolds, Jr. </author> <title> An efficient framework for parallel simulations. </title> <booktitle> In Advances in Parallel and Distributed Simulation, </booktitle> <volume> volume 23, </volume> <pages> pages 167-174. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: Similarly, optimistic protocols require information that is distributed across the system to compute global virtual time. Reynolds has proposed a hardware mechanism to rapidly collect, operate on, and disseminate synchronization information throughout a parallel simulation system <ref> [64, 78] </ref>. The hardware is configured as a binary tree, with a processor assigned to each node. <p> This approach has some similarity to ring-based algorithms for detecting deadlock [65]. Bellenot uses a statically defined tree to to initiate, compute, and disseminate GVT values [6]. Reynolds also uses a tree structure to compute GVT in his hardware synchronization network, described earlier <ref> [64] </ref>. 7.2 Incremental and Infrequent State Saving Nearly all Time Warp based memory management schemes use fossil collection to reclaim state. However, fossil collection is not, by itself, sufficient because the computation may still consume excessive amounts of memory.
Reference: [65] <author> J. Misra. </author> <title> Distributed-discrete event simulation. </title> <journal> ACM Computing Surveys, </journal> <volume> 18(1) </volume> <pages> 39-65, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: Preiss uses a token passing scheme where the processors making up the simulation are organized in a ring, and continually compute GVT as the token is passed from one processor to the next [81]. This approach has some similarity to ring-based algorithms for detecting deadlock <ref> [65] </ref>. Bellenot uses a statically defined tree to to initiate, compute, and disseminate GVT values [6].
Reference: [66] <author> B. Nandy and W. Loucks. </author> <title> An algorithm for partitioning and mapping conservative parallel simulation onto multicomputers. </title> <booktitle> In 6 th Workshop on Parallel and Distributed Simulation, </booktitle> <volume> volume 24, </volume> <pages> pages 139-146. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> Jan. </month> <year> 1992. </year>
Reference-contexts: The policy was empirically studied on a parallelized time-stepped combat model [77], where remapping may occur between the advancement and engagement phases of the simulation. Nandy and Loucks use an iterative, static load balancing algorithm for parallel simulation using the Chandy/Misra/Bryant synchronization protocol (null messages) <ref> [66] </ref>. The algorithm begins with an initial, random, partitioning of the task graph, and then continually evaluates possible movement of nodes (logical processes) from one partition to another.
Reference: [67] <author> L. M. Ni, C. W. Zu, and T. B. Gendreau. </author> <title> A distributed drafting algorithm for load balancing. </title> <journal> IEEE Transactions on Software Engineering, </journal> <month> October </month> <year> 1985. </year>
Reference-contexts: Load balancing has been widely studied for general (i.e., not necessarily simulation) parallel and distributed computation. Many of the techniques that have been proposed, e.g., simulated annealing [45], distributed drafting <ref> [67] </ref>, pressure based load migration [48], among others, can be applied to parallel simulation programs. 6.2 Static Load Balancing Techniques Early work on static load balancing is found in [76, 74].
Reference: [68] <author> D. Nicol. </author> <title> Optimistic barrier synchronization. </title> <type> Technical Report ICASE Report 91-34, </type> <institution> ICASE, </institution> <month> July </month> <year> 1992. </year> <note> To appear in Parallel Computing. </note>
Reference-contexts: Within a window [t; t + ] processors execute standard Time Warp, except that no event with a time-stamp greater than or equal to t + is executed. Once all processors have synchronized at time t (which is itself a non-trivial problem addressed in <ref> [68] </ref>), a new window [t + ; t + 2] is simulated. This basic proposal is found originally in [85], with variations appearing in [90] and [5]. <p> The above problem could be solved using a barrier synchronization to ensure that all simulation computations halt before the GVT computation is begun. In fact, Nicol has developed a barrier algorithm for optimistic computations that can effectively serve to compute GVT <ref> [68] </ref>. The processors agree to synchronize globally at some simulation time t. A processor enters the barrier once it has no events to process with time-stamps less than t, but rolls back out of the barrier if it later receives a message with time-stamp less than t.
Reference: [69] <author> D. Nicol, A. Greenberg, B. Lubachevsky, and S. Roy. </author> <title> Massively parallel algorithms for trace-driven cache simulation. </title> <booktitle> In 6 th Workshop on Parallel and Distributed Simulation, </booktitle> <volume> volume 24, </volume> <pages> pages 3-11. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> Jan. </month> <year> 1992. </year>
Reference-contexts: Convergence was rapid on a 16K PE SIMD architecture; typically thousands of calls were classified using only a handful of sweep iterations. 4.2 Other Methods Time parallelism was also noticed in LRU trace-driven cache simulations [42] for MIMD architectures (each memory reference constitutes an event); this observation was extended in <ref> [69] </ref> for more general replacement policies, and SIMD machines. The latter approach also involves the parallel solution of recurrence equations, but in a less direct fashion than the methods described so far.
Reference: [70] <author> D. Nicol and P. Heidelberger. </author> <title> Optimistic parallel simuation of continuous time markov chains using uniformization. </title> <type> Technical Report IBM Research Report RC-17932, </type> <institution> IBM Research Division, </institution> <month> April </month> <year> 1992. </year> <note> To appear in Journal of Parallel and Distributed Computing. 33 </note>
Reference-contexts: At the completion of the holding time the CTMC makes a random transition into another state. The probability distribution of the transition also depends on s. CTMCs are very general constructs, and are often used to model complex computer systems and communication networks. In a series of papers <ref> [41, 70, 71] </ref> it is shown that the mathematical structure of CTMC models can be exploited for the purposes of synchronization. Using the notion of uniformization, it is possible to simulate a CTMC on a parallel machine in two phases.
Reference: [71] <author> D. Nicol and P. Heidelberger. </author> <title> Parallel simuation of markovian queueing networks using adaptive uni--formization. </title> <booktitle> In Proceedings of the 1993 SIGMETRICS Conference, </booktitle> <pages> pages 135-145, </pages> <address> Santa Clara, Cali-fornia, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: At the completion of the holding time the CTMC makes a random transition into another state. The probability distribution of the transition also depends on s. CTMCs are very general constructs, and are often used to model complex computer systems and communication networks. In a series of papers <ref> [41, 70, 71] </ref> it is shown that the mathematical structure of CTMC models can be exploited for the purposes of synchronization. Using the notion of uniformization, it is possible to simulate a CTMC on a parallel machine in two phases.
Reference: [72] <author> D. Nicol and S. Roy. </author> <title> Parallel simulation of timed petri nets. </title> <booktitle> In Proceedings of the 1991 Winter Simulation Conference, </booktitle> <pages> pages 574-583, </pages> <address> Phoenix, Arizona, </address> <month> December </month> <year> 1991. </year>
Reference-contexts: The semantics of a TPN simulation do not fit easily into the CMB world-view. As a consequence, extensions to the CMB protocol have been proposed in [47] and [89]. However, it is possible to simulate a TPN using a general windowing protocol, as shown in <ref> [72] </ref>. 2.5 Future Directions Synchronization will always be an interesting area of study. However, the fact remains that a number of different approaches have been shown to work, albeit under varying circumstances and with varying degrees of success.
Reference: [73] <author> D. M. Nicol. </author> <title> Performance bounds on parallel self-initiating discrete-event simulations. </title> <journal> ACM Trans. on Modeling and Computer Simulation, </journal> <volume> 1(1) </volume> <pages> 24-50, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: A conservative windowing algorithm is compared with Time Warp in <ref> [73] </ref>. This analysis includes overheads for both methods, and captures the dependence of performance on lookahead. Not surprising, the results of the comparison depend on the magnitudes of the overhead costs. In this model each of P processors is assumed to always be busy. <p> Furthermore, the bounds become close to observed simulated rates as k grows. The only difference between the models in <ref> [73] </ref> and [24] are distributional, and yet the results are very different. Both analyses look at how GVT advances; the difference in results derive immediately from the stochastic component of GVT advance.
Reference: [74] <author> D.M. Nicol. </author> <title> The Automated Partitioning of Simulations for Parallel Execution. </title> <type> PhD thesis, </type> <institution> University of Virginia, </institution> <month> August </month> <year> 1985. </year>
Reference-contexts: Many of the techniques that have been proposed, e.g., simulated annealing [45], distributed drafting [67], pressure based load migration [48], among others, can be applied to parallel simulation programs. 6.2 Static Load Balancing Techniques Early work on static load balancing is found in <ref> [76, 74] </ref>. The basic idea behind the mapping algorithm is to examine the critical paths through multiple executions of a simulation, and cluster in such a way that the critical paths are left as undisturbed as possible.
Reference: [75] <author> D.M. Nicol. </author> <title> The cost of conservative synchronization in parallel discrete-event simulations. </title> <journal> Journal of the ACM, </journal> <volume> 40(2) </volume> <pages> 304-333, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: These protocols typically compute, distribute and are controlled by global system information. In this they reflect a philosophical shift away from the roots of parallel simulation in asynchronous distributed system theory. The algorithms studied in <ref> [85, 14, 75, 3, 87, 32] </ref> all compute a minimum time defining a time beyond which a processor will not venture until the next window "phase". Typically, this calculation involves lookahead of some kind. <p> The algorithm studied in <ref> [75] </ref> reasons as follows. Since we know all there 4 is to know about the job's departure at the time it enters service, we may as well immediately report the job's arrival at its next queue (this sort of pre-sending is also implicit with Time Warp messages). <p> One attraction is that they are relatively easier to analyze than are completely asynchronous algorithms, since one's attention need only be focused on one representative window. The conservative windowing algorithm described in x2.3 is analyzed in <ref> [75] </ref>. While the details are complex, the general idea is simple.
Reference: [76] <author> D.M. Nicol and P.F. Reynolds, Jr. </author> <title> A statistical approach to dynamic partitioning. </title> <booktitle> In Distributed Simulation 85, </booktitle> <volume> volume 15, </volume> <pages> pages 53-56. </pages> <booktitle> SCS Simulation Series, </booktitle> <year> 1985. </year>
Reference-contexts: Many of the techniques that have been proposed, e.g., simulated annealing [45], distributed drafting [67], pressure based load migration [48], among others, can be applied to parallel simulation programs. 6.2 Static Load Balancing Techniques Early work on static load balancing is found in <ref> [76, 74] </ref>. The basic idea behind the mapping algorithm is to examine the critical paths through multiple executions of a simulation, and cluster in such a way that the critical paths are left as undisturbed as possible.
Reference: [77] <author> D.M. Nicol and P.F Reynolds, Jr. </author> <title> Optimal dynamic remapping of data parallel computations. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 39(2) </volume> <pages> 206-219, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: The policy was empirically studied on a parallelized time-stepped combat model <ref> [77] </ref>, where remapping may occur between the advancement and engagement phases of the simulation. Nandy and Loucks use an iterative, static load balancing algorithm for parallel simulation using the Chandy/Misra/Bryant synchronization protocol (null messages) [66].
Reference: [78] <author> C. Pancerella. </author> <title> Improving the efficiency of a framework for parallel simulations. </title> <booktitle> In 6 th Workshop on Parallel and Distributed Simulation, </booktitle> <volume> volume 24, </volume> <pages> pages 22-32. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> Jan. </month> <year> 1992. </year>
Reference-contexts: Similarly, optimistic protocols require information that is distributed across the system to compute global virtual time. Reynolds has proposed a hardware mechanism to rapidly collect, operate on, and disseminate synchronization information throughout a parallel simulation system <ref> [64, 78] </ref>. The hardware is configured as a binary tree, with a processor assigned to each node.
Reference: [79] <author> B. Preiss, W. Loucks, I. MacIntyre, and J. </author> <title> Field. Null message cancellation in conservative distributed simulation. </title> <booktitle> In Advances in Parallel and Distributed Simulation, </booktitle> <volume> volume 23, </volume> <pages> pages 33-38. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: Q3 appends (Q3,5) and sends a copy to Q4, who appends (Q4,4) and sends a copy to Q1. The feedback on both incoming arcs permits Q1 to infer that it may proceed. Even with carrier null messages, CMB algorithms still generate many null messages. Another optimization, explored in <ref> [79] </ref>, attempts to reduce null message propagation by recognizing when a null message becomes stale. In the earlier example, Q1 sends a stream of null messages to Q2 (and Q4), successive ones increasing in time-stamp by 0.1.
Reference: [80] <author> B. Preiss, I. MacIntyre, and W. Loucks. </author> <title> On the trade-off between time and space in optimistic parallel discrete-event simulation. </title> <booktitle> In 6 th Workshop on Parallel and Distributed Simulation, </booktitle> <volume> volume 24, </volume> <pages> pages 33-42. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> Jan. </month> <year> 1992. </year>
Reference-contexts: State saving overheads can incur a significant overhead [27]. One can alleviate this overhead to some extent by reducing the frequency of checkpointing, however, analytic and experimental data suggest that the optimal checkpoint interval may be frequent (e.g., every few events) <ref> [80] </ref>. Fujimoto, et al. propose a component called the rollback chip that provides hardware support for state saving and rollback in Time Warp [30]. This component was the forerunner to the space-time memory system described above. The rollback chip can be viewed as a special memory management unit. <p> Preiss, MacIntyre, and Loucks <ref> [80] </ref> and Bellenot [7] validate Lin's results experimentally. Bellenot also observes that benefits in reducing state saving frequency diminish or become liabilities as the number of processors is increased. <p> Storage for these events could be reclaimed if state saving were more frequent. Empirical studies of queueing network simulations indicate, however, that total memory utilization is reduced with infrequent state saving <ref> [80] </ref>. 7.3 Rollback Based Protocols The strategies discussed thus far (fossil collection, incremental/infrequent state saving, limiting optimism) all have the following drawback: if the system does run out of memory, the simulation must be terminated unless some additional mechanism is provided to reclaim memory.
Reference: [81] <author> B. R. Preiss. </author> <title> The Yaddes distributed discrete event simulation specification language and execution environments. </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <volume> 21(2) </volume> <pages> 139-144, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: Other approaches to computing GVT have been proposed. Preiss uses a token passing scheme where the processors making up the simulation are organized in a ring, and continually compute GVT as the token is passed from one processor to the next <ref> [81] </ref>. This approach has some similarity to ring-based algorithms for detecting deadlock [65]. Bellenot uses a statically defined tree to to initiate, compute, and disseminate GVT values [6].
Reference: [82] <author> P. L. Reiher and D. Jefferson. </author> <title> Dynamic load management in the Time Warp Operating System. </title> <journal> Transactions of the Society for Computer Simulation, </journal> <volume> 7(2) </volume> <pages> 91-120, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: To address this issue, Reiher and Jefferson propose a new metric called effective processor utilization which is defined as the fraction of the time during which a processor is executing computations that are eventually committed <ref> [82] </ref>. This effectively treats time spent executing wrong computations as idle time. Based on this metric, they propose a strategy that migrates processes from processors with high effective utilization to those with low utilization. An algorithm that is similar in spirit is proposed in [36]. <p> Reiher and Jefferson demonstrate that phase splitting and the effective utilization metric are useful to dynamically balance the load in simulations of a communication network, a system of colliding pucks, and a combat models <ref> [82] </ref>. Goldberg describes an interesting approach to the load distribution problem [37]. If a process becomes a bottleneck, it is replicated to form two or more identical copies, each able to execute concurrently with the others.
Reference: [83] <author> H.S. Ross. </author> <title> Stochastic Processes. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: The distribution of time between t min and a processor's next message time is the equilibrium distribution <ref> [83] </ref> associated with the time-stamp advancement distribution, which in the case of the exponential is the exponential itself. The minimum of k independent exponentials with mean is well-known to be exponential with mean =k.
Reference: [84] <author> B. Samadi. </author> <title> Distributed simulation, algorithms and performance analysis. </title> <type> Ph. D. Thesis, </type> <institution> University of California, </institution> <address> Los Angeles, </address> <year> 1985. </year>
Reference-contexts: Samadi proposes another approach that tags messages sent after a GVT computation initiates, but has not yet been completed, allowing messages such as that in the preceding example to be accounted for <ref> [84] </ref>. Other approaches to computing GVT have been proposed. Preiss uses a token passing scheme where the processors making up the simulation are organized in a ring, and continually compute GVT as the token is passed from one processor to the next [81].
Reference: [85] <author> L. M. Sokol, D. P. Briscoe, and A. P. Wieland. MTW: </author> <title> a strategy for scheduling discrete simulation events for concurrent execution. </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <volume> 19(3) </volume> <pages> 34-42, </pages> <month> July </month> <year> 1988. </year> <month> 34 </month>
Reference-contexts: Once all processors have synchronized at time t (which is itself a non-trivial problem addressed in [68]), a new window [t + ; t + 2] is simulated. This basic proposal is found originally in <ref> [85] </ref>, with variations appearing in [90] and [5]. <p> These protocols typically compute, distribute and are controlled by global system information. In this they reflect a philosophical shift away from the roots of parallel simulation in asynchronous distributed system theory. The algorithms studied in <ref> [85, 14, 75, 3, 87, 32] </ref> all compute a minimum time defining a time beyond which a processor will not venture until the next window "phase". Typically, this calculation involves lookahead of some kind.
Reference: [86] <author> L. Soule and A. Gupta. </author> <title> An evaluation of the Chand-Misra-Byrant algorithm for digital logic simulation. </title> <journal> ACM Trans. on Modeling and Computer Simulation, </journal> <volume> 1(4), </volume> <month> October </month> <year> 1991. </year>
Reference-contexts: One such example is the simulation of digital logic networks. VLSI simulation is notorious for its computational demands, the significance of successful parallelization would be large. Standard CMB and Time Warp approaches have been attempted <ref> [86] </ref>, [63], with only mixed results. Recognizing that feedback loops pose one of the hardest problems for a conservative synchronization algorithm, [20] propose an approach where the network to be simulated is transformed into another (larger) one containing no feedback loops.
Reference: [87] <author> J. Steinman. </author> <title> Speedes:synchronous parallel environment for emulation and discrete event simulation. </title> <booktitle> In Advances in Parallel and Distributed Simulation, </booktitle> <volume> volume 23, </volume> <pages> pages 95-103. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: These protocols typically compute, distribute and are controlled by global system information. In this they reflect a philosophical shift away from the roots of parallel simulation in asynchronous distributed system theory. The algorithms studied in <ref> [85, 14, 75, 3, 87, 32] </ref> all compute a minimum time defining a time beyond which a processor will not venture until the next window "phase". Typically, this calculation involves lookahead of some kind. <p> Analysis of the extension shows that the window construct prevents rollbacks from cascading very far. Furthermore if state-saving costs are not large, the benefit of extending the window exceeds the costs, and better performance than the conservative window scheme may be achieved. Essentially the same algorithm is analyzed in <ref> [87] </ref>, but in a very different way. A differential wave equation is constructed expressing the density of events within a window at time t (assuming the window starts at 0). Numerical solution shows excellent agreement both with empirical results, and with the values predicted by the earlier model.
Reference: [88] <author> W. K. Su and C. L. Seitz. </author> <title> Variants of the Chandy-Misra-Bryant distributed discrete-event simulation algorithm. </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <volume> 21(2) </volume> <pages> 38-43, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: This provides a strong incentive to pack logical messages together into a single physical message. CMB variations doing this are explored in <ref> [88] </ref>. A number of issues are examined, including receiver or sender initiated transfer, as well as lazy or eager transmission. 2.2 Enhancements to Time Warp Another body of work examines optimizations to the basic Time Warp mechanism.
Reference: [89] <author> G. Thomas and J. Zahorjan. </author> <title> Parallel simulation of performance petri nets: Extending the domain of parallel simulation. </title> <booktitle> In Proceedings of the 1991 Winter Simulation Conference, </booktitle> <pages> pages 564-573, </pages> <address> Phoenix, Arizona, </address> <month> December </month> <year> 1991. </year>
Reference-contexts: A final illustration of application dependent protocols occurs considering the simulation of Timed Petri 7 Nets (TPN). The semantics of a TPN simulation do not fit easily into the CMB world-view. As a consequence, extensions to the CMB protocol have been proposed in [47] and <ref> [89] </ref>. However, it is possible to simulate a TPN using a general windowing protocol, as shown in [72]. 2.5 Future Directions Synchronization will always be an interesting area of study.
Reference: [90] <author> S. Turner and M. Xu. </author> <title> Performance evaluation of the bounded Time Warp algorithm. </title> <booktitle> In 6 th Workshop on Parallel and Distributed Simulation, </booktitle> <volume> volume 24, </volume> <pages> pages 117-128. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> Jan. </month> <year> 1992. </year> <month> 35 </month>
Reference-contexts: Once all processors have synchronized at time t (which is itself a non-trivial problem addressed in [68]), a new window [t + ; t + 2] is simulated. This basic proposal is found originally in [85], with variations appearing in <ref> [90] </ref> and [5]. A similar proposal to extend constrained optimistism to the Bounded-Lag protocol is found in [60]. 2.3 Protocols Based on Windows One emerging theme in protocol research is to study protocols that constrain all concurrent simulation activity to be within some window of global synchronization time.
References-found: 90

