URL: http://www.cs.columbia.edu/~library/theses/breslauer.ps.gz
Refering-URL: http://www.cs.columbia.edu/home/phd_prog/alumni.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Efficient String Algorithmics  
Degree: (Ph.D. Thesis)  Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy in the Graduate School of Arts and Sciences  
Date: 1992  
Address: CUCS-024-92  
Affiliation: Dany Breslauer  COLUMBIA UNIVERSITY  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. V. Aho. </author> <title> Algorithms for finding pattern in strings. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, </booktitle> <pages> pages 257-300. </pages> <publisher> Elsevier Science Publishers B. V., </publisher> <address> Amsterdam, the Netherlands, </address> <year> 1990. </year>
Reference-contexts: For a survey on string matching algorithms see Aho's <ref> [1] </ref> paper. Most of these algorithms work in two phases: in the first phase the pattern is preprocessed and some information is computed to be used later in a text processing phase that finds the actual occurrences of the pattern in the text. <p> The new witnesses in the first k i -block obviously satisfy W [l] 2k i k i+1 . Hence, the new witnesses in the other k i -blocks satisfy W [l] &lt; l + k i+2 . So all computed witnesses satisfy <ref> [1] </ref> with i increased by 1. If all witnesses in the first k i -block have been computed we proceed in a non-periodic stage i + 1; otherwise, we verify p to be the period length of the first k i+1 -block.
Reference: [2] <author> A. V. Aho and M. J. Corasick. </author> <title> Efficient string matching: An aid to bibliographic search. </title> <journal> Comm. of the ACM, </journal> <volume> 18(6) </volume> <pages> 333-340, </pages> <year> 1975. </year>
Reference-contexts: As in the case of a fixed alphabet, input symbols can be used for direct indexing of an array of a constant size. This is especially useful for fast implementations of algorithms that use an automaton description or trie data structures <ref> [2, 5, 35, 60, 75, 89] </ref>. In addition to the advantages of a fixed alphabet, this assumption allows several algorithms in the literature to pack few input symbols into a single register and compare them in a single operation [7, 46, 73]. <p> It is still an open problem whether a k-head one-way finite automaton can perform string matching. The only known cases are for k = 1; 2; 3 [53, 68, 69] where the answer is negative. The Aho-Corasick <ref> [2] </ref> algorithm which can search for multiple patterns in linear time is also used widely in practice. This algorithm works under the constant alphabet assumption and it does not use character comparisons.
Reference: [3] <author> A. V. Aho, J. E. Hopcroft, and J. D. Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA., </address> <year> 1974. </year>
Reference-contexts: That is, in most cases we concentrate on asymptotic growth rates of these formulae rather than on their exact mathematical expressions. The terminology used is standard and described next. For the motivation for this terminology and a more elaborate discussion see any algorithms textbook <ref> [3, 4] </ref>. We use the notation O (f (n)) ("big Oh" notation) to describe the set of functions that grow asymptotically no faster than f (n). The notation (f (n)) describes the set of functions that grow asymptotically at least as fast as f (n). <p> This idea is usually referred to as the four-Russions technique after the Boolean matrix multiplication algorithm that was invented by Arlazarov, Dinic, Kronrod and Faradzev <ref> [3, 13] </ref>. To demonstrate the importance of the assumptions on the alphabet we consider a simple problem that has different solutions under these different assumptions. The element distinctness problem is defined as follows: Given an input sequence S [1::k], check if all elements of the input sequence are distinct. <p> Proof: The elements of the input sequence can be sorted in O (k log k) time by a sorting algorithm such as merge-sort <ref> [3, 4] </ref>. In the sorted sequence, any elements that are equal appear consecutively and it suffices to compare each element to the next one to check if all elements are distinct. <p> However, if the sequence to be sorted contains only small integers, one can use bucket-sort that works in O (k) time. For an introduction to sorting algorithms and the information theoretic lower bound see the chapters on sorting in <ref> [3, 4] </ref>. Theorem 1.3 The element distinctness problem cannot be solved in fewer than O (k log k) order comparisons. Proof: We use the information theoretic lower bound for sorting. A proof of this bound can be found in standard textbooks [3]. <p> Theorem 1.3 The element distinctness problem cannot be solved in fewer than O (k log k) order comparisons. Proof: We use the information theoretic lower bound for sorting. A proof of this bound can be found in standard textbooks <ref> [3] </ref>. That lower bound shows that a sequence of length k cannot be sorted in fewer than (k log k) comparisons and it holds even if all elements of the input sequence are distinct. <p> The models that are used in this dissertation are standard models and their description can be found in several text books <ref> [3, 4, 54] </ref>. <p> We will not give a formal description of this model here, but rather describe the features and limitations of this model that might not be very obvious. For a formal discussion see <ref> [3, 4] </ref>. As a theoretical model, the RAM has infinite amount of memory cells, each capable to store an integer of infinite size. <p> The exact complexity of finding the third largest element is known for all but a finite number of cases [63, 64, 90]. The complexity of sorting is determined up to sorting 12 elements <ref> [3, 44] </ref>. The exact complexity of finding the median is another intriguing open problem: it has been known for several years that the number of comparisons required is between 2n [15] and 3n [81].
Reference: [4] <author> A. V. Aho, J. E. Hopcroft, and J. D. Ullman. </author> <title> Data Structures and Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA., </address> <year> 1983. </year>
Reference-contexts: That is, in most cases we concentrate on asymptotic growth rates of these formulae rather than on their exact mathematical expressions. The terminology used is standard and described next. For the motivation for this terminology and a more elaborate discussion see any algorithms textbook <ref> [3, 4] </ref>. We use the notation O (f (n)) ("big Oh" notation) to describe the set of functions that grow asymptotically no faster than f (n). The notation (f (n)) describes the set of functions that grow asymptotically at least as fast as f (n). <p> Proof: The elements of the input sequence can be sorted in O (k log k) time by a sorting algorithm such as merge-sort <ref> [3, 4] </ref>. In the sorted sequence, any elements that are equal appear consecutively and it suffices to compare each element to the next one to check if all elements are distinct. <p> However, if the sequence to be sorted contains only small integers, one can use bucket-sort that works in O (k) time. For an introduction to sorting algorithms and the information theoretic lower bound see the chapters on sorting in <ref> [3, 4] </ref>. Theorem 1.3 The element distinctness problem cannot be solved in fewer than O (k log k) order comparisons. Proof: We use the information theoretic lower bound for sorting. A proof of this bound can be found in standard textbooks [3]. <p> The models that are used in this dissertation are standard models and their description can be found in several text books <ref> [3, 4, 54] </ref>. <p> We will not give a formal description of this model here, but rather describe the features and limitations of this model that might not be very obvious. For a formal discussion see <ref> [3, 4] </ref>. As a theoretical model, the RAM has infinite amount of memory cells, each capable to store an integer of infinite size.
Reference: [5] <author> A. Apostolico. </author> <title> The myriad virtues of subword trees. </title> <editor> In A. Apostolico and Z. Galil, editors, </editor> <booktitle> Combinatorial Algorithms on Words, NATO ASI Series F, </booktitle> <volume> Vol 12, </volume> <pages> pages 85-96. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany, </address> <year> 1984. </year>
Reference-contexts: As in the case of a fixed alphabet, input symbols can be used for direct indexing of an array of a constant size. This is especially useful for fast implementations of algorithms that use an automaton description or trie data structures <ref> [2, 5, 35, 60, 75, 89] </ref>. In addition to the advantages of a fixed alphabet, this assumption allows several algorithms in the literature to pack few input symbols into a single register and compare them in a single operation [7, 46, 73].
Reference: [6] <author> A. Apostolico. </author> <title> Efficient CRCW-PRAM Algorithms for Universal Substring Searching. </title> <type> Technical Report CS-TR-91-009, </type> <institution> Dept. of Computer Science, Purdue University, </institution> <year> 1991. </year>
Reference-contexts: Galil's [47] new algorithm requires the same time as Vishkin's algorithm for the preprocessing step. 2 The function log fl m is defined as the smallest k such that log (k) m 2, where log (1) m = logm and log (i+1) m = log log (i) m. 39 Apostolico <ref> [6] </ref> also designed a constant time optimal parallel string matching algorithm over an ordered alphabet. Apostolico's algorithm uses the alphabet order to compute a special representation for the input strings in a preprocessing step that takes O (log n) time and is not optimal.
Reference: [7] <author> A. Apostolico. </author> <title> Optimal parallel detection of squares in strings. </title> <type> Technical Report CS-TR-91-026, </type> <institution> Dept. of Computer Science, Purdue University, </institution> <year> 1991. </year>
Reference-contexts: An ordered alphabet Under this assumption the alphabet is a totally ordered set and any pair of symbols can be compared to obtain a less than, equal, or greater than answer <ref> [7, 37, 38] </ref>. 3 Some string problems require alphabet order in their definition but an arbitrary alphabet order can be helpful also for problems that do not require it. <p> In addition to the advantages of a fixed alphabet, this assumption allows several algorithms in the literature to pack few input symbols into a single register and compare them in a single operation <ref> [7, 46, 73] </ref>. This idea is usually referred to as the four-Russions technique after the Boolean matrix multiplication algorithm that was invented by Arlazarov, Dinic, Kronrod and Faradzev [3, 13]. <p> Crochemore [36] also gave a linear time algorithm for this problem. In parallel, an algorithm by Crochemore and Rytter [39] can test if a string is square-free in optimal O (log n) time. This algorithm uses O (n 2 ) space. Other algorithms by Apostolico <ref> [7] </ref> can test if a string is square-free and even detect all the squares in the same time and processor bounds using only linear auxiliary space. Apostolico's [7] algorithm for testing if a string is square-free is even more efficient in the case of a finite alphabet and achieves the same <p> This algorithm uses O (n 2 ) space. Other algorithms by Apostolico <ref> [7] </ref> can test if a string is square-free and even detect all the squares in the same time and processor bounds using only linear auxiliary space. Apostolico's [7] algorithm for testing if a string is square-free is even more efficient in the case of a finite alphabet and achieves the same time bound of O (log n) using only n log n processors. Apos-tolico's algorithms [7] assume that the alphabet is ordered, an assumption which is not necessary <p> Apostolico's <ref> [7] </ref> algorithm for testing if a string is square-free is even more efficient in the case of a finite alphabet and achieves the same time bound of O (log n) using only n log n processors. Apos-tolico's algorithms [7] assume that the alphabet is ordered, an assumption which is not necessary to solve this problem. <p> All these algorithms are designed for the CRCW-PRAM computation model. 58 All the parallel algorithms mentioned above are optimal since the time-processor product is O (n log n) which is the best possible in the case of a general alphabet. Apostolico's <ref> [7] </ref> algorithm for testing square-freeness in the case of a finite alphabet is also optimal since the time-processor product is O (n), the best running time of a sequential algorithm for this problem.
Reference: [8] <author> A. Apostolico and D. Breslauer. </author> <title> On the exact complexity of comparison based string matching. </title> <type> manuscript, </type> <year> 1992. </year>
Reference-contexts: This bound is achieved also by an algorithm of Apostolico and Breslauer <ref> [8] </ref>, that is obtained by a combination of the techniques of Knuth, Morris and Pratt [66] and Crochemore and Perrin [38], but is much easier to analyze.
Reference: [9] <author> A. Apostolico, D. Breslauer, and Z. Galil. </author> <title> Optimal Parallel Algorithms for Periods, Palindromes and Squares. </title> <booktitle> In Proc. 19th International Colluquium on Automata, Languages, and Programming. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany, </address> <year> 1992. </year> <note> To appear. </note>
Reference-contexts: Thus, the lower bound holds also for any such string matching algorithm. 2 3.4 Finding All Periods In this section we present an O (log log n) time n log log n -processor CRCW-PRAM algorithm to compute all periods of a string <ref> [9, 27] </ref>. If the number of available processors is p we show that the algorithm takes O (d n p e + log log d1+p=ne 2p) time.
Reference: [10] <author> A. Apostolico and R. Giancarlo. </author> <title> The Boyer-Moore-Galil string searching strategies revisited. </title> <journal> SIAM J. Comput., </journal> <volume> 15(1) </volume> <pages> 98-105, </pages> <year> 1986. </year>
Reference-contexts: The Boyer-Moore algorithm performs 3n comparisons in the worst case as proven recently by Cole [29], but seems to perform better in practice. A variant the Boyer-Moore algorithm that performs 2n comparisons in the worst case was designed by Apostolico and Giancarlo <ref> [10] </ref>. All these algorithms use an O (m) auxiliary space. At a certain time it was known that a logarithmic space solution was possible [51], and the string matching problem was conjectured to have a time-space trade-off [21]. <p> We define also c l (n; m) to be the maximum of c l P (n) over all pattern strings P of length m. Colussi [32] showed that c online (n; m) 1:5n :5 (m 1) while the best previous bound was c online (n; m) 2n m <ref> [10, 38, 66] </ref>. Galil and Giancarlo [48] proved that the number of comparisons performed by Colussi's algorithm can be bounded also by n + b (n m) m c n + b nm 2 c, where 1 is the length of the shortest period of the pattern.
Reference: [11] <author> A. Apostolico, C. Ilioupulis, G. Landau, B. Schieber, and U. Vishkin. </author> <title> Parallel construction of a suffix tree, with applications. </title> <journal> Algorithmica, </journal> <volume> 3 </volume> <pages> 98-105, </pages> <year> 1988. </year> <pages> 72 73 </pages>
Reference-contexts: Again, we distinguish between two cases. 3. A fixed alphabet Under this assumption the symbols of the alphabet are integers from a small range. This assumption is usually used in the literature to allow direct indexing of an array by an alphabet symbol <ref> [11, 40, 59, 62] </ref> 4. A constant alphabet Under this assumption the symbols of the alphabet are integers from a constant range. As in the case of a fixed alphabet, input symbols can be used for direct indexing of an array of a constant size.
Reference: [12] <author> A. Apostolico and F. P. Preparata. </author> <title> Optimal off-line detection of repetitions in a string. </title> <journal> Theoret. Comput. Sci., </journal> <volume> 22 </volume> <pages> 297-315, </pages> <year> 1983. </year>
Reference-contexts: Algorithms by Apostolico and Preparata <ref> [12] </ref>, by Crochemore [35], Rabin [77] and by Main and Lorentz [72] find all the squares in a string of length n in O (n log n) time. Main and Lorentz [72] also show that O (n log n) comparisons are necessary even to decide if a string is square-free.
Reference: [13] <author> V. L. Arlazarov, E. A. Dinic, M. A. Kronrod, and I. A. Faradzev. </author> <title> On economic construction of the transitive closure of a directed graph. </title> <journal> Soviet Math. Dokl., </journal> <volume> 11 </volume> <pages> 1209-1210, </pages> <year> 1970. </year>
Reference-contexts: This idea is usually referred to as the four-Russions technique after the Boolean matrix multiplication algorithm that was invented by Arlazarov, Dinic, Kronrod and Faradzev <ref> [3, 13] </ref>. To demonstrate the importance of the assumptions on the alphabet we consider a simple problem that has different solutions under these different assumptions. The element distinctness problem is defined as follows: Given an input sequence S [1::k], check if all elements of the input sequence are distinct.
Reference: [14] <author> P. Beame and J. Hastad. </author> <title> Optimal bound for decision problems on the CRCW-PRAM. </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 36(3) </volume> <pages> 643-670, </pages> <year> 1989. </year>
Reference-contexts: Theorem 1.12 (Cook, Dwork, Reischuk [34]) Any CREW-PRAM algorithm that computes the Boolean AND function of n input bits takes (log n) time on any number of processors. Theorem 1.13 (Beame and Hastad <ref> [14] </ref>) A CRCW-PRAM algorithm that computes the parity function of n input bits takes ( log n log log n ) time on any polynomial number of proces sors. The next theorem is used extensively in all the parallel algorithms that we describe.
Reference: [15] <author> S. W. Bent and J. W. John. </author> <title> Finding the median requires 2n comparisons. </title> <booktitle> In Proc. 17th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 213-216, </pages> <year> 1985. </year>
Reference-contexts: The complexity of sorting is determined up to sorting 12 elements [3, 44]. The exact complexity of finding the median is another intriguing open problem: it has been known for several years that the number of comparisons required is between 2n <ref> [15] </ref> and 3n [81]. In this chapter we present a new string matching algorithm with improved bounds on the number of comparisons performed.
Reference: [16] <author> O. Berkman, D. Breslauer, Z. Galil, B. Schieber, and U. Vishkin. </author> <title> Highly parallelize-able problems. </title> <booktitle> In Proc. 21st ACM Symp. on Theory of Computing, </booktitle> <pages> pages 309-319, </pages> <year> 1989. </year>
Reference-contexts: Optimal solutions are usually more difficult to obtain and make up a major part of this dissertation. There are many other problems that have fast parallel algorithms that get around the lower bound of Theorem 1.13 <ref> [16, 17, 20, 67, 83, 86] </ref>. Lemma 3.2 The string matching problem can be solved in constant time by an nm processor CRCW-PRAM. <p> algorithm and it has the advantage that it can report occurrences of any substring of the preprocessed strings in any other substring. 3.2 An O (log log n) Time Algorithm In this section we present an O (log log n) time n log log n -processor CRCW-PRAM string matching algorithm <ref> [16, 24, 25] </ref>. If the number of available processors is p we show that the algorithm takes O (d n p e + log log d1+p=ne 2p) time.
Reference: [17] <author> O. Berkman, B. Schieber, and U. Vishkin. </author> <title> Some doubly logarithmic optimal parallel algorithms based on finding nearest smallers. </title> <type> manuscript, </type> <year> 1988. </year>
Reference-contexts: Optimal solutions are usually more difficult to obtain and make up a major part of this dissertation. There are many other problems that have fast parallel algorithms that get around the lower bound of Theorem 1.13 <ref> [16, 17, 20, 67, 83, 86] </ref>. Lemma 3.2 The string matching problem can be solved in constant time by an nm processor CRCW-PRAM.
Reference: [18] <author> O. Berkman and U. Vishkin. </author> <title> Recursive fl-tree parallel data-structure. </title> <booktitle> In Proc. 30th IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 196-202, </pages> <year> 1989. </year>
Reference-contexts: Another possible representation that can be computed even more efficiently by algorithms of Berkman and Vishkin <ref> [18] </ref> and Ragde [78] is a linked list of all the occurrences. Note that Theorem 3.1 does not imply that the string matching problem as we defined it cannot be solved faster.
Reference: [19] <author> R. Boppana. </author> <title> Optimal separation between concurrent-write parallel machines. </title> <booktitle> In Proc. 21st ACM Symp. on Theory of Computing, </booktitle> <pages> pages 320-326, </pages> <year> 1989. </year>
Reference-contexts: An even stronger model, the priority CRCW-PRAM assumes each processor has a preassigned priority and the highest priority processor succeeds to write. These three versions of the CRCW-PRAM model differ in their ability to solve certain problems as shown by Boppana <ref> [19] </ref>. All the CRCW-PRAM algorithms described in this dissertation can be implemented in the common model. In fact these algorithms can be implemented even if we assume that the same constant value is always written in case of concurrent writes.
Reference: [20] <author> A. Borodin and J. E. Hopcroft. </author> <title> Routing, merging and sorting on parallel models of comparison. </title> <journal> J. Comput. System Sci., </journal> <volume> 30 </volume> <pages> 130-145, </pages> <year> 1985. </year>
Reference-contexts: Optimal solutions are usually more difficult to obtain and make up a major part of this dissertation. There are many other problems that have fast parallel algorithms that get around the lower bound of Theorem 1.13 <ref> [16, 17, 20, 67, 83, 86] </ref>. Lemma 3.2 The string matching problem can be solved in constant time by an nm processor CRCW-PRAM.
Reference: [21] <author> A. B. Borodin, M. J. Fischer, D. G. Kirkpatrick, N. A. Lynch, and M. Tompa. </author> <title> A time-space tradeoff for sorting on non-oblivious machines. </title> <booktitle> In Proc. 20th IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 294-301, </pages> <year> 1979. </year>
Reference-contexts: All these algorithms use an O (m) auxiliary space. At a certain time it was known that a logarithmic space solution was possible [51], and the string matching problem was conjectured to have a time-space trade-off <ref> [21] </ref>. This conjecture was later disproved when a linear-time constant-space algorithm was discovered by Galil and Seiferas [38, 52]. It 16 was shown that even a 6-head two-way finite automaton can perform string matching in linear time.
Reference: [22] <author> R. S. Boyer and J. S. Moore. </author> <title> A fast string searching algorithm. </title> <journal> Comm. of the ACM, </journal> <volume> 20 </volume> <pages> 762-772, </pages> <year> 1977. </year>
Reference-contexts: A recent work by Colussi [32] showed that the string matching problem can be solved using 3 2 n comparisons improving the previous bound of 2n that was achieved by several algorithms <ref> [22, 38, 66] </ref>. Colussi's bound was later improved to 4 3 n by Galil and Giancarlo [48] who proved also lower bounds that match their upper bound for short patterns [49].
Reference: [23] <author> R. P. Brent. </author> <title> Evaluation of general arithmetic expressions. </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 21 </volume> <pages> 201-206, </pages> <year> 1974. </year>
Reference-contexts: Our goal will be to design fast parallel algorithm that achieve optimal speedup. The following theorem will be used throughout the paper. Theorem 1.11 (Brent <ref> [23] </ref>) Any PRAM algorithm of time t that consists of x elementary operations can be implemented on p processors in dx=pe + t time. (Elementary operations are the operations that are supported by each of the processors.) Brent's theorem has two aspects that are used in this dissertation: 1.
Reference: [24] <author> D. Breslauer. </author> <title> An optimal O(log log n) time parallel string matching algorithm. </title> <type> Master's thesis, </type> <institution> Dept. of Computer Science, Tel-Aviv University, Tel-Aviv, Israel, </institution> <year> 1989. </year>
Reference-contexts: Breslauer and Galil [25] obtained an optimal O (log log m) time algorithm for a general alphabet. This algorithm is based on the algorithms of Galil [46] and Vishkin [87]. The algorithm that was a part of my Master's Thesis <ref> [24] </ref> is described later for completeness. An analysis of the algorithm for any number of available processors that was not included in the original work is also presented. The simplest known parallel string matching algorithm is perhaps the optimal O (log n) time randomized algorithm of Karp and Rabin [60]. <p> algorithm and it has the advantage that it can report occurrences of any substring of the preprocessed strings in any other substring. 3.2 An O (log log n) Time Algorithm In this section we present an O (log log n) time n log log n -processor CRCW-PRAM string matching algorithm <ref> [16, 24, 25] </ref>. If the number of available processors is p we show that the algorithm takes O (d n p e + log log d1+p=ne 2p) time.
Reference: [25] <author> D. Breslauer and Z. Galil. </author> <title> An optimal O(log log n) time parallel string matching algorithm. </title> <journal> SIAM J. Comput., </journal> <volume> 19(6) </volume> <pages> 1051-1058, </pages> <year> 1990. </year>
Reference-contexts: This algorithm uses the four-Russions method and packs many input symbols into one register to compare them in a single operation. Galil's [46] algorithm was soon improved by Vishkin [87] to an algorithm that works over a general alphabet. Breslauer and Galil <ref> [25] </ref> obtained an optimal O (log log m) time algorithm for a general alphabet. This algorithm is based on the algorithms of Galil [46] and Vishkin [87]. The algorithm that was a part of my Master's Thesis [24] is described later for completeness. <p> algorithm and it has the advantage that it can report occurrences of any substring of the preprocessed strings in any other substring. 3.2 An O (log log n) Time Algorithm In this section we present an O (log log n) time n log log n -processor CRCW-PRAM string matching algorithm <ref> [16, 24, 25] </ref>. If the number of available processors is p we show that the algorithm takes O (d n p e + log log d1+p=ne 2p) time. <p> These three steps seem to require simultaneous write of different values but an implementation on the weaker common CRCW-PRAM is also possible <ref> [25] </ref>. In order to make our algorithm optimal, we take a more careful look at the algorithm described above.
Reference: [26] <author> D. Breslauer and Z. Galil. </author> <title> Closing the Gap in the Exact Complexity of String Matching. </title> <type> manuscript, </type> <year> 1992. </year> <month> 74 </month>
Reference-contexts: In this chapter we present a new string matching algorithm with improved bounds on the number of comparisons performed. The new algorithm requires at most n+o m!1 (1)n comparisons, closing the gap between the lower and upper bounds as the length of the pattern grows <ref> [26] </ref>. 2.1 Historical Overview There are several algorithms that solve the string matching problem in linear time. For a survey on string matching algorithms see Aho's [1] paper.
Reference: [27] <author> D. Breslauer and Z. Galil. </author> <title> Finding all periods and initial palindromes of a string in parallel. </title> <type> manuscript, </type> <year> 1992. </year>
Reference-contexts: Thus, the lower bound holds also for any such string matching algorithm. 2 3.4 Finding All Periods In this section we present an O (log log n) time n log log n -processor CRCW-PRAM algorithm to compute all periods of a string <ref> [9, 27] </ref>. If the number of available processors is p we show that the algorithm takes O (d n p e + log log d1+p=ne 2p) time.
Reference: [28] <author> Y. Cesari and M. Vincent. </author> <title> Une caracterisation des mots periodiques. </title> <type> C.R. </type> <institution> Acad. Sci. Paris, 286(A):1175-1177, </institution> <year> 1978. </year>
Reference-contexts: A non-trivial factorization of a string S is called a critical factorization if the local period at the factorization is of the same length as the period of S. The following lemma shows that critical factorizations exist. Theorem 1.8 (The critical factorization theorem <ref> [28, 70] </ref>) Let S [1::k] be a string that has period length S 1 . Then, if we consider any S 1 1 consecutive non-trivial factorizations at least one is a critical factorization. 1.2.4 Squares A string S is called primitive if it is not a repetition of shorter string.
Reference: [29] <author> R. Cole. </author> <title> Tight bounds on the complexity of the Boyer-Moore pattern matching algorithm. </title> <booktitle> In Proc. 2nd ACM-SIAM Symp. on Discrete Algorithms, </booktitle> <pages> pages 224-233, </pages> <year> 1991. </year>
Reference-contexts: The algorithm reported in the final paper is a refined version of these two independent discoveries. The Knuth-Morris-Pratt [66] algorithm performs at most 2n comparisons in its text processing phase. The Boyer-Moore algorithm performs 3n comparisons in the worst case as proven recently by Cole <ref> [29] </ref>, but seems to perform better in practice. A variant the Boyer-Moore algorithm that performs 2n comparisons in the worst case was designed by Apostolico and Giancarlo [10]. All these algorithms use an O (m) auxiliary space.
Reference: [30] <author> R. Cole and U. Vishkin. </author> <title> Faster optimal prefix sums and list ranking. </title> <journal> Information and Computation, </journal> <volume> 81 </volume> <pages> 334-352, </pages> <year> 1989. </year>
Reference-contexts: Such a representation is useful sometimes and can be computed from our representation in O ( log n log log n ) time on a n log log n log n processor CRCW-PRAM <ref> [30] </ref>. Another possible representation that can be computed even more efficiently by algorithms of Berkman and Vishkin [18] and Ragde [78] is a linked list of all the occurrences. Note that Theorem 3.1 does not imply that the string matching problem as we defined it cannot be solved faster.
Reference: [31] <author> L. Colussi. </author> <title> Combinatorial properties of strings. </title> <type> manuscript, </type> <year> 1991. </year>
Reference-contexts: If ff &lt; m we say that the prefix period terminates at ff + 1. The set of periods of a string has a special structure as shown by Lyndon and Schutzen-berger [71]. Guibas and Odlyzko [55] and independently Colussi <ref> [31] </ref> gave a complete characterization of the possible structures this set can have. They have shown also that the set of periods is independent of the alphabet size. Their results are summarized in the following two lemmata. <p> Lemma 1.7 (Characterization of all sets of periods of a string <ref> [31, 55] </ref>) Let = f i j0 = 0 &lt; 1 &lt; &lt; z = kg and let ffi i = i+1 i , for i = 0::z 1.
Reference: [32] <author> L. Colussi. </author> <title> Correctness and efficiency of string matching algorithms. </title> <journal> Inform. and Control, </journal> <volume> 95 </volume> <pages> 225-251, </pages> <year> 1991. </year>
Reference-contexts: We distinguish between two classes of comparisons. 1. A general alphabet Under this assumption any pair of symbols can be compared to obtain an equal or unequal answer. This type of comparisons are sufficient to solve almost all problems that will be discussed in this thesis <ref> [32, 48, 52, 66, 72] </ref>. 2. <p> This algorithm works under the constant alphabet assumption and it does not use character comparisons. It uses direct indexing to an array and therefor it does not fall in the class of algorithms studied in this chapter. A recent work by Colussi <ref> [32] </ref> showed that the string matching problem can be solved using 3 2 n comparisons improving the previous bound of 2n that was achieved by several algorithms [22, 38, 66]. <p> All previous algorithms that solve the string matching problem are oblivious in the sense that they sometimes "forget" or do not use information that was obtained in previous comparisons. Colussi <ref> [32] </ref> developed his algorithm by using program correctness proof techniques to avoid performing comparisons which are not necessary as a result of previous comparisons. The algorithms described in Section 2.2 do not forget answers to previous comparisons, which makes their analysis much easier. <p> However, some versions of our algorithms which are described in Sections 2.2.5 and 2.2.7 are almost identical to Colussi's <ref> [32] </ref> and Galil and Giancarlo's [48] algorithms if they are modified to use a buffer. The length of the text, n, can be much larger than the length of the pattern, m. <p> Obviously, if k l then c l P (n) c k offline P (n) c online P (n). We define also c l (n; m) to be the maximum of c l P (n) over all pattern strings P of length m. Colussi <ref> [32] </ref> showed that c online (n; m) 1:5n :5 (m 1) while the best previous bound was c online (n; m) 2n m [10, 38, 66]. <p> This structure is described in an inefficient manner, and the emphasis is on the number of comparisons performed. Although our algorithm is developed in a different way from Colussi's <ref> [32] </ref> algorithm we obtain a very similar algorithm. The similarity to Colussi's algorithm will be discussed in Section 2.2.5. Two important properties of the new algorithms that make them comparison efficient and easier to analyze are: 1. They do not "forget" any comparison. 2. <p> Note that no comparison is made. We call this text position a hole and record it for a later processing. See Figure 2.1 for an example. The similarity to the holes in the algorithms of Colussi <ref> [32] </ref> and Galil and Giancarlo [48] is discussed in Sections 2.2.5 and 2.2.7. * Invariant two is violated. This means that if we align copies of the pattern starting at all text positions in and look at the column under text position , then some symbols are different. <p> Therefore, the number of comparisons charged to the special fund is bounded by 4 log m+2 m (n m) and the total number of comparisons performed by the algorithm is bounded by n + 4 log m+2 m (n m). 2.2.5 Colussi's Algorithm In this section we describe Colussi's <ref> [32] </ref> algorithm and discuss the relation between that algorithm and a similar version of our algorithm. Colussi [32] calls a pattern position h a hole if all periods of P [1::h 1] continue to P [1::h]. <p> m (n m) and the total number of comparisons performed by the algorithm is bounded by n + 4 log m+2 m (n m). 2.2.5 Colussi's Algorithm In this section we describe Colussi's <ref> [32] </ref> algorithm and discuss the relation between that algorithm and a similar version of our algorithm. Colussi [32] calls a pattern position h a hole if all periods of P [1::h 1] continue to P [1::h]. <p> This corresponds to the choice of ffi = 1 in Figure 2.2. There are two major differences between this version of our algorithm and Colussi's <ref> [32] </ref> algorithm. These differences are related to what happen when a comparison to a pattern position that is not a hole fails. Note, that in this case, the shift performed by Colussi's algorithm corresponds to the removal of 1 and possibly other members from . <p> The proof now follows from the fact that the total shift of throughout the algorithm is n m and by adding the n comparisons charged to the text symbols. 2 2.2.7 Galil and Giancarlo's Algorithm Galil and Giancarlo [48] noticed that Colussi's <ref> [32] </ref> algorithm performs very badly if the pattern starts and ends with a sequence of repetitions of the same symbol. For these patterns Colussi's algorithm shifts by a single position and 3 2 n comparisons are actually performed.
Reference: [33] <author> S. A. Cook. </author> <title> Linear time simulation of deterministic two-way pushdown automata. </title> <booktitle> In Information Processing 71, </booktitle> <pages> pages 75-80. </pages> <publisher> North Holland Publishing Co., </publisher> <address> Amsterdam, the Netherlands, </address> <year> 1972. </year>
Reference-contexts: The discovery of the algorithm has a very interesting history as reported in the paper [66] that was published only in 1977. Most notable probably is the fact that a result of Cook <ref> [33] </ref> on linear-time simulation of two-way deterministic pushdown automaton by a random access machine implied that a linear-time string matching algorithm exists. Knuth went through the laborious details of Cook's construction and discovered the algorithm.
Reference: [34] <author> S. A. Cook, C. Dwork, and R. Reischuk. </author> <title> Upper and lower time bounds for parallel random access machines without simultaneous writes. </title> <journal> SIAM J. Comput., </journal> <volume> 15(1) </volume> <pages> 87-97, </pages> <year> 1986. </year>
Reference-contexts: Brent's theorem gives the actual running time on any number of processors. 10 The following two lower bounds will be used in the sequel to argue that certain fast computations are not possible. Theorem 1.12 (Cook, Dwork, Reischuk <ref> [34] </ref>) Any CREW-PRAM algorithm that computes the Boolean AND function of n input bits takes (log n) time on any number of processors.
Reference: [35] <author> M. Crochemore. </author> <title> An optimal algorithm for computing the repetitions in a word. </title> <journal> Inform. Process. Lett., </journal> <volume> 12(5) </volume> <pages> 244-250, </pages> <year> 1981. </year>
Reference-contexts: As in the case of a fixed alphabet, input symbols can be used for direct indexing of an array of a constant size. This is especially useful for fast implementations of algorithms that use an automaton description or trie data structures <ref> [2, 5, 35, 60, 75, 89] </ref>. In addition to the advantages of a fixed alphabet, this assumption allows several algorithms in the literature to pack few input symbols into a single register and compare them in a single operation [7, 46, 73]. <p> However, there exist strings of infinite length on three letter alphabet that are square-free as shown by Axel Thue [84, 85] at the beginning of the century. The maximal number of squares that a string of length k can have is fi (k log k) as shown by Crochemore <ref> [35] </ref> who gave the following construction: Theorem 1.9 (Crochemore [35]) Define the Fibonacci strings as f 0 = `b', f 1 = `a' and f q+1 = f q f q1 . <p> The maximal number of squares that a string of length k can have is fi (k log k) as shown by Crochemore <ref> [35] </ref> who gave the following construction: Theorem 1.9 (Crochemore [35]) Define the Fibonacci strings as f 0 = `b', f 1 = `a' and f q+1 = f q f q1 . <p> Algorithms by Apostolico and Preparata [12], by Crochemore <ref> [35] </ref>, Rabin [77] and by Main and Lorentz [72] find all the squares in a string of length n in O (n log n) time. Main and Lorentz [72] also show that O (n log n) comparisons are necessary even to decide if a string is square-free.
Reference: [36] <author> M. Crochemore. </author> <title> Transducer and repetitions. </title> <journal> Theoret. Comput. Sci., </journal> <volume> 12 </volume> <pages> 63-86, </pages> <year> 1986. </year>
Reference-contexts: In another paper, Main and Lorentz [73] show that the latter problem of deciding whether a string is square-free can be solved in O (n) time on a constant alphabet. Crochemore <ref> [36] </ref> also gave a linear time algorithm for this problem. In parallel, an algorithm by Crochemore and Rytter [39] can test if a string is square-free in optimal O (log n) time. This algorithm uses O (n 2 ) space.
Reference: [37] <author> M. Crochemore. </author> <title> String-matching and periods. </title> <journal> Bulletin of EATCS, </journal> <month> October </month> <year> 1989. </year>
Reference-contexts: An ordered alphabet Under this assumption the alphabet is a totally ordered set and any pair of symbols can be compared to obtain a less than, equal, or greater than answer <ref> [7, 37, 38] </ref>. 3 Some string problems require alphabet order in their definition but an arbitrary alphabet order can be helpful also for problems that do not require it.
Reference: [38] <author> M. Crochemore and D. Perrin. </author> <title> Two-way string-matching. </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 38(3) </volume> <pages> 651-675, </pages> <year> 1991. </year>
Reference-contexts: An ordered alphabet Under this assumption the alphabet is a totally ordered set and any pair of symbols can be compared to obtain a less than, equal, or greater than answer <ref> [7, 37, 38] </ref>. 3 Some string problems require alphabet order in their definition but an arbitrary alphabet order can be helpful also for problems that do not require it. <p> At a certain time it was known that a logarithmic space solution was possible [51], and the string matching problem was conjectured to have a time-space trade-off [21]. This conjecture was later disproved when a linear-time constant-space algorithm was discovered by Galil and Seiferas <ref> [38, 52] </ref>. It 16 was shown that even a 6-head two-way finite automaton can perform string matching in linear time. It is still an open problem whether a k-head one-way finite automaton can perform string matching. <p> A recent work by Colussi [32] showed that the string matching problem can be solved using 3 2 n comparisons improving the previous bound of 2n that was achieved by several algorithms <ref> [22, 38, 66] </ref>. Colussi's bound was later improved to 4 3 n by Galil and Giancarlo [48] who proved also lower bounds that match their upper bound for short patterns [49]. <p> We define also c l (n; m) to be the maximum of c l P (n) over all pattern strings P of length m. Colussi [32] showed that c online (n; m) 1:5n :5 (m 1) while the best previous bound was c online (n; m) 2n m <ref> [10, 38, 66] </ref>. Galil and Giancarlo [48] proved that the number of comparisons performed by Colussi's algorithm can be bounded also by n + b (n m) m c n + b nm 2 c, where 1 is the length of the shortest period of the pattern. <p> This bound is achieved also by an algorithm of Apostolico and Breslauer [8], that is obtained by a combination of the techniques of Knuth, Morris and Pratt [66] and Crochemore and Perrin <ref> [38] </ref>, but is much easier to analyze. Galil and Giancarlo 18 [48] presented also an improved version of Colussi's algorithm that performs n + b (n m) min (1=3; 2m )c 4 3 m comparisons in the worst case.
Reference: [39] <author> M. Crochemore and W. Rytter. </author> <title> Efficient parallel algorithms to test square-freeness and factorize strings. </title> <journal> Inform. Process. Lett., </journal> <volume> 28 </volume> <pages> 57-60, </pages> <year> 1991. </year>
Reference-contexts: In another paper, Main and Lorentz [73] show that the latter problem of deciding whether a string is square-free can be solved in O (n) time on a constant alphabet. Crochemore [36] also gave a linear time algorithm for this problem. In parallel, an algorithm by Crochemore and Rytter <ref> [39] </ref> can test if a string is square-free in optimal O (log n) time. This algorithm uses O (n 2 ) space.
Reference: [40] <author> M. Crochemore and W. Rytter. </author> <title> Usefulness of the Karp-Miller-Rosenberg algorithm in parallel computations on strings and arrays. </title> <journal> Theoret. Comput. Sci., </journal> <volume> 88 </volume> <pages> 59-82, </pages> <year> 1991. </year>
Reference-contexts: Again, we distinguish between two cases. 3. A fixed alphabet Under this assumption the symbols of the alphabet are integers from a small range. This assumption is usually used in the literature to allow direct indexing of an array by an alphabet symbol <ref> [11, 40, 59, 62] </ref> 4. A constant alphabet Under this assumption the symbols of the alphabet are integers from a constant range. As in the case of a fixed alphabet, input symbols can be used for direct indexing of an array of a constant size. <p> This algorithm works under the constant alphabet assumption and is also the only optimal EREW-PRAM parallel algorithm known. All other optimal algorithms require the more powerful CRCW-PRAM model. Other parallel algorithms based on Karp, Miller and Rosenberg's [59] method <ref> [40, 62] </ref> also work in O (log n) time over a fixed alphabet. This method is useful for many other string problems but it seems to require O (log n) time and a larger memory space. Vishkin [88] developed an optimal O (log fl m) 2 time algorithm. <p> Galil and Seiferas [50] discovered an on-line algorithm that can recognize strings in both versions of Palstar in linear time. In parallel computation, Crochemore and Perrin <ref> [40] </ref> consider the problems of finding all palindromes in a string and recognizing strings in Palstar and give O (log n) time n log n - processor algorithms. 64 5.2 An O (log log n) Time Algorithm Theorem 5.1 All initial palindromes of a string S can be computed in O <p> Does a fast parallel algorithm that finds all palindromes of a string exist? The best result so far is an O (log n) time n-processor algorithm of Crochemore and Rytter <ref> [40] </ref> . 2. Does a fast parallel algorithm that recognizes strings that are in Palstar exist? The best result so far is an O (log n) time n-processor algorithm [40] that works only for some special cases. 71 Chapter 6 Conclusion The research reported in this thesis has been focused on <p> of a string exist? The best result so far is an O (log n) time n-processor algorithm of Crochemore and Rytter <ref> [40] </ref> . 2. Does a fast parallel algorithm that recognizes strings that are in Palstar exist? The best result so far is an O (log n) time n-processor algorithm [40] that works only for some special cases. 71 Chapter 6 Conclusion The research reported in this thesis has been focused on theoretical aspects of algorithm involving strings. In sequential computation we studied an interesting combinatorial problem of the number of comparisons required for string matching.
Reference: [41] <author> D. Eppstein and Z. Galil. </author> <title> Parallel algorithmic techniques for combinatorial computation. </title> <journal> In Ann. Rev. Comput. Sci., </journal> <volume> volume 3, </volume> <pages> pages 233-283, </pages> <year> 1988. </year> <month> 75 </month>
Reference-contexts: We assume that the reader is familiar with the PRAM family of models and the standard algorithmic techniques in this model. For an introduction to these models and the common algorithmic techniques see <ref> [41, 54, 61] </ref>. We present next a brief overview of some essential facts. The PRAM family of models consists of several synchronous processors that have access to a shared memory and each processor has the complete capabilities of a Random Access Machine.
Reference: [42] <author> F. E. Fich, R. L. Ragde, and A. Wigderson. </author> <title> Relations between concurrent-write models of parallel computation. </title> <booktitle> In Proc. 3rd ACM Symp. on Principles of Distributed Computing, </booktitle> <pages> pages 179-189, </pages> <year> 1984. </year>
Reference-contexts: The next theorem is used extensively in all the parallel algorithms that we describe. Theorem 1.14 (Fich, Ragde and Wigderson <ref> [42] </ref>) The minimum of n integers in the range 1::n can be computed by an n-processor CRCW-PRAM in constant time. 1.3.3 The Comparison Model The comparison model is an unrealistic model of computation that accounts only for comparisons and ignores computation and communication between processors. <p> The lower and upper bounds for the string matching problem over a general alphabet are identical to those for comparison based maximum finding algorithm obtained by Valiant [86]. A constant time algorithm can find the maximum of integers in a restricted range <ref> [42] </ref> what suggests the possibility of a faster string matching algorithm. 2. String matching with long text strings. If the text string is much longer than the pattern, the lower bound of Section 3.3 does not apply. <p> Using the algorithm of Fich, Ragde and Wigderson <ref> [42] </ref> we can find L and R in constant time and O (l ) operations. 2 We show that there is a substring xx of length l containing the block S [(k1)l +1::kl ] if an only if L R 1.
Reference: [43] <author> M. J. Fischer and M. S. Paterson. </author> <title> String matching and other products. </title> <editor> In R. M. Karp, editor, </editor> <booktitle> Complexity of Computation, </booktitle> <pages> pages 113-125. </pages> <publisher> American Mathematical Society, </publisher> <address> Prividence, RI., </address> <year> 1974. </year>
Reference-contexts: Two copies of the string w$w R are aligned with each other shifted by some offset and the overlapping parts are identical if and only if the overlapping part is an initial palindrome of w. This reduction was used by Fischer and Paterson <ref> [43] </ref>.
Reference: [44] <author> L. R. Ford and S. M. Johnson. </author> <title> A tournament problem. </title> <journal> American Mathematical Monthly, </journal> <volume> 66 </volume> <pages> 387-389, </pages> <year> 1959. </year>
Reference-contexts: The exact complexity of finding the third largest element is known for all but a finite number of cases [63, 64, 90]. The complexity of sorting is determined up to sorting 12 elements <ref> [3, 44] </ref>. The exact complexity of finding the median is another intriguing open problem: it has been known for several years that the number of comparisons required is between 2n [15] and 3n [81].
Reference: [45] <author> Z. Galil. </author> <title> Palindrome Recognition in Real Time by a Multitape Turing Machine. </title> <journal> J. Comput. System Sci., </journal> <volume> 16(2) </volume> <pages> 140-157, </pages> <year> 1978. </year>
Reference-contexts: A string S of odd length is a palindrome if and only if S = waw R for some string w and some alphabet symbol a. Palindromes were known as word puzzles for hundreds of years. Some palindromes from Galil's <ref> [45] </ref> paper are listed below. A man, a plan, a canal, Panama. Dennis and Edna Sinned. Red rum, sir, is murder. <p> This algorithm can be modified easily to find all initial palindromes and even all other palindromes in a string. Galil <ref> [45] </ref> improved Manacher's algorithm to a real-time algorithm. Knuth,Morris and Pratt [66] consider the problem of recognizing strings in Palstar, that is defined as P fl , when P is the set of all even palindromes (the concatenation of all even length palindromes).
Reference: [46] <author> Z. Galil. </author> <title> Optimal parallel algorithms for string matching. </title> <journal> Inform. and Control, </journal> <volume> 67 </volume> <pages> 144-157, </pages> <year> 1985. </year>
Reference-contexts: In addition to the advantages of a fixed alphabet, this assumption allows several algorithms in the literature to pack few input symbols into a single register and compare them in a single operation <ref> [7, 46, 73] </ref>. This idea is usually referred to as the four-Russions technique after the Boolean matrix multiplication algorithm that was invented by Arlazarov, Dinic, Kronrod and Faradzev [3, 13]. <p> We obtain a bound of fi (log log n) time on n log log n processors and a bound of fi (d n p e + log log d1+p=ne 2p) time on p processors. 3.1 Historical Overview The first optimal parallel string matching algorithm was discovered by Galil <ref> [46] </ref>. It solves the problem over a constant alphabet in O (log m) time on a n log m -processor CRCW-PRAM. This algorithm uses the four-Russions method and packs many input symbols into one register to compare them in a single operation. Galil's [46] algorithm was soon improved by Vishkin [87] <p> string matching algorithm was discovered by Galil <ref> [46] </ref>. It solves the problem over a constant alphabet in O (log m) time on a n log m -processor CRCW-PRAM. This algorithm uses the four-Russions method and packs many input symbols into one register to compare them in a single operation. Galil's [46] algorithm was soon improved by Vishkin [87] to an algorithm that works over a general alphabet. Breslauer and Galil [25] obtained an optimal O (log log m) time algorithm for a general alphabet. This algorithm is based on the algorithms of Galil [46] and Vishkin [87]. <p> Galil's <ref> [46] </ref> algorithm was soon improved by Vishkin [87] to an algorithm that works over a general alphabet. Breslauer and Galil [25] obtained an optimal O (log log m) time algorithm for a general alphabet. This algorithm is based on the algorithms of Galil [46] and Vishkin [87]. The algorithm that was a part of my Master's Thesis [24] is described later for completeness. An analysis of the algorithm for any number of available processors that was not included in the original work is also presented. <p> Assuming we can eliminate all but l of the possible occurrences (ignoring the problem of assigning the processors to their tasks), we can use the same method to get a constant time parallel algorithm that uses only lm processors. Both Galil <ref> [46] </ref> and Vishkin [87] use this approach. The only problem is that one can have many occurrences of the pattern in the text, even much more than the n=m needed for optimality in the discussion above. To eliminate this problem, we use properties of periods as suggested by Galil [46] and <p> Galil <ref> [46] </ref> and Vishkin [87] use this approach. The only problem is that one can have many occurrences of the pattern in the text, even much more than the n=m needed for optimality in the discussion above. To eliminate this problem, we use properties of periods as suggested by Galil [46] and also used by Vishkin [87]. Lemma 3.5 If the pattern P [1::m] has period length P 1 , then there cannot be two occurrences of the pattern at text positions i and j, for jj ij &lt; P 1 .
Reference: [47] <author> Z. Galil. </author> <title> A Constant-Time Optimal Parallel String-Matching Algorithm. </title> <booktitle> In Proc. 24th ACM Symp. </booktitle> <institution> on Theory of Computing, </institution> <note> page to appear, </note> <year> 1992. </year>
Reference-contexts: We partially settle this question by giving a lower bound of (log log m) for parallel string matching over a general alphabet. The lower bound proves that a slower preprocessing is crucial for Vishkin's algorithm. Vishkin's algorithm was improved recently by Galil <ref> [47] </ref> who discovered a constant time optimal algorithm. Galil's [47] new algorithm requires the same time as Vishkin's algorithm for the preprocessing step. 2 The function log fl m is defined as the smallest k such that log (k) m 2, where log (1) m = logm and log (i+1) m <p> The lower bound proves that a slower preprocessing is crucial for Vishkin's algorithm. Vishkin's algorithm was improved recently by Galil <ref> [47] </ref> who discovered a constant time optimal algorithm. Galil's [47] new algorithm requires the same time as Vishkin's algorithm for the preprocessing step. 2 The function log fl m is defined as the smallest k such that log (k) m 2, where log (1) m = logm and log (i+1) m = log log (i) m. 39 Apostolico [6] also
Reference: [48] <author> Z. Galil and R. Giancarlo. </author> <title> The exact complexity of string matching: upper bounds. </title> <type> manuscript, </type> <year> 1991. </year>
Reference-contexts: We distinguish between two classes of comparisons. 1. A general alphabet Under this assumption any pair of symbols can be compared to obtain an equal or unequal answer. This type of comparisons are sufficient to solve almost all problems that will be discussed in this thesis <ref> [32, 48, 52, 66, 72] </ref>. 2. <p> A recent work by Colussi [32] showed that the string matching problem can be solved using 3 2 n comparisons improving the previous bound of 2n that was achieved by several algorithms [22, 38, 66]. Colussi's bound was later improved to 4 3 n by Galil and Giancarlo <ref> [48] </ref> who proved also lower bounds that match their upper bound for short patterns [49]. These lower bounds were generalized and improved by Zwick and Paterson [91] who presented also other results that we will mention later. <p> However, some versions of our algorithms which are described in Sections 2.2.5 and 2.2.7 are almost identical to Colussi's [32] and Galil and Giancarlo's <ref> [48] </ref> algorithms if they are modified to use a buffer. The length of the text, n, can be much larger than the length of the pattern, m. To simplify the analysis, we assume that any pattern preprocessing is free and we do not account for it in our bounds. <p> Since we try to get matching lower and upper bounds, a lower bound in the comparison model has the advantage of the pattern string being fixed and not having to account for preprocessing it. Galil and Giancarlo <ref> [48] </ref> distinguish between on-line and off-line algorithm. <p> Colussi [32] showed that c online (n; m) 1:5n :5 (m 1) while the best previous bound was c online (n; m) 2n m [10, 38, 66]. Galil and Giancarlo <ref> [48] </ref> proved that the number of comparisons performed by Colussi's algorithm can be bounded also by n + b (n m) m c n + b nm 2 c, where 1 is the length of the shortest period of the pattern. <p> This bound is achieved also by an algorithm of Apostolico and Breslauer [8], that is obtained by a combination of the techniques of Knuth, Morris and Pratt [66] and Crochemore and Perrin [38], but is much easier to analyze. Galil and Giancarlo 18 <ref> [48] </ref> presented also an improved version of Colussi's algorithm that performs n + b (n m) min (1=3; 2m )c 4 3 m comparisons in the worst case. <p> They show that an on-line algorithm must take 5 4 n comparisons to find all occurrences of `abaa' in a text of length n (this bound is actually achieved by the Galil-Giancarlo <ref> [48] </ref> algorithm); while a 4-look-ahead algorithm can find all occurrences of the same pattern in 8 7 n comparisons, and this number of comparisons is necessary even for an off-line algorithm. 2.2 A Fast Algorithm In this section we study the exact number of comparisons necessary to solve the string matching <p> Note that no comparison is made. We call this text position a hole and record it for a later processing. See Figure 2.1 for an example. The similarity to the holes in the algorithms of Colussi [32] and Galil and Giancarlo <ref> [48] </ref> is discussed in Sections 2.2.5 and 2.2.7. * Invariant two is violated. This means that if we align copies of the pattern starting at all text positions in and look at the column under text position , then some symbols are different. <p> The proof now follows from the fact that the total shift of throughout the algorithm is n m and by adding the n comparisons charged to the text symbols. 2 2.2.7 Galil and Giancarlo's Algorithm Galil and Giancarlo <ref> [48] </ref> noticed that Colussi's [32] algorithm performs very badly if the pattern starts and ends with a sequence of repetitions of the same symbol. For these patterns Colussi's algorithm shifts by a single position and 3 2 n comparisons are actually performed. <p> Is a finite look-ahead sufficient? Namely, is there a finite value of l for which c l offline P (n)? 3. Galil and Giancarlo <ref> [48, 49] </ref> consider also the number of comparisons required by algorithms that only test if there is an occurrence of the pattern in the text and do not find all occurrences. What are the bounds for these type of problems? 4.
Reference: [49] <author> Z. Galil and R. Giancarlo. </author> <title> On the exact complexity of string matching: lower bounds. </title> <journal> SIAM J. Comput., </journal> <volume> 20(6) </volume> <pages> 1008-1020, </pages> <year> 1991. </year>
Reference-contexts: Colussi's bound was later improved to 4 3 n by Galil and Giancarlo [48] who proved also lower bounds that match their upper bound for short patterns <ref> [49] </ref>. These lower bounds were generalized and improved by Zwick and Paterson [91] who presented also other results that we will mention later. <p> Galil and Giancarlo 18 [48] presented also an improved version of Colussi's algorithm that performs n + b (n m) min (1=3; 2m )c 4 3 m comparisons in the worst case. In another paper, Galil and Giancarlo <ref> [49] </ref> prove that if m is an odd integer that is larger than or equal to three, then for infinitely many n's c online (n; m) n + b 2 (n m) c and if m is an even integer that is larger than three, then for infinitely many n's c <p> Other lower bounds for off-line algorithms and for on-line algorithms that search only for the first occurrence of the pattern in the text are also presented in the same paper. Zwick and Paterson [91] generalized the lower bound of Galil and Giancarlo <ref> [49] </ref> for on-line algorithms to c online (n; m) n + b n m c where 2 is the second period of the pattern if it exists, and 2 = 1 if the second period does not exist. This is a generalization of the Galil-Giancarlo [49] bound since it gives the <p> bound of Galil and Giancarlo <ref> [49] </ref> for on-line algorithms to c online (n; m) n + b n m c where 2 is the second period of the pattern if it exists, and 2 = 1 if the second period does not exist. This is a generalization of the Galil-Giancarlo [49] bound since it gives the same lower bounds for the pattern strings `a k ba k ' and `a k ba k+1 ' used by Galil and Giancarlo for their bounds. <p> Is a finite look-ahead sufficient? Namely, is there a finite value of l for which c l offline P (n)? 3. Galil and Giancarlo <ref> [48, 49] </ref> consider also the number of comparisons required by algorithms that only test if there is an occurrence of the pattern in the text and do not find all occurrences. What are the bounds for these type of problems? 4.
Reference: [50] <author> Z. Galil and J. Seiferas. </author> <title> A Linear-Time On-Line Recognition Algorithm for "Palstar". </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 25(1) </volume> <pages> 102-111, </pages> <year> 1978. </year>
Reference-contexts: They give a linear time algorithm for this problem. This algorithm does not extend to recognize strings in a different version of Palstar that includes concatenations of all non-trivial palindromes (length two or more, odd and even). Galil and Seiferas <ref> [50] </ref> discovered an on-line algorithm that can recognize strings in both versions of Palstar in linear time.
Reference: [51] <author> Z. Galil and J. Seiferas. </author> <title> Saving space in fast string-matching. </title> <journal> SIAM J. Comput., </journal> <volume> 2 </volume> <pages> 417-438, </pages> <year> 1980. </year>
Reference-contexts: A variant the Boyer-Moore algorithm that performs 2n comparisons in the worst case was designed by Apostolico and Giancarlo [10]. All these algorithms use an O (m) auxiliary space. At a certain time it was known that a logarithmic space solution was possible <ref> [51] </ref>, and the string matching problem was conjectured to have a time-space trade-off [21]. This conjecture was later disproved when a linear-time constant-space algorithm was discovered by Galil and Seiferas [38, 52]. It 16 was shown that even a 6-head two-way finite automaton can perform string matching in linear time.
Reference: [52] <author> Z. Galil and J. Seiferas. </author> <title> Time-space-optimal string matching. </title> <journal> J. Comput. System Sci., </journal> <volume> 26 </volume> <pages> 280-294, </pages> <year> 1983. </year>
Reference-contexts: We distinguish between two classes of comparisons. 1. A general alphabet Under this assumption any pair of symbols can be compared to obtain an equal or unequal answer. This type of comparisons are sufficient to solve almost all problems that will be discussed in this thesis <ref> [32, 48, 52, 66, 72] </ref>. 2. <p> At a certain time it was known that a logarithmic space solution was possible [51], and the string matching problem was conjectured to have a time-space trade-off [21]. This conjecture was later disproved when a linear-time constant-space algorithm was discovered by Galil and Seiferas <ref> [38, 52] </ref>. It 16 was shown that even a 6-head two-way finite automaton can perform string matching in linear time. It is still an open problem whether a k-head one-way finite automaton can perform string matching.
Reference: [53] <author> M. Gereb-Graus and M. Li. </author> <title> Three one-way heads cannot do string matching. </title> <type> manuscript, </type> <year> 1990. </year>
Reference-contexts: It 16 was shown that even a 6-head two-way finite automaton can perform string matching in linear time. It is still an open problem whether a k-head one-way finite automaton can perform string matching. The only known cases are for k = 1; 2; 3 <ref> [53, 68, 69] </ref> where the answer is negative. The Aho-Corasick [2] algorithm which can search for multiple patterns in linear time is also used widely in practice. This algorithm works under the constant alphabet assumption and it does not use character comparisons.
Reference: [54] <author> A. Gibbons and W. Rytter. </author> <title> Efficient Parallel Algorithms. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, Great Britain, </address> <year> 1988. </year>
Reference-contexts: The models that are used in this dissertation are standard models and their description can be found in several text books <ref> [3, 4, 54] </ref>. <p> We assume that the reader is familiar with the PRAM family of models and the standard algorithmic techniques in this model. For an introduction to these models and the common algorithmic techniques see <ref> [41, 54, 61] </ref>. We present next a brief overview of some essential facts. The PRAM family of models consists of several synchronous processors that have access to a shared memory and each processor has the complete capabilities of a Random Access Machine.
Reference: [55] <author> L. Guibas and A. M. Odlyzko. </author> <title> Periods in strings. </title> <journal> Journal of Combinatorial Theory, Series A, </journal> <volume> 30 </volume> <pages> 19-42, </pages> <year> 1981. </year>
Reference-contexts: We say that the prefix period extends to S [1::ff]. If ff &lt; m we say that the prefix period terminates at ff + 1. The set of periods of a string has a special structure as shown by Lyndon and Schutzen-berger [71]. Guibas and Odlyzko <ref> [55] </ref> and independently Colussi [31] gave a complete characterization of the possible structures this set can have. They have shown also that the set of periods is independent of the alphabet size. Their results are summarized in the following two lemmata. <p> Lemma 1.7 (Characterization of all sets of periods of a string <ref> [31, 55] </ref>) Let = f i j0 = 0 &lt; 1 &lt; &lt; z = kg and let ffi i = i+1 i , for i = 0::z 1.
Reference: [56] <author> T. Hagerup. </author> <title> Constant-Time Parallel Integer Sorting. </title> <booktitle> In Proc. 23rd ACM Symp. on Theory of Computing, </booktitle> <pages> pages 299-306, </pages> <year> 1991. </year> <month> 76 </month>
Reference-contexts: On the other hand, Theorem 1.13 implies that such a sequence cannot be sorted in less than log n log log n time with any polynomial 3 number of processors. An intermediate compromise that is called chain sorting <ref> [56] </ref> can be implemented more efficiently on a PRAM. * Lower bounds on the comparison model do not hold for RAM or PRAM algorithms that can access the input by operations other than comparisons.
Reference: [57] <author> J. E. Hopcroft and J. D. Ullman. </author> <title> Introduction to Automata Theory, Languages and Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA., </address> <year> 1979. </year>
Reference-contexts: We denote by S the set of symbols that are included in a string S. Many problems in computer science and other fields can be formalized in terms of strings. In fact, the whole theory of computation can be formalized in terms of machines that manipulate strings <ref> [57] </ref>. In this section we describe some basic properties of strings that will be used in the sequel. 1.2.1 The Alphabet The assumptions we make on the alphabet that the strings are chosen from has a crucial effect on the efficiency of the algorithms that can solve certain problems. <p> An other example comes from formal language theory: the language of all strings that contain palindromes over a given alphabet is easily shown to be context-free <ref> [57] </ref>, while the language of all strings that contain squares is not context-free as shown by Ross and Winklmann [79]. In the sequential setting, we study the exact number of comparisons that need to be performed by any comparison based string matching algorithm. <p> Although the definitions of squares and palindromes are very similar, there is a big computational difference between these two terms. An example from formal language is that the language of all strings that contain palindromes over a given alphabet is easily shown to be context-free <ref> [57] </ref>, while the language of all strings that contain squares is not context-free as shown by Ross and Winklmann [79].
Reference: [58] <author> A. Hume. </author> <title> A tale of two greps. </title> <journal> Software Practice and Experience, </journal> <volume> 18 </volume> <pages> 1063-1072, </pages> <year> 1988. </year>
Reference-contexts: Although linear-time string matching algorithms exist for quite some time, there is a hugh difference between the efficiency of the known algorithms <ref> [58] </ref>. In parallel computation we studied the complexity of the string matching problem and three other problems and provided tight lower and upper bounds under the general alphabet assumption.
Reference: [59] <author> R. M. Karp, R. E. Miller, and A. L. Rosenberg. </author> <title> Rapid identification of repeated patterns in strings, trees and arrays. </title> <booktitle> In Proc. 4th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 125-136, </pages> <year> 1972. </year>
Reference-contexts: Again, we distinguish between two cases. 3. A fixed alphabet Under this assumption the symbols of the alphabet are integers from a small range. This assumption is usually used in the literature to allow direct indexing of an array by an alphabet symbol <ref> [11, 40, 59, 62] </ref> 4. A constant alphabet Under this assumption the symbols of the alphabet are integers from a constant range. As in the case of a fixed alphabet, input symbols can be used for direct indexing of an array of a constant size. <p> This algorithm works under the constant alphabet assumption and is also the only optimal EREW-PRAM parallel algorithm known. All other optimal algorithms require the more powerful CRCW-PRAM model. Other parallel algorithms based on Karp, Miller and Rosenberg's <ref> [59] </ref> method [40, 62] also work in O (log n) time over a fixed alphabet. This method is useful for many other string problems but it seems to require O (log n) time and a larger memory space. Vishkin [88] developed an optimal O (log fl m) 2 time algorithm.
Reference: [60] <author> R. M. Karp and M. O. Rabin. </author> <title> Efficient randomized pattern matching algorithms. </title> <journal> IBM J. Res. Develop., </journal> <volume> 31(2) </volume> <pages> 249-260, </pages> <year> 1987. </year>
Reference-contexts: As in the case of a fixed alphabet, input symbols can be used for direct indexing of an array of a constant size. This is especially useful for fast implementations of algorithms that use an automaton description or trie data structures <ref> [2, 5, 35, 60, 75, 89] </ref>. In addition to the advantages of a fixed alphabet, this assumption allows several algorithms in the literature to pack few input symbols into a single register and compare them in a single operation [7, 46, 73]. <p> An analysis of the algorithm for any number of available processors that was not included in the original work is also presented. The simplest known parallel string matching algorithm is perhaps the optimal O (log n) time randomized algorithm of Karp and Rabin <ref> [60] </ref>. This algorithm works under the constant alphabet assumption and is also the only optimal EREW-PRAM parallel algorithm known. All other optimal algorithms require the more powerful CRCW-PRAM model.
Reference: [61] <author> R. M. Karp and V. Ramachandran. </author> <title> Parallel algorithms for shared-memory machines. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, </booktitle> <pages> pages 869-941. </pages> <publisher> Elsevier Science Publishers B. V., </publisher> <address> Amsterdam, the Netherlands, </address> <year> 1990. </year>
Reference-contexts: We assume that the reader is familiar with the PRAM family of models and the standard algorithmic techniques in this model. For an introduction to these models and the common algorithmic techniques see <ref> [41, 54, 61] </ref>. We present next a brief overview of some essential facts. The PRAM family of models consists of several synchronous processors that have access to a shared memory and each processor has the complete capabilities of a Random Access Machine.
Reference: [62] <author> Z. Kedem, G. M. Landau, and K. Palem. </author> <title> Optimal parallel suffix-prefix matching algorithm and applications. </title> <type> manuscript, </type> <year> 1988. </year>
Reference-contexts: Again, we distinguish between two cases. 3. A fixed alphabet Under this assumption the symbols of the alphabet are integers from a small range. This assumption is usually used in the literature to allow direct indexing of an array by an alphabet symbol <ref> [11, 40, 59, 62] </ref> 4. A constant alphabet Under this assumption the symbols of the alphabet are integers from a constant range. As in the case of a fixed alphabet, input symbols can be used for direct indexing of an array of a constant size. <p> This algorithm works under the constant alphabet assumption and is also the only optimal EREW-PRAM parallel algorithm known. All other optimal algorithms require the more powerful CRCW-PRAM model. Other parallel algorithms based on Karp, Miller and Rosenberg's [59] method <ref> [40, 62] </ref> also work in O (log n) time over a fixed alphabet. This method is useful for many other string problems but it seems to require O (log n) time and a larger memory space. Vishkin [88] developed an optimal O (log fl m) 2 time algorithm.
Reference: [63] <author> D. G. Kirkpatrick. </author> <title> Topics in the complexity of combinatorial algorithms. </title> <type> Technical report, </type> <institution> Dept. of Computer Science, University of Toronto, Toronto, Canada, </institution> <year> 1974. </year>
Reference-contexts: The exact complexity of finding the third largest element is known for all but a finite number of cases <ref> [63, 64, 90] </ref>. The complexity of sorting is determined up to sorting 12 elements [3, 44]. The exact complexity of finding the median is another intriguing open problem: it has been known for several years that the number of comparisons required is between 2n [15] and 3n [81].
Reference: [64] <author> D. G. Kirkpatrick. </author> <title> A unified lower bound for selection and set partitioning problems. </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 28 </volume> <pages> 150-165, </pages> <year> 1981. </year>
Reference-contexts: The exact complexity of finding the third largest element is known for all but a finite number of cases <ref> [63, 64, 90] </ref>. The complexity of sorting is determined up to sorting 12 elements [3, 44]. The exact complexity of finding the median is another intriguing open problem: it has been known for several years that the number of comparisons required is between 2n [15] and 3n [81].
Reference: [65] <author> S. S. Kislitsyn. </author> <title> On the selection of the kth element of an ordered set by pairwise comparison. </title> <journal> Sibirskii Mat. Zhurnal, </journal> <volume> 5 </volume> <pages> 557-564, </pages> <year> 1964. </year>
Reference-contexts: It has been shown also that computing both the minimum and maximum requires d 3 2 n 2e comparisons [76] and that computing the largest two elements requires n 1 + dlog ne comparisons <ref> [65, 82] </ref>. The exact complexity of finding the third largest element is known for all but a finite number of cases [63, 64, 90]. The complexity of sorting is determined up to sorting 12 elements [3, 44].
Reference: [66] <author> D. E. Knuth, J. H. Morris, and V. R. Pratt. </author> <title> Fast pattern matching in strings. </title> <journal> SIAM J. Comput., </journal> <volume> 6 </volume> <pages> 322-350, </pages> <year> 1977. </year>
Reference-contexts: We distinguish between two classes of comparisons. 1. A general alphabet Under this assumption any pair of symbols can be compared to obtain an equal or unequal answer. This type of comparisons are sufficient to solve almost all problems that will be discussed in this thesis <ref> [32, 48, 52, 66, 72] </ref>. 2. <p> Most of these algorithms work in two phases: in the first phase the pattern is preprocessed and some information is computed to be used later in a text processing phase that finds the actual occurrences of the pattern in the text. The Knuth-Morris-Pratt <ref> [66] </ref> algorithm is the first linear time that was discovered. It dates back to 1969 when Morris implemented an early version of the algorithm in a text-editor for the CDC 6400 computer. The discovery of the algorithm has a very interesting history as reported in the paper [66] that was published <p> The Knuth-Morris-Pratt <ref> [66] </ref> algorithm is the first linear time that was discovered. It dates back to 1969 when Morris implemented an early version of the algorithm in a text-editor for the CDC 6400 computer. The discovery of the algorithm has a very interesting history as reported in the paper [66] that was published only in 1977. Most notable probably is the fact that a result of Cook [33] on linear-time simulation of two-way deterministic pushdown automaton by a random access machine implied that a linear-time string matching algorithm exists. <p> Knuth went through the laborious details of Cook's construction and discovered the algorithm. The algorithm reported in the final paper is a refined version of these two independent discoveries. The Knuth-Morris-Pratt <ref> [66] </ref> algorithm performs at most 2n comparisons in its text processing phase. The Boyer-Moore algorithm performs 3n comparisons in the worst case as proven recently by Cole [29], but seems to perform better in practice. <p> A recent work by Colussi [32] showed that the string matching problem can be solved using 3 2 n comparisons improving the previous bound of 2n that was achieved by several algorithms <ref> [22, 38, 66] </ref>. Colussi's bound was later improved to 4 3 n by Galil and Giancarlo [48] who proved also lower bounds that match their upper bound for short patterns [49]. <p> We define also c l (n; m) to be the maximum of c l P (n) over all pattern strings P of length m. Colussi [32] showed that c online (n; m) 1:5n :5 (m 1) while the best previous bound was c online (n; m) 2n m <ref> [10, 38, 66] </ref>. Galil and Giancarlo [48] proved that the number of comparisons performed by Colussi's algorithm can be bounded also by n + b (n m) m c n + b nm 2 c, where 1 is the length of the shortest period of the pattern. <p> This bound is achieved also by an algorithm of Apostolico and Breslauer [8], that is obtained by a combination of the techniques of Knuth, Morris and Pratt <ref> [66] </ref> and Crochemore and Perrin [38], but is much easier to analyze. Galil and Giancarlo 18 [48] presented also an improved version of Colussi's algorithm that performs n + b (n m) min (1=3; 2m )c 4 3 m comparisons in the worst case. <p> If the number of available processors is p we show that the algorithm takes O (d n p e + log log d1+p=ne 2p) time. The period of a string is computed in linear time as a step in Knuth, Morris and Pratt's sequential string matching algorithm <ref> [66] </ref> and in optimal O (log log n) parallel time on a CRCW-PRAM as shown in Section 3.2.4. <p> This algorithm can be modified easily to find all initial palindromes and even all other palindromes in a string. Galil [45] improved Manacher's algorithm to a real-time algorithm. Knuth,Morris and Pratt <ref> [66] </ref> consider the problem of recognizing strings in Palstar, that is defined as P fl , when P is the set of all even palindromes (the concatenation of all even length palindromes). They give a linear time algorithm for this problem.
Reference: [67] <author> C. P. Kruskal. </author> <title> Searching, merging, and sorting in parallel computation. </title> <journal> IEEE trans. on computers 32, </journal> <volume> 32 </volume> <pages> 942-946, </pages> <year> 1983. </year>
Reference-contexts: Optimal solutions are usually more difficult to obtain and make up a major part of this dissertation. There are many other problems that have fast parallel algorithms that get around the lower bound of Theorem 1.13 <ref> [16, 17, 20, 67, 83, 86] </ref>. Lemma 3.2 The string matching problem can be solved in constant time by an nm processor CRCW-PRAM.
Reference: [68] <author> M. Li. </author> <title> Lower bounds on string-matching. </title> <type> Technical Report TR 84-63, </type> <institution> Cornell University, Department of Computer Science, </institution> <year> 1984. </year>
Reference-contexts: It 16 was shown that even a 6-head two-way finite automaton can perform string matching in linear time. It is still an open problem whether a k-head one-way finite automaton can perform string matching. The only known cases are for k = 1; 2; 3 <ref> [53, 68, 69] </ref> where the answer is negative. The Aho-Corasick [2] algorithm which can search for multiple patterns in linear time is also used widely in practice. This algorithm works under the constant alphabet assumption and it does not use character comparisons.
Reference: [69] <author> M. Li and Y. Yesha. </author> <title> String-matching cannot be done by a two-head one-way deterministic finite automaton. </title> <journal> Inform. Process. Lett., </journal> <volume> 22 </volume> <pages> 231-235, </pages> <year> 1986. </year>
Reference-contexts: It 16 was shown that even a 6-head two-way finite automaton can perform string matching in linear time. It is still an open problem whether a k-head one-way finite automaton can perform string matching. The only known cases are for k = 1; 2; 3 <ref> [53, 68, 69] </ref> where the answer is negative. The Aho-Corasick [2] algorithm which can search for multiple patterns in linear time is also used widely in practice. This algorithm works under the constant alphabet assumption and it does not use character comparisons.
Reference: [70] <author> M. </author> <title> Lothaire. Combinatorics on Words. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. U. S.A., </address> <year> 1983. </year>
Reference-contexts: A non-trivial factorization of a string S is called a critical factorization if the local period at the factorization is of the same length as the period of S. The following lemma shows that critical factorizations exist. Theorem 1.8 (The critical factorization theorem <ref> [28, 70] </ref>) Let S [1::k] be a string that has period length S 1 . Then, if we consider any S 1 1 consecutive non-trivial factorizations at least one is a critical factorization. 1.2.4 Squares A string S is called primitive if it is not a repetition of shorter string.
Reference: [71] <author> R. C. Lyndon and M. P. Schutzenberger. </author> <title> The equation a m = b n c p in a free group. </title> <journal> Michigan Math. J., </journal> <volume> 9 </volume> <pages> 289-298, </pages> <year> 1962. </year>
Reference-contexts: We say that the prefix period extends to S [1::ff]. If ff &lt; m we say that the prefix period terminates at ff + 1. The set of periods of a string has a special structure as shown by Lyndon and Schutzen-berger <ref> [71] </ref>. Guibas and Odlyzko [55] and independently Colussi [31] gave a complete characterization of the possible structures this set can have. They have shown also that the set of periods is independent of the alphabet size. Their results are summarized in the following two lemmata. <p> They have shown also that the set of periods is independent of the alphabet size. Their results are summarized in the following two lemmata. Lemma 1.6 (The periodicity lemma of Lyndon and Schutzenberger <ref> [71] </ref>) If a string S [1::k] has two periods p and q that satisfy p + q k + gcd (p; q), then S has also a period of length gcd (p; q).
Reference: [72] <author> G. M. Main and R. J. Lorentz. </author> <title> An O(n log n) algorithm for finding all repetitions in a string. </title> <journal> J. Algorithms, </journal> <volume> 5 </volume> <pages> 422-432, </pages> <year> 1984. </year> <month> 77 </month>
Reference-contexts: We distinguish between two classes of comparisons. 1. A general alphabet Under this assumption any pair of symbols can be compared to obtain an equal or unequal answer. This type of comparisons are sufficient to solve almost all problems that will be discussed in this thesis <ref> [32, 48, 52, 66, 72] </ref>. 2. <p> Manacher [74] gave a simple and elegant linear-time algorithm that can find all palindromes in a string. However, O (n log n) time is necessary even to test if a string over a general alphabet contains a square, as shown by Main and Lorentz <ref> [72] </ref>. An other example comes from formal language theory: the language of all strings that contain palindromes over a given alphabet is easily shown to be context-free [57], while the language of all strings that contain squares is not context-free as shown by Ross and Winklmann [79]. <p> Algorithms by Apostolico and Preparata [12], by Crochemore [35], Rabin [77] and by Main and Lorentz <ref> [72] </ref> find all the squares in a string of length n in O (n log n) time. Main and Lorentz [72] also show that O (n log n) comparisons are necessary even to decide if a string is square-free. <p> Algorithms by Apostolico and Preparata [12], by Crochemore [35], Rabin [77] and by Main and Lorentz <ref> [72] </ref> find all the squares in a string of length n in O (n log n) time. Main and Lorentz [72] also show that O (n log n) comparisons are necessary even to decide if a string is square-free. In another paper, Main and Lorentz [73] show that the latter problem of deciding whether a string is square-free can be solved in O (n) time on a constant alphabet. <p> Proof: Any optimal algorithm performs at most n log n comparisons in each round since the fastest sequential algorithm that solves this problem takes O (n log n) time as shown by Main and Lorentz <ref> [72] </ref>. Assume an algorithm performs p = n log n comparisons in each round. <p> 4.6 after (log log n) rounds the adversary can decide if S [1::n] is square-free or not, fooling any algorithm which terminates in less rounds. 2 4.4 Open Problems There are few open questions about squares in sequential and parallel computation: 62 * The lower bound of Main and Lorentz <ref> [72] </ref> is for a general alphabet and it does not hold when order comparisons are allowed. <p> It is unknown if a linear time sequential algorithm to test for squares is possible under the ordered alphabet assumption. * The only algorithm for squares that works over a general alphabet is the off-line algorithm of Main and Lorentz <ref> [72] </ref>. It is unknown if an on-line algorithm exists over a general alphabet. 63 Chapter 5 Parallel Algorithms for Finding Palindromes Palindromes have been studied for centuries as word puzzles and have some uses in formal language theory.
Reference: [73] <author> G. M. Main and R. J. Lorentz. </author> <title> Linear time recognition of squarefree strings. </title> <editor> In A. Apostolico and Z. Galil, editors, </editor> <booktitle> Combinatorial Algorithms on Words, NATO ASI Series F, </booktitle> <volume> Vol 12, </volume> <pages> pages 271-278. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany, </address> <year> 1985. </year>
Reference-contexts: In addition to the advantages of a fixed alphabet, this assumption allows several algorithms in the literature to pack few input symbols into a single register and compare them in a single operation <ref> [7, 46, 73] </ref>. This idea is usually referred to as the four-Russions technique after the Boolean matrix multiplication algorithm that was invented by Arlazarov, Dinic, Kronrod and Faradzev [3, 13]. <p> Main and Lorentz [72] also show that O (n log n) comparisons are necessary even to decide if a string is square-free. In another paper, Main and Lorentz <ref> [73] </ref> show that the latter problem of deciding whether a string is square-free can be solved in O (n) time on a constant alphabet. Crochemore [36] also gave a linear time algorithm for this problem. <p> We give tight bounds for solving the problem on a general alphabet for any number of processors used. The algorithm described in this chapter is a parallel version of the sequential algorithm of Main and Lorentz <ref> [73] </ref>. 4.2 An O (log log n) Time Algorithm In this section we describe optimal parallel algorithm for testing if a string is square-free on the CRCW-PRAM model.
Reference: [74] <author> G. Manacher. </author> <title> A new Linear-Time "On-Line" Algorithm for Finding the Smallest Initial Palindrome of a String. </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 22, </volume> <year> 1975. </year>
Reference-contexts: Palindromes have been studied for centuries as word puzzles and have some uses in formal language theory. Although the definitions of squares and palindromes are very similar, there is a big computational difference between these two terms. Manacher <ref> [74] </ref> gave a simple and elegant linear-time algorithm that can find all palindromes in a string. However, O (n log n) time is necessary even to test if a string over a general alphabet contains a square, as shown by Main and Lorentz [72]. <p> We prove also a matching lower bound for this problem under the assumption of a general alphabet. 5.1 Historical Overview Manacher <ref> [74] </ref> presented an on-line algorithm that finds the smallest initial palindromes of a string. This algorithm can be modified easily to find all initial palindromes and even all other palindromes in a string. Galil [45] improved Manacher's algorithm to a real-time algorithm.
Reference: [75] <author> E. M. McCreight. </author> <title> A space economical suffix tree construction algorithm. </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 23 </volume> <pages> 262-272, </pages> <year> 1976. </year>
Reference-contexts: As in the case of a fixed alphabet, input symbols can be used for direct indexing of an array of a constant size. This is especially useful for fast implementations of algorithms that use an automaton description or trie data structures <ref> [2, 5, 35, 60, 75, 89] </ref>. In addition to the advantages of a fixed alphabet, this assumption allows several algorithms in the literature to pack few input symbols into a single register and compare them in a single operation [7, 46, 73].
Reference: [76] <author> I. Pohl. </author> <title> A sorting problem and its complexity. </title> <journal> Comm. of the ACM, </journal> <volume> 15 </volume> <pages> 462-464, </pages> <year> 1972. </year>
Reference-contexts: It is easy to show that n 1 comparisons are necessary and sufficient to find the minimum or the maximum of n elements. It has been shown also that computing both the minimum and maximum requires d 3 2 n 2e comparisons <ref> [76] </ref> and that computing the largest two elements requires n 1 + dlog ne comparisons [65, 82]. The exact complexity of finding the third largest element is known for all but a finite number of cases [63, 64, 90].
Reference: [77] <author> M. O. Rabin. </author> <title> Discovering Repetitions in Strings. </title> <editor> In A. Apostolico and Z. Galil, editors, </editor> <booktitle> Combinatorial Algorithms on Words, NATO ASI Series F, </booktitle> <volume> Vol 12, </volume> <pages> pages 279-288. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany, </address> <year> 1984. </year>
Reference-contexts: Algorithms by Apostolico and Preparata [12], by Crochemore [35], Rabin <ref> [77] </ref> and by Main and Lorentz [72] find all the squares in a string of length n in O (n log n) time. Main and Lorentz [72] also show that O (n log n) comparisons are necessary even to decide if a string is square-free.
Reference: [78] <author> P. Ragde. </author> <title> The parallel simplicity of compaction and chaining. </title> <booktitle> In Proc. 17th International Colluquium on Automata, Languages, and Programming, Lecture Notes in Computer Science, </booktitle> <pages> pages 774-751. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany, </address> <year> 1990. </year>
Reference-contexts: Another possible representation that can be computed even more efficiently by algorithms of Berkman and Vishkin [18] and Ragde <ref> [78] </ref> is a linked list of all the occurrences. Note that Theorem 3.1 does not imply that the string matching problem as we defined it cannot be solved faster.
Reference: [79] <author> R. Ross and R. Winklmann. </author> <title> Repetitive strings are not context-free. </title> <type> Technical Report CS-81-070, </type> <institution> Washington State University, </institution> <address> Pullman, WA, </address> <year> 1981. </year>
Reference-contexts: An other example comes from formal language theory: the language of all strings that contain palindromes over a given alphabet is easily shown to be context-free [57], while the language of all strings that contain squares is not context-free as shown by Ross and Winklmann <ref> [79] </ref>. In the sequential setting, we study the exact number of comparisons that need to be performed by any comparison based string matching algorithm. The following result is presented: * A new algorithm that performs better for longer patterns. <p> An example from formal language is that the language of all strings that contain palindromes over a given alphabet is easily shown to be context-free [57], while the language of all strings that contain squares is not context-free as shown by Ross and Winklmann <ref> [79] </ref>. We show how the algorithm that finds all periods of a string from Section 3.4 can be used also to find all initial palindromes in the same time and processor bounds.
Reference: [80] <author> J. B. Rosser and L. Schoenfeld. </author> <title> Approximate formulas for some functions of prime numbers. </title> <journal> Illinois Journal of Mathematics, </journal> <volume> 6 </volume> <pages> 64-94, </pages> <year> 1962. </year>
Reference-contexts: A comparison S [l] = S [j] must be answered as equal if l j mod k i+1 . We say that k i+1 forces this comparison. Theorem 3.11 (Rosser and Schoenfeld <ref> [80] </ref>) For large enough 3 n, the number of primes between 1 and n denoted by (n) satisfies, n ln n (n) 5 n Corollary 3.12 The number of primes between 1 2 n and n is greater than 1 4 log n .
Reference: [81] <author> A. Schonhage, M. Paterson, and N. Pippenger. </author> <title> Finding the median. </title> <journal> J. Comput. System Sci., </journal> <volume> 13 </volume> <pages> 184-199, </pages> <year> 1976. </year>
Reference-contexts: The complexity of sorting is determined up to sorting 12 elements [3, 44]. The exact complexity of finding the median is another intriguing open problem: it has been known for several years that the number of comparisons required is between 2n [15] and 3n <ref> [81] </ref>. In this chapter we present a new string matching algorithm with improved bounds on the number of comparisons performed.
Reference: [82] <author> J. Schreier. </author> <title> On tournament elimination systems. </title> <journal> Mathesis Polska, </journal> <volume> 7 </volume> <pages> 154-160, </pages> <year> 1932. </year>
Reference-contexts: It has been shown also that computing both the minimum and maximum requires d 3 2 n 2e comparisons [76] and that computing the largest two elements requires n 1 + dlog ne comparisons <ref> [65, 82] </ref>. The exact complexity of finding the third largest element is known for all but a finite number of cases [63, 64, 90]. The complexity of sorting is determined up to sorting 12 elements [3, 44].
Reference: [83] <author> Y. Shiloach and U. Vishkin. </author> <title> Finding the maximum, merging and sorting in a parallel computation model. </title> <journal> J. Algorithms, </journal> <volume> 2 </volume> <pages> 88-102, </pages> <year> 1981. </year>
Reference-contexts: Optimal solutions are usually more difficult to obtain and make up a major part of this dissertation. There are many other problems that have fast parallel algorithms that get around the lower bound of Theorem 1.13 <ref> [16, 17, 20, 67, 83, 86] </ref>. Lemma 3.2 The string matching problem can be solved in constant time by an nm processor CRCW-PRAM. <p> Theorem 1.11 as well as other computations described below require the 40 assignment of processors to their tasks which in our case is done using standard techniques as in Shiloach and Vishkin's <ref> [83] </ref> maximum finding algorithm. 3.2.1 Witnesses An important idea in our algorithm is a method suggested by Vishkin [87], that enables us to eliminate many possible occurrences in constant time. <p> We describe an optimal O (log log m) time text analysis algorithm that works similarly to the maximum finding algorithm of Shiloach 42 and Vishkin <ref> [83] </ref> and is based on having W [2 r], for r = min ( P 1 ; dm=2e) computed in the pattern analysis phase.
Reference: [84] <author> A. </author> <title> Thue. Uber unendliche zeichenreihen. </title> <journal> Norske Vid. Selsk. Skr. Mat. Nat. Kl. (Cristiania), </journal> (7):1-22, 1906. 
Reference-contexts: It is trivial to show that any string of length larger than three on an alphabet of two symbols contains a square. However, there exist strings of infinite length on three letter alphabet that are square-free as shown by Axel Thue <ref> [84, 85] </ref> at the beginning of the century.
Reference: [85] <author> A. </author> <title> Thue. </title> <journal> Uber die gegenseitige lage gleicher teile gewisser zeichenreihen. Norske Vid. Selsk. Skr. Mat. Nat. Kl. (Cristiania), </journal> (1):1-67, 1912. 
Reference-contexts: It is trivial to show that any string of length larger than three on an alphabet of two symbols contains a square. However, there exist strings of infinite length on three letter alphabet that are square-free as shown by Axel Thue <ref> [84, 85] </ref> at the beginning of the century.
Reference: [86] <author> L. G. Valiant. </author> <title> Parallelism in comparison models. </title> <journal> SIAM J. Comput., </journal> <volume> 4 </volume> <pages> 348-355, </pages> <year> 1975. </year>
Reference-contexts: Optimal solutions are usually more difficult to obtain and make up a major part of this dissertation. There are many other problems that have fast parallel algorithms that get around the lower bound of Theorem 1.13 <ref> [16, 17, 20, 67, 83, 86] </ref>. Lemma 3.2 The string matching problem can be solved in constant time by an nm processor CRCW-PRAM. <p> Proof: The proof follows from Lemma 3.8 and Lemma 3.9. 2 3.3 An (log log n) Lower Bound In this section we describe a lower bound for a model that is similar to Valiant's parallel comparison tree model <ref> [86] </ref>. We assume the only access the algorithm has to the input strings is by comparisons that check whether two symbols are equal or not. The algorithm is allowed m comparisons in each round, after which it can proceed to the next round or 47 terminate with the answer. <p> The lower bound also does not hold for CRCW-PRAM over a fixed alphabet strings, but no better algorithm is known at the moment. Similarly, finding the maximum in the parallel decision tree model has exactly the same lower bound <ref> [86] </ref>, but for small integers the maximum can be found in constant time on a CRCW-PRAM as shown in Theorem 1.14. We start by proving a lower bound for a related problem of finding the period length of a string. <p> The lower and upper bounds for the string matching problem over a general alphabet are identical to those for comparison based maximum finding algorithm obtained by Valiant <ref> [86] </ref>. A constant time algorithm can find the maximum of integers in a restricted range [42] what suggests the possibility of a faster string matching algorithm. 2. String matching with long text strings.
Reference: [87] <author> U. Vishkin. </author> <title> Optimal parallel pattern matching in strings. </title> <journal> Inform. and Control, </journal> <volume> 67 </volume> <pages> 91-113, </pages> <year> 1985. </year>
Reference-contexts: It solves the problem over a constant alphabet in O (log m) time on a n log m -processor CRCW-PRAM. This algorithm uses the four-Russions method and packs many input symbols into one register to compare them in a single operation. Galil's [46] algorithm was soon improved by Vishkin <ref> [87] </ref> to an algorithm that works over a general alphabet. Breslauer and Galil [25] obtained an optimal O (log log m) time algorithm for a general alphabet. This algorithm is based on the algorithms of Galil [46] and Vishkin [87]. <p> Galil's [46] algorithm was soon improved by Vishkin <ref> [87] </ref> to an algorithm that works over a general alphabet. Breslauer and Galil [25] obtained an optimal O (log log m) time algorithm for a general alphabet. This algorithm is based on the algorithms of Galil [46] and Vishkin [87]. The algorithm that was a part of my Master's Thesis [24] is described later for completeness. An analysis of the algorithm for any number of available processors that was not included in the original work is also presented. <p> Assuming we can eliminate all but l of the possible occurrences (ignoring the problem of assigning the processors to their tasks), we can use the same method to get a constant time parallel algorithm that uses only lm processors. Both Galil [46] and Vishkin <ref> [87] </ref> use this approach. The only problem is that one can have many occurrences of the pattern in the text, even much more than the n=m needed for optimality in the discussion above. <p> The only problem is that one can have many occurrences of the pattern in the text, even much more than the n=m needed for optimality in the discussion above. To eliminate this problem, we use properties of periods as suggested by Galil [46] and also used by Vishkin <ref> [87] </ref>. Lemma 3.5 If the pattern P [1::m] has period length P 1 , then there cannot be two occurrences of the pattern at text positions i and j, for jj ij &lt; P 1 . <p> Theorem 1.11 as well as other computations described below require the 40 assignment of processors to their tasks which in our case is done using standard techniques as in Shiloach and Vishkin's [83] maximum finding algorithm. 3.2.1 Witnesses An important idea in our algorithm is a method suggested by Vishkin <ref> [87] </ref>, that enables us to eliminate many possible occurrences in constant time. One computes some information about the pattern which is called the witness array and denoted by W [1::m], and uses it in the second phase for the analysis of the text. <p> Having computed the W array in the pattern preprocessing phase, Vishkin <ref> [87] </ref> suggests the following method that he calls a duel to eliminate close possible occurrences. Suppose we suspect that an occurrence of the pattern may start at text positions i and j, where 0 &lt; j i &lt; P 1 . <p> While in the non-periodic case, the witness information 41 i j i+r-1 j-i+1 r r+i-j is based on the whole pattern and positions where there is no occurrence of it can be eliminated. Having many such duels in pairs, Vishkin's <ref> [87] </ref> algorithm eliminates enough possible occurrences of P [1:: P 1 ] in the text in O (log m) time and verifies them using the constant time algorithm described above. We manage to reduce the time of Vishkin's [87] algorithm to an O (log log m) time algorithm using the following <p> Having many such duels in pairs, Vishkin's <ref> [87] </ref> algorithm eliminates enough possible occurrences of P [1:: P 1 ] in the text in O (log m) time and verifies them using the constant time algorithm described above. We manage to reduce the time of Vishkin's [87] algorithm to an O (log log m) time algorithm using the following observations: * Duels "work like" maximum. Having a block of the text of length equal to P 1 , only one occurrence of the pattern might start in it.
Reference: [88] <author> U. Vishkin. </author> <title> Deterministic sampling anew technique for fast pattern matching. </title> <journal> SIAM J. Comput., </journal> <volume> 20(1) </volume> <pages> 22-40, </pages> <year> 1990. </year> <month> 78 </month>
Reference-contexts: Other parallel algorithms based on Karp, Miller and Rosenberg's [59] method [40, 62] also work in O (log n) time over a fixed alphabet. This method is useful for many other string problems but it seems to require O (log n) time and a larger memory space. Vishkin <ref> [88] </ref> developed an optimal O (log fl m) 2 time algorithm. Unlike the case of the other algorithms this time bound does not account for the preprocessing of the pattern. The preprocessing in Vishkin's algorithm takes optimal O ( log 2 m log log m ) time. <p> Therefore, a lower bound for the number of rounds in the parallel comparison model immediately translates into a lower bound for the time of the CRCW-PRAM. If the pattern is given in advance and any preprocessing is free, then this lower bound does not hold, as Vishkin's <ref> [88] </ref> O (log fl m) algorithm shows. The lower bound also does not hold for CRCW-PRAM over a fixed alphabet strings, but no better algorithm is known at the moment.
Reference: [89] <author> P. Weiner. </author> <title> Linear pattern matching algorithms. </title> <booktitle> In Proc. 14th Symposium on Switching and Automata Theory, </booktitle> <pages> pages 1-11, </pages> <year> 1973. </year>
Reference-contexts: As in the case of a fixed alphabet, input symbols can be used for direct indexing of an array of a constant size. This is especially useful for fast implementations of algorithms that use an automaton description or trie data structures <ref> [2, 5, 35, 60, 75, 89] </ref>. In addition to the advantages of a fixed alphabet, this assumption allows several algorithms in the literature to pack few input symbols into a single register and compare them in a single operation [7, 46, 73].
Reference: [90] <author> C. K. Yap. </author> <title> New upper bounds for selection. </title> <journal> Comm. of the ACM, </journal> <volume> 19 </volume> <pages> 501-508, </pages> <year> 1979. </year>
Reference-contexts: The exact complexity of finding the third largest element is known for all but a finite number of cases <ref> [63, 64, 90] </ref>. The complexity of sorting is determined up to sorting 12 elements [3, 44]. The exact complexity of finding the median is another intriguing open problem: it has been known for several years that the number of comparisons required is between 2n [15] and 3n [81].
Reference: [91] <author> U. Zwick and M. S. Paterson. </author> <title> Lower bounds for string matching in the sequential comparison model. </title> <type> manuscript, </type> <year> 1991. </year>
Reference-contexts: Colussi's bound was later improved to 4 3 n by Galil and Giancarlo [48] who proved also lower bounds that match their upper bound for short patterns [49]. These lower bounds were generalized and improved by Zwick and Paterson <ref> [91] </ref> who presented also other results that we will mention later. In this chapter we concentrate on the complexity of the string matching problem in a deterministic comparison model that can perform comparisons to test if two symbols are equal or not. <p> Note that in a sliding window algorithm the length of the window must be larger than or equal to the length of the pattern; an off-line algorithm can be thought of as an algorithm with a sliding window of infinite length. We follow the notation of Zwick and Paterson <ref> [91] </ref> and say that an algorithm with a sliding window of length m + k has k look-ahead. For the sake of consistency we will call a zero-look-ahead algorithm on-line and an infinite-look-ahead algorithm off-line. <p> Other lower bounds for off-line algorithms and for on-line algorithms that search only for the first occurrence of the pattern in the text are also presented in the same paper. Zwick and Paterson <ref> [91] </ref> generalized the lower bound of Galil and Giancarlo [49] for on-line algorithms to c online (n; m) n + b n m c where 2 is the second period of the pattern if it exists, and 2 = 1 if the second period does not exist.
References-found: 91

