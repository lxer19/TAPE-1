URL: http://www.stat.colostate.edu/~tweedie/documents/diffconv.ps
Refering-URL: http://www.stats.bris.ac.uk/MCMC/pages/list.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Geometric and Subgeometric Convergence of Diffusions with Given Stationary Distributions, and Their Discretizations  
Author: O. Stramer and R.L. Tweedie 
Keyword: Markov chain Monte Carlo, diffusions, Langevin models, posterior distributions, irreducible Markov processes, exponential ergodicity, uniform ergodicity, Euler schemes  
Note: AMS Subject Classifications: 60J05, 60J10, 60K25  
Date: November 11, 1997  
Abstract: We describe algorithms for estimating a given measure known up to a constant of proportionality, based on a large class of diffusions (extending the Langevin model) for which is invariant. We show that under weak conditions one can choose from this class in such a way that the diffusions converge at exponential rate to , and one can even ensure that convergence is independent of the starting point of the algorithm. When convergence is less than exponential we show that it is often polynomial at known rates. We then consider methods of discretizing the diffusion in time, and find methods which inherit the convergence rates of the continuous time process. These contrast with the behaviour of the naive or Euler discretization, which can behave badly even in simple cases. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.E. Besag and P.J. Green. </author> <title> Spatial statistics and Bayesian computation (with discussion). </title> <journal> J. Roy. Statist. Soc. Ser. B, </journal> <volume> 55 </volume> <pages> 25-38, </pages> <year> 1993. </year>
Reference-contexts: This is especially relevant for Markov chain Monte Carlo methods, where for example one seeks to simulate a Bayesian posterior distribution, but many other contexts also use this approach: see <ref> [1, 2, 10, 16, 21, 24, 17] </ref>. One such class of algorithms, given by so-called Langevin diffusions, was introduced in [15] and has been recently studied in Roberts and Tweedie [16].
Reference: [2] <author> J.E. Besag, P.J. Green, D. Higdon, and K.L. Mengersen. </author> <title> Bayesian computation and stochastic systems (with discussion). </title> <journal> Statistical Science, </journal> <volume> 10 </volume> <pages> 3-66, </pages> <year> 1995. </year>
Reference-contexts: This is especially relevant for Markov chain Monte Carlo methods, where for example one seeks to simulate a Bayesian posterior distribution, but many other contexts also use this approach: see <ref> [1, 2, 10, 16, 21, 24, 17] </ref>. One such class of algorithms, given by so-called Langevin diffusions, was introduced in [15] and has been recently studied in Roberts and Tweedie [16].
Reference: [3] <author> P. Billingsley. </author> <title> Probability and Measure. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1986. </year>
Reference-contexts: If V is bounded or fl b 1 then by Problem 5.3.15 of [7] sup h E [jG h (t)j j+" ] &lt; 1, for any positive integer j and any " &gt; 0 and the proof follows directly from Theorem 25.12 of <ref> [3] </ref>. If fl b &gt; 1 then for every ffi &gt; 0 and h &gt; 0 there exists M &gt; 0 such that for all jxj &gt; M , we have x;h &lt; ffi. <p> Again the proof follows directly from Theorem 25.12 of <ref> [3] </ref>.
Reference: [4] <author> D. Down, S. P. Meyn, and R. L. Tweedie. </author> <title> Exponential and uniform ergodicity of Markov processes. </title> <journal> Ann. Probab., </journal> <volume> 23 </volume> <pages> 1671-1691, </pages> <year> 1995. </year>
Reference-contexts: Using this idea we show that convergence in (7) can be made uniformly ergodic: that is, for all x and some &lt; 1; M &lt; 1 kP t 3 Exponential rates of convergence for diffusions As shown in <ref> [11, 4, 16, 26] </ref>, an appropriate norm by which to measure rates of convergence is the V -norm kk V , defined for a measurable function V 1 on IR and a signed measure as sup jgjV j (g)j.
Reference: [5] <author> U. Grenander and M.I. Miller. </author> <title> Representations of knowledge in complex systems (with discussion). </title> <journal> J. Roy. Statist. Soc. Ser. B, </journal> <volume> 56 </volume> <pages> 549-603, </pages> <year> 1994. </year>
Reference-contexts: 1 Introduction The use of diffusions has been suggested <ref> [15, 25, 18, 5, 16] </ref> as a continuous-time method of approach to the problem of simulating a probability density (x) which is only known up to a constant factor.
Reference: [6] <author> C.R. Hwang, S.Y. Hwang-Ma, and S.J. Sheu. </author> <title> Accelerating Gaussian diffusions. </title> <journal> Ann. App. Prob., </journal> <volume> 3 </volume> <pages> 897-913, </pages> <year> 1993. </year>
Reference-contexts: However, more general solutions to (3) can be hard to find, and it is not yet known how to obtain a general class of non-reversible diffusions with a given target probability density , although one class of non-reversible Langevin diffusions for the multi-dimensional Gaussian density is given in <ref> [6] </ref>, and is shown to behave better in some senses than the reversible Langevin diffusions. Apart from some special cases such as this, the theory of diffusions in k &gt; 1 dimension is far less complete than that for one-dimensional diffusions. <p> We choose 2 (x) 1 (i.e. the Langevin diffusion choice) and b (x) = 1 2 0 (x) . We consider two hybrid models, both defined by the scheme given by (26) on jxj &gt; 6: one with the local linearization scheme on C = <ref> [6; 6] </ref> and the other with the Euler scheme on C = [6; 6]. This model is uniformly ergodic, whereas the naive Euler scheme here is not geometrically ergodic because of the cubic term (in fact [16] the Euler approximation is transient). <p> We consider two hybrid models, both defined by the scheme given by (26) on jxj &gt; 6: one with the local linearization scheme on C = <ref> [6; 6] </ref> and the other with the Euler scheme on C = [6; 6]. This model is uniformly ergodic, whereas the naive Euler scheme here is not geometrically ergodic because of the cubic term (in fact [16] the Euler approximation is transient). We will now assess the behavior of a single long series.
Reference: [7] <author> Ioannis Karatzas and Steven E. Shreve. </author> <title> Brownian Motion and Stochastic Calculus. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: x; (ii) some skeleton chain of X (t) is Leb -irreducible; (iii) is invariant for X (t); and (iv) P t X (x; A) converges to in the total variation norm: that is, for all x kP t A2B X (x; A) (A)j ! 0: (7) Proof We have from <ref> [7] </ref> that the process p (X (t)) with p 0 defined as in (6) is B (A t ), where fB (t); F (t)g is a standard Brownian motion and A t is a stopping time for F (t). Hence some skeleton chain is Leb -irreducible. <p> Dynkin's formula (see [13], Section 1.3) shows that for any h &gt; 0 E x [jX h j k ] = jxj k + E x [ 0 L V (X (t)) dt] R h (17) Using Problem 5.3.15 of <ref> [7] </ref> and (14) we conclude that for any h &gt; 0 there exists c fl ; &gt; 0 such that E x [jX h j k ] jxj k c fl jxj kr ; jxj &gt; : (18) The same argument as in the proof of Proposition 5.2 of [26] now <p> Proof We first show that E (V (G h (t))jG h (0) = x) converges to E (V (X (t))jX (0) = x) as h ! 0 for any t &gt; 0. If V is bounded or fl b 1 then by Problem 5.3.15 of <ref> [7] </ref> sup h E [jG h (t)j j+" ] &lt; 1, for any positive integer j and any " &gt; 0 and the proof follows directly from Theorem 25.12 of [3].
Reference: [8] <author> J. Kent. </author> <title> Time-reversible diffusions. </title> <journal> Adv. Appl. Probab., </journal> <volume> 10 </volume> <pages> 819-835, </pages> <year> 1978. </year>
Reference-contexts: As in <ref> [8] </ref> we can define a broad class of diffusions with a given stationary density as follows.
Reference: [9] <author> P. E. Kloeden and E. Platen. </author> <title> Numerical Solution of Stochastic Differential Equations. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1992. </year>
Reference-contexts: leads to the discrete time chain U h (n) = U h (n 1) + 1 p where h &gt; 0 and the random variables Z n are distributed as independent standard normal distributions; that is, P U (x; ) ~ N (x + 2 p It is known (see <ref> [9] </ref>) that this Euler discretization gives good numerical results when the drift and the diffusion coefficients are nearly constant. However it is known to be rather inappropriate in many circumstances, as is born out in the work in [16]. <p> Higher order schemes based on stochastic Taylor expansions are available (see <ref> [9] </ref>), but have the same problem of explosion when the drift b is a polynomial of degree &gt; 1. Moreover, they are complicated since they involve higher powers of white noise and thus harder to employ as candidate distributions in the Hastings-Metropolis algorithm. <p> Solution of such linear stochastic differential equations is standard (see for instance Example 4.1 in <ref> [9] </ref>) and hence it is easy to check that D h (t + h) given D h (t) = x, x 2 IR, has Normal distribution Q h (x; ) with mean x;h and variance 2 x;h defined as, x;h = x + b 0 (x) (exp (b 0 (x)h) 1)
Reference: [10] <author> K.L. Mengersen and R.L. Tweedie. </author> <title> Rates of convergence of the Hastings and Metropolis algorithms. </title> <journal> Annals of Statistics, </journal> <volume> 24 </volume> <pages> 101-121, </pages> <year> 1996. </year>
Reference-contexts: This is especially relevant for Markov chain Monte Carlo methods, where for example one seeks to simulate a Bayesian posterior distribution, but many other contexts also use this approach: see <ref> [1, 2, 10, 16, 21, 24, 17] </ref>. One such class of algorithms, given by so-called Langevin diffusions, was introduced in [15] and has been recently studied in Roberts and Tweedie [16].
Reference: [11] <author> S. P. Meyn and R. L. Tweedie. </author> <title> Markov Chains and Stochastic Stability. </title> <publisher> Springer-Verlag, </publisher> <address> London, </address> <year> 1993. </year>
Reference-contexts: Using this idea we show that convergence in (7) can be made uniformly ergodic: that is, for all x and some &lt; 1; M &lt; 1 kP t 3 Exponential rates of convergence for diffusions As shown in <ref> [11, 4, 16, 26] </ref>, an appropriate norm by which to measure rates of convergence is the V -norm kk V , defined for a measurable function V 1 on IR and a signed measure as sup jgjV j (g)j. <p> We call X (t) exponentially ergodic if it is V -uniformly ergodic for some such V , and uniformly ergodic if X (t) is V -uniformly ergodic for V 1, which 5 ensures that the convergence is independent of the starting point. It is known <ref> [11] </ref> that if convergence is indeed independent of the starting point then it must be exponential in the sense of (8) with V 1, and that if there is exponential convergence in any sense, then there exists a V such that we have V -exponential ergodicity; so we lose no generality <p> We will apply drift criteria to verify the exponential convergence of X (t), as developed in <ref> [11] </ref>. Our results will be based on the It^o formula, in the following form. Let V : IR ! IR be a non-negative twice continuously differentiable function. <p> It is easily seen that L V is the extended generator as defined in [13]. We then have from Theorem 2.1 and Theorem 6.1 of <ref> [11] </ref> that X (t) is V -uniformly ergodic for any V 1 that is twice continuously differentiable such that L V cV + b1l C (10) for some constants b; c &gt; 0, and some compact non-empty set C. If V is bounded then we conclude from Theorem 16.0.2 of [11] <p> <ref> [11] </ref> that X (t) is V -uniformly ergodic for any V 1 that is twice continuously differentiable such that L V cV + b1l C (10) for some constants b; c &gt; 0, and some compact non-empty set C. If V is bounded then we conclude from Theorem 16.0.2 of [11] and It^o's formula that X (t) is uniformly ergodic. <p> Proof It is easy to check that G h , D h are Leb -irreducible and from Proposition 6.1.2 of <ref> [11] </ref> it is weak Feller. Hence, from Theorem 15.0.1 of [11] it suffices for geometric ergodicity to find a test function V 1 such that for some compact set C and some &lt; 1, b &lt; 1 Q h (x; dy)V (y) V (x) + b1l C (x); (28) (28) can <p> Proof It is easy to check that G h , D h are Leb -irreducible and from Proposition 6.1.2 of <ref> [11] </ref> it is weak Feller. Hence, from Theorem 15.0.1 of [11] it suffices for geometric ergodicity to find a test function V 1 such that for some compact set C and some &lt; 1, b &lt; 1 Q h (x; dy)V (y) V (x) + b1l C (x); (28) (28) can easily be verified for the test function V (x) = <p> Theorem 6.2 Assume that the conditions of Theorem 3.1 hold , and that is in class E fl with fi 2. If fl s + fi 2 &gt; 0, then the hybrid discrete approximation G h is uniformly ergodic. Proof It follows from <ref> [11, Chapter 16] </ref> that G h is uniformly ergodic if for some compact set C, sup x2C E x [t C ] &lt; 1; where t C = infft 0 : G h (t) 2 Cg is the first return time to C.
Reference: [12] <author> S. P. Meyn and R. L. Tweedie. </author> <title> Stability of Markovian processes II: Continuous time processes and sampled chains. </title> <journal> Adv. Appl. Probab., </journal> <volume> 25 </volume> <pages> 487-517, </pages> <year> 1993. </year>
Reference-contexts: that (i) is an arbitrary probability density on IR which is positive and twice differentiable almost everywhere; (ii) 2 is bounded on compact subintervals of IR; (iii) p (1) = 1, p (+1) = +1 where p 0 (x) = (x) 2 (x) We now have, using the notation of <ref> [12] </ref> Theorem 2.1 Suppose X (t) satisfies (1) with b; satisfying (2). <p> Hence some skeleton chain is Leb -irreducible. The Feller property follows from Exercise 7.3.3 and Lemma 11.1.1 of [23], and so (7) follows directly from Theorem 6.1 of <ref> [12] </ref>. The convergence for all x follows from continuity of sample paths. ut There is no unique way to choose a continuous time process that converges to .
Reference: [13] <author> S. P. Meyn and R. L. Tweedie. </author> <title> Stability of Markovian processes III: Foster-Lyapunov criteria for continuous time processes. </title> <journal> Adv. Appl. Probab., </journal> <volume> 25 </volume> <pages> 518-548, </pages> <year> 1993. </year>
Reference-contexts: It is easily seen that L V is the extended generator as defined in <ref> [13] </ref>. We then have from Theorem 2.1 and Theorem 6.1 of [11] that X (t) is V -uniformly ergodic for any V 1 that is twice continuously differentiable such that L V cV + b1l C (10) for some constants b; c &gt; 0, and some compact non-empty set C. <p> Dynkin's formula (see <ref> [13] </ref>, Section 1.3) shows that for any h &gt; 0 E x [jX h j k ] = jxj k + E x [ 0 L V (X (t)) dt] R h (17) Using Problem 5.3.15 of [7] and (14) we conclude that for any h &gt; 0 there exists c <p> We next show that subgeometric convergence of the h-skeletons guarantees the same behaviour for the process X (t). Similarly to the proof of Theorem 6.1 of <ref> [13] </ref>, we write for arbitrary t 0, t = nh + s with 0 s &lt; h t Lk jjP t X (x; ) jj f = (nh + s) Lk sup jgjf jP nh+s X g gdj jP nh (x; dw) (dw)j P s Using (17), for any s 2
Reference: [14] <author> T. Ozaki. </author> <title> A bridge between nonlinear time series models and nonlinear stochastic dynamical systems: A local linearization approach. </title> <journal> Statistica Sinica, </journal> <volume> 2 </volume> <pages> 113-135, </pages> <year> 1992. </year> <month> 27 </month>
Reference-contexts: (t), we choose a (usually small) time-step h &gt; 0 and work with a diffusion which essentially linearizes the drift and 11 keeps the scale constant over small time intervals [kh; (k + 1)h], k = 1; 2; : : : in a manner similar to that proposed by Ozaki <ref> [14] </ref>. <p> However it is known to be rather inappropriate in many circumstances, as is born out in the work in [16]. In particular, Ozaki <ref> [14] </ref> notes that with the Euler method, if the true drift b is a polynomial of degree &gt; 1, then the Euler approximation explodes to infinity if it starts from some large initial value, even though the diffusion itself tends to move quickly to some compact interval. <p> Moreover, they are complicated since they involve higher powers of white noise and thus harder to employ as candidate distributions in the Hastings-Metropolis algorithm. Based on <ref> [14, 19, 20] </ref> we now define a local linear approximation which leads to discretizations which are better behaved. We assume that b (x) is twice continuously 12 differentiable with respect to x.
Reference: [15] <author> J.D. Doll, P.J. Rossky and H.L. Friedman. </author> <title> Brownian dynamics as smart Monte Carlo simulation. </title> <journal> Journal of Chemical Physics, </journal> <volume> 69 </volume> <pages> 4628-4633, </pages> <year> 1978. </year>
Reference-contexts: 1 Introduction The use of diffusions has been suggested <ref> [15, 25, 18, 5, 16] </ref> as a continuous-time method of approach to the problem of simulating a probability density (x) which is only known up to a constant factor. <p> One such class of algorithms, given by so-called Langevin diffusions, was introduced in <ref> [15] </ref> and has been recently studied in Roberts and Tweedie [16].
Reference: [16] <author> G.O. Roberts and R.L. Tweedie. </author> <title> Exponential convergence of Langevin diffusions and their discrete approximations. </title> <journal> Bernouilli, </journal> <volume> 2 </volume> <pages> 341-364, </pages> <year> 1996. </year>
Reference-contexts: 1 Introduction The use of diffusions has been suggested <ref> [15, 25, 18, 5, 16] </ref> as a continuous-time method of approach to the problem of simulating a probability density (x) which is only known up to a constant factor. <p> This is especially relevant for Markov chain Monte Carlo methods, where for example one seeks to simulate a Bayesian posterior distribution, but many other contexts also use this approach: see <ref> [1, 2, 10, 16, 21, 24, 17] </ref>. One such class of algorithms, given by so-called Langevin diffusions, was introduced in [15] and has been recently studied in Roberts and Tweedie [16]. <p> One such class of algorithms, given by so-called Langevin diffusions, was introduced in [15] and has been recently studied in Roberts and Tweedie <ref> [16] </ref>. <p> For a discussion of some of the stability properties enjoyed by geometrically ergodic chains in simulation, we refer the reader to <ref> [16] </ref>: we note for example that such chains have Central Limit Theorems and the like available, which makes it much easier to work with the outcomes of the algorithms. <p> If c (x) 0 then X (t) is reversible; otherwise X (t) is not reversible. One example of such a process is the reversible Langevin diffusion for the density , studied in <ref> [16] </ref>. This is the diffusion process L (t), defined as a solution to (1) with (x) I (I is the identity matrix); c (x) 0; and, from (2), b (x) = 1 2 r log (x). <p> Using this idea we show that convergence in (7) can be made uniformly ergodic: that is, for all x and some &lt; 1; M &lt; 1 kP t 3 Exponential rates of convergence for diffusions As shown in <ref> [11, 4, 16, 26] </ref>, an appropriate norm by which to measure rates of convergence is the V -norm kk V , defined for a measurable function V 1 on IR and a signed measure as sup jgjV j (g)j. <p> We now show that different choices of b and will give us different rates of convergence: that is, convergence is not just a function of the shape of the tails of the target distribution, as might be conjectured from the results in <ref> [16] </ref>. In particular we show that for virtually any , one can always construct some diffusion that converges uniformly fast to as its stationary distribution. <p> The class P fl : has asymptotically polynomial tails such that (x)jxj fi ! ff; x ! 1; (12) for some ff &gt; 0, fi &gt; 2. Note that the exponential-tail class E fl is larger than the class E in <ref> [16] </ref> which included only those measures with log (x) / jxj fi in the tails, rather than allowing asymptotic proportionality. We now have Theorem 3.1 (i) Let be in class E fl . <p> Now the It^o formula shows that for large values of f (X (t)), where for both cases, b (f (X (t))) kjf (x)j r for some k &gt; 0 and r &lt; 0. It follows from <ref> [16] </ref> that f (X (t)) is not exponentially ergodic if r &lt; 0 and hence since 1 fl s 2 &gt; 0 we conclude that X (t) is also not exponentially ergodic. ut Remark: It follows from Theorem 3.1 that if we choose 2 (x) jxj fl s , then for <p> fl s , then for all in class E fl , and all in class P fl with fi &gt; fl s , we always get geometric rates of convergence if fl s 2 and uniform rates of convergence if 2 &lt; fl s , while (as noted also in <ref> [16] </ref>) the Langevin diffusion is exponentially ergodic if and only if is in class E fl and fi 1. 4 Subgeometric rates of convergence Although one can find exponentially or even uniformly converging diffusions for virtually all distributions as in Theorem 3.1, these may require some computation and it may be <p> Thus although the convergence is not geometric, the total variation (for example) does converge faster than any particular polynomial rate. ut 5 Approaches to Discretization One of the most striking results in <ref> [16] </ref> is that naive discretizations of the Langevin diffusion do not converge in the same manner as the diffusion itself: in particular it is possible that such discretizations may even be transient when the diffusion is exponentially ergodic. <p> taken dependent on the value Y (kh), thus leading to a time-inhomogeneous diffusion approximation in continuous time, or a Markov chain if we consider the process at the times 0; h; 2h; : : : The simplest such linear model is the h-step Euler scheme used by Roberts and Tweedie <ref> [16] </ref>. <p> However it is known to be rather inappropriate in many circumstances, as is born out in the work in <ref> [16] </ref>. <p> Example: Changing the Discretization We will say that 2 E E fl , as introduced in <ref> [16] </ref>, if for some x 0 , and some constants fl &gt; 0 and 0 &lt; fi &lt; 1, takes the form (x) / e fljxj fi ; jxj x 0 ; (30) so that for jxj x 0 r log (x) = flfi (sgn (x))jxj fi1 (31) 17 The behaviour <p> We consider first the Langevin diffusion with 1, and compare the behaviour of the Euler scheme of discretization used in <ref> [16] </ref> and the discretization D h of Theorem 6.1 or Theorem 6.2. (i) For 0 &lt; fi &lt; 1, it follows from [16] that the Euler approximation is transient, and it is not hard to see that D h , G h are also transient in this case. (ii) For 1 <p> We consider first the Langevin diffusion with 1, and compare the behaviour of the Euler scheme of discretization used in <ref> [16] </ref> and the discretization D h of Theorem 6.1 or Theorem 6.2. (i) For 0 &lt; fi &lt; 1, it follows from [16] that the Euler approximation is transient, and it is not hard to see that D h , G h are also transient in this case. (ii) For 1 fi 2, D h , G h are geometrically ergodic for all h &gt; 0, while for the Euler approximation this is <p> This model is uniformly ergodic, whereas the naive Euler scheme here is not geometrically ergodic because of the cubic term (in fact <ref> [16] </ref> the Euler approximation is transient). We will now assess the behavior of a single long series. <p> The value of h needed to obtain a good approximation to the stationary distribution 26 might be very small (see Figure 6), and hence one needs to be careful in using this scheme. In general it makes clear sense to "Metropolise" the discrete chain as in <ref> [16] </ref>, and we address this in [22]. Acknowledgement We are grateful to Gareth Roberts for valuable discussions, and for showing us unpublished work with Jeff Rosenthal concerning possible use of the diffusion satisfying (4).
Reference: [17] <author> G.O. Roberts and R.L. Tweedie. </author> <title> Geometric convergence and central limit theorems for multidimensional Hastings and Metropolis algorithms. </title> <journal> Biometrika, </journal> <volume> 83, </volume> <year> 1996. </year>
Reference-contexts: This is especially relevant for Markov chain Monte Carlo methods, where for example one seeks to simulate a Bayesian posterior distribution, but many other contexts also use this approach: see <ref> [1, 2, 10, 16, 21, 24, 17] </ref>. One such class of algorithms, given by so-called Langevin diffusions, was introduced in [15] and has been recently studied in Roberts and Tweedie [16].
Reference: [18] <author> B.J. Pendleton S. Duane, A.D. Kennedy and D. Roweth. </author> <title> Hybrid Monte Carlo. </title> <journal> Physics Letters B, </journal> <volume> 195 </volume> <pages> 216-222, </pages> <year> 1987. </year>
Reference-contexts: 1 Introduction The use of diffusions has been suggested <ref> [15, 25, 18, 5, 16] </ref> as a continuous-time method of approach to the problem of simulating a probability density (x) which is only known up to a constant factor.
Reference: [19] <author> I. Shoji. </author> <title> Estimation for a continuous time stochastic process: a new local linearization approach. </title> <type> Technical report. </type> <institution> The Institute of Statistical Mathematics, </institution> <address> Tokyo, </address> <year> 1995. </year>
Reference-contexts: Moreover, they are complicated since they involve higher powers of white noise and thus harder to employ as candidate distributions in the Hastings-Metropolis algorithm. Based on <ref> [14, 19, 20] </ref> we now define a local linear approximation which leads to discretizations which are better behaved. We assume that b (x) is twice continuously 12 differentiable with respect to x.
Reference: [20] <author> I. Shoji. </author> <title> Approximation of continuous time stochastic processes by a local linearisation method. </title> <type> Technical report, </type> <institution> The Institute of Statistical Mathematics, </institution> <address> Tokyo, </address> <year> 1995. </year>
Reference-contexts: Moreover, they are complicated since they involve higher powers of white noise and thus harder to employ as candidate distributions in the Hastings-Metropolis algorithm. Based on <ref> [14, 19, 20] </ref> we now define a local linear approximation which leads to discretizations which are better behaved. We assume that b (x) is twice continuously 12 differentiable with respect to x. <p> Moreover, it is shown in <ref> [20] </ref> that for fixed T the rate of convergence of the discrete approximation D h , to the diffusion X (T ), defined as fE [jD h (T ) X (T )jjX (0) = x]g 1=2 , is O (h) while for the Euler scheme it is O (h 1=2 ).
Reference: [21] <author> A.F.M. Smith and G.O. Roberts. </author> <title> Bayesian computation via the Gibbs sampler and related Markov chain Monte Carlo methods (with discussion). </title> <journal> J. Roy. Statist. Soc. Ser. B, </journal> <volume> 55 </volume> <pages> 3-24, </pages> <year> 1993. </year>
Reference-contexts: This is especially relevant for Markov chain Monte Carlo methods, where for example one seeks to simulate a Bayesian posterior distribution, but many other contexts also use this approach: see <ref> [1, 2, 10, 16, 21, 24, 17] </ref>. One such class of algorithms, given by so-called Langevin diffusions, was introduced in [15] and has been recently studied in Roberts and Tweedie [16].
Reference: [22] <author> O. Stramer and R.L. Tweedie. </author> <title> Self-targeting candidates for Hastings-Metropolis algorithms. </title> <note> (submitted for publication). </note>
Reference-contexts: In a related paper <ref> [22] </ref>, we also address the question of emplying the discrete approximations to these diffusions as candidate distributions in the Hastings-Metropolis algorithm, and show that in this context also, convergence can be made both rapid and (in surprising generality) uniform in the starting point of the algorithm. 2 Diffusions with given stationary <p> Unless h is small, h need not be close to , of course, although for sufficiently fine discretizations we would hope it would be similar. In <ref> [22] </ref> we develop ways of using Metropolis algorithms to adjust for this: here we consider the chains G h and D h without such adjustment. Theorem 6.1 Assume that the conditions of Theorem 3.1 hold. <p> In general it makes clear sense to "Metropolise" the discrete chain as in [16], and we address this in <ref> [22] </ref>. Acknowledgement We are grateful to Gareth Roberts for valuable discussions, and for showing us unpublished work with Jeff Rosenthal concerning possible use of the diffusion satisfying (4).
Reference: [23] <author> D.W. Stroock and S.R.S. Varadhan. </author> <title> Multidimensional Diffusion Processes. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1979. </year>
Reference-contexts: Hence some skeleton chain is Leb -irreducible. The Feller property follows from Exercise 7.3.3 and Lemma 11.1.1 of <ref> [23] </ref>, and so (7) follows directly from Theorem 6.1 of [12]. The convergence for all x follows from continuity of sample paths. ut There is no unique way to choose a continuous time process that converges to .
Reference: [24] <author> L. Tierney. </author> <title> Markov chains for exploring posterior distributions (with discussion). </title> <journal> Ann. Statist., </journal> <volume> 22 </volume> <pages> 1701-1762, </pages> <year> 1994. </year>
Reference-contexts: This is especially relevant for Markov chain Monte Carlo methods, where for example one seeks to simulate a Bayesian posterior distribution, but many other contexts also use this approach: see <ref> [1, 2, 10, 16, 21, 24, 17] </ref>. One such class of algorithms, given by so-called Langevin diffusions, was introduced in [15] and has been recently studied in Roberts and Tweedie [16].
Reference: [25] <author> D. Toussaint. </author> <title> Introduction to algorithm for Monte Carlo simulations and their application to qcd. </title> <journal> Computer Physics Communications, </journal> <volume> 56 </volume> <pages> 69-92, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction The use of diffusions has been suggested <ref> [15, 25, 18, 5, 16] </ref> as a continuous-time method of approach to the problem of simulating a probability density (x) which is only known up to a constant factor.
Reference: [26] <author> P. Tuominen and R.L. Tweedie. </author> <title> Subgeometric rates of convergence of f-ergodic Markov chains. </title> <journal> Adv. Appl. Probab., </journal> <volume> 26 </volume> <pages> 775-798, </pages> <year> 1994. </year> <month> 28 </month>
Reference-contexts: Using this idea we show that convergence in (7) can be made uniformly ergodic: that is, for all x and some &lt; 1; M &lt; 1 kP t 3 Exponential rates of convergence for diffusions As shown in <ref> [11, 4, 16, 26] </ref>, an appropriate norm by which to measure rates of convergence is the V -norm kk V , defined for a measurable function V 1 on IR and a signed measure as sup jgjV j (g)j. <p> We have seen that such simple diffusions may not be exponentially ergodic. We therefore consider conditions under which we can show that there is at least a sub-geometric rate of convergence to . Guided by the results in <ref> [26] </ref>, our goal will be to find conditions under which r (t)kP t as t ! 1, where for some k 1 ; k 2 2 Z + , f (x) = jxj k 1 _ 1, x 2 IR and r (t) = t k 2 _ 1, t 0. <p> A more general subgeometric rate of convergence was studied in <ref> [26] </ref>, but for our purposes this combination is more than sufficient, and it seems plausible that such polynomial convergence of polynomial moments is what we might expect in this case. <p> of [7] and (14) we conclude that for any h &gt; 0 there exists c fl ; &gt; 0 such that E x [jX h j k ] jxj k c fl jxj kr ; jxj &gt; : (18) The same argument as in the proof of Proposition 5.2 of <ref> [26] </ref> now gives (16). We next show that subgeometric convergence of the h-skeletons guarantees the same behaviour for the process X (t). <p> It now follows from (16) that t Lk jjP t X (x; ) jj f ! 0 as t ! 1. ut As in <ref> [26] </ref>, such results stated in terms of polynomial moments and rates of conver gence can be extended to fractional indices, and for simplicity we will assume that ff b , ff s , and fl s are fractional indices. The following is then a direct consequence of Theorem 4.1. <p> To discretize in this situation, we can use the Euler scheme, and in this case we show that we get a specific subgeometric rate of convergence. The same argument as in the proof of Proposition 5.2 of <ref> [26] </ref> gives r (n)kP n where P n u is the transition law of U n as defined in (20), and f (x) = jxj kr _ 1 x 2 IR; here L is any positive index fractional index for case A and L = fl s 2fi for case B.
References-found: 26

