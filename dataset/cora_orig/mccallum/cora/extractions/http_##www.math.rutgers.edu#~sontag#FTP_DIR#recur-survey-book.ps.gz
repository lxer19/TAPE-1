URL: http://www.math.rutgers.edu/~sontag/FTP_DIR/recur-survey-book.ps.gz
Refering-URL: http://www.math.rutgers.edu/~sontag/papers.html
Root-URL: 
Email: sontag@control.rutgers.edu  
Title: Recurrent Neural Networks: Some Systems-Theoretic Aspects  
Author: Eduardo Sontag 
Address: New Brunswick, NJ 08903  
Affiliation: Dept. of Mathematics, Rutgers University  
Abstract: This paper provides an exposition of some recent research regarding system-theoretic aspects of continuous-time recurrent (dynamic) neural networks with sigmoidal activation functions. The class of systems is introduced and discussed, and a result is cited regarding their universal approximation properties. Known characterizations of controllability, ob-servability, and parameter identifiability are reviewed, as well as a result on minimality. Facts regarding the computational power of recurrent nets are also mentioned. fl Supported in part by US Air Force Grant AFOSR-94-0293
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Albertini, F., and P. Dai Pra, </author> <title> "Forward accessibility for recurrent neural networks," </title> <journal> IEEE Trans. Automat. Control 40 (1995): </journal> <pages> 1962-1968 </pages>
Reference-contexts: A related fact is that "forward accessibility" (the reachable set from each state has nonempty interior) holds for every net as in Theorem 2, provided that has the "IP property" to be discussed below. This result had been earlier shown in the paper <ref> [1] </ref> (which dealt mainly with accessibility for the much harder discrete-time case). It is an immediate consequence of the fact that, when the IP property holds, the linear span of f~ (n) (Ax + Bu); u 2 R m g equals the tangent space at each point x. <p> We have avoided discussion of system-theoretic issues for discrete-time networks. Approximation, observability, and identifiability results are known for the discrete time case, and most are similar to those for continuous time (see the respective references). The controllability case is still open, though partial characterizations are known (see <ref> [1] </ref>). 11
Reference: [2] <editor> Albertini, F., and E.D. Sontag, </editor> <title> "For neural networks, function determines form," </title> <booktitle> Neural Networks 6(1993): </booktitle> <pages> 975-990. </pages>
Reference-contexts: The paper <ref> [2] </ref> showed how, at least for certain problems, it is possible to transform among the different models, in such a way that once that results are obtained for (1), corollaries for the variants are easily obtained.
Reference: [3] <editor> Albertini, F., and E.D. Sontag, </editor> <title> "Uniqueness of weights for recurrent nets," Systems and Networks: </title> <journal> Math Theory and Applics, Proc. </journal> <volume> MTNS '93, Vol. 2, </volume> <publisher> Akademie Verlag, Regensburg, </publisher> <pages> pp. 599-602. </pages> <note> Extended version: http://www.math.rutgers.edu/~sontag/ftp dir/93mtns-nn-extended.ps.gz </note>
Reference-contexts: Any initialized net ( b ; b ~) obtained in this fashion is said to be sign-permutation equivalent to (; ~). It is easy to see that sign-permutation equivalent nets are also i/o equivalent. We have the following converse from <ref> [3] </ref>: Theorem 4 Assume that and b are two observable nets with = tanh and B; b B 2 B. Then, and b are sign-permutation equivalent if and only if they are i/o equivalent. <p> : :; ng, where i : R n ! R n is the projection on the ith axis, i.e., i e j = ffi ij e i . (We let ffi ij be the Kronecker delta and e i the ith canonical basis vector.) Parameter Identifiability Theorem 4 is from <ref> [3] </ref>. It is proved there for every that is odd and satisfies the IP property. Thus it holds as well for any for which any of the sufficient conditions stated above are verified. Minimality Theorem 5 does not appear to have been mentioned in the literature.
Reference: [4] <editor> Albertini, F., and E.D. Sontag, </editor> <title> "State observability in recurrent neural networks," </title> <journal> Systems & Control Letters 22(1994): </journal> <pages> 235-244. </pages>
Reference-contexts: For linear systems, observability is equivalent to the requirement that the transposed pair (A 0 ; C 0 ) be a reachable matrix pair. For nets, we have as follows, from <ref> [4] </ref>. Consider the directed graph G, with node set f1; : : :; ng, in which there is an edge from i to j whenever a ij 6= 0. Let N be the set consisting of those nodes i for which the ith column of C is nonzero. <p> It is an immediate consequence of the fact that, when the IP property holds, the linear span of f~ (n) (Ax + Bu); u 2 R m g equals the tangent space at each point x. Observability Theorem 3 is from <ref> [4] </ref>. It is proved for every that satisfies the independence property (IP). This property is basically a dual to the spanning property. <p> i ) with a i 6= 0 but now requiring also (a i ; b i ) 6= (a j ; b j ) for all i 6= j.) A simple sufficient condition can be used to show that many maps, including tanh and arctan, satisfy the IP property (cf. <ref> [4] </ref>): it is enough that admit an extension as a complex analytic function : C ! C defined on a subset of the form jImzj g n fz 0 ; z 0 for some &gt; 0, where Imz 0 = and z 0 and z 0 are singularities.
Reference: [5] <author> Hautus, M., </author> <title> "A set of IP-functions," </title> <type> unpublished manuscript, </type> <institution> Eindhoven University, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: Another way of establishing the IP property is by an asymptotic analysis of , in the spirit as in the statement given above for the controllability property; this was the approach taken in [14]. For instance, cf. <ref> [5] </ref>, has the IP property if it is continuously differentiable, (s)= 0 (s) is defined and has constant sign for all s large, and: lim (s) = 0 : As remarked in [5], this establishes the IP property whenever (s) = q (s)e p (s) , and p; q are polynomials <p> For instance, cf. <ref> [5] </ref>, has the IP property if it is continuously differentiable, (s)= 0 (s) is defined and has constant sign for all s large, and: lim (s) = 0 : As remarked in [5], this establishes the IP property whenever (s) = q (s)e p (s) , and p; q are polynomials with deg p 2. Even weaker conditions from [5] are to require that for each b &gt; 0, (s + b)=(s) be defined and bounded for all sufficiently large s, and (s <p> differentiable, (s)= 0 (s) is defined and has constant sign for all s large, and: lim (s) = 0 : As remarked in <ref> [5] </ref>, this establishes the IP property whenever (s) = q (s)e p (s) , and p; q are polynomials with deg p 2. Even weaker conditions from [5] are to require that for each b &gt; 0, (s + b)=(s) be defined and bounded for all sufficiently large s, and (s + b)=(s) ! 0 as s ! +1 ; or that the same property hold for 1=. 8 The condition that every variable affects the output can
Reference: [6] <author> Leshno, M., V.Ya. Lin, A. Pinkus, and S. Schocken, </author> <title> "Multilayer feedfor-ward networks with a non-polynomial activation function can approximate any function," </title> <booktitle> Neural Networks 6(1993): </booktitle> <pages> 861-867. </pages>
Reference-contexts: Not every function has the spanning property. For instance, if is a polynomial of degree k then the above span is the set of all polynomials of degree k, hence it forms a closed subspace and cannot be dense. This turns out to be the only exception: <ref> [6] </ref> shows that any locally Riemann integrable (i.e., any function which is continuous except at most in a set of measure zero, and bounded on each compact) has the spanning property if and only if it is not a polynomial. Controllability Theorem 2 is from [12].
Reference: [7] <author> Siegelmann, H.T., and E.D. Sontag, </author> <title> "Analog computation, neural networks, and circuits," </title> <journal> Theor. Comp. Sci. </journal> <volume> 131(1994): </volume> <pages> 331-360. </pages>
Reference-contexts: The papers <ref> [7, 8] </ref> considered discrete-time networks with the "semilinear" or "saturated linearity" activation (x) = &lt; 1 if x 1 x otherwise. It is assumed, for simplicity (but not changing the results in any substantial way) that there are just one input and one output channel (m = p = 1). <p> In the latter case, we say that the response to the input sequence w was computed in time s + l. If is obtained in this form, the (partial) function is said to be realized by the initialized network (; ~). It is shown in <ref> [7] </ref> that any partial function : f1; 1g + ! f1; 1g + can be realized by some admissible pair, and can be realized by some rational admissible pair if and only if is a partial recursive function. 10 Constraints in computational time are of course more interesting. <p> Using real weights, a new class, "analog P," arises. This class can be characterized as the class of all languages recognizable by arbitrary nonlinear (but Lipschitz-continuous) dynamical systems, see <ref> [7] </ref> for details. The class analog P strictly contains P, and it turns out to coincide with a class already studied in computer science, namely the languages recognized in polynomial time by Turing machines which consult oracles, where the oracles are sparse sets.
Reference: [8] <editor> Siegelmann, H.T., and E.D. Sontag, </editor> <booktitle> "On the computational power of neural nets," </booktitle> <institution> J. Comp. Syst. Sci. </institution> <month> 50( </month> <year> 1995): </year> <pages> 132-150. </pages>
Reference-contexts: The papers <ref> [7, 8] </ref> considered discrete-time networks with the "semilinear" or "saturated linearity" activation (x) = &lt; 1 if x 1 x otherwise. It is assumed, for simplicity (but not changing the results in any substantial way) that there are just one input and one output channel (m = p = 1).
Reference: [9] <author> Sontag, E.D., </author> <title> "Neural nets as systems models and controllers," </title> <booktitle> in Proc. Seventh Yale Workshop on Adaptive and Learning Systems, </booktitle> <pages> pp. 73-79, </pages> <institution> Yale University, </institution> <year> 1992. </year>
Reference-contexts: From <ref> [9] </ref> we have: Theorem 1 For each system b and for each K 1 , K 2 , ", T as above, there is a net , with = tanh, which simulates b on the sets K 1 ; K 2 in time T and up to accuracy ". <p> 3, and 4 hold for activations more general than tanh, as we discuss next. (In every case, in addition to the conditions stated, one assumes that : R ! R is locally Lipschitz, so that solutions of the evolution equations are defined at least locally.) Approximation Theorem 1 is from <ref> [9] </ref>.
Reference: [10] <author> Sontag, E.D., </author> <title> "Neural networks for control," in Essays on Control: Perspectives in the Theory and its Applications (H.L. </title> <editor> Trentelman and J.C. Willems, eds.), </editor> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1993, </year> <pages> pp. 339-380. </pages>
Reference: [11] <author> Sontag, E.D., </author> <title> Mathematical Control Theory: Deterministic Finite Dimensional Systems, </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: One advantage of this form is that the linear systems customarily studied in control theory are precisely those nets for which the activation is the identity function. This suggests that the above model may be amenable to a theoretical development parallel to linear systems theory (for which see e.g. <ref> [11] </ref>). Indeed, there are complete characterizations of basic systems theoretic properties such as controllability, observability, minimality, and parameter identifiability. This paper presents a brief survey of some such results. We also review the fact that recurrent nets can approximate arbitrary nonlinear systems (albeit in a restricted fashion). <p> We consider systems b (cf. <ref> [11] </ref>) dt with input space R m , output space R p and state space R bn (the integer bn is called the dimension of the system) where h : R bn ! R p is continuous, and f : R bn fi R m ! R bn is continuously differentiable
Reference: [12] <author> Sontag, E.D., and H.J. Sussmann, </author> <title> "Complete controllability of continuous-time recurrent neural networks," </title> <journal> Systems and Control Letters 30(1997): </journal> <pages> 177-183. </pages>
Reference-contexts: For nets with activation tanh, we have the following from <ref> [12] </ref>: Theorem 2 Assume that B 2 B and = tanh. Then the net (1) is controllable. <p> Controllability Theorem 2 is from <ref> [12] </ref>.
Reference: [13] <author> Sussmann, H.J., </author> <title> "Existence and uniqueness of minimal realizations of nonlinear systems," </title> <journal> Math. Sys. Theory 10(1977): </journal> <pages> 263-284. </pages>
Reference-contexts: The restriction of the dynamics of b to the orbit X 0 passing through the initial state b ~ provides an initialized system ( b 0 ; b ~ 0 ) which is orbit-minimal in the sense of <ref> [13] </ref> and is again i/o equivalent to (; ~). One may then apply Theorem 1 in [13] to conclude that there is also an initialized analytic system ( b 0 ; b ~ 0 ) with state space X 0 , i/o equivalent to (; ~) and minimal in the sense <p> dynamics of b to the orbit X 0 passing through the initial state b ~ provides an initialized system ( b 0 ; b ~ 0 ) which is orbit-minimal in the sense of <ref> [13] </ref> and is again i/o equivalent to (; ~). One may then apply Theorem 1 in [13] to conclude that there is also an initialized analytic system ( b 0 ; b ~ 0 ) with state space X 0 , i/o equivalent to (; ~) and minimal in the sense of [13], and an analytic onto mapping 0 : X 0 ! X 0 such that <p> One may then apply Theorem 1 in <ref> [13] </ref> to conclude that there is also an initialized analytic system ( b 0 ; b ~ 0 ) with state space X 0 , i/o equivalent to (; ~) and minimal in the sense of [13], and an analytic onto mapping 0 : X 0 ! X 0 such that h (q) = h 0 ( 0 (q)) for all q 2 X 0 and, for each input u : [0; T ] ! R m and each t 2 [0; T ], bx (t; b <p> for each input u : [0; T ] ! R m and each t 2 [0; T ], bx (t; b ~; u) 2 X 0 and (with the obvious notations) 0 (bx (t; b ~; u)) = bx 0 (t; ~; u) : (The statement of Theorem 1 in <ref> [13] </ref> is somewhat weaker than this, but the proof actually shows the claimed facts.) Next, Theorem 5 in [13], applied to the two minimal systems ( b 0 ; b ~ 0 ) and (; ~) provides an isomorphism 1 , which composed with 0 provides the mapping desired for Theorem <p> bx (t; b ~; u) 2 X 0 and (with the obvious notations) 0 (bx (t; b ~; u)) = bx 0 (t; ~; u) : (The statement of Theorem 1 in <ref> [13] </ref> is somewhat weaker than this, but the proof actually shows the claimed facts.) Next, Theorem 5 in [13], applied to the two minimal systems ( b 0 ; b ~ 0 ) and (; ~) provides an isomorphism 1 , which composed with 0 provides the mapping desired for Theorem 5 in this paper.
Reference: [14] <author> Sussmann, H.J., </author> <title> "Uniqueness of the weights for minimal feedforward nets with a given input-output map," </title> <booktitle> Neural Networks 5(1992): </booktitle> <pages> 589-593. </pages>
Reference-contexts: Another way of establishing the IP property is by an asymptotic analysis of , in the spirit as in the statement given above for the controllability property; this was the approach taken in <ref> [14] </ref>.
Reference: [15] <author> Zbikowski, R., </author> <title> "Lie algebra of recurrent neural networks and identifiabil-ity," </title> <booktitle> Proc. Amer. Auto. Control Conf., </booktitle> <address> San Francisco, </address> <year> 1993, </year> <note> pp.2900-2901. 12 </note>
References-found: 15

