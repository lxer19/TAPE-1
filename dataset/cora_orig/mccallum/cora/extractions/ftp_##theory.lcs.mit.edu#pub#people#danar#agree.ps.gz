URL: ftp://theory.lcs.mit.edu/pub/people/danar/agree.ps.gz
Refering-URL: http://theory.lcs.mit.edu/~danar/papers.html
Root-URL: 
Title: Agreement In the Presence of Faults, On Networks of Bounded Degree  
Author: Michael Ben-Or Dana Ron 
Address: Jerusalem, Israel  Cambridge, MA, USA  
Affiliation: Institute of Computer Science Hebrew University  Laboratory of Computer Science MIT  
Abstract: We present networks of bounded degree and a fully polynomial almost everywhere agreement scheme which tolerate, with high probability, randomly located faulty processors, where processors fail independently with some constant probability.
Abstract-found: 1
Intro-found: 1
Reference: [BG1] <author> P. Berman and J.A Garay, </author> <title> "Asymptotically Optimal Distributed Consensus", </title> <booktitle> Proc. 16th ICALP, </booktitle> <year> 1989, </year> <pages> pp. 80-94. </pages>
Reference-contexts: They exhibited a communication network of bounded degree connecting N processors along with a polynomial protocol enabling all but O (t) of the correct processors to reach agreement in the presence of up to t = O (N= log N ) faulty processors. Further work by Berman and Garay <ref> [BG1, BG2] </ref> improved the efficiency of these protocols. Handling more faults is a considerably more difficult problem. Note that in networks of bounded degree the distance between most pairs of nodes is (log N ).
Reference: [BG2] <author> P. Berman and J.A Garay, </author> <title> "Fast Consensus in Networks of Bounded Degree", </title> <booktitle> 4th International workshop on Distributed Algorithms, </booktitle> <year> 1990, </year> <pages> pp. 321-333. </pages>
Reference-contexts: They exhibited a communication network of bounded degree connecting N processors along with a polynomial protocol enabling all but O (t) of the correct processors to reach agreement in the presence of up to t = O (N= log N ) faulty processors. Further work by Berman and Garay <ref> [BG1, BG2] </ref> improved the efficiency of these protocols. Handling more faults is a considerably more difficult problem. Note that in networks of bounded degree the distance between most pairs of nodes is (log N ).
Reference: [D] <author> D. Dolev, </author> <title> "The Byzantine Generals Strike Again", </title> <journal> J. of Algorithms, </journal> <volume> Vol. 3, </volume> <month> No.1 </month> <year> (1982), </year> <pages> pp. 14-30. </pages>
Reference-contexts: If all the correct processors hold the same initial value, then this must be the agreement value. Since Byzantine agreement is achievable only if the number of faulty processors is less then one-half of the connectivity of the network <ref> [D] </ref>, Dwork et. al. [DPPU] suggest a relaxation of the standard definition of Byzantine agreement. Instead of requiring that all correct processors must reach a common decision, they require that all but a small fraction of the correct processors reach a common decision.
Reference: [DPPU] <author> C. Dwork, D. Peleg, N. Pippenger and E. Upfal, </author> <title> "Fault Tolerance in Networks of Bounded Degree", </title> <booktitle> 18th Annual Symposium on Theory of Computing, </booktitle> <year> 1986, </year> <pages> pp. 370-379. </pages>
Reference-contexts: 1 Introduction Dwork et. al. <ref> [DPPU] </ref> introduced the notion of almost everywhere agreement. <p> If all the correct processors hold the same initial value, then this must be the agreement value. Since Byzantine agreement is achievable only if the number of faulty processors is less then one-half of the connectivity of the network [D], Dwork et. al. <ref> [DPPU] </ref> suggest a relaxation of the standard definition of Byzantine agreement. Instead of requiring that all correct processors must reach a common decision, they require that all but a small fraction of the correct processors reach a common decision. Namely, Definition 2.1 [DPPU] Let G be a network, consisting of N <p> connectivity of the network [D], Dwork et. al. <ref> [DPPU] </ref> suggest a relaxation of the standard definition of Byzantine agreement. Instead of requiring that all correct processors must reach a common decision, they require that all but a small fraction of the correct processors reach a common decision. Namely, Definition 2.1 [DPPU] Let G be a network, consisting of N processors, and T and X be integers so that X &lt; NT 2 . <p> A protocol P achieves t ()-resilient x ()-agreement on the family G if for all sufficiently large N 2 N , the protocol P achieves t (N )-resilient x (N )-agreement on G N . Based on this definition we quote the following result. Theorem 2.1 <ref> [DPPU] </ref> There exist a constant 0 &lt; ff &lt; 1, an explicitly constructible family of networks fG N g of constant degree, and an agreement protocol P , such that P admits a t (N )- resilient (t (N ) + 1)-agreement on fG N g, for t (N ) ff <p> Hence, in order to execute <ref> [DPPU] </ref>'s agreement scheme on our network, we need only describe how the atomic operations of processors in the butterfly-compressor networks are translated into procedures on the committees of processors. [DPPU] use two atomic operations: sending a message to a neighbouring processor, and receiving a message from a neighbouring processor. Deciding on the majority value among values received can be seen as a special case of receiving. <p> Procedure agree (g) (an agreement procedure for a committee g 2 G i ) If i = s then g undergoes B.A. Else (i &lt; s) 1. Every subcommittee g 0 in g executes agree (g') 2. Run the <ref> [DPPU] </ref> agreement, exchanging each message passage between processors p 1 and p 2 , with a call to transmit (g 1 ; g 2 ), g 1 ; g 2 being the respective subcommittees. 4 When a committee has to decide, according to the [DPPU] protocol, on a majority value among <p> Run the <ref> [DPPU] </ref> agreement, exchanging each message passage between processors p 1 and p 2 , with a call to transmit (g 1 ; g 2 ), g 1 ; g 2 being the respective subcommittees. 4 When a committee has to decide, according to the [DPPU] protocol, on a majority value among values received from it's neighbors, then the majority decision is made by the individual processors on the lowest level of recursion of the receiving phase, before executing any agreement. The agreement can be viewed bottom-up instead of top-down.
Reference: [LPS] <author> A. Lubotzky, R. Phillips, and P. Sarnak, </author> <title> "Ramanugan Conjecture and Explicit Construction of Expanders", </title> <booktitle> 18th Annual Symposium on Theory of Computing, </booktitle> <year> 1986, </year> <pages> pp. 240-246. </pages>
Reference: [P] <author> N. Pippenger, </author> <title> "On Network of Noisy Gates", </title> <booktitle> 26th Annual Symposium on Foundation of Computer Science, </booktitle> <year> 1985, </year> <pages> pp. 30-38. 7 </pages>
Reference: [Ra] <author> P. Raghaven, </author> <title> "Probabilistic Construction of Deterministic Algorithms: Approximat--ing Packing Integer Programs", </title> <booktitle> 27th Annual Symposium on Foundation of Computer Science, </booktitle> <year> 1986, </year> <pages> pp. 10-18. </pages>
Reference-contexts: &lt; 1, such that if the lowest level committees of a network are of size N s m, and processors fail with probability ff N s * , the probability that the network is bad, is 2 ff N Proof : We use the following theorem by Raghavan and Spencer <ref> [Ra] </ref>. Let Y 1 ; :::; Y n be indepen dent Bernoully trials, and let be the expected value of their sum.
Reference: [U] <author> E. Upfal, </author> <title> "Tolerating Linear Number of Faults in Networks of Bounded Degree", </title> <booktitle> To appear in 10th Annual Symposium on Principles of Distributed Computing, </booktitle> <year> 1992. </year>
Reference-contexts: Therefore, even if the faulty processors are picked randomly, each independently with probability * &gt;&gt; 1= log N , then with high probability most communication paths between most pairs of correct processors will contain a faulty processor. Recently Upfal <ref> [U] </ref> proved that for any strong enough expander there is an almost everywhere agreement protocol tolerating linearly many faulty processors. The communication complexity of this protocol is polynomial but the local computation required by the processors themselves is exponential.
References-found: 8

