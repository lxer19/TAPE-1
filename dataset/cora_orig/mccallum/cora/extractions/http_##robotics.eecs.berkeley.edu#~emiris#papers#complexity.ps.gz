URL: http://robotics.eecs.berkeley.edu/~emiris/papers/complexity.ps.gz
Refering-URL: http://robotics.eecs.berkeley.edu/~emiris/
Root-URL: 
Email: emiris@cs.Berkeley.edu  
Title: On the Complexity of Sparse Elimination  
Author: Ioannis Z. Emiris 
Date: November 9, 1994  
Address: Berkeley, CA 94720, U.S.A  
Affiliation: Computer Science Division University of California  
Abstract: Sparse elimination exploits the structure of a set of multivariate polynomials by measuring complexity in terms of Newton polytopes. We examine polynomial systems that generate 0-dimensional ideals: a generic monomial basis for the coordinate ring of such a system is defined from a mixed subdivision. We offer a simple proof of this known fact and relate the computation of a monomial basis to the calculation of Mixed Volume. The proof relies on the construction of sparse resultant matrices and leads to the efficient computation of multiplication maps in the coordinate ring and the calculation of common zeros. It is shown that the size of monomial bases and multiplication maps in the context of sparse elimination theory is a function of the Mixed Volume of the Newton polytopes, whereas classical elimination considers simply total degree. Our algorithm for the sparse resultant and for root-finding has worst-case complexity proportional to the volume of the Minkowski Sum of these polytopes. We derive new bounds on the Minkowski Sum volume as a function of the Mixed Volume and use these results in order to give general upper bounds on the complexity of computing monomial bases, sparse resultants and common zeros. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A.D. Aleksandrov. </author> <title> On the Theory of Mixed Volumes of Convex Bodies, II. New Inequalities Between Mixed Volumes and Their Applications. Math. Sb. </title> <editor> N. S., </editor> <volume> 2 </volume> <pages> 1205-1238, </pages> <year> 1937. </year> <note> In Russian. </note>
Reference-contexts: Proof One of the most important inequalities in convexity theory is the Aleksandrov-Fenchel inequality <ref> [14, 1] </ref> which states that MV 2 (Q 1 ; : : : ; Q n ) MV (Q 1 ; Q 1 ; Q 3 ; : : : ; Q n ) MV (Q 2 ; Q 2 ; Q 3 ; : : :; Q n ); for
Reference: [2] <author> W. Auzinger and H.J. Stetter. </author> <title> An Elimination Algorithm for the Computation of all Zeros of a System of Multivariate Polynomial Equations. </title> <booktitle> In Proc. Intern. Conf. on Numerical Math., Intern. Series of Numerical Math., </booktitle> <volume> 86, </volume> <pages> pages 12-30. </pages> <publisher> Birkhauser Verlag, </publisher> <address> Basel, </address> <year> 1988. </year> <month> 18 </month>
Reference-contexts: Root-finding methods based on matrices have a long history. The classical resultant provides a means for root-finding by the use of U -resultants [37, 22, 32, 5]. The reduction to an eigenvalue and eigenvector problem was formalized in <ref> [2] </ref> and, independently, in [25, 24]. The latter articles discuss alternative strategies for dealing with ill-conditioned or singular matrices, some leading to the generalized eigenproblem; this issue is revisited at the end of Section 7.
Reference: [3] <author> D.N. Bernstein. </author> <title> The Number of Roots of a System of Equations. </title> <journal> Funct. Anal. and Appl., </journal> <volume> 9(2) </volume> <pages> 183-185, </pages> <year> 1975. </year>
Reference-contexts: A generalization of the Sylvester resultant for two univariate polynomials is the sparse resultant for an arbitrary number of multivariate polynomials, which, in many cases, has lower degree than its classical counterpart, since its degree depends on the Bernstein bound <ref> [3] </ref> as explained in the next section. Bernstein's bound is at most equal to Bezout's bound on the number of roots for an n fi n polynomial system and for sparse systems it is often smaller; the comparison between the two approaches is formalized in the following section. <p> This bound is also called the BKK bound to underline the contributions of Kushnirenko and Khovanskii in its development and proof [21, 19]. 3 Theorem 2.5 <ref> [3] </ref> Let f 1 ; : : : ; f n 2 K [x 1 ; x 1 ; : : : ; x n ; x 1 n ] with Newton polytopes Q 1 ; : : :; Q n .
Reference: [4] <author> Y.D. Burago and V.A. Zalgaller. </author> <title> Geometric Inequalities. </title> <journal> Grundlehren der mathematischen Wis-senschaften, </journal> <volume> 285. </volume> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1988. </year>
Reference-contexts: A consequence of this is MV n (Q 1 ; : : : ; Q n ) (n!) n V (Q 1 ) V (Q n ): These results, along with an extensive treatment of the theory, can be found in <ref> [4, 34] </ref>.
Reference: [5] <author> J. Canny. </author> <title> Generalised Characteristic Polynomials. </title> <journal> J. Symbolic Computation, </journal> <volume> 9 </volume> <pages> 241-250, </pages> <year> 1990. </year>
Reference-contexts: Root-finding methods based on matrices have a long history. The classical resultant provides a means for root-finding by the use of U -resultants <ref> [37, 22, 32, 5] </ref>. The reduction to an eigenvalue and eigenvector problem was formalized in [2] and, independently, in [25, 24]. The latter articles discuss alternative strategies for dealing with ill-conditioned or singular matrices, some leading to the generalized eigenproblem; this issue is revisited at the end of Section 7. <p> Algebraic multiplicity captures the usual notion of multiplicity, whereas geometric multiplicity expresses the dimension of the eigenspace associated with an eigenvalue. If there exist eigenvalues of higher geometric multiplicity we can use the properties of the U -resultant to recover the root coordinates <ref> [37, 22, 32, 5] </ref>. By Lemma 5.1 each eigenvector v 0 ff of M 0 contains the values of monomials B at some common root ff 2 (K ) n . Define vector v ff = M 1 ff (10) of size jEj m, indexed by E n B.
Reference: [6] <author> J. Canny and I. Emiris. </author> <title> An Efficient Algorithm for the Sparse Mixed Resultant. </title> <editor> In G. Cohen, T. Mora, and O. Moreno, editors, </editor> <booktitle> Proc. Intern. Symp. Applied Algebra, Algebraic Algor. and Error-Corr. Codes, Lect. Notes in Comp. Science 263, </booktitle> <pages> pages 89-104, </pages> <address> Puerto Rico, May 1993. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: Rege, Monomial Bases and Polynomial System Solving, in Proc. ACM Intern. Symp. on Symbolic and Algebraic Computation, 1994, pp. 114-122. Supported by a David and Lucile Packard Foundation Fellowship and by NSF P.Y.I. Grant IRI-8958577. matrix formulae for the sparse resultant already exist. We rely on the construction of <ref> [6] </ref> in order to offer a simple proof of the fact that a mixed subdivision defines a monomial basis for the coordinate ring of the given polynomial system. We consider the important case of square polynomial systems, i.e. systems of n polynomials in n variables. <p> A crucial hypothesis is that the given polynomials are generic, which is also assumed here. Our approach is based on a matrix formula for the sparse resultant <ref> [6] </ref> which leads to an immediate proof and applies also to arbitrary systems. <p> Under appropriate choice of the various parameters our approach obtains the same bases. 4 Sparse resultants have been studied by several authors and effective methods for the construction of matrix formulae have been proposed in <ref> [6, 36, 11, 35] </ref>. The first efficient and general method [6] is sketched in the next section. <p> Under appropriate choice of the various parameters our approach obtains the same bases. 4 Sparse resultants have been studied by several authors and effective methods for the construction of matrix formulae have been proposed in [6, 36, 11, 35]. The first efficient and general method <ref> [6] </ref> is sketched in the next section. The heuristic in [11] takes a different tack in an effort to improve upon the upper bounds, namely by avoiding the extraneous factor; it has been implemented and has given some encouraging preliminary results [13]. <p> The first efficient algorithm was proposed by Canny and Emiris <ref> [6] </ref> and subsequently generalized by Sturmfels [35]. Given are polynomials f 0 ; : : :; f n 2 K [x; x 1 ]. <p> Coefficient c ik appears in the column indexed by column monomial x q if c ik x q is a term of x pa ij f i . The entries of this row that do not correspond to any column monomial are zero. Lemma 4.2 <ref> [6] </ref> The above construction of M produces a well-defined and square matrix with size jEj, where j j denotes set cardinality. 6 We now sketch the proof establishing the generic nonsingularity of M , i.e. nonsingularity when the polynomials have generic, or indeterminate, coefficients. <p> Let matrix c M be obtained from M by specializing all coefficients to powers of a new variable t and denote by c M pq the entry of c M with row index p and column index q, for some p; q 2 E, then Lemma 4.3 <ref> [6, Lemma 16] </ref> For all non-zero elements c M pq with p 6= q, deg t ( c M pq ) &gt; deg t ( c M qq ). Lemma 4.4 Every principal minor of M is generically nonzero. <p> Now det N equals the product of det b N multiplied by a power in t, therefore it is also generically nonzero. 2 This also implies that M is generically nonsingular. We can now formalize the properties of M . Theorem 4.5 <ref> [6] </ref> Matrix M is well-defined, square, generically nonsingular and its determinant is divisible by the sparse resultant R (f 0 ; : : : ; f n ). <p> A greedy variant of this algorithm that typically leads to smaller matrices has been implemented by J. Canny and P. Pedersen and described in [13]. The construction of M leads to the explicit construction of the sparse resultant R by two alternative methods discussed in <ref> [6, 8] </ref>. 5 Monomial Bases for Coordinate Rings For n generic Laurent polynomials f 1 ; : : : ; f n in n variables, the definition of monomial bases from mixed subdivisions was first demonstrated by Pedersen and Sturmfels [31]. <p> Notice that, although the computation of monomial bases did not require the use of f 0 , here we do need this extra polynomial. In computing matrix M by the algorithm in <ref> [6] </ref>, f 0 is linear with generic coefficients, as in the proof of Theorem 5.2. <p> Before analyzing complexities, then, we establish some results on the relation of these two quantities. We denote by e the basis of the natural logarithm. For completeness we start with the result on the class of unmixed systems, first shown in <ref> [6] </ref>. Q 1 = = Q n : Lemma 8.1 For unmixed systems, V (Q) = fi (e n )MV (Q 1 ; : : : ; Q n ). <p> Again the construction of the mixed subdivision 0 ffi requires several applications of Linear Programming for which any polynomial-time algorithm may be used; the following bounds were based on Karmarkar's algorithm. Lemma 9.2 <ref> [6] </ref> Given are n+1 polynomials in n variables. Constructing resultant matrix M has worst-case bit complexity O fl ((nr) 5:5 jEj), where r is the maximum number of vertices in any Newton polytope and E = (Q 0 + ffi) " ZZ n .
Reference: [7] <author> J. Canny and J.M. Rojas. </author> <title> An optimal condition for determining the exact number of roots of a polynomial system. </title> <booktitle> In Proc. ACM Intern. Symp. on Symbolic and Algebraic Computation, </booktitle> <pages> pages 96-102, </pages> <address> Bonn, </address> <month> July </month> <year> 1991. </year>
Reference-contexts: For almost all specializations of the coefficients the number of common zeros is exactly MV (Q 1 ; : : : ; Q n ). Interesting extensions to this theorem concern the weakening of the genericity condition <ref> [7] </ref> and the case of roots in (K) n [33, 23]. We state the latter result.
Reference: [8] <author> J.F. Canny. </author> <title> The Complexity of Robot Motion Planning. </title> <publisher> M.I.T. Press, </publisher> <address> Cambridge, Mass., </address> <year> 1988. </year>
Reference-contexts: A greedy variant of this algorithm that typically leads to smaller matrices has been implemented by J. Canny and P. Pedersen and described in [13]. The construction of M leads to the explicit construction of the sparse resultant R by two alternative methods discussed in <ref> [6, 8] </ref>. 5 Monomial Bases for Coordinate Rings For n generic Laurent polynomials f 1 ; : : : ; f n in n variables, the definition of monomial bases from mixed subdivisions was first demonstrated by Pedersen and Sturmfels [31].
Reference: [9] <author> D. Coppersmith and S. Winograd. </author> <title> Matrix multiplication via arithmetic progressions. </title> <journal> J. Symbolic Computation, </journal> <volume> 9 </volume> <pages> 251-280, </pages> <year> 1990. </year>
Reference-contexts: Let MM () be the asymptotic complexity of matrix multiply as a function of the matrix size; currently MM (k) = O (n 2:376 ) <ref> [9] </ref>. It is known that inverting a matrix and computing its determinant and characteristic polynomial all have the same asymptotic complexity as matrix multiply [40]. The overall bit complexity depends on the bit sizes of the given coefficients and the root coordinates.
Reference: [10] <author> E. Ehrart. </author> <title> Sur un probleme de geometrie diophantienne, I. </title> <journal> Polyedres et reseaux. J. Reine Angew. Math., </journal> <volume> 226 </volume> <pages> 1-29, </pages> <year> 1967. </year>
Reference-contexts: The complexity of explicitly constructing the sparse resultant is bounded by a polynomial in jEj and n. The cardinality of an integer point set is asymptotically bounded by the volume of their Convex Hull <ref> [10] </ref>, hence jEj = O (V (Q 0 )). Recall that the scaling factor s of an overconstrained system with Newton polytopes Q 0 ; : : : ; Q n is the minimum real such that Q i sQ , where Q has minimum volume.
Reference: [11] <author> I. Emiris and J. Canny. </author> <title> A Practical Method for the Sparse Resultant. </title> <booktitle> In Proc. ACM Intern. Symp. on Symbolic and Algebraic Computation, </booktitle> <pages> pages 183-192, </pages> <address> Kiev, </address> <year> 1993. </year>
Reference-contexts: Under appropriate choice of the various parameters our approach obtains the same bases. 4 Sparse resultants have been studied by several authors and effective methods for the construction of matrix formulae have been proposed in <ref> [6, 36, 11, 35] </ref>. The first efficient and general method [6] is sketched in the next section. <p> The first efficient and general method [6] is sketched in the next section. The heuristic in <ref> [11] </ref> takes a different tack in an effort to improve upon the upper bounds, namely by avoiding the extraneous factor; it has been implemented and has given some encouraging preliminary results [13].
Reference: [12] <author> I.Z. Emiris and J.F. Canny. </author> <title> Efficient Incremental Algorithms for the Sparse Resultant and the Mixed Volume. </title> <note> Submitted for publication, </note> <year> 1994. </year>
Reference-contexts: We relate our proof on monomial bases to the most efficient general Mixed Volume algorithm to date, originating from Sturmfels' Lifting Algorithm [35] and modified by the heuristic proposed by Emiris and Canny <ref> [12] </ref>. Empirical results of this algorithm are reported in [13]. <p> The third step is the main part of the algorithm and, together with the equivalent problem of Mixed Volume computation, has been addressed by several authors as described in Section 3. The main idea of the algorithm from <ref> [35, 12] </ref> is to test all edge combinations, each combination including exactly one 10 edge from each Newton polytope: The combinations that pass all tests define a mixed cell. <p> n!1 (1 + 1=n) n = e. 2 9 Asymptotic Complexity We have sketched an algorithm for computing monomial bases that consists of testing various edge combinations on whether they lie on the lower envelope of the respective lifted Minkowski Sum or not; the algorithm is described in detail in <ref> [12] </ref>. Ignoring the pruning, the algorithm has to test g n combinations, where g is an upper bound on the number of edges in every Newton polytope.
Reference: [13] <author> I.Z. Emiris and A. </author> <title> Rege. Monomial Bases and Polynomial System Solving. </title> <booktitle> In Proc. ACM Intern. Symp. on Symbolic and Algebraic Computation, </booktitle> <pages> pages 114-122, </pages> <address> Oxford, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: Currently, problems from computer vision, direct kinematics and molecular structure are being successfully solved by the general sparse elimination methods discussed in this paper, thus illustrating their practical relevance <ref> [13, 28] </ref>. We start with an introduction to the theory of sparse elimination in the next section and we continue with a comparative exposition of previous work in Section 3 and a more detailed presentation of an efficient resultant matrix construction in Section 4. <p> The first efficient and general method [6] is sketched in the next section. The heuristic in [11] takes a different tack in an effort to improve upon the upper bounds, namely by avoiding the extraneous factor; it has been implemented and has given some encouraging preliminary results <ref> [13] </ref>. Exact matrix formulae for particular classes of polynomial systems are suggested in [36]; they are called of Sylvester-type since they generalize the Sylvester determinant for two univariate polynomials. Root-finding methods based on matrices have a long history. <p> We relate our proof on monomial bases to the most efficient general Mixed Volume algorithm to date, originating from Sturmfels' Lifting Algorithm [35] and modified by the heuristic proposed by Emiris and Canny [12]. Empirical results of this algorithm are reported in <ref> [13] </ref>. <p> M generalizes the classical Macaulay matrix since it reduces to the latter on dense systems. A greedy variant of this algorithm that typically leads to smaller matrices has been implemented by J. Canny and P. Pedersen and described in <ref> [13] </ref>. <p> This produces an overconstrained system without adding extra polynomial f 0 , thus keeping the problem dimension low. Our experience with the implementation of this algorithm suggests that hiding a variable is preferable for several systems in robotics and vision <ref> [13] </ref>. Formally, we consider the given polynomials as f 1 ; : : : ; f n 2 K (x n )[x 1 ; x 1 n1 ] and proceed with the construction of M and M 0 as before.
Reference: [14] <author> W. Fenchel. Inegalites quadratiques entre les volumes mixtes des corps convexes. C. R. </author> <title> Acad. </title> <journal> Sci. Paris, </journal> <volume> 203 </volume> <pages> 647-650, </pages> <year> 1936. </year>
Reference-contexts: Proof One of the most important inequalities in convexity theory is the Aleksandrov-Fenchel inequality <ref> [14, 1] </ref> which states that MV 2 (Q 1 ; : : : ; Q n ) MV (Q 1 ; Q 1 ; Q 3 ; : : : ; Q n ) MV (Q 2 ; Q 2 ; Q 3 ; : : :; Q n ); for
Reference: [15] <author> I.M. Gelfand, </author> <title> M.M. Kapranov, and A.V. Zelevinsky. Discriminants of Polynomials in Several Variables and Triangulations of Newton Polytopes. </title> <journal> Leningrad Math. J., </journal> <volume> 2(3) </volume> <pages> 449-505, </pages> <year> 1991. </year> <note> (Translated from Algebra i Analiz 2, </note> <year> 1990, </year> <pages> 1-62). </pages>
Reference-contexts: This leads to stronger algebraic and combinatorial results in general, whose complexity depends on effective rather than total degree. The foundations were laid in the work of Gelfand, Kapranov and Zelevinsky <ref> [15, 16] </ref>. The central object in elimination theory is the resultant, which characterizes the solvability of an overconstrained system.
Reference: [16] <editor> I.M. Gelfand, M.M. Kapranov, and A.V. Zelevinsky. Discriminants and Resultants. </editor> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1994. </year>
Reference-contexts: This leads to stronger algebraic and combinatorial results in general, whose complexity depends on effective rather than total degree. The foundations were laid in the work of Gelfand, Kapranov and Zelevinsky <ref> [15, 16] </ref>. The central object in elimination theory is the resultant, which characterizes the solvability of an overconstrained system.
Reference: [17] <author> B. Huber and B. Sturmfels. </author> <title> A polyhedral method for solving sparse polynomial systems. </title> <journal> Math. Comp. </journal> <note> To appear. A preliminary version presented at the Workshop on Real Algebraic Geometry, </note> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: Empirical results of this algorithm are reported in [13]. Other methods, exploiting special cases, were proposed in <ref> [17, 39, 38] </ref> in conjunction to defining sparse homotopies for solving polynomial systems by continuation. 4 Sparse Resultant Matrices The main construction in our approach for establishing the result on monomial bases and for obtaining the sparse resultant is the construction of a matrix M in the polynomial coefficients, whose determinant
Reference: [18] <author> N. Karmarkar. </author> <title> A New Polynomial-Time Algorithm for Linear Programming. </title> <journal> Combinatorica, </journal> <volume> 4 </volume> <pages> 373-395, </pages> <year> 1984. </year>
Reference-contexts: The constraints ensure that b p lies on the same vertical as a variable point defined as the Minkowski sum of points from the lifted polytopes. Linear Programming may be solved by any polynomial-time algorithm; in what follows we apply Karmarkar's algorithm <ref> [18] </ref>.
Reference: [19] <author> A.G. Khovanskii. </author> <title> Newton Polyhedra and the Genus of Complete Intersections. </title> <journal> Funktsional'nyi Analiz i Ego Prilozheniya, </journal> <volume> 12(1) </volume> <pages> 51-61, </pages> <address> Jan.-Mar. </address> <year> 1978. </year>
Reference-contexts: The Newton polytopes offer a convenient model for the sparseness of a polynomial system, in light of Bernstein's upper bound on the number of common roots. This bound is also called the BKK bound to underline the contributions of Kushnirenko and Khovanskii in its development and proof <ref> [21, 19] </ref>. 3 Theorem 2.5 [3] Let f 1 ; : : : ; f n 2 K [x 1 ; x 1 ; : : : ; x n ; x 1 n ] with Newton polytopes Q 1 ; : : :; Q n .
Reference: [20] <editor> A.G. Khovanskii. Fewnomials. </editor> <publisher> AMS Press, </publisher> <address> Providence, Rhode Island, </address> <year> 1991. </year>
Reference-contexts: If r is an upper bound on the number of polytope vertices, g r 2 and the number of tests is O (r 2n ). Note that r is bounded by the maximum number of monomials in any polynomial; the latter provides a different model of sparseness studied in <ref> [20] </ref>.
Reference: [21] <author> A.G. Kushnirenko. </author> <title> Newton polytopes and the Bezout theorem. </title> <journal> Funktsional'nyi Analiz i Ego Prilozheniya, </journal> <volume> 10(3), Jul.-Sep. </volume> <year> 1976. </year>
Reference-contexts: The Newton polytopes offer a convenient model for the sparseness of a polynomial system, in light of Bernstein's upper bound on the number of common roots. This bound is also called the BKK bound to underline the contributions of Kushnirenko and Khovanskii in its development and proof <ref> [21, 19] </ref>. 3 Theorem 2.5 [3] Let f 1 ; : : : ; f n 2 K [x 1 ; x 1 ; : : : ; x n ; x 1 n ] with Newton polytopes Q 1 ; : : :; Q n .
Reference: [22] <author> D. Lazard. </author> <title> Resolution des systemes d'equations algebriques. </title> <journal> Theor. Comp. Science, </journal> <volume> 15 </volume> <pages> 77-110, </pages> <year> 1981. </year>
Reference-contexts: Root-finding methods based on matrices have a long history. The classical resultant provides a means for root-finding by the use of U -resultants <ref> [37, 22, 32, 5] </ref>. The reduction to an eigenvalue and eigenvector problem was formalized in [2] and, independently, in [25, 24]. The latter articles discuss alternative strategies for dealing with ill-conditioned or singular matrices, some leading to the generalized eigenproblem; this issue is revisited at the end of Section 7. <p> Algebraic multiplicity captures the usual notion of multiplicity, whereas geometric multiplicity expresses the dimension of the eigenspace associated with an eigenvalue. If there exist eigenvalues of higher geometric multiplicity we can use the properties of the U -resultant to recover the root coordinates <ref> [37, 22, 32, 5] </ref>. By Lemma 5.1 each eigenvector v 0 ff of M 0 contains the values of monomials B at some common root ff 2 (K ) n . Define vector v ff = M 1 ff (10) of size jEj m, indexed by E n B.
Reference: [23] <author> T.Y. Li and X. Wang. </author> <title> The BKK root count in C N . Manuscript, </title> <year> 1994. </year>
Reference-contexts: For almost all specializations of the coefficients the number of common zeros is exactly MV (Q 1 ; : : : ; Q n ). Interesting extensions to this theorem concern the weakening of the genericity condition [7] and the case of roots in (K) n <ref> [33, 23] </ref>. We state the latter result. <p> Interesting extensions to this theorem concern the weakening of the genericity condition [7] and the case of roots in (K) n [33, 23]. We state the latter result. Theorem 2.6 <ref> [23] </ref> For polynomials f 1 ; : : : ; f n 2 C [x; x 1 ] with supports A 1 ; : : : ; A n the number of common isolated zeros in C n , counting multiplicities, is upwards bounded by MV (A 1 [f0g; : :
Reference: [24] <author> D. Manocha and J. Canny. </author> <title> Multipolynomial Resultants and Linear Algebra. </title> <booktitle> In Proc. ACM Intern. Symp. on Symbolic and Algebraic Computation, </booktitle> <pages> pages 96-102, </pages> <year> 1992. </year>
Reference-contexts: Sparse resultants have a significant potential for applications reducing to questions in elimination and to polynomial system solving. Techniques based on ad-hoc resultants have led to impressive results on certain problems in inverse kinematics, graphics and modeling <ref> [25, 24] </ref>. Currently, problems from computer vision, direct kinematics and molecular structure are being successfully solved by the general sparse elimination methods discussed in this paper, thus illustrating their practical relevance [13, 28]. <p> Root-finding methods based on matrices have a long history. The classical resultant provides a means for root-finding by the use of U -resultants [37, 22, 32, 5]. The reduction to an eigenvalue and eigenvector problem was formalized in [2] and, independently, in <ref> [25, 24] </ref>. The latter articles discuss alternative strategies for dealing with ill-conditioned or singular matrices, some leading to the generalized eigenproblem; this issue is revisited at the end of Section 7.
Reference: [25] <author> D. Manocha and J. Canny. </author> <title> Real Time Inverse Kinematics of General 6R Manipulators. </title> <booktitle> In Proc. IEEE Intern. Conf. Robotics and Automation, </booktitle> <pages> pages 383-389, </pages> <address> Nice, </address> <month> May </month> <year> 1992. </year> <month> 19 </month>
Reference-contexts: Sparse resultants have a significant potential for applications reducing to questions in elimination and to polynomial system solving. Techniques based on ad-hoc resultants have led to impressive results on certain problems in inverse kinematics, graphics and modeling <ref> [25, 24] </ref>. Currently, problems from computer vision, direct kinematics and molecular structure are being successfully solved by the general sparse elimination methods discussed in this paper, thus illustrating their practical relevance [13, 28]. <p> Root-finding methods based on matrices have a long history. The classical resultant provides a means for root-finding by the use of U -resultants [37, 22, 32, 5]. The reduction to an eigenvalue and eigenvector problem was formalized in [2] and, independently, in <ref> [25, 24] </ref>. The latter articles discuss alternative strategies for dealing with ill-conditioned or singular matrices, some leading to the generalized eigenproblem; this issue is revisited at the end of Section 7.
Reference: [26] <author> H.M. Moller. </author> <title> Systems of Algebraic Equations Solved by Means of Endomorphisms. </title> <editor> In G. Cohen, T. Mora, and O. Moreno, editors, </editor> <booktitle> Proc. Intern. Symp. Applied Algebra, Algebraic Algorithms and Error-Corr. Codes, Lect. Notes in Comp. Science 263, </booktitle> <pages> pages 43-56, </pages> <address> Puerto Rico, May 1993. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: The definition of monomial bases and multiplication maps is also possible through Grobner bases, so we can again reduce polynomial system solving to an eigenproblem; this approach is surveyed in <ref> [26] </ref>. The problem of monomial bases is equivalent to computing Mixed Volumes, for which various algorithms have been proposed.
Reference: [27] <author> H.M. Moller and H.J. Stetter. </author> <title> Multivariate polynomial equations with multiple zeros solved by matrix eigenproblems. </title> <note> Submitted for Publication, </note> <year> 1994. </year>
Reference-contexts: Suggestions and ideas may originate from current work on the same problem in the context of Grobner bases <ref> [27] </ref>. An interesting question is to quantify the relation between Mixed Volume and Minkowski Sum volume when polytopes are allowed to have zero n-dimensional volume.
Reference: [28] <author> D. Parsons and J. Canny. </author> <title> Geometric problems in molecular biology and robotics. </title> <booktitle> In Proc. 2nd Intern. Conf. on Intelligent Systems for Molecular Biology, </booktitle> <pages> pages 322-330, </pages> <address> Palo Alto, CA, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Currently, problems from computer vision, direct kinematics and molecular structure are being successfully solved by the general sparse elimination methods discussed in this paper, thus illustrating their practical relevance <ref> [13, 28] </ref>. We start with an introduction to the theory of sparse elimination in the next section and we continue with a comparative exposition of previous work in Section 3 and a more detailed presentation of an efficient resultant matrix construction in Section 4.
Reference: [29] <author> P. Pedersen. </author> <title> AMS-IMS-SIAM Summer Conference on Continuous Algorithms and Complexity. Mt. </title> <address> Holyoke, Mass., </address> <month> July </month> <year> 1994. </year>
Reference-contexts: The general bound is now immediate. 2 This is asymptotically optimal because the monomial basis problem is equivalent to Mixed Volume which generalizes Convex Hull Volume which is #P-hard. Moreover, it has recently been shown that the Mixed Volume problem is #P-complete <ref> [29] </ref>. For most practical applications the extra hypothesis is satisfied and the tighter bound r O (n) applies.
Reference: [30] <author> P. Pedersen and B. Sturmfels. </author> <title> Product Formulas for Resultants and Chow Forms. </title> <journal> Math. Zeitschrift, </journal> <volume> 214 </volume> <pages> 377-396, </pages> <year> 1993. </year>
Reference-contexts: It is a single polynomial in the polynomial coefficients which characterizes the existence of nontrivial common zeros. In sparse elimination, nontrivial roots lie in (K fl ) n and the sparse resultant of an overconstrained system is defined as follows <ref> [30] </ref>. Let c be the vector of all polynomial coefficients, regarded as indeterminates, and let Z 0 be the set of all such vectors c for which the polynomials have a common zero. Let Z be the Zariski closure of Z 0 .
Reference: [31] <author> P. Pedersen and B. Sturmfels. </author> <title> Mixed Monomial Bases. </title> <booktitle> In Proc. MEGA '94, Santander, </booktitle> <address> Spain, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: It is interesting to note that it subsumes the classical definition of the resultant [37]. 3 Related Work A method for constructing generic vector bases of coordinate rings as monomials indexed by the lattice points in the mixed cells of a mixed subdivision was first demonstrated by Pedersen and Sturmfels <ref> [31] </ref>. The term mixed monomial bases highlights the fact that they apply to arbitrary systems and that they are obtained through a mixed subdivision. A crucial hypothesis is that the given polynomials are generic, which is also assumed here. <p> the sparse resultant R by two alternative methods discussed in [6, 8]. 5 Monomial Bases for Coordinate Rings For n generic Laurent polynomials f 1 ; : : : ; f n in n variables, the definition of monomial bases from mixed subdivisions was first demonstrated by Pedersen and Sturmfels <ref> [31] </ref>. Their proof relies on reducing the general problem to binomial systems via Puiseux series. Theorem 5.4 verifies their result.
Reference: [32] <author> J. Renegar. </author> <title> On the Computational Complexity of the First-Order Theory of the Reals, parts I, II, III. </title> <journal> J. Symbolic Computation, </journal> <volume> 13(3) </volume> <pages> 255-352, </pages> <year> 1992. </year>
Reference-contexts: Root-finding methods based on matrices have a long history. The classical resultant provides a means for root-finding by the use of U -resultants <ref> [37, 22, 32, 5] </ref>. The reduction to an eigenvalue and eigenvector problem was formalized in [2] and, independently, in [25, 24]. The latter articles discuss alternative strategies for dealing with ill-conditioned or singular matrices, some leading to the generalized eigenproblem; this issue is revisited at the end of Section 7. <p> Algebraic multiplicity captures the usual notion of multiplicity, whereas geometric multiplicity expresses the dimension of the eigenspace associated with an eigenvalue. If there exist eigenvalues of higher geometric multiplicity we can use the properties of the U -resultant to recover the root coordinates <ref> [37, 22, 32, 5] </ref>. By Lemma 5.1 each eigenvector v 0 ff of M 0 contains the values of monomials B at some common root ff 2 (K ) n . Define vector v ff = M 1 ff (10) of size jEj m, indexed by E n B.
Reference: [33] <author> J.M. Rojas. </author> <title> A Convex Geometric Approach to Counting the Roots of a Polynomial System. </title> <note> To appear. Presented at the Workshop on Continuous Algorithms and Complexity, </note> <month> Oct. </month> <year> 1993. </year>
Reference-contexts: For almost all specializations of the coefficients the number of common zeros is exactly MV (Q 1 ; : : : ; Q n ). Interesting extensions to this theorem concern the weakening of the genericity condition [7] and the case of roots in (K) n <ref> [33, 23] </ref>. We state the latter result.
Reference: [34] <author> R. Schneider. </author> <title> Convex Bodies: The Brunn-Minkowski Theory. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1993. </year>
Reference-contexts: Definition 2.2 The Minkowski Sum A + B of convex polytopes A and B in IR n is the set A + B = fa + b j a 2 A; b 2 Bg IR n : It is easy to prove that A + B is a convex polytope <ref> [34] </ref>. <p> An equivalent definition <ref> [34] </ref> is Definition 2.4 For 1 ; : : : ; n 2 IR 0 and convex polytopes A 1 ; : : : ; A n IR n , the Mixed Volume MV (A 1 ; : : :; A n ) is precisely the coefficient of 1 2 n <p> A consequence of this is MV n (Q 1 ; : : : ; Q n ) (n!) n V (Q 1 ) V (Q n ): These results, along with an extensive treatment of the theory, can be found in <ref> [4, 34] </ref>.
Reference: [35] <author> B. Sturmfels. </author> <title> On the Newton Polytope of the Resultant. </title> <journal> J. of Algebr. Combinatorics, </journal> <volume> 3 </volume> <pages> 207-236, </pages> <year> 1994. </year>
Reference-contexts: Let c be the vector of all polynomial coefficients, regarded as indeterminates, and let Z 0 be the set of all such vectors c for which the polynomials have a common zero. Let Z be the Zariski closure of Z 0 . Definition 2.7 <ref> [35] </ref> The sparse resultant R = R (A 0 ; : : : ; A n ) of polynomials f 0 ; f 1 ; : : : ; f n 2 K [x; x 1 ] is an irreducible polynomial in ZZ [c]. <p> Under appropriate choice of the various parameters our approach obtains the same bases. 4 Sparse resultants have been studied by several authors and effective methods for the construction of matrix formulae have been proposed in <ref> [6, 36, 11, 35] </ref>. The first efficient and general method [6] is sketched in the next section. <p> The problem of monomial bases is equivalent to computing Mixed Volumes, for which various algorithms have been proposed. We relate our proof on monomial bases to the most efficient general Mixed Volume algorithm to date, originating from Sturmfels' Lifting Algorithm <ref> [35] </ref> and modified by the heuristic proposed by Emiris and Canny [12]. Empirical results of this algorithm are reported in [13]. <p> The first efficient algorithm was proposed by Canny and Emiris [6] and subsequently generalized by Sturmfels <ref> [35] </ref>. Given are polynomials f 0 ; : : :; f n 2 K [x; x 1 ]. <p> To this end, we adopt a technique from <ref> [35] </ref>. Select n + 1 linear lifting forms l i : IR n ! IR for 0 i n. <p> The third step is the main part of the algorithm and, together with the equivalent problem of Mixed Volume computation, has been addressed by several authors as described in Section 3. The main idea of the algorithm from <ref> [35, 12] </ref> is to test all edge combinations, each combination including exactly one 10 edge from each Newton polytope: The combinations that pass all tests define a mixed cell.
Reference: [36] <author> B. Sturmfels and A. Zelevinsky. </author> <title> Multigraded Resultants of Sylvester Type. </title> <journal> J. of Algebra, </journal> <volume> 163(1) </volume> <pages> 115-127, </pages> <year> 1994. </year>
Reference-contexts: Under appropriate choice of the various parameters our approach obtains the same bases. 4 Sparse resultants have been studied by several authors and effective methods for the construction of matrix formulae have been proposed in <ref> [6, 36, 11, 35] </ref>. The first efficient and general method [6] is sketched in the next section. <p> The heuristic in [11] takes a different tack in an effort to improve upon the upper bounds, namely by avoiding the extraneous factor; it has been implemented and has given some encouraging preliminary results [13]. Exact matrix formulae for particular classes of polynomial systems are suggested in <ref> [36] </ref>; they are called of Sylvester-type since they generalize the Sylvester determinant for two univariate polynomials. Root-finding methods based on matrices have a long history. The classical resultant provides a means for root-finding by the use of U -resultants [37, 22, 32, 5].
Reference: [37] <author> B.L. van der Waerden. </author> <title> Modern Algebra. </title> <editor> F. </editor> <publisher> Ungar Publishing Co., </publisher> <address> New York, </address> <note> 3rd edition, </note> <year> 1950. </year>
Reference-contexts: Some authors call this the Newton resultant to underline its dependence on the Newton polytopes. It is interesting to note that it subsumes the classical definition of the resultant <ref> [37] </ref>. 3 Related Work A method for constructing generic vector bases of coordinate rings as monomials indexed by the lattice points in the mixed cells of a mixed subdivision was first demonstrated by Pedersen and Sturmfels [31]. <p> Root-finding methods based on matrices have a long history. The classical resultant provides a means for root-finding by the use of U -resultants <ref> [37, 22, 32, 5] </ref>. The reduction to an eigenvalue and eigenvector problem was formalized in [2] and, independently, in [25, 24]. The latter articles discuss alternative strategies for dealing with ill-conditioned or singular matrices, some leading to the generalized eigenproblem; this issue is revisited at the end of Section 7. <p> Algebraic multiplicity captures the usual notion of multiplicity, whereas geometric multiplicity expresses the dimension of the eigenspace associated with an eigenvalue. If there exist eigenvalues of higher geometric multiplicity we can use the properties of the U -resultant to recover the root coordinates <ref> [37, 22, 32, 5] </ref>. By Lemma 5.1 each eigenvector v 0 ff of M 0 contains the values of monomials B at some common root ff 2 (K ) n . Define vector v ff = M 1 ff (10) of size jEj m, indexed by E n B.
Reference: [38] <author> J. Verschelde and K. Gatermann. </author> <title> Symmetric Newton Polytopes for Solving Sparse Polynomial Systems. </title> <type> Technical Report 3, </type> <institution> Konrad-Zuse-Zentrum fur Informationstechnik Berlin, </institution> <year> 1994. </year>
Reference-contexts: Empirical results of this algorithm are reported in [13]. Other methods, exploiting special cases, were proposed in <ref> [17, 39, 38] </ref> in conjunction to defining sparse homotopies for solving polynomial systems by continuation. 4 Sparse Resultant Matrices The main construction in our approach for establishing the result on monomial bases and for obtaining the sparse resultant is the construction of a matrix M in the polynomial coefficients, whose determinant
Reference: [39] <author> J. Verschelde, P. Verlinden, and R. Cools. </author> <title> Homotopies Exploiting Newton Polytopes for Solving Sparse Polynomial Systems. </title> <journal> SIAM J. Numerical Analysis, </journal> <volume> 31(3) </volume> <pages> 915-930, </pages> <year> 1994. </year>
Reference-contexts: Empirical results of this algorithm are reported in [13]. Other methods, exploiting special cases, were proposed in <ref> [17, 39, 38] </ref> in conjunction to defining sparse homotopies for solving polynomial systems by continuation. 4 Sparse Resultant Matrices The main construction in our approach for establishing the result on monomial bases and for obtaining the sparse resultant is the construction of a matrix M in the polynomial coefficients, whose determinant
Reference: [40] <author> J. von zur Gathen. </author> <title> Algebraic complexity theory. </title> <editor> In J. Traub, editor, </editor> <booktitle> Annual Review of Computer Science, </booktitle> <pages> pages 317-347. </pages> <publisher> Annual Reviews, </publisher> <address> Palo Alto, CA, </address> <year> 1988. </year> <month> 20 </month>
Reference-contexts: Let MM () be the asymptotic complexity of matrix multiply as a function of the matrix size; currently MM (k) = O (n 2:376 ) [9]. It is known that inverting a matrix and computing its determinant and characteristic polynomial all have the same asymptotic complexity as matrix multiply <ref> [40] </ref>. The overall bit complexity depends on the bit sizes of the given coefficients and the root coordinates.
References-found: 40

