URL: http://www.cs.colorado.edu/~grunwald/LowPowerWorkshop/FinalPapers/PS/iris-brown.ps
Refering-URL: http://www.cs.colorado.edu/~grunwald/LowPowerWorkshop/agenda.html
Root-URL: http://www.cs.colorado.edu
Title: Power and Performance Tradeoffs using Various Cache Configurations  
Author: Gianluca Albera xy and R. Iris Bahar 
Address: Torino, ITALY 10129  Providence, RI 02912  
Affiliation: Politecnico di Torino Dip. di Automatica e Informatica  Brown University Division of Engineering  
Abstract: In this paper, we will propose several different data and instruction cache configurations and analyze their power as well as performance implications on the processor. Unlike most existing work in low power microprocessor design, we explore a high performance processor with the latest innovations for performance. Using a detailed, architectural-level simulator, we evaluate full system performance using several different power/performance sensitive cache configurations. We then use the information obtained from the simulator to calculate the energy consumption of the memory hierarchy of the system. Based on the results obtained from these simulations, we will determine the general characteristics of each cache configuration. We will also make recommendations on how best to balance power and performance tradeoffs in memory hierarchy design. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> I. Bahar, G. Albera and S. Manne, </author> <title> Using Confidence to Reduce Energy Consumption in High-Performance Microprocessors, </title> <booktitle> to appear in International Symposium on Low Power Electronics and Design (ISLEPD), </booktitle> <year> 1998 </year>
Reference-contexts: The energy consump tion is computed as: E cache = E bit + E word + E output (1) A brief description of each of these components follows. Further details can be found in <ref> [1] </ref>. Energy dissipated in bit-lines E bit is the energy consumption in the bit-lines; it is due to precharging lines (including driving the precharge logic) and reading or writing data. <p> Given a shorter latency L2 cache (e.g. on-chip L2 design), this algorithm might present better overall results. 3.5 Speculative buffer The idea of a speculative buffer is based on previous work by the authors <ref> [1] </ref> and based on use of confidence predictors presented in Manne et al. [8]. In this case we mark every cache access with a confidence level obtained by examining the processor speculation state and the current branch prediction estimate.
Reference: [2] <author> D. Burger, and T. M. Austin, </author> <title> The SimpleScalar Tool Set, Version 2.0, </title> <type> Technical Report TR#1342, </type> <institution> University of Wisconsin, </institution> <month> June </month> <year> 1997. </year>
Reference-contexts: First, the CPU simulator will be briefly introduced and then we will describe how we obtained data about the energy consumption of the caches. Finally we will describe each architectural design we considered in our analysis. 2.1 Full model simulator We use an extension of the SimpleScalar <ref> [2] </ref> tool suite. SimpleScalar is an execution-driven simulator that uses binaries compiled to a MIPS-like target. SimpleScalar can accurately model a high-performance, dynamically-scheduled, multi-issue processor.
Reference: [3] <author> D. Burger, and T. M. </author> <title> Austin, </title> <booktitle> SimpleScalar Tutorial , presented at 30th International Symposium on Microar-chitecture, </booktitle> <address> Research Triangle Park, NC, </address> <month> December, </month> <year> 1997. </year>
Reference-contexts: SimpleScalar is an execution-driven simulator that uses binaries compiled to a MIPS-like target. SimpleScalar can accurately model a high-performance, dynamically-scheduled, multi-issue processor. We use an extended version of the simulator that more accurately models all the memory hierarchy, implementing non-blocking caches and complete bus bandwidth and contention modeling <ref> [3] </ref>. Other modifications were added to handle precise modeling of cache fills. Tables 1, 2, and 3 show the configuration of the processor modeled. Note that first level caches are on-chip, while the unified second level cache is off-chip.
Reference: [4] <author> T. L. Johnson, and W. W. Hwu, </author> <title> Run-time Adaptive Cache Hierarchy Management via Reference Analysis, </title> <booktitle> ISCA-97: ACM/IEEE International Symposium on Computer Architecture, </booktitle> <pages> pp. 315326, </pages> <address> Denver, CO, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: Reducing cache misses due to line conflicts has been shown to be effective in improving overall system performance in high-performance processors. Techniques to reduce conflicts include increasing cache associativity, use of victim caches [5], or cache bypassing with and without the aid of a buffer <ref> [4, 9, 11] </ref>. Figure 1 shows the design of the memory hierarchy when using buffers alongside the first level caches. alongside the L1 caches. These buffers may be used as victim caches, non-temporal buffers, or speculative buffers.
Reference: [5] <author> N. Jouppi, </author> <title> Improving Direct-Mapped Cache Performance by the Addition of a Small Fully-Associative Cache and Prefetch Buffers, </title> <booktitle> ISCA-17: ACM/IEEE International Symposium on Computer Architecture, </booktitle> <pages> pp. 364 373, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: This paper will propose memory hierarchy configurations that reduce power while retaining performance. Reducing cache misses due to line conflicts has been shown to be effective in improving overall system performance in high-performance processors. Techniques to reduce conflicts include increasing cache associativity, use of victim caches <ref> [5] </ref>, or cache bypassing with and without the aid of a buffer [4, 9, 11]. Figure 1 shows the design of the memory hierarchy when using buffers alongside the first level caches. alongside the L1 caches. These buffers may be used as victim caches, non-temporal buffers, or speculative buffers. <p> In the following sections we will presents various architectures we have analyzed. We will start with some literature examples such as the victim cache <ref> [5] </ref> and non-temporal buffer [9] and then we will present new schemes based on the use of buffers and how to combine them. 3.3 Victim cache We consider the idea of a victim cache originally presented by Jouppi in [5], with small changes. The author presented the following algorithm. <p> will start with some literature examples such as the victim cache <ref> [5] </ref> and non-temporal buffer [9] and then we will present new schemes based on the use of buffers and how to combine them. 3.3 Victim cache We consider the idea of a victim cache originally presented by Jouppi in [5], with small changes. The author presented the following algorithm. <p> This result generally applies to all uses of buffers we tried. As stated before, we show that the non swapping mechanism generally outperforms the original algorithm presented by the author. In <ref> [5] </ref>, the data cache of a single issue processor was considered, where a memory access occurs approximately one out of four cycles; thus the victim cache had ample time to perform the necessary swapping.
Reference: [6] <author> M. B. Kamble and K. Ghose, </author> <title> Analytical Energy Dissipation Models for Low Power Caches, </title> <booktitle> ACM/IEEE International Symposium on Low-Power Electronics and Design, </booktitle> <month> August, </month> <year> 1997. </year>
Reference-contexts: Their model assumes a 0.8 m process; if a different process is used, only the transistor capacitances need to be recomputed. To obtain the number of transitions that occur on each transistor, we refer to Kamble and Ghose <ref> [6] </ref>, adapting their work to our overall architecture. An m-way set associative cache consists of three main parts: a data array, a tag array and the necessary control logic. The data array consists of S rows containing m lines. <p> In order to maximize speed we kept the arrays as square as possible by splitting the data and tag array vertically and/or horizontally. Sub-arrays can also be folded. We used the tool CACTI [12] to compute sub-arraying parameters for all our caches. According to <ref> [6] </ref> we consider the main sources of power to be the following three components:E bit , E word , E output . We are not considering the energy dissipated in the address decoders, since we found this value to be negligible compared to the other components.
Reference: [7] <author> J. Kin, M. Gupta, and W. H. Mangione-Smith, </author> <title> The Filter Cache: An Energy Efficient Memory Structure, </title> <booktitle> MICRO-97: ACM/IEEE International Symposium on Microarchi-tecture, </booktitle> <pages> pp. 184193, </pages> <address> Research Triangle Park, NC, </address> <month> De-cember </month> <year> 1997. </year>
Reference-contexts: Prior research has been aimed at measuring and recommending optimal cache configuration for power. For instance, in [10], the authors determined that high performance caches were also the lowest power consuming caches since they reduce the traffic to the lower level of the memory system. The work by Kin <ref> [7] </ref> proposed accessing a small filter cache before accessing the first level cache to reduce the accesses (and energy consumption) from DL1. The idea lead to a large reduction in memory hierarchy energy consumption, but also resulted in a substantial reduction in processor performance. <p> We are not considering the energy dissipated in the address decoders, since we found this value to be negligible compared to the other components. Similar to Kin <ref> [7] </ref>, we found that the energy consumption of the decoders is about three orders of magnitude smaller than that of the other components.
Reference: [8] <author> S. Manne, D. Grunwald, A. Klauser, </author> <title> Pipeline Gating: Speculation Control for Energy Reduction, to appear in ISCA-25: </title> <booktitle> ACM/IEEE International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1998. </year>
Reference-contexts: Given a shorter latency L2 cache (e.g. on-chip L2 design), this algorithm might present better overall results. 3.5 Speculative buffer The idea of a speculative buffer is based on previous work by the authors [1] and based on use of confidence predictors presented in Manne et al. <ref> [8] </ref>. In this case we mark every cache access with a confidence level obtained by examining the processor speculation state and the current branch prediction estimate.
Reference: [9] <author> J. A. Rivers, and E. S. Davidson, </author> <title> Reducing Conflicts in Direct-Mapped Caches with a Temporality-Based Design, </title> <booktitle> International Conference on Parallel Processing, </booktitle> <pages> pp. 154-163, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: Reducing cache misses due to line conflicts has been shown to be effective in improving overall system performance in high-performance processors. Techniques to reduce conflicts include increasing cache associativity, use of victim caches [5], or cache bypassing with and without the aid of a buffer <ref> [4, 9, 11] </ref>. Figure 1 shows the design of the memory hierarchy when using buffers alongside the first level caches. alongside the L1 caches. These buffers may be used as victim caches, non-temporal buffers, or speculative buffers. <p> Since one of our architectures was intended by the authors for floating point applications <ref> [9] </ref>, we also ran a subset of SPECfp95 in this case. Since we are executing a full model on a a very detailed simulator, the benchmarks take several hours to complete; due to time constraints we feed the simulator with a small set of inputs. <p> In the following sections we will presents various architectures we have analyzed. We will start with some literature examples such as the victim cache [5] and non-temporal buffer <ref> [9] </ref> and then we will present new schemes based on the use of buffers and how to combine them. 3.3 Victim cache We consider the idea of a victim cache originally presented by Jouppi in [5], with small changes. The author presented the following algorithm. <p> It is interesting to note that increased performance and energy reduction usually go hand-in-hand, since a reduced L2 activity helps both of them. 3.4 Non-temporal buffer This idea has been formulated by Rivers and Davidson in <ref> [9] </ref> and refers to data buffer only.
Reference: [10] <author> C. Su, and A. Despain, </author> <title> Cache Design Tradeoffs for Power and Performance Optimization: A Case Study, </title> <booktitle> ACM/IEEE International Symposium on Low-Power Design, </booktitle> <pages> pp. </pages> <month> 6368, </month> <title> Dana Point, </title> <address> CA, </address> <year> 1995. </year>
Reference-contexts: In particular, we will investigate architectural-level solutions that achieve a power reduction in the memory subsystem of the processor without compromising performance. Prior research has been aimed at measuring and recommending optimal cache configuration for power. For instance, in <ref> [10] </ref>, the authors determined that high performance caches were also the lowest power consuming caches since they reduce the traffic to the lower level of the memory system.
Reference: [11] <author> G. Tyson, M. Farrens, J. Matthews, and A. R. Pleszkun, </author> <title> Managing Data Caches using Selective Cache Line Replacement, </title> <journal> Journal of Parallel Programming, </journal> <volume> Vol. 25, No. 3, </volume> <pages> pp. 213242, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Reducing cache misses due to line conflicts has been shown to be effective in improving overall system performance in high-performance processors. Techniques to reduce conflicts include increasing cache associativity, use of victim caches [5], or cache bypassing with and without the aid of a buffer <ref> [4, 9, 11] </ref>. Figure 1 shows the design of the memory hierarchy when using buffers alongside the first level caches. alongside the L1 caches. These buffers may be used as victim caches, non-temporal buffers, or speculative buffers.
Reference: [12] <author> S. J. E. Wilton, and N. Jouppi, </author> <title> An Enhanced Access and Cycle Time Model for On-Chip Caches, </title> <note> Digital WRL Research Report 93/5, </note> <month> July </month> <year> 1994. </year>
Reference-contexts: To obtain the values for the equivalent capacitances, C eq , for the components in the memory subsystem, we follow the model given by Wilton and Jouppi <ref> [12] </ref>. Their model assumes a 0.8 m process; if a different process is used, only the transistor capacitances need to be recomputed. To obtain the number of transitions that occur on each transistor, we refer to Kamble and Ghose [6], adapting their work to our overall architecture. <p> In order to maximize speed we kept the arrays as square as possible by splitting the data and tag array vertically and/or horizontally. Sub-arrays can also be folded. We used the tool CACTI <ref> [12] </ref> to compute sub-arraying parameters for all our caches. According to [6] we consider the main sources of power to be the following three components:E bit , E word , E output .
Reference: [13] <author> Jeffrey Gee, Mark Hill, Dinoisions Pnevmatikatos, Alan J. Smith, </author> <title> Cache Performance of the SPEC Benchmark Suite, </title> <journal> IEEE Micro, </journal> <volume> Vol. 13, Number 4, </volume> <pages> pp. </pages> <month> 17-27 (August </month> <year> 1993) </year>
Reference-contexts: Note that we chose a 8K first level cache configuration in order to obtain a reasonable hit/miss rate from our benchmarks <ref> [13] </ref>. In Tables 2 and 3 note that some types of resource units (e.g., the FP Mult/Div/Sqrt unit) may have different latency and occupancy values depending on the type of operation being performed by the unit. Table 1: Machine configuration parameters.
References-found: 13

