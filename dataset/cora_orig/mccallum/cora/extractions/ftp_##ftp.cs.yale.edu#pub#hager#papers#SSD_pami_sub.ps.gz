URL: ftp://ftp.cs.yale.edu/pub/hager/papers/SSD_pami_sub.ps.gz
Refering-URL: http://www.cs.yale.edu/users/hager/papers.html
Root-URL: http://www.cs.yale.edu
Title: Efficient Region Tracking With Parametric Models of Geometry and Illumination  
Author: Gregory D. Hager Peter N. Belhumeur 
Keyword: Visual Tracking, Illumination, Motion Estimation, Robust Statistics.  
Address: P.O. Box 208285 New Haven, CT, 06520  P.O. Box 208267 New Haven, CT, 06520  
Affiliation: Department of Computer Science Yale Univ.,  Department of Electrical Engineering Yale Univ.,  
Note: Submitted to IEEE Trans. on PAMI  
Email: E-mail: hager@cs.yale.edu  E-mail: belhumeur@yale.edu  
Phone: Phone: (203) 432-6432, Fax: (203) 432-0593  Phone: (203) 432-4249, Fax: (203) 432-7481  
Abstract: As an object moves through the field of view of a camera, the images of the object may change dramatically. This is not simply due to the translation of the object across the image plane. Rather, complications arise due to the fact that the object undergoes changes in pose relative to viewing camera, changes in illumination relative to light sources, and may even be partially or fully occluded. Thus to successfully track an object, complications arising from varying pose, illumination, and partial occlusion must be accounted for. In this paper, we develop an efficient, general framework for object tracking one which addresses each of these complications. We first develop a computationally efficient method for handling the geometric distortions produced by changes in pose. We then combine geometry and illumination into an algorithm that tracks large image regions using no more computation than would be required to track with no accommodation for illumination changes. Finally, we augment these methods with techniques from robust statistics and treat occluded regions on the object as statistical outliers. Throughout, we present experimental results performed on live video sequences demonstrating the effectiveness and efficiency of our methods. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P.K. Allen, B. Yoshimi, and A. Timcenko. </author> <title> Hand-eye coordination for robotics tracking and grasping. </title> <editor> In K. Hashimoto, editor, </editor> <booktitle> Visual Servoing, </booktitle> <pages> pages 33-70. </pages> <publisher> World Scientific, </publisher> <year> 1994. </year>
Reference-contexts: 1 Introduction Visual tracking has emerged as an important component of systems in several application areas including vision-based control <ref> [1, 32, 38, 15] </ref>, human-computer interfaces [10, 14, 20], surveillance [30, 29, 19], agricultural automation [27, 41], medical imaging [12, 4, 45] and visual reconstruction [11, 42, 48].
Reference: [2] <author> P. Anandan. </author> <title> A computational framework and an algorithm for the measurement of structure from motion. </title> <journal> Int. J. Computer Vision, </journal> <volume> 2 </volume> <pages> 283-310, </pages> <year> 1989. </year>
Reference-contexts: The approach to matching described in this paper is based on comparing the so-called sum-of-squared differences (SSD) between two regions, an idea that has been explored in a variety of contexts including stereo matching [35], optical flow computation <ref> [2] </ref>, hand-eye coordination [38], and visual motion analysis [44].
Reference: [3] <author> N. Ayache and O.D. Faugeras. </author> <title> Maintaining representations of the environment of a mobile robot. </title> <journal> IEEE Trans. on Robotics and Automation, </journal> <volume> 5(6) </volume> <pages> 804-819, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: For example, Blake et al. [9] and Isaard and Blake [33] describe a variety of novel methods for incorporating both spatial and temporal constraints on feature evolution for snake-like contour tracking. Lowe [34] and Gennery [21] describe edge-based tracking methods using rigid three-dimensional geometric models. Earlier work by Ayache <ref> [3] </ref> and Crowley [13] use incrementally constructed rigid models to constrain image matching. In practice, feature-based and region-based methods can be viewed as complementary techniques.
Reference: [4] <author> E. Bardinet, L. Cohen, and N. Ayache. </author> <title> Tracking medical 3D data with a deformable parametric model. </title> <booktitle> In Proc. European Conf. on Computer Vision, </booktitle> <pages> pages I:317-328, </pages> <year> 1996. </year>
Reference-contexts: 1 Introduction Visual tracking has emerged as an important component of systems in several application areas including vision-based control [1, 32, 38, 15], human-computer interfaces [10, 14, 20], surveillance [30, 29, 19], agricultural automation [27, 41], medical imaging <ref> [12, 4, 45] </ref> and visual reconstruction [11, 42, 48]. The central challenge in visual tracking is to determine the image position of a target region (or features) of an object as it moves through a camera's field of view.
Reference: [5] <author> P. N. Belhumeur and D. J. Kriegman. </author> <title> What is the set of images of an object under all possible lighting conditions. </title> <booktitle> In Proc. IEEE Conf. on Comp. Vision and Patt. </booktitle> <address> Recog., </address> <year> 1996. </year> <note> In press. </note>
Reference-contexts: This permits us to develop low-order parametric models for the image motion of points within a target region|models that can be used to predict the movement of the points and track the target through an image sequence. In the case of illumination, we exploit the observations of <ref> [25, 17, 5] </ref> to model image variation due to changing illumination by low-dimensional linear subspaces. The motion and illumination models are then woven together in an efficient algorithm which establishes temporal correspondence of the target region by simultaneously determining motion and illumination parameters. <p> Thus, shadowing or shading changes of the target object over time lead 14 to bias, or, in the worst case, complete loss of the target. Recently, it has been shown that a relatively small number of "basis" images can often be used to account for large changes in illumination <ref> [5, 17, 22, 24, 43] </ref>. Briefly, the reason for this is as follows. <p> A complication comes when handling shadowing: all images are no longer guaranteed to lie in a linear subspace <ref> [5] </ref>. Nevertheless, as done in [24], we can still use a linear model as an approximation: a small set of basis images can account for much of the shading changes that occur on patches of non-specular surfaces.
Reference: [6] <author> M. Betke and N. Makris. </author> <title> Fast object recognition in noisy images using simulated annealing. </title> <booktitle> In Proceedings of the ICCV, </booktitle> <pages> pages 523-530, </pages> <year> 1995. </year>
Reference-contexts: Thus, in the absence of a good starting point, this problem will usually require some type of costly global optimization procedure to solve <ref> [6] </ref>. In the case of visual tracking, the continuity of motion provides such a starting point.
Reference: [7] <author> M.J. Black and A.D. Jepson. Eigentracking: </author> <title> Robust matching and tracking of articulated objects using a view-based representation. </title> <booktitle> In Proc. European Conf. on Computer Vision, </booktitle> <year> 1996. </year>
Reference-contexts: More recently, Black and Yacoob [8] describe an algorithm for recognizing facial expressions using motion models which include both affine and simple polynomial deformations of the face and its features. Black and Jepson <ref> [7] </ref> develop a robust algorithm for tracking a target undergoing changes in pose or appearance by combining a simple parametric motion model with an image subspace method [37]. <p> as a function on ffi O (ffi) = kI ((t) + ffi; t + t ) I (0; t 0 )k 2 : (7) If the magnitude of the components of ffi are small, then it is possible to apply continuous optimization procedures to a linearized version of the problem <ref> [7, 28, 35, 47, 44] </ref>. <p> A common approach to this problem is to assume that occlusions create large image differences which can be viewed as "outliers" by the estimation process <ref> [7] </ref>. The error metric is then modified to reduce sensitivity to "outliers" by solving a robust optimization problem of the form O R () = x2R where is one of a variety of "robust" regression metrics [31]. <p> Affine warping augmented with brightness and contrast compensation is the best possible linear approximation to this case (it is exact for an orthographic camera model and purely Lambertian surface). As a point of comparison, recent work by Black and Jepson <ref> [7] </ref> used the rigid motion plus scaling model for SSD-based region tracking. Their reduced model is more efficient and may be more stable since fewer parameters must be computed, but it does ignore the effects of changing aspect ratio and shear. <p> In fact, the linear form of the solution makes it straightforward to incorporate the estimation algorithm into a Kalman filter or similar iterative estimation procedure. Performance can also be improved by operating the tracking algorithm at multiple levels of resolution. One possibility, as is used by many authors <ref> [7, 44] </ref>, is to perform a complete coarse to fine progression of estimation steps on each image in the sequence. Another possibility, which we have used successfully in prior work [23], is to dynamically adapt resolution based on the motion of the target. <p> One area that still needs attention is the problem of determining an illumination basis online, i.e. while tracking the object. Initial experiments in this direction have shown that online determination of the illumination basis can be achieved, although we have not included such results in this paper. As in <ref> [7] </ref>, we are also exploring the use of basis images to handle changes of view or aspect not well addressed by warping. We are also looking at the problem of extending the method to utilize shape information on the target when such information is available.
Reference: [8] <author> M.J. Black and Y. Yacoob. </author> <title> Tracking and recognizing rigid and non-rigid facial motions using local parametric models of image motion. </title> <booktitle> In Proceedings of the ICCV, </booktitle> <pages> pages 374-381, </pages> <year> 1995. </year>
Reference-contexts: For example, Rehg and Witkin [40] describe energy-based algorithms for tracking deforming image regions, and Rehg and Kanade [39] consider articulated objects undergoing self-occlusion. More recently, Black and Yacoob <ref> [8] </ref> describe an algorithm for recognizing facial expressions using motion models which include both affine and simple polynomial deformations of the face and its features. <p> of the form f (x; u; v; a) = x + u # where x = (x; y) T : Intuitively, this model performs a quadratic distortion of the image according to the equation y = 1=2ax 2 : For example, a polynomial model of this form was used in <ref> [8] </ref> to model the motions of lips and eyebrows on a face. <p> Here, the difference between the two geometric models is clearly evident. 5.3 Human Face Tracking There has been a great deal of recent interest in face tracking in the computer vision literature <ref> [8, 14, 36] </ref>. Although faces can produce images with significant variation due to illumination, empirical results suggest that a small number of basis images of a face gathered under different illuminations is sufficient to accurately account for most gross shading and illumination effects [24].
Reference: [9] <author> A. Blake, R. Curwen, and A. Zisserman. </author> <title> A framework for spatio-temporal control in the tracking of visual contour. </title> <journal> Int. J. Computer Vision, </journal> <volume> 11(2) </volume> <pages> 127-145, </pages> <year> 1993. </year>
Reference-contexts: To date, most tracking algorithms achieving frame-rate performance track only a sparse collection of features (or contours). For example, Blake et al. <ref> [9] </ref> and Isaard and Blake [33] describe a variety of novel methods for incorporating both spatial and temporal constraints on feature evolution for snake-like contour tracking. Lowe [34] and Gennery [21] describe edge-based tracking methods using rigid three-dimensional geometric models.
Reference: [10] <author> A. F. Bobick and A. D. Wilson. </author> <title> A state-based technique for the summarization of recognition of gesture. </title> <booktitle> In Proceedings of the ICCV, </booktitle> <pages> pages 382-388, </pages> <year> 1995. </year>
Reference-contexts: 1 Introduction Visual tracking has emerged as an important component of systems in several application areas including vision-based control [1, 32, 38, 15], human-computer interfaces <ref> [10, 14, 20] </ref>, surveillance [30, 29, 19], agricultural automation [27, 41], medical imaging [12, 4, 45] and visual reconstruction [11, 42, 48].
Reference: [11] <author> E. Boyer. </author> <title> Object models from contour sequences. </title> <booktitle> In Proc. European Conf. on Computer Vision, </booktitle> <pages> pages II:109-118, </pages> <year> 1996. </year>
Reference-contexts: 1 Introduction Visual tracking has emerged as an important component of systems in several application areas including vision-based control [1, 32, 38, 15], human-computer interfaces [10, 14, 20], surveillance [30, 29, 19], agricultural automation [27, 41], medical imaging [12, 4, 45] and visual reconstruction <ref> [11, 42, 48] </ref>. The central challenge in visual tracking is to determine the image position of a target region (or features) of an object as it moves through a camera's field of view.
Reference: [12] <author> J.J. Crisco, K. Hentel, S.W. Wolfe, and J.S. Duncan. </author> <title> Two-dimensional rigid-body kinematics using image registration. </title> <journal> J. </journal> <note> Biomechanics, 1994. In press. </note>
Reference-contexts: 1 Introduction Visual tracking has emerged as an important component of systems in several application areas including vision-based control [1, 32, 38, 15], human-computer interfaces [10, 14, 20], surveillance [30, 29, 19], agricultural automation [27, 41], medical imaging <ref> [12, 4, 45] </ref> and visual reconstruction [11, 42, 48]. The central challenge in visual tracking is to determine the image position of a target region (or features) of an object as it moves through a camera's field of view.
Reference: [13] <author> J. L. Crowley, P. Stelmaszyk, T. Skordas, and P. Puget. </author> <title> Measurement and integration of 3-D structures by tracking edge lines. </title> <journal> Int'l Journal of Computer Vision, </journal> <volume> 8(1) </volume> <pages> 29-52, </pages> <year> 1992. </year> <month> 31 </month>
Reference-contexts: Lowe [34] and Gennery [21] describe edge-based tracking methods using rigid three-dimensional geometric models. Earlier work by Ayache [3] and Crowley <ref> [13] </ref> use incrementally constructed rigid models to constrain image matching. In practice, feature-based and region-based methods can be viewed as complementary techniques.
Reference: [14] <author> T. Darrell, B. Moghaddam, </author> <title> and A.P. Pentland. Active face tracking and pose estimation in an interactive room. </title> <booktitle> In Proc. IEEE Conf. Comp. Vision and Patt. Recog., </booktitle> <pages> pages 67-72, </pages> <year> 1996. </year>
Reference-contexts: 1 Introduction Visual tracking has emerged as an important component of systems in several application areas including vision-based control [1, 32, 38, 15], human-computer interfaces <ref> [10, 14, 20] </ref>, surveillance [30, 29, 19], agricultural automation [27, 41], medical imaging [12, 4, 45] and visual reconstruction [11, 42, 48]. <p> Here, the difference between the two geometric models is clearly evident. 5.3 Human Face Tracking There has been a great deal of recent interest in face tracking in the computer vision literature <ref> [8, 14, 36] </ref>. Although faces can produce images with significant variation due to illumination, empirical results suggest that a small number of basis images of a face gathered under different illuminations is sufficient to accurately account for most gross shading and illumination effects [24].
Reference: [15] <author> E.D. Dickmanns and V. Graefe. </author> <title> Dynamic monocular machine vision. </title> <journal> Machine Vision and Applications, </journal> <volume> 1 </volume> <pages> 223-240, </pages> <year> 1988. </year>
Reference-contexts: 1 Introduction Visual tracking has emerged as an important component of systems in several application areas including vision-based control <ref> [1, 32, 38, 15] </ref>, human-computer interfaces [10, 14, 20], surveillance [30, 29, 19], agricultural automation [27, 41], medical imaging [12, 4, 45] and visual reconstruction [11, 42, 48].
Reference: [16] <author> R. Dutter and P.J. Huber. </author> <title> Numerical methods for the nonlinear robust regression problem. </title> <journal> J. Statist. Comput. Simulation, </journal> <volume> 13(2) </volume> <pages> 79-113, </pages> <year> 1981. </year>
Reference-contexts: These parameters not only shift and deform image coordinates, but also adjust brightness values within the target region to provide the best match to a fixed reference image. Finally, in the case of partial 1 occlusion, we apply results from robust statistics <ref> [16] </ref> to show that this matching algorithm is easily extended to include automatic rejection of outlier pixels in a computationally efficient manner. <p> It is well-known that optimization of (47) is closely related to another approach to robust estimation|iteratively reweighted least squares (IRLS). We have chosen to implement the 17 optimization using a somewhat unusual form of IRLS due to Dutter and Huber <ref> [16] </ref>. In order to formulate the algorithm, we introduce the notation of an "inner iteration" which is performed one or more times at each time step. We will use a superscript to denote this iteration. <p> In addition, the error vector e is fixed over all inner iterations, so these iterations do not require the additional overhead of acquiring and warping images. As discussed in <ref> [16] </ref>, on linear problems this procedure is guaranteed to converge to a unique global minimum for a large variety of choices of : In this article, is taken to be a so-called "windsorizing" function [31] which is of the form (r) = r 2 =2 if jrj t cjrj c 2
Reference: [17] <author> R. Epstein, P. Hallinan, and A.L. Yuille. </author> <title> 5 2 Eigenimages suffice: An empirical investigation of low-dimensional lighting models. </title> <type> Technical Report 94-11, </type> <institution> Harvard University, </institution> <year> 1994. </year>
Reference-contexts: This permits us to develop low-order parametric models for the image motion of points within a target region|models that can be used to predict the movement of the points and track the target through an image sequence. In the case of illumination, we exploit the observations of <ref> [25, 17, 5] </ref> to model image variation due to changing illumination by low-dimensional linear subspaces. The motion and illumination models are then woven together in an efficient algorithm which establishes temporal correspondence of the target region by simultaneously determining motion and illumination parameters. <p> Thus, shadowing or shading changes of the target object over time lead 14 to bias, or, in the worst case, complete loss of the target. Recently, it has been shown that a relatively small number of "basis" images can often be used to account for large changes in illumination <ref> [5, 17, 22, 24, 43] </ref>. Briefly, the reason for this is as follows.
Reference: [18] <author> J.D. Foley, A. van Dam, S.K. Feiner, and J.F. Hughes. </author> <title> Computer Graphics. </title> <publisher> Addison Wesley, </publisher> <year> 1993. </year>
Reference-contexts: This factoring allows image warping to be implemented in two stages. In the first stage, an image region surrounding the target is acquired and rotated using a variant on standard Bresenham line-drawing algorithms <ref> [18] </ref>. The acquired image is then scaled and sheared using a bilinear interpolation. The resolution of the region is then reduced by averaging neighboring pixels. Spatial and temporal derivatives are computed by applying Prewitt operators on the reduced scale images.
Reference: [19] <author> T. Frank, M. Haag, H. Kollnig, and H.-H. Nagel. </author> <title> Tracking of occluded vehicles in traffic scenes. </title> <booktitle> In Proc. European Conf. on Computer Vision, </booktitle> <pages> pages II:485-494, </pages> <year> 1996. </year>
Reference-contexts: 1 Introduction Visual tracking has emerged as an important component of systems in several application areas including vision-based control [1, 32, 38, 15], human-computer interfaces [10, 14, 20], surveillance <ref> [30, 29, 19] </ref>, agricultural automation [27, 41], medical imaging [12, 4, 45] and visual reconstruction [11, 42, 48]. The central challenge in visual tracking is to determine the image position of a target region (or features) of an object as it moves through a camera's field of view.
Reference: [20] <author> D.M. Gavrila and L.S. Davis. </author> <title> Tracking humans in action: A 3D model-based approach. </title> <booktitle> In Proc. Image Understanding Workshop, </booktitle> <pages> pages 737-746, </pages> <year> 1996. </year>
Reference-contexts: 1 Introduction Visual tracking has emerged as an important component of systems in several application areas including vision-based control [1, 32, 38, 15], human-computer interfaces <ref> [10, 14, 20] </ref>, surveillance [30, 29, 19], agricultural automation [27, 41], medical imaging [12, 4, 45] and visual reconstruction [11, 42, 48].
Reference: [21] <author> D. B. Gennery. </author> <title> Visual tracking of known three-dimensional objects. </title> <journal> Int'l Journal of Computer Vision, </journal> <volume> 7(3) </volume> <pages> 243-270, </pages> <year> 1992. </year>
Reference-contexts: For example, Blake et al. [9] and Isaard and Blake [33] describe a variety of novel methods for incorporating both spatial and temporal constraints on feature evolution for snake-like contour tracking. Lowe [34] and Gennery <ref> [21] </ref> describe edge-based tracking methods using rigid three-dimensional geometric models. Earlier work by Ayache [3] and Crowley [13] use incrementally constructed rigid models to constrain image matching. In practice, feature-based and region-based methods can be viewed as complementary techniques.
Reference: [22] <author> G. D. Hager and P.N. Belhumeur. </author> <title> Real-time tracking of image regions with changes in geometry and illumination. </title> <booktitle> In Proc. IEEE Conf. Comp. Vision and Patt. Recog., </booktitle> <pages> pages 403-410. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1996. </year>
Reference-contexts: Thus, shadowing or shading changes of the target object over time lead 14 to bias, or, in the worst case, complete loss of the target. Recently, it has been shown that a relatively small number of "basis" images can often be used to account for large changes in illumination <ref> [5, 17, 22, 24, 43] </ref>. Briefly, the reason for this is as follows.
Reference: [23] <author> G. D. Hager and K. Toyama. XVision: </author> <title> A portable substrate for real-time vision applications. </title> <booktitle> Computer Vision and Image Understanding, </booktitle> <year> 1996. </year> <note> In Press. </note>
Reference-contexts: All experiments were performed on live video sequences by an SGI Indy equipped with a 175Mhz R4400 SC processor and VINO image acquisition system. 19 X Y Rotation Scale Aspect Ratio Shear 5.1 Implementation We have implemented the methods described above within the X Vision environment <ref> [23] </ref>. The implemented system incorporates all of the linear motion models described in Section 2, non-orthonormal illumination bases as described in Section 3, and outlier rejection using the algorithm described in Section 4. <p> The resolution of the region is then reduced by averaging neighboring pixels. Spatial and temporal derivatives are computed by applying Prewitt operators on the reduced scale images. More details on this level of the implementation can be found in <ref> [23] </ref>. Timings of the algorithm 2 indicate that it can perform frame rate (30 Hz) tracking of image regions of up to 100 fi 100 pixels at one-half resolution undergoing affine distortions and illumination changes. Similar performance has been achieved on a 120Mhz Pentium processor and 70 Mhz Sun SparcStation. <p> One possibility, as is used by many authors [7, 44], is to perform a complete coarse to fine progression of estimation steps on each image in the sequence. Another possibility, which we have used successfully in prior work <ref> [23] </ref>, is to dynamically adapt resolution based on the motion of the target. That is, when the target moves quickly estimation is performed at a coarse resolution, and when it moves slowly the algorithm changes to a higher resolution.
Reference: [24] <author> Peter Hallinan. </author> <title> A low-dimensional representation of human faces for arbitrary lighting conditions. </title> <booktitle> In Proc. IEEE Conf. on Comp. Vision and Patt. Recog., </booktitle> <pages> pages 995-999, </pages> <year> 1994. </year>
Reference-contexts: Thus, shadowing or shading changes of the target object over time lead 14 to bias, or, in the worst case, complete loss of the target. Recently, it has been shown that a relatively small number of "basis" images can often be used to account for large changes in illumination <ref> [5, 17, 22, 24, 43] </ref>. Briefly, the reason for this is as follows. <p> A complication comes when handling shadowing: all images are no longer guaranteed to lie in a linear subspace [5]. Nevertheless, as done in <ref> [24] </ref>, we can still use a linear model as an approximation: a small set of basis images can account for much of the shading changes that occur on patches of non-specular surfaces. <p> Although faces can produce images with significant variation due to illumination, empirical results suggest that a small number of basis images of a face gathered under different illuminations is sufficient to accurately account for most gross shading and illumination effects <ref> [24] </ref>. At the same time, the depth variations exhibited by facial features are small enough to be well-approximated by an affine warping model.
Reference: [25] <author> Peter Hallinan. </author> <title> A Deformable Model for Face Recognition Under Arbitrary Lighting Conditions. </title> <type> PhD thesis, </type> <institution> Harvard University, </institution> <year> 1995. </year>
Reference-contexts: This permits us to develop low-order parametric models for the image motion of points within a target region|models that can be used to predict the movement of the points and track the target through an image sequence. In the case of illumination, we exploit the observations of <ref> [25, 17, 5] </ref> to model image variation due to changing illumination by low-dimensional linear subspaces. The motion and illumination models are then woven together in an efficient algorithm which establishes temporal correspondence of the target region by simultaneously determining motion and illumination parameters.
Reference: [26] <author> R. M. Haralick and L. G. Shapiro. </author> <title> Computer and Robot Vision. </title> <publisher> Addison Wesley, </publisher> <year> 1993. </year>
Reference-contexts: Let Q (x) to be the pixel values in the eight-neighborhood of the image coordinate x plus the value at x itself. We use two common morphological operators <ref> [26] </ref> close (x) = max v2Q (x) and open (x) = min v2Q (x) When applied to a weighting matrix image, close has the effect of removing small areas of outlier pixels, while open increases their size.
Reference: [27] <author> R. C. Harrell, D. C. Slaughter, and P. D. Adsit. </author> <title> A fruit-tracking system for robotic harvesting. </title> <journal> Machine Vision and Applications, </journal> <volume> 2 </volume> <pages> 69-80, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction Visual tracking has emerged as an important component of systems in several application areas including vision-based control [1, 32, 38, 15], human-computer interfaces [10, 14, 20], surveillance [30, 29, 19], agricultural automation <ref> [27, 41] </ref>, medical imaging [12, 4, 45] and visual reconstruction [11, 42, 48]. The central challenge in visual tracking is to determine the image position of a target region (or features) of an object as it moves through a camera's field of view.
Reference: [28] <author> B.K.P. Horn. </author> <title> Computer Vision. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1986. </year>
Reference-contexts: It follows that for any time t &gt; t 0 there is a parameter vector fl (t) such that I (x; t 0 ) = I (f (x; fl (t)); t) for all x 2 R: (1) This a generalization of the so-called image constancy assumption <ref> [28] </ref>. The motion parameter vector of the target region can be estimated at time t by minimizing the following least squares objective function O () = x2R For later developments, it is convenient to rewrite this optimization problem in vector notation. <p> as a function on ffi O (ffi) = kI ((t) + ffi; t + t ) I (0; t 0 )k 2 : (7) If the magnitude of the components of ffi are small, then it is possible to apply continuous optimization procedures to a linearized version of the problem <ref> [7, 28, 35, 47, 44] </ref>. <p> Due to the image constancy assumption (1), it follows that _ I = 0 when = fl : This is, of course, a parameterized version of Horn's optical flow constraint equation <ref> [28] </ref>. In this form, it is clear that the role of M is to relate variations in motion parameters to variations in brightness values in the target region. The solution given in (13) effectively reverses this relationship and provides a method for interpreting observed changes in brightness as motion. <p> This is not surprising, as the incremental estimation step is effectively computing a structured optical flow, and optical flow methods are well-known to be sensitive to illumination changes <ref> [28] </ref>. Thus, shadowing or shading changes of the target object over time lead 14 to bias, or, in the worst case, complete loss of the target. <p> The irradiance at the point p is given by E = a n s (41) where n is the unit inwards normal vector to the surface at p and a is the non-negative absorption coefficient (albedo) of the surface at the point p <ref> [28] </ref>. This shows that the irradiance at the point p, and hence the gray level seen by a camera, is linear on s 2 IR 3 .
Reference: [29] <author> R. Howarth and H. Buxton. </author> <title> Visual surveillance monitoring and watching. </title> <booktitle> In Proc. European Conf. on Computer Vision, </booktitle> <pages> pages II:321-334, </pages> <year> 1996. </year>
Reference-contexts: 1 Introduction Visual tracking has emerged as an important component of systems in several application areas including vision-based control [1, 32, 38, 15], human-computer interfaces [10, 14, 20], surveillance <ref> [30, 29, 19] </ref>, agricultural automation [27, 41], medical imaging [12, 4, 45] and visual reconstruction [11, 42, 48]. The central challenge in visual tracking is to determine the image position of a target region (or features) of an object as it moves through a camera's field of view.
Reference: [30] <author> Eric Huber and David Kortenkamp. </author> <title> Using stereo vision to pursue moving agents with a mobile robot. </title> <booktitle> In Proc. 1995 IEEE Conf. on Rob. and Autom., </booktitle> <pages> pages 2340-2346, </pages> <address> Nagoya, Japan, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Visual tracking has emerged as an important component of systems in several application areas including vision-based control [1, 32, 38, 15], human-computer interfaces [10, 14, 20], surveillance <ref> [30, 29, 19] </ref>, agricultural automation [27, 41], medical imaging [12, 4, 45] and visual reconstruction [11, 42, 48]. The central challenge in visual tracking is to determine the image position of a target region (or features) of an object as it moves through a camera's field of view.
Reference: [31] <author> P.J. Huber. </author> <title> Robust Statistics. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, NY, </address> <year> 1981. </year>
Reference-contexts: The error metric is then modified to reduce sensitivity to "outliers" by solving a robust optimization problem of the form O R () = x2R where is one of a variety of "robust" regression metrics <ref> [31] </ref>. It is well-known that optimization of (47) is closely related to another approach to robust estimation|iteratively reweighted least squares (IRLS). We have chosen to implement the 17 optimization using a somewhat unusual form of IRLS due to Dutter and Huber [16]. <p> As discussed in [16], on linear problems this procedure is guaranteed to converge to a unique global minimum for a large variety of choices of : In this article, is taken to be a so-called "windsorizing" function <ref> [31] </ref> which is of the form (r) = r 2 =2 if jrj t cjrj c 2 =2 if jrj &gt; t (51) where r is normalized to have unit variance.
Reference: [32] <author> S. Hutchinson, G.D. Hager, and P. Corke. </author> <title> A tutorial introduction to visual servo control. </title> <journal> IEEE Trans. on Robotics and Automation, </journal> <volume> 12(5), </volume> <year> 1996. </year>
Reference-contexts: 1 Introduction Visual tracking has emerged as an important component of systems in several application areas including vision-based control <ref> [1, 32, 38, 15] </ref>, human-computer interfaces [10, 14, 20], surveillance [30, 29, 19], agricultural automation [27, 41], medical imaging [12, 4, 45] and visual reconstruction [11, 42, 48].
Reference: [33] <author> M. Isard and A. Blake. </author> <title> Contour tracking by stochastic propagation of conditional density. </title> <booktitle> In ECCV96, </booktitle> <pages> pages I:343-356, </pages> <year> 1996. </year>
Reference-contexts: To date, most tracking algorithms achieving frame-rate performance track only a sparse collection of features (or contours). For example, Blake et al. [9] and Isaard and Blake <ref> [33] </ref> describe a variety of novel methods for incorporating both spatial and temporal constraints on feature evolution for snake-like contour tracking. Lowe [34] and Gennery [21] describe edge-based tracking methods using rigid three-dimensional geometric models. <p> However, in less structured situations strong edges are often sparsely distributed in an image, and are difficult to detect and match robustly without a strong predictive model <ref> [33] </ref>. In such cases, the fact that region-based methods make direct and complete use of all available image intensity information eliminates the need to identify and model a special set of features to track.
Reference: [34] <author> D. G. Lowe. </author> <title> Robust model-based motion tracking through the integration of search and estimation. </title> <journal> Int'l Journal of Computer Vision, </journal> <volume> 8(2) </volume> <pages> 113-122, </pages> <year> 1992. </year>
Reference-contexts: To date, most tracking algorithms achieving frame-rate performance track only a sparse collection of features (or contours). For example, Blake et al. [9] and Isaard and Blake [33] describe a variety of novel methods for incorporating both spatial and temporal constraints on feature evolution for snake-like contour tracking. Lowe <ref> [34] </ref> and Gennery [21] describe edge-based tracking methods using rigid three-dimensional geometric models. Earlier work by Ayache [3] and Crowley [13] use incrementally constructed rigid models to constrain image matching. In practice, feature-based and region-based methods can be viewed as complementary techniques.
Reference: [35] <author> B. D. Lucas and T. Kanade. </author> <title> An iterative image registration technique with an application to stereo vision. </title> <booktitle> In Proc. Int. Joint Conf. Artificial Intelligence, </booktitle> <pages> pages 674-679, </pages> <year> 1981. </year>
Reference-contexts: The approach to matching described in this paper is based on comparing the so-called sum-of-squared differences (SSD) between two regions, an idea that has been explored in a variety of contexts including stereo matching <ref> [35] </ref>, optical flow computation [2], hand-eye coordination [38], and visual motion analysis [44]. <p> as a function on ffi O (ffi) = kI ((t) + ffi; t + t ) I (0; t 0 )k 2 : (7) If the magnitude of the components of ffi are small, then it is possible to apply continuous optimization procedures to a linearized version of the problem <ref> [7, 28, 35, 47, 44] </ref>.
Reference: [36] <author> S. McKenna, S. Gong, and J.J. Collins. </author> <title> Face tracking and pose representation. </title> <booktitle> In British Maching Vision Conference, </booktitle> <year> 1996. </year>
Reference-contexts: Here, the difference between the two geometric models is clearly evident. 5.3 Human Face Tracking There has been a great deal of recent interest in face tracking in the computer vision literature <ref> [8, 14, 36] </ref>. Although faces can produce images with significant variation due to illumination, empirical results suggest that a small number of basis images of a face gathered under different illuminations is sufficient to accurately account for most gross shading and illumination effects [24].
Reference: [37] <author> H. Murase and S. Nayar. </author> <title> Visual learning and recognition of 3-D objects from appearence. </title> <journal> Int. J. Computer Vision, </journal> <pages> 14(5-24), </pages> <year> 1995. </year>
Reference-contexts: Black and Jepson [7] develop a robust algorithm for tracking a target undergoing changes in pose or appearance by combining a simple parametric motion model with an image subspace method <ref> [37] </ref>. These algorithms require from several seconds to several minutes per frame to compute, and most do not address the problems of changes in appearance due to illumination. In contrast, we develop a mathematical framework for the region tracking problem that naturally incorporates models for geometric distortions and varying illumination.
Reference: [38] <author> N. Papanikolopoulos, P. Khosla, and T. Kanade. </author> <title> Visual tracking of a moving target by a camera mounted on a robot: A combination of control and vision. </title> <journal> IEEE Trans. on Robotics and Automation, </journal> <volume> 9(1), </volume> <year> 1993. </year>
Reference-contexts: 1 Introduction Visual tracking has emerged as an important component of systems in several application areas including vision-based control <ref> [1, 32, 38, 15] </ref>, human-computer interfaces [10, 14, 20], surveillance [30, 29, 19], agricultural automation [27, 41], medical imaging [12, 4, 45] and visual reconstruction [11, 42, 48]. <p> The approach to matching described in this paper is based on comparing the so-called sum-of-squared differences (SSD) between two regions, an idea that has been explored in a variety of contexts including stereo matching [35], optical flow computation [2], hand-eye coordination <ref> [38] </ref>, and visual motion analysis [44]. <p> Much of the previous work using SSD matching for tracking has modeled the motion of the target region as pure translation in the image plane <ref> [48, 38] </ref>, which implicitly assumes that the underlying object is translating parallel to the image plane and is being viewed orthographically. For inter-frame calculations such as those required for optical flow or motion analysis, pure translation is typically adequate.
Reference: [39] <author> J.M. Rehg and T. Kanade. </author> <title> Visual tracking of high DOF articulated structures: An application to human hand tracking. </title> <booktitle> In Computer Vision - ECCV '94, </booktitle> <volume> volume B, </volume> <pages> pages 35-46, </pages> <year> 1994. </year>
Reference-contexts: Attempts have been made to include more elaborate models for image change in region tracking algorithms, but with sizable increases in the computational effort required to establish correspondence. For example, Rehg and Witkin [40] describe energy-based algorithms for tracking deforming image regions, and Rehg and Kanade <ref> [39] </ref> consider articulated objects undergoing self-occlusion. More recently, Black and Yacoob [8] describe an algorithm for recognizing facial expressions using motion models which include both affine and simple polynomial deformations of the face and its features.
Reference: [40] <author> J.M. Rehg and A.P. Witkin. </author> <title> Visual tracking with deformation models. </title> <booktitle> In Proc. IEEE Int. Conf. on Robotics and Automation, </booktitle> <pages> pages 844-850, </pages> <year> 1991. </year>
Reference-contexts: Attempts have been made to include more elaborate models for image change in region tracking algorithms, but with sizable increases in the computational effort required to establish correspondence. For example, Rehg and Witkin <ref> [40] </ref> describe energy-based algorithms for tracking deforming image regions, and Rehg and Kanade [39] consider articulated objects undergoing self-occlusion. More recently, Black and Yacoob [8] describe an algorithm for recognizing facial expressions using motion models which include both affine and simple polynomial deformations of the face and its features.
Reference: [41] <author> D. Reynard, A. Wildenberg, A. Blake, and J. Marchant. </author> <title> Learning dynamics of complex motions from image sequences. </title> <booktitle> In Proc. European Conf. on Computer Vision, </booktitle> <pages> pages I:357-368, </pages> <year> 1996. </year>
Reference-contexts: 1 Introduction Visual tracking has emerged as an important component of systems in several application areas including vision-based control [1, 32, 38, 15], human-computer interfaces [10, 14, 20], surveillance [30, 29, 19], agricultural automation <ref> [27, 41] </ref>, medical imaging [12, 4, 45] and visual reconstruction [11, 42, 48]. The central challenge in visual tracking is to determine the image position of a target region (or features) of an object as it moves through a camera's field of view.
Reference: [42] <author> L.S. Shapiro. </author> <title> Affine Analysis of Image Sequences. </title> <publisher> Cambridge Univ. Press, </publisher> <year> 1995. </year>
Reference-contexts: 1 Introduction Visual tracking has emerged as an important component of systems in several application areas including vision-based control [1, 32, 38, 15], human-computer interfaces [10, 14, 20], surveillance [30, 29, 19], agricultural automation [27, 41], medical imaging [12, 4, 45] and visual reconstruction <ref> [11, 42, 48] </ref>. The central challenge in visual tracking is to determine the image position of a target region (or features) of an object as it moves through a camera's field of view.
Reference: [43] <author> Amnon Shashua. </author> <title> Geometry and Photometry in 3D Visual Recognition. </title> <type> PhD thesis, </type> <institution> MIT, </institution> <year> 1992. </year>
Reference-contexts: Thus, shadowing or shading changes of the target object over time lead 14 to bias, or, in the worst case, complete loss of the target. Recently, it has been shown that a relatively small number of "basis" images can often be used to account for large changes in illumination <ref> [5, 17, 22, 24, 43] </ref>. Briefly, the reason for this is as follows. <p> Alternatively, one can reconstruct the image of the surface under a novel lighting direction by a linear combination of the three original images <ref> [43] </ref>. In other words, if the surface is purely Lambertian and there is no shadowing, then all images under varying illumination lie within a 3-D linear subspace of IR N , the space of all possible images (where N is the number of pixels in the images).
Reference: [44] <author> J. Shi and C. Tomasi. </author> <title> Good features to track. </title> <booktitle> In Proc. IEEE Conf. Comp. Vision and Patt. Recog., </booktitle> <pages> pages 593-600. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference-contexts: The approach to matching described in this paper is based on comparing the so-called sum-of-squared differences (SSD) between two regions, an idea that has been explored in a variety of contexts including stereo matching [35], optical flow computation [2], hand-eye coordination [38], and visual motion analysis <ref> [44] </ref>. Much of the previous work using SSD matching for tracking has modeled the motion of the target region as pure translation in the image plane [48, 38], which implicitly assumes that the underlying object is translating parallel to the image plane and is being viewed orthographically. <p> For inter-frame calculations such as those required for optical flow or motion analysis, pure translation is typically adequate. However, for tracking applications in which the correspondence for a finite size image patch must be computed over a long time span, the pure translation assumption is soon violated <ref> [44] </ref>. In such cases, both geometric image distortions such as rotation, scaling, shear, and illumination changes introduce significant changes in the appearance of the target region and, hence, must be accounted for in order to achieve reliable matching. <p> as a function on ffi O (ffi) = kI ((t) + ffi; t + t ) I (0; t 0 )k 2 : (7) If the magnitude of the components of ffi are small, then it is possible to apply continuous optimization procedures to a linearized version of the problem <ref> [7, 28, 35, 47, 44] </ref>. <p> In fact, the linear form of the solution makes it straightforward to incorporate the estimation algorithm into a Kalman filter or similar iterative estimation procedure. Performance can also be improved by operating the tracking algorithm at multiple levels of resolution. One possibility, as is used by many authors <ref> [7, 44] </ref>, is to perform a complete coarse to fine progression of estimation steps on each image in the sequence. Another possibility, which we have used successfully in prior work [23], is to dynamically adapt resolution based on the motion of the target.
Reference: [45] <author> P. Shi, G. Robinson, T. Constable, A. Sinusas, and J. Duncan. </author> <title> A model-based integrated approach to track myocardial deformation using displacement and velocity constraints. </title> <booktitle> In Proc. Internal Conf. on Computer Vision, </booktitle> <pages> pages 687-692, </pages> <year> 1995. </year>
Reference-contexts: 1 Introduction Visual tracking has emerged as an important component of systems in several application areas including vision-based control [1, 32, 38, 15], human-computer interfaces [10, 14, 20], surveillance [30, 29, 19], agricultural automation [27, 41], medical imaging <ref> [12, 4, 45] </ref> and visual reconstruction [11, 42, 48]. The central challenge in visual tracking is to determine the image position of a target region (or features) of an object as it moves through a camera's field of view.
Reference: [46] <author> W.M. Silver. </author> <title> Determining Shape and Reflectance Using Multiple Images. </title> <type> PhD thesis, </type> <institution> MIT, </institution> <address> Cambridge, MA, </address> <year> 1980. </year>
Reference-contexts: Therefore, in the absence of self-shadowing, given three images of a Lambertian surface from the same viewpoint taken under three known, linearly independent light source directions, the albedo and surface normal can be recovered; this is the well-known method of photometric stereo <ref> [50, 46] </ref>. Alternatively, one can reconstruct the image of the surface under a novel lighting direction by a linear combination of the three original images [43].
Reference: [47] <author> R. Szeliski. </author> <title> Image mosaicing for tele-reality applications. </title> <booktitle> In Proceedings of the Workshop on Applications of Computer Vision, </booktitle> <pages> pages 44-53, </pages> <year> 1994. </year> <month> 33 </month>
Reference-contexts: as a function on ffi O (ffi) = kI ((t) + ffi; t + t ) I (0; t 0 )k 2 : (7) If the magnitude of the components of ffi are small, then it is possible to apply continuous optimization procedures to a linearized version of the problem <ref> [7, 28, 35, 47, 44] </ref>.
Reference: [48] <author> C. Tomasi and T. Kanade. </author> <title> Shape and motion from image streams under orthography: A factorization method. </title> <journal> Int. J. Computer Vision, </journal> <volume> 9(2) </volume> <pages> 137-154, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction Visual tracking has emerged as an important component of systems in several application areas including vision-based control [1, 32, 38, 15], human-computer interfaces [10, 14, 20], surveillance [30, 29, 19], agricultural automation [27, 41], medical imaging [12, 4, 45] and visual reconstruction <ref> [11, 42, 48] </ref>. The central challenge in visual tracking is to determine the image position of a target region (or features) of an object as it moves through a camera's field of view. <p> Much of the previous work using SSD matching for tracking has modeled the motion of the target region as pure translation in the image plane <ref> [48, 38] </ref>, which implicitly assumes that the underlying object is translating parallel to the image plane and is being viewed orthographically. For inter-frame calculations such as those required for optical flow or motion analysis, pure translation is typically adequate.
Reference: [49] <author> S. Ullman and R. Basri. </author> <title> Recognition by a linear combination of models. </title> <journal> IEEE Trans. Pattern Anal. Mach. Intelligence, </journal> <volume> 13 </volume> <pages> 992-1006, </pages> <year> 1991. </year>
Reference-contexts: We are also looking at the problem of extending the method to utilize shape information on the target when such information is available. In particular, it is well known <ref> [49] </ref> that under orthographic projection, the image deformations of a surface due to motion can be described with a linear motion model. This suggests that our methods can be extended to handle such models.
Reference: [50] <author> R.J. Woodham. </author> <title> Analysing images of curved surfaces. </title> <journal> Artificial Intelligence, </journal> <volume> 17 </volume> <pages> 117-140, </pages> <year> 1981. </year> <month> 34 </month>
Reference-contexts: Therefore, in the absence of self-shadowing, given three images of a Lambertian surface from the same viewpoint taken under three known, linearly independent light source directions, the albedo and surface normal can be recovered; this is the well-known method of photometric stereo <ref> [50, 46] </ref>. Alternatively, one can reconstruct the image of the surface under a novel lighting direction by a linear combination of the three original images [43].
References-found: 50

