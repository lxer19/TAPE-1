URL: ftp://ftp.ai.mit.edu/pub/users/npcarter/thesis.ps.Z
Refering-URL: http://www.ai.mit.edu/people/npcarter/npcarter.html
Root-URL: 
Title: Improving the Performance of Cache Memories Without Increasing Size or Associativity  
Author: by Nicholas P. Carter Thomas Knight Daniel Prener Arthur C. Smith 
Degree: Submitted to the Department of Electrical Engineering and Computer Science in Partial Fulfillment of the Requirements for the degrees of Bachelor of Science and Master of Science  c Nicholas Carter, 1991 The author hereby grants to MIT permission to reproduce and to distribute copies of this thesis document in whole or in part. Author  Certified by  Thesis Supervisor Certified by  Thesis Supervisor Accepted by  Chair, Department Committee on Graduate Students  
Date: June 1991  May 10, 1991  
Affiliation: at the Massachusetts Institute of Technology  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Anant Agarwal. </author> <title> Analysis of Cache Performance for Operating Systems and Multiprogramming. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1989. </year>
Reference-contexts: Increasing the size and/or associativity of a cache has been shown to improve the hit rate of that cache, but in a decreasing manner [4] <ref> [1] </ref>. Each increase in size or associativity of a cache increases the hit rate of the system containing that cache, but each increase brings less improvement than the first.
Reference: [2] <author> Chi-Hung Chi and Henry Dietz. </author> <title> Improving cache performance by selective cache bypass. </title> <booktitle> In Proceedings of the 22nd Hawaii International Conference on System Science, </booktitle> <volume> volume 1, </volume> <pages> pages 277-85, </pages> <year> 1903. </year>
Reference-contexts: the cycle time of the machine could not be reduced by 17 doing so, as the time and chip space saved could be used to improve the system in other ways, or to lower its cost. 2.4 Other Ideas for Improving Cache Performance Chi-Hung Chi and Henry Dietz have studied <ref> [2] </ref> the idea of using compile-time analysis of the behavior of a program do determine if it is worthwhile to bring a given datum into the cache when it is referenced, bypassing the cache and returning the datum directly to the processor if it is not.
Reference: [3] <author> William R. Hardell, Jr., Dwain A. Hicks, Lawrence C. Howell, Jr., Warren E. Maule, Robert Montoye, and David P. Tuttle. </author> <title> Data cache and storage control units. </title> <booktitle> In IBM RISC System/6000 Technology, </booktitle> <pages> pages 44-50. </pages> <institution> IBM Corporation, </institution> <year> 1990. </year>
Reference-contexts: The data cache chips contain bypasses which allow the desired data to be returned to the processor on a cache miss before the entire line has been loaded into the cache <ref> [3] </ref>. This is an important feature, as the RISC System/6000's data cache lines take eight cycles to load after the initial memory latency of eight cycles has expired; bypassing the cache lines in this manner halves the wait time between a cache miss and the use of the required data. <p> These minor improvements consist mainly of making the data stored in the cache accessible while the cache is fetching data from the main store to satisfy a cache miss. This facility was included in the original design of the RISC System/6000's cache <ref> [3] </ref>, but was not implemented. The current RISC System/6000 is only able to access data in the line containing the datum which caused the cache miss while a miss is being handled.
Reference: [4] <author> Mark D. Hill and Alan Jay Smith. </author> <title> Evaluating associativity in cpu caches. </title> <journal> IEEE Transactions on Computing, </journal> <volume> 38(12) </volume> <pages> 1612-1630, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Increasing the size and/or associativity of a cache has been shown to improve the hit rate of that cache, but in a decreasing manner <ref> [4] </ref> [1]. Each increase in size or associativity of a cache increases the hit rate of the system containing that cache, but each increase brings less improvement than the first.
Reference: [5] <author> Gary A. Hoffman. </author> <title> Adaptive cache management. </title> <booktitle> In Proceedings of the 1987 IEEE International Conference on Computer Design: VLSI in Computers and Processors, </booktitle> <pages> pages 129-32, </pages> <year> 1987. </year>
Reference-contexts: They find that bypassing the cache can produce significant speed improvements, especially in the case where there is substantial conflict over one or more cache lines. Gary Hoffman has explored <ref> [5] </ref> the use of an adaptive cache, which would decide how much data should be brought into the cache on a cache miss based on the recent behavior of the cache.
Reference: [6] <author> Norman P. Jouppi. </author> <title> Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers. </title> <booktitle> In Proceedings of the 17th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 364-372, </pages> <year> 1990. </year>
Reference-contexts: Unfortunately, direct-mapped caches have significantly lower hit rates than equally large associative caches, due to conflicts between data for cache lines. A possible way of overcoming this disadvantage, which was proposed recently by Jouppi in <ref> [6] </ref>, is the victim cache. A victim cache is a small buffer, which Jouppi proposed would be made fully-associative, that is attached to the main cache, and into which all lines that are removed from the main cache to make room for new lines are placed. <p> However, this often does not increase the performance of a machine containing such a cache, as more time is spent bringing data into the cache that is not used than is saved by the increased hit rate. 16 2.3 Victim Cache In his paper <ref> [6] </ref>, Norman Jouppi suggested several methods for improving the performance of direct-mapped caches, focusing on the use of such caches in systems with very small cycle times. <p> use of load interruption, which leads to the conclusion that the load history table's explicit prediction of which data are not likely to be used by examining the past behavior of instructions is more effective than the implicit prediction done by load interruption. 5.5 Victim Cache As reported by Jouppi <ref> [6] </ref>, the addition of a victim cache to a direct-mapped cache is an effective way of improving the performance of a system containing such a cache. <p> However, in very high-performance machines, such as the ones described by Jouppi in his paper where he examines the victim cache <ref> [6] </ref>, the cycle time reduction allowed by the use of direct-mapped caches is enough that they are attractive.
Reference: [7] <author> Bowen Liu and Nelson Strother. </author> <title> Programming in VS Fortran on the IBM 3090 for maximum vector performance. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 65-76, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Few programs access more than this much data, and most of those that do, such as matrix manipulation programs, can often be modified so that their performance is not significantly degraded by being unable to fit all of their data into the cache at one time. In their paper <ref> [7] </ref>, Bowen Liu and Nelson Strother describe how this can be done for matrix-oriented algorithms.
Reference: [8] <author> Kevin O'Brien, Bill Hay, Joanne Minish, Hartmann Schaffer, Bob Schloss, Arvin Shepherd, and Matthew Zaleski. </author> <title> Advanced compiler technology for the RISC System/6000 architecture. </title> <booktitle> In IBM RISC System/6000 Technology, </booktitle> <pages> pages 154-161. </pages> <institution> IBM Corporation, </institution> <year> 1990. </year>
Reference-contexts: The RISC System/6000's compilers perform several optimizations that are specifically tailored to the configuration of the hardware, in addition to more traditional optimizations such as common subexpression elimination and register allocation <ref> [8] </ref>.
Reference: [9] <author> Steven Przybylski, Mark Horowitz, and John Hennessy. </author> <title> Performance tradeoffs in cache design. </title> <booktitle> In Proceedings of the 15th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 290-8, </pages> <year> 1988. </year> <month> 42 </month>
Reference-contexts: Many researchers attempt to simulate a multiprogramming environment by interleaving traces from several different programs, to recreate the effect that context switches have on the contents of the cache. Steven Przybylski, Mark Horowitz, and John Hennessy have done work <ref> [9] </ref> that quantifies the tradeoff between cache size, associativity, and clock speed in determining the performance of a computer.
Reference: [10] <author> Alan Jay Smith. </author> <title> Sequential program prefetching in memory hierarchies. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 7-21, </pages> <month> December </month> <year> 1978. </year> <month> 43 </month>
Reference-contexts: In his paper <ref> [10] </ref>, Alan J.
References-found: 10

