URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P723.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts98.htm
Root-URL: http://www.mcs.anl.gov
Email: luskg@mcs.anl.gov  
Title: Data Sieving and Collective I/O in ROMIO  
Author: Rajeev Thakur William Gropp Ewing Lusk fthakur, gropp, 
Date: August 1998  
Address: Argonne, IL 60439, USA  
Affiliation: Mathematics and Computer Science Division Argonne National Laboratory  
Pubnum: Preprint ANL/MCS-P723-0898  
Abstract: The I/O access patterns of parallel programs often consist of accesses to a large number of small, noncontiguous pieces of data. If an application's I/O needs are met by making many small, distinct I/O requests, however, the I/O performance degrades drastically. To avoid this problem, MPI-IO allows users to access a noncontiguous data set with a single I/O function call. This feature provides MPI-IO implementations an opportunity to optimize data access. We describe how our MPI-IO implementation, ROMIO, delivers high performance in the presence of noncontiguous requests. We explain in detail the two key optimizations ROMIO performs: data sieving for noncontiguous requests from one process and collective I/O for noncontiguous requests from multiple processes. We describe how one can implement these optimizations portably on multiple machines and file systems, control their memory requirements, and also achieve high performance. We demonstrate the performance and portability with performance results for three applications|an astrophysics-application template (DIST3D), the NAS BTIO benchmark, and an unstructured code (UNSTRUC)|on five different parallel machines: HP Exemplar, IBM SP, Intel Paragon, NEC SX-4, and SGI Origin2000. fl This work was supported by the Mathematical, Information, and Computational Sciences Division subprogram of the Office of Computational and Technology Research, U.S. Department of Energy, under Contract W-31-109-Eng-38; and by the Scalable I/O Initiative, a multiagency project funded by the Defense Advanced Research Projects Agency (contract number DABT63-94-C-0049), the Department of Energy, the National Aeronautics and Space Administration, and the National Science Foundation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Baylor and C. Wu. </author> <title> Parallel I/O Workload Characteristics Using Vesta. </title> <editor> In R. Jain, J. Werth, and J. Browne, editors, </editor> <booktitle> Input/Output in Parallel and Distributed Computer Systems, chapter 7, </booktitle> <pages> pages 167-185. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1996. </year>
Reference-contexts: 1 Introduction Numerous studies of the I/O characteristics of parallel applications have shown that many applications need to access a large number of small, noncontiguous pieces of data from a file <ref> [1, 3, 9, 11, 12, 16] </ref>. For good I/O performance, however, the size of an I/O request must be large (on the order of megabytes). The I/O performance suffers considerably, on the other hand, if applications access data by making many small I/O requests.
Reference: [2] <author> J. Bruno and P. Cappello. </author> <title> Implementing the Beam and Warming Method on the Hypercube. </title> <booktitle> In Proceedings of the Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <month> January </month> <year> 1988. </year>
Reference-contexts: The benchmark only performs writes, but we modified it to also perform reads, in order to measure the read bandwidths. In BTIO, a three-dimensional array (actually four-dimensional, but the first dimension has only five elements and is not distributed) is distributed among processes by using a multipartition distribution <ref> [2] </ref>. In this distribution, each process is responsible for several disjoint sub-blocks of points (cells) of the grid.
Reference: [3] <author> P. Crandall, R. Aydt, A. Chien, and D. Reed. </author> <title> Input-Output Characteristics of Scalable Parallel Applications. </title> <booktitle> In Proceedings of Supercomputing '95. </booktitle> <publisher> ACM Press, </publisher> <month> December </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Numerous studies of the I/O characteristics of parallel applications have shown that many applications need to access a large number of small, noncontiguous pieces of data from a file <ref> [1, 3, 9, 11, 12, 16] </ref>. For good I/O performance, however, the size of an I/O request must be large (on the order of megabytes). The I/O performance suffers considerably, on the other hand, if applications access data by making many small I/O requests.
Reference: [4] <author> J. del Rosario, R. Bordawekar, and A. Choudhary. </author> <title> Improved Parallel I/O via a Two-Phase Run-time Access Strategy. </title> <booktitle> In Proceedings of the Workshop on I/O in Parallel Computer Systems at IPPS '93, </booktitle> <pages> pages 56-70, </pages> <month> April </month> <year> 1993. </year> <note> Also published in Computer Architecture News, </note> <month> 21(5) </month> <pages> 31-38, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Collective I/O can be performed in different ways and has been studied by many researchers in recent years. It can be done at the disk level (disk-directed I/O [7]), at the server level (server-directed I/O [10]), or at the client level (two-phase I/O <ref> [4] </ref>). Each method has its merits and demerits. Since ROMIO is a portable, user-level library with no separate I/O servers, it performs collective I/O at the client level. <p> Since ROMIO is a portable, user-level library with no separate I/O servers, it performs collective I/O at the client level. For this purpose, it uses a generalized version of the extended two-phase method described in [13]. 4.1 Two-Phase I/O Two-phase I/O was first proposed in <ref> [4] </ref> in the context of accessing distributed arrays from files. Consider the example of reading a two-dimensional array from a file into a (block,block) distribution in memory, as shown in Figure 3. Assume that the array is stored in the file in row-major order. <p> The basic two-phase method was extended in [13] to access sections of out-of-core arrays. Since MPI-IO is a general parallel-I/O interface, I/O requests in MPI-IO can represent any access pattern, not just arrays. The two-phase method in <ref> [4] </ref> must therefore be generalized to handle any noncontiguous I/O request. Such a generalized implementation of two-phase I/O, explained below, is used in ROMIO. Two-phase I/O does increase the memory requirements of a program.
Reference: [5] <author> S. Fineberg, P. Wong, B. Nitzberg, and C. Kuszmaul. </author> <title> PMPIO|A Portable Implementation of MPI-IO. </title> <booktitle> In Proceedings of the Sixth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 188-195. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1996. </year>
Reference-contexts: The three applications we used are the following: 1. DIST3D, a template representing the I/O access pattern in an astrophysics application, ASTRO3D [16], from the University of Chicago; 2. the NAS BTIO benchmark <ref> [5] </ref>; and 3. an unstructured code (which we call UNSTRUC) written by Larry Schoof and Wilbur Johnson of Sandia National Laboratories. We note that ROMIO can perform the optimizations described in this paper only if users provide complete access information in a single function call. <p> It measures the 10 performance of reading/writing a three-dimensional array distributed in a (block,block,block) fashion among processes from/to a file containing the global array in row-major order. The second application is the BTIO benchmark <ref> [5] </ref> from NASA Ames Research Center, which simulates the I/O required by a time-stepping flow solver that periodically writes its solution matrix. The benchmark only performs writes, but we modified it to also perform reads, in order to measure the read bandwidths.
Reference: [6] <author> W. Gropp, E. Lusk, N. Doss, and A. Skjellum. </author> <title> A High-Performance, Portable Implementation of the MPI Message-Passing Interface Standard. </title> <journal> Parallel Computing, </journal> <volume> 22(6) </volume> <pages> 789-828, </pages> <month> September </month> <year> 1996. </year>
Reference-contexts: By following such an approach, we achieved portability with very low overhead [15]. Now that MPI-IO has emerged as the standard, we use ADIO as a mechanism for implementing MPI-IO, as illustrated in Figure 1. A similar abstract-device interface is used in MPICH <ref> [6] </ref> for implementing MPI portably. 3 Data Sieving To reduce the effect of high I/O latency, it is critical to make as few requests to the file system as possible.
Reference: [7] <author> D. Kotz. </author> <title> Disk-directed I/O for MIMD Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 15(1) </volume> <pages> 41-74, </pages> <month> February </month> <year> 1997. </year>
Reference-contexts: Collective I/O can be performed in different ways and has been studied by many researchers in recent years. It can be done at the disk level (disk-directed I/O <ref> [7] </ref>), at the server level (server-directed I/O [10]), or at the client level (two-phase I/O [4]). Each method has its merits and demerits. Since ROMIO is a portable, user-level library with no separate I/O servers, it performs collective I/O at the client level.
Reference: [8] <author> Message Passing Interface Forum. </author> <title> MPI-2: Extensions to the Message-Passing Interface. </title> <month> July </month> <year> 1997. </year> <note> On the World-Wide Web at http://www.mpi-forum.org/docs/docs.html. </note>
Reference-contexts: Such is the case when applications perform I/O by using the Unix read and write functions, which can access only a single contiguous chunk of data at a time. MPI-IO, the I/O part of the MPI-2 standard <ref> [8] </ref>, is a new interface designed specifically for portable, high-performance parallel I/O. To avoid the above-mentioned problem of many distinct, small I/O requests, MPI-IO allows users to specify the entire noncontiguous access pattern and read or write all the data with a single I/O function call. <p> The process is also assured that concurrent writes from processes not involved in this collective-I/O operation will not occur, because MPI-IO's consistency semantics <ref> [8] </ref> do not automatically guarantee consistency for such writes. (In such cases, users must use MPI File sync and ensure that the operations are not concurrent.) 4.2.3 Performance Issues Even if I/O is performed in large contiguous chunks, the performance of the collective-I/O implementation can be significantly affected by the amount
Reference: [9] <author> N. Nieuwejaar, D. Kotz, A. Purakayastha, C. Ellis, and M. </author> <title> Best. File-Access Characteristics of Parallel Scientific Workloads. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 7(10) </volume> <pages> 1075-1089, </pages> <month> October </month> <year> 1996. </year> <month> 16 </month>
Reference-contexts: 1 Introduction Numerous studies of the I/O characteristics of parallel applications have shown that many applications need to access a large number of small, noncontiguous pieces of data from a file <ref> [1, 3, 9, 11, 12, 16] </ref>. For good I/O performance, however, the size of an I/O request must be large (on the order of megabytes). The I/O performance suffers considerably, on the other hand, if applications access data by making many small I/O requests.
Reference: [10] <author> K. Seamons, Y. Chen, P. Jones, J. Jozwiak, and M. Winslett. </author> <title> Server-Directed Collective I/O in Panda. </title> <booktitle> In Proceedings of Supercomputing '95. </booktitle> <publisher> ACM Press, </publisher> <month> December </month> <year> 1995. </year>
Reference-contexts: Collective I/O can be performed in different ways and has been studied by many researchers in recent years. It can be done at the disk level (disk-directed I/O [7]), at the server level (server-directed I/O <ref> [10] </ref>), or at the client level (two-phase I/O [4]). Each method has its merits and demerits. Since ROMIO is a portable, user-level library with no separate I/O servers, it performs collective I/O at the client level.
Reference: [11] <author> E. Smirni, R. Aydt, A. Chien, and D. Reed. </author> <title> I/O Requirements of Scientific Applications: An Evolutionary View. </title> <booktitle> In Proceedings of the Fifth IEEE International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 49-59. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1996. </year>
Reference-contexts: 1 Introduction Numerous studies of the I/O characteristics of parallel applications have shown that many applications need to access a large number of small, noncontiguous pieces of data from a file <ref> [1, 3, 9, 11, 12, 16] </ref>. For good I/O performance, however, the size of an I/O request must be large (on the order of megabytes). The I/O performance suffers considerably, on the other hand, if applications access data by making many small I/O requests.
Reference: [12] <author> E. Smirni and D. Reed. </author> <title> Lessons from Characterizing the Input/Output Behavior of Parallel Scientific Applications. Performance Evaluation: </title> <journal> An International Journal, </journal> <volume> 33(1) </volume> <pages> 27-44, </pages> <month> June </month> <year> 1998. </year>
Reference-contexts: 1 Introduction Numerous studies of the I/O characteristics of parallel applications have shown that many applications need to access a large number of small, noncontiguous pieces of data from a file <ref> [1, 3, 9, 11, 12, 16] </ref>. For good I/O performance, however, the size of an I/O request must be large (on the order of megabytes). The I/O performance suffers considerably, on the other hand, if applications access data by making many small I/O requests.
Reference: [13] <author> R. Thakur and A. Choudhary. </author> <title> An Extended Two-Phase Method for Accessing Sections of Out-of-Core Arrays. </title> <journal> Scientific Programming, </journal> <volume> 5(4) </volume> <pages> 301-317, </pages> <month> Winter </month> <year> 1996. </year>
Reference-contexts: Each method has its merits and demerits. Since ROMIO is a portable, user-level library with no separate I/O servers, it performs collective I/O at the client level. For this purpose, it uses a generalized version of the extended two-phase method described in <ref> [13] </ref>. 4.1 Two-Phase I/O Two-phase I/O was first proposed in [4] in the context of accessing distributed arrays from files. Consider the example of reading a two-dimensional array from a file into a (block,block) distribution in memory, as shown in Figure 3. <p> The advantage of this method is that by making all file accesses large and contiguous, the I/O time is reduced significantly. The added cost of interprocess communication for redistribution is small compared with the savings in I/O time. The basic two-phase method was extended in <ref> [13] </ref> to access sections of out-of-core arrays. Since MPI-IO is a general parallel-I/O interface, I/O requests in MPI-IO can represent any access pattern, not just arrays. The two-phase method in [4] must therefore be generalized to handle any noncontiguous I/O request.
Reference: [14] <author> R. Thakur, A. Choudhary, R. Bordawekar, S. More, and S. Kuditipudi. </author> <title> Passion: Optimized I/O for Parallel Applications. </title> <journal> Computer, </journal> <volume> 29(6) </volume> <pages> 70-78, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: A similar abstract-device interface is used in MPICH [6] for implementing MPI portably. 3 Data Sieving To reduce the effect of high I/O latency, it is critical to make as few requests to the file system as possible. Data sieving <ref> [14] </ref> is a technique that enables an implementation to make a few large, contiguous requests to the file system even if the user's request consists of several small, noncontiguous accesses. read request for five noncontiguous pieces of data.
Reference: [15] <author> R. Thakur, W. Gropp, and E. Lusk. </author> <title> An Abstract-Device Interface for Implementing Portable Parallel-I/O Interfaces. </title> <booktitle> In Proceedings of the 6th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 180-187, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: ROMIO has been designed to be used with any MPI-1 implementation|both portable and vendor-specific implementations. It is currently included as part of two MPI implementations: MPICH and HP MPI. A key component of ROMIO that enables such a portable MPI-IO implementation is an internal layer called ADIO <ref> [15] </ref>. ADIO, an abstract-device interface for I/O, is a mechanism for implementing multiple parallel-I/O APIs (application programming interfaces) portably on multiple file systems. We developed ADIO before MPI-IO became a standard, as a means to implement and experiment with various parallel-I/O APIs that existed at the time. <p> We used ADIO to implement Intel's PFS API and subsets of IBM's PIOFS and the original MPI-IO proposal [18] on PFS, PIOFS, Unix, and NFS file systems. By following such an approach, we achieved portability with very low overhead <ref> [15] </ref>. Now that MPI-IO has emerged as the standard, we use ADIO as a mechanism for implementing MPI-IO, as illustrated in Figure 1. <p> Data sieving and collective I/O are implemented as ADIO functions <ref> [15] </ref>: Data sieving is used in the ADIO functions that read/write noncontiguous data, and collective I/O is used in ADIO's collective-I/O functions. Both these optimizations ultimately make contiguous I/O requests to the underlying file system, which are implemented by using ADIO's contiguous-I/O functions.
Reference: [16] <author> R. Thakur, W. Gropp, and E. Lusk. </author> <title> An Experimental Evaluation of the Parallel I/O Systems of the IBM SP and Intel Paragon Using a Production Application. </title> <booktitle> In Proceedings of the 3rd International Conference of the Austrian Center for Parallel Computation (ACPC) with Special Emphasis on Parallel Databases and Parallel I/O, </booktitle> <pages> pages 24-35. </pages> <booktitle> Lecture Notes in Computer Science 1127. </booktitle> <publisher> Springer-Verlag., </publisher> <month> September </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Numerous studies of the I/O characteristics of parallel applications have shown that many applications need to access a large number of small, noncontiguous pieces of data from a file <ref> [1, 3, 9, 11, 12, 16] </ref>. For good I/O performance, however, the size of an I/O request must be large (on the order of megabytes). The I/O performance suffers considerably, on the other hand, if applications access data by making many small I/O requests. <p> The three applications we used are the following: 1. DIST3D, a template representing the I/O access pattern in an astrophysics application, ASTRO3D <ref> [16] </ref>, from the University of Chicago; 2. the NAS BTIO benchmark [5]; and 3. an unstructured code (which we call UNSTRUC) written by Larry Schoof and Wilbur Johnson of Sandia National Laboratories. <p> We then present the performance results. 5.1 Applications The first application we used is DIST3D, a template representing the I/O access pattern in an astrophysics application, ASTRO3D <ref> [16] </ref>, from the University of Chicago. It measures the 10 performance of reading/writing a three-dimensional array distributed in a (block,block,block) fashion among processes from/to a file containing the global array in row-major order.
Reference: [17] <author> R. Thakur, W. Gropp, and E. Lusk. </author> <title> A Case for Using MPI's Derived Datatypes to Improve I/O Performance. </title> <booktitle> In Proceedings of SC98: High Performance Networking and Computing, </booktitle> <month> November </month> <year> 1998. </year> <note> To appear. </note>
Reference-contexts: We note that ROMIO can perform the optimizations described in this paper only if users provide complete access information in a single function call. In <ref> [17] </ref>, we explained how users can do so by using MPI's derived datatypes to create file views and by using the collective-I/O functions whenever possible. In this paper, we describe the optimizations in detail and provide extensive performance results. The rest of this paper is organized as follows.
Reference: [18] <author> The MPI-IO Committee. </author> <title> MPI-IO: A Parallel File I/O Interface for MPI, </title> <note> Version 0.5. World-Wide Web http://lovelace.nas.nasa.gov/MPI-IO, April 1996. 17 </note>
Reference-contexts: ADIO thus separates the machine-dependent and machine-independent aspects involved in implementing an API. We used ADIO to implement Intel's PFS API and subsets of IBM's PIOFS and the original MPI-IO proposal <ref> [18] </ref> on PFS, PIOFS, Unix, and NFS file systems. By following such an approach, we achieved portability with very low overhead [15]. Now that MPI-IO has emerged as the standard, we use ADIO as a mechanism for implementing MPI-IO, as illustrated in Figure 1.
References-found: 18

