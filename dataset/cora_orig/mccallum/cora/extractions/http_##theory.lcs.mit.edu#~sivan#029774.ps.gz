URL: http://theory.lcs.mit.edu/~sivan/029774.ps.gz
Refering-URL: http://theory.lcs.mit.edu/~sivan/papers.html
Root-URL: 
Title: LOCALITY OF REFERENCE IN LU DECOMPOSITION WITH PARTIAL PIVOTING  
Author: SIVAN TOLEDO 
Keyword: Key words. LU factorization, Gaussian elimination, partial pivoting, locality of reference, cache misses  
Note: AMS subject classifications. 15A23, 65F05, 65Y10, 65Y20  
Abstract: This paper presents a new partitioned algorithm for LU decomposition with partial pivoting. The new algorithm, called the recursively-partitioned algorithm, is based on a recursive partitioning of the matrix. The paper analyzes the locality of reference in the new algorithm and the locality of reference in a known and widely used partitioned algorithm for LU decomposition called the right-looking algorithm. The analysis reveals that the new algorithm performs a factor of fi( M=n) fewer I/O operations (or cache misses) than the right-looking algorithm, where n is the order of the matrix and M is the size of primary memory. The analysis also determines the optimal block size for the right-looking algorithm. Experimental comparisons between the new algorithm and the right-looking algorithm show that an implementation of the new algorithm outperforms a similarly coded right-looking algorithm on 6 different RISC architectures, that the new algorithm performs fewer cache misses than any other algorithm tested, and that it benefits more from Strassen's matrix-multiplication algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. D. Croz, A. Green baum, S. Hammarling, A. McKenney, S. Ostrouchov, and D. Sorensen, </author> <title> LAPACK User's Guide, </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <note> 2nd ed., 1994. Also available online from http://www.netlib.org. </note>
Reference-contexts: This paper describes a new partitioned algorithm for LU-factorization with partial pivoting, called the recursively-partitioned algorithm. The paper also analyzes the number of data transfers in a popular partitioned LU-factorization algorithm, the so-called right-looking algorithm, which is used in LAPACK <ref> [1] </ref>. The performance characteristics of other popular partitioned LU-factorization algorithms, in particular Crout and the left-looking algorithm used in the NAG library [4], are similar to those of the right-looking algorithm so they are not analyzed. The analysis of the two algorithms leads to two interesting conclusions.
Reference: [2] <author> D. H. Bailey, K. Lee, and H. D. Simon, </author> <title> Using Strassen's algorithm to accelerate the solution of linear systems, </title> <editor> J. </editor> <booktitle> of Supercomputing, 4 (1990), </booktitle> <pages> pp. 357-371. </pages>
Reference-contexts: The change would have no effect on the right-looking algorithm, since in all the matrices it multiplies at least one dimension is r which was smaller than 192 in all the experiments. A similar experiment carried out by Bailey, Lee, and Simon <ref> [2] </ref> showed that Strassen's algorithm can accelerate the LAPACK's right-looking LU factorization on a Cray Y-MP. The largest improvements in performance, however, occured when large values of r were used. The fastest factorization of a matrix of order n = 2048, for example, was obtained with r = 512.
Reference: [3] <author> C. C. Douglas, M. Heroux, G. Slishman, and R. M. Smith, GEMMW: </author> <title> A portable level 3 BLAS Winograd variant of Strassen's matrix-matrix multiply algorithm, </title> <journal> J. of Computational Physics, </journal> <volume> 110 (1994), </volume> <pages> pp. 1-10. </pages>
Reference-contexts: We replaced the call to DGEMM, the level-3 BLA subroutine for matrix multiply-add by a call to DGEMMB, a public domain implementation 2 of a variant of Strassen algorithm <ref> [3] </ref>. (Replacing the calls to DGEMM by calls to a Strassen matrix-multiplication subroutine in IBM's ESSL gave similar results). DGEMMB uses Strassen's algorithm only when all the dimensions of the input matrices are greater than a machine-dependent constant.
Reference: [4] <author> J. J. Du Cruz, S. M. Nugent, J. K. Reid, and D. B. Taylor, </author> <title> Solving large full sets of linear equations in a paged virtual store, </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 7 (1981), </volume> <pages> pp. 527-536. </pages>
Reference-contexts: The paper also analyzes the number of data transfers in a popular partitioned LU-factorization algorithm, the so-called right-looking algorithm, which is used in LAPACK [1]. The performance characteristics of other popular partitioned LU-factorization algorithms, in particular Crout and the left-looking algorithm used in the NAG library <ref> [4] </ref>, are similar to those of the right-looking algorithm so they are not analyzed. The analysis of the two algorithms leads to two interesting conclusions. First, there is a simple system-independent formula for choosing the block size for the right-looking algorithm which is almost always optimal. <p> Designers of out-of-core LU decomposition codes often propose to use block-column (or row) algorithms. Many of them propose to choose r = M=n so that an entire block of columns fits within primary memory <ref> [4, 6, 7, 15] </ref>. This approach works well when the columns are short and a large number of them fits within primary memory, but the performance of such algorithms would be unacceptable when only few columns fit within primary memory.
Reference: [5] <author> P. C. Fischer and R. L. Probert, </author> <title> A note on matrix multiplication in a paging environment, </title> <booktitle> in ACM `76: Proceedings of the Annual Conference, </booktitle> <year> 1976, </year> <pages> pp. 17-21. </pages>
Reference-contexts: The analysis here assumes the use of a conventional triangular solver and matrix multiplication, rather than so-called "fast" or Strassen-like algorithms. The asymptotic bounds for fast matrix-multiplication algorithms are better <ref> [5] </ref>. We analyze the recursively-partitioned algorithm using induction. Initially, the analysis that does not take into account the permutation of rows that the algorithm performs. We shall return to these permutations later in this section.
Reference: [6] <author> N. Geers and R. Klees, </author> <title> Out-of-core solver for large dense nonsymmetric linear systems, </title> <journal> Manuscripta Geodetica, </journal> <volume> 18 (1993), </volume> <pages> pp. </pages> <month> 331-342. </month> <title> LOCALITY OF REFERENCE IN LU DECOMPOSITION 17 </title>
Reference-contexts: Designers of out-of-core LU decomposition codes often propose to use block-column (or row) algorithms. Many of them propose to choose r = M=n so that an entire block of columns fits within primary memory <ref> [4, 6, 7, 15] </ref>. This approach works well when the columns are short and a large number of them fits within primary memory, but the performance of such algorithms would be unacceptable when only few columns fit within primary memory.
Reference: [7] <author> R. G. Grimes, </author> <title> Solving systems of large dense linear equations, </title> <editor> J. </editor> <booktitle> of Supercomputing, 1 (1988), </booktitle> <pages> pp. 291-299. </pages>
Reference-contexts: Designers of out-of-core LU decomposition codes often propose to use block-column (or row) algorithms. Many of them propose to choose r = M=n so that an entire block of columns fits within primary memory <ref> [4, 6, 7, 15] </ref>. This approach works well when the columns are short and a large number of them fits within primary memory, but the performance of such algorithms would be unacceptable when only few columns fit within primary memory. <p> This approach works well when the columns are short and a large number of them fits within primary memory, but the performance of such algorithms would be unacceptable when only few columns fit within primary memory. Some researchers <ref> [7, 8, 9] </ref> suggest that algorithms that use less primary memory than is necessary for storing a few columns might have difficulty implementing partial pivoting.
Reference: [8] <author> A. C. McKeller and E. G. Coffman, Jr., </author> <title> Organizing matrices and matrix operations for paged memory systems, </title> <journal> Communications of the ACM, </journal> <volume> 12 (1969), </volume> <pages> pp. 153-165. </pages>
Reference-contexts: This approach works well when the columns are short and a large number of them fits within primary memory, but the performance of such algorithms would be unacceptable when only few columns fit within primary memory. Some researchers <ref> [7, 8, 9] </ref> suggest that algorithms that use less primary memory than is necessary for storing a few columns might have difficulty implementing partial pivoting.
Reference: [9] <author> C. B. Moler, </author> <title> Matrix computations with Fortran and paging, </title> <journal> Communications of the ACM, </journal> <volume> 15 (1972), </volume> <pages> pp. 268-270. </pages>
Reference-contexts: This approach works well when the columns are short and a large number of them fits within primary memory, but the performance of such algorithms would be unacceptable when only few columns fit within primary memory. Some researchers <ref> [7, 8, 9] </ref> suggest that algorithms that use less primary memory than is necessary for storing a few columns might have difficulty implementing partial pivoting.
Reference: [10] <author> V. Strassen, </author> <title> Gaussian elimination is not optimal, </title> <journal> Numer. Math., </journal> <volume> 13 (1969), </volume> <pages> pp. 354-355. </pages>
Reference-contexts: The results also show that permuting columns is almost always faster than exchanging rows. Experiments using Strassen's Algorithm. Performing the updates of the trailing submatrix using a variant of Strassen's algorithm <ref> [10] </ref> improved the performance of the recursively partitioned algorithm.
Reference: [11] <author> S. Toledo, </author> <title> Locality of reference in LU decomposition with partial pivoting, </title> <type> Tech. Report RC20344, </type> <institution> IBM T.J. Watson Research Center, </institution> <address> Yorktown Heights, NY, </address> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: The code can be compiled by many Fortran 77 compilers, in-luding compilers from IBM, Silicon Graphics, and Digital, by removing the RECURSIVE keyword and using a compiler option that enables recursion (see <ref> [11] </ref> for details). LOCALITY OF REFERENCE IN LU DECOMPOSITION 9 of I/O's in the algorithm, but the distribution of the I/O within the algorithm is significant. Finally, the analysis uses a simplified model of a two-level hierarchical memory that does not capture all the subtleties of actual memory systems. <p> Some of the technical details of the experiments, such as operating system versions, compiler versions, and compiler options are omitted from this paper. These details are fully described in our technical report <ref> [11] </ref>. Detailed Experimental Analyzes. The first set of experiments was performed on an IBM RS/6000 workstation with a 66.5 MHz POWER2 processor [14], 128 Kbytes 4-way set associative level-1 data-cache, a 1 Mbytes direct mapped level-2 cache, and a 128-bit-wide main memory bus.
Reference: [12] <author> S. Toledo and F. G. Gustavson, </author> <title> The design and implementation of SOLAR, a portable li brary for scalable out-of-core linear algebra computations, </title> <booktitle> in Proceedings of the 4th Annual Workshop on I/O in Parallel and Distributed Systems, </booktitle> <address> Philadelphia, </address> <month> May </month> <year> 1996, </year> <pages> pp. 28-40. </pages>
Reference-contexts: They claimed, without a proof, that pivoting can be incorporated into the algorithm without asymptotically increasing the number of I/O's the algorithm performs. They suggested that a recursive algorithm would be difficult to implement, so they implemented instead a partitioned left-looking algorithm using r = M=n. Toledo and Gustavson <ref> [12] </ref> describe a recursively-partitioned algorithm for out-of-core LU decomposition with partial pivoting. Their algorithm uses recursion on large submatrices, but switches to a left-looking variant on smaller submatrices (that would still not fit within main memory).
Reference: [13] <author> E. H. Welbon, C. C. Chan-Nui, D. J. Shippy, and D. A. Hicks, </author> <title> The POWER2 performance monitor, </title> <journal> IBM Journal of Research and Development, </journal> <month> 38 </month> <year> (1994). </year>
Reference-contexts: The performance of the algorithms was assessed using measurements of both running time and cache misses. Time was measured using the machines real-time clock, which has a resolution of one cycle. The number of cache misses was measured using the POWER2 performance monitor <ref> [13] </ref>. The performance monitor is a hardware subsystem in the processor capable of counting cache misses and other processor events. Both the real-time clock and the performance monitor are oblivious to time sharing.
Reference: [14] <author> S. W. White and S. Dhawan, POWER2: </author> <title> Next generation of the RISC System/6000 family, </title> <journal> IBM Journal of Research and Development, </journal> <month> 38 </month> <year> (1994). </year>
Reference-contexts: These details are fully described in our technical report [11]. Detailed Experimental Analyzes. The first set of experiments was performed on an IBM RS/6000 workstation with a 66.5 MHz POWER2 processor <ref> [14] </ref>, 128 Kbytes 4-way set associative level-1 data-cache, a 1 Mbytes direct mapped level-2 cache, and a 128-bit-wide main memory bus. The POWER2 processor is capable of issuing two double-precision multiply-add instructions per clock cycle.
Reference: [15] <author> D. Womble, D. Greenberg, S. Wheat, and R. Riesen, </author> <title> Beyond core: Making parallel com puter I/O practical, </title> <booktitle> in Proceeings of the 1993 DAGS/PC Symposium, </booktitle> <address> Hanover, NH, </address> <month> June </month> <year> 1993, </year> <institution> Dartmouth Institute for Advanced Graduate Studies, </institution> <note> pp. 56-63. Also available online from http://www.cs.sandia.gov/~dewombl/parallel io dags93.html. </note>
Reference-contexts: Designers of out-of-core LU decomposition codes often propose to use block-column (or row) algorithms. Many of them propose to choose r = M=n so that an entire block of columns fits within primary memory <ref> [4, 6, 7, 15] </ref>. This approach works well when the columns are short and a large number of them fits within primary memory, but the performance of such algorithms would be unacceptable when only few columns fit within primary memory. <p> The analysis in this paper shows that it is possible to achieve a low number of data transfers even when a single row or column does not fit within primary memory. Womble et al. <ref> [15] </ref> presented a recursively-partitioned LU decomposition algorithm without pivoting. They claimed, without a proof, that pivoting can be incorporated into the algorithm without asymptotically increasing the number of I/O's the algorithm performs.
References-found: 15

