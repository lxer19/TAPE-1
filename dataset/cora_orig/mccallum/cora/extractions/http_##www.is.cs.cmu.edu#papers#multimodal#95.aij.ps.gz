URL: http://www.is.cs.cmu.edu/papers/multimodal/95.aij.ps.gz
Refering-URL: http://www.cs.cmu.edu:80/afs/cs.cmu.edu/user/tue/WWW/resume.html
Root-URL: 
Phone: 2  
Title: MULTIMODAL INTERFACES  
Author: Alex Waibel , Minh Tue Vo Paul Duchnowski Stefan Manke 
Address: Pittsburgh, PA 15213-3890, U.S.A  Am Fasanengarten 5 76128 Karlsruhe, Germany  
Affiliation: 1 School of Computer Science Carnegie Mellon University  University of Karlsruhe Computer Science Department, ILKD  
Date: 1  
Pubnum: AIRJ94  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> S. Austin and R. Schwartz. </author> <title> A Comparison of Several Approximate Algorithms for Finding N-best Hypotheses. </title> <booktitle> In Proc. </booktitle> <address> ICASSP91. </address>
Reference: [2] <author> S. Baluja and D. Pomerleau. </author> <title> Non-Intrusive Gaze Tracking Using Artificial Neural Networks. </title> <booktitle> To appear in Advances in Neural Information Processing Systems 6, </booktitle> <publisher> Mor-gan Kaufmann, </publisher> <year> 1993. </year>
Reference: [3] <author> U. Bodenhausen, S. Manke, and A. Waibel. </author> <title> Connectionist Architectural Learning for High Performance Character and Speech Recognition. </title> <booktitle> In Proc. </booktitle> <address> ICASSP93. </address>
Reference: [4] <author> U. Bodenhausen and S. Manke. </author> <title> Automatically Structured Neural Networks for Handwritten Character and Word Recognition. </title> <booktitle> In Proc. </booktitle> <address> ICANN-93. </address>
Reference: [5] <author> L.D. Braida. </author> <title> Crossmodal Integration in the Identification of Consonant Segments. </title> <journal> The Quarterly Journal of Experimental Psychology, </journal> <volume> 43A(3), </volume> <year> 1991, </year> <pages> pp. 647-677. </pages>
Reference: [6] <author> C. Bregler, H. Hild, S. Manke, and A. Waibel. </author> <title> Improving Connected Letter Recognition by Lipreading. </title> <booktitle> In Proc. </booktitle> <address> ICASSP93. </address>
Reference-contexts: Vector quantization of the input allowed the use of discrete Hidden Markov Models (HMM) in the recognition process. The system using generalized triseme models achieved 25% recognition rate (visual information only). 2.2 Initial System Our integrated acoustic/visual continuous-speech ASR system was first reported in <ref> [6] </ref>. It was developed for a spelling task using the German alphabet. Training and test utterances comprise spelled (without pauses) names and nonsense letter sequences of arbitrary and unknown to the recognizer lengths. <p> Conversely, high entropy corresponds to near equal probability of most phonemes or visemes. Accordingly, we symmetrically weight the acoustic and visual contributions to the combined layer in inverse proportion to their respective entropies. 2.2.2 Results Table 1 shows recognition performance originally achieved on a speaker-dependent task <ref> [6] </ref>. Training data consisted of 75/200 training and 39/150 testing sequences for speaker msm/mcb. Misclassified, omitted and inserted letters were counted as errors. In the noisy experiments the acoustic data was corrupted with broadband noise until the acoustic-alone performance was significantly reduced.
Reference: [7] <author> C. Bregler. Lippenlesen als Unterstuetzung zur robusten automatischen Spracherken-nung. </author> <title> M.S. </title> <type> Thesis. </type> <institution> Fakultaet fuer Informatik, Universitaet Karlsruhe, </institution> <year> 1993. </year>
Reference-contexts: Word Accuracy of Speech/Lip System AIRJ94 7 Further experiments were carried out on a database of 6 speakers (2 female, 4 male) to test the performance on a multispeaker task <ref> [7] </ref>. 80 sequences per speaker were used for training and 10 for testing. Visual-alone mode achieved only 12.2% word accuracy. <p> This is of course contingent on the availability of sufficient training data to robustly train the magnified network that results from increasing the size of the input vector. Preliminary experiments <ref> [7] </ref> suggest that this approach to modality integration is, in fact, not feasible without visual data reduction. This observation supplies more motivation still for the work described in 2.3.2. Low level modality integration allows us also to avoid the tricky problem of viseme specification.
Reference: [8] <author> A.J. Goldschen. </author> <title> Continuous Automatic Speech Recognition by Lipreading. </title> <type> Ph.D. Dissertation. </type> <institution> George Washington University, </institution> <year> 1993. </year>
Reference-contexts: Separate Time Delay Neural Networks (TDNN) processed acoustic and optical data to render a decision on one of 10 English letters spoken in isolation. Visual and acoustic-alone recognition was 51 and 64 percent, respectively. By combining the outputs in a Bayesian framework, they achieved overall performance of 91%. Goldschen <ref> [8] </ref> used 13 visual features extracted from processed image frames acquired with a head-mounted camera as in [22] to identify one out of 150 possible TIMIT sentences spoken by a single talker. It appears that the sentences were treated essentially as very long words in this setup.
Reference: [9] <author> I. Guyon, P. Albrecht, Y. LeCun, J. Denker, and W. Hubbard. </author> <title> Design of a Neural Network Character Recognizer for a Touch Terminal. </title> <journal> Pattern Recognition, </journal> <year> 1991. </year>
Reference-contexts: A gesture is captured as a sequence of coordinates tracking the stylus as it moves over the tablets surface, as opposed to a static bit-mapped representation of the shape of the gesture. This dynamic representation was motivated by its successful use in handwritten character recognition (Section 3 & <ref> [9] </ref>). Results of experiments described in [9] suggest that the time-sequential signal contains more information relevant to classification than the static image, leading to better recognition performance. In our current implementation, the stream of data from the digitizing tablet goes through a preprocessing phase [9]. <p> This dynamic representation was motivated by its successful use in handwritten character recognition (Section 3 & <ref> [9] </ref>). Results of experiments described in [9] suggest that the time-sequential signal contains more information relevant to classification than the static image, leading to better recognition performance. In our current implementation, the stream of data from the digitizing tablet goes through a preprocessing phase [9]. <p> handwritten character recognition (Section 3 & <ref> [9] </ref>). Results of experiments described in [9] suggest that the time-sequential signal contains more information relevant to classification than the static image, leading to better recognition performance. In our current implementation, the stream of data from the digitizing tablet goes through a preprocessing phase [9]. The coordinates are normalized and resampled at regular intervals to eliminate differences in size and drawing speed; from these resampled coordinates we extract local geometric information at each point, such as the direction of pen movement and the curvature of the trajectory.
Reference: [10] <author> P. Haffner, M. Franzini, and A. Waibel. </author> <title> Integrating Time Alignment and Neural Networks for High Performance Continuous Speech Recognition. </title> <booktitle> In Proc. </booktitle> <address> ICASSP91. </address>
Reference: [11] <author> P. Haffner and A. Waibel. </author> <title> Multi-State Time Delay Neural Networks for Continuous Speech Recognition. </title> <booktitle> Advances in Neural Network Information Processing Systems (NIPS-4), </booktitle> <year> 1992. </year>
Reference-contexts: This input representation is used with a connectionist recognizer, which is well suited for handling temporal sequences of patterns as provided by this kind of input representation. This recognizer, a Multi-State Time Delay Neural Network (MS-TDNN) <ref> [11] </ref>, integrates the segmentation and recognition of words into a single network architecture.
Reference: [12] <author> A. Hauptmann. </author> <title> Speech and Gestures for Graphic Image Manipulation. </title> <booktitle> In Proc. </booktitle> <address> CHI89. </address>
Reference: [13] <author> H. Hild and A. Waibel. </author> <title> Connected Letter Recognition with a Multi-State Time Delay Neural Network. </title> <booktitle> Neural Information Processing Systems (NIPS-5), </booktitle> <year> 1993. </year> <note> AIRJ94 20 </note>
Reference-contexts: The resulting 384 gray level values were then normalized for each frame to lie between -1.0 and 1.0 and constituted the visual evidence available to the classification algorithms. We use a modular Multi-State Time Delay Neural Network (MS-TDNN) <ref> [13] </ref> to perform the actual recognition. Figure 2 shows the architecture. Through the first three layers (input-hidden-phoneme/viseme) the acoustic and visual inputs are processed separately. The third of these layers produces activations for 65 phoneme states on the acoustic side and 42 viseme states on the visual side.
Reference: [14] <author> X. Huang, F. Alleva, H. Hon, M. Hwang, K. Lee, and R. Rosenfeld. </author> <title> The SPHINX-II Speech Recognition System: An Overview. Computer Speech and Language (in press), </title> <year> 1993. </year>
Reference-contexts: The TDNN-based gesture recognizer was described in 4.2. For the speech component we use many alternative speech recognition strategies; these include a keyword spotter developed by Zeppenfeld [39][40] as well as full-scale continuous speech recognition modules such as Sphinx <ref> [14] </ref> and Janus [37]. The speech recognition module is coupled with an RTN-parser [35] using a semantic grammar developed for the editing task. For the keyword-spotting version, the word spotter was trained to spot 11 keywords representing editing commands such as move, delete,... and textual units such as character, word,...
Reference: [15] <author> P.L. Jackson. </author> <title> The Theoretical Minimal Unit for Visual Speech Perception: Visemes and Coarticulation. </title> <journal> The Volta Review, </journal> <volume> 90(5), </volume> <month> Sept. </month> <year> 1988, </year> <pages> pp. 99-115. </pages>
Reference-contexts: The usefulness of lip movement stems in large part from its rough complementariness to the acoustic signal: the former is most reliable for distinguishing the place of articulation (ex. <ref> [15] </ref>), the latter conveys most robustly manner and voicing information (ex. [17]).
Reference: [16] <author> S. Manke and U. Bodenhausen. </author> <title> A Connectionist Recognizer for On-Line Cursive Handwriting Recognition. </title> <booktitle> In Proc. </booktitle> <address> ICASSP94. </address>
Reference: [17] <author> G.A. Miller and P.E. </author> <title> Nicely. An analysis of perceptual confusions among some English consonants. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 27(2), </volume> <month> Mar. </month> <year> 1955, </year> <pages> pp. 338-352. </pages>
Reference-contexts: The usefulness of lip movement stems in large part from its rough complementariness to the acoustic signal: the former is most reliable for distinguishing the place of articulation (ex. [15]), the latter conveys most robustly manner and voicing information (ex. <ref> [17] </ref>). The task of exploiting lip-reading in an automatic speech recognition system requires the solution of two conceptually distinct but not independent problems: suitable representation and recognition of the visual signal and the integration of thus obtained visual evidence with the acoustic side.
Reference: [18] <author> H. Ney. </author> <title> The Use of a One-Stage Dynamic Programming Algorithm for Connected Word Recognition. </title> <booktitle> In Proc. </booktitle> <address> ICASSP84. </address>
Reference-contexts: Some visemes will therefore inuence more than one of the combined layer units. In the final layer (which copies the activations from the combined layer) a one stage Dynamic Time Warping algorithm <ref> [18] </ref> is applied to find the optimal path through the phone-hypotheses that corresponds to a sequence of letter models. Network training is done in two phases. First, the acoustic and visual sub-nets are trained separately to fit phoneme/viseme targets. Second, the complete network is trained to fit letter targets. <p> The MS-TDNN, which was originally proposed for continuous speech recognition tasks [13][6], combines shift invariant high accuracy pattern recognition capabilities of a TDNN [33][9] with a non-linear time alignment procedure (dynamic time warping) <ref> [18] </ref> for aligning strokes into character sequences. recognition system is integrated into the example application, which is shown in architecture, and present recognition results for writer-independent, single-character recognition tasks and large-vocabulary, writer-dependent, cursive handwriting recognition tasks with vocabulary sizes from 400 up to 20000 words.
Reference: [19] <author> C. Nodine, H. Kundel, L. Toto, and E. Krupinski. </author> <title> Recording and Analyzing Eye-position Data Using a Microcomputer Workstation. Behavior Research Methods, </title> <type> Instruments & Computers 24 (3), </type> <year> 1992, </year> <pages> pp. 475-584. </pages>
Reference: [20] <author> K. Mase and A. Pentland. </author> <title> Automatic Lipreading by Optical-Flow Analysis. </title> <journal> Systems and Computers in Japan, </journal> <volume> 22(6), </volume> <year> 1991, </year> <pages> pp. 67-76. </pages>
Reference-contexts: The combination of optical and acoustic decisions was achieved via a set of 30 heuristic rules. The overall performance was similar to the earlier system. Pentland and Mase <ref> [20] </ref> chose to parameterize the oral cavity image by computing average optical ow vectors in four regions of the picture designed to capture the movement of particular facial muscles. The regions were selected manually by the experimenters.
Reference: [21] <author> E.D. Petajan. </author> <title> Automatic lipreading to enhance speech recognition. </title> <booktitle> In Proc. IEEE Communications Society Global Telecommunications Conference, </booktitle> <address> Atlanta GA, </address> <month> Nov. </month> <year> 1984. </year>
Reference-contexts: Clearly, a given type of visual pre-processing will constrain the options available for further combination of the two sources. 2.1 Related Research The first significant attempt to supplement acoustic ASR with lip-reading was the system built by Petajan and applied to a speaker-dependent isolated-word (vocabulary of 100 words) recognition task <ref> [21] </ref>. Four static features were extracted from each image frame and a linear time-warping procedure was used to identify the most probable word. By combining the output of the optical recognizer with that of a commercial ASR system the recognition rate was improved from 65 to 78 percent.
Reference: [22] <author> E.D. Petajan, B. Bischoff, and D. Bodoff. </author> <title> An improved automatic lipreading system to enhance speech recognition. </title> <booktitle> ACM SIGCHI-88, </booktitle> <year> 1988, </year> <pages> pp. 19-25. </pages>
Reference-contexts: By combining the output of the optical recognizer with that of a commercial ASR system the recognition rate was improved from 65 to 78 percent. In a follow-up effort <ref> [22] </ref> simplified optical processing was used to achieve near real-time performance. The image of the speakers mouth area was captured by a camera and lighting system installed in a head AIRJ94 4 mounted harness, circumventing some image-processing problems. <p> Visual and acoustic-alone recognition was 51 and 64 percent, respectively. By combining the outputs in a Bayesian framework, they achieved overall performance of 91%. Goldschen [8] used 13 visual features extracted from processed image frames acquired with a head-mounted camera as in <ref> [22] </ref> to identify one out of 150 possible TIMIT sentences spoken by a single talker. It appears that the sentences were treated essentially as very long words in this setup. Vector quantization of the input allowed the use of discrete Hidden Markov Models (HMM) in the recognition process.
Reference: [23] <author> D. Pomerleau. </author> <title> Neural Network Perception for Mobile Robot Guidance. </title> <type> Ph.D. Thesis, </type> <institution> Carnegie Mellon University, CMU-CS-92-115. </institution>
Reference: [24] <author> R. Rose and D. Paul. </author> <title> A Hidden Markov Model Based Keyword Recognition Systems. </title> <booktitle> In Proc. </booktitle> <address> ICASSP90. </address>
Reference: [25] <author> M. Schenkel, I. Guyon, and D. Henderson. </author> <title> On-Line Cursive Script Recognition Using Time Delay Neural Networks and Hidden Markov Models. </title> <booktitle> In Proc. </booktitle> <address> ICASSP94. </address>
Reference: [26] <author> O. Schmidbauer and J. Tebelskis. </author> <title> An LVQ-based Reference Model for Speaker-Adaptive Speech Recognition. </title> <booktitle> In Proc. </booktitle> <address> ICASSP92. </address>
Reference: [27] <author> D.G. Stork, Greg Wolff, and E. Levine. </author> <title> Neural network lipreading system for improved speech recognition. </title> <booktitle> In Proc. </booktitle> <address> IJCNN92. </address>
Reference-contexts: Furthermore, the relative contribution of visual and acoustic information was adjusted according to the SNR by an omniscient controller (i.e., the value of the SNR is explicitly given). The visual input was shown to compensate for noise-induced performance drop in purely acoustic recognition. Stork et al. <ref> [27] </ref> measured the position of ten reective markers placed on the lips of the talker thus significantly simplifying the issue of optical data capture. From these measurements they derived five parameters as the visual input.
Reference: [28] <author> Q. Summerfield. </author> <title> Audio-visual Speech Perception, Lipreading and Artificial Stimulation. In Hearing Science and Hearing Disorders, M.E. </title> <editor> Lutman and M.P. Haggard eds., </editor> <address> New York: </address> <publisher> Academic Press, </publisher> <year> 1983. </year>
Reference: [29] <author> J. Tebelskis and A. Waibel. </author> <title> Performance Through Consistency: MS-TDNNs for Large Vocabulary Continuous Speech Recognition. </title> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: [30] <author> M. Turk and A. Pentland. </author> <title> Eigenfaces for Recognition. </title> <journal> Journal of Cognitive Neuro-science, </journal> <volume> 3(1), </volume> <year> 1991, </year> <pages> pp. 71-86. </pages> <address> AIRJ94 21 </address>
Reference-contexts: It should be noted that this representation (relying as it does on the correlations among the original data points) is sensitive to image shifts, as also found in other studies (ex. <ref> [30] </ref>). We are also investigating linear discriminant analysis, a related technique, which might prove better for image classification (as opposed to representation). 2.3.3 Acoustic/Visual Integration There is evidence that humans combine acoustic and visual information before classification, i.e., without making separate decisions based on each modality [28][5].
Reference: [31] <author> M.T. Vo and A. Waibel. </author> <title> A Multimodal Human-Computer Interface: Combination of Speech and Gesture Recognition. </title> <booktitle> In Adjunct Proc. </booktitle> <address> InterCHI93. </address>
Reference-contexts: In order to pursue this direction of investigation, we developed a multimodal text editor capable of recognizing speech and gesture commands <ref> [31] </ref>. The initial multimodal editor we developed currently uses 8 editing gestures (see Table 4). Some of these were inspired by standard mark-up symbols used by human editors.
Reference: [32] <author> M.T. Vo. </author> <title> Incremental Learning Using the Time Delay Neural Network. </title> <booktitle> In Proc. </booktitle> <address> ICASSP94. </address>
Reference-contexts: Such a fallback method would offer a reasonable level of performance until the network can be retrained off-line. We have developed a method to accomplish this using an Incremental TDNN (ITDNN) architecture <ref> [32] </ref>. We start by training a regular TDNN using all the available data to obtain a base network. <p> Extra unit Excitatory connection (weight &gt; 0) Time Score = average Match Time AIRJ94 17 We tested the incremental learning capability of the ITDNN in a series of experiments involving simple handwritten digit recognition <ref> [32] </ref>. This task was chosen because it is simple enough so that we can easily eliminate the inuence of factors extraneous to what we want to measure: what is the degradation in performance on old input patterns when the ITDNN is trained on new input patterns.
Reference: [33] <author> A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. Lang. </author> <title> Phoneme Recognition Using Time-Delay Neural Networks. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <year> 1989. </year>
Reference-contexts: Select Begin selection Delete End selection Delete Transpose Paste Split line Table 4. Text-Editing Gestures AIRJ94 15 4.2 Gesture Classification Using Neural Networks We use a TDNN <ref> [33] </ref> (see Figure 7) to classify each preprocessed time-sequential signal as a gesture among the predefined set of 8 gestures. Each gesture in the set is represented by an output unit. <p> The network is trained on a set of manually classified gestures using a modified backpropagation algorithm <ref> [33] </ref>. During training, the 10 units in the first hidden layer essentially become feature detectors that extract low-level patterns from the input, and the 8 hidden units in the next layer learn to spot those features that contribute to the recognition of each of the 8 gestures.
Reference: [34] <author> A. Waibel, A. Jain, A. McNair, H. Saito, A. Hauptmann, J. Tebelskis. </author> <title> JANUS: a Speech-to-speech Translation System Using Connectionist and Symbolic Processing Strategies. </title> <booktitle> In Proc. </booktitle> <address> ICASSP91. </address>
Reference: [35] <author> W. Ward. </author> <title> Understanding Spontaneous Speech: </title> <booktitle> the Phoenix System. In Proc. </booktitle> <address> ICASSP91. </address>
Reference-contexts: For the speech component we use many alternative speech recognition strategies; these include a keyword spotter developed by Zeppenfeld [39][40] as well as full-scale continuous speech recognition modules such as Sphinx [14] and Janus [37]. The speech recognition module is coupled with an RTN-parser <ref> [35] </ref> using a semantic grammar developed for the editing task. For the keyword-spotting version, the word spotter was trained to spot 11 keywords representing editing commands such as move, delete,... and textual units such as character, word,...
Reference: [36] <author> C. Ware and H. Mikaelian. </author> <title> An Evaluation of an Eye Tracker as a Device for Computer Input. </title> <booktitle> In Human Factors in Computing Systems IV, </booktitle> <year> 1987. </year>
Reference: [37] <editor> M. Woszczyna et al. </editor> <booktitle> Recent Advances in JANUS:A Speech Translation System. In Proc. </booktitle> <address> EUROSPEECH93. </address>
Reference-contexts: The TDNN-based gesture recognizer was described in 4.2. For the speech component we use many alternative speech recognition strategies; these include a keyword spotter developed by Zeppenfeld [39][40] as well as full-scale continuous speech recognition modules such as Sphinx [14] and Janus <ref> [37] </ref>. The speech recognition module is coupled with an RTN-parser [35] using a semantic grammar developed for the editing task. For the keyword-spotting version, the word spotter was trained to spot 11 keywords representing editing commands such as move, delete,... and textual units such as character, word,...
Reference: [38] <author> B.P. Yuhas, M.H. Goldstein, Jr., and T.J. Sejnowski. </author> <title> Integration of acoustic and visual speech signals using neural networks. </title> <journal> IEEE Communications Magazine, </journal> <month> Nov. </month> <year> 1989. </year>
Reference-contexts: The regions were selected manually by the experimenters. They used template matching (on optical data alone) to recognize strings of three to five digits from three speakers. Average word recognition rate was roughly 75%. Neural networks were used by Yuhas et al. <ref> [38] </ref> with both optical and acoustic input to distinguish among 9 vowel phonemes under varying acoustic signal-to-noise ratio (SNR). Only static images (not sequences) were used as the optical input.
Reference: [39] <author> T. Zeppenfeld and A. Waibel. </author> <title> A Hybrid Neural Network, Dynamic Programming Word Spotter. </title> <booktitle> In Proc. </booktitle> <address> ICASSP92. </address>

References-found: 39

