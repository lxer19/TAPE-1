URL: ftp://ftp.idsia.ch/pub/juergen/zif.ps.gz
Refering-URL: http://www.idsia.ch/~juergen/topics.html
Root-URL: 
Email: juergen@idsia.ch  
Title: NEURAL PREDICTORS FOR DETECTING AND REMOVING REDUNDANT INFORMATION  
Author: Jurgen Schmidhuber 
Web: http://www.idsia.ch/~juergen  
Address: Corso Elvezia 36, 6900 Lugano, Switzerland  
Affiliation: IDSIA,  
Abstract: The components of most real-world patterns contain redundant information. However, most pattern classifiers (e.g., statistical classifiers and neural nets) work better if pattern components are nonredundant. I present various unsupervised nonlinear predictor-based "neural" learning algorithms that transform patterns and pattern sequences into less redundant patterns without loss of information. The first part of the paper shows how a neural predictor can be used to remove redundant information from input sequences. Experiments with artificial sequences demonstrate that certain supervised classification techniques can greatly benefit from this kind of unsupervised preprocessing. In the second part of the paper, a neural predictor is used to remove redundant information from natural text. With certain short newspaper articles, the neural method can achieve better compression ratios than the widely used asymptotically optimal Lempel-Ziv string compression algorithm. The third part of the paper shows how a system of co-evolving neural predictors and neural code generating modules can build factorial (statistically nonredundant) codes of pattern ensembles. The method is successfully applied to images of letters randomly presented according to the probabilities of English language.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. B. Barlow, T. P. Kaushal, and G. J. Mitchison. </author> <title> Finding minimum entropy codes. </title> <journal> Neural Computation, </journal> <volume> 1(3) </volume> <pages> 412-423, </pages> <year> 1989. </year>
Reference-contexts: There it was mentioned that the ideal of a low-redundancy code is a factorial code (e.g. <ref> [1] </ref>). <p> The i-th code unit produces an output value y p i 2 <ref> [0; 1] </ref> in response to the current external input vector x p . The central idea is: For each code unit there is an adaptive predictor network that tries to predict the code unit from the remaining n1 code units.
Reference: [2] <author> G.J. Chaitin. </author> <title> On the length of programs for computing finite binary sequences: statistical considerations. </title> <journal> Journal of the ACM, </journal> <volume> 16 </volume> <pages> 145-159, </pages> <year> 1969. </year>
Reference-contexts: Variants of algorithmic redundancy allow for things like "learning by analogy", "learning by chunking", "learning how to learn", etc. [31]. Shannon information, however, is not the right concept to exploit the potential benefits of algorithmic redundancy. Instead we need to look at Kolmogorov complexity or "algorithmic information" [8] [34] <ref> [2] </ref> [11] [3] and especially at its computationally tractable generalizations (e.g. [4] [10] [12] [35]) to properly treat general (as opposed to conventional statistical) sources of redundant information.
Reference: [3] <author> G.J. Chaitin. </author> <title> A theory of program size formally identical to information theory. </title> <journal> Journal of the ACM, </journal> <volume> 22 </volume> <pages> 329-340, </pages> <year> 1975. </year>
Reference-contexts: Shannon information, however, is not the right concept to exploit the potential benefits of algorithmic redundancy. Instead we need to look at Kolmogorov complexity or "algorithmic information" [8] [34] [2] [11] <ref> [3] </ref> and especially at its computationally tractable generalizations (e.g. [4] [10] [12] [35]) to properly treat general (as opposed to conventional statistical) sources of redundant information.
Reference: [4] <author> J. Hartmanis. </author> <title> Generalized Kolmogorov complexity and the structure of feasible computations. </title> <booktitle> In Proc. 24th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 439-445, </pages> <year> 1983. </year>
Reference-contexts: Shannon information, however, is not the right concept to exploit the potential benefits of algorithmic redundancy. Instead we need to look at Kolmogorov complexity or "algorithmic information" [8] [34] [2] [11] [3] and especially at its computationally tractable generalizations (e.g. <ref> [4] </ref> [10] [12] [35]) to properly treat general (as opposed to conventional statistical) sources of redundant information.
Reference: [5] <author> G. </author> <title> Held. Data Compression. </title> <publisher> Wiley and Sons LTD, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: The predictor updates turn out to be quite time consuming, which makes special neural net hardware recommendable. The limited software simulations presented in this section, however, will show that the "neural" compression technique can achieve "excellent" compression ratios. Here the term "excellent" is defined by a statement from <ref> [5] </ref>: "In general, good algorithms can be expected to achieve an average compression ratio of 1.5, while excellent algorithms based upon sophisticated processing techniques will achieve an average compres sion ratio exceeding 2.0." Here the average compression ratio is the average ratio between the lengths of original and compressed files. <p> The results were compared to those obtained with standard encoding techniques provided by the operating system UNIX, namely "pack", "compress", and "gzip". The corresponding decoding algorithms are "unpack", "uncompress", and "gunzip", respectively. "pack" is based on Huffman-Coding (e.g. <ref> [5] </ref>), while "compress" and "gzip" are based on the Lempel-Ziv technique [43]. As the file size goes to infinity, Lempel-Ziv becomes asymptotically optimal in a certain information theoretic sense [42].
Reference: [6] <author> S. Hochreiter and J. Schmidhuber. </author> <title> Flat minima. </title> <journal> Neural Computation, </journal> <volume> 9(1) </volume> <pages> 1-42, </pages> <year> 1997. </year>
Reference-contexts: Many potential sources of redundant information have been neglected, however (see, e.g., <ref> [6] </ref>). Among the things I did not address is the possibility of redundancy among learning strategies [31]. Suppose learning algorithm A is good at solving problems of class A but tends to fail with problems of class B.
Reference: [7] <author> S. Hochreiter and J. Schmidhuber. </author> <title> Long short-term memory. </title> <journal> Neural Computation, </journal> <volume> 9 </volume> <pages> 1681-1726, </pages> <year> 1997. </year>
Reference-contexts: A feedforward neural net cannot learn the problem, due to the arbitrary time lags that may occur. Instead, we need something like a recurrent net (e.g. [18] [37] [17] [40] [15] [39] [38] [20] <ref> [7] </ref> ). <p> Two related reasons are: * Time lags are too long (error signals become less significant while moving "back into time"). See, e.g., <ref> [7] </ref>. * The presumed search space is huge (in principle, the recurrent net considers all possible symbol combinations as equal candidates for being the reason for the final classification). But the "real" search space ought to be small, because most possible symbol combinations can never occur.
Reference: [8] <author> A.N. </author> <title> Kolmogorov. Three approaches to the quantitative definition of information. </title> <journal> Problems of Information Transmission, </journal> <volume> 1 </volume> <pages> 1-11, </pages> <year> 1965. </year>
Reference-contexts: Variants of algorithmic redundancy allow for things like "learning by analogy", "learning by chunking", "learning how to learn", etc. [31]. Shannon information, however, is not the right concept to exploit the potential benefits of algorithmic redundancy. Instead we need to look at Kolmogorov complexity or "algorithmic information" <ref> [8] </ref> [34] [2] [11] [3] and especially at its computationally tractable generalizations (e.g. [4] [10] [12] [35]) to properly treat general (as opposed to conventional statistical) sources of redundant information.
Reference: [9] <author> Y. LeCun. </author> <title> Une procedure d'apprentissage pour reseau a seuil asymetrique. </title> <booktitle> Proceedings of Cognitiva 85, Paris, </booktitle> <pages> pages 599-604, </pages> <year> 1985. </year>
Reference: [10] <author> L. A. Levin. </author> <title> Universal sequential search problems. </title> <journal> Problems of Information Transmission, </journal> <volume> 9(3) </volume> <pages> 265-266, </pages> <year> 1973. </year>
Reference-contexts: Shannon information, however, is not the right concept to exploit the potential benefits of algorithmic redundancy. Instead we need to look at Kolmogorov complexity or "algorithmic information" [8] [34] [2] [11] [3] and especially at its computationally tractable generalizations (e.g. [4] <ref> [10] </ref> [12] [35]) to properly treat general (as opposed to conventional statistical) sources of redundant information.
Reference: [11] <author> L. A. Levin. </author> <title> Laws of information (nongrowth) and aspects of the foundation of probability theory. </title> <journal> Problems of Information Transmission, </journal> <volume> 10(3) </volume> <pages> 206-210, </pages> <year> 1974. </year>
Reference-contexts: Shannon information, however, is not the right concept to exploit the potential benefits of algorithmic redundancy. Instead we need to look at Kolmogorov complexity or "algorithmic information" [8] [34] [2] <ref> [11] </ref> [3] and especially at its computationally tractable generalizations (e.g. [4] [10] [12] [35]) to properly treat general (as opposed to conventional statistical) sources of redundant information.
Reference: [12] <author> M. Li and P. M. B. Vitanyi. </author> <title> An introduction to Kolmogorov complexity and its applications. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, </booktitle> <pages> pages 188-254. </pages> <publisher> Elsevier Science Publishers B.V., </publisher> <year> 1990. </year>
Reference-contexts: Shannon information, however, is not the right concept to exploit the potential benefits of algorithmic redundancy. Instead we need to look at Kolmogorov complexity or "algorithmic information" [8] [34] [2] [11] [3] and especially at its computationally tractable generalizations (e.g. [4] [10] <ref> [12] </ref> [35]) to properly treat general (as opposed to conventional statistical) sources of redundant information. This is a recent focus of my research [25, 26, 32, 31]. 6 ACKNOWLEDGMENTS Thanks to Peter Dayan, Richard Zemel and Alex Pouget for sharing their insight regarding the equivalence of equations (4) and (3).
Reference: [13] <author> S. Lindstadt. </author> <title> Comparison of two unsupervised neural network models for redundancy reduction. </title> <editor> In M. C. Mozer, P. Smolensky, D. S. Touretzky, J. L. Elman, and A. S. Weigend, editors, </editor> <booktitle> Proc. of the 1993 Connectionist Models Summer School, </booktitle> <pages> pages 308-315. </pages> <address> Hillsdale, NJ: </address> <publisher> Erlbaum Associates, </publisher> <year> 1993. </year>
Reference-contexts: The redundancy (see the definition in section 1.2) corresponding to the original DEC dataset is 13:41. The redundancy corresponding to a 16-bit code discovered by the system is 2:5. See [14], <ref> [13] </ref>, and [24] for details. This result corresponds to a dramatic reduction of redundant information, although the achieved value is not optimal. In many realistic cases, however, approximations of nonredundant codes should be satisfactory.
Reference: [14] <author> S. Lindstadt. </author> <title> Comparison of unsupervised neural networks for redundancy reduction, 1993. </title> <type> Master's thesis, </type> <institution> Dept. of Comp. Sci., University of Col-orado at Boulder. </institution> <month> 13 </month>
Reference-contexts: The redundancy (see the definition in section 1.2) corresponding to the original DEC dataset is 13:41. The redundancy corresponding to a 16-bit code discovered by the system is 2:5. See <ref> [14] </ref>, [13], and [24] for details. This result corresponds to a dramatic reduction of redundant information, although the achieved value is not optimal. In many realistic cases, however, approximations of nonredundant codes should be satisfactory.
Reference: [15] <author> M. C. Mozer. </author> <title> A focused back-propagation algorithm for temporal sequence recognition. </title> <journal> Complex Systems, </journal> <volume> 3 </volume> <pages> 349-381, </pages> <year> 1989. </year>
Reference-contexts: Otherwise it will not be able to classify correctly. 2.1.1 SIMULATION We use local input representation. A feedforward neural net cannot learn the problem, due to the arbitrary time lags that may occur. Instead, we need something like a recurrent net (e.g. [18] [37] [17] [40] <ref> [15] </ref> [39] [38] [20] [7] ).
Reference: [16] <author> D. B. Parker. Learning-logic. </author> <type> Technical Report TR-47, </type> <institution> Center for Comp. Research in Economics and Management Sci., MIT, </institution> <year> 1985. </year>
Reference: [17] <author> B. A. Pearlmutter. </author> <title> Learning state space trajectories in recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 1(2) </volume> <pages> 263-269, </pages> <year> 1989. </year>
Reference-contexts: Otherwise it will not be able to classify correctly. 2.1.1 SIMULATION We use local input representation. A feedforward neural net cannot learn the problem, due to the arbitrary time lags that may occur. Instead, we need something like a recurrent net (e.g. [18] [37] <ref> [17] </ref> [40] [15] [39] [38] [20] [7] ).
Reference: [18] <author> A. J. Robinson and F. Fallside. </author> <title> The utility driven dynamic error propagation network. </title> <type> Technical Report CUED/F-INFENG/TR.1, </type> <institution> Cambridge University Engineering Department, </institution> <year> 1987. </year>
Reference-contexts: Otherwise it will not be able to classify correctly. 2.1.1 SIMULATION We use local input representation. A feedforward neural net cannot learn the problem, due to the arbitrary time lags that may occur. Instead, we need something like a recurrent net (e.g. <ref> [18] </ref> [37] [17] [40] [15] [39] [38] [20] [7] ).
Reference: [19] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representations by error propagation. </title> <booktitle> In Parallel Distributed Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 318-362. </pages> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference: [20] <author> J. Schmidhuber. </author> <title> A fixed size storage O(n 3 ) time complexity learning algorithm for fully recurrent continually running networks. </title> <journal> Neural Computation, </journal> <volume> 4(2) </volume> <pages> 243-248, </pages> <year> 1992. </year>
Reference-contexts: Otherwise it will not be able to classify correctly. 2.1.1 SIMULATION We use local input representation. A feedforward neural net cannot learn the problem, due to the arbitrary time lags that may occur. Instead, we need something like a recurrent net (e.g. [18] [37] [17] [40] [15] [39] [38] <ref> [20] </ref> [7] ).
Reference: [21] <author> J. Schmidhuber. </author> <title> Learning complex, extended sequences using the principle of history compression. </title> <journal> Neural Computation, </journal> <volume> 4(2) </volume> <pages> 234-242, </pages> <year> 1992. </year>
Reference-contexts: We need to look at the unpredictable inputs only. Only the unexpected deserves attention <ref> [21] </ref>. We apply this insight to sequences generated by the modified automaton above. 2.3.1 SIMULATION We start with 100 training sequences for a recurrent predictor network with 5 input units, 5 output units, 1 bias unit, and a learning rate of 0.5. <p> Recall that conventional recurrent networks failed to learn the task within 10 7 training sequences. In this special case, the speed-up factor obtained by adaptive redundancy reduction is at least 10 3 . More details and extensions of the principle above can be found in <ref> [21] </ref>, [23], [29], and especially in [24]. The next section describes a somewhat different kind of neural predictor for compressing natural text (as opposed to artificial symbol strings). 3 EXAMPLE 2: Text Compression The example from the previous section was based on artificial data from a stochastic automaton.
Reference: [22] <author> J. Schmidhuber. </author> <title> Learning factorial codes by predictability minimization. </title> <journal> Neural Computation, </journal> <volume> 4(6) </volume> <pages> 863-879, </pages> <year> 1992. </year>
Reference-contexts: But each code unit in turn tries to become as unpredictable as possible. The only way it can do so is by representing environmental properties that are statistically independent from environmental properties represented by the remaining code units. The principle of predictability minimization was first described in <ref> [22] </ref>. The predictor network for code unit i is called P i . Its output in response to the fy k ; k 6= ig is called P p i . <p> But the code units try to maximize the same (!) objective function the predictors try to minimize: V C = i;p p p Predictors and code units co-evolve by fighting each other. See details in <ref> [22] </ref> and especially in [24]. Let us assume that the P i do not get trapped in local minima and perfectly learn the conditional expectations. It then turns out that the objective function V C (first given in [22]) is essentially equivalent to the following one (also given in [22]): X <p> See details in <ref> [22] </ref> and especially in [24]. Let us assume that the P i do not get trapped in local minima and perfectly learn the conditional expectations. It then turns out that the objective function V C (first given in [22]) is essentially equivalent to the following one (also given in [22]): X V AR (y i ) i;p p where y i denotes the mean activation of unit i, and V AR denotes the variance operator. <p> in <ref> [22] </ref> and especially in [24]. Let us assume that the P i do not get trapped in local minima and perfectly learn the conditional expectations. It then turns out that the objective function V C (first given in [22]) is essentially equivalent to the following one (also given in [22]): X V AR (y i ) i;p p where y i denotes the mean activation of unit i, and V AR denotes the variance operator. The equivalence of (3) and (4) was observed by Peter Dayan, Richard Zemel and Alex Pouget (SALK Institute, 1992).
Reference: [23] <author> J. Schmidhuber. </author> <title> Learning unambiguous reduced sequence descriptions. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippman, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 291-298. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Recall that conventional recurrent networks failed to learn the task within 10 7 training sequences. In this special case, the speed-up factor obtained by adaptive redundancy reduction is at least 10 3 . More details and extensions of the principle above can be found in [21], <ref> [23] </ref>, [29], and especially in [24]. The next section describes a somewhat different kind of neural predictor for compressing natural text (as opposed to artificial symbol strings). 3 EXAMPLE 2: Text Compression The example from the previous section was based on artificial data from a stochastic automaton.
Reference: [24] <author> J. </author> <type> Schmidhuber. </type> <institution> Netzwerkarchitekturen, Zielfunktionen und Ketten-regel. Habilitationsschrift, Institut fur Informatik, Technische Universitat Munchen, </institution> <year> 1993. </year>
Reference-contexts: In this special case, the speed-up factor obtained by adaptive redundancy reduction is at least 10 3 . More details and extensions of the principle above can be found in [21], [23], [29], and especially in <ref> [24] </ref>. The next section describes a somewhat different kind of neural predictor for compressing natural text (as opposed to artificial symbol strings). 3 EXAMPLE 2: Text Compression The example from the previous section was based on artificial data from a stochastic automaton. <p> But the code units try to maximize the same (!) objective function the predictors try to minimize: V C = i;p p p Predictors and code units co-evolve by fighting each other. See details in [22] and especially in <ref> [24] </ref>. Let us assume that the P i do not get trapped in local minima and perfectly learn the conditional expectations. <p> The equivalence of (3) and (4) was observed by Peter Dayan, Richard Zemel and Alex Pouget (SALK Institute, 1992). See <ref> [24] </ref> for details. (4) gives some intuition about what is going on while (3) is maximized. The first term of (4) tends to enforce binary units, while the second term tends to make the conditional expectations equal to the unconditional expectations, thus encouraging statistical independence. <p> The redundancy (see the definition in section 1.2) corresponding to the original DEC dataset is 13:41. The redundancy corresponding to a 16-bit code discovered by the system is 2:5. See [14], [13], and <ref> [24] </ref> for details. This result corresponds to a dramatic reduction of redundant information, although the achieved value is not optimal. In many realistic cases, however, approximations of nonredundant codes should be satisfactory. It is intended to apply the method to the problem of unsupervised segmentation of real world images.
Reference: [25] <author> J. Schmidhuber. </author> <title> Discovering solutions with low Kolmogorov complexity and high generalization capability. </title> <editor> In A. Prieditis and S. Russell, editors, </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> pages 488-496. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA, </address> <year> 1995. </year>
Reference-contexts: Instead we need to look at Kolmogorov complexity or "algorithmic information" [8] [34] [2] [11] [3] and especially at its computationally tractable generalizations (e.g. [4] [10] [12] [35]) to properly treat general (as opposed to conventional statistical) sources of redundant information. This is a recent focus of my research <ref> [25, 26, 32, 31] </ref>. 6 ACKNOWLEDGMENTS Thanks to Peter Dayan, Richard Zemel and Alex Pouget for sharing their insight regarding the equivalence of equations (4) and (3). Thanks to David MacKay for directing my attention towards Arithmetic Coding. Thanks to Michael C. Mozer for many fruitful discussions.
Reference: [26] <author> J. Schmidhuber. </author> <title> Discovering neural nets with low Kolmogorov complexity and high generalization capability. </title> <booktitle> Neural Networks, </booktitle> <volume> 10(5) </volume> <pages> 857-873, </pages> <year> 1997. </year>
Reference-contexts: Instead we need to look at Kolmogorov complexity or "algorithmic information" [8] [34] [2] [11] [3] and especially at its computationally tractable generalizations (e.g. [4] [10] [12] [35]) to properly treat general (as opposed to conventional statistical) sources of redundant information. This is a recent focus of my research <ref> [25, 26, 32, 31] </ref>. 6 ACKNOWLEDGMENTS Thanks to Peter Dayan, Richard Zemel and Alex Pouget for sharing their insight regarding the equivalence of equations (4) and (3). Thanks to David MacKay for directing my attention towards Arithmetic Coding. Thanks to Michael C. Mozer for many fruitful discussions.
Reference: [27] <author> J. Schmidhuber and S. Heil. </author> <title> Predictive coding with neural nets: Application to text compression. </title> <editor> In G. Tesauro, D. S. Touretzky, and T. K. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 1047 - 1054. </pages> <publisher> MIT Press, </publisher> <address> Cambridge MA, </address> <year> 1995. </year> <month> 14 </month>
Reference-contexts: In collaboration with Stefan Heil (a former student at TUM), the method was applied to German newspaper articles <ref> [27, 28] </ref>. The results were compared to those obtained with standard encoding techniques provided by the operating system UNIX, namely "pack", "compress", and "gzip".
Reference: [28] <author> J. Schmidhuber and S. Heil. </author> <title> Sequential neural text compression. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 7(1) </volume> <pages> 142-146, </pages> <year> 1996. </year>
Reference-contexts: Can neural predictors offer something for redundancy 6 reduction in natural language? How do they compare to standard data com-pression algorithms? The method <ref> [28] </ref> reviewed in this section is an instance of a strategy known as "predictive coding" or "model-based coding". A neural predictor network P is trained to approximate the conditional probability distribution of possible characters, given the previous characters. <p> P 's outputs are fed into the Arithmetic Coding algorithm (e.g. [41]) that generates short codes for characters with low information content (characters with high predicted probability) and long codes for characters conveying a lot of information (highly unpredictable characters) <ref> [28] </ref>. 3.1 PREDICTING CONDITIONAL PROBABILITIES With the o*ine variant of the approach, P 's training phase is based on a set F of training files. Assume that the alphabet contains k possible characters z 1 ; z 2 ; : : : ; z k . <p> In collaboration with Stefan Heil (a former student at TUM), the method was applied to German newspaper articles <ref> [27, 28] </ref>. The results were compared to those obtained with standard encoding techniques provided by the operating system UNIX, namely "pack", "compress", and "gzip".
Reference: [29] <author> J. Schmidhuber, M. C. Mozer, and D. Prelinger. </author> <title> Continuous history compression. </title> <editor> In H. Huning, S. Neuhauser, M. Raus, and W. Ritschel, editors, </editor> <booktitle> Proc. of Intl. Workshop on Neural Networks, RWTH Aachen, </booktitle> <pages> pages 87-95. </pages> <address> Augustinus, </address> <year> 1993. </year>
Reference-contexts: Recall that conventional recurrent networks failed to learn the task within 10 7 training sequences. In this special case, the speed-up factor obtained by adaptive redundancy reduction is at least 10 3 . More details and extensions of the principle above can be found in [21], [23], <ref> [29] </ref>, and especially in [24]. The next section describes a somewhat different kind of neural predictor for compressing natural text (as opposed to artificial symbol strings). 3 EXAMPLE 2: Text Compression The example from the previous section was based on artificial data from a stochastic automaton.
Reference: [30] <author> J. Schmidhuber and D. Prelinger. </author> <title> Discovering predictable classifications. </title> <journal> Neural Computation, </journal> <volume> 5(4) </volume> <pages> 625-635, </pages> <year> 1993. </year>
Reference-contexts: This result corresponds to a dramatic reduction of redundant information, although the achieved value is not optimal. In many realistic cases, however, approximations of nonredundant codes should be satisfactory. It is intended to apply the method to the problem of unsupervised segmentation of real world images. See <ref> [30] </ref> for an application to simple stereo vision.
Reference: [31] <author> J. Schmidhuber, J. Zhao, and N. Schraudolph. </author> <title> Reinforcement learning with self-modifying policies. </title> <editor> In S. Thrun and L. Pratt, editors, </editor> <booktitle> Learning to learn, </booktitle> <pages> pages 293-309. </pages> <publisher> Kluwer, </publisher> <year> 1997. </year>
Reference-contexts: Many potential sources of redundant information have been neglected, however (see, e.g., [6]). Among the things I did not address is the possibility of redundancy among learning strategies <ref> [31] </ref>. Suppose learning algorithm A is good at solving problems of class A but tends to fail with problems of class B. Suppose learning algorithm B is good at solving problems of class B but tends to fail with problems of class A. <p> This implies that there is "algorithmic redundancy" between A and B. Variants of algorithmic redundancy allow for things like "learning by analogy", "learning by chunking", "learning how to learn", etc. <ref> [31] </ref>. Shannon information, however, is not the right concept to exploit the potential benefits of algorithmic redundancy. <p> Instead we need to look at Kolmogorov complexity or "algorithmic information" [8] [34] [2] [11] [3] and especially at its computationally tractable generalizations (e.g. [4] [10] [12] [35]) to properly treat general (as opposed to conventional statistical) sources of redundant information. This is a recent focus of my research <ref> [25, 26, 32, 31] </ref>. 6 ACKNOWLEDGMENTS Thanks to Peter Dayan, Richard Zemel and Alex Pouget for sharing their insight regarding the equivalence of equations (4) and (3). Thanks to David MacKay for directing my attention towards Arithmetic Coding. Thanks to Michael C. Mozer for many fruitful discussions.
Reference: [32] <author> J. Schmidhuber, J. Zhao, and M. Wiering. </author> <title> Shifting inductive bias with success-story algorithm, adaptive Levin search, and incremental self-improvement. </title> <journal> Machine Learning, </journal> <volume> 28 </volume> <pages> 105-130, </pages> <year> 1997. </year>
Reference-contexts: Instead we need to look at Kolmogorov complexity or "algorithmic information" [8] [34] [2] [11] [3] and especially at its computationally tractable generalizations (e.g. [4] [10] [12] [35]) to properly treat general (as opposed to conventional statistical) sources of redundant information. This is a recent focus of my research <ref> [25, 26, 32, 31] </ref>. 6 ACKNOWLEDGMENTS Thanks to Peter Dayan, Richard Zemel and Alex Pouget for sharing their insight regarding the equivalence of equations (4) and (3). Thanks to David MacKay for directing my attention towards Arithmetic Coding. Thanks to Michael C. Mozer for many fruitful discussions.
Reference: [33] <author> C. E. Shannon. </author> <title> A mathematical theory of communication (parts I and II). </title> <journal> Bell System Technical Journal, </journal> <volume> XXVII:379-423, </volume> <year> 1948. </year>
Reference-contexts: One might speculate about whether the brain uses a similar principle based on "code neurons" trying to escape the predictions of "predictor neurons". 5 OUTLOOK This paper presented a number of possibilities for "neural" redundancy reduction based on Shannon's concept of information <ref> [33] </ref>. Many potential sources of redundant information have been neglected, however (see, e.g., [6]). Among the things I did not address is the possibility of redundancy among learning strategies [31].
Reference: [34] <author> R.J. Solomonoff. </author> <title> A formal theory of inductive inference. Part I. </title> <journal> Information and Control, </journal> <volume> 7 </volume> <pages> 1-22, </pages> <year> 1964. </year>
Reference-contexts: Variants of algorithmic redundancy allow for things like "learning by analogy", "learning by chunking", "learning how to learn", etc. [31]. Shannon information, however, is not the right concept to exploit the potential benefits of algorithmic redundancy. Instead we need to look at Kolmogorov complexity or "algorithmic information" [8] <ref> [34] </ref> [2] [11] [3] and especially at its computationally tractable generalizations (e.g. [4] [10] [12] [35]) to properly treat general (as opposed to conventional statistical) sources of redundant information.
Reference: [35] <author> O. Watanabe. </author> <title> Kolmogorov complexity and computational complexity. </title> <booktitle> EATCS Monographs on Theoretical Computer Science, </booktitle> <publisher> Springer, </publisher> <year> 1992. </year>
Reference-contexts: Shannon information, however, is not the right concept to exploit the potential benefits of algorithmic redundancy. Instead we need to look at Kolmogorov complexity or "algorithmic information" [8] [34] [2] [11] [3] and especially at its computationally tractable generalizations (e.g. [4] [10] [12] <ref> [35] </ref>) to properly treat general (as opposed to conventional statistical) sources of redundant information. This is a recent focus of my research [25, 26, 32, 31]. 6 ACKNOWLEDGMENTS Thanks to Peter Dayan, Richard Zemel and Alex Pouget for sharing their insight regarding the equivalence of equations (4) and (3).
Reference: [36] <author> P. J. Werbos. </author> <title> Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. </title> <type> PhD thesis, </type> <institution> Harvard University, </institution> <year> 1974. </year>
Reference: [37] <author> P. J. Werbos. </author> <title> Generalization of backpropagation with application to a recurrent gas market model. Neural Networks, </title> <type> 1, </type> <year> 1988. </year>
Reference-contexts: Otherwise it will not be able to classify correctly. 2.1.1 SIMULATION We use local input representation. A feedforward neural net cannot learn the problem, due to the arbitrary time lags that may occur. Instead, we need something like a recurrent net (e.g. [18] <ref> [37] </ref> [17] [40] [15] [39] [38] [20] [7] ).
Reference: [38] <author> R. J. Williams. </author> <title> Complexity of exact gradient computation algorithms for recurrent neural networks. </title> <type> Technical Report Technical Report NU-CCS-89-27, </type> <institution> Boston: Northeastern University, College of Computer Science, </institution> <year> 1989. </year>
Reference-contexts: Otherwise it will not be able to classify correctly. 2.1.1 SIMULATION We use local input representation. A feedforward neural net cannot learn the problem, due to the arbitrary time lags that may occur. Instead, we need something like a recurrent net (e.g. [18] [37] [17] [40] [15] [39] <ref> [38] </ref> [20] [7] ).
Reference: [39] <author> R. J. Williams and J. Peng. </author> <title> An efficient gradient-based algorithm for online training of recurrent network trajectories. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 491-501, </pages> <year> 1990. </year>
Reference-contexts: Otherwise it will not be able to classify correctly. 2.1.1 SIMULATION We use local input representation. A feedforward neural net cannot learn the problem, due to the arbitrary time lags that may occur. Instead, we need something like a recurrent net (e.g. [18] [37] [17] [40] [15] <ref> [39] </ref> [38] [20] [7] ).
Reference: [40] <author> R. J. Williams and D. Zipser. </author> <title> Experimental analysis of the real-time recurrent learning algorithm. </title> <journal> Connection Science, </journal> <volume> 1(1) </volume> <pages> 87-111, </pages> <year> 1989. </year>
Reference-contexts: Otherwise it will not be able to classify correctly. 2.1.1 SIMULATION We use local input representation. A feedforward neural net cannot learn the problem, due to the arbitrary time lags that may occur. Instead, we need something like a recurrent net (e.g. [18] [37] [17] <ref> [40] </ref> [15] [39] [38] [20] [7] ).
Reference: [41] <author> I. H. Witten, R. M. Neal, and J. G. Cleary. </author> <title> Arithmetic coding for data compression. </title> <journal> Communications of the ACM, </journal> <volume> 30(6) </volume> <pages> 520-540, </pages> <year> 1987. </year> <month> 15 </month>
Reference-contexts: A neural predictor network P is trained to approximate the conditional probability distribution of possible characters, given the previous characters. P 's outputs are fed into the Arithmetic Coding algorithm (e.g. <ref> [41] </ref>) that generates short codes for characters with low information content (characters with high predicted probability) and long codes for characters conveying a lot of information (highly unpredictable characters) [28]. 3.1 PREDICTING CONDITIONAL PROBABILITIES With the o*ine variant of the approach, P 's training phase is based on a set F <p> The estimate of P (c f f f m1 ) is given by P f m (k). The code of c f m , the bitstring code (c f m ), is generated by feeding the probability estimates of the predictor into the Arithmetic Coding algorithm (see e.g. <ref> [41] </ref>). code (c f m ) is written into the compressed file. The information in the compressed file is sufficient for reconstructing the original file. <p> (sequentially) emits its output P f m based on the n previous characters, where the c f l with n &lt; l &lt; m were gained sequentially by feeding the approximations P f the probabilities P (c f f f l1 ) into the inverse Arithmetic Coding procedure ( e.g. <ref> [41] </ref>). The latter is able to correctly decode c f l from code (c f In other words, to correctly decode some character, we first need to decode all previous characters. 3.3 SIMULATIONS Our current computing environment prohibits extensive experimental evaluations of the method above.
Reference: [42] <author> A. Wyner and J. Ziv. </author> <title> Fixed data base version of the Lempel-Ziv data compression algorithm. </title> <journal> IEEE Transactions Information Theory, </journal> <volume> 37 </volume> <pages> 878-880, </pages> <year> 1991. </year>
Reference-contexts: The corresponding decoding algorithms are "unpack", "uncompress", and "gunzip", respectively. "pack" is based on Huffman-Coding (e.g. [5]), while "compress" and "gzip" are based on the Lempel-Ziv technique [43]. As the file size goes to infinity, Lempel-Ziv becomes asymptotically optimal in a certain information theoretic sense <ref> [42] </ref>. The training set for the predictor was given by a set of 40 articles from the newspaper Munchner Merkur, each containing between 10000 and 20000 characters.
Reference: [43] <author> J. Ziv and A. Lempel. </author> <title> A universal algorithm for sequential data compression. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-23(5):337-343, </volume> <year> 1977. </year> <month> 16 </month>
Reference-contexts: The results were compared to those obtained with standard encoding techniques provided by the operating system UNIX, namely "pack", "compress", and "gzip". The corresponding decoding algorithms are "unpack", "uncompress", and "gunzip", respectively. "pack" is based on Huffman-Coding (e.g. [5]), while "compress" and "gzip" are based on the Lempel-Ziv technique <ref> [43] </ref>. As the file size goes to infinity, Lempel-Ziv becomes asymptotically optimal in a certain information theoretic sense [42]. The training set for the predictor was given by a set of 40 articles from the newspaper Munchner Merkur, each containing between 10000 and 20000 characters.
References-found: 43

