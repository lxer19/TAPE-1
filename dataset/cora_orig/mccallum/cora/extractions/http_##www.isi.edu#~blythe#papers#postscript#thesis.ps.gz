URL: http://www.isi.edu/~blythe/papers/postscript/thesis.ps.gz
Refering-URL: http://www.isi.edu/~blythe/papers.html
Root-URL: http://www.isi.edu
Title: Planning under Uncertainty in Dynamic Domains  
Author: Jim Blythe Jaime Carbonell Manuela Veloso Reid Simmons Judea Pearl, 
Degree: Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy. Thesis Committee:  
Note: c 1997 Jim Blythe The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of any other parties.  
Address: Pittsburgh, PA  
Affiliation: Computer Science Department Carnegie Mellon University  U.C.L.A.  
Date: May 25, 1998  
Abstract-found: 0
Intro-found: 1
Reference: [Agre & Chapman 1987] <author> Agre, P. E., and Chapman, D. </author> <year> 1987. </year> <title> Pengi: An implementation of a theory of activity. </title> <booktitle> In National Conference on Artificial Intelligence. </booktitle>
Reference-contexts: The planner does not know if the traffic lights will be red or green, or what traffic will be encountered. This led some researchers to abandon programs that choose steps in advance in favour of "reactive" programs that choose each step as it is to be made <ref> [Agre & Chapman 1987; Schoppers 1989b] </ref>. However it is often necessary to make contingency plans ahead of time even if not every situation can be predicted and planned for. <p> These systems have no explicit representation of a global plan and some, such as Brook's subsumption architecture [Brooks 1986] and Agre and Chapman's Pengi <ref> [Agre & Chapman 1987] </ref>, have little or no internal state. The subsumption architecture arranges reactive sub-sytems into hierarchies, so that higher-level behaviour is achieved by one system over-riding, or subsuming, a more basic system.
Reference: [Allen et al. 1991] <author> Allen, J. F.; Kautz, H. A.; Pelavin, R. N.; and Tenenberg, J. D. </author> <year> 1991. </year> <title> Reasoning About Plans. </title> <address> 2929 Campus Drive, Suite 260, San Mateo, Ca 94403: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: ((add (at &lt;barge&gt; &lt;dest&gt;)) (del (at &lt;barge&gt; &lt;source&gt;)) (del (operational &lt;barge&gt;))))) ((0.1 () ((add (at &lt;barge&gt; &lt;dest&gt;)) (del (at &lt;barge&gt; &lt;source&gt;)))) (0.9 () ((add (at &lt;barge&gt; &lt;dest&gt;)) (del (at &lt;barge&gt; &lt;source&gt;)) (del (operational &lt;barge&gt;)))))))) a duration and a probability distribution of possible outcomes. which the prodigy 4.0 representation rests e.g. <ref> [Allen et al. 1991; Kartha 1995; Shanahan 1995; Baral 1995] </ref>. In this thesis I have chosen a simple and restricted language for events that is still adequate for planning with some interesting domains.
Reference: [Ambite & Knoblock 1997] <author> Ambite, J. L., and Knoblock, C. A. </author> <year> 1997. </year> <title> Planning by rewriting: Efficiently generating high-quality plans. </title> <booktitle> In Proc. Ord National Conference on Artificial Intelligence. </booktitle> <publisher> AAAI Press. </publisher>
Reference-contexts: Despite promising work in planning under uncertainty with partial-order and hierarchical task network planners [Draper, Hanks, & Weld 1994; Haddawy, Doan, & Goodwin 1995], exogenous events have not yet been addressed in these systems. Experience with Weaver suggests that a planner based on iterative repair principles such as <ref> [Ambite & Knoblock 1997; Kautz & Selman 1996] </ref> could be very appropriate for probabilistic planning. * Weaver computes a lower bound on the probability of success of a plan, improving its speed by (1) ignoring fortuitous exogenous events that are not explicitly used by the conditional planner and (2) assuming that
Reference: [Bacchus et al. 1997] <author> Bacchus, F.; Grove, A. J.; Halpern, J. Y.; and Koller, D. </author> <year> 1997. </year> <title> From statistical knowledge bases to degrees of belief. </title> <booktitle> Artificial Intelligence 87 </booktitle> <pages> 75-143. </pages>
Reference-contexts: This approach was chosen partly because the existing techniques did not handle probabilistic reasoning well, although the random-worlds approach is promising <ref> [Bacchus et al. 1997] </ref>, and partly to make it easier to compare the planning system developed in this thesis with other approaches to planning under uncertainty, many of which are based on Markov decision processes [Boutilier & Dearden 1994; Dean & Lin 1995; Littman 1996].
Reference: [Bagchi, Biswas, & Kawamura 1994] <author> Bagchi, S.; Biswas, G.; and Kawamura, K. </author> <year> 1994. </year> <title> Generating plans to succeed in uncertain environments. </title> <editor> In Hammond, K., ed., </editor> <booktitle> Proc. Second International Conference on Artificial Intelligence Planning Systems, </booktitle> <pages> 1-6. </pages> <institution> University of Chicago, </institution> <address> Illinois: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Cassandra [Pryor & Collins 1993; 1996] is another planner based on snlp and uses an explicit representation of decision steps. Bagchi et al. describe a planner that uses a spreading activation technique to choose actions with highest expected utility <ref> [Bagchi, Biswas, & Kawamura 1994] </ref>. Onder and Pollack also extend snlp with explicit reasoning about the contingencies to plan for [Onder & Pollack 1997]. Haddawy's thesis work [Haddawy 1991] introduces a logic with extensions to represent probabilities of terms, intended for representing plans.
Reference: [Baral 1995] <author> Baral, C. </author> <year> 1995. </year> <title> Reasoning about actions: Non-deterministic effects, constraints, and qualification. </title> <booktitle> In Proc. 14th International Joint Conference on Artificial Intelligence, </booktitle> <pages> 2017-2024. </pages> <address> Montreal, Quebec: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: ((add (at &lt;barge&gt; &lt;dest&gt;)) (del (at &lt;barge&gt; &lt;source&gt;)) (del (operational &lt;barge&gt;))))) ((0.1 () ((add (at &lt;barge&gt; &lt;dest&gt;)) (del (at &lt;barge&gt; &lt;source&gt;)))) (0.9 () ((add (at &lt;barge&gt; &lt;dest&gt;)) (del (at &lt;barge&gt; &lt;source&gt;)) (del (operational &lt;barge&gt;)))))))) a duration and a probability distribution of possible outcomes. which the prodigy 4.0 representation rests e.g. <ref> [Allen et al. 1991; Kartha 1995; Shanahan 1995; Baral 1995] </ref>. In this thesis I have chosen a simple and restricted language for events that is still adequate for planning with some interesting domains.
Reference: [Blythe & Fink 1997] <author> Blythe, J., and Fink, E. </author> <year> 1997. </year> <title> Rasputin: A complete bidirectional-chaining planner. </title> <type> Technical Report forthcoming, </type> <institution> Computer Science Department, Carnegie Mellon University. </institution>
Reference-contexts: Prodigy can make use of protection nodes independently of Weaver to ensure that some chosen conditional effects of a step will not occur. In fact, some mechanism of this kind is required for planners in the Prodigy family to be complete <ref> [Blythe & Fink 1997] </ref>. As an illustration, suppose that the Move-Barge and Pump-Oil operators are extended so that Pump-Oil requires that the barge be operational, and Move-Barge may conditionally delete that the barge is operational if the literal barge-ready is false. <p> This is described in <ref> [Blythe & Fink 1997] </ref>. Table 4.1 shows the sequence of nodes in the search tree that correspond to a solution based to the new problem based on the previous solution sequence in Table 3.2. 4.1. <p> In earlier work, completeness is shown for a version of prodigy 4.0 in a limited case when no lisp functions are used in the bindings of operators and no control rules are used <ref> [Blythe & Fink 1997] </ref>. Extensions to control 4.2. <p> A version of Prodigy that can add preventive steps to avoid undesired conditional effects in plans can be shown to be complete when there are no lisp functions used in operator bindings or control rules <ref> [Blythe & Fink 1997] </ref> 1 .
Reference: [Blythe & Mitchell 1989] <author> Blythe, J., and Mitchell, T. </author> <year> 1989. </year> <title> On becoming reactive. </title> <booktitle> In International Conference on Machine Learning. </booktitle>
Reference-contexts: Systems have been developed for this purpose that combine reactive execution with classical planning, using the latter when no pre-programmed response is available for some contingency. Both the Theo-agent <ref> [Blythe & Mitchell 1989] </ref> and the "anytime synthetic projection" technique of Drummond and Bresina [Drummond & Bresina 1990] follow reactive rules if they are present, and otherwise fall back on planning and then compile the resulting plan into new reactive rules to be used in future episodes.
Reference: [Blythe & Veloso 1992] <author> Blythe, J., and Veloso, M. </author> <year> 1992. </year> <title> An analysis of search techniques for a totally-ordered nonlinear planner. </title> <editor> In Hendler, J., ed., </editor> <title> Proc. </title> <journal> First International Conference on Artificial Intelligence Planning Systems. </journal> <note> Available as http://www.cs.cmu.edu/ ~ jblythe/papers/search.ps. 157 158 BIBLIOGRAPHY </note>
Reference-contexts: The choice of this particular sequence was largely due to search heuristics that prodigy 4.0 uses, which will not be discussed here (but see <ref> [Blythe & Veloso 1992] </ref> and [Carbonell et al. 1992]). 3.2 A representation for planning under uncer tainty In Section 3.1 I provided a brief description of planning domains, planning problems and plans in prodigy 4.0. <p> This can be viewed as a probabilistic version of the minimum-unsolved-preconditions heuristic used in Prodigy <ref> [Blythe & Veloso 1992] </ref> although a more faithful version would minimize the expected number of unsolved preconditions. These heuristics all make estimates using the current state, which arises from applying the operators whose order in the plan prefix is already chosen.
Reference: [Blythe & Veloso 1996] <author> Blythe, J., and Veloso, M. </author> <year> 1996. </year> <title> Learning to improve uncertainty handling in a hybrid planning system. </title> <editor> In Kasif, S., ed., </editor> <booktitle> AAAI Fall Symposium on Learning Complex Behaviors in Intelligent Adaptive Systems. </booktitle>
Reference-contexts: Some preliminary work has shown the potential to compile experience gained from the probabilistic evaluation of plans into search control for the planner although it largely avoids probabilistic evaluations <ref> [Blythe & Veloso 1996] </ref>. This approach may be one way to improve the integration of the plan creation and plan evaluations modules. 8.2.
Reference: [Blythe & Veloso 1997] <author> Blythe, J., and Veloso, M. </author> <year> 1997. </year> <title> Using analogy in conditional planners. </title> <booktitle> In Proc. Ord National Conference on Artificial Intelligence. </booktitle> <publisher> AAAI Press. </publisher>
Reference-contexts: Further experiments are described in Chapter 7, and some comparisons are made with other probabilistic planning systems in Chapter 2. Some of the work described in this section was done jointly with Manuela Veloso <ref> [Blythe & Veloso 1997] </ref>. 86 Chapter 6. Efficiency improvements in planning 6.2.1 The need to share search effort within the conditional planner Consider a simple planning problem in which a package is to be loaded into a truck.
Reference: [Bonissone & Dutta 1990] <author> Bonissone, P., and Dutta, S. </author> <year> 1990. </year> <title> Merging strategic and tactical planning in dynamic, uncertain environments. In DARPA Workshop on Innovative Approaches to Planning, </title> <journal> Scheduling and Control, </journal> <pages> 379-389. </pages>
Reference-contexts: For this purpose it is useful to have some way to compare the different contingencies to find the most important. A probabilistic representation of the sources of uncertainty is well suited for this, although some have used a Dempster-Shaffer formalism [Mansell 1993], fuzzy reasoning <ref> [Bonissone & Dutta 1990] </ref> or a minimax approach [Koenig & Simmons 1995]. Work on extensions to planning systems using decision theory began shortly after the initial work on strips [Feldman & Sproull 1977], but this line of work was largely discontinued until the 1990's.
Reference: [Boutilier & Dearden 1994] <author> Boutilier, C., and Dearden, R. </author> <year> 1994. </year> <title> Using abstractions for decision-theoretic planning with time constraints. </title> <booktitle> In Proc. Ord National Conference on Artificial Intelligence, </booktitle> <pages> 1016-1022. </pages> <publisher> AAAI Press. </publisher>
Reference-contexts: While the envelope extension method ignores portions of the state space, other techniques have considered abstractions of the state space that try to group together sets of states that behave similarly under the chosen actions of the optimal policy. Boutilier and Dearden <ref> [Boutilier & Dearden 1994] </ref> assume a representation for actions that is similar to that used in Buridan [Kushmerick, Hanks, & Weld 1994] described in Section 2.3 and a state utility function that is described in terms of domain literals. <p> the existing techniques did not handle probabilistic reasoning well, although the random-worlds approach is promising [Bacchus et al. 1997], and partly to make it easier to compare the planning system developed in this thesis with other approaches to planning under uncertainty, many of which are based on Markov decision processes <ref> [Boutilier & Dearden 1994; Dean & Lin 1995; Littman 1996] </ref>. Exogenous events have a very similar representation to operators. Figure 3.6 shows 30 Chapter 3. Planning under Uncertainty an example of a simple exogenous event called Weather-Brightens.
Reference: [Boutilier, Brafman, & Geib 1997] <author> Boutilier, C.; Brafman, R. I.; and Geib, C. </author> <year> 1997. </year> <title> Prioritized goal decomposition of markov decision processes: Toward a synthesis of classical and decision theoretic planning. </title> <booktitle> In Proc. 15th International Joint Conference on Artificial Intelligence. </booktitle> <address> Nagoya, Japan: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: occur at some fixed time, but not about uncertain events. 2.4 Approaches based on Markov decision pro cesses In the past five years, much of the research activity in AI planning under uncertainty has built on standard techniques for solving Markov decision problems (mdps) and partially-observable Markov decision problems pomdps <ref> [Dean et al. 1995; Littman 1996; Boutilier, Brafman, & Geib 1997] </ref>. This formalism has the advantage of a sound mathematical underpinning, and the standard solution techniques aim at an optimal policy while most systems based on classical planning try to meet a lower bound.
Reference: [Boutilier, Dean, & Hanks 1995] <author> Boutilier, C.; Dean, T.; and Hanks, S. </author> <year> 1995. </year> <title> Planning under uncertainty: structural assumptions and computational leverage. </title> <editor> In Ghallab, M., and Milani, A., eds., </editor> <booktitle> New Directions in AI Planning, </booktitle> <pages> 157-172. </pages> <address> As-sissi, Italy: </address> <publisher> IOS Press. </publisher>
Reference-contexts: This thesis pursues an approach based on classical planning, but uses an mdp representation to describe planning problems and their solutions. 2.4.1 Overview of Markov Decision Processes This description of Markov decision processes follows [Littman 1996] and <ref> [Boutilier, Dean, & Hanks 1995] </ref>.
Reference: [Boutilier, Dearden, & Goldszmidt 1995] <author> Boutilier, C.; Dearden, R.; and Gold-szmidt, M. </author> <year> 1995. </year> <title> Exploiting structure in policy construction. </title> <booktitle> In Proc. 14th International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1104-1111. </pages> <address> Montreal, Quebec: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Lin and Dean assume the partition of the state space is given by some external oracle. Boutilier et al. extend modified policy iteration to propose a technique called structured policy iteration that makes use of a structured action representation in the form of 2-stage Bayesian networks <ref> [Boutilier, Dearden, & Goldszmidt 1995] </ref>. The representation of the policy and utility functions are also structured in their approach, using decision trees.
Reference: [Brafman 1997] <author> Brafman, R. </author> <year> 1997. </year> <title> A heuristic variable grid solution method of pomdps. </title> <booktitle> In Proc. Ord National Conference on Artificial Intelligence, </booktitle> <pages> 727-733. </pages> <publisher> AAAI Press. </publisher>
Reference-contexts: Parr and Russell use a smooth approximation of the value function that can be updated with gradient 18 Chapter 2. Related work descent [Parr & Russell 1995]. Brafman introduces a grid-based method in <ref> [Brafman 1997] </ref>. Although work on pomdps is promising, it is still preliminary, and the largest completely solved pomdps have about 10 states [Brafman 1997]. <p> Related work descent [Parr & Russell 1995]. Brafman introduces a grid-based method in <ref> [Brafman 1997] </ref>. Although work on pomdps is promising, it is still preliminary, and the largest completely solved pomdps have about 10 states [Brafman 1997]. Chapter 3 Planning under Uncertainty In this chapter I provide an overview of the prodigy 4.0 planning algorithm and discuss extensions made to allow it to create and evaluate plans under conditions of uncertainty. I present extensions to prodigy 4.0's model to represent uncertainty in Section 3.2.
Reference: [Brooks 1986] <author> Brooks, R. </author> <year> 1986. </year> <title> A robust layered control system for a mobile robot. </title> <journal> IEEE Journal of Robotics and Automation RA-2:14-23. </journal>
Reference-contexts: Reactive planning and approaches that mix planning with execution. 11 ronment and chooses an action based on this information, similar to a policy on a Markov decision process described in Section 2.4. These systems have no explicit representation of a global plan and some, such as Brook's subsumption architecture <ref> [Brooks 1986] </ref> and Agre and Chapman's Pengi [Agre & Chapman 1987], have little or no internal state. The subsumption architecture arranges reactive sub-sytems into hierarchies, so that higher-level behaviour is achieved by one system over-riding, or subsuming, a more basic system.
Reference: [Carbonell et al. 1992] <author> Carbonell, J. G.; Blythe, J.; Etzioni, O.; Gil, Y.; Joseph, R.; Kahn, D.; Knoblock, C.; Minton, S.; Perez, A.; Reilly, S.; Veloso, M.; and Wang, M. </author> <year> 1992. </year> <title> PRODIGY4.0: The manual and tutorial. </title> <type> Technical Report CMU-CS-92-150, </type> <institution> School of Computer Science, Carnegie Mellon University. </institution>
Reference-contexts: The effects specify literals that are either 1 A domain also contains inference rules, which are very similar to operators but add inferences to the state rather than changing it. Full details can be found in <ref> [Carbonell et al. 1992] </ref>. 19 20 Chapter 3. Planning under Uncertainty added to (made true in) the state or deleted from (made false in) the state. <p> Conditional effects may also be specified, in which the conditional test has the same form as the preconditions of the operator. An example of an operator is shown in details on the syntax of planning domains and problems in prodigy 4.0 can be found in <ref> [Carbonell et al. 1992] </ref>. (Operator Move-Barge (preconds ((&lt;barge&gt; Barge) (&lt;from&gt; Place) (&lt;to&gt; (and Place ( diff &lt;from&gt; &lt;to&gt;)))) (at &lt;barge&gt; &lt;from&gt;)) (effects () ((del (at &lt;barge&gt; &lt;from&gt;)) (add (at &lt;barge&gt; &lt;to&gt;))))) bound to variable &lt;from&gt; to the location bound to variable &lt;to&gt;. <p> The choice of this particular sequence was largely due to search heuristics that prodigy 4.0 uses, which will not be discussed here (but see [Blythe & Veloso 1992] and <ref> [Carbonell et al. 1992] </ref>). 3.2 A representation for planning under uncer tainty In Section 3.1 I provided a brief description of planning domains, planning problems and plans in prodigy 4.0.
Reference: [Cassandra, Kaelbling, & Littman 1994] <author> Cassandra, A. R.; Kaelbling, L. P.; and Littman, M. L. </author> <year> 1994. </year> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proc. Ord National Conference on Artificial Intelligence, </booktitle> <pages> 1023-1028. </pages> <publisher> AAAI Press. BIBLIOGRAPHY 159 </publisher>
Reference-contexts: Cassandra et al. introduce the witness algorithm for solving pomdps <ref> [Cassandra, Kaelbling, & Littman 1994] </ref>.
Reference: [Chapman 1987] <author> Chapman, D. </author> <year> 1987. </year> <title> Planning for conjunctive goals. </title> <booktitle> Artificial Intelligence 32 </booktitle> <pages> 333-378. </pages>
Reference-contexts: Introduction Chapter 2 Related work Work in automatic planning began early in the field of artificial intelligence, with such programs as GPS [Newell & Simon 1963] and strips [Fikes & Nilsson 1971]. As well as introducing new planning algorithms <ref> [Chapman 1987; McAllester & Rosenblitt 1991] </ref>, later work considered more general conditions, for example where external events could take place during plan execution [Vere 1983], where the domain theory is incomplete or incorrect [Gil 1992; Wang 1996; Gervasio 1996] or where the agent must consider the relative quality of alternative plans <p> A more detailed description of Prodigy's action representation, which is an example of a strips-like representation, can be found in Section 3.1. Work in classical planning has typically focussed on improving the efficiency with which a plan is created. For example partial-order planners, introduced in the late eighties <ref> [Chapman 1987; McAllester & Rosenblitt 1991; Penberthy & Weld 1992] </ref>, attempt to reduce the search space using a "least commitment" principle where steps in a plan are only ordered with respect to one another as required to prove that the plan is successful.
Reference: [Cooper 1990] <author> Cooper, G. F. </author> <year> 1990. </year> <title> The computational complexity of probabilistic inference using bayesian belief networks. </title> <booktitle> Artificial Intelligence 42 </booktitle> <pages> 393-405. </pages>
Reference-contexts: The time to evaluate this belief net would be at least linear in the length of these paths and frequently worse, since belief net evaluation is NP-hard in general <ref> [Cooper 1990] </ref>. One can use the ability to describe exogenous events using Markov chains to escape this difficulty. The probability distribution of states in a Markov chain at some future time t given the distribution at time 0 can be computed in time logarithmic in t. <p> Algorithms for computing the probability distribution and for propagating observations on the variables as evidence to update the posterior distributions of the other variables can make use of the dependency information to improve efficiency [Pearl 1988; Lauritzen & Spiegelhalter 1988]. This problem is, however, NP-hard <ref> [Cooper 1990] </ref>. Formally, if a belief net models a probability distribution, then dependence relationships among variables in the distribution can be read from the graph using the criterion of d-separation. <p> Although there are methods for evaluating belief nets that can make efficient use of the structure that they show for the probability distribution, evaluating belief nets is NP-hard in the general case <ref> [Cooper 1990] </ref>. Evaluating the belief net can quickly become the bottleneck in the process described in Chapter 4. In this chapter I describe some techniques that can reduce the computation required to evaluate the plan.
Reference: [Darwiche 1995] <author> Darwiche, A. </author> <year> 1995. </year> <title> Conditioning methods for exact and approximate inference in causal networks. </title> <editor> In Besnard, P., and Hanks, S., eds., </editor> <booktitle> Proc. Eleventh Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> 99-107. </pages> <address> Montreal, Quebec: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [Dean & Lin 1995] <author> Dean, T., and Lin, S.-H. </author> <year> 1995. </year> <title> Decomposition techniques for planning in stochastic domains. </title> <booktitle> In Proc. 14th International Joint Conference on Artificial Intelligence, 1121 - 1127. </booktitle> <address> Montreal, Quebec: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Lin and Dean further refine this idea by splitting the mdp into subsets and allowing a different abstraction of the states to be considered in each one <ref> [Dean & Lin 1995] </ref>. This approach can have extra power because typically different literals may be relevant in different parts of the state space. However there is an added cost to re-combining the separate pieces unless they happen to decompose very cleanly. <p> the existing techniques did not handle probabilistic reasoning well, although the random-worlds approach is promising [Bacchus et al. 1997], and partly to make it easier to compare the planning system developed in this thesis with other approaches to planning under uncertainty, many of which are based on Markov decision processes <ref> [Boutilier & Dearden 1994; Dean & Lin 1995; Littman 1996] </ref>. Exogenous events have a very similar representation to operators. Figure 3.6 shows 30 Chapter 3. Planning under Uncertainty an example of a simple exogenous event called Weather-Brightens.
Reference: [Dean & Wellman 1991] <author> Dean, T. L., and Wellman, M. P. </author> <year> 1991. </year> <title> Planning and Control. </title> <publisher> Morgan Kaufmann. </publisher>
Reference: [Dean et al. 1993] <author> Dean, T.; Kaelbling, L. P.; Kirman, J.; and Nicholson, A. </author> <year> 1993. </year> <title> Planning with deadlines in stochastic domains. </title> <booktitle> In National Conference on Artificial Intelligence, National Conference on Artificial Intelligence. </booktitle>
Reference-contexts: Attempts to build on these and other techniques for solving mdps have concentrated on ways to gain leverage from the structure of the planning problem to reduce the computation time require. Dean et al. used policy iteration in a restricted state space called an envelope <ref> [Dean et al. 1993] </ref>. A subset of the states is selected, and each transition in the mdp that leaves the subset is replaced with a new transition to a new state OUT with zero reward. No transitions leave the OUT state.
Reference: [Dean et al. 1995] <author> Dean, T.; Kaelbling, L.; Kirman, J.; and Nicholson, A. </author> <year> 1995. </year> <title> Planning under time constraints in stochastic domains. </title> <journal> Artificial Intelligence 76(1-2):35-74. </journal>
Reference-contexts: occur at some fixed time, but not about uncertain events. 2.4 Approaches based on Markov decision pro cesses In the past five years, much of the research activity in AI planning under uncertainty has built on standard techniques for solving Markov decision problems (mdps) and partially-observable Markov decision problems pomdps <ref> [Dean et al. 1995; Littman 1996; Boutilier, Brafman, & Geib 1997] </ref>. This formalism has the advantage of a sound mathematical underpinning, and the standard solution techniques aim at an optimal policy while most systems based on classical planning try to meet a lower bound. <p> The algorithm converges to an optimal policy considerably more quickly than standard policy iteration on the whole state space, but as the authors point out <ref> [Dean et al. 1995] </ref>, it makes some assumptions which limit its applicability, including that of a sparse mdp in which each state has only a small number of outward transitions.
Reference: [Dean 1994] <author> Dean, T. </author> <year> 1994. </year> <title> Decision-theoretic planning and markov decision processes. </title> <note> To be submitted to AI Magazine. </note>
Reference: [Dearden & Boutilier 1997] <author> Dearden, R., and Boutilier, C. </author> <year> 1997. </year> <title> Abstraction and approximate decision theoretic planning. </title> <note> Artificial Intelligence To appear. </note>
Reference-contexts: Related work all the states of the system. Work to make mdp approaches tractable has attempted to incorporate ideas from classical planning and other areas of AI to exploit any underlying structure in the state space <ref> [Dearden & Boutilier 1997] </ref> as well as to relax the requirement of optimalty [Parr & Russell 1995]. In this section I give a brief overview of Markov decision processes and survey some of the work in this area.
Reference: [Desimone & Agosta 1994] <author> Desimone, R. V., and Agosta, J. M. </author> <year> 1994. </year> <title> Spill response system configuration study | final report. </title> <type> Technical Report ITAD-4368-FR-94-236, </type> <institution> SRI International. </institution>
Reference-contexts: In this chapter I present an experimental demonstration of these techniques in the domain of oil-spill clean-up. This is a real-world domain, consisting of 62 operator and inference rule schemata, that was originally developed by SRI for the US Coast Guard <ref> [Desimone & Agosta 1994] </ref>. In the next section I provide an overview of the oil-spill domain. Sections 7.2 and 7.3 discuss how Weaver improves plans in this domain. <p> These problems are mod-elled with 183 objects belonging to subtypes of place, 55 belonging to subtypes of equipment, 30 belonging to subtypes of vessel and 119 other objects. Details can be found in <ref> [Desimone & Agosta 1994] </ref>. <p> The geographic regions and the specific properties of the response equipment in this problem correspond exactly to objects in the scenarios designed by SRI in conjunction with the coast guard <ref> [Desimone & Agosta 1994] </ref>. The example has been made short by modelling a small oil spill that endangers only a small number of locations. <p> The domain encoding includes operators, inference rules, external events, control rules and bindings functions. The original encoding of this domain in sipe can be found in <ref> [Desimone & Agosta 1994] </ref>. In order to keep the specification in this document manageable, several parts have been omitted or contracted. Many simple operators have no preconditions. These have been listed simply by name and effects along with a comment. <p> The geographic information models the San Francisco Bay area as defined in the version of the domain written at sri <ref> [Desimone & Agosta 1994] </ref>. This fixed base of information includes 130 objects and 320 predicates. The reader who is interested in this domain and generator is encouraged to send electronic mail to jblythe@cs.cmu.edu for a copy.
Reference: [Draper & Hanks 1994] <author> Draper, D., and Hanks, S. </author> <year> 1994. </year> <title> Localized partial evaluation of belief networks. </title> <editor> In de Mantaras, R. L., and Poole, D., eds., </editor> <booktitle> Proc. Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> 170-177. </pages> <address> Seattle, WA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [Draper, Hanks, & Weld 1994] <author> Draper, D.; Hanks, S.; and Weld, D. </author> <year> 1994. </year> <title> Probabilistic planning with information gathering and contingent execution. </title> <editor> In Hammond, K., ed., </editor> <booktitle> Proc. Second International Conference on Artificial Intelligence Planning Systems, </booktitle> <pages> 31-37. </pages> <institution> University of Chicago, </institution> <address> Illinois: </address> <publisher> AAAI Press. 160 BIBLIOGRAPHY </publisher>
Reference-contexts: In another important paper, Peot and Smith describe an extension to the snlp algorithm, called cnlp for building conditional plans when some of the domain actions can have non-deterministic effects [Peot & Smith 1992]. Among the best-known probabilistic planners are Buridan [Kushmerick, Hanks, & Weld 1995] and C-Buridan <ref> [Draper, Hanks, & Weld 1994] </ref>. Buridan uses a variant of the snlp algorithm, and a strips-like action representation with probabilistic effects. <p> Several planning systems are capable of solving problems like this, such as cnlp [Peot & Smith 1992], Cassandra [Pryor & Collins 1996] and C-Buridan <ref> [Draper, Hanks, & Weld 1994] </ref>. <p> Future research directions 119 * Although this thesis concentrated on a planner based on Prodigy 4.0, it is in principle quite possible to apply the same ideas to other planning algorithms. Despite promising work in planning under uncertainty with partial-order and hierarchical task network planners <ref> [Draper, Hanks, & Weld 1994; Haddawy, Doan, & Goodwin 1995] </ref>, exogenous events have not yet been addressed in these systems.
Reference: [Drummond & Bresina 1990] <author> Drummond, M., and Bresina, J. </author> <year> 1990. </year> <title> Anytime synthetic projection: Maximizing the probability of goal satisfaction. </title> <booktitle> In Proc. Ord National Conference on Artificial Intelligence, </booktitle> <pages> 138-144. </pages> <publisher> AAAI Press. </publisher>
Reference-contexts: Systems have been developed for this purpose that combine reactive execution with classical planning, using the latter when no pre-programmed response is available for some contingency. Both the Theo-agent [Blythe & Mitchell 1989] and the "anytime synthetic projection" technique of Drummond and Bresina <ref> [Drummond & Bresina 1990] </ref> follow reactive rules if they are present, and otherwise fall back on planning and then compile the resulting plan into new reactive rules to be used in future episodes. The two systems differ on the action and goal representation and on the planning technique.
Reference: [Etzioni 1990] <author> Etzioni, O. </author> <year> 1990. </year> <title> A Structural Theory of Explanation-Based Learning. </title> <type> Ph.D. Dissertation, </type> <institution> School of Computer Science, Carnegie Mellon University. </institution> <note> Available as technical report CMU-CS-90-185. </note>
Reference-contexts: Given the amount of effort that has been spent on efficient classical planners over the past decade e.g <ref> [Minton 1988; Etzioni 1990; Knoblock 1991; Gil 1992] </ref>, it is significant that many of the ideas can be re-used. I begin this chapter with a discussion of the motivation for using analogical replay in the conditional planner, then discuss the algorithm and illustrate its performance on some synthetic domains.
Reference: [Feldman & Sproull 1977] <author> Feldman, J. A., and Sproull, R. F. </author> <year> 1977. </year> <booktitle> Decision theory and artificial intelligence ii: The hungy monkey. Cognitive Science 1 </booktitle> <pages> 158-192. </pages>
Reference-contexts: Work on extensions to planning systems using decision theory began shortly after the initial work on strips <ref> [Feldman & Sproull 1977] </ref>, but this line of work was largely discontinued until the 1990's.
Reference: [Fikes & Nilsson 1971] <author> Fikes, R. E., and Nilsson, N. J. </author> <year> 1971. </year> <title> Strips: A new approach to the application of theorem proving to problem solving. </title> <booktitle> Artificial Intelligence 2 </booktitle> <pages> 189-208. </pages>
Reference-contexts: Following strips <ref> [Fikes & Nilsson 1971] </ref>, the effects are usually represented as a list of facts to be deleted from the current state and a list of facts to be added. <p> Finally, chapter 8 summarises the main contributions of the thesis. 8 Chapter 1. Introduction Chapter 2 Related work Work in automatic planning began early in the field of artificial intelligence, with such programs as GPS [Newell & Simon 1963] and strips <ref> [Fikes & Nilsson 1971] </ref>. <p> Systems referred to as "classical" planning systems typically make the following additional assumptions: * The environment can be represented in a sequence of discrete states, which are represented using a logical language. * The actions are represented in a language similar to that used by strips <ref> [Fikes & Nilsson 1971] </ref>, in which the action's preconditions form a logical sentence over the language used to define states, and the action's effects are represented by a set of terms to be added to or deleted from the terms used to represent a state. * The goal is a property <p> and how it searches a space of partial plans to find a solution. 3.1.1 Domains, problems and plans A planning domain in prodigy 4.0 consists of a type hierarchy T , a set of literals L and a set of operators O 1 , based on the strips domain definition <ref> [Fikes & Nilsson 1971] </ref>. Each operator is defined by specifying its preconditions and effects.
Reference: [Fink & Veloso 1995] <author> Fink, E., and Veloso, M. </author> <year> 1995. </year> <title> Formalizing the prodigy planning algorithm. </title> <editor> In Ghallab, M., and Milani, A., eds., </editor> <booktitle> New Directions in AI Planning, </booktitle> <pages> 261-272. </pages> <address> Assissi, Italy: </address> <publisher> IOS Press. </publisher>
Reference-contexts: This can be achieved by the following four-step plan: (Move-Barge &lt;barge&gt;=barge1 &lt;from&gt;=Richmond &lt;to&gt;=west-coast) (Pump-Oil &lt;barge&gt;=barge1 &lt;sector&gt;=west-coast) (Move-Barge &lt;barge&gt;=barge1 &lt;from&gt;=west-coast &lt;to&gt;=Richmond) (Unload-Oil &lt;barge&gt;=barge1 &lt;dock&gt;=Richmond) prodigy 4.0 searches a space of candidate plans, in which each candidate plan is represented by two structures, a head plan and a tail plan <ref> [Fink & Veloso 1995] </ref>. The head plan is a totally-ordered sequence of steps that can be applied to the initial state.
Reference: [Firby 1987] <author> Firby, J. R. </author> <year> 1987. </year> <title> An investigation into reactive planning in complex domains. </title> <booktitle> In Proc. Ord National Conference on Artificial Intelligence, </booktitle> <pages> 202-206. </pages> <publisher> AAAI Press. </publisher>
Reference-contexts: Schoppers [Schoppers 1989a] develops a similar scheme in his "universal plans" and describes how they can be created automatically. Other reactive planning systems such as prs [Georgeff & Lansky 1987], rap <ref> [Firby 1987; 1989] </ref> and hap [Loyall & Bates 1991] do include internal state | prs represents the agent's beliefs, desires and intentions explicitly | and also control structures such as iteration.
Reference: [Firby 1989] <author> Firby, J. R. </author> <year> 1989. </year> <title> Adaptive Execution in Complex Dynamic Worlds. </title> <type> Ph.D. Dissertation, </type> <institution> Department of Computer Science, Yale University. </institution>
Reference: [Georgeff & Lansky 1987] <author> Georgeff, M. P., and Lansky, A. L. </author> <year> 1987. </year> <title> Reactive reasoning and planning. </title> <booktitle> In Proc. Ord National Conference on Artificial Intelligence, </booktitle> <pages> 677-681. </pages> <publisher> AAAI Press. </publisher>
Reference-contexts: Neither of these systems, however, shows how a reactive plan could be built from a declarative description of the environment. Schoppers [Schoppers 1989a] develops a similar scheme in his "universal plans" and describes how they can be created automatically. Other reactive planning systems such as prs <ref> [Georgeff & Lansky 1987] </ref>, rap [Firby 1987; 1989] and hap [Loyall & Bates 1991] do include internal state | prs represents the agent's beliefs, desires and intentions explicitly | and also control structures such as iteration.
Reference: [Gerevini & Schubert 1996] <author> Gerevini, A., and Schubert, L. </author> <year> 1996. </year> <title> Accelerating partial-order planners: Some techniques for effective search control and pruning. </title> <journal> Journal or Artificial Intelligence Research 5 </journal> <pages> 95-137. </pages>
Reference: [Gervasio & DeJong 1994] <author> Gervasio, M. T., and DeJong, G. F. </author> <year> 1994. </year> <title> An incremental learning approach for completable planning. </title> <booktitle> In International Conference on Machine Learning. </booktitle>
Reference-contexts: On the other hand, delaying planning for these goals can drastically reduce the number of alternatives to consider. Gervasio shows how to build "completable plans" which are designed to be incomplete, and amenable to being further elaborated during execution <ref> [Gervasio & DeJong 1994; Gervasio 1996] </ref>. Goodwin [Goodwin 1994] considers the question of when to switch from planning to executing in a time-dependent planning problem. Onder and Pollack [Onder & Pollack 1997] describe a probabilistic planner that reasons about which contingencies to plan for before execution and which to defer.
Reference: [Gervasio 1996] <author> Gervasio, M. T. </author> <year> 1996. </year> <title> An Incremental Curative Learning Approach for Planning with Incorrect Domain Theories. </title> <type> Ph.D. Dissertation, </type> <institution> University of Illinois at Urbana-Chapmaign. </institution>
Reference-contexts: As well as introducing new planning algorithms [Chapman 1987; McAllester & Rosenblitt 1991], later work considered more general conditions, for example where external events could take place during plan execution [Vere 1983], where the domain theory is incomplete or incorrect <ref> [Gil 1992; Wang 1996; Gervasio 1996] </ref> or where the agent must consider the relative quality of alternative plans [Perez 1995]. There has been a wide variety of work done in the design of systems for planning under uncertainty. <p> On the other hand, delaying planning for these goals can drastically reduce the number of alternatives to consider. Gervasio shows how to build "completable plans" which are designed to be incomplete, and amenable to being further elaborated during execution <ref> [Gervasio & DeJong 1994; Gervasio 1996] </ref>. Goodwin [Goodwin 1994] considers the question of when to switch from planning to executing in a time-dependent planning problem. Onder and Pollack [Onder & Pollack 1997] describe a probabilistic planner that reasons about which contingencies to plan for before execution and which to defer.
Reference: [Gil 1992] <author> Gil, Y. </author> <year> 1992. </year> <title> Acquiring Domain Knowledge for Planning by Experimentation. </title> <type> Ph.D. Dissertation, </type> <institution> School of Computer Science, Carnegie Mellon University. </institution> <note> Available as technical report CMU-CS-92-175. BIBLIOGRAPHY 161 </note>
Reference-contexts: As well as introducing new planning algorithms [Chapman 1987; McAllester & Rosenblitt 1991], later work considered more general conditions, for example where external events could take place during plan execution [Vere 1983], where the domain theory is incomplete or incorrect <ref> [Gil 1992; Wang 1996; Gervasio 1996] </ref> or where the agent must consider the relative quality of alternative plans [Perez 1995]. There has been a wide variety of work done in the design of systems for planning under uncertainty. <p> Given the amount of effort that has been spent on efficient classical planners over the past decade e.g <ref> [Minton 1988; Etzioni 1990; Knoblock 1991; Gil 1992] </ref>, it is significant that many of the ideas can be re-used. I begin this chapter with a discussion of the motivation for using analogical replay in the conditional planner, then discuss the algorithm and illustrate its performance on some synthetic domains.
Reference: [Goldman & Boddy 1994] <author> Goldman, R. P., and Boddy, M. S. </author> <year> 1994. </year> <note> Epsilon-safe planning. </note> <editor> In de Mantaras, R. L., and Poole, D., eds., </editor> <booktitle> Proc. Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> 253-261. </pages> <address> Seattle, WA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [Goldszmidt & Darwiche 1995] <author> Goldszmidt, M., and Darwiche, A. </author> <year> 1995. </year> <title> Plan simulation using bayesian networks. </title> <booktitle> In IEEE-CAIA. </booktitle>
Reference-contexts: Wellman also shows [Wellman 1990a] how proofs that one partial plan "dominates" another, in that each of its possible completions will have a higher utility than those of the other, can be used to reduce the search space for high-utility plans. Goldszmidt and Darwiche <ref> [Goldszmidt & Darwiche 1995] </ref> also discuss using belief nets to evaluate plans, but unlike the work described in this thesis they do not construct belief nets automatically.
Reference: [Goodwin 1994] <author> Goodwin, R. </author> <year> 1994. </year> <title> Reasoning about when to start acting. </title> <editor> In Ham-mond, K., ed., </editor> <booktitle> Proc. Second International Conference on Artificial Intelligence Planning Systems, </booktitle> <pages> 86-91. </pages> <institution> University of Chicago, </institution> <address> Illinois: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: On the other hand, delaying planning for these goals can drastically reduce the number of alternatives to consider. Gervasio shows how to build "completable plans" which are designed to be incomplete, and amenable to being further elaborated during execution [Gervasio & DeJong 1994; Gervasio 1996]. Goodwin <ref> [Goodwin 1994] </ref> considers the question of when to switch from planning to executing in a time-dependent planning problem. Onder and Pollack [Onder & Pollack 1997] describe a probabilistic planner that reasons about which contingencies to plan for before execution and which to defer. Washington 12 Chapter 2.
Reference: [Haddawy & Suwandi 1994] <author> Haddawy, P., and Suwandi, M. </author> <year> 1994. </year> <title> Decision-theoretic refinement planning using inheritance abstraction. </title> <editor> In Hammond, K., ed., </editor> <booktitle> Proc. Second International Conference on Artificial Intelligence Planning Systems. </booktitle> <institution> University of Chicago, </institution> <address> Illinois: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: The effect of these heuristics is explored in Section 7.5. 6.1.2 The footprint principle Probabilistic planners such as Weaver, Buridan [Kushmerick, Hanks, & Weld 1995] and drips <ref> [Haddawy & Suwandi 1994] </ref> build plans by incrementally refining some candidate plan to increase its probability of success, stopping when the threshold probability is reached and backtracking if no extensions achieve the desired probability.
Reference: [Haddawy, Doan, & Goodwin 1995] <author> Haddawy, P.; Doan, A.; and Goodwin, R. </author> <year> 1995. </year> <title> Efficient decision-theoretic planning: Techniques and empirical analysis. </title> <editor> In Besnard, P., and Hanks, S., eds., </editor> <booktitle> Proc. Eleventh Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> 229-326. </pages> <address> Montreal, Quebec: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Future research directions 119 * Although this thesis concentrated on a planner based on Prodigy 4.0, it is in principle quite possible to apply the same ideas to other planning algorithms. Despite promising work in planning under uncertainty with partial-order and hierarchical task network planners <ref> [Draper, Hanks, & Weld 1994; Haddawy, Doan, & Goodwin 1995] </ref>, exogenous events have not yet been addressed in these systems.
Reference: [Haddawy, Doan, & Kahn 1996] <author> Haddawy, P.; Doan, A.; and Kahn, C. E. </author> <year> 1996. </year> <title> Decision-theoretic refinement planning in medical decision making: Management of acute deep venous thrombosis. Medical Decision Making. </title>
Reference: [Haddawy 1991] <author> Haddawy, P. </author> <year> 1991. </year> <title> Representing Plans Under Uncertainty: a Logic of Time, Chance and Action. </title> <type> Ph.D. Dissertation, </type> <institution> University of Illinois at Urbana-Champaign. </institution>
Reference-contexts: Bagchi et al. describe a planner that uses a spreading activation technique to choose actions with highest expected utility [Bagchi, Biswas, & Kawamura 1994]. Onder and Pollack also extend snlp with explicit reasoning about the contingencies to plan for [Onder & Pollack 1997]. Haddawy's thesis work <ref> [Haddawy 1991] </ref> introduces a logic with extensions to represent probabilities of terms, intended for representing plans. He and his group have developed hierarchical task-network (htn) planners that use a similar representation [Haddawy & Suwandi 1994; Haddawy, Doan, & Goodwin 1995; Haddawy, Doan, & Kahn 1996].
Reference: [Hickman, Shell, & Carbonell 1990] <author> Hickman, A. K.; Shell, P.; and Carbonell, J. G. </author> <year> 1990. </year> <title> Internal analogy: Reducing search during problem solving. </title> <editor> In Copetas, C., ed., </editor> <booktitle> The Computer Science Research Review 1990. </booktitle> <institution> The School of Computer Science, Carnegie Mellon University. </institution>
Reference-contexts: In this integration of conditional planning and analogy, the analogical replay within the context of different branches of the same problem can be viewed as an instance of internal analogy <ref> [Hickman, Shell, & Carbonell 1990] </ref>. The accumulation of a library of cases is not required, and there is no need to analyze the similarity between a new problem and a potentially large number of cases.
Reference: [Howard 1960] <author> Howard, R. A. </author> <year> 1960. </year> <title> Dynamic Programming and Markov Processes. </title> <publisher> MIT Press. </publisher>
Reference-contexts: A solution to an mdp is a policy that maximises its expected value. For the discounted infinite-horizon case with any given discount factor fi, there is a policy 2.4. Approaches based on Markov decision processes 15 V fl that is optimal regardless of the starting state <ref> [Howard 1960] </ref>, which satisfies the following equation: V fl (s) = max fR (s; a) + fi u2S Two popular methods for solving this equation and finding an optimal policy for an mdp are value iteration and policy iteration [Puterman 1994].
Reference: [Kartha 1995] <author> Kartha, G. N. </author> <year> 1995. </year> <title> A Mathematical Investigation of Reasoning About Actions. </title> <type> Ph.D. Dissertation, </type> <institution> University of Texas at Austin. </institution>
Reference-contexts: ((add (at &lt;barge&gt; &lt;dest&gt;)) (del (at &lt;barge&gt; &lt;source&gt;)) (del (operational &lt;barge&gt;))))) ((0.1 () ((add (at &lt;barge&gt; &lt;dest&gt;)) (del (at &lt;barge&gt; &lt;source&gt;)))) (0.9 () ((add (at &lt;barge&gt; &lt;dest&gt;)) (del (at &lt;barge&gt; &lt;source&gt;)) (del (operational &lt;barge&gt;)))))))) a duration and a probability distribution of possible outcomes. which the prodigy 4.0 representation rests e.g. <ref> [Allen et al. 1991; Kartha 1995; Shanahan 1995; Baral 1995] </ref>. In this thesis I have chosen a simple and restricted language for events that is still adequate for planning with some interesting domains.
Reference: [Kautz & Selman 1996] <author> Kautz, H. A., and Selman, B. </author> <year> 1996. </year> <title> Pushing the envelope: Planning, propositional logic, and stochastic search. </title> <booktitle> In Proc. Ord National Conference on Artificial Intelligence. </booktitle> <publisher> AAAI Press. </publisher>
Reference-contexts: Despite promising work in planning under uncertainty with partial-order and hierarchical task network planners [Draper, Hanks, & Weld 1994; Haddawy, Doan, & Goodwin 1995], exogenous events have not yet been addressed in these systems. Experience with Weaver suggests that a planner based on iterative repair principles such as <ref> [Ambite & Knoblock 1997; Kautz & Selman 1996] </ref> could be very appropriate for probabilistic planning. * Weaver computes a lower bound on the probability of success of a plan, improving its speed by (1) ignoring fortuitous exogenous events that are not explicitly used by the conditional planner and (2) assuming that
Reference: [Knoblock 1991] <author> Knoblock, C. A. </author> <year> 1991. </year> <title> Automatically Generating Abstractions for Problem Solving. </title> <type> Ph.D. Dissertation, </type> <institution> Carnegie Mellon University. </institution> <address> 162 BIBLIOGRAPHY </address>
Reference-contexts: Planners can be made faster by solving a series of abstractions of the planning problem, each more detailed until the problem is solved in full complexity, at each stage using the solution from the previous stage <ref> [Sacerdoti 1974; Yang & Tenenberg 1990; Knoblock 1991] </ref>. Work has also been done on controlling search with various heuristics [Blythe & Veloso 1992; Gerevini & Schubert 1996; Pollack, Joslin, & Paolucci 1997] and with explicit control rules [Minton 1988; Minton et al. 1989]. <p> pick a subset of the literals that account for the greatest variation in the state utility and use the action representation to find literals which can directly or indirectly affect the chosen set, using a technique similar to the one developed by Knoblock for building abstraction hierarchies for classical planners <ref> [Knoblock 1991] </ref>. This subset of literals then forms the basis for an abstract mdp by projection of the original states. Since the state space size is exponential in the set of literals, this 2.4. <p> This state property of validity could be partially derived by considering the states reachable by applying actions to any physically possible initial state, or it could be enforced by adding domain axioms such as those used in <ref> [Knoblock 1991] </ref>. In what follows I will ignore this distinction. In Weaver the planning domain is generalised as follows. 1. The operators in the domain include a duration which is an integer-valued function of the bindings of the operator (and therefore a integer for an instantiated action or step). <p> Given the amount of effort that has been spent on efficient classical planners over the past decade e.g <ref> [Minton 1988; Etzioni 1990; Knoblock 1991; Gil 1992] </ref>, it is significant that many of the ideas can be re-used. I begin this chapter with a discussion of the motivation for using analogical replay in the conditional planner, then discuss the algorithm and illustrate its performance on some synthetic domains.
Reference: [Koenig & Simmons 1995] <author> Koenig, S., and Simmons, R. </author> <year> 1995. </year> <title> Real-time search in nondeterministic domains. </title> <booktitle> In Proc. 14th International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1660-1667. </pages> <address> Montreal, Quebec: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A probabilistic representation of the sources of uncertainty is well suited for this, although some have used a Dempster-Shaffer formalism [Mansell 1993], fuzzy reasoning [Bonissone & Dutta 1990] or a minimax approach <ref> [Koenig & Simmons 1995] </ref>. Work on extensions to planning systems using decision theory began shortly after the initial work on strips [Feldman & Sproull 1977], but this line of work was largely discontinued until the 1990's.
Reference: [Kushmerick, Hanks, & Weld 1994] <author> Kushmerick, N.; Hanks, S.; and Weld, D. </author> <year> 1994. </year> <title> An algorithm for probabilistic least-commitment planning. </title> <booktitle> In Proc. Ord National Conference on Artificial Intelligence, </booktitle> <pages> 1073-1078. </pages> <publisher> AAAI Press. </publisher>
Reference-contexts: Boutilier and Dearden [Boutilier & Dearden 1994] assume a representation for actions that is similar to that used in Buridan <ref> [Kushmerick, Hanks, & Weld 1994] </ref> described in Section 2.3 and a state utility function that is described in terms of domain literals.
Reference: [Kushmerick, Hanks, & Weld 1995] <author> Kushmerick, N.; Hanks, S.; and Weld, D. </author> <year> 1995. </year> <title> An algorithm for probabilistic planning. </title> <booktitle> Artificial Intelligence 76:239 - 286. </booktitle>
Reference-contexts: These techniques can be divided into those that affect the process of plan creation and those that affect the process of plan evaluation. Plan creation Although previous uncertain planners, such as Buridan and C-Buridan <ref> [Kushmerick, Hanks, & Weld 1995] </ref>, represented probabilistic effects of actions, their techniques did not scale well because of the joint expenses of handling all the contingencies the planner may need to consider and evaluating a plan with a number of sources of uncertainty. <p> In another important paper, Peot and Smith describe an extension to the snlp algorithm, called cnlp for building conditional plans when some of the domain actions can have non-deterministic effects [Peot & Smith 1992]. Among the best-known probabilistic planners are Buridan <ref> [Kushmerick, Hanks, & Weld 1995] </ref> and C-Buridan [Draper, Hanks, & Weld 1994]. Buridan uses a variant of the snlp algorithm, and a strips-like action representation with probabilistic effects. <p> The effect of these heuristics is explored in Section 7.5. 6.1.2 The footprint principle Probabilistic planners such as Weaver, Buridan <ref> [Kushmerick, Hanks, & Weld 1995] </ref> and drips [Haddawy & Suwandi 1994] build plans by incrementally refining some candidate plan to increase its probability of success, stopping when the threshold probability is reached and backtracking if no extensions achieve the desired probability.
Reference: [Lauritzen & Spiegelhalter 1988] <author> Lauritzen, S., and Spiegelhalter, D. </author> <year> 1988. </year> <title> Local computation with probabilities on graphical structures and their application to expert systems. </title> <journal> Journal of the Royal Statistical Society B 50(2) </journal> <pages> 154-227. </pages>
Reference-contexts: Algorithms for computing the probability distribution and for propagating observations on the variables as evidence to update the posterior distributions of the other variables can make use of the dependency information to improve efficiency <ref> [Pearl 1988; Lauritzen & Spiegelhalter 1988] </ref>. This problem is, however, NP-hard [Cooper 1990]. Formally, if a belief net models a probability distribution, then dependence relationships among variables in the distribution can be read from the graph using the criterion of d-separation.
Reference: [Littman 1996] <author> Littman, M. L. </author> <year> 1996. </year> <title> Algorithms for Sequential Decision Making. </title> <type> Ph.D. Dissertation, </type> <institution> Department of Computer Science, Brown University. </institution>
Reference-contexts: occur at some fixed time, but not about uncertain events. 2.4 Approaches based on Markov decision pro cesses In the past five years, much of the research activity in AI planning under uncertainty has built on standard techniques for solving Markov decision problems (mdps) and partially-observable Markov decision problems pomdps <ref> [Dean et al. 1995; Littman 1996; Boutilier, Brafman, & Geib 1997] </ref>. This formalism has the advantage of a sound mathematical underpinning, and the standard solution techniques aim at an optimal policy while most systems based on classical planning try to meet a lower bound. <p> This thesis pursues an approach based on classical planning, but uses an mdp representation to describe planning problems and their solutions. 2.4.1 Overview of Markov Decision Processes This description of Markov decision processes follows <ref> [Littman 1996] </ref> and [Boutilier, Dean, & Hanks 1995]. <p> the existing techniques did not handle probabilistic reasoning well, although the random-worlds approach is promising [Bacchus et al. 1997], and partly to make it easier to compare the planning system developed in this thesis with other approaches to planning under uncertainty, many of which are based on Markov decision processes <ref> [Boutilier & Dearden 1994; Dean & Lin 1995; Littman 1996] </ref>. Exogenous events have a very similar representation to operators. Figure 3.6 shows 30 Chapter 3. Planning under Uncertainty an example of a simple exogenous event called Weather-Brightens.
Reference: [Loyall & Bates 1991] <author> Loyall, A. B., and Bates, J. </author> <year> 1991. </year> <title> Hap: A reactive, adaptive architecture for agents. </title> <type> Technical Report CMU-CS-91-147, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA. </address>
Reference-contexts: Schoppers [Schoppers 1989a] develops a similar scheme in his "universal plans" and describes how they can be created automatically. Other reactive planning systems such as prs [Georgeff & Lansky 1987], rap [Firby 1987; 1989] and hap <ref> [Loyall & Bates 1991] </ref> do include internal state | prs represents the agent's beliefs, desires and intentions explicitly | and also control structures such as iteration.
Reference: [Mansell 1993] <author> Mansell, T. M. </author> <year> 1993. </year> <title> A method for planning given uncertain and incomplete information. </title> <editor> In Heckerman, D., and Mamdani, A., eds., </editor> <booktitle> Uncertainty in Artificial Intelligence, </booktitle> <volume> volume 9, </volume> <pages> 350-358. </pages> <address> Washington, D.C.: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For this purpose it is useful to have some way to compare the different contingencies to find the most important. A probabilistic representation of the sources of uncertainty is well suited for this, although some have used a Dempster-Shaffer formalism <ref> [Mansell 1993] </ref>, fuzzy reasoning [Bonissone & Dutta 1990] or a minimax approach [Koenig & Simmons 1995]. Work on extensions to planning systems using decision theory began shortly after the initial work on strips [Feldman & Sproull 1977], but this line of work was largely discontinued until the 1990's.
Reference: [McAllester & Rosenblitt 1991] <author> McAllester, D., and Rosenblitt, D. </author> <year> 1991. </year> <title> Systematic nonlinear planning. </title> <booktitle> In Proc. Ord National Conference on Artificial Intelligence, </booktitle> <pages> 634-639. </pages> <publisher> AAAI Press. </publisher>
Reference-contexts: Introduction Chapter 2 Related work Work in automatic planning began early in the field of artificial intelligence, with such programs as GPS [Newell & Simon 1963] and strips [Fikes & Nilsson 1971]. As well as introducing new planning algorithms <ref> [Chapman 1987; McAllester & Rosenblitt 1991] </ref>, later work considered more general conditions, for example where external events could take place during plan execution [Vere 1983], where the domain theory is incomplete or incorrect [Gil 1992; Wang 1996; Gervasio 1996] or where the agent must consider the relative quality of alternative plans <p> A more detailed description of Prodigy's action representation, which is an example of a strips-like representation, can be found in Section 3.1. Work in classical planning has typically focussed on improving the efficiency with which a plan is created. For example partial-order planners, introduced in the late eighties <ref> [Chapman 1987; McAllester & Rosenblitt 1991; Penberthy & Weld 1992] </ref>, attempt to reduce the search space using a "least commitment" principle where steps in a plan are only ordered with respect to one another as required to prove that the plan is successful. <p> that is able to notice this condition and to add a protection node to the search tree that replaces the preconditions of this step with the conjunction of its preconditions and the negation of the conditions of the conditional effect, similar in spirit to the confrontation step of ucpop-style planners <ref> [McAllester & Rosenblitt 1991] </ref>. This is described in [Blythe & Fink 1997]. Table 4.1 shows the sequence of nodes in the search tree that correspond to a solution based to the new problem based on the previous solution sequence in Table 3.2. 4.1.
Reference: [Minton et al. 1989] <author> Minton, S.; Carbonell, J. G.; Knoblock, C. A.; Kuokka, D. R.; Etzioni, O.; and Gil, Y. </author> <year> 1989. </year> <title> Explanation-based learning: A problem solving perspective. </title> <booktitle> Artificial Intelligence 40 </booktitle> <pages> 63-118. </pages>
Reference-contexts: Work has also been done on controlling search with various heuristics [Blythe & Veloso 1992; Gerevini & Schubert 1996; Pollack, Joslin, & Paolucci 1997] and with explicit control rules <ref> [Minton 1988; Minton et al. 1989] </ref>. The strips action representation does not support non-deterministic action outcomes and classical planners do not have a means to represent sources of change in the domain other than the actions taken by the performance agent. <p> Prodigy includes a rich language for specifying search control knowledge in the form of explicit, domain-dependent control rules <ref> [Minton et al. 1989; Veloso et al. 1995] </ref>. Domain-independent search heuristics are also vital in generic planning systems where complete domain-specific control knowledge is not always available [Stone, Veloso, & Blythe 1994].
Reference: [Minton 1988] <author> Minton, S. </author> <year> 1988. </year> <title> Learning Effective Search Control Knowledge: An Explanation-Based Approach. </title> <address> Boston, MA: </address> <publisher> Kluwer. </publisher>
Reference-contexts: Work has also been done on controlling search with various heuristics [Blythe & Veloso 1992; Gerevini & Schubert 1996; Pollack, Joslin, & Paolucci 1997] and with explicit control rules <ref> [Minton 1988; Minton et al. 1989] </ref>. The strips action representation does not support non-deterministic action outcomes and classical planners do not have a means to represent sources of change in the domain other than the actions taken by the performance agent. <p> Given the amount of effort that has been spent on efficient classical planners over the past decade e.g <ref> [Minton 1988; Etzioni 1990; Knoblock 1991; Gil 1992] </ref>, it is significant that many of the ideas can be re-used. I begin this chapter with a discussion of the motivation for using analogical replay in the conditional planner, then discuss the algorithm and illustrate its performance on some synthetic domains.
Reference: [Newell & Simon 1963] <author> Newell, A., and Simon, H. A. </author> <year> 1963. </year> <title> Gps: a program that simulates human thought. </title> <editor> In Feigenbaum, E. A., and Feldman, J., eds., </editor> <booktitle> Computers and Thought, </booktitle> <pages> 279-293. </pages> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference-contexts: Finally, chapter 8 summarises the main contributions of the thesis. 8 Chapter 1. Introduction Chapter 2 Related work Work in automatic planning began early in the field of artificial intelligence, with such programs as GPS <ref> [Newell & Simon 1963] </ref> and strips [Fikes & Nilsson 1971].
Reference: [Onder & Pollack 1997] <author> Onder, N., and Pollack, M. </author> <year> 1997. </year> <title> Contingency selection in plan generation. </title> <booktitle> In European Conference on Planning. </booktitle>
Reference-contexts: Gervasio shows how to build "completable plans" which are designed to be incomplete, and amenable to being further elaborated during execution [Gervasio & DeJong 1994; Gervasio 1996]. Goodwin [Goodwin 1994] considers the question of when to switch from planning to executing in a time-dependent planning problem. Onder and Pollack <ref> [Onder & Pollack 1997] </ref> describe a probabilistic planner that reasons about which contingencies to plan for before execution and which to defer. Washington 12 Chapter 2. <p> Bagchi et al. describe a planner that uses a spreading activation technique to choose actions with highest expected utility [Bagchi, Biswas, & Kawamura 1994]. Onder and Pollack also extend snlp with explicit reasoning about the contingencies to plan for <ref> [Onder & Pollack 1997] </ref>. Haddawy's thesis work [Haddawy 1991] introduces a logic with extensions to represent probabilities of terms, intended for representing plans.
Reference: [Parr & Russell 1995] <author> Parr, R., and Russell, S. </author> <year> 1995. </year> <title> Approximating optimal policies for partially observable stochastic domains. </title> <booktitle> In Proc. 14th International Joint Conference on Artificial Intelligence. </booktitle> <address> Montreal, Quebec: </address> <publisher> Morgan Kaufmann. BIBLIOGRAPHY 163 </publisher>
Reference-contexts: Related work all the states of the system. Work to make mdp approaches tractable has attempted to incorporate ideas from classical planning and other areas of AI to exploit any underlying structure in the state space [Dearden & Boutilier 1997] as well as to relax the requirement of optimalty <ref> [Parr & Russell 1995] </ref>. In this section I give a brief overview of Markov decision processes and survey some of the work in this area. <p> The witness algorithm includes an improved technique for updating the basis of the convex value function on each iteration. Parr and Russell use a smooth approximation of the value function that can be updated with gradient 18 Chapter 2. Related work descent <ref> [Parr & Russell 1995] </ref>. Brafman introduces a grid-based method in [Brafman 1997]. Although work on pomdps is promising, it is still preliminary, and the largest completely solved pomdps have about 10 states [Brafman 1997].
Reference: [Pearl 1988] <author> Pearl, J. </author> <year> 1988. </year> <title> Probabilistic Reasoning in Intelligent Systems. </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Plan evaluation A plan's probability of success is computed by automatically creating a Bayesian belief net to represent the plan and evaluating it using a standard join tree method <ref> [Pearl 1988] </ref>. A contribution of this thesis is an algorithm to produce a belief net that can be efficiently evaluated when there are exogenous events. <p> This makes it possible to factor the formula that describes the probability of plan success, using for example the standard techniques for Bayesian belief networks <ref> [Pearl 1988] </ref>. Wellman also shows [Wellman 1990a] how proofs that one partial plan "dominates" another, in that each of its possible completions will have a higher utility than those of the other, can be used to reduce the search space for high-utility plans. <p> This representation has a number of advantages for reasoning probabilistically about plans. It makes efficient use of the dependency structure between domain features when computing probabilities, and has a sound theoretical basis <ref> [Pearl 1988] </ref>. It can also efficiently maintain beliefs about unobservable world features based on observations during plan execution. 52 Chapter 4. The Weaver Algorithm to the conditional plan to transfer the oil with one of two barges. <p> Overview of Bayesian belief networks Before describing in detail the algorithm used to construct a belief network that models the plan's probability of success, I give here a brief overview of Bayesian belief networks, based on <ref> [Pearl 1988] </ref>. In the rest of the thesis I will refer to Bayesian 4.2. Calculating the probability of success 53 belief networks as "belief networks" or "belief nets". <p> Algorithms for computing the probability distribution and for propagating observations on the variables as evidence to update the posterior distributions of the other variables can make use of the dependency information to improve efficiency <ref> [Pearl 1988; Lauritzen & Spiegelhalter 1988] </ref>. This problem is, however, NP-hard [Cooper 1990]. Formally, if a belief net models a probability distribution, then dependence relationships among variables in the distribution can be read from the graph using the criterion of d-separation.
Reference: [Pearl 1994] <author> Pearl, J. </author> <year> 1994. </year> <title> A probabilistic calculus of actions. </title> <editor> In de Mantaras, R. L., and Poole, D., eds., </editor> <booktitle> Proc. Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> 454-462. </pages> <address> Seattle, WA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The effect of the action is a direct intervention into the natural evolution of the variables represented by the Markov chain, resulting in a model similar to that of Pearl in <ref> [Pearl 1994] </ref>. of the example plan along with some of the marginal probabilities. The node shown in gray is added by the algorithm to allow the Markov chain to be expressed as a conditional probability table in the belief net.
Reference: [Penberthy & Weld 1992] <author> Penberthy, J. S., and Weld, D. S. </author> <year> 1992. </year> <title> Ucpop: A sound, complete, partial order planner for adl. </title> <booktitle> In Third International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <pages> 103-114. </pages>
Reference-contexts: A more detailed description of Prodigy's action representation, which is an example of a strips-like representation, can be found in Section 3.1. Work in classical planning has typically focussed on improving the efficiency with which a plan is created. For example partial-order planners, introduced in the late eighties <ref> [Chapman 1987; McAllester & Rosenblitt 1991; Penberthy & Weld 1992] </ref>, attempt to reduce the search space using a "least commitment" principle where steps in a plan are only ordered with respect to one another as required to prove that the plan is successful.
Reference: [Peot & Smith 1992] <author> Peot, M. A., and Smith, D. E. </author> <year> 1992. </year> <title> Conditional nonlinear planning. </title> <editor> In Hendler, J., ed., </editor> <booktitle> Proc. First International Conference on Artificial Intelligence Planning Systems, </booktitle> <pages> 189-197. </pages> <address> College Park, Maryland: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: In another important paper, Peot and Smith describe an extension to the snlp algorithm, called cnlp for building conditional plans when some of the domain actions can have non-deterministic effects <ref> [Peot & Smith 1992] </ref>. Among the best-known probabilistic planners are Buridan [Kushmerick, Hanks, & Weld 1995] and C-Buridan [Draper, Hanks, & Weld 1994]. Buridan uses a variant of the snlp algorithm, and a strips-like action representation with probabilistic effects. <p> Buridan can provably find a plan that achieves the threshold probability of success, if a non-branching plan exists that does so. C-Buridan is an extension to Buridan that can create branching plans, using a technique similar to that introduced with cnlp <ref> [Peot & Smith 1992] </ref>. Essentially, one new way that C-Buridan can fix a candidate plan in which two actions might interfere with each other is to add a test and restrict the actions to different conditional branches based on the test. <p> If the planning problem contains more than one barge, then a plan to transfer the oil with high probability should be conditional on the state of the barge being used. Several planning systems are capable of solving problems like this, such as cnlp <ref> [Peot & Smith 1992] </ref>, Cassandra [Pryor & Collins 1996] and C-Buridan [Draper, Hanks, & Weld 1994].
Reference: [Perez & Carbonell 1994] <author> Perez, M. A., and Carbonell, J. </author> <year> 1994. </year> <title> Control knowledge to improve plan quality. </title> <editor> In Hammond, K., ed., </editor> <booktitle> Proc. Second International Conference on Artificial Intelligence Planning Systems, </booktitle> <pages> 323-328. </pages> <institution> University of Chicago, </institution> <address> Illinois: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: While this thesis concentrates mainly on extensions to classical planning that support probabilistic planning in uncertain domains, other work develops extensions to enable planners to find high-quality plans according to some specified criteria. Perez <ref> [Perez & Carbonell 1994; Perez 1995] </ref> describes how to learn control knowledge from interactions with a human expert that allows Prodigy to produce higher-quality plans. Williamson and Hanks [Williamson & Hanks 1994; Williamson 1996] develop a decision-theoretic extension to ucpop.
Reference: [Perez 1995] <author> Perez, M. A. </author> <year> 1995. </year> <title> Learning Search Control Knowledge to Improve Plan Quality. </title> <type> Ph.D. Dissertation, </type> <institution> School of Computer Science, Carnegie Mellon University. </institution>
Reference-contexts: 1987; McAllester & Rosenblitt 1991], later work considered more general conditions, for example where external events could take place during plan execution [Vere 1983], where the domain theory is incomplete or incorrect [Gil 1992; Wang 1996; Gervasio 1996] or where the agent must consider the relative quality of alternative plans <ref> [Perez 1995] </ref>. There has been a wide variety of work done in the design of systems for planning under uncertainty. <p> While this thesis concentrates mainly on extensions to classical planning that support probabilistic planning in uncertain domains, other work develops extensions to enable planners to find high-quality plans according to some specified criteria. Perez <ref> [Perez & Carbonell 1994; Perez 1995] </ref> describes how to learn control knowledge from interactions with a human expert that allows Prodigy to produce higher-quality plans. Williamson and Hanks [Williamson & Hanks 1994; Williamson 1996] develop a decision-theoretic extension to ucpop.
Reference: [Pollack, Joslin, & Paolucci 1997] <author> Pollack, M. E.; Joslin, D.; and Paolucci, M. </author> <year> 1997. </year> <title> Flaw selection strategies for partial-order planning. </title> <journal> Journal of Artificial Intelligence Research 6 </journal> <pages> 223-262. </pages>
Reference: [Pryor & Collins 1993] <author> Pryor, L., and Collins, G. </author> <year> 1993. </year> <title> Cassandra: Planning for contingencies. </title> <type> Technical Report 41, </type> <institution> The Institute for the Learning Sciences. </institution>
Reference-contexts: Information about the branch that an action belongs to is then propagated to the action's descendants and ancestors in the goal tree. Cassandra <ref> [Pryor & Collins 1993; 1996] </ref> is another planner based on snlp and uses an explicit representation of decision steps. Bagchi et al. describe a planner that uses a spreading activation technique to choose actions with highest expected utility [Bagchi, Biswas, & Kawamura 1994].
Reference: [Pryor & Collins 1996] <author> Pryor, L., and Collins, G. </author> <year> 1996. </year> <title> Planning for contingencies: A decision-based approach. </title> <journal> Journal of Artificial Intelligence Research 4 </journal> <pages> 287-339. </pages>
Reference-contexts: If the planning problem contains more than one barge, then a plan to transfer the oil with high probability should be conditional on the state of the barge being used. Several planning systems are capable of solving problems like this, such as cnlp [Peot & Smith 1992], Cassandra <ref> [Pryor & Collins 1996] </ref> and C-Buridan [Draper, Hanks, & Weld 1994].
Reference: [Puterman 1994] <author> Puterman, M. </author> <year> 1994. </year> <title> Markov Decision Processes : Discrete Stochastic Dynamic Programming. </title> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: 15 V fl that is optimal regardless of the starting state [Howard 1960], which satisfies the following equation: V fl (s) = max fR (s; a) + fi u2S Two popular methods for solving this equation and finding an optimal policy for an mdp are value iteration and policy iteration <ref> [Puterman 1994] </ref>. In policy iteration, the current policy is repeatedly improved by finding some action in each state that has a higher value than the action chosen by the current policy for that state. <p> The policy is initially chosen at random, and the process terminates when no improvement can be found. Tha algorithm is shown in Table 2.1. This process converges to an optimal policy <ref> [Puterman 1994] </ref>. Policy-Iteration (S; A; ; R; fi): 1. For each s 2 S, (s) = RandomElement (A) 2. Compute V (:) 3. For each s 2 S f 4. <p> Modified policy iteration replaces this step with an iterative approximation of the value function V by a series of value functions V 0 ; V 1 ; . . . given by V i (s) = R (s) + fi u2S Stopping criteria are given in <ref> [Puterman 1994] </ref>. In structured policy iteration, the value function is again built in a series of approximations, but in each one it is represented as a decision tree over the domain literals. Similarly the policy is built up as a decision tree.
Reference: [Russell & Wefald 1991] <author> Russell, S., and Wefald, E. </author> <year> 1991. </year> <title> Do the Right Thing. </title> <publisher> MIT Press. </publisher>
Reference-contexts: If this is determined to be small, the computation of the exact probability could be passed over in favour of more planning work to improve the probability of succes on more likely branches. A decision-theoretic framework for making such decisions such as developed in <ref> [Russell & Wefald 1991] </ref> and [Zilberstein 1993] could be used. * More research is needed in machine learning and planning, using derivational analogy and other forms of machine learning such as learning search control rules from experience.
Reference: [Sacerdoti 1974] <author> Sacerdoti, E. </author> <year> 1974. </year> <title> Planning in a hierarchy of abstraction spaces. </title> <booktitle> Artificial Intelligence 5 </booktitle> <pages> 115-135. </pages>
Reference-contexts: Planners can be made faster by solving a series of abstractions of the planning problem, each more detailed until the problem is solved in full complexity, at each stage using the solution from the previous stage <ref> [Sacerdoti 1974; Yang & Tenenberg 1990; Knoblock 1991] </ref>. Work has also been done on controlling search with various heuristics [Blythe & Veloso 1992; Gerevini & Schubert 1996; Pollack, Joslin, & Paolucci 1997] and with explicit control rules [Minton 1988; Minton et al. 1989].
Reference: [Schoppers 1989a] <author> Schoppers, M. J. </author> <year> 1989a. </year> <title> In defense of reaction plans as caches. </title> <journal> AI Magazine 10(4) </journal> <pages> 51-60. 164 BIBLIOGRAPHY </pages>
Reference-contexts: The subsumption architecture arranges reactive sub-sytems into hierarchies, so that higher-level behaviour is achieved by one system over-riding, or subsuming, a more basic system. Neither of these systems, however, shows how a reactive plan could be built from a declarative description of the environment. Schoppers <ref> [Schoppers 1989a] </ref> develops a similar scheme in his "universal plans" and describes how they can be created automatically.
Reference: [Schoppers 1989b] <author> Schoppers, M. J. </author> <year> 1989b. </year> <title> Representation and Automatic Synthesis of Reaction Plans. </title> <type> Ph.D. Dissertation, </type> <institution> University of Illinois at Urbana-Champaign. </institution> <note> Available as technical report UIUCDCS-R-89-1546. </note>
Reference-contexts: The planner does not know if the traffic lights will be red or green, or what traffic will be encountered. This led some researchers to abandon programs that choose steps in advance in favour of "reactive" programs that choose each step as it is to be made <ref> [Agre & Chapman 1987; Schoppers 1989b] </ref>. However it is often necessary to make contingency plans ahead of time even if not every situation can be predicted and planned for.
Reference: [Shanahan 1995] <author> Shanahan, M. </author> <year> 1995. </year> <title> A circumscriptive calculus of events. </title> <booktitle> Artificial Intelligence 75(2). </booktitle>
Reference-contexts: ((add (at &lt;barge&gt; &lt;dest&gt;)) (del (at &lt;barge&gt; &lt;source&gt;)) (del (operational &lt;barge&gt;))))) ((0.1 () ((add (at &lt;barge&gt; &lt;dest&gt;)) (del (at &lt;barge&gt; &lt;source&gt;)))) (0.9 () ((add (at &lt;barge&gt; &lt;dest&gt;)) (del (at &lt;barge&gt; &lt;source&gt;)) (del (operational &lt;barge&gt;)))))))) a duration and a probability distribution of possible outcomes. which the prodigy 4.0 representation rests e.g. <ref> [Allen et al. 1991; Kartha 1995; Shanahan 1995; Baral 1995] </ref>. In this thesis I have chosen a simple and restricted language for events that is still adequate for planning with some interesting domains. <p> If Move-Barge comes before Weather-Brightens in the order, (fair-weather) will be false, otherwise it will be true. More sophisticated schemes for reasoning about simultaneous events are possible, such as the one proposed in <ref> [Shanahan 1995] </ref>. 3.3. A Markov decision process model of uncertainty 31 from the initial state shown, when Weather-Brightens is the only exogenous event in the domain and the action has a duration of one time unit.
Reference: [Stone, Veloso, & Blythe 1994] <author> Stone, P.; Veloso, M.; and Blythe, J. </author> <year> 1994. </year> <title> The need for different domain-independent heuristics. </title> <editor> In Hammond, K., ed., </editor> <booktitle> Proc. Second International Conference on Artificial Intelligence Planning Systems, </booktitle> <pages> 164-169. </pages> <institution> University of Chicago, </institution> <address> Illinois: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Prodigy includes a rich language for specifying search control knowledge in the form of explicit, domain-dependent control rules [Minton et al. 1989; Veloso et al. 1995]. Domain-independent search heuristics are also vital in generic planning systems where complete domain-specific control knowledge is not always available <ref> [Stone, Veloso, & Blythe 1994] </ref>. In this chapter I introduce some principles for defining domain-independent search heuristics that are specific to the problem of planning under uncertainty.
Reference: [Tash & Russell 1994] <author> Tash, J. K., and Russell, S. </author> <year> 1994. </year> <title> Control strategies for a stochastic planner. </title> <booktitle> In Proc. Ord National Conference on Artificial Intelligence, </booktitle> <pages> 1079-1085. </pages> <publisher> AAAI Press. </publisher>
Reference-contexts: Tash and Russell extend the idea of an envelope with an initial estimate of distance-to-goal for each state and a model that takes the time of computation into account <ref> [Tash & Russell 1994] </ref>. While the envelope extension method ignores portions of the state space, other techniques have considered abstractions of the state space that try to group together sets of states that behave similarly under the chosen actions of the optimal policy.
Reference: [Tate 1977] <author> Tate, A. </author> <year> 1977. </year> <title> Generating project networks. </title> <booktitle> In International Joint Conference on Artificial Intelligence. </booktitle>
Reference: [Veloso & Stone 1995] <author> Veloso, M., and Stone, P. </author> <year> 1995. </year> <title> Flecs: Planning with a flexible commitment strategy. </title> <journal> Journal of Artificial Intelligence Research 3 </journal> <pages> 25-52. </pages>
Reference: [Veloso et al. 1995] <author> Veloso, M.; Carbonell, J.; Perez, A.; Borrajo, D.; Fink, E.; and Blythe, J. </author> <year> 1995. </year> <title> Integrating planning and learning: The prodigy architecture. </title> <journal> Journal of Experimental and Theoretical AI 7 </journal> <pages> 81-120. </pages>
Reference-contexts: In addition the logical representation allows a plan to be proved correct. Prodigy, the planning system extended in this thesis, is a classical planner according to this definition <ref> [Veloso et al. 1995] </ref>. A more detailed description of Prodigy's action representation, which is an example of a strips-like representation, can be found in Section 3.1. Work in classical planning has typically focussed on improving the efficiency with which a plan is created. <p> The plan in Figure 3.4 cannot be expanded by adding a step to the tail plan because there are no open preconditions in the tail plan. A summary of the prodigy 4.0 algorithm is shown in Table 3.1 (taken from al. <ref> [Veloso et al. 1995] </ref>). prodigy 4.0 (G,I) 1. Current state C := initial state I, Head-Plan := null, Tail-Plan := null. 2. If the goal statement G is satisfied in the current state C, then return Head-Plan. 3. <p> Prodigy includes a rich language for specifying search control knowledge in the form of explicit, domain-dependent control rules <ref> [Minton et al. 1989; Veloso et al. 1995] </ref>. Domain-independent search heuristics are also vital in generic planning systems where complete domain-specific control knowledge is not always available [Stone, Veloso, & Blythe 1994]. <p> The representation of uncertainty was defined in terms of a Markov decision process model. The planning algorithm harnesses a novel conditional planner that extends prodigy 4.0, and as such is able to make use of control rules, machine learning techniques and a graphical user interface <ref> [Veloso et al. 1995] </ref>. The conditional planner attempts to create a plan that is certain to succeed given a set of non-deterministic actions.
Reference: [Veloso 1992] <author> Veloso, M. M. </author> <year> 1992. </year> <title> Learning by Analogical Reasoning in General Problem Solving. </title> <type> Ph.D. Dissertation, </type> <institution> Carnegie Mellon University. </institution>
Reference-contexts: A plan in prodigy 4.0 is a totally-ordered sequence of instantiated operators (O 1 ; O 2 ; . . . O n ). The plan can be re-formulated in a partial order after creation if desired <ref> [Veloso 1992] </ref>. I will refer to an instantiated operator in a plan as a step in the plan. A plan is a solution to the planning problem if: 1.
Reference: [Veloso 1994] <author> Veloso, M. M. </author> <year> 1994. </year> <title> Planning and Learning by Analogical Reasoning. </title> <publisher> Springer Verlag. </publisher>
Reference-contexts: Prodigy's algorithm for derivational analogy was originally developed by Manuela Veloso <ref> [Veloso 1994] </ref> for No-Limit, a predecessor to Prodigy 4.0, and subsequently ported. Weaver is able to make use of the algorithm with very few changes because it is a conservative extension to Prodigy, itself having as few changes as are needed to perform conditional planning.
Reference: [Vere 1983] <author> Vere, S. </author> <year> 1983. </year> <title> Planning in time: Windows and durations for activities and goals. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence 5(3) </journal> <pages> 246-67. </pages>
Reference-contexts: As well as introducing new planning algorithms [Chapman 1987; McAllester & Rosenblitt 1991], later work considered more general conditions, for example where external events could take place during plan execution <ref> [Vere 1983] </ref>, where the domain theory is incomplete or incorrect [Gil 1992; Wang 1996; Gervasio 1996] or where the agent must consider the relative quality of alternative plans [Perez 1995]. There has been a wide variety of work done in the design of systems for planning under uncertainty. <p> Williamson and Hanks [Williamson & Hanks 1994; Williamson 1996] develop a decision-theoretic extension to ucpop. The planner described in this thesis is unique in its ability to reason explicitly about exogenous events, and to perform a relevance analysis of those events. Vere <ref> [Vere 1983] </ref> introduced a planner that could reason about exogenous events that were known to occur at some fixed time, but not about uncertain events. 2.4 Approaches based on Markov decision pro cesses In the past five years, much of the research activity in AI planning under uncertainty has built on
Reference: [Verma 1986] <author> Verma, T. S. </author> <year> 1986. </year> <title> Causal networks: Semantics and expressiveness. </title> <type> Technical Report R-65, </type> <institution> UCLA Cognitive Systems Laboratory. </institution>
Reference: [Wang 1996] <author> Wang, X. </author> <year> 1996. </year> <title> Learning Planning Operators by Observation and Practice. </title> <type> Ph.D. Dissertation, </type> <institution> Carnegie Mellon University. </institution>
Reference-contexts: As well as introducing new planning algorithms [Chapman 1987; McAllester & Rosenblitt 1991], later work considered more general conditions, for example where external events could take place during plan execution [Vere 1983], where the domain theory is incomplete or incorrect <ref> [Gil 1992; Wang 1996; Gervasio 1996] </ref> or where the agent must consider the relative quality of alternative plans [Perez 1995]. There has been a wide variety of work done in the design of systems for planning under uncertainty.
Reference: [Washington 1994] <author> Washington, R. </author> <year> 1994. </year> <title> Abstraction planning in real time. </title> <type> Ph.D. Dissertation, </type> <institution> Stanford University. </institution>
Reference-contexts: Washington 12 Chapter 2. Related work makes use an abstraction hierarchy, planning at some abstract level before execution and picking a concrete instance of the abstract plan during execution <ref> [Washington 1994] </ref>. 2.3 Extensions to classical planning systems Within the broader context of systems for planning and execution in uncertain environments, some recent work has focussed on creating a plan before execution that accounts for a significant subset of the contingencies that might occur during execution, but not necessarily all of
Reference: [Wellman 1990a] <author> Wellman, M. P. </author> <year> 1990a. </year> <title> Formulation of Tradeoffs in Planning Under Uncertainty. </title> <publisher> Pitman. BIBLIOGRAPHY 165 </publisher>
Reference-contexts: This makes it possible to factor the formula that describes the probability of plan success, using for example the standard techniques for Bayesian belief networks [Pearl 1988]. Wellman also shows <ref> [Wellman 1990a] </ref> how proofs that one partial plan "dominates" another, in that each of its possible completions will have a higher utility than those of the other, can be used to reduce the search space for high-utility plans.
Reference: [Wellman 1990b] <author> Wellman, M. P. </author> <year> 1990b. </year> <title> The strips assumption for planning under uncertainty. </title> <booktitle> In Proc. Ord National Conference on Artificial Intelligence, </booktitle> <pages> 198-203. </pages> <publisher> AAAI Press. </publisher>
Reference-contexts: Work on extensions to planning systems using decision theory began shortly after the initial work on strips [Feldman & Sproull 1977], but this line of work was largely discontinued until the 1990's. Wellman lays the ground-work for developing probabilistic extensions to classical planning systems, pointing out in <ref> [Wellman 1990b] </ref> that the strips representation of action embodies a Markov assumption | since the success of an operator depends on its preconditions, this is conditionally independent of all other knowledge if the current state is known.
Reference: [Williamson & Hanks 1994] <author> Williamson, M., and Hanks, S. </author> <year> 1994. </year> <title> Optimal planning with a a goal-directed utility model. </title> <editor> In Hammond, K., ed., </editor> <booktitle> Proc. Second International Conference on Artificial Intelligence Planning Systems, </booktitle> <pages> 176-181. </pages> <institution> University of Chicago, </institution> <address> Illinois: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Perez [Perez & Carbonell 1994; Perez 1995] describes how to learn control knowledge from interactions with a human expert that allows Prodigy to produce higher-quality plans. Williamson and Hanks <ref> [Williamson & Hanks 1994; Williamson 1996] </ref> develop a decision-theoretic extension to ucpop. The planner described in this thesis is unique in its ability to reason explicitly about exogenous events, and to perform a relevance analysis of those events.
Reference: [Williamson 1996] <author> Williamson, M. </author> <year> 1996. </year> <title> A Value-directed Approach to Planning. </title> <type> Ph.D. Dissertation, </type> <institution> University of Washington. </institution>
Reference-contexts: Perez [Perez & Carbonell 1994; Perez 1995] describes how to learn control knowledge from interactions with a human expert that allows Prodigy to produce higher-quality plans. Williamson and Hanks <ref> [Williamson & Hanks 1994; Williamson 1996] </ref> develop a decision-theoretic extension to ucpop. The planner described in this thesis is unique in its ability to reason explicitly about exogenous events, and to perform a relevance analysis of those events.
Reference: [Yang & Tenenberg 1990] <author> Yang, Q., and Tenenberg, J. D. </author> <year> 1990. </year> <title> Abtweak, abstracting a nonlinear, least commitment planner. </title> <booktitle> In Proc. Ord National Conference on Artificial Intelligence, </booktitle> <pages> 204-209. </pages> <publisher> AAAI Press. </publisher>
Reference-contexts: Planners can be made faster by solving a series of abstractions of the planning problem, each more detailed until the problem is solved in full complexity, at each stage using the solution from the previous stage <ref> [Sacerdoti 1974; Yang & Tenenberg 1990; Knoblock 1991] </ref>. Work has also been done on controlling search with various heuristics [Blythe & Veloso 1992; Gerevini & Schubert 1996; Pollack, Joslin, & Paolucci 1997] and with explicit control rules [Minton 1988; Minton et al. 1989].
Reference: [Zilberstein 1993] <author> Zilberstein, S. </author> <year> 1993. </year> <title> Operational Rationality through Compilation of Anytime Algorithms. </title> <type> Ph.D. Dissertation, </type> <institution> University of California at Berkeley. </institution> <address> 166 BIBLIOGRAPHY </address>
Reference-contexts: If this is determined to be small, the computation of the exact probability could be passed over in favour of more planning work to improve the probability of succes on more likely branches. A decision-theoretic framework for making such decisions such as developed in [Russell & Wefald 1991] and <ref> [Zilberstein 1993] </ref> could be used. * More research is needed in machine learning and planning, using derivational analogy and other forms of machine learning such as learning search control rules from experience.
References-found: 101

