URL: http://www.cs.cornell.edu/home/selman/compute/compute.ps
Refering-URL: http://www.cs.cornell.edu/home/selman/compute/index.html
Root-URL: http://www.cs.brown.edu/
Email: selman@cs.cornell.edu  
Title: Compute-Intensive Methods for Artificial Intelligence  
Author: Bart Selman 
Note: Proposal for NSF Faculty Early Career Development Award (1998-2002).  
Address: Ithaca, NY 14853  
Affiliation: Computer Science Department Cornell University  
Abstract-found: 0
Intro-found: 1
Reference: <author> Bacchus, F. and Kabanza, F. </author> <year> (1995). </year> <title> Using temporal logic to control search in a forward chaining planner. </title> <booktitle> Proc. EWSP-95, </booktitle> <pages> 157-169. </pages>
Reference: <author> Backstrom, C. </author> <year> (1992). </year> <title> Computational complexity of reasoning about plans, </title> <type> Ph.D. thesis, </type> <institution> Linkoping University, Linkoping, Sweden. </institution>
Reference: <author> Barrett, A. and Weld, D. </author> <year> (1994). </year> <title> Partial-order planning: evaluating possible efficiency gains. </title> <journal> Artificial Intelligence, </journal> <volume> 67 </volume> <pages> 71-112, </pages> <year> 1994. </year>
Reference: <author> Blum, A. and Furst, </author> <title> M.L. (1995). Fast planning through planning graph analysis. </title> <booktitle> Proc. IJCAI-95, </booktitle> <address> Canada. </address>
Reference: <author> Boufkhad, Y. </author> <title> (1996) Aspects probabilistes et algorithmiques du probleme de satisfiabilite. </title> <type> Ph.D. Thesis, </type> <institution> Univ. of Paris, </institution> <year> 1996. </year>
Reference: <author> Cheeseman, P., Kanefsky, B., Taylor, and William M. </author> <year> (1991). </year> <title> Where the really hard problems are. </title> <booktitle> Proc. IJCAI-91 (1991) 331-336. </booktitle>
Reference: <author> Chvatal, V. and Szemeredi, E. </author> <year> (1988). </year> <title> Many hard examples for resolution. </title> <journal> J. of the ACM, </journal> <month> 35(4) </month> <year> (1988) </year> <month> 759-208. </month>
Reference: <author> Clearwater, S.H., Huberman, B.A., Hogg, T. </author> <year> (1991). </year> <title> Cooperative solution of constraint satisfaction problems. </title> <booktitle> Science, </booktitle> <month> 254 </month> <year> (1991). </year>
Reference: <author> Cook, S. and Mitchell, D. </author> <year> (1997). </year> <title> Finding hard instances of the satisfiability problem: a survey. </title> <journal> DIMACS Series in Discr. Math. and Theoretical Comp. </journal> <note> Sci.. (to appear) Crawford, </note> <author> J.M. and Auton, L.D. </author> <year> (1993). </year> <title> Experimental results on the cross-over point in satisfiability problems. </title> <note> AAAI-93 (1993) 21-27. (Ext. version in Artif. </note> <author> Intel.) Crawford, J. and Baker, A.B. </author> <year> (1994). </year> <title> Experimental results on the application of satisfiability algorithms to scheduling problems. </title> <booktitle> Proc. AAAI-94, </booktitle> <address> Seattle, WA. </address>
Reference: <author> Crawford, J.M., Ginsberg, M., Luks, E., and Roy, A. </author> <year> (1996). </year> <booktitle> Proc. </booktitle> <address> KR-96, Boston, MA, 148-158. </address> <institution> CS Faculty. </institution> <year> (1996). </year> <title> Cornell computer science: Vision for the next decade. </title> <type> White paper, </type> <institution> Computer Science Department, Cornell University, </institution> <year> 1996. </year>
Reference: <author> Davis, M. and Putnam, H. </author> <year> (1960). </year> <title> A computing procedure for quantification theory. </title> <journal> J. of the ACM, </journal> <month> 7 </month> <year> (1960). </year>
Reference: <author> Dean, T. and Boddy, M. </author> <title> (1988) An analysis of time-dependent planning. </title> <booktitle> Proc. AAAI-88, </booktitle> <address> St. Paul, MI (1988) 49-54. </address>
Reference: <author> Dean, T. and Kambhampati, R. </author> <year> (1996). </year> <title> Planning and scheduling. </title> <booktitle> In CRC Handbook of Computer Science (1996). </booktitle>
Reference-contexts: Our work also provides a first step in the direction of obtaining more realistic benchmark problems for evaluating reasoning and search procedures. An obvious next step is to design problem generators for instances with properties that are even closer to those of real-world instances. <ref> (See also Dean et al. 1996, and Zhang and Korf 1996) </ref>. The study of more structured benchmark problems has a direct payoff in terms of the design of algorithms. I plan to study further the heavy-tailed and self-similar nature of combinatorial search and its implications for algorithm design.
Reference: <author> Dean, T., Kirman, K., Lin, S.H., Roussos, K., Weaver, M. </author> <year> (1996). </year> <title> Theory and practice in planning. </title> <type> Techn. report, </type> <institution> Brown University, </institution> <year> 1996. </year>
Reference-contexts: Our work also provides a first step in the direction of obtaining more realistic benchmark problems for evaluating reasoning and search procedures. An obvious next step is to design problem generators for instances with properties that are even closer to those of real-world instances. <ref> (See also Dean et al. 1996, and Zhang and Korf 1996) </ref>. The study of more structured benchmark problems has a direct payoff in terms of the design of algorithms. I plan to study further the heavy-tailed and self-similar nature of combinatorial search and its implications for algorithm design.
Reference: <author> Dechter, R. </author> <year> (1991). </year> <title> Constraint networks. </title> <booktitle> Encyclopedia of Artificial Intelligence John Wiley, </booktitle> <address> New York (1991) 276-285. </address>
Reference: <author> Dechter, R. and Kask, K. </author> <year> (1995). </year> <title> GSAT and local consistency. </title> <booktitle> Proc. IJCAI-95, </booktitle> <address> Montreal, Canada (1995). </address>
Reference: <author> Dechter, R. and Meiri, I. </author> <year> (1994). </year> <title> Experimental evaluation of preprocessing algorithms for constraint satisfaction problems. </title> <note> Artificial Intelligence 68 (1994) 211-241. 15 Dechter, </note> <author> R. and Rish, I. </author> <year> (1994). </year> <title> Directional resolution: the Davis-Putnam procedure revisited. </title> <booktitle> Proc. </booktitle> <address> KR--94, Bonn, Germany. </address>
Reference: <author> Delahaye, J.P. </author> <year> (1995). </year> <title> Les Lois de Tout ou Rien, (0-1 Laws). Pour La Science (French ed. </title> <journal> of Scientific American), </journal> <volume> Vol. 213, </volume> <month> July </month> <year> 1995. </year> <title> de la Tour, </title> <editor> T. and Demri, S. </editor> <year> (1995). </year> <title> On the complexity of extending ground resolution with symmetry rules. </title> <booktitle> Proc. IJCAI-95, </booktitle> <address> Montreal, Canada, </address> <pages> 289-295. </pages>
Reference: <author> Doyle, J. and Dean, T. </author> <title> (1996) Strategic directions in computer science. </title> <journal> ACM Computing Surveys, </journal> <volume> Vol. 28, no. 4., </volume> <year> 1996. </year>
Reference: <author> Dubois, O., Andre, P., Boufkhad, Y., and Carlier, J. </author> <year> (1996). </year> <title> SAT versus UNSAT. </title> <journal> DIMACS Series in Discr. Math. and Theoretical Comp. Sci., </journal> <volume> Vol. 26, </volume> <year> 1996, </year> <pages> 415-433. </pages>
Reference-contexts: Somewhat surprisingly, it appears that the simplest backtrack strategies with only a limited amount of preprocessing are the most effective overall <ref> (Dubois et al. 1996) </ref>. Nevertheless, these simple but extremely fast procedures (exploring 20,000 to 30,000 nodes per second) do not necessarily generate the most compact search trees.
Reference: <author> Ellman, T. </author> <year> (1993). </year> <title> Abstraction via approximate symmetry. </title> <booktitle> In Proceedings of IJCAI-93, </booktitle> <pages> 916-921. </pages>
Reference: <author> Ernst, M. , Millstein, T.D, and Weld, D. </author> <year> (1997). </year> <title> Automatic SAT-compilation of planning problems. </title> <booktitle> Proc. IJCAI-97, </booktitle> <address> Nagoya, Japan, </address> <year> 1997. </year>
Reference: <author> Erol, K., Nau, D.S., and Subrahmanian, </author> <title> V.S. (1992). On the complexity of domain-independent planning. </title> <booktitle> Proc. AAAI-92, </booktitle> <pages> 381-386. </pages>
Reference: <author> Etzioni, O. and Weld, D. S. </author> <year> (1994). </year> <title> A softbot-based interface to the internet. </title> <journal> Comm. ACM, </journal> <month> July </month> <year> 1994. </year>
Reference: <author> Fikes, R.E. and Nilsson, N.J. </author> <year> (1971). </year> <title> STRIPS: A new approach to the application of theorem proving to problem solving. </title> <journal> Artificial Intelligence, </journal> <volume> 2(3/4), </volume> <pages> 189-208. </pages>
Reference-contexts: See, for example, Kambhampati (1996, 1997) and Ernst et al. (1997). Critical to the success of our approach is the use of concise propositional representations of planning problems. In Kautz et al. (1996), we describe several general polynomial-time reductions from STRIPS-style planning <ref> (Fikes and Nilsson 1971) </ref> to propositional theories. We found that the most concise encodings do not necessarily provide the best performance in practice. In particular, the highly compact lifted causal encodings are surprisingly difficult to solve for current satisfiability procedures.
Reference: <editor> Freuder, E. and Mackworth, A. (Eds.). </editor> <title> Constraint-based reasoning. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, USA, </address> <year> 1994. </year>
Reference: <author> Fujita, M., Slaney, J., and Bennett, F. </author> <year> (1993). </year> <title> Automatic generation of results in finite algebra Proc. </title> <address> IJ-CAI, </address> <year> 1993. </year>
Reference-contexts: There has been significant recent progress along this front using general constraint-based representations and propositional encodings. Examples include work on planning (Blum and Furst 1995; Kautz and Selman 1996), problems in finite algebra <ref> (Fujita et al. 1993) </ref>, verification of hardware and software protocols, scheduling (Crawford and Baker 1994), circuit synthesis and diagnosis (Larrabee 1992), and many other domains. Experience has shown that different encodings of the same problem can have vastly different computational properties.
Reference: <author> Gent, I. and Walsh, T. </author> <year> (1993). </year> <title> Towards an understanding of hill-climbing procedures for SAT. </title> <booktitle> Proc. AAAI-93, </booktitle> <pages> 28-33. </pages>
Reference: <author> Ginsberg, M. </author> <year> (1993a). </year> <title> Dynamic Backtracking. </title> <journal> Journal of Artificial Intelligence, </journal> <volume> Vol. 1, </volume> <year> 1993, </year> <pages> 25-46. </pages>
Reference-contexts: I believe that it should be possible, using careful algorithmic design guided by rigorous experimental evaluations, to develop more sophisticated backtrack style methods that do outperform | in terms of overall computational cost | the very fast simple procedures. <ref> (See also Ginsberg 1993a, and Freuder and Mackworth 1994.) </ref> Another fundamental limitation of the best current backtrack-style search procedures is that they are all based on creating some form of resolution-style proof for inconsistency or unsatisfiability.
Reference: <author> Ginsberg, M. </author> <year> (1993b). </year> <booktitle> Essentials of artificial intelligence. </booktitle> <publisher> Morgan Kaufmann Publ., </publisher> <address> San Francisco, CA, </address> <year> 1993. </year>
Reference: <author> Ginsberg, M. </author> <year> (1995). </year> <booktitle> First Intl. Workshop on AI&OR. </booktitle> <address> Timberline, Oregon, OR (1995). </address>
Reference-contexts: Progress in this area has been spurred by the interaction of researchers in AI, operations re 7 search, and theory, by making available benchmark problems (DIMACS, see Trick and Johnson 1996), holding joint workshops <ref> (Ginsberg 1995) </ref>, and sharing algorithms and code. There is a growing consensus, however, that certain key technical challenges must be addressed in order to continue to increase the range of problems that can be practically solved. <p> However, much remains to be done to synergetically combine techniques from AI and OR. For example, at least for certain kinds of problems, such as the pigeon hole problem, the OR style representations offer a distinct computational advantage <ref> (Ginsberg 1995) </ref>. I plan to investigate ways to extend constraint-based and propositional representations, to incorporate features from OR-style representations based on linear and mixed integer programs. Another interesting application domain for compute-intensive methods from AI is in the area of computational biology.
Reference: <author> Ginsberg, M.L. and McAllester, D.A. </author> <year> (1994). </year> <title> GSAT and dynamic backtracking. </title> <booktitle> In Proc. </booktitle> <address> KR-94, Bonn, Germany, </address> <year> 1994. </year>
Reference: <author> Gogic, G., Kautz, H., Papadimitriou, C., and Selman, B. </author> <year> (1995). </year> <title> The comparative linguistics of knowledge representation. </title> <booktitle> Proc. IJCAI-95, </booktitle> <address> Montreal, Canada, </address> <year> 1995. </year>
Reference: <author> Gomes, C.P. and Selman, B. </author> <year> (1997a). </year> <title> Problem structure in the presence of perturbations Proc. </title> <address> AAAI-97, Providence, RI, </address> <year> 1997. </year>
Reference: <author> Gomes, C.P. and Selman, B. </author> <year> (1997b). </year> <title> Algorithm portfolio design: theory vs. practice, </title> <booktitle> Proc. </booktitle> <address> UAI-97, Providence, RI, </address> <year> 1997. </year>
Reference: <author> Gomes, C.P., Selman, B., and Crato N. </author> <year> (1997). </year> <title> Heavy-tailed distributions in combinatorial search. </title> <booktitle> Proc. Constraint Programming '97, </booktitle> <year> 1997. </year>
Reference-contexts: Hard instances can be generated at a certain critical level of perturbation of the original problem. Using these problem distributions, we showed that the runtime distribution of a stochastic backtrack-style procedure is highly non-standard <ref> (Gomes et al. 1997) </ref>. More specifically, we encountered so-called heavy-tailed distributions, characterized by infinite mean and infinite variance. We have since also found such non-standard distributions when solving real-world planning and scheduling problems. These results suggest a possible fractal dimension and self-similar characteristics of the combinatorial search space (Mandelbrot 1983).
Reference: <author> Haken, A. </author> <year> (1985). </year> <title> The intractability of resolution. </title> <journal> Theoretical Computer Science, </journal> <volume> Vol. </volume> <month> 39 </month> <year> (1985) </year> <month> 297-308. </month> <type> 16 Halpern, </type> <institution> J.Y. </institution> <year> (1997). </year> <title> A logical approach to reasoning about uncertainty: a tutorial. To appear in Discourse, Interaction, and Communication, by X. Arrazola, </title> <editor> K. Korta, and F.J. Pelletier (Eds.) </editor> <publisher> Kluwer, </publisher> <year> 1997. </year>
Reference: <author> Hampson, S. and Kibler, D. </author> <year> (1993). </year> <title> Plateaus and plateau search in Boolean satisfiability problems: When to give up searching and start again. DIMACS Challenge on Satisfiability Testing, </title> <year> 1993. </year>
Reference: <author> Hansen J. and Jaumard, B. </author> <year> (1990). </year> <title> Algorithms for the maximum satisfiability problem. </title> <journal> Computing, </journal> <volume> 44, </volume> <year> 1990, </year> <pages> 279-303. </pages>
Reference: <author> Hayes, B. </author> <year> (1997). </year> <title> Can't Get No Satisfaction. </title> <journal> American Scientist, </journal> <volume> Vol. 85, </volume> <month> March/April </month> <year> 1997, </year> <pages> pp. 108-112. </pages>
Reference: <author> Hogg, T., Huberman, B.A., and Williams, C.P. </author> <year> (1996). </year> <title> Phase transitions in problem solving. </title> <journal> Artificial Intelligence (special issue), </journal> <volume> Vol. 81, </volume> <year> 1996. </year>
Reference: <author> Hogg, T. and Williams, C.P. </author> <year> (1994a). </year> <title> Expected gains from parallelizing constraint solving for hard problems. </title> <note> AAAI-94 (1994) 331-336. </note>
Reference: <author> Hogg, T. and Williams, C.P. </author> <year> (1994b). </year> <title> The hardest constraint problems: a double phase transition. </title> <booktitle> Artif. Intell., </booktitle> <month> 69 </month> <year> (1994) </year> <month> 359-377. </month>
Reference: <author> Hooker, J.N. </author> <year> (1988). </year> <title> Resolution vs. cutting plane solution of inference problems: Some computational experience. </title> <type> Oper. </type> <institution> Res. Letter, </institution> <month> 7(1) </month> <year> (1988) </year> <month> 1-7. </month>
Reference: <editor> Horvitz, E. and Zilberstein S. (1996) (Eds.) </editor> <booktitle> Proc. Flexible Computation, AAAI Fall Symposium, </booktitle> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference: <author> Huberman, B.A., Lukose, R.M., and Hogg, T. </author> <year> (1997). </year> <title> An economics approach to hard computational problems. </title> <journal> Science, </journal> <volume> 265, </volume> <pages> 51-54. </pages>
Reference-contexts: Such strategies will therefore need to take into account the uncertainty in the availability of resources combined with the uncertainty about the exact nature of the instances under consideration. For related work, see the work on flexible computation <ref> (Huberman et al. 1997, Horvitz and Zilberstein 1996) </ref> and on bounded rationality (Russell and Norvig 1995, Russell and Subramanian 1993). 5 3 Robustness, Uncertainty, and the Brittleness of Encodings Encodings The success of compute-intensive approaches depends on our ability to find suitable encodings of real-world problems.
Reference: <author> Jiang, Y., Kautz, K., and Selman, B. </author> <year> (1995). </year> <title> Solving problems with hard and soft constraints using a stochastic algorithm for MAX-SAT. </title> <address> AI/OR Wrksh., OR (1995). </address>
Reference: <author> Johnson, D. </author> <year> (1996). </year> <title> Experimental analysis of algorithms: The good, the bad, and the ugly. </title> <booktitle> Invited Lecture, AAAI-96, </booktitle> <address> Portland, OR. </address>
Reference-contexts: However, there is the concern that we may be reaching a point where this is no longer the case, and that the simple random distributions now used for testing may be driving us in the wrong direction in our research <ref> (Johnson 1996) </ref>. In Gomes and Selman (1997a, 1997b), we introduce an alternative to pure random instances. In our model, we take a highly structured problem from the domain of finite algebra, and introduce an element of randomness to perturb its structure. <p> Being able to handle problem encodings of this size has significantly extended the practical value of constraint satisfaction and satisfiability procedures. Progress in this area has been spurred by the interaction of researchers in AI, operations re 7 search, and theory, by making available benchmark problems <ref> (DIMACS, see Trick and Johnson 1996) </ref>, holding joint workshops (Ginsberg 1995), and sharing algorithms and code. There is a growing consensus, however, that certain key technical challenges must be addressed in order to continue to increase the range of problems that can be practically solved.
Reference: <author> Johnson, D.S., Aragon, C.R., McGeoch, L.A., and Schevon, C. </author> <title> (1991) Optimization by simulated annealing: an experimental evaluation; part II. </title> <type> Oper. </type> <institution> Res., </institution> <month> 39 </month> <year> (1991). </year>
Reference: <author> Joslin, D. and Pollack, M. </author> <year> (1995). </year> <title> Passive and active decision postponement in plan generation. </title> <booktitle> In the European Workshop on Planning (EWSP), </booktitle> <address> Assisi, Italy, </address> <month> Sept. </month> <year> 1995. </year>
Reference: <author> Kambhampati, S. </author> <year> (1996). </year> <title> Refinement planning: status and prospectus. </title> <booktitle> Proc. AAAI-96, </booktitle> <address> Portland, OR., </address> <year> 1996, </year> <pages> 1331-1336. </pages> <note> (Invited Lecture) Kambhampati, </note> <author> S. </author> <year> (1997). </year> <title> Can we bridge refinement-based and SAT-based planning techniques? Proc. </title> <booktitle> IJCAI-97, </booktitle> <address> Nagoya, Japan, </address> <year> 1997. </year>
Reference: <author> Kasparov, G. </author> <year> (1997). </year> <title> The day that I sensed a new kind of intelligence. Time, </title> <month> May 26, </month> <year> 1997, </year> <pages> 66-67. </pages> <note> See also Time, </note> <month> March 25, </month> <year> 1996, </year> <note> p. 55. </note>
Reference-contexts: Deep Blue's performance led Kasparov to exclaim, "I could feel | I could smell | a new kind of intelligence across the table." Deep Blue derives its strength mainly from highly optimized search <ref> (Kasparov 1997, McDermott 1997) </ref>. Another dramatic development in the compute-intensive approach was the recent computer proof resolving the Robbins problem (Kolata 1996). The Robbins problem is a well-known problem in Boolean algebra, and was open for over sixty years.
Reference: <author> Kautz, H., Kearns, M., and Selman, B. </author> <year> (1995). </year> <title> Horn approximations of empirical data. </title> <journal> Artificial Intelligence, </journal> <volume> Vol. 74, </volume> <year> 1995, </year> <pages> 129-145. </pages>
Reference: <author> Kautz, H., McAllester, D., and Selman, B. </author> <year> (1996). </year> <title> Encoding plans in propositional logic. </title> <booktitle> Proceedings of the Fifth International Conference on Knowledge Representation and Reasoning (KR-96), </booktitle> <address> Cambridge, MA, </address> <year> 1996, </year> <pages> 374-384. </pages>
Reference-contexts: Deep Blue's performance and the resolution of Robbin's theorem are good examples of a qualitative change in performance of compute-intensive approaches compared to just a few years ago. In my own work, I've focused on compute-intensive methods with rich underlying representations. SATPLAN <ref> (Kautz and Selman 1996) </ref> is a planner that challenges the widespread belief in the AI community that planning is not amenable to general theorem-proving techniques. SATPLAN shows that general propositional satisfiability algorithms can outperform specialized planning systems on a range of benchmark problems. <p> Experience has shown that different encodings of the same problem can have vastly different computational properties. For example, in planning, "causal" encodings appear to be harder to solve than "state-based" encodings <ref> (Kautz et al. 1996) </ref>. The challenge that I'm interested in addressing is how to develop a general characterization of encodings that can be efficiently solved. This characterization may involve, for example, understanding the relationship between the encoding and the shape of the combinatorial search space. <p> So far, most work in this area has focused on compilation using a polynomially decidable target 10 language. Unfortunately, in certain cases the resulting approximations are exponentially longer than the originally theory, thus defeating the initial purpose of the compilation <ref> (Selman and Kautz 1996) </ref>. I plan to investigate compilation and approximation methods that transform a theory in one that is not necessarily polynomially decidable, but nevertheless has good computational properties for subsequent processing with the recent fast stochastic and systematic search and reasoning procedures. <p> I will restrict myself here to three example domains. However, I also plan to explore potential applications in other AI domains with hard computational problems, such as learning. Let's consider planning as a first example of an application domain. SATPLAN <ref> (Kautz and Sel-man 1996) </ref> showed that general propositional satisfiability algorithms can be used to build efficient general planners. Our approach, related to the work on constraint-based planning by Blum and Furst (1995), provided a novel perspective on AI planning. See, for example, Kambhampati (1996, 1997) and Ernst et al. (1997).
Reference: <author> Kautz, H. and Selman, B. </author> <year> (1992). </year> <title> Planning as satisfiability. </title> <type> ECAI-92, </type> <institution> Vienna, Austria, </institution> <year> 1992, </year> <pages> 359-363. </pages> <note> 17 Kautz, </note> <author> H. and Selman, B. </author> <title> (1994) An empirical evaluation of knowledge compilation. </title> <booktitle> Proc. AAAI-94, </booktitle> <address> Seat--tle, WA, </address> <year> 1994, </year> <pages> 155-161. </pages>
Reference: <author> Kautz, H. and Selman, B. </author> <year> (1996). </year> <title> Pushing the envelope: planning, propositional logic, and stochastic search. </title> <booktitle> Proc. AAAI-96, </booktitle> <address> Portland, OR, </address> <year> 1996, </year> <pages> 1194-1201. </pages>
Reference-contexts: Deep Blue's performance and the resolution of Robbin's theorem are good examples of a qualitative change in performance of compute-intensive approaches compared to just a few years ago. In my own work, I've focused on compute-intensive methods with rich underlying representations. SATPLAN <ref> (Kautz and Selman 1996) </ref> is a planner that challenges the widespread belief in the AI community that planning is not amenable to general theorem-proving techniques. SATPLAN shows that general propositional satisfiability algorithms can outperform specialized planning systems on a range of benchmark problems. <p> Experience has shown that different encodings of the same problem can have vastly different computational properties. For example, in planning, "causal" encodings appear to be harder to solve than "state-based" encodings <ref> (Kautz et al. 1996) </ref>. The challenge that I'm interested in addressing is how to develop a general characterization of encodings that can be efficiently solved. This characterization may involve, for example, understanding the relationship between the encoding and the shape of the combinatorial search space. <p> So far, most work in this area has focused on compilation using a polynomially decidable target 10 language. Unfortunately, in certain cases the resulting approximations are exponentially longer than the originally theory, thus defeating the initial purpose of the compilation <ref> (Selman and Kautz 1996) </ref>. I plan to investigate compilation and approximation methods that transform a theory in one that is not necessarily polynomially decidable, but nevertheless has good computational properties for subsequent processing with the recent fast stochastic and systematic search and reasoning procedures. <p> I will restrict myself here to three example domains. However, I also plan to explore potential applications in other AI domains with hard computational problems, such as learning. Let's consider planning as a first example of an application domain. SATPLAN <ref> (Kautz and Sel-man 1996) </ref> showed that general propositional satisfiability algorithms can be used to build efficient general planners. Our approach, related to the work on constraint-based planning by Blum and Furst (1995), provided a novel perspective on AI planning. See, for example, Kambhampati (1996, 1997) and Ernst et al. (1997).
Reference: <author> Kirkpatrick, S., Gelatt, </author> <title> C.D., and Vecchi, </title> <publisher> M.P. </publisher> <year> (1983). </year> <title> Optimization by simulated annealing. </title> <journal> Science, </journal> <volume> 220, </volume> <year> 1983, </year> <pages> 671-680. </pages>
Reference-contexts: Given that the three-dimensional structure of the protein determines its function, being able to predict the structure would have a tremendous impact on methods for designing drugs. Current methods for protein folding are generally based on numerical optimization techniques, such as simulated annealing <ref> (Kirkpatrick et al. 1983) </ref>. An interesting question is whether some of the enhancements of stochastic search methods developed for purely combinatorial problems, such as the mixed random walk technique (Selman et al. 1994, 1996), can be of use in the protein folding problem.
Reference: <author> Kirkpatrick, S. and Selman, B. </author> <year> (1994). </year> <title> Critical behavior in the satisfiability of random Boolean expressions. </title> <booktitle> Science, </booktitle> <month> 264 (May </month> <year> 1994) </year> <month> 1297-1301. </month> <note> Also p. 1249: "Mathematical Logic: Pinning Down a Treacherous Border in Logical Statements" by B. Cipra. </note>
Reference: <author> Kolata, G. </author> <year> (1996). </year> <title> Computer math proof shows reasoning power. </title> <address> New York Times, </address> <month> Dec. 10, </month> <year> 1996. </year>
Reference-contexts: Another dramatic development in the compute-intensive approach was the recent computer proof resolving the Robbins problem <ref> (Kolata 1996) </ref>. The Robbins problem is a well-known problem in Boolean algebra, and was open for over sixty years. The computer proof was found by applying powerful search techniques guided by general search tactics. Several aspects of the computer proof could be called "creative" by mathematicians' standards.
Reference: <author> Konolige, K. </author> <year> (1994). </year> <title> Easy to be hard: Difficult problems for greedy algorithms. </title> <address> KR-94, Bonn, Germany, </address> <year> 1994, </year> <pages> 374-378. </pages>
Reference: <author> Korf, R. </author> <year> (1991). </year> <title> Search. </title> <booktitle> Encyclopedia of Artificial Intelligence John Wiley, </booktitle> <address> New York (1991). </address>
Reference: <author> Korf, R. </author> <year> (1993). </year> <title> Linear-space best-first search. </title> <journal> Artificial Intelligence, </journal> <volume> Vol. 62(1), </volume> <pages> 41-78. </pages>
Reference: <author> Kosslyn, S.M. </author> <year> (1981). </year> <title> The medium and message in mental imagery: a theory. In Imagery by N. Block (Ed.), </title> <address> Cambridge MA: </address> <publisher> Bradford/MIT Press, </publisher> <year> 1981, </year> <pages> 207-244. </pages>
Reference-contexts: Pictorial representations are an example of analogues. The study of such representations is related to a large body of research on the use of visual images in human cognition <ref> (Kosslyn 1981, Levesque 1986, Selman 1997c, and Sloman 1971) </ref>. Uncertainty The real world is a highly uncertain place, and most reasoning involves determining what is likely, not just what is possible.
Reference: <author> Larrabee, T. </author> <year> (1992). </year> <title> Efficient generation of test patterns using Boolean satisfiability. </title> <journal> IEEE Trans. on CAD, </journal> <volume> Vol. 11, </volume> <year> 1992, </year> <pages> pp 4-15. </pages>
Reference-contexts: Examples include work on planning (Blum and Furst 1995; Kautz and Selman 1996), problems in finite algebra (Fujita et al. 1993), verification of hardware and software protocols, scheduling (Crawford and Baker 1994), circuit synthesis and diagnosis <ref> (Larrabee 1992) </ref>, and many other domains. Experience has shown that different encodings of the same problem can have vastly different computational properties. For example, in planning, "causal" encodings appear to be harder to solve than "state-based" encodings (Kautz et al. 1996).
Reference: <author> Larkin, J.H. and Simon, H.A. </author> <year> (1987). </year> <title> Why a picture is (sometimes) worth ten thousand Words. </title> <booktitle> Cognitive Science, </booktitle> <year> 1987. </year>
Reference: <author> Levesque, H.J. </author> <year> (1986). </year> <title> Making believers out of computers. </title> <journal> Artificial Intelligence, </journal> <volume> Vol. 30, </volume> <year> 1986, </year> <pages> 81-108. </pages>
Reference: <author> Levesque, H.J. and Brachman, R.J. </author> <year> (1985). </year> <title> A Fundamental Tradeoff in Knowledge Representation and Reasoning (Revised Version). In Readings in Knowledge Representation by R.J. </title> <note> Brachman and H.J. </note>
Reference: <editor> Levesque (Eds.), </editor> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1985, </year> <pages> 41-70. </pages>
Reference: <author> Mandelbrot, B. </author> <year> (1983). </year> <title> The fractal geometry of nature. </title> <publisher> Freeman: </publisher> <address> New York. </address> <year> 1983. </year>
Reference-contexts: More specifically, we encountered so-called heavy-tailed distributions, characterized by infinite mean and infinite variance. We have since also found such non-standard distributions when solving real-world planning and scheduling problems. These results suggest a possible fractal dimension and self-similar characteristics of the combinatorial search space <ref> (Mandelbrot 1983) </ref>. Our work also provides a first step in the direction of obtaining more realistic benchmark problems for evaluating reasoning and search procedures.
Reference: <author> Mazure, B., Sais, L., and Gregoire E. </author> <year> (1996). </year> <title> Boosting complete techniques thanks to local search methods. </title> <journal> Proc. Math & AI, </journal> <year> 1996. </year>
Reference: <author> McAllester, D., Selman, B., and Kautz, H. </author> <year> (1997). </year> <title> Evidence for invariants in local search. </title> <booktitle> Proc. </booktitle> <address> AAAI-97, Providence, RI, </address> <year> 1997. </year>
Reference: <author> McCune, W. </author> <year> (1996). </year> <title> Solution of the Robbins problem. </title> <type> Draft, </type> <year> 1996. </year>
Reference: <author> McDermott, D. </author> <year> (1997). </year> <title> Yes, Computers can think. </title> <address> New York Times, </address> <month> May 14, </month> <year> 1997. </year> <note> See ftp://ftp.cs.yale.edu/pub/mcdermott/papers/deepblue.txt. </note>
Reference-contexts: Deep Blue's performance led Kasparov to exclaim, "I could feel | I could smell | a new kind of intelligence across the table." Deep Blue derives its strength mainly from highly optimized search <ref> (Kasparov 1997, McDermott 1997) </ref>. Another dramatic development in the compute-intensive approach was the recent computer proof resolving the Robbins problem (Kolata 1996). The Robbins problem is a well-known problem in Boolean algebra, and was open for over sixty years.
Reference: <author> Minton, S., Johnston, M., Philips, A.B., and Laird, P. </author> <year> (1992). </year> <title> Minimizing conflicts: a heuristic repair method for constraint satisfaction and scheduling problems. </title> <journal> Artificial Intelligence, </journal> <month> 58 </month> <year> (1992) </year> <month> 161-205. </month>
Reference: <author> Mitchell, D., Selman, B., and Levesque, H.J. </author> <year> (1992). </year> <title> Hard and easy distributions of SAT problems. </title> <booktitle> Proc. AAAI-92, </booktitle> <address> San Jose, CA (1992) 459-465. </address>
Reference: <author> Monasson, R., Zecchina, R., Kirkpatrick, S., Selman, B., and Troyansky, L. </author> <year> (1996). </year> <title> Phase Transition and Search Cost in the 2+p-SAT Problem. </title> <booktitle> Proceedings of PhysComp-96, </booktitle> <address> Boston, MA, </address> <year> 1996. </year>
Reference-contexts: In fact, some of the mathematical models from statistical physics can be used to capture key properties of combinatorial search problems. In our most recent work <ref> (Monasson et al. 1996) </ref>, we study a mixture of instance distributions from a tractable and an intractable class. More specifically, we consider a mixture of binary and ternary propositional clauses.
Reference: <author> Newell, A. and Simon, H.A. </author> <year> (1971). </year> <title> Human problem solving. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ. 18 Nilsson, N.J. </address> <booktitle> (1986) Probbabilistic logic. Artificial Intelligence, </booktitle> <volume> Vol. 28(1), </volume> <year> 1986. </year>
Reference: <author> Papadimitriou, C.H. </author> <year> (1993). </year> <title> Computational Complexity. </title> <publisher> Addison Wesley, </publisher> <year> 1993. </year>
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic reasoning in intelligent systems: Networks of plausible Inference. </title> <publisher> Morgan Kaufmann Publ., </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: Another research challenge is therefore to find proper ways of extending purely constraint-based and propositional encodings to incorporate notions of probability and uncertainty. In recent years, there has been tremendous activity in the area of uncertainty and AI, as witnessed by the increasing prominence of the UAI community <ref> (Pearl 1988) </ref>. With respect to knowledge representation schemes, there has also been significant progress in incorporating notions of probability and uncertainty in more standard logical representations. (For a recent review of the area, see Halpern 1997.) In general such proposals increase the computational complexity of the underlying representations.
Reference: <author> Russell, S. and Norvig P. </author> <year> (1995). </year> <title> Artificial Intelligence a Modern Approach. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1995. </year>
Reference-contexts: For related work, see the work on flexible computation (Huberman et al. 1997, Horvitz and Zilberstein 1996) and on bounded rationality <ref> (Russell and Norvig 1995, Russell and Subramanian 1993) </ref>. 5 3 Robustness, Uncertainty, and the Brittleness of Encodings Encodings The success of compute-intensive approaches depends on our ability to find suitable encodings of real-world problems. There has been significant recent progress along this front using general constraint-based representations and propositional encodings.
Reference: <author> Russell, S. and Subramanian, D. </author> <year> (1993). </year> <title> Provably bounded optimal agents. </title> <booktitle> Proc. IJCAI-93, </booktitle> <address> Chambery, France, </address> <year> 1993. </year>
Reference: <author> Selman, B. </author> <year> (1993). </year> <title> Non-systematic search methods for model finding. </title> <booktitle> Proc. of the 1993 IEEE Intl. Conf. on Tools with AI, </booktitle> <address> Boston, MA, </address> <year> 1993. </year> <note> (Invited position paper.) </note> <author> Selman, B. </author> <year> (1994a). </year> <title> Near-optimal plans, tractability, and reactivity. </title> <booktitle> Proceedings of the Fourth International Conference on Knowledge Representation and Reasoning (KR-94), </booktitle> <address> Bonn, Germany, </address> <year> 1994, </year> <pages> 521-529. </pages>
Reference: <author> Selman, B. </author> <year> (1994b). </year> <title> Domain-specific complexity tradeoffs. </title> <booktitle> Proc. of the Twelfth European Conference on Artificial Intelligence (ECAI-94), </booktitle> <address> Amsterdam, The Netherlands. </address>
Reference: <author> Selman, B. </author> <year> (1995a). </year> <title> Stochastic search and phase transitions: AI meets physics. </title> <booktitle> Proc. IJCAI-95, 1995. (Invited lecture.) Selman, Bart. </booktitle> <year> (1995b). </year> <title> Randomized search strategies for model finding. </title> <booktitle> Proc. IJCAI-95, </booktitle> <address> Montreal, Canada, </address> <year> 1995. </year> <title> (Invited position statement.) </title> <editor> Selman, B. </editor> <booktitle> (1996). Computational challenges in artificial intelligence. ACM Computing Surveys, </booktitle> <address> 28(4es), </address> <year> 1996. </year>
Reference: <author> Selman, B. </author> <year> (1997a). </year> <title> Stochastic Search. </title> <journal> Artificial Intelligence Magazine. </journal> <note> (Invited survey; to appear.) </note> <author> Selman, B. </author> <year> (1997b). </year> <title> Greedy local search. </title> <booktitle> In MIT Encyclopedia of the Cognitive Sciences (MITECS), </booktitle> <address> Cam-bridge: MIT Press. </address> <note> (to appear.) </note> <author> Selman, B. </author> <year> (1997c). </year> <title> Analogical Representations. In Cognition and Knowledge Representation, </title> <journal> Z. </journal> <note> Pylyshyn (Ed.). (to appear) Selman, </note> <author> B., Brooks, R., Dean, T., and Horvitz, E., Mitchell, T., and Nilsson, N. </author> <title> (1996) Challenge problems for artificial intelligence. </title> <booktitle> Proc. AAAI-96, </booktitle> <address> Portland, OR, </address> <year> 1996, </year> <pages> 1340-1345. </pages>
Reference: <author> Selman, B. and Kautz, H.A. </author> <year> (1993a). </year> <title> An empirical study of greedy local search strategies for satisfiability testing. </title> <booktitle> Proc. AAAI-93, </booktitle> <address> Wash., DC (1993) 46-51. </address>
Reference: <author> Selman, B. and Kautz, H.A. </author> <year> (1993b). </year> <title> Domain-independent extensions to GSAT: Solving large structured satisfiability problems. </title> <booktitle> IJCAI-93, </booktitle> <address> France (1993) 290-295. </address>
Reference: <author> Selman, B. and Kautz, H. </author> <year> (1996). </year> <title> Knowledge compilation and theory approximation. </title> <journal> Journal of the ACM, </journal> <volume> Vol. 43, No. 2, </volume> <year> 1996, </year> <pages> 193-224. </pages>
Reference-contexts: Deep Blue's performance and the resolution of Robbin's theorem are good examples of a qualitative change in performance of compute-intensive approaches compared to just a few years ago. In my own work, I've focused on compute-intensive methods with rich underlying representations. SATPLAN <ref> (Kautz and Selman 1996) </ref> is a planner that challenges the widespread belief in the AI community that planning is not amenable to general theorem-proving techniques. SATPLAN shows that general propositional satisfiability algorithms can outperform specialized planning systems on a range of benchmark problems. <p> So far, most work in this area has focused on compilation using a polynomially decidable target 10 language. Unfortunately, in certain cases the resulting approximations are exponentially longer than the originally theory, thus defeating the initial purpose of the compilation <ref> (Selman and Kautz 1996) </ref>. I plan to investigate compilation and approximation methods that transform a theory in one that is not necessarily polynomially decidable, but nevertheless has good computational properties for subsequent processing with the recent fast stochastic and systematic search and reasoning procedures.
Reference: <author> Selman, B., Kautz, H., and Cohen, B. </author> <year> (1994). </year> <title> Noise strategies for local search. </title> <booktitle> Proc. (AAAI-94), </booktitle> <address> Seattle, WA, </address> <year> 1994, </year> <pages> 337-343. </pages>
Reference-contexts: Current methods for protein folding are generally based on numerical optimization techniques, such as simulated annealing (Kirkpatrick et al. 1983). An interesting question is whether some of the enhancements of stochastic search methods developed for purely combinatorial problems, such as the mixed random walk technique <ref> (Selman et al. 1994, 1996) </ref>, can be of use in the protein folding problem. This is an open question at this point, but in other numerical optimization problems there has been success in using combinatorial techniques, such as graph coloring methods, as effective subroutines in numerical procedures.
Reference: <author> Selman, B., Kautz, H., and Cohen, B. </author> <year> (1996). </year> <title> Local search strategies for satisfiability testing. </title> <booktitle> Dimacs Series in Discrete Mathematics and Theoretical Computer Science, </booktitle> <volume> Vol. 26, </volume> <year> 1996, </year> <pages> 521-532. </pages>
Reference-contexts: Deep Blue's performance and the resolution of Robbin's theorem are good examples of a qualitative change in performance of compute-intensive approaches compared to just a few years ago. In my own work, I've focused on compute-intensive methods with rich underlying representations. SATPLAN <ref> (Kautz and Selman 1996) </ref> is a planner that challenges the widespread belief in the AI community that planning is not amenable to general theorem-proving techniques. SATPLAN shows that general propositional satisfiability algorithms can outperform specialized planning systems on a range of benchmark problems. <p> So far, most work in this area has focused on compilation using a polynomially decidable target 10 language. Unfortunately, in certain cases the resulting approximations are exponentially longer than the originally theory, thus defeating the initial purpose of the compilation <ref> (Selman and Kautz 1996) </ref>. I plan to investigate compilation and approximation methods that transform a theory in one that is not necessarily polynomially decidable, but nevertheless has good computational properties for subsequent processing with the recent fast stochastic and systematic search and reasoning procedures.
Reference: <author> Selman, B., Kautz, H., and McAllester, D. </author> <title> (1997) Computational challenges in propositional reasoning and search. </title> <booktitle> Proc. IJCAI-97, </booktitle> <address> Nagoya, Japan, </address> <year> 1997. </year> <note> 19 Selman, </note> <author> B. and Kirkpatrick, S. </author> <year> (1996). </year> <title> Finite-size scaling of the computational cost of systematic search. </title> <journal> Artificial Intelligence, </journal> <volume> Vol. 81, </volume> <year> 1996, </year> <pages> 273-295. </pages>
Reference: <author> Selman, B., and Levesque, H. </author> <year> (1996). </year> <title> Support set selection for abductive and default reasoning. </title> <journal> Artificial Intelligence, </journal> <volume> Vol. 82, </volume> <year> 1996, </year> <pages> 259-272. </pages>
Reference-contexts: Deep Blue's performance and the resolution of Robbin's theorem are good examples of a qualitative change in performance of compute-intensive approaches compared to just a few years ago. In my own work, I've focused on compute-intensive methods with rich underlying representations. SATPLAN <ref> (Kautz and Selman 1996) </ref> is a planner that challenges the widespread belief in the AI community that planning is not amenable to general theorem-proving techniques. SATPLAN shows that general propositional satisfiability algorithms can outperform specialized planning systems on a range of benchmark problems. <p> So far, most work in this area has focused on compilation using a polynomially decidable target 10 language. Unfortunately, in certain cases the resulting approximations are exponentially longer than the originally theory, thus defeating the initial purpose of the compilation <ref> (Selman and Kautz 1996) </ref>. I plan to investigate compilation and approximation methods that transform a theory in one that is not necessarily polynomially decidable, but nevertheless has good computational properties for subsequent processing with the recent fast stochastic and systematic search and reasoning procedures.
Reference: <author> Selman, B., Levesque, H., and Mitchell, D. </author> <year> (1992). </year> <title> A new method for solving hard satisfiability problems. </title> <booktitle> Proc. (AAAI-92), </booktitle> <address> San Jose, CA, </address> <year> 1992, </year> <pages> 440-446. </pages>
Reference-contexts: SATPLAN shows that general propositional satisfiability algorithms can outperform specialized planning systems on a range of benchmark problems. At the core of SATPLAN lies a powerful model finding procedure. This procedure was developed as part of my work on stochastic model finding methods for propositional theories, such as GSAT <ref> (Selman et al. 1992, Selman 1995a, 1997a) </ref>. These stochastic methods have significantly extended the range and size of constraint and satisfiability problems that can be solved effectively. It has now become feasible to solve problem instances with tens of thousands of variables and up to several million constraints.
Reference: <author> Selman, B., Mitchell, D., and Levesque, H. </author> <year> (1996). </year> <title> Generating hard satisfiability problems. </title> <journal> Artificial Intelligence, </journal> <volume> Vol. 81, </volume> <year> 1996, </year> <pages> 17-29. </pages>
Reference-contexts: Deep Blue's performance and the resolution of Robbin's theorem are good examples of a qualitative change in performance of compute-intensive approaches compared to just a few years ago. In my own work, I've focused on compute-intensive methods with rich underlying representations. SATPLAN <ref> (Kautz and Selman 1996) </ref> is a planner that challenges the widespread belief in the AI community that planning is not amenable to general theorem-proving techniques. SATPLAN shows that general propositional satisfiability algorithms can outperform specialized planning systems on a range of benchmark problems. <p> So far, most work in this area has focused on compilation using a polynomially decidable target 10 language. Unfortunately, in certain cases the resulting approximations are exponentially longer than the originally theory, thus defeating the initial purpose of the compilation <ref> (Selman and Kautz 1996) </ref>. I plan to investigate compilation and approximation methods that transform a theory in one that is not necessarily polynomially decidable, but nevertheless has good computational properties for subsequent processing with the recent fast stochastic and systematic search and reasoning procedures.
Reference: <author> Smith, B. and Grant S.A. </author> <year> (1996). </year> <title> Sparse constraint graphs and exceptionally hard problems. </title> <booktitle> IJCAI-95, </booktitle> <pages> 646-651, </pages> <year> 1995. </year> <note> Full version in AIJ (Hogg et al. </note> <year> 1996). </year>
Reference: <author> Sloman, A. </author> <year> (1971). </year> <title> Interactions between philosophy and AI | the role of intuition and non-logical reasoning in intelligence. </title> <journal> Artificial Intelligence, </journal> <volume> 2, </volume> <year> 1971, </year> <pages> 209-225. </pages>
Reference: <author> Smith, B. and Dyer, M. </author> <year> (1996). </year> <title> Locating the phase transition in binary constraint satisfaction problems. </title> <journal> Artificial Intelligence, </journal> <volume> 81, </volume> <year> 1996. </year>
Reference: <author> Tesauro, G. </author> <year> (1995). </year> <title> Temporal difference learning and TD-Gammon. </title> <journal> Communications of the ACM, March 1995, </journal> <volume> Vol. 38, No. </volume> <pages> 3. </pages>
Reference: <editor> Trick, M. and Johnson, D. (Eds.) </editor> <title> (1996) Cliques, Coloring and Satisfiability. </title> <booktitle> DIMACS Series in Discrete Mathematics and Theoretical Computer Science, </booktitle> <volume> Vol. </volume> <pages> 26. </pages> <note> For benchmarks, see http://dimacs.rutgers.edu/Challenges/index.html. van Hentenryck, </note> <author> P., Deville, Y., </author> <title> and Teng Choh-Man (1992) A generic arc consistency algorithm and its specializations. </title> <journal> Artificial Intelligence, </journal> <volume> 57, </volume> <year> 1992. </year>
Reference-contexts: Being able to handle problem encodings of this size has significantly extended the practical value of constraint satisfaction and satisfiability procedures. Progress in this area has been spurred by the interaction of researchers in AI, operations re 7 search, and theory, by making available benchmark problems <ref> (DIMACS, see Trick and Johnson 1996) </ref>, holding joint workshops (Ginsberg 1995), and sharing algorithms and code. There is a growing consensus, however, that certain key technical challenges must be addressed in order to continue to increase the range of problems that can be practically solved.
Reference: <author> Williams, B.C. and Nayak, P. </author> <title> (1996) A model-based approach to reactive self-configuring systems. </title> <booktitle> Proceedings AAAI-96, </booktitle> <address> Portland, OR. </address> <pages> 971-978. </pages>
Reference: <author> Zhang, W. and Korf, R. </author> <year> (1996). </year> <title> A Study of Complexity Transitions on the Asymmetric Travelling Salesman Problem. </title> <journal> Artificial Intelligence, </journal> <volume> 81, </volume> <year> 1996. </year> <month> 20 </month>
Reference-contexts: Our work also provides a first step in the direction of obtaining more realistic benchmark problems for evaluating reasoning and search procedures. An obvious next step is to design problem generators for instances with properties that are even closer to those of real-world instances. <ref> (See also Dean et al. 1996, and Zhang and Korf 1996) </ref>. The study of more structured benchmark problems has a direct payoff in terms of the design of algorithms. I plan to study further the heavy-tailed and self-similar nature of combinatorial search and its implications for algorithm design.
References-found: 101

