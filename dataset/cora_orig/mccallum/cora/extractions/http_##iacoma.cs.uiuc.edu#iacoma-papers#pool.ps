URL: http://iacoma.cs.uiuc.edu/iacoma-papers/pool.ps
Refering-URL: http://iacoma.cs.uiuc.edu/papers.html
Root-URL: http://www.cs.uiuc.edu
Email: Email: lyang,torrella@csrd.uiuc.edu  
Title: Optimizing Primary Data Caches for Parallel Scientific Applications: The Pool Buffer Approach 1  
Author: Liuxi Yang and Josep Torrellas 
Address: IL 61801  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign,  
Abstract: Optimizing on-chip primary data caches for parallel scientific applications is challenging because different applications exhibit different behavior. Indeed, while some applications exhibit good spatial locality, others have accesses with long strides that prevent the effective use of cache lines. Finally, other applications cannot exploit long lines because they exhibit false sharing. To help processors execute these three types of applications efficiently, we introduce the Pool Buffer, a small direct-mapped cache accessed in parallel with the primary cache. The function of the pool buffer is to fetch long sectors of relatively short cache lines from memory on a miss, while only letting into the cache the lines that the processor actually references. The pool buffer can also perform sequential prefetching of sectors. An evaluation of the pool buffer based on simulations of five 32-processor Perfect Club codes yields encouraging results. Adding a pool buffer of one-quarter the size of the cache causes a small increase in area while usually achieving large reductions in execution time. For example, for a range of caches with 32-byte lines, the execution time decreases by an average of about 20%. We also show that small 1-Kbyte buffers are often large enough to get most of the potential benefits. Finally, caches with pool buffers are more effective than caches with long lines and no pool buffer. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Berry et al. </author> <title> The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference-contexts: In this section, we describe the applications used, the simulator, and the evaluation methodology. 3.1 Applications We trace the parallel versions of the five Perfect Club fortran codes <ref> [1] </ref> presented in Table 1. These versions run with 32 processors and were parallelized using a parallelizing compiler and later by hand [4]. Loop iterations are statically assigned to processors in chunks.
Reference: [2] <author> B. Bray and M. Flynn. </author> <title> Fetch Caches. </title> <type> Technical Report CSL-TR-93-561, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: For example, miss caches [5], victim caches [5], and selective victim caches [12] all try to eliminate conflict misses. Fetch caches <ref> [2] </ref> are useful for caches with multicycle access times, while two-level on-chip caches [6] try to increase the effective size of the on-chip cache without excessively increasing its average access time. Overall, while all these schemes are useful, they do not specifically focus on the problem that we mentioned above.
Reference: [3] <author> F. Dahlgren, M. Dubois, and P. Stenstrom. </author> <title> Fixed and Adaptive Sequential Prefetching in Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <pages> pages I:56-63, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: Clearly, a plain primary cache does not handle most of these cases optimally. Several hardware schemes have been proposed to address some of these problems. One approach is to combine relatively short cache lines with prefetching. In the simplest prefetching scheme, called sequential prefetching <ref> [3] </ref>, a cache access or a cache miss triggers the prefetch of the next line or next several lines. With this approach, spatial locality can be exploited while minimizing false sharing. <p> is minimized if relatively short cache lines are used because each cache line in a sector can be separately invalidated. 2.3 Comparison to Other Schemes To finish this section, we compare the pool buffer to two other related schemes that improve the performance of the primary cache. * Sequential Prefetching <ref> [3] </ref>. The main advantage of a non-prefetching pool buffer over access- or miss-triggered sequential prefetching of cache lines is that the primary cache is not polluted with cache lines that will not be used.
Reference: [4] <author> R. Eigenmann, J. Hoeflinger, G. Jaxon, and D. Padua. </author> <title> The Cedar Fortran Project. </title> <type> Technical Report 1262, </type> <institution> Center for Supercomputing Research and Development, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: These versions run with 32 processors and were parallelized using a parallelizing compiler and later by hand <ref> [4] </ref>. Loop iterations are statically assigned to processors in chunks. Since the codes take a long time to run, we reduced their time requirements while preserving their parallelism and reference behavior. We did this by reducing the number of iterations rather than the data set sizes where possible.
Reference: [5] <author> N. Jouppi. </author> <title> Improving Direct-Mapped Cache Performance by the Addition of a Small Fully-Associative Cache and Prefetch Buffers. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 364-373, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: There are other, more advanced forms of hardware prefetching that are more efficient. However, while they may bring fewer useless lines into the cache, they are more complex to implement and use more chip area. A related scheme that has been proposed for uniprocessors is stream buffers <ref> [5, 9] </ref>. Stream buffers are usually placed off-chip. They identify vector strides and then prefetch vector elements that are expected to be referenced. However, they may not always recognize the stride of the vector access correctly and, for best results, they need to be organized as FIFO buffers. <p> Furthermore, they may not work very well with non-vector data. Finally, there are many other optimization schemes for primary data caches that are either specifically designed to eliminate a certain type of misses or that try to overcome a certain obstacle. For example, miss caches <ref> [5] </ref>, victim caches [5], and selective victim caches [12] all try to eliminate conflict misses. Fetch caches [2] are useful for caches with multicycle access times, while two-level on-chip caches [6] try to increase the effective size of the on-chip cache without excessively increasing its average access time. <p> Furthermore, they may not work very well with non-vector data. Finally, there are many other optimization schemes for primary data caches that are either specifically designed to eliminate a certain type of misses or that try to overcome a certain obstacle. For example, miss caches <ref> [5] </ref>, victim caches [5], and selective victim caches [12] all try to eliminate conflict misses. Fetch caches [2] are useful for caches with multicycle access times, while two-level on-chip caches [6] try to increase the effective size of the on-chip cache without excessively increasing its average access time. <p> Indeed, loading a sector in one message is less disruptive that loading several, smaller-sized lines as in sequential prefetching. * Stream Buffer <ref> [5] </ref>. One limitation of the stream buffer is that, for the most part, it is targeted to vector accesses only. Furthermore, it may not always recognize the stride of the vector access.
Reference: [6] <author> N. Jouppi and S. Wilton. </author> <title> Tradeoffs in Two Level On-Chip Caching. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 34-45, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: For example, miss caches [5], victim caches [5], and selective victim caches [12] all try to eliminate conflict misses. Fetch caches [2] are useful for caches with multicycle access times, while two-level on-chip caches <ref> [6] </ref> try to increase the effective size of the on-chip cache without excessively increasing its average access time. Overall, while all these schemes are useful, they do not specifically focus on the problem that we mentioned above. In this paper, we introduce a hardware-only solution to the problem indicated above.
Reference: [7] <author> C. Kruskal and M. Snir. </author> <title> The Performance of Multistage Interconnection Networks for Multiprocessors. </title> <journal> In IEEE Trans. on Computers, </journal> <pages> pages 1091-98, </pages> <month> December </month> <year> 1983. </year>
Reference-contexts: Since we use the release consistency model, write misses do not stall the processors. Read misses, however, stall the processors. In our simulations, we accurately model the contention in the system. For the network, we use the analytical delay model for indirect multistage networks presented in <ref> [7] </ref>. The channels of the network are 64-bit wide. As indicated before, both primary cache and pool buffer are accessed at the same time. If the access hits in any of the two, the data is returned to the processor in one cycle.
Reference: [8] <author> J. M. Mulder, N. T. Quach, and M. Flynn. </author> <title> An Area Model for On-Chip Memories and its Application. </title> <journal> In IEEE Journal of Solid-State Circuits, </journal> <pages> pages 98-106, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: To determine the area, we use the model proposed by Mulder et al <ref> [8] </ref>. The model gives the area in register-bit equivalents (or rbes). Figure 3 shows the rbe cost for different cache configurations. The figure shows two sets of two curves: the set at the bottom corresponds to plain caches, while the set at the top corresponds to caches with pool buffers.
Reference: [9] <author> S. Palacharla and R. Kessler. </author> <title> Evaluating Stream Buffers as a Secondary Cache Replacement. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 24-33, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: There are other, more advanced forms of hardware prefetching that are more efficient. However, while they may bring fewer useless lines into the cache, they are more complex to implement and use more chip area. A related scheme that has been proposed for uniprocessors is stream buffers <ref> [5, 9] </ref>. Stream buffers are usually placed off-chip. They identify vector strides and then prefetch vector elements that are expected to be referenced. However, they may not always recognize the stride of the vector access correctly and, for best results, they need to be organized as FIFO buffers.
Reference: [10] <author> C. D. Polychronopoulos et al. </author> <title> Parafrase-2: An Environment for Parallelizing, Partitioning, Synchronizing, and Scheduling Programs on Multiprocessors. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 39-48, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: Table 1 shows the number of lines of code and the number of references simulated in each application. We trace array data references only. 3.2 Simulation System For our simulations, we use the EPG-sim execution-driven simulator [11]. We start by compiling our applications with the Parafrase-2 compiler <ref> [10] </ref>. For each array access in the program, a pass in the compiler introduces a call to the simulator. Each call takes as arguments the address of the data accessed, the processor ID, the type of access, and the time-stamp.
Reference: [11] <author> D. K. Poulsen and P.-C. Yew. </author> <title> Execution Driven Tools for Parallel Simulation of Parallel Architectures and Applications. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 860-869, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Table 1 shows the number of lines of code and the number of references simulated in each application. We trace array data references only. 3.2 Simulation System For our simulations, we use the EPG-sim execution-driven simulator <ref> [11] </ref>. We start by compiling our applications with the Parafrase-2 compiler [10]. For each array access in the program, a pass in the compiler introduces a call to the simulator.
Reference: [12] <author> D. Stiliadis and A. Varma. </author> <title> Selective Victim Caching: </title>
Reference-contexts: Finally, there are many other optimization schemes for primary data caches that are either specifically designed to eliminate a certain type of misses or that try to overcome a certain obstacle. For example, miss caches [5], victim caches [5], and selective victim caches <ref> [12] </ref> all try to eliminate conflict misses. Fetch caches [2] are useful for caches with multicycle access times, while two-level on-chip caches [6] try to increase the effective size of the on-chip cache without excessively increasing its average access time.
References-found: 12

