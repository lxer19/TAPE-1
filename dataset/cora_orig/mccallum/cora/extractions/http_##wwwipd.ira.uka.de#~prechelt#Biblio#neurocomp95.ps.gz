URL: http://wwwipd.ira.uka.de/~prechelt/Biblio/neurocomp95.ps.gz
Refering-URL: 
Root-URL: 
Title: Some Notes on Neural Learning Algorithm Benchmarking  
Author: Lutz Prechelt 
Keyword: benchmarks, methodology, validity, reproducibility, comparability  
Note: Appeared in journal "Neurocomputing",  
Address: D-76128 Karlsruhe, Germany  
Affiliation: Fakultat fur Informatik Universitat Karlsruhe  
Email: (prechelt@ira.uka.de)  
Phone: +49/721/608-4068, Fax: +49/721/694092  
Date: 1995  
Abstract: New neural learning algorithms are often benchmarked only poorly. This article gathers some important DOs and DON'Ts for researchers in order to improve on that situation. The essential requirements are (1) Volume: benchmarking has to be broad enough, i.e., must use several problems; (2) Validity: common errors that invalidate the results have to be avoided; (3) Reproducibility: benchmarking has to be documented well enough to be completely reproducible; and (4) Comparability: benchmark results should, if possible, be directly comparable with the results achieved by others using different algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> David W. Aha. </author> <title> Generalizing from case studies: A case study. </title> <editor> In Derek Sleeman and Peter Edwards, editors, </editor> <booktitle> Machine Learning Proc. of the 9th Intl. Workshop, </booktitle> <pages> pages 1-10, </pages> <address> San Mateo, CA, July 1992. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: With a smaller number it is impossible to characterize the behavior of a new algorithm in comparison to known ones. The most useful setup is to use both artificial datasets [3], whose characteristics are known exactly, and real datasets, which may have some surprising and very irregular properties. <ref> [1] </ref> outlines a method for deriving additional artificial datasets from existing real datasets with known characteristics; the method can be used if insufficient amounts of real data are available or if the influence of certain dataset characteristics are to be explored systematically. 3 Validity Every once in a while articles appear
Reference: [2] <author> Larry B. Christensen. </author> <title> Experimental Methodology. </title> <publisher> Allyn and Bacon, </publisher> <address> Needham Heights, MA, 6th edition, </address> <year> 1994. </year>
Reference-contexts: This violates one 2 of the most basic requirements for valid experimental science <ref> [2] </ref>. The most frequent problems are incomplete specification of the values used for the free parameters of the algorithm and the use of training data that nobody else can exactly reproduce.
Reference: [3] <author> Ray J. Hickey. </author> <title> Artificial universes: Towards a systematic approach to evaluating algorithms which learn from examples. </title> <editor> In Derek Sleeman and Peter Edwards, editors, </editor> <booktitle> Machine Learning Proc. of the 9th Intl. Workshop, </booktitle> <pages> pages 196-205, </pages> <address> San Mateo, CA, July 1992. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: With a smaller number it is impossible to characterize the behavior of a new algorithm in comparison to known ones. The most useful setup is to use both artificial datasets <ref> [3] </ref>, whose characteristics are known exactly, and real datasets, which may have some surprising and very irregular properties. [1] outlines a method for deriving additional artificial datasets from existing real datasets with known characteristics; the method can be used if insufficient amounts of real data are available or if the influence <p> FTP availability of data is preferable even for artificial datasets (that could also be represented by their generation rules <ref> [3] </ref>) in order to avoid human error and stochastic deviations during a reproduction. 5 Comparability A benchmark is most useful if its results can directly be compared with results obtained by others for other algorithms. However, this is hardly ever the case in neural learning algorithm benchmarking today.
Reference: [4] <author> Lutz Prechelt. </author> <title> PROBEN1 | A set of benchmarks and benchmarking rules for neural network training algorithms. </title> <type> Technical Report 21/94, </type> <institution> Fakultat fur Informatik, Universitat Karlsruhe, Germany, </institution> <month> September </month> <year> 1994. </year> <note> Anonymous FTP: /pub/papers/techreports/1994/1994-21.ps.gz on ftp.ira.uka.de. </note>
Reference-contexts: This dataset collection 1 is documented in a technical report <ref> [4] </ref>, which also contains advice on how to perform and report benchmarks. This is how Proben1 tries to avoid the problems: Volume is possible because the collection contains 45 datasets for 15 different learning problems from 12 different domains.
Reference: [5] <author> Lutz Prechelt. </author> <title> A quantitative study of neural network learning algorithm evaluation practices. </title> <booktitle> In Proc. 4th Intl. Conf. on Artificial Neural Networks, </booktitle> <address> Cambridge, UK, </address> <month> June 26-28, </month> <year> 1995. </year> <note> IEE. Anonymous FTP: /pub/papers/techreports/1994/1994-19.ps.gz on ftp.ira.uka.de. 4 </note>
Reference-contexts: The purpose of the present article is to make the researchers in the field more aware of these problems and to help avoiding them in the future. 2 Volume A recent investigation <ref> [5] </ref> has found benchmarking to be remarkably scarce for neural network learning algorithms, even in journal articles. 34% of all articles presenting a learning algorithm (113 articles were investigated) used zero non-toy learning problems for benchmarking, 41% used but one, and only 6% used more than two! It is impossible to
References-found: 5

