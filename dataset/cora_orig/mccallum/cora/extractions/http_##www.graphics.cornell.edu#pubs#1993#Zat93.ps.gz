URL: http://www.graphics.cornell.edu/pubs/1993/Zat93.ps.gz
Refering-URL: http://www.graphics.cornell.edu/pubs/1993/Zat93.html
Root-URL: 
Title: GALERKIN RADIOSITY: A HIGHER ORDER SOLUTION METHOD FOR GLOBAL ILLUMINATION SIGGRAPH '93 edition  
Author: Harold R. Zatz 
Degree: A Thesis Presented to the Faculty of the Graduate School  in Partial Fulfillment of the Requirements for the Degree of Master of Science by  
Date: August 1992  
Affiliation: of Cornell University  
Abstract-found: 0
Intro-found: 1
Reference: [BAUM 89] <author> Daniel R. Baum, Holly E. Rushmeier, and James M. Winget, </author> <title> Improved Radiosity Solutions Through the Use of Analytically Determined Form-Factors, </title> <journal> Computer Graphics, </journal> <volume> 23(3), </volume> <pages> pp. 325-334, </pages> <year> 1989. </year>
Reference-contexts: Both approximate [GORA 84,COHE 85] and analytic <ref> [BAUM 89] </ref> approaches use planar surfaces, even though such a restriction is not a fundamental part of the radiosity model. Wallace's approach [WALL 89] allows the surface normal to vary across planar surfaces, reducing the error caused by tessellating a curved surface. <p> between differential areas i and j i Angle between the surface normal of differential area i and a line between differential areas i and j 9 and j. tional radiosity techniques reduce the problem to finite dimensions by replacing differential areas with finite areas. 2.1 Conventional Radiosity Conventional radiosity methods <ref> [BAUM 89] </ref> [CAMP 90] [COHE 85] [COHE 88] [GORA 84] [HANR 91] [NISH 85] [WALL 89] all depend upon the assumption that radiosity values are constant over finite regions. This has been referred to as the Constant Radiosity Assumption, or CRA [TAMP 91]. <p> The ray-traced form factor method [WALL 89] samples the double-differential form factor between a number of points on each transmitting and receiving surface. For polygonal surfaces, the CRA allows the use of an analytic formulation for the polygon-to-point form factor <ref> [BAUM 89] </ref>. This reduces the problem of computing an accurate form factor between two polygons to a sampling problem across one of the polygons. <p> Conventional radiosity uses a collocation technique; radiosity values are computed at a number of sample points, based upon assumptions about radiosity values elsewhere in the scene. The usual assumption is that radiosity values within some finite region are constant, allowing the use of an analytic form factor <ref> [BAUM 89] </ref> or other form factor approximation technique. Galerkin techniques have not yet been applied to the radiosity equation, an absence noted by Heckbert [HECK 91a].
Reference: [CAMP 90] <author> A. T. Campbell, III and Donald S. Fussell, </author> <title> Adaptive Mesh Generation for Global Diffuse Illumination, </title> <journal> Computer Graphics, </journal> <volume> 24(4), </volume> <pages> pp. 155-164, </pages> <year> 1990. </year>
Reference-contexts: The original occlusion approach of Nishita and Nakamae [NISH 85], although restricted to convex polyhedra, used the explicit, geometrically determined shadow boundaries to guide their simple subdivision scheme. Campbell <ref> [CAMP 90] </ref> improved upon this technique by subdividing polygons along their linear shadow boundaries after each radiosity iteration. Hanrahan's technique [HANR 91] combined adaptive subdivision techniques with partial occlusion testing within its multigrid model. <p> areas i and j i Angle between the surface normal of differential area i and a line between differential areas i and j 9 and j. tional radiosity techniques reduce the problem to finite dimensions by replacing differential areas with finite areas. 2.1 Conventional Radiosity Conventional radiosity methods [BAUM 89] <ref> [CAMP 90] </ref> [COHE 85] [COHE 88] [GORA 84] [HANR 91] [NISH 85] [WALL 89] all depend upon the assumption that radiosity values are constant over finite regions. This has been referred to as the Constant Radiosity Assumption, or CRA [TAMP 91].
Reference: [COHE 85] <author> M. Cohen and Donald P. Greenberg, </author> <title> The Hemi-Cube: A Radiosity Solution For Complex Environments, </title> <journal> Computer Graphics, </journal> <volume> 19(3), </volume> <year> 1985, </year> <pages> pp. 31-40. </pages>
Reference-contexts: and j i Angle between the surface normal of differential area i and a line between differential areas i and j 9 and j. tional radiosity techniques reduce the problem to finite dimensions by replacing differential areas with finite areas. 2.1 Conventional Radiosity Conventional radiosity methods [BAUM 89] [CAMP 90] <ref> [COHE 85] </ref> [COHE 88] [GORA 84] [HANR 91] [NISH 85] [WALL 89] all depend upon the assumption that radiosity values are constant over finite regions. This has been referred to as the Constant Radiosity Assumption, or CRA [TAMP 91]. <p> Goral [GORA 84] originally followed the approach of [SPAR 78], reducing the environment to simple polygons and using an analytic expression for the form factor between two polygons. Although analytical, this approach does not take into account the mutual visibility of the two polygons. The hemicube method <ref> [COHE 85] </ref> projects the environment onto a surface, and then samples the area-to-point form factor between a particular point on the receiving surface and separate regions of the transmitting environment defined by the hemicube grid.
Reference: [COHE 88] <author> M. Cohen, S. Chen, John R. Wallace, Donald P. Greenberg, </author> <title> A Progressive Refinement Approach to Fast Radiosity Image Generation, </title> <journal> Computer Graphics, </journal> <volume> 22(4), </volume> <year> 1988, </year> <pages> pp. 75-84. </pages>
Reference-contexts: In the original computer graphics implementation [GORA 84], radiosities were computed for an empty box with colored walls, with each wall meshed into up to 49 parts. Subsequent developments have enhanced the power and speed of the radios-ity method. Using a progressive refinement technique <ref> [COHE 88] </ref>, the effective speed of the original algorithm was vastly increased, adding the ability to observe pictures improving slowly over time. Sillion [SILL 91] showed how the Lambertian assumption could be removed from the radiosity model, allowing 3 for general reflectance functions. <p> i Angle between the surface normal of differential area i and a line between differential areas i and j 9 and j. tional radiosity techniques reduce the problem to finite dimensions by replacing differential areas with finite areas. 2.1 Conventional Radiosity Conventional radiosity methods [BAUM 89] [CAMP 90] [COHE 85] <ref> [COHE 88] </ref> [GORA 84] [HANR 91] [NISH 85] [WALL 89] all depend upon the assumption that radiosity values are constant over finite regions. This has been referred to as the Constant Radiosity Assumption, or CRA [TAMP 91]. <p> Consider the environment in Figure 3.2. The top square is non-reflective, but has a constant emittance of one, while the bottom square is gray, and has no emittance. This system is the physical analog of the first progressive shot <ref> [COHE 88] </ref> from the top square to the bottom. <p> refers to some function representing part of the distribution of radiosity across a surface, as opposed to a constant value across a surface. 5.2 Progressive Refinement Methods The matrix equation (5.6) can be solved using any standard matrix technique, such as Gaussian elimination, or even by using progressive refinement techniques <ref> [COHE 88] </ref>. If progressive methods are used, they must be slightly modified because the radiosity coefficients B l j may have negative values. <p> When using a progressive solution method <ref> [COHE 88] </ref>, this kernel matrix formulation allows a wide range of memory/CPU time tradeoffs. The approach requiring the least memory is to shoot from one basis function E l j at a time. <p> It avoids the inconsistency of computing radiosity values for a scene as if they were constant, and then rendering these constant values with 104 interpolation. It is also orthogonal to other radiosity enhancements, such as the progressive refinement approach <ref> [COHE 88] </ref>, or arbitrary reflectance functions [SILL 91]. Because of the high expense of raising the solution order, the order of every surface should be kept as low as possible. Using Gaussian quadrature, an energy transfer between two surfaces of order N requires (N + 1) 4 kernel function samples.
Reference: [COHE 90] <author> M. Cohen and John R. Wallace, </author> <title> Basic Radiosity Formulation for Diffuse Reflections, </title> <booktitle> in SIGGRAPH '90 Radiosity Course Notes, </booktitle> <year> 1990. </year>
Reference: [CRC 59] <editor> Charles D. Hodgman, ed., </editor> <publisher> C.R.C. Standard Mathematical Tables-Twelfth Edition, Chemical Rubber Publishing Company, </publisher> <address> USA, </address> <year> 1959. </year>
Reference: [DAVI 63] <author> Philip J. Davis, </author> <title> Interpolation and Approximation, </title> <publisher> Blaisdell, </publisher> <address> New York, </address> <year> 1963. </year>
Reference-contexts: The Jacobi polynomials are conventionally written as P (ff;fi) The unnormalized Jacobi polynomials have a more complex recursion rule than the Legendre polynomials <ref> [DAVI 63] </ref>: P 0 (x) = 1 (ff;fi) ff fi + 2 P n+1 (x) = n x B (ff;fi) C n where A (ff;fi) fi (2n + ff + fi)P (ff;fi) B (ff;fi) (ff;fi) C (ff;fi) These polynomials can be normalized by multiplying them by [GRAD 65]: s (ff +
Reference: [DELV 81] <author> L. M. Delves and T. L. Freeman, </author> <title> Analysis of Global Expansion Methods: Weakly Asymptotically Diagonal Systems, </title> <publisher> Academic Press, Inc., </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: Ignoring the singularity is not a generally viable option; the system will converge extremely slowly (requiring many basis functions) for a mediocre basis set, and may even fail entirely for a bad basis set. Delves <ref> [DELV 81] </ref> shows that such a singularity will lead to complete loss of convergence for a Chebyshev basis set. Choosing an appropriate basis set is essential for accurate solution. 44 1. forall i; k: 2. B k 3.
Reference: [DELV 85] <author> L. M. Delves and J. L. Mohamed, </author> <title> Computational Methods for Integral Equations, </title> <publisher> Cambridge University Press, </publisher> <address> New York, </address> <year> 1985. </year> <pages> 112 113 </pages>
Reference-contexts: However, an examination of some of the simpler non-constant radiosity systems illustrates the difficulties involved in finding an analytic solution. Much of the general information on integral equations in this chapter comes from either [KANW 71] or <ref> [DELV 85] </ref>. 3.1 Characterization of the Radiosity Equation Integral equations can be classified into different types depending on their form. <p> radiosity equation can be expressed as B = E + KB: (3:2) This can be factored and solved, as follows: (I K)B = E As long as kKk &lt; 1, the equation can be solved by expanding it into an infinite Neumann series which converges to the correct unique solution <ref> [DELV 85] </ref>. <p> Further, the two-dimensional polynomial bases used in this thesis are simple products of polynomials in different variables. 31 4.3 Quadrature Rules A relatively readable explanation of one-dimensional quadrature rules can be found in Delves and Mohamed's book <ref> [DELV 85] </ref>. A condensed version is presented here. A quadrature rule is a method for approximating the integral of the product of a class of functions f (x) and a particular weight function W (x). Quadrature rules can be used to approximate inner product integrals, like that in (4.1). <p> Therefore, a one-dimensional Gaussian quadrature rule will have to have at least N + 1 sample points to accurately perform the integration <ref> [DELV 85] </ref>. One-dimensional quadrature rules can be combined to create two and higher dimensional rules by applying the one-dimensional rules independently in each 34 direction. For a discussion of this approach, as well as other higher-dimensional quadrature rules, see [STRO 71]. <p> In a function-based radiosity formulation, the kernel matrix K ij expresses the energy transfer between basis functions on different surfaces. 39 5.1 The Galerkin Method The Galerkin method approximates integral equations in terms of a set of basis functions <ref> [DELV 85] </ref>. Given an orthonormal basis set of easily integrable functions, the Galerkin technique finds the best possible fit to the integral equation's solution. Heckbert [HECK 91a,HECK 91b] suggested that the Galerkin method could be used to solve the radiosity integral equation in a plane. <p> If the radiosity equation were not singular, the best basis set would probably be the Chebyshev basis set, since transforming functions in and out of such a basis set can be achieved via a Fast Fourier Transform <ref> [DELV 85] </ref>. Unfortunately, the radiosity equation does not have a well-behaved kernel, so some other method must be used. Equation (2.2) shows that near the common edge of two non-coplanar surfaces, the 1 r 2 term in the double-differential form factor approaches infinity.
Reference: [DORS 91] <author> Julie O'B. Dorsey, Fran~cois X. Sillion, Donald P. Greenberg, </author> <title> Design and Simulation of Opera Lighting and Projection Effects, </title> <journal> Computer Graphics, </journal> <volume> 25(4), </volume> <year> 1991, </year> <pages> pp. 41-50. </pages>
Reference-contexts: Hanrahan [HANR 91] applied multigridding techniques to the problem, restructuring radiosity's mathematical model and thereby producing a significantly faster algorithm. The radiosity technique has also been applied to significantly more complex environments than the original box, as can be seen in <ref> [DORS 91] </ref>'s simulation of opera set lighting effects. 1.1 Limitations of Conventional Radiosity Despite all of these advances, existing radiosity implementations still suffer from serious limitations. * The mathematical techniques used to evaluate radiosity systems always require that objects be flat or approximated by a large number of small polygonal surfaces.
Reference: [GORA 84] <author> Cindy M. Goral, Kenneth E. Torrance, Donald P. Greenberg, and Bennett Battaile, </author> <title> Modeling the Interaction of Light Between Diffuse Surfaces, </title> <journal> Computer Graphics, </journal> <volume> 18(3), </volume> <month> July </month> <year> 1984, </year> <pages> pp. 213-222. </pages>
Reference-contexts: To produce computer-generated pictures in a reasonable amount of time, approximations must be used. The radiosity approximation to the global illumination problem was first introduced to computer graphics by Goral et al. <ref> [GORA 84] </ref>. Goral's work was an application of the theory of radiative heat transport within an enclosure, 1 100Watt 20%efficiency=(30 frame s E photon ) 2 fi 10 20 1 2 dating back at least to [HOTT 54]. <p> To allow for variations in intensity across a surface, a surface might be meshed, or divided into a large number of smaller pieces. In the original computer graphics implementation <ref> [GORA 84] </ref>, radiosities were computed for an empty box with colored walls, with each wall meshed into up to 49 parts. Subsequent developments have enhanced the power and speed of the radios-ity method. <p> between the surface normal of differential area i and a line between differential areas i and j 9 and j. tional radiosity techniques reduce the problem to finite dimensions by replacing differential areas with finite areas. 2.1 Conventional Radiosity Conventional radiosity methods [BAUM 89] [CAMP 90] [COHE 85] [COHE 88] <ref> [GORA 84] </ref> [HANR 91] [NISH 85] [WALL 89] all depend upon the assumption that radiosity values are constant over finite regions. This has been referred to as the Constant Radiosity Assumption, or CRA [TAMP 91]. <p> With the subsidiary assumptions that emittance and reflectivity are also constant over finite regions, 10 the radiosity equation (2.1) can be rewritten B i = E i + i j Z The remaining form factor integral has been approximated in several ways. Goral <ref> [GORA 84] </ref> originally followed the approach of [SPAR 78], reducing the environment to simple polygons and using an analytic expression for the form factor between two polygons. Although analytical, this approach does not take into account the mutual visibility of the two polygons.
Reference: [GORA 85] <author> Cindy M. Goral, </author> <title> A Model for the Interaction of Light Between Diffuse Surfaces, </title> <type> Masters Thesis, </type> <institution> Cornell University, </institution> <address> Ithaca NY, </address> <month> January </month> <year> 1985. </year>
Reference-contexts: A derivation of this procedure can be found in <ref> [GORA 85] </ref>, and is not repeated here. <p> t d 2 + (1 s) 2 5 + p " p + tan 1 s d 2 + t 2 + q 2 q + tan 1 s d 2 + (1 t) 2 5 : This result is equivalent to the contour integral formulation of [SPAR 78] and <ref> [GORA 85] </ref>. Because of the complexity of this single-shot result in a nearly trivial geometry, it seems unlikely that analytic methods could be reasonably used for more complex environments.
Reference: [GLAS 89] <author> Andrew Glassner, ed., </author> <title> An Introduction to Ray Tracing, </title> <publisher> Academic Press, Inc., </publisher> <address> New York, </address> <year> 1989. </year>
Reference: [GRAD 65] <author> I. S. Gradshteyn and I. M. Ryzhik, </author> <title> Table of Integrals, Series, and Products, 4th edition, </title> <publisher> Academic Press, Inc., </publisher> <address> New York, </address> <year> 1965. </year>
Reference-contexts: For the inner product with weight function identically 1, i.e. hf j gi Legendre = Z 1 f (x)g (x)dx; (4:7) the polynomials formed are the Legendre polynomials. The unnormalized Leg endre polynomials can be generated by a recursion rule <ref> [GRAD 65] </ref>, P 0 (x) = 1 (n + 1)P n+1 (x) = (2n + 1)xP n (x) nP n1 (x): (4.8) The normalized Legendre polynomials are P n (x) = n + 2 Polynomial sets can also be created with non-constant inner product weight functions W (x). <p> than the Legendre polynomials [DAVI 63]: P 0 (x) = 1 (ff;fi) ff fi + 2 P n+1 (x) = n x B (ff;fi) C n where A (ff;fi) fi (2n + ff + fi)P (ff;fi) B (ff;fi) (ff;fi) C (ff;fi) These polynomials can be normalized by multiplying them by <ref> [GRAD 65] </ref>: s (ff + 1 + n)(fi + 1 + n)2 ff+fi+1 (4:14) Note that the Jacobi polynomial P (0;0) i (x) is equivalent to the Legendre polynomial P i (x).
Reference: [HANR 91] <author> Pat Hanrahan, David Salzman, and Larry Aupperle, </author> <title> A Rapid Hierarchical Radiosity Algorithm, </title> <journal> Computer Graphics, </journal> <volume> 25(4), </volume> <pages> pp. 197-206, </pages> <year> 1991. </year>
Reference-contexts: Using a progressive refinement technique [COHE 88], the effective speed of the original algorithm was vastly increased, adding the ability to observe pictures improving slowly over time. Sillion [SILL 91] showed how the Lambertian assumption could be removed from the radiosity model, allowing 3 for general reflectance functions. Hanrahan <ref> [HANR 91] </ref> applied multigridding techniques to the problem, restructuring radiosity's mathematical model and thereby producing a significantly faster algorithm. <p> The original occlusion approach of Nishita and Nakamae [NISH 85], although restricted to convex polyhedra, used the explicit, geometrically determined shadow boundaries to guide their simple subdivision scheme. Campbell [CAMP 90] improved upon this technique by subdividing polygons along their linear shadow boundaries after each radiosity iteration. Hanrahan's technique <ref> [HANR 91] </ref> combined adaptive subdivision techniques with partial occlusion testing within its multigrid model. Lischinski and Tampieri's approach [LISC 92] adds discontinuity order information to Campbell's method, improving the ability to reconstruct the shadows into a reasonable radiosity picture. <p> surface normal of differential area i and a line between differential areas i and j 9 and j. tional radiosity techniques reduce the problem to finite dimensions by replacing differential areas with finite areas. 2.1 Conventional Radiosity Conventional radiosity methods [BAUM 89] [CAMP 90] [COHE 85] [COHE 88] [GORA 84] <ref> [HANR 91] </ref> [NISH 85] [WALL 89] all depend upon the assumption that radiosity values are constant over finite regions. This has been referred to as the Constant Radiosity Assumption, or CRA [TAMP 91]. <p> A method combining adaptive meshing and a low order Galerkin solution might produce reasonable images rapidly. Extending Hanrahan's hierarchical multigridding technique <ref> [HANR 91] </ref> to higher order functions could produce a means to do this. The method of this thesis uses a Legendre basis set for non-singular energy transfers.
Reference: [HECK 91a] <author> Paul S. Heckbert, </author> <title> Simulating Global Illumination Using Adaptive Meshing, </title> <type> Report No. </type> <institution> UCB/CSD 91/636, University of California, Berkeley, </institution> <year> 1991. </year>
Reference-contexts: In thermal radiosity, variational methods have been used to create single element, higher-order solutions for some special cases [USIS 60,SPAR 60]. Heckbert experimented with using a large number of linear <ref> [HECK 91a] </ref> and quadratic [HECK 91b] elements to solve computer graphics radiosity problems in the plane. 1.2 Thesis Objective In this thesis, the core of the radiosity method is rebuilt in a mathematically consistent manner that eliminates many of the difficulties present with current techniques. <p> Heckbert performed a detailed study of radiosity <ref> [HECK 91a] </ref> in flatlandthe one-dimensional analogy to conventional radiosity. Most of the existing one-dimensional integral equation theory can be easily extended to two-dimensional radiosity equations. 17 approaches zero, and the point-to-point form factor approaches infinity. Singular equations The most significant mathematical feature of the radiosity equation is its singularity. <p> Although the integration process prevents any such singularities from appearing in the solution, the presence of these singularities in the kernel requires special attention when a more analytic approach is taken to the radiosity equation. 18 Heckbert's planar collocation technique <ref> [HECK 91a] </ref> produces its greatest errors near so-called `reflex corners'. The error appears because the method does not explicitly deal with the singularity at these points of intersection between two elements. <p> Heckbert <ref> [HECK 91a] </ref> experimented with a planar analog to radiosity and encountered similar resistance. No general solution method is yet known, although some radiosity problems can be solved for simple cases. 3.2 A Simple Radiosity System The simple case presented in this section illustrates the complexity of the ra-diosity problem. <p> The usual assumption is that radiosity values within some finite region are constant, allowing the use of an analytic form factor [BAUM 89] or other form factor approximation technique. Galerkin techniques have not yet been applied to the radiosity equation, an absence noted by Heckbert <ref> [HECK 91a] </ref>. <p> to the progressive Galerkin radiosity algorithm. 63 Shadow masks can be implemented as a grid of sample points across s and t, an adaptive tree structure, or any other data structure which can take advantage of particular characteristics of the environment, such as actual shadow boundaries as described by Heckbert <ref> [HECK 91a] </ref> or Lischinski [LISC 92]. Shadow masks should produce approximately the same level of accuracy as meshing methods; however, they only increment the number of surfaces in the environment, and therefore add little to the radiosity solution time in environments with a limited number of important light sources.
Reference: [HECK 91b] <author> Paul S. Heckbert and James M. Winget, </author> <title> Finite Element Methods for Global Illumination, </title> <type> Report No. </type> <institution> UCB/CSD 91/643, University of California, Berkeley, </institution> <year> 1991. </year>
Reference-contexts: In thermal radiosity, variational methods have been used to create single element, higher-order solutions for some special cases [USIS 60,SPAR 60]. Heckbert experimented with using a large number of linear [HECK 91a] and quadratic <ref> [HECK 91b] </ref> elements to solve computer graphics radiosity problems in the plane. 1.2 Thesis Objective In this thesis, the core of the radiosity method is rebuilt in a mathematically consistent manner that eliminates many of the difficulties present with current techniques. <p> Finding edge singularities in a scene with planar surfaces is fairly simple. Any collinear or near-collinear edges between two non-coplanar surfaces can form a singularity. Because singularities are produced at all non-parallel intersections of distinct surfaces, geometries with T-intersections (the three-dimensional analog to Heck-bert's T-corners <ref> [HECK 91b] </ref>) like those in Figure 5.7 significantly complicate the process of finding and dealing with singularities.
Reference: [HOTT 54] <editor> Hoyt C. Hottel, Radiant-Heat Transmission, in William H. McAdams, </editor> <title> Heat Transmission, </title> <publisher> McGraw-Hill Book Company, Inc., </publisher> <address> New York, </address> <year> 1954. </year>
Reference-contexts: Goral's work was an application of the theory of radiative heat transport within an enclosure, 1 100Watt 20%efficiency=(30 frame s E photon ) 2 fi 10 20 1 2 dating back at least to <ref> [HOTT 54] </ref>. Within the theory of heat transport, radiosity refers to the total radiative energy emitted from a surface per unit area per unit time [SPAR 78].
Reference: [KAJI 86] <author> James T. Kajiya, </author> <title> The Rendering Equation, </title> <journal> Computer Graphics, </journal> <volume> 20(4), </volume> <year> 1986, </year> <pages> pp. 143-150. </pages>
Reference-contexts: Since energy is conserved, such a transfer can at most have kKk = 1. If the environment's reflectivities are not identically one, then some light energy must be absorbed with each step, yielding kKk &lt; 1. Kajiya's analysis of the radiosity method in terms of the rendering equation <ref> [KAJI 86] </ref> notes the similarity between conventional full-matrix radiosity methods and a Neumann series approximation to the rendering equation. Each iteration of the matrix solution technique adds another term to the Neumann series expansion (3.4).
Reference: [KANW 71] <author> Ram P. Kanwal, </author> <title> Linear Integral Equations; Theory and Technique, </title> <publisher> Academic Press, Inc., </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: Integral equations are analogous to differential equations, with integration replacing differentiation, and are usually only a little more difficult to solve <ref> [KANW 71] </ref>. The next chapter explores some of the difficulties of solving the radiosity integral equation with standard methods. Chapter 3 Difficulties with Analytic Methods The ideal solution to any mathematical problem is one in closed form. <p> However, an examination of some of the simpler non-constant radiosity systems illustrates the difficulties involved in finding an analytic solution. Much of the general information on integral equations in this chapter comes from either <ref> [KANW 71] </ref> or [DELV 85]. 3.1 Characterization of the Radiosity Equation Integral equations can be classified into different types depending on their form.
Reference: [LISC 92] <author> Dani Lischinski, Filippo Tampieri, and Donald P. Greenberg, </author> <title> A Discontinuity Meshing Algorithm for Accurate Radiosity, </title> <journal> IEEE CG&A, </journal> <note> 1992, to appear. 114 </note>
Reference-contexts: Campbell [CAMP 90] improved upon this technique by subdividing polygons along their linear shadow boundaries after each radiosity iteration. Hanrahan's technique [HANR 91] combined adaptive subdivision techniques with partial occlusion testing within its multigrid model. Lischinski and Tampieri's approach <ref> [LISC 92] </ref> adds discontinuity order information to Campbell's method, improving the ability to reconstruct the shadows into a reasonable radiosity picture. All these subdivision techniques can be enhanced by taking multiple occlusion samples across the emitter and receiver to provide intermediate levels of radios-ity in partially occluded regions [WALL 89]. <p> Lischinski and Tampieri <ref> [LISC 92] </ref> also begin by computing the exact shadow boundaries for each occluding object, but then use this information to explicitly mesh the shadow receiver along lines of low order discontinuities. <p> radiosity algorithm. 63 Shadow masks can be implemented as a grid of sample points across s and t, an adaptive tree structure, or any other data structure which can take advantage of particular characteristics of the environment, such as actual shadow boundaries as described by Heckbert [HECK 91a] or Lischinski <ref> [LISC 92] </ref>. Shadow masks should produce approximately the same level of accuracy as meshing methods; however, they only increment the number of surfaces in the environment, and therefore add little to the radiosity solution time in environments with a limited number of important light sources. <p> Again, when such error-prone energy transfers are significant, the receiving surface may need to be subdivided. 7.2 Comparison with Conventional Radiosity Lischinski and Tampieri provided the reference solution to the radiosity environment in Figure 7.5. This solution was computed using the discontinuity meshing techniques of <ref> [LISC 92] </ref>, with adaptive integration using Wallace [WALL 89] point-to-point form factors. Individual triangles in the mesh were treated consistently as quadratic elements, reducing error caused by the Constant Radiosity Assumption [TAMP 91]. <p> So, when solutions progress to the same level of unshot magnitude, but are computed at different order, the higher-order solution will distribute more energy, although some of it might not be in exactly the right place. 88 7.3 Discussion In this particular test case, the method of <ref> [LISC 92] </ref> took about the same amount of time as the highest-order Galerkin solution. However, the Galerkin method only required 6.5 Megabytes of memory, compared to 75 Megabytes for a more conventional, meshing approach. <p> It may not always be easy to determine ahead of time where detailed shadow masking or meshing will be necessary, possibly requiring multiple solution attempts before all shadows are properly accounted for. Detailed shadow meshing can be very computationally expensive <ref> [LISC 92] </ref>, especially when applied to curved surfaces. In the current implementation, the regular grid structure of shadow masks limits their spatial accuracy and requires extensive computation to produce detailed shadows. Higher order methods also have the potential to be computationally ex 105 pensive.
Reference: [NISH 85] <author> Tomoyuki Nishita and Eihachiro Nakamae, </author> <title> Continuous Tone Representation of Three-Dimensional Objects Taking Account of Shadows and Interreflection, </title> <journal> Computer Graphics, </journal> <volume> 19(3), </volume> <year> 1985, </year> <pages> pp. 23-30. </pages>
Reference-contexts: The assumption that radiosity values are locally 4 constant is accurate over an extremely limited domain, and has significant implications for the accuracy of conventional radiosity solution methods. * Generating images with accurately placed shadows is difficult using existing radiosity techniques. The original occlusion approach of Nishita and Nakamae <ref> [NISH 85] </ref>, although restricted to convex polyhedra, used the explicit, geometrically determined shadow boundaries to guide their simple subdivision scheme. Campbell [CAMP 90] improved upon this technique by subdividing polygons along their linear shadow boundaries after each radiosity iteration. <p> of differential area i and a line between differential areas i and j 9 and j. tional radiosity techniques reduce the problem to finite dimensions by replacing differential areas with finite areas. 2.1 Conventional Radiosity Conventional radiosity methods [BAUM 89] [CAMP 90] [COHE 85] [COHE 88] [GORA 84] [HANR 91] <ref> [NISH 85] </ref> [WALL 89] all depend upon the assumption that radiosity values are constant over finite regions. This has been referred to as the Constant Radiosity Assumption, or CRA [TAMP 91]. <p> Both methods are explored in this chapter. 6.1 Shadow Meshing Attempts to subdivide surfaces to find shadow boundaries have been investigated as part of radiosity solution schemes since radiosity's introduction to computer graphics. Nishita and Nakamae <ref> [NISH 85] </ref> accounted for shadow boundaries in polyhedral environments by finding the explicit shadow boundaries cast by a particular object onto a shadow receiver, meshing the shadow receiver independent of that information, and then using those boundaries to determine whether or not each element of the mesh was in the shadow
Reference: [PENT 89] <author> Alex Pentland and J. Williams, </author> <title> Good Vibrations: Modal Dynamics for Graphics and Animation, </title> <journal> Computer Graphics, </journal> <volume> 23(4), </volume> <year> 1989, </year> <pages> pp. 215-222. </pages>
Reference: [PRES 88] <author> W. Press, B. Flannery, S. Teukolsky, and W. Vetterling, </author> <title> Numerical Recipes in C, </title> <publisher> Cambridge University Press, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: The square of the first coefficient of the i th eigenvector is the quadrature weight w i . Finding eigenvectors and eigenvalues for tridiagonal symmetric matrices using the QR factorization is a fairly well-understood process, explained in <ref> [PRES 88] </ref>, for example.
Reference: [STAR 90] <editor> Starbase Radiosity and Ray Tracing Programmer's Manual, </editor> <publisher> Hewlett Packard Co., </publisher> <address> USA, </address> <year> 1990. </year>
Reference-contexts: Figure 8.6 shows an even more complex environment computed with Galerkin radiosity. The Galerkin radiosity method was applied directly to the surfaces in these environments; the curved surfaces were not tiled. For comparison purposes, the teapot environment was also computed using a commercially-available radiosity package <ref> [STAR 90] </ref> (Figure 8.8). This package uses the Wallace point-sampling algorithm [WALL 89] to compute form factors, but only provides spatially uniform, non-adaptive meshing.
Reference: [SILL 91] <author> Fran~cois Sillion, James Arvo, Steve Westin, and Donald P. Green-berg, </author> <title> A Global Illumination Solution for General Reflectance Distributions, </title> <journal> Computer Graphics, </journal> <volume> 25(4), </volume> <month> July </month> <year> 1991, </year> <pages> pp. 187-196. </pages>
Reference-contexts: Subsequent developments have enhanced the power and speed of the radios-ity method. Using a progressive refinement technique [COHE 88], the effective speed of the original algorithm was vastly increased, adding the ability to observe pictures improving slowly over time. Sillion <ref> [SILL 91] </ref> showed how the Lambertian assumption could be removed from the radiosity model, allowing 3 for general reflectance functions. Hanrahan [HANR 91] applied multigridding techniques to the problem, restructuring radiosity's mathematical model and thereby producing a significantly faster algorithm. <p> It avoids the inconsistency of computing radiosity values for a scene as if they were constant, and then rendering these constant values with 104 interpolation. It is also orthogonal to other radiosity enhancements, such as the progressive refinement approach [COHE 88], or arbitrary reflectance functions <ref> [SILL 91] </ref>. Because of the high expense of raising the solution order, the order of every surface should be kept as low as possible. Using Gaussian quadrature, an energy transfer between two surfaces of order N requires (N + 1) 4 kernel function samples.
Reference: [STOE 80] <author> J. Stoer and R. </author> <title> Bulirsch, Introduction to Numerical Analysis, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1980. </year>
Reference-contexts: This sequence of polynomials (to within a normalizing constant factor) can be expressed in terms of recursion rules <ref> [STOE 80] </ref>: T 1 (x) 0; T i+1 (x) (x ffi i+1 )T i (x) fl 2 Take these ffi i and fl i coefficients, and construct a tridiagonal symmetric matrix: 2 6 6 6 6 6 6 6 4 fl 2 ffi 2 fl 3 . . . . .
Reference: [STRO 71] <author> A. H. Stroud, </author> <title> Approximate Calculation of Multiple Integrals, </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1971. </year>
Reference-contexts: One-dimensional quadrature rules can be combined to create two and higher dimensional rules by applying the one-dimensional rules independently in each 34 direction. For a discussion of this approach, as well as other higher-dimensional quadrature rules, see <ref> [STRO 71] </ref>. Chapter 5 Non-Constant Radiosity Consider the effect of meshing a single surface into constant radiosity patches (Figure 5.1). Although the radiosity on each individual patch is smooth, when combined, the patches describe a stair-step radiosity function for the entire surface.
Reference: [SPAR 60] <author> E. M. </author> <title> Sparrow, Application of Variational Methods to Radiation Heat-Transfer Calculations, </title> <journal> Journal of Heat Transfer, </journal> <month> November </month> <year> 1960, </year> <pages> pp. 375-380. </pages>
Reference-contexts: method, the energy may still be distributed to the wrong place until all surfaces have shot. 68 Finally, as with any computer algorithm, the floating point representation of the underlying real numbers always generates some error. 7.1 Energy Transfer Error For the simple environment used by Sparrow's variational radiosity solution <ref> [SPAR 60] </ref>, a fourth-order solution produced a relative error of less than one percent. Using the method of this thesis, error computations for a single energy transfer between parallel and perpendicular rectangles have produced similar levels of accuracy, except when the rectangle's centers are close relative to their size.
Reference: [SPAR 78] <author> E. M. Sparrow and R. D. Cess, </author> <title> Radiation Heat Transfer Augmented Edition, </title> <publisher> Hemisphere Publishing Corp., </publisher> <address> Washington, </address> <year> 1978. </year>
Reference-contexts: Within the theory of heat transport, radiosity refers to the total radiative energy emitted from a surface per unit area per unit time <ref> [SPAR 78] </ref>. Radiosity simulations have been used for many years to simulate the interiors of ovens and combustion engines, as well as in the design of spacecraft heat radiators and other systems dominated by thermal blackbody emissions. <p> Goral [GORA 84] originally followed the approach of <ref> [SPAR 78] </ref>, reducing the environment to simple polygons and using an analytic expression for the form factor between two polygons. Although analytical, this approach does not take into account the mutual visibility of the two polygons. <p> everywhere, so the equations describing this system are: B 0 (s; t) = 0+ :5 0 0 B 1 (s; t) = 1+ 0 0 0 (3:5) which can be simplified to B 0 (s; t) = :5 0 0 B 1 (s; t) = 1: (3.7) Sparrow and Cess <ref> [SPAR 78] </ref> suggest that Stoke's theorem can be applied to 22 rated by a distance d. integrals such as (3.6), transforming the surface integral into a contour integral. A derivation of this procedure can be found in [GORA 85], and is not repeated here. <p> All comparisons in this section are made against an analytic solution using the formulation of Sparrow and Cess <ref> [SPAR 78] </ref>. The relative error metric used is * jB Galerkin (s; t) B exact (s; t)j B exact (s; t) + ; (7:1) where the error is evaluated on a 500 by 500 grid of sample points on the receiving surface. <p> + tan 1 t d 2 + (1 s) 2 5 + p " p + tan 1 s d 2 + t 2 + q 2 q + tan 1 s d 2 + (1 t) 2 5 : This result is equivalent to the contour integral formulation of <ref> [SPAR 78] </ref> and [GORA 85]. Because of the complexity of this single-shot result in a nearly trivial geometry, it seems unlikely that analytic methods could be reasonably used for more complex environments.
Reference: [TAMP 91] <author> Filippo Tampieri and Dani Lischinski, </author> <title> The Constant Radiosity Assumption Syndrome, </title> <booktitle> in the Proceedings of the Second Eurograph-ics Workshop on Rendering, </booktitle> <address> Barcelona, </address> <year> 1991. </year>
Reference-contexts: Wallace's approach [WALL 89] allows the surface normal to vary across planar surfaces, reducing the error caused by tessellating a curved surface. However, no radiosity method to date has incorporated curved surfaces directly. * Outside regions of total shadow, the radiosity of surfaces is not generally constant <ref> [TAMP 91] </ref>. Different points on surfaces are different distances from light sources, causing subtle variations in intensity. Partial occlusions or curvature variations across surfaces can lead to more dramatic changes in illumination. <p> This has been referred to as the Constant Radiosity Assumption, or CRA <ref> [TAMP 91] </ref>. With the subsidiary assumptions that emittance and reflectivity are also constant over finite regions, 10 the radiosity equation (2.1) can be rewritten B i = E i + i j Z The remaining form factor integral has been approximated in several ways. <p> These equations form the standard representation of radiosity 11 systems: B i = E i + i j 2.2 The Constant Radiosity Syndrome Unfortunately, this constant, polygonal approach to the radiosity problem limits the accuracy of generated pictures. As explained by Tampieri and Lischinski <ref> [TAMP 91] </ref>, true radiosity values are rarely constant outside regions of complete shadow. Because conventional radiosity techniques use an iterative scheme that does not compensate for CRA-induced error, their results may not converge to a physically accurate solution. <p> Although the radiosity on each individual patch is smooth, when combined, the patches describe a stair-step radiosity function for the entire surface. Adjacent patches with different radiosity values cannot combine to form a smooth radiosity function for the entire surface. To solve radiosity without the constant radiosity assumption <ref> [TAMP 91] </ref>, we need to formulate radiosity in terms of smooth functions across an entire surface, instead of disjoint patches on parts of a surface. functions might look like. In addition to the conventional constant part, one might use linear, quadratic, and higher-order parts. <p> This solution was computed using the discontinuity meshing techniques of [LISC 92], with adaptive integration using Wallace [WALL 89] point-to-point form factors. Individual triangles in the mesh were treated consistently as quadratic elements, reducing error caused by the Constant Radiosity Assumption <ref> [TAMP 91] </ref>. Error in the solution is primarily limited to a few meshing artifacts, visible near the corners of the top wall.
Reference: [USIS 60] <author> C. M. Usiskin and R. Siegel, </author> <title> Thermal Radiation From a Cylindrical Enclosure With Specified Wall Heat Flux, </title> <journal> Journal of Heat Transfer, </journal> <pages> pp. 369-374, </pages> <month> November, </month> <year> 1960 </year>
Reference: [WALL 89] <author> John R. Wallace, Kells A. Elmquist, Eric A. Haines, </author> <title> A Ray Tracing Algorithm for Progressive Radiosity, </title> <journal> Computer Graphics, </journal> <volume> 23(3), </volume> <year> 1989, </year> <pages> pp. 315-324. </pages>
Reference-contexts: Both approximate [GORA 84,COHE 85] and analytic [BAUM 89] approaches use planar surfaces, even though such a restriction is not a fundamental part of the radiosity model. Wallace's approach <ref> [WALL 89] </ref> allows the surface normal to vary across planar surfaces, reducing the error caused by tessellating a curved surface. However, no radiosity method to date has incorporated curved surfaces directly. * Outside regions of total shadow, the radiosity of surfaces is not generally constant [TAMP 91]. <p> All these subdivision techniques can be enhanced by taking multiple occlusion samples across the emitter and receiver to provide intermediate levels of radios-ity in partially occluded regions <ref> [WALL 89] </ref>. The difficulties of accurate shadow generation in radiosity have not yet been resolved. Although all these methods work to varying degrees, at their highest level of accuracy they all must produce a relatively large mesh consisting of many small elements. <p> area i and a line between differential areas i and j 9 and j. tional radiosity techniques reduce the problem to finite dimensions by replacing differential areas with finite areas. 2.1 Conventional Radiosity Conventional radiosity methods [BAUM 89] [CAMP 90] [COHE 85] [COHE 88] [GORA 84] [HANR 91] [NISH 85] <ref> [WALL 89] </ref> all depend upon the assumption that radiosity values are constant over finite regions. This has been referred to as the Constant Radiosity Assumption, or CRA [TAMP 91]. <p> The hemicube method [COHE 85] projects the environment onto a surface, and then samples the area-to-point form factor between a particular point on the receiving surface and separate regions of the transmitting environment defined by the hemicube grid. The ray-traced form factor method <ref> [WALL 89] </ref> samples the double-differential form factor between a number of points on each transmitting and receiving surface. For polygonal surfaces, the CRA allows the use of an analytic formulation for the polygon-to-point form factor [BAUM 89]. <p> This solution was computed using the discontinuity meshing techniques of [LISC 92], with adaptive integration using Wallace <ref> [WALL 89] </ref> point-to-point form factors. Individual triangles in the mesh were treated consistently as quadratic elements, reducing error caused by the Constant Radiosity Assumption [TAMP 91]. Error in the solution is primarily limited to a few meshing artifacts, visible near the corners of the top wall. <p> The kernel term's form factor as expressed in (2.6), like the ray-traced form factors in <ref> [WALL 89] </ref>, includes surface normals explicitly. Curved surfaces are treated no differently from planar surfaces, except that their surface normals are a function varying over parametric space, instead 91 parametric coordinates without any poles. of a constant vector. <p> The Galerkin radiosity method was applied directly to the surfaces in these environments; the curved surfaces were not tiled. For comparison purposes, the teapot environment was also computed using a commercially-available radiosity package [STAR 90] (Figure 8.8). This package uses the Wallace point-sampling algorithm <ref> [WALL 89] </ref> to compute form factors, but only provides spatially uniform, non-adaptive meshing. Since this radiosity package cannot use bicubic patches directly, each of the teapot's patches were tessellated with a 20 by 20 gridenough so that tessellation artifacts would be only barely visible.
References-found: 33

