URL: http://www.cs.berkeley.edu/~arvindk/papers/t3d-isca95.ps
Refering-URL: http://www.cs.berkeley.edu/~arvindk/
Root-URL: 
Title: Empirical Evaluation of the CRAY-T3D: A Compiler Perspective  
Author: Remzi H. Arpaci, David E. Culler, Arvind Krishnamurthy, Steve G. Steinberg, and Katherine Yelick 
Address: Berkeley  
Affiliation: Computer Science Division University of California,  
Abstract: Most recent MPP systems employ a fast microprocessor surrounded by a shell of communication and synchronization logic. The CRAY-T3D 1 provides an elaborate shell to support global-memory access, prefetch, atomic operations, barriers, and block transfers. We provide a detailed empirical performance characterization of these primitives using micro-benchmarks and evaluate their utility in compiling for a parallel language. We have found that the raw performance of the machine is quite impressive and the most effective forms of communication are prefetch and write. Other shell provisions, such as the bulk transfer engine and the external Annex register set, are cumbersome and of little use. By evaluating the system in the context of a language implementation, we shed light on important trade-offs and pitfalls in the machine architecture. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Barton, J. Cownie, and M. McLaren. </author> <title> Message passing on the Meiko CS-2. </title> <journal> Parallel Computing, </journal> <volume> 20(4) </volume> <pages> 497-507, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: 1 Introduction In 1991 and 1992 a wave of large-scale parallel machines were announced that followed the "shell" approach [25], including the Thinking Machines CM-5 [15], Intel Paragon [8], Meiko CS-2 <ref> [1] </ref>, and CRAY-T3D [11]. In this approach the core of each node is realized by a state-of-the-art commercial microprocessor and its memory system, surrounded by a shell of additional logic to support global operations, such as communication and synchronization.
Reference: [2] <institution> BBN Advanced Computers Inc. </institution> <type> TC2000 Technical Product Summary, </type> <year> 1989. </year>
Reference-contexts: This has a significant impact on how the machine supports shared memory. Many shared-memory machines map all memory accessible to a process into the virtual address space and a virtual address is translated into a global physical address <ref> [2, 16, 10, 13, 14] </ref>. The memory system extracts the node number and physical location from the global physical address and performs either a local memory access or a message transaction with a remote memory controller.
Reference: [3] <author> M. Berry, P. Chen, P. Koss, and David J. Kuck. </author> <title> The Perfect Club benchmarks: Effective performance evaluation of supercomputers. </title> <type> Technical Report 827, </type> <month> November </month> <year> 1988. </year>
Reference-contexts: The results prove critical to understanding the costs and consistency issues of global operations because the memory system is the primary gateway to the shell. 2.1 Micro-benchmarking Benchmarking massively parallel processors usually refers to measuring the execution time of a set of applications (such as ParkBench [20] or Perfect Club <ref> [3] </ref>) designed to represent a meaningful workload. This measures the performance of the system as a whole, including the processor, the memory system, and the compiler, which is appropriate if one is taking all of these as fixed.
Reference: [4] <author> K.M. Chandy and C. Kesselman. </author> <title> Compositional C++: Compositional Parallel Programming. </title> <booktitle> In 5th International Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 124-44, </pages> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: Together these dictate the code-generation strategy. In this paper we document the results of a "gray-box" language-implementation study on the CRAY-T3D. The language is Split-C, constructed as an extension of gcc, but the study would apply to many languages with similar goals, such as CC++ <ref> [4] </ref> and HPF [9]. In the remainder of this section we outline the language and the basic machine architecture.
Reference: [5] <author> Cray Research Incorporated. </author> <title> CRAY Hardware Reference Manual, </title> <year> 1993. </year>
Reference-contexts: Here we discuss the key features of the design; see [11] or [12] for an overview and <ref> [5] </ref> for a complete functional description. The DEC Alpha 21064 [24] is a 64-bit, dual-issue, second generation RISC processor, clocked at 150 MHz (6.67 ns cycle), with 8 KB instruction and data caches, each with 32-byte lines.
Reference: [6] <author> David E. Culler, Andrea Dusseau, Seth Copen Goldstein, Arvind Krishnamurthy, Steven Lumetta, Thorsten von Eicken, and Katherine Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 262-273, </pages> <address> Portland, Oregon, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Based on the announced designs, a simple parallel extension to the C language was designed with the goal of extracting the full performance capability out of this wave of machines <ref> [6] </ref>. The basic approach was to provide a full C on each node operating out of the local memory, augmented with a rich set of assignment operations on the collective global address space. <p> sections, we outline the requirements of the language model as well as the structure, constraints, or performance characteristics of the machine that dictate how the language is implemented. 1.1 Language overview Split-C is a simple parallel extension to C for programming distributed memory machines using a global address space abstraction <ref> [6] </ref>. It has been implemented on the CM-5, Paragon, SP-1, and a variety of networks of workstations, using Active Messages to implement the global address space [17, 26, 19]. <p> We have developed six versions of the application with varying degrees of optimizations using the capabilities offered in Split-C. We consider a collection of synthetic graphs with 500 vertices on each processor with each vertex having a degree of 20, the same inputs seen in <ref> [6] </ref>. The communication requirements of the problem are scaled by adjusting the fraction of edges in the graph that cross processors in a synthetic graph. The useful performance metric when scaling both problem and machine size is the average time per edge, as shown in Figure 9 for the T3D.
Reference: [7] <author> Digital Equipment Corporation. </author> <title> DECchip 21064-AA Microprocessor Hardware Reference Manual, </title> <year> 1992. </year>
Reference-contexts: This is a feature known as write-merging [24]. Second, since main memory access time is roughly 145 nanoseconds, dividing this by 35 nanoseconds gives an estimated write buffer size of 4. This is corroborated by the Alpha 21064 Reference Manual <ref> [7] </ref>. This section has provided a detailed examination of the local memory system. We determined that the cost of an off-chip memory access is 23 cycles, that the large page size essentially eliminates TLB costs, and that the write buffer contains four entries and supports write-merging.
Reference: [8] <author> W. Groscup. </author> <title> The Intel Paragon XP/S supercomputer. </title> <booktitle> In Proceedings of the Fifth ECMWF Workshop on the Use of Parallel Processors in Meteorology., </booktitle> <pages> pages 262-273, </pages> <month> Nov </month> <year> 1992. </year>
Reference-contexts: 1 Introduction In 1991 and 1992 a wave of large-scale parallel machines were announced that followed the "shell" approach [25], including the Thinking Machines CM-5 [15], Intel Paragon <ref> [8] </ref>, Meiko CS-2 [1], and CRAY-T3D [11]. In this approach the core of each node is realized by a state-of-the-art commercial microprocessor and its memory system, surrounded by a shell of additional logic to support global operations, such as communication and synchronization.
Reference: [9] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification version 1.0. </title> <type> Draft, </type> <month> January </month> <year> 1993. </year>
Reference-contexts: Together these dictate the code-generation strategy. In this paper we document the results of a "gray-box" language-implementation study on the CRAY-T3D. The language is Split-C, constructed as an extension of gcc, but the study would apply to many languages with similar goals, such as CC++ [4] and HPF <ref> [9] </ref>. In the remainder of this section we outline the language and the basic machine architecture. In Section 2 we explain our micro-benchmarking methodology and characterize the data access performance of the individual node, including a comparison with a standard workstation using the same microprocessor (DEC Alpha 21064).
Reference: [10] <institution> Kendall Square Research. </institution> <type> KSR1 Technical Summary, </type> <year> 1992. </year>
Reference-contexts: This has a significant impact on how the machine supports shared memory. Many shared-memory machines map all memory accessible to a process into the virtual address space and a virtual address is translated into a global physical address <ref> [2, 16, 10, 13, 14] </ref>. The memory system extracts the node number and physical location from the global physical address and performs either a local memory access or a message transaction with a remote memory controller.
Reference: [11] <author> R.E. Kessler and J.L. Schwarzmeier. </author> <title> Cray T3D: a New Dimension for Cray Research. </title> <booktitle> In Digest of Papers. COMPCON Spring '93, </booktitle> <pages> pages 176-82, </pages> <address> San Francisco, CA, </address> <month> February </month> <year> 1993. </year>
Reference-contexts: 1 Introduction In 1991 and 1992 a wave of large-scale parallel machines were announced that followed the "shell" approach [25], including the Thinking Machines CM-5 [15], Intel Paragon [8], Meiko CS-2 [1], and CRAY-T3D <ref> [11] </ref>. In this approach the core of each node is realized by a state-of-the-art commercial microprocessor and its memory system, surrounded by a shell of additional logic to support global operations, such as communication and synchronization. <p> Here we discuss the key features of the design; see <ref> [11] </ref> or [12] for an overview and [5] for a complete functional description. The DEC Alpha 21064 [24] is a 64-bit, dual-issue, second generation RISC processor, clocked at 150 MHz (6.67 ns cycle), with 8 KB instruction and data caches, each with 32-byte lines.
Reference: [12] <author> R.K. Koeninger, M. Furtney, and M. Walker. </author> <title> A Shared-Memory MPP from Cray Research. </title> <journal> Digital Technical Journal, </journal> <volume> 6(2) </volume> <pages> 8-21, </pages> <year> 1994. </year>
Reference-contexts: Here we discuss the key features of the design; see [11] or <ref> [12] </ref> for an overview and [5] for a complete functional description. The DEC Alpha 21064 [24] is a 64-bit, dual-issue, second generation RISC processor, clocked at 150 MHz (6.67 ns cycle), with 8 KB instruction and data caches, each with 32-byte lines. <p> This supports the vendors' claim that eliminating the L2 cache allows for higher memory bandwidth when streaming through very large data sets, as is typical of vector-style scientific codes <ref> [12] </ref>. The T3D can deliver roughly 220 MB/s from memory into the processor and the workstation only about half that amount. 2.3 Local Writes Our second probe, which updates the array in the inner loop, produces the write latency profile shown in Figure 2.
Reference: [13] <author> John Kubiatowicz and Anant Agarwal. </author> <title> Anatomy of a Message in the Alewife Multiprocessor. </title> <booktitle> In 7th ACM International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1993. </year>
Reference-contexts: This has a significant impact on how the machine supports shared memory. Many shared-memory machines map all memory accessible to a process into the virtual address space and a virtual address is translated into a global physical address <ref> [2, 16, 10, 13, 14] </ref>. The memory system extracts the node number and physical location from the global physical address and performs either a local memory access or a message transaction with a remote memory controller.
Reference: [14] <author> Jeffrey Kuskin, David Ofelt, Mark Heinrich, John Hein-lein, Richard Simoni, Kourosh Gharachorloo, John Chapin, David Nakahira, Joel Baxter, Mark Horowitz, Anoop Gupta, Mendel Rosenblum, and John Hennessy. </author> <title> The Stanford Flash Multiprocessor. </title> <booktitle> In 21st International Symposium on Computer Architecture, </booktitle> <pages> pages 302-13, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: This has a significant impact on how the machine supports shared memory. Many shared-memory machines map all memory accessible to a process into the virtual address space and a virtual address is translated into a global physical address <ref> [2, 16, 10, 13, 14] </ref>. The memory system extracts the node number and physical location from the global physical address and performs either a local memory access or a message transaction with a remote memory controller.
Reference: [15] <author> C. E. Leiserson, Z. S. Abuhamdeh, D. C. Douglas, C. R. Feynman, M. N. Ganmukhi, J. V. Hill, W. D. Hillis, B. C. Kuszmaul, M. A. St. Pierre, D. S. Wells, M. C. Wong, S. Yang, and R. Zak. </author> <title> The Network Architecture of the CM-5. </title> <booktitle> In Symposium on Parallel and Distributed Algorithms '92, </booktitle> <pages> pages 272-285, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: 1 Introduction In 1991 and 1992 a wave of large-scale parallel machines were announced that followed the "shell" approach [25], including the Thinking Machines CM-5 <ref> [15] </ref>, Intel Paragon [8], Meiko CS-2 [1], and CRAY-T3D [11]. In this approach the core of each node is realized by a state-of-the-art commercial microprocessor and its memory system, surrounded by a shell of additional logic to support global operations, such as communication and synchronization.
Reference: [16] <author> D. Lenoski, J. Laundon, K. Gharachorloo, A. Gupta, and J. L. Hennessy. </author> <title> The Directory Based Cache Coherance Protocol for the DASH Multiprocessor. </title> <booktitle> In Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <year> 1990. </year>
Reference-contexts: This has a significant impact on how the machine supports shared memory. Many shared-memory machines map all memory accessible to a process into the virtual address space and a virtual address is translated into a global physical address <ref> [2, 16, 10, 13, 14] </ref>. The memory system extracts the node number and physical location from the global physical address and performs either a local memory access or a message transaction with a remote memory controller.
Reference: [17] <author> Steve Luna. </author> <title> Implementing an Efficient Global Memory Portability Layer on Distributed Memory Multiprocessors. </title> <type> Master's thesis, </type> <institution> University of California, Berkeley, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: It has been implemented on the CM-5, Paragon, SP-1, and a variety of networks of workstations, using Active Messages to implement the global address space <ref> [17, 26, 19] </ref>. The language has the following salient features: * A program is comprised of a thread of control on each processor from a single code image. * Threads interact through reads and writes on shared data, referenced by global pointers or spread arrays.
Reference: [18] <author> Niel K. Madsen. </author> <title> Divergence Preserving Discrete Surface Integral Methods for Maxwell's Curl Equations Using Non-Orthogonal Unstructured Grids. </title> <type> Technical Report 92.04, </type> <institution> RIACS, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: various components of the language implementation on the T3D in a performance several versions of EM3D using a synthetic kernel graph with 16,000 nodes of degree 20 on 32 T3D processors. study of a scalable Split-C application, EM3D, that models the propagation of electro-magnetic waves through objects in three dimensions <ref> [18] </ref>. A preprocessing step casts this problem into a simple computation on an irregular bipartite graph containing nodes representing electric and magnetic field values.
Reference: [19] <author> Richard Martin. HPAM: </author> <title> An Active Message Layer for a Network of HP Workstations. In Hot Interconnects II, </title> <month> August </month> <year> 1994. </year>
Reference-contexts: It has been implemented on the CM-5, Paragon, SP-1, and a variety of networks of workstations, using Active Messages to implement the global address space <ref> [17, 26, 19] </ref>. The language has the following salient features: * A program is comprised of a thread of control on each processor from a single code image. * Threads interact through reads and writes on shared data, referenced by global pointers or spread arrays.
Reference: [20] <institution> The Parkbench Committee. PARKBENCH Committee Report-1, </institution> <year> 1993. </year> <type> Technical Report CS-93-213, </type> <institution> Computer Science Department, University of Tennesse, Knoxville. </institution>
Reference-contexts: The results prove critical to understanding the costs and consistency issues of global operations because the memory system is the primary gateway to the shell. 2.1 Micro-benchmarking Benchmarking massively parallel processors usually refers to measuring the execution time of a set of applications (such as ParkBench <ref> [20] </ref> or Perfect Club [3]) designed to represent a meaningful workload. This measures the performance of the system as a whole, including the processor, the memory system, and the compiler, which is appropriate if one is taking all of these as fixed.
Reference: [21] <author> R. H. Saavedra, R. S. Gaines, and M. J. Carlton. </author> <title> Micro Benchmark Analysis of the KSR1. </title> <booktitle> In Supercomputing '93, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: However, the results tell us little about the performance of the individual components of the system, which is important if we are developing a new compiler or, in many cases, optimizing applications. In this paper we take a different path toward performance evaluation inspired by Saavedra's micro-benchmarks <ref> [22, 21] </ref>. We treat the system as a "gray box" admitting that we have some a priori knowledge of the system, but that it is both incomplete and unverified. Simple probes are used to determine the parameters and characteristics of the machine.
Reference: [22] <author> R. H. Saavedra-Barrera. </author> <title> CPU Performance Evaluation and Execution Time Prediction Using Narrow Spectrum Bench-marking. </title> <type> PhD thesis, </type> <institution> U.C. Berkeley, Computer Science Division, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: However, the results tell us little about the performance of the individual components of the system, which is important if we are developing a new compiler or, in many cases, optimizing applications. In this paper we take a different path toward performance evaluation inspired by Saavedra's micro-benchmarks <ref> [22, 21] </ref>. We treat the system as a "gray box" admitting that we have some a priori knowledge of the system, but that it is both incomplete and unverified. Simple probes are used to determine the parameters and characteristics of the machine. <p> By varying the parameters that define the address stream, i.e., by varying the stimulus, and observing the variations in the response we can infer specific properties of the memory system. The probe, derived from <ref> [22] </ref> with slight modifications, simply steps through an array of a given size with a given stride. By increasing the size of the array, we increase the range of addresses within the stream. By increasing the stride, we increase the frequency at which the address varies from low to high.
Reference: [23] <author> J. P. Singh, T. Joe, J. L. Hennessy, and A. Gupta. </author> <title> An Empirical Comparison of the Kendall Square Research KSR-1 and Stanford DASH Multiprocessors. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 214-225, </pages> <address> Portland, Oregon, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: This also is significantly faster than similar MPPs: the cost of a read to a remote node on the DASH multiprocessor is roughly 3 s, and is about 7.5 s on the KSR <ref> [23] </ref>. 4.3 T3D Remote Writes The next primitive we examine is the remote write. Just as the remote read is an extension of the load instruction, remote writes use the Alpha store instruction on a global address.
Reference: [24] <author> Richard L. </author> <title> Sites. Alpha Architecture Reference Manual. </title> <institution> Digital Equipment Corporation, </institution> <year> 1992. </year>
Reference-contexts: Here we discuss the key features of the design; see [11] or [12] for an overview and [5] for a complete functional description. The DEC Alpha 21064 <ref> [24] </ref> is a 64-bit, dual-issue, second generation RISC processor, clocked at 150 MHz (6.67 ns cycle), with 8 KB instruction and data caches, each with 32-byte lines. The Alpha operates on 64-bit data values, whether integer or floating point, and has only word (32-bit) and long word (64-bit) memory operations. <p> We can draw two conclusions from these measurements. First, at very small strides, successive writes to the same line are written to the same entry in the write buffer. This is a feature known as write-merging <ref> [24] </ref>. Second, since main memory access time is roughly 145 nanoseconds, dividing this by 35 nanoseconds gives an estimated write buffer size of 4. This is corroborated by the Alpha 21064 Reference Manual [7]. This section has provided a detailed examination of the local memory system. <p> The designers chose to simplify the memory interface by not supporting byte read/write operations, but instead provided a family of byte manipulation instructions that operate on register values <ref> [24] </ref>. A byte store operation could therefore be implemented as a read-modify-write sequence. However, on a multiprocessor like the T3D, we cannot guarantee correct execution of a byte-store operation when multiple processors are updating the same word.
Reference: [25] <author> Jim Smith. </author> <title> Using Standard Microprocessors in MPPs. Presentation, </title> <publisher> ISCA, </publisher> <year> 1992. </year>
Reference-contexts: 1 Introduction In 1991 and 1992 a wave of large-scale parallel machines were announced that followed the "shell" approach <ref> [25] </ref>, including the Thinking Machines CM-5 [15], Intel Paragon [8], Meiko CS-2 [1], and CRAY-T3D [11].
Reference: [26] <author> Thorsten von Eicken, David E. Culler, Seth Copen Gold-stein, and Klaus Erik Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In International Symposium on Computer Architecture, </booktitle> <pages> pages 256-66, </pages> <year> 1992. </year>
Reference-contexts: It has been implemented on the CM-5, Paragon, SP-1, and a variety of networks of workstations, using Active Messages to implement the global address space <ref> [17, 26, 19] </ref>. The language has the following salient features: * A program is comprised of a thread of control on each processor from a single code image. * Threads interact through reads and writes on shared data, referenced by global pointers or spread arrays. <p> The fetch&increment operation is essentially the cost of a remote read, i.e., about 1 microsecond. To deposit a message of four data words and one control word into a remote queue (essentially equivalent to a CMAM Active Message call <ref> [26] </ref>), takes 2.9 s, whereas dispatching on the receiving end and accessing the message takes 1.5 s.
References-found: 26

