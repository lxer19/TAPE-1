URL: http://www.cse.ucsc.edu/research/ccrg/publications/chane.masters.ps.gz
Refering-URL: http://www.cse.ucsc.edu/research/ccrg/publications.html
Root-URL: http://www.cse.ucsc.edu
Title: Implementation of Adaptive Flow Control Mechanism to the Swift Redundant Distributed File Architecture  
Author: Professor Darrell D. E. Long Professor J. J. Garcia-Luna Dean 
Degree: A thesis submitted in partial satisfaction of the requirements for the degree of MASTER OF SCIENCE in COMPUTER AND INFORMATION SCIENCE by Chane Lee Fullmer  The thesis of Chane Lee Fullmer is approved:  Professor Anujan Varma  
Date: June 1994  
Affiliation: UNIVERSITY OF CALIFORNIA SANTA CRUZ  of Graduate Studies and Research  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> D. Bertsekas and R. Gallager, </author> <title> Data Networks, </title> <booktitle> 2nd Edition. </booktitle> <address> Prentice-Hall,Inc., </address> <year> 1992. </year>
Reference-contexts: Packets are transmitted one at a time and the sender waits until an acknowledgment is received from the intended destination before new packets are sent. This type of operation amounts to an automatic repeat request (ARQ) Stop-and-Wait protocol <ref> [1] </ref>. The protocol is simple and error free, but does not achieve as high a throughput as is available with other protocols such as ARQ Go back N [1]. <p> This type of operation amounts to an automatic repeat request (ARQ) Stop-and-Wait protocol <ref> [1] </ref>. The protocol is simple and error free, but does not achieve as high a throughput as is available with other protocols such as ARQ Go back N [1]. Even with this handicap, the non-redundant prototype was able to achieve throughputs of 80% of the theoretical maximum of 1.25 MB/s on a local ethernet for write operations. <p> It has also been called congestion control in the same context and we make no distinction between the two here. Flow control mechanisms have been studied for many years <ref> [8, 15, 1] </ref>. 12 FIGURE 2.6: Logical organization of buffers for the Swift architecture. The underlying system that the Swift architecture runs on can be thought of as a group of buffers interconnected by a fixed capacity pipe (See Figure 2.6). <p> Making this determination and performing adjustments dynamically during a session is a difficult problem to solve <ref> [1] </ref>. Eldridge also discusses the use of windows as a rate control mechanism in [6]. One solution to the dynamic window adjustment has been suggested by Van Jacobson in his addition of congestion avoidance and control mechanisms to TCP/IP [12]. <p> The operation of these two counters are all that is required to implement a basic ARQ Go Back N protocol <ref> [1] </ref>. Operational code was added to two routines in the module trans driver.c and a new routine was added. To the routine activate instructions a few simple lines of code were added to do the difference calculation and comparison to the available window size. <p> It is also evident from the results a window size of two provides the maximal throughput for write operations. This result is consistent with Bertsekas and Gallager <ref> [1] </ref> discussion of the recommended window size for end-to-end window flow control as typically between n and 3n, where n is the path length between the communicating entities. <p> Our prototype is running on a local network and hence the path length is one, and we would therefore expect to use a window size of from one to three, with two being a good value. Bertsekas and Gallager <ref> [1] </ref> also suggest that determining the proper window size and adjusting it to congestion is not an easy problem, but their estimate of a value between n and 3n seems accurate for our system. <p> Because of this writes don't achieve full network utilization until block requests of over (stripesize fi (windowsize + 2)). This mechanism was originally designed for the Internet and sessions with large numbers of packets to transmit, and is perhaps not suited for the smaller sessions. Bertsekas and Gallager <ref> [1] </ref> have discussed that one of the limitations of end-to-end windows flow control is the tradeoff of choosing a window size small window sizes keep packets in the subnet low and congestion to a minimum, but large windows allow higher rates of transmission and maximum throughput during light traffic conditions. 43
Reference: [2] <author> D. R. Boggs, J. C. Mogul, and C. A. Kent, </author> <title> Measured capacity of an ethernet: Myths and reality, </title> <booktitle> in Proceedings of SIGCOMM 88, </booktitle> <pages> pp. 222-234, </pages> <publisher> ACM, </publisher> <year> 1988. </year>
Reference-contexts: The improvement is less dramatic for larger node populations. However, it should be noted that in all cases the read operation is brought up to about 0.9 MB/s, which is very close to the available bandwidth of the network <ref> [2] </ref>, limiting any further improvements. Write throughput starts to drop off as the number of blocks transmitted by the client reaches, or exceeds, the capacity of the OS buffers. <p> FIGURE 5.2: RAID-0 adaptive prototype performance. 29 5.2 RAID-4 Performance Evaluation Read operations for the RAID-4 adaptive prototype were also able to push the throughput on the network to 0.92 MB/s close to the maximum available of 1.15 MB/s <ref> [2] </ref>. This is an improvement of 35% over the Stop and Wait protocol for 2-nodes. The 3-node and 4-node systems also attained over 0.90 MB/s which again, is close to the maximum bandwidth available, and the limiting factor in obtaining further increases in throughput.
Reference: [3] <author> L. S. Brakmo, S. W. O'Malley, and L. L. Peterson, </author> <title> TCP Vegas: New techniques for congestion detection and avoidance, </title> <type> Tech. Rep. TR 94 04, </type> <institution> The University of Arizona, </institution> <year> 1994. </year>
Reference-contexts: We then use Jacobson's round trip time estimator to compute our retransmit timer. This gives us a more accurate timeout calculation for retransmissions. Brakmo, O'Malley and Peterson also use similar modifications (as well as others) in their proposed improvements to TCP called Vegas <ref> [3] </ref>. 25 Chapter 5 Results This chapter discusses the results obtained after extensive testing of the Swift/RAID adaptive window prototype. Tests were performed for all available versions of the architecture, including RAID-0, RAID-4 and RAID-5.
Reference: [4] <author> L.-F. Cabrera and D. D. E. Long, Swift: </author> <title> Using distributed disk striping to provide high I/O data rates, </title> <journal> Computing Systems, </journal> <volume> vol. 4, no. 4, </volume> <pages> pp. 405-36, </pages> <year> 1991. </year>
Reference-contexts: These applications may use huge files containing images or digitized sounds. Current systems can only offer a small percentage of the data rates required to feed these applications. The Swift distributed file architecture was introduced in 1991 <ref> [4] </ref> to address this problem. A prototype was built to test the viability of the overall design, but did not include redundancy [7]. <p> Chapter 6 discusses our conclusions and future work. 3 Chapter 2 Background 2.1 Swift Architecture and Prototype The Swift distributed file system is a client/server architecture developed to address the problem of data rate mismatches between the requirements of an application, storage devices, and the interconnection medium <ref> [4] </ref>. The goal of Swift is to support high data transfer rates in a distributed system by exploiting the inherent parallelism in these systems. The Swift architecture stripes files [29] across several networked workstations acting as servers. <p> MIPS processors on the client/servers and typical high-performance disk drives 1 could achieve throughputs on the order of 0.6 MB/s per available disk for pure reads and 0.3 MB/s for pure writes (i.e., a system with 30 disks available could achieve 18 MB/s for reads, and 9 MB/s for writes) <ref> [4] </ref>. A Swift prototype has been implemented on top of the UNIX 2 file system running on Sun Sparc workstations [7]. The prototype implements file read, write, open, close and seek operations. <p> The prototype implements file read, write, open, close and seek operations. However, it did not provide for the redundancy of stored data as described in the original Swift architecture design <ref> [4] </ref>. The communications protocols used in the prototype operate in a synchronous manner (as opposed to the asynchronous operation proposed in the original Swift design). Packets are transmitted one at a time and the sender waits until an acknowledgment is received from the intended destination before new packets are sent. <p> This is in contrast to the four operations presently required, all of which involve the client. 44 Cabrera and Long have shown through simulations that the Swift design will scale to high speed networks with fast processors and high speed disk drives <ref> [4] </ref>. Also Lee and Katz have shown that RAIDs are scalable in their simulations [17, 19]. Lee, et al, have designed the RAID-II system [18] to operate in a high-performance environment using RAID level 5 to capitalize on this scalability.
Reference: [5] <author> J.-K. Choi and C. K. </author> <title> Un, On acknowledgment schemes of sliding window flow control, </title> <journal> IEEE Transactions on Communications, </journal> <volume> vol. 37, no. 11, </volume> <pages> pp. 1184-1191, </pages> <year> 1989. </year>
Reference-contexts: Upon receiving the permit from the destination the sender continues by sending another batch of packets. This method is efficient, can approach optimal performance of a given system [15] and, if necessary, the permit packet traffic can be minimized with a delayed acknowledgment scheme <ref> [5] </ref>. A disadvantage of this scheme is that of choosing a window size. The choice of a window size is a trade-off: small window sizes limit the congestion and tend to avoid large delays, and large window sizes allow full-speed transmission and maximum throughput under lightly loaded conditions.
Reference: [6] <author> C. A. Eldridge, </author> <title> Rate controls in standard transport layer protocols, </title> <journal> ACM Computer Communication Review, </journal> <volume> vol. 22, no. 3, </volume> <year> 1992. </year>
Reference-contexts: Making this determination and performing adjustments dynamically during a session is a difficult problem to solve [1]. Eldridge also discusses the use of windows as a rate control mechanism in <ref> [6] </ref>. One solution to the dynamic window adjustment has been suggested by Van Jacobson in his addition of congestion avoidance and control mechanisms to TCP/IP [12]. <p> Bertsekas and Gallager [1] also suggest that determining the proper window size and adjusting it to congestion is not an easy problem, but their estimate of a value between n and 3n seems accurate for our system. This result is also supported by Eldridge <ref> [6] </ref> where it was noted that on a high-bandwidth, low delay networks (i.e., Ethernets), a small window size will keep the service queues relatively full. <p> Other window sizes of 1,3,4,5 showed little, or no improvement for writes and in the case for a window size of five the results tended to be below all others because of the swamping of the client Ethernet queue. This is also supported by Eldridge <ref> [6] </ref>. Read operations did better with higher window sizes, but not significantly better (approximately 5% in most cases). Therefore on our subnetwork a window size of two provides a maximal throughput in write operations, and a close to maximal throughput for read operations.
Reference: [7] <author> A. T. Emigh, </author> <title> The Swift Architecture: Anatomy of a Prototype, </title> <type> Tech. Rep. Technical Report, </type> <institution> UCSC Concurrent Systems Laboratory, </institution> <year> 1992. </year>
Reference-contexts: Current systems can only offer a small percentage of the data rates required to feed these applications. The Swift distributed file architecture was introduced in 1991 [4] to address this problem. A prototype was built to test the viability of the overall design, but did not include redundancy <ref> [7] </ref>. A redundancy scheme similar in operation to a RAID (Redundant Arrays of Inexpensive Disks) [27] has since been implemented to the original prototype known as the Swift/RAID distributed file architecture [22, 20]. <p> RAID level 5 shows an increase in throughput of 25% for reads and a 51% increase in throughput for writes for the asynchronous operation. Our design has also attained a 30% increase over the reported throughput for the non-redundant Swift prototype <ref> [20, 7] </ref> for read operations (both RAID levels 4 and 5), and has achieved one half of the write throughput for RAID level 4 and two thirds of the throughput for RAID level 5 write operations. <p> A Swift prototype has been implemented on top of the UNIX 2 file system running on Sun Sparc workstations <ref> [7] </ref>. The prototype implements file read, write, open, close and seek operations. However, it did not provide for the redundancy of stored data as described in the original Swift architecture design [4]. <p> It was noted that the operating system could drop UDP packets of over 8 KB at random <ref> [7] </ref>. To guarantee we did not lose packets over 8 KB our packet size including header information had to stay below that level.
Reference: [8] <author> M. Gerla and L. Kleinrock, </author> <title> Flow control: A comparative survey, </title> <journal> IEEE Transactions on Communications, </journal> <volume> vol. COM-28, no. 4, </volume> <pages> pp. 553-574, </pages> <year> 1980. </year>
Reference-contexts: It has also been called congestion control in the same context and we make no distinction between the two here. Flow control mechanisms have been studied for many years <ref> [8, 15, 1] </ref>. 12 FIGURE 2.6: Logical organization of buffers for the Swift architecture. The underlying system that the Swift architecture runs on can be thought of as a group of buffers interconnected by a fixed capacity pipe (See Figure 2.6). <p> In addition, because of the communications primitives available to us in the Swift/RAID prototype at the client/server session level, the specific class of end-to-end windowing (also know as entry-to-exit flow control <ref> [8] </ref>) could be chosen. In this scheme the sender has knowledge of the destination's buffer capacity and only sends packets out in batches of sizes less than or equal to the available buffer capacity.
Reference: [9] <author> G. A. Gibson, </author> <title> Redundant Disk Arrays: Reliable, Parallel Secondary Storage. </title> <publisher> The MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: RAID (Redundant Arrays of Inexpensive Disks) <ref> [27, 26, 14, 9] </ref> is a hardware solution to the high data rate problem accomplishing high data rates by striping data across multiple disk devices on a common bus architecture. <p> The writing of this one block requires two reads and two writes a ratio of four disk operations to one small write operation. As a result, small writes are expensive on both RAID-4 and RAID-5 systems, and is known as the small write problem <ref> [30, 9] </ref>. Parity logging [30] and floating parity [21] have been suggested as mechanisms to lessen the impact of small writes in redundant systems. <p> Reconstruction mode is where the failed drive has been replaced and the data is being reconstructed and written to the new drive. Issues and performance analysis of 9 degraded and reconstruction operation are discussed by Lee [17, 19], Ng and Mattson [23] and Gibson <ref> [9] </ref>. 2.3 Swift/RAID Architecture and Prototype The Swift/Raid prototype was developed to accomplish redundancy to the original Swift prototype [22, 20]. The new prototype uses a table-driven distributed real-time state machine approach. The prototype operations are defined as cooperating sequences of serially executed atomic primitives [20].
Reference: [10] <author> J. H. Hartman and J. K. Ousterhout, </author> <title> Zebra: A striped network file system, </title> <booktitle> in Proceedings of the Usenix File System Workshop, </booktitle> <year> 1992. </year>
Reference-contexts: Similar work has been presented by Stonebraker and Schloss in their design of RADD (Redundant Array of Distributed Disks) [32, 31] based on a RAID level 5 scheme, and by Ousterhout and Hartman for the Zebra <ref> [24, 10, 11] </ref> striped file system. <p> Parity logging [30] and floating parity [21] have been suggested as mechanisms to lessen the impact of small writes in redundant systems. The Zebra striped file system <ref> [10, 24] </ref> uses an approach that stripes a log-structured file system [28] across disks, with parity information, and provides high performance for small writes [11]. RAID-4 and RAID-5 have three basic operating modes: normal mode, degraded mode and reconstruction mode. Normal mode is where all drives are fully operational.
Reference: [11] <author> J. H. Hartman and J. K. Ousterhout, </author> <title> The Zebra striped network file system, </title> <booktitle> in Proceedings of SIGOPS 1993, </booktitle> <pages> pp. 29-43, </pages> <year> 1993. </year>
Reference-contexts: Similar work has been presented by Stonebraker and Schloss in their design of RADD (Redundant Array of Distributed Disks) [32, 31] based on a RAID level 5 scheme, and by Ousterhout and Hartman for the Zebra <ref> [24, 10, 11] </ref> striped file system. <p> The Zebra striped file system [10, 24] uses an approach that stripes a log-structured file system [28] across disks, with parity information, and provides high performance for small writes <ref> [11] </ref>. RAID-4 and RAID-5 have three basic operating modes: normal mode, degraded mode and reconstruction mode. Normal mode is where all drives are fully operational. Degraded mode is where one drive has failed and its data is being reconstructed from the remaining disks.
Reference: [12] <author> V. Jacobson, </author> <title> Congestion avoidance and control, </title> <booktitle> in Proceedings of SIGCOMM 88, ACM, </booktitle> <year> 1988. </year> <month> 46 </month>
Reference-contexts: Eldridge also discusses the use of windows as a rate control mechanism in [6]. One solution to the dynamic window adjustment has been suggested by Van Jacobson in his addition of congestion avoidance and control mechanisms to TCP/IP <ref> [12] </ref>. The solution is based on seven fairly simple techniques to accomplish the following: round-trip time variance estimations, exponential retransmit back-off, slow-start, an aggressive acknowledgment policy, dynamic window sizing on congestion, a fast retransmit and Karn's clamped retransmit back-off. <p> The round-trip time estimation algorithm is used to retransmit packets that are not acknowledged within a normal bound (and therefore are probably lost), and does not wait for the receiver to timeout and request the retransmission. The algorithms provided by Jacobson in <ref> [12] </ref> are simple and make the solution fairly easy to implement. <p> the transaction driver for storing and ordering the packets in the window, also, the driver already has a mechanism in place to return a NAK (negative acknowledgment) in the form of a restart whenever a packet is received out of order.) The congestion avoidance is that recommended by Van Jacobson <ref> [12] </ref> for TCP. <p> It is also less than 8K size limitation. 20 Chapter 4 Implementation of the Adaptive Prototype Design The design chosen and implemented for the flow control of the Swift/RAID transaction driver architecture is an ARQ Go Back N protocol with congestion avoidance as set forth by Van Jacobson <ref> [12] </ref> for TCP. The Swift/RAID architecture implementation is very modular, with all of the communications code placed in one tightly cohesive C language module, trans driver.c. All modifications to accomplish the flow-control/congestion avoidance were applied to this one module and its corresponding include file trans.h. <p> Specifically, a current window size (called svi cwnd), a threshold value (named svi thresh) and a round trip timer (labeled svi rtt) were added. Code has been added to the trans driver.c module similar to that specified in the Jacobson paper <ref> [12] </ref>. 3 The svi cwnd value is used as the maximum current window size and is changed dynamically based on system conditions. As congestion is detected the svi thresh value is set to one half of svi cwnd, and the svi cwnd value is then set to one. <p> It would appear these types of mechanisms are best kept to wide are networks with longer delays. Most of our improvement in throughput was gained by the simple self-clocking <ref> [12] </ref> of the data packets with the acknowledgments sent by the destination as packets were received. The implementation of the addition of a flow control mechanism was enhanced by the design of the Swift/RAID prototype.
Reference: [13] <author> O. G. Johnson, </author> <title> Three-dimensional wave equation computations on vector computers, </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol. 72, </volume> <year> 1984. </year>
Reference-contexts: as one spinning at 3600 RPM, with average rotational latency of 8.3ms, and average seek time of 12ms. 2 UNIX is a registered trademark of UNIX Systems Laboratories Incorporated. 6 2.2 RAID Overview File striping is an idea that has been used successfully in the supercomputing arena for several years <ref> [13] </ref> and has been formally discussed since 1986 [29]. RAID (Redundant Arrays of Inexpensive Disks) [27, 26, 14, 9] is a hardware solution to the high data rate problem accomplishing high data rates by striping data across multiple disk devices on a common bus architecture.
Reference: [14] <author> R. H. Katz, G. Gibson, and D. A. Patterson, </author> <title> Disk system architectures for high performance computing, </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol. 77, no. 12, </volume> <pages> pp. 1842-58, </pages> <year> 1989. </year>
Reference-contexts: RAID (Redundant Arrays of Inexpensive Disks) <ref> [27, 26, 14, 9] </ref> is a hardware solution to the high data rate problem accomplishing high data rates by striping data across multiple disk devices on a common bus architecture. <p> Data is striped across the units along with some form of redundancy to achieve fault tolerance in the case of a disk failure. (A similar discussion concerning magnetic memories has been presented by Patel [25]). A variety of redundancy schemes have been discussed <ref> [27, 26, 14, 19] </ref>. These choices have been labeled RAID-0 through RAID-6. RAID-0 is a non-redundant disk array supporting only disk striping. RAID-1 is a simple mirroring of data onto two disks accomplishing 100% redundancy.
Reference: [15] <author> D. D. Kouvatsos and A. T. Othman, </author> <title> Optimal flow control of end-to-end packet-switched network with random routing, </title> <journal> IEEE Proceedings, </journal> <volume> vol. 136 Pt E, no. 2, </volume> <pages> pp. 90-100, </pages> <year> 1989. </year>
Reference-contexts: It has also been called congestion control in the same context and we make no distinction between the two here. Flow control mechanisms have been studied for many years <ref> [8, 15, 1] </ref>. 12 FIGURE 2.6: Logical organization of buffers for the Swift architecture. The underlying system that the Swift architecture runs on can be thought of as a group of buffers interconnected by a fixed capacity pipe (See Figure 2.6). <p> The destination sends permits (basically acknowledgments) of the packets received and 13 the buffer space now available. Upon receiving the permit from the destination the sender continues by sending another batch of packets. This method is efficient, can approach optimal performance of a given system <ref> [15] </ref> and, if necessary, the permit packet traffic can be minimized with a delayed acknowledgment scheme [5]. A disadvantage of this scheme is that of choosing a window size.
Reference: [16] <author> T. F. La Porta and M. Schwartz, </author> <title> Architectures, features, and implementation of high-speed transport protocols, </title> <journal> IEEE Network Magazine, </journal> <volume> vol. 5, no. 3, </volume> <pages> pp. 14-22, </pages> <year> 1991. </year>
Reference-contexts: Note the erratic behavior when congestion starts evidenced by the wide error bars (showing minimum and maximum throughput values). effort was a basic form of rate control and has support in the literature <ref> [33, 16] </ref>. However, the implementation suffered from an inability to react quickly to periods of severe congestion and showed an erratic behavior (See Figure 3.1). This approach was not investigated further.
Reference: [17] <author> E. K. Lee, </author> <title> Software and implementation issues in the implementation of a RAID prototype, </title> <type> tech. rep., </type> <institution> UC Berkeley, </institution> <year> 1990. </year>
Reference-contexts: Reconstruction mode is where the failed drive has been replaced and the data is being reconstructed and written to the new drive. Issues and performance analysis of 9 degraded and reconstruction operation are discussed by Lee <ref> [17, 19] </ref>, Ng and Mattson [23] and Gibson [9]. 2.3 Swift/RAID Architecture and Prototype The Swift/Raid prototype was developed to accomplish redundancy to the original Swift prototype [22, 20]. The new prototype uses a table-driven distributed real-time state machine approach. <p> Also Lee and Katz have shown that RAIDs are scalable in their simulations <ref> [17, 19] </ref>. Lee, et al, have designed the RAID-II system [18] to operate in a high-performance environment using RAID level 5 to capitalize on this scalability. Our Swift/RAID adaptive design provides the asynchronous operation of the original Swift design, and provides the RAID level 5 redundancy to the system.
Reference: [18] <author> E. K. Lee, P. M. Chen, J. H. Hartman, A. L. C. Drapeau, E. L. Miller, R. H. Katz, G. A. Gibson, and D. A. Patterson, </author> <title> RAID-II: A scalable storage architecture for high-bandwidth network file service, </title> <type> tech. rep., </type> <institution> UC Berkeley, </institution> <year> 1992. </year>
Reference-contexts: Also Lee and Katz have shown that RAIDs are scalable in their simulations [17, 19]. Lee, et al, have designed the RAID-II system <ref> [18] </ref> to operate in a high-performance environment using RAID level 5 to capitalize on this scalability. Our Swift/RAID adaptive design provides the asynchronous operation of the original Swift design, and provides the RAID level 5 redundancy to the system.
Reference: [19] <author> E. K. Lee and R. Katz, </author> <title> Performance consequences of parity placement in disk arrays, </title> <booktitle> in Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1991. </year>
Reference-contexts: Data is striped across the units along with some form of redundancy to achieve fault tolerance in the case of a disk failure. (A similar discussion concerning magnetic memories has been presented by Patel [25]). A variety of redundancy schemes have been discussed <ref> [27, 26, 14, 19] </ref>. These choices have been labeled RAID-0 through RAID-6. RAID-0 is a non-redundant disk array supporting only disk striping. RAID-1 is a simple mirroring of data onto two disks accomplishing 100% redundancy. <p> Reconstruction mode is where the failed drive has been replaced and the data is being reconstructed and written to the new drive. Issues and performance analysis of 9 degraded and reconstruction operation are discussed by Lee <ref> [17, 19] </ref>, Ng and Mattson [23] and Gibson [9]. 2.3 Swift/RAID Architecture and Prototype The Swift/Raid prototype was developed to accomplish redundancy to the original Swift prototype [22, 20]. The new prototype uses a table-driven distributed real-time state machine approach. <p> Also Lee and Katz have shown that RAIDs are scalable in their simulations <ref> [17, 19] </ref>. Lee, et al, have designed the RAID-II system [18] to operate in a high-performance environment using RAID level 5 to capitalize on this scalability. Our Swift/RAID adaptive design provides the asynchronous operation of the original Swift design, and provides the RAID level 5 redundancy to the system.
Reference: [20] <author> D. D. E. Long, B. R. Montague, and L.-F. Cabrera, Swift/RAID: </author> <title> A distributed RAID system, </title> <journal> Computing Systems, </journal> <volume> vol. 7, no. 3, </volume> <year> 1994. </year>
Reference-contexts: A prototype was built to test the viability of the overall design, but did not include redundancy [7]. A redundancy scheme similar in operation to a RAID (Redundant Arrays of Inexpensive Disks) [27] has since been implemented to the original prototype known as the Swift/RAID distributed file architecture <ref> [22, 20] </ref>. Similar work has been presented by Stonebraker and Schloss in their design of RADD (Redundant Array of Distributed Disks) [32, 31] based on a RAID level 5 scheme, and by Ousterhout and Hartman for the Zebra [24, 10, 11] striped file system. <p> RAID level 5 shows an increase in throughput of 25% for reads and a 51% increase in throughput for writes for the asynchronous operation. Our design has also attained a 30% increase over the reported throughput for the non-redundant Swift prototype <ref> [20, 7] </ref> for read operations (both RAID levels 4 and 5), and has achieved one half of the write throughput for RAID level 4 and two thirds of the throughput for RAID level 5 write operations. <p> Even with this handicap, the non-redundant prototype was able to achieve throughputs of 80% of the theoretical maximum of 1.25 MB/s on a local ethernet for write operations. However, it was only able to reach about 60% of maximum throughput for read operations <ref> [20] </ref>. 1 For these simulations a typical high-performance disk drive was described as one spinning at 3600 RPM, with average rotational latency of 8.3ms, and average seek time of 12ms. 2 UNIX is a registered trademark of UNIX Systems Laboratories Incorporated. 6 2.2 RAID Overview File striping is an idea that <p> Issues and performance analysis of 9 degraded and reconstruction operation are discussed by Lee [17, 19], Ng and Mattson [23] and Gibson [9]. 2.3 Swift/RAID Architecture and Prototype The Swift/Raid prototype was developed to accomplish redundancy to the original Swift prototype <ref> [22, 20] </ref>. The new prototype uses a table-driven distributed real-time state machine approach. The prototype operations are defined as cooperating sequences of serially executed atomic primitives [20]. These primitives are referred to as instructions. <p> The new prototype uses a table-driven distributed real-time state machine approach. The prototype operations are defined as cooperating sequences of serially executed atomic primitives <ref> [20] </ref>. These primitives are referred to as instructions. A transfer plan is a set of these instructions used between a client and server (currently no server to server transfer plans are used) to perform a specific file operation task. <p> The measurements performed to evaluate the prototype were basically the same as those reported in <ref> [20] </ref>. In fact, the identical test programs and RAID-4 and RAID-5 modules 26 were used. The blocksize was changed from 8192 bytes to 7340 bytes to minimize internal fragmentation of the Ethernet packet 1 . <p> Parity calculations have been shown to be a factor in limiting the ability of the workstations used to achieve better performance. Long, Montague and Cabrera found the cost of parity to be as much as 200KB/s in the Swift/RAID Stop and Wait prototype <ref> [20] </ref>. Additionally, the bottleneck created by the use of a single parity node becomes as issue for large writes, and tends to keep RAID-4 performance below others, such as RAID-5. Also, as node and window sizes increase, the operating system Ethernet buffer limitation has a detrimental affect on throughput. <p> The adaptive RAID-4 and RAID-5 prototype both are able to achieve read throughputs in excess of 0.90 MB/s, pushing the network bandwidth limits. This is an improvement over the original Swift non-redundant prototype of over 30% (as reported in <ref> [20] </ref>). Write operations for the adaptive prototype obtained 50% of the original Swift/RAID prototype for both RAID-4 and RAID-5 (with a window size of two).
Reference: [21] <author> J. Menon, J. Roche, and J. Kasson, </author> <title> Floating parity and data disk arrays, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 17, no. 1 and 2, </volume> <pages> pp. 129-39, </pages> <year> 1993. </year>
Reference-contexts: As a result, small writes are expensive on both RAID-4 and RAID-5 systems, and is known as the small write problem [30, 9]. Parity logging [30] and floating parity <ref> [21] </ref> have been suggested as mechanisms to lessen the impact of small writes in redundant systems. The Zebra striped file system [10, 24] uses an approach that stripes a log-structured file system [28] across disks, with parity information, and provides high performance for small writes [11].
Reference: [22] <author> B. R. Montague, </author> <title> The Swift/RAID distributed transaction driver, </title> <type> Tech. Rep. </type> <institution> UCSC-CRL-93-03, Computer and Information Sciences Board, UCSC., </institution> <year> 1993. </year>
Reference-contexts: A prototype was built to test the viability of the overall design, but did not include redundancy [7]. A redundancy scheme similar in operation to a RAID (Redundant Arrays of Inexpensive Disks) [27] has since been implemented to the original prototype known as the Swift/RAID distributed file architecture <ref> [22, 20] </ref>. Similar work has been presented by Stonebraker and Schloss in their design of RADD (Redundant Array of Distributed Disks) [32, 31] based on a RAID level 5 scheme, and by Ousterhout and Hartman for the Zebra [24, 10, 11] striped file system. <p> Issues and performance analysis of 9 degraded and reconstruction operation are discussed by Lee [17, 19], Ng and Mattson [23] and Gibson [9]. 2.3 Swift/RAID Architecture and Prototype The Swift/Raid prototype was developed to accomplish redundancy to the original Swift prototype <ref> [22, 20] </ref>. The new prototype uses a table-driven distributed real-time state machine approach. The prototype operations are defined as cooperating sequences of serially executed atomic primitives [20]. These primitives are referred to as instructions.
Reference: [23] <author> S. W. Ng and R. L. Mattson, </author> <title> Maintaining good performance in disk arrays during failure via uniform parity group distribution, </title> <booktitle> in Proceedings of the 5th International Symposium on High-Performance Distributed Computing, </booktitle> <pages> pp. 260-69, </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1992. </year>
Reference-contexts: Reconstruction mode is where the failed drive has been replaced and the data is being reconstructed and written to the new drive. Issues and performance analysis of 9 degraded and reconstruction operation are discussed by Lee [17, 19], Ng and Mattson <ref> [23] </ref> and Gibson [9]. 2.3 Swift/RAID Architecture and Prototype The Swift/Raid prototype was developed to accomplish redundancy to the original Swift prototype [22, 20]. The new prototype uses a table-driven distributed real-time state machine approach. The prototype operations are defined as cooperating sequences of serially executed atomic primitives [20]. <p> Reads are unaffected for RAID-4 if the parity disk has failed, otherwise RAID-4, as well as RAID-5, show a significant performance degradation. The decrease was about 20% for RAID-4 and 11% for RAID-5. 4 Ng and Mattson's <ref> [23] </ref> results show a decrease in total load due to writes for RAID-5 systems where the number of disks is less than eight, and average load decrease for less than four disks.
Reference: [24] <author> J. K. Ousterhout and J. H. Hartman, </author> <title> Zebra: A striped network file system, </title> <type> tech. rep., </type> <institution> UC Berkeley, </institution> <year> 1992. </year>
Reference-contexts: Similar work has been presented by Stonebraker and Schloss in their design of RADD (Redundant Array of Distributed Disks) [32, 31] based on a RAID level 5 scheme, and by Ousterhout and Hartman for the Zebra <ref> [24, 10, 11] </ref> striped file system. <p> Parity logging [30] and floating parity [21] have been suggested as mechanisms to lessen the impact of small writes in redundant systems. The Zebra striped file system <ref> [10, 24] </ref> uses an approach that stripes a log-structured file system [28] across disks, with parity information, and provides high performance for small writes [11]. RAID-4 and RAID-5 have three basic operating modes: normal mode, degraded mode and reconstruction mode. Normal mode is where all drives are fully operational.
Reference: [25] <author> A. M. Patel, </author> <title> Error and failure-control procedure for a large-size bubble memory, </title> <journal> IEEE Transactions on Magnetics, </journal> <volume> vol. MAG-18, no. 6, </volume> <pages> pp. 1319-21, </pages> <year> 1982. </year>
Reference-contexts: Data is striped across the units along with some form of redundancy to achieve fault tolerance in the case of a disk failure. (A similar discussion concerning magnetic memories has been presented by Patel <ref> [25] </ref>). A variety of redundancy schemes have been discussed [27, 26, 14, 19]. These choices have been labeled RAID-0 through RAID-6. RAID-0 is a non-redundant disk array supporting only disk striping. RAID-1 is a simple mirroring of data onto two disks accomplishing 100% redundancy.
Reference: [26] <author> D. A. Patterson, P. Chen, G. Gibson, and R. H. Katz, </author> <title> Introduction to redundant arrays of inexpensive disks (RAID), </title> <booktitle> in Proceedings of COMPCON Spring '89, </booktitle> <pages> pp. 112-17, </pages> <publisher> IEEE, </publisher> <year> 1989. </year>
Reference-contexts: RAID (Redundant Arrays of Inexpensive Disks) <ref> [27, 26, 14, 9] </ref> is a hardware solution to the high data rate problem accomplishing high data rates by striping data across multiple disk devices on a common bus architecture. <p> Data is striped across the units along with some form of redundancy to achieve fault tolerance in the case of a disk failure. (A similar discussion concerning magnetic memories has been presented by Patel [25]). A variety of redundancy schemes have been discussed <ref> [27, 26, 14, 19] </ref>. These choices have been labeled RAID-0 through RAID-6. RAID-0 is a non-redundant disk array supporting only disk striping. RAID-1 is a simple mirroring of data onto two disks accomplishing 100% redundancy. <p> RAID-6 organizes the array of disks into a two dimensional array and calculates parity for both row and column data, and allows this scheme to tolerate two or more disk failures. Further details may be found in <ref> [26] </ref>. This discussion will be limited to the RAID-4 and RAID-5 schemes. 3 For single-error correction RAID-2 uses dlg (K)e parity bits, where K is the number of data disks, and can correct soft errors on the fly as well as hard errors from a disk failure.
Reference: [27] <author> D. A. Patterson, G. Gibson, and R. H. Katz, </author> <title> A case for redundant arrays of inexpensive disks (RAID), </title> <booktitle> in Proceedings of the ACM SIGMOD Conference., </booktitle> <pages> pp. 109-16, </pages> <year> 1988. </year> <month> 47 </month>
Reference-contexts: The Swift distributed file architecture was introduced in 1991 [4] to address this problem. A prototype was built to test the viability of the overall design, but did not include redundancy [7]. A redundancy scheme similar in operation to a RAID (Redundant Arrays of Inexpensive Disks) <ref> [27] </ref> has since been implemented to the original prototype known as the Swift/RAID distributed file architecture [22, 20]. <p> RAID (Redundant Arrays of Inexpensive Disks) <ref> [27, 26, 14, 9] </ref> is a hardware solution to the high data rate problem accomplishing high data rates by striping data across multiple disk devices on a common bus architecture. <p> Data is striped across the units along with some form of redundancy to achieve fault tolerance in the case of a disk failure. (A similar discussion concerning magnetic memories has been presented by Patel [25]). A variety of redundancy schemes have been discussed <ref> [27, 26, 14, 19] </ref>. These choices have been labeled RAID-0 through RAID-6. RAID-0 is a non-redundant disk array supporting only disk striping. RAID-1 is a simple mirroring of data onto two disks accomplishing 100% redundancy.
Reference: [28] <author> M. Rosenblum and J. K. Ousterhout, </author> <title> The design and implementation of a log-structured file system, </title> <journal> ACM Transaction on Computer Systems, </journal> <volume> vol. 10, no. 1, </volume> <pages> pp. 26-52, </pages> <year> 1992. </year>
Reference-contexts: Parity logging [30] and floating parity [21] have been suggested as mechanisms to lessen the impact of small writes in redundant systems. The Zebra striped file system [10, 24] uses an approach that stripes a log-structured file system <ref> [28] </ref> across disks, with parity information, and provides high performance for small writes [11]. RAID-4 and RAID-5 have three basic operating modes: normal mode, degraded mode and reconstruction mode. Normal mode is where all drives are fully operational.
Reference: [29] <author> K. Salem and H. Garcia-Molina, </author> <title> Disk striping, </title> <booktitle> in Proceedings of the 2nd International Conference on Data Engineering, </booktitle> <pages> pp. 336-42, </pages> <publisher> IEEE, </publisher> <year> 1986. </year>
Reference-contexts: The goal of Swift is to support high data transfer rates in a distributed system by exploiting the inherent parallelism in these systems. The Swift architecture stripes files <ref> [29] </ref> across several networked workstations acting as servers. A file is logically partitioned into stripes of size N fi blocksize where N is the number of servers in the system and blocksize is the size of blocks that each server uses. <p> rotational latency of 8.3ms, and average seek time of 12ms. 2 UNIX is a registered trademark of UNIX Systems Laboratories Incorporated. 6 2.2 RAID Overview File striping is an idea that has been used successfully in the supercomputing arena for several years [13] and has been formally discussed since 1986 <ref> [29] </ref>. RAID (Redundant Arrays of Inexpensive Disks) [27, 26, 14, 9] is a hardware solution to the high data rate problem accomplishing high data rates by striping data across multiple disk devices on a common bus architecture.
Reference: [30] <author> D. Stodolsky, G. Gibson, and M. Holland, </author> <title> Parity logging: Overcoming the small write problem in redundant disk arrays, </title> <booktitle> in Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 64-75, </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1993. </year>
Reference-contexts: The writing of this one block requires two reads and two writes a ratio of four disk operations to one small write operation. As a result, small writes are expensive on both RAID-4 and RAID-5 systems, and is known as the small write problem <ref> [30, 9] </ref>. Parity logging [30] and floating parity [21] have been suggested as mechanisms to lessen the impact of small writes in redundant systems. <p> The writing of this one block requires two reads and two writes a ratio of four disk operations to one small write operation. As a result, small writes are expensive on both RAID-4 and RAID-5 systems, and is known as the small write problem [30, 9]. Parity logging <ref> [30] </ref> and floating parity [21] have been suggested as mechanisms to lessen the impact of small writes in redundant systems. The Zebra striped file system [10, 24] uses an approach that stripes a log-structured file system [28] across disks, with parity information, and provides high performance for small writes [11].
Reference: [31] <author> M. Stonebraker and G. A. Schloss, </author> <title> Distributed RAID anew multiple copy algorithm, </title> <type> tech. rep., </type> <institution> Electronics Res. Lab., UCB., </institution> <year> 1989. </year>
Reference-contexts: Similar work has been presented by Stonebraker and Schloss in their design of RADD (Redundant Array of Distributed Disks) <ref> [32, 31] </ref> based on a RAID level 5 scheme, and by Ousterhout and Hartman for the Zebra [24, 10, 11] striped file system.
Reference: [32] <author> M. Stonebraker and G. A. Schloss, </author> <title> Distributed RAIDs anew multiple copy algorithm, </title> <booktitle> in Proceedings of the 6th International Conference on Data Engineering, </booktitle> <pages> pp. 430-37, </pages> <publisher> IEEE Somputer Society, </publisher> <year> 1990. </year>
Reference-contexts: Similar work has been presented by Stonebraker and Schloss in their design of RADD (Redundant Array of Distributed Disks) <ref> [32, 31] </ref> based on a RAID level 5 scheme, and by Ousterhout and Hartman for the Zebra [24, 10, 11] striped file system.
Reference: [33] <author> C. L. Williamson and D. R. Cheriton, </author> <title> Loss-load curves: Support for rate-based congestion control in high-speed datagram networks, </title> <type> tech. rep., </type> <institution> Stanford University, </institution> <year> 1991. </year>
Reference-contexts: Note the erratic behavior when congestion starts evidenced by the wide error bars (showing minimum and maximum throughput values). effort was a basic form of rate control and has support in the literature <ref> [33, 16] </ref>. However, the implementation suffered from an inability to react quickly to periods of severe congestion and showed an erratic behavior (See Figure 3.1). This approach was not investigated further.
References-found: 33

