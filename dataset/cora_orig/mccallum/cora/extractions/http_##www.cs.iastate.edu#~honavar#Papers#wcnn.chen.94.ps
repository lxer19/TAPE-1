URL: http://www.cs.iastate.edu/~honavar/Papers/wcnn.chen.94.ps
Refering-URL: http://www.cs.iastate.edu/~honavar/publist.html
Root-URL: 
Title: Neural Network Automata  
Author: by Chun-Hsien Chen and Vasant Honavar 
Address: Ames, Iowa 50011  
Affiliation: Computer Science Department Iowa State University  
Abstract: Artificial neural networks (ANN), due to their inherent parallelism and potential fault tolerance offer an attractive paradigm for robust and efficient implementations of functional modules for symbol processing. This paper presents designs of such ANN modules for simulating deterministic finite automata (DFA) and deterministic pushdown automata (DDPA). The designs use an implementation of a class of partially recurrent ANN (modified Elman networks) constructed using a general-purpose binary mapping module (BMP) which in turn is synthesized from multi-layer perceptrons. The paper also discusses some relevant mathematical properties of multi-layer perceptrons that facilitate automated synthesis of BMP modules and points out several potential appli cations of ANN implementations of DFA and DDPA. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. B. Baum, </author> <title> On the Capabilities of Multilayer Perceptron, </title> <journal> Journal of Complexity, </journal> <volume> vol. 4, </volume> <pages> pp. 193-215, </pages> <year> 1988. </year>
Reference-contexts: threshold of output neuron i, x j is input value at input neuron j, and f h is hardlimiter function. f h (x) = 1 if x &gt; 0 0 otherwise It is well known that such a 1-layer perceptron can implement only linearly separable functions from R n to <ref> [0; 1] </ref> m [Minsky & Papert, 1969]. A 2-layer Perceptron has one hidden layer of k hidden neurons (and hence 2 layers of weights with each hidden neuron being connected to every input neuron as well as every output neuron).
Reference: [2] <author> C. Chen and V. Honavar, </author> <title> Modularized Neural Networks for Lexical Analyzer and Grammar Parser. </title> <note> To appear. </note>
Reference-contexts: 1. Introduction Artificial neural networks (ANN), due to their inherent parallelism and fault tolerance offer an attractive paradigm for fast and robust implementations of functional modules for a variety of applications. Of particular interest are ANN modules for symbol processing <ref> [2] </ref>, [12], [13] & [14]. This paper focuses on the efficient designs for neural network automata for regular and context-free language recognition. Such modules have a variety of practical applications in computer science, linguistics, systems modeling and control, artificial intelligence, and structural pattern recognition. <p> be used in a variety of tasks that require the integration of symbol processing capabilities into neural networks (for examples of such applications, see Shastri [12], Smolensky [13], Sun [14].) Other applications of NN automata include efficient hardware implementations of lexical analyzers and parsers used in compilers (Chen & Honavar <ref> [2] </ref>) and natural language processing, and language recognizers used in syntactic pattern recognition.
Reference: [3] <author> J. L. Elman, </author> <title> Finding Structure in Time, </title> <type> Technical report, </type> <institution> Center for Research in Language (CRL) UCSD, </institution> <month> April </month> <year> 1988. </year>
Reference: [4] <author> S. Grossberg, </author> <title> Nonlinear neural networks: </title> <booktitle> Principles, mechanisms, and architectures, Neural Networks, </booktitle> <volume> vol. 1, </volume> <pages> pp. 17-61, </pages> <year> 1988. </year>
Reference: [5] <author> J. Hao, S. Tan and J. Vandewalle, </author> <title> A Geometric Approach to the Structural Synthesis of Multilayer Perceptron Neural Networks, </title> <booktitle> Proceeding of 1990 International Joint Conference on Neural Networks, </booktitle> <volume> vol. 2, </volume> <pages> pp. 881-885. </pages>
Reference: [6] <author> J. E. Hopcroft and J. D. Ullman, </author> <title> Introduction to Automata Theory, Languages, and Computation, </title> <publisher> Addison-Wesley, </publisher> <year> 1979. </year>
Reference: [7] <author> S. C. Huang and Y. F. Huang, </author> <title> Bounds on the Number of Hidden Neurons in Multilayer Perceptrons, </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 2, no. 1, pp.47-55, </volume> <month> January </month> <year> 1991. </year>
Reference: [8] <author> M. Minsky and S. Papert, </author> <title> Perceptrons: An Introduction to Computational Geometry, </title> <publisher> MIT Press, </publisher> <year> 1969. </year>
Reference: [9] <author> T. M. Kwon, </author> <title> A Guaranteed Training of Binary Pattern Mappings Using Gaussian Perceptron Networks, </title> <booktitle> IEEE International Joint Conference on Neural Networks, </booktitle> <volume> vol. 3, </volume> <pages> pp. 614-619, </pages> <address> Baltimore, </address> <year> 1992. </year>
Reference: [10] <author> U. Ramacher and M. </author> <title> Wesseling, A Geometrical Approach to Neural Network Design, </title> <booktitle> Proceeding of International Joint Conference on Neural Networks, </booktitle> <volume> vol. 2, </volume> <pages> pp. 147-153, </pages> <year> 1989. </year>
Reference: [11] <author> P. Rujan, </author> <title> A Geometric Approach to Learning in Neural Networks, </title> <booktitle> Proceeding of International Joint Conference on Neural Networks, </booktitle> <volume> vol. 2, </volume> <pages> pp. 105-109, </pages> <year> 1989. </year>
Reference: [12] <author> L. Shastri, </author> <title> A Connectionist Approach to Knowledge Representation and Limited Inference, </title> <journal> Cognitive Science, </journal> <volume> 12, </volume> <pages> pp. 331-392, </pages> <year> 1988. </year>
Reference-contexts: 1. Introduction Artificial neural networks (ANN), due to their inherent parallelism and fault tolerance offer an attractive paradigm for fast and robust implementations of functional modules for a variety of applications. Of particular interest are ANN modules for symbol processing [2], <ref> [12] </ref>, [13] & [14]. This paper focuses on the efficient designs for neural network automata for regular and context-free language recognition. Such modules have a variety of practical applications in computer science, linguistics, systems modeling and control, artificial intelligence, and structural pattern recognition. <p> We also present efficient NN architectures for simulation of finite state and pushdown automata. Such designs can be used in a variety of tasks that require the integration of symbol processing capabilities into neural networks (for examples of such applications, see Shastri <ref> [12] </ref>, Smolensky [13], Sun [14].) Other applications of NN automata include efficient hardware implementations of lexical analyzers and parsers used in compilers (Chen & Honavar [2]) and natural language processing, and language recognizers used in syntactic pattern recognition.
Reference: [13] <author> P. Smolensky, </author> <title> On Variable Binding and Representation of Symbolic Structure, </title> <type> Tech Report, </type> <institution> University of Colorado, Boulder, CO, </institution> <year> 1987. </year>
Reference-contexts: 1. Introduction Artificial neural networks (ANN), due to their inherent parallelism and fault tolerance offer an attractive paradigm for fast and robust implementations of functional modules for a variety of applications. Of particular interest are ANN modules for symbol processing [2], [12], <ref> [13] </ref> & [14]. This paper focuses on the efficient designs for neural network automata for regular and context-free language recognition. Such modules have a variety of practical applications in computer science, linguistics, systems modeling and control, artificial intelligence, and structural pattern recognition. <p> We also present efficient NN architectures for simulation of finite state and pushdown automata. Such designs can be used in a variety of tasks that require the integration of symbol processing capabilities into neural networks (for examples of such applications, see Shastri [12], Smolensky <ref> [13] </ref>, Sun [14].) Other applications of NN automata include efficient hardware implementations of lexical analyzers and parsers used in compilers (Chen & Honavar [2]) and natural language processing, and language recognizers used in syntactic pattern recognition.
Reference: [14] <author> R. Sun, </author> <title> Logics and Variables in Connectionist Models: A Brief Overview, Symbolic Processors and Connectionist Networks for Artificial Intelligence and Cognitive Modeling, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: 1. Introduction Artificial neural networks (ANN), due to their inherent parallelism and fault tolerance offer an attractive paradigm for fast and robust implementations of functional modules for a variety of applications. Of particular interest are ANN modules for symbol processing [2], [12], [13] & <ref> [14] </ref>. This paper focuses on the efficient designs for neural network automata for regular and context-free language recognition. Such modules have a variety of practical applications in computer science, linguistics, systems modeling and control, artificial intelligence, and structural pattern recognition. <p> We also present efficient NN architectures for simulation of finite state and pushdown automata. Such designs can be used in a variety of tasks that require the integration of symbol processing capabilities into neural networks (for examples of such applications, see Shastri [12], Smolensky [13], Sun <ref> [14] </ref>.) Other applications of NN automata include efficient hardware implementations of lexical analyzers and parsers used in compilers (Chen & Honavar [2]) and natural language processing, and language recognizers used in syntactic pattern recognition.
Reference: [15] <author> S. Tan and J. Vandewalle, </author> <title> Efficient Algorithm for The Design of Multilayer Feedforward Neural Networks, </title> <booktitle> IEEE International Joint Conference on Neural Networks, </booktitle> <volume> vol. 2, </volume> <pages> pp. 190-195, </pages> <address> Baltimore, </address> <year> 1992. </year>
References-found: 15

