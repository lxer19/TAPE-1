URL: http://www.cs.ucsb.edu/~kshen/lu2d_final.ps
Refering-URL: http://www.cs.ucsb.edu/Research/rapid_sweb/RAPID.html
Root-URL: http://www.cs.ucsb.edu
Email: kshen@cs.ucsb.edu  jiao@cs.uiuc.edu  tyang@cs.ucsb.edu  
Title: Elimination Forest Guided 2D Sparse LU Factorization  
Author: Kai Shen Xiangmin Jiao Tao Yang 
Address: Santa Barbara, CA 93106  IL 61801  Santa Barbara, CA 93106  
Affiliation: Dept. of Computer Science University of California  Dept. of Computer Science University of Illinois Urbana-Champaign,  Dept. of Computer Science University of California  
Abstract: Sparse LU factorization with partial pivoting is important for many scientific applications and delivering high performance for this problem is difficult on distributed memory machines. Our previous work has developed an approach called S fl that incorporates static symbolic factorization, supernode partitioning and graph scheduling. This paper studies the properties of elimination forests and uses them to guide supernode partitioning/amalgamation and execution scheduling. The new design with 2D mapping effectively identifies dense structures without introducing too many zeros in the BLAS computation and exploits asynchronous parallelism with low buffer space cost. The implementation of this code, called S + , uses supernodal matrix multiplication which retains the BLAS-3 level efficiency and avoids unnecessary arithmetic operations. The experiments show that S + improves our previous code substantially and can achieve up to 11.04GFLOPS on 128 Cray T3E 450MHz nodes, which is the highest performance reported in the literature. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Ashcraft, R. Grimes, J. Lewis, B. Peyton, and H. Simon. </author> <title> Progress in Sparse Matrix Methods for Large Sparse Linear Systems on Vector Supercomputers. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 1 </volume> <pages> 10-30, </pages> <year> 1987. </year>
Reference-contexts: Our second goal is to incorporate 2D block-based mapping in our framework. In the literature 2D mapping has been shown more scalable than 1D for dense LU and sparse Cholesky <ref> [1, 20, 21] </ref>. However there are difficulties to apply the 2D block-oriented mapping to the case of sparse LU factorization even the static structure is predicted in advance. First, pivoting operations and row interchanges require frequent and well-synchronized inter-processor communication when each column is distributed to multiprocessors. <p> Thus in designing 2D codes, we paid special attention to the usage of buffer space so that 2D codes are able to factorize large matrices under memory constraints. 3 Elimination forests and nonsymmetric supernode partitioning In this section, we extend the previous work on elimination forests <ref> [1, 13] </ref> and identify the properties of elimination forests to design more robust strategies for supernode partitioning and detect when pivoting for different columns can be conducted concurrently. <p> Our design for LU task scheduling using the above forest concept is different from the ones for Cholesky <ref> [1, 19] </ref> because pivoting and row interchanges complicate the flow control in LU. Using Theorem 4, we are able to exploit some parallelism among F actor () tasks.
Reference: [2] <author> J. Demmel. </author> <title> Numerical Linear Algebra on Parallel Processors. </title> <booktitle> Lecture Notes for NSF-CBMS Regional Conference in the Mathematical Sciences, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Solution of sparse linear systems is a computational bottleneck in many problems. If a matrix is symmetric and positive definite, Cholesky factorization can be used, for which fast parallel algorithms have been developed [15, 19, 20]. When pivoting is required to maintain numerical stability for non-symmetric linear systems <ref> [2, 14] </ref>, it is very hard to produce high performance for this problem because partial pivoting operations dynamically change computation and communication patterns during the elimination process, and cause severe caching miss and load imbalance on modern computers with memory hierarchies. <p> The pivoting sequence is held until the factorization of the k th column block is completed. Then the pivoting sequence is applied to the rest of the matrix. This is called "delayed pivoting" <ref> [2] </ref>. 2) Task U pdate (k; j) uses column block k (A k;k ; A k+1;k ; ; A N;k ) to modify column block j.
Reference: [3] <author> J. Demmel, S. Eisenstat, J. Gilbert, X. Li, and J. </author> <note> Liu. </note>
Reference-contexts: The previous work has addressed parallelization using shared memory platforms or restricted pivoting <ref> [3, 12, 13, 16] </ref>. Most notably, the recent shared memory implementation of SuperLU [3, 4, 18] has achieved up to 2.58GFLOPS on 8 Cray C90 nodes. <p> The previous work has addressed parallelization using shared memory platforms or restricted pivoting [3, 12, 13, 16]. Most notably, the recent shared memory implementation of SuperLU <ref> [3, 4, 18] </ref> has achieved up to 2.58GFLOPS on 8 Cray C90 nodes. <p> For example, it costs less than one second for most of our tested matrices, at worst it costs 2 seconds on a single node of Cray T3E, and the memory requirement is relatively small. The dynamic factorization, which is used in the sequential and share-memory versions of SuperLU <ref> [3, 18] </ref>, provides more accurate data structure prediction on the fly, but it is challenging to parallelize SuperLU with low runtime control overhead on distributed memory machines.
References-found: 3

