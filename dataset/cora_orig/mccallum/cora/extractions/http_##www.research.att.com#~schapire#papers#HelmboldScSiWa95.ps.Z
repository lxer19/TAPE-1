URL: http://www.research.att.com/~schapire/papers/HelmboldScSiWa95.ps.Z
Refering-URL: http://www.research.att.com/~schapire/publist.html
Root-URL: 
Email: dph@cse.ucsc.edu  schapire@research.att.com  singer@research.att.com  manfred@cse.ucsc.edu  
Title: Machine Learning, 27(1):97-119, 1997. A Comparison of New and Old Algorithms for A Mixture Estimation Problem  
Author: DAVID P. HELMBOLD ROBERT E. SCHAPIRE YORAM SINGER MANFRED K. WARMUTH 
Address: Santa Cruz, CA 95064  600 Mountain Avenue, Murray Hill, NJ 07974  600 Mountain Avenue, Murray Hill, NJ 07974  Santa Cruz, CA 95064  
Affiliation: Computer and Information Sciences, University of California,  AT&T Labs,  AT&T Labs,  Computer and Information Sciences, University of California,  
Abstract: We investigate the problem of estimating the proportion vector which maximizes the likelihood of a given sample for a mixture of given densities. We adapt a framework developed for supervised learning and give simple derivations for many of the standard iterative algorithms like gradient projection and EM. In this framework, the distance between the new and old proportion vectors is used as a penalty term. The square distance leads to the gradient projection update, and the relative entropy to a new update which we call the exponentiated gradient update (EG ). Curiously, when a second order Taylor expansion of the relative entropy is used, we arrive at an update EM which, for = 1, gives the usual EM update. Experimentally, both the EM -update and the EG -update for &gt; 1 outperform the EM algorithm and its variants. We also prove a polynomial bound on the rate of convergence of the EG algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> N. Abe, J. Takeuchi, and M. K. Warmuth. </author> <title> Polynomial learnability of probablistic con-cepts with respect to the Kullback-Leibler divergence. </title> <booktitle> In Proceedings of the Fourth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 277-289. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: Similar observations were made in the supervised setting [8], [9]. We also show below how to obtain bounds when the entries in the matrix have zero-valued components. We essentially average the data matrix with a uniform matrix (this *-Bayesian averaging was also used in <ref> [1] </ref>) and then use the averaged matrix to run our algorithm. <p> Z t = i=1 P Y exp x p;i N X w t;i p=1 exp x p;i 1=P Since x t;i 2 <ref> [0; 1] </ref> and since fi x 1 (1 fi)x for fi &gt; 0 and x 2 [0; 1] we can upper bound the right-hand side by: N X w t;i p=1 1 1 exp 1=P N X P Y w t x p w t;i x p;i We will need the <p> Z t = i=1 P Y exp x p;i N X w t;i p=1 exp x p;i 1=P Since x t;i 2 <ref> [0; 1] </ref> and since fi x 1 (1 fi)x for fi &gt; 0 and x 2 [0; 1] we can upper bound the right-hand side by: N X w t;i p=1 1 1 exp 1=P N X P Y w t x p w t;i x p;i We will need the following fact: For non-negative numbers A i;p , 12 D.P. HELMBOLD, R.E. SCHAPIRE, Y. <p> w t;i 1 exp 1=P yields an upper bound on Z t of P Y N X w t x p w t;i x p;i (11) P Y w t x p : To further bound ln Z t , we apply the following: Lemma 1 For all ff 2 <ref> [0; 1] </ref> and x 2 R, ln (1 ff (1 e x )) ffx + x 2 =8 : Proof: Fix ff 2 [0; 1], and let f (x) = ffx + x 2 =8 ln (1 ff (1 e x )) : We wish to show that f (x) 0. <p> x p;i (11) P Y w t x p : To further bound ln Z t , we apply the following: Lemma 1 For all ff 2 <ref> [0; 1] </ref> and x 2 R, ln (1 ff (1 e x )) ffx + x 2 =8 : Proof: Fix ff 2 [0; 1], and let f (x) = ffx + x 2 =8 ln (1 ff (1 e x )) : We wish to show that f (x) 0. <p> When some of the components x p;i are zero, or very close to zero, we can use the following algorithm which is parameterized by a real number ff 2 <ref> [0; 1] </ref>. Let ~ x p = (1 ff=N )x p + (ff=N )1 where 1 is the all 1's vector. <p> Since x p;i 2 <ref> [r; 1] </ref>, and assuming that w t;i 0, it follows that this is bounded by P p=1 w t x p + 2r 2 : Thus, summing over all t T , we get 1 ku w T+1 k 2 2 T X P X ln u x p 2 N <p> This algorithm's performance was analyzed in the PAC model in <ref> [1] </ref>. 5. In one form, Holder's inequality states that, for non-negative a i , b i , X a i b i X a p ! 1=p i i for any positive p; q satisfying 1=p + 1=q = 1. 6.
Reference: 2. <author> J. Bridle. </author> <title> Probabilistic interpretation of feedforward classification network outputs with relationships to statistical pattern recognition. </title> <editor> In F. Fogelman-Souli and J. Herault, editors, Neuro-Computing: </editor> <booktitle> Algorithms, Architectures, and Applications. </booktitle> <address> New York: </address> <publisher> Springer Verlag, </publisher> <year> 1989. </year>
Reference-contexts: The contour lines for d RE are deformed ellipses that bend towards the vertices of the triangular feasible region. One can also get an update by re-parameterizing the probability vectors and doing unconstrained gradient ascent in the new parameter space. We use the standard exponential parameterization <ref> [2] </ref>: w i = e r i = P N j=1 e r j and maximize the function A COMPARISON OF NEW AND OLD ALGORITHMS 7 d RE (w t+1 jjw t ) (second row) and d 2 (w t+1 jjw t ) (third row) as a function of w t+1
Reference: 3. <author> T. </author> <title> Cover. Universal portfolios. </title> <journal> Mathematical Finance, </journal> <volume> 1(1) </volume> <pages> 1-29, </pages> <year> 1991. </year>
Reference-contexts: In particular, we have derived an on-line version of EM . Experimentally, this version outperforms the known on-line versions of EM which is the EM algorithm with = 1 We have also applied the on-line versions of our algorithms to a portfolio selection problem [7] investigated by Cover <ref> [3] </ref>. Although Cover's analytical bounds appear better than ours, experimental results indicate that EM and EG outperform Cover's algorithm on historical stock market data. Furthermore, our algorithms are computationally efficient while Cover's algorithm is exponential in the number of possible investments.
Reference: 4. <author> A. P. Dempster, N. M. Laird, and D. B. Rubin. </author> <title> Maximum-likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> B39:1-38, </volume> <year> 1977. </year>
Reference-contexts: Most of the common techniques to solve this problem are based on either gradient ascent iterative schemes [11] or on the Expectation Maximization (EM) algorithm for parameter estimation from incomplete data <ref> [4] </ref>, [16]. We derive the standard iterative algorithms for the unsupervised mixture proportions estimation problem by placing them in a common hill-climbing framework. This framework is analogous to the one developed by Kivinen and Warmuth [8] 2 D.P. HELMBOLD, R.E. SCHAPIRE, Y. SINGER, M.K. WARMUTH for supervised on-line learning. <p> At first, a high learning rate is used to quickly approach the ML solution. Later iterations use a lower learning rate to aid convergence. The EM algorithm is in fact a limiting case of a more general approach usually called Generalized EM (GEM) <ref> [4] </ref>, [12]. Neal and Hinton [13] considered another extension of EM which involves examining only a portion of the observation matrix X on each iteration.
Reference: 5. <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference-contexts: 1. Introduction The problem of maximum-likelihood (ML) estimation of a mixture of densities is an important and well known learning problem <ref> [5] </ref>. ML estimators are asymptotically unbiased and are a basic tool for other more complicated problems such as clustering and learning hidden Markov models. We investigate the ML-estimation problem when the densities are given and only the mixture proportions are unknown.
Reference: 6. <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns-Hopkins University Press, </publisher> <year> 1989. </year>
Reference-contexts: The conjugate gradient search is a method for iteratively searching a quadratic cost function [11], <ref> [6] </ref>. When the cost function is non-quadratic, as is the likelihood function in our case, a variant of the conjugate gradient method can be devised.
Reference: 7. <author> D. Helmbold, R. E. Schapire, Y. Singer, and M. K. Warmuth. </author> <title> On-line portfolio selection using multiplicative updates. </title> <booktitle> In Proc. 13th International Conference on Machine Learning, </booktitle> <pages> pages 243-251. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, </address> <year> 1996. </year>
Reference-contexts: In particular, we have derived an on-line version of EM . Experimentally, this version outperforms the known on-line versions of EM which is the EM algorithm with = 1 We have also applied the on-line versions of our algorithms to a portfolio selection problem <ref> [7] </ref> investigated by Cover [3]. Although Cover's analytical bounds appear better than ours, experimental results indicate that EM and EG outperform Cover's algorithm on historical stock market data. Furthermore, our algorithms are computationally efficient while Cover's algorithm is exponential in the number of possible investments.
Reference: 8. <author> J. Kivinen and M. K. Warmuth. </author> <title> Additive versus exponentiated gradient updates. </title> <booktitle> In Proceedings of the Twenty-Seventh Annual ACM Symposium on Theory of Computing, </booktitle> <year> 1995. </year>
Reference-contexts: We derive the standard iterative algorithms for the unsupervised mixture proportions estimation problem by placing them in a common hill-climbing framework. This framework is analogous to the one developed by Kivinen and Warmuth <ref> [8] </ref> 2 D.P. HELMBOLD, R.E. SCHAPIRE, Y. SINGER, M.K. WARMUTH for supervised on-line learning. Our goal is to maximize the log likelihood of the observations as a function of the mixture vector w, denoted by LogLike (w). This is computationally hard and requires iterative methods. <p> The Updates Kivinen and Warmuth <ref> [8] </ref> studied a general framework for on-line learning in which they derived algorithms for a broad class of loss functions. Here, we apply their method specifically to negative log-likelihood. <p> More precisely, we use the same distance function that motivates the update as a potential function to obtain worst-case cumulative loss bounds over sequences of updates (similar to the methods applied to the supervised case <ref> [8] </ref>). The natural loss of a mixture vector w t for our problem is LogLike (w t ). Note that this loss is unbounded since the likelihood for w t is zero when there is some x p for which w t x p = 0. <p> In the supervised case, one can obtain firm worst-case loss bounds with respect to the square loss for various updates by analyzing the progress <ref> [8] </ref>. But the square loss is bounded and it is not surprising that it is much harder to obtain strong loss bounds for our (unbounded loss) unsupervised setting. <p> HELMBOLD, R.E. SCHAPIRE, Y. SINGER, M.K. WARMUTH versus the square-root growth of the latter family. Similar observations were made in the supervised setting <ref> [8] </ref>, [9]. We also show below how to obtain bounds when the entries in the matrix have zero-valued components. We essentially average the data matrix with a uniform matrix (this *-Bayesian averaging was also used in [1]) and then use the averaged matrix to run our algorithm. <p> One of the main observation in the experiments is the following: EG and EM clearly outperform GP when the solution is sparse (see Figure 5). This is consistent with other settings <ref> [8] </ref>, [9], [10], where updates derived using the relative entropy distance outperform gradient-descent-type updates when the solution is "sparse". 20 D.P. HELMBOLD, R.E. SCHAPIRE, Y. SINGER, M.K. WARMUTH EM algorithm. The EM -update clearly outperforms the standard EM algorithm, even when a fixed conservative scheduling is used. <p> In the on-line setting each iteration typically uses only a single observation. It is therefore desirable to preserve information about the previous observations while improving the likelihood of the current observation. 2. A similar update for the case of linear regression was first given by Kivinen and Warmuth <ref> [8] </ref>. 3. i=1 w t;i rL (w t ) i = i=1 P p=1 x p w t = 1 P P w t x p 4. This algorithm's performance was analyzed in the PAC model in [1]. 5.
Reference: 9. <author> J. Kivinen and M. K. Warmuth. </author> <title> The perceptron algorithm vs. winnow: linear vs. logarithmic mistake bounds when few input variables are relevant. </title> <booktitle> In Proceedings of the Eighth Annual Workshop on Computational Learning Theory, </booktitle> <month> July </month> <year> 1995. </year> <note> 24 D.P. </note> <author> HELMBOLD, R.E. SCHAPIRE, Y. SINGER, </author> <title> M.K. </title> <type> WARMUTH </type>
Reference-contexts: HELMBOLD, R.E. SCHAPIRE, Y. SINGER, M.K. WARMUTH versus the square-root growth of the latter family. Similar observations were made in the supervised setting [8], <ref> [9] </ref>. We also show below how to obtain bounds when the entries in the matrix have zero-valued components. We essentially average the data matrix with a uniform matrix (this *-Bayesian averaging was also used in [1]) and then use the averaged matrix to run our algorithm. <p> One of the main observation in the experiments is the following: EG and EM clearly outperform GP when the solution is sparse (see Figure 5). This is consistent with other settings [8], <ref> [9] </ref>, [10], where updates derived using the relative entropy distance outperform gradient-descent-type updates when the solution is "sparse". 20 D.P. HELMBOLD, R.E. SCHAPIRE, Y. SINGER, M.K. WARMUTH EM algorithm. The EM -update clearly outperforms the standard EM algorithm, even when a fixed conservative scheduling is used.
Reference: 10. <author> N. Littlestone. </author> <title> Learning when irrelevant attributes abound: A new linear-threshold algo-rithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference-contexts: One of the main observation in the experiments is the following: EG and EM clearly outperform GP when the solution is sparse (see Figure 5). This is consistent with other settings [8], [9], <ref> [10] </ref>, where updates derived using the relative entropy distance outperform gradient-descent-type updates when the solution is "sparse". 20 D.P. HELMBOLD, R.E. SCHAPIRE, Y. SINGER, M.K. WARMUTH EM algorithm. The EM -update clearly outperforms the standard EM algorithm, even when a fixed conservative scheduling is used.
Reference: 11. <author> D. G. Luenberger. </author> <title> Linear and Nonlinear Programming. </title> <publisher> Addison-Wesley, </publisher> <year> 1984. </year>
Reference-contexts: Most of the common techniques to solve this problem are based on either gradient ascent iterative schemes <ref> [11] </ref> or on the Expectation Maximization (EM) algorithm for parameter estimation from incomplete data [4], [16]. We derive the standard iterative algorithms for the unsupervised mixture proportions estimation problem by placing them in a common hill-climbing framework. <p> In particular, the standard EM algorithm (using = 1) has the property that the non-negativity constraints are always preserved. 4. Convergence and Progress In this section we discuss the convergence properties of the algorithms. Using standard methods (with the usual assumptions for convergence proofs) as in Lu- enberger <ref> [11] </ref>, it can be shown that all updates described in the previous section converge locally to an optimal ML solution, provided that the current mixture vector w t is close to the ML solution and given the usual assumptions. <p> The conjugate gradient search is a method for iteratively searching a quadratic cost function <ref> [11] </ref>, [6]. When the cost function is non-quadratic, as is the likelihood function in our case, a variant of the conjugate gradient method can be devised.
Reference: 12. <author> X.L. Meng and D.B. Rubin. </author> <title> Recent extensions of the EM algorithm (with discussion). </title> <editor> In J.M. Bernardo, J.O. Berger, A.P. Dawid, and A.F.M. Smith, editors, </editor> <booktitle> Bayesian Statistics, 4. </booktitle> <address> Oxfod: </address> <publisher> Clarendon Press, </publisher> <year> 1992. </year>
Reference-contexts: At first, a high learning rate is used to quickly approach the ML solution. Later iterations use a lower learning rate to aid convergence. The EM algorithm is in fact a limiting case of a more general approach usually called Generalized EM (GEM) [4], <ref> [12] </ref>. Neal and Hinton [13] considered another extension of EM which involves examining only a portion of the observation matrix X on each iteration.
Reference: 13. <author> R. M. Neal and G. E. Hinton. </author> <title> A new view of the EM algorithm that justifies incremental and other variants. </title> <type> Unpublished manuscript, </type> <year> 1993. </year>
Reference-contexts: A COMPARISON OF NEW AND OLD ALGORITHMS 3 The derivations of the learning rules using the above framework are simple and can readily be applied to other settings. They are similar to previous derivations found in the literature [16], <ref> [13] </ref>. 2. Definitions and Problem Statement Let R represent the real numbers. We say a vector v = (v 1 ; :::; v N ) 2 R N is a probability vector if, 8i : v i 0 and P n i=1 v i = 1. <p> At first, a high learning rate is used to quickly approach the ML solution. Later iterations use a lower learning rate to aid convergence. The EM algorithm is in fact a limiting case of a more general approach usually called Generalized EM (GEM) [4], [12]. Neal and Hinton <ref> [13] </ref> considered another extension of EM which involves examining only a portion of the observation matrix X on each iteration.
Reference: 14. <author> B. C. Peters and H. F. Walker. </author> <title> An iterative procedure for obtaining maximum-likelihood estimates of the parameters for a mixture of normal distributions. </title> <journal> SIAM Journal of Applied Mathematics, </journal> <volume> 35 </volume> <pages> 362-378, </pages> <year> 1978. </year>
Reference-contexts: Our experimental evidence suggests that setting &gt; 1 results in a more effective update. These results agree with the infinitesimal analysis in the limit of n ! 1 based on a stochastic approximation approach <ref> [14] </ref>, [15], [16]. For the exponentiated gradient algorithm, we are able to prove rigorous polynomial bounds on the number of iterations needed to get an arbitrarily good ML- estimator. <p> The EM 1 update can be motivated by the likelihood equations, and the generalization to arbitrary was studied by Peters and Walker <ref> [14] </ref>, [15]. Since the 2 distance approximates the relative entropy it may not be surprising that the EM -update (7) also approximates the EG -update (6).
Reference: 15. <author> B. C. Peters and H. F. Walker. </author> <title> The numerical evaluation of the maximum-likelihood esti-mates of a subset of mixture proportions. </title> <journal> SIAM Journal of Applied Mathematics, </journal> <volume> 35 </volume> <pages> 447-452, </pages> <year> 1978. </year>
Reference-contexts: Our experimental evidence suggests that setting &gt; 1 results in a more effective update. These results agree with the infinitesimal analysis in the limit of n ! 1 based on a stochastic approximation approach [14], <ref> [15] </ref>, [16]. For the exponentiated gradient algorithm, we are able to prove rigorous polynomial bounds on the number of iterations needed to get an arbitrarily good ML- estimator. <p> The EM 1 update can be motivated by the likelihood equations, and the generalization to arbitrary was studied by Peters and Walker [14], <ref> [15] </ref>. Since the 2 distance approximates the relative entropy it may not be surprising that the EM -update (7) also approximates the EG -update (6). <p> Moreover, using techniques similar to those in <ref> [15] </ref>, [16], it can be shown that it is better to use a learning rate &gt; 1 rather than the rate = 1. This implies that the EM algorithm is not optimal for this family of update rules.
Reference: 16. <author> R. A. Redner and H. F. Walker. </author> <title> Mixture densities, maximum likelihood, and the EM algo-rithm. </title> <journal> Siam Review, </journal> <volume> 26 </volume> <pages> 195-239, </pages> <year> 1984. </year>
Reference-contexts: Most of the common techniques to solve this problem are based on either gradient ascent iterative schemes [11] or on the Expectation Maximization (EM) algorithm for parameter estimation from incomplete data [4], <ref> [16] </ref>. We derive the standard iterative algorithms for the unsupervised mixture proportions estimation problem by placing them in a common hill-climbing framework. This framework is analogous to the one developed by Kivinen and Warmuth [8] 2 D.P. HELMBOLD, R.E. SCHAPIRE, Y. SINGER, M.K. WARMUTH for supervised on-line learning. <p> Our experimental evidence suggests that setting &gt; 1 results in a more effective update. These results agree with the infinitesimal analysis in the limit of n ! 1 based on a stochastic approximation approach [14], [15], <ref> [16] </ref>. For the exponentiated gradient algorithm, we are able to prove rigorous polynomial bounds on the number of iterations needed to get an arbitrarily good ML- estimator. <p> A COMPARISON OF NEW AND OLD ALGORITHMS 3 The derivations of the learning rules using the above framework are simple and can readily be applied to other settings. They are similar to previous derivations found in the literature <ref> [16] </ref>, [13]. 2. Definitions and Problem Statement Let R represent the real numbers. We say a vector v = (v 1 ; :::; v N ) 2 R N is a probability vector if, 8i : v i 0 and P n i=1 v i = 1. <p> Moreover, using techniques similar to those in [15], <ref> [16] </ref>, it can be shown that it is better to use a learning rate &gt; 1 rather than the rate = 1. This implies that the EM algorithm is not optimal for this family of update rules.
Reference: 17. <author> Y. Singer and M. K. Warmuth. </author> <title> Training algorithms for hidden markov models using entropy based distance functions. </title> <booktitle> To appear in Advances in Neural Information Processing Systems, </booktitle> <volume> 8, </volume> <year> 1996. </year>
Reference-contexts: We have already applied our methodology for deriving updates to more complicated mixture estimation problems such as training hidden Markov models <ref> [17] </ref> and we are currently applying this methodology to mixtures of Gaussians with arbitrary mean and variance. In this more complicated setting we need distance functions that depend on the means and variances given to the Gaussians as well as the mixture probabilities assigned to them.
References-found: 17

