URL: http://www.cis.ohio-state.edu/~rjmiller/Teaching/788dir/papers/birch-journal.ps
Refering-URL: http://www.cis.ohio-state.edu/~rjmiller/Teaching/788dir/projects.html
Root-URL: 
Title: Data Clustering System BIRCH and Its Applications  
Author: TIAN ZHANG, RAGHU RAMAKRISHNAN, MIRON LIVNY 
Keyword: Very Large Databases, Data Clustering, Incremental Algorithm, Data Classification  
Address: Wisconsin, Madison, WI 53706,U.S.A.  
Affiliation: Computer Sciences Department, University of  
Note: Small Journal Name, 1-39 c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  and Compression  
Email: zhang,raghu,miron@cs.wisc.edu  Editor:  
Phone: Phone: 608-262-1204, Fax: 608-262-9777  
Abstract: Finding useful patterns in large datasets has attracted considerable interest recently, and one of the most widely studied problems in this area is the identification of clusters, or densely populated regions, in a multi-dimensional dataset. Prior work does not adequately address the problem of large datasets and minimization of I/O costs. This paper presents a data clustering method named BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies), and demonstrates that it is especially suitable for very large databases. BIRCH incrementally and dynamically clusters incoming multi-dimensional metric data points to try to produce the best quality clustering with the available resources (i.e., available memory and time constraints). BIRCH can typically find a good clustering with a single scan of the data, and improve the quality further with a few additional scans. BIRCH is also the first clustering algorithm proposed in the database area to handle "noise" (data points that are not part of the underlying pattern) effectively. We evaluate BIRCH's time/space efficiency, data input order sensitivity, and clustering quality through several experiments. We also present a performance comparison of BIRCH versus the well known KMEANS method [11], and CLARANS, a clustering method proposed recently for large datasets [18], and show that BIRCH is consistently superior. We also present two promising applications of BIRCH to real problems. In the first, we integrate BIRCH with the data visualization environment DEVISE [2], to build an interactive data classification tool that helps users to classify pixels in images taken via 2-band techniques. In the second, we use BIRCH to efficiently generate the codebook for the vector quantizer [9] in image compression. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Peter Cheeseman, James Kelly, Matthew Self, et al., </author> <title> AutoClass : A Bayesian Classification System, </title> <booktitle> Proc. of the 5th Int'l Conf. on Machine Learning, </booktitle> <publisher> Morgan Kaufman, </publisher> <month> Jun. </month> <year> 1988. </year>
Reference-contexts: Section 7 presents two applications of BIRCH to real life problems. Finally our conclusions and directions for future research are presented in Section 8. 2. Summary of Relevant Research Data clustering has been studied in the Statistics [3, 4, 16, 17], Machine Learning <ref> [1, 7, 8, 15] </ref> and Database [18, 5, 6] communities with different methods and different emphases. <p> Probability-based approaches: They typically <ref> [7, 1] </ref> make the assumption that probability distributions on separate attributes are statistically independent of each other. In reality, this is far from true. Correlation between attributes exists, and sometimes this kind of correlation is exactly what we are looking for.
Reference: 2. <author> Michael Cheng, Miron Livny, and Raghu Ramakrishnan, </author> <title> Visual Analysis of Stream Data, </title> <booktitle> Proc. of IS&T/SPIE Conf. on Visual Data Exploration and Analysis, </booktitle> <address> San Jose, CA, </address> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: In conclusion, for the base workload, BIRCH uses much less memory, but is faster, more accurate, and less order-sensitive compared with CLARANS and KMEANS. 7. BIRCH Applications 7.1. Interactive and Iterative Classification Tool BIRCH and DEVISE (A Data Exploration via VISualization Environment) <ref> [2] </ref> are integrated as shown in Figure 23 to form a user-friendly, interactive and iterative data classification tool.
Reference: 3. <author> Richard Duda, and Peter E. Hart, </author> <title> Pattern Classification and Scene Analysis, </title> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference-contexts: Section 7 presents two applications of BIRCH to real life problems. Finally our conclusions and directions for future research are presented in Section 8. 2. Summary of Relevant Research Data clustering has been studied in the Statistics <ref> [3, 4, 16, 17] </ref>, Machine Learning [1, 7, 8, 15] and Database [18, 5, 6] communities with different methods and different emphases. <p> It guarantees a local minimum, but the quality of the local minimum is very sensitive to the initial partition, and the worst case time complexity is still exponential. Hierarchical Clustering (HC) <ref> [3, 17] </ref> does not try to find "best" clusters, but keeps merging the closest pair (or splitting the farthest pair) of objects to form clusters. With a reasonable distance measurement, the best time complexity of a practical HC algorithm is O (N 2 ).
Reference: 4. <author> R. Dubes, and A.K. Jain, </author> <title> Clustering Methodologies in Exploratory Data Analysis, Advances in Computers, Edited by M.C. </title> <journal> Yovits, </journal> <volume> Vol. 19, </volume> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1980. </year>
Reference-contexts: Data clustering identifies the sparse and the crowded places, and hence discovers the overall distribution patterns of the dataset. Be sides, the derived clusters can be visualized more efficiently and effectively than the original dataset <ref> [16, 4] </ref>. <p> Section 7 presents two applications of BIRCH to real life problems. Finally our conclusions and directions for future research are presented in Section 8. 2. Summary of Relevant Research Data clustering has been studied in the Statistics <ref> [3, 4, 16, 17] </ref>, Machine Learning [1, 7, 8, 15] and Database [18, 5, 6] communities with different methods and different emphases.
Reference: 5. <author> Martin Ester, Hans-Peter Kriegel, and Xiaowei Xu, </author> <title> A Database Interface for Clustering in Large Spatial Databases, </title> <booktitle> Proc. of 1st Int'l Conf. on Knowledge Discovery and Data Mining, </booktitle> <year> 1995. </year>
Reference-contexts: Section 7 presents two applications of BIRCH to real life problems. Finally our conclusions and directions for future research are presented in Section 8. 2. Summary of Relevant Research Data clustering has been studied in the Statistics [3, 4, 16, 17], Machine Learning [1, 7, 8, 15] and Database <ref> [18, 5, 6] </ref> communities with different methods and different emphases. Previous approaches, probability-based (like most approaches in Machine Learning) or distance-based (like most work in Statistics) , do not adequately consider the case that the dataset can be too large to fit in main memory. <p> CLARANS suffers from the same drawbacks as the above PC methods wrt. efficiency. In addition, it may not find a real local minimum due to the random searching trimming controlled by maxneighbor. Later <ref> [5] </ref> and [6] propose focusing techniques (based on R fl -trees) to improve CLARANS's ability to deal with data objects that may reside on disks by (1) clustering a sample of the dataset that is drawn from each R fl -tree data page; and (2) focusing on relevant data points for
Reference: 6. <author> Martin Ester, Hans-Peter Kriegel, and Xiaowei Xu, </author> <title> Knowledge Discovery in Large Spatial Databases: Focusing Techniques for Efficient Class Identification, </title> <booktitle> Proc. of 4th Int'l Symposium on Large Spatial Databases, </booktitle> <address> Portland, Maine, U.S.A., </address> <year> 1995. </year>
Reference-contexts: Section 7 presents two applications of BIRCH to real life problems. Finally our conclusions and directions for future research are presented in Section 8. 2. Summary of Relevant Research Data clustering has been studied in the Statistics [3, 4, 16, 17], Machine Learning [1, 7, 8, 15] and Database <ref> [18, 5, 6] </ref> communities with different methods and different emphases. Previous approaches, probability-based (like most approaches in Machine Learning) or distance-based (like most work in Statistics) , do not adequately consider the case that the dataset can be too large to fit in main memory. <p> CLARANS suffers from the same drawbacks as the above PC methods wrt. efficiency. In addition, it may not find a real local minimum due to the random searching trimming controlled by maxneighbor. Later [5] and <ref> [6] </ref> propose focusing techniques (based on R fl -trees) to improve CLARANS's ability to deal with data objects that may reside on disks by (1) clustering a sample of the dataset that is drawn from each R fl -tree data page; and (2) focusing on relevant data points for distance and
Reference: 7. <author> Douglas H. Fisher, </author> <title> Knowledge Acquisition via Incremental Conceptual Clustering, </title> <journal> Machine Learning, </journal> <volume> 2(2), </volume> <year> 1987 </year>
Reference-contexts: Section 7 presents two applications of BIRCH to real life problems. Finally our conclusions and directions for future research are presented in Section 8. 2. Summary of Relevant Research Data clustering has been studied in the Statistics [3, 4, 16, 17], Machine Learning <ref> [1, 7, 8, 15] </ref> and Database [18, 5, 6] communities with different methods and different emphases. <p> Probability-based approaches: They typically <ref> [7, 1] </ref> make the assumption that probability distributions on separate attributes are statistically independent of each other. In reality, this is far from true. Correlation between attributes exists, and sometimes this kind of correlation is exactly what we are looking for. <p> A related problem is that often (e.g., <ref> [7] </ref>), the probability-based tree that is built to identify clusters is not height-balanced. For skewed input data, this may cause the performance to degrade dramatically. Distance-based approaches: They assume that all data points are given in advance and can be scanned frequently.
Reference: 8. <author> Douglas H. Fisher, </author> <title> Iterative Optimization and Simplification of Hierarchical Clusterings, </title> <type> Technical Report CS-95-01, </type> <institution> Dept. of Computer Science, Vanderbilt University, </institution> <address> Nashville, TN 37235. </address>
Reference-contexts: Section 7 presents two applications of BIRCH to real life problems. Finally our conclusions and directions for future research are presented in Section 8. 2. Summary of Relevant Research Data clustering has been studied in the Statistics [3, 4, 16, 17], Machine Learning <ref> [1, 7, 8, 15] </ref> and Database [18, 5, 6] communities with different methods and different emphases.
Reference: 9. <author> A. Gersho, and R. Gray, </author> <title> Vector quantization and signal compression, </title> <address> Boston, Ma.: </address> <publisher> Kluwer Academic Publishers, </publisher> <year> 1992. </year>
Reference-contexts: Phase 4 can be extended with additional passes if desired by the user, and it has been proved to converge to a minimum <ref> [9] </ref>. As a bonus, during this pass each data point can be labeled with the cluster that it belongs to, if we wish to identify the data points in each cluster. DATA CLUSTERING SYSTEM BIRCH AND ITS APPLICATIONS 11 Phase 4 also provides us with the option of discarding outliers. <p> Codebook Generalization in Image Compression Digital image compression [13] is the technology of reducing image data to save storage space and transmission bandwidth. Vector quantization <ref> [9] </ref> is a widely used image compression/decompression technique which operates on blocks of pixels instead of pixels for better efficiency. In vector quantization, the original image is first decomposed into small rectangle blocks, and each block is represented as a vector. <p> To try to avoid getting stuck at a very bad local optimal code-book, LBG starts with an initial codebook of size 1 instead of an initial codebook of the desired size, and proceeds as below: 1. It uses the GLA algorithm <ref> [9] </ref> to find the "optimal" codebook of current size.
Reference: 10. <author> C. Huang, Q. Bi, G. Stiles, R. Harris, </author> <title> Fast Full Search Equivalent Encoding Algorithms for Image Compression Using Vector Quantization, </title> <journal> IEEE Trans. on Image Processing, </journal> <volume> vol. 1, no. 3, </volume> <month> July, </month> <year> 1992. </year>
Reference-contexts: Phase 4 scans the dataset again and puts each data point into the proper cluster; the time taken is proportional to N fl K. However using the newest "nearest neighbor" techniques, it is improved <ref> [10] </ref> to be better than N fl K in general. 6.2. Synthetic Dataset Generator DATA CLUSTERING SYSTEM BIRCH AND ITS APPLICATIONS 17 Table 1. <p> The running time for the first 3 phases is again confirmed to grow linearly wrt. N consistently for all three patterns. With K and N growing at the same time, although the worst case complexity of Phase 4 is still O (K fl N ), it has been improved <ref> [10] </ref> to be almost linear wrt. N when K is not too large, but with different rate for each pattern. <p> As for the "nearest neighbor" algorithm <ref> [10] </ref> used in Phase 4, when K is large, its speed is not stable and is relevant to the layout patterns of clusters. It works best DATA CLUSTERING SYSTEM BIRCH AND ITS APPLICATIONS 25 Table 7. BIRCH Performance on Base Workload wrt.
Reference: 11. <author> J. A. Hartigan, and M. A. Wong, </author> <title> A K-Means Clustering Algorithm, </title> <journal> Appl. Statist., </journal> <volume> vol. 28, no. 1, </volume> <year> 1979. </year>
Reference-contexts: Its numlocal value is still 2. We have implemented the KMEANS algorithm provided in <ref> [11] </ref> and the initial seeds are selected randomly. of the location of the cluster centers is distorted. (2) The number of data points in a CLARANS or KMEANS cluster can be as many as 50% different from the number in the actual cluster. (3) The radii of CLARANS clusters varies largely
Reference: 12. <author> Leonard Kaufman, and Peter J. Rousseeuw, </author> <title> Finding Groups in Data An Introduction to Cluster Analysis, Wiley Series in Probability and Mathematical Statistics, 1990. DATA CLUSTERING SYSTEM BIRCH AND ITS APPLICATIONS 39 </title>
Reference-contexts: This is a nonconvex discrete <ref> [12] </ref> optimization problem. Due to an abundance of local minima, there is typically no way to find a global minimal solution without trying all possible partitions. <p> Hence none of them have linear time scalability with stable quality. There are two types of clustering algorithms <ref> [12] </ref>: Partitioning and Hierarchical algorithms in this category. Partitioning Clustering (PC) starts with an initial partition, then tries all possible moving or swapping of data points from one group to another iteratively to optimize the objective measurement function.
Reference: 13. <author> Weidong Kou, </author> <title> Digital Image Compression Algorithms and Standards, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1995. </year> <note> 14. </note> , <author> Y. Linde, A. Buzo, and R. M. Gray, </author> <title> An Algorithm for Vector Quantization Design, </title> <journal> IEEE Trans. on Communications, </journal> <volume> vol. 28, no. 1, </volume> <year> 1980. </year>
Reference-contexts: In summary, this tool provides soil scientists with a convenient environment in which to explore different feature selections and weight settings in order to find good classifications. 7.2. Codebook Generalization in Image Compression Digital image compression <ref> [13] </ref> is the technology of reducing image data to save storage space and transmission bandwidth. Vector quantization [9] is a widely used image compression/decompression technique which operates on blocks of pixels instead of pixels for better efficiency.
Reference: 15. <author> Michael Lebowitz, </author> <title> Experiments with Incremental Concept Formation : UNIMEM, </title> <booktitle> Machine Learning, </booktitle> <year> 1987. </year>
Reference-contexts: Section 7 presents two applications of BIRCH to real life problems. Finally our conclusions and directions for future research are presented in Section 8. 2. Summary of Relevant Research Data clustering has been studied in the Statistics [3, 4, 16, 17], Machine Learning <ref> [1, 7, 8, 15] </ref> and Database [18, 5, 6] communities with different methods and different emphases.
Reference: 16. <author> R.C.T.Lee, </author> <title> Clustering analysis and its applications, </title> <booktitle> Advances in Information Systems Science, Edited by J.T.Toum, </booktitle> <volume> Vol. 8, </volume> <pages> pp. 169-292, </pages> <publisher> Plenum Press, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: Data clustering identifies the sparse and the crowded places, and hence discovers the overall distribution patterns of the dataset. Be sides, the derived clusters can be visualized more efficiently and effectively than the original dataset <ref> [16, 4] </ref>. <p> Section 7 presents two applications of BIRCH to real life problems. Finally our conclusions and directions for future research are presented in Section 8. 2. Summary of Relevant Research Data clustering has been studied in the Statistics <ref> [3, 4, 16, 17] </ref>, Machine Learning [1, 7, 8, 15] and Database [18, 5, 6] communities with different methods and different emphases.
Reference: 17. <author> F. Murtagh, </author> <title> A Survey of Recent Advances in Hierarchical Clustering Algorithms, </title> <journal> The Computer Journal, </journal> <year> 1983. </year>
Reference-contexts: Section 7 presents two applications of BIRCH to real life problems. Finally our conclusions and directions for future research are presented in Section 8. 2. Summary of Relevant Research Data clustering has been studied in the Statistics <ref> [3, 4, 16, 17] </ref>, Machine Learning [1, 7, 8, 15] and Database [18, 5, 6] communities with different methods and different emphases. <p> It guarantees a local minimum, but the quality of the local minimum is very sensitive to the initial partition, and the worst case time complexity is still exponential. Hierarchical Clustering (HC) <ref> [3, 17] </ref> does not try to find "best" clusters, but keeps merging the closest pair (or splitting the farthest pair) of objects to form clusters. With a reasonable distance measurement, the best time complexity of a practical HC algorithm is O (N 2 ).
Reference: 18. <author> Raymond T. Ng and Jiawei Han, </author> <title> Efficient and Effective Clustering Methods for Spatial Data Mining, </title> <booktitle> Proc. of VLDB, </booktitle> <year> 1994. </year>
Reference-contexts: Section 7 presents two applications of BIRCH to real life problems. Finally our conclusions and directions for future research are presented in Section 8. 2. Summary of Relevant Research Data clustering has been studied in the Statistics [3, 4, 16, 17], Machine Learning [1, 7, 8, 15] and Database <ref> [18, 5, 6] </ref> communities with different methods and different emphases. Previous approaches, probability-based (like most approaches in Machine Learning) or distance-based (like most work in Statistics) , do not adequately consider the case that the dataset can be too large to fit in main memory. <p> With a reasonable distance measurement, the best time complexity of a practical HC algorithm is O (N 2 ). So it is still unable to scale up well with large N . Clustering has been recognized as a useful spatial data mining method recently. <ref> [18] </ref> presents CLARANS, which is a K-medoids algorithm with randomized search strategy, and proposes that CLARANS outperforms the traditional K-medoids algorithms.
Reference: 19. <author> Clark F. Olson, </author> <title> Parallel Algorithms for Hierarchical Clustering, </title> <type> Technical Report, </type> <institution> Computer Science Division, Univ. of California at Berkeley, Dec.,1993. </institution>
Reference-contexts: In this paper, we adapted an agglomerative hierarchical clustering algorithm based on the description in <ref> [19] </ref> and applied it directly to the subclusters represented by their CF vectors. It has a complexity of O (m 2 ), where m is the number of subclusters.
Reference: 20. <author> Tian Zhang, Raghu Ramakrishnan, and Miron Livny, </author> <title> BIRCH: An Efficient Data Clustering Method for Very Large Databases, </title> <type> Technical Report, </type> <institution> Computer Sciences Dept., Univ. of Wisconsin-Madison, </institution> <year> 1995. </year> <note> Received Date Accepted Date Final Manuscript Date </note>
Reference-contexts: Since disk space (R) is just used for outliers, we assume that R &lt; M and set R = 20% of M . The experiments on the effects of the 5 distance metrics in the first 3 phases <ref> [20] </ref> indicate that (1) using D3 in Phases 1 and 2 results in a much higher ending threshold, and hence produces clusters of poorer quality; (2) however, there is no distinctive performance difference among the others. So we DATA CLUSTERING SYSTEM BIRCH AND ITS APPLICATIONS 19 Table 3. <p> The smaller D is, the better the quality is. The threshold is defined as the threshold for cluster diameter as default. In Phase 1, the initial threshold is default to 0. Based on a study of how page size affects performance <ref> [20] </ref>, we selected P = 1024. The delay-split option is off for simplicity. The outlier-handling option is on so that BIRCH can remove outliers and concentrate on the dense places with the given amount of memory.
References-found: 19

