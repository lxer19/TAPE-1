URL: http://www-ai.ijs.si/AramKaralic/bibliography/1991e.ps
Refering-URL: http://www-ai.ijs.si/AramKaralic/bibliography/1991e.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: The Estimation of Probabilities in Attribute Selection Measures for Decision Tree Induction  
Author: Bojan Cestnik and Aram Karalic 
Keyword: inductive learning, attribute selection, estimation of probabilities  
Address: Jamova 39, 61000 Ljubljana Yugoslavia  
Affiliation: Jozef Stefan Institute  
Abstract: In this paper we analyze two well-known measures for attribute selection in decision tree induction, informativity and gini index. In particular, we are interested in the influence of different methods for estimating probabilities on these two measures. The results of experiments show that different measures, which are obtained by different probability estimation methods, determine the preferential order of attributes in a given node. Therefore, they determine the structure of a constructed decision tree. This feature can be very beneficial, especially in real-world applications where several different trees are often required. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Berger, J.O. </author> <year> (1985), </year> <title> Statistical Decision Theory and Bayesian Analysis, </title> <booktitle> Springer Series in Statistics, </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference-contexts: In this article we use m-estimate instead of relative frequency to estimate probabilities that are re quired by the two attribute selection measures. m-estimate is based on Bayesian approach to estimating probabilities <ref> (Berger, 1985) </ref> and was originally developed by Cestnik (1990). <p> The well-known difficulty of the Bayesian analysis <ref> (Berger, 1985) </ref> lies in the determination of prior probabilities. The m-estimate of conditional probabilities (given t) "avoids" this difficulty by taking unconditional probability of class c n , which is computed from the whole learning set L, as its prior.
Reference: <author> Breiman, L., Friedman, J.H., Olshen, R.A., Stone, C.J. </author> <year> (1984), </year> <title> Classification and Regression Trees, </title> <address> Bel-mont, California: </address> <publisher> Wadsworth Int. Group. </publisher>
Reference-contexts: There are several different measures for selecting an attribute in a given node. Many of them have been reviewed and empirically compared by Mingers (1989). In particular, we will consider two measures that are most commonly used, namely informativity measure (Quinlan, 1979) and gini index of diversity <ref> (Breiman et al, 1984) </ref>. They are described in more detail in section 2. Both measures, informativity and gini index, are based on probabilities that have to be estimated from learning examples. Usually, relative frequencies are taken as approximations for probabilities.
Reference: <author> Cestnik, B. </author> <year> (1990), </year> <title> Estimating Probabilities: A Crucial Task in Machine Learning, </title> <booktitle> Proc. of ECAI 90 , Stockholm. </booktitle>
Reference-contexts: It was shown that it can significantly improve the classification accuracy of "naive" Bayesian formula <ref> (Cestnik, 1990) </ref>, and that it can be successfully used in decision tree pruning (Cestnik & Bratko, 1991). 2 Informativity and gini index of diversity Let L denote a set of learning examples described with K attributes, A 1 ; : : : ; A K . <p> Such an estimation can be very problematic when the number of examples is small (e.g. Cestnik, 1990; Quinlan, 1991). In order to avoid such problems, an estimate based on Bayesian approach to estimating probabilities was developed in <ref> (Cestnik, 1990) </ref>. It is called m-estimate.
Reference: <author> Cestnik, B., Bratko, I. </author> <year> (1991), </year> <title> On Estimating Probabilities in Tree Pruning, </title> <booktitle> Proc. of EWSL 91 , Porto, </booktitle> <address> Portugal, March 6-8, </address> <year> 1991. </year>
Reference-contexts: It was shown that it can significantly improve the classification accuracy of "naive" Bayesian formula (Cestnik, 1990), and that it can be successfully used in decision tree pruning <ref> (Cestnik & Bratko, 1991) </ref>. 2 Informativity and gini index of diversity Let L denote a set of learning examples described with K attributes, A 1 ; : : : ; A K . <p> However, the size of constructed trees (number of leaves) was observed to grow with increasing m in all experimental domains. 5 Discussion In <ref> (Cestnik & Bratko, 1991) </ref> it is argued that in the tree construction phase relative frequencies should be used to estimate probabilities in attribute selection measures. A tree constructed in this way is only another way of representing the learning data set.
Reference: <author> Cheeseman, P. </author> <year> (1990), </year> <title> On Finding the Most Probable Model, Computational Models of Scientific Discovery and Theory Formation, </title> <editor> Eds. J.Shranger & P.Langley, </editor> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: However, we showed that by changing the probability estimation method, different attributes can be selected in a given node. This can be very important in real-world domains because, due to the lack of a single domain model, several probabilistic models can be constructed <ref> (Cheeseman, 1990) </ref>. These models can be latter on combined together or, on the other hand, one of them, which proved to be the best according to some criteria, can be selected as the final model.
Reference: <author> Kononenko, I., Bratko, I., Roskar, E. </author> <year> (1984), </year> <title> Experiments in automatic learning of medical diagnostic rules Technical report, </title> <institution> Jozef Stefan Institute, Ljubljana, </institution> <address> Yugoslavia. </address>
Reference: <author> Mingers, J. </author> <year> (1989), </year> <title> An empirical Comparison of Selection Measures for Decision-Tree Induction, </title> <journal> Machine Learning vol. </journal> <volume> 3, no. 4, </volume> <publisher> Kluwer Academic Publishers. </publisher>
Reference: <author> Quinlan, J.R. </author> <year> (1979), </year> <title> Discovering Rules by Induction from Large Collections of Examples, Expert Systems in the Microelectronic Age, </title> <editor> Ed. D.Michie, </editor> <publisher> Edinburgh University Press. </publisher>
Reference-contexts: There are several different measures for selecting an attribute in a given node. Many of them have been reviewed and empirically compared by Mingers (1989). In particular, we will consider two measures that are most commonly used, namely informativity measure <ref> (Quinlan, 1979) </ref> and gini index of diversity (Breiman et al, 1984). They are described in more detail in section 2. Both measures, informativity and gini index, are based on probabilities that have to be estimated from learning examples. Usually, relative frequencies are taken as approximations for probabilities.
Reference: <author> Quinlan, J.R. </author> <year> (1985), </year> <title> Decision trees and multi-valued attributes, </title> <journal> Machine Intelligence vol. </journal> <volume> 11, </volume> <pages> Eds. </pages>
Reference: <editor> J.Hayes & D.Michie, </editor> <address> Chichester, England: </address> <publisher> Ellis Horwood. </publisher>
Reference: <author> Quinlan, J.R. </author> <year> (1986), </year> <title> Induction of decision trees, </title> <journal> Machine Learning vol. </journal> <volume> 1, no. 1, </volume> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: In addition, the constructed rule should be relatively easily understood by humans. One approach to inductive learning is to build decision trees. Systems that are based on this approach belong to TDIDT (Top Down Induction of Decision Trees) family <ref> (Quinlan, 1986) </ref>. The most famous member of this family is Quinlan's (1979) ID3. More recent members improved upon the original ID3 so as to become more suited to real-world problems, where data are often incomplete and unreliable.
Reference: <author> Quinlan, J.R. </author> <year> (1991), </year> <title> Improved Estimates for the Accuracy of Small Disjuncts, Technical Note, </title> <journal> Machine Learning vol. </journal> <volume> 6, no. 1, </volume> <publisher> Kluwer Academic Publishers. </publisher> <editor> Shannon, </editor> <booktitle> Weaver (1949), The mathematical theory of communications, </booktitle> <address> Urbana: </address> <publisher> The University of Illinois Press. </publisher>
References-found: 12

