URL: http://www.cs.utah.edu/~retrac/papers/lcr98.ps.Z
Refering-URL: http://www.cs.utah.edu/~retrac/papers.html
Root-URL: 
Title: Memory System Support for Irregular Applications  
Author: John Carter, Wilson Hsieh, Mark Swanson, Al Davis, Michael Parker, Lambert Schaelicke, Leigh Stoller, Terry Tateyama, Lixin Zhang 
Affiliation: Department of Computer Science, University of Utah  
Abstract: Because irregular applications have unpredictable memory access patterns, their performance is dominated by memory behavior. The Impulse configurable memory controller will enable significant performance improvements for irregular applications, because it can be configured to optimize memory accesses on an application-by-application basis. In this paper we describe the optimizations that the Impulse controller supports for sparse matrix-vector multiplication, an important computational kernel, and outline the transformations that the compiler and runtime system must perform to exploit these optimizations.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Alverson, D. Callahan, D. Cummings, B. Koblenz, A. Porterfield, and B. Smith. </author> <title> The Tera computer system. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 272-277, </pages> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: A number of ongoing projects have proposed significant modifications to conventional CPU or DRAM designs to attack this memory problem: supporting massive multithreading <ref> [1] </ref>, moving processing power on to the DRAM chips [6], or building completely programmable architectures [14].
Reference: [2] <author> D. Bailey et al. </author> <title> The NAS parallel benchmarks. </title> <type> Technical Report RNR-94-007, </type> <institution> NASA Ames Research Center, </institution> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: For example, most of the time spent performing a conjugate gradient computation <ref> [2] </ref> is spent in a sparse matrix-vector multiply. Similarly, the Spark98 [9] kernels are all sparse matrix-vector multiplication codes.
Reference: [3] <author> J. Boisseau, L. Carter, K. S. Gatlin, A. Majumdar, and A. Snavely. </author> <title> NAS benchmarks on the Tera MTA. In Proceedings of the Multithreaded Execution Architecture and Compilation, </title> <address> Las Vegas, NV, Jan. 31-Feb. 1, </address> <year> 1998. </year>
Reference-contexts: The Tera MTA prototype achieves good single-node performance on the NAS conjugate gradient benchmark because its support for large numbers of cheap threads allows it to tolerate almost arbitrary memory latencies <ref> [3] </ref>. However, both of these solutions are very expensive compared to conventional hardware. In Impulse, we take a different position we employ off-the-shelf CPUs and memories, but build an adaptable memory controller. Our memory controller supports two major optimizations for improving the memory behavior of sparse matrix-vector multiplication.
Reference: [4] <author> D. Burger, J. Goodman, and A. Kagi. </author> <title> Memory bandwidth limitations of future microprocessors. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 78-89, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: This factor of eight difference in performance is a compelling indication that caches are beginning to fail in their role of hiding the latency of main memory from processors. Other studies present similar results for other applications <ref> [4, 5] </ref>. Fundamentally, modern caches and memory systems are optimized for applications that sequentially access dense regions of memory. Programs with high degrees of spatial and temporal locality achieve near 100% cache hit rates, and will not be affected significantly as the latency of main memory increases.
Reference: [5] <author> A. Huang and J. Shen. </author> <title> The intrinsic bandwidth requirements of ordinary programs. </title> <booktitle> In Proceedings of the 7th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 105-114, </pages> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: This factor of eight difference in performance is a compelling indication that caches are beginning to fail in their role of hiding the latency of main memory from processors. Other studies present similar results for other applications <ref> [4, 5] </ref>. Fundamentally, modern caches and memory systems are optimized for applications that sequentially access dense regions of memory. Programs with high degrees of spatial and temporal locality achieve near 100% cache hit rates, and will not be affected significantly as the latency of main memory increases.
Reference: [6] <editor> C. E. Kozyrakis et al. </editor> <booktitle> Scalable processors in the billion-transistor era: IRAM. IEEE Computer, </booktitle> <pages> pages 75-78, </pages> <month> Sept. </month> <year> 1997. </year>
Reference-contexts: A number of ongoing projects have proposed significant modifications to conventional CPU or DRAM designs to attack this memory problem: supporting massive multithreading [1], moving processing power on to the DRAM chips <ref> [6] </ref>, or building completely programmable architectures [14]. While these projects show promise, non-traditional CPU or DRAM designs will likely be slower than conventional designs, due to slow industry acceptance and the fact that processors built on current DRAM processes are 50% slower than conventional processors. <p> The RADram project at UC Davis is building a memory system that lets the memory perform computation [10]. RADram is a PIM (processor-in-memory) project similar to IRAM <ref> [6] </ref>, where the goal is to put processors close to memory. In contrast, Impulse does not seek to put a processor in memory; instead, its memory controller is programmable. In summary, the Impulse project is developing a memory system that will help to improve the memory performance of irregular applications.
Reference: [7] <author> M. S. Lam, E. E. Rothberg, and M. E. Wolf. </author> <title> The cache performance and optimizations of blocked algorithms. </title> <booktitle> In Proceedings of the 4th ASPLOS, </booktitle> <pages> pages 63-74, </pages> <address> Santa Clara, CA, </address> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: An Impulse system can also improve the performance of dense matrix kernels. Dense matrices are tiled to improve cache behavior, but the effectiveness of tiling is limited by the fact that tiles do not necessarily occupy contiguous blocks in caches. Tile copying <ref> [7] </ref> can improve the performance of tiled algorithms by reducing cache conflicts, but the cost of copying is fairly high. The Impulse controller allows tiles to be copied virtually. The cost that virtual copying incurs is that read-write sharing between virtual copies requires cache flushing to maintain coherence.
Reference: [8] <author> S. McKee and W. A. Wulf. </author> <title> Access ordering and memory-conscious cache utilization. </title> <booktitle> In Proceedings of the First IEEE Symposium on High Performance Computer Architecture, </booktitle> <pages> pages 253-262, </pages> <address> Raleigh, NC, </address> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: In effect, we can use a small part of the second-level cache, e.g., two pages each for DATA, ROWS, and COLUMNS, as a stream buffer <ref> [8] </ref>. 3 Conclusions We have described two optimizations that the Impulse controller supports for irregular applications. An Impulse system can also improve the performance of dense matrix kernels.
Reference: [9] <author> D. R. O'Hallaron. Spark98: </author> <title> Sparse matrix kernels for shared memory and message passing systems. </title> <type> Technical Report CMu-CS-97-178, </type> <institution> Carnegie Mellon University School of Computer Science, </institution> <month> Oct. </month> <year> 1997. </year>
Reference-contexts: For example, most of the time spent performing a conjugate gradient computation [2] is spent in a sparse matrix-vector multiply. Similarly, the Spark98 <ref> [9] </ref> kernels are all sparse matrix-vector multiplication codes. In this section, we describe several optimizations that Impulse enables to improve the performance of sparse matrix-vector muliplication, and sketch the transformations that the compiler and runtime system must perform to exploit these optimizations.
Reference: [10] <author> M. Oskin, F. T. Chong, and T. Sherwood. </author> <title> Active pages: A model of computation for intelligent memory. </title> <booktitle> In Proceedings of the 25th International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1998. </year> <note> To appear. </note>
Reference-contexts: The Impulse project is designing hardware that will be built in an academic environment, which requires that we attack hardware outside of the processor. The RADram project at UC Davis is building a memory system that lets the memory perform computation <ref> [10] </ref>. RADram is a PIM (processor-in-memory) project similar to IRAM [6], where the goal is to put processors close to memory. In contrast, Impulse does not seek to put a processor in memory; instead, its memory controller is programmable.
Reference: [11] <author> S. E. Perl and R. </author> <title> Sites. Studies of Windows NT performance using dynamic execution traces. </title> <booktitle> In Proceedings of the Second Symposium on Operating System Design and Implementation, </booktitle> <pages> pages 169-184, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: For example, in a recent study of the cache behavior of the SQLserver database running on an Alphastation 8400, the system achieved only 12% of its peak memory bandwidth <ref> [11] </ref>; the resulting CPI was 2 (compared to a minimum CPI of 1/4). This factor of eight difference in performance is a compelling indication that caches are beginning to fail in their role of hiding the latency of main memory from processors.
Reference: [12] <author> S. Saini and D. H. Bailey. </author> <title> NAS parallel benchmark (version 1.0) results. </title> <type> Technical Report NAS-96-18, </type> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA, </address> <month> Nov. </month> <year> 1996. </year>
Reference-contexts: Two previously studied solutions to this problem are to build main memory exclusively from fast SRAM or support massive multithreading. The Cray T916, whose main memory consists solely of fast (and expensive) SRAM, achieved 759.9 Mflops/second on the NAS CG benchmark in July 1995 <ref> [12] </ref>. The Tera MTA prototype achieves good single-node performance on the NAS conjugate gradient benchmark because its support for large numbers of cheap threads allows it to tolerate almost arbitrary memory latencies [3]. However, both of these solutions are very expensive compared to conventional hardware.
Reference: [13] <author> M. Swanson, L. Stoller, and J. Carter. </author> <title> Increasing TLB reach using superpages backed by shadow memory. </title> <booktitle> In Proceedings of the 25th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1998. </year>
Reference-contexts: The Impulse controller allows tiles to be copied virtually. The cost that virtual copying incurs is that read-write sharing between virtual copies requires cache flushing to maintain coherence. An Impulse memory controller can be used to dynamically build superpages, so as to save processor TLB entries <ref> [13] </ref>. This optimization can dramatically improve the performance of applications with large working sets. Unfor 2 In the NAS CG benchmarks, x ranges from 600 kilobytes to 1.2 megabytes, and the other structures range from 100-300 megabytes.
Reference: [14] <author> X. Zhang, A. Dasdan, M. Schulz, R. K. Gupta, and A. A. Chien. </author> <title> Architectural adaptation for application-specific locality optimizations. </title> <booktitle> In Proceedings of the 1997 IEEE International Conference on Computer Design, </booktitle> <year> 1997. </year> <month> 6 </month>
Reference-contexts: A number of ongoing projects have proposed significant modifications to conventional CPU or DRAM designs to attack this memory problem: supporting massive multithreading [1], moving processing power on to the DRAM chips [6], or building completely programmable architectures <ref> [14] </ref>. While these projects show promise, non-traditional CPU or DRAM designs will likely be slower than conventional designs, due to slow industry acceptance and the fact that processors built on current DRAM processes are 50% slower than conventional processors. <p> Instead, an Impulse controller could handle the translations necessary for messages that span multiple pages. Other research projects are looking at similar issues in designing memory systems; we briefly describe a few of them. The Morph architecture <ref> [14] </ref> is almost entirely configurable: programmable logic is embedded in virtually every datapath in the system. As a result, optimizations similar to those that we have described are possible using Morph.
References-found: 14

