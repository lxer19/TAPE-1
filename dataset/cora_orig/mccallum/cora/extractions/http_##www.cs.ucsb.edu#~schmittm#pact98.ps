URL: http://www.cs.ucsb.edu/~schmittm/pact98.ps
Refering-URL: http://www.cs.ucsb.edu/~schmittm/res.html
Root-URL: http://www.cs.ucsb.edu
Email: fschmittm, ibel, acha, schauserg@cs.ucsb.edu  
Title: Adaptive Receiver Notification for Non-Dedicated Workstation Clusters  
Author: Michael Schmitt Max Ibel Anurag Acharya Klaus Schauser 
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California, Santa Barbara  
Abstract: Efficient communication in a NOW environment can be a challenging task. Depending on the application, the architecture of the nodes and the characteristics of other processes running on the nodes, different communication strategies can be appropriate. In this paper, we evaluate an adaptive scheme which selects between multiple communication strategies depending on the current situation. We focus on strategies for receiver notification under varying load conditions. Previous research has shown that in a non-dedicated environment, spin-block strategies can avoid scheduling anomalies which can cause several orders of magnitude of performance degradation. However, research on these strategies has focused on a fixed spin-delay. Our scheme achieves better performance than static spin-block schemes on workstation clusters with varying load. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Dusseau and D. Culler. </author> <title> Extending proportional-share scheduling to a network of workstations. </title> <booktitle> In International Conference on Parallel and Distributed Processing Techniques and Applications (PDPTA'97), </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: This problem becomes worse as the load on the NOW increases. In a non-dedicated setting, misaligned scheduling not only slows down the parallel application, but also wastes time in polling which could be gainfully utilized by other processes. Implicit co-scheduling <ref> [1, 2, 3, 4] </ref> has been proposed as a way to improve the scheduling coordination without modifying the operating system. It takes advantage of the communication performed by the parallel application to keep communicating peers (roughly) in sync.
Reference: [2] <author> A. Dusseau, D. Culler, and A. Mainwaring. </author> <title> Scheduling with implicit information in distributed systems. </title> <booktitle> In Sigmetrics'98 Conference on the Measurement and Modeling of Computer Systems, </booktitle> <month> June </month> <year> 1998. </year>
Reference-contexts: This problem becomes worse as the load on the NOW increases. In a non-dedicated setting, misaligned scheduling not only slows down the parallel application, but also wastes time in polling which could be gainfully utilized by other processes. Implicit co-scheduling <ref> [1, 2, 3, 4] </ref> has been proposed as a way to improve the scheduling coordination without modifying the operating system. It takes advantage of the communication performed by the parallel application to keep communicating peers (roughly) in sync. <p> A hybrid strategy amortizes these costs by requiring the receiver to spin for a given time-period (spin-delay) and to block thereafter. This mechanism, referred to as spin-block, was proven 2-competitive in [5] and has been successfully used in <ref> [2, 3, 4] </ref>. Figure 1 sketches the spin-only quantum size) for scheduling a parallel application that circulates a token among its nodes. Right: Implementation of spin-only vs spin-block. and spin-block protocol. 1 However, research on spin-block strategies has focused on fixed spin-delays. <p> The right two graphs show the performance of the adaptive schemes. 5. Related work Scheduling anomalies are a major problem for parallel applications running on a non-dedicated cluster. 3 Recent research on communication libraries for non-dedicated clusters has focused on implicit co-scheduling and spin-block protocols <ref> [2, 3, 4] </ref>. Dusseau et al [2] investigate multiple bulk-synchronous parallel jobs with varying load-imbalance. Their work differs from ours in several ways. They propose the use of micro-benchmarks to determine the optimal spin-delay for a cluster. These micro-benchmarks are run on an unloaded cluster. <p> Related work Scheduling anomalies are a major problem for parallel applications running on a non-dedicated cluster. 3 Recent research on communication libraries for non-dedicated clusters has focused on implicit co-scheduling and spin-block protocols [2, 3, 4]. Dusseau et al <ref> [2] </ref> investigate multiple bulk-synchronous parallel jobs with varying load-imbalance. Their work differs from ours in several ways. They propose the use of micro-benchmarks to determine the optimal spin-delay for a cluster. These micro-benchmarks are run on an unloaded cluster.
Reference: [3] <author> P. Sobalvarro, S. Pakin, A. Chien, and W. Weihl. </author> <title> Dynamic coscheduling on workstation clusters. </title> <booktitle> In Proceedings of the International Parallel Processing Symposium, </booktitle> <year> 1998. </year>
Reference-contexts: This problem becomes worse as the load on the NOW increases. In a non-dedicated setting, misaligned scheduling not only slows down the parallel application, but also wastes time in polling which could be gainfully utilized by other processes. Implicit co-scheduling <ref> [1, 2, 3, 4] </ref> has been proposed as a way to improve the scheduling coordination without modifying the operating system. It takes advantage of the communication performed by the parallel application to keep communicating peers (roughly) in sync. <p> A hybrid strategy amortizes these costs by requiring the receiver to spin for a given time-period (spin-delay) and to block thereafter. This mechanism, referred to as spin-block, was proven 2-competitive in [5] and has been successfully used in <ref> [2, 3, 4] </ref>. Figure 1 sketches the spin-only quantum size) for scheduling a parallel application that circulates a token among its nodes. Right: Implementation of spin-only vs spin-block. and spin-block protocol. 1 However, research on spin-block strategies has focused on fixed spin-delays. <p> The right two graphs show the performance of the adaptive schemes. 5. Related work Scheduling anomalies are a major problem for parallel applications running on a non-dedicated cluster. 3 Recent research on communication libraries for non-dedicated clusters has focused on implicit co-scheduling and spin-block protocols <ref> [2, 3, 4] </ref>. Dusseau et al [2] investigate multiple bulk-synchronous parallel jobs with varying load-imbalance. Their work differs from ours in several ways. They propose the use of micro-benchmarks to determine the optimal spin-delay for a cluster. These micro-benchmarks are run on an unloaded cluster. <p> Finally, they investigate the performance of clusters running multiple parallel applications whereas we focus on the the situation with a single parallel application with sequential processes on individual nodes. Sobalvarro et al <ref> [3] </ref> use a similar approach with a fixed spin-delay. Their goal is to design schedulers and network interfaces to better support implicit co-scheduling. To achieve that, they need modifications to the network driver, the network interface firmware and to the operating system.
Reference: [4] <author> S. Damianakis, Y. Chen, and E. Felten. </author> <title> Reducing waiting costs in user-level communication. </title> <booktitle> In 11th Intl. Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1997. </year>
Reference-contexts: This problem becomes worse as the load on the NOW increases. In a non-dedicated setting, misaligned scheduling not only slows down the parallel application, but also wastes time in polling which could be gainfully utilized by other processes. Implicit co-scheduling <ref> [1, 2, 3, 4] </ref> has been proposed as a way to improve the scheduling coordination without modifying the operating system. It takes advantage of the communication performed by the parallel application to keep communicating peers (roughly) in sync. <p> A hybrid strategy amortizes these costs by requiring the receiver to spin for a given time-period (spin-delay) and to block thereafter. This mechanism, referred to as spin-block, was proven 2-competitive in [5] and has been successfully used in <ref> [2, 3, 4] </ref>. Figure 1 sketches the spin-only quantum size) for scheduling a parallel application that circulates a token among its nodes. Right: Implementation of spin-only vs spin-block. and spin-block protocol. 1 However, research on spin-block strategies has focused on fixed spin-delays. <p> The right two graphs show the performance of the adaptive schemes. 5. Related work Scheduling anomalies are a major problem for parallel applications running on a non-dedicated cluster. 3 Recent research on communication libraries for non-dedicated clusters has focused on implicit co-scheduling and spin-block protocols <ref> [2, 3, 4] </ref>. Dusseau et al [2] investigate multiple bulk-synchronous parallel jobs with varying load-imbalance. Their work differs from ours in several ways. They propose the use of micro-benchmarks to determine the optimal spin-delay for a cluster. These micro-benchmarks are run on an unloaded cluster. <p> Their goal is to design schedulers and network interfaces to better support implicit co-scheduling. To achieve that, they need modifications to the network driver, the network interface firmware and to the operating system. In contrast, our goal is to build portable and adaptive communication schemes. Damianakis et al <ref> [4] </ref> also use a fixed spin-delay and employ the network interface on the receiver side to wake up blocking parallel jobs. Special hardware of their SHRIMP multicomputer allows them to enable and disable the fixed spin-delay protocol at memory page granularity.
Reference: [5] <author> A. Karlin, K. Li, M. Manasse, and S. Owicki. </author> <title> Empirical studies of competitive spinning for shared memory multiprocessors. </title> <booktitle> In Proc. 13th ACM Symposium on Operating Systems Principles, </booktitle> <year> 1991. </year>
Reference-contexts: A hybrid strategy amortizes these costs by requiring the receiver to spin for a given time-period (spin-delay) and to block thereafter. This mechanism, referred to as spin-block, was proven 2-competitive in <ref> [5] </ref> and has been successfully used in [2, 3, 4]. Figure 1 sketches the spin-only quantum size) for scheduling a parallel application that circulates a token among its nodes. Right: Implementation of spin-only vs spin-block. and spin-block protocol. 1 However, research on spin-block strategies has focused on fixed spin-delays.
Reference: [6] <editor> IEEE, </editor> <address> 345 East 47th Street, New York. </address> <institution> IEEE Standard for Scalable Coherent Interface (SCI), </institution> <year> 1993. </year>
Reference-contexts: Experimental Testbed Our testbed consists of two clusters of Sparc-based workstations. In this study we use a 4-node cluster consisting of 250 MHz PCI-based Ultra-30 workstations running Solaris 2.6 and connected by the Scalable Coherent Interface (SCI) <ref> [6] </ref>. SCI provides a limited form of (noncoherent) shared memory. A node can export a memory segment to other nodes which can then directly read and write to it. The network adaptors provide no caching and translate all remote accesses into network transactions.
Reference: [7] <author> M. Ibel, K. E. Schauser, C. J. Scheiman, and M. Weis. </author> <title> High-Performance Cluster Computing Using SCI. </title> <booktitle> In Proc. of Hot Interconnects V, </booktitle> <month> August </month> <year> 1997. </year>
Reference-contexts: This allows low latency, high-bandwidth communication (3s latency for one-word stores and bandwidths of 66 MB/s on our cluster). Unlike many user-level communication architectures, SCI completely virtualizes the network interface; multiple processes can concurrently and transparently use the network interface. For detailed performance numbers on our cluster, refer to <ref> [7] </ref>. We use an Active Messages-based [8] communication library. We chose this approach as it uses simple and efficient protocols, yet is flexible enough to implement higher-level message-passing abstractions. An Active Message contains the address of a handler function which is executed 2 on a receiving processor.
Reference: [8] <author> T. von Eicken. </author> <title> Active Messages: an Efficient Communications Architecture for Multiprocessors. </title> <type> PhD thesis, </type> <institution> Computer Science Division EECS, U.C. Berkeley, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: Unlike many user-level communication architectures, SCI completely virtualizes the network interface; multiple processes can concurrently and transparently use the network interface. For detailed performance numbers on our cluster, refer to [7]. We use an Active Messages-based <ref> [8] </ref> communication library. We chose this approach as it uses simple and efficient protocols, yet is flexible enough to implement higher-level message-passing abstractions. An Active Message contains the address of a handler function which is executed 2 on a receiving processor.
Reference: [9] <author> Dror G. Feitelson. </author> <title> A Survey of Scheduling in Multipro-grammed Parallel Systems. </title> <type> Research report RC 19790 (87657), </type> <institution> IBM T.J. Watson Research Center, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: Damianakis et al [4] also use a fixed spin-delay and employ the network interface on the receiver side to wake up blocking parallel jobs. Special hardware of their SHRIMP multicomputer allows them to enable and disable the fixed spin-delay protocol at memory page granularity. The porta 3 See <ref> [9] </ref> for a survey on scheduling strategies for parallel machines and workstation clusters. 4 Dusseau et al also investigate an adaptive algorithm based on barrier execution time but discard this approach because of its high overhead. bility of their approach is limited to the SHRIMP architecture. 6.
Reference: [10] <author> D. Dunning and G. Regnier. </author> <title> The Virtual Interface Architecture. In Hot Interconnects V Symposium on High Performance Interconnects, </title> <note> August 97. 8 </note>
Reference-contexts: Our implementation of spin-block is based on explicit notification from the sender and thus does not require the involvement of the network interface on the receiver side. While our approach is portable and lends itself naturally to new architectures like VIA <ref> [10] </ref>, it is unclear if its sender-initiated nature could cause too many unnecessary notifications on larger clusters. We plan to study the performance 7 and 100ms granularity and different load patterns for the additional workload.
References-found: 10

