URL: ftp://ftp.aic.nrl.navy.mil/pub/papers/1994/AIC-94-014.ps
Refering-URL: http://www.aic.nrl.navy.mil/~schultz/papers.html
Root-URL: 
Email: -gref, schultz-@aic.nrl.navy.mil  
Title: An Evolutionary Approach to Learning in Robots  
Author: John Grefenstette and Alan Schultz 
Address: Washington, DC 20375  
Affiliation: Navy Center for Artificial Intelligence Naval Research Laboratory  
Abstract: Evolutionary learning methods have been found to be useful in several areas in the development of intelligent robots. In the approach described here, evolutionary algorithms are used to explore alternative robot behaviors within a simulation model as a way of reducing the overall knowledge engineering effort. This paper presents some initial results of applying the SAMUEL genetic learning system to a collision avoidance and navigation task for mobile robots.
Abstract-found: 1
Intro-found: 1
Reference: <author> Booker, L. B. </author> <year> (1988). </year> <title> Classifier systems that learn internal world models. </title> <booktitle> Machine Learning 3(3), </booktitle> <pages> 161-192. </pages>
Reference-contexts: However, some recent studies have used more heuristic operators that make more directed changes based on the learning agent's experience. For example, some genetic classifier systems use triggered operators such a creating a new rule to cover a novel situation <ref> (Booker, 1988) </ref>. In SAMUEL, we use generalization and specialization operators that are triggered by specific conditions relating the measured utilities of individual rules and the outcome of the task.
Reference: <author> Brooks, R. </author> <title> Artificial Life and Real Robots. </title> <publisher> Cambridge: MIT Press. </publisher> <year> 1992. </year>
Reference-contexts: In response to such shortcomings, some researchers argue for the development of adaptive robots that evolve behaviors without using a pre-specified model of the world <ref> (Brooks, 1992) </ref>. Here again, we prefer a middle ground, assuming the existence of a limited fidelity simulation model of the robot and its environment.
Reference: <author> De Jong, K. A. and W. </author> <title> Spears (1993). </title> <booktitle> On the state of evolutionary computation. Proceedings of the Fifth International Conference on Genetic Algorithms, </booktitle> <pages> pp. 618-626, </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: One class of methods that has shown its utility on a number of relevant problems is called Evolutionary Computation <ref> (De Jong and Spears, 1993) </ref>. This term applies to computational methods that incorporate principles from biological population genetics to perform search, optimization, and machine learning, and includes a variety of specific formulations with names such as genetic algorithms, evolutionary programming, evolution strategies, and genetic programming.
Reference: <author> Dorigo, M. and U. </author> <title> Schnepf (1993). Genetics-based machine learning and behavior-based robotics: a new synthesis. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, SMC-23, </journal> <volume> 1. </volume>
Reference-contexts: evolutionary algorithms have been used to learn rule sets for rule-based autonomous agents (Grefenstette, Ramsey and Schultz, 1990), topologies and weights for neural nets for robotic control (Whitley et al, 1993; Yamauchi, 1994), fuzzy logic control systems (Karr, 1991), programs for LISP-controlled robots (Koza, 1992), and rules for behavior-based robots <ref> (Dorigo and Schnepf, 1993) </ref>. In our approach evolutionary algorithms are used to explore alternative robot behaviors within a simulation model as a way of reducing the overall knowledge engineering effort.
Reference: <author> Grefenstette, J. </author> <year> (1987). </year> <title> Incorporating problem specific knowledge into genetic algorithms, in Genetic Algorithms and Simulated Annealing, </title> <editor> L. Davis (Ed.), </editor> <publisher> London: Pitman. </publisher>
Reference-contexts: In addition, we will continue to explore ways on speeding the learning process by including domain-speci fic biases in the evolutionary learning method <ref> (Grefenstette, 1987) </ref>. Here, we outline specific extensions used for our robot navigation problem. Choice of Search Space and Representation. The first choice the user of an evolutionary algorithm needs to make is the choice of the search space, along with an appropriate representation. <p> Evolutionary methods are less efficient at fine-tuning candidate solutions. Therefore, a natural hybrid is to use an efficient local optimization method to improve the final solutions found by the evolutionary system <ref> (Grefenstette, 1987) </ref>. Another promising hybrid approach is to combine the SAMUEL learning method with a case-based module, to provide a way to dynamically modify the simulation model based on the robot's experience with the external environment.
Reference: <author> Grefenstette, J. J. </author> <year> (1991). </year> <title> Lamarckian learning in multi-agent environments. </title> <booktitle> Proceedings of the Fourth International Conference of Genetic Algorithms pp. </booktitle> <pages> 303-310, </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In SAMUEL, we use generalization and specialization operators that are triggered by specific conditions relating the measured utilities of individual rules and the outcome of the task. These may be viewed as Lamarckian forms of evolution <ref> (Grefenstette, 1991) </ref>, and show that artificial evolution need not proceed along purely Darwinian lines. Hybrid Approaches. Finally, it is often useful to use evolutionary methods in concert with other methods that have complementary strengths.
Reference: <author> Grefenstette, J. </author> <year> (1992). </year> <title> The evolution of strategies for multi-agent environments. </title> <booktitle> Adaptive Behavior 1(1), </booktitle> <pages> 65-90. </pages>
Reference-contexts: As illustrated in Fig. 1, the current best behavior can be placed in the on-line execution system, while learning continues in the off-line system <ref> (Grefenstette and Ramsey, 1992) </ref>. <p> Constraints are intended to limit the robot's actions within physically safe parameters, but still allow freedom to explore a large set of alternative strategies <ref> (Grefenstette, 1992) </ref>. Initial Population. Evolutionary algorithms often begin with candidate solutions selected at random from the search space. Often, the approach can be sped up by the use of heuristics to select the starting population.
Reference: <author> Grefenstette, J. J. and H. C. </author> <note> Cobb (1994). User's guide for SAMUEL. Version 4.0. NRL Report, </note> <institution> Naval Research Lab, </institution> <address> Washington, DC. </address>
Reference-contexts: This cycle is repeated until the task is accomplished or failure occurs. In SAMUEL, the user can further limit the search space by defining a set of constraints in the form of rules that specify conditions under which certain actions are either forbidden or required <ref> (Grefenstette and Cobb, 1994) </ref>. Constraints are intended to limit the robot's actions within physically safe parameters, but still allow freedom to explore a large set of alternative strategies (Grefenstette, 1992). Initial Population. Evolutionary algorithms often begin with candidate solutions selected at random from the search space. <p> This paper reports some early tests of the learned knowledge on a real physical system. For more details of the SAMUEL system, see <ref> (Grefenstette and Cobb, 1994) </ref>. Future work will continue examining the process of building robotic systems through evolution. We want to know how multiple behaviors that will be required for a higher level task interact, and how multiple behaviors can be evolved simultaneously.
Reference: <author> Grefenstette, J. J. and C. L. </author> <title> Ramsey (1992). An approach to anytime learning. </title> <booktitle> Proceedings of the Ninth International Conference on Machine Learning pp. </booktitle> <pages> 189-195, </pages> <editor> D. Sleeman and P. Edwards (eds.), </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: As illustrated in Fig. 1, the current best behavior can be placed in the on-line execution system, while learning continues in the off-line system <ref> (Grefenstette and Ramsey, 1992) </ref>. <p> Constraints are intended to limit the robot's actions within physically safe parameters, but still allow freedom to explore a large set of alternative strategies <ref> (Grefenstette, 1992) </ref>. Initial Population. Evolutionary algorithms often begin with candidate solutions selected at random from the search space. Often, the approach can be sped up by the use of heuristics to select the starting population.
Reference: <author> Grefenstette, J. J., C. L. Ramsey and A. C. </author> <title> Schultz (1990). Learning sequential decision rules using simulation models and competition. </title> <booktitle> Machine Learning 5(4), </booktitle> <pages> 355-381. </pages>
Reference-contexts: Previous studies have illustrated that knowledge learned under simulation is robust and might be applicable to the real world if the simulation is more general (i.e. has more noise, more varied conditions, etc.) than the real world environment <ref> (Ramsey, Schultz and Grefenstette, 1990) </ref>. Where this is not possible, it is important to identify the differences between the simulation and the world and measure the effect upon the learning process. <p> Evolutionary methods have found applications that span the range of architectures for intelligent robotics. For example, evolutionary algorithms have been used to learn rule sets for rule-based autonomous agents <ref> (Grefenstette, Ramsey and Schultz, 1990) </ref>, topologies and weights for neural nets for robotic control (Whitley et al, 1993; Yamauchi, 1994), fuzzy logic control systems (Karr, 1991), programs for LISP-controlled robots (Koza, 1992), and rules for behavior-based robots (Dorigo and Schnepf, 1993). <p> In our approach evolutionary algorithms are used to explore alternative robot behaviors within a simulation model as a way of reducing the overall knowledge engineering effort. The remainder of this paper will focus on the SAMUEL evolutionary learning system being developed at NRL <ref> (Grefenstette et al., 1990) </ref> and will present initial results of applying the SAMUEL system to a collision avoidance and navigation task for mobile robots. In SAMUEL, the population is composed of candidate behaviors for solving the task. <p> Each trial begins with a different configuration of the obstacles in the room. The robot was given 80 decision time steps to cross the room to the goal position. The initial heterogeneous population <ref> (Schultz and Grefenstette, 1990) </ref> consisted of a variety of rule sets from different sources. This included a combination of manually (human) generated rules sets and automatically generated variants of those rules. <p> This must be done with care, however, since a lack of sufficient diversity in the initial population is almost guaranteed to produced premature convergence to suboptimal solutions. In SAMUEL, the rule representation was designed to encourage the user to include heuristic strategies in the initial population <ref> (Schultz and Grefenstette, 1990) </ref>. In fact, for many complex robotic tasks, it is unlikely that the system will be able to evolve solutions without some initial heuristics.
Reference: <author> Karr, C, L. </author> <year> (1991). </year> <title> Design of an adaptive fuzzy logic controller using a genetic algorithm. </title> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <pages> pp. 450-457, </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For example, evolutionary algorithms have been used to learn rule sets for rule-based autonomous agents (Grefenstette, Ramsey and Schultz, 1990), topologies and weights for neural nets for robotic control (Whitley et al, 1993; Yamauchi, 1994), fuzzy logic control systems <ref> (Karr, 1991) </ref>, programs for LISP-controlled robots (Koza, 1992), and rules for behavior-based robots (Dorigo and Schnepf, 1993). In our approach evolutionary algorithms are used to explore alternative robot behaviors within a simulation model as a way of reducing the overall knowledge engineering effort.
Reference: <editor> Koza, J. R. </editor> <booktitle> (1992). Genetic Programming. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: For example, evolutionary algorithms have been used to learn rule sets for rule-based autonomous agents (Grefenstette, Ramsey and Schultz, 1990), topologies and weights for neural nets for robotic control (Whitley et al, 1993; Yamauchi, 1994), fuzzy logic control systems (Karr, 1991), programs for LISP-controlled robots <ref> (Koza, 1992) </ref>, and rules for behavior-based robots (Dorigo and Schnepf, 1993). In our approach evolutionary algorithms are used to explore alternative robot behaviors within a simulation model as a way of reducing the overall knowledge engineering effort.
Reference: <author> Ramsey, C. L., A. C. Schultz and J. J. </author> <title> Grefenstette (1990). Simulation-assisted learning by competition: Effects of noise differences between training model and target environment. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning pp. </booktitle> <pages> 211-215. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Previous studies have illustrated that knowledge learned under simulation is robust and might be applicable to the real world if the simulation is more general (i.e. has more noise, more varied conditions, etc.) than the real world environment <ref> (Ramsey, Schultz and Grefenstette, 1990) </ref>. Where this is not possible, it is important to identify the differences between the simulation and the world and measure the effect upon the learning process. <p> Evolutionary methods have found applications that span the range of architectures for intelligent robotics. For example, evolutionary algorithms have been used to learn rule sets for rule-based autonomous agents <ref> (Grefenstette, Ramsey and Schultz, 1990) </ref>, topologies and weights for neural nets for robotic control (Whitley et al, 1993; Yamauchi, 1994), fuzzy logic control systems (Karr, 1991), programs for LISP-controlled robots (Koza, 1992), and rules for behavior-based robots (Dorigo and Schnepf, 1993). <p> Evolutionary algorithms may also exploit unexpected features of the simulation model to maximize performance. Of course, this implies that the model should accurately reflect the conditions in the target environment. To the extent that this is not possible, our studies <ref> (Ramsey et al., 1990) </ref> have shown that it is still possible to learn from limited-fidelity simulations that err on the side of difficulty (e.g., have more noisy sensors that the real robots). In such cases, the learning time increases, but so does the robustness of the learned rules.
Reference: <author> Ramsey, C. L. and J. J. </author> <title> Grefenstette (1993). Case-based initialization of genetic algorithms. </title> <booktitle> Proc. Fifth Int. Conf. on Genetic Algorithms. </booktitle> <pages> pp. 84-91, </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Schultz, A. C., </author> <title> Using a genetic algorithm to learn strategies for collision avoidance and local navigation. </title> <booktitle> Seventh International Symposium on Unmanned, Untethered, Submersible Technology, </booktitle> <year> 1991, </year> <pages> (pp 213-225). </pages> <address> Durham, NH. </address>
Reference-contexts: show that the longer the epochs last and the longer the system runs and gathers a base of experiences of different environmental cases, the greater the expected benefit of case-based anytime learning is. 5 SUMMARY The SAMUEL system has been used to learn behaviors for controlling simulated autonomous underwater vehicles <ref> (Schultz, 1991) </ref>, missile evasion, and other simulated tasks. This paper reports some early tests of the learned knowledge on a real physical system. For more details of the SAMUEL system, see (Grefenstette and Cobb, 1994). Future work will continue examining the process of building robotic systems through evolution.
Reference: <author> Schultz, A. C. and J. J. </author> <title> Grefenstette (1990). Improving tactical plans with genetic algorithms. </title> <booktitle> Proceedings of IEEE Conference on Tools for AI 90 (pp 328-334). </booktitle> <address> Washington, DC: </address> <publisher> IEEE. </publisher>
Reference-contexts: Previous studies have illustrated that knowledge learned under simulation is robust and might be applicable to the real world if the simulation is more general (i.e. has more noise, more varied conditions, etc.) than the real world environment <ref> (Ramsey, Schultz and Grefenstette, 1990) </ref>. Where this is not possible, it is important to identify the differences between the simulation and the world and measure the effect upon the learning process. <p> Evolutionary methods have found applications that span the range of architectures for intelligent robotics. For example, evolutionary algorithms have been used to learn rule sets for rule-based autonomous agents <ref> (Grefenstette, Ramsey and Schultz, 1990) </ref>, topologies and weights for neural nets for robotic control (Whitley et al, 1993; Yamauchi, 1994), fuzzy logic control systems (Karr, 1991), programs for LISP-controlled robots (Koza, 1992), and rules for behavior-based robots (Dorigo and Schnepf, 1993). <p> Each trial begins with a different configuration of the obstacles in the room. The robot was given 80 decision time steps to cross the room to the goal position. The initial heterogeneous population <ref> (Schultz and Grefenstette, 1990) </ref> consisted of a variety of rule sets from different sources. This included a combination of manually (human) generated rules sets and automatically generated variants of those rules. <p> This must be done with care, however, since a lack of sufficient diversity in the initial population is almost guaranteed to produced premature convergence to suboptimal solutions. In SAMUEL, the rule representation was designed to encourage the user to include heuristic strategies in the initial population <ref> (Schultz and Grefenstette, 1990) </ref>. In fact, for many complex robotic tasks, it is unlikely that the system will be able to evolve solutions without some initial heuristics.
Reference: <author> Whitley, D., S. Dominic, R. Das, C. </author> <title> Anderson (1993). Genetic reinforcement learning for neurocontrol problems. </title> <booktitle> Machine Learning 13(2/3), </booktitle> <pages> 259-284. </pages>
Reference: <author> Yamauchi, B. </author> <year> (1994). </year> <title> Dynamic neural networks for mobile robot control. </title> <address> ISRAM 94. </address>
References-found: 18

