URL: http://www-ai.ijs.si/DunjaMladenic/papers/ICL/atrdbias.ps
Refering-URL: http://www-ai.ijs.si/DunjaMladenic/bibICL.html
Root-URL: 
Email: E-mail: Dunja.Mladenic@ijs.si  
Phone: Phone: (+386)(61) 1259 199, Fax: (+386)(61) 1258 058  
Title: Declarative bias in Atris rule induction shell  
Author: Dunja Mladenic J. Stefan 
Address: Jamova 39, 61000 Ljubljana, Slovenia  
Affiliation: Institute Artificial Intelligence Laboratory  
Abstract: The paper presents Atris rule induction shell in the light of its declarative biases. Atris was developed to enable simple use of different combinatorial optimisation algorithms and noise-handling mechanisms, to enable simple "cross validation" experiments, experiments with different attributes considered as class and different attribute subsets used for learning. Atris currently incorporates seven local optimisation algorithms and four heuristic estimates, enables use of simple built-in background knowledge, use of pruning parameters and validation of the induced hypothesis.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Cestnik, B., Kononenko, I. and Bratko, I. </author> <title> (1987) ASSISTANT 86: A knowledge elicitation tool for sophisticated users. </title> <editor> In Bratko, I. and Lavrac, N. (eds.) </editor> <booktitle> Progress in machine learning, </booktitle> <pages> pp. 31-45. </pages> <address> Wilmslow, </address> <publisher> Sigma Press. </publisher>
Reference-contexts: There is additional validation bias applied on each rule at the end of its construction that is similar to the informativity pre-pruning parameter in the Assistant algorithm <ref> [1] </ref>. This bias is given by the user in the form of the lowest bound of an accepted rule quality. <p> Stochastic mechanism used at the beginning of rule induction makes it possible for Atris. Validation of the induced hypothesis Atris performs by calculating classification accuracy on training, testing, or all (training and testing) examples <ref> [1] </ref>. Example set for this validation is specified by the user in the domain definition. 3 Conclusion Atris rule induction shell enables user to represent several control information in a declarative way.
Reference: [2] <author> Lavrac, N., Dzeroski, S. and Grobelnik, M. </author> <title> (1991) Learning nonrecursive definitions of relations with LINUS. </title> <booktitle> Proc. of EWSL 91, </booktitle> <pages> pp. 265-281. </pages> <address> Porto, Portugal, </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Rules are of the form if Complex then Class, where Complex is a conjunction of selectors and Selector relates an attribute to a value or a disjunction (so-called internal disjunction) of values. Using the idea from the Linus relation learning system <ref> [2] </ref>, Atris enables use of simple built-in background knowledge. Eg. equality: generate new attribute from pair of attributes of the same or compatible type using equality of attribute values. 2.2 Search bias Inductive learning can be defined as search of the space of concept descriptions guided by some heuristic function.
Reference: [3] <author> Mladenic, D. </author> <title> (1993) Combinatorial optimization in inductive concept learning. </title> <booktitle> In Machine Learning Proceedings of the Tenth International Conference, </booktitle> <address> (P.E.Utgoff, </address> <publisher> ed.), </publisher> <pages> pp. 205-211. </pages> <publisher> Morgan Kaufmann Pubishers, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: Deterministic algorithms more or less systematically search neighbourhood, always moving to a better position and thus can be used for "fine-tuning" of the current solution. In our experiments <ref> [3] </ref> the best results were achieved by the combinations of stochastic algorithm (learning phase) and deterministic algorithm (post-processing phase). 2 Each local combinatorial optimisation algorithm uses some cost function to guide the search. Atris incorporates 4 heuristic estimates that are used as cost functions in the above algorithms.
Reference: [4] <author> Mladenic, D. </author> <title> (1993) FUN: UNiversal domain Formalism (In Slovene), </title> <type> Technical report, </type> <institution> J. Stefan Institute, IJS DP-6845, Ljubljana, </institution> <year> 1993. </year>
Reference-contexts: In Section 2 Atris is described in the light of each of its declarative bias. Section 3 concludes with summary and some ideas for the further work on declarative bias in Atris. 1 2 Declarative bias in ATRIS The input to Atris is domain described in FUN format <ref> [4] </ref> that enables multidimensional visualisation of domain data and, representation of control knowledge in a declarative way. In a domain definition, experiments are described by means of training and testing examples, class and attributes, value types and declarative biases.
Reference: [5] <author> Papadimitriou, C.H. and Steiglitz, K. </author> <title> (1982) Optimization problems. In Combinatorial optimization: </title> <booktitle> Algorithms and complexity, </booktitle> <pages> pp. 1-24. </pages> <publisher> Prentice-Hall, Inc. </publisher> <address> Englewood Cliffs </address>
Reference-contexts: Both stochastic algorithms are some sort of Simulated annealing [6]. The method of Simulated annealing is based on the analogy with thermodynamics; (for slowly cooled systems, nature is able to find the minimum energy state). For a selected cost function <ref> [5] </ref> which is to be minimised, the method performs stochastic search in a discrete state space. All five incorporated deterministic algorithms are variants of k-opt algorithm [7]. In general, k-opt algorithm have some sort of k-size window inside which deterministic search is performed.
Reference: [6] <author> Press, W.H., Flannery, B.P., Teukolsky, S.A. and Vetterling, W.T. </author> <title> (1987) Combinatorial minimization: Method of simulated annealing. In Numerical Recipes: </title> <booktitle> The art of scientific computing, </booktitle> <pages> pp. 326-334. </pages> <publisher> Cambridge University Press. </publisher>
Reference-contexts: Atris examples are described by attribute value tuples and induced knowledge is represented by a language of if-then rules. Learning is performed inside the covering paradigm where for a single rule induction, local combinatorial optimisation algorithm is used. Atris currently incorporates stochastic local optimisation algorithms (based on Simulated annealing <ref> [6] </ref>) and deterministic local optimisation algorithms (k-opt, known from solving the "travelling salesman" problem [7]). Stochastic mechanism used at the beginning of each rule induction enables Atris to escape from the unpromising parts of the search space. The whole search process is guided by a heuristic function. <p> Atris perform this search by some local combinatorial optimisation algorithm. It currently incorporates 7 local combinatorial optimisation algorithms; two stochastic and five deterministic. Both stochastic algorithms are some sort of Simulated annealing <ref> [6] </ref>. The method of Simulated annealing is based on the analogy with thermodynamics; (for slowly cooled systems, nature is able to find the minimum energy state). For a selected cost function [5] which is to be minimised, the method performs stochastic search in a discrete state space.
Reference: [7] <author> Syslo, M.M., Deo, N. and Kowalik, J.S. </author> <title> (1983) Local search heuristics. </title> <booktitle> In Discrete optimization algorithms, </booktitle> <pages> pp. 368-380. </pages> <publisher> Prentice-Hall. Inc. </publisher> <address> Englewood Cliffs 4 </address>
Reference-contexts: Learning is performed inside the covering paradigm where for a single rule induction, local combinatorial optimisation algorithm is used. Atris currently incorporates stochastic local optimisation algorithms (based on Simulated annealing [6]) and deterministic local optimisation algorithms (k-opt, known from solving the "travelling salesman" problem <ref> [7] </ref>). Stochastic mechanism used at the beginning of each rule induction enables Atris to escape from the unpromising parts of the search space. The whole search process is guided by a heuristic function. <p> For a selected cost function [5] which is to be minimised, the method performs stochastic search in a discrete state space. All five incorporated deterministic algorithms are variants of k-opt algorithm <ref> [7] </ref>. In general, k-opt algorithm have some sort of k-size window inside which deterministic search is performed. Atris learning process can be divided into two phases: construction of a hypothesis and its post-processing. For the first phase, each of the seven optimisation algorithms can be selected.
References-found: 7

