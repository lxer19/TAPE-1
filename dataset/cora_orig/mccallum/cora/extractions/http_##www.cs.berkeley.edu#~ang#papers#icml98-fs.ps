URL: http://www.cs.berkeley.edu/~ang/papers/icml98-fs.ps
Refering-URL: http://www.cs.berkeley.edu/~ang/
Root-URL: http://www.cs.berkeley.edu/~ang/
Email: ayn@ai.mit.edu  
Title: On Feature Selection: Learning with Exponentially many Irrelevant Features as Training Examples  
Author: Andrew Y. Ng 
Address: Cambridge, MA 02139  
Affiliation: Artificial Intelligence Laboratory Massachusetts Institute of Technology  
Abstract: We consider feature selection in the "wrapper" model of feature selection. This typically involves an NP-hard optimization problem that is approximated by heuristic search for a "good" feature subset. First considering the idealization where this optimization is performed exactly, we give a rigorous bound for generalization error under feature selection. The search heuristics typically used are then immediately seen as trying to achieve the error given in our bounds, and succeeding to the extent that they succeed in solving the optimization. The bound suggests that, in the presence of many "irrelevant" features, the main source of error in wrapper model feature selection is from "overfit-ting" hold-out or cross-validation data. This motivates a new algorithm that, again under the idealization of performing search exactly, has sample complexity (and error) that grows logarithmically in the number of "irrelevant" features which means it can tolerate having a number of "irrelevant" features exponential in the number of training examples and search heuristics are again seen to be directly trying to reach this bound. Experimental results on a problem using simulated data show the new algorithm having much higher tolerance to irrelevant features than the standard wrapper model. Lastly, we also discuss ramifications that sample complexity logarithmic in the number of irrelevant features might have for feature design in actual applications of learning. 
Abstract-found: 1
Intro-found: 1
Reference: [ Almuallim and Dietterich, 1994 ] <author> Almuallim, H. and Dietterich, T. </author> <year> (1994). </year> <title> Learning boolean concepts in the presence of many irrelevant features. </title> <journal> Artificial Intelligence, 69(1-2):279-305. </journal>
Reference: [ Caruana and Frietag, 1994 ] <author> Caruana, R. and Frietag, D. </author> <year> (1994). </year> <title> Greedy attribute selection. </title> <booktitle> In Proceed ings of the Eleventh International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Garey and Johnson, 1979 ] <author> Garey, M. R. and John-son, D. S. </author> <year> (1979). </year> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. </title> <publisher> Freeman. </publisher>
Reference-contexts: Note that in performing the search, enumeration over all the 2 f possible feature sets is usually intractable, and there is no known algorithm for otherwise performing this optimization tractably. Indeed, the Feature Selection problem in general is NP-hard <ref> [ Garey and Johnson, 1979 ] </ref> , but much work over recent years has developed a large number of heuristics for performing this search efficiently. (Again, the literature is too wide to survey here, but examples include [ Moore and Lee, 1994, Caruana and Frietag, 1994, Yang and Hoavar, 1997 ]
Reference: [ John et al., 1994 ] <author> John, G., Kohavi, R., and Pfleger, K. </author> <year> (1994). </year> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Using the terminology introduced by <ref> [ John et al., 1994 ] </ref> , feature selection algorithms broadly fall into the "filter" and the "wrapper" models. The filter model relies on general characteristics of the training 1 Aside from good generalization error, other goals of feature selection might be user-interpretability and parsimony of hypotheses for fast prediction. <p> x2D X [h (x) 6= c (x)] (where the dependence of "(h) on D X has been suppressed for notational brevity,) and the empirical error on a set of data S as ^" S (h) = 1 2.2 The wrapper model In the wrapper model of feature selection suggested by <ref> [ John et al., 1994 ] </ref> , we are given a learning algorithm L that, for any set of features F , takes a training set Sj F , and outputs a hypothesis h : Xj F 7! f0; 1g.
Reference: [ Kearns, 1996 ] <author> Kearns, M. J. </author> <year> (1996). </year> <title> A bound on the error of Cross Validation using the approximation and estimation rates, with consequences for the training-test split. </title> <booktitle> In Advances in Neural Information Processing Systems 8, </booktitle> <pages> pages 183-189. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In this paper, we study only the wrapper model of feature selection, and largely in the context of classification. Our analysis is largely inspired by <ref> [ Kearns, 1996 ] </ref> , with our theoretical results heavily based on the techniques given there and those outlined in [ Kearns et al., 1997 ] . <p> This closely ties in with the learning framework studied by [ Vapnik, 1982 ] , and is also used in <ref> [ Kearns, 1996 ] </ref> and [ Kearns et al., 1997 ] in proving bounds on generalization error. <p> for performing this search efficiently. (Again, the literature is too wide to survey here, but examples include [ Moore and Lee, 1994, Caruana and Frietag, 1994, Yang and Hoavar, 1997 ] , and [ Langley, 1994, Miller, 1990 ] include overviews.) In this development, we will, in the style of <ref> [ Kearns, 1996 ] </ref> , give bounds for generalization error when this optimization is performed exactly. Of course, the extent to which our bounds predict actual performance will in part depend on the extent to which the optimization algorithms succeed in performing this search on "real life" distributions of data. <p> this is a bound for learning from noiseless data; when the training data labels have independently been corrupted at some noise rate j, the second term in the bound becomes O iq (12j) 2 m (log m j Bound for performance of wrapper model Applying the proof technique given in <ref> [ Kearns, 1996 ] </ref> (used to bound the error of hold-out) to feature selection, we obtain the following theorem: Theorem 2 Given L; S; fl, the hypothesis ^ h output by standard-wrap, given by ^ h = L (S 0 j ^ F ) where ^ F = argmin F ^" <p> All experimental results reported here are averages of 200 independent trials. For both algorithms, the hold-out fraction fl is a parameter that had to be chosen. The analysis of <ref> [ Kearns, 1996 ] </ref> suggests that, for a wide range of hold-out testing applications, fl 0:3 is a good choice (though it is unclear standard-wrap would fall into his framework). Using this as an initial choice for fl, we obtain Figure 1, as we vary the total number of features.
Reference: [ Kearns et al., 1997 ] <author> Kearns, M. J., Mansour, Y., Ng, A. Y., and Ron, D. </author> <year> (1997). </year> <title> An experimental and theoretical comparison of model selection methods. </title> <journal> Machine Learning Journal, </journal> <volume> 27(1) </volume> <pages> 7-50. </pages>
Reference-contexts: In this paper, we study only the wrapper model of feature selection, and largely in the context of classification. Our analysis is largely inspired by [ Kearns, 1996 ] , with our theoretical results heavily based on the techniques given there and those outlined in <ref> [ Kearns et al., 1997 ] </ref> . <p> This closely ties in with the learning framework studied by [ Vapnik, 1982 ] , and is also used in [ Kearns, 1996 ] and <ref> [ Kearns et al., 1997 ] </ref> in proving bounds on generalization error. We believe it to be a very natural model, and that it is a rich enough class of learning algorithms to merit detailed study. (But also see [ Kearns et al., 1997 ] for comments regarding relations to learning <p> , and is also used in [ Kearns, 1996 ] and <ref> [ Kearns et al., 1997 ] </ref> in proving bounds on generalization error. We believe it to be a very natural model, and that it is a rich enough class of learning algorithms to merit detailed study. (But also see [ Kearns et al., 1997 ] for comments regarding relations to learning algorithms that do not exactly do this; for example, it is not difficult to derive rigorous generalizations of all of our results if L manages to only approximately minimize training error.) More formally, for any feature set F ,
Reference: [ Kearns and Ron, 1997 ] <author> Kearns, M. J. and Ron, D. </author> <year> (1997). </year> <title> Algorithmic stability and sanity-check bounds for leave-one-out cross-validation. </title> <booktitle> In Proceedings of the Tenth Annual Conference on Computational Learning Theory. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: But as they asymptotically yield at best small-constant improvements over using hold-out and as leave-one-out is at worst little better than training error in estimating generalization error, while rendering the algorithm's performance much less tractable to analysis <ref> [ Kearns and Ron, 1997 ] </ref> , we will not explicitly consider them here, though we believe our results will be suggestive of the performance of these schemes as well.
Reference: [ Kivinen and Warmuth, 1994 ] <author> Kivinen, J. and War-muth, M. K. </author> <year> (1994). </year> <title> Exponentiated gradient versus gradient descent for linear predictors. </title> <type> Technical Report UCSC-CRL-94-16, </type> <institution> Univ. of California Santa Cruz, Computer Research Laboratory. </institution>
Reference-contexts: Likewise, the EG algorithm for linear regression with quadratic error also has such loss (and indeed sample complexity) that grows logarithmically in the number of irrelevant features <ref> [ Kivinen and Warmuth, 1994 ] </ref> . <p> However, there have been difficulties with a number of definitions of "relevance" [ Kohavi and John, 1997 ] , and we take the alternative view, which is quite similar in flavor to those in [ Littlestone, 1988 ] and <ref> [ Kivinen and Warmuth, 1994 ] </ref> , of the goal of feature selection as this: If there exists a hypothesis that, using only a "small" number of features, gives good generalization error, then we want our classifier to achieve close to this level of performance with high probability.
Reference: [ Kohavi and John, 1997 ] <author> Kohavi, R. and John, G. H. </author> <year> (1997). </year> <title> Wrappers for feature subset selection. </title> <journal> Artificial Intelligence, </journal> <volume> 97 </volume> <pages> 273-324. </pages>
Reference-contexts: Next, the notion of "relevance" is closely related to feature selection. Intuitively, one goal of feature selection is to eliminate all but a small set of "relevant" features, which are then given to an induction algorithm. However, there have been difficulties with a number of definitions of "relevance" <ref> [ Kohavi and John, 1997 ] </ref> , and we take the alternative view, which is quite similar in flavor to those in [ Littlestone, 1988 ] and [ Kivinen and Warmuth, 1994 ] , of the goal of feature selection as this: If there exists a hypothesis that, using only a
Reference: [ Kohavi and Sommerfield, 1995 ] <author> Kohavi, R. and Som-merfield, D. </author> <year> (1995). </year> <title> Feature subset selection using the wrapper model: Overfitting and dynamic search space topology. </title> <booktitle> In Proceedings of the First International Conference on Knowledge Discovery and Data Mining. </booktitle>
Reference-contexts: Its increase with f reflects the fact that we are testing a set of hypotheses of size exponential in f, and that there is potential for "overfitting" the flm holdout samples. (In the context of feature selection, the issue of overfitting of hold-out data was also raised by <ref> [ Kohavi and Sommerfield, 1995 ] </ref> ; see also [ Ng, 1997 ] for a detailed discussion of overfitting of hold-out data in hypothesis selection.) But since this is a worst-case bound, it holds in particular for the "bad case" where all 2 f hypotheses are "very different" from each other.
Reference: [ Langley, 1994 ] <author> Langley, P. </author> <year> (1994). </year> <title> Selection of relevant features in machine learning. </title> <booktitle> In Proceedings of the AAAI Fall Symposium on Relevance. </booktitle> <publisher> AAAI Press. </publisher>
Reference-contexts: 1 Introduction In recent years, Feature Selection for classification and regression has been enjoying increasing interest in the Machine Learning community. Impressive performance gains have been reported by numerous authors, and numerous feature subset search heuristics have been proposed. (The literature is too wide to survey here, but see <ref> [ Langley, 1994 ] </ref> and [ Miller, 1990 ] for overviews.) In view of these significant empirical successes, one central question is: What theoretical justification is there for feature selection? For example, in parametric function approximation schemes such as linear regression, it is often the case that excluding a feature is <p> While the wrapper model tends to be more computationally expensive, it also unsurprisingly tends to find feature sets better suited to the inductive biases of our learning algorithm, and tends to give superior performance <ref> [ Langley, 1994 ] </ref> . In this paper, we study only the wrapper model of feature selection, and largely in the context of classification. <p> Johnson, 1979 ] , but much work over recent years has developed a large number of heuristics for performing this search efficiently. (Again, the literature is too wide to survey here, but examples include [ Moore and Lee, 1994, Caruana and Frietag, 1994, Yang and Hoavar, 1997 ] , and <ref> [ Langley, 1994, Miller, 1990 ] </ref> include overviews.) In this development, we will, in the style of [ Kearns, 1996 ] , give bounds for generalization error when this optimization is performed exactly.
Reference: [ Littlestone, 1988 ] <author> Littlestone, N. </author> <year> (1988). </year> <title> Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318. </pages>
Reference-contexts: is: How does the performance of feature selection scale with the number of irrelevant features? The Winnow algorithm of Littlestone for learning Boolean monomials, or more generally also k-DNF formulae and r-of-k threshold functions (over boolean inputs), from noiseless data enjoys worst-case loss logarithmic in the number of irrelevant features <ref> [ Littlestone, 1988 ] </ref> . Likewise, the EG algorithm for linear regression with quadratic error also has such loss (and indeed sample complexity) that grows logarithmically in the number of irrelevant features [ Kivinen and Warmuth, 1994 ] . <p> However, there have been difficulties with a number of definitions of "relevance" [ Kohavi and John, 1997 ] , and we take the alternative view, which is quite similar in flavor to those in <ref> [ Littlestone, 1988 ] </ref> and [ Kivinen and Warmuth, 1994 ] , of the goal of feature selection as this: If there exists a hypothesis that, using only a "small" number of features, gives good generalization error, then we want our classifier to achieve close to this level of performance with
Reference: [ McCullagh and Nelder, 1989 ] <author> McCullagh, P. and Nelder, J. A. </author> <year> (1989). </year> <title> Generalized Linear Models (second edition). </title> <publisher> Chapman and Hall. </publisher>
Reference-contexts: To test this hypothesis, we ran both algorithms on a small, artificial feature selection problem. The learning algorithm used was logistic regression <ref> [ McCullagh and Nelder, 1989 ] </ref> , used to fit a linear discriminant function, and which, while not minimizing training error, approximates that reasonably.
Reference: [ Miller, 1990 ] <author> Miller, A. J. </author> <year> (1990). </year> <title> Subset Selection in Regression. </title> <publisher> Chapman and Hall. </publisher>
Reference-contexts: Impressive performance gains have been reported by numerous authors, and numerous feature subset search heuristics have been proposed. (The literature is too wide to survey here, but see [ Langley, 1994 ] and <ref> [ Miller, 1990 ] </ref> for overviews.) In view of these significant empirical successes, one central question is: What theoretical justification is there for feature selection? For example, in parametric function approximation schemes such as linear regression, it is often the case that excluding a feature is mathematically identical to setting the <p> Johnson, 1979 ] , but much work over recent years has developed a large number of heuristics for performing this search efficiently. (Again, the literature is too wide to survey here, but examples include [ Moore and Lee, 1994, Caruana and Frietag, 1994, Yang and Hoavar, 1997 ] , and <ref> [ Langley, 1994, Miller, 1990 ] </ref> include overviews.) In this development, we will, in the style of [ Kearns, 1996 ] , give bounds for generalization error when this optimization is performed exactly. <p> The search heuristic was beam search/forward search (starting out with the empty set of features, and incrementally adding features until we have the full set of features). Forward search is a popular choice that appears to usually do well <ref> [ Miller, 1990 ] </ref> , and beam search, with a beam width of 50 in our case, should be a strict improvement. (Notice also that, while ordered-fs was originally formulated as consisting of f + 1 separate searches, it is probably most naturally implemented as carrying out all the searches "together"; <p> Indeed, we believe that much work remains to be done on this field, perhaps particularly in designing algorithms for finding feature subsets that minimize training error such as ordered-fs requires; for example, we have very efficient algorithms for performing forward and backward search for linear regression <ref> [ Miller, 1990 ] </ref> , but few generalizations or fast approximations thereof to other algorithms.
Reference: [ Moore and Lee, 1994 ] <author> Moore, A. W. and Lee, M. S. </author> <year> (1994). </year> <title> Efficient algorithms for minimizing cross validation error. </title> <booktitle> In Proceedings of the 11th International Conference on Machine Learning. </booktitle>
Reference: [ Ng, 1997 ] <author> Ng, A. Y. </author> <year> (1997). </year> <title> Preventing "overfitting" of Cross-Validation data. </title> <booktitle> In Proceedings of the Fourteenth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: that we are testing a set of hypotheses of size exponential in f, and that there is potential for "overfitting" the flm holdout samples. (In the context of feature selection, the issue of overfitting of hold-out data was also raised by [ Kohavi and Sommerfield, 1995 ] ; see also <ref> [ Ng, 1997 ] </ref> for a detailed discussion of overfitting of hold-out data in hypothesis selection.) But since this is a worst-case bound, it holds in particular for the "bad case" where all 2 f hypotheses are "very different" from each other. <p> the performance suggested by our bounds. (However, one other surprising effect not modeled by our bounds and which deserves mention is that when standard-wrap is "badly" overfitting the hold-out data, then our earlier work suggests that even randomly throwing some subset of the 2 f hypotheses away may improve performance <ref> [ Ng, 1997 ] </ref> . This suggests that in such somewhat-degenerate cases, using a weaker search heuristic may actually be helpful.
Reference: [ Vapnik, 1982 ] <author> Vapnik, V. N. </author> <year> (1982). </year> <title> Estimation of dependencies based on empirical data. </title> <publisher> Springer Ver-lag. </publisher>
Reference-contexts: Our analysis is largely inspired by [ Kearns, 1996 ] , with our theoretical results heavily based on the techniques given there and those outlined in [ Kearns et al., 1997 ] . We also rely heavily on tools from <ref> [ Vapnik, 1982 ] </ref> , that give a very general framework for bounding the deviation of training error from generalization error. 2 Preliminaries 2.1 Feature Selection Let X be the fixed f -dimensional input space, where f is the number of features in the inputs we are provided. <p> For this analysis, therefore, we make the (rather strong) assumption that given a particular data set Sj F , L chooses the hypothesis h from some class of hypotheses (shortly to be formalized) so as to minimize training error. This closely ties in with the learning framework studied by <ref> [ Vapnik, 1982 ] </ref> , and is also used in [ Kearns, 1996 ] and [ Kearns et al., 1997 ] in proving bounds on generalization error. <p> For the remainder of this paper, we will implicitly assume L meets these two assumptions that it treats features "uniformly," and that it minimizes training error over H jF j . One more definition we need is to let r VC be the Vapnik-Chervonenkis dimension <ref> [ Vapnik and Chervo-nenkis, 1971, Vapnik, 1982 ] </ref> of the hypothesis class H r . Normally, we expect 0 VC &lt; 1 VC &lt; 2 VC &lt; , though this is not an assumption we use. <p> Now, for any fixed r, we want to uniformly bound the deviation of training error from generalization error for all r hypotheses that use exactly r features. Taking a standard union bound (see <ref> [ Vapnik, 1982 ] </ref> ), we replace (1=m) log (1=ffi) with (1=m) log r =ffi , which (noting log f gives the second term.
Reference: [ Vapnik and Chervonenkis, 1971 ] <author> Vapnik, V. N. and Chervonenkis, A. Y. </author> <year> (1971). </year> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theory of Probability and its Applications, </journal> <volume> 16(2) </volume> <pages> 264-280. </pages>
Reference-contexts: Bound for performance without feature selec tion The Universal Estimate Rate bound of Vapnik and Chervonenkis <ref> [ Vapnik and Chervonenkis, 1971, Vap-nik, 1982 ] </ref> gives a bound on generalization error when learning using all f features without feature selection.
Reference: [ Yang and Hoavar, 1997 ] <author> Yang, J. and Hoavar, V. </author> <year> (1997). </year> <title> Feature subset selection using a genetic algorithm. In IEEE Expert (Special Issue on Feature Transformation and Subset Selection). </title> <publisher> In press. </publisher>
References-found: 19

