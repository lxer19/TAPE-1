URL: http://www-sal.cs.uiuc.edu/~pitt/Papers/alt97.ps
Refering-URL: http://www-sal.cs.uiuc.edu/~pitt/
Root-URL: http://www.cs.uiuc.edu
Title: On Exploiting Knowledge and Concept Use in Learning Theory  
Author: Leonard Pitt 
Address: Urbana, IL, 61801, USA  
Affiliation: Department of Computer Science, University of Illinois,  
Abstract: In the past fifteen years, various formal models of concept learning have successfully been employed to answer the question of what types of concepts can be efficiently inferred from examples. The answer appears to be "only simple ones". Perhaps due to the ease of formal analysis, our investigations have focused on learning artificial, syntactically-described concepts in "sterile", knowledge-free environments. We discuss analogous results from the literature on human concept learning (people don't do too well either), and review current theories as to how people are able to more effectively learn in the presence of background knowledge and the discovery of information via execution of tasks related to the concept acquisition process. We consider the formal modeling of such phenomena as an important challenge for learning theory.
Abstract-found: 1
Intro-found: 1
Reference: <author> Aizenstein, H. & Pitt, L. </author> <year> (1991). </year> <title> Exact learning of read-twice DNF formulas. </title> <booktitle> In Proc. 32nd Annu. IEEE Sympos. Found. Comput. Sci., </booktitle> <pages> pp. 170-179. </pages> <publisher> IEEE Computer Society Press. </publisher>
Reference: <author> Aizenstein, H., Hegedus, T., Hellerstein, L., & Pitt, L. </author> <title> (in press). Complexity-theoretic hardness results for query learning. </title> <note> To appear, computational complexity. </note>
Reference: <author> Angluin, D. </author> <year> (1987). </year> <title> Learning regular sets from queries and counterexamples. </title> <journal> Information and Computation, </journal> <volume> 75, </volume> <pages> 87-106. </pages>
Reference: <author> Angluin, D. </author> <year> (1988). </year> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <pages> 319-342. </pages>
Reference: <author> Angluin, D., Frazier, M. & Pitt, L. </author> <year> (1992). </year> <title> Learning conjunctions of Horn clauses. </title> <journal> Machine Learning, </journal> <volume> 9, </volume> <pages> 147-164. </pages>
Reference: <author> Angluin, D., Hellerstein, L., & Karpinski, M. </author> <year> (1993). </year> <title> Learning read-once formulas with queries. </title> <journal> Journal of the ACM, </journal> <volume> 40, </volume> <pages> 185-210. </pages>
Reference: <author> Angluin, D. & Kharitonov, M. </author> <year> (1995). </year> <title> When won't membership queries help? Journal of Computer and System Sciences, </title> <type> 50. </type>
Reference: <author> Angluin, D., and Slonim, D. </author> <year> (1994). </year> <title> Randomly fallible teachers: Learning monotone DNF with an incomplete membership oracle Machine Learning, </title> <booktitle> 14(1) </booktitle> <pages> 7-26. </pages>
Reference: <author> Barsalou, L.W. </author> <year> (1983). </year> <title> Ad hoc categories. </title> <journal> Memory & Cognition, </journal> <volume> 11, </volume> <pages> 211-227. </pages>
Reference: <author> Baum, E., and Haussler, D. </author> <year> (1989). </year> <title> What Size Net Gives Valid Generalization? `Neural Computation, </title> <booktitle> 1, </booktitle> <pages> 151-160. </pages>
Reference: <author> Ben-David, S. & Dichterman, E. </author> <year> (1993). </year> <title> Learning with restricted focus of attention, </title> <booktitle> Proc. 6th Annu. Workshop on Comput. Learning Theory, </booktitle> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <pages> 287-296. </pages>
Reference: <author> Ben-David, S. & Dichterman, E. </author> <year> (1994). </year> <title> Learnability with restricted focus of attention guarantees noise-tolerance. </title> <booktitle> Proc. 5th Int. Workshop on Algorithmic Learning Theory, </booktitle> <publisher> Springer-Verlag, </publisher> <pages> 248-259. </pages>
Reference: <author> Billman, D. & Knutson, J. F. </author> <year> (1996). </year> <title> Unsupervised concept learning and value systematicity: A complex whole aids learning the parts. J. Experimental Psychology: Learning, Memory, </title> <journal> and Cognition, </journal> <volume> 22, </volume> <pages> 458-475. </pages>
Reference: <author> Blum, A., Hellerstein, L., Littlestone, N. </author> <year> (1995). </year> <title> Learning in the Presence of Finitely or Infinitely Many Irrelevant Attributes. </title> <journal> JCSS, </journal> <volume> 50, </volume> <pages> 32-40. </pages>
Reference-contexts: Yet even this simple setting (Horn rules, conjunctive concept) suggests interesting extensions: Is there an algorithm that learns in time that depends only logarithmically on the number of irrelevant features? What if the feature space is infinite? Notice that this model is quite different than the infinite attribute space model <ref> (Blum et al., 1995) </ref> because in the latter case, a positive example must contain features relevant for classifica tion. The example above addresses only how some features present in an example can bring attention to other, missing ones, via deduction using a theory.
Reference: <author> Blum, A., Chalasani, P., Goldman, S., and Slonim, D. </author> <year> (1995). </year> <title> Learning with unreliable boundary queries. </title> <booktitle> Proc. 8th Annu. Conf. on Comput. Learning Theory. </booktitle> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <pages> 98-107. </pages>
Reference-contexts: Yet even this simple setting (Horn rules, conjunctive concept) suggests interesting extensions: Is there an algorithm that learns in time that depends only logarithmically on the number of irrelevant features? What if the feature space is infinite? Notice that this model is quite different than the infinite attribute space model <ref> (Blum et al., 1995) </ref> because in the latter case, a positive example must contain features relevant for classifica tion. The example above addresses only how some features present in an example can bring attention to other, missing ones, via deduction using a theory.
Reference: <author> Blumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M.K. </author> <year> (1987). </year> <title> Occam's razor. </title> <journal> Infonnation Processing Letters, </journal> <volume> 24, </volume> <pages> 377-380. </pages>
Reference: <author> Blumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M.K. </author> <year> (1989). </year> <title> Learnabil-ity and the Vapnik-Chervonenkis dimension. </title> <journal> J. ACM, </journal> <volume> 36, </volume> <pages> 929-965. </pages>
Reference-contexts: can outperform humans in such artificial learning tasks (for example, learning a conjunction or a disjunction of only 3 variables from among several hundred), it has been shown that even when the VC-dimension remains small (hence, there is sufficient information in a small number of examples to deduce the concept) <ref> (Blumer et al., 1989) </ref>, concepts that have anything approaching a complicated, nested structure cannot be PAC-learned, unless widely-held beliefs from cryp tography and the theory of computational complexity break down (Kearns & Valiant, 1994; Pitt & Warmuth, 1990).
Reference: <author> Bruner, J. S., Goodnow, J. J. & Austin G. A. </author> <year> (1956). </year> <title> A study of thinking. </title> <address> New Yourk: </address> <publisher> Wiley. </publisher>
Reference-contexts: The goal is usually to learn some conjunctive concept defined over a very few (3 or 4) dimensions, each of which may take on only a few (2, 3, 4) possible values. It is well accepted that people learn conjunctive categories more readily than disjunctive ones <ref> (Bruner, Goodnow & Austin, 1956) </ref>, so this has been the focus of much research.
Reference: <editor> Bshouty, </editor> <publisher> N.H. </publisher> <year> (1995). </year> <title> Exact Learning via the monotone theory. </title> <journal> Information and Computation, </journal> <volume> 123, </volume> <pages> 146-153. </pages>
Reference: <author> Chapman, L. J. & Chapman, J. P. </author> <year> (1967). </year> <title> Genesis of popular but erroneous psycho-diagnostic observations. </title> <journal> J. of Abnormal Psychology, </journal> <volume> 72, </volume> <pages> 193-204. </pages>
Reference: <author> Chen, Z. & Maass, W. </author> <year> (1992). </year> <title> On-line learning of rectangles. </title> <booktitle> In Proc. 5th Annual Workshop on Comput. Learning Theory (pp.16-28). </booktitle> <publisher> ACM Press, </publisher> <address> New York, NY. </address>
Reference: <author> Chi, M., Bassok, M., Lewis, M., Reimann, P., & Glaser, R. </author> <year> (1989). </year> <title> Self-explanations: How students study and use examples in learning to solve problems. </title> <journal> Cognitive Science, </journal> <volume> 13, </volume> <pages> 145-182. </pages>
Reference-contexts: The use of the category affected what features were more relevant to the classification. Finally, research in AI (in particular, explanation-based generalization or learning systems (Mitchell et al., 1986 ; Dejong, 1977), and cognitive science <ref> (Chi et al., 1989) </ref>) have provided evidence that explanations associated with the category membership or goal satisfaction provide additional, detailed information as to intermediate concepts or features that might be relevant in solving related tasks.
Reference: <author> Cohen, W.W. & Hirsh, H. </author> <year> (1994). </year> <title> Learnability of description logics with equality constraints. </title> <journal> Machine Learning, </journal> <volume> 17, </volume> <pages> 169-199. </pages>
Reference: <author> DeJong, G. </author> <title> (1997) Explanation-Based Learning. </title> <editor> In A. Tucker (Ed.), </editor> <booktitle> Encyclopedia of Computer Science (pp. </booktitle> <pages> 499-520). </pages> <publisher> CRC Press: </publisher> <address> Boca Raton. </address>
Reference: <author> Ehrenfeucht, A., Haussler, D., Kearns M., & Valiant, L.G. </author> <year> (1989). </year> <title> A general lower bound on the number of examples needed for learning. </title> <journal> Information and Computation, </journal> <volume> 82, </volume> <pages> 247-261. </pages>
Reference: <author> Ehrenfeucht, A., & Haussler, D. </author> <year> (1989). </year> <title> Learning decision trees from random examples. </title> <journal> Information and Computation, </journal> <volume> 82, </volume> <pages> 231-246. </pages>
Reference: <author> Frazier M., Goldman, S., Mishra, N., & Pitt, L. </author> <year> (1996). </year> <title> Learning from a Consistently Ignorant Teacher. </title> <journal> JCSS 52(3): </journal> <pages> 471-492. </pages>
Reference: <author> Frazier, M., & Pitt, L. </author> <year> (1993). </year> <title> Learning from entailment: An application to propositional Hom sentences. </title> <booktitle> In Proc. Tenth International Conf: Machine Learning (pp. </booktitle> <pages> 120-127). </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Frazier, M., & Pitt, L. </author> <year> (1996). </year> <title> CLASSIC Learning. </title> <journal> Machine Learning, </journal> <volume> 25, </volume> <pages> 151-193. </pages>
Reference: <author> Goldman, S. A., & Kearns, M. J. </author> <year> (1995). </year> <title> On the complexity of teaching. </title> <journal> JCSS 50(1): </journal> <pages> 20-31. </pages>
Reference: <author> Goldman, S., & Mathias, D. </author> <year> (1991). </year> <title> Teaching a smarter learner. </title> <booktitle> Proceedings of the Sixth Annual ACM Conference on Computational Learning Theory. </booktitle> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <pages> 67-76. </pages>
Reference: <author> Hampton, J. </author> <year> (1993). </year> <title> Prototype models of concept representation. In Categories and Concepts: Theoretical views and Inductive Data Analysis, </title> <editor> Mechelen, Hamp-ton, Michalski, & Theuns (Eds.), </editor> <booktitle> Cognitive Science Series, </booktitle> <publisher> Academic Press. </publisher>
Reference: <author> Hampton, J. & Dubois, D. </author> <year> (1993). </year> <title> Psychological models of concepts: Introduction. In Categories and Concepts: Theoretical views and Inductive Data Analysis, </title> <editor> Mechelen, Hampton, Michalski, & Theuns (Eds.), </editor> <booktitle> Cognitive Science Series, </booktitle> <publisher> Academic Press. </publisher>
Reference: <author> Haussler, D. </author> <year> (1988). </year> <title> Quantifying inductive bias: AI learning algorithms and Valiant's learning framework. </title> <journal> Artificial Intelligence, </journal> <volume> 36, </volume> <pages> 177-221. </pages>
Reference: <author> Hegedus, T. </author> <year> (1995). </year> <title> Generalized teaching dimensions and the query complexity of learning. </title> <booktitle> Proc. 8th Annu. Conf. on Comput. Learning Theory, </booktitle> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <pages> 108-117. </pages>
Reference: <author> Hovland, C. I., & W. </author> <title> Weiss (1953). Transmission of information concerning concepts through positive and negative instances. </title> <journal> J. Experimental Psychology, </journal> <volume> vol. 45, No. </volume> <pages> 3. </pages>
Reference: <author> Jackson, J. & Tomkins, A. </author> <year> (1992). </year> <title> A computational model of teaching. </title> <booktitle> Proc. 5th Annu. Workshop on Comput. Learning Theory. </booktitle> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <pages> 319-326. </pages>
Reference: <author> Kearns, M. J., and Schapire, R. E. </author> <year> (1994). </year> <title> Efficient Distribution-Free Learning of Probabilistic Concepts. </title> <journal> JCSS 48(3): </journal> <pages> 464-497. </pages>
Reference-contexts: Note that no additional information about either the particular picture, or about the world, would help resolve the issue. The category of furniture is in essence ill-defined. Such vague boundaries are not captured well by traditional rule-based category descriptors. Notice also that the notion of probabilistic concept, or p-concept <ref> (Kearns & Schapire, 1994) </ref> does not capture this phenomenon, although "fuzzy boundary" models may (Angluin & Slonim 1994; Blum et al., 1995; Frazier et al., 1996). The issue of typicality of an example relative to a category has been well investigated.
Reference: <author> Kearns, M. & Valiant, L.G. </author> <year> (1994). </year> <title> Cryptographic limitations on learning Boolean formulae and finite automata. </title> <journal> Journal of the ACM, </journal> <volume> 41, </volume> <pages> 67-95. </pages>
Reference-contexts: Note that no additional information about either the particular picture, or about the world, would help resolve the issue. The category of furniture is in essence ill-defined. Such vague boundaries are not captured well by traditional rule-based category descriptors. Notice also that the notion of probabilistic concept, or p-concept <ref> (Kearns & Schapire, 1994) </ref> does not capture this phenomenon, although "fuzzy boundary" models may (Angluin & Slonim 1994; Blum et al., 1995; Frazier et al., 1996). The issue of typicality of an example relative to a category has been well investigated.
Reference: <author> Keil, F.C. </author> <year> (1989). </year> <title> Concepts, kinds and cognitive development. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Kersten, A. W., & Billman, D. </author> <year> (1997). </year> <title> Event category learning. J. Experimental Psychology: Learning, Memory, </title> <journal> and Cognition, </journal> <volume> 23, </volume> <pages> 638-658. </pages>
Reference: <author> Khardon R. & Roth, D. </author> <year> (1994). </year> <title> Learning to Reason. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 682-687). </pages>
Reference: <author> Kruschke, J. K. </author> <year> (1992). </year> <title> ALCOVE: An exemplar-based connectionist model of category learning. </title> <journal> Psychological Review, </journal> <volume> 99, </volume> <pages> 22-44. </pages>
Reference: <author> Littlestone, N. </author> <year> (1988). </year> <title> Learning when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <pages> 285-318. </pages>
Reference: <author> Matheus, C.J. </author> <year> (1991). </year> <title> The need for constructive induction. </title> <editor> In L. Birnbaum and G. Collins (Eds.), </editor> <booktitle> Proceedings of the Eighth Annual Workshop on Machine Learning. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Mathias, D. H. </author> <year> (1995). </year> <title> DNF if you can't learn 'em, teach 'em: an interactive model of teaching, </title> <booktitle> Proc. 8th Annu. Conf. on Comput. Learning Theory. </booktitle> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <pages> 222-229. </pages>
Reference: <author> Michalski, R.S. </author> <year> (1983). </year> <title> A Theory and Methedology of Inductive Learning. In R.S. </title> <editor> Michalski, J.G. Carbonell, and T.M. Mitchell (Eds.), </editor> <booktitle> Machine Learning: An AI Approach (pp. </booktitle> <pages> 83-134). </pages> <publisher> Tioga: </publisher> <address> Palo Alto. </address>
Reference: <author> Mitchell, T., R. Keller, & S. </author> <month> Kedar-Cabelli </month> <year> (1986). </year> <title> Explanation-Based Generalization: A Unifying View. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 47-80. </pages>
Reference: <author> Muggleton, S., </author> <year> (1992). </year> <title> Inductive Logic Programming. </title> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference: <author> Muggleton, S. & W. </author> <title> Buntine (1988). Machine Invention of First-Order Predicates by Inverting Resolution. </title> <booktitle> In International Conference on Machine Learning (339-352). </booktitle> <address> Ann Arbor: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Murphy, G.L. </author> <year> (1993). </year> <title> Theories and concept formation In Categories and Concepts: Theoretical views and Inductive Data Analysis, </title> <editor> Mechelen, Hampton, Michal-ski, & Theuns (Eds.), </editor> <booktitle> Cognitive Science Series, </booktitle> <publisher> Academic Press. </publisher>
Reference: <author> Murphy, G.L., & Allopenna, P.D. </author> <year> (1994). </year> <title> The locus of knowledge effects in concept learning. Journal of Experimental Psychology: Learning, Memory, </title> <journal> and Cognition, </journal> <volume> 20, </volume> <pages> 904-919. </pages>
Reference: <author> Murphy, G.L., & Medin, D.L. </author> <year> (1985). </year> <title> The role of theories in conceptual coher-ence. </title> <journal> Psychological Review, </journal> <volume> 92, </volume> <pages> 289-316. </pages>
Reference: <author> Murphy, G.L., & Spalding, T.L. </author> <year> (1995). </year> <title> Knowledge, similarity, and concept formation. </title> <journal> Psychological Belgica, </journal> <pages> 35-2/3, 127-144. </pages>
Reference: <author> Neisser, U., & Weene, P. </author> <year> (1962). </year> <title> Hierarchies in concept attainment. </title> <journal> Journal of Experimental Psychology, </journal> <volume> vol. 64, no. 6, </volume> <pages> 640-645. </pages> <editor> Nosofsky, R. M. </editor> <year> (1992). </year> <title> Eemplars, prototypes, and similarity rules. </title> <editor> In A. F. Healy, S. M. Kosslyn, & R. M Shiffrin (Eds.), </editor> <title> From learning theory to connectionist theory: </title> <booktitle> Essays in honor of William K. </booktitle> <address> Estes. </address>
Reference: <author> Pazzani, M. J. </author> <year> (1991). </year> <title> Influence of prior knowledge on concept acquisition: experimental and computational results. J. Experimental Psychology: Learning, Memory, </title> <journal> and Cognition, </journal> <volume> 17, </volume> <pages> 416-432. </pages>
Reference: <author> Pillaipakkamnatt, K. & Raghavan, V. </author> <year> (1994). </year> <title> Read-twice DNF formulas are properly learnable. </title> <booktitle> In Computational Learning Theory: Eurocolt '93, volume New Series Number 53 of The Institute of Mathematics and its Applications Conference Series (pp. </booktitle> <pages> 121-132). </pages> <publisher> Oxford Universlty Press. </publisher>
Reference: <author> Pitt, L. & Warmuth, M.K. </author> <year> (1990). </year> <title> Prediction preserving reducibility. </title> <journal> J. of Com-put. Syst. Sci., </journal> <volume> 41, </volume> <pages> 430-467. </pages>
Reference: <author> Quinlan, J.R. </author> <year> (1979). </year> <title> Discovering Rules from Large Collections of Examples: </title>
References-found: 59

