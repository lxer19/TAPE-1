URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/dst/www/Skinnerbots/pubs/skinnerbots-AB97.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/dst/www/bib.html
Root-URL: 
Email: dst@cs.cmu.edu  saksida@ri.cmu.edu  
Title: Operant Conditioning in Skinnerbots  
Author: David S. Touretzky Lisa M. Saksida 
Keyword: operant conditioning, instrumental learning, shaping, chaining, learning robots  
Note: To appear in Adaptive Behavior 5(3/4), 1997. Copyright c 1997 The MIT Press.  Running Head: Operant Conditioning in Skinnerbots  
Address: Pittsburgh, PA 15213-3891  Pittsburgh, PA 15213-3891  
Affiliation: Computer Science Department Center for the Neural Basis of Cognition Carnegie Mellon University  Robotics Institute Center for the Neural Basis of Cognition Carnegie Mellon University  
Abstract: Instrumental (or operant) conditioning, a form of animal learning, is similar to reinforcement learning (Watkins, 1989) in that it allows an agent to adapt its actions to gain maximally from the environment while only being rewarded for correct performance. But animals learn much more complicated behaviors through instrumental conditioning than robots presently acquire through reinforcement learning. We describe a new computational model of the conditioning process that attempts to capture some of the aspects that are missing from simple reinforcement learning: conditioned reinforcers, shifting reinforcement contingencies, explicit action sequencing, and state space refinement. We apply our model to a task commonly used to study working memory in rats and monkeys: the DMTS (Delayed Match to Sample) task. Animals learn this task in stages. In simulation, our model also acquires the task in stages, in a similar manner. We have used the model to train an RWI B21 robot. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Asada, M., Uchibe, E., Noda, S., Tawaratsumida, S., and Hosoda, K. </author> <year> (1994). </year> <title> Coordination of multiple behaviors acquired by a vision-based reinforcement learning. </title> <booktitle> In International Conference on Intelligent Robots and Systems. IEEE. </booktitle>
Reference-contexts: Asada et al. constructed a robot soccer player that learned to push a ball into a goal while avoiding an opponent <ref> (Asada et al., 1994) </ref>. They examine techniques for combining ball shooting and collision avoidance behaviors, each acquired separately using Q-learning, into a policy function that accomplishes both tasks.
Reference: <author> Barnett, S. </author> <year> (1981). </year> <title> Modern Ethology. </title> <publisher> Oxford University Press. </publisher>
Reference-contexts: See <ref> (Barnett, 1981) </ref> for additional examples of fixed action patterns.
Reference: <author> Barto, A. G. and Sutton, R. S. </author> <year> (1990). </year> <title> Time-derivative models of Pavlovian conditioning. </title> <editor> In Gabriel, M. and Moore, J., editors, </editor> <booktitle> Learning and Computational Neuroscience: Foundations of Adaptive Networks, </booktitle> <pages> pages 497-537. </pages> <publisher> MIT Press, </publisher> <address> Cambdridge, MA. </address>
Reference: <author> Baxter, D. A., Buonomano, D. V., Raymond, J. L., Cook, D. G., Kuenzi, F. M., Carew, T. J., and Byrne, J. H. </author> <year> (1991). </year> <title> Empirically derived adaptive elements and networks simulate associative learning. In Commons, </title> <editor> M. L., Grossberg, S., and Staddon, J. E. R., editors, </editor> <booktitle> Neural Network Models of Conditioning and Action, </booktitle> <pages> pages 13-52. </pages> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address>
Reference: <author> Berridge, K., J.C.Fentress, and Parr, H. </author> <year> (1987). </year> <title> Natural syntax rules control action sequence of rats. </title> <journal> Behavioural Brain Research, </journal> <volume> 23 </volume> <pages> 59-68. </pages>
Reference-contexts: What would then be needed is a means for refining operators with experience, similar to the animal training technique called "shaping". Evidence for the existence of innate behavioral elements <ref> (Berridge et al., 1987) </ref> combined with the success of shaping in animal learning paradigms suggests that this would be a very powerful mechanism for improving the performance of our learning algorithm.
Reference: <author> Blough, D. </author> <year> (1959). </year> <title> Delayed matching in the pigeon. </title> <journal> Journal of the Experimental Analysis of Behavior, </journal> <volume> 2 </volume> <pages> 151-160. </pages>
Reference-contexts: We developed a learning algorithm that incorporates aspects of chaining which reinforcement learning techniques do not address, such as shifting reinforcement contingencies and learning of conditioned reinforcers. We chose a classic cognitive assessment task that involves behavioral sequences, the Delayed Match to Sample (DMTS) task <ref> (Blough, 1959) </ref>, as the first test case for our learning model. We have also implemented the model on an RWI B21 mobile robot. 1.1. Operant Conditioning In operant conditioning, the acquisition and further performance of an action depends on the consequences experienced upon its completion.
Reference: <author> Breland, K. and Breland, M. </author> <year> (1961). </year> <title> The misbehavior of organisms. </title> <journal> American Psychologist, </journal> <volume> 16 </volume> <pages> 681-684. </pages>
Reference-contexts: Chaining Complex behaviors can often be broken down into components and analyzed as a sequence of operants. 1 For example, a chick trained to "play the piano" pecks a sequence of keys to obtain a food reinforcement at the end of the tune <ref> (Breland and Breland, 1961) </ref>. A pig taught to "grocery shop" pushes a cart and selects specific items to place in it, one after the other (Breland and Breland, 1961). A behavioral chain can be analyzed as a sequence of stimuli and responses. <p> For example, a chick trained to "play the piano" pecks a sequence of keys to obtain a food reinforcement at the end of the tune <ref> (Breland and Breland, 1961) </ref>. A pig taught to "grocery shop" pushes a cart and selects specific items to place in it, one after the other (Breland and Breland, 1961). A behavioral chain can be analyzed as a sequence of stimuli and responses. The core unit of a chain is called a link; it consists of a discriminative stimulus, a response, and a reinforcer. The chain begins with the presentation of the first discriminative stimulus.
Reference: <author> Brown, P. and Jenkins, H. </author> <year> (1968). </year> <title> Auto-shaping of the pigeon's keypeck. </title> <journal> Journal of the Experimental Analysis of Behavior, </journal> <volume> 11 </volume> <pages> 1-8. </pages>
Reference-contexts: Conversely, in a process known as autoshaping, classical contingencies can produce the sort of voluntary behavior normally associated with instrumental conditioning, rather than purely autonomic or reflexive responses <ref> (Brown and Jenkins, 1968) </ref>. In the original autoshaping experiments, done with pigeons, a standard operant chamber with a lighted response key and food hopper was used. The key was illuminated for a fixed, brief period of time, and food was presented at the offset of each illumination.
Reference: <author> Bussey, T. J., Muir, J. L., and Robbins, T. W. </author> <year> (1994). </year> <title> A novel automated touchscreen procedure for assessing learning in the rat using computer graphic stimuli. </title> <journal> Neuroscience Research Communications, </journal> <volume> 15(2) </volume> <pages> 103-109. </pages>
Reference-contexts: In the nonspatial version, the sample appears in one location and the two probe stimuli appear in other locations; it is the visual characteristics of the stimuli that matter <ref> (Bussey et al., 1994) </ref>.
Reference: <author> Catania, A. C. and Harnad, S., </author> <title> editors (1988). The Selection of Behavior. </title> <publisher> Cambridge University Press. </publisher> <month> CCI </month> <year> (1995). </year> <title> The CCI Program. Canine Companions for Independence, </title> <address> Santa Rosa, CA. </address> <note> Informational page available at http://grunt.berkeley.edu/cci/cci.html. </note>
Reference-contexts: Due to the pioneering work of B. F. Skinner on operant conditioning <ref> (Catania and Harnad, 1988) </ref>, we have coined the term "skinnerbot" to describe autonomous learning robots that employ strategies and exhibit behavioral effects characteristic of instrumental learning (Touretzky and Saksida, 1996).
Reference: <author> Chomsky, N. </author> <year> (1959). </year> <title> Review of Skinner's Verbal Behavior. </title> <booktitle> Language, </booktitle> <pages> 35(26-58). </pages>
Reference-contexts: See (Barnett, 1981) for additional examples of fixed action patterns. The idea that patterns of responding can be reduced to a succession of stimulus-response units has been controversial: Skinner (Skinner, 1938) claimed that all behavior, including language, could be represented this way, while others, such as Chomsky <ref> (Chomsky, 1959) </ref> and Lashley (Lashley, 1951) held that sequential behavior could not be adequately accounted for in these terms. There is now considerable evidence, however, that many, though probably not all types of behavior sequences are held together this way (Gollub, 1977).
Reference: <author> Colombetti, M., Dorigo, M., and Borghi, G. </author> <year> (1996). </year> <title> Behavior analysis and training: A methodology for behavior engineering. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics Part B, </journal> <volume> 26(3) </volume> <pages> 365-380. </pages>
Reference: <author> Dickinson, A. </author> <year> (1995). </year> <title> Instrumental conditioning. </title> <editor> In Mackintosh, N. J., editor, </editor> <booktitle> Handbook of Perception and Cognition. </booktitle> <volume> Volume 9. </volume> <publisher> Academic Press, </publisher> <address> Orlando, FL. </address>
Reference-contexts: Once we have laid more of the computational-level groundwork for our model, we will be able to move on to a model that addresses some of the presently unsettled psychological issues in instrumental learning <ref> (Dickinson, 1995) </ref>. 15 Acknowledgments This work was supported by National Science Foundation grant IRI-9530975.
Reference: <author> Dorigo, M. and Columbetti, M. </author> <year> (1994). </year> <title> Robot shaping: Developing autonomous agents through learning. </title> <journal> Artificial Intelligence, </journal> <volume> 70(2) </volume> <pages> 321-370. </pages>
Reference: <author> Drescher, G. L. </author> <year> (1991). </year> <title> Made-Up Minds. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: predictors and should be retained for further exploration, even if their reward counts are relatively low (meaning they each account for only a limited number of the rewards that have been received.) Drescher makes a similar observation in his Piagetian learning model, where he distinguishes between relevance and reliability measures <ref> (Drescher, 1991) </ref>. During learning, conjunctions that are sufficiently well correlated with rewards generate "predictors," i.e., rules for predicting reward. These may displace earlier predictors that have not performed as well.
Reference: <author> Gallistel, C. R. </author> <year> (1990). </year> <title> The Organization of Learning. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: The difference between the merit and demerit values can be taken as an uncertainty measure for our estimate of reward rate <ref> (Gallistel, 1990, chap. 13) </ref>. See Figure 2. As n approaches 1, merit and demerit both converge to r=n, the true reward rate. The constants in the equations for merit and demerit were chosen empirically to give an appropriate balance between reward estimates r=n and confidence values when n is small.
Reference: <author> Gollub, L. </author> <year> (1977). </year> <title> Conditioned reinforcement: Schedule effects. </title> <editor> In Honig, W. and Staddon, J., editors, </editor> <booktitle> Handbook of operant behavior. </booktitle> <publisher> Prentice-Hall. </publisher>
Reference-contexts: There is now considerable evidence, however, that many, though probably not all types of behavior sequences are held together this way <ref> (Gollub, 1977) </ref>. The concept of constructing behavioral sequences for mobile robots from small elements is appealing in that the programmer's responsibility would be limited to the construction of just these behavioral primitives, plus the learning algorithm for putting them together.
Reference: <author> Graham, J., Alloway, T., and Krames, L. </author> <year> (1994). </year> <title> Sniffy, the virtual rat: Simulated operant conditioning. </title> <journal> Behavio Research Methods, Instruments, & Computers, </journal> <volume> 26(2) </volume> <pages> 134-141. </pages>
Reference-contexts: None of these models approach the full richness of vertebrate learning, involving, for example, acquisition of secondary reinforcers and construction of behavior chains. Graham, Alloway and Krames <ref> (Graham et al., 1994) </ref> describe a "virtual rat" designed to let undergraduates try their hand at operant conditioning.
Reference: <author> Grossberg, S. </author> <year> (1972). </year> <title> A neural theory of punishment and avoidance, ii: Quantitative theory. </title> <journal> Mathematical Biosciences, </journal> <volume> 15 </volume> <pages> 253-285. </pages> <note> 18 Gutnikov, </note> <author> S., Barnes, J., and Rawlins, J. </author> <year> (1994). </year> <title> Working memory tasks in five-choice operant chambers: use of relative and absolute spatial memories. </title> <journal> Behavioral Neuroscience, </journal> <volume> 108(5) </volume> <pages> 899-910. </pages>
Reference: <author> Hammer, M. </author> <year> (1993). </year> <title> An identified neuron mediates the unconditioned stimulus in associative olfactory learning in honeybees. </title> <journal> Nature, </journal> <volume> 366 </volume> <pages> 59-63. </pages>
Reference-contexts: And reinforcement learning does capture some aspects of animal behavior, such as adaptive foraging in bees (Montague et al., 1995). Furthermore, a neural representation of the kind of reward signal required for reinforcement learning has been found in both bees (the VUMmx1 neuron, <ref> (Hammer, 1993) </ref>) and primates (dopamine neurons in the substantia nigra pars compacta and ventral tegmental area, (Schultz et al., 1995).) But as we argue in the following section, operant conditioning in mammals is a richer phenomenon than can be addressed by current reinforcement learning theories.
Reference: <author> Hampson, R. E., Heyser, C. J., and Deadwyler, S. A. </author> <year> (1993). </year> <title> Hippocampal cell firing correlates of delayed-match-to-sample performance in the rat. </title> <journal> Behavioral Neuroscience, </journal> <volume> 107(5) </volume> <pages> 715-739. </pages>
Reference-contexts: There are both spatial and nonspatial versions of this task. In the spatial version, all stimuli are identical; they are distinguished on the basis of the location at which they appear <ref> (Hampson et al., 1993) </ref>. In the nonspatial version, the sample appears in one location and the two probe stimuli appear in other locations; it is the visual characteristics of the stimuli that matter (Bussey et al., 1994). <p> Our learning algorithm can be applied to either the spatial or non-spatial version of the task. ======================================== ======================================== Hampson, Heyser, and Deadwyler <ref> (Hampson et al., 1993) </ref> describe a spatial version of DMTS for rats that uses as stimuli two retractable switches mounted on the wall of a Skinner box, as shown in Figure 3. <p> Now the rat must return to the switch it pressed previously and press it again. If it chooses the correct switch, it receives a water reward. 10 Rats are taught this task in stages. Hampson et al. report a training time of two to three months <ref> (Hampson et al., 1993) </ref>. 3.1. Shifting Reinforcement Contingencies in the DMTS Task Both rats and our model require multiple training stages to learn the DMTS task. In our simulation there are four stages.
Reference: <author> Holland, P. C. </author> <year> (1993). </year> <title> Cognitive aspects of classical conditioning. </title> <booktitle> Current Opinion in Neurobiology, </booktitle> <volume> 3 </volume> <pages> 230-236. </pages>
Reference-contexts: Pavlovian responses are known to occur as part of instrumentally conditioned behaviors, e.g., a rat that has learned to press a bar to get food will begin to salivate when the bar is pressed <ref> (Holland, 1993) </ref>. Conversely, in a process known as autoshaping, classical contingencies can produce the sort of voluntary behavior normally associated with instrumental conditioning, rather than purely autonomic or reflexive responses (Brown and Jenkins, 1968).
Reference: <author> Klopf, A. H. </author> <year> (1988). </year> <title> A neuronal model of clasical conditioning. </title> <journal> Psychobiology, </journal> <volume> 16 </volume> <pages> 85-125. </pages>
Reference: <author> Krames, L., Graham, J., and Alloway, T. </author> <year> (1995). </year> <title> Sniffy, the Virtual Rat. </title> <address> Brooks/Cole, Pacific Grove, CA. </address> <note> Includes software diskette. </note>
Reference-contexts: Their program is hard-wired to acquire a particular conditioned reinforcer (the sound of a food dispenser) and to respond to a specific shaping strategy to teach the simulated rat to bar press for food <ref> (Krames et al., 1995) </ref>.
Reference: <author> Lashley, K. </author> <year> (1951). </year> <title> The problem of serial order in behavior. </title> <editor> In Jeffries, L., editor, </editor> <title> Cerebral mechanisms in behavior. </title> <publisher> John Wiley and Sons. </publisher>
Reference-contexts: The idea that patterns of responding can be reduced to a succession of stimulus-response units has been controversial: Skinner (Skinner, 1938) claimed that all behavior, including language, could be represented this way, while others, such as Chomsky (Chomsky, 1959) and Lashley <ref> (Lashley, 1951) </ref> held that sequential behavior could not be adequately accounted for in these terms. There is now considerable evidence, however, that many, though probably not all types of behavior sequences are held together this way (Gollub, 1977).
Reference: <author> Mahadevan, S. and Connell, J. </author> <year> (1992). </year> <title> Automatic programing of behavior-based robots using reinforcement learning. </title> <journal> Artificial Intelligence, </journal> <volume> 55 </volume> <pages> 311-365. </pages>
Reference-contexts: Some work has been done on reinforcement learning of sequential tasks. Singh (Singh, 1992) describes a sequential task learner in which separate "Q-modules" learn different elemental and composite tasks, and 3 Mahadevan and Connell <ref> (Mahadevan and Connell, 1992) </ref> use Q-learning to acquire multiple behaviors that can then be controlled by a hardwired switching scheme to designate which should be active at a given time.
Reference: <author> Maki, W. S. and Abunawass, A. M. </author> <year> (1991). </year> <title> A connectionist approach to conditional discriminations: Learning, short-term memory, and attention. In Commons, </title> <editor> M. L., Grossberg, S., and Staddon, J. E. R., editors, </editor> <booktitle> Neural Network Models of Conditioning and Action, </booktitle> <pages> pages 241-278. </pages> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address>
Reference-contexts: There are also presently no provisions for chaining behaviors, for modifying the qualities of a particular motor response, or for refining the simulated animal's perceptual abilities. Maki and Abunawass <ref> (Maki and Abunawass, 1991) </ref> model learning of a match-to-sample task (no delay) using a backpropagation network. In animals, this task requires learning a complex sequence of actions (see section 3).
Reference: <author> Mataric, M. J. </author> <year> (1995). </year> <title> Designing and understanding adaptive group behavior. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 4(1) </volume> <pages> 50-81. </pages>
Reference-contexts: Composing behaviors from primitives, a more general problem than chaining, has also been investigated for mobile robots. Mataric showed how foraging behavior could be constructed from routines for wandering, homing, aggregation, and dispersion, plus some innate reflexes such as grasping an object whenever one is encountered <ref> (Mataric, 1995) </ref>. In later work, she used reinforcement learning to implement a complex behavior by deriving a policy for selecting the appropriate primitive behavior based on the robot's current state (Mataric, 1996). Other examples of reinforcement-based robot learning include (Dorigo and Columbetti, 1994; Colombetti et al., 1996) and (Millan, 1996).
Reference: <author> Mataric, M. J. </author> <year> (1996). </year> <title> Reinforcement learning in multi-robot domains. </title> <booktitle> Autonomous Robots, </booktitle> <volume> 4(1) </volume> <pages> 73-83. </pages>
Reference-contexts: In later work, she used reinforcement learning to implement a complex behavior by deriving a policy for selecting the appropriate primitive behavior based on the robot's current state <ref> (Mataric, 1996) </ref>. Other examples of reinforcement-based robot learning include (Dorigo and Columbetti, 1994; Colombetti et al., 1996) and (Millan, 1996).
Reference: <author> Millan, J. d. R. </author> <year> (1994). </year> <title> Learning efficient reactive behavioral sequences from basic reflexes in a goal-directed autonomous robot. </title> <booktitle> In From Animals to Animats 3: Proceedings of the Third International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 266-274, </pages> <address> Cambridge, MA. </address> <publisher> MIT Press. </publisher>
Reference-contexts: In this way, the animal broadens its exploration of the action space and may discover a variant of the learned action that will once again produce the expected reward. Animal trainers exploit this phenomenon to shape complex behaviors. See <ref> (Millan, 1994) </ref> for a related idea: increasing the variability of actions of a robot learner as a response to failure to improve performance.
Reference: <author> Millan, J. d. R. </author> <year> (1996). </year> <title> Rapid, safe, and incremental learning of navigation strategies. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics Part B, </journal> <volume> 26(3) </volume> <pages> 408-420. </pages>
Reference-contexts: In later work, she used reinforcement learning to implement a complex behavior by deriving a policy for selecting the appropriate primitive behavior based on the robot's current state (Mataric, 1996). Other examples of reinforcement-based robot learning include (Dorigo and Columbetti, 1994; Colombetti et al., 1996) and <ref> (Millan, 1996) </ref>. However, these authors rely on immediate reinforcement strategies in which the robot's action at every time step is positively or negatively reinforced by an automated trainer. (A human trainer would be unable to keep up.) This does not make for a realistic model of animal learning. 2.
Reference: <author> Montague, P. R., Dayan, P., Person, C., and Sejnowski, T. J. </author> <year> (1995). </year> <title> Bee foraging in uncertain environments using predictive Hebbian learning. </title> <journal> Nature, </journal> <volume> 377 </volume> <pages> 725-728. </pages>
Reference-contexts: Like operant conditioning, reinforcement learning is appealing because it theoretically allows an agent to autonomously adapt its actions to get the most from its environment as it gains information over time. And reinforcement learning does capture some aspects of animal behavior, such as adaptive foraging in bees <ref> (Montague et al., 1995) </ref>.
Reference: <author> Pavlov, I. P. </author> <year> (1927). </year> <title> Conditioned Reflexes. </title> <publisher> Oxford University Press. </publisher>
Reference-contexts: For example, it may learn that food can be produced by pressing a lever. It follows that instrumental learning enables animals to cope with a dynamic environment in which the consequences of their actions may vary. This contrasts with classical, or Pavlovian conditioning <ref> (Pavlov, 1927) </ref>, in which learning is limited to associating a possibly arbitrary conditioned stimulus with a reinforcing (unconditioned) stimulus that elicits some type of innate behavioral response.
Reference: <author> Pryor, K. </author> <year> (1975). </year> <title> Lads Before the Wind. </title> <publisher> Harper and Row, </publisher> <address> New York. </address>
Reference: <author> Raymond, J. L., Baxter, D. A., Buonomano, D. V., and Byrne, J. H. </author> <year> (1992). </year> <title> A learning rule based on empirically derived activity-dependent neuromodulation supports operant conditning in a small network. </title> <booktitle> Neural Networks, </booktitle> <volume> 5(5) </volume> <pages> 789-803. </pages>
Reference: <author> Rescorla, R. A. and Wagner, A. R. </author> <year> (1972). </year> <title> A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement. </title> <editor> In Black, A. H. and Prokasy, W. F., editors, </editor> <title> Classical Conditioning II: </title> <booktitle> Theory and Research. </booktitle> <address> Appleton-Century-Crofts, New York. </address>
Reference: <author> Reynolds, G. S. </author> <year> (1968). </year> <title> A Primer of Operant Conditioning. Scott, </title> <publisher> Foresman. </publisher>
Reference-contexts: least two classes of behavioral responses (Schwartz, 1989): (i) respondents, which originate with the stimuli that elicit them (e.g., a reflex), and (ii) operants, which are determined by their effects on the environment since they do not require eliciting stimuli. 2 At least two types of reinforcers can be distinguished <ref> (Reynolds, 1968) </ref>: (i) primary reinforcers can reinforce behavior without the animal having had any prior experience with them (e.g., food, water). (ii) conditioned reinforcers acquire the power to reinforce behavior during the lifetime of the animal via a Pavlovian mechanism in which the stimulus that becomes the conditioned reinforcer is repeatedly
Reference: <author> Schmajuk, N. A. and Urry, D. W. </author> <year> (1995). </year> <title> The frigtening complexity of avoidance: An adaptive neural network. In Models of Action. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address> <note> 19 Schultz, </note> <author> W., Romo, R., Ljungberg, T., Mirenowicz, J., Hollerman, J. R., and Dickinson, A. </author> <year> (1995). </year> <title> Reward--related signals carried by dopamine neurons. </title> <editor> In Houk, J. C., Davis, J. L., and Beiser, D. G., editors, </editor> <booktitle> Models of Information Processing in the Basal Ganglia, chapter 12, </booktitle> <pages> pages 233-248. </pages> <publisher> MIT Press. </publisher>
Reference: <author> Schwartz, B. </author> <year> (1989). </year> <title> Psychology of Learning and Behavior. W.W. </title> <publisher> Norton. </publisher>
Reference-contexts: Thus this type of behavior is much less flexible 1 The animal learning literature defines at least two classes of behavioral responses <ref> (Schwartz, 1989) </ref>: (i) respondents, which originate with the stimuli that elicit them (e.g., a reflex), and (ii) operants, which are determined by their effects on the environment since they do not require eliciting stimuli. 2 At least two types of reinforcers can be distinguished (Reynolds, 1968): (i) primary reinforcers can reinforce
Reference: <author> Simmons, R. </author> <year> (1994). </year> <title> Structured control for autonomous robots. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 10(1) </volume> <pages> 34-43. </pages>
Reference-contexts: Computing power is provided by three on-board Pentium processors. A 1 Mbps radio modem links Amelia to a network of Sparc stations that contribute additional processing cycles. For our experiments, we ran the learning program in Allegro Common Lisp on a Sparc 5 and used TCA (Task Control Architecture) <ref> (Simmons, 1994) </ref> to communicate with the robot. ======================================== ======================================== To provide reinforcement stimuli to the robot, we added a Logitech three-button radio trackball. The human trainer can stand anywhere in the vicinity of the robot and press a button to send a reward signal when a desired response occurs.
Reference: <author> Singh, S. </author> <year> (1992). </year> <title> Transfer of learning across sequential tasks. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 323-339. </pages>
Reference-contexts: Some work has been done on reinforcement learning of sequential tasks. Singh <ref> (Singh, 1992) </ref> describes a sequential task learner in which separate "Q-modules" learn different elemental and composite tasks, and 3 Mahadevan and Connell (Mahadevan and Connell, 1992) use Q-learning to acquire multiple behaviors that can then be controlled by a hardwired switching scheme to designate which should be active at a given
Reference: <author> Skinner, B. </author> <year> (1938). </year> <title> Behavior of Organisms. </title> <publisher> Appleton-Century-Crofts. </publisher>
Reference-contexts: See (Barnett, 1981) for additional examples of fixed action patterns. The idea that patterns of responding can be reduced to a succession of stimulus-response units has been controversial: Skinner <ref> (Skinner, 1938) </ref> claimed that all behavior, including language, could be represented this way, while others, such as Chomsky (Chomsky, 1959) and Lashley (Lashley, 1951) held that sequential behavior could not be adequately accounted for in these terms.
Reference: <author> Skinner, B. </author> <year> (1948). </year> <title> "Superstition" in the pigeon. </title> <journal> Journal of Experimental Psychology, </journal> <volume> 38 </volume> <pages> 168-172. </pages>
Reference-contexts: If water is actually being dispensed randomly, but at sufficiently frequent intervals, the predictor will be successful often enough to be retained and perhaps even strengthened. This sort of "superstitious" behavior has been observed in real animals <ref> (Skinner, 1948) </ref> although the underlying mechanisms are at this point unclear (Staddon and Simmelhag, 1971). In the second training stage the pump sound is reliably predicted by switch pressing, so the model replaces the superstitious predictors with accurate ones.
Reference: <author> Staddon, J. and Simmelhag, V. </author> <year> (1971). </year> <title> The "superstition" experiment: A reexamination of its implications for the principle of adaptive behavior. </title> <journal> Psychological Review, </journal> <volume> 78 </volume> <pages> 3-43. </pages>
Reference-contexts: If water is actually being dispensed randomly, but at sufficiently frequent intervals, the predictor will be successful often enough to be retained and perhaps even strengthened. This sort of "superstitious" behavior has been observed in real animals (Skinner, 1948) although the underlying mechanisms are at this point unclear <ref> (Staddon and Simmelhag, 1971) </ref>. In the second training stage the pump sound is reliably predicted by switch pressing, so the model replaces the superstitious predictors with accurate ones. It then begins forming superstitious predictors for the appearance of a switch that occurs at the start of each trial. 4.
Reference: <author> Sutton, R. S. and Barto, A. G. </author> <year> (1981). </year> <title> Toward a modern theory of adaptive networks: Expectation and prediction. </title> <journal> Psychological Review, </journal> <volume> 88 </volume> <pages> 135-170. </pages>
Reference: <author> Touretzky, D. S. and Saksida, L. </author> <year> (1996). </year> <editor> Skinnerbots. In Maes, P., Mataric, M., Meyer, J.-A., Pollack, J., and Wilson, S. W., editors, </editor> <booktitle> From animals to animats 4: Proceedings of the fourth international conference on simulation of adaptive behavior, </booktitle> <pages> pages 285-294. </pages> <publisher> MIT Press. </publisher>
Reference-contexts: Due to the pioneering work of B. F. Skinner on operant conditioning (Catania and Harnad, 1988), we have coined the term "skinnerbot" to describe autonomous learning robots that employ strategies and exhibit behavioral effects characteristic of instrumental learning <ref> (Touretzky and Saksida, 1996) </ref>. The present paper describes the investigation of a particular conditioning technique called chaining | in which behavioral routines are built up from smaller action segments | and how it can be applied to mobile robot learning.
Reference: <author> Verschure, P. F. M. J., Wray, J., Sporns, O., Tononi, G., and Edelman, G. M. </author> <year> (1995). </year> <title> Multilevel analysis of classical conditioning in a real world artifact. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <address> 16(2-4):247-265. </address>
Reference-contexts: In addition, some simple models of classical conditioning have been implemented on robots <ref> (Verschure et al., 1995) </ref>. Classical conditioning is of some value to robots: it is useful for a robot to be able to learn predictive values of stimuli and possibly follow them with innate anticipatory responses.

References-found: 47

