URL: http://www.cs.mu.oz.au/tr_db/mu_94_31.ps.gz
Refering-URL: http://www.cs.mu.oz.au/tr_db/TR.html
Root-URL: 
Title: Towards Optimal Storage Design for Efficient Query Processing in Relational Database Systems  
Author: Evan Philip Harris 
Degree: Ph.D. thesis The University of Melbourne Supervisor: Prof. Kotagiri Ramamohanarao  
Date: Submitted: November 1994 Revised: May 1995  
Address: Parkville, Victoria 3052 Australia  
Affiliation: Department of Computer Science The University of Melbourne  
Pubnum: Technical Report 94/31  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> E. Aarts and J. Korst. </author> <title> Simulated Annealing and Boltzmann Machines. </title> <publisher> Wiley, </publisher> <year> 1989. </year>
Reference-contexts: It is very easy to modify the minimal marginal increase algorithm so that an attribute is not allocated more than its constraining number of bits. 2.4.3 Simulated annealing Simulated annealing (SA) <ref> [1] </ref> is a class of optimisation algorithms based on Monte Carlo techniques. The algorithm we used is substantially the same as that used by Ramamohanarao et al. [67]. <p> The values we typically used for these two variables were 1 and 0.95, respectively. These values were chosen to ensure that the running time of the simulated annealing algorithm was acceptable for our larger problems, and are in the range suggested by Aarts and Korst <ref> [1] </ref>. When used to search for the optimal bit allocation, the simulated annealing algorithm can be modified to ensure that an attribute is not allocated more than its constraining number of bits. <p> Let this number be d i . Note that n X d i = d: Let r i (q) be the proportion of the total range of attribute A i that query q specifies. For example, if the domain of attribute A i is <ref> [1; 100] </ref>, and a query, q, specifies the range [2; 7] then r i (q) = (7 2 + 1)=100 = 0:06: We assume that the cost of a range query is C range (q) = n Y l m In general, the value given by r i (q) for each <p> We define the range size to be the number of possible distinct values for that attribute. If we assume that the domain size of an attribute, A i , is D i , so that the domain is <ref> [1; D i ] </ref>, and that there are r i range sizes, such that 1 r i D i , then each range size may have D i r i + 1 starting points.
Reference: [2] <author> A. V. Aho and J. D. Ullman. </author> <title> Optimal partial-match retrieval when fields are independently specified. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 4(2) </volume> <pages> 168-179, </pages> <month> June </month> <year> 1979. </year>
Reference-contexts: For example, Moran [54] showed that designing a particular optimal partial-match retrieval system was NP-hard. However, efficient algorithms have been found which can quickly find optimal or near-optimal solutions to this problem <ref> [2, 48, 49, 67] </ref>. Little work has been done on attempting to determine an optimal clustering of data for queries other than partial-match retrieval. Other clustering techniques have been proposed; however, they rarely consider the probability of an operation being asked to be performed. <p> This is our aim in Chapters 3, 4 and 6. Aho and Ullman <ref> [2] </ref> described how to determine the optimal number of bits to take from the bit string of each attribute to make up the choice vector for partial-match retrieval. We refer to this as the optimal bit allocation. <p> Note that n X d i = d: Let r i (q) be the proportion of the total range of attribute A i that query q specifies. For example, if the domain of attribute A i is [1; 100], and a query, q, specifies the range <ref> [2; 7] </ref> then r i (q) = (7 2 + 1)=100 = 0:06: We assume that the cost of a range query is C range (q) = n Y l m In general, the value given by r i (q) for each i should result in either l m l m
Reference: [3] <author> J. L. Bentley. </author> <title> Multidimensional binary search trees used for associative searching. </title> <journal> Communications of the ACM, </journal> <volume> 18(9) </volume> <pages> 509-517, </pages> <month> September </month> <year> 1975. </year>
Reference-contexts: A choice vector can be used to determine which grid region a record should be contained in and in which order the dimensions should be split. 2.2.6 Multidimensional binary search tree The multidimensional binary search tree (k-d-tree) was introduced by Bentley <ref> [3] </ref>. He later summarised further research into the data structure by himself and others, and placed the research in the context of database management systems [4]. The k-d-tree is a generalisation of the binary search tree, extended to k dimensions. It has two forms, homogeneous and non-homogeneous. <p> If a multi-attribute bit string and choice vector is used, this may be further reduced to just the left and right subtree pointers for each node. In the original paper <ref> [3] </ref>, Bentley suggested that each of the k keys should be used in turn as the discriminator. In a subsequent paper [4], he acknowledges that one may do better by choosing a discriminator which is often specified in queries. The choice vector can do just that.
Reference: [4] <author> J. L. Bentley. </author> <title> Multidimensional binary search trees in database applications. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-5(4):333-340, </volume> <month> July </month> <year> 1979. </year>
Reference-contexts: He later summarised further research into the data structure by himself and others, and placed the research in the context of database management systems <ref> [4] </ref>. The k-d-tree is a generalisation of the binary search tree, extended to k dimensions. It has two forms, homogeneous and non-homogeneous. In the homogeneous form, a single record is stored in each node of the tree. <p> In the original paper [3], Bentley suggested that each of the k keys should be used in turn as the discriminator. In a subsequent paper <ref> [4] </ref>, he acknowledges that one may do better by choosing a discriminator which is often specified in queries. The choice vector can do just that.
Reference: [5] <author> D. Bitton, H. Boral, D. J. DeWitt, and W. K. Wilkinson. </author> <title> Parallel algorithms for the execution of relational database operations. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 8(3) </volume> <pages> 324-353, </pages> <month> September </month> <year> 1983. </year>
Reference-contexts: Therefore, 184 the scope exists to both design algorithms which are guaranteed to produce optimal results, and to design algorithms which produce similar, or superior, results in less time. In recent years, much research has been conducted into the designing of parallel algorithms to implement relational databases operations <ref> [5, 43, 47, 68, 71, 73, 81] </ref>. We believe that the work in this thesis can be applied in this environment. For example, parallel join algorithms can be analysed with the cost model described in Chapter 5 to determine whether their use of memory can be improved.
Reference: [6] <author> D. Bitton, D. J. DeWitt, and C. Turbyfill. </author> <title> Benchmarking database systems a systematic approach. </title> <booktitle> In Proceedings of the Ninth International Conference on Very Large Data Bases, </booktitle> <pages> pages 8-19, </pages> <address> Florence, Italy, </address> <month> November </month> <year> 1983. </year>
Reference-contexts: However, network and other operating system related activities were not removed from the machine. The data files used in the experiments consisted of 184 byte records, similar to those used in the Wisconsin benchmark <ref> [6] </ref>. Each record consisted of a unique identifier attribute, six integer attributes and three string attributes, each of length 52 bytes. The values of each of integer attribute was chosen from a uniform random distribution.
Reference: [7] <author> M. W. Blasgen and K. P. Eswaran. </author> <title> Storage and access in relational data bases. </title> <journal> IBM Systems Journal, </journal> <volume> 16(4), </volume> <year> 1977. </year>
Reference-contexts: The outer relation is typically the smaller of the two relations. In practice, more than one record of the outer relation is read before the inner relation is scanned. For example, Blasgen and Eswaran <ref> [7] </ref> held as many records of the outer relation in memory as possible and read one record at a time from the inner relation. A similar algorithm has been suggested on the disk block level. <p> Note that n X d i = d: Let r i (q) be the proportion of the total range of attribute A i that query q specifies. For example, if the domain of attribute A i is [1; 100], and a query, q, specifies the range <ref> [2; 7] </ref> then r i (q) = (7 2 + 1)=100 = 0:06: We assume that the cost of a range query is C range (q) = n Y l m In general, the value given by r i (q) for each i should result in either l m l m <p> This can be modelled by setting T K = 1 and T T = 0. 5.2 Join algorithm costs Many papers have used the number of blocks transferred in their descriptions of the cost of disk operations <ref> [7, 15, 35, 36, 56, 72] </ref>. Even recent papers have used this cost model [31, 59, 60, 81]. These papers assume that the cost of transferring a number of consecutive blocks at once is the same as transferring them individually from random parts of the disk. <p> We now compare the four join algorithms, using the same example as above. This is shown in Figures 5.9 and 5.10. Figure 5.10 contains an enlarged version of part of Figure 5.9. Using the standard cost model, others, such as Blasgen and Eswaran <ref> [7] </ref>, found that when the outer relation may be contained within main memory, the nested loop algorithm performs the best. Figures 5.9 and 5.10 show that this is still the case under our cost model.
Reference: [8] <author> K. Bratbergsengen. </author> <title> Hashing methods and relational algebra operations. </title> <booktitle> In Proceedings of the Tenth International Conference on Very Large Data Bases, </booktitle> <pages> pages 323-333, </pages> <address> Singapore, </address> <month> August </month> <year> 1984. </year>
Reference-contexts: Merrett [51] argued that the sort-merge algorithm gives the best implementation of the natural join based on a theory of clustering relations. This conclusion has since been disputed by many, including Kitsuregawa et al. [36], Bratbergsengen <ref> [8] </ref>, and DeWitt et al. [15]. They showed that variations of the hash join algorithm perform better than the sort-merge algorithm when the initial data is unsorted. They also showed that, for small numbers of records, the nested loop algorithm performs better than the sort-merge algorithm.
Reference: [9] <author> W. A. Burkhard. </author> <title> Interpolation-based index maintenance. </title> <journal> BIT, </journal> <volume> 23 </volume> <pages> 274-294, </pages> <year> 1983. </year>
Reference-contexts: do not have overflow blocks, and the number of blocks in the directory which are accessed is small when compared with the number of leaf blocks which must be retrieved to answer a partial-match query. 2.2.2.2 Multilevel order preserving linear hashing Order preserving linear hashing was independently discovered by Burkhard <ref> [9] </ref>, Oren-stein, and Ouksel and Scheuermann [62], according to Orenstein [61]. It is implemented by using an order preserving hash function to generate the hash key for records which are then stored in a linear hash file. <p> Another advantage is that the large indexing data structures which are traditionally used with spatial data indexing techniques are avoided, while the good performance of multi-attribute hashing is maintained as the data file changes size by several orders of magnitude. Related work includes that of Burkhard <ref> [9] </ref>, Kriegel and Seeger [39] and Chen et al. [10]. As we discussed in Section 2.2.2, Burkhard proposed an extension to linear hashing which uses order preserving hash functions to support range queries. Algorithms to insert, update, delete and perform range queries on records were described.
Reference: [10] <author> C. Y. Chen, C. C. Chang, and R. C. T. Lee. </author> <title> Optimal MMI file systems for orthogonal range queries. </title> <journal> Information Systems, </journal> <volume> 18(1) </volume> <pages> 37-54, </pages> <year> 1993. </year>
Reference-contexts: Related work includes that of Burkhard [9], Kriegel and Seeger [39] and Chen et al. <ref> [10] </ref>. As we discussed in Section 2.2.2, Burkhard proposed an extension to linear hashing which uses order preserving hash functions to support range queries. Algorithms to insert, update, delete and perform range queries on records were described. <p> Lloyd and Ramamohanarao [49] showed that the cost of accessing an extendible hash file directory was small for partial-match retrieval, when compared to the cost of retrieving the data blocks. After the research described in this chapter was completed, Chen et al. <ref> [10] </ref> published a very similar scheme. They used MMI to determine an optimal bit allocation for range queries, using order preserving hashing to store the data in a multikey or extendible hash file, just as we described above. <p> It remains an open problem to compare the performance of the two schemes. As our cost formula is more accurate, we expect our method to produce results which are equal to, or better than, that of Chen et al. <ref> [10] </ref>. <p> Another area of further research would be a quantitative comparison of our techniques with some of the other dynamic file indexing techniques which are used with range queries, including the work of Chen et al. <ref> [10] </ref>. 55 56 Chapter 4 Clustering Relations for Join Operations In general purpose database systems, only a small number of the retrieval operations will be used to directly answer partial-match queries. Other common relational operations include projection, join, division, intersection, union and set difference. <p> The cost model we used to calculate the cost of a range query is more accurate than those used by others, such as Ullman [78] and Chen et al. <ref> [10] </ref>. Furthermore, the approach used is general enough that it can be applied to many other data structures, including the k-d-tree, grid file and BANG file. The primary difficulty when range query distributions are considered is the number of queries which can be asked.
Reference: [11] <author> J. Cheng, D. Haderle, R. Hedges, B. R. Iyer, T. Messinger, C. Mohan, and Y. Wang. </author> <title> An efficient hybrid join algorithm: a DB2 prototype. </title> <booktitle> In Proceedings of the Seventh International Conference on Data Engineering, </booktitle> <pages> pages 171-180, </pages> <address> Kobe, Japan, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: They also showed that, for small numbers of records, the nested loop algorithm performs better than the sort-merge algorithm. Some are still unsure that hash join algorithms provide a significant advantage over sort-merge algorithms. For example, Cheng et al. <ref> [11] </ref> claim that the I/O cost of the algorithms are similar when a large amount of memory is involved, and that hash join is only superior when there is no index on the join column of the inner table. <p> Our analysis is a generalisation of these two cost models and allows the relative, or absolute, cost of each disk and CPU operation to be specified. Other researchers, such as Cheng et al. <ref> [11] </ref> and Pang et al. [64], have used a cost model of similar power to ours when evaluating their algorithms. However, they do not attempt to optimise the buffer usage based on this information and often read a block at a time from disk during each I/O operation.
Reference: [12] <author> R. Cichelli. </author> <title> Minimal perfect hash functions made simple. </title> <journal> Communications of the ACM, </journal> <volume> 23(1) </volume> <pages> 17-19, </pages> <month> January </month> <year> 1980. </year> <month> 187 </month>
Reference-contexts: For example, if the data is stored using in a linear hash file, long overflow chains can result. This increases the average search length. There have been a number of proposals to try to reduce this problem. One approach is the perfect hash function <ref> [12, 69] </ref> which hashes the data in such a way that for any attribute value, there is a unique hash value. However, this method assumes a static distribution of a fixed amount data. <p> The hash function for an attribute of this type must map each attribute value to a unique bit string. Hash functions of this type are referred to as perfect hash functions. Generating perfect hash functions can be time consuming <ref> [12, 69] </ref>, but only needs to be performed once. For attributes such as "month", a hash function of this type can be constructed easily using a lookup table for the values. We do not expect that a high proportion attributes will be maximally allocated in any given index.
Reference: [13] <author> D. Comer. </author> <title> The ubiquitous B-tree. </title> <journal> ACM Computing Surveys, </journal> <volume> 11(2) </volume> <pages> 121-138, </pages> <month> June </month> <year> 1979. </year>
Reference-contexts: hash key must be order preserving, and the most significant bits must be taken from the bit strings to form the hash key of a record. 2.2.2.3 Adaptive hashing The adaptive hashing of Hsiao and Tharp [30] aims to combine the features of order preserving linear hashing and the B+-tree <ref> [13] </ref>. The hash functions used in adaptive 15 Hash key 0 1 2 3 BP 0 1 3 3 3 4 6 7 10 11 12 14 15 18 21 hashing are order preserving. organisation of a file indexed using adaptive hashing.
Reference: [14] <author> H. Dang and D. Abramson. </author> <title> Cooling schedules for simulated annealing based scheduling algorithms. </title> <booktitle> In Proceedings of the Seventeenth Annual Computer Science Conference, </booktitle> <pages> pages 541-550, </pages> <address> Christchurch, New Zealand, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: These include iterative improvement, which was used by Swami [76] for join query optimisation, the tabu search [22] and genetic algorithms. Additionally, more complex simulated annealing algorithms with sophisticated cooling functions and domain specific knowledge can perform better than the more general simulated annealing algorithms <ref> [14, 32] </ref>. It is likely that many of these techniques will find near-optimal solutions to the problems in the following chapters faster than the simulated annealing algorithm we used, if suitably tuned.
Reference: [15] <author> D. J. DeWitt, R. H. Katz, F. Olken, L. D. Shapiro, M. R. Stonebraker, and D. Wood. </author> <title> Implementation techniques for main memory database systems. </title> <booktitle> In Proceedings of the 1984 ACM SIGMOD International Conference on the Management of Data, </booktitle> <pages> pages 1-8, </pages> <address> Boston, Massachusetts, USA, </address> <month> June </month> <year> 1984. </year>
Reference-contexts: However, P may vary between partitioning passes. 29 Main memory, B B I B H B P B P B R 2.3.3.2 Hybrid hash join The hybrid hash join algorithm of DeWitt et al. <ref> [15] </ref> is similar to the GRACE hash join algorithm. The primary differences are that it assumes that there is sufficient memory so that only one partitioning pass is required, and during the partitioning pass an area of memory is reserved to perform joins in. <p> Merrett [51] argued that the sort-merge algorithm gives the best implementation of the natural join based on a theory of clustering relations. This conclusion has since been disputed by many, including Kitsuregawa et al. [36], Bratbergsengen [8], and DeWitt et al. <ref> [15] </ref>. They showed that variations of the hash join algorithm perform better than the sort-merge algorithm when the initial data is unsorted. They also showed that, for small numbers of records, the nested loop algorithm performs better than the sort-merge algorithm. <p> This can be modelled by setting T K = 1 and T T = 0. 5.2 Join algorithm costs Many papers have used the number of blocks transferred in their descriptions of the cost of disk operations <ref> [7, 15, 35, 36, 56, 72] </ref>. Even recent papers have used this cost model [31, 59, 60, 81]. These papers assume that the cost of transferring a number of consecutive blocks at once is the same as transferring them individually from random parts of the disk. <p> These papers assume that the cost of transferring a number of consecutive blocks at once is the same as transferring them individually from random parts of the disk. Some have attempted to differentiate between different types of disk accesses. For example, DeWitt et al. <ref> [15] </ref> had separate I/O costs for sequential and random accesses. However, these models are less general than our I/O model. <p> This is similar to results reported in the past using the standard cost model. However, using the standard cost model, DeWitt et al. <ref> [15] </ref> reported a greater increase in performance by the hybrid hash join over the GRACE hash join than we have observed. <p> The amount of data which does not have to be written to disk, read back in and then joined will be large enough to ensure that the cost of the hybrid hash algorithm is significantly smaller than the other methods. Again, as reported by DeWitt et al. <ref> [15] </ref> using the standard cost model, the sort-merge algorithm does not perform as well as either hash join algorithm, despite the fact that our version of the sort-merge algorithm has a lower cost than the version usually reported. Note that in common with DeWitt et al. [15] and many others, we <p> by DeWitt et al. <ref> [15] </ref> using the standard cost model, the sort-merge algorithm does not perform as well as either hash join algorithm, despite the fact that our version of the sort-merge algorithm has a lower cost than the version usually reported. Note that in common with DeWitt et al. [15] and many others, we have assumed that the data is uniformly distributed. If the data was skewed, it is quite possible that the sort-merge algorithm would perform better than the hash join algorithms.
Reference: [16] <author> R. Fagin, J. Nievergelt, and H. R. </author> <title> Strong. Extendible hashing|a fast access method for dynamic files. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 4(3) </volume> <pages> 315-344, </pages> <month> September </month> <year> 1979. </year>
Reference-contexts: Some will require minor modifications to the cost functions. We now outline some of these schemes. 2.2.2.1 Extendible hashing Extendible hashing, introduced by Fagin et al. <ref> [16] </ref>, is a representative example of a set of hashing schemes which use a directory. A directory consists of a set of entries. Each directory entry has an associated hash key prefix and contains a pointer to a data (leaf) block.
Reference: [17] <author> C. Faloutsos. </author> <title> Multiattribute hashing using Gray codes. </title> <booktitle> In Proceedings of the 1986 ACM SIGMOD International Conference on the Management of Data, </booktitle> <pages> pages 227-238, </pages> <year> 1986. </year>
Reference-contexts: For example, two consecutive disk blocks can be retrieved faster than two non-consecutive disk blocks because no seeking is required to locate the second block once the first has been read. Faloutsos <ref> [17] </ref> suggested using Gray codes to map the hash keys to disk blocks so that records with similar hash keys are clustered together. Any two block addresses which only differ in precisely one of the last two bit positions will be located in consecutive blocks.
Reference: [18] <author> C. Faloutsos and S. Roseman. </author> <title> Fractals for secondary key retrieval. </title> <booktitle> In Eighth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems, </booktitle> <pages> pages 247-252, </pages> <address> Philadelphia, Pennsylvania, USA, </address> <month> March </month> <year> 1989. </year>
Reference-contexts: Little work has been done on attempting to determine an optimal clustering of data for queries other than partial-match retrieval. Other clustering techniques have been proposed; however, they rarely consider the probability of an operation being asked to be performed. For example, Faloutsos and Roseman <ref> [18] </ref> proposed using fractals to cluster multidimensional data in one dimension for storage on disk. They showed that this clustering technique performed better for range queries than a number of older clustering techniques, but they did not consider varying the frequency of queries.
Reference: [19] <author> M. J. Folk and B. Zoellick. </author> <title> File structures. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mas-sachusetts, USA, </address> <year> 1992. </year>
Reference-contexts: For example, if the load factor is 80% with 50 records per block, the multiplying factor is 1.0725 (the unsuccessful search length). The method for calculating these multiplying factors for various load and blocking factors has been described by Folk and Zoellick <ref> [19] </ref> and Ramamohanarao and Lloyd [65]. * The size of the data file is of the form 2 d blocks. We make this assumption because we wish to construct a choice vector of length d for a multi-attribute hash file, where d is an integer. <p> By having each algorithm optimise its own buffer usage, it has the opportunity to modify its own behaviour to improve the buffer usage. The use of an extent based file system <ref> [19] </ref>, even under UNIX [50], provides greater support for our technique than standard file systems, which do not guarantee that consecutive blocks are even on the same part of the disk. <p> This is a valid assumption in extent based file systems, which do just that <ref> [19] </ref>. As we discussed previously, many commercially available file systems, such as that of SunOS [50], now implement features of extent based file systems and provide similar performance. * The records are uniformly distributed amongst all blocks in a multi-attribute hash file.
Reference: [20] <author> M. Freeston. </author> <title> The BANG file: a new kind of grid file. </title> <booktitle> In Proceedings of the 1987 ACM SIGMOD International Conference on the Management of Data, </booktitle> <pages> pages 260-269, </pages> <address> San Francisco, California, USA, </address> <month> May </month> <year> 1987. </year>
Reference-contexts: A solution is to take these relationships into account when implementing the hash functions and allocating bits from the hash values of each attribute. Another solution is to use a data structure which has a greater tolerance for correlated attributes, such as the BANG file <ref> [20] </ref> or multilevel grid file [83]. These data structures are discussed in the next section. 2.2 Multi-attribute hashing and other data structures A choice vector can be viewed as specifying the order in which the n dimensional attribute space of a relation is divided. <p> Fewer disk accesses would typically be necessary with the multilevel grid file for queries other than point queries, and many of the upper directory levels would be able to be contained within memory. 2.2.5 BANG file The BANG file of Freeston <ref> [20, 21] </ref> is a multilevel, multidimensional file structure in the same class as the grid file and multilevel grid file. Unlike those two structures, in which the data blocks spanned disjoint regions, the data blocks in the BANG file can span nested regions.
Reference: [21] <author> M. Freeston. </author> <title> Grid files for efficient Prolog clause access. In Prolog and Databases Implementations and Future Directions, </title> <booktitle> chapter 12, </booktitle> <pages> pages 188-211. </pages> <publisher> Ellis Hor-wood, </publisher> <year> 1988. </year>
Reference-contexts: Fewer disk accesses would typically be necessary with the multilevel grid file for queries other than point queries, and many of the upper directory levels would be able to be contained within memory. 2.2.5 BANG file The BANG file of Freeston <ref> [20, 21] </ref> is a multilevel, multidimensional file structure in the same class as the grid file and multilevel grid file. Unlike those two structures, in which the data blocks spanned disjoint regions, the data blocks in the BANG file can span nested regions.
Reference: [22] <author> F. Glover. </author> <title> Tabu search: a tutorial. </title> <journal> Interfaces, </journal> <volume> 20(4) </volume> <pages> 74-94, </pages> <month> July-August </month> <year> 1990. </year>
Reference-contexts: There are a number of other techniques which could be used in addition to MMI and simulated annealing to search for optimal bit allocations. These include iterative improvement, which was used by Swami [76] for join query optimisation, the tabu search <ref> [22] </ref> and genetic algorithms. Additionally, more complex simulated annealing algorithms with sophisticated cooling functions and domain specific knowledge can perform better than the more general simulated annealing algorithms [14, 32].
Reference: [23] <author> D. E. Goldberg. </author> <title> Genetic algorithms in search, optimization, and machine learning. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, USA, </address> <year> 1989. </year>
Reference-contexts: We implemented a number of simple versions of these other techniques during the course of our research, namely iterative improvement and very fast simulated annealing [32]. We also tried two genetic algorithm packages, Genocop (version 2.0) [52] and SGA-C <ref> [23, 74] </ref>. Each of these performed worse than simulated annealing in our tests. Iterative improvement and very fast simulated annealing took longer to find near-optimal solutions of similar quality to simulated annealing.
Reference: [24] <author> G. Graefe. </author> <title> Relational division: four algorithms and their performance. </title> <booktitle> In Proceedings of the Fifth International Conference on Data Engineering, </booktitle> <pages> pages 94-101, </pages> <address> New York, USA, 1989. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: The implementation of aggregate functions is discussed in Section 6.2.9, while the join was discussed in Section 6.2.3. Graefe <ref> [24, 25] </ref> found that the best algorithm for implementing the quotient operation is a direct method based on hashing.
Reference: [25] <author> G. Graefe. </author> <title> Query evaluation techniques for large databases. </title> <journal> ACM Computing Surveys, </journal> <volume> 25(2) </volume> <pages> 73-170, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: However, they do not attempt to optimise the buffer usage based on this information and often read a block at a time from disk during each I/O operation. Graefe noted the importance of reading and writing clusters of blocks <ref> [25] </ref>. He stated that, for sorting, "the optimal cluster size and fan-in basically do not depend on the input size." This implies that the cluster size should be a small multiple of the block size. <p> This produces the same effect as the clusters described by Graefe <ref> [25] </ref>, which we discussed in Section 5.2. We varied the amount of memory reserved for the memory buffer, from 100 to 310 of the 56 kbyte blocks. Thus, the total of amount of memory used varied between 5.5 and 17 Mbytes. <p> Graefe <ref> [25] </ref> noted that the quotient operation can be implemented in four different ways. Two of these are based on sorting, and two on hashing. <p> The implementation of aggregate functions is discussed in Section 6.2.9, while the join was discussed in Section 6.2.3. Graefe <ref> [24, 25] </ref> found that the best algorithm for implementing the quotient operation is a direct method based on hashing. <p> If this is not the case, the cost writing the final relation will be less than that give by Equation 6.8. Thus, Equation 6.8 represents an upper bound on the cost of duplicate removal. In a survey paper on query evaluation techniques for large databases, Graefe <ref> [25] </ref> noted that duplicate removal must group potentially equal records together, compare them, and eliminate one if they are the same. Aggregate functions must often group records together, and produce some computed output for each group before "eliminating" the records.
Reference: [26] <author> G. Graefe, A. Linville, and L. D. Shapiro. </author> <title> Sort vs. hash revisited. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 6(6) </volume> <pages> 934-944, </pages> <month> December </month> <year> 1994. </year> <month> 188 </month>
Reference-contexts: There are three main types of join algorithm which are used when no indexes are available: nested loop, sort-merge and hash join. While often only one of the sort-merge and hash join algorithms is implemented in a database system, Graefe et al. <ref> [26] </ref> demonstrated that each performs better than the other under known circumstances and argued that each should be available in a database system. Variations of all three algorithms are analysed in subsequent chapters. <p> Many commercial database management systems implement the join using 57 the nested loop and sort-merge algorithms. Graefe et al. <ref> [26] </ref> demonstrated that the sort-merge join algorithm can perform better than the hash join algorithm under some circumstances, such as when the data is skewed, and the hash join algorithm performs better under others, such as when the relations being joined are different sizes. <p> If the data was skewed, it is quite possible that the sort-merge algorithm would perform better than the hash join algorithms. Thus, we do not believe that these results contradict those of Graefe et al. <ref> [26] </ref>. 112 Variable Minimum Maximum V 1 97 (0.8 Mb) 199627 (1.56 Gb) V R 60 (0.5 Mb) 256119 (2.00 Gb) Table 5.3: Range of values taken by V 1 , V 2 and V R , in blocks. 5.4.5 Join algorithm comparison: minimisation times We have shown that the minimal
Reference: [27] <author> P. J. Haas and A. N. Swami. </author> <title> Sequential sampling procedures for query size estimation. </title> <booktitle> In Proceedings of the 1991 ACM SIGMOD International Conference on the Management of Data, </booktitle> <pages> pages 341-350, </pages> <address> San Diego, California, USA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Sampling has been shown to produce good results for query optimisation by Lipton et al. [44], and Haas and Swami <ref> [27] </ref>. In our method a sample of each relation is read. Each record is hashed into a range of hash values. The size of the range is a multiple of the size of the desired number of partitions.
Reference: [28] <author> R. B. Hagmann. </author> <title> An observation on database buffering performance metrics. </title> <booktitle> In Proceedings of the Twelfth International Conference on Very Large Data Bases, </booktitle> <pages> pages 289-293, </pages> <address> Kyoto, Japan, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: Some have attempted to differentiate between different types of disk accesses. For example, DeWitt et al. [15] had separate I/O costs for sequential and random accesses. However, these models are less general than our I/O model. Hagmann <ref> [28] </ref> argued that, for current (1986) disk drive technology, when small numbers of blocks are transferred, the cost of locating the blocks is much greater than the cost of transferring them. He analysed the nested loop algorithm, counting only the number of disk I/Os (seeks). <p> The Nested loop (std) line corresponds to the standard version of the nested loop algorithm commonly described in the literature, in which B 1 = B 2, B 2 = B R = 1. The Nested loop (Hag) line corresponds to the version proposed by Hagmann <ref> [28] </ref>, in which B 1 = B 2 = (B 1)=2, B R = 1. Note that while Hagmann's version is faster than the standard version for larger relations, it is approximately twice as slow for most smaller relations, when B=2 &lt; V 1 &lt; B.
Reference: [29] <author> L. Harada, M. Nakano, M. Kitsuregawa, and M. Takagi. </author> <title> Query processing method for multi-attribute clustered relations. </title> <booktitle> In Proceedings of the Sixteenth International Conference on Very Large Data Bases, </booktitle> <pages> pages 59-70, </pages> <address> Brisbane, Australia, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: Using the clustering provided by a data structure to increase the performance of the join has been considered in the past, by Ozkarahan and Ouksel [63], Thom et al. [77] and Harada et al. <ref> [29] </ref>. However, none of the authors attempted to find the optimal clustering organisation. <p> the hash join. 4.5.6 Related work The idea of using a data structure to partition the data to implement a more efficient join algorithm is a feature of the partitioned join of Ozkarahan and Ouksel [63], the superjoin of Thom et al. [77], and the work of Harada et al. <ref> [29] </ref>. The partitioned join is applied to a multidimensional data structure, not unlike a multilevel version of the grid file. The superjoin is applied to a multikey hash file.
Reference: [30] <author> Y. Hsiao and A. L. Tharp. </author> <title> Adaptive hashing. </title> <journal> Information Systems, </journal> <volume> 13(1) </volume> <pages> 111-127, </pages> <year> 1988. </year>
Reference-contexts: That is, all the hash functions used to compose the multi-attribute hash key must be order preserving, and the most significant bits must be taken from the bit strings to form the hash key of a record. 2.2.2.3 Adaptive hashing The adaptive hashing of Hsiao and Tharp <ref> [30] </ref> aims to combine the features of order preserving linear hashing and the B+-tree [13].
Reference: [31] <author> K. A. Hua and C. Lee. </author> <title> Handling data skew in multiprocessor database computers using partition tuning. </title> <booktitle> In Proceedings of the Seventeenth International Conference on Very Large Data Bases, </booktitle> <pages> pages 525-535, </pages> <address> Barcelona, Spain, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: Even recent papers have used this cost model <ref> [31, 59, 60, 81] </ref>. These papers assume that the cost of transferring a number of consecutive blocks at once is the same as transferring them individually from random parts of the disk. Some have attempted to differentiate between different types of disk accesses.
Reference: [32] <author> L. Ingber and B. Rosen. </author> <title> Genetic algorithms and very fast simulated reannealing: a comparison. </title> <journal> Mathematical and Computer Modelling, </journal> <volume> 16(11) </volume> <pages> 87-100, </pages> <year> 1992. </year>
Reference-contexts: These include iterative improvement, which was used by Swami [76] for join query optimisation, the tabu search [22] and genetic algorithms. Additionally, more complex simulated annealing algorithms with sophisticated cooling functions and domain specific knowledge can perform better than the more general simulated annealing algorithms <ref> [14, 32] </ref>. It is likely that many of these techniques will find near-optimal solutions to the problems in the following chapters faster than the simulated annealing algorithm we used, if suitably tuned. <p> We implemented a number of simple versions of these other techniques during the course of our research, namely iterative improvement and very fast simulated annealing <ref> [32] </ref>. We also tried two genetic algorithm packages, Genocop (version 2.0) [52] and SGA-C [23, 74]. Each of these performed worse than simulated annealing in our tests. Iterative improvement and very fast simulated annealing took longer to find near-optimal solutions of similar quality to simulated annealing.
Reference: [33] <author> Y. E. Ioannidis and Y. C. Kang. </author> <title> Randomized algorithms for optimizing large join queries. </title> <booktitle> In Proceedings of the 1990 ACM SIGMOD International Conference on the Management of Data, </booktitle> <pages> pages 312-321, </pages> <address> Atlantic City, New Jersey, USA, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: It has also been suggested for use as a basis for other techniques in query optimisation. Some of these include the join query optimisation of Swami [76], the two phase optimisa-tion of Ioannidis and Kang <ref> [33] </ref>, and optimisation in parallel execution spaces by 35 Lanzelotte et al. [40].
Reference: [34] <author> W. Kim. </author> <title> A new way to compute the product and join of relations. </title> <booktitle> In Proceedings of the 1980 ACM SIGMOD International Conference on the Management of Data, </booktitle> <pages> pages 179-187, </pages> <year> 1980. </year>
Reference-contexts: The records in B 2 are then used to probe the hash 25 Main memory, B B 1 B 2 B R table, searching for records to join with. This significantly reduces the number of comparisons required. Rocking, suggested by Kim <ref> [34] </ref>, is used when the outer relation is larger than its memory buffer, B 1 . On the first pass through the inner relation, the inner relation is read from disk. On subsequent passes, part of the inner relation will already be in memory, from the previous pass. <p> However, their model still only counted the number of disk operations. Wolf et al. found their buffer allocation for this model using an exhaustive search of worthwhile buffer sizes, pruned using a branch-and-bound algorithm. They showed that their method was superior to Kim's <ref> [34] </ref> heuristic algorithm for dividing up the main memory buffer (which starts with the buffer divided equally between the input relations and searches for a nearby solution), and a number of other algorithms of their own.
Reference: [35] <author> M. Kitsuregawa, M. Nakayama, and M. Takagi. </author> <title> The effect of bucket size tuning in the dynamic hybrid GRACE hash join method. </title> <booktitle> In Proceedings of the Fifteenth International Conference on Very Large Data Bases, </booktitle> <pages> pages 257-266, </pages> <address> Amsterdam, The Netherlands, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: A number of algorithms have been proposed which do not assume this. Some of these were discussed in Section 2.3.3, and include the work by Kitsuregawa et al. <ref> [35, 56] </ref>. Any of these algorithms can be used in place of the GRACE hash join in our method without affecting the way in which we determine the optimal bit allocation for a file. Our method attempts to determine an optimal bit allocation for the hash join algorithm. <p> This can be modelled by setting T K = 1 and T T = 0. 5.2 Join algorithm costs Many papers have used the number of blocks transferred in their descriptions of the cost of disk operations <ref> [7, 15, 35, 36, 56, 72] </ref>. Even recent papers have used this cost model [31, 59, 60, 81]. These papers assume that the cost of transferring a number of consecutive blocks at once is the same as transferring them individually from random parts of the disk. <p> Their method dynamically determines which partitions will be stored in memory and which will be stored on disk. This depends upon the distribution of data. They later provided an analysis of DHGH and showed the effect of varying partition sizes <ref> [35] </ref>. They showed that a large number of small partitions is the best method to handle non-uniform distributions. These small partitions are combined for the partition joining phase to minimise the join cost.
Reference: [36] <author> M. Kitsuregawa, H. Tanaka, and T. Moto-oka. </author> <title> Application of hash to data base machine and its architecture. </title> <journal> New Generation Computing, </journal> <volume> 1(1) </volume> <pages> 66-74, </pages> <year> 1983. </year>
Reference-contexts: Here we describe the original partitioning hash join algorithm, the GRACE hash join, and an important variation, the hybrid hash join. Both algorithms work in two phases. 2.3.3.1 GRACE hash join During the first phase of the GRACE hash join algorithm <ref> [36] </ref>, each record of the input relations is read and a hash function is applied to the join attributes of the records. The result of applying the hash function is used to form a hash key for each record. <p> Merrett [51] argued that the sort-merge algorithm gives the best implementation of the natural join based on a theory of clustering relations. This conclusion has since been disputed by many, including Kitsuregawa et al. <ref> [36] </ref>, Bratbergsengen [8], and DeWitt et al. [15]. They showed that variations of the hash join algorithm perform better than the sort-merge algorithm when the initial data is unsorted. They also showed that, for small numbers of records, the nested loop algorithm performs better than the sort-merge algorithm. <p> This can be modelled by setting T K = 1 and T T = 0. 5.2 Join algorithm costs Many papers have used the number of blocks transferred in their descriptions of the cost of disk operations <ref> [7, 15, 35, 36, 56, 72] </ref>. Even recent papers have used this cost model [31, 59, 60, 81]. These papers assume that the cost of transferring a number of consecutive blocks at once is the same as transferring them individually from random parts of the disk.
Reference: [37] <author> G. D. Knott. </author> <title> Expandable open addressing hash table storage and retrieval. </title> <booktitle> In Proceedings of the ACM SIGFIDET Workshop on Data Description, Access and Control, </booktitle> <pages> pages 186-206, </pages> <year> 1971. </year>
Reference-contexts: A directory consists of a set of entries. Each directory entry has an associated hash key prefix and contains a pointer to a data (leaf) block. Other hashing schemes of this type include expandable hashing <ref> [37] </ref>, dynamic hashing [41], and virtual hashing [45]. The depth of a leaf block is defined to be the minimum number of bits in the hash key required to uniquely identify the block. The depth of the directory is the maximum depth amongst all the leaf blocks.
Reference: [38] <author> D. E. Knuth. </author> <title> Sorting and Searching, </title> <booktitle> volume 3 of The Art of Computer Programming. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, USA, </address> <year> 1973. </year>
Reference-contexts: During the merging phase, the available memory is divided into buffers for each partition of each relation, as shown in Figure 2.12. Note that the buffers of size B 1 and B 2 need not all be the same size. Replacement selection, as described by Knuth <ref> [38] </ref>, can also be used to generate the initial sorted partitions. <p> In addition to the uniform distribution, we performed experiments using relations in which the values of the attributes formed a Zipf distribution <ref> [38] </ref>. A representative example of the results is shown in Figure 5.20. In Figure 5.20 we compare the MGHU (Minimal GH with No Sampling) and MGHNU (Minimal GH with Sampling) algorithms.
Reference: [39] <author> H. Kriegel and B. Seeger. </author> <title> Multidimensional order preserving linear hashing with partial expansions. </title> <booktitle> In Proceedings of the International Conference on Database Theory, </booktitle> <pages> pages 203-220, </pages> <address> Rome, Italy, </address> <month> September </month> <year> 1986. </year> <month> 189 </month>
Reference-contexts: of records to hash keys the cost will be similar to extendible hashing, if the directory is stored on disk, or to linear hashing, if the directory can be maintained in memory. 2.2.2.4 Multidimensional order preserving linear hashing with partial expansions A dynamic hashing scheme introduced by Kriegel and Seeger <ref> [39] </ref>, multidimensional order preserving linear hashing with partial expansions (MOLHPE), combines multidimensional (multi-attribute) hashing, order preserving linear hashing, and linear hashing with partial expansions. <p> In MOLHPE, each dimension is treated equally. The key space of each dimension is mapped into [0; 1) by an order preserving hash function. Figure 2.7 (a) shows an example, by Kriegel and Seeger <ref> [39] </ref>, of a two dimensional data structure and the arrangement of (hashed) key space regions to block addresses. As in linear hashing with partial expansions, the file size is doubled by a series of partial expansions. Only one dimension is expanded at a time. <p> Instead of splitting a dimension into two by dividing the key space in half, it is split into two using the half quantile points. These points are stored in memory in a binary search tree. Figure 2.8 shows an example, similar to one by Kriegel and Seeger <ref> [39] </ref>, in which a key space is divided using half quantile points. They claimed that this method guarantees that MOLHPE performs almost as well for non-uniform distributions as it does for uniform distributions. <p> Related work includes that of Burkhard [9], Kriegel and Seeger <ref> [39] </ref> and Chen et al. [10]. As we discussed in Section 2.2.2, Burkhard proposed an extension to linear hashing which uses order preserving hash functions to support range queries. Algorithms to insert, update, delete and perform range queries on records were described.
Reference: [40] <author> R. S. G. Lanzelotte, P. Valduriez, and M. </author> <title> Za it. On the effectiveness of opti-mization search strategies for parallel execution spaces. </title> <booktitle> In Proceedings of the Nineteenth International Conference on Very Large Data Bases, </booktitle> <pages> pages 493-504, </pages> <address> Dublin, Ireland, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: It has also been suggested for use as a basis for other techniques in query optimisation. Some of these include the join query optimisation of Swami [76], the two phase optimisa-tion of Ioannidis and Kang [33], and optimisation in parallel execution spaces by 35 Lanzelotte et al. <ref> [40] </ref>.
Reference: [41] <author> P. A. Larson. </author> <title> Dynamic hashing. </title> <journal> BIT, </journal> <volume> 18(2) </volume> <pages> 184-201, </pages> <year> 1978. </year>
Reference-contexts: A directory consists of a set of entries. Each directory entry has an associated hash key prefix and contains a pointer to a data (leaf) block. Other hashing schemes of this type include expandable hashing [37], dynamic hashing <ref> [41] </ref>, and virtual hashing [45]. The depth of a leaf block is defined to be the minimum number of bits in the hash key required to uniquely identify the block. The depth of the directory is the maximum depth amongst all the leaf blocks.
Reference: [42] <author> P. A. Larson. </author> <title> Linear hashing with partial expansions. </title> <booktitle> In Proceedings of the Sixth International Conference on Very Large Data Bases, </booktitle> <pages> pages 224-232, </pages> <address> Montreal, Canada, </address> <month> October </month> <year> 1980. </year>
Reference-contexts: A linear hash file may be increased in size by either a single expansion stage, or by a series of partial expansions, as suggested by Larson <ref> [42] </ref>. We now describe the process of increasing its size using a single expansion stage. Consider the file represented in Figure 2.2. <p> If a fs , the block matching the address can be retrieved. If a &gt; fs , the block matching the address a 2 d must be retrieved. When the file size is increased, the steps of the following algorithm, which is functionally equivalent to one by Larson <ref> [42] </ref>, are performed: increment fs by one for each record r in the block indicated by sp calculate the hash key k using the first d + 1 bits of the choice vector add r to the block indicated by k (either block sp or sp + 2 d ) increment
Reference: [43] <author> J. Li, D. Rotem, and J. Srivastava. </author> <title> Algorithms for loading parallel grid files. </title> <booktitle> In Proceedings of the 1993 ACM SIGMOD International Conference on the Management of Data, </booktitle> <pages> pages 347-356, </pages> <address> Washington, DC, USA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Therefore, 184 the scope exists to both design algorithms which are guaranteed to produce optimal results, and to design algorithms which produce similar, or superior, results in less time. In recent years, much research has been conducted into the designing of parallel algorithms to implement relational databases operations <ref> [5, 43, 47, 68, 71, 73, 81] </ref>. We believe that the work in this thesis can be applied in this environment. For example, parallel join algorithms can be analysed with the cost model described in Chapter 5 to determine whether their use of memory can be improved.
Reference: [44] <author> R. J. Lipton, J. F. Naughton, and D. A. Schneider. </author> <title> Practical selectivity estimation through adaptive sampling. </title> <booktitle> In Proceedings of the 1990 ACM SIGMOD International Conference on the Management of Data, </booktitle> <pages> pages 1-11, </pages> <address> Atlantic City, New Jersey, USA, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: Additionally, their method does not support partitioning of the data in place, which has significant benefits for the extended GRACE and hybrid hash join algorithms. 5.6.1 Sampling Our proposal is to use sampling. Sampling has been shown to produce good results for query optimisation by Lipton et al. <ref> [44] </ref>, and Haas and Swami [27]. In our method a sample of each relation is read. Each record is hashed into a range of hash values. The size of the range is a multiple of the size of the desired number of partitions.
Reference: [45] <author> W. Litwin. </author> <title> Virtual hashing: A dynamically changing hashing. </title> <booktitle> In Proceedings of the Fourth International Conference on Very Large Data Bases, </booktitle> <pages> pages 517-523, </pages> <address> Berlin, West Germany, </address> <month> September </month> <year> 1978. </year>
Reference-contexts: A directory consists of a set of entries. Each directory entry has an associated hash key prefix and contains a pointer to a data (leaf) block. Other hashing schemes of this type include expandable hashing [37], dynamic hashing [41], and virtual hashing <ref> [45] </ref>. The depth of a leaf block is defined to be the minimum number of bits in the hash key required to uniquely identify the block. The depth of the directory is the maximum depth amongst all the leaf blocks.
Reference: [46] <author> W. Litwin. </author> <title> Linear hashing: a new tool for file and table addressing. </title> <booktitle> In Proceedings of the Sixth International Conference on Very Large Data Bases, </booktitle> <pages> pages 212-223, </pages> <address> Montreal, Canada, </address> <month> October </month> <year> 1980. </year>
Reference-contexts: An attribute is a field that can be specified in a query. Multi-attribute hashing is a method of creating a hash key from multiple attributes. It is used for storing records in a data file clustered using a hashing scheme, such as the linear hashing of Litwin <ref> [46] </ref>. We refer to the generation and use of hash keys from multiple attributes as multi-attribute hashing. A hash file organised using multikey hashing is referred to as having a multi-attribute hash index (MAH index). In this thesis, we refer to the block as the basic unit of storage. <p> If this is maintained, the file size must double to increase the size of a file. Instead of increasing the file size in one step from 2 d to 2 d+1 , Litwin proposed the linear hash file <ref> [46] </ref>, in which the size of the file increases one block at a time. Increasing the file size directly from 2 d to 2 d+1 is an expensive operation because all the records in the file must be examined to determine their new location.
Reference: [47] <author> W. Litwin and M.-A. Neimat. </author> <title> Distributed linear hashing. </title> <institution> Hewlett Packard Technical Memo HPL-DTD-92-7, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: Therefore, 184 the scope exists to both design algorithms which are guaranteed to produce optimal results, and to design algorithms which produce similar, or superior, results in less time. In recent years, much research has been conducted into the designing of parallel algorithms to implement relational databases operations <ref> [5, 43, 47, 68, 71, 73, 81] </ref>. We believe that the work in this thesis can be applied in this environment. For example, parallel join algorithms can be analysed with the cost model described in Chapter 5 to determine whether their use of memory can be improved.
Reference: [48] <author> J. W. Lloyd. </author> <title> Optimal partial-match retrieval. </title> <journal> BIT, </journal> <volume> 20 </volume> <pages> 406-413, </pages> <year> 1980. </year>
Reference-contexts: For example, Moran [54] showed that designing a particular optimal partial-match retrieval system was NP-hard. However, efficient algorithms have been found which can quickly find optimal or near-optimal solutions to this problem <ref> [2, 48, 49, 67] </ref>. Little work has been done on attempting to determine an optimal clustering of data for queries other than partial-match retrieval. Other clustering techniques have been proposed; however, they rarely consider the probability of an operation being asked to be performed. <p> Moran [54] showed that for the general problem, when the probability of an attribute appearing in a query is not independent of the other attributes, finding the optimal bit allocation is NP-hard. Lloyd <ref> [48] </ref> presented an efficient heuristic algorithm for finding a good solution to this general problem. 2.1.2 Dynamic files using linear hashing with partial expansions In the previous subsection, we assumed that the size of a hash file was of the form 2 d , where d is a non-negative integer. <p> Combining Equations 3.1 and 3.2, the average cost of a range query becomes C avg range = X n Y l m : (3.3) Note that the average cost of point queries, which is equivalent to the cost given by Lloyd <ref> [48] </ref>, is C avg point = X 0 Y 2 d i A : (3.4) Equation 3.3 can easily be extended to include point queries.
Reference: [49] <author> J. W. Lloyd and K. Ramamohanarao. </author> <title> Partial-match retrieval for dynamic files. </title> <journal> BIT, </journal> <volume> 22 </volume> <pages> 150-168, </pages> <year> 1982. </year>
Reference-contexts: For example, Moran [54] showed that designing a particular optimal partial-match retrieval system was NP-hard. However, efficient algorithms have been found which can quickly find optimal or near-optimal solutions to this problem <ref> [2, 48, 49, 67] </ref>. Little work has been done on attempting to determine an optimal clustering of data for queries other than partial-match retrieval. Other clustering techniques have been proposed; however, they rarely consider the probability of an operation being asked to be performed. <p> would be more likely to be b 1 1 b 3 1 b 1 2 b 3 2 b 1 3 b 3 3 , rather than b 1 1 b 1 3 b 1 1 b 2 3 b 2 1 b 3 3 b 3 Lloyd and Ramamohanarao <ref> [49] </ref> showed how near-optimal choice vectors can be constructed for dynamic files for partial-match retrieval if the probabilities of the partial-match retrieval queries are known. 10 2.1.3 Disadvantages of multi-attribute hashing One of the biggest disadvantages of multi-attribute hashing is that indexing information is wasted if attributes in a choice vector <p> For large numbers of large relations the directories would have to be held on disk. However, by implementing a directory cache, the cost of retrieving a disk block from the hash key of a record should be much less than two disk accesses, on average. Lloyd and Ramamohanarao <ref> [49] </ref> showed that extendible hashing is better than linear hashing for partial-match retrieval, even though extendible hashing has the overhead of directory lookups. <p> if c is less than the best cost, c 0 set the best cost, c 0 , to be c set ~ d min for the current d to be ~ d decrement d i by 1 set ~ d to be ~ d min According to Lloyd and Ramamohanarao <ref> [49] </ref>, this algorithm usually finds the optimal bit allocation for a relation when considering partial-match queries. <p> For example, if the directory of the grid file must be read from disk prior to each data block, the cost functions should be modified appropriately. However, if it can be stored in memory, no additional costs will be incurred. Lloyd and Ramamohanarao <ref> [49] </ref> showed that the cost of accessing an extendible hash file directory was small for partial-match retrieval, when compared to the cost of retrieving the data blocks. After the research described in this chapter was completed, Chen et al. [10] published a very similar scheme.
Reference: [50] <author> L. W. McVoy and S. R. Kleiman. </author> <title> Extent-like performance from a UNIX file system. </title> <booktitle> In Proceedings of the USENIX 1991 Winter Conference, </booktitle> <pages> pages 33-43, </pages> <address> Dallas, Texas, USA, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: The experiments were performed on an unloaded Sun SPARCstation IPX with 28 Mbytes of main memory. The elapsed time was recorded. The block size used was 56 kbytes, because the SunOS file system, as described by McVoy and Kleiman <ref> [50] </ref>, has some features of extent based file 72 Distrib. A : Cost ratio Distrib. <p> By having each algorithm optimise its own buffer usage, it has the opportunity to modify its own behaviour to improve the buffer usage. The use of an extent based file system [19], even under UNIX <ref> [50] </ref>, provides greater support for our technique than standard file systems, which do not guarantee that consecutive blocks are even on the same part of the disk. Although standard file systems do typically try to cluster contiguous blocks, extent based file systems achieve this to a greater degree. <p> Thus, we had no control over the physical disk accesses required to retrieve the data. As we briefly mentioned in Section 4.2.2, the SunOS file system, described by McVoy and Kleiman <ref> [50] </ref>, has a number of features of extent based file systems. It attempts to keep consecutive blocks in a cylinder group, it buffers disk blocks, and it reads disk blocks ahead in the expectation that they will be required. <p> This is a valid assumption in extent based file systems, which do just that [19]. As we discussed previously, many commercially available file systems, such as that of SunOS <ref> [50] </ref>, now implement features of extent based file systems and provide similar performance. * The records are uniformly distributed amongst all blocks in a multi-attribute hash file. Overflow blocks are not considered in our cost formulae.
Reference: [51] <author> T. H. Merrett. </author> <title> Why sort-merge gives the best implementation of the natural join. </title> <booktitle> SIGMOD Record, </booktitle> <volume> 13(2) </volume> <pages> 39-51, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: Merrett <ref> [51] </ref> argued that the sort-merge algorithm gives the best implementation of the natural join based on a theory of clustering relations. This conclusion has since been disputed by many, including Kitsuregawa et al. [36], Bratbergsengen [8], and DeWitt et al. [15].
Reference: [52] <author> Z. Michalewicz. </author> <title> Genetic algorithms + data structures = evolution programs. </title> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: We implemented a number of simple versions of these other techniques during the course of our research, namely iterative improvement and very fast simulated annealing [32]. We also tried two genetic algorithm packages, Genocop (version 2.0) <ref> [52] </ref> and SGA-C [23, 74]. Each of these performed worse than simulated annealing in our tests. Iterative improvement and very fast simulated annealing took longer to find near-optimal solutions of similar quality to simulated annealing.
Reference: [53] <author> P. Mishra and M. H. Eich. </author> <title> Join processing in relational databases. </title> <journal> ACM Computing Surveys, </journal> <volume> 24(1) </volume> <pages> 63-113, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: be easily used to find an optimal clustering arrangement? * If it is too large, can the amount of statistical information be reduced without significantly degrading the quality of the clustering arrangement which is then found? The join is a very important and expensive operation in relational database management systems <ref> [53, 78, 79] </ref>. A large amount of research has been conducted to find methods of efficiently implementing the join. <p> It was recently surveyed by Mishra and Eich <ref> [53] </ref>. There are three main types of join algorithm which are used when no indexes are available: nested loop, sort-merge and hash join.
Reference: [54] <author> S. Moran. </author> <title> On the complexity of designing optimal partial-match retrieval systems. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 8(4) </volume> <pages> 543-551, </pages> <month> December </month> <year> 1983. </year> <month> 190 </month>
Reference-contexts: For example, Moran <ref> [54] </ref> showed that designing a particular optimal partial-match retrieval system was NP-hard. However, efficient algorithms have been found which can quickly find optimal or near-optimal solutions to this problem [2, 48, 49, 67]. <p> We refer to this as the optimal bit allocation. Their method assumes that the probability of each attribute appearing in a query is specified, and that the probabilities are independent of each other. Moran <ref> [54] </ref> showed that for the general problem, when the probability of an attribute appearing in a query is not independent of the other attributes, finding the optimal bit allocation is NP-hard.
Reference: [55] <author> S. Nahar, S. Sahni, and E. Shargowitz. </author> <title> Experiments with simulated annealing. </title> <booktitle> In Proceedings of the 22nd Design Automation Conference, </booktitle> <pages> pages 748-752, </pages> <year> 1985. </year>
Reference-contexts: When used to search for the optimal bit allocation, the simulated annealing algorithm can be modified to ensure that an attribute is not allocated more than its constraining number of bits. While simulated annealing is not ideal for all optimisation applications, as shown by Nahar et al. <ref> [55] </ref>, in the past it has proved to be a useful means of obtaining near-optimal indexes in applications of the type we will consider [67]. It has also been suggested for use as a basis for other techniques in query optimisation.
Reference: [56] <author> M. Nakayama, M. Kitsuregawa, and M. Takagi. </author> <title> Hash-partitioned join method using dynamic destaging strategy. </title> <booktitle> In Proceedings of the Fifteenth International Conference on Very Large Data Bases, </booktitle> <pages> pages 468-478, </pages> <address> Los Angeles, California, USA, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: The following chapters primarily use the GRACE or hybrid hash algorithms as examples of hash join algorithms. However, the following join algorithms could also be used. Nakayama et al. <ref> [56] </ref> proposed using a dynamic destaging strategy during the partitioning phase. It creates many buckets into which the input relations are partitioned. The number of buckets is typically much greater than P . <p> A number of algorithms have been proposed which do not assume this. Some of these were discussed in Section 2.3.3, and include the work by Kitsuregawa et al. <ref> [35, 56] </ref>. Any of these algorithms can be used in place of the GRACE hash join in our method without affecting the way in which we determine the optimal bit allocation for a file. Our method attempts to determine an optimal bit allocation for the hash join algorithm. <p> This can be modelled by setting T K = 1 and T T = 0. 5.2 Join algorithm costs Many papers have used the number of blocks transferred in their descriptions of the cost of disk operations <ref> [7, 15, 35, 36, 56, 72] </ref>. Even recent papers have used this cost model [31, 59, 60, 81]. These papers assume that the cost of transferring a number of consecutive blocks at once is the same as transferring them individually from random parts of the disk. <p> As we discussed in Section 2.3.3, Nakayama et al. proposed an extension to the hybrid hash method, called the Dynamic Hybrid GRACE Hash join method (DHGH) <ref> [56] </ref>, to address this problem. Their method dynamically determines which partitions will be stored in memory and which will be stored on disk. This depends upon the distribution of data. They later provided an analysis of DHGH and showed the effect of varying partition sizes [35].
Reference: [57] <author> J. Nievergelt, H. Hinterberger, and K. C. Sevcik. </author> <title> The grid file: An adaptable, symmetric multikey file structure. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 9(1) </volume> <pages> 38-71, </pages> <month> March </month> <year> 1984. </year>
Reference-contexts: Similarly, the bits of a given attribute within a multi-attribute hash key can be used to indicate where in the MOLHPE key space an attribute value lies. 2.2.3 Grid file The grid file of Nievergelt et al. <ref> [57] </ref> is a multikey data structure which is general enough to encompass a number of different implementation strategies. Like the extendible hash file, it is composed of a (multidimensional) directory and data blocks. Several entries in the directory may point to the same data block.
Reference: [58] <author> K. J. Nurmela. </author> <title> Constructing combinatorial designs by local search. </title> <type> Technical Report A-27, </type> <institution> Digital Systems Laboratory, Department of Computer Science, Helsinki University of Technology, Finland, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: That is, genetic algorithms found a local minima faster than simulated annealing, but was unsuccessful in doing significantly better given more time. Our results correspond closely with those reported by Nurmela <ref> [58] </ref> in a different 36 problem domain. He found that simulated annealing typically performed as well or better than iterative improvement and a number of other combinatorial opti-misation methods including tabu search, threshold accepting and record-to-record travel.
Reference: [59] <author> E. Omiecinski. </author> <title> Performance analysis of a load balancing hash-join algorithm for a shared memory multiprocessor. </title> <booktitle> In Proceedings of the Seventeenth International Conference on Very Large Data Bases, </booktitle> <pages> pages 375-385, </pages> <address> Barcelona, Spain, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: Even recent papers have used this cost model <ref> [31, 59, 60, 81] </ref>. These papers assume that the cost of transferring a number of consecutive blocks at once is the same as transferring them individually from random parts of the disk. Some have attempted to differentiate between different types of disk accesses.
Reference: [60] <author> E. Omiecinski and E. T. Lin. </author> <title> The adaptive-hash join algorithm for a hypercube multicomputer. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(3) </volume> <pages> 334-349, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Even recent papers have used this cost model <ref> [31, 59, 60, 81] </ref>. These papers assume that the cost of transferring a number of consecutive blocks at once is the same as transferring them individually from random parts of the disk. Some have attempted to differentiate between different types of disk accesses.
Reference: [61] <author> J. A. Orenstein. </author> <title> A dynamic hash file for random and sequential access. </title> <booktitle> In Proceedings of the Ninth International Conference on Very Large Data Bases, </booktitle> <pages> pages 132-141, </pages> <address> Florence, Italy, </address> <month> November </month> <year> 1983. </year>
Reference-contexts: in the directory which are accessed is small when compared with the number of leaf blocks which must be retrieved to answer a partial-match query. 2.2.2.2 Multilevel order preserving linear hashing Order preserving linear hashing was independently discovered by Burkhard [9], Oren-stein, and Ouksel and Scheuermann [62], according to Orenstein <ref> [61] </ref>. It is implemented by using an order preserving hash function to generate the hash key for records which are then stored in a linear hash file. <p> 14 Block 0 1 2 3 4 5 6 7 8 9 N 2 4 2 6 - 5 6 5 - 4 Level 2 4 3 3 - 3 3 3 - 4 Normal Level 4 3 4 To overcome these problems, Orenstein proposed multilevel order preserving linear hashing <ref> [61] </ref>. The problem of long overflow chains was reduced by storing the overflow blocks of each hash key in a B+-tree instead of in a list. <p> The problem of sparse blocks was reduced by assigning different levels (depths) to the blocks stored in order preserving linear hash files, and by eliminating sparsely filled blocks. hash file. It is taken from an example by Orenstein <ref> [61] </ref>. and 9 would normally be at level four, while the other blocks would normally be at level three. Let us assume that the capacity of a block is four records, and a block is sparsely filled if it contains zero or one records.
Reference: [62] <author> M. Ouksel and P. Scheuermann. </author> <title> Storage mappings for multidimensional linear dynamic hashing. </title> <booktitle> In Proceedings of the Second ACM SIGACT-SIGMOD Symposium on Principles of Database Systems, </booktitle> <pages> pages 90-105, </pages> <year> 1983. </year>
Reference-contexts: the number of blocks in the directory which are accessed is small when compared with the number of leaf blocks which must be retrieved to answer a partial-match query. 2.2.2.2 Multilevel order preserving linear hashing Order preserving linear hashing was independently discovered by Burkhard [9], Oren-stein, and Ouksel and Scheuermann <ref> [62] </ref>, according to Orenstein [61]. It is implemented by using an order preserving hash function to generate the hash key for records which are then stored in a linear hash file.
Reference: [63] <author> E. A. Ozkarahan and M. Ouksel. </author> <title> Dynamic and order preserving data partitioning for database machines. </title> <booktitle> In Proceedings of the Eleventh International Conference on Very Large Data Bases, </booktitle> <pages> pages 358-368, </pages> <address> Stockholm, Sweden, </address> <year> 1985. </year>
Reference-contexts: A large amount of research has been conducted to find methods of efficiently implementing the join. Using the clustering provided by a data structure to increase the performance of the join has been considered in the past, by Ozkarahan and Ouksel <ref> [63] </ref>, Thom et al. [77] and Harada et al. [29]. However, none of the authors attempted to find the optimal clustering organisation. <p> implicit partitioning can also be used to reduce the cost of the partitioning phase of the hash join. 4.5.6 Related work The idea of using a data structure to partition the data to implement a more efficient join algorithm is a feature of the partitioned join of Ozkarahan and Ouksel <ref> [63] </ref>, the superjoin of Thom et al. [77], and the work of Harada et al. [29]. The partitioned join is applied to a multidimensional data structure, not unlike a multilevel version of the grid file. The superjoin is applied to a multikey hash file.
Reference: [64] <author> H. Pang, M. J. Carey, and M. Livny. </author> <title> Partially preemptible hash joins. </title> <booktitle> In Proceedings of the 1993 ACM SIGMOD International Conference on the Management of Data, </booktitle> <pages> pages 59-68, </pages> <address> Washington, DC, USA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Partitions can be split by dividing the buckets between new partitions if the amount of memory available decreases, and partitions can be joined if the amount of memory available increases. Pang et al. <ref> [64] </ref> introduced a class of partially preemptible hash joins. It is more general than the scheme of Zeller and Gray. Not only does it turn internal partitions into external partitions when the amount of available memory decreases, but it turns external partitions into internal partitions if more memory becomes available. <p> Our analysis is a generalisation of these two cost models and allows the relative, or absolute, cost of each disk and CPU operation to be specified. Other researchers, such as Cheng et al. [11] and Pang et al. <ref> [64] </ref>, have used a cost model of similar power to ours when evaluating their algorithms. However, they do not attempt to optimise the buffer usage based on this information and often read a block at a time from disk during each I/O operation. <p> This assumption may be relaxed during the execution of the query by using a partially preemptible hash join <ref> [64] </ref>, which was discussed in Sec tion 2.3.3. * Two blocks with consecutive hash keys are located contiguously on the disk, and can therefore be retrieved with one read operation. This is a valid assumption in extent based file systems, which do just that [19].
Reference: [65] <author> K. Ramamohanarao and J. W. Lloyd. </author> <title> Dynamic hashing schemes. </title> <journal> The Computer Journal, </journal> <volume> 25 </volume> <pages> 478-485, </pages> <year> 1982. </year>
Reference-contexts: The value of N is then set to 2N , so that there are G groups of 2N blocks instead of 2G groups of N blocks, and the process starts again. This was analysed and discussed in more detail by Ramamohanarao and Lloyd <ref> [65] </ref>. this example G = 2. The first partial expansion moves some records from two blocks to a third, and the second partial expansion moves some records from three blocks into a fourth. <p> For example, if the load factor is 80% with 50 records per block, the multiplying factor is 1.0725 (the unsuccessful search length). The method for calculating these multiplying factors for various load and blocking factors has been described by Folk and Zoellick [19] and Ramamohanarao and Lloyd <ref> [65] </ref>. * The size of the data file is of the form 2 d blocks. We make this assumption because we wish to construct a choice vector of length d for a multi-attribute hash file, where d is an integer.
Reference: [66] <author> K. Ramamohanarao and R. Sacks-Davis. </author> <title> Recursive linear hashing. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 8(9) </volume> <pages> 369-391, </pages> <month> September </month> <year> 1984. </year>
Reference-contexts: Thus, the blocks to be retrieved are identified without accessing the disk. 2.2.1.1 Recursive linear hashing The recursive linear hashing scheme of Ramamohanarao and Sacks-Davis <ref> [66] </ref> is an alternative approach to the storing of overflow blocks in linear hash files.
Reference: [67] <author> K. Ramamohanarao, J. Shepherd, and R. Sacks-Davis. </author> <title> Multi-attribute hashing with multiple file copies for high performance partial-match retrieval. </title> <journal> BIT, </journal> <volume> 30 </volume> <pages> 404-423, </pages> <year> 1990. </year>
Reference-contexts: For example, Moran [54] showed that designing a particular optimal partial-match retrieval system was NP-hard. However, efficient algorithms have been found which can quickly find optimal or near-optimal solutions to this problem <ref> [2, 48, 49, 67] </ref>. Little work has been done on attempting to determine an optimal clustering of data for queries other than partial-match retrieval. Other clustering techniques have been proposed; however, they rarely consider the probability of an operation being asked to be performed. <p> The algorithm we used is substantially the same as that used by Ramamohanarao et al. <ref> [67] </ref>. Our simulated annealing algorithm implementation performs a number of trials, T , and returns the ~ d with the smallest value for the cost function from amongst the trials. <p> While simulated annealing is not ideal for all optimisation applications, as shown by Nahar et al. [55], in the past it has proved to be a useful means of obtaining near-optimal indexes in applications of the type we will consider <ref> [67] </ref>. It has also been suggested for use as a basis for other techniques in query optimisation. Some of these include the join query optimisation of Swami [76], the two phase optimisa-tion of Ioannidis and Kang [33], and optimisation in parallel execution spaces by 35 Lanzelotte et al. [40]. <p> When perturbing, a relation is first chosen at random to perturb. Then two of its attributes and the number of bits to perturb by are chosen. Another method of implementing MMI for multiple file copies is exhaustive MMI <ref> [67] </ref>. However, Ramamohanarao et al. [67] demonstrated that it does not perform as well as simulated annealing when both the average query cost and min-imisation time is taken into account, so we did not use it. 2.4.6 Terminology The combinatorial optimisation algorithms we have described are not guaranteed to find the <p> When perturbing, a relation is first chosen at random to perturb. Then two of its attributes and the number of bits to perturb by are chosen. Another method of implementing MMI for multiple file copies is exhaustive MMI <ref> [67] </ref>. However, Ramamohanarao et al. [67] demonstrated that it does not perform as well as simulated annealing when both the average query cost and min-imisation time is taken into account, so we did not use it. 2.4.6 Terminology The combinatorial optimisation algorithms we have described are not guaranteed to find the optimal solution to any problem. <p> Three simulated annealing algorithms were tested, each with different parameters. The sets of parameters were the same as that used by Ra-mamohanarao et al. <ref> [67] </ref>. The values of T and P are shown in Table 3.2. We tested ten query distributions on a lightly loaded Sun SPARCstation 2 with 48 Mbytes of main memory. Distribution 1 had three attributes and five query bits per attribute, resulting in 32768 queries. <p> Ramamohanarao et al. <ref> [67] </ref> showed that the performance of point queries can be significantly improved if bits are allocated optimally, rather than equally. Range queries are a generalisation of point queries. Therefore, there must be a set of range queries whose performance can be significantly improved if the bits are allocated optimally. <p> In this chapter, we discuss how multi-attribute hash indexes can be constructed to increase the performance of join queries. As the cost of mass storage devices decreases, it is becoming feasible to store multiple copies of data, each with a different clustering organisation. Ramamoha-narao et al. <ref> [67] </ref> previously used multiple copies of data files, each with a different clustering organisation, to improve the performance of partial-match retrieval.
Reference: [68] <author> J. P. Richardson, H. Lu, and K. Mikkilineni. </author> <title> Design and evaluation of parallel pipelined join algorithms. </title> <booktitle> In Proceedings of the 1987 ACM SIGMOD International Conference on the Management of Data, </booktitle> <pages> pages 399-409, </pages> <address> San Francisco, California, USA, </address> <month> May </month> <year> 1987. </year>
Reference-contexts: If the increased number of variables results in the minimisation algorithms becoming too slow, simulated annealing can be used to find a minimal buffer allocation. 5.8 Parallelism In recent years, a large amount of research has taken place into parallel join algorithms, particularly parallel hash join algorithms <ref> [68, 71, 73, 81] </ref>. Many of these algorithms are based on existing hash join algorithms, often the hybrid hash join algorithm. We believe that our technique will be just as important in this domain as in the sequential case. <p> Therefore, 184 the scope exists to both design algorithms which are guaranteed to produce optimal results, and to design algorithms which produce similar, or superior, results in less time. In recent years, much research has been conducted into the designing of parallel algorithms to implement relational databases operations <ref> [5, 43, 47, 68, 71, 73, 81] </ref>. We believe that the work in this thesis can be applied in this environment. For example, parallel join algorithms can be analysed with the cost model described in Chapter 5 to determine whether their use of memory can be improved.
Reference: [69] <author> T. J. Sager. </author> <title> A polynomial time generator for minimal perfect hash functions. </title> <journal> Communications of the ACM, </journal> <volume> 28(5) </volume> <pages> 523-532, </pages> <month> May </month> <year> 1985. </year>
Reference-contexts: For example, if the data is stored using in a linear hash file, long overflow chains can result. This increases the average search length. There have been a number of proposals to try to reduce this problem. One approach is the perfect hash function <ref> [12, 69] </ref> which hashes the data in such a way that for any attribute value, there is a unique hash value. However, this method assumes a static distribution of a fixed amount data. <p> The hash function for an attribute of this type must map each attribute value to a unique bit string. Hash functions of this type are referred to as perfect hash functions. Generating perfect hash functions can be time consuming <ref> [12, 69] </ref>, but only needs to be performed once. For attributes such as "month", a hash function of this type can be constructed easily using a lookup table for the values. We do not expect that a high proportion attributes will be maximally allocated in any given index.
Reference: [70] <author> H. Samet. </author> <title> The Design and Analysis of Spatial Data Structures. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, USA, </address> <year> 1989. </year>
Reference-contexts: They can be categorised as spatial data structures which are able to efficiently store and access point data, in addition to the region data and queries with which they are primarily concerned. Samet <ref> [70] </ref> discusses more of these.
Reference: [71] <author> D. A. Schneider and D. J. DeWitt. </author> <title> A performance evaluation of four parallel join algorithms in a shared-nothing multiprocessor environment. </title> <booktitle> In Proceedings of the 1989 ACM SIGMOD International Conference on the Management of Data, </booktitle> <pages> pages 110-121, </pages> <address> Portland, Oregon, USA, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: If the increased number of variables results in the minimisation algorithms becoming too slow, simulated annealing can be used to find a minimal buffer allocation. 5.8 Parallelism In recent years, a large amount of research has taken place into parallel join algorithms, particularly parallel hash join algorithms <ref> [68, 71, 73, 81] </ref>. Many of these algorithms are based on existing hash join algorithms, often the hybrid hash join algorithm. We believe that our technique will be just as important in this domain as in the sequential case. <p> Therefore, 184 the scope exists to both design algorithms which are guaranteed to produce optimal results, and to design algorithms which produce similar, or superior, results in less time. In recent years, much research has been conducted into the designing of parallel algorithms to implement relational databases operations <ref> [5, 43, 47, 68, 71, 73, 81] </ref>. We believe that the work in this thesis can be applied in this environment. For example, parallel join algorithms can be analysed with the cost model described in Chapter 5 to determine whether their use of memory can be improved.
Reference: [72] <author> L. D. Shapiro. </author> <title> Join processing in database systems with large main memories. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 11(3) </volume> <pages> 239-264, </pages> <month> September </month> <year> 1986. </year>
Reference-contexts: This can be modelled by setting T K = 1 and T T = 0. 5.2 Join algorithm costs Many papers have used the number of blocks transferred in their descriptions of the cost of disk operations <ref> [7, 15, 35, 36, 56, 72] </ref>. Even recent papers have used this cost model [31, 59, 60, 81]. These papers assume that the cost of transferring a number of consecutive blocks at once is the same as transferring them individually from random parts of the disk. <p> We generalise the hybrid hash join algorithm and allow it to have multiple partitioning passes, although this was not the application for which it was originally intended <ref> [72] </ref>, nor was it the version described in Section 2.3.3. We set the number of partitions created, P , to be a single value. However, it could vary for each of the passes.
Reference: [73] <author> A. Shatdal and J. F. Naughton. </author> <title> Using shared virtual memory for parallel join processing. </title> <booktitle> In Proceedings of the 1993 ACM SIGMOD International Conference on the Management of Data, </booktitle> <pages> pages 119-128, </pages> <address> Washington, DC, USA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: If the increased number of variables results in the minimisation algorithms becoming too slow, simulated annealing can be used to find a minimal buffer allocation. 5.8 Parallelism In recent years, a large amount of research has taken place into parallel join algorithms, particularly parallel hash join algorithms <ref> [68, 71, 73, 81] </ref>. Many of these algorithms are based on existing hash join algorithms, often the hybrid hash join algorithm. We believe that our technique will be just as important in this domain as in the sequential case. <p> Therefore, 184 the scope exists to both design algorithms which are guaranteed to produce optimal results, and to design algorithms which produce similar, or superior, results in less time. In recent years, much research has been conducted into the designing of parallel algorithms to implement relational databases operations <ref> [5, 43, 47, 68, 71, 73, 81] </ref>. We believe that the work in this thesis can be applied in this environment. For example, parallel join algorithms can be analysed with the cost model described in Chapter 5 to determine whether their use of memory can be improved.
Reference: [74] <author> R. E. Smith, D. E. Goldberg, and J. A. Earickson. SGA-C: </author> <title> A C-language implementation of a simple genetic algorithm. </title> <type> Technical Report 91002, </type> <institution> The Clearinghouse for Genetic Algorithms, Department of Engineering Mechanics, The University of Alabama, Tuscaloosa, Alabama, USA, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: We implemented a number of simple versions of these other techniques during the course of our research, namely iterative improvement and very fast simulated annealing [32]. We also tried two genetic algorithm packages, Genocop (version 2.0) [52] and SGA-C <ref> [23, 74] </ref>. Each of these performed worse than simulated annealing in our tests. Iterative improvement and very fast simulated annealing took longer to find near-optimal solutions of similar quality to simulated annealing.
Reference: [75] <author> W. Sun, Y. Ling, N. Rishe, and Y. Deng. </author> <title> An instant and accurate size estimation method for joins and selection in a retrieval-intensive environment. </title> <booktitle> In Proceedings of the 1993 ACM SIGMOD International Conference on the Management of Data, </booktitle> <pages> pages 79-88, </pages> <address> Washington, DC, USA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Further work arising from this chapter includes a comparison of how other methods of handling non-uniform data distributions impact on our scheme. For example, a comparison with other methods of determining data distributions, such as the size estimation techniques of Sun and Ling <ref> [75] </ref>, would be interesting. 131 132 Chapter 6 Clustering Relations for General Queries There are a number of basic operations required for a database management system to answer queries. Efficient algorithms must be available to perform these operations.
Reference: [76] <author> A. Swami. </author> <title> Optimization of large join queries: combining heuristic and combinatorial techniques. </title> <booktitle> In Proceedings of the 1989 ACM SIGMOD International Conference on the Management of Data, </booktitle> <pages> pages 367-376, </pages> <address> Portland, Oregon, USA, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: It has also been suggested for use as a basis for other techniques in query optimisation. Some of these include the join query optimisation of Swami <ref> [76] </ref>, the two phase optimisa-tion of Ioannidis and Kang [33], and optimisation in parallel execution spaces by 35 Lanzelotte et al. [40]. <p> There are a number of other techniques which could be used in addition to MMI and simulated annealing to search for optimal bit allocations. These include iterative improvement, which was used by Swami <ref> [76] </ref> for join query optimisation, the tabu search [22] and genetic algorithms. Additionally, more complex simulated annealing algorithms with sophisticated cooling functions and domain specific knowledge can perform better than the more general simulated annealing algorithms [14, 32].
Reference: [77] <author> J. A. Thom, K. Ramamohanarao, and L. Naish. </author> <title> A superjoin algorithm for deductive databases. </title> <booktitle> In Proceedings of the Twelfth International Conference on Very Large Data Bases, </booktitle> <pages> pages 189-196, </pages> <address> Kyoto, Japan, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: A large amount of research has been conducted to find methods of efficiently implementing the join. Using the clustering provided by a data structure to increase the performance of the join has been considered in the past, by Ozkarahan and Ouksel [63], Thom et al. <ref> [77] </ref> and Harada et al. [29]. However, none of the authors attempted to find the optimal clustering organisation. <p> The relations can be sorted using the hashed sort key instead of the attribute value in the sort-merge algorithm. A variation of this idea appears in the superjoin of Thom et al. <ref> [77] </ref>, and all hash joins. We now define a notation to denote sorting on combinations of attributes. Let (A 1 ; : : : ; A n ) be the result of sorting a relation on A 1 , then A 2 , and so on. <p> reduce the cost of the partitioning phase of the hash join. 4.5.6 Related work The idea of using a data structure to partition the data to implement a more efficient join algorithm is a feature of the partitioned join of Ozkarahan and Ouksel [63], the superjoin of Thom et al. <ref> [77] </ref>, and the work of Harada et al. [29]. The partitioned join is applied to a multidimensional data structure, not unlike a multilevel version of the grid file. The superjoin is applied to a multikey hash file.
Reference: [78] <author> J. D. Ullman. </author> <booktitle> Principles of Database and Knowledge-Base Systems, </booktitle> <volume> volume 1. </volume> <publisher> Computer Science Press, </publisher> <address> Rockville, Maryland, USA, </address> <year> 1988. </year>
Reference-contexts: be easily used to find an optimal clustering arrangement? * If it is too large, can the amount of statistical information be reduced without significantly degrading the quality of the clustering arrangement which is then found? The join is a very important and expensive operation in relational database management systems <ref> [53, 78, 79] </ref>. A large amount of research has been conducted to find methods of efficiently implementing the join. <p> The cost model we used to calculate the cost of a range query is more accurate than those used by others, such as Ullman <ref> [78] </ref> and Chen et al. [10]. Furthermore, the approach used is general enough that it can be applied to many other data structures, including the k-d-tree, grid file and BANG file. The primary difficulty when range query distributions are considered is the number of queries which can be asked.
Reference: [79] <author> J. D. Ullman. </author> <booktitle> Principles of Database and Knowledge-Base Systems, </booktitle> <volume> volume 2. </volume> <publisher> Computer Science Press, </publisher> <address> Rockville, Maryland, USA, </address> <year> 1989. </year>
Reference-contexts: be easily used to find an optimal clustering arrangement? * If it is too large, can the amount of statistical information be reduced without significantly degrading the quality of the clustering arrangement which is then found? The join is a very important and expensive operation in relational database management systems <ref> [53, 78, 79] </ref>. A large amount of research has been conducted to find methods of efficiently implementing the join. <p> Efficient algorithms must be available to perform these operations. The standard basic relational operations are selection, projection, join, intersection, union, difference and quotient, as described by, for example, Ullman <ref> [79] </ref>. The two operations which have been the subject of the most research are selection and join. The reason that the other relational operations have not received as much attention is primarily because the implementation of the other operations, except projection, is similar to that of the join.
Reference: [80] <author> J. Vaghani, K. Ramamohanarao, D. B. Kemp, Z. Somogyi, P. J. Stuckey, T. S. Leask, and J. Harland. </author> <title> The Aditi deductive database system. </title> <journal> The VLDB Journal, </journal> <volume> 3(2) </volume> <pages> 245-288, </pages> <year> 1994. </year> <month> 192 </month>
Reference-contexts: By first sorting both relations, the merging phase is performed in linear time in the size of the relations. The sort-merge algorithm presented below is similar to the version used in the Aditi deductive database system <ref> [80] </ref>, and is more general than that described above. During the sorting phase, the relations are not fully sorted. Instead, each relation is divided into partitions which are sorted. The size of the partitions is the size of the memory buffer, B. <p> When the cost of a join is calculated, the difference in this time should be taken into account. The CPU cost of a join should also be taken into account. Experience with the Aditi deductive database, by Vaghani et al. <ref> [80] </ref>, showed that the disk access and transfer times amount to between 10% and 20% of the time taken to perform a join. Thus, the CPU time is an important factor which should be considered when determining the most efficient method to perform any given join.
Reference: [81] <author> C. B. Walton, A. G. Dale, and R. M. Jenevein. </author> <title> A taxonomy and performance model of data skew effects in parallel joins. </title> <booktitle> In Proceedings of the Seventeenth International Conference on Very Large Data Bases, </booktitle> <pages> pages 537-548, </pages> <address> Barcelona, Spain, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: Even recent papers have used this cost model <ref> [31, 59, 60, 81] </ref>. These papers assume that the cost of transferring a number of consecutive blocks at once is the same as transferring them individually from random parts of the disk. Some have attempted to differentiate between different types of disk accesses. <p> If the increased number of variables results in the minimisation algorithms becoming too slow, simulated annealing can be used to find a minimal buffer allocation. 5.8 Parallelism In recent years, a large amount of research has taken place into parallel join algorithms, particularly parallel hash join algorithms <ref> [68, 71, 73, 81] </ref>. Many of these algorithms are based on existing hash join algorithms, often the hybrid hash join algorithm. We believe that our technique will be just as important in this domain as in the sequential case. <p> Therefore, 184 the scope exists to both design algorithms which are guaranteed to produce optimal results, and to design algorithms which produce similar, or superior, results in less time. In recent years, much research has been conducted into the designing of parallel algorithms to implement relational databases operations <ref> [5, 43, 47, 68, 71, 73, 81] </ref>. We believe that the work in this thesis can be applied in this environment. For example, parallel join algorithms can be analysed with the cost model described in Chapter 5 to determine whether their use of memory can be improved.
Reference: [82] <author> G. Weikum. </author> <title> Set-oriented disk access to large complex objects. </title> <booktitle> In Proceedings of the Fifth International Conference on Data Engineering, </booktitle> <pages> pages 426-433, </pages> <year> 1989. </year>
Reference-contexts: We believe that a minimal buffer allocation should be calculated rather than using a single ad hoc cluster size for all joins. Techniques such as the chain reading of Weikum <ref> [82] </ref> can be used to read a set of related blocks at one time to reduce the seek time.
Reference: [83] <author> K.-Y. Whang and R. Krishnamurthy. </author> <title> The multilevel grid file | a dynamic hierarchical multidimensional file structure. </title> <booktitle> In International Symposium on Database Systems for Advanced Applications, </booktitle> <pages> pages 449-459, </pages> <address> Tokyo, Japan, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Another solution is to use a data structure which has a greater tolerance for correlated attributes, such as the BANG file [20] or multilevel grid file <ref> [83] </ref>. These data structures are discussed in the next section. 2.2 Multi-attribute hashing and other data structures A choice vector can be viewed as specifying the order in which the n dimensional attribute space of a relation is divided. <p> The order of the bits in the choice vector can be used to determine which dimension to split on. The multilevel grid file is also based on this approach. 2.2.4 Multilevel grid file The multilevel grid file of Whang and Krishnamurthy <ref> [83] </ref> was designed to overcome the problem of the grid file directory size when it is implemented as a multidimensional array. It achieves this by redefining how a grid entry is calculated and by making the directory a tree. Figure 2.9, by Whang and Krishnamurthy [83], shows a partitioned data space <p> file of Whang and Krishnamurthy <ref> [83] </ref> was designed to overcome the problem of the grid file directory size when it is implemented as a multidimensional array. It achieves this by redefining how a grid entry is calculated and by making the directory a tree. Figure 2.9, by Whang and Krishnamurthy [83], shows a partitioned data space in which the dashed boxes represent data blocks. Below it there is a two level directory for the data space. The design of the multilevel grid file differs in some details from the grid file.
Reference: [84] <author> J. L. Wolf, B. R. Iyer, K. R. Pattipati, and J. Turek. </author> <title> Optimal buffer partitioning for the nested block join algorithm. </title> <booktitle> In Proceedings of the Seventh International Conference on Data Engineering, </booktitle> <pages> pages 510-519, </pages> <address> Kobe, Japan, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Under the cost model which counts the number of blocks transferred, the inner relation is provided with one block of memory, and the remaining memory is devoted to the outer relation. This minimises the number of passes over the inner relation. Wolf et al. <ref> [84] </ref> provided a similar analysis to Hagmann, with a more accurate cost model. Hagmann's cost function was continuous, that of Wolf et al. contains 93 the ceiling function. However, their model still only counted the number of disk operations.
Reference: [85] <author> H. Zeller and J. Gray. </author> <title> An adaptive hash join algorithm for multiuser environments. </title> <booktitle> In Proceedings of the Sixteenth International Conference on Very Large Data Bases, </booktitle> <pages> pages 186-197, </pages> <address> Brisbane, Australia, </address> <month> August </month> <year> 1990. </year> <pages> 193 194 </pages>
Reference-contexts: No comparison was made between the amount of time saved using this method and the amount of time taken to determine a good grouping. Zeller and Gray <ref> [85] </ref> described an adaptive hash join. It is a variation of the hybrid hash join which allows the amount of memory available to perform the join in to fluctuate during the execution of the join.
References-found: 85

