URL: file://ftp.cis.ohio-state.edu/pub/communication/papers/ipps98-hipiqs.ps.Z
Refering-URL: http://www.cis.ohio-state.edu/~sivaram/publications.html
Root-URL: 
Email: Email: fsivaram,pandag@cis.ohio-state.edu Email: stunkel@watson.ibm.com  
Title: HIPIQS: A High-Performance Switch Architecture using Input Queuing  
Author: Rajeev Sivaram Craig B. Stunkel Dhabaleswar K. Panda 
Address: P. O. Box 218 Columbus, OH 43210 Yorktown Heights, NY 10598  
Affiliation: Dept. of Computer and Information Science IBM T. J. Watson Research Center The Ohio State University  
Note: To appear in the Proceedings of the 12th IEEE International Parallel Processing Symposium (IPPS '98), April 1998, Orlando, FL  
Abstract: Switch-based interconnects are used in a number of application domains including parallel system interconnects, local area networks, and wide area networks. However, very few switches have been designed that are suitable for more than one of these application domains. Such a switch must offer both extremely low latency and very high throughput for a variety of different message sizes. While some architectures with output queuing have been shown to perform extremely well in terms of throughput, their performance can suffer when used in systems where a significant portion of the packets are extremely small. On the other hand, architectures with input queuing offer limited throughput, or require fairly complex and centralized arbitration that increases latency. In this paper we present a new input queue-based switch architecture called HIPIQS (HIgh-Performance Input-Queued Switch). It offers low latency for a range of message sizes, and provides throughput comparable to that of output queuing approaches. Furthermore, it allows simple and distributed arbitration. HIPIQS uses a dynamically allocated multi-queue organization, pipelined access to multi-bank input buffers, and small cross-point buffers, to deliver high performance. Our simulation results show that HIPIQS can deliver performance close to that of output queuing approaches over a range of message sizes, system sizes, and traffic. The switch architecture can therefore be used to build high performance switches that are useful for both parallel system interconnects and for building computer networks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> ATM Forum. </author> <title> ATM User-Network Interface Specification, </title> <note> Version 3.1, </note> <month> September </month> <year> 1994. </year>
Reference-contexts: On the other hand, network switches place greater emphasis on throughput while tolerating latencies that are higher by orders of magnitude. Furthermore, many of these switches are built for fixed size packets (such as cells in ATM networks <ref> [1] </ref>). An important design decision involves the queuing of blocked packets in the buffers of a switch.
Reference: [2] <author> N. J. Boden, D. Cohen, and et al. Myrinet: </author> <title> A Gigabit-per Second Local Area Network. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 2935, </pages> <month> Feb </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Switch-based interconnects are used for a large number of applications. Such applications vary from low-latency interconnects for parallel systems based on networks of workstations <ref> [2, 5, 6, 17] </ref> to local area and wide area networks [4, 10]. Current generation switches have typically been built keeping particular application domains in mind. For example, parallel system interconnects place heavy emphasis on reducing latency and on accommodating a range of packet sizes.
Reference: [3] <author> G. A. Boughton. </author> <title> Arctic routing chip. </title> <booktitle> In Lecture Notes in Computer Science, </booktitle> <volume> volume 853, </volume> <pages> pages 310317. </pages> <publisher> Springer Verlag, </publisher> <year> 1994. </year>
Reference-contexts: Instead, an arriving packet may use any of the buffers, usually the one that has the least number of packets in it. Such a scheme has been used for the PaRC [7], Arctic <ref> [3] </ref>, and HAL's PRC [11] switch chips. In these switches, k buffers are maintained at every input port (where k is the number of input and output ports in the switch) and an arriving packet may use any of these k buffers. <p> The crossbar complexity of such a scheme is higher than those of the DAMQ or cross-point schemes <ref> [3] </ref> as each buffer is an input to the crossbar. <p> The DAMQ approach adds complexity of the required central arbitration. The cost of this arbitration varies with its optimality and with the time constraints in which it operates. It must be noted that several contemporary switches such as Arctic <ref> [3] </ref>, HAL's PRC [11], and IBM's Prizma [4], already incur crossbar complexity similar to HIPIQS (approximately k 3 Furthermore, the HIPIQS architecture has significant potential as a basis for multicast.
Reference: [4] <author> W. E. Denzel, A. P. J. Engbersen, and I. Iliadis. </author> <title> Flexible shared-buffer switch for ATM at Gb/s rates. Computer Net works and ISDN Systems, </title> <address> 27(4):611624, </address> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Switch-based interconnects are used for a large number of applications. Such applications vary from low-latency interconnects for parallel systems based on networks of workstations [2, 5, 6, 17] to local area and wide area networks <ref> [4, 10] </ref>. Current generation switches have typically been built keeping particular application domains in mind. For example, parallel system interconnects place heavy emphasis on reducing latency and on accommodating a range of packet sizes. <p> In particular, a form of output queuing that uses a shared central buffer has been shown to achieve extremely good performance and such an approach is currently being used in a number of current day switches <ref> [4, 17] </ref>. In such a switch, multiple input and output ports may want to access this central buffer concurrently. We therefore need some mechanism to match the input and output bandwidths through the switch. <p> Such an approach has been used in the IBM Prizma switch <ref> [4] </ref>. The primary drawback of this scheme is that when it is used in systems with varying packet sizes, the solution leads to extensive memory fragmentation or large crossbars. <p> The DAMQ approach adds complexity of the required central arbitration. The cost of this arbitration varies with its optimality and with the time constraints in which it operates. It must be noted that several contemporary switches such as Arctic [3], HAL's PRC [11], and IBM's Prizma <ref> [4] </ref>, already incur crossbar complexity similar to HIPIQS (approximately k 3 Furthermore, the HIPIQS architecture has significant potential as a basis for multicast.
Reference: [5] <author> M. Galles. Spider: </author> <title> A High-Speed Network Interconnect. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 3439, </pages> <month> January/February </month> <year> 1997. </year>
Reference-contexts: 1 Introduction Switch-based interconnects are used for a large number of applications. Such applications vary from low-latency interconnects for parallel systems based on networks of workstations <ref> [2, 5, 6, 17] </ref> to local area and wide area networks [4, 10]. Current generation switches have typically been built keeping particular application domains in mind. For example, parallel system interconnects place heavy emphasis on reducing latency and on accommodating a range of packet sizes. <p> Current-day switches such as the SGI Spider <ref> [5] </ref> and Tiny Tera [10] use such a queuing scheme coupled with centralized arbiters. Input queuing approaches may also use multiple buffers per input port. One example of such a buffering scheme has separate buffers for every output port.
Reference: [6] <author> D. Garcia and W. Watson. </author> <title> Servernet II. </title> <booktitle> In Proceedings of the 2nd Parallel Computer Routing and Communication Workshop, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: 1 Introduction Switch-based interconnects are used for a large number of applications. Such applications vary from low-latency interconnects for parallel systems based on networks of workstations <ref> [2, 5, 6, 17] </ref> to local area and wide area networks [4, 10]. Current generation switches have typically been built keeping particular application domains in mind. For example, parallel system interconnects place heavy emphasis on reducing latency and on accommodating a range of packet sizes. <p> If the destination output port for the packet at the head of the queue is busy, the packets behind it are subjected to head of line (HOL) blocking delays. Such a scheme has been shown to have a maximum throughput of around 60% [8]. The switches of Servernet <ref> [6] </ref>, In-tel Paragon, and the CM5 use such a queuing scheme. To overcome this HOL blocking, schemes with multiple input queues within the same input buffer have been proposed [19]. Typically, there is one queue corresponding to every output port in every input buffer.
Reference: [7] <author> C. F. Joerg and A. Boughton. </author> <title> The Monsoon Interconnection Network. </title> <booktitle> In Proceedings of the International Conference on Computer Design, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: Instead, an arriving packet may use any of the buffers, usually the one that has the least number of packets in it. Such a scheme has been used for the PaRC <ref> [7] </ref>, Arctic [3], and HAL's PRC [11] switch chips. In these switches, k buffers are maintained at every input port (where k is the number of input and output ports in the switch) and an arriving packet may use any of these k buffers. <p> More interestingly, the performance of HIPIQS very nearly matches the performance of output queuing for a significant range of message sizes. Furthermore, HIPIQS also does better than the published result of [19]. Thus, HIP-IQS performs much better than the other input queued ar 9 chitectures presented in the literature <ref> [19, 7] </ref> As mentioned above, this establishes that the performance of HIPIQS is close to the best achievable using either input or output queu ing. * The performance of HIPIQS depends on the size of the messages as a fraction of the input buffer size.
Reference: [8] <author> M. Karol, M. Hluchyj, and S. Morgan. </author> <title> Input versus Output Queuing on a Space-Division Packet Switch. </title> <journal> IEEE Trans actions on Communications, </journal> <volume> 35(12):13471356, </volume> <month> Dec. </month> <year> 1987. </year>
Reference-contexts: Two major queuing organi fl This research is supported in part by NSF Career Award MIP-9502294, NSF Grant CCR-9704512, and an IBM Cooperative Fellowship. zations have been proposed in the literature: input queuing and output queuing <ref> [8, 19] </ref>. In input queuing, packet queues are maintained at input buffers. The packets in an input queue may be destined for any of the switch's output ports. However, the packets in a given input queue all arrive from a particular input port of the switch. <p> Furthermore, these queues are in dedicated buffers associated with every output port or within a single buffer shared by all output ports. It has been shown that output queuing performs extremely well <ref> [8] </ref>, and much better than input queuing. In particular, a form of output queuing that uses a shared central buffer has been shown to achieve extremely good performance and such an approach is currently being used in a number of current day switches [4, 17]. <p> If the destination output port for the packet at the head of the queue is busy, the packets behind it are subjected to head of line (HOL) blocking delays. Such a scheme has been shown to have a maximum throughput of around 60% <ref> [8] </ref>. The switches of Servernet [6], In-tel Paragon, and the CM5 use such a queuing scheme. To overcome this HOL blocking, schemes with multiple input queues within the same input buffer have been proposed [19]. Typically, there is one queue corresponding to every output port in every input buffer. <p> As can be seen from Fig. 7 the architecture that uses input queuing with a single queue (labeled `FIFO') saturates well before the applied load reaches 0.60. This is consistent with previous research results <ref> [8] </ref>. Since the performance of the single FIFO queue approach is considerably worse than the performance of both HIPIQS and output queuing, we do not consider its performance in the remaining part of this section. The relative performance of input and output queuing is of greater interest.
Reference: [9] <author> M. Katevenis, P. Vatsolaki, and A. Efthymiou. </author> <title> Pipelined memory shared buffer for VLSI switches. </title> <booktitle> In Proc. ACM SIGCOMM Conference, </booktitle> <pages> pages 3948, </pages> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: Furthermore, it must be capable of delivering high throughput (close to 100%). The switch must also be capable of handling a wide range of message sizes efficiently. In this paper, we present the architecture of such a switch HIPIQS (HIgh-Performance Input-Queued Switch)which uses input queuing, pipelined multi-queue input buffers <ref> [9] </ref>, cut-through switching, and simple distributed arbitration to achieve very high performance (low latency as well as high throughput). <p> Input queuing approaches may also use multiple buffers per input port. One example of such a buffering scheme has separate buffers for every output port. Such a scheme is referred to as cross-point buffering <ref> [9] </ref>, and is equivalent to the SAFC (Statically Allocated, Fully Connected) scheme studied in [19]. A crosspoint buffering scheme can theoretically achieve 100% throughput. <p> A similar principle applies to the reads by the output ports. This scheme achieves a performance similar to the `wide memory' approach described above, but may result in a simpler implementation. Such a scheme is described in <ref> [9] </ref>. The main problem with these single module approaches occurs when packets are smaller than a chunk. <p> In the context of output queuing with shared (centralized) buffers, we have already seen that a buffer with a single read port can satisfy the requests of k output ports concurrently by allowing ports to read in k-flit chunks or by pipelining the concurrent reads through a multi-bank buffer <ref> [9] </ref>. The same principle can be applied to make an input buffer with a DAMQ organization behave like a buffer with multiple read ports. This would eliminate the necessity for output ports to remain idle if all packets destined for them are in buffers that are already being read from. <p> This helps reduce the amount of fragmented space in the buffers. More importantly, it almost eliminates the loss of bandwidth due to multiple output ports accessing packets which are smaller than a chunk. An architecture with cross-point buffering <ref> [9] </ref> also has certain advantages. Most importantly, since every input port-output port pair has a dedicated buffer, an output port can immediately begin transferring data from a corresponding dedicated buffer as soon as it has decided which input port it is to read from next. <p> In this section we use the term output queuing to refer to architectures with a shared central buffer, in particular, a shared buffer consisting of a single module <ref> [9, 17] </ref>. The previously proposed input queuing schemes perform worse than output queuing [19]. By establishing that HIPIQS performs almost as well as output queuing, we show that HIPIQS performs better than previously proposed input queuing schemes.
Reference: [10] <author> N. McKeown, M. Izzard, A. Mekkittikul, W. Ellersick, and M. Horowitz. </author> <title> Tiny Tera: A Packet Switch Core. </title> <booktitle> IEEE Mi cro, </booktitle> <pages> pages 2633, </pages> <month> January/February </month> <year> 1997. </year>
Reference-contexts: 1 Introduction Switch-based interconnects are used for a large number of applications. Such applications vary from low-latency interconnects for parallel systems based on networks of workstations [2, 5, 6, 17] to local area and wide area networks <ref> [4, 10] </ref>. Current generation switches have typically been built keeping particular application domains in mind. For example, parallel system interconnects place heavy emphasis on reducing latency and on accommodating a range of packet sizes. <p> Typically, there is one queue corresponding to every output port in every input buffer. Thus, every queue has packets that have arrived from a given input port and that are all destined for the same output port. This scheme is also referred to as virtual output queuing <ref> [10] </ref> because of this. The space for these multiple queues may be allocated statically, wherein, the buffer space is divided among the input queues (according to some distribution) apriori. <p> Current-day switches such as the SGI Spider [5] and Tiny Tera <ref> [10] </ref> use such a queuing scheme coupled with centralized arbiters. Input queuing approaches may also use multiple buffers per input port. One example of such a buffering scheme has separate buffers for every output port.
Reference: [11] <author> A. Mu, J. Larson, R. Sastry, T. Wicki, and W. W. Wilcke. </author> <title> A 9.6 GigaByte/s Throughput Plesiochronous Routing Chip. </title> <booktitle> In Proceedings of COMPCON '96, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: Instead, an arriving packet may use any of the buffers, usually the one that has the least number of packets in it. Such a scheme has been used for the PaRC [7], Arctic [3], and HAL's PRC <ref> [11] </ref> switch chips. In these switches, k buffers are maintained at every input port (where k is the number of input and output ports in the switch) and an arriving packet may use any of these k buffers. <p> The DAMQ approach adds complexity of the required central arbitration. The cost of this arbitration varies with its optimality and with the time constraints in which it operates. It must be noted that several contemporary switches such as Arctic [3], HAL's PRC <ref> [11] </ref>, and IBM's Prizma [4], already incur crossbar complexity similar to HIPIQS (approximately k 3 Furthermore, the HIPIQS architecture has significant potential as a basis for multicast.
Reference: [12] <author> D. K. Panda, D. Basak, D. Dai, R. Kesavan, R. Sivaram, M. Banikazemi, and V. Moorthy. </author> <title> Simulation of Modern Par allel Systems: A CSIM-based approach. </title> <booktitle> In Proceedings of the Winter Simulation Conference (WSC'97), </booktitle> <pages> pages 1013 1020, </pages> <month> Dec. </month> <year> 1997. </year>
Reference-contexts: We first describe our basic methodology and our base configuration. We then examine the effect of various system parameters on the performance of HIPIQS in greater detail. 5.1 Experiments We carried out experiments using a C++/CSIM based simulation testbed <ref> [12] </ref> which models cut-through routing on a flit by flit basis. A bidirectional Multistage Interconnection Network (BMIN) based parallel system was assumed for our experiments and various synthetic workloads were used to evaluate the influence of the different system parameters on performance.
Reference: [13] <author> C. Partridge. </author> <title> Gigabit Networking. </title> <publisher> Addison-Wesley, </publisher> <year> 1994. </year>
Reference-contexts: The latter approach performs better, is easier to implement, and is the most prevalent <ref> [13] </ref>. We will therefore restrict the following discussion to this approach. Since all output queues exist in a single shared buffer, at any given time, all input ports could be contending to write to the same buffer, while all output ports could be contending to read from the same buffer.
Reference: [14] <author> R. Sivaram, D. K. Panda, and C. B. Stunkel. </author> <title> Efficient Broad cast and Multicast on Multistage Interconnection Networks using Multiport Encoding. </title> <journal> IEEE Transactions on Parallel and Distributed Systems. </journal> <note> In Press. </note>
Reference-contexts: In this paper we have focussed on point to point (unicast) message communication. Multicasting of packets which have multiple destination ports at a given switch can also be supported in HIP-IQS with architectural enhancements similar to the ones proposed in <ref> [14, 18] </ref>. However, the details of multicast implementation are beyond the scope of the current paper and hence are not included. 4 Qualitative Comparison of Alternative Switch Architectures HIPIQS is targeted for switching environments that require high throughput even for very small packets.
Reference: [15] <author> R. Sivaram, C. B. Stunkel, and D. K. Panda. HIPIQS: </author> <title> A High Performance Switch Architecture using Input Queu ing. </title> <type> Technical Report OSU-CISRC-10/97-TR45, </type> <month> Oct. </month> <year> 1997. </year>
Reference-contexts: We examine the impact of two types of non-uniform trafficbit-reverse and transpose. Our experiments on 16-node systems with HIPIQS have shown that bit-reverse and transpose traffic can be transmitted without saturation for applied loads close to 1.0 <ref> [15] </ref>. We therefore only report our results for the more interesting case of a 64-node system. We also provide a comparison with the shared central buffer-based output queuing approach.
Reference: [16] <author> C. B. </author> <title> Stunkel. </title> <booktitle> Challenges in the Design of Contemporary Routers. In Proceedings of the 2nd Parallel Computer Rout ing and Communication Workshop, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: It can also be observed that the bandwidth of links used for interconnection has continued to increase in the recent years <ref> [16] </ref>. This increase in link bandwidth has caused a further increase in the width of the data paths within output-queued switches with shared central buffers. <p> The 1996 version of the IBM SP switches contain a total of 5 KBytes of storage with 2 bytes/flit. This corresponds to 320 flit buffers for an 8-port input buffered switch with equal total storage. Recognizing improvements in VLSI technology and the trend towards using larger buffers in switches <ref> [16] </ref>, we assume a default buffer size of 640 flits for the switches with input buffers and an equivalent amount of space for the central buffered switch.
Reference: [17] <author> C. B. Stunkel, D. G. Shea, B. Abali, et al. </author> <title> The SP2 High Performance Switch. </title> <journal> IBM System Journal, </journal> <volume> 34(2):185204, </volume> <year> 1995. </year>
Reference-contexts: 1 Introduction Switch-based interconnects are used for a large number of applications. Such applications vary from low-latency interconnects for parallel systems based on networks of workstations <ref> [2, 5, 6, 17] </ref> to local area and wide area networks [4, 10]. Current generation switches have typically been built keeping particular application domains in mind. For example, parallel system interconnects place heavy emphasis on reducing latency and on accommodating a range of packet sizes. <p> In particular, a form of output queuing that uses a shared central buffer has been shown to achieve extremely good performance and such an approach is currently being used in a number of current day switches <ref> [4, 17] </ref>. In such a switch, multiple input and output ports may want to access this central buffer concurrently. We therefore need some mechanism to match the input and output bandwidths through the switch. <p> We therefore need some mechanism to match the input and output bandwidths through the switch. A practical approach for matching input and output bandwidths through a switch with a shared central buffer is to use wider data paths within the switch <ref> [17] </ref>. Input and output bandwidths are matched by scheduling one write and one read to/from the shared buffer every cycle. <p> This requires the input ports to buffer k flits before writing and output ports to serialize the transmission of the k-flit chunk after reading. Such an approach is used by the IBM SP2 switch <ref> [17] </ref>. Another solution that eliminates the requirement for such a `wide memory' is to split the memory module into multiple banks. <p> In this section we use the term output queuing to refer to architectures with a shared central buffer, in particular, a shared buffer consisting of a single module <ref> [9, 17] </ref>. The previously proposed input queuing schemes perform worse than output queuing [19]. By establishing that HIPIQS performs almost as well as output queuing, we show that HIPIQS performs better than previously proposed input queuing schemes.
Reference: [18] <author> C. B. Stunkel, R. Sivaram, and D. K. Panda. </author> <title> Implementing Multidestination Worms in Switch-Based Parallel Systems: Architectural Alternatives and their Impact. </title> <booktitle> In Proceedings of the 24th IEEE/ACM Annual International Symposium on Computer Architecture (ISCA-24), </booktitle> <pages> pages 5061, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: In this paper we have focussed on point to point (unicast) message communication. Multicasting of packets which have multiple destination ports at a given switch can also be supported in HIP-IQS with architectural enhancements similar to the ones proposed in <ref> [14, 18] </ref>. However, the details of multicast implementation are beyond the scope of the current paper and hence are not included. 4 Qualitative Comparison of Alternative Switch Architectures HIPIQS is targeted for switching environments that require high throughput even for very small packets.
Reference: [19] <author> Y. Tamir and G. L. Frazier. </author> <title> Dynamically-Allocated Multi Queue Buffers for VLSI Communication Switches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(6):725737, </volume> <month> June </month> <year> 1996. </year> <month> 10 </month>
Reference-contexts: Two major queuing organi fl This research is supported in part by NSF Career Award MIP-9502294, NSF Grant CCR-9704512, and an IBM Cooperative Fellowship. zations have been proposed in the literature: input queuing and output queuing <ref> [8, 19] </ref>. In input queuing, packet queues are maintained at input buffers. The packets in an input queue may be destined for any of the switch's output ports. However, the packets in a given input queue all arrive from a particular input port of the switch. <p> Such a scheme has been shown to have a maximum throughput of around 60% [8]. The switches of Servernet [6], In-tel Paragon, and the CM5 use such a queuing scheme. To overcome this HOL blocking, schemes with multiple input queues within the same input buffer have been proposed <ref> [19] </ref>. Typically, there is one queue corresponding to every output port in every input buffer. Thus, every queue has packets that have arrived from a given input port and that are all destined for the same output port. <p> An alternative is to dynamically allocate space among the queues according to the proportion of the traffic that is intended for the corresponding destination output ports. Such a scheme is 2 referred to as DAMQ <ref> [19] </ref> (dynamically allocated multi-queue). Under the DAMQ scheme, packets at the head of the individual (FIFO) queues are eligible for transmission. However, only one of these packets may be transmitted from the buffer at any given time. Thus, there is a single data output per input buffer. <p> Input queuing approaches may also use multiple buffers per input port. One example of such a buffering scheme has separate buffers for every output port. Such a scheme is referred to as cross-point buffering [9], and is equivalent to the SAFC (Statically Allocated, Fully Connected) scheme studied in <ref> [19] </ref>. A crosspoint buffering scheme can theoretically achieve 100% throughput. However, the main problem with cross-point buffering is that the space associated with an input port is not shared dynamically: traffic in which some output ports are more frequently used than others will be adversely affected. <p> We then describe the structure of each of the input buffers in the switch. Finally, we describe the steps involved in message communication using the proposed architecture. 3.1 Basic Idea One of the limiting factors of the DAMQ <ref> [19] </ref> performance is that each of its (input) buffers has a single read port. Because of this, no more than one output port can read from a buffer at any given time. <p> In this section we use the term output queuing to refer to architectures with a shared central buffer, in particular, a shared buffer consisting of a single module [9, 17]. The previously proposed input queuing schemes perform worse than output queuing <ref> [19] </ref>. By establishing that HIPIQS performs almost as well as output queuing, we show that HIPIQS performs better than previously proposed input queuing schemes. Furthermore, this also demonstrates that the performance of HIPIQS is close to the best achievable by either input or output queuing. ulations: a 16-node BMIN. <p> More interestingly, the performance of HIPIQS very nearly matches the performance of output queuing for a significant range of message sizes. Furthermore, HIPIQS also does better than the published result of <ref> [19] </ref>. <p> More interestingly, the performance of HIPIQS very nearly matches the performance of output queuing for a significant range of message sizes. Furthermore, HIPIQS also does better than the published result of [19]. Thus, HIP-IQS performs much better than the other input queued ar 9 chitectures presented in the literature <ref> [19, 7] </ref> As mentioned above, this establishes that the performance of HIPIQS is close to the best achievable using either input or output queu ing. * The performance of HIPIQS depends on the size of the messages as a fraction of the input buffer size.
References-found: 19

