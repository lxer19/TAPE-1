URL: ftp://ftp.aic.nrl.navy.mil/pub/papers/1990/AIC-90-012.ps
Refering-URL: http://www.aic.nrl.navy.mil/~schultz/papers.html
Root-URL: 
Email: EMAIL: schultz@aic.nrl.navy.mil  
Phone: (202) 767-2685  
Title: Improving Tactical Plans with Genetic Algorithms  
Author: Alan C. Schultz John J. Grefenstette 
Address: (Code 5514),  DC 20375-5000, U.S.A.  
Affiliation: Navy Center for Applied Research in Artificial Intelligence  Naval Research Laboratory, Washington,  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A. G., R. S. Sutton and C. J. C. H. </author> <title> Watkins (1989). Learning and sequential decision making. </title> <type> COINS Technical Report, </type> <institution> University of Massachusetts, Amherst. </institution>
Reference-contexts: In fact, the paradigm is quite broad since it includes any problem solving task by defining the payoff to be positive for any goal state and null for non-goal states <ref> (Barto, Sutton & Watkins, 1989) </ref>. For many interesting sequential decision tasks, there exists neither a database of examples nor a complete and tractable domain theory. <p> For many interesting sequential decision tasks, there exists neither a database of examples nor a complete and tractable domain theory. In these cases, one _____ ____________ _ 1 If payoff is accumulated over an infinite period, the total payoff is usually defined to be a (finite) time-weighted sum <ref> (Barto et. al, 1989) </ref>. method for manually developing a set of decision rules is to test a hypothetical set of rules against a simulation model of the task environment, and to incrementally modify the decision rules on the basis of the simulated experience.
Reference: <author> Erickson, M. D. & J. M. </author> <title> Zytkow (1988). Utilizing experience for improving the tactical manager. </title> <booktitle> Proceedings of the Fifth International Conference on Machine Learning. </booktitle> <address> Ann Arbor, MI. </address> <pages> (pp. 444-450). </pages>
Reference: <author> Goldberg, D. E. </author> <year> (1983). </year> <title> Computer-aided gas pipeline operation using genetic algorithms and machine learning, </title> <type> Doctoral dissertation, </type> <institution> Department Civil Engineering, University of Michigan, </institution> <address> Ann Arbor. </address>
Reference-contexts: This cycle repeats indefinitely. The objective is to find a set of decision rules that maximizes the expected total payoff. 1 Several sequential decision tasks have been investigated in the machine learning literature, including pole balancing (Selfridge, Sutton & Barto, 1985), gas pipeline control <ref> (Goldberg, 1983) </ref>, and the animat problem (Wilson, 1985; Wilson, 1987). For many problems, including the one considered here, payoff is delayed in the sense that non-null payoff occurs only at the end of an episode that may span several decision steps.
Reference: <author> Gordon, D. G & J. J. </author> <title> Grefenstette (1990). Explanations of empirically derived reactive plans. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning. </booktitle> <address> Austin, TX: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: First, it is easier to transfer the knowledge learned to human operators. Second, it makes it possible to combine empirical methods such as genetic algorithms with analytic learning methods that explain the success of the empirically derived rules <ref> (Gordon & Grefenstette, 1990) </ref>. Finally, it makes it easier to incorporate existing knowledge, whether acquired from experts or by symbolic learning programs. This paper addresses this final point by comparing two mechanisms for initializing the knowledge structures in SAMUEL. <p> An analysis of the credit assignment methods in SAMUEL appears in (Grefenstette, 1988). A study of the effects of sensor noise on SAMUEL appears in (Schultz, Ramsey & Gre-fenstette, 1990), and techniques for applying explanation-based learning methods to the empirically derived rules learned by SAMUEL appears in <ref> (Gordon & Grefenstette, 1990) </ref>. The Evasive Maneuvers Problem We will focus our discussion on a particular sequential decision task called the Evasive Maneuvers (EM) problem, inspired in part by Erickson and Zytkow (1988). There are two object of interest in this problem, a plane and a missile.
Reference: <author> Grefenstette, J. J. </author> <year> (1987). </year> <title> Incorporating problem specific knowledge into genetic algorithms. In Genetic algorithms and simulated annealing. </title> <publisher> L. </publisher>
Reference-contexts: These results should provide an interesting contrast with most published work on genetic algorithms, which usually assume tabula rasa initial conditions, although some studies have investigated seeding the initial population with available knowledge <ref> (Grefenstette, 1987) </ref>. The results presented here show that genetic algorithms can be used to improve partially correct decision rules, as well as to __ _____________ ___ 2 SAMUEL stands for Strategy Acquisition Method Using Empirical Learning.
Reference: <editor> Davis (ed.), </editor> <publisher> London: Pitman Press. </publisher>
Reference: <author> Grefenstette, J. J. </author> <year> (1988). </year> <title> Credit assignment in rule discovery system based on genetic algorithms. </title> <journal> Machine Learning, </journal> <volume> 3(2/3), </volume> <pages> (pp. 225-245). </pages>
Reference-contexts: The current system is detailed in (Grefenstette, Ramsey & Schultz, 1990). The design of SAMUEL owes much to Smith's LS-1 system (Smith, 1980), and draws on some ideas from classifier systems (Holland, 1986). An analysis of the credit assignment methods in SAMUEL appears in <ref> (Grefenstette, 1988) </ref>. A study of the effects of sensor noise on SAMUEL appears in (Schultz, Ramsey & Gre-fenstette, 1990), and techniques for applying explanation-based learning methods to the empirically derived rules learned by SAMUEL appears in (Gordon & Grefenstette, 1990). <p> In addition to matching, CPS implements conflict resolution as a competition among rules based on rule strength and performs credit assignment based on payoff <ref> (Grefenstette, 1988) </ref>. <p> Recombination of two selected plan is controlled by a CROSSOVER operator that exchanges a randomly chosen number of rules. The rules to be exchanged are selected so that rules that fire in sequence are more likely to be inherited as a group <ref> (Grefenstette, 1988) </ref>. A background MUTATION operator makes small changes to plans by randomly altering individual conditions or actions. For details about the genetic learning operators, see (Grefenstette, 1989a). <p> the experiments below, we elected to seed half of the initial population with the heuristic plans, with the remaining half being maximally general. _____ ____________ _ 4 However, conflict resolution is probabilistic, so it is possible that a low strength rule might be selected over a conflicting high strength rule <ref> (Grefenstette, 1988) </ref>. An Experimental Comparison This section presents an empirical study of the use of existing domain knowledge to initialize the knowledge structures that comprise the initial population.
Reference: <author> Grefenstette, J. J. </author> <year> (1989a). </year> <title> A system for learning control strategies with genetic algorithms. </title> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms. </booktitle> <address> Fairfax, VA: </address> <publisher> Morgan Kauf-mann. </publisher> <pages> (pp. 183-190) Grefenstette, </pages> <editor> J. J. </editor> <booktitle> (1989b). Incremental learning of control strategies with genetic algorithms Proceedings of the Sixth International Workshop on Machine Learning. </booktitle> <address> Ithaca, NY: </address> <publisher> Morgan Kaufmann. </publisher> <pages> (pp. 340-344). </pages>
Reference-contexts: The rules to be exchanged are selected so that rules that fire in sequence are more likely to be inherited as a group (Grefenstette, 1988). A background MUTATION operator makes small changes to plans by randomly altering individual conditions or actions. For details about the genetic learning operators, see <ref> (Grefenstette, 1989a) </ref>. The remainder of this section focuses on how to form the knowledge structures in the initial population, if no prior knowledge about the task is available.
Reference: <author> Grefenstette, J. J., C. Loggia Ramsey, and A. C. </author> <note> Schultz (1990). Simulation-assisted learning by competition. To appear in Machine Learning. </note>
Reference-contexts: First, it is easier to transfer the knowledge learned to human operators. Second, it makes it possible to combine empirical methods such as genetic algorithms with analytic learning methods that explain the success of the empirically derived rules <ref> (Gordon & Grefenstette, 1990) </ref>. Finally, it makes it easier to incorporate existing knowledge, whether acquired from experts or by symbolic learning programs. This paper addresses this final point by comparing two mechanisms for initializing the knowledge structures in SAMUEL. <p> The final section summarizes our findings and discusses topics for further study. This work is part of an on-going study of genetic algorithms for learning tactical plans. The current system is detailed in <ref> (Grefenstette, Ramsey & Schultz, 1990) </ref>. The design of SAMUEL owes much to Smith's LS-1 system (Smith, 1980), and draws on some ideas from classifier systems (Holland, 1986). An analysis of the credit assignment methods in SAMUEL appears in (Grefenstette, 1988). <p> An analysis of the credit assignment methods in SAMUEL appears in (Grefenstette, 1988). A study of the effects of sensor noise on SAMUEL appears in (Schultz, Ramsey & Gre-fenstette, 1990), and techniques for applying explanation-based learning methods to the empirically derived rules learned by SAMUEL appears in <ref> (Gordon & Grefenstette, 1990) </ref>. The Evasive Maneuvers Problem We will focus our discussion on a particular sequential decision task called the Evasive Maneuvers (EM) problem, inspired in part by Erickson and Zytkow (1988). There are two object of interest in this problem, a plane and a missile.
Reference: <author> Holland, J. H. </author> <year> (1975). </year> <booktitle> Adaptation in natural and artificial systems. </booktitle> <address> Ann Arbor: </address> <publisher> University Michi-gan Press. </publisher>
Reference-contexts: In this approach, each decision policy, or tactical plan, is represented as a set of condition-action rules. A simulation of the sequential decision task provides the basis for measuring the performance for any proposed plan, and a genetic algorithm <ref> (Holland, 1975) </ref> is used to evolve high-performance plans. The approach has been implemented in a system called SAMUEL 2 (Grefenstette, 1989b; Grefenstette, Ramsey & Schultz, 1990).
Reference: <author> Holland J. H. </author> <year> (1986). </year> <title> Escaping brittleness: The possibilities of general-purpose learning algorithms applied to parallel rule-based systems. In R.S. </title>
Reference-contexts: This work is part of an on-going study of genetic algorithms for learning tactical plans. The current system is detailed in (Grefenstette, Ramsey & Schultz, 1990). The design of SAMUEL owes much to Smith's LS-1 system (Smith, 1980), and draws on some ideas from classifier systems <ref> (Holland, 1986) </ref>. An analysis of the credit assignment methods in SAMUEL appears in (Grefenstette, 1988).
Reference: <editor> Michalski, J. G. Carbonell, & T. M. Mitchell (Eds.), </editor> <booktitle> Machine learning: An artificial intelligence approach (Vol. 2). </booktitle> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Selfridge, O., R. S. Sutton and A. G. </author> <title> Barto (1985). Training and tracking in robotics. </title> <booktitle> Proceedings of the Ninth International Conference on Artificial Intelligence. </booktitle> <address> Los Angeles, CA. </address> <month> August, </month> <year> 1985. </year>
Reference-contexts: This cycle repeats indefinitely. The objective is to find a set of decision rules that maximizes the expected total payoff. 1 Several sequential decision tasks have been investigated in the machine learning literature, including pole balancing <ref> (Selfridge, Sutton & Barto, 1985) </ref>, gas pipeline control (Goldberg, 1983), and the animat problem (Wilson, 1985; Wilson, 1987). For many problems, including the one considered here, payoff is delayed in the sense that non-null payoff occurs only at the end of an episode that may span several decision steps.
Reference: <author> Schultz, A. C., C. L. Ramsey, & J. J. </author> <title> Grefenstette (1990). Simulation-assisted learning by competition: Effects of noise differences between training model and target environment. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning. </booktitle> <address> Austin, TX: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: The final section summarizes our findings and discusses topics for further study. This work is part of an on-going study of genetic algorithms for learning tactical plans. The current system is detailed in <ref> (Grefenstette, Ramsey & Schultz, 1990) </ref>. The design of SAMUEL owes much to Smith's LS-1 system (Smith, 1980), and draws on some ideas from classifier systems (Holland, 1986). An analysis of the credit assignment methods in SAMUEL appears in (Grefenstette, 1988). <p> The design of SAMUEL owes much to Smith's LS-1 system (Smith, 1980), and draws on some ideas from classifier systems (Holland, 1986). An analysis of the credit assignment methods in SAMUEL appears in (Grefenstette, 1988). A study of the effects of sensor noise on SAMUEL appears in <ref> (Schultz, Ramsey & Gre-fenstette, 1990) </ref>, and techniques for applying explanation-based learning methods to the empirically derived rules learned by SAMUEL appears in (Gordon & Grefenstette, 1990).
Reference: <author> Smith, S. F. </author> <year> (1980). </year> <title> A learning system based on genetic adaptive algorithms, </title> <type> Doctoral dissertation, </type> <institution> Department of Computer Science, University of Pittsburgh. </institution>
Reference-contexts: The final section summarizes our findings and discusses topics for further study. This work is part of an on-going study of genetic algorithms for learning tactical plans. The current system is detailed in (Grefenstette, Ramsey & Schultz, 1990). The design of SAMUEL owes much to Smith's LS-1 system <ref> (Smith, 1980) </ref>, and draws on some ideas from classifier systems (Holland, 1986). An analysis of the credit assignment methods in SAMUEL appears in (Grefenstette, 1988).
Reference: <author> Wilson, S. W. </author> <year> (1985). </year> <title> Knowledge growth in an artificial animal. </title> <booktitle> Proceedings of the International Conference Genetic Algorithms and Their Applications (pp. </booktitle> <pages> 16-23). </pages> <address> Pittsburgh, PA. </address>
Reference: <author> Wilson, S. W. </author> <year> (1987). </year> <title> Classifier systems and the animat problem. </title> <journal> Machine Learning, </journal> <volume> 2(3), </volume> <pages> (pp. 199-228). </pages>
References-found: 17

