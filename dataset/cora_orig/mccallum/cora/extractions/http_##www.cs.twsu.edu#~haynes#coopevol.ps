URL: http://www.cs.twsu.edu/~haynes/coopevol.ps
Refering-URL: http://adept.cs.twsu.edu/~thomas/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: e-mail: [haynes,sandip]@euler.mcs.utulsa.edu  
Title: Cooperation of the Fittest  
Author: Thomas Haynes and Sandip Sen 
Address: 600 South College Ave.  Tulsa, OK 74104-3189  
Affiliation: Department of Mathematical Computer Sciences  The University of Tulsa  
Abstract: We introduce a cooperative co-evolutionary system to facilitate the development of teams of heterogeneous agents. We believe that k different behavioral strategies for controlling the actions of a group of k agents can combine to form a cooperation strategy which efficiently achieves global goals. We examine the on-line adaption of behavioral strategies utilizing genetic programming. Specifically, we deal with the credit assignment problem of how to fairly split the fitness of a team to all of its participants. We present several crossover mechanisms in a genetic programming system to facilitate the evolution of more than one member in the team during each crossover operation. Our goal is to reduce the time needed to either evolve a good team or reach convergence. 
Abstract-found: 1
Intro-found: 1
Reference: [ Andre, 1995 ] <author> David Andre. </author> <title> The evolution of agents that build mental models and create simple plans using genetic programming. </title> <booktitle> In Proceedings of the Sixth International Conference on Genetic Algorithms, </booktitle> <pages> pages 248-255. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1995. </year> [ <editor> Angeline and Pollack, 1993 ] Peter J. Angeline and Jordan B. Pollack. </editor> <title> Competitive environments evolve better solutions for complex tasks. </title> <booktitle> In Proceedings of the Fifth International Conference on Genetic Algorithms, </booktitle> <pages> pages 264-278. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1993. </year>
Reference-contexts: Each crossover point i is not tied to any one program. 5 Related Work There has been previous research into S-expressions containing more than one executable branch. Both An-dre <ref> [ Andre, 1995 ] </ref> and Haynes [ Haynes, 1994 ] have investigated systems in which one branch of the S-expression manipulates a memory structure and the other branch utilizes the memory structure to interact with an environment.
Reference: [ Benda et al., 1986 ] <author> M. Benda, V. Jagannathan, and R. Dodhiawala. </author> <title> On optimal cooperation of knowledge sources an empirical investigation. </title> <type> Technical Report BCS-G2010-28, </type> <institution> Boeing Advanced Technology Center, Boeing Computing Services, </institution> <address> Seattle, Washington, </address> <month> July </month> <year> 1986. </year>
Reference-contexts: We can then measure their efficiency and effectiveness by some criteria relevant to the domain. Populations of such structures are evolved to produce increasingly efficient coordination strategies. We have used the predator-prey pursuit game <ref> [ Benda et al., 1986 ] </ref> to test our hypothesis that useful coordination strategies can be evolved using the STGP paradigm for non-trivial problems. This domain involves multiple predator agents trying to capture a mobile prey agent in a grid world by surrounding it.
Reference: [ Bond and Gasser, 1988 ] <editor> Alan H. Bond and Les Gasser, editors. </editor> <booktitle> Readings in Distributed Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: In effect, we want to evolve behavioral strategies that guide the actions of agents in a given domain. The identification, design, and implementation of strategies for coordination is a central research issue in DAI <ref> [ Bond and Gasser, 1988 ] </ref> . Current research techniques in developing coordination strategies are mostly off-line mechanisms that use extensive domain 1 knowledge to design from scratch the most appropriate cooperation strategy. It is nearly impossible to identify or even prove the existence of the best coordination strategy.
Reference: [ DeJong, 1990 ] <author> Kenneth A. DeJong. </author> <title> Genetic-algorithm-based learning. </title> <editor> In Y. Kodratoff and R. S. Michal-ski, editors, </editor> <booktitle> Machine Learning, Volume III. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Alamos, CA, </address> <year> 1990. </year>
Reference-contexts: Each team member always participates in the same team. Thus all of the points it is awarded, for both its individual contribution and the teams contribution, are correctly apportioned to the entire team. This approach is similar to "the Pitt approach" used for evolving Genetic-Based Machine Learning systems <ref> [ DeJong, 1990 ] </ref> .
Reference: [ Gasser et al., 1989 ] <author> Les Gasser, Nicolas Rouquette, Randall W. Hill, and John Lieb. </author> <title> Representing and using organizational knowledge in DAI systems. </title> <editor> In Les Gasser and Michael N. Huhns, editors, </editor> <booktitle> Distributed Artificial Intelligence, volume 2 of Research Notes in Artificial Intelligence, </booktitle> <pages> pages 55-78. </pages> <publisher> Pitman, </publisher> <year> 1989. </year>
Reference: [ Grefenstette, 1988 ] <author> John Grefenstette. </author> <title> Credit assignment in rule discovery systems. </title> <journal> Machine Learning, </journal> 3(2/3):225-246, 1988. 
Reference-contexts: it fair to evenly divide the score? Assuming k members to a team, if the actions of one individual accounted for a large share of the team's score, why should it only get 1 k th of the score? This problem is the same as the credit assignment problem in <ref> [ Grefenstette, 1988 ] </ref> . Another partitioning of this strategy is to deterministically split the population into k sized teams. Thus the first k individuals would always form the first team. The problem with this is that it imposes an artificial ordering on the population.
Reference: [ Haynes and Sen, 1996 ] <author> Thomas Haynes and Sandip Sen. </author> <title> Evolving behavioral strategies in predators and prey. </title> <editor> In Gerhard Wei and Sandip Sen, editors, </editor> <booktitle> Adaptation and Learning in Multi-Agent Systems, Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 113-126. </pages> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1996. </year>
Reference-contexts: 1 Introduction In our previous research <ref> [ Haynes and Sen, 1996, Haynes et al., 1995 ] </ref> , we utilized genetic programming (GP) [ Koza, 1992 ] to evolve behavioral strategies which enabled a team of loosely-coupled agents to cooperatively achieve a common goal.
Reference: [ Haynes et al., 1995 ] <author> Thomas Haynes, Roger Wainwright, Sandip Sen, and Dale Schoenefeld. </author> <title> Strongly typed genetic programming in evolving cooperation strategies. </title> <editor> In Larry Eshelman, editor, </editor> <booktitle> Proceedings of the Sixth International Conference on Genetic Algorithms, </booktitle> <pages> pages 271-278, </pages> <address> San Francisco, CA, 1995. </address> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: 1 Introduction In our previous research <ref> [ Haynes and Sen, 1996, Haynes et al., 1995 ] </ref> , we utilized genetic programming (GP) [ Koza, 1992 ] to evolve behavioral strategies which enabled a team of loosely-coupled agents to cooperatively achieve a common goal. <p> It is nearly impossible to identify or even prove the existence of the best coordination strategy. In most cases a coordination strategy is chosen if it is reasonably good. In <ref> [ Haynes et al., 1995 ] </ref> , we presented a new approach for developing coordination strategies for multia-gent problem solving situations, which is different from most of the existing techniques for constructing coordination strategies in two ways: * Strategies for coordination are incrementally constructed by repeatedly solving problems in the do
Reference: [ Haynes et al., 1996 ] <author> Thomas Haynes, Kit Lau, and Sandip Sen. </author> <title> Learning cases to compliment rules for conflict resolution in multiagent systems. </title> <editor> In Sandip Sen, editor, </editor> <booktitle> Working Notes for the AAAI Symposium on Adaptation, Co-evolution and Learning in Multia-gent Systems, </booktitle> <pages> pages 51-56, </pages> <address> Stanford University, CA, </address> <month> March </month> <year> 1996. </year>
Reference-contexts: Since they each shared the same behavioral strategy, the agents were homogeneous, i.e., each chromosome in the population implicitly represented the program of k agents. A simple algorithm to model the actions of others is to believe that they behave as you would in the same situation <ref> [ Haynes et al., 1996 ] </ref> . With homogeneous agents, the agents can employ this algorithm since their models of other agents matches the actions of others. A key issue in distributed artificial intelligence (DAI) research is how can heterogeneous agents interact to form a team. <p> With the heterogeneous behavioral strategies, deadlock situations have the potential to be avoided. In <ref> [ Haynes et al., 1996 ] </ref> we present an overview of the deadlock conditions that can beset homogeneous predator agents. From our analysis of the best four teams per crossover system, we determined that the some of the Team-Branch and TeamAll behavioral strategies allow the prey to escape capture.
Reference: [ Haynes, 1994 ] <author> Thomas D. Haynes. </author> <title> A simulation of adaptive agents in a hostile environment. </title> <type> Master's thesis, </type> <institution> University of Tulsa, Tulsa, OK., </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: Each crossover point i is not tied to any one program. 5 Related Work There has been previous research into S-expressions containing more than one executable branch. Both An-dre [ Andre, 1995 ] and Haynes <ref> [ Haynes, 1994 ] </ref> have investigated systems in which one branch of the S-expression manipulates a memory structure and the other branch utilizes the memory structure to interact with an environment.
Reference: [ Knight and Sen, 1995 ] <author> Leslie Knight and Sandip Sen. </author> <title> PLEASE: A prototype learning system using genetic algorithms. </title> <booktitle> In Proceedings of the Sixth International Conference on Genetic Algorithms, </booktitle> <pages> pages 429-435. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1995. </year>
Reference-contexts: The Pitt approach bypasses the credit assignment problem, in that rules are only evaluated in the context of a ruleset. A similar mechanism as proposed in this paper has been used to successfully co-evolve a set of prototypes for supervised concept classification problems <ref> [ Knight and Sen, 1995 ] </ref> .
Reference: [ Korf, 1992 ] <author> Richard E. Korf. </author> <title> A simple solution to pursuit games. </title> <booktitle> In Working Papers of the 11th International Workshop on Distributed Artificial Intelligence, </booktitle> <pages> pages 183-194, </pages> <month> February </month> <year> 1992. </year>
Reference: [ Koza, 1992 ] <author> John R. Koza. </author> <title> Genetic Programming: On the Programming of Computers by Natural Selection. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1992. </year>
Reference-contexts: 1 Introduction In our previous research [ Haynes and Sen, 1996, Haynes et al., 1995 ] , we utilized genetic programming (GP) <ref> [ Koza, 1992 ] </ref> to evolve behavioral strategies which enabled a team of loosely-coupled agents to cooperatively achieve a common goal. Since they each shared the same behavioral strategy, the agents were homogeneous, i.e., each chromosome in the population implicitly represented the program of k agents.
Reference: [ Koza, 1994 ] <author> John R. Koza. </author> <title> Genetic Programming II: Automatic Discovery of Reusable Programs. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Both of these systems can be considered to utilize the Pitt approach to credit assignment. While Koza's Automatically Defined Functions (ADF) <ref> [ Koza, 1994 ] </ref> are not separate "agents", they utilize many different branches, say k, to facilitate learning.
Reference: [ Levy and Rosenschein, 1992 ] <author> Ran Levy and Jeffrey S. Rosenschein. </author> <title> A game theoretic approach to the pursuit problem. </title> <booktitle> In Working Papers of the 11th International Workshop on Distributed Artificial Intelligence, </booktitle> <pages> pages 195-213, </pages> <month> February </month> <year> 1992. </year>
Reference: [ Montana, 1995 ] <author> David J. Montana. </author> <title> Strongly typed genetic programming. </title> <journal> Evolutionary Computation, </journal> <volume> 3(2) </volume> <pages> 199-230, </pages> <year> 1995. </year>
Reference-contexts: Our approach for developing coordination strategies for multi-agent problems is completely domain independent, and uses the strongly typed genetic programming (STGP) paradigm <ref> [ Montana, 1995 ] </ref> , which is an extension of GP. To use the STGP approach for evolving coordination strategies, the strategies are encoded as symbolic expressions (S-expressions) and an evaluation criterion is chosen for evaluating arbitrary S-expressions.
Reference: [ Reynolds, 1994 ] <author> Craig W. Reynolds. </author> <title> Evolution of obstacle avoidance behavior: Using noise to promote robust solutions. </title> <editor> In Kenneth E. Kinnear, Jr., editor, </editor> <booktitle> Advances in Genetic Programming, </booktitle> <pages> pages 221-241. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference: [ Siegel, 1994 ] <author> Eric Siegel. </author> <title> ADF Crossover response summary. Genetic Programming Mailing List, </title> <note> June 15th 1994. </note>
Reference-contexts: Both of these systems can be considered to utilize the Pitt approach to credit assignment. While Koza's Automatically Defined Functions (ADF) [ Koza, 1994 ] are not separate "agents", they utilize many different branches, say k, to facilitate learning. In the GP mailing list, Siegel <ref> [ Siegel, 1994 ] </ref> posed the question as to whether some form of crossover utilizing k &gt; 1 would help in the learning process? The replies were mixed, and pointed out the need for further research. 6 Results In a series of experiments, we have evaluated the different crossover mechanisms for
Reference: [ Stephens and Merx, 1989 ] <author> Larry M. Stephens and Mat-thias B. Merx. </author> <title> Agent organization as an effector of DAI system performance. </title> <booktitle> In Working Papers of the 9th International Workshop on Distributed Artificial Intelligence, </booktitle> <month> September </month> <year> 1989. </year>
Reference: [ Stephens and Merx, 1990 ] <author> Larry M. Stephens and Mat-thias B. Merx. </author> <title> The effect of agent control strategy on the performance of a DAI pursuit problem. </title> <booktitle> In Proceedings of the 1990 Distributed AI Workshop, </booktitle> <month> October </month> <year> 1990. </year>
References-found: 20

