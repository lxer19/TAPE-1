URL: ftp://ai.iit.nrc.ca/pub/iit-papers/NRC-35073.ps.Z
Refering-URL: http://ai.iit.nrc.ca/cgi-bin/ftpsearch/?turney
Root-URL: 
Title: Error and Voting  Theoretical Analyses of Cross-Validation Error and Voting in Instance-Based Learning  
Author: Peter Turney 
Keyword: Running Head: Error and Voting  
Address: Ottawa, Ontario, Canada K1A 0R6  
Affiliation: Knowledge Systems Laboratory Institute for Information Technology National Research Council Canada  
Note: Submitted to the Journal of Experimental and Theoretical Artificial Intelligence  
Email: peter@ai.iit.nrc.ca  
Date: May 13, 1993 1  
Abstract-found: 0
Intro-found: 1
Reference: <author> Aha, D.W., Kibler, D., & Albert, </author> <title> M.K. (1991) Instance-based learning algorithms, </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 37-66. </pages>
Reference-contexts: This section has presented a general theory and the remainder of the paper will demonstrate that the theory can be fruitfully applied to a real, concrete machine learning algorithm. 3 Single Nearest Neighbor Instance-based learning may be used either for predicting boolean-valued attributes <ref> (Aha et al., 1991) </ref> or for predicting real-valued attributes (Kibler et al., 1989). Instance-based learning is a paradigm for a class of learning algorithms; it is not a single algorithm. It is related to the nearest neighbor pattern recognition paradigm (Dasarathy, 1991).
Reference: <author> Cover, T.M., & Hart, P.E. </author> <title> (1967) Nearest neighbor pattern classification, </title> <journal> IEEE Transactions on Information Theory, </journal> <note> IT-13:21-27. Also in (Dasarathy, </note> <year> 1991). </year>
Reference-contexts: This theory is closely related to Akaike Information Criterion statistics (Sakamoto et al., 1986), as is discussed elsewhere (Turney, 1993). There is also an interesting connection with some prior work in nearest neighbor pattern classification <ref> (Cover & Hart, 1967) </ref>. Finally, Section 7 considers future work. One weakness of this general theory of cross-validation error is that it does not model interpolation and extrapolation. <p> The Bayes probability of error is the minimum probability of error over all decision rules taking underlying probability structure into account <ref> (Cover & Hart, 1967) </ref>. This paper assumes boolean-valued attributes, so : (199) Compare this with Theorem 6: (200) There are clearly some similarities between (199) and (200). However, there are also some differences. (200) is an exact equality, while (199) is an inequality.
Reference: <author> Dasarathy, </author> <title> B.V. (1991) Nearest Neighbor Pattern Classification Techniques, Edited collection (California: </title> <publisher> IEEE Press). </publisher>
Reference-contexts: Section 3 examines the cross-validation error of a simple form of instance-based learning (Aha et al., 1991; Kibler et al., 1989). Instance-based learning is not a single learning algorithm; it is a paradigm for a class of learning algorithms. It is related to the nearest neighbor pattern classification paradigm <ref> (Dasarathy, 1991) </ref>. In instance-based learning, the students model of the data consists of simply storing the training set. Given an example from the testing set, the student makes a prediction by looking for similar examples in the training set. <p> Section 4 deals with instance-based learning algorithms that look for the k most similar examples, where . In general, the k most similar examples will not all agree on the value of the attribute that is to be predicted. This section examines algorithms that resolve this conict by voting <ref> (Dasarathy, 1991) </ref>. For example, suppose and the value of the attribute that is to be predicted is 1 for two of the three examples and 0 for the remaining example. <p> Instance-based learning is a paradigm for a class of learning algorithms; it is not a single algorithm. It is related to the nearest neighbor pattern recognition paradigm <ref> (Dasarathy, 1991) </ref>. With instance-based learning, the model is constructed by simply storing the data . These stored data are the instances. In order to make a prediction for the input , we examine the row vectors of the matrix X. <p> The k row vectors in X that are most similar to the input are called the k nearest neighbors of <ref> (Dasarathy, 1991) </ref>. We may use to represent instance-based learning with k nearest neighbors. This section focuses on . The next theorem shows that instance-based learning using the single nearest neighbor gives sub-optimal cross-validation error. Theorem 7: Let the model use instance-based learning with the single nearest neighbor. <p> If we plot cross-validation error as a function of k, then we expect to see a curve with a single minimum at the optimal value of k, somewhere between the extremes and . Empirical tests of nearest neighbor algorithms report exactly this result <ref> (Dasarathy, 1991) </ref>.
Reference: <author> Fix, E., & Hodges, J.L. </author> <title> (1951) Discriminatory analysis: nonparametric discrimination: consistency properties, Project 21-49-004, </title> <type> Report Number 4, </type> <institution> USAF School of Aviation Medicine, Randolph Field, Texas, </institution> <note> 261-279. Also in (Dasarathy, </note> <year> 1991). </year>
Reference-contexts: Let us assume a reasonable measure of similarity. Let be the outputs corresponding to the rows . The model predicts the output for the input to be the value of the majority of <ref> (Fix & Hodges, 1951) </ref>: (110) Let us assume that k is an odd number, , so there will never be a tie in majority voting with . Of course, we require that . In general, .
Reference: <author> Fraser, D.A.S. </author> <title> (1976) Probability and Statistics: Theory and Applications (Massachusetts: </title> <publisher> Duxbury Press). </publisher>
Reference-contexts: This section shows that cross-validation error has two components: error on the training set and sensitivity to noise. Error on the training set is commonly used as a measure of accuracy <ref> (Fraser, 1976) </ref>. Turney (1990) introduces a formal definition of stability as a measure of the sensitivity of algorithms to noise. Section 2 proves that cross-validation error is bounded by the sum of the training set error and the instability. <p> In this paper, the former terminology is used. 2 It is assumed that our goal is to minimize the expected number of errors that the model makes on the testing set. is the expectation operator from probability theory <ref> (Fraser, 1976) </ref>. If is a function of a random variable x, where ( is the set of possible values of x) and the probability of observing a particular value of x is , then is: (29) The expected cross-validation error depends on the random boolean vectors and . <p> It is assumed that the noise vector is a sequence of n independent random boolean variables, each having the value 1 with probability p. That is, is a sequence of samples from a Bernoulli (p) distribution <ref> (Fraser, 1976) </ref>. This is a very weak assumption, which is likely to be (approximately) satisfied in most real-world data (given assumption 6 below). 0 p 1 2 0 p 0.5 p a p= 2 0.5 p&lt; 1 p a 1 p-= 2 2 2p- 2- 4p 2p -+= . <p> E e s ( ) 2np 2np -= P 5 6p 15p - 10p += Error and Voting Submitted to the Journal of Experimental and Theoretical Artificial Intelligence May 13, 1993 24 To find the behavior of the model in the limit, , we can use the Central Limit Theorem <ref> (Fraser, 1976) </ref>. Consider the following sum: (130) The mean and variance of this sum are: (131) Consider the following expression: (133) By the Central Limit Theorem, the distribution of (133) approaches a standard normal distribution as k approaches infinity. <p> Obtaining an estimate of is relatively simple. We can use the actual error on the training set as an estimator for the expected error on the training set . By the Law of Large Numbers <ref> (Fraser, 1976) </ref>, as n increases, becomes increasingly close to . Obtaining an estimate of is more difficult. Suppose , , and the value of p is known by the modeler. We can use Theorem 14 to estimate for .
Reference: <author> Kibler, D., Aha, D.W., & Albert, </author> <title> M.K. (1989) Instance-based prediction of real-valued attributes, </title> <journal> Computational Intelligence, </journal> <volume> 5 </volume> <pages> 51-57. </pages>
Reference-contexts: has presented a general theory and the remainder of the paper will demonstrate that the theory can be fruitfully applied to a real, concrete machine learning algorithm. 3 Single Nearest Neighbor Instance-based learning may be used either for predicting boolean-valued attributes (Aha et al., 1991) or for predicting real-valued attributes <ref> (Kibler et al., 1989) </ref>. Instance-based learning is a paradigm for a class of learning algorithms; it is not a single algorithm. It is related to the nearest neighbor pattern recognition paradigm (Dasarathy, 1991). With instance-based learning, the model is constructed by simply storing the data . <p> This makes a substantial difference in the details of the two cases, real-valued and boolean-valued. Turney (1993) also examined the cross-validation error of instance-based learning with k nearest neighbors. When instance-based learning is used to predict real-values, the k nearest neighbors are averaged together <ref> (Kibler et al., 1989) </ref>. Averaging the k nearest neighbors for real-valued attributes is analogous to voting for boolean-valued attributes. There is a surprising difference, however. Turney (1993) proves that averaging, in the best case, improves stability and, in the worst case, does not affect stability (Theorem 10 in (Turney, 1993)).
Reference: <author> Langley, P. </author> <title> (1993) Average-case analysis of a nearest neighbor algorithm, </title> <booktitle> Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <address> Chambry, France, </address> <publisher> in press. </publisher>
Reference: <author> Sakamoto, Y., Ishiguro, M., & Kitagawa, G. </author> <title> (1986) Akaike Information Criterion Statistics (Dordrecht, </title> <publisher> Holland: Kluwer Academic Publishers). </publisher>
Reference-contexts: The most closely related work is Turney (1993), which first introduced many of the concepts used here. There are interesting differences, which arise because Turney (1993) involves real-valued attributes and classes, while this paper involves boolean-valued attributes and classes. This theory is closely related to Akaike Information Criterion statistics <ref> (Sakamoto et al., 1986) </ref>, as is discussed elsewhere (Turney, 1993). There is also an interesting connection with some prior work in nearest neighbor pattern classification (Cover & Hart, 1967). Finally, Section 7 considers future work. <p> Turney (1993) mentions that the theory of cross-validation error is similar to the work in Akaike Information Criterion (AIC) statistics <ref> (Sakamoto et al., 1986) </ref>. The similarity stems from the idea of finding the best model for one set of outputs , then evaluating the models performance with a second set of outputs . This is the definition of the expected cross-validation error. <p> The similarity stems from the idea of finding the best model for one set of outputs , then evaluating the models performance with a second set of outputs . This is the definition of the expected cross-validation error. Akaike uses the same approach to define mean expected log likelihood <ref> (Sakamoto et al., 1986) </ref>. For a more thorough comparison of the approach described here with AIC, see Turney (1993). There is an interesting connection between the work here and previous work in nearest neighbor pattern classification.
Reference: <author> Tomek, I. </author> <title> (1976) A generalization of the k-NN rule, </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <note> SMC-6:121-126. Also in (Dasarathy, </note> <year> 1991). </year>
Reference-contexts: Of course, we require that . In general, . More generally, let us consider the following model <ref> (Tomek, 1976) </ref>: (111) In this model, t is a threshold, such that . With majority voting, . When , it may be appropriate to consider values of k other than the odd numbers.
Reference: <author> Turney, </author> <title> P.D. (1990) The curve fitting problem: a solution, </title> <journal> British Journal for the Philosophy of Science, </journal> <volume> 41 </volume> <pages> 509-530. </pages>
Reference: <author> Turney, </author> <title> P.D. (1993) A theory of cross-validation error. </title> <journal> Submitted to the Journal of Experimental and Theoretical Artificial Intelligence. </journal>
Reference-contexts: A mismatch between the students prediction and the actual value is counted as an error. The students goal is to minimize the number of errors that it makes on the testing set. In a recent paper <ref> (Turney, 1993) </ref>, a general theory of error in cross-validation testing of algorithms for predicting real-valued attributes is presented. 1 Section 2 of this paper extends the theory of Turney (1993) to algorithms for predicting boolean-valued attributes. <p> There are interesting differences, which arise because Turney (1993) involves real-valued attributes and classes, while this paper involves boolean-valued attributes and classes. This theory is closely related to Akaike Information Criterion statistics (Sakamoto et al., 1986), as is discussed elsewhere <ref> (Turney, 1993) </ref>. There is also an interesting connection with some prior work in nearest neighbor pattern classification (Cover & Hart, 1967). Finally, Section 7 considers future work. One weakness of this general theory of cross-validation error is that it does not model interpolation and extrapolation. <p> The theory is then applied to linear regression and instance-based learning. The theory of cross-validation error for algorithms that predict boolean-valued attributes, presented in Section 2, is similar to the theory for real-valued attributes <ref> (Turney, 1993) </ref>, but there are some differences. Real-valued noise is quite different from boolean-valued noise. Boolean noise z is a random variable in , such that the probability that z is 1 is p. <p> Averaging the k nearest neighbors for real-valued attributes is analogous to voting for boolean-valued attributes. There is a surprising difference, however. Turney (1993) proves that averaging, in the best case, improves stability and, in the worst case, does not affect stability (Theorem 10 in <ref> (Turney, 1993) </ref>). Voting, in the worst case, can be destabilizing (Section 4.2).
Reference: <author> Weiss, </author> <title> S.M., & Kulikowski, C.A. (1991) Computer Systems that Learn: Classification and Prediction Methods from Statistics, Neural Nets, Machine Learning, and Expert Systems (California: </title> <note> Morgan Kaufmann). Error and Voting Submitted to the Journal of Experimental and Theoretical Artificial Intelligence May 13, 1993 43 </note>
Reference-contexts: To get a good indication of the quality of the model, we must test it on an independent set of data. Some authors call this error measure cross-validation error, while other authors call it train-and-test error <ref> (Weiss and Kulikowski, 1991) </ref>. In this paper, the former terminology is used. 2 It is assumed that our goal is to minimize the expected number of errors that the model makes on the testing set. is the expectation operator from probability theory (Fraser, 1976).
References-found: 12

