URL: http://www.cs.berkeley.edu/~russell/papers/aij-principles.ps
Refering-URL: http://www.cs.berkeley.edu/~russell/publications.html
Root-URL: 
Title: Principles of Metareasoning  
Author: Stuart Russell and Eric Wefald 
Address: Berkeley, CA 94720  
Affiliation: Computer Science Division University of California  
Abstract: This paper is dedicated to the memory of Eric Wefald Abstract In this paper we outline a general approach to the study of metareasoning, not in the sense of explicating the semantics of explicitly specified meta-level control policies, but in the sense of providing a basis for selecting and justifying computational actions. This research contributes to a developing attack on the problem of resource-bounded rationality, by providing a means for analysing and generating optimal computational strategies. Because reasoning about a computation without doing it necessarily involves uncertainty as to its outcome, probability and decision theory will be our main tools. We develop a general formula for the utility of computations, this utility being derived directly from the ability of computations to affect an agent's external actions. We address some philosophical difficulties that arise in specifying this formula, given our assumption of limited rationality. We also describe a methodology for applying the theory to particular problem-solving systems, and provide a brief sketch of the resulting algorithms and their performance. fl This research has been supported by an equipment grant from the AT&T Foundation, and by funding from the Lockheed AI Center, California MICRO Program, and the National Science Foundation under grant no. IRI-8903146. Eric Wefald was supported by a GE Foundation Fellowship and more recently by a Shell Foundation Doctoral Fellowship. We gratefully acknowledge this assistance. In addition, we would like to thank Steve Bradley, Jack Breese, Murray Campbell, Jon Doyle, Michael Fehling, Michael Genesereth, Eric Horvitz, Maurice Karnaugh, Richard Karp, David McAllester, Devika Subramanian, Michael Wellman and two reviewers for their valuable comments and suggestions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Agre, P. and Chapman, D. </author> <year> (1987) </year> <month> Pengo: </month> <title> An implementation of a theory of activity. </title> <booktitle> In Proc. 6th National Conference on Artificial Intelligence, </booktitle> <address> Seattle, WA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A notion popularized by Brooks [5] and by Agre and Chapman <ref> [1] </ref> is that all this deliberation is a waste of time | why don't we just build agents that "do the right thing"? Substantive rationality, however, does not come for free.
Reference: [2] <author> Agogino, A. M. </author> <title> (1989) Real-time reasoning about time constraints and model precision in complex, distributed mechanical systems. </title> <booktitle> In Proceedings of the AAAI Spring Symposium on AI and Limited Rationality, </booktitle> <address> Stanford, CA: </address> <publisher> AAAI. </publisher>
Reference-contexts: His survey paper [9] provides a good summary of much of the technical work on decision-theoretic control of inference. Fehling and Breese [13] have applied a decision-theoretic approach to the control of information-gathering actions in a mobile robot domain. Agogino <ref> [2] </ref> has investigated the use of decision-theoretic modelling of computation in the control of mechanical systems. Bratman, Israel and Pollack [4] sketch a system design combining decision theory with AI planning techniques, using plans to limit the set of actions to be considered. <p> 15 It should be noted that the depth limit for RTA fl is an additional parameter chosen by the user, and that for many problem instances the best performance was obtained with the limit set to 1, i.e., hillclimbing. 25 algorithm wins nodes/game sec/game MGSS fl 24.5 3,666 40 ff-fi <ref> [2] </ref> 7.5 2,501 23 MGSS fl 22 6,132 68 ff-fi [3] 10 9,104 82 MGSS fl 20 12,237 170 ff-fi [4] 12 42,977 403 MGSS fl 16.5 21,155 435 ff-fi [5] 15.5 133,100 1,356 MGSS fl 17 45,120 1,590 ff-fi [6] 15 581,433 6,863 Table 1: Summary of results for Othello
Reference: [3] <author> Batali, J. </author> <title> (1985) A computational theory of rational action (draft). </title> <publisher> Cambridge: MIT AI Lab. </publisher>
Reference-contexts: In other words, no computation can be executed until a computation has been executed to decide on it. Regress has been mentioned by many researchers concerned with bounded rationality <ref> [3, 11, 13, 27, 34] </ref> but so far only Lipman [36] has claimed any progress on the problem. Clearly we must back off from insisting on optimal control of all reasoning, just as we back off from insisting on optimal decisions to act. <p> RTA fl is an additional parameter chosen by the user, and that for many problem instances the best performance was obtained with the limit set to 1, i.e., hillclimbing. 25 algorithm wins nodes/game sec/game MGSS fl 24.5 3,666 40 ff-fi [2] 7.5 2,501 23 MGSS fl 22 6,132 68 ff-fi <ref> [3] </ref> 10 9,104 82 MGSS fl 20 12,237 170 ff-fi [4] 12 42,977 403 MGSS fl 16.5 21,155 435 ff-fi [5] 15.5 133,100 1,356 MGSS fl 17 45,120 1,590 ff-fi [6] 15 581,433 6,863 Table 1: Summary of results for Othello 26 more refined search control method in [42], though preliminary,
Reference: [4] <author> Bratman, M., Israel, D., and Pollack, M. </author> <title> (in press) Plans and Resource-Bounded Practical Reasoning. </title> <journal> Computational Intelligence, </journal> <note> to appear. </note>
Reference-contexts: Fehling and Breese [13] have applied a decision-theoretic approach to the control of information-gathering actions in a mobile robot domain. Agogino [2] has investigated the use of decision-theoretic modelling of computation in the control of mechanical systems. Bratman, Israel and Pollack <ref> [4] </ref> sketch a system design combining decision theory with AI planning techniques, using plans to limit the set of actions to be considered. <p> and that for many problem instances the best performance was obtained with the limit set to 1, i.e., hillclimbing. 25 algorithm wins nodes/game sec/game MGSS fl 24.5 3,666 40 ff-fi [2] 7.5 2,501 23 MGSS fl 22 6,132 68 ff-fi [3] 10 9,104 82 MGSS fl 20 12,237 170 ff-fi <ref> [4] </ref> 12 42,977 403 MGSS fl 16.5 21,155 435 ff-fi [5] 15.5 133,100 1,356 MGSS fl 17 45,120 1,590 ff-fi [6] 15 581,433 6,863 Table 1: Summary of results for Othello 26 more refined search control method in [42], though preliminary, show a factor of fifty-nine improvement.
Reference: [5] <author> Brooks, R. A. </author> <title> (1986) A robust, layered control system for a mobile robot. </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> 2(1), </volume> <pages> 14-23. </pages>
Reference-contexts: He pointed out that procedurally rational systems, whose execution architectures are based on explicit use of declarative knowledge to reach decisions to act, seem to suffer from a good deal of overhead, both in terms of time and extra cognitive machinery. A notion popularized by Brooks <ref> [5] </ref> and by Agre and Chapman [1] is that all this deliberation is a waste of time | why don't we just build agents that "do the right thing"? Substantive rationality, however, does not come for free. <p> obtained with the limit set to 1, i.e., hillclimbing. 25 algorithm wins nodes/game sec/game MGSS fl 24.5 3,666 40 ff-fi [2] 7.5 2,501 23 MGSS fl 22 6,132 68 ff-fi [3] 10 9,104 82 MGSS fl 20 12,237 170 ff-fi [4] 12 42,977 403 MGSS fl 16.5 21,155 435 ff-fi <ref> [5] </ref> 15.5 133,100 1,356 MGSS fl 17 45,120 1,590 ff-fi [6] 15 581,433 6,863 Table 1: Summary of results for Othello 26 more refined search control method in [42], though preliminary, show a factor of fifty-nine improvement.
Reference: [6] <author> Cherniak, C. </author> <title> (1986) Minimal Rationality. </title> <publisher> Cambridge: MIT Press. </publisher>
Reference-contexts: Section 7 discusses these and other directions for future research. 2 Approaches to bounded rationality Two conditions conspire to create what has been called the `finitary predicament' <ref> [6] </ref>: first, real agents have only finite computational power; second, they don't have all the time in the world. One characterization of a `real-time' problem situation is the following: the utility of performing a given action varies significantly over the time necessary for a complete solution to the decision problem. <p> algorithm wins nodes/game sec/game MGSS fl 24.5 3,666 40 ff-fi [2] 7.5 2,501 23 MGSS fl 22 6,132 68 ff-fi [3] 10 9,104 82 MGSS fl 20 12,237 170 ff-fi [4] 12 42,977 403 MGSS fl 16.5 21,155 435 ff-fi [5] 15.5 133,100 1,356 MGSS fl 17 45,120 1,590 ff-fi <ref> [6] </ref> 15 581,433 6,863 Table 1: Summary of results for Othello 26 more refined search control method in [42], though preliminary, show a factor of fifty-nine improvement. We do not, however expect this performance trend to persist against deeper-searching alpha-beta programs.
Reference: [7] <author> Dean, T. </author> <title> (1987) Intractability and time-dependent planning. </title> <editor> In Georgeff, M. P., and Lansky, A. L., (Eds.), </editor> <booktitle> The 1986 Workshop on Reasoning about Actions and Plans. </booktitle> <address> Los Altos: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 245-266. </pages>
Reference-contexts: Dean's work on real-time planning <ref> [7, 8] </ref> assumes a known variation of the intrinsic utility of the results of a computation method with the amount of resources allocated to that method, and hence allocates resources optimally in various resource-bounded scenarios. <p> The reason for this is most obvious if we consider the case where 8 This provides the performance profiles for the available computations, as used by Dean <ref> [7] </ref>; his deliberation scheduling algorithm follows from equation 10 with each S j consisting of running one of the available decision procedures for a small increment of time. 14 ff S j = ff, but ^ U S:S j ([ff S j ]) &gt; ^ U S ([ff]); i.e., S j <p> Furthermore, the use of information returned from each subtree to select the direction for further search gives the scheme a clear advantage over IDA fl [31]. The decision-theoretic metalevel can be seen as a means to construct a near-optimal anytime algorithm, in Dean's sense <ref> [7] </ref>, from atomic computation steps. Given several such anytime algorithms, together with their performance profiles | mappings from time allocation to output quality | one may wish to construct a more complex real-time system by applying simple composition operators, without having to explicitly specify time allocations among the subsystems.
Reference: [8] <author> Dean, T., and Boddy, M. </author> <title> (1988) An Analysis of Time-Dependent Planning. </title> <booktitle> In Proc. 7th National Conference on Artificial Intelligence, </booktitle> <address> Minneapolis, MN: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 49-54. </pages>
Reference-contexts: Dean's work on real-time planning <ref> [7, 8] </ref> assumes a known variation of the intrinsic utility of the results of a computation method with the amount of resources allocated to that method, and hence allocates resources optimally in various resource-bounded scenarios.
Reference: [9] <author> Dean, T. </author> <title> (in press) Decision-theoretic control of inference for time-critical applications. </title> <journal> International Journal of Intelligent Systems, </journal> <note> to appear. </note>
Reference-contexts: Dean's work on real-time planning [7, 8] assumes a known variation of the intrinsic utility of the results of a computation method with the amount of resources allocated to that method, and hence allocates resources optimally in various resource-bounded scenarios. His survey paper <ref> [9] </ref> provides a good summary of much of the technical work on decision-theoretic control of inference. Fehling and Breese [13] have applied a decision-theoretic approach to the control of information-gathering actions in a mobile robot domain.
Reference: [10] <author> Doyle, J. </author> <title> (1983) What is rational psychology? Toward a modern mental philosophy. </title> <journal> AI Magazine 4 (3), </journal> <pages> 50-53. </pages>
Reference-contexts: This premise is shared by several other researchers in AI, and has been the source of a number of projects that have, 4 until recently, developed more or less independently.. Doyle's `rational psychology' project <ref> [10, 11] </ref> is based on the idea that computations, or internal state changes, are actions to be chosen like any other actions. He has applied this idea to clarify the notions of belief, intention and learning.
Reference: [11] <author> Doyle, J. </author> <title> (1988) Artificial Intelligence and Rational Self-Government. </title> <type> Technical report no. </type> <institution> CMU-CS-88-124, Computer Science Department, Carnegie-Mellon University, </institution> <address> Pittsburgh, PA. </address> <month> 30 </month>
Reference-contexts: This premise is shared by several other researchers in AI, and has been the source of a number of projects that have, 4 until recently, developed more or less independently.. Doyle's `rational psychology' project <ref> [10, 11] </ref> is based on the idea that computations, or internal state changes, are actions to be chosen like any other actions. He has applied this idea to clarify the notions of belief, intention and learning. <p> Other computational actions, such as inductive learning and compilation, should be covered in any general theory of metareasoning (cf. Doyle's analysis <ref> [11] </ref>). <p> In other words, no computation can be executed until a computation has been executed to decide on it. Regress has been mentioned by many researchers concerned with bounded rationality <ref> [3, 11, 13, 27, 34] </ref> but so far only Lipman [36] has claimed any progress on the problem. Clearly we must back off from insisting on optimal control of all reasoning, just as we back off from insisting on optimal decisions to act. <p> The concept of universal subgoaling [33, 34] is intended to capture the notion of a complete decision model, by making every aspect of the agent's deliberation recursively open to metareasoning. In 16 Obtaining a satisfactory definition of computational action, as distinct from action in general, is nontrivial. Doyle <ref> [11] </ref> simply posits a division of the total world state into internal and external portions. The addition of time into such a framework is reasonably straightforward.
Reference: [12] <author> Elkan, C. </author> <title> (1989) Conspiracy numbers and caching for searching and/or trees and theorem--proving. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <address> Detroit, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: However, in order to do this one needs something amounting to a `current best move'. As currently implemented, theorem provers have no partial information about the success of the branch under consideration (but see <ref> [12] </ref> for an application of conspiracy numbers to theorem-proving search). The notions of `guaranteed solution' and `proof' must be replaced by tentative/abstract/partial solution and justification. Algorithms using defaults, abstraction hierarchies and island-driving strategies thus seem more amenable to meta-level control and therefore much more robust in the face of complexity.
Reference: [13] <author> Fehling, M. R., and Breese, J. S. </author> <title> (1988) A computational model for decision-theoretic control of problem-solving under uncertainty. </title> <booktitle> In Proceedings of the Fourth Workshop on Uncertainty in Artificial Intelligence, </booktitle> <address> Minneapolis, MN: </address> <publisher> AAAI. </publisher>
Reference-contexts: His survey paper [9] provides a good summary of much of the technical work on decision-theoretic control of inference. Fehling and Breese <ref> [13] </ref> have applied a decision-theoretic approach to the control of information-gathering actions in a mobile robot domain. Agogino [2] has investigated the use of decision-theoretic modelling of computation in the control of mechanical systems. <p> In other words, no computation can be executed until a computation has been executed to decide on it. Regress has been mentioned by many researchers concerned with bounded rationality <ref> [3, 11, 13, 27, 34] </ref> but so far only Lipman [36] has claimed any progress on the problem. Clearly we must back off from insisting on optimal control of all reasoning, just as we back off from insisting on optimal decisions to act. <p> However, any probability estimate we can arrive at for the win will be less than 1. This is one reason why we adopted the superscript notation for the computation used to arrive at a utility estimate. Fehling and Breese <ref> [13] </ref> simply write the computation as additional conditioning in the conditional probabilities used to make decisions. While this seems natural, it is perhaps misleading since one is no longer dealing in probabilities.
Reference: [14] <author> Genesereth, M., and Smith, D. </author> <title> (1981) Meta-level architecture. Stanford Heuristic Programming Project, </title> <institution> Memo HPP-81-6, Stanford University, Stanford, </institution> <address> CA. </address>
Reference-contexts: All meta-level systems share a common methodology for implementation: the base-level problem solver operates via the explicit formulation and solution of meta-level problems. For example, the base-level problem-solver in Genesereth's MRS <ref> [14, 40] </ref> is essentially a theorem-prover, but one that takes the user's goal and, instead of simply running a resolution algorithm, sets up its own goal of finding a way to prove the user's goal.
Reference: [15] <author> Good, I. J. </author> <title> (1968) A five year plan for automatic chess. </title> <journal> Machine Intelligence, </journal> <volume> 2. </volume>
Reference-contexts: Commit to the action ff that is preferred according to the internal state resulting from step 1. This algorithm, and the intuitive notion of the value of computation which we have defined precisely above, were also proposed by Good <ref> [15] </ref>. Obviously, the calculation of the expected values of the various possible computations cannot be instantaneous; in fact, as we describe below, it can be arbitrarily hard. It is, however, possible to approximate the ideal algorithm by making simplifying assumptions.
Reference: [16] <author> Good, I. J. </author> <title> (1971) Twenty-seven principles of rationality. </title> <editor> In Godambe, V. P., and Sprott, D. A. (Eds.) </editor> <title> Foundations of Statistical Inference. </title> <publisher> Toronto: Holt, Rinehart, Winston, </publisher> <pages> 108-141. </pages>
Reference-contexts: A suitable generalization of complexity theory has not yet been developed. A somewhat longer tradition of considering the effects of boundednesss on decision-making exists in economics and the decision sciences, where human characteristics must sometimes be considered. Since the 1960's, I. J. Good <ref> [16] </ref> has emphasized the conceptual distinction between classical or "type I" rationality, and what he called "type II" rationality, or the maximization of expected utility taking into account deliberation costs. Researchers in decision analysis, especially Howard [28], have studied the problem of the value of information.
Reference: [17] <author> Hacking, I. </author> <title> (1967) Slightly more realistic personal probability. </title> <journal> Philosophy of Science, </journal> <volume> 34, </volume> <pages> 311-325. </pages>
Reference-contexts: Note that this already goes against the classical Bayesian assumption that the agent's probabilities and expectations are conditioned upon the sum total of his "knowledge", which is assumed to be deductively closed. For a philosophical discussion of this point which supports our view, see Hacking <ref> [17] </ref>. Therefore, from a formal standpoint, the results in sections 5 and 6 have as yet only a heuristic justification, borne out by practical results. We expect, however, that the structure of the theory will be retained when it is put on a firmer footing.
Reference: [18] <author> Hansson, O., Holt, G., and Mayer, A. </author> <title> (1986) The comparison and optimization of search algorithm performance. </title> <type> Unpublished manuscript, </type> <institution> Columbia University Computer Science Department. </institution>
Reference-contexts: Heckerman and Jimison [23] have also used medical decision-making as an example domain, showing how to vary the depth of analysis of a therapy problem according to its expected benefits. A third independent project was started in 1986 by Hansson and Mayer <ref> [18, 19, 20, 21] </ref>, who proposed the use of information value as a means of controlling heuristic search, which in turn is implemented as probabilistic inference using information from the heuristic function as evidence.
Reference: [19] <author> Hansson, O., and Mayer, A. </author> <title> (1988) The Optimality of Satisficing Solutions. </title> <booktitle> Proceedings of the Fourth Workshop on Uncertainty in Artificial Intelligence, </booktitle> <address> Minneapolis, MN, </address> <year> 1988. </year>
Reference-contexts: These are often called probably correct algorithms. Some researchers, notably Valiant and others in the field of inductive learning [22, 47] have studied `probably approximately correct' algorithms, combining the above properties in the obvious way. However, as Horvitz [26] and Hansson and Mayer <ref> [19] </ref> have pointed out, what is needed is a theory of algorithms that maximize the comprehensive value of computation. <p> Heckerman and Jimison [23] have also used medical decision-making as an example domain, showing how to vary the depth of analysis of a therapy problem according to its expected benefits. A third independent project was started in 1986 by Hansson and Mayer <ref> [18, 19, 20, 21] </ref>, who proposed the use of information value as a means of controlling heuristic search, which in turn is implemented as probabilistic inference using information from the heuristic function as evidence.
Reference: [20] <author> Hansson, O., and Mayer, A. </author> <title> (in press) Probabilistic Heuristic Estimates. </title> <journal> Annals of Mathematics and Artificial Intelligence, </journal> <note> to appear. </note>
Reference-contexts: Heckerman and Jimison [23] have also used medical decision-making as an example domain, showing how to vary the depth of analysis of a therapy problem according to its expected benefits. A third independent project was started in 1986 by Hansson and Mayer <ref> [18, 19, 20, 21] </ref>, who proposed the use of information value as a means of controlling heuristic search, which in turn is implemented as probabilistic inference using information from the heuristic function as evidence.
Reference: [21] <author> Hansson, O., and Mayer, A. </author> <title> (1989) Heuristic Search as Evidential Reasoning. </title> <booktitle> In Proceedings of the Fifth Workshop on Uncertainty in Artificial Intelligence, </booktitle> <address> Windsor, Ontario, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: Heckerman and Jimison [23] have also used medical decision-making as an example domain, showing how to vary the depth of analysis of a therapy problem according to its expected benefits. A third independent project was started in 1986 by Hansson and Mayer <ref> [18, 19, 20, 21] </ref>, who proposed the use of information value as a means of controlling heuristic search, which in turn is implemented as probabilistic inference using information from the heuristic function as evidence.
Reference: [22] <author> Haussler, D. </author> <title> (1988) Quantifying inductive bias: AI learning algorithms and Valiant's learning framework. </title> <journal> Artificial Intelligence, </journal> <volume> 36(2), </volume> <pages> 177-221. </pages>
Reference-contexts: This is called approximation. 2. A probability of at least 1 ffi that the algorithm will return the correct or optimal solution. These are often called probably correct algorithms. Some researchers, notably Valiant and others in the field of inductive learning <ref> [22, 47] </ref> have studied `probably approximately correct' algorithms, combining the above properties in the obvious way. However, as Horvitz [26] and Hansson and Mayer [19] have pointed out, what is needed is a theory of algorithms that maximize the comprehensive value of computation.
Reference: [23] <author> Heckerman, D., and Jimison, H. </author> <title> (1987) A perspective on confidence and its use in focusing attention during knowledge acquisition. </title> <booktitle> In Proc. Third Workshop on Uncertainty in AI, </booktitle> <address> Seattle, WA: </address> <publisher> AAAI, </publisher> <pages> 123-131. </pages>
Reference-contexts: His work on the control of probabilistic inference in medical decision-making parallels our own on the control of search. Heckerman and Jimison <ref> [23] </ref> have also used medical decision-making as an example domain, showing how to vary the depth of analysis of a therapy problem according to its expected benefits.
Reference: [24] <author> Horvitz, E. J. </author> <title> (1987) Problem-solving design: Reasoning about computational value, tradeoffs, and resources. </title> <booktitle> In Proc. Second Annual NASA Research Forum, </booktitle> <institution> Moffett Field, CA: NASA Ames, </institution> <month> 26-43. </month>
Reference-contexts: Doyle's `rational psychology' project [10, 11] is based on the idea that computations, or internal state changes, are actions to be chosen like any other actions. He has applied this idea to clarify the notions of belief, intention and learning. Horvitz <ref> [24, 25] </ref> was also an early AI contributor to the study of rational choice of computation, recognizing its potential to provide a new foundation for the design of intelligent systems. His work on the control of probabilistic inference in medical decision-making parallels our own on the control of search.
Reference: [25] <author> Horvitz, E. J. </author> <title> (1988) Reasoning about beliefs and actions under computational resource constraints. </title> <booktitle> In Uncertainty in Artificial Intelligence Vol. </booktitle> <volume> 3., </volume> <editor> (T. Levitt, J. Lemmer, and L. Kanal, eds.), </editor> <publisher> Amsterdam: North Holland. </publisher>
Reference-contexts: Doyle's `rational psychology' project [10, 11] is based on the idea that computations, or internal state changes, are actions to be chosen like any other actions. He has applied this idea to clarify the notions of belief, intention and learning. Horvitz <ref> [24, 25] </ref> was also an early AI contributor to the study of rational choice of computation, recognizing its potential to provide a new foundation for the design of intelligent systems. His work on the control of probabilistic inference in medical decision-making parallels our own on the control of search.
Reference: [26] <author> Horvitz, E. J. </author> <title> (1988) Reasoning under varying and uncertain resource constraints. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <address> Minneapolis, MN: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 139-144. 31 </pages>
Reference-contexts: These are often called probably correct algorithms. Some researchers, notably Valiant and others in the field of inductive learning [22, 47] have studied `probably approximately correct' algorithms, combining the above properties in the obvious way. However, as Horvitz <ref> [26] </ref> and Hansson and Mayer [19] have pointed out, what is needed is a theory of algorithms that maximize the comprehensive value of computation.
Reference: [27] <author> Horvitz, E. J., and Russell, S. J. </author> <note> (forthcoming) What is to be done? In preparation. </note>
Reference-contexts: We begin in Section 2 by defining the notion of real-time problem-solving, and discuss the various approaches that have been taken to the problem of boundedness in this context. 1 This view is developed in greater depth in <ref> [27] </ref>. 2 Ralphs inhabit the ralph (Rational Agents with Limited Performance Hardware) project at Berkeley. 2 We see that metareasoning has a vital role to play, along with meta-level learning and compilation. <p> In other words, no computation can be executed until a computation has been executed to decide on it. Regress has been mentioned by many researchers concerned with bounded rationality <ref> [3, 11, 13, 27, 34] </ref> but so far only Lipman [36] has claimed any progress on the problem. Clearly we must back off from insisting on optimal control of all reasoning, just as we back off from insisting on optimal decisions to act.
Reference: [28] <author> Howard, R. A. </author> <title> (1966) Information value theory. </title> <journal> IEEE Transactions on Systems Science and Cybernetics, </journal> <volume> SSC-2(1), </volume> <pages> 22-26. </pages>
Reference-contexts: Since the 1960's, I. J. Good [16] has emphasized the conceptual distinction between classical or "type I" rationality, and what he called "type II" rationality, or the maximization of expected utility taking into account deliberation costs. Researchers in decision analysis, especially Howard <ref> [28] </ref>, have studied the problem of the value of information. Although the theory given below was developed independently, there is a strong parallel with Howard's approach. <p> This would not be the case if we used the later utility estimate for the new best move and the current utility estimate for the current best move. 9 Examining Howard's Information Value Theory <ref> [28] </ref> we find a formula which, in the above context, would amount to defining the expected net value of S j as E [ ^ V (S j )] = E [ ^ U S:S j ([ff S j ; [S j ]])] ^ U S ([ff]) (12) This will only
Reference: [29] <author> Kautz, H. </author> <booktitle> (1989) Computers and Thought Lecture, Eleventh International Joint Congress on Artificial Intelligence, </booktitle> <address> Detroit, MI. </address>
Reference-contexts: We indicated some areas for further research in this vein. Underlying the idea of decision-theoretic control is a distinct approach to dealing with complexity in AI systems. A view prevalent in the inference community, and eloquently 29 described by Kautz <ref> [29] </ref>, has it that intractable problem classes must be avoided, and progress can be made by concentrating on finding polynomial-time subclasses that are as general as possible.
Reference: [30] <author> Korf, R. E. </author> <title> (1987) Real-time heuristic search: First results. </title> <booktitle> In Proc. 6th National Conference on Artificial Intelligence, </booktitle> <address> Seattle, WA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 133-138. </pages>
Reference-contexts: We have tested DTA fl , for which the cost-ratio is a given parameter, against both A fl , which always finds shortest paths regardless of search cost, and RTA fl <ref> [30] </ref>, which uses a limited search horizon. 15 For each algorithm, the total solution cost is found for various values of the "rate of exchange" between computation cost and solution path length.
Reference: [31] <author> Korf, R. E. </author> <title> (1985) Depth-first iterative deepening: An optimal admissible tree search. </title> <journal> Artificial Intelligence, </journal> <volume> 27(1), </volume> <pages> 97-109. </pages>
Reference-contexts: In this way the algorithm uses only linear space, and in the limit wastes a negligible amount of time in repeated subtree expansions. Furthermore, the use of information returned from each subtree to select the direction for further search gives the scheme a clear advantage over IDA fl <ref> [31] </ref>. The decision-theoretic metalevel can be seen as a means to construct a near-optimal anytime algorithm, in Dean's sense [7], from atomic computation steps.
Reference: [32] <author> Laffey, T. J., Cox, P. A., Schmidt, J. L., Kao, S. M., and Read, J. Y. </author> <title> (1988) Real-time knowledge-based systems. </title> <journal> AI Magazine, </journal> <volume> 9(1), </volume> <pages> 27-45. </pages>
Reference-contexts: As a result, designers typically choose a fixed level of output quality, and then perform the necessary precompilation and optimization to achieve that level within a fixed time limit. Laffey et al. <ref> [32] </ref> survey a large number of application programs for real-time AI, and note, somewhat despairingly, that "Currently, ad hoc techniques are used for making a system produce a response within a specified time interval." The inappropriateness of solution optimality as a criterion for success has been recognized 3 in theoretical computer
Reference: [33] <author> Laird, J. E. </author> <title> (1984) Universal Subgoaling. </title> <type> Doctoral dissertation, </type> <institution> Computer Science Department, Carnegie-Mellon University, </institution> <address> Pittsburgh, PA. </address>
Reference-contexts: Within the framework already established, we plan to extend the analysis from evaluation search to other forms of decision-making computations, and to construct a general problem-solving architecture that employs `normative' control over all its activities. The concept of universal subgoaling <ref> [33, 34] </ref> is intended to capture the notion of a complete decision model, by making every aspect of the agent's deliberation recursively open to metareasoning. In 16 Obtaining a satisfactory definition of computational action, as distinct from action in general, is nontrivial.
Reference: [34] <author> Laird, J. E., Newell, A., and Rosenbloom, P. S. </author> <title> (1987) SOAR: An architecture for general intelligence. </title> <booktitle> Artificial Intelligence 33, </booktitle> <pages> 1-64. </pages>
Reference-contexts: In other words, no computation can be executed until a computation has been executed to decide on it. Regress has been mentioned by many researchers concerned with bounded rationality <ref> [3, 11, 13, 27, 34] </ref> but so far only Lipman [36] has claimed any progress on the problem. Clearly we must back off from insisting on optimal control of all reasoning, just as we back off from insisting on optimal decisions to act. <p> Within the framework already established, we plan to extend the analysis from evaluation search to other forms of decision-making computations, and to construct a general problem-solving architecture that employs `normative' control over all its activities. The concept of universal subgoaling <ref> [33, 34] </ref> is intended to capture the notion of a complete decision model, by making every aspect of the agent's deliberation recursively open to metareasoning. In 16 Obtaining a satisfactory definition of computational action, as distinct from action in general, is nontrivial. <p> The addition of time into such a framework is reasonably straightforward. Actions such as information-gathering experiments are problematic, since the rules governing their rationality are isomorphic to those for computations rather than ordinary external actions. 27 the SOAR system <ref> [34] </ref>, however, the basic deliberation mode is goal-directed search.
Reference: [35] <author> Lesser, V., Pavlin, J., and Durfee, E. </author> <title> (1988) Approximate processing in real-time problem-solving. </title> <journal> AI Magazine, </journal> <volume> 9(1), </volume> <pages> 49-61. </pages>
Reference-contexts: Bratman, Israel and Pollack [4] sketch a system design combining decision theory with AI planning techniques, using plans to limit the set of actions to be considered. Lesser and his group have studied real-time problem solving in the context of a distributed vehicle-tracking application <ref> [35] </ref>, modulating approximate problem-solving to achieve robust performance. 3 A framework for metareasoning By metareasoning we mean deliberation concerning possible changes to the computational state of the agent.
Reference: [36] <author> Lipman, B. </author> <title> (1989) How to decide how to decide how to : : : Limited rationality in decisions and games. </title> <booktitle> In Proc. AAAI Symposium on AI and Limited Rationality, </booktitle> <address> Stanford, CA: </address> <publisher> AAAI. </publisher>
Reference-contexts: In other words, no computation can be executed until a computation has been executed to decide on it. Regress has been mentioned by many researchers concerned with bounded rationality [3, 11, 13, 27, 34] but so far only Lipman <ref> [36] </ref> has claimed any progress on the problem. Clearly we must back off from insisting on optimal control of all reasoning, just as we back off from insisting on optimal decisions to act.
Reference: [37] <author> McAllester, D.A. </author> <title> (1988) Conspiracy Numbers for Min-Max Search. </title> <journal> Artificial Intelligence, </journal> <volume> 35, </volume> <pages> 287-310. </pages>
Reference: [38] <author> McCarthy, J. </author> <title> (1958) Programs with common sense. In Readings in Knowledge Representation (R. </title> <editor> J. Brachman and H. J. Levesque, eds.), </editor> <publisher> Los Altos: Morgan Kaufmann (1985), </publisher> <pages> 300-307. </pages>
Reference-contexts: A second, and more fundamental, reason is that existing formal models, by neglecting the fact of limited resources for computation, fail to provide an adequate theoretical basis on which to build a science of artificial intelligence. The `logicist' approach to AI, exemplified by McCarthy's Advice Taker proposal <ref> [38] </ref>, emphasizes the ability to reach correct conclusions from correct premises. The `rational agent' approach, derived from philosophical and economic notions of rational behaviour, emphasizes maximal achievement of goals via decisions to act. When resource bounds come into play, direct implementation of either approach results in suboptimal performance.
Reference: [39] <author> Pearl, J. </author> <title> (1988) Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference San Mateo, </title> <address> CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Hence there can be no utility to computations whose sole possible outcome is to increase confidence. 7 variable is more familiar to AI researchers. Just how the standard axioms of probability and utility theory <ref> [39, 48] </ref> should be revised to allow for the limited rationality of real agents without making them vulnerable to a charge of incoherence is an important open philosophical problem, which we shall not attempt to tackle here. <p> This leads to the standard minimax or "expecti-max" approach of decision analysis (see Pearl, <ref> [39] </ref>, ch. 6). However, an agent with only limited rationality might not have any good reason to assume that she will actually succeed in taking the action with highest expected utility. Much less is such an assumption warranted when modeling an agent from the outside. <p> Moreover, it is difficult to estimate the value that computations provide by making further computations possible or more valuable. Thus, it is necessary to employ simplifying assumptions or approximations. Here we present two such simplifications. They are closely related to what Pearl <ref> [39] </ref> has called a "myopic policy". They can be validated only by consideration of the domain of application. <p> This is a standard approach taken in decision analysis; see for instance, Pearl <ref> [39] </ref>. Let S k be the action of continuing to compute in state [S j ].
Reference: [40] <author> Russell, S. J. </author> <note> (1985) The Compleat Guide to MRS Technical Report no. </note> <institution> STAN-CS-85-1080, Computer Science Department, Stanford University, Stanford, </institution> <address> CA. </address>
Reference-contexts: All meta-level systems share a common methodology for implementation: the base-level problem solver operates via the explicit formulation and solution of meta-level problems. For example, the base-level problem-solver in Genesereth's MRS <ref> [14, 40] </ref> is essentially a theorem-prover, but one that takes the user's goal and, instead of simply running a resolution algorithm, sets up its own goal of finding a way to prove the user's goal.
Reference: [41] <author> Russell, S. J. </author> <title> (1989) Execution architectures and compilation. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <address> Detroit, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For the above reasons, among others, two important topics in the ralph project are inductive learning of meta-level policies and compilation of reasoning. Meta-level learning is discussed briefly in Section 5.4.1, and at greater length in [50, 51]. On compilation of decision-making see <ref> [41] </ref>. We now turn to the topic of how an agent can select its computations optimally without knowing their outcome | the topic of rational metareasoning. 6 3.1 Rational metareasoning The construction of a system capable of rational metareasoning rests on two basic principles: 1. <p> We intend to construct a problem-solving architecture in which decision-theoretic deliberation and its various possible compilations <ref> [41] </ref> are the basic modes of computation, and in which metareasoning is carried out in the principled fashion outlined above, rather than through hand-generated condition-action rules. One can also consider the possibility of applying these ideas to control search in theorem provers.
Reference: [42] <author> Russell, S. J. </author> <title> (1990) Fine-grained decision-theoretic search control. </title> <booktitle> To appear in Proceedings of the Sixth Workshop on Uncertainty in Artificial Intelligence, </booktitle> <address> Cambridge, MA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In the class of estimated utility systems, this means updating one or more of the action utility estimates. In our applications work <ref> [42, 45, 51] </ref>, we have concentrated on metareasoning in forward search programs | programs that revise the utility estimates for outcome states by generating and evaluating their successors. <p> The main effort involved in applying our theory consists of identifying the function f for the decision algorithm and proving simplifying theorems for the value of computation formula. For an extended example of this process, see <ref> [42] </ref>. 6.1 Subtree Independence Many base-level decision-making algorithms satisfy the general condition that a given computation affects the utility estimate of only one action. This is the case of subtree independence, and it enables us to obtain a further simplification of equation 11. <p> 68 ff-fi [3] 10 9,104 82 MGSS fl 20 12,237 170 ff-fi [4] 12 42,977 403 MGSS fl 16.5 21,155 435 ff-fi [5] 15.5 133,100 1,356 MGSS fl 17 45,120 1,590 ff-fi [6] 15 581,433 6,863 Table 1: Summary of results for Othello 26 more refined search control method in <ref> [42] </ref>, though preliminary, show a factor of fifty-nine improvement. We do not, however expect this performance trend to persist against deeper-searching alpha-beta programs.
Reference: [43] <author> Russell, S. J., and Wefald, E. H. </author> <title> (1988) Multi-Level Decision-Theoretic Search. </title> <booktitle> Proceedings of the AAAI Spring Symposium Series on Computer Game-Playing, </booktitle> <address> Stanford, CA, </address> <month> March, </month> <year> 1988. </year> <month> 32 </month>
Reference: [44] <author> Russell, S. J., and Wefald, E. H. </author> <title> (1988) Decision-theoretic control of search: General theory and an application to game-playing. </title> <type> Technical report UCB/CSD 88/435, </type> <institution> Computer Science Division, University of California, Berkeley, </institution> <address> CA. </address>
Reference-contexts: These values can then be used as data points to induce an intensional characterization of the utility of computations. A more detailed description of this procedure is given in <ref> [44, 50] </ref>. Data derived in the above way will of course be error-prone, since the whole point of doing the analysis is that we think that the agent often makes incorrect judgments concerning when to stop and when to continue computing.
Reference: [45] <author> Russell, S. J., and Wefald, E. H. </author> <title> (1989) On Optimal Game-tree Search using Rational Metarea-soning. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <address> Detroit, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In the class of estimated utility systems, this means updating one or more of the action utility estimates. In our applications work <ref> [42, 45, 51] </ref>, we have concentrated on metareasoning in forward search programs | programs that revise the utility estimates for outcome states by generating and evaluating their successors. <p> For game-playing, we have derived a formula for the value of expanding a leaf node that can be computed with very little overhead, given the simplifying assumptions outlined above <ref> [45] </ref>. We implemented a search algorithm, MGSS fl , using this formula and played five thirty-two-game tournaments against an alpha-beta algorithm with depth limits from two to six; both algorithms used the same evaluation function.
Reference: [46] <author> Simon, H. A. </author> <title> (1982) Models of Bounded Rationality, </title> <booktitle> Volume 2. </booktitle> <address> Cambridge: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Researchers in decision analysis, especially Howard [28], have studied the problem of the value of information. Although the theory given below was developed independently, there is a strong parallel with Howard's approach. In the field of economics, Simon <ref> [46] </ref> made clear the distinction between systems that compute the rational thing to do (procedural rationality), and systems that simply do the rational thing (substantive rationality).
Reference: [47] <author> Valiant, L. G. </author> <title> (1984) A theory of the learnable. </title> <journal> Comm. A.C.M. </journal> <volume> 18 (11), </volume> <pages> 1134-1142. </pages> <note> [48] von Neumann, </note> <author> J., and Morgenstern, O. </author> <title> (1947) Theory of Games and Economic Behavior. </title> <publisher> Princeton: Princeton University Press. </publisher>
Reference-contexts: This is called approximation. 2. A probability of at least 1 ffi that the algorithm will return the correct or optimal solution. These are often called probably correct algorithms. Some researchers, notably Valiant and others in the field of inductive learning <ref> [22, 47] </ref> have studied `probably approximately correct' algorithms, combining the above properties in the obvious way. However, as Horvitz [26] and Hansson and Mayer [19] have pointed out, what is needed is a theory of algorithms that maximize the comprehensive value of computation.
Reference: [49] <author> Wefald, E. H., and Russell, S. J. </author> <title> (1989) Estimating the vaue of computation: The case of real-time search. </title> <booktitle> In Proceedings of the AAAI Spring Symposium on AI and Limited Rationality, </booktitle> <address> Stanford, CA, </address> <month> March, </month> <year> 1989. </year>
Reference-contexts: A weaker form of this approach is to consider, for purposes of the meta-level analysis, some, but not all, finite sequences of computation steps. For instance, in the context of single-agent heuristic search <ref> [49] </ref>, we consider the expansion of all leaf nodes to any given finite depth, up to some fixed depth horizon. <p> Here we only sketch the the nature of the applications and their performance. Details appear in the various papers cited below. For single-agent search with an admissible heuristic, we have shown <ref> [49, 51] </ref> that only nodes in the subtree of the current best move should be expanded; derived a computable formula for the expected benefit for expanding a set of such nodes; shown that a best-first search (such as A fl ) is in fact the best policy given only the heuristic
Reference: [50] <author> Wefald, E. H., and Russell, S. J. </author> <title> (1989) Adaptive Learning of Decision-Theoretic Search Control Knowledge. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <address> Ithaca, NY: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For the above reasons, among others, two important topics in the ralph project are inductive learning of meta-level policies and compilation of reasoning. Meta-level learning is discussed briefly in Section 5.4.1, and at greater length in <ref> [50, 51] </ref>. On compilation of decision-making see [41]. <p> These values can then be used as data points to induce an intensional characterization of the utility of computations. A more detailed description of this procedure is given in <ref> [44, 50] </ref>. Data derived in the above way will of course be error-prone, since the whole point of doing the analysis is that we think that the agent often makes incorrect judgments concerning when to stop and when to continue computing.
Reference: [51] <author> Wefald, E. H., and Russell, S. J. </author> <title> (1989) Decision-Theoretic Control of Heuristic Search. </title> <type> U.C. Berkeley Technical Report, forthcoming. 33 </type>
Reference-contexts: For the above reasons, among others, two important topics in the ralph project are inductive learning of meta-level policies and compilation of reasoning. Meta-level learning is discussed briefly in Section 5.4.1, and at greater length in <ref> [50, 51] </ref>. On compilation of decision-making see [41]. <p> In the class of estimated utility systems, this means updating one or more of the action utility estimates. In our applications work <ref> [42, 45, 51] </ref>, we have concentrated on metareasoning in forward search programs | programs that revise the utility estimates for outcome states by generating and evaluating their successors. <p> Here we only sketch the the nature of the applications and their performance. Details appear in the various papers cited below. For single-agent search with an admissible heuristic, we have shown <ref> [49, 51] </ref> that only nodes in the subtree of the current best move should be expanded; derived a computable formula for the expected benefit for expanding a set of such nodes; shown that a best-first search (such as A fl ) is in fact the best policy given only the heuristic
References-found: 50

