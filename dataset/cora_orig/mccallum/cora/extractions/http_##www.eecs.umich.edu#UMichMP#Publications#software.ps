URL: http://www.eecs.umich.edu/UMichMP/Publications/software.ps
Refering-URL: http://www.eecs.umich.edu/UMichMP/abstracts.html
Root-URL: http://www.eecs.umich.edu
Phone: Telephone: Intl. 908-562-3966.  
Address: Center 445 Hoes Lane P.O. Box 1331 Piscataway, NJ 08855-1331, USA.  
Affiliation: Service  
Note: Copyright 1997 IEEE. Published in the Proceedings of the Third International Symposium on High Performance Computer Architecture February 1-5, 1997 in San Antonio, Texas, USA. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribu tion to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions IEEE  
Abstract: In this paper we explore software-managed address translation. The purpose of the study is to specify the memory management design for a high clock-rate PowerPC implementation in which a simple design is a prerequisite for a fast clock and a short design cycle. We show that software-managed address translation is just as efficient as hardware-managed address translation, and it is much more exible. Operating systems such as OSF/1 and Mach charge between 0.10 and 0.28 cycles per instruction (CPI) for address translation using dedicated memory-management hardware. Software-managed translation requires 0.05 CPI. Mechanisms to support such features as shared memory, superpages, sub-page protection, and sparse address spaces can be defined completely in software, allowing much more exibility than in hardware-defined mechanisms. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. E. Anderson, H. M. Levy, B. N. Bershad, and E. D. Lazowska. </author> <title> The interaction of architecture and operating system design. </title> <booktitle> In Proc. Fourth Intl Conf. on Architectural Support for Programming Languages and Operating Systems (ASPLOS 4) , April 1991, </booktitle> <pages> pp. 108120. </pages>
Reference-contexts: However, several recent studies have found that the handling overhead of memory management hardware can get as high as 50% of application execution time <ref> [1, 28, 44] </ref>. Taken together these trends beg the question, is dedicated memory-management hardware buying us anythingdo its ben efits outweigh its overhead? In this paper we demonstrate a memory management design that stays within an acceptable performance overhead and that does not require complex hardware. <p> Aliasing can give rise to the synonym problem when memory is shared at different virtual addresses [22], and this has been shown to cause significant overhead [54]; protection-bit modification is used to implement such features as copy-on-write <ref> [1, 42] </ref>, and can also cause significant overhead when used frequently. The synonym problem has been solved in hardware using schemes such as dual tag sets [22] or back-pointers [51], but these require complex control logic that can impede high clock rates. <p> Another required hardware structure, the per-process context register, points to the process control block of the active process. Virtual address causes a CacheMiss exception Virtual address causes a CacheMiss exception (physical+cacheable address) 20 bits 5 Discussion Many studies have shown that significant overhead is spent servicing TLB misses <ref> [1, 4, 9, 28, 41, 44, 47] </ref>. <p> Virtual address causes a CacheMiss exception Virtual address causes a CacheMiss exception (physical+cacheable address) 20 bits 5 Discussion Many studies have shown that significant overhead is spent servicing TLB misses [1, 4, 9, 28, 41, 44, 47]. In particular, Anderson, et al. <ref> [1] </ref> show TLB miss handlers to be among the most commonly executed primitives, Huck and Hays [28] show that TLB miss handling can account for more than 40% of total run time, and Rosenblum, et al. [44] show that TLB miss handling can account for more than 80% of the kernels
Reference: [2] <author> A. W. Appel and K. Li. </author> <title> Virtual memory primitives for user programs. </title> <booktitle> In Proc. Fourth Intl Conf. on Architectural Support for Programming Languages and Operating Systems (ASPLOS 4) , April 1991, </booktitle> <pages> pp. 96107. </pages>
Reference-contexts: Fine-grained protection. Fine-grained protection marks objects as read-only, read-write, execute-only, etc. The granularity is usually a page, though a larger or smaller granularity is sometimes desirable. Many systems have used protection to implement various memory-system support functions, from copy-on-write to garbage collec tion to distributed shared virtual memory <ref> [2] </ref>. Sparse address spaces. Dynamically loaded shared libraries and multithreaded processes are becoming commonplace, and these features require support for sparse address spaces. This simply means that holes are left in the address space between different objects to leave room for dynamic growth.
Reference: [3] <author> M. J. Bach. </author> <title> The Design of the UNIX Operating System . Prentice-Hall, </title> <publisher> Inc., </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1986. </year>
Reference-contexts: These are found in nearly every modern microar-chitecture and operating system (e.g., UNIX <ref> [3] </ref>, Windows NT [15], OS/2 [16], 4.3 BSD [34], DEC Alpha [17, 46], MIPS [23, 32], PARISC [25], PowerPC [29, 39], Pentium [31], and SPARC [52]), and include the following: Address space protection. User-level applications should not have unrestricted access to the data of other applications or the operating system.
Reference: [4] <author> K. Bala, M. F. Kaashoek, and W. E. Weihl. </author> <title> Software prefetching and caching for translation lookaside buffers. </title> <booktitle> In Proc. First USENIX Symposium on Operating Systems Design and Implementation , November 1994. </booktitle>
Reference-contexts: For example, the Intel Pentium Proces sor Users Manual devotes 100 of its 700+ pages to memory-management structures [31], most of which exist for backward compatibility and are unused by todays system software. Typical virtual memory systems exact a runtime overhead of 5-10% <ref> [4, 9, 41, 47] </ref>, an apparently acceptable cost that has changed little in ten years [14], despite significant changes in cache sizes and organizations. <p> Another required hardware structure, the per-process context register, points to the process control block of the active process. Virtual address causes a CacheMiss exception Virtual address causes a CacheMiss exception (physical+cacheable address) 20 bits 5 Discussion Many studies have shown that significant overhead is spent servicing TLB misses <ref> [1, 4, 9, 28, 41, 44, 47] </ref>. <p> CPI overhead is the total number of cycles spent in TLB handling routines divided by the total number of cycles in the benchmarks. The data is taken from previous TLB studies <ref> [4, 40, 41] </ref> performed on MIPS-based DECstations, which use a software-managed TLB. <p> It is very similar to the MIPS TLB refill handler that requires less than 10 instructions including one load, taking 10 cycles when the load hits in the cache, or 40+ when the load misses in the cache, thereby forcing the reference to main memory <ref> [4] </ref>. In our model, the L2 cache miss handler always takes 10 cycles, and runs whenever we take an L2 cache miss (labeled L2 I-Cache miss L2 D-Cache miss the table).
Reference: [5] <author> B. N. Bershad, C. Chambers, S. Eggers, C. Maeda, D. McNamee, P. Pardyak, S. Savage, and E. G. Sirer. </author> <title> SPIN an extensible microkernel for application-specific operating system services. </title> <type> Tech. Rep. </type> <institution> 94-03-03, University of Washington, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: User-level applications should not have unrestricted access to the data of other applications or the operating system. A common hardware assist uses address space identifiers (ASIDs), which extend virtual addresses and distinguish them from addresses generated by different processes. Alternatively, protection can be provided by software means <ref> [5, 19, 50] </ref>. Shared memory. Shared memory allows multiple processes to reference the same physical data through (potentially) different virtual addresses. Space requirements can be reduced by sharing code between processes. Using shared memory for communication avoids the data-copying of traditional message-passing schemes.
Reference: [6] <author> A. Chang and M. F. Mergen. </author> <title> 801 storage: Architecture and programming. </title> <journal> ACM Transactions on Computer Systems , vol. </journal> <volume> 6, no. 1, </volume> <month> February </month> <year> 1988. </year>
Reference-contexts: can still exist in two different blocks of the same set in an associative, virtually-indexed, virtually-tagged cache. ushing the entire cache or sweeping through the entire cache and modifying the affected lines. 3.2 Segmented translation The IBM 801 introduced a segmented design that persisted through the POWER and PowerPC architectures <ref> [6, 29, 39, 53] </ref>; it is illustrated in Fig 2. Applications generate 32-bit effective addresses that are mapped onto a larger vir tual address space at the granularity of segments virtual regions. Sixteen segments comprise an applications address space.
Reference: [7] <author> J. S. Chase, H. M. Levy, M. Baker-Harvey, and E. D. Lazowska. </author> <title> How to use a 64-bit virtual address space. </title> <type> Tech. Rep. </type> <institution> 92-03-02, University of Washington, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: SunOS requires shared pages to be aligned in virtual space on extremely large boundaries (at least the size of the largest cache) so that aliases will map to the same cache line [10, 24] . Single address space operating systems such as Opal <ref> [7, 8] </ref> or Psyche [45] solve the problem by eliminating the need for virtual-address aliasing entirely. In a single address space all shared data is referenced through global addresses; as in OS/2, this allows pointers to be shared freely across process boundaries. <p> This type of windowing mechanism is used on the PARISC [27]. Shared memory. The sharing mechanism is defined by the page table. One can simplify virtual cache management by sharing memory via global addresses, a scheme used in many systems <ref> [7, 8, 16, 18, 20, 21, 45] </ref>, and shown to have good performance. Alternatively, one could share memory through virtual-address aliasing. Fine-grained protection. One can maintain protection bits in the cache, or in an associated structure like a TLB.
Reference: [8] <author> J. S. Chase, H. M. Levy, E. D. Lazowska, and M. Baker-Harvey. </author> <title> Lightweight shared objects in a 64-bit operating system. </title> <type> Tech. Rep. </type> <institution> 92-03-09, University of Washington, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: SunOS requires shared pages to be aligned in virtual space on extremely large boundaries (at least the size of the largest cache) so that aliases will map to the same cache line [10, 24] . Single address space operating systems such as Opal <ref> [7, 8] </ref> or Psyche [45] solve the problem by eliminating the need for virtual-address aliasing entirely. In a single address space all shared data is referenced through global addresses; as in OS/2, this allows pointers to be shared freely across process boundaries. <p> This type of windowing mechanism is used on the PARISC [27]. Shared memory. The sharing mechanism is defined by the page table. One can simplify virtual cache management by sharing memory via global addresses, a scheme used in many systems <ref> [7, 8, 16, 18, 20, 21, 45] </ref>, and shown to have good performance. Alternatively, one could share memory through virtual-address aliasing. Fine-grained protection. One can maintain protection bits in the cache, or in an associated structure like a TLB.
Reference: [9] <author> J. B. Chen, A. Borg, and N. P. Jouppi. </author> <title> A simulation based study of TLB performance. </title> <booktitle> In Proc. 19th Annual International Symposium on Computer Architecture (ISCA 19) , May 1992. </booktitle>
Reference-contexts: For example, the Intel Pentium Proces sor Users Manual devotes 100 of its 700+ pages to memory-management structures [31], most of which exist for backward compatibility and are unused by todays system software. Typical virtual memory systems exact a runtime overhead of 5-10% <ref> [4, 9, 41, 47] </ref>, an apparently acceptable cost that has changed little in ten years [14], despite significant changes in cache sizes and organizations. <p> Another required hardware structure, the per-process context register, points to the process control block of the active process. Virtual address causes a CacheMiss exception Virtual address causes a CacheMiss exception (physical+cacheable address) 20 bits 5 Discussion Many studies have shown that significant overhead is spent servicing TLB misses <ref> [1, 4, 9, 28, 41, 44, 47] </ref>.
Reference: [10] <author> R. Cheng. </author> <title> Virtual address cache in UNIX. </title> <booktitle> In Proceedings of the Summer 1987 USENIX Technical Conference , June 1987. </booktitle>
Reference-contexts: SunOS requires shared pages to be aligned in virtual space on extremely large boundaries (at least the size of the largest cache) so that aliases will map to the same cache line <ref> [10, 24] </ref> . Single address space operating systems such as Opal [7, 8] or Psyche [45] solve the problem by eliminating the need for virtual-address aliasing entirely.
Reference: [11] <author> D. R. Cheriton, H. A. Goosen, and P. D. Boyle. </author> <title> Multi-level shared caching techniques for scalability in VMP-MC. </title> <booktitle> In Proc. 16th Annual International Symposium on Computer Architecture (ISCA 16) , June 1989. </booktitle>
Reference-contexts: The design is software-managed address translation softvm for short. It dispenses with hardware such as the translation lookaside buffers (TLBs) found in every modern microar-chitecture and the page-table-walking state machines found in x86 and PowerPC architectures. It uses a software-handled cache miss, as in the VMP multiprocessor <ref> [11, 12, 13] </ref>, except that VMP used the mechanism to explore cache coherence in a multiprocessor, while we use it to simplify memory management hardware in a uniprocessor. <p> The SPUR design eliminated specialized, dedicated hardware to store mapping information. However, it replaced the TLB with another specialized hardware translation mechanisma finite state machine that searched for PTEs in general-purpose storage (the cache) instead of special purpose storage (TLB slots). 3.5 VMP: Software-controlled caches The VMP multiprocessor <ref> [11, 12, 13] </ref> places virtual caches under software control. Each processor node contains several hardware structures, including a central processing unit, a software-controlled virtual cache, a cache controller, and special memory.
Reference: [12] <author> D. R. Cheriton, A. Gupta, P. D. Boyle, and H. A. Goosen. </author> <title> The VMP multiprocessor: Initial experience, refinements and performance evaluation. </title> <booktitle> In Proc. 15th Annual International Symposium on Computer Architecture (ISCA 15) , May 1988. </booktitle>
Reference-contexts: The design is software-managed address translation softvm for short. It dispenses with hardware such as the translation lookaside buffers (TLBs) found in every modern microar-chitecture and the page-table-walking state machines found in x86 and PowerPC architectures. It uses a software-handled cache miss, as in the VMP multiprocessor <ref> [11, 12, 13] </ref>, except that VMP used the mechanism to explore cache coherence in a multiprocessor, while we use it to simplify memory management hardware in a uniprocessor. <p> The SPUR design eliminated specialized, dedicated hardware to store mapping information. However, it replaced the TLB with another specialized hardware translation mechanisma finite state machine that searched for PTEs in general-purpose storage (the cache) instead of special purpose storage (TLB slots). 3.5 VMP: Software-controlled caches The VMP multiprocessor <ref> [11, 12, 13] </ref> places virtual caches under software control. Each processor node contains several hardware structures, including a central processing unit, a software-controlled virtual cache, a cache controller, and special memory.
Reference: [13] <author> D. R. Cheriton, G. A. Slavenburg, and P. D. Boyle. </author> <title> Software-controlled caches in the VMP multiprocessor. </title> <booktitle> In Proc. 13th Annual International Symposium on Computer Architecture (ISCA 13) , January 1986. </booktitle>
Reference-contexts: The design is software-managed address translation softvm for short. It dispenses with hardware such as the translation lookaside buffers (TLBs) found in every modern microar-chitecture and the page-table-walking state machines found in x86 and PowerPC architectures. It uses a software-handled cache miss, as in the VMP multiprocessor <ref> [11, 12, 13] </ref>, except that VMP used the mechanism to explore cache coherence in a multiprocessor, while we use it to simplify memory management hardware in a uniprocessor. <p> The SPUR design eliminated specialized, dedicated hardware to store mapping information. However, it replaced the TLB with another specialized hardware translation mechanisma finite state machine that searched for PTEs in general-purpose storage (the cache) instead of special purpose storage (TLB slots). 3.5 VMP: Software-controlled caches The VMP multiprocessor <ref> [11, 12, 13] </ref> places virtual caches under software control. Each processor node contains several hardware structures, including a central processing unit, a software-controlled virtual cache, a cache controller, and special memory.
Reference: [14] <author> D. W. Clark and J. S. Emer. </author> <title> Performance of the VAX-11/780 translation buffer: Simulation and measurement. </title> <journal> ACM Transactions on Computer Systems , vol. </journal> <volume> 3, no. 1, </volume> <month> February </month> <year> 1985. </year>
Reference-contexts: Typical virtual memory systems exact a runtime overhead of 5-10% [4, 9, 41, 47], an apparently acceptable cost that has changed little in ten years <ref> [14] </ref>, despite significant changes in cache sizes and organizations. However, several recent studies have found that the handling overhead of memory management hardware can get as high as 50% of application execution time [1, 28, 44].
Reference: [15] <author> H. Custer. </author> <title> Inside Windows/NT. </title> <type> Tech. Rep., </type> <institution> Microsoft Press, </institution> <year> 1993. </year>
Reference-contexts: These are found in nearly every modern microar-chitecture and operating system (e.g., UNIX [3], Windows NT <ref> [15] </ref>, OS/2 [16], 4.3 BSD [34], DEC Alpha [17, 46], MIPS [23, 32], PARISC [25], PowerPC [29, 39], Pentium [31], and SPARC [52]), and include the following: Address space protection. User-level applications should not have unrestricted access to the data of other applications or the operating system.
Reference: [16] <author> H. </author> <title> Deitel. </title> <publisher> Inside OS/2 . Addison-Wesley, </publisher> <address> Reading MA, </address> <year> 1990. </year>
Reference-contexts: These are found in nearly every modern microar-chitecture and operating system (e.g., UNIX [3], Windows NT [15], OS/2 <ref> [16] </ref>, 4.3 BSD [34], DEC Alpha [17, 46], MIPS [23, 32], PARISC [25], PowerPC [29, 39], Pentium [31], and SPARC [52]), and include the following: Address space protection. User-level applications should not have unrestricted access to the data of other applications or the operating system. <p> Synonyms can be avoided by setting policy in the operating systemfor example, OS/2 requires all shared segments to be located at identical virtual addresses in all processes so that processes use the same address for the same data <ref> [16] </ref>. SunOS requires shared pages to be aligned in virtual space on extremely large boundaries (at least the size of the largest cache) so that aliases will map to the same cache line [10, 24] . <p> This type of windowing mechanism is used on the PARISC [27]. Shared memory. The sharing mechanism is defined by the page table. One can simplify virtual cache management by sharing memory via global addresses, a scheme used in many systems <ref> [7, 8, 16, 18, 20, 21, 45] </ref>, and shown to have good performance. Alternatively, one could share memory through virtual-address aliasing. Fine-grained protection. One can maintain protection bits in the cache, or in an associated structure like a TLB.
Reference: [17] <author> Digital. </author> <title> DECchip 21064 and DECchip 21064A Alpha AXP Microprocessors Hardware Reference Manual . Digital Equipment Corporation, </title> <address> Maynard MA, </address> <year> 1994. </year>
Reference-contexts: These are found in nearly every modern microar-chitecture and operating system (e.g., UNIX [3], Windows NT [15], OS/2 [16], 4.3 BSD [34], DEC Alpha <ref> [17, 46] </ref>, MIPS [23, 32], PARISC [25], PowerPC [29, 39], Pentium [31], and SPARC [52]), and include the following: Address space protection. User-level applications should not have unrestricted access to the data of other applications or the operating system. <p> Flushing is avoided until the system runs out of identifiers and must reuse them. For example, the address space identifiers on the MIPS R3000 and Alpha 21064 are six bits wide, with a maximum of 64 active processes <ref> [17, 32] </ref>. If more processes are desired, identifiers must be constantly reassigned, requiring TLB & virtual-cache ushes. searches the cache for that address.
Reference: [18] <author> P. Druschel and L. L. Peterson. Fbufs: </author> <title> A high-bandwidth cross-domain transfer facility. </title> <booktitle> In Proc. Fourteenth ACM Symposium on Operating Systems Principles , December 1993, </booktitle> <pages> pp. 189202. </pages>
Reference-contexts: Since a system call is typically an order of magnitude faster than copying a page of data, many researchers have investigated zero-copy schemes, in which the operating system unmaps pages from the senders address space and re-maps them into the receivers address space <ref> [18, 20, 35] </ref>. Large address spaces. Applications require increasingly large virtual spaces; industry has responded with 64-bit machines. However, a large address space does not imply a large address: large addresses are simply one way to implement large address spaces. <p> This type of windowing mechanism is used on the PARISC [27]. Shared memory. The sharing mechanism is defined by the page table. One can simplify virtual cache management by sharing memory via global addresses, a scheme used in many systems <ref> [7, 8, 16, 18, 20, 21, 45] </ref>, and shown to have good performance. Alternatively, one could share memory through virtual-address aliasing. Fine-grained protection. One can maintain protection bits in the cache, or in an associated structure like a TLB.
Reference: [19] <author> D. Engler, R. Dean, A. Forin, and R. Rashid. </author> <title> The operating system as a secure programmable machine. </title> <booktitle> In Proc. 1994 European SIGOPS Workshop September 1994. </booktitle>
Reference-contexts: User-level applications should not have unrestricted access to the data of other applications or the operating system. A common hardware assist uses address space identifiers (ASIDs), which extend virtual addresses and distinguish them from addresses generated by different processes. Alternatively, protection can be provided by software means <ref> [5, 19, 50] </ref>. Shared memory. Shared memory allows multiple processes to reference the same physical data through (potentially) different virtual addresses. Space requirements can be reduced by sharing code between processes. Using shared memory for communication avoids the data-copying of traditional message-passing schemes.
Reference: [20] <author> W. E. Garrett, M. L. Scott, R. Bianchini, L. I. Kontothanassis, . R. A. McCallumm, J. A. Thomas, R. Wisniewski, and S. Luk. </author> <title> Linking shared segments. </title> <booktitle> In USENIX Technical Conference Proceedings , January 1993. </booktitle>
Reference-contexts: Since a system call is typically an order of magnitude faster than copying a page of data, many researchers have investigated zero-copy schemes, in which the operating system unmaps pages from the senders address space and re-maps them into the receivers address space <ref> [18, 20, 35] </ref>. Large address spaces. Applications require increasingly large virtual spaces; industry has responded with 64-bit machines. However, a large address space does not imply a large address: large addresses are simply one way to implement large address spaces. <p> This type of windowing mechanism is used on the PARISC [27]. Shared memory. The sharing mechanism is defined by the page table. One can simplify virtual cache management by sharing memory via global addresses, a scheme used in many systems <ref> [7, 8, 16, 18, 20, 21, 45] </ref>, and shown to have good performance. Alternatively, one could share memory through virtual-address aliasing. Fine-grained protection. One can maintain protection bits in the cache, or in an associated structure like a TLB.
Reference: [21] <author> W. E. Garrett, R. Bianchini, L. Kontothanassis, . R. A. McCallum, J. Thomas, R. Wisniewski, and M. L. Scott. </author> <title> Dynamic sharing and backward compatibility on 64-bit machines. </title> <type> Tech. Rep. TR 418, </type> <institution> University of Rochester, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: This type of windowing mechanism is used on the PARISC [27]. Shared memory. The sharing mechanism is defined by the page table. One can simplify virtual cache management by sharing memory via global addresses, a scheme used in many systems <ref> [7, 8, 16, 18, 20, 21, 45] </ref>, and shown to have good performance. Alternatively, one could share memory through virtual-address aliasing. Fine-grained protection. One can maintain protection bits in the cache, or in an associated structure like a TLB.
Reference: [22] <author> J. R. Goodman. </author> <title> Coherency for multiprocessor virtual address caches. </title> <booktitle> In Proc. Second Intl Conf. on Architectural Support for Programming Languages and Operating Systems (ASPLOS 2) , October 1987, </booktitle> <pages> pp. 7281. </pages>
Reference-contexts: Aliasing can give rise to the synonym problem when memory is shared at different virtual addresses <ref> [22] </ref>, and this has been shown to cause significant overhead [54]; protection-bit modification is used to implement such features as copy-on-write [1, 42], and can also cause significant overhead when used frequently. The synonym problem has been solved in hardware using schemes such as dual tag sets [22] or back-pointers [51], <p> different virtual addresses <ref> [22] </ref>, and this has been shown to cause significant overhead [54]; protection-bit modification is used to implement such features as copy-on-write [1, 42], and can also cause significant overhead when used frequently. The synonym problem has been solved in hardware using schemes such as dual tag sets [22] or back-pointers [51], but these require complex control logic that can impede high clock rates.
Reference: [23] <author> J. Heinrich, Ed. </author> <title> MIPS R10000 Microprocessor Users Manual, version 1.0 MIPS Technologies, </title> <publisher> Inc., </publisher> <address> Mountain View CA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: These are found in nearly every modern microar-chitecture and operating system (e.g., UNIX [3], Windows NT [15], OS/2 [16], 4.3 BSD [34], DEC Alpha [17, 46], MIPS <ref> [23, 32] </ref>, PARISC [25], PowerPC [29, 39], Pentium [31], and SPARC [52]), and include the following: Address space protection. User-level applications should not have unrestricted access to the data of other applications or the operating system. <p> Segment Offset 32-bit Effective Address Page Offset DATA Segno Segment Registers Segment Offset Page OffsetSegment ID 52-bit Virtual Address TLB and Page Table TAG COMPARE Virtual Page Number Cache 3.3 MIPS: A simple 32-bit page table design MIPS <ref> [23, 32] </ref> eliminated the page-table-walking hardware found in traditional memory management units, and in doing so demonstrated that software can table-walk with reasonable efficiency. It also presented a simple hierarchical page table design, shown in Fig 3.
Reference: [24] <author> J. L. Hennessy and D. A. Patterson. </author> <title> Computer Architecture: </title> <publisher> A Quantitative Approach . Morgan Kaufmann Publishers, Inc., </publisher> <year> 1990. </year>
Reference-contexts: SunOS requires shared pages to be aligned in virtual space on extremely large boundaries (at least the size of the largest cache) so that aliases will map to the same cache line <ref> [10, 24] </ref> . Single address space operating systems such as Opal [7, 8] or Psyche [45] solve the problem by eliminating the need for virtual-address aliasing entirely.
Reference: [25] <author> Hewlett-Packard. </author> <title> PA-RISC 1.1 Architecture and Instruction Set Reference Manual . Hewlett-Packard Company, </title> <year> 1990. </year>
Reference-contexts: These are found in nearly every modern microar-chitecture and operating system (e.g., UNIX [3], Windows NT [15], OS/2 [16], 4.3 BSD [34], DEC Alpha [17, 46], MIPS [23, 32], PARISC <ref> [25] </ref>, PowerPC [29, 39], Pentium [31], and SPARC [52]), and include the following: Address space protection. User-level applications should not have unrestricted access to the data of other applications or the operating system. <p> However, a large address space does not imply a large address: large addresses are simply one way to implement large address spaces. Another is to provide each process a 4GB window into a larger global virtual address space, the approach used by the PARISC 1.X and 32-bit PowerPC architectures <ref> [25, 39] </ref>. Fine-grained protection. Fine-grained protection marks objects as read-only, read-write, execute-only, etc. The granularity is usually a page, though a larger or smaller granularity is sometimes desirable.
Reference: [26] <author> M. D. Hill, et al. </author> <title> Design Decisions in SPUR. </title> <journal> IEEE Computer , vol. </journal> <volume> 19, no. 11, </volume> <month> November </month> <year> 1986. </year>
Reference-contexts: It uses a software-handled cache miss, as in the VMP multiprocessor [11, 12, 13], except that VMP used the mechanism to explore cache coherence in a multiprocessor, while we use it to simplify memory management hardware in a uniprocessor. It also resembles the in-cache address translation mechanism of SPUR <ref> [26, 43, 56] </ref> in its lack of TLBs, but takes the design one step further by eliminating table-walking hardware. Software-managed address translation supports common operating systems features such as address space protection, fine-grained protection, sparse address spaces, and superpages. <p> We base our page table and cache miss examples on this scheme for simplicity and clarity; however, any other organization could be used as well. 3.4 SPUR: In-cache address translation SPUR <ref> [26, 43, 55, 56] </ref> demonstrated that the storage slots of the TLB are not a necessary component in address translation. The architecture uses a virtually indexed, virtually tagged cache to delay the need for address translation until a cache miss occurs.
Reference: [27] <author> J. Huck. </author> <type> Personal communication </type>
Reference-contexts: A process could use its 4GB space as a window onto the larger space, moving virtual segments in and out of its working set as necessary. This type of windowing mechanism is used on the PARISC <ref> [27] </ref>. Shared memory. The sharing mechanism is defined by the page table. One can simplify virtual cache management by sharing memory via global addresses, a scheme used in many systems [7, 8, 16, 18, 20, 21, 45], and shown to have good performance.
Reference: [28] <author> J. Huck and J. Hays. </author> <title> Architectural support for translation table management in large address space machines. </title> <booktitle> In Proc. 20th Annual International Symposium on Computer Architecture (ISCA 20) , May 1993. </booktitle>
Reference-contexts: However, several recent studies have found that the handling overhead of memory management hardware can get as high as 50% of application execution time <ref> [1, 28, 44] </ref>. Taken together these trends beg the question, is dedicated memory-management hardware buying us anythingdo its ben efits outweigh its overhead? In this paper we demonstrate a memory management design that stays within an acceptable performance overhead and that does not require complex hardware. <p> Another required hardware structure, the per-process context register, points to the process control block of the active process. Virtual address causes a CacheMiss exception Virtual address causes a CacheMiss exception (physical+cacheable address) 20 bits 5 Discussion Many studies have shown that significant overhead is spent servicing TLB misses <ref> [1, 4, 9, 28, 41, 44, 47] </ref>. <p> In particular, Anderson, et al. [1] show TLB miss handlers to be among the most commonly executed primitives, Huck and Hays <ref> [28] </ref> show that TLB miss handling can account for more than 40% of total run time, and Rosenblum, et al. [44] show that TLB miss handling can account for more than 80% of the kernels computation time. Typical measurements put TLB handling at 5-10% of a normal systems run time.
Reference: [29] <author> IBM and Motorola. </author> <title> PowerPC 601 RISC Microprocessor Users Manual . IBM Microelectronics and Motorola, </title> <year> 1993. </year>
Reference-contexts: These are found in nearly every modern microar-chitecture and operating system (e.g., UNIX [3], Windows NT [15], OS/2 [16], 4.3 BSD [34], DEC Alpha [17, 46], MIPS [23, 32], PARISC [25], PowerPC <ref> [29, 39] </ref>, Pentium [31], and SPARC [52]), and include the following: Address space protection. User-level applications should not have unrestricted access to the data of other applications or the operating system. <p> can still exist in two different blocks of the same set in an associative, virtually-indexed, virtually-tagged cache. ushing the entire cache or sweeping through the entire cache and modifying the affected lines. 3.2 Segmented translation The IBM 801 introduced a segmented design that persisted through the POWER and PowerPC architectures <ref> [6, 29, 39, 53] </ref>; it is illustrated in Fig 2. Applications generate 32-bit effective addresses that are mapped onto a larger vir tual address space at the granularity of segments virtual regions. Sixteen segments comprise an applications address space.
Reference: [30] <author> J. Inouye, R. Konuru, J. Walpole, and B. Sears. </author> <title> The effects of virtually addressed caches on virtual memory design and performance. </title> <type> Tech. Rep. CS/ E 92-010, </type> <institution> Oregon Graduate Institute, </institution> <year> 1992. </year>
Reference: [31] <author> Intel. </author> <title> Pentium Processor Users Manual . Intel Corporation, Mt. </title> <address> Prospect IL, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction In many commercial architectures the hardware support for memory management is unnecessarily complicated, places constraints on the operating system, and often frustrates porting efforts [37]. For example, the Intel Pentium Proces sor Users Manual devotes 100 of its 700+ pages to memory-management structures <ref> [31] </ref>, most of which exist for backward compatibility and are unused by todays system software. Typical virtual memory systems exact a runtime overhead of 5-10% [4, 9, 41, 47], an apparently acceptable cost that has changed little in ten years [14], despite significant changes in cache sizes and organizations. <p> These are found in nearly every modern microar-chitecture and operating system (e.g., UNIX [3], Windows NT [15], OS/2 [16], 4.3 BSD [34], DEC Alpha [17, 46], MIPS [23, 32], PARISC [25], PowerPC [29, 39], Pentium <ref> [31] </ref>, and SPARC [52]), and include the following: Address space protection. User-level applications should not have unrestricted access to the data of other applications or the operating system. A common hardware assist uses address space identifiers (ASIDs), which extend virtual addresses and distinguish them from addresses generated by different processes. <p> Superpages. By removing the TLB one removes hardware support for superpages, but as with sparse address spaces one also frees the operating system to provide support through the page table. For instance, a top-down hierarchical page table (as in the x86 <ref> [31] </ref>) would provide easy support for superpages. A guarded page table [36, 38] would also provide support, and would map a large address space more efficiently, as would the inverted page table variant described by Talluri, et al. [48]. Direct memory access.
Reference: [32] <author> G. Kane and J. Heinrich. </author> <title> MIPS RISC Architecture . Prentice-Hall, </title> <address> Englewood Cliffs NJ, </address> <year> 1992. </year>
Reference-contexts: These are found in nearly every modern microar-chitecture and operating system (e.g., UNIX [3], Windows NT [15], OS/2 [16], 4.3 BSD [34], DEC Alpha [17, 46], MIPS <ref> [23, 32] </ref>, PARISC [25], PowerPC [29, 39], Pentium [31], and SPARC [52]), and include the following: Address space protection. User-level applications should not have unrestricted access to the data of other applications or the operating system. <p> Segment Offset 32-bit Effective Address Page Offset DATA Segno Segment Registers Segment Offset Page OffsetSegment ID 52-bit Virtual Address TLB and Page Table TAG COMPARE Virtual Page Number Cache 3.3 MIPS: A simple 32-bit page table design MIPS <ref> [23, 32] </ref> eliminated the page-table-walking hardware found in traditional memory management units, and in doing so demonstrated that software can table-walk with reasonable efficiency. It also presented a simple hierarchical page table design, shown in Fig 3. <p> Flushing is avoided until the system runs out of identifiers and must reuse them. For example, the address space identifiers on the MIPS R3000 and Alpha 21064 are six bits wide, with a maximum of 64 active processes <ref> [17, 32] </ref>. If more processes are desired, identifiers must be constantly reassigned, requiring TLB & virtual-cache ushes. searches the cache for that address. <p> As later graphs will show, the small linesize gives the worst-case performance for the software-managed scheme. The model includes a simulated MIPS-style TLB <ref> [32] </ref> with 64 entries, a random replace ment policy, and 8 slots reserved for root PTEs. The table shows what steps the operating system and hardware take when cache and TLB misses occur.
Reference: [33] <author> Y. A. Khalidi, M. Talluri, M. N. Nelson, and D. Williams. </author> <title> Virtual memory support for multiple page sizes. </title> <booktitle> In Proc. Fourth Workshop on Workstation Operating Systems , October 1993. </booktitle>
Reference-contexts: Several studies have shown significant performance gains for reducing the number of TLB entries to cover the current working set <ref> [33, 47, 49] </ref>. Direct memory access. Direct memory access (DMA) allows asynchronous copying of data from I/O devices directly to main memory. It is difficult to implement with virtual caches, as the I/O space is usually physically mapped.
Reference: [34] <author> S. J. Leffler, M. K. McKusick, M. J. Karels, and J. S. Quarterman. </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System . Addison-Wesley Publishing Company, </title> <year> 1989. </year>
Reference-contexts: These are found in nearly every modern microar-chitecture and operating system (e.g., UNIX [3], Windows NT [15], OS/2 [16], 4.3 BSD <ref> [34] </ref>, DEC Alpha [17, 46], MIPS [23, 32], PARISC [25], PowerPC [29, 39], Pentium [31], and SPARC [52]), and include the following: Address space protection. User-level applications should not have unrestricted access to the data of other applications or the operating system. <p> Sparse address spaces. Dynamically loaded shared libraries and multithreaded processes are becoming commonplace, and these features require support for sparse address spaces. This simply means that holes are left in the address space between different objects to leave room for dynamic growth. In contrast, the 4.3BSD UNIX <ref> [34] </ref> address space was composed of two continuous regions, depicted in Fig 1. This arrangement allowed the user page tables to occupy minimal space, which was important because the original virtual memory design did not allow the page tables to be paged. Superpages.
Reference: [35] <author> J. Liedtke. </author> <title> Improving IPC by kernel design. </title> <booktitle> In Proc. Fourteenth ACM Symposium on Operating Systems Principles , December 1993, </booktitle> <pages> pp. 175187. </pages>
Reference-contexts: Since a system call is typically an order of magnitude faster than copying a page of data, many researchers have investigated zero-copy schemes, in which the operating system unmaps pages from the senders address space and re-maps them into the receivers address space <ref> [18, 20, 35] </ref>. Large address spaces. Applications require increasingly large virtual spaces; industry has responded with 64-bit machines. However, a large address space does not imply a large address: large addresses are simply one way to implement large address spaces.
Reference: [36] <author> J. Liedtke. </author> <title> Address space sparsity and fine granularity. </title> <journal> ACM Operating Systems Review , vol. </journal> <volume> 29, no. 1, </volume> <pages> pp. 8790, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: For instance, a top-down hierarchical page table (as in the x86 [31]) would provide easy support for superpages. A guarded page table <ref> [36, 38] </ref> would also provide support, and would map a large address space more efficiently, as would the inverted page table variant described by Talluri, et al. [48]. Direct memory access.
Reference: [37] <author> J. Liedtke. </author> <title> On micro-kernel construction. </title> <booktitle> In Proc. Fifteenth ACM Symposium on Operating Systems Principles , December 1995. </booktitle>
Reference-contexts: 1 Introduction In many commercial architectures the hardware support for memory management is unnecessarily complicated, places constraints on the operating system, and often frustrates porting efforts <ref> [37] </ref>. For example, the Intel Pentium Proces sor Users Manual devotes 100 of its 700+ pages to memory-management structures [31], most of which exist for backward compatibility and are unused by todays system software.
Reference: [38] <author> J. Liedtke and K. Elphinstone. </author> <title> Guarded page tables on MIPS R4600. </title> <journal> ACM Operating Systems Review , vol. </journal> <volume> 30, no. 1, </volume> <pages> pp. 415, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: For instance, a top-down hierarchical page table (as in the x86 [31]) would provide easy support for superpages. A guarded page table <ref> [36, 38] </ref> would also provide support, and would map a large address space more efficiently, as would the inverted page table variant described by Talluri, et al. [48]. Direct memory access.
Reference: [39] <author> C. </author> <month> May, </month> <editor> E. Silha, R. Simpson, and H. Warren, Eds. </editor> <title> The PowerPC Architecture: A Specification for a New Family of RISC Processors . Morgan Kaufmann Publishers, </title> <address> San Francisco CA, </address> <year> 1994. </year>
Reference-contexts: L. Jacob and T. Mudge. Software-managed address translation.Proc. 3rd Symp. High Performance Computer Architectur, San Antonio, TX, Feb. 1997, pp. 156-167. vital for a fast clock and a short design cycle. The example implementation adds PowerPC segments <ref> [39] </ref> to the softvm design; these support address space protection, shared memory, and provide access to a large virtual address space. They are not an essential component of software-managed address translationfor example, they could be replaced by long address space identifiers or a 64-bit address space. <p> These are found in nearly every modern microar-chitecture and operating system (e.g., UNIX [3], Windows NT [15], OS/2 [16], 4.3 BSD [34], DEC Alpha [17, 46], MIPS [23, 32], PARISC [25], PowerPC <ref> [29, 39] </ref>, Pentium [31], and SPARC [52]), and include the following: Address space protection. User-level applications should not have unrestricted access to the data of other applications or the operating system. <p> However, a large address space does not imply a large address: large addresses are simply one way to implement large address spaces. Another is to provide each process a 4GB window into a larger global virtual address space, the approach used by the PARISC 1.X and 32-bit PowerPC architectures <ref> [25, 39] </ref>. Fine-grained protection. Fine-grained protection marks objects as read-only, read-write, execute-only, etc. The granularity is usually a page, though a larger or smaller granularity is sometimes desirable. <p> can still exist in two different blocks of the same set in an associative, virtually-indexed, virtually-tagged cache. ushing the entire cache or sweeping through the entire cache and modifying the affected lines. 3.2 Segmented translation The IBM 801 introduced a segmented design that persisted through the POWER and PowerPC architectures <ref> [6, 29, 39, 53] </ref>; it is illustrated in Fig 2. Applications generate 32-bit effective addresses that are mapped onto a larger vir tual address space at the granularity of segments virtual regions. Sixteen segments comprise an applications address space.
Reference: [40] <author> D. </author> <title> Nagle. </title> <type> Personal communication </type>
Reference-contexts: CPI overhead is the total number of cycles spent in TLB handling routines divided by the total number of cycles in the benchmarks. The data is taken from previous TLB studies <ref> [4, 40, 41] </ref> performed on MIPS-based DECstations, which use a software-managed TLB.
Reference: [41] <author> D. Nagle, R. Uhlig, T. Stanley, S. Sechrest, T. Mudge, and R. Brown. </author> <title> Design tradeoffs for software-managed TLBs. </title> <booktitle> In Proc. 20th Annual International Symposium on Computer Architecture (ISCA 20) , May 1993. </booktitle>
Reference-contexts: For example, the Intel Pentium Proces sor Users Manual devotes 100 of its 700+ pages to memory-management structures [31], most of which exist for backward compatibility and are unused by todays system software. Typical virtual memory systems exact a runtime overhead of 5-10% <ref> [4, 9, 41, 47] </ref>, an apparently acceptable cost that has changed little in ten years [14], despite significant changes in cache sizes and organizations. <p> Another required hardware structure, the per-process context register, points to the process control block of the active process. Virtual address causes a CacheMiss exception Virtual address causes a CacheMiss exception (physical+cacheable address) 20 bits 5 Discussion Many studies have shown that significant overhead is spent servicing TLB misses <ref> [1, 4, 9, 28, 41, 44, 47] </ref>. <p> CPI overhead is the total number of cycles spent in TLB handling routines divided by the total number of cycles in the benchmarks. The data is taken from previous TLB studies <ref> [4, 40, 41] </ref> performed on MIPS-based DECstations, which use a software-managed TLB. <p> The advantage of this scheme is that the choice of protection granularity is completely up to the operating system. In this section, we determine the overhead. We performed a study on the frequency of page protection modifications in the Mach operating system. The benchmarks are the same as in <ref> [41] </ref>, and the operating system is Mach3. We chose Mach as it uses copy-on-write lib-e r a l l y, p r o d u c i n g 1 0 0 0 t i m e s t h e p a g e protection modifications seen in Ultrix [41]. <p> <ref> [41] </ref>, and the operating system is Mach3. We chose Mach as it uses copy-on-write lib-e r a l l y, p r o d u c i n g 1 0 0 0 t i m e s t h e p a g e protection modifications seen in Ultrix [41]. We use these numbers to determine the protection overhead of our system; this should give a conservative estimate for the upper bound. The results are shown in Table 5. Page-protection modifications occur on the average of 11.3 for every million instructions.
Reference: [42] <author> R. Rashid, A. Tevanian, M. Young, D. Young, R. Baron, D. Black, W. Bolosky, and J. Chew. </author> <title> Machine-independent virtual memory management for paged uniprocessor and multiprocessor architectures. </title> <journal> IEEE Transactions on Computers , vol. </journal> <volume> 37, no. 8, </volume> <pages> pp. 896908, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Aliasing can give rise to the synonym problem when memory is shared at different virtual addresses [22], and this has been shown to cause significant overhead [54]; protection-bit modification is used to implement such features as copy-on-write <ref> [1, 42] </ref>, and can also cause significant overhead when used frequently. The synonym problem has been solved in hardware using schemes such as dual tag sets [22] or back-pointers [51], but these require complex control logic that can impede high clock rates.
Reference: [43] <author> S. A. Ritchie. </author> <title> TLB for free: In-cache address translation for a multiprocessor workstation. </title> <type> Tech. Rep. </type> <institution> UCB/CSD 85/233, University of California, </institution> <month> May </month> <year> 1985. </year>
Reference-contexts: It uses a software-handled cache miss, as in the VMP multiprocessor [11, 12, 13], except that VMP used the mechanism to explore cache coherence in a multiprocessor, while we use it to simplify memory management hardware in a uniprocessor. It also resembles the in-cache address translation mechanism of SPUR <ref> [26, 43, 56] </ref> in its lack of TLBs, but takes the design one step further by eliminating table-walking hardware. Software-managed address translation supports common operating systems features such as address space protection, fine-grained protection, sparse address spaces, and superpages. <p> We base our page table and cache miss examples on this scheme for simplicity and clarity; however, any other organization could be used as well. 3.4 SPUR: In-cache address translation SPUR <ref> [26, 43, 55, 56] </ref> demonstrated that the storage slots of the TLB are not a necessary component in address translation. The architecture uses a virtually indexed, virtually tagged cache to delay the need for address translation until a cache miss occurs.
Reference: [44] <author> M. Rosenblum, E. Bugnion, S. A. Herrod, E. Witchel, and A. Gupta. </author> <title> The impact of architectural trends on operating system performance. </title> <booktitle> In Proc. Fifteenth ACM Symposium on Operating Systems Principles , December 1995. </booktitle>
Reference-contexts: However, several recent studies have found that the handling overhead of memory management hardware can get as high as 50% of application execution time <ref> [1, 28, 44] </ref>. Taken together these trends beg the question, is dedicated memory-management hardware buying us anythingdo its ben efits outweigh its overhead? In this paper we demonstrate a memory management design that stays within an acceptable performance overhead and that does not require complex hardware. <p> Another required hardware structure, the per-process context register, points to the process control block of the active process. Virtual address causes a CacheMiss exception Virtual address causes a CacheMiss exception (physical+cacheable address) 20 bits 5 Discussion Many studies have shown that significant overhead is spent servicing TLB misses <ref> [1, 4, 9, 28, 41, 44, 47] </ref>. <p> In particular, Anderson, et al. [1] show TLB miss handlers to be among the most commonly executed primitives, Huck and Hays [28] show that TLB miss handling can account for more than 40% of total run time, and Rosenblum, et al. <ref> [44] </ref> show that TLB miss handling can account for more than 80% of the kernels computation time. Typical measurements put TLB handling at 5-10% of a normal systems run time.
Reference: [45] <author> M. L. Scott, T. J. LeBlanc, and B. D. Marsh. </author> <title> Design rationale for Psyche, a general-purpose multiprocessor operating system. </title> <booktitle> In Proc. 1988 International Conference on Parallel Processing , August 1988. </booktitle>
Reference-contexts: SunOS requires shared pages to be aligned in virtual space on extremely large boundaries (at least the size of the largest cache) so that aliases will map to the same cache line [10, 24] . Single address space operating systems such as Opal [7, 8] or Psyche <ref> [45] </ref> solve the problem by eliminating the need for virtual-address aliasing entirely. In a single address space all shared data is referenced through global addresses; as in OS/2, this allows pointers to be shared freely across process boundaries. Protection-bit modification in virtual caches can also be problematic. <p> This type of windowing mechanism is used on the PARISC [27]. Shared memory. The sharing mechanism is defined by the page table. One can simplify virtual cache management by sharing memory via global addresses, a scheme used in many systems <ref> [7, 8, 16, 18, 20, 21, 45] </ref>, and shown to have good performance. Alternatively, one could share memory through virtual-address aliasing. Fine-grained protection. One can maintain protection bits in the cache, or in an associated structure like a TLB.
Reference: [46] <author> R. L. Sites, Ed. </author> <title> Alpha Architecture Reference Manual . Digital Equipment Corporation, </title> <address> Maynard MA, </address> <year> 1992. </year>
Reference-contexts: These are found in nearly every modern microar-chitecture and operating system (e.g., UNIX [3], Windows NT [15], OS/2 [16], 4.3 BSD [34], DEC Alpha <ref> [17, 46] </ref>, MIPS [23, 32], PARISC [25], PowerPC [29, 39], Pentium [31], and SPARC [52]), and include the following: Address space protection. User-level applications should not have unrestricted access to the data of other applications or the operating system.
Reference: [47] <author> M. Talluri and M. D. Hill. </author> <title> Surpassing the TLB performance of superpages with less operating system support. </title> <booktitle> In Proc. Sixth Intl Conf. on Architectural Support for Programming Languages and Operating Systems (ASPLOS 6) , October 1994. </booktitle>
Reference-contexts: For example, the Intel Pentium Proces sor Users Manual devotes 100 of its 700+ pages to memory-management structures [31], most of which exist for backward compatibility and are unused by todays system software. Typical virtual memory systems exact a runtime overhead of 5-10% <ref> [4, 9, 41, 47] </ref>, an apparently acceptable cost that has changed little in ten years [14], despite significant changes in cache sizes and organizations. <p> Several studies have shown significant performance gains for reducing the number of TLB entries to cover the current working set <ref> [33, 47, 49] </ref>. Direct memory access. Direct memory access (DMA) allows asynchronous copying of data from I/O devices directly to main memory. It is difficult to implement with virtual caches, as the I/O space is usually physically mapped. <p> Another required hardware structure, the per-process context register, points to the process control block of the active process. Virtual address causes a CacheMiss exception Virtual address causes a CacheMiss exception (physical+cacheable address) 20 bits 5 Discussion Many studies have shown that significant overhead is spent servicing TLB misses <ref> [1, 4, 9, 28, 41, 44, 47] </ref>.
Reference: [48] <author> M. Talluri, M. D. Hill, and Y. A. Khalidi. </author> <title> A new page table for 64-bit address spaces. </title> <booktitle> In Proc. Fifteenth ACM Symposium on Operating Systems Principles , December 1995. </booktitle>
Reference-contexts: For instance, a top-down hierarchical page table (as in the x86 [31]) would provide easy support for superpages. A guarded page table [36, 38] would also provide support, and would map a large address space more efficiently, as would the inverted page table variant described by Talluri, et al. <ref> [48] </ref>. Direct memory access. While software-managed address translation provides no explicit support for DMA, and actually makes DMA more difficult by requiring a virtual cache, direct memory access is still possible.
Reference: [49] <author> M. Talluri, S. Kong, M. D. Hill, and D. A. Patterson. </author> <title> Tradeoffs in supporting two page sizes. </title> <booktitle> In Proc. 19th Annual International Symposium on Computer Architecture (ISCA 19) , May 1992. </booktitle>
Reference-contexts: Several studies have shown significant performance gains for reducing the number of TLB entries to cover the current working set <ref> [33, 47, 49] </ref>. Direct memory access. Direct memory access (DMA) allows asynchronous copying of data from I/O devices directly to main memory. It is difficult to implement with virtual caches, as the I/O space is usually physically mapped.
Reference: [50] <author> R. Wahbe, S. Lucco, T. E. Anderson, and S. L. Graham. </author> <title> Efficient software based fault isolation. </title> <booktitle> In Proc. Fourteenth ACM Symposium on Operating Systems Principles , December 1993, </booktitle> <pages> pp. </pages> <year> 203216. </year>
Reference-contexts: User-level applications should not have unrestricted access to the data of other applications or the operating system. A common hardware assist uses address space identifiers (ASIDs), which extend virtual addresses and distinguish them from addresses generated by different processes. Alternatively, protection can be provided by software means <ref> [5, 19, 50] </ref>. Shared memory. Shared memory allows multiple processes to reference the same physical data through (potentially) different virtual addresses. Space requirements can be reduced by sharing code between processes. Using shared memory for communication avoids the data-copying of traditional message-passing schemes.
Reference: [51] <author> W.-H. Wang, J.-L. Baer, and H. M. Levy. </author> <title> Organizaiton and performance of a two-level virtual-real cache hierarchy. </title> <booktitle> In Proc. 16th Annual International Symposium on Computer Architecture (ISCA 16) , June 1989, </booktitle> <pages> pp. 140148. </pages>
Reference-contexts: The synonym problem has been solved in hardware using schemes such as dual tag sets [22] or back-pointers <ref> [51] </ref>, but these require complex control logic that can impede high clock rates.
Reference: [52] <author> D. L. Weaver and T. Germand, Eds. </author> <title> The SPARC Architecture Manual version 9 . PTR Prentice Hall, </title> <address> Englewood Cliffs NJ, </address> <year> 1994. </year>
Reference-contexts: These are found in nearly every modern microar-chitecture and operating system (e.g., UNIX [3], Windows NT [15], OS/2 [16], 4.3 BSD [34], DEC Alpha [17, 46], MIPS [23, 32], PARISC [25], PowerPC [29, 39], Pentium [31], and SPARC <ref> [52] </ref>), and include the following: Address space protection. User-level applications should not have unrestricted access to the data of other applications or the operating system. A common hardware assist uses address space identifiers (ASIDs), which extend virtual addresses and distinguish them from addresses generated by different processes.
Reference: [53] <author> S. Weiss and J. E. </author> <title> Smith. </title> <publisher> POWER and PowerPC . Morgan Kaufmann Publishers, </publisher> <address> San Francisco CA, </address> <year> 1994. </year>
Reference-contexts: can still exist in two different blocks of the same set in an associative, virtually-indexed, virtually-tagged cache. ushing the entire cache or sweeping through the entire cache and modifying the affected lines. 3.2 Segmented translation The IBM 801 introduced a segmented design that persisted through the POWER and PowerPC architectures <ref> [6, 29, 39, 53] </ref>; it is illustrated in Fig 2. Applications generate 32-bit effective addresses that are mapped onto a larger vir tual address space at the granularity of segments virtual regions. Sixteen segments comprise an applications address space.
Reference: [54] <author> B. Wheeler and B. N. Bershad. </author> <title> Consistency management for virtually indexed caches. </title> <booktitle> In Proc. Fifth Intl Conf. on Architectural Support for Programming Languages and Operating Systems (ASPLOS 5) , October 1992. </booktitle>
Reference-contexts: Aliasing can give rise to the synonym problem when memory is shared at different virtual addresses [22], and this has been shown to cause significant overhead <ref> [54] </ref>; protection-bit modification is used to implement such features as copy-on-write [1, 42], and can also cause significant overhead when used frequently.
Reference: [55] <author> D. A. Wood. </author> <title> The Design and Evaluation of In-Cache Address Translation PhD thesis, </title> <institution> University of California at Berkeley, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: We base our page table and cache miss examples on this scheme for simplicity and clarity; however, any other organization could be used as well. 3.4 SPUR: In-cache address translation SPUR <ref> [26, 43, 55, 56] </ref> demonstrated that the storage slots of the TLB are not a necessary component in address translation. The architecture uses a virtually indexed, virtually tagged cache to delay the need for address translation until a cache miss occurs. <p> If software resolves cache misses, the operating system is free to implement whatever virtual-to-physical mapping it chooses. Wood demonstrated that with a reasonably large cache (128KB+) the elimination of a TLB is practical <ref> [55] </ref>. For the cache sizes we are considering, we reach the same conclusion (see the Discussion section for details).
Reference: [56] <author> D. A. Wood, S. J. Eggers, G. Gibson, M. D. Hill, J. M. Pendleton, S. A. Ritchie, G. S. Taylor, R. H. Katz, and D. A. Patterson. </author> <title> An in-cache address translation mechanism. </title> <booktitle> In Proc. 13th Annual International Symposium on Computer Architecture (ISCA 13) , January 1986. </booktitle>
Reference-contexts: It uses a software-handled cache miss, as in the VMP multiprocessor [11, 12, 13], except that VMP used the mechanism to explore cache coherence in a multiprocessor, while we use it to simplify memory management hardware in a uniprocessor. It also resembles the in-cache address translation mechanism of SPUR <ref> [26, 43, 56] </ref> in its lack of TLBs, but takes the design one step further by eliminating table-walking hardware. Software-managed address translation supports common operating systems features such as address space protection, fine-grained protection, sparse address spaces, and superpages. <p> We base our page table and cache miss examples on this scheme for simplicity and clarity; however, any other organization could be used as well. 3.4 SPUR: In-cache address translation SPUR <ref> [26, 43, 55, 56] </ref> demonstrated that the storage slots of the TLB are not a necessary component in address translation. The architecture uses a virtually indexed, virtually tagged cache to delay the need for address translation until a cache miss occurs.
References-found: 56

