URL: http://polaris.cs.uiuc.edu/reports/902.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: EXPERIMENTS WITH ELLIPTIC PROBLEM SOLVERS ON THE CEDAR MULTICLUSTER  
Author: G. FRANK E. GALLOPOULOS AND U. MEIER 
Abstract: The implementation and performance of algorithms for the solution of elliptic problems on the Cedar multiprocessor is examined. The algorithms considered are conjugate gradient schemes with or without preconditioning on a rectangular domain and domain decomposition techniques with overlapping (Schwarz Alternating Method) and nonoverlapping subdomains on a T-shaped domain. We present some preliminary results from our current research effort. 1. Introduction. We have presented elsewhere ([1, 2]) algorithms for solving 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. GALLOPOULOS and Y. SAAD, </author> <title> Parallel Block Cyclic Reduction Algorithm for the Fast Solution of Elliptic Equations, </title> <booktitle> Parallel Computing, 10 (1989), </booktitle> <pages> pp. 143-160. </pages>
Reference-contexts: As seen in Section 5, these algorithms necessitate the use of a suitable single-cluster solver for each subdomain. In accordance with our hierarchical decomposition methodology, we have used the best such algorithm in our possession, namely parallel Block Cyclic Reduction <ref> [1] </ref> . The results presented in this paper are by no means complete, but further results are to be expected in [3]. 2. Cedar architecture. Cedar consists of 4 multiple processor clusters which are connected through an interconnection network to a globally shared memory.
Reference: [2] <author> U. MEIER and A. SAMEH, </author> <title> The Behavior of Conjugate Gradient Schemes on a Multi-Vector Processor with a Hierarchical Memory, </title> <journal> J. Comp. App. Math., </journal> <volume> 24 (1988), </volume> <pages> pp. 13-32. </pages>
Reference-contexts: This can also be seen clearly in Fig. 2 where in parentheses we give the number of iterations for each cluster and algorithm for n=255. 3.3. The reduced system approach. Another approach that was implemented on Cedar is the reduced system approach <ref> [2] </ref>.
Reference: [3] <author> G. FRANK, E. GALLOPOULOS and U. MEIER, </author> <title> Experiments with Elliptic Problem Solvers on the Cedar Multicluster, </title> <type> CSRD Report, </type> <note> to appear. </note>
Reference-contexts: In accordance with our hierarchical decomposition methodology, we have used the best such algorithm in our possession, namely parallel Block Cyclic Reduction [1] . The results presented in this paper are by no means complete, but further results are to be expected in <ref> [3] </ref>. 2. Cedar architecture. Cedar consists of 4 multiple processor clusters which are connected through an interconnection network to a globally shared memory. To prevent long global memory access delays, array data can be prefetched into local buffers before they are needed.
Reference: [4] <author> U. MEIER, </author> <title> Preconditioned Conjugate Gradient Schemes for Hierarchical Parallel Architectures, </title> <type> CSRD Report, </type> <year> 1990. </year>
Reference-contexts: Data decomposition for Cedar implementation for larger systems the global memory-version was superior as in this case the cache could be better exploited <ref> [4] </ref>. 3.2. A block diagonal-block Incomplete Cholesky preconditioner. For this approach, A is approximated by a preconditioning matrix M which was obtained through the following construction: Partition A into a kfik-block matrix where k is the number of clusters.
Reference: [5] <author> G. MEURANT, </author> <title> The block preconditioned conjugate gradient algorithm on vector computers, </title> <journal> BIT, </journal> <volume> 24(1984), </volume> <pages> pp. 623-633. </pages>
Reference-contexts: Consider the block diagonal matrix obtained by taking the block diagonal of A and approximate each diagonal block by a block Incomplete Cholesky preconditioner. For the sake of vectorizability we chose the vectorized block Incomplete Cholesky preconditioner INVC3 (1) <ref> [5] </ref>. This preconditioner is completey parallelizable across clusters, has however the disadvantage that the number of iterations depends on the number of clusters used. This can also be seen clearly in Fig. 2 where in parentheses we give the number of iterations for each cluster and algorithm for n=255. 3.3.
Reference: [6] <author> T. CHAN and D. RESASCO, </author> <title> A Framework for the Analysis and Construction of Domain Decomposition Preconditioners, Domain Decomposition Methods for Partial Differential Equations, </title> <editor> R. Glowinski, G.H. Golub, G.A. Meurant, J. Periaux Editors, </editor> <publisher> SIAM, </publisher> <year> 1988. </year>
Reference-contexts: Consequently, it is important to keep this number of iterations small, which can be done with the choice of a good preconditioner for C. Various candidates have been proposed <ref> [6] </ref>. For our problem, those suggested by Bjorstad&Widlund and Chan are equivalent, and we use this, resulting in a modest improvement in convergence over Dryja's choice and a significant improvement over non-preconditioned CG, roughly halving the number of iterations needed for the largest problems.
Reference: [7] <author> H.A. SCHWARZ, </author> <title> Uber einige Abbildungsaufgaben, </title> <journal> Ges. Math. Abh., </journal> <volume> 11(1869), </volume> <pages> pp. 65-83. </pages>
Reference-contexts: For the speedups given in Table 1, note the load imbalance caused by the one subdomain being of only half the size of the other. 5.2. Schwarz Alternating Method (SAM). The Schwarz Alternating Method is a classical method <ref> [7] </ref> that can be applied to a variety of general problems on arbitrary domains. We consider here the domain in Fig. 4 (b).
Reference: [8] <author> W.-P. TANG, </author> <title> Schwarz Splitting and Template Operators, </title> <type> Ph.D. Thesis, </type> <institution> Stanford University, </institution> <year> 1987. </year>
Reference-contexts: As this approach is not parallel, we apply the block Jacobi algorithm to (3). This method converges very slowly. We therefore considered the use of a multilevel technique which was also suggested by Tang <ref> [8] </ref> to speed up convergence.
References-found: 8

