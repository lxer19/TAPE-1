URL: ftp://ftp.cs.virginia.edu/pub/techreports/CS-95-11.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/techreports/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: -jwd,sj3e-@virginia.edu  
Title: Improving Instruction-level Parallelism by Loop Unrolling and Dynamic Memory Disambiguation Modern high-performance processors include a
Author: JACK W. DAVIDSON and SANJAY JINTURKAR S. A. 
Keyword: loop unrolling, dynamic memory disambiguation, instruction-level parallelism, register renaming.  
Note: U.  1: Introduction  
Address: Thornton Hall  Charlottesville, VA 22903  
Affiliation: Department of Computer Science,  University of Virginia  
Abstract: Exploitation of instruction-level parallelism is an effective mechanism for improving the performance of modern superscalar/VLIW processors. Various software techniques can be applied to increase instruction-level parallelism. This paper describes and evaluates a software technique, dynamic memory disambiguation, that permits loops containing loads and stores to be scheduled more aggressively, thereby exposing more instruction-level parallelism. The results of our evaluation show that when dynamic memory disambiguation is applied in conjunction with loop unrolling, register renaming, and static memory disambiguation, the ILP of memory-intensive benchmarks can be increased by as much as 300 percent over loops where dynamic memory disambiguation is not performed. Our measurements also indicate that for the programs that benefit the most from these optimizations, the register usage does not exceed the number of registers on most high-performance processors. One such code improvement technique is loop unrolling (LU) [9, 18] which in conjunction with register renaming (RR) [3, 14] can increase ILP. LU replicates the original loop body multiple times, adjusts the loop termination code and eliminates redundant branch instructions. The resulting larger basic block increases the probability that the instruction scheduler can reorder instructions to exploit ILP. However, the schedulers effectiveness is limited by artificial dependencies created by LUs naive reuse of registers and other data dependencies between instructions. A p p li c a ti o n o f R R ca n e li m i na t e t h e a rt if ic ia l dependencies. The resulting loop has more ILP exposed than the original, rolled loop. The determination of data dependencies requires some analysis by the compiler. Dependencies involving registers can be determined by symbolic comparison, which is a relatively simple process. But dependencies which involve memory references are not easy to resolve. Two memory references, which are symbolically dissimilar, may still access the same memory location. On the other hand, two memory references, which are symbolically the same, may access different memory loc a tion s. D eter m i ning w he th e r th e tw o me m o ry references access the same memory location or not is known as the aliasing problem. In the absence of precise information, a compiler must assume that all the memory references are aliases for the same memory location. Such a conservative approach limits the compilers ability to reorganize the instructions in the program to increase ILP. In this paper, we discuss a technique, called dynamic memory disambiguation (DMD), which disambiguates memory references in loops at execution time. Our research indicates that when DMD is used in conjunction with LU and RR, ILP in benchmark loops can be increased by as much as three times. 
Abstract-found: 1
Intro-found: 1
Reference: [1]] <author> Aho A., Sethi, R., and Ullman, J. D., </author> <booktitle> Compilers Principles, Techniques and Tools, </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1986. </year>
Reference: [2] <author> Alexander, M. J., Bailey, M. W., Childers, B. R., Davidson, J. W., and Jinturkar, S., </author> <title> Memory Bandwidth Optimizations for Wide-Bus Machines, </title> <booktitle> Proceedings of the 25th Hawaii International Conference on System Sciences , Mauii, HA, </booktitle> <month> January </month> <year> 1993, </year> <pages> pp. 466-475. </pages>
Reference-contexts: This research is a continuation of our efforts to evaluate the effectiveness of techniques which determine critical pieces of information regarding data alignment and aliasing at execution time. This technique has proven effective at reducing the memory bandwidth requirements of memory-intensive programs <ref> [2, 8] </ref>. We extend this approach to enhance the exploitation of ILP in loops. Note that the application of DMD allows the exploitation of ILP even when there are multiple call sites and aliasing exists at some but not all of the call sites. <p> <ref> [2] </ref>;r [11]=r [11]+4; nop;nop When the loop is unrolled once, the following code is obtained. // r [11]:address of a, r [10]: address of b // r [4]: address of a + (n * 4) L16: nop;nop M [r [11]]=r [2];nop; r [2]=M [r [11]+4];r [3]=M [r [10]+4]; r [2]=r [2] + r [3];nop; PC=r [11]&lt;r [4]-&gt;L16;nop; Here, each iteration takes 5 cycles. This is a 20 percent performance increase over the rolled loop. Now RR and static memory disambiguation (SMD) can be applied to the loop.
Reference: [3] <author> Bacon, D. F., Graham, S. L., and Sharp, O. J., </author> <title> Compiler Transformations for High-Performance Computing, </title> <journal> ACM Computing Surveys , 26 (4), </journal> <month> Dec. </month> <year> 1994, </year> <pages> pp. 345-420. </pages>
Reference-contexts: As hardware mechanisms for exploiting ILP have become more prevalent, software techniques for increasing the available ILP in programs have become increasingly important. One such code improvement technique is loop unrolling (LU) [9, 18] which in conjunction with register renaming (RR) <ref> [3, 14] </ref> can increase ILP. LU replicates the original loop body multiple times, adjusts the loop termination code and eliminates redundant branch instructions. The resulting larger basic block increases the probability that the instruction scheduler can reorder instructions to exploit ILP.
Reference: [4] <author> Benitez, M. E. and Davidson, J. W., </author> <booktitle> The Advantages of MachineDependant Global Optimization , Proceedings of SIGPLAN 88 Conference on Programming Language Design and Implementation , Atlanta, </booktitle> <address> GA, </address> <month> June </month> <year> 1988, </year> <pages> pp. 329-338. </pages>
Reference-contexts: On this machine, the latency of a memory load and a conditional branch is two cycles. All other instructions have a latency of one cycle. The cycle width of the machine is two. The example is presented using register transfer lists (RTLs) to describe instructions <ref> [4, 5] </ref>. In the examples, M [addr] denotes a memory reference, while r [n] is a register reference. <p> The machine instructions for the above code are given below. Each iteration of this loop takes six cycles to execute. // r [11]:address of a, r [10]: address of b // r <ref> [4] </ref>: address of a + (n * 4) L16: r [10]=r [10]+4;nop; M [r [11]]=r [2];r [11]=r [11]+4; nop;nop When the loop is unrolled once, the following code is obtained. // r [11]:address of a, r [10]: address of b // r [4]: address of a + (n * 4) L16: <p> a, r [10]: address of b // r <ref> [4] </ref>: address of a + (n * 4) L16: r [10]=r [10]+4;nop; M [r [11]]=r [2];r [11]=r [11]+4; nop;nop When the loop is unrolled once, the following code is obtained. // r [11]:address of a, r [10]: address of b // r [4]: address of a + (n * 4) L16: nop;nop M [r [11]]=r [2];nop; r [2]=M [r [11]+4];r [3]=M [r [10]+4]; r [2]=r [2] + r [3];nop; PC=r [11]&lt;r [4]-&gt;L16;nop; Here, each iteration takes 5 cycles. This is a 20 percent performance increase over the rolled loop. <p> The compiler inserts checks to select the appropriate copy at run time. Code to select the appropriate loop to execute and the two loops are shown below. // r [11]:address of a, r [10]: address of b // r [12]: n * 4, r <ref> [4] </ref>: address of a + (n * 4) // check if a + n &lt; b r [13]=r [11]+r [12];nop; nop;nop; // check if b + n &gt;= a r [13]=r [10]+r [12]; nop;nop; // L16 begins aggressive loop L16: r [6]=M [r [11]+4];r [7]=M [r [10]+4]; M [r [11]]=r [2];r <p> Thus, RR is performed only if it will be useful. Our approach, while delivering benefits, does not increase the register utilization unnecessarily. 5: Experimental Results 5.1: Framework We have implemented the above algorithms in the portable C compiler vpcc-vpo <ref> [4, 5] </ref>. The compiler was retargeted to a hypothetical VLIW machine. The compiler employs the same instruction set as that of the MIPS R4000 [12] architecture family with the instruction latencies given in Table 1. The latency of the instructions are comparable to those on current high performance superscalar/VLIW processors.
Reference: [5] <author> Benitez, M. E., </author> <title> Register Allocation and Phase Interactions in Retargetable Optimizing Compilers, </title> <type> PhD Dissertation, </type> <institution> University of Virginia, Charlottesville, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: On this machine, the latency of a memory load and a conditional branch is two cycles. All other instructions have a latency of one cycle. The cycle width of the machine is two. The example is presented using register transfer lists (RTLs) to describe instructions <ref> [4, 5] </ref>. In the examples, M [addr] denotes a memory reference, while r [n] is a register reference. <p> Thus, RR is performed only if it will be useful. Our approach, while delivering benefits, does not increase the register utilization unnecessarily. 5: Experimental Results 5.1: Framework We have implemented the above algorithms in the portable C compiler vpcc-vpo <ref> [4, 5] </ref>. The compiler was retargeted to a hypothetical VLIW machine. The compiler employs the same instruction set as that of the MIPS R4000 [12] architecture family with the instruction latencies given in Table 1. The latency of the instructions are comparable to those on current high performance superscalar/VLIW processors.
Reference: [6] <author> Bernstein, D., D. Cohen, D. E. Maydan, </author> <title> Dynamic Memory Disambiguation for Array References, </title> <booktitle> Proceedings of the 27th Annual International Symposium on Microarchitec-ture, </booktitle> <address> San Jose, CA, </address> <month> Dec. </month> <year> 1994, </year> <pages> pp. 105-112. </pages>
Reference-contexts: In our approach, we rename registers only if it will lead to an improved instruction schedule. Bernstein, Cohen, and Maydan evaluate the effect of DMD on software pipelining, loop invariant code motion, and redundant load elimination <ref> [6] </ref>. In their approach, the difference between the array reference expressions of the memory references which are to be disambiguated is computed. If the absolute value of the difference is greater than the data access size, then the references are not aliased. The resultant expressions are inserted as checks. <p> of a + (n * 4) // check if a + n &lt; b r [13]=r [11]+r [12];nop; nop;nop; // check if b + n &gt;= a r [13]=r [10]+r [12]; nop;nop; // L16 begins aggressive loop L16: r <ref> [6] </ref>=M [r [11]+4];r [7]=M [r [10]+4]; M [r [11]]=r [2];r [6]=r [6] + r [7]; PC=r [11]&lt;r [4]-&gt;L16;nop; ... // L24 begins the safe loop L24: At execution time, if the aggressive copy of the loop is executed, then 3.5 cycles per iteration are required, which is a 70 percent increase over the original code.
Reference: [7] <author> Davidson, J. W. and Whalley, D. B., </author> <title> Ease: An Environment for Architecture Study and Experimentation, </title> <booktitle> Proceedings of the 1990 ACM Sigmetrics Conference on Measurement and Modelling of Computer Systems , Boulder, </booktitle> <publisher> CO, </publisher> <month> May </month> <year> 1990, </year> <pages> pp. 259-260. </pages>
Reference-contexts: (n * 4) // check if a + n &lt; b r [13]=r [11]+r [12];nop; nop;nop; // check if b + n &gt;= a r [13]=r [10]+r [12]; nop;nop; // L16 begins aggressive loop L16: r [6]=M [r [11]+4];r <ref> [7] </ref>=M [r [10]+4]; M [r [11]]=r [2];r [6]=r [6] + r [7]; PC=r [11]&lt;r [4]-&gt;L16;nop; ... // L24 begins the safe loop L24: At execution time, if the aggressive copy of the loop is executed, then 3.5 cycles per iteration are required, which is a 70 percent increase over the original code. <p> To measure ILP, we used the architecture measurement tool Ease to instrument the code and measure the dynamic instruction counts and latency of the code <ref> [7] </ref>. In all the experiments, the unroll factor was three and the cycle width was four unless otherwise stated. The measurements reported in this study were performed on the set of benchmarks listed in Table 2.
Reference: [8] <author> Davidson, J. W. and Jinturkar, S., </author> <title> Memory Access Coalescing: A Technique for Eliminating Redundant Memory Accesses, </title> <booktitle> Proceedings of SIGPLAN 94 Conference on Programming Language Design and Implementation , Orlando, </booktitle> <address> FL, </address> <month> June </month> <year> 1994, </year> <pages> pp 186-195. </pages>
Reference-contexts: This research is a continuation of our efforts to evaluate the effectiveness of techniques which determine critical pieces of information regarding data alignment and aliasing at execution time. This technique has proven effective at reducing the memory bandwidth requirements of memory-intensive programs <ref> [2, 8] </ref>. We extend this approach to enhance the exploitation of ILP in loops. Note that the application of DMD allows the exploitation of ILP even when there are multiple call sites and aliasing exists at some but not all of the call sites.
Reference: [9] <author> Davidson, J. W. and Jinturkar, S., </author> <title> An Aggressive Approach to Loop Unrolling, </title> <type> Technical Report CS-95-26, </type> <institution> Department of Computer Science, University of Virginia, Charlot-tesville, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: The potential to overlap execution of instructions is often referred to as instruction-level parallelism or ILP. As hardware mechanisms for exploiting ILP have become more prevalent, software techniques for increasing the available ILP in programs have become increasingly important. One such code improvement technique is loop unrolling (LU) <ref> [9, 18] </ref> which in conjunction with register renaming (RR) [3, 14] can increase ILP. LU replicates the original loop body multiple times, adjusts the loop termination code and eliminates redundant branch instructions. The resulting larger basic block increases the probability that the instruction scheduler can reorder instructions to exploit ILP. <p> In this paper, only the high-level algorithm to perform DMD is presented. Algorithms to perform LU, RR and instruction scheduling are presented in other reports <ref> [9, 10] </ref>. A portion of the high-level algorithm to implement LU, RR and DMD is contained in Figure 1.
Reference: [10] <author> Davidson, J. W. and Jinturkar, S., </author> <title> Improving Instruction-level Parallelism by Loop Unrolling and Dynamic memory Disambiguation, </title> <type> Technical Report CS-95-13, </type> <institution> Department of Computer Science, University of Virginia, Charlottes-ville, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: The machine instructions for the above code are given below. Each iteration of this loop takes six cycles to execute. // r [11]:address of a, r <ref> [10] </ref>: address of b // r [4]: address of a + (n * 4) L16: r [10]=r [10]+4;nop; M [r [11]]=r [2];r [11]=r [11]+4; nop;nop When the loop is unrolled once, the following code is obtained. // r [11]:address of a, r [10]: address of b // r [4]: address of <p> to execute. // r [11]:address of a, r <ref> [10] </ref>: address of b // r [4]: address of a + (n * 4) L16: r [10]=r [10]+4;nop; M [r [11]]=r [2];r [11]=r [11]+4; nop;nop When the loop is unrolled once, the following code is obtained. // r [11]:address of a, r [10]: address of b // r [4]: address of a + (n * 4) L16: nop;nop M [r [11]]=r [2];nop; r [2]=M [r [11]+4];r [3]=M [r [10]+4]; r [2]=r [2] + r [3];nop; PC=r [11]&lt;r [4]-&gt;L16;nop; Here, each iteration takes 5 cycles. <p> be improved further if the load of M [r <ref> [10] </ref>+4] is scheduled before the store of M [r [11]] . However, that is not possible since the load of M [r [10]+4] may be an alias for the store of M [r [11]]. Since the contents of the registers r [10] and r [11] are parameters to the function enclosing the loop, the relationship between the contents of the two registers cannot be determined by intra-procedural analysis. To resolve this problem, DMD is applied. DMD generates a new copy of the loop called the aggressive loop. <p> In the safe copy, the code remains the same as that after the application of SMD. The compiler inserts checks to select the appropriate copy at run time. Code to select the appropriate loop to execute and the two loops are shown below. // r [11]:address of a, r <ref> [10] </ref>: address of b // r [12]: n * 4, r [4]: address of a + (n * 4) // check if a + n &lt; b r [13]=r [11]+r [12];nop; nop;nop; // check if b + n &gt;= a r [13]=r [10]+r [12]; nop;nop; // L16 begins aggressive loop L16: <p> In this paper, only the high-level algorithm to perform DMD is presented. Algorithms to perform LU, RR and instruction scheduling are presented in other reports <ref> [9, 10] </ref>. A portion of the high-level algorithm to implement LU, RR and DMD is contained in Figure 1. <p> The increase, however, is minimal, and is not shown here, but can be found in another report <ref> [10] </ref>. application of DMD increases the usage of registers for almost all the benchmarks. This is because the application of DMD facilitates RR in these benchmarks, which in turn enables the scheduling of high latency load instructions in parallel. <p> We measured the effect of unroll factors of 0, 1, 3 and 7 on the ILP of the loops in numerical benchmarks, the results of which are shown in Figure 5. Other results are available in the detailed technical report <ref> [10] </ref>. The cycle width is kept constant at 8, so that only the effect of changing the unroll factor is measured. Also, RR, SMD and DMD have been applied in each case. Ideally, increasing the unroll factor should increase the ILP, but that is not always the case.
Reference: [11] <author> Huang, A., Slavenburg, G., and Shen J., </author> <title> Speculative disambiguation: A Compilation Technique for Dynamic Memory Disambiguation, </title> <booktitle> Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <address> Chicago, IL, </address> <month> April </month> <year> 1994, </year> <pages> pp 200-210. </pages>
Reference-contexts: If the absolute value of the difference is greater than the data access size, then the references are not aliased. The resultant expressions are inserted as checks. Another approach by Huang proposes that the cost of dynamic disambiguation be hidden using speculative instructions and predicted execution <ref> [11] </ref>. In this paper, we evaluate the impact of DMD on ILP, when it is applied in conjunction with LU, RR and SMD. Unlike earlier approaches, we apply DMD to array references in unrolled loops only, so that benefits are maximized. <p> [3]=M [r [10]+4]; r [2]=r [2] + r [3];nop; PC=r <ref> [11] </ref>&lt;r [4]-&gt;L16;nop; Here, each iteration takes 5 cycles. This is a 20 percent performance increase over the rolled loop. Now RR and static memory disambiguation (SMD) can be applied to the loop. SMD can determine if memory references M [r [11] ] and M [r [11]+4] are aliases for the same memory location. Compile-time analysis indicates that they are not aliases. This is because symbolic comparison shows that the memory locations accessed by these two references are separated by a distance of four bytes. <p> Compile-time analysis indicates that they are not aliases. This is because symbolic comparison shows that the memory locations accessed by these two references are separated by a distance of four bytes. This indicates that scheduling the load M [r <ref> [11] </ref>+4] before the store M [r [11]] will not change the semantics of the code. Using this information, the scheduler produces the following code. L16: r [6]=M [r [11]+4];nop; M [r [11]]=r [2];nop; nop;nop; M [r [11]+4]=r [6];r [11]=r [11]+8; nop;nop; This code also requires 5 cycles per iteration of the loop. <p> M [r <ref> [11] </ref>+4]=r [6];r [11]=r [11]+8; nop;nop; This code also requires 5 cycles per iteration of the loop. Thus, there a no improvement. A closer examination indicates that the instruction schedule can be improved further if the load of M [r [10]+4] is scheduled before the store of M [r [11]] . However, that is not possible since the load of M [r [10]+4] may be an alias for the store of M [r [11]]. Since the contents of the registers r [10] and r [11] are parameters to the function enclosing the loop, the relationship between the contents of the <p> closer examination indicates that the instruction schedule can be improved further if the load of M [r [10]+4] is scheduled before the store of M [r <ref> [11] </ref>] . However, that is not possible since the load of M [r [10]+4] may be an alias for the store of M [r [11]]. Since the contents of the registers r [10] and r [11] are parameters to the function enclosing the loop, the relationship between the contents of the two registers cannot be determined by intra-procedural analysis. To resolve this problem, DMD is applied. <p> if the load of M [r [10]+4] is scheduled before the store of M [r <ref> [11] </ref>] . However, that is not possible since the load of M [r [10]+4] may be an alias for the store of M [r [11]]. Since the contents of the registers r [10] and r [11] are parameters to the function enclosing the loop, the relationship between the contents of the two registers cannot be determined by intra-procedural analysis. To resolve this problem, DMD is applied. DMD generates a new copy of the loop called the aggressive loop. <p> To resolve this problem, DMD is applied. DMD generates a new copy of the loop called the aggressive loop. In the aggressive loop, the scheduler ignores the potential aliasing between the memory references M [r [10]+4] and M [r <ref> [11] </ref>] . Consequently, the scheduler is able to place the load of M [r [10]+4] before the store of M [r [11]] . In the safe copy, the code remains the same as that after the application of SMD. <p> In the aggressive loop, the scheduler ignores the potential aliasing between the memory references M [r [10]+4] and M [r <ref> [11] </ref>] . Consequently, the scheduler is able to place the load of M [r [10]+4] before the store of M [r [11]] . In the safe copy, the code remains the same as that after the application of SMD. The compiler inserts checks to select the appropriate copy at run time. Code to select the appropriate loop to execute and the two loops are shown below. // r [11]:address of a, r
Reference: [12] <author> Kane, G., </author> <title> MIPS RISC Architecture, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1992. </year>
Reference-contexts: The compiler inserts checks to select the appropriate copy at run time. Code to select the appropriate loop to execute and the two loops are shown below. // r [11]:address of a, r [10]: address of b // r <ref> [12] </ref>: n * 4, r [4]: address of a + (n * 4) // check if a + n &lt; b r [13]=r [11]+r [12];nop; nop;nop; // check if b + n &gt;= a r [13]=r [10]+r [12]; nop;nop; // L16 begins aggressive loop L16: r [6]=M [r [11]+4];r [7]=M [r <p> below. // r [11]:address of a, r [10]: address of b // r <ref> [12] </ref>: n * 4, r [4]: address of a + (n * 4) // check if a + n &lt; b r [13]=r [11]+r [12];nop; nop;nop; // check if b + n &gt;= a r [13]=r [10]+r [12]; nop;nop; // L16 begins aggressive loop L16: r [6]=M [r [11]+4];r [7]=M [r [10]+4]; M [r [11]]=r [2];r [6]=r [6] + r [7]; PC=r [11]&lt;r [4]-&gt;L16;nop; ... // L24 begins the safe loop L24: At execution time, if the aggressive copy of the loop is executed, then 3.5 cycles per <p> The compiler was retargeted to a hypothetical VLIW machine. The compiler employs the same instruction set as that of the MIPS R4000 <ref> [12] </ref> architecture family with the instruction latencies given in Table 1. The latency of the instructions are comparable to those on current high performance superscalar/VLIW processors. In addition, the machine has unlimited supply of all functional units except the branch unit. There is only one branch unit available.
Reference: [13] <author> Kuck, D. J., R. H. Kuhn, D. A. Padua, B. Leasure, and M. Wolfe, </author> <title> Dependence Graphs and Compiler Optimizations, </title> <booktitle> Proceedings of the ACM SIGPLAN 81 Symposium on Principles of Programming Languages , Jan 1981, </booktitle> <pages> pp. 207-281. </pages>
Reference-contexts: This study also evaluates the effect of LU on instruction buffer size and register pressure within the loop for Livermore loops [15]. Register renaming is used to eliminate artificial dependencies. Kuck discusses techniques such as scalar expansion and variable renaming that can eliminate anti and output dependencies <ref> [13] </ref>. Techniques to eliminate dependencies were implemented in the Bulldog and Cydra-5 compilers [16]. Ma hlke discusses the effect on performance of renaming registers in an unrolled loop [14]. To minimize conflicts and increase ILP, all register uses in the unrolled loop are assigned unique registers.
Reference: [14] <author> Mahlke, S. A., Chen, W. Y., Gyllenhaal, J. C. and Hwu, W. W., </author> <title> Compiler Code Transformations for Superscalar-Based High-Performance Systems, </title> <booktitle> Proceedings of Supercomputing 92, </booktitle> <address> Portland, OR, </address> <month> Nov. </month> <year> 1992, </year> <pages> pp. 808-817. </pages>
Reference-contexts: As hardware mechanisms for exploiting ILP have become more prevalent, software techniques for increasing the available ILP in programs have become increasingly important. One such code improvement technique is loop unrolling (LU) [9, 18] which in conjunction with register renaming (RR) <ref> [3, 14] </ref> can increase ILP. LU replicates the original loop body multiple times, adjusts the loop termination code and eliminates redundant branch instructions. The resulting larger basic block increases the probability that the instruction scheduler can reorder instructions to exploit ILP. <p> Kuck discusses techniques such as scalar expansion and variable renaming that can eliminate anti and output dependencies [13]. Techniques to eliminate dependencies were implemented in the Bulldog and Cydra-5 compilers [16]. Ma hlke discusses the effect on performance of renaming registers in an unrolled loop <ref> [14] </ref>. To minimize conflicts and increase ILP, all register uses in the unrolled loop are assigned unique registers. In our approach, we rename registers only if it will lead to an improved instruction schedule.
Reference: [15] <author> McMohan, F. H, </author> <title> The Livermore Fortran Kernels: A Computer Test of the Numerical Performance Range, </title> <institution> Lawrence Livermore National Laboratory, Livermore, </institution> <address> CA, </address> <year> 1986. </year>
Reference-contexts: One such technique is LU. Weiss discusses LU from the perspective of automatic scheduling by the compiler [18]. This study also evaluates the effect of LU on instruction buffer size and register pressure within the loop for Livermore loops <ref> [15] </ref>. Register renaming is used to eliminate artificial dependencies. Kuck discusses techniques such as scalar expansion and variable renaming that can eliminate anti and output dependencies [13]. Techniques to eliminate dependencies were implemented in the Bulldog and Cydra-5 compilers [16].
Reference: [16] <author> Rau, B. R., Yen, D. W. L., and Towle, R. A., </author> <title> The Cydra Departmental Supercomputer, </title> <booktitle> IEEE Computer , January 1989, </booktitle> <pages> pp. 12-35. </pages>
Reference-contexts: Register renaming is used to eliminate artificial dependencies. Kuck discusses techniques such as scalar expansion and variable renaming that can eliminate anti and output dependencies [13]. Techniques to eliminate dependencies were implemented in the Bulldog and Cydra-5 compilers <ref> [16] </ref>. Ma hlke discusses the effect on performance of renaming registers in an unrolled loop [14]. To minimize conflicts and increase ILP, all register uses in the unrolled loop are assigned unique registers. In our approach, we rename registers only if it will lead to an improved instruction schedule.
Reference: [17] <author> Wall, D. W., </author> <title> Limits of Instruction-Level Parallelism, </title> <note> WRL Research Report , 93/6, </note> <institution> Digital Equipment Corporation, </institution> <address> Palo Alto, CA, </address> <year> 1993. </year>
Reference-contexts: Not constraining the number of functional units allows us to completely exploit the ILP exposed by the compiler transformations. In this study, we concentrate on the measurement of ILP. ILP is measured using the formula proposed by Wall <ref> [17] </ref>. ILP = Total latency / Total cycles We chose this measure because it gives an idea of the resources required to fully exploit the available parallelism in a piece of code. Unlike speedup, which is a relative Instruction Latency Instruction Latency Memory load 3 Single prec.
Reference: [18] <author> Weiss, S,. and Smith, J. E., </author> <title> A Study of Scalar Compilation Techniques for Pipelined Supercomputers, </title> <booktitle> Proceedings of Second International Conference on Architectural Support for Programming Languages and Operating Systems , Palo Alto, </booktitle> <address> CA, </address> <month> Oct. </month> <year> 1987, </year> <pages> pp. 105-109. </pages>
Reference-contexts: The potential to overlap execution of instructions is often referred to as instruction-level parallelism or ILP. As hardware mechanisms for exploiting ILP have become more prevalent, software techniques for increasing the available ILP in programs have become increasingly important. One such code improvement technique is loop unrolling (LU) <ref> [9, 18] </ref> which in conjunction with register renaming (RR) [3, 14] can increase ILP. LU replicates the original loop body multiple times, adjusts the loop termination code and eliminates redundant branch instructions. The resulting larger basic block increases the probability that the instruction scheduler can reorder instructions to exploit ILP. <p> One such technique is LU. Weiss discusses LU from the perspective of automatic scheduling by the compiler <ref> [18] </ref>. This study also evaluates the effect of LU on instruction buffer size and register pressure within the loop for Livermore loops [15]. Register renaming is used to eliminate artificial dependencies. Kuck discusses techniques such as scalar expansion and variable renaming that can eliminate anti and output dependencies [13].
References-found: 18

