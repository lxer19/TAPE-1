URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-425.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: thad,joshw,sandy@media.mit.edu  
Title: A Wearable Computer Based American Sign Language Recognizer  
Author: Thad Starner, Joshua Weaver, and Alex Pentland 
Address: 20 Ames Street, Cambridge MA 02139  
Affiliation: Room E15-383, The Media Laboratory Massachusetts Institute of Technology  
Abstract: Modern wearable computer designs package workstation level performance in systems small enough to be worn as clothing. These machines enable technology to be brought where it is needed the most for the handicapped: everyday mobile environments. This paper describes a research effort to make a wearable computer that can recognize (with the possible goal of translating) sentence level American Sign Language (ASL) using only a baseball cap mounted camera for input. Current accuracy exceeds 97% per word on a 40 word lexicon. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Baum. </author> <title> "An inequality and associated maximization technique in statistical estimation of probabilistic functions of Markov processes." </title> <journal> Inequalities, </journal> <volume> 3 </volume> <pages> 1-8, </pages> <year> 1972. </year>
Reference-contexts: A substantial body of literature exists on HMM technology <ref> [1, 9, 19, 30] </ref>, and tutorials on their use can be found in [9, 24]. Instead, this section will describe the issues for this application. The initial topology for an HMM can be determined by estimating how many different states are involved in specifying a sign.
Reference: [2] <author> L. Campbell, D. Becker, A. Azarbayejani, A. Bo-bick, and A. </author> <title> Pentland "Invariant features for 3-D gesture recognition," </title> <booktitle> Intl. Conf. on Face and Gesture Recogn., </booktitle> <pages> pp. 157-162, </pages> <year> 1996 </year>
Reference-contexts: Closer to the task of this paper, Wilson and Bobick [28] explore incorporating multiple representations in HMM frameworks, and Campbell et. al. <ref> [2] </ref> use a HMM-based gesture system to recognize 18 T'ai Chi gestures with 98% accuracy. 4 Tracking Hands in Video Previous systems have shown that, given some constraints, relatively detailed models of the hands can be recovered from video images [6, 20].
Reference: [3] <author> C. Charayaphan and A. Marble. </author> <title> "Image processing system for interpreting motion in American Sign Language." </title> <journal> Journal of Biomedical Engineering, </journal> <volume> 14 </volume> <pages> 419-425, </pages> <year> 1992. </year>
Reference-contexts: Research in the area can be divided into image based systems and instrumented glove systems. Tamura and Kawasaki demonstrate an early image processing system which recognizes 20 Japanese signs based on matching cheremes [27]. Charayaphan and Marble <ref> [3] </ref> demonstrate a feature set that distinguishes between the 31 isolated ASL signs in their training set (which also acts as the test set). More recently, Cui and Weng [4] have shown an image-based system with 96% accuracy on 28 isolated gestures.
Reference: [4] <author> Y. Cui and J. Weng. </author> <title> "Learning-based hand sign recognition." Intl. Work. Auto. Face Gest. </title> <booktitle> Recog. (IWAFGR) '95 Proceedings, p. </booktitle> <pages> 201-206, </pages> <year> 1995 </year>
Reference-contexts: Charayaphan and Marble [3] demonstrate a feature set that distinguishes between the 31 isolated ASL signs in their training set (which also acts as the test set). More recently, Cui and Weng <ref> [4] </ref> have shown an image-based system with 96% accuracy on 28 isolated gestures. Takahashi and Kishino [26] discuss a user dependent Dataglove-based system that recognizes 34 of the 46 Japanese kana alphabet gestures, isolated in time, using a joint angle and hand orientation coding technique.
Reference: [5] <author> T. Darrell and A. Pentland. "Space-time gestures." CVPR, p. </author> <month> 335-340, </month> <year> 1993. </year>
Reference-contexts: This experiment is sig-nificant because it uses a 25x25 pixel quantized sub-sampled camera image as a feature vector. Even with such low-level information, the model can learn the set of motions and recognize them with respectable accuracy. Darrell and Pentland <ref> [5] </ref> use dynamic time warping, a technique similar to HMM's, to match the interpolated responses of several learned image templates.
Reference: [6] <author> B. Dorner. </author> <title> "Hand shape identification and track-ing for sign language interpretation." </title> <booktitle> IJCAI Workshop on Looking at People, </booktitle> <year> 1993. </year>
Reference-contexts: In addition, these systems have mostly concentrated on finger spelling, in which the user signs each word with finger and hand positions corresponding to the letters of the alphabet <ref> [6] </ref>. However, most signing does not involve finger spelling, but instead uses gestures which represent whole words, allowing signed conversations to proceed at or above the pace of spoken conversation. <p> incorporating multiple representations in HMM frameworks, and Campbell et. al. [2] use a HMM-based gesture system to recognize 18 T'ai Chi gestures with 98% accuracy. 4 Tracking Hands in Video Previous systems have shown that, given some constraints, relatively detailed models of the hands can be recovered from video images <ref> [6, 20] </ref>. However, many of these constraints conflict with recognizing ASL in a natural context, either by requiring simple, unchanging backgrounds (unlike clothing); not allowing occlusion; requiring carefully labelled gloves; or being difficult to run in real time.
Reference: [7] <author> I. Essa, T. Darrell, and A. Pentland. </author> <title> "Tracking facial motion." </title> <booktitle> IEEE Workshop on Nonrigid and articulated Motion, </booktitle> <address> Austin TX, </address> <month> Nov. 94. </month>
Reference-contexts: For the purposes of this experiment, this aspect of ASL will be ignored. Furthermore, in ASL the eyebrows are raised for a question, relaxed for a statement, and furrowed for a directive. While we have also built systems that track facial features <ref> [7] </ref>, this source of information will not be used to aid recognition in the task addressed here. The scope of this work is not to create a user independent, full lexicon system for recognizing ASL, the system should be extensible toward this goal.
Reference: [8] <author> B. Horn. </author> <title> Robot Vision. </title> <publisher> MIT Press, </publisher> <address> NY, </address> <year> 1986. </year>
Reference-contexts: I 0 b = I 0 c = I 0 (x 0 and y 0 are the x and y coordinates normalized to the centroid) The axis of least inertia is then determined by the major axis of the bounding ellipse, which corresponds to the primary eigenvector of the matrix <ref> [8] </ref>. Note that this leaves a 180 degree ambiguity in the angle of the ellipses. To address this problem, the angles were only allowed to range from -90 to +90 degrees. When tracking skin tones, the above analysis helps to model situations of hand ambiguity implicitly.
Reference: [9] <author> X. Huang, Y. Ariki, and M. Jack. </author> <title> Hidden Markov Models for Speech Recognition. </title> <publisher> Edinburgh Univ. Press, Edinburgh, </publisher> <year> 1990. </year>
Reference-contexts: Explicit segmentation on the word level is not necessary for either training or recognition [25]. Language and context models can be applied on several different levels, and much related development of this technology has already been done by the speech recognition community <ref> [9] </ref>. Consequently, sign language recognition seems an ideal machine vision application of HMM technology, offering the benefits of problem scalability, well defined meanings, a predetermined language model, a large base of users, and immediate applications for a recognizer. <p> A substantial body of literature exists on HMM technology <ref> [1, 9, 19, 30] </ref>, and tutorials on their use can be found in [9, 24]. Instead, this section will describe the issues for this application. The initial topology for an HMM can be determined by estimating how many different states are involved in specifying a sign. <p> A substantial body of literature exists on HMM technology [1, 9, 19, 30], and tutorials on their use can be found in <ref> [9, 24] </ref>. Instead, this section will describe the issues for this application. The initial topology for an HMM can be determined by estimating how many different states are involved in specifying a sign. Fine tuning this topology can be performed empirically.
Reference: [10] <author> T. Humphries, C. Padden, and T. O'Rourke. </author> <title> A Basic Course in American Sign Language. </title> <editor> T. J. </editor> <publisher> Publ., Inc., </publisher> <address> Silver Spring, MD, </address> <year> 1980. </year>
Reference-contexts: Table 1 shows the words chosen for each class. Six personal pronouns, nine verbs, twenty nouns, and five adjectives are included making a total lexicon of forty words. The words were chosen by paging through Humphries et al. <ref> [10] </ref> and selecting those which would generate coherent sentences when chosen randomly for each part of speech. 2 Machine Sign Language Recognition Attempts at machine sign language recognition have begun to appear in the literature over the past five years.
Reference: [11] <author> B. Juang. </author> <title> "Maximum likelihood estimation for mixture multivariate observations of Markov chains." </title> <journal> AT&T Tech. J., </journal> <volume> 64 </volume> <pages> 1235-1249, </pages> <year> 1985. </year>
Reference: [12] <author> W. Kadous. </author> <title> "Recognition of Australian Sign Language using instrumented gloves." </title> <type> Bachelor's thesis, </type> <institution> University of New South Wales, </institution> <month> October </month> <year> 1995. </year>
Reference-contexts: With minimal training, the glove system discussed by Lee and Xu [13] can recognize 14 isolated finger signs using a HMM representation. Messing et. al. [16] have shown a neural net based glove system that recognizes isolated finger-spelling with 96.5% accuracy after 30 training samples. Kadous <ref> [12] </ref> describes an inexpensive glove-based system using instance-based learning which can recognize 95 discrete Auslan (Australian Sign Language) signs with 80% accuracy. However, the most encouraging work with glove-based recognizers comes from Liang and Ouhyoung's recent treatment of Taiwanese Sign language [14].
Reference: [13] <author> C. Lee and Y. Xu, </author> <title> "Online, interactive learning of gestures for human/robot interfaces." </title> <booktitle> IEEE Int. Conf. on Robotics and Automation, </booktitle> <pages> pp 2982-2987, </pages> <year> 1996. </year>
Reference-contexts: This study also demonstrates a separate 10 word gesture lexicon with user dependent accuracies up to 96% in constrained situations. With minimal training, the glove system discussed by Lee and Xu <ref> [13] </ref> can recognize 14 isolated finger signs using a HMM representation. Messing et. al. [16] have shown a neural net based glove system that recognizes isolated finger-spelling with 96.5% accuracy after 30 training samples.
Reference: [14] <author> R. Liang and M. Ouhyoung, </author> <title> "A real-time continuous gesture interface for Taiwanese Sign Language." </title> <note> Submitted to UIST, </note> <year> 1997. </year>
Reference-contexts: Kadous [12] describes an inexpensive glove-based system using instance-based learning which can recognize 95 discrete Auslan (Australian Sign Language) signs with 80% accuracy. However, the most encouraging work with glove-based recognizers comes from Liang and Ouhyoung's recent treatment of Taiwanese Sign language <ref> [14] </ref>. This HHM-based system recognizes 51 postures, 8 orientations, and 8 motion primitives.
Reference: [15] <author> S. </author> <title> Mann "Mediated reality", "MIT Media Lab, Perceptual Computing Group TR# 260" </title>
Reference-contexts: When simulating the self-contained wearable computer under development, a wireless transmission system is used to send real-time video to the SGI for processing <ref> [15] </ref>. In the first method, the subject wears distinctly colored cloth gloves on each hand (a pink glove for the right hand and a blue glove for the left). To find each hand initially, the algorithm scans the image until it finds a pixel of the appropriate color.
Reference: [16] <author> L. Messing, R. Erenshteyn, R. Foulds, S. Galuska, and G. Stern. </author> <title> "American Sign Language computer recognition: Its Present and its Promise" Conf. </title> <booktitle> the Intl. Society for Augmentative and Alternative Communication, </booktitle> <year> 1994, </year> <pages> pp. 289-291. </pages>
Reference-contexts: This study also demonstrates a separate 10 word gesture lexicon with user dependent accuracies up to 96% in constrained situations. With minimal training, the glove system discussed by Lee and Xu [13] can recognize 14 isolated finger signs using a HMM representation. Messing et. al. <ref> [16] </ref> have shown a neural net based glove system that recognizes isolated finger-spelling with 96.5% accuracy after 30 training samples. Kadous [12] describes an inexpensive glove-based system using instance-based learning which can recognize 95 discrete Auslan (Australian Sign Language) signs with 80% accuracy.
Reference: [17] <author> K. Murakami and H. </author> <title> Taguchi. "Gesture recognition using recurrent neural networks." </title> <booktitle> CHI '91 Conference Proceedings, p. </booktitle> <pages> 237-241, </pages> <year> 1991. </year>
Reference-contexts: Takahashi and Kishino [26] discuss a user dependent Dataglove-based system that recognizes 34 of the 46 Japanese kana alphabet gestures, isolated in time, using a joint angle and hand orientation coding technique. Murakami and Taguchi <ref> [17] </ref> describe a similar Dataglove system using recurrent neural networks. However, in this experiment a 42 static-pose finger alphabet is used, and the system achieves up to 98% recognition for trainers of the system and 77% for users not in the training set.
Reference: [18] <author> H. Poizner, U. Bellugi, and V. Lutes-Driscoll. </author> <title> "Perception of American Sign Language in dynamic point-light displays." </title> <editor> J. Exp. Pyschol.: </editor> <booktitle> Human Perform., </booktitle> <volume> 7 </volume> <pages> 430-440, </pages> <year> 1981. </year>
Reference-contexts: The hand tracking stage of the system does not attempt a fine description of hand shape; studies of human sign readers have shown that such detailed information is not necessary for humans to interpret sign language <ref> [18, 22] </ref>. Instead, the tracking process produces only a coarse description of hand shape, orientation, and trajectory. The hands are tracked by their color: in the first experiment via solidly colored gloves and in the second, via their natural skin tone. <p> Processing proceeds normally except for simple rules to handle hand and nose ambiguity described in the next section. 5 Feature Extraction and Hand Ambi guity Psychophysical studies of human sign readers have shown that detailed information about hand shape is not necessary for humans to interpret sign language <ref> [18, 22] </ref>. Consequently, we began by considering only very simple hand shape features, and evolved a more complete feature set as testing progressed [25].
Reference: [19] <author> L. Rabiner and B. Juang. </author> <title> "An introduction to hidden Markov models." </title> <journal> IEEE ASSP Magazine, </journal> <volume> p. </volume> <pages> 4-16, </pages> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: A substantial body of literature exists on HMM technology <ref> [1, 9, 19, 30] </ref>, and tutorials on their use can be found in [9, 24]. Instead, this section will describe the issues for this application. The initial topology for an HMM can be determined by estimating how many different states are involved in specifying a sign.
Reference: [20] <author> J. Rehg and T. Kanade. "DigitEyes: </author> <title> vision-based human hand tracking." </title> <institution> School of Computer Science Technical Report CMU-CS-93-220, Carnegie Mellon Univ., </institution> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: incorporating multiple representations in HMM frameworks, and Campbell et. al. [2] use a HMM-based gesture system to recognize 18 T'ai Chi gestures with 98% accuracy. 4 Tracking Hands in Video Previous systems have shown that, given some constraints, relatively detailed models of the hands can be recovered from video images <ref> [6, 20] </ref>. However, many of these constraints conflict with recognizing ASL in a natural context, either by requiring simple, unchanging backgrounds (unlike clothing); not allowing occlusion; requiring carefully labelled gloves; or being difficult to run in real time.
Reference: [21] <author> J. Schlenzig, E. Hunter, and R. Jain. </author> <title> "Recursive identification of gesture inputers using hidden Markov models." </title> <booktitle> Proc. Second Ann. Conf. on Appl. of Comp. Vision, p. </booktitle> <pages> 187-194, </pages> <year> 1994. </year>
Reference-contexts: Even with such low-level information, the model can learn the set of motions and recognize them with respectable accuracy. Darrell and Pentland [5] use dynamic time warping, a technique similar to HMM's, to match the interpolated responses of several learned image templates. Schlenzig et al. <ref> [21] </ref> use hidden Markov models to recognize "hello," "good-bye," and "rotate." While Baum-Welch re-estimation was not implemented, this study shows the continuous gesture recognition capabilities of HMM's by recognizing gesture sequences.
Reference: [22] <author> G. Sperling, M. Landy, Y. Cohen, and M. Pavel. </author> <title> "Intelligible encoding of ASL image sequences at extremely low information rates." </title> <journal> Comp. Vision, Graphics, and Image Proc., </journal> <volume> 31 </volume> <pages> 335-391, </pages> <year> 1985. </year>
Reference-contexts: The hand tracking stage of the system does not attempt a fine description of hand shape; studies of human sign readers have shown that such detailed information is not necessary for humans to interpret sign language <ref> [18, 22] </ref>. Instead, the tracking process produces only a coarse description of hand shape, orientation, and trajectory. The hands are tracked by their color: in the first experiment via solidly colored gloves and in the second, via their natural skin tone. <p> ASL uses approximately 6000 gestures for common words and communicates obscure words or proper nouns through finger spelling. Conversants in ASL may describe a person, place, or thing and then point to a place in space to store that object temporarily for later reference <ref> [22] </ref>. For the purposes of this experiment, this aspect of ASL will be ignored. Furthermore, in ASL the eyebrows are raised for a question, relaxed for a statement, and furrowed for a directive. <p> Processing proceeds normally except for simple rules to handle hand and nose ambiguity described in the next section. 5 Feature Extraction and Hand Ambi guity Psychophysical studies of human sign readers have shown that detailed information about hand shape is not necessary for humans to interpret sign language <ref> [18, 22] </ref>. Consequently, we began by considering only very simple hand shape features, and evolved a more complete feature set as testing progressed [25].
Reference: [23] <author> T. Starner and A. Pentland. </author> <title> "Real-Time Ameer-ican Sign Language Recognition from Video Using Hidden Markov Models." </title> <institution> MIT Media Laboratory, Perceptual Computing Group TR#375, </institution> <note> Presented at ISCV'95. </note>
Reference-contexts: To date, most work on sign language recognition has employed expensive "datagloves" which tether the user to a stationary machine [26] or computer vision systems limited to a calibrated area <ref> [23] </ref>. In addition, these systems have mostly concentrated on finger spelling, in which the user signs each word with finger and hand positions corresponding to the letters of the alphabet [6]. <p> As such, context modeling would tend to suppress this sentence in recognition, perhaps preferring "they like food yellow they," except when the evidence is particularly strong for the previous hypothesis. Unlike our previous study <ref> [23] </ref> with desk mounted camera, there was little confusion between the signs "pack," "car," and "gray." These signs have very similar motions and are generally distinguished by finger position. <p> However, the error rates for the strong grammar cases are almost identical. This result was unexpected since, in previous experiments with desktop mounted camera systems <ref> [23] </ref>, gloveless experiments had significantly lower accuracies. The reason for this difference may be in the amount of ambiguity caused by the user's face in the previous experiments whereas, with the cap mounted system, the nose provided little problems.
Reference: [24] <author> T. Starner. </author> <title> "Visual Recognition of American Sign Language Using Hidden Markov Models." </title> <type> Master's thesis, </type> <institution> MIT Media Laboratory, </institution> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: A substantial body of literature exists on HMM technology [1, 9, 19, 30], and tutorials on their use can be found in <ref> [9, 24] </ref>. Instead, this section will describe the issues for this application. The initial topology for an HMM can be determined by estimating how many different states are involved in specifying a sign. Fine tuning this topology can be performed empirically.
Reference: [25] <author> T. Starner, J. Makhoul, R. Schwartz, and G. Chou. </author> <title> "On-line cursive handwriting recognition using speech recognition methods." </title> <booktitle> ICASSP, </booktitle> <address> V-125, </address> <year> 1994. </year>
Reference-contexts: Hidden Markov models have intrinsic properties which make them very attractive for sign language recognition. Explicit segmentation on the word level is not necessary for either training or recognition <ref> [25] </ref>. Language and context models can be applied on several different levels, and much related development of this technology has already been done by the speech recognition community [9]. <p> Consequently, we began by considering only very simple hand shape features, and evolved a more complete feature set as testing progressed <ref> [25] </ref>. Since finger spelling is not allowed and there are few ambiguities in the test vocabulary based on individual finger motion, a relatively coarse tracking system may be used. <p> For example, if two signs are often seen together, recognizing the two signs as one group may be beneficial. Such groupings of 2 or 3 units together for recognition has been shown to halve error rates in speech and handwriting recognition <ref> [25] </ref>. A final use of context is on the inter-word (when recognizing single character signs) or phrase level (when recognizing word signs). Statistical grammars relating the probability of the co-occurrence of two or more words can be used to weight the recognition process. <p> Statistical grammars relating the probability of the co-occurrence of two or more words can be used to weight the recognition process. In handwriting, where the units are letters, words, and sentences, a statistical grammar can quarter error rates <ref> [25] </ref>. In the absence of enough data to form a statistical grammar, rule-based grammars can effectively reduce error rates. 7 Experimentation Since we could not exactly recreate the signing conditions between the first and second experiments, direct comparison of the gloved and no-glove experiments is impossible.
Reference: [26] <author> T. Takahashi and F. Kishino. </author> <title> "Hand gesture coding based on experiments using a hand gesture interface device." </title> <journal> SIGCHI Bul., </journal> <volume> 23(2) </volume> <pages> 67-73, </pages> <year> 1991. </year>
Reference-contexts: In sign language, where each gesture already has assigned meaning, strong rules of context and grammar may be applied to make recognition tractable. To date, most work on sign language recognition has employed expensive "datagloves" which tether the user to a stationary machine <ref> [26] </ref> or computer vision systems limited to a calibrated area [23]. In addition, these systems have mostly concentrated on finger spelling, in which the user signs each word with finger and hand positions corresponding to the letters of the alphabet [6]. <p> Charayaphan and Marble [3] demonstrate a feature set that distinguishes between the 31 isolated ASL signs in their training set (which also acts as the test set). More recently, Cui and Weng [4] have shown an image-based system with 96% accuracy on 28 isolated gestures. Takahashi and Kishino <ref> [26] </ref> discuss a user dependent Dataglove-based system that recognizes 34 of the 46 Japanese kana alphabet gestures, isolated in time, using a joint angle and hand orientation coding technique. Murakami and Taguchi [17] describe a similar Dataglove system using recurrent neural networks.
Reference: [27] <author> S. Tamura and S. Kawasaki. </author> <title> "Recognition of sign language motion images." </title> <journal> Pattern Recognition, </journal> <volume> 21 </volume> <pages> 343-353, </pages> <year> 1988. </year>
Reference-contexts: Research in the area can be divided into image based systems and instrumented glove systems. Tamura and Kawasaki demonstrate an early image processing system which recognizes 20 Japanese signs based on matching cheremes <ref> [27] </ref>. Charayaphan and Marble [3] demonstrate a feature set that distinguishes between the 31 isolated ASL signs in their training set (which also acts as the test set). More recently, Cui and Weng [4] have shown an image-based system with 96% accuracy on 28 isolated gestures.
Reference: [28] <author> A. Wilson and A. Bobick. </author> <title> "Learning visual behavior for gesture analysis." </title> <booktitle> Proc. IEEE Int'l. Symp. on Comp. Vis., </booktitle> <month> Nov. </month> <year> 1995. </year>
Reference-contexts: Schlenzig et al. [21] use hidden Markov models to recognize "hello," "good-bye," and "rotate." While Baum-Welch re-estimation was not implemented, this study shows the continuous gesture recognition capabilities of HMM's by recognizing gesture sequences. Closer to the task of this paper, Wilson and Bobick <ref> [28] </ref> explore incorporating multiple representations in HMM frameworks, and Campbell et. al. [2] use a HMM-based gesture system to recognize 18 T'ai Chi gestures with 98% accuracy. 4 Tracking Hands in Video Previous systems have shown that, given some constraints, relatively detailed models of the hands can be recovered from video
Reference: [29] <author> J. Yamato, J. Ohya, and K. Ishii. </author> <title> "Recognizing human action in time-sequential images using hidden Markov models." </title> <booktitle> Proc. 1992 ICCV, p. </booktitle> <pages> 379-385. </pages> <publisher> IEEE Press, </publisher> <year> 1992. </year>
Reference-contexts: An early effort by Yamato et al. <ref> [29] </ref> uses discrete HMM's to recognize image sequences of six different tennis strokes among three subjects. This experiment is sig-nificant because it uses a 25x25 pixel quantized sub-sampled camera image as a feature vector.
Reference: [30] <author> S. Young. </author> <title> HTK: Hidden Markov Model Toolkit V1.5. </title> <institution> Cambridge Univ. Eng. Dept. Speech Group and Entropic Research Lab. Inc., </institution> <address> Washington DC, </address> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: A substantial body of literature exists on HMM technology <ref> [1, 9, 19, 30] </ref>, and tutorials on their use can be found in [9, 24]. Instead, this section will describe the issues for this application. The initial topology for an HMM can be determined by estimating how many different states are involved in specifying a sign. <p> Embedded training goes on step further and trains the models in situ allowing model boundaries to shift through a probabilistic entry into the initial states of each model <ref> [30] </ref>. Again, the process is automated. In this manner, a more realistic model can be made of the onset and offset of a particular sign in a natural context. Generally, a sign can be affected by both the sign in front of it and the sign behind it.
References-found: 30

