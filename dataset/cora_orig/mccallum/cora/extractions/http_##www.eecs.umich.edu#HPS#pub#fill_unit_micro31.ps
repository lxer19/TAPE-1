URL: http://www.eecs.umich.edu/HPS/pub/fill_unit_micro31.ps
Refering-URL: http://www.eecs.umich.edu/HPS/hps_tracecache.html
Root-URL: http://www.cs.umich.edu
Email: fites, sanjayp, pattg@eecs.umich.edu  
Title: Putting the Fill Unit to Work: Dynamic Optimizations for Trace Cache Microprocessors  
Author: Daniel Holmes Friendly Sanjay Jeram Patel Yale N. Patt 
Address: Ann Arbor, Michigan 48109-2122  
Affiliation: Department of Electrical Engineering and Computer Science The University of Michigan  
Abstract: The fill unit is the structure which collects blocks of instructions and combines them into multi-block segments for storage in a trace cache. In this paper, we expand the role of the fill unit to include four dynamic optimizations: (1) Register move instructions are explicitly marked, enabling them to be executed within the decode logic. (2) Immediate values of dependent instructions are combined, if possible, which removes a step in the dependency chain. (3) Dependent pairs of shift and add instructions are combined into scaled add instructions. (4) Instructions are arranged within the trace segment to minimize the impact of the latency through the operand bypass network. Together, these dynamic trace optimizations improve performance on the SPECint95 benchmarks by more than 17% and over all the benchmarks studied by slightly more than 18%. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Burger, T. Austin, and S. Bennett. </author> <title> Evaluating future microprocessors: The SimpleScalar tool set. </title> <type> Technical Report 1308, </type> <institution> University of Wisconsin - Madison Technical Report, </institution> <month> July </month> <year> 1996. </year>
Reference-contexts: The simulator was implemented using the SimpleScalar 2.0 tool suite <ref> [1] </ref>. The SimpleScalar instruction set is a superset of the MIPS-IV ISA [13], modified such that architected delay slots have been removed and that indexed (register plus register) memory operations have been added. In the execution model, all instructions undergo four stages of processing before retirement: fetch, issue, schedule, execute.
Reference: [2] <author> D. W. Clift, J. M. Arnold, R. P. Colwell, and A. F. Glew. </author> <title> Floating point register alias table FXCH and retirement floating point register array. </title> <type> U.S. Patent Number 5,466,352, </type> <year> 1996. </year>
Reference-contexts: As instructions retire, the associated count is decremented. When the count reaches zero, the physical register is returned to the free list. A similar mechanism, which allows the x86 instruc tion FXCH to execute in zero cycles, has been patented by Intel <ref> [2] </ref>. move instructions by the rename logic. A partial snapshot of the speculative state after each instruction is renamed is shown for both checkpoint repair and a reorder buffer. In order to simplify the diagram, only the relevant fields of each RAT and reorder buffer entry are shown.
Reference: [3] <author> M. Franklin and M. Smotherman. </author> <title> A fill-unit approach to multiple instruction issue. </title> <booktitle> In Proceedings of the 27th Annual ACM/IEEE International Symposium on Microarchi-tecture, </booktitle> <pages> pages 162171, </pages> <year> 1994. </year>
Reference-contexts: The concept was demonstrated by Rotenberg et al. [14] to be a low latency fetch device and developed by Patel et al. [11, 10] to be a very high bandwidth device. Franklin and Smotherman <ref> [3] </ref> as well as Nair and Hop-kins [9] have explored the run-time manipulation of the code stream by the fill unit. In both cases the fill unit is used to dynamically retarget a scalar instruction stream into pre-scheduled instruction groups for a statically-scheduled execution engine.
Reference: [4] <author> D. H. Friendly, S. J. Patel, and Y. N. Patt. </author> <title> Alternative fetch and issue techniques from the trace cache fetch mechanism. </title> <booktitle> In Proceedings of the 30th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <year> 1997. </year>
Reference-contexts: The trace cache has been developed for high bandwidth instruction delivery. It has been demonstrated as an effective, low latency technique for delivering instructions to very wide issue machines <ref> [14, 4] </ref>. By placing logically contiguous instructions in physically contiguous storage, the trace cache is able to deliver multiple blocks of instructions in the same cycle without support from a compiler and without modifying the instruction set. <p> The logic needed to prepare instructions for issue can be put in the trace cache fill pipeline. Our earlier work has shown that the latency of the fill pipeline has a negligible performance impact <ref> [4] </ref>. The techniques we present in this paper exploit the latency-tolerant nature of the fill pipeline by performing trace transformations within the major logic structure of the pipeline the fill unit. <p> Misses in the second level cache take 50 cycles to be fetched from memory if there is no bus contention. The baseline configuration uses inactive issue <ref> [4] </ref>. With inactive issue, all blocks within a trace cache line are issued into the processor whether or not they match the predicted path. The blocks that do not match the prediction are issued inactively.
Reference: [5] <author> W. W. Hwu and Y. N. Patt. </author> <title> Checkpoint repair for out-of-order execution machines. </title> <booktitle> In Proceedings of the 14th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 1826, </pages> <year> 1987. </year>
Reference-contexts: Instructions are dispatched for execution from a 32-entry reservation station associated with each functional unit. A 64KB, 4-way set associative, L1 data cache is used for data supply. It has a load latency of one cycle after the address generation is complete. The model uses checkpoint repair <ref> [5] </ref> to recover from branch mispredictions and exceptions. The execution engine is capable of creating up to three checkpoints each cycle, one for each block supplied. The memory scheduler waits for addresses to be generated before scheduling memory operations. No memory operation can bypass a store with an unknown address.
Reference: [6] <author> Intel Corporation. </author> <title> Pentium Processor User's Manual Volume 1: Pentium Processor Data Book, </title> <year> 1993. </year>
Reference-contexts: Although reassociation requires no changes to the execution hardware, the scaled add instruction does. The ALU must be able to shift an input operand by a variable amount before the addition begins. As some current architectures support scaled adds and scaled loads <ref> [16, 6] </ref> and current implementations of them handle the operation in a single cycle, we expect that this additional logic will not affect the critical path of the processor. To ensure this, we have limited the size of the shift to be no more than 3 bits.
Reference: [7] <author> J. D. Johnson. </author> <title> Expansion caches for superscalar microprocessors. </title> <type> Technical Report CSL-TR-94-630, </type> <institution> Stanford University, </institution> <address> Palo Alto CA, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: Its initial incarnations were developed by Melvin and Patt [8], Peleg and Weiser [12], and Johnson <ref> [7] </ref>. The concept was demonstrated by Rotenberg et al. [14] to be a low latency fetch device and developed by Patel et al. [11, 10] to be a very high bandwidth device.
Reference: [8] <author> S. W. Melvin and Y. N. Patt. </author> <title> Performance benefits of large execution atomic units in dynamically scheduled machines. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <pages> pages 427 432, </pages> <year> 1989. </year>
Reference-contexts: Its initial incarnations were developed by Melvin and Patt <ref> [8] </ref>, Peleg and Weiser [12], and Johnson [7]. The concept was demonstrated by Rotenberg et al. [14] to be a low latency fetch device and developed by Patel et al. [11, 10] to be a very high bandwidth device.
Reference: [9] <author> R. Nair and M. E. Hopkins. </author> <title> Exploiting instruction level parallelism in processors by caching scheduled groups. </title> <booktitle> In Proceedings of the 24th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 1325, </pages> <year> 1997. </year>
Reference-contexts: The techniques we present in this paper exploit the latency-tolerant nature of the fill pipeline by performing trace transformations within the major logic structure of the pipeline the fill unit. Generally speaking, the transformations can perform a wide variety of tasks: dynamic retarget-ing of an ISA <ref> [9] </ref>, pre-analysis of the dependencies within a trace [19], and dynamic predication of hard-to-predict short forward branches are some examples. Additionally the fill unit provides a strong framework for the dynamic tuning of code sequences. It is this class of transformations we will examine in this paper. <p> The concept was demonstrated by Rotenberg et al. [14] to be a low latency fetch device and developed by Patel et al. [11, 10] to be a very high bandwidth device. Franklin and Smotherman [3] as well as Nair and Hop-kins <ref> [9] </ref> have explored the run-time manipulation of the code stream by the fill unit. In both cases the fill unit is used to dynamically retarget a scalar instruction stream into pre-scheduled instruction groups for a statically-scheduled execution engine.
Reference: [10] <author> S. J. Patel, M. Evers, and Y. N. Patt. </author> <title> Improving trace cache effectiveness with branch promotion and trace packing. </title> <booktitle> In Proceedings of the 25th Annual International Symposium on Computer Architecture, </booktitle> <year> 1998. </year>
Reference-contexts: Its initial incarnations were developed by Melvin and Patt [8], Peleg and Weiser [12], and Johnson [7]. The concept was demonstrated by Rotenberg et al. [14] to be a low latency fetch device and developed by Patel et al. <ref> [11, 10] </ref> to be a very high bandwidth device. Franklin and Smotherman [3] as well as Nair and Hop-kins [9] have explored the run-time manipulation of the code stream by the fill unit. <p> If the prediction was incorrect, the processor has already fetched, issued and possibly executed some instructions along the correct path. Furthermore, the trace cache implements both branch promotion and trace packing <ref> [10] </ref>. Branch promotion dynamically identifies conditional branches which are strongly biased. These branches are then promoted to receive a static prediction. The bias threshold was set to 64 consecutive occurrences. <p> As mentioned in the previous section, each trace can contain up to 16 instructions and include 3 conditional branches. The baseline fill unit also performs branch promotion, as described in <ref> [10] </ref>. Furthermore, the fill unit explicitly marks dependency information within the traces it constructs. Doing so simplifies the dependency checking required when the trace is later fetched from the trace cache. Schemes for explicitly recording dependency information within a group of instructions have been proposed before [17, 19].
Reference: [11] <author> S. J. Patel, D. H. Friendly, and Y. N. Patt. </author> <title> Critical issues regarding the trace cache fetch mechanism. </title> <type> Technical Report CSE-TR-335-97, </type> <institution> University of Michigan Technical Report, </institution> <month> May </month> <year> 1997. </year>
Reference-contexts: Its initial incarnations were developed by Melvin and Patt [8], Peleg and Weiser [12], and Johnson [7]. The concept was demonstrated by Rotenberg et al. [14] to be a low latency fetch device and developed by Patel et al. <ref> [11, 10] </ref> to be a very high bandwidth device. Franklin and Smotherman [3] as well as Nair and Hop-kins [9] have explored the run-time manipulation of the code stream by the fill unit.
Reference: [12] <author> A. Peleg and U. Weiser. </author> <title> Dynamic flow instruction cache memory organized around trace segments independant of virtual address line. </title> <type> U.S. Patent Number 5,381,533, </type> <year> 1994. </year>
Reference-contexts: Its initial incarnations were developed by Melvin and Patt [8], Peleg and Weiser <ref> [12] </ref>, and Johnson [7]. The concept was demonstrated by Rotenberg et al. [14] to be a low latency fetch device and developed by Patel et al. [11, 10] to be a very high bandwidth device.
Reference: [13] <author> C. Price. </author> <title> MIPS IV Instruction Set, revision 3.1. MIPS Technologies, </title> <publisher> Inc., </publisher> <address> Mountain View, CA, </address> <year> 1995. </year>
Reference-contexts: We present four types of dynamic code tuning techniques. The first is a technique that marks instructions which move a value from one register to another register as explicit move instructions. These can be completely handled by the register renaming logic. Many ISAs, such as the MIPS <ref> [13] </ref> and Alpha [16] architectures, do not support an explicit register-to-register move instruction so instructions which pass an input operand unchanged to the destination, ADD Rx Ry + 0 for example, are used by compilers to perform the move. <p> The simulator was implemented using the SimpleScalar 2.0 tool suite [1]. The SimpleScalar instruction set is a superset of the MIPS-IV ISA <ref> [13] </ref>, modified such that architected delay slots have been removed and that indexed (register plus register) memory operations have been added. In the execution model, all instructions undergo four stages of processing before retirement: fetch, issue, schedule, execute. All stages take at least one cycle.
Reference: [14] <author> E. Rotenberg, S. Bennett, and J. E. Smith. </author> <title> Trace cache: a low latency approach to high bandwidth instruction fetching. </title> <booktitle> In Proceedings of the 29th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <year> 1996. </year>
Reference-contexts: The trace cache has been developed for high bandwidth instruction delivery. It has been demonstrated as an effective, low latency technique for delivering instructions to very wide issue machines <ref> [14, 4] </ref>. By placing logically contiguous instructions in physically contiguous storage, the trace cache is able to deliver multiple blocks of instructions in the same cycle without support from a compiler and without modifying the instruction set. <p> Its initial incarnations were developed by Melvin and Patt [8], Peleg and Weiser [12], and Johnson [7]. The concept was demonstrated by Rotenberg et al. <ref> [14] </ref> to be a low latency fetch device and developed by Patel et al. [11, 10] to be a very high bandwidth device. Franklin and Smotherman [3] as well as Nair and Hop-kins [9] have explored the run-time manipulation of the code stream by the fill unit.
Reference: [15] <author> Y. Sazeides, S. Vassiliadis, and J. E. Smith. </author> <title> The performance potential of data dependence speculation and collapsing. </title> <booktitle> In Proceedings of the 29th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <year> 1996. </year>
Reference-contexts: As the fill unit allows the creation of segments across function calls, reassociation can also be applied across procedure boundaries. The third technique creates scaled add instructions. It is an application of dependence collapsing <ref> [15] </ref> using the fill unit as the dynamic mechanism to perform the collapsing. Many add instructions are directly dependent on shift instructions where the shift is a short distance immediate shift operation. Such shift-add combinations are commonly used when accessing arrays of data items. <p> In both cases the fill unit is used to dynamically retarget a scalar instruction stream into pre-scheduled instruction groups for a statically-scheduled execution engine. The creation of the scaled add instructions is an instance of a concept called instruction collapsing explored by Vas-siliadis et al [20]. Sazeides et al <ref> [15] </ref> looked into the performance potential of generalized instruction collapsing and the frequency of occurrence of certain instruction groups. 3 Experimental model A pipeline simulator that allows the modeling of wrong path effects was used for this study. The simulator was implemented using the SimpleScalar 2.0 tool suite [1]. <p> For instance, to calculate the memory address of an entry in an array of 32-bit integers on a byte addressable machine, it suffices to shift the integer array index by 2 bits and add it to the base address of the array. In their work, Sazeides et al <ref> [15] </ref> presented results showing that approximately 5% of the instruction stream was composed of this type of pairing for the SPARC v.8 architecture. Our results with the SimpleScalar architecture are very similar. Although reassociation requires no changes to the execution hardware, the scaled add instruction does.
Reference: [16] <author> R. L. </author> <title> Sites. Alpha Architecture Reference Manual. </title> <publisher> Digital Press, </publisher> <address> Burlington, MA, </address> <year> 1992. </year>
Reference-contexts: The first is a technique that marks instructions which move a value from one register to another register as explicit move instructions. These can be completely handled by the register renaming logic. Many ISAs, such as the MIPS [13] and Alpha <ref> [16] </ref> architectures, do not support an explicit register-to-register move instruction so instructions which pass an input operand unchanged to the destination, ADD Rx Ry + 0 for example, are used by compilers to perform the move. <p> Although reassociation requires no changes to the execution hardware, the scaled add instruction does. The ALU must be able to shift an input operand by a variable amount before the addition begins. As some current architectures support scaled adds and scaled loads <ref> [16, 6] </ref> and current implementations of them handle the operation in a single cycle, we expect that this additional logic will not affect the critical path of the processor. To ensure this, we have limited the size of the shift to be no more than 3 bits.
Reference: [17] <author> E. Sprangle and Y. Patt. </author> <title> Facilitating superscalar processing via a combined static/dynamic register renaming scheme. </title> <booktitle> In Proceedings of the 27th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pages 143147, </pages> <year> 1994. </year>
Reference-contexts: Furthermore, the fill unit explicitly marks dependency information within the traces it constructs. Doing so simplifies the dependency checking required when the trace is later fetched from the trace cache. Schemes for explicitly recording dependency information within a group of instructions have been proposed before <ref> [17, 19] </ref>. In our scheme, we record dependencies using an extra 7 bits per instruction. 3 bits are added to an instruction's destination to identify whether the destination is live-out of its checkpoint.
Reference: [18] <author> J. Stark, P. Racunas, and Y. N. Patt. </author> <title> Reducing the performance impact of instruction cache misses by writing instructions into the reservation stations out-of-order. </title> <booktitle> In Proceedings of the 30th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pages 34 43, </pages> <year> 1997. </year>
Reference-contexts: The memory scheduler waits for addresses to be generated before scheduling memory operations. No memory operation can bypass a store with an unknown address. All experiments were performed on the SPECint95 benchmarks and on several common UNIX applications <ref> [18] </ref>. The benchmark executables were compiled using gcc version 2.6.3 with -O3 optimizations. Table 1 lists the number of instructions simulated and the input set, if the input was derived from a standard input set 1 .
Reference: [19] <author> S. Vajapeyam and T. Mitra. </author> <title> Improving superscalar instruction dispatch and issue by exploiting dynamic code sequences. </title> <booktitle> In Proceedings of the 24th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 112, </pages> <year> 1997. </year>
Reference-contexts: Generally speaking, the transformations can perform a wide variety of tasks: dynamic retarget-ing of an ISA [9], pre-analysis of the dependencies within a trace <ref> [19] </ref>, and dynamic predication of hard-to-predict short forward branches are some examples. Additionally the fill unit provides a strong framework for the dynamic tuning of code sequences. It is this class of transformations we will examine in this paper. <p> Furthermore, the fill unit explicitly marks dependency information within the traces it constructs. Doing so simplifies the dependency checking required when the trace is later fetched from the trace cache. Schemes for explicitly recording dependency information within a group of instructions have been proposed before <ref> [17, 19] </ref>. In our scheme, we record dependencies using an extra 7 bits per instruction. 3 bits are added to an instruction's destination to identify whether the destination is live-out of its checkpoint.
Reference: [20] <author> S. Vassiliadis, B. Blaner, and R. J. Eickemeyer. </author> <title> Scism:a scalable compound instruction set machine. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 38:5978, </volume> <year> 1994. </year>
Reference-contexts: In both cases the fill unit is used to dynamically retarget a scalar instruction stream into pre-scheduled instruction groups for a statically-scheduled execution engine. The creation of the scaled add instructions is an instance of a concept called instruction collapsing explored by Vas-siliadis et al <ref> [20] </ref>. Sazeides et al [15] looked into the performance potential of generalized instruction collapsing and the frequency of occurrence of certain instruction groups. 3 Experimental model A pipeline simulator that allows the modeling of wrong path effects was used for this study.
References-found: 20

