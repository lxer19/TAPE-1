URL: http://www.neci.nj.nec.com/homepages/omlin/papers/nc.paper.ps.gz
Refering-URL: http://www.neci.nj.nec.com/homepages/omlin/
Root-URL: http://www.neci.nj.nec.com
Email: E-mail: omlin@cs.sun.ac.za giles@research.nj.nec.com  
Title: Recurrent Neural Networks Learn Deterministic Representations of Fuzzy Finite-State Automata  
Author: Christian W. Omlin a C. Lee Giles b;c 
Keyword: Recurrent neural networks, fuzzy knowledge extraction, automata, languages, nonlinear sys tems.  
Address: Princeton, NJ 08540 USA  College Park, MD 20742 USA  
Affiliation: a Department of Computer Science, University of Stellenbosch 7600 Stellenbosch, SOUTH AFRICA b NEC Research Institute,  c UMIACS, U. of Maryland,  
Abstract: The paradigm of deterministic finite-state automata (DFAs) and their corresponding regular languages have been shown to be very useful for addressing fundamental issues in recurrent neural networks. The issues that have been addressed include knowledge representation, extraction, and refinement as well development of advanced learning algorithms. Recurrent neural networks are also very promising tool for modeling discrete dynamical systems through learning, particularly when partial prior knowledge is available. The drawback of the DFA paradigm is that it is inappropriate for modeling vague or uncertain dynamics; however, many real-world applications deal with vague or uncertain information. One way to model vague information in a dynamical system is to allow for vague state transitions, i.e. the system may be in several states at the same time with varying degree of certainty; fuzzy finite-state automata (FFAs) are a formal equivalent of such systems. It is therefore of interest to study how uncertainty in the form of FFAs can be modeled by deterministic recurrent neural networks. We have previously proven that second-order recurrent neural networks are able to represent FFAs, i.e. recurrent networks can be constructed that assign fuzzy memberships to input strings with arbitrary accuracy. In such networks, the classification performance is independent of the string length. In this paper, we are concerned with recurrent neural networks that have been trained to behave like FFAs. In particular, we are interested in the internal representation of fuzzy states and state transitions and in the extraction of knowledge in symbolic form. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Alon, A. Dewdney, and T. Ott, </author> <title> "Efficient simulation of finite automata by neural nets," </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> vol. 38, no. 2, </volume> <pages> pp. 495-514, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Definition 2.1 A fuzzy finite-state automaton (FFA) f M is a 6-tuple f M =&lt; ; Q; Z; e R; ffi; ! &gt; where and Q are the same as in DFAs; Z is a finite output alphabet, e R is the fuzzy initial state, ffi : fi Q fi <ref> [0; 1] </ref> ! Q is the fuzzy transition map and ! : Q ! Z is the output map. <p> result is the basis for mapping FFAs into deterministic recurrent neural networks [39]: Theorem 2.2 Given a regular fuzzy automaton f M , there exists a deterministic finite-state automaton M with output alphabet Z f : is a production weightg [ f0g which computes the membership function : fl ! <ref> [0; 1] </ref> of the language L ( f M ). An example of FFA-to-DFA transformation is shown in Figure 1b 3 . <p> Corollary 2.1 Given a regular fuzzy grammar e G, there exists an equivalent grammar G in which productions have the form A 1:0 3 Network Architecture for Fuzzy Automata Theorem 2.2 enables us to transform any FFA into a deterministic automaton which computes the same membership function : fl ! <ref> [0; 1] </ref>. Various methods have been proposed for implementing deterministic automata in recurrent neural networks [1, 2, 15, 14, 20, 26, 31, 28]. <p> Various methods have been proposed for implementing deterministic automata in recurrent neural networks <ref> [1, 2, 15, 14, 20, 26, 31, 28] </ref>. <p> In particular, the following theorem developed for DFA extraction also applies to FFAs [5]: Theorem 5.1 The state space of a recurrent network modeling a given FFA must have mutually disjoint, closed sets Q i in <ref> [0; 1] </ref> N . The sets Q i correspond to states q i of the deterministic acceptor of some FFA. <p> The representation used the model of equivalent deterministic acceptors to encode FFAs. In this paper, we have empirically demonstrated that such networks also learn such representations, i.e. networks trained on strings with fuzzy membership 2 <ref> [0; 1] </ref> also represent the generating automata in the form of deterministic acceptors.
Reference: [2] <author> R. Alquezar and A. Sanfeliu, </author> <title> "An algebraic framework to represent finite state machines in single-layer recurrent neural networks," </title> <journal> Neural Computation, </journal> <volume> vol. 7, no. 5, </volume> <editor> p. </editor> <volume> 931, </volume> <year> 1995. </year>
Reference-contexts: Various methods have been proposed for implementing deterministic automata in recurrent neural networks <ref> [1, 2, 15, 14, 20, 26, 31, 28] </ref>.
Reference: [3] <author> Y. Bengio, P. Simard, and P. Frasconi, </author> <title> "Learning long-term dependencies with gradient descent is difficult," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 5, </volume> <pages> pp. 157-166, </pages> <year> 1994. </year> <title> Special Issue on Recurrent Neural Networks. </title>
Reference-contexts: Empirical and theoretical evidence suggests that the order in which strings are presented as input during the learning phase can have a profound impact on a network's training performance <ref> [3, 27] </ref>. The order of positive and negative training examples and the length of the training strings are important factors. <p> Long example strings represent long-term dependencies; because the gradient information vanishes for long strings, learning becomes increasingly inefficient as the temporal span of the dependencies increases <ref> [3] </ref>. Our strategy is to learn in cycles, where each cycle consists of three distinct phases: (1) Train the network on a small subset of the training data set (`working set'); this initial data set will contain the shortest strings only.
Reference: [4] <author> J. Bezdek, </author> <title> ed., </title> <journal> IEEE Transactions on Neural Networks Special Issue on Fuzzy Logic and Neural Networks, </journal> <volume> vol. 3. </volume> <booktitle> IEEE Neural Networks Council, </booktitle> <year> 1992. </year>
Reference-contexts: 1 Introduction There has been an increased interest in combining fuzzy systems with neural networks because fuzzy neural systems merge the advantages of both paradigms (see <ref> [4] </ref> for a collection of papers). Fuzzy logic [45] provides a mathematical foundation for approximate reasoning; fuzzy logic has proven very successful in a variety of applications [7, 9, 13, 18, 21, 22, 34, 43].
Reference: [5] <author> M. Casey, </author> <title> "The dynamics of discrete-time computation, with application to recurrent neural networks and finite state machine extraction," </title> <journal> Neural Computation, </journal> <volume> vol. 8, no. 6, </volume> <pages> pp. 1135-1178, </pages> <year> 1996. </year>
Reference-contexts: As a consequence, the theoretical foundations and algo-rithms for the extraction of deterministic finite-state automata also apply to the extraction of deterministic representations of FFAs. In particular, the following theorem developed for DFA extraction also applies to FFAs <ref> [5] </ref>: Theorem 5.1 The state space of a recurrent network modeling a given FFA must have mutually disjoint, closed sets Q i in [0; 1] N . The sets Q i correspond to states q i of the deterministic acceptor of some FFA. <p> The sets Q i correspond to states q i of the deterministic acceptor of some FFA. The following theorem asserts that FFA extraction based on an equal partitioning of the output state of recurrent neurons is appropriate <ref> [5] </ref>: Theorem 5.2 It is sufficient to consider only partitions created by dividing each recurrent neuron's output range into r partitions to always succeed in extracting the deterministic acceptor of some FFA that is being modeled for a sufficiently large quantization level r.
Reference: [6] <author> M. Casey, </author> <title> Computation in discrete-time dynamical systems. </title> <type> PhD thesis, </type> <institution> Department of Mathematics, University of California at San Diego, La Jolla, </institution> <address> CA, </address> <year> 1995. </year>
Reference-contexts: It has been shown that recurrent neural networks can represent DFAs, i.e. they can be trained to behave like DFAs, and a description of the learned knowledge can be extracted in the form of DFAs <ref> [6, 8, 10, 12, 15, 16, 28, 36, 40, 42, 46] </ref>. Thus, it is only natural to investigate whether recurrent neural networks can also be trained to behave like FFAs and whether a symbolic description can be extracted from trained networks.
Reference: [7] <author> S. Chiu, S. Chand, D. Moore, and A. Chaudhary, </author> <title> "Fuzzy logic for control of roll and moment for a flexible wing aircraft," </title> <journal> IEEE Control Systems Magazine, </journal> <volume> vol. 11, no. 4, </volume> <pages> pp. 42-48, </pages> <year> 1991. </year>
Reference-contexts: Fuzzy logic [45] provides a mathematical foundation for approximate reasoning; fuzzy logic has proven very successful in a variety of applications <ref> [7, 9, 13, 18, 21, 22, 34, 43] </ref>. On the one hand, parameters in fuzzy systems have clear physical meanings, and rule-based and linguistic information can be incorporated into adaptive fuzzy systems in a systematic way. On the other hand, there exist powerful algorithms for training various neural network models.
Reference: [8] <author> A. Cleeremans, D. Servan-Schreiber, and J. McClelland, </author> <title> "Finite state automata and simple recurrent recurrent networks," </title> <journal> Neural Computation, </journal> <volume> vol. 1, no. 3, </volume> <pages> pp. 372-381, </pages> <year> 1989. </year>
Reference-contexts: It has been shown that recurrent neural networks can represent DFAs, i.e. they can be trained to behave like DFAs, and a description of the learned knowledge can be extracted in the form of DFAs <ref> [6, 8, 10, 12, 15, 16, 28, 36, 40, 42, 46] </ref>. Thus, it is only natural to investigate whether recurrent neural networks can also be trained to behave like FFAs and whether a symbolic description can be extracted from trained networks.
Reference: [9] <author> J. Corbin, </author> <title> "A fuzzy logic-based financial transaction system," </title> <journal> Embedded Systems Programming, </journal> <volume> vol. 7, no. 12, </volume> <editor> p. </editor> <volume> 24, </volume> <year> 1994. </year>
Reference-contexts: Fuzzy logic [45] provides a mathematical foundation for approximate reasoning; fuzzy logic has proven very successful in a variety of applications <ref> [7, 9, 13, 18, 21, 22, 34, 43] </ref>. On the one hand, parameters in fuzzy systems have clear physical meanings, and rule-based and linguistic information can be incorporated into adaptive fuzzy systems in a systematic way. On the other hand, there exist powerful algorithms for training various neural network models.
Reference: [10] <author> S. Das and M. Mozer, </author> <title> "A unified gradient-descent/clustering architecture for finite state machine induction," </title> <booktitle> in Advances in Neural Information Processing Systems 6 (J. </booktitle> <editor> Cowan, G. Tesauro, and J. Alspector, eds.), </editor> <address> (San Francisco, CA), </address> <pages> pp. 19-26, </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: It has been shown that recurrent neural networks can represent DFAs, i.e. they can be trained to behave like DFAs, and a description of the learned knowledge can be extracted in the form of DFAs <ref> [6, 8, 10, 12, 15, 16, 28, 36, 40, 42, 46] </ref>. Thus, it is only natural to investigate whether recurrent neural networks can also be trained to behave like FFAs and whether a symbolic description can be extracted from trained networks. <p> We applied an extraction algorithm based on a partitioning of the output space of recurrent neurons [29]; other algorithms have been proposed (e.g. <ref> [10] </ref>). The extraction algorithm divides the output of each of the N state neurons into r intervals of equal size, yielding r N partitions in the space of outputs of the state neurons. We also refer to r as the quantization level.
Reference: [11] <author> D. Dubois and H. Prade, </author> <title> Fuzzy sets and systems: </title> <journal> theory and applications, </journal> <volume> vol. </volume> <booktitle> 144 of Mathematics in Science and Engineering, </booktitle> <pages> pp. 220-226. </pages> <publisher> Academic Press, </publisher> <year> 1980. </year>
Reference-contexts: Any fuzzy automaton as described in Definition 2.1 is equivalent to a restricted fuzzy automaton <ref> [11] </ref>. Notice that a FFA reduces to a conventional DFA by restricting the transition weights to 1. An example of a FFAs is shown in Figure 1a 2 . <p> This leaves some uncertainty or ambiguity about the generated string. Whether to choose stochastic or fuzzy regular grammars depends on the particular application. There exists a correspondence between FFAs and fuzzy regular grammars <ref> [11] </ref>: Theorem 2.1 For a given fuzzy fuzzy automaton f M , there exists a fuzzy grammar e G, such that L ( f M ) = L ( e G).
Reference: [12] <author> J. Elman, </author> <title> "Finding structure in time," </title> <journal> Cognitive Science, </journal> <volume> vol. 14, </volume> <pages> pp. 179-211, </pages> <year> 1990. </year>
Reference-contexts: It has been shown that recurrent neural networks can represent DFAs, i.e. they can be trained to behave like DFAs, and a description of the learned knowledge can be extracted in the form of DFAs <ref> [6, 8, 10, 12, 15, 16, 28, 36, 40, 42, 46] </ref>. Thus, it is only natural to investigate whether recurrent neural networks can also be trained to behave like FFAs and whether a symbolic description can be extracted from trained networks.
Reference: [13] <author> L. Franquelo and J. Chavez, "Fasy: </author> <title> A fuzzy-logic based tool for analog synthesis," </title> <journal> IEEE Transactions on Computer-Aided Design of Integrated Circuits, </journal> <volume> vol. 15, no. 7, </volume> <editor> p. </editor> <volume> 705, </volume> <year> 1996. </year> <month> 11 </month>
Reference-contexts: Fuzzy logic [45] provides a mathematical foundation for approximate reasoning; fuzzy logic has proven very successful in a variety of applications <ref> [7, 9, 13, 18, 21, 22, 34, 43] </ref>. On the one hand, parameters in fuzzy systems have clear physical meanings, and rule-based and linguistic information can be incorporated into adaptive fuzzy systems in a systematic way. On the other hand, there exist powerful algorithms for training various neural network models.
Reference: [14] <author> P. Frasconi, M. Gori, M. Maggini, and G. </author> <title> Soda, "Representation of finite state automata in recurrent radial basis function networks," </title> <journal> Machine Learning, </journal> <volume> vol. 23, no. 1, </volume> <pages> pp. 5-32, </pages> <year> 1996. </year>
Reference-contexts: Various methods have been proposed for implementing deterministic automata in recurrent neural networks <ref> [1, 2, 15, 14, 20, 26, 31, 28] </ref>.
Reference: [15] <author> P. Frasconi, M. Gori, M. Maggini, and G. </author> <title> Soda, "Unified integration of explicit rules and learning by example in recurrent networks," </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> vol. 7, no. 2, </volume> <pages> pp. 340-346, </pages> <year> 1995. </year>
Reference-contexts: It has been shown that recurrent neural networks can represent DFAs, i.e. they can be trained to behave like DFAs, and a description of the learned knowledge can be extracted in the form of DFAs <ref> [6, 8, 10, 12, 15, 16, 28, 36, 40, 42, 46] </ref>. Thus, it is only natural to investigate whether recurrent neural networks can also be trained to behave like FFAs and whether a symbolic description can be extracted from trained networks. <p> Various methods have been proposed for implementing deterministic automata in recurrent neural networks <ref> [1, 2, 15, 14, 20, 26, 31, 28] </ref>.
Reference: [16] <author> C. Giles, C. Miller, D. Chen, H. Chen, G. Sun, and Y. Lee, </author> <title> "Learning and extracting finite state automata with second-order recurrent neural networks," </title> <journal> Neural Computation, </journal> <volume> vol. 4, no. 3, </volume> <editor> p. </editor> <volume> 380, </volume> <year> 1992. </year>
Reference-contexts: It has been shown that recurrent neural networks can represent DFAs, i.e. they can be trained to behave like DFAs, and a description of the learned knowledge can be extracted in the form of DFAs <ref> [6, 8, 10, 12, 15, 16, 28, 36, 40, 42, 46] </ref>. Thus, it is only natural to investigate whether recurrent neural networks can also be trained to behave like FFAs and whether a symbolic description can be extracted from trained networks.
Reference: [17] <author> J. Grantner and M. Patyra, </author> <title> "Synthesis and analysis of fuzzy logic finite state machine models," </title> <booktitle> in Proceedings of the Third IEEE Conference on Fuzzy Systems, </booktitle> <volume> vol. I, </volume> <pages> pp. 205-210, </pages> <year> 1994. </year>
Reference-contexts: Fuzzy grammars have been found to be useful in a variety of applications such as digital circuit design [25] and in the analysis of X-rays [35], Neural network implementations of fuzzy automata have been proposed in the literature <ref> [17, 41] </ref>. The synthesis method proposed in [17] uses digital design technology to implement fuzzy representations of states and outputs. <p> Fuzzy grammars have been found to be useful in a variety of applications such as digital circuit design [25] and in the analysis of X-rays [35], Neural network implementations of fuzzy automata have been proposed in the literature [17, 41]. The synthesis method proposed in <ref> [17] </ref> uses digital design technology to implement fuzzy representations of states and outputs.
Reference: [18] <author> T. L. Hardy, </author> <title> "Multi-objective decision-making under uncertainty fuzzy logic methods," </title> <type> Tech. Rep. TM 106796, </type> <institution> NASA, </institution> <address> Washington, D.C., </address> <year> 1994. </year>
Reference-contexts: Fuzzy logic [45] provides a mathematical foundation for approximate reasoning; fuzzy logic has proven very successful in a variety of applications <ref> [7, 9, 13, 18, 21, 22, 34, 43] </ref>. On the one hand, parameters in fuzzy systems have clear physical meanings, and rule-based and linguistic information can be incorporated into adaptive fuzzy systems in a systematic way. On the other hand, there exist powerful algorithms for training various neural network models.
Reference: [19] <author> J. Hopcroft and J. Ullman, </author> <title> Introduction to Automata Theory, </title> <booktitle> Languages, and Computation. </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley Publishing Company, Inc., </publisher> <year> 1979. </year>
Reference-contexts: An example of FFA-to-DFA transformation is shown in Figure 1b 3 . An immediate consequence of this theorem is the following corollary: 3 This algorithm is an extension to the standard algorithm which transforms non-deterministic finite-state automata into equivalent deterministic finite-state automata <ref> [19] </ref>; unlike the standard transformation algorithm, we must distinguish accepting states with different fuzzy membership labels. <p> For a 9 given state, its label is set to the fuzzy membership occurring in the training set that is closest to the actual network output for that state. We apply a modified automaton minimization algorithm to each extracted automaton. The algorithm differs from the standard algorithm <ref> [19] </ref> in that the former not only distinguishes between accepting and rejecting states, but also considers the different fuzzy memberships with which an automaton accepts strings. Clearly, the extracted automaton depends on the quantization level r chosen, i.e., in general, different automata can be extracted for different values of r.
Reference: [20] <author> B. Horne and D. Hush, </author> <title> "Bounds on the complexity of recurrent neural network implementations of finite state machines," </title> <booktitle> in Advances in Neural Information Processing Systems 6, </booktitle> <pages> pp. 359-366, </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: Various methods have been proposed for implementing deterministic automata in recurrent neural networks <ref> [1, 2, 15, 14, 20, 26, 31, 28] </ref>.
Reference: [21] <author> W. J. M. Kickert and H. van Nauta Lemke, </author> <title> "Application of a fuzzy controller in a warm water plant," </title> <journal> Automatica, </journal> <volume> vol. 12, no. 4, </volume> <pages> pp. 301-308, </pages> <year> 1976. </year>
Reference-contexts: Fuzzy logic [45] provides a mathematical foundation for approximate reasoning; fuzzy logic has proven very successful in a variety of applications <ref> [7, 9, 13, 18, 21, 22, 34, 43] </ref>. On the one hand, parameters in fuzzy systems have clear physical meanings, and rule-based and linguistic information can be incorporated into adaptive fuzzy systems in a systematic way. On the other hand, there exist powerful algorithms for training various neural network models.
Reference: [22] <author> C. Lee, </author> <title> "Fuzzy logic in control systems: fuzzy logic controller," </title> <journal> IEEE Transactions on Man, Systems, and Cybernetics, </journal> <volume> vol. SMC-20, no. 2, </volume> <pages> pp. 404-435, </pages> <year> 1990. </year>
Reference-contexts: Fuzzy logic [45] provides a mathematical foundation for approximate reasoning; fuzzy logic has proven very successful in a variety of applications <ref> [7, 9, 13, 18, 21, 22, 34, 43] </ref>. On the one hand, parameters in fuzzy systems have clear physical meanings, and rule-based and linguistic information can be incorporated into adaptive fuzzy systems in a systematic way. On the other hand, there exist powerful algorithms for training various neural network models.
Reference: [23] <author> R. Maclin and J. Shavlik, </author> <title> "Refining algorithms with knowledge-based neural networks: Improving the Chou-Fasman algorithm for protein folding," in Computational Learning Theory and Natural Learning Systems (S. </title> <editor> Hanson, G. Drastal, and R. Rivest, eds.), </editor> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: This approach outperformed the best known `traditional' algorithm for protein folding <ref> [23, 24] </ref>. 2 The purpose of this paper is to show that recurrent networks can learn the behavior of FFAs, and that the learned knowledge can be extracted in a symbolic form. The latter objective requires an understanding of how trained networks represent FFAs.
Reference: [24] <author> R. Maclin and J. Shavlik, </author> <title> "Refining domain theories expressed as finite-state automata," </title> <booktitle> in Proceedings of the Eighth International Workshop on Machine Learning (ML'91) (L. </booktitle> <editor> B. . G. Collins, ed.), </editor> <address> (San Mateo, CA), </address> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: This approach outperformed the best known `traditional' algorithm for protein folding <ref> [23, 24] </ref>. 2 The purpose of this paper is to show that recurrent networks can learn the behavior of FFAs, and that the learned knowledge can be extracted in a symbolic form. The latter objective requires an understanding of how trained networks represent FFAs.
Reference: [25] <author> S. Mensch and H. Lipp, </author> <title> "Fuzzy specification of finite state machines," </title> <booktitle> in Proceedings of the European Design Automation Conference, </booktitle> <pages> pp. 622-626, </pages> <year> 1990. </year>
Reference-contexts: Thus, it is only natural to investigate whether recurrent neural networks can also be trained to behave like FFAs and whether a symbolic description can be extracted from trained networks. Fuzzy grammars have been found to be useful in a variety of applications such as digital circuit design <ref> [25] </ref> and in the analysis of X-rays [35], Neural network implementations of fuzzy automata have been proposed in the literature [17, 41]. The synthesis method proposed in [17] uses digital design technology to implement fuzzy representations of states and outputs.
Reference: [26] <author> M. Minsky, </author> <title> Computation: Finite and Infinite Machines, </title> <journal> ch. </journal> <volume> 3, </volume> <pages> pp. 32-66. </pages> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall, Inc., </publisher> <year> 1967. </year>
Reference-contexts: Various methods have been proposed for implementing deterministic automata in recurrent neural networks <ref> [1, 2, 15, 14, 20, 26, 31, 28] </ref>.
Reference: [27] <author> M. Mozer, </author> <title> "Induction of multiscale temporal structure," </title> <booktitle> in Advances in Neural Information Processing Systems 4 (J. </booktitle> <editor> Moody, S. Hanson, and R. Lippmann, eds.), </editor> <address> (San Mateo, CA), </address> <pages> pp. 275-282, </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1992. </year> <month> 12 </month>
Reference-contexts: Empirical and theoretical evidence suggests that the order in which strings are presented as input during the learning phase can have a profound impact on a network's training performance <ref> [3, 27] </ref>. The order of positive and negative training examples and the length of the training strings are important factors.
Reference: [28] <author> C. Omlin and C. Giles, </author> <title> "Constructing deterministic finite-state automata in recurrent neural networks," </title> <journal> Journal of the ACM, </journal> <volume> vol. 43, no. 6, </volume> <pages> pp. 937-972, </pages> <year> 1996. </year>
Reference-contexts: It has been shown that recurrent neural networks can represent DFAs, i.e. they can be trained to behave like DFAs, and a description of the learned knowledge can be extracted in the form of DFAs <ref> [6, 8, 10, 12, 15, 16, 28, 36, 40, 42, 46] </ref>. Thus, it is only natural to investigate whether recurrent neural networks can also be trained to behave like FFAs and whether a symbolic description can be extracted from trained networks. <p> A summary and directions for future work in Section 6 conclude this paper. 2 Fuzzy Finite-State Automata Here, we discuss the relationship between finite-state automata and recurrent neural networks for mapping fuzzy automata into recurrent networks; details can be found in <ref> [28, 33] </ref>. <p> Various methods have been proposed for implementing deterministic automata in recurrent neural networks <ref> [1, 2, 15, 14, 20, 26, 31, 28] </ref>. <p> These recurrent state neurons are connected to a linear output neuron which computes string membership. We have proven that any deterministic automaton can be encoded in discrete-time, second-order recurrent neural networks with sigmoidal discriminant functions such that the internal state representation remains stable for strings of arbitrary length <ref> [28] </ref>. The recurrent state neurons S j connect to a linear output neuron as follows: = j The weights v j are just the memberships assigned to the DFA states after the transformation of a FFA into an equivalent DFA.
Reference: [29] <author> C. Omlin and C. Giles, </author> <title> "Extraction of rules from discrete-time recurrent neural networks," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 9, no. 1, </volume> <pages> pp. 41-52, </pages> <year> 1996. </year>
Reference-contexts: We applied an extraction algorithm based on a partitioning of the output space of recurrent neurons <ref> [29] </ref>; other algorithms have been proposed (e.g. [10]). The extraction algorithm divides the output of each of the N state neurons into r intervals of equal size, yielding r N partitions in the space of outputs of the state neurons. We also refer to r as the quantization level. <p> Clearly, the extracted automaton depends on the quantization level r chosen, i.e., in general, different automata can be extracted for different values of r. We solve the model selection problem by choosing the first automaton that correctly classifies the training set <ref> [29] </ref>. Furthermore, because of condition (2), different automata may be extracted depending on the order in which the successors of a node in the search tree are visited.
Reference: [30] <author> C. Omlin and C. Giles, </author> <title> "Rule revision with recurrent neural networks," </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> vol. 8, no. 1, </volume> <pages> pp. 183-188, </pages> <year> 1996. </year>
Reference-contexts: They are particularly well-suited for problem domains where incomplete or contradictory prior knowledge is available. We have previously shown that recurrent networks can be initialized with such prior knowledge; the objective of training networks then becomes that of knowledge revision or refinement <ref> [30] </ref>. Fuzzy finite-state automata (FFAs) can model a large class of dynamical processes whose current state depends on the current input and previous states. <p> similar, it is very likely that our results on training recur 10 rent networks with prior knowledge training times for networks which are initialized with prior knowledge improve by a factor which is `proportional' to the amount of correct prior knowledge also apply to networks trained to behave like FFAs <ref> [30] </ref>. It would be interesting to apply these methods to a real-world problem where vague prior knowledge is available that can be modeled in the form of FFAs.
Reference: [31] <author> C. Omlin and C. Giles, </author> <title> "Stable encoding of large finite-state automata in recurrent neural networks with sigmoid discriminants," </title> <journal> Neural Computation, </journal> <volume> vol. 8, no. 7, </volume> <pages> pp. 675-696, </pages> <year> 1996. </year>
Reference-contexts: Various methods have been proposed for implementing deterministic automata in recurrent neural networks <ref> [1, 2, 15, 14, 20, 26, 31, 28] </ref>.
Reference: [32] <author> C. Omlin, K. Thornber, and C. Giles, </author> <title> "Equivalence in knowledge representation: Automata, recurrent neural networks, and dynamical fuzzy systems," </title> <type> tech. rep., </type> <year> 1998. </year> <note> Submitted. </note>
Reference-contexts: It remains an open problem whether other FFA representations exist for this network architecture. We have shown that a recurrent network architecture with a slightly enriched neuron functionality is able to represent the `fuzziness' of state transitions, i.e. the transition weights are also parameters of the network <ref> [32] </ref>. Whether such networks can be trained to behave like FFAs and whether FFAs can be directly extracted from such networks remains to be seen.
Reference: [33] <author> C. Omlin, K. Thornber, and C. Giles, </author> <title> "Fuzzy finite-state automata can be deterministically encoded into recurrent neural networks," </title> <journal> IEEE Transactions on Fuzzy Systems, </journal> <volume> vol. 6, no. 1, </volume> <pages> pp. 76-89, </pages> <year> 1998. </year>
Reference-contexts: The latter objective requires an understanding of how trained networks represent FFAs. We proved in <ref> [33] </ref> that the computational structure of deterministic recurrent neural networks is in principle rich enough to represent FFAs. We proposed an algorithm for mapping arbitrary FFAs into recurrent networks; the constructed networks assign the correct fuzzy membership to strings of arbitrary length with arbitrary accuracy. <p> A summary and directions for future work in Section 6 conclude this paper. 2 Fuzzy Finite-State Automata Here, we discuss the relationship between finite-state automata and recurrent neural networks for mapping fuzzy automata into recurrent networks; details can be found in <ref> [28, 33] </ref>. <p> This augmented, second-order recurrent neural network architecture is shown in Figure 2. We have previously proven the following theorem <ref> [33] </ref>: Theorem 3.1 For any given fuzzy finite-state automaton f M there exists an augmented, second-order neural network with sigmoidal recurrent neurons and a single linear output neuron which computes the fuzzy membership RNN (s) for all strings s 2 fl with arbitrary accuracy, i.e. fi fi e M (s) RNN
Reference: [34] <author> C. Pappis and E. Mamdani, </author> <title> "A fuzzy logic controller for a traffic junction," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> vol. SMC-7, no. 10, </volume> <pages> pp. 707-717, </pages> <year> 1977. </year>
Reference-contexts: Fuzzy logic [45] provides a mathematical foundation for approximate reasoning; fuzzy logic has proven very successful in a variety of applications <ref> [7, 9, 13, 18, 21, 22, 34, 43] </ref>. On the one hand, parameters in fuzzy systems have clear physical meanings, and rule-based and linguistic information can be incorporated into adaptive fuzzy systems in a systematic way. On the other hand, there exist powerful algorithms for training various neural network models.
Reference: [35] <author> A. Pathak and S. Pal, </author> <title> "Fuzzy grammars in syntactic recognition of skeletal maturity from x-rays," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> vol. 16, no. 5, </volume> <pages> pp. 657-667, </pages> <year> 1986. </year>
Reference-contexts: Fuzzy grammars have been found to be useful in a variety of applications such as digital circuit design [25] and in the analysis of X-rays <ref> [35] </ref>, Neural network implementations of fuzzy automata have been proposed in the literature [17, 41]. The synthesis method proposed in [17] uses digital design technology to implement fuzzy representations of states and outputs.
Reference: [36] <author> J. Pollack, </author> <title> "The induction of dynamical recognizers," </title> <journal> Machine Learning, </journal> <volume> vol. 7, </volume> <pages> pp. 227-252, </pages> <year> 1991. </year>
Reference-contexts: It has been shown that recurrent neural networks can represent DFAs, i.e. they can be trained to behave like DFAs, and a description of the learned knowledge can be extracted in the form of DFAs <ref> [6, 8, 10, 12, 15, 16, 28, 36, 40, 42, 46] </ref>. Thus, it is only natural to investigate whether recurrent neural networks can also be trained to behave like FFAs and whether a symbolic description can be extracted from trained networks.
Reference: [37] <author> M. Rabin, </author> <title> "Probabilistic automata," </title> <journal> Information and Control, </journal> <volume> vol. 6, </volume> <pages> pp. 230-245, </pages> <year> 1963. </year>
Reference-contexts: used: e G (s) = e G (S ) s) = max fl min [ e G (S ! ff 1 ); e G (ff 1 ! ff 2 ); : : : ; e G (ff m ! s)] This is akin to the definition of stochastic regular languages <ref> [37] </ref> where the min- and max-operators are replaced by the product- and sum-operators, respectively. Both fuzzy and stochastic regular languages are examples of weighted regular languages [38].
Reference: [38] <author> A. Salommaa, </author> <title> "Probabilistic and weighted grammars," </title> <journal> Information and Control, </journal> <volume> vol. 15, </volume> <pages> pp. 529-544, </pages> <year> 1969. </year>
Reference-contexts: Both fuzzy and stochastic regular languages are examples of weighted regular languages <ref> [38] </ref>. However, there are also distinct differences: In stochastic regular languages, the production rules are applied according to a probability distribution (i.e., the production weights are interpreted as probabilities).
Reference: [39] <author> M. Thomason and P. Marinos, </author> <title> "Deterministic acceptors of regular fuzzy languages," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> no. 3, </volume> <pages> pp. 228-230, </pages> <year> 1974. </year>
Reference-contexts: The obvious correspondence is between non-terminal and terminal symbols and non-accepting and accepting FFA states, respectively, where transitions between states are weighted with the corresponding production weight . The following result is the basis for mapping FFAs into deterministic recurrent neural networks <ref> [39] </ref>: Theorem 2.2 Given a regular fuzzy automaton f M , there exists a deterministic finite-state automaton M with output alphabet Z f : is a production weightg [ f0g which computes the membership function : fl ! [0; 1] of the language L ( f M ).
Reference: [40] <author> P. Tino and J. Sajda, </author> <title> "Learning and extracting initial mealy machines with a modular neural network model," </title> <journal> Neural Computation, </journal> <volume> vol. 7, no. 4, </volume> <pages> pp. 822-844, </pages> <year> 1995. </year>
Reference-contexts: It has been shown that recurrent neural networks can represent DFAs, i.e. they can be trained to behave like DFAs, and a description of the learned knowledge can be extracted in the form of DFAs <ref> [6, 8, 10, 12, 15, 16, 28, 36, 40, 42, 46] </ref>. Thus, it is only natural to investigate whether recurrent neural networks can also be trained to behave like FFAs and whether a symbolic description can be extracted from trained networks.
Reference: [41] <author> F. Unal and E. Khan, </author> <title> "A fuzzy finite state machine implementation based on a neural fuzzy system," </title> <booktitle> in Proceedings of the Third International Conference on Fuzzy Systems, </booktitle> <volume> vol. 3, </volume> <pages> pp. 1749-1754, </pages> <year> 1994. </year>
Reference-contexts: On the other hand, there exist powerful algorithms for training various neural network models. It is very rare for such hybrid systems to contain feedback in their structure. In <ref> [41] </ref>, the implementation of a finite-state machine with fuzzy inputs and crisp states is realized by training a feedforward network explicitly on the state transition table using a modified backpropagation algorithm. Such implementations are inadequate for modeling systems whose state depends on variables which are not observable. <p> Fuzzy grammars have been found to be useful in a variety of applications such as digital circuit design [25] and in the analysis of X-rays [35], Neural network implementations of fuzzy automata have been proposed in the literature <ref> [17, 41] </ref>. The synthesis method proposed in [17] uses digital design technology to implement fuzzy representations of states and outputs.
Reference: [42] <author> R. Watrous and G. Kuhn, </author> <title> "Induction of finite-state languages using second-order recurrent networks," </title> <journal> Neural Computation, </journal> <volume> vol. 4, no. 3, </volume> <editor> p. </editor> <volume> 406, </volume> <year> 1992. </year>
Reference-contexts: It has been shown that recurrent neural networks can represent DFAs, i.e. they can be trained to behave like DFAs, and a description of the learned knowledge can be extracted in the form of DFAs <ref> [6, 8, 10, 12, 15, 16, 28, 36, 40, 42, 46] </ref>. Thus, it is only natural to investigate whether recurrent neural networks can also be trained to behave like FFAs and whether a symbolic description can be extracted from trained networks.
Reference: [43] <author> X. Yang and G. Kalambur, </author> <title> "Design for machining using expert system and fuzzy logic approach," </title> <journal> Journal of Materials Engineering and Performance, </journal> <volume> vol. 4, no. 5, </volume> <editor> p. </editor> <volume> 599, </volume> <year> 1995. </year>
Reference-contexts: Fuzzy logic [45] provides a mathematical foundation for approximate reasoning; fuzzy logic has proven very successful in a variety of applications <ref> [7, 9, 13, 18, 21, 22, 34, 43] </ref>. On the one hand, parameters in fuzzy systems have clear physical meanings, and rule-based and linguistic information can be incorporated into adaptive fuzzy systems in a systematic way. On the other hand, there exist powerful algorithms for training various neural network models.
Reference: [44] <author> L. Zadeh, </author> <title> "Fuzzy languages and their relation to human and machine intelligence," </title> <type> Tech. Rep. </type> <institution> ERL-M302, Electronics Research Laboratory, University of California, Berkeley, </institution> <year> 1971. </year> <month> 13 </month>
Reference-contexts: Another method of decomposing a fuzzy grammar into crisp grammars has been investigated by Zadeh using the concept of level set <ref> [44] </ref>. 4 1 3 1 3 5 0,1/0.3 1/0.2 1/0.4 0 0 1 0.2 1 0 1 1 with weighted state transitions. State 1 is the automaton's start state; accepting states are drawn with double circles.
Reference: [45] <author> L. Zadeh, </author> <title> "Fuzzy sets," </title> <journal> Information and Control, </journal> <volume> vol. 8, </volume> <pages> pp. 338-353, </pages> <year> 1965. </year>
Reference-contexts: 1 Introduction There has been an increased interest in combining fuzzy systems with neural networks because fuzzy neural systems merge the advantages of both paradigms (see [4] for a collection of papers). Fuzzy logic <ref> [45] </ref> provides a mathematical foundation for approximate reasoning; fuzzy logic has proven very successful in a variety of applications [7, 9, 13, 18, 21, 22, 34, 43].
Reference: [46] <author> Z. Zeng, R. Goodman, and P. Smyth, </author> <title> "Learning finite state machines with self-clustering recurrent networks," </title> <journal> Neural Computation, </journal> <volume> vol. 5, no. 6, </volume> <pages> pp. 976-990, </pages> <year> 1993. </year> <month> 14 </month>
Reference-contexts: It has been shown that recurrent neural networks can represent DFAs, i.e. they can be trained to behave like DFAs, and a description of the learned knowledge can be extracted in the form of DFAs <ref> [6, 8, 10, 12, 15, 16, 28, 36, 40, 42, 46] </ref>. Thus, it is only natural to investigate whether recurrent neural networks can also be trained to behave like FFAs and whether a symbolic description can be extracted from trained networks.
References-found: 46

