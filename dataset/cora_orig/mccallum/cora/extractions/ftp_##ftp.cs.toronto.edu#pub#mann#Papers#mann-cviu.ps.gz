URL: ftp://ftp.cs.toronto.edu/pub/mann/Papers/mann-cviu.ps.gz
Refering-URL: http://www.cs.toronto.edu/~mann/abstracts/cviu.html
Root-URL: 
Email: fMann,Jepsong@CS.Toronto.EDU  Qobi@EMBA.UVM.EDU  
Phone: 2  Tel: 416/978-6488, Fax: 416/978-1455.  
Title: The Computational Perception of Scene Dynamics  
Author: Richard Mann Allan Jepson ; Jeffrey Mark Siskind 
Keyword: Motion understanding, Scene dynamics, Perceptual inference, Knowledge-based perception, Domain theory, View-based representations.  
Note: Running Head: Perception of Scene Dynamics Correspondence should be sent to A. Jepson,  
Address: 6 Kings College Road Toronto Ontario M5S 3H5 CANADA  Vermont Burlington VT 05405 USA  Toronto, 6 Kings College Road, Toronto, Ontario M5S 3H5,  
Affiliation: 1 Department of Computer Science University of Toronto  Canadian Institute for Advanced Research 3 Department of Electrical Engineering and Computer Science University of  Department of Computer Science, University of  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Norman I. Badler. </author> <title> Temporal scene analysis: Conceptual descriptions of object movements. </title> <type> Technical Report 80, </type> <institution> University of Toronto Department of Computer Science, </institution> <month> February </month> <year> 1975. </year>
Reference-contexts: Most work in motion understanding has focussed on extracting event descriptions from image sequences based on the spatio-temporal features of the input (see Badler <ref> [1] </ref>, Tsot-sos et. al. [32], Neumann and Novak [24], Borchardt [5], and Kuniyoshi and Inoue [19] for examples). In contrast to these approaches, our work attempts to form descriptions based on a general physical model of the dynamics of the scene.
Reference: [2] <author> David Baraff. </author> <title> Interactive simulation of solid rigid bodies. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 15(3) </volume> <pages> 63-75, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Perception of Scene Dynamics 15 4.2 Assertions In order to explore the space of possible interpretations, we must first construct a set of admissible assertions. Given that the allowable forces between objects depend on the contact geometry and the relative motion of the objects <ref> [8, 2] </ref>, an analysis of the scene kinematics is necessary. Since we do not have exact shape or motion information, however, we need a way to determine which contact relations are possible. In general, the determination of possible contact relations among moving objects is a non-linear programming problem. <p> In addition, while that system allowed the representation of uncertainty, preferences were not used to choose among feasible interpretations. While the problem is quite different, our representation of geometric and dynamic models borrows heavily from the physical simulation and graphics communities <ref> [8, 2] </ref>. Finally, it is worth noting that there is evidence that humans generate qualitative physical descriptions of scenes. In particular, there is evidence that humans perceive the force-dynamic relationships among objects in static and dynamic scenes [9, 20].
Reference: [3] <author> Michael J. Black and Allan D. Jepson. EigenTracking: </author> <title> Robust matching and tracking of articulated objects using a view-based representation. International Journal for Computer Vision. To appear. Perception of Scene Dynamics 30 </title>
Reference-contexts: Furthermore, we believe that this limitation is not fundamental to our general approach. In particular, our current system is able to reason about 3D scenes given suitable 3D input. We are currently investigating approaches for tracking 3D deforming objects such as the methods described by Black and Jepson <ref> [3] </ref>. Another limitation is that our system does not currently integrate information over time or reason about object properties. For example, consider the coke sequence once again. During the reaching phase of this sequence, the hand is seen to be above the table and accelerating upwards.
Reference: [4] <author> M. Blum, A. K. Griffith, and B. Neumann. </author> <title> A stability test for configurations of blocks. </title> <editor> A. I. </editor> <volume> Memo 188, </volume> <publisher> MIT Artificial Intelligence Laboratory, </publisher> <month> February </month> <year> 1970. </year>
Reference-contexts: In particular, we show how the test for consistency within the physical theory can be expressed as a set of algebraic constraints that, when provided with an admissible interpretation, can be tested with linear programming. We use a "force balancing" approach similar to that proposed by Blum et. al. <ref> [4] </ref>. Our approach, however, models dynamics as well as static force balancing. In this section, we present a theory for the general three-dimensional case. The experimental results we describe later were produced using a two-dimensional variant of this theory. <p> In contrast to these approaches, our system uses an explicit, quantitative, representation of the dynamics based on Newtonian mechanics. A number of other systems have used physics-based representations for scenes in terms of forces in static scenes (see Blum et. al. <ref> [4] </ref>), and changing kinematic relations in time-varying scenes (see Ikeuchi and Suehiro [13] and Siskind [30]). Our system extends these approaches to consider both kinematic and dynamic relations in time-varying scenes containing rigid objects.
Reference: [5] <author> Gary C. Borchardt. </author> <title> Event calculus. </title> <booktitle> In Proceedings of the Ninth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 524-527, </pages> <address> Los Angeles, CA, </address> <month> August </month> <year> 1985. </year>
Reference-contexts: Most work in motion understanding has focussed on extracting event descriptions from image sequences based on the spatio-temporal features of the input (see Badler [1], Tsot-sos et. al. [32], Neumann and Novak [24], Borchardt <ref> [5] </ref>, and Kuniyoshi and Inoue [19] for examples). In contrast to these approaches, our work attempts to form descriptions based on a general physical model of the dynamics of the scene.
Reference: [6] <author> Matthew Brand, Lawrence Birnbaum, and Paul Cooper. </author> <title> Sensible scenes: Visual understanding of complex scenes through causal analysis. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artifical Intelligence, </booktitle> <pages> pages 588-593, </pages> <address> Washington, DC, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: A number of systems have attempted to represent physical knowledge in static and dynamic scenes using qualitative physical models or rule-based systems (see Fahlman [7], Funt [10], Joskowicz and Sacks [18], Siskind [29, 31], and Brand et. al. <ref> [6] </ref>). In contrast to these approaches, our system uses an explicit, quantitative, representation of the dynamics based on Newtonian mechanics.
Reference: [7] <author> Scott Elliott Fahlman. </author> <title> A planning system for robot construction tasks. </title> <journal> Artificial Intelligence, </journal> <volume> 5(1) </volume> <pages> 1-49, </pages> <year> 1974. </year>
Reference-contexts: In contrast to these approaches, our work attempts to form descriptions based on a general physical model of the dynamics of the scene. A number of systems have attempted to represent physical knowledge in static and dynamic scenes using qualitative physical models or rule-based systems (see Fahlman <ref> [7] </ref>, Funt [10], Joskowicz and Sacks [18], Siskind [29, 31], and Brand et. al. [6]). In contrast to these approaches, our system uses an explicit, quantitative, representation of the dynamics based on Newtonian mechanics.
Reference: [8] <author> Roy Featherstone. </author> <title> Robot Dynamics Algorithms. </title> <publisher> Kluwer, </publisher> <address> Boston, </address> <year> 1987. </year>
Reference-contexts: Perception of Scene Dynamics 15 4.2 Assertions In order to explore the space of possible interpretations, we must first construct a set of admissible assertions. Given that the allowable forces between objects depend on the contact geometry and the relative motion of the objects <ref> [8, 2] </ref>, an analysis of the scene kinematics is necessary. Since we do not have exact shape or motion information, however, we need a way to determine which contact relations are possible. In general, the determination of possible contact relations among moving objects is a non-linear programming problem. <p> Given a scene with convex polygonal objects, we can represent the forces between contacting objects by a set of forces acting on the vertices of the convex hull of their contact region <ref> [8] </ref>. <p> In addition, while that system allowed the representation of uncertainty, preferences were not used to choose among feasible interpretations. While the problem is quite different, our representation of geometric and dynamic models borrows heavily from the physical simulation and graphics communities <ref> [8, 2] </ref>. Finally, it is worth noting that there is evidence that humans generate qualitative physical descriptions of scenes. In particular, there is evidence that humans perceive the force-dynamic relationships among objects in static and dynamic scenes [9, 20].
Reference: [9] <author> Jennifer J. Freyd, Teresa M. Pantzer, and Jeannette L. Cheng. </author> <title> Representing statics as forces in equilibrium. </title> <journal> Journal of Experimental Psychology: General, </journal> <volume> 117(4) </volume> <pages> 395-407, </pages> <month> December </month> <year> 1988. </year>
Reference-contexts: Finally, it is worth noting that there is evidence that humans generate qualitative physical descriptions of scenes. In particular, there is evidence that humans perceive the force-dynamic relationships among objects in static and dynamic scenes <ref> [9, 20] </ref>. However, it also appears that humans have a limited understanding of some dynamic events, such as those involving angular motion [25] and collisions between objects [11]. 9 Conclusion In this paper we have presented an implemented computational theory that can derive force-dynamic representations directly from camera input.
Reference: [10] <author> Brian V. Funt. </author> <title> Problem-solving with diagrammatic representations. </title> <journal> Artificial Intelligence, </journal> <volume> 13(3) </volume> <pages> 201-230, </pages> <month> May </month> <year> 1980. </year>
Reference-contexts: In contrast to these approaches, our work attempts to form descriptions based on a general physical model of the dynamics of the scene. A number of systems have attempted to represent physical knowledge in static and dynamic scenes using qualitative physical models or rule-based systems (see Fahlman [7], Funt <ref> [10] </ref>, Joskowicz and Sacks [18], Siskind [29, 31], and Brand et. al. [6]). In contrast to these approaches, our system uses an explicit, quantitative, representation of the dynamics based on Newtonian mechanics.
Reference: [11] <author> David L Gilden and Dennis R Proffitt. </author> <title> Understanding collision dynamics. </title> <journal> Journal of Experimental Psychology: Human Perception and Performance, </journal> <volume> 15(2) </volume> <pages> 372-383, </pages> <year> 1989. </year>
Reference-contexts: In particular, there is evidence that humans perceive the force-dynamic relationships among objects in static and dynamic scenes [9, 20]. However, it also appears that humans have a limited understanding of some dynamic events, such as those involving angular motion [25] and collisions between objects <ref> [11] </ref>. 9 Conclusion In this paper we have presented an implemented computational theory that can derive force-dynamic representations directly from camera input. Our system embodies a rich ontology that includes both kinematic and dynamic properties of the observed objects.
Reference: [12] <author> Herbert Goldstein. </author> <title> Classical Mechanics. </title> <publisher> Addison-Wesley, </publisher> <address> second edition, </address> <year> 1980. </year>
Reference-contexts: In this section, we present a theory for the general three-dimensional case. The experimental results we describe later were produced using a two-dimensional variant of this theory. For rigid bodies under continuous motion, the dynamics are described by the Newton-Euler equations of motion <ref> [12] </ref>. For rigid bodies of non-varying mass, the appropriate equations are: F = _p N = _ L The first equation relates the total applied force F to the rate of change of linear momentum _p.
Reference: [13] <author> Katsushi Ikeuchi and T. Suehiro. </author> <title> Towards an assembly plan from observation, part i: Task recognition with polyhedral objects. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 10(3) </volume> <pages> 368-385, </pages> <year> 1994. </year> <title> Perception of Scene Dynamics 31 </title>
Reference-contexts: A number of other systems have used physics-based representations for scenes in terms of forces in static scenes (see Blum et. al. [4]), and changing kinematic relations in time-varying scenes (see Ikeuchi and Suehiro <ref> [13] </ref> and Siskind [30]). Our system extends these approaches to consider both kinematic and dynamic relations in time-varying scenes containing rigid objects. Shavit and Jepson [28] present a different approach to classifying motion based on the dynamic properties of non-rigid objects.
Reference: [14] <author> Michael Jenkin and Allan D. Jepson. </author> <title> Detecting floor anomalies. </title> <booktitle> In Proceedings of the British Machine Vision Conference, </booktitle> <pages> pages 731-740, </pages> <address> York, UK, </address> <year> 1994. </year>
Reference-contexts: We describe the various components of our implementation below. 4.1 Configuration To acquire the position and orientation of the object polygons for each frame we use a view-based tracking algorithm similar to the optical flow and stereo disparity algorithms described in <ref> [15, 14] </ref>. (The full details of the tracking algorithm are described in [22].) In particular, a template image is provided for each object, along with information about where the object is located within the template.
Reference: [15] <author> Allan D. Jepson and Michael J. Black. </author> <title> Mixture models for optical flow. </title> <editor> In Ingmer Cox, Pierre Hansen, and Bela Julesz, editors, </editor> <booktitle> Proceedings of the DIMACS Workshop on Partitioning Data Sets, </booktitle> <pages> pages 271-286. </pages> <publisher> AMS Publications, </publisher> <address> Providence, RI, </address> <year> 1993. </year>
Reference-contexts: We describe the various components of our implementation below. 4.1 Configuration To acquire the position and orientation of the object polygons for each frame we use a view-based tracking algorithm similar to the optical flow and stereo disparity algorithms described in <ref> [15, 14] </ref>. (The full details of the tracking algorithm are described in [22].) In particular, a template image is provided for each object, along with information about where the object is located within the template.
Reference: [16] <author> Allan D. Jepson and Whitman Richards. </author> <note> What is a percept? Technical Report RBCV-TR-93-43, </note> <institution> Department of Computer Science, University of Toronto, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: Given that our ontology includes nothing about hands or cans, that is, we just have moving polygons and notions of mass and force, arriving at these two interpretations for this single frame is intuitively the right thing to do. Indeed, Jepson and Richards <ref> [16] </ref> and Richards, Jepson, and Feldman [27] propose that such maximal interpretations provide a computational model for a `percept.' Moreover, Richards, Jepson, and Feldman [27] explore the relationship between such preference orderings and qualitative probabilistic models (see also Jepson, Richards, and Knill [17]).
Reference: [17] <author> Allan D. Jepson, Whitman Richards, and David Knill. </author> <title> Modal structure and reliable inference. </title> <editor> In David Knill and Whitman Richards, editors, </editor> <booktitle> Perception as Bayesian Inference, </booktitle> <pages> pages 63-92. </pages> <publisher> Cambridge University Press, </publisher> <year> 1996. </year>
Reference-contexts: Indeed, Jepson and Richards [16] and Richards, Jepson, and Feldman [27] propose that such maximal interpretations provide a computational model for a `percept.' Moreover, Richards, Jepson, and Feldman [27] explore the relationship between such preference orderings and qualitative probabilistic models (see also Jepson, Richards, and Knill <ref> [17] </ref>). Perception of Scene Dynamics 9 2.3 Implementation and Limitations Our current implementation has a number of limitations. One limitation is that our system uses a 2D layered representation. Given this limited representation, we can process only fronto-parallel scenes and cannot reason about occlusion or motion in depth.
Reference: [18] <author> Leo Joskowicz and Elisha P. Sacks. </author> <title> Computational kinematics. </title> <journal> Artificial Intelligence, </journal> <volume> 51(1-3):381-416, </volume> <month> October </month> <year> 1991. </year>
Reference-contexts: A number of systems have attempted to represent physical knowledge in static and dynamic scenes using qualitative physical models or rule-based systems (see Fahlman [7], Funt [10], Joskowicz and Sacks <ref> [18] </ref>, Siskind [29, 31], and Brand et. al. [6]). In contrast to these approaches, our system uses an explicit, quantitative, representation of the dynamics based on Newtonian mechanics.
Reference: [19] <author> Yasuo Kuniyoshi and Hirochika Inoue. </author> <title> Qualitative recognition of ongoing human action sequences. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1600-1609, </pages> <address> Chambery, France, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Most work in motion understanding has focussed on extracting event descriptions from image sequences based on the spatio-temporal features of the input (see Badler [1], Tsot-sos et. al. [32], Neumann and Novak [24], Borchardt [5], and Kuniyoshi and Inoue <ref> [19] </ref> for examples). In contrast to these approaches, our work attempts to form descriptions based on a general physical model of the dynamics of the scene.
Reference: [20] <author> Alan M. Leslie and Stephanie Keeble. </author> <title> Do six-month-old infants perceive causality? Cognition, </title> <booktitle> 25 </booktitle> <pages> 265-288, </pages> <year> 1987. </year>
Reference-contexts: Finally, it is worth noting that there is evidence that humans generate qualitative physical descriptions of scenes. In particular, there is evidence that humans perceive the force-dynamic relationships among objects in static and dynamic scenes <ref> [9, 20] </ref>. However, it also appears that humans have a limited understanding of some dynamic events, such as those involving angular motion [25] and collisions between objects [11]. 9 Conclusion In this paper we have presented an implemented computational theory that can derive force-dynamic representations directly from camera input.
Reference: [21] <author> David G. Luenberger. </author> <title> Linear and Nonlinear Programming. </title> <publisher> Addison Wesley, </publisher> <year> 1984. </year>
Reference-contexts: Note that the use of tolerances is necessary since the observed motion will never be exactly zero. Since all of the above equations and inequalities are linear, dynamic feasibility can be reduced to a feasibility test using linear programming <ref> [21] </ref>. 6 Preferences As described in x3, we have a fixed set of elementary preference relations, namely * P bodymotor (o) : :BodyMotor (o) BodyMotor (o); * P linearmotor (c) : :LinearMotor (o 1 ; o 2 ; c) LinearMotor (o 1 ; o 2 ; c); * P angularmotor (c)
Reference: [22] <author> Richard Mann. </author> <title> Computational Perception of Scene Dynamics. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Toronto. </institution> <note> In preparation. </note>
Reference-contexts: various components of our implementation below. 4.1 Configuration To acquire the position and orientation of the object polygons for each frame we use a view-based tracking algorithm similar to the optical flow and stereo disparity algorithms described in [15, 14]. (The full details of the tracking algorithm are described in <ref> [22] </ref>.) In particular, a template image is provided for each object, along with information about where the object is located within the template. <p> Note that robustness also allows the tracker to deal with objects that have changing occlusion relationships. (We show such examples in x7.) Given the pose (that is, x (t), y (t), and (t)) of the objects in each frame, we estimate the motion by a robust interpolation procedure (see <ref> [22] </ref> for details). Specifically, at each point in time, we robustly fit a cubic polynomial to the data over a sliding temporal window that is seven frames wide. We then differentiate these polynomials to obtain estimates for the velocity, acceleration, angular velocity, and angular accelerations of each object. <p> This property justifies the algorithm above where we set all of the lower priority assertions to the most permissive settings during each stage of the minimization. In general we refer to this property as monotonicity. (Details of the search algorithm and a proof of correctness are given in <ref> [22] </ref>.) 7 Experimental Results We have applied our system to several image sequences taken from a desktop environment (see Figure 1). The sequences were taken from a video camera attached to a SunVideo imaging system. <p> Given this input, the tracker provides estimates for the object poses and motions in each frame of the sequence. These estimates, along with the polygonal shapes, are used by the interpretation-construction module. respectively. (Tracking data for all sequences is given in <ref> [22] </ref>.) In each figure, the upper left graph shows the estimates for the x and y component velocities of the object polygons, while the upper right graph shows the estimates for the angular velocities. The lower graphs show the corresponding estimates for the linear and angular accelerations. <p> We believe our current system provides the building blocks for such a representation, but additional work will be required to show how our ontology can be built into a more complex system. (See <ref> [22] </ref> for preliminary work on time-varying scenes.) Acknowledgments. The authors are grateful to IRIS and NSERC Canada for financial support. The authors would like to thank Whitman Richards, Michael Black, and Chakra Chennubhotla for helpful comments on this work.
Reference: [23] <author> John McCarthy. </author> <title> Applications of circumscription to formalizing common sense reasoning. </title> <journal> Artificial Intelligence, </journal> <volume> 28 </volume> <pages> 89-116, </pages> <year> 1986. </year> <title> Perception of Scene Dynamics 32 </title>
Reference-contexts: Finally, in the case that the assertions at the highest priority are the same in both interpretations, then we check the assertions at the next lower priority, and so on. This approach, based upon prioritized ordering of elementary preference relations, is similar to prioritized circumscription <ref> [23] </ref>. To find maximally-preferred models, we search the space of possible interpretations. We perform a breadth-first search, starting with the empty set of assertions, incrementally adding new assertions to this set. Each branch of the search terminates upon finding a minimal set of assertions required for feasible force balancing.
Reference: [24] <author> Bernd Neumann and Hans-Joachim Novak. </author> <title> Event models for recognition and natural language description of events in real-world image sequences. </title> <booktitle> In Proceedings of the Eighth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 724-726, </pages> <address> Karl-sruhe, </address> <month> August </month> <year> 1983. </year>
Reference-contexts: Most work in motion understanding has focussed on extracting event descriptions from image sequences based on the spatio-temporal features of the input (see Badler [1], Tsot-sos et. al. [32], Neumann and Novak <ref> [24] </ref>, Borchardt [5], and Kuniyoshi and Inoue [19] for examples). In contrast to these approaches, our work attempts to form descriptions based on a general physical model of the dynamics of the scene.
Reference: [25] <author> Dennis R Proffitt, Mary K Kaiser, and S M Whelan. </author> <title> Understanding wheel dynamics. </title> <journal> Cognitive Psychology, </journal> <volume> 22 </volume> <pages> 342-373, </pages> <year> 1990. </year>
Reference-contexts: In particular, there is evidence that humans perceive the force-dynamic relationships among objects in static and dynamic scenes [9, 20]. However, it also appears that humans have a limited understanding of some dynamic events, such as those involving angular motion <ref> [25] </ref> and collisions between objects [11]. 9 Conclusion In this paper we have presented an implemented computational theory that can derive force-dynamic representations directly from camera input. Our system embodies a rich ontology that includes both kinematic and dynamic properties of the observed objects.
Reference: [26] <author> Raymond Reiter and Alan Mackworth. </author> <title> A logical framework for depiction and image interpretation. </title> <journal> Artificial Intelligence, </journal> <volume> 41 </volume> <pages> 125-155, </pages> <year> 1989. </year>
Reference-contexts: Shavit and Jepson [28] present a different approach to classifying motion based on the dynamic properties of non-rigid objects. Our representation of an interpretation as a set of logical propositions about scene properties is similar to the approach presented by Reiter and Macworth <ref> [26] </ref>. Unlike our system, Perception of Scene Dynamics 28 however, that system considered a simple domain (maps) where feasibility could be expressed as a set of logical constraints. In addition, while that system allowed the representation of uncertainty, preferences were not used to choose among feasible interpretations.
Reference: [27] <author> Whitman Richards, Allan D. Jepson, and Jacob Feldman. </author> <title> Priors, preferences and categorical percepts. </title> <editor> In David Knill and Whitman Richards, editors, </editor> <booktitle> Perception as Bayesian Inference, </booktitle> <pages> pages 93-122. </pages> <publisher> Cambridge University Press, </publisher> <year> 1996. </year>
Reference-contexts: Rather, we seek interpretations that require, in some specified sense, the Perception of Scene Dynamics 7 weakest properties of the various objects. Model preference relations, as discussed by Richards, Jepson, and Feldman <ref> [27] </ref>, can be used to express suitable preference orderings. The basic idea is simple, namely to compare two different interpretations in terms of a prioritized set of elementary preference relations. Our current ontology includes the elementary preference for the absence of the assertion BodyMotor (o) for each object o. <p> Given that our ontology includes nothing about hands or cans, that is, we just have moving polygons and notions of mass and force, arriving at these two interpretations for this single frame is intuitively the right thing to do. Indeed, Jepson and Richards [16] and Richards, Jepson, and Feldman <ref> [27] </ref> propose that such maximal interpretations provide a computational model for a `percept.' Moreover, Richards, Jepson, and Feldman [27] explore the relationship between such preference orderings and qualitative probabilistic models (see also Jepson, Richards, and Knill [17]). <p> Indeed, Jepson and Richards [16] and Richards, Jepson, and Feldman <ref> [27] </ref> propose that such maximal interpretations provide a computational model for a `percept.' Moreover, Richards, Jepson, and Feldman [27] explore the relationship between such preference orderings and qualitative probabilistic models (see also Jepson, Richards, and Knill [17]). Perception of Scene Dynamics 9 2.3 Implementation and Limitations Our current implementation has a number of limitations. One limitation is that our system uses a 2D layered representation. <p> We wish to find interpretations that are maximally-preferred subject to these preference relations. In this paper we consider a special case of the more general orderings described by Richards, Jepson, and Feldman <ref> [27] </ref>, where the elementary preference relations can be of the form P (x) Q (x) for predicates P and Q. In addition, we adopt the convention that the absence of an assertion indicates its negation.
Reference: [28] <author> Eyal Shavit and Allan D. Jepson. </author> <title> Qualitative motion from visual dynamics. </title> <booktitle> In IEEE Workshop on Qualitative Vision, </booktitle> <address> New York, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Our system extends these approaches to consider both kinematic and dynamic relations in time-varying scenes containing rigid objects. Shavit and Jepson <ref> [28] </ref> present a different approach to classifying motion based on the dynamic properties of non-rigid objects. Our representation of an interpretation as a set of logical propositions about scene properties is similar to the approach presented by Reiter and Macworth [26].
Reference: [29] <author> Jeffrey Mark Siskind. </author> <title> Naive Physics, Event Perception, Lexical Semantics, and Language Acquisition. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, </address> <month> January </month> <year> 1992. </year>
Reference-contexts: A number of systems have attempted to represent physical knowledge in static and dynamic scenes using qualitative physical models or rule-based systems (see Fahlman [7], Funt [10], Joskowicz and Sacks [18], Siskind <ref> [29, 31] </ref>, and Brand et. al. [6]). In contrast to these approaches, our system uses an explicit, quantitative, representation of the dynamics based on Newtonian mechanics.
Reference: [30] <author> Jeffrey Mark Siskind. </author> <title> Axiomatic support for event perception. </title> <editor> In Paul McKevitt, editor, </editor> <booktitle> Proceedings of the AAAI-94 Workshop on the Integration of Natural Language and Vision Processing, </booktitle> <pages> pages 153-160, </pages> <address> Seattle, WA, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: A number of other systems have used physics-based representations for scenes in terms of forces in static scenes (see Blum et. al. [4]), and changing kinematic relations in time-varying scenes (see Ikeuchi and Suehiro [13] and Siskind <ref> [30] </ref>). Our system extends these approaches to consider both kinematic and dynamic relations in time-varying scenes containing rigid objects. Shavit and Jepson [28] present a different approach to classifying motion based on the dynamic properties of non-rigid objects.
Reference: [31] <author> Jeffrey Mark Siskind. </author> <title> Grounding language in perception. </title> <journal> Artificial Intelligence Review, </journal> <volume> 8 </volume> <pages> 371-391, </pages> <year> 1995. </year> <title> Perception of Scene Dynamics 33 </title>
Reference-contexts: A number of systems have attempted to represent physical knowledge in static and dynamic scenes using qualitative physical models or rule-based systems (see Fahlman [7], Funt [10], Joskowicz and Sacks [18], Siskind <ref> [29, 31] </ref>, and Brand et. al. [6]). In contrast to these approaches, our system uses an explicit, quantitative, representation of the dynamics based on Newtonian mechanics.
Reference: [32] <author> John K. Tsotsos, John Mylopoulos, H. Dominic Covvey, and Steven W. Zucker. </author> <title> A framework for visual motion understanding. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 2(6) </volume> <pages> 563-573, </pages> <month> November </month> <year> 1980. </year> <title> Perception of Scene Dynamics 34 </title>
Reference-contexts: Most work in motion understanding has focussed on extracting event descriptions from image sequences based on the spatio-temporal features of the input (see Badler [1], Tsot-sos et. al. <ref> [32] </ref>, Neumann and Novak [24], Borchardt [5], and Kuniyoshi and Inoue [19] for examples). In contrast to these approaches, our work attempts to form descriptions based on a general physical model of the dynamics of the scene.
References-found: 32

