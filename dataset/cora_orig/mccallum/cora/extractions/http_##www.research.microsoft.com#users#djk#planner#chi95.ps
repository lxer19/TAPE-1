URL: http://www.research.microsoft.com/users/djk/planner/chi95.ps
Refering-URL: http://www.research.microsoft.com/users/djk/planner/chi95.htm
Root-URL: http://www.research.microsoft.com
Keyword: Animation, planning, User Interface Management Systems, UIMS, user interface components, 3D interfaces.  
Address: One Microsoft Way, Redmond, WA 98052  
Affiliation: Microsoft Research,  
Date: 472-479.  January 1995.  ABSTRACT  
Note: 1 of 8 Reprinted from the Proceedings of CHI 95 (Denver, May 7-11), pp.  
Pubnum: Technical Report MSR-TR-95-21.  
Abstract: Animations express a sense of process and continuity that is difficult to convey through other techniques. Although interfaces can often benefit from animation, User Interface Management Systems (UIMSs) rarely provide the tools necessary to easily support complex, statedependent application output, such as animations. Here we describe Player, an interface component that facilitates sequencing these animations. One difficulty of integrating animations into interactive systems is that animation scripts typically only work in very specific contexts. Care must be taken to establish the required context prior to executing an animation. Player employs a precondition and postcondition-based specification language, and automatically computes which animation scripts should be invoked to establish the necessary state. Players specification language has been designed to make it easy to express the desired behavior of animation controllers. Since planning can be a time-consuming process inappropriate for interactive systems, Player precompiles the plan-based specification into a state machine that executes far more quickly. Serving as an animation controller, Player hides animation script dependencies from the application. Player has been incorporated into the Persona UIMS, and is currently used in the Peedy application. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Ball, J. E. et al. </author> <title> Reactor: A System for Real-Time, Reactive Animations. </title> <booktitle> In CHI 94 Conference Companion. </booktitle> <address> (Boston, MA, April 24-28). </address> <publisher> ACM, </publisher> <address> New York. </address> <year> 1994. </year> <pages> 39-40. </pages>
Reference-contexts: To better motivate the need for this capability in a UIMS, consider the Persona UIMS currently being developed by our group, and a prototype agent-based interface called Peedy, built on Persona, and demonstrated at CHI 94 <ref> [1] </ref>. Peedys visual representation is that of a 3D animated parrot. Peedy has a rich repertoire of animated birdlike and humanlike behaviors, and responds to spoken natural language requests for musical selections.
Reference: 2. <author> Card, S. K., Robertson, G. G., and Mackinlay, J. D. </author> <title> The Information Visualizer, an Information Workspace. </title> <booktitle> In CHI 91 Proceedings. </booktitle> <address> (New Orleans, LA, April 27-May 2). </address> <publisher> ACM, </publisher> <address> New York. </address> <year> 1991. </year> <pages> 181-188. </pages>
Reference-contexts: However, the UIDE work does not use planning to sequence general animation presentations, and does not employ precompilation techniques to maintain real-time constraints. Many researchers have addressed other aspects of incorporating animation into the interface. For example, PARCs Cognitive Coprocessor deals with timing issues <ref> [2] </ref>. Several interfaces incorporate traditional animation effects [3] [10]. Virtual reality research frequently coordinates simulation and novel interface devices with animation [13]. Researchers in artificial intelligence have studied planning issues extensively, and their work is summarized in several surveys [7] [6].
Reference: 3. <author> Chang, B. and Ungar, D. </author> <title> Animation: From Cartoons to the User Interface. </title> <booktitle> In UIST 93 Proceedings. </booktitle> <address> (Atlanta, GA, </address> <month> November 3-5). </month> <booktitle> ACM, </booktitle> <address> New York. </address> <year> 1993. </year> <pages> 45-55. </pages> <note> 4. </note> <author> de Graaff, J. J., Sukaviriya, P., and van der Mast, C. A. P. G. </author> <title> Automatic Generation of ContextSensitive Textual Help. </title> <type> Tech. Report GIT-GVU-93-11. </type> <institution> GVU Center. Georgia Institute of Technology. </institution> <year> 1993. </year>
Reference-contexts: Many researchers have addressed other aspects of incorporating animation into the interface. For example, PARCs Cognitive Coprocessor deals with timing issues [2]. Several interfaces incorporate traditional animation effects <ref> [3] </ref> [10]. Virtual reality research frequently coordinates simulation and novel interface devices with animation [13]. Researchers in artificial intelligence have studied planning issues extensively, and their work is summarized in several surveys [7] [6].
Reference: 5. <author> Foley, J. et al. </author> <title> A Knowledge-based User Interface Management System. </title> <booktitle> In CHI 88 Conference Proceedings. </booktitle> <address> (Washington, DC, May 15-19). </address> <publisher> ACM, </publisher> <address> New York. </address> <year> 1988. </year> <pages> 67-72. </pages>
Reference-contexts: This data display facet of the model has not been fully realized in any UIMS [14]. It is this facet that Player attempts to address. The UIDE system also relies on preconditions and postcon-ditions for dialogue control <ref> [5] </ref> [8], using them to determine when to enable application actions, and to support interface transformations. However, UIDE does not use planning to automatically sequence visual presentations. An exception to this is Sukaviriyas Cartoonist system, that extends the FIGURE 2. The Seeheim UIMS model (adapted from [14]).
Reference: 6. <author> Genesereth, M. R., and Nilsson, N. J. </author> <booktitle> Logical Foundations of Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address> <year> 1988. </year>
Reference-contexts: For example, PARCs Cognitive Coprocessor deals with timing issues [2]. Several interfaces incorporate traditional animation effects [3] [10]. Virtual reality research frequently coordinates simulation and novel interface devices with animation [13]. Researchers in artificial intelligence have studied planning issues extensively, and their work is summarized in several surveys [7] <ref> [6] </ref>. The planning technique described here is a simple variant of goal regression, and is not intended to contribute to the planning literature. However, we do believe that this paper presents a novel application for planning technology.
Reference: 7. <author> Georgeff, M. P. </author> <note> Planning. In Readings in Planning, edited by J. </note> <editor> Allen et al. </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address> <year> 1990. </year> <pages> 5-25. </pages>
Reference-contexts: For example, PARCs Cognitive Coprocessor deals with timing issues [2]. Several interfaces incorporate traditional animation effects [3] [10]. Virtual reality research frequently coordinates simulation and novel interface devices with animation [13]. Researchers in artificial intelligence have studied planning issues extensively, and their work is summarized in several surveys <ref> [7] </ref> [6]. The planning technique described here is a simple variant of goal regression, and is not intended to contribute to the planning literature. However, we do believe that this paper presents a novel application for planning technology.
Reference: 8. <author> Gieskens, D. F., and Foley, J. D. </author> <title> Controlling User Interface Objects through Pre and Postconditions. </title> <booktitle> In CHI 92 Conference Proceedings. </booktitle> <address> (Monterey, CA, May 3-7). </address> <publisher> ACM, </publisher> <address> New York. </address> <year> 1992. </year> <pages> 189-194. </pages>
Reference-contexts: This data display facet of the model has not been fully realized in any UIMS [14]. It is this facet that Player attempts to address. The UIDE system also relies on preconditions and postcon-ditions for dialogue control [5] <ref> [8] </ref>, using them to determine when to enable application actions, and to support interface transformations. However, UIDE does not use planning to automatically sequence visual presentations. An exception to this is Sukaviriyas Cartoonist system, that extends the FIGURE 2. The Seeheim UIMS model (adapted from [14]).
Reference: 9. <author> Green, M. </author> <title> A Survey of Three Dialogue Models. </title> <journal> ACM Transactions on Graphics 5, </journal> <volume> 3. </volume> <month> (July </month> <year> 1986). </year> <pages> 244-275. </pages>
Reference-contexts: UIMSs have employed several different techniques for dialogue control. Green compares the state machine, grammar, and event-based approaches in <ref> [9] </ref>. However, these techniques have focused on interpreting sequences of input, producing graphics only incidentally in the process.
Reference: 10. <author> Hudson, S. and Stasko, J. T. </author> <title> Animation Support in a User Interface Toolkit: Flexible, Robust, and Reusable Abstractions. </title> <booktitle> In UIST 93 Proceedings. </booktitle> <address> (Atlanta, GA, </address> <month> November 3-5). </month> <booktitle> ACM, </booktitle> <address> New York. </address> <year> 1993. </year> <pages> 57-67. </pages>
Reference-contexts: Many researchers have addressed other aspects of incorporating animation into the interface. For example, PARCs Cognitive Coprocessor deals with timing issues [2]. Several interfaces incorporate traditional animation effects [3] <ref> [10] </ref>. Virtual reality research frequently coordinates simulation and novel interface devices with animation [13]. Researchers in artificial intelligence have studied planning issues extensively, and their work is summarized in several surveys [7] [6].
Reference: 11. <author> Koga, Y. et al. </author> <title> Planning Motions with Intentions. </title> <booktitle> In SIGGRAPH 94 Proceedings. </booktitle> <address> (Orlando, FL, July 24-29). </address> <publisher> ACM, </publisher> <address> New York. </address> <year> 1994. </year> <pages> 395-408. </pages>
Reference-contexts: Others have applied planning to computer animation. For example, Lengyel uses graphics hardware to accelerate path planning, and then animates his results [12]. Koga uses special purpose planning techniques to compute grasping animations <ref> [11] </ref>. None of this work exploits precompilation, nor is integrated in a UIMS. THE LANGUAGE This section describes the language used to express the desired runtime behavior of the animation controller. Presenting this language here serves two purposes. The language helps us communicate the capabilities of the controller.
Reference: 12. <author> Lengyel, J. et al. </author> <title> Real-Time Robot Motion Planning Using Rasterizing Computer Graphics Hardware. </title> <booktitle> In SIGGRAPH 90 Proceedings. </booktitle> <address> (Dallas, TX, </address> <month> August 6-10). </month> <booktitle> ACM, </booktitle> <address> New York. </address> <year> 1990. </year> <pages> 327-335. </pages>
Reference-contexts: Schoppers also precompiles plans, though his methods differ somewhat from ours and he applies his work to the robotics domain [15]. Others have applied planning to computer animation. For example, Lengyel uses graphics hardware to accelerate path planning, and then animates his results <ref> [12] </ref>. Koga uses special purpose planning techniques to compute grasping animations [11]. None of this work exploits precompilation, nor is integrated in a UIMS. THE LANGUAGE This section describes the language used to express the desired runtime behavior of the animation controller. Presenting this language here serves two purposes.
Reference: 13. <author> Lewis, J. B., Koved, L., and Ling, D. T. </author> <title> Dialogue Structures for Virtual Worlds. </title> <booktitle> In CHI 91 Proceedings. </booktitle> <address> (New Orleans, LA, April 27 - May 2). </address> <publisher> ACM, </publisher> <address> New York. </address> <year> 1991. </year> <pages> 131-136. </pages>
Reference-contexts: Many researchers have addressed other aspects of incorporating animation into the interface. For example, PARCs Cognitive Coprocessor deals with timing issues [2]. Several interfaces incorporate traditional animation effects [3] [10]. Virtual reality research frequently coordinates simulation and novel interface devices with animation <ref> [13] </ref>. Researchers in artificial intelligence have studied planning issues extensively, and their work is summarized in several surveys [7] [6]. The planning technique described here is a simple variant of goal regression, and is not intended to contribute to the planning literature.
Reference: 14. <author> Olsen, D. R. Jr. </author> <title> User Interface Management Systems: Models and Algorithms. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address> <year> 1992. </year>
Reference-contexts: The subsequent section explains Players planning algorithm. Additional implementation issues are described following this, and then the paper presents our conclusions and possible future directions. RELATED WORK One way to discuss UIMSs is in terms of the Seeheim model <ref> [14] </ref>, presented in Figure 2. The user interacts directly with the presentation component, which passes interaction events on to the dialogue control. The dialogue control component then determines which application services should be requested through the application interface model. <p> This data display facet of the model has not been fully realized in any UIMS <ref> [14] </ref>. It is this facet that Player attempts to address. The UIDE system also relies on preconditions and postcon-ditions for dialogue control [5] [8], using them to determine when to enable application actions, and to support interface transformations. However, UIDE does not use planning to automatically sequence visual presentations. <p> However, UIDE does not use planning to automatically sequence visual presentations. An exception to this is Sukaviriyas Cartoonist system, that extends the FIGURE 2. The Seeheim UIMS model (adapted from <ref> [14] </ref>). Application Interface Model Dialogue Control Presentation User 3 of 8 UIDE model to automatically provide contextsensitive help animations [16]. UIDE also uses planning to generate contextsensitive textual help [4].
Reference: 15. <author> Schoppers, M. J. </author> <title> Universal Plans for Reactive Robots in Unpredictable Environments. </title> <booktitle> In IJCAI 87 Conference Proceedings. </booktitle> <volume> Vol. </volume> <pages> 2. </pages> <address> (Milan, Italy, August 23-28). </address> <publisher> Mor-gan Kaufmann. </publisher> <pages> 1039-1046. </pages>
Reference-contexts: However, we do believe that this paper presents a novel application for planning technology. Our approach differs from traditional planning by precompiling all necessary plans prior to execution. Schoppers also precompiles plans, though his methods differ somewhat from ours and he applies his work to the robotics domain <ref> [15] </ref>. Others have applied planning to computer animation. For example, Lengyel uses graphics hardware to accelerate path planning, and then animates his results [12]. Koga uses special purpose planning techniques to compute grasping animations [11]. None of this work exploits precompilation, nor is integrated in a UIMS.
Reference: 16. <author> Sukaviriya, P. and Foley, J. D. </author> <title> Coupling a UI Framework with Automatic ContextSensitive Animated Help. </title> <booktitle> In UIST 90 Conference Proceedings. </booktitle> <address> (Snowbird, UT, October 3-5). </address> <publisher> ACM, </publisher> <address> New York. </address> <year> 1990. </year> <pages> 152-166. </pages>
Reference-contexts: However, UIDE does not use planning to automatically sequence visual presentations. An exception to this is Sukaviriyas Cartoonist system, that extends the FIGURE 2. The Seeheim UIMS model (adapted from [14]). Application Interface Model Dialogue Control Presentation User 3 of 8 UIDE model to automatically provide contextsensitive help animations <ref> [16] </ref>. UIDE also uses planning to generate contextsensitive textual help [4]. However, the UIDE work does not use planning to sequence general animation presentations, and does not employ precompilation techniques to maintain real-time constraints. Many researchers have addressed other aspects of incorporating animation into the interface.
References-found: 15

