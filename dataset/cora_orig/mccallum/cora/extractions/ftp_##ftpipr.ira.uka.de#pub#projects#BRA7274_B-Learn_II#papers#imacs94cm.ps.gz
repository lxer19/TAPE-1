URL: ftp://ftpipr.ira.uka.de/pub/projects/BRA7274_B-Learn_II/papers/imacs94cm.ps.gz
Refering-URL: http://wwwipr.ira.uka.de/projects/blearn/blearnpub.html
Root-URL: 
Title: Adaptive Control in Visual Sensing visual sensing problem, beyond any consideration from the computational point
Author: C.A.Moneta, F.G.B.De Natale, and G.Vernazza 
Keyword: Detection Module Extraction Module Processing Module Camera Module Scheduler Environment Higher Levels control data Pre Edge Edge  
Note: One of the most evident drawbacks concerned with the  The main goal of this work has then been to develop a  As the  has been considered, even if preliminary experiments have been done with the  
Address: Via All'Opera Pia 11A, I-16145 Genova, Italy  
Affiliation: Dept. of Biophysical and Electronic Engineering University of Genoa  
Abstract: The work this paper refers to concerns the conferment of learning capabilities to a visual sensing system. In order to describe the way neural structures have been employed to actively control the intrinsic parameters of a CCD sensor, a preface is drawn to introduce the overall underlying framework. Then, the obtained results are mentioned and a discussion is made. The visual sensing system dealt with in this paper has been developed in the context of a research 1 aimed at joining Robotics and Machine Learning. In particular, the main overall goal is to enhance a robotic architecture by means of both symbolic and subsymbolic learning capabilities. The application case is concerned with navigation and assembly tasks. The visual system deals with the management of the visual sensorial channel, ranging from the acquisition up to the symbolic description of the perceived scene, in terms of visual primitives. The developed multilevel architecture (Fig. A) consists of a chain of four processing Modules: the Camera, the PreProcessor, the Edge-Extractor, the EdgeDetector. Each one performs a specific data transformation on data received by the lower Module. The main problem is here to achieve an optimal regulation of both the physical (CCD camera) and virtual (Perona-Malik, Canny and Hough transformations) sensor, respectively associated to each Module. 
Abstract-found: 1
Intro-found: 1
Reference: [Vo88] <author> T.P.Vogl et al. </author> <title> "Accelerating the convergence of the BackPropagation method", </title> <journal> Biological Cybernetics, </journal> <volume> Vol. 59, </volume> <pages> pp 257-263, </pages> <year> 1988 </year>
Reference-contexts: i j i j l l , ( ) ( )t t = - + - e For what regards the camera regulation, four intrinsic parameters can be controlled by the ANN: where t stands for the iteration index. focus To speed up convergence, the following heuristic criteria are applied <ref> [Vo88] </ref>; the weights are updated aperture of the diaphragm electronic gain (sensitivity of the CCD layer) according to the above equation; if a global error turns out to be smaller than the previous one, then e = fe , with f &gt;&gt;1, and the weights are updated; otherwise, e = be
Reference: [To90] <author> T.Tollanaere "SuperSAB: </author> <title> fast adaptive Back Propagtion with good scaling properties", Neural Networks, </title> <booktitle> Vol.3, </booktitle> <pages> pp 561-573, </pages> <year> 1990 </year>
Reference-contexts: SYSTEM DESCRIPTION black-level (polarization of the CCD sensor). Concerning the offline training strategy, the error BackPropagation technique has been adopted. Two accelerated implementations have been both tested, which feature a good speedup in comparison with the basic algorithm ([Vo88], <ref> [To90] </ref>). As mentioned above, Artificial Neural Networks have been employed to control the regulation of the camera intrinsic parameters settings. To this aim, the structure depicted in Fig. B has been implemented.
Reference: [LCH89] <author> P.Liang, Y.L.Chang, </author> <title> and S.Hackwood "Adaptive Self-Calibration of Vision-Based Robot Systems", </title> <journal> IEEE Trans. on System, Man & Cybernetics, </journal> <volume> Vol. 19, </volume> <pages> pp. 811-824, </pages> <month> July/August </month> <year> 1989 </year>
Reference: [LU89] <editor> Z.Li and L.Uhr "Pyramid vision using key features to integrate imagedriven bottom-up and model-driven top-down processes", </editor> <booktitle> IEEE Trans. on System, Man & Cybernetics,pp. </booktitle> <pages> 422-428, </pages> <month> March/April </month> <year> 1989 </year> <month> Fig. </month> <title> E - Test image acquired wth random parameters </title>
References-found: 4

