URL: http://www.cs.utexas.edu/users/plapack/papers/SC97.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/plapack/new/pubs.html
Root-URL: 
Email: rvdg@cs.utexas.edu  rvdg@cs.utexas.edu  
Phone: (512) 471-9720  (512) 471-8885 (fax)  phone: (512) 471-9720, fax: (512) 471-8885,  
Title: PLAPACK: Parallel Linear Algebra Libraries Design Overview  on Invariant Subspace Methods (PRISM)  
Author: Philip Alpatov Greg Baker Carter Edwards John Gunnels Greg Morrow James Overfelt Robert van de Geijn yz Robert van de Geijn 
Address: Austin, TX 78712  Austin, TX 78712  Austin, Austin, TX 78712,  
Affiliation: Department of Computer Sciences The University of Texas at Austin  The University of Texas,  Department of Computer Sciences, The University of Texas at  
Note: An Extended Abstract Submitted to SC97 Corresponding Author:  (office)  This project was sponsored in part by the Parallel Research  project (ARPA grant P-95006), the NASA High Performance Computing and Communications Program's Earth and Space Sciences Project (NRA Grants NAG5-2497 and NAG5-2511), the Environmental Molecular Sciences construction project at Pacific Northwest National Laboratory (PNNL) (PNNL is a multiprogram national laboratory operated by Battelle Memorial Institute for the U.S. Department of Energy under Contract DE-AC06-76RLO 1830), and the Intel Research Council.  
Abstract: Over the last twenty years, dense linear algebra libraries have gone through three generations of public domain general purpose packages. In the seventies, the first generation of packages were EISPACK and LINPACK, which implemented a broad spectrum of algorithms for solving dense linear eigenproblems and dense linear systems. In the late eighties, the second generation package called LAPACK was developed. This package attains high performance in a portable fashion while also improving upon the functionality and robustness of LINPACK and EISPACK. Finally, Since the early nineties, an effort to port LAPACK to distributed memory networks of computers has been underway as part of the ScaLAPACK project. PLAPACK is a maturing fourth generation package which uses a new, more application-centric, view of vector and matrix distribution, Physically Based Matrix Distribution. It also uses an "MPI-like" programming interface that hides distribution and indexing details in opaque objects, provides a natural layering in the library, and provides a straight-forward application interface. In this paper, we give an overview of the design of PLAPACK. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Balay, W. Gropp, L. Curfman McInnes, and B. Smith, </author> <title> PETSc 2.0 users manual, </title> <type> Tech. Report ANL-95/11, </type> <institution> Argonne National Laboratory, </institution> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: To achieve this, we have adopted an "object based" approach to programming. This object based approach has already been popularized for high performance parallel computing by libraries like the Toolbox being developed at Mississippi State University [2], the PETSc library at Argonne National Laboratory <ref> [1] </ref>, and the Message-Passing Interface [11]. The PLAPACK infrastructure uses a data distribution that starts by partitioning the vectors associated with the linear algebra problem and assigning the sub-vectors to nodes. The matrix distribution is then induced by the distribution of these vectors.
Reference: [2] <author> P. Bangalore, A. Skjellum, C. Baldwin, and S. G. Smith, </author> <title> Dense and iterative concurrent linear algebra in the multicomputer toolbox, </title> <booktitle> in Proceedings of the Scalable Parallel Libraries Conference (SPLC '93) (1993), </booktitle> <pages> pp. 132-141. </pages>
Reference-contexts: To achieve this, we have adopted an "object based" approach to programming. This object based approach has already been popularized for high performance parallel computing by libraries like the Toolbox being developed at Mississippi State University <ref> [2] </ref>, the PETSc library at Argonne National Laboratory [1], and the Message-Passing Interface [11]. The PLAPACK infrastructure uses a data distribution that starts by partitioning the vectors associated with the linear algebra problem and assigning the sub-vectors to nodes.
Reference: [3] <author> J. Choi, J. J. Dongarra, R. Pozo, and D. W. Walker, </author> <title> Scalapack: A scalable linear algebra library for distributed memory concurrent computers, </title> <booktitle> in Proceedings of the Fourth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <publisher> IEEE Comput. Soc. Press (1992), </publisher> <pages> pp. 120-127. </pages>
Reference-contexts: However, the surprising discovery has been that this approach greatly simplifies the implementation of the infrastructure, allowing much more generality (in future extensions of the infrastructure) while reducing the amount of code required when compared to previous generation parallel dense linear algebra libraries <ref> [3] </ref>.
Reference: [4] <author> Almadena Chtchelkanova, John Gunnels, Greg Morrow, James Overfelt, and Robert A. van de Geijn, </author> <title> Block Parallel implementation of BLAS: General techniques for level 3 BLAS, in Concurrency: </title> <journal> Practice and Experience, </journal> <note> to appear. </note>
Reference: [5] <author> G. Baker, </author> <title> Application of Parallel Processing to Selected Problems in Satellite Geodesy, </title> <type> Ph.D. dissertation, </type> <institution> Department of Aerospace Engineering, The University of Texas at Austin, </institution> <note> in preparation. </note>
Reference-contexts: use of PLAPACK to implement a parallel reduction to banded form algorithm as part of the PRISM project [14], its use in solving an interface problem as part of a parallel multifrontal solver for an h-p adaptive finite element method [8], and its use in solving problems in satellite geodesy <ref> [5] </ref>. Acknowledgments We would like to thank Dr. Yuan-Jye (Jason) Wu for agreeing to be a one-man alpha-release test site for PLAPACK.
Reference: [6] <author> J. J. Dongarra, J. Du Croz, S. Hammarling, and I. Duff, </author> <title> A set of level 3 Basic Linear Algebra Subprograms, </title> <journal> ACM TOMS, </journal> <volume> 16(1) (1990), </volume> <pages> pp. 1-17. </pages>
Reference: [7] <author> H. Carter Edwards, MMPI: </author> <title> Asynchronous Message Management for the Message-Passing Interface, </title> <type> TICAM Report 96-44, </type> <institution> The University of Texas at Austin, </institution> <year> 1996. </year>
Reference-contexts: However, for portability reasons, such a mechanism itself must be implemented using only standard MPI. The Managed Message-Passing Interface (MMPI) developed by Carter Edwards at the University of Texas provides such a mechanism <ref> [7] </ref>.
Reference: [8] <author> H. Carter Edwards, </author> <title> A Parallel Infrastructure for Scalable Adaptive Finite Element Methods and its application to Least Squares C-infinity Collocation, </title> <type> Ph.D. dissertation, </type> <institution> Computational and Applied Mathematics Program, The University of Texas at Austin, </institution> <year> 1997. </year>
Reference-contexts: Examples include the use of PLAPACK to implement a parallel reduction to banded form algorithm as part of the PRISM project [14], its use in solving an interface problem as part of a parallel multifrontal solver for an h-p adaptive finite element method <ref> [8] </ref>, and its use in solving problems in satellite geodesy [5]. Acknowledgments We would like to thank Dr. Yuan-Jye (Jason) Wu for agreeing to be a one-man alpha-release test site for PLAPACK.
Reference: [9] <author> C. Edwards, P. Geng, A. Patra, and R. van de Geijn, </author> <title> Parallel matrix distributions: have we been doing it all wrong?, </title> <type> Tech. Report TR-95-40, </type> <institution> Dept of Computer Sciences, The University of Texas at Austin, </institution> <year> 1995. </year>
Reference: [10] <author> P. Mitra, D. Payne, L. Shuler, R. van de Geijn, and J. Watts, </author> <title> "Fast Collective Communication Libraries, Please," </title> <booktitle> Proceedings of the Intel Supercomputing Users' Group Meeting 1995. </booktitle>
Reference-contexts: In addition, often vendor implementations of collective communication are either inefficient or unpredictable in their cost. Predictability is important when trying to hybridize implementations for different situations. Such hybridizations are sometimes called poly-algorithms. Our own experience implementing 3 the Interprocessor Collective Communication Library (InterCom) <ref> [10] </ref> will allow us in the future more flexibility in minimizing the overhead from communication within PLAPACK. PLAPACK-memory management interface: PLAPACK heavily uses dynamic memory allocation to create space for storing data.
Reference: [11] <author> M. Snir, S. W. Otto, S. Huss-Lederman, D. W. Walker, and J. Dongarra, </author> <title> MPI: The Complete Reference, </title> <publisher> The MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: To achieve this, we have adopted an "object based" approach to programming. This object based approach has already been popularized for high performance parallel computing by libraries like the Toolbox being developed at Mississippi State University [2], the PETSc library at Argonne National Laboratory [1], and the Message-Passing Interface <ref> [11] </ref>. The PLAPACK infrastructure uses a data distribution that starts by partitioning the vectors associated with the linear algebra problem and assigning the sub-vectors to nodes. The matrix distribution is then induced by the distribution of these vectors.
Reference: [12] <author> Robert van de Geijn, </author> <title> Using PLAPACK: Parallel Linear Algebra Package, </title> <publisher> The MIT Press, </publisher> <year> 1997. </year>
Reference-contexts: Development of such implementations can often be attained by incrementally replacing calls to global BLAS with calls that explicitly expose parallelism by using objects that are duplicated in nature. For details, we refer the reader to <ref> [12] </ref>. 3.6 PLAPACK Application Interface A highlight of the PLAPACK infracture is the inclusion of a set of routines that allow an application to build matrices and vectors in an application friendly manner.
Reference: [13] <author> R. van de Geijn and J. Watts, SUMMA: </author> <title> Scalable universal matrix multiplication algorithm, </title> <note> in Concur-rency: Practice and Experience, to appear. </note>

References-found: 13

