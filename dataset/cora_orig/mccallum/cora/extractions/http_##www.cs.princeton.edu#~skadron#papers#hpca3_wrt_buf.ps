URL: http://www.cs.princeton.edu/~skadron/papers/hpca3_wrt_buf.ps
Refering-URL: http://www.cs.princeton.edu/~skadron/pub_list.html
Root-URL: http://www.cs.princeton.edu
Email: fskadron, dougg@cs.princeton.edu  
Title: Design Issues and Tradeoffs for Write Buffers  
Author: Kevin Skadron and Douglas W. Clark 
Address: Princeton University  
Affiliation: Department of Computer Science  
Abstract: Processors with write-through caches typically require a write buffer to hide the write latency to the next level of memory hierarchy and to reduce write traffic. A write buffer can cause processor stalls when it is full, when it contends with a cache miss for access to the next level of the hierarchy, and when it contains the freshest copy of data needed by a load. This paper uses instruction-level simulation of SPEC92 benchmarks to investigate how different write buffer depths, retirement policies, and load-hazard policies affect these three types of write-buffer stalls. Deeper buffers with adequate headroom, lazier retirement policies, and the ability to read data directly from the write buffer combine to substantially reduce write-buffer-induced stalls. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Anderson, H. Levy, B. Bershad, and E. Lazowska. </author> <title> The interaction of architecture and operating system design. </title> <booktitle> In Proceedings of the Fourth Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 108-20, </pages> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: Literature on write buffers typically appears as part of work focusing on other aspects of memory system design, and so little detailed analysis of write buffer issues is available. Anderson et al. <ref> [1] </ref>, Chen [5], and Nagle et al. [21] find that write buffers contribute significantly to memory stalls, but do not consider alternative buffer designs. Chen and Baer [6] and Chen and Somani [4] discuss the benefits of read-bypassing write buffers.
Reference: [2] <author> R. Bianchini, T. LeBlanc, and J. Veenstra. </author> <title> Eliminating useless messages in write-update protocols on scalable multiprocessors. </title> <type> Technical Report TR-539, </type> <institution> University of Rochester Department of Computer Science, </institution> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: A few papers focus more directly on write buffer issues. Smith [24] describes a queuing model for write-through and presents a number of trace-based statistics about write behavior and write buffer depth. Bianchini et al. <ref> [2] </ref>, Dahlgren and Stenstrom [8], Gharachorloo et al. [11], Mounes-Toussi and Lilja [19], and Veenstra and Fowler [27] discuss write buffers and write caches in the multiprocessor context. Finally, Chu and Gottipati [7] consider gross write buffer performance.
Reference: [3] <author> B. K. Bray and M. J. Flynn. </author> <title> Specialized Caches to Improve Data Access Performance. </title> <type> PhD thesis, </type> <institution> Stanford Computer Systems Laboratory, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: Experiments focus on depth, retirement policy, and load-hazard policy. it must evict one of its entries before writing that data to the next level. Bray and Flynn <ref> [3] </ref> also examine write caches. Goncalves and Appel [12] study the effects of cache write policy and write buffer configuration on their implementation of the ML language. They find performance of ML programs to be quite sensitive to these design choices.
Reference: [4] <author> C.-H. Chen and A. K. Somani. </author> <title> A unified architectural tradeoff methodology. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 348-57, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Anderson et al. [1], Chen [5], and Nagle et al. [21] find that write buffers contribute significantly to memory stalls, but do not consider alternative buffer designs. Chen and Baer [6] and Chen and Somani <ref> [4] </ref> discuss the benefits of read-bypassing write buffers. Jouppi [15] considers cache write policies and discusses some write-buffer issues, especially the policy for writing blocks to L2 (or main memory). He also considers a write cache, a write buffer organized as a small, fully associative cache with LRU replacement. <p> Our experiments retire entries based only on occupancy. The maximum rate at which the write buffer can retire entries depends on the write buffer's priority in arbitrating for L2, and on the write bandwidth at L2. Chen and Baer [6], Chen and So-mani <ref> [4] </ref>, and Hennessy and Patterson [22] all describe the benefits of read-bypassing write buffers: L1 load misses, which are more urgent, may bypass write buffer entries waiting to retire.
Reference: [5] <author> J. B. Chen. </author> <title> Memory behavior of an X11 window system. </title> <booktitle> In Proceedings of the USENIX Winter 1994 Technical Conference, </booktitle> <pages> pages 189-200, </pages> <year> 1994. </year>
Reference-contexts: Literature on write buffers typically appears as part of work focusing on other aspects of memory system design, and so little detailed analysis of write buffer issues is available. Anderson et al. [1], Chen <ref> [5] </ref>, and Nagle et al. [21] find that write buffers contribute significantly to memory stalls, but do not consider alternative buffer designs. Chen and Baer [6] and Chen and Somani [4] discuss the benefits of read-bypassing write buffers.
Reference: [6] <author> T.-F. Chen and J.-L. Baer. </author> <title> Reducing memory latency via non-blocking and prefetching caches. </title> <booktitle> In Proceedings of the Fifth Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 51-61, </pages> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: Anderson et al. [1], Chen [5], and Nagle et al. [21] find that write buffers contribute significantly to memory stalls, but do not consider alternative buffer designs. Chen and Baer <ref> [6] </ref> and Chen and Somani [4] discuss the benefits of read-bypassing write buffers. Jouppi [15] considers cache write policies and discusses some write-buffer issues, especially the policy for writing blocks to L2 (or main memory). <p> Our experiments retire entries based only on occupancy. The maximum rate at which the write buffer can retire entries depends on the write buffer's priority in arbitrating for L2, and on the write bandwidth at L2. Chen and Baer <ref> [6] </ref>, Chen and So-mani [4], and Hennessy and Patterson [22] all describe the benefits of read-bypassing write buffers: L1 load misses, which are more urgent, may bypass write buffer entries waiting to retire.
Reference: [7] <author> P. P. Chu and R. Gottipati. </author> <title> Write buffer design for on-chip cache. </title> <booktitle> In Proceedings of the International Conference on Computer Design, </booktitle> <pages> pages 311-16, </pages> <year> 1994. </year>
Reference-contexts: Bianchini et al. [2], Dahlgren and Stenstrom [8], Gharachorloo et al. [11], Mounes-Toussi and Lilja [19], and Veenstra and Fowler [27] discuss write buffers and write caches in the multiprocessor context. Finally, Chu and Gottipati <ref> [7] </ref> consider gross write buffer performance. <p> Flush-full flushes the entire write buffer when the miss hits in the buffer; the Alpha 21064 uses this policy [9]. Flush-partial, the 21164's policy [10], saves some work by flushing entries in FIFO order only as far as necessary to purge the hit entry. Flush-item-only, which Chu and Gottipati <ref> [7] </ref> suggest but do not study, saves even more work by flushing only the hit entry. If a different entry is already being retired when the load hazard occurs, we assume this transaction completes first. <p> By counting all stalls, we in effect measure the write buffer against a perfect buffer that never overflows and never delays loads. This represents a lower bound, the best performance a write buffer configuration can achieve. Chu and Got-tipati <ref> [7] </ref> also compare their results to an ideal buffer, but instead measure average cycles per memory reference.
Reference: [8] <author> F. Dahlgren and P. Stenstrom. </author> <title> Using write caches to improve performance of cache coherence protocols in shared-memory multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 26(2) </volume> <pages> 193-210, </pages> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: A few papers focus more directly on write buffer issues. Smith [24] describes a queuing model for write-through and presents a number of trace-based statistics about write behavior and write buffer depth. Bianchini et al. [2], Dahlgren and Stenstrom <ref> [8] </ref>, Gharachorloo et al. [11], Mounes-Toussi and Lilja [19], and Veenstra and Fowler [27] discuss write buffers and write caches in the multiprocessor context. Finally, Chu and Gottipati [7] consider gross write buffer performance.
Reference: [9] <author> Digital Semiconductor. </author> <title> DECchip 21064/21064A Alpha AXP Microprocessors: Hardware Reference Manual, </title> <month> Jun. </month> <year> 1994. </year>
Reference-contexts: Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966. L1s often use write-through [15]. The DEC Alpha 21064 <ref> [9] </ref>, 21164 [10], and the SUN UltraSPARC-I [13] all use small, on-chip, write-through, L1s. The large second-level (L2) cache usually uses write-back to minimize main memory traffic. Figure 1 shows a typical system with a small, on-chip, write-through L1, a coalescing write buffer, and an off-chip L2. <p> Retirement policy determines when to retire that entry. The Alpha 21064 and 21164 retire the oldest entry if 2 or more entries are valid. A lone entry in the write buffer may remain until it becomes too oldthis occurs after 256 cycles in the 21064 <ref> [9] </ref> and after 64 cycles in the 21164 [10]. Waiting until 2 or more entries are valid before retiring means that sequential writes can achieve maximal coalescing: the most recently allocated entry cannot be retired until a new entry is allocated. <p> With read-bypassing, L1 load misses can tie up L2 and delay write buffer retires, making overflow more likely if stores appear among the loads. The UltraSPARC-I [13] uses read-bypassing until the buffer becomes too full, at which point the write buffer gets priority for L2. Like the Alphas <ref> [9, 10] </ref>, we use straight read-bypassing and we assume that write transactions already underway to L2 cannot be interrupted. Read-bypassing write buffers raise two correctness issues. <p> When a load hazard does occur, a variety of options exist. We consider four load-hazard policies. Flush-full flushes the entire write buffer when the miss hits in the buffer; the Alpha 21064 uses this policy <ref> [9] </ref>. Flush-partial, the 21164's policy [10], saves some work by flushing entries in FIFO order only as far as necessary to purge the hit entry. Flush-item-only, which Chu and Gottipati [7] suggest but do not study, saves even more work by flushing only the hit entry. <p> As a baseline model, we choose a 4-deep, cache-line-wide (32B), read-bypassing write buffer that uses retire-at-2 and flush-full. This closely resembles the Alpha 21064's write buffer <ref> [9] </ref>, lacking only that system's policy of periodic retirement of old entries. The 21164 has a similar buffer that is 6 entries deep and uses flush-partial [10]. <p> Pipeline bubbles spread out stores, so that the write buffer sees a lower store rate and is less likely to overflow. * We assume datapaths that are a cache line wide, but current machines like the Alphas <ref> [9, 10] </ref>, the UltraSPARC [13], and others have datapaths only half a cache line wide.
Reference: [10] <author> Digital Semiconductor. </author> <title> Alpha 21164 Microprocessor: Hardware Reference Manual, </title> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966. L1s often use write-through [15]. The DEC Alpha 21064 [9], 21164 <ref> [10] </ref>, and the SUN UltraSPARC-I [13] all use small, on-chip, write-through, L1s. The large second-level (L2) cache usually uses write-back to minimize main memory traffic. Figure 1 shows a typical system with a small, on-chip, write-through L1, a coalescing write buffer, and an off-chip L2. <p> Hits in L1 take a single cycle. Write misses, which as mentioned do not allocate in L1, also take a single cycle unless the write buffer is full. L2 has a fixed latency of 6 cycles, similar to the Alpha 21164 <ref> [10] </ref> and SUN UltraSPARC [13]. In the absence of contention, then, L1 load misses take a total of 7 cycles (1 + 6) and writing a write-buffer block to L2 takes 6 cycles, regardless of whether the entry being written is full or not. <p> The Alpha 21064 and 21164 retire the oldest entry if 2 or more entries are valid. A lone entry in the write buffer may remain until it becomes too oldthis occurs after 256 cycles in the 21064 [9] and after 64 cycles in the 21164 <ref> [10] </ref>. Waiting until 2 or more entries are valid before retiring means that sequential writes can achieve maximal coalescing: the most recently allocated entry cannot be retired until a new entry is allocated. <p> With read-bypassing, L1 load misses can tie up L2 and delay write buffer retires, making overflow more likely if stores appear among the loads. The UltraSPARC-I [13] uses read-bypassing until the buffer becomes too full, at which point the write buffer gets priority for L2. Like the Alphas <ref> [9, 10] </ref>, we use straight read-bypassing and we assume that write transactions already underway to L2 cannot be interrupted. Read-bypassing write buffers raise two correctness issues. <p> When a load hazard does occur, a variety of options exist. We consider four load-hazard policies. Flush-full flushes the entire write buffer when the miss hits in the buffer; the Alpha 21064 uses this policy [9]. Flush-partial, the 21164's policy <ref> [10] </ref>, saves some work by flushing entries in FIFO order only as far as necessary to purge the hit entry. Flush-item-only, which Chu and Gottipati [7] suggest but do not study, saves even more work by flushing only the hit entry. <p> This closely resembles the Alpha 21064's write buffer [9], lacking only that system's policy of periodic retirement of old entries. The 21164 has a similar buffer that is 6 entries deep and uses flush-partial <ref> [10] </ref>. Table 2 summarizes some write buffer param eters and our baseline model. 3 2.3 Write-Buffer-Induced Stalls Three types of stalls can be blamed on the write buffer. <p> may be an option, but a simple 6-deep or 8-deep, flush-full buffer using retire-at-2 may be better. 4 Cache effects We next look at the sensitivity of write-buffer performance to some size and latency parameters of the memory hierarchy components. 4.1 L1 Caches Several current machinesfor example, the Alpha 21164 <ref> [10] </ref> and the Pentium Pro [14]have 8K, on-chip L1s like our model, but future machines will probably use larger caches. Already, the SUN UltraSPARC [13] has a 16K cache, and the MIPS R10000 [18] a 32K cache. <p> Pipeline bubbles spread out stores, so that the write buffer sees a lower store rate and is less likely to overflow. * We assume datapaths that are a cache line wide, but current machines like the Alphas <ref> [9, 10] </ref>, the UltraSPARC [13], and others have datapaths only half a cache line wide. <p> The resulting increase in contention at L2 delays write buffer retirements. This makes write buffer overflows more likely and increases the duration of load-hazards. * Current machines have varying degrees of superscalarness. The Alpha 21164 <ref> [10] </ref>, for example, can issue 4 instructions per cycle. All else being equal, as issue width increases, store density increases. Write-buffer-induced stalls rise as a result. * A final effect we did not examine is the possible extra cost under read-from-WB of loads that hit the write buffer.
Reference: [11] <author> K. Gharachorloo, A. Gupta, and J. L. Hennessy. </author> <title> Performance evaluation of memory consistency models for shared-memory multiprocessors. </title> <booktitle> In Proceedings of the Fourth Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 245-57, </pages> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: A few papers focus more directly on write buffer issues. Smith [24] describes a queuing model for write-through and presents a number of trace-based statistics about write behavior and write buffer depth. Bianchini et al. [2], Dahlgren and Stenstrom [8], Gharachorloo et al. <ref> [11] </ref>, Mounes-Toussi and Lilja [19], and Veenstra and Fowler [27] discuss write buffers and write caches in the multiprocessor context. Finally, Chu and Gottipati [7] consider gross write buffer performance.
Reference: [12] <author> M. J. R. Goncalves and A. W. Appel. </author> <title> Cache performance of fast-allocating programs. </title> <booktitle> In Proceedings of the Seventh International Conference on Functional Programming and Computer Architecture, </booktitle> <pages> pages 293-305, </pages> <month> Jun. </month> <year> 1995. </year>
Reference-contexts: Experiments focus on depth, retirement policy, and load-hazard policy. it must evict one of its entries before writing that data to the next level. Bray and Flynn [3] also examine write caches. Goncalves and Appel <ref> [12] </ref> study the effects of cache write policy and write buffer configuration on their implementation of the ML language. They find performance of ML programs to be quite sensitive to these design choices. Mowry [20] briefly considers combining a write buffer with a prefetch buffer, but finds little benefit.
Reference: [13] <author> D. Greenley et al. </author> <title> UltraSPARC (T M) : The next generation superscalar 64-bit SPARC. </title> <booktitle> In Proceedings of CompCon '95, </booktitle> <pages> pages 442-50, </pages> <month> Mar. </month> <year> 1995. </year>
Reference-contexts: Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966. L1s often use write-through [15]. The DEC Alpha 21064 [9], 21164 [10], and the SUN UltraSPARC-I <ref> [13] </ref> all use small, on-chip, write-through, L1s. The large second-level (L2) cache usually uses write-back to minimize main memory traffic. Figure 1 shows a typical system with a small, on-chip, write-through L1, a coalescing write buffer, and an off-chip L2. <p> Hits in L1 take a single cycle. Write misses, which as mentioned do not allocate in L1, also take a single cycle unless the write buffer is full. L2 has a fixed latency of 6 cycles, similar to the Alpha 21164 [10] and SUN UltraSPARC <ref> [13] </ref>. In the absence of contention, then, L1 load misses take a total of 7 cycles (1 + 6) and writing a write-buffer block to L2 takes 6 cycles, regardless of whether the entry being written is full or not. Table 1 summarizes our machine model and the assumed parameters. <p> With read-bypassing, L1 load misses can tie up L2 and delay write buffer retires, making overflow more likely if stores appear among the loads. The UltraSPARC-I <ref> [13] </ref> uses read-bypassing until the buffer becomes too full, at which point the write buffer gets priority for L2. Like the Alphas [9, 10], we use straight read-bypassing and we assume that write transactions already underway to L2 cannot be interrupted. Read-bypassing write buffers raise two correctness issues. <p> Already, the SUN UltraSPARC <ref> [13] </ref> has a 16K cache, and the MIPS R10000 [18] a 32K cache. What should happen to the write buffer as L1 size increases? We can identify three effects: 1. <p> Pipeline bubbles spread out stores, so that the write buffer sees a lower store rate and is less likely to overflow. * We assume datapaths that are a cache line wide, but current machines like the Alphas [9, 10], the UltraSPARC <ref> [13] </ref>, and others have datapaths only half a cache line wide. Narrower datapaths mean that write buffer retirements and flushes take longer, increasing all three types of stalls. * When caches are non-blocking, L2 read-access and load-hazard stalls can be overlapped with other computation, reducing their impact on processor performance.
Reference: [14] <author> L. Gwennap. </author> <title> Intel's P6 uses decoupled superscalar design. </title> <type> Microprocessor Report, </type> <pages> pages 9-15, </pages> <month> Feb. 16, </month> <year> 1995. </year>
Reference: [15] <author> N. P. Jouppi. </author> <title> Cache write policies and performance. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 191-201, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966. L1s often use write-through <ref> [15] </ref>. The DEC Alpha 21064 [9], 21164 [10], and the SUN UltraSPARC-I [13] all use small, on-chip, write-through, L1s. The large second-level (L2) cache usually uses write-back to minimize main memory traffic. <p> Anderson et al. [1], Chen [5], and Nagle et al. [21] find that write buffers contribute significantly to memory stalls, but do not consider alternative buffer designs. Chen and Baer [6] and Chen and Somani [4] discuss the benefits of read-bypassing write buffers. Jouppi <ref> [15] </ref> considers cache write policies and discusses some write-buffer issues, especially the policy for writing blocks to L2 (or main memory). He also considers a write cache, a write buffer organized as a small, fully associative cache with LRU replacement. <p> Given a sufficiently deep buffer, retirement can be lazier (or less eager): a 12-deep or 16-deep buffer, for example, could implement retire-at-4, retire-at-8, etc. Lazier retirement keeps entries in the write buffer longer to allow more opportunities for coalescing. Retirement policy need not be based on occupancy; Jouppi <ref> [15] </ref> considers a buffer that retires entries at a fixed rate. He finds that in order to avoid overflow, an 8-deep, cache-line-wide buffer must retire entries at a rate too rapid to achieve much coalescing.
Reference: [16] <author> A. R. Lebeck and D. A. Wood. </author> <title> Cache profiling and the SPEC benchmarks: A case study. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 15-26, </pages> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: A loop interchange for gmtry and an array transposition for cholsky solve the problemsee Table 6. In fact, the new versions suffer almost no write-buffer-induced stalls under the baseline model. In <ref> [16] </ref>, Lebeck and Wood describe some general techniques for improving cache behavior, such as loop interchange and loop fusion. In addition to the above improvements to gmtry and chol-sky, they obtain good speedups for several other SPEC92 benchmarks, notably tomcatv. <p> But they also point out that tuning cache performance is difficult. Most programs are not as easily analyzed as the NASA kernelsfinding opportunities for improvement often requires cache profiling <ref> [16, 17] </ref>, and even then conflict misses can make cache behavior vary from input to input. We therefore focus on simple changes to the write buffer itself, and not on compiler techniques or application-specific modifications. <p> transformations Benchmark L1 cache WB L1 cache WB hit rate hit rate hit rate hit rate gmtry 43.2% 9.8% 88.5% 72.2% cholsky 48.8% 32.3% 82.1% 73.5% Table 6: L1 cache and write buffer performance of two NASA kernels, before and after transformations to achieve column-major array traversal in inner loops <ref> [16] </ref>. shows buffer-full stall cycles. The deeper the buffer, the more room for bursts of stores. For most benchmarks, buffer-full stalls cycles fall below 0.2% by the time a depth of 8 is reached. Wave5 requires 10 entries to reach that level.
Reference: [17] <author> M. Martonosi, A. Gupta, , and T. E. Anderson. </author> <title> Tuning memory performance in sequential and parallel programs. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 32-40, </pages> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: But they also point out that tuning cache performance is difficult. Most programs are not as easily analyzed as the NASA kernelsfinding opportunities for improvement often requires cache profiling <ref> [16, 17] </ref>, and even then conflict misses can make cache behavior vary from input to input. We therefore focus on simple changes to the write buffer itself, and not on compiler techniques or application-specific modifications.
Reference: [18] <institution> MIPS Technologies. </institution> <note> MIPS R10000 Microprocessor User's Manual, </note> <month> Jun. </month> <year> 1995. </year> <note> Version 1.0. </note>
Reference-contexts: Already, the SUN UltraSPARC [13] has a 16K cache, and the MIPS R10000 <ref> [18] </ref> a 32K cache. What should happen to the write buffer as L1 size increases? We can identify three effects: 1. Fewer loads miss in L1, so fewer load misses hit in the write buffer and fewer blocks are flushed to L2.
Reference: [19] <author> F. Mounes-Toussi and D. J. Lilja. </author> <title> Write buffer design for cache-coherent shared-memory multiprocessors. </title> <booktitle> In Proceedings of the International Conference on Computer Design, </booktitle> <pages> pages 506-11, </pages> <month> Oct. </month> <year> 1995. </year>
Reference-contexts: A few papers focus more directly on write buffer issues. Smith [24] describes a queuing model for write-through and presents a number of trace-based statistics about write behavior and write buffer depth. Bianchini et al. [2], Dahlgren and Stenstrom [8], Gharachorloo et al. [11], Mounes-Toussi and Lilja <ref> [19] </ref>, and Veenstra and Fowler [27] discuss write buffers and write caches in the multiprocessor context. Finally, Chu and Gottipati [7] consider gross write buffer performance.
Reference: [20] <author> T. C. Mowry. </author> <title> Tolerating Latency Through Software-Controlled Data Prefetching. </title> <type> PhD thesis, </type> <institution> Stanford Computer Systems Laboratory, </institution> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: Bray and Flynn [3] also examine write caches. Goncalves and Appel [12] study the effects of cache write policy and write buffer configuration on their implementation of the ML language. They find performance of ML programs to be quite sensitive to these design choices. Mowry <ref> [20] </ref> briefly considers combining a write buffer with a prefetch buffer, but finds little benefit. A few papers focus more directly on write buffer issues. Smith [24] describes a queuing model for write-through and presents a number of trace-based statistics about write behavior and write buffer depth.
Reference: [21] <author> D. Nagle, R. Uhlig, T. Mudge, and S. Sechrest. </author> <title> Optimal allocation of on-chip memory for multiple-API operating systems. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 358-69, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Literature on write buffers typically appears as part of work focusing on other aspects of memory system design, and so little detailed analysis of write buffer issues is available. Anderson et al. [1], Chen [5], and Nagle et al. <ref> [21] </ref> find that write buffers contribute significantly to memory stalls, but do not consider alternative buffer designs. Chen and Baer [6] and Chen and Somani [4] discuss the benefits of read-bypassing write buffers.
Reference: [22] <author> D. A. Patterson and J. L. Hennessy. </author> <booktitle> Computer Architecture: </booktitle>
Reference-contexts: Our experiments retire entries based only on occupancy. The maximum rate at which the write buffer can retire entries depends on the write buffer's priority in arbitrating for L2, and on the write bandwidth at L2. Chen and Baer [6], Chen and So-mani [4], and Hennessy and Patterson <ref> [22] </ref> all describe the benefits of read-bypassing write buffers: L1 load misses, which are more urgent, may bypass write buffer entries waiting to retire. With read-bypassing, L1 load misses can tie up L2 and delay write buffer retires, making overflow more likely if stores appear among the loads.
References-found: 22

