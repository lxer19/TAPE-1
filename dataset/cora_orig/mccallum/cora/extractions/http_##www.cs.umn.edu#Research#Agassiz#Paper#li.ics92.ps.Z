URL: http://www.cs.umn.edu/Research/Agassiz/Paper/li.ics92.ps.Z
Refering-URL: http://www.cs.umn.edu/Research/Agassiz/agassiz_pubs.html
Root-URL: http://www.cs.umn.edu
Email: li@cs.umn.edu  
Title: Array Privatization for Parallel Execution of Loops  
Author: Zhiyuan Li 
Address: 200 Union St. SE, Minneapolis, Minnesota 55455  
Affiliation: Department of Computer Science University of Minnesota  
Abstract: In recent experiments, array privatization played a critical role in successful parallelization of several real programs. This paper presents compiler algorithms for the program analysis for this transformation. The paper also addresses issues in the implementation. 
Abstract-found: 1
Intro-found: 1
Reference: [ABC + 88] <author> F. Allen, M. Burke, R. Cytron, J. Ferrante, W. Hsieh, and V. Sarkar. </author> <title> A framework for determining useful parallelism. </title> <booktitle> In Proc. of the 1988 ACM Int'l Conf. on Supercomputing, </booktitle> <pages> pages 207-215, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: In this paper, we discuss compiler algorithms for automatic array privatization. This technique may be viewed as an extension of scalar privatization which has been used in parallelizing compilers <ref> [ABC + 88] </ref>, [CF87]. We may call both techniques variable privatization. If a variable is modified in different iterations of a loop, writing conflicts result when the iterations are executed by multiple processors. Variable privatization removes such conflicts by allocating in each processor a private storage for the offending variable.
Reference: [All70] <author> F. E. Allen. </author> <title> Control flow analysis. </title> <journal> ACM SIG-PLAN Notices, </journal> <volume> 5(7) </volume> <pages> 1-19, </pages> <year> 1970. </year>
Reference-contexts: The remaining statements in L d are grouped into basic blocks in the same way as in the conventional control flow graphs. Each basic block is represented by a regular node in G d . A condensed node is different from an interval <ref> [All70] </ref>, [Coc70] in that only an indexed DO loop may be condensed. A loop of any other type is decomposed into basic blocks and is represented by a number of regular nodes.
Reference: [ASU86] <author> A.V. Aho, R. Sethi, and J.D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1986. </year>
Reference-contexts: Definition If hP 2 ; P 1 i is an edge in a control flow graph G such that P 1 dominates P 2 , hP 2 ; P 1 i is called a back edge in G <ref> [ASU86] </ref>. Claim 4 If P 1 dominates P 2 in a condensed control flow graph G of the body of a DO loop at level d, then C OU T P 1 hi 1 ;i 2 ;:::;i d i . <p> Our traversal algorithm (Figure 6) first prunes G according to Claim 5 and then propagates cover sets through the pruned graph. If G is reducible <ref> [ASU86] </ref>, the pruned graph will be acyclic, in which each node needs to be visited only once. If G is irreducible, the traversal algorithm partitions the pruned graph into maximum strong components (MSC) and then visits the MSCs in a topsort order.
Reference: [Ban88] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: Similarly, in Figure 1 (b), w 1 is an SDD in the outer loop, but not in the inner loop. SDDs can be recognized by the well-understood analysis of output dependences <ref> [Ban88] </ref>. Definition An SDD w is privatizable in loop L iff the value generated by w is not used across the iterations of L. In Figure 1 (a), both w 1 and w 2 are privatizable in the outer loop. In Figure 1 (b), w 1 is not privatizable. <p> This is a typical data dependence test problem 1 They may overlap in an outer loop. 2 and has been studied extensively <ref> [Ban88] </ref>. Hence, we need to examine Criterion A only. Once the privatizable arrays are identified, the compiler can prune the data dependence graph. All loop carried dependences whose sources are SDDs can be eliminated. Of course, other transformations such as scalar privatization can also contribute to the graph pruning.
Reference: [BT82] <author> T. Belytschko and C. S. Tsay. Whamse: </author> <title> A program for three-dimensional nonlinear nonlinear structural dynamics. </title> <type> Tech. </type> <institution> Rept. No. NP-2250, Dept. of Civil Engin., Northwestern Univ, </institution> <address> Evanston, IL, </address> <month> Feb. </month> <year> 1982. </year>
Reference-contexts: We have improved other application programs through array privatization, although we have yet to compile the result data to show the effect. These programs include ADM and SPEC77 in the Perfect program suite fl This work is supported by the Graduate Collage of University of Minnesota. [PER89] and WHAMS3D <ref> [BT82] </ref>, a program for structural analysis. In this paper, we discuss compiler algorithms for automatic array privatization. This technique may be viewed as an extension of scalar privatization which has been used in parallelizing compilers [ABC + 88], [CF87]. We may call both techniques variable privatization.
Reference: [CF87] <author> R. Cytron and J. Ferrante. </author> <title> What's in a name? or the value of renaming for parallelism detection and storage allocation. </title> <booktitle> In Proc. of the 1987 Int'l Conf. on Parallel Processing, </booktitle> <pages> pages 19-27, </pages> <month> August </month> <year> 1987. </year>
Reference-contexts: In this paper, we discuss compiler algorithms for automatic array privatization. This technique may be viewed as an extension of scalar privatization which has been used in parallelizing compilers [ABC + 88], <ref> [CF87] </ref>. We may call both techniques variable privatization. If a variable is modified in different iterations of a loop, writing conflicts result when the iterations are executed by multiple processors. Variable privatization removes such conflicts by allocating in each processor a private storage for the offending variable.
Reference: [Coc70] <author> J. Cocke. </author> <title> Global common subexpression elimination. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 5(7) </volume> <pages> 20-24, </pages> <year> 1970. </year>
Reference-contexts: The remaining statements in L d are grouped into basic blocks in the same way as in the conventional control flow graphs. Each basic block is represented by a regular node in G d . A condensed node is different from an interval [All70], <ref> [Coc70] </ref> in that only an indexed DO loop may be condensed. A loop of any other type is decomposed into basic blocks and is represented by a number of regular nodes.
Reference: [EB91] <author> R. Eigenmann and W. Blume. </author> <title> An effectiveness study of parallelizing compiler techniques. </title> <booktitle> In Proc. of the Int'l Conf. on Parallel Processing, </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: For a long time, many researchers have pursued the goal of automatic transformation of sequential programs into parallel machine code. Unfortunately, the result has been unsatisfactory. Many transformation techniques used in existing compilers do not prove to be effective in practice <ref> [EB91] </ref>, mainly because they handle relatively simple cases. On the other hand, recent experiments show significant results by solving more complex cases, using hand-performed new analyses and transformations [EHJ + 91], [EHLP91].
Reference: [EHJ + 91] <author> R. Eigenmann, J. Hoeflinger, G. Jaxon, Z. Li, and D. Padua. </author> <title> Restructuring fortran programs for cedar. </title> <booktitle> In Proc. of the 1991 Int'l Conf. on Parallel Processing, </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: Unfortunately, the result has been unsatisfactory. Many transformation techniques used in existing compilers do not prove to be effective in practice [EB91], mainly because they handle relatively simple cases. On the other hand, recent experiments show significant results by solving more complex cases, using hand-performed new analyses and transformations <ref> [EHJ + 91] </ref>, [EHLP91]. A technique called array privatization, along with other techniques, improves the performance of all four programs reported in [EHLP91]. Tables 1 and 2 summarize the effect of array privatization on those programs.
Reference: [EHLP91] <author> R. Eigenmann, J. Hoeflinger, Z. Li, and D. Padua. </author> <title> Experience in the automatic paral-lelization of four perfect-benchmark programs. In Proc. of the 4-th Workshop on Languages and Compilers for Parallel Computing. </title> <note> also available as CSRD Tech. Rept No. 1114, </note> <institution> Univ. of Illinois at Urbana-Champaign, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: Many transformation techniques used in existing compilers do not prove to be effective in practice [EB91], mainly because they handle relatively simple cases. On the other hand, recent experiments show significant results by solving more complex cases, using hand-performed new analyses and transformations [EHJ + 91], <ref> [EHLP91] </ref>. A technique called array privatization, along with other techniques, improves the performance of all four programs reported in [EHLP91]. Tables 1 and 2 summarize the effect of array privatization on those programs. <p> On the other hand, recent experiments show significant results by solving more complex cases, using hand-performed new analyses and transformations [EHJ + 91], <ref> [EHLP91] </ref>. A technique called array privatization, along with other techniques, improves the performance of all four programs reported in [EHLP91]. Tables 1 and 2 summarize the effect of array privatization on those programs. The fifth column of Table 2 shows the percentage of the sequential execution time (over the whole program) of each loop which is made parallel after array pri-vatization. The percentage indicates the significance of each loop. <p> If either of the criteria is broken, the compiler has to analyze array reaching definitions fully and exactly, which is extremely time-consuming. On the other hand, it seems unlikely in practice that a better performance could result by breaking the two criteria. In the experiment reported in <ref> [EHLP91] </ref>, the privatized arrays meet both the above criteria. To check whether an array A meets Criterion B in a DO loop, we intersect the set of the A elements defined by SDDs and the set of the A elements defined by NS-DDs. <p> The group performed hand analysis and transformation on several programs, which led to the cited paper <ref> [EHLP91] </ref> and eventually motivated the author to pursue the study in this paper. The practical cases cited here constitute only a small part in that tremendous effort. The author takes the sole responsibility for any potential errors in the description of those cases.
Reference: [Fea88] <author> P Feautrier. </author> <title> Array expansion. </title> <booktitle> In Proc. of the 1988 ACM Int'l Conf. on Supercomputing, </booktitle> <pages> pages 429-441, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: Nonetheless, we have shown that the results in data dependence analysis are useful for identification of self dependent definitions and for verification of Criterion B. Our work has been inspired by recent related work. Feautrier studies the technique of array expansion <ref> [Fea88] </ref>. His method expands arrays to higher dimensions to allow a program transformed into a single assignment form. He considers programs which have DO loops, assignment statements and conditional assignment statements. Gross and Steenkiste devise a dataflow analysis on arrays [GS90].
Reference: [GS90] <author> T. Gross and P Steenkiste. </author> <title> Structured dataflow analysis for arrays and its use in an optimizing compiler. </title> <journal> Software Practice and Experience, </journal> <volume> 20(2) </volume> <pages> 133-155, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: Feautrier studies the technique of array expansion [Fea88]. His method expands arrays to higher dimensions to allow a program transformed into a single assignment form. He considers programs which have DO loops, assignment statements and conditional assignment statements. Gross and Steenkiste devise a dataflow analysis on arrays <ref> [GS90] </ref>. Their algorithm examines whether a definition reaches a use in the same loop iteration and whether it reaches the end of the loop body. Our work focuses on the information required to determine array privatizability.
Reference: [Li92] <author> Z. Li. </author> <title> Array privatization: A loop transformation for parallel execution. </title> <type> Tech. </type> <institution> Rept. No. 9226, Dept. of Computer Science, Univ. of Minnesota, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: If they are, we do not privatize A. If they are not, then we mark A as a candidate for privatiza-tion. The copy-out code will be described in the next subsection. The algorithms for the live array analysis will be briefly discussed in Section 4. Reference <ref> [Li92] </ref> discusses algorithms for determining LDIs. 2.3 Storage allocation The storages are allocated as follows. For each parallel loop, the compiler checks to see whether it immediately contains any self dependent definitions (SDDs) of a candidate for privatization. <p> Note that a cover set, like other types of sets in this paper, often has loop index variables as its parameters. If we assign different values to the index variables, we obtain different sets of array elements. Set representation is briefly discussed in <ref> [Li92] </ref>. Let U S ff denote the array elements used by S ff . <p> Moreover, we perform the live array analysis for the privatizable arrays only. We describe one scheme for the live array analysis in this paper. Two other schemes are described in <ref> [Li92] </ref>. The UEE scheme Consider privatizable SDDs of array A in loop L d+1 at loop level d + 1. L d+1 is represented by a condensed node in G d , where G d is the condensed flow graph at the d-th level as defined in Section 3. <p> The time for each symbolic set operation depends on the complexity of array references. Set representations are discussed in <ref> [Li92] </ref>. Due to lack of space, we do not explore further. In the loops listed in Table 2, array references are quite simple. Set operations are therefore quite simple. However, the compiler does need machineries that can manipulate symbolic expressions in the subscripts. Interprocedural analysis is also often required.
Reference: [LT88] <author> A. Lichnewsky and F. Thomasset. </author> <title> Introducing symbolic problem solving techniques in the dependence testing phases of a vectorizer. </title> <booktitle> In Proc. of the 1988 Int'l Conf. on Supercomputing, </booktitle> <pages> pages 396-405, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: In the case of OCEAN, the conditions are in the form of "if ( RS (k) &gt; CUT2 )", which involves arrays as well. Compiler mechanisms that deal with scalar relations do exist <ref> [LT88] </ref>, [TIF86]. But mechanisms dealing with arrays are yet to be devised. Nonetheless, to separate two different issues, we do not examine the contents of IF conditions in this paper. As a result, we must make the assumption that one branch decision will not affect another.
Reference: [PER89] <author> M. berry et al. </author> <title> the PERFECT club benchmarks: Effective performance evaluation of supercomputers. </title> <journal> International Journal of Supercomputing Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <year> 1989. </year>
Reference-contexts: We have improved other application programs through array privatization, although we have yet to compile the result data to show the effect. These programs include ADM and SPEC77 in the Perfect program suite fl This work is supported by the Graduate Collage of University of Minnesota. <ref> [PER89] </ref> and WHAMS3D [BT82], a program for structural analysis. In this paper, we discuss compiler algorithms for automatic array privatization. This technique may be viewed as an extension of scalar privatization which has been used in parallelizing compilers [ABC + 88], [CF87]. We may call both techniques variable privatization. <p> The hierarchy among the 3 homes for the same array can be represented by a tree whose root is the scope of the original array. In the programs in Table 2, all privatized SDDs have one home only. Nonetheless, in program SPEC77 in the Perfect code suite <ref> [PER89] </ref>, some SDDs have two homes, one of which being the scope of the original array. For each array use, the compiler must determine the storage from which it reads. This depends on which array elements are used.
Reference: [Ros77] <author> B. K. Rosen. </author> <title> High-level data flow analysis. </title> <journal> Communication of the ACM, </journal> 20(10) 712-724, 1977. 
Reference-contexts: We are currently building an interprocedural analyzer to experiment on aggressive transformations, including array privatization. We adopt a hierarchical approach to dealing with complexity of data flows in large programs. (A similar approach is proposed by Rosen in <ref> [Ros77] </ref> for scalar analysis.) This approach takes advantage of structures in high level languages, which makes data flow analysis and dependence analysis efficient. Array privatization fits naturally in this approach, because a condensed node is just one example of using structures.
Reference: [TIF86] <author> R. Triolet, F. Irigoin, and P Feautrier. </author> <title> Direct parallelization of CALL statements. </title> <booktitle> In Proc. of SIGPLAN '86 Symp. Compiler Construction, </booktitle> <pages> pages 176-185, </pages> <month> July </month> <year> 1986. </year> <month> 10 </month>
Reference-contexts: In the case of OCEAN, the conditions are in the form of "if ( RS (k) &gt; CUT2 )", which involves arrays as well. Compiler mechanisms that deal with scalar relations do exist [LT88], <ref> [TIF86] </ref>. But mechanisms dealing with arrays are yet to be devised. Nonetheless, to separate two different issues, we do not examine the contents of IF conditions in this paper. As a result, we must make the assumption that one branch decision will not affect another.
References-found: 17

