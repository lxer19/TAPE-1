URL: ftp://www.cs.rutgers.edu/pub/technical-reports/hpcd-tr-26.ps.Z
Refering-URL: http://www.cs.rutgers.edu/pub/technical-reports/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Tree codes for vortex dynamics: Application of a programming framework  
Author: Sandeep Bhatt Pangfeng Liu Victor Fernandez Norman Zabusky 
Address: Morristown NJ 07960, and  Piscataway NJ 08855.  Piscataway NJ 08855.  
Affiliation: Bell Communications Research,  Computer Science, Rutgers University,  Mechanical and Aerospace Engineering, Rutgers University,  
Abstract: This paper describes the implementation of a fast N -body algorithm for the study of multi-filament vortex simulations. The simulations involve distributions that are irregular and time-varying. We adapt our programming framework which was earlier used to develop high-performance implementations of the Barnes-Hut and Greengard-Rokhlin algorithms for gravitational fields. We describe how the additional subtleties involved in vortex filament simulations are accommodated efficiently within the framework. We describe the ongoing experimental studies and report preliminary results with simulations of one million particles on the 128-node Connection Machine CM-5. The implementation sustains a rate of almost 30% of the peak machine rate. These simulations are more than an order of magnitude larger in size than previously studied using direct methods. We expect that the use of the fast algorithm will allow us to study the generation of small scales in the vortex collapse and reconnection problem with adequate resolution. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. S. Almgren, T. Buttke, and P. Colella. </author> <title> A fast adaptive vortex method in three dimensions. </title> <journal> Journal of Computational Physics, </journal> <volume> 113 </volume> <pages> 177-200, </pages> <year> 1994. </year>
Reference-contexts: Recent multi-filament simulations have raised doubts about the convergence of the results <ref> [1, 3] </ref>. thickness ffi C = 0:07595 and radius a = 1. The resolution is represented (along the horizontal axis) by the nondimensional grid size q a = 2ffi=h a , where ffi is the smoothing parameter in the simulation and h a is the minimum distance between grid points.
Reference: [2] <author> C. Anderson. </author> <title> An implementation of the fast mul-tipole method without multipoles. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 13, </volume> <year> 1992. </year>
Reference-contexts: Barnes and Hut [5] applied this idea to gravitational simulations. More sophisticated schemes were developed by Greengard and Rokhlin [15] and subsequently refined by Zhao [37], Anderson <ref> [2] </ref>. Better data structures have recently been developed by Callahan and Kosaraju [9]. Several parallel implementations of the algorithms mentioned above have been developed.
Reference: [3] <author> C. Anderson and C. Greengard. </author> <title> The vortex merger problem at infinite Reynolds number. </title> <journal> Communications on Pure and Applied Mathematics, </journal> <volume> XLII:1123-1139, </volume> <year> 1989. </year>
Reference-contexts: For vortex simulations the largest number of particles reported is around fifty thousand <ref> [3, 21, 36] </ref>. Larger simulations require faster methods involving fewer interactions to evaluate the field at any point. In the last decade a number of approximation algorithms have emerged. <p> This not only reduces considerably the computational expense, but allows better resolution for high Reynolds number flows. Biot-Savart models also allow us to perform effectively inviscid computations with no accumulation of numerical diffusion <ref> [3, 22, 23] </ref>. Vortex methods are also appropriate for studying the many complex flows that present coherent structures. These coherent structures frequently are closely interacting tube-like vortices. <p> Recent multi-filament simulations have raised doubts about the convergence of the results <ref> [1, 3] </ref>. thickness ffi C = 0:07595 and radius a = 1. The resolution is represented (along the horizontal axis) by the nondimensional grid size q a = 2ffi=h a , where ffi is the smoothing parameter in the simulation and h a is the minimum distance between grid points.
Reference: [4] <author> R. Anderson. </author> <title> Nearest neighbor trees and N-body simulation. </title> <type> manuscript, </type> <year> 1994. </year>
Reference-contexts: The partition is computed recursively by dividing the original box into eight octants of equal volume until each undivided box contains exactly one body. Alternative tree decompositions have been suggested <ref> [4, 9] </ref>; the Barnes-Hut algorithm applies to these as well. Each internal node of the oct-tree represents a cluster. Once the oct-tree has been built, the moments of the internal nodes are computed in one phase up the tree, starting at the leaves.
Reference: [5] <author> J. Barnes and P. Hut. </author> <title> A hierarchical O(N log N ) force-calculation algorithm. </title> <journal> Nature, </journal> <volume> 324, </volume> <year> 1986. </year>
Reference-contexts: This leads to an obvious divide-and-conquer algorithm in which the particles are organized in a hierarchy of clusters which allows the approximation to be applied efficiently. Barnes and Hut <ref> [5] </ref> applied this idea to gravitational simulations. More sophisticated schemes were developed by Greengard and Rokhlin [15] and subsequently refined by Zhao [37], Anderson [2]. Better data structures have recently been developed by Callahan and Kosaraju [9]. Several parallel implementations of the algorithms mentioned above have been developed.
Reference: [6] <author> J. T. Beale and A. Majda. </author> <title> Vortex methods. I: Convergence in three dimensions. </title> <journal> Mathematics of Computation, </journal> <volume> 39(159) </volume> <pages> 1-27, </pages> <year> 1982. </year>
Reference-contexts: We observe in Figure 1 that, in order for convergence, it is necessary to have both a sufficiently high number of grid points N and a sufficiently small value of the smoothing parameter ffi, in agreement with the convergence theorems <ref> [6, 14] </ref>. The small values of ffi necessary for convergence make it necessary to employ millions of particles for these particular values of the ratio ffi C =ffi. 2.1 The Biot-Savart model The version of the vortex method we use was developed by Knio and Ghoniem [21].
Reference: [7] <author> S. Bhatt, M. Chen, J. Cowie, , C. Lin, and P. Liu. </author> <title> Object-oriented support for adaptive methods on parallel machines. </title> <booktitle> In Object Oriented Numerics Conference, </booktitle> <year> 1993. </year>
Reference-contexts: For this reason we separated the construction of the tree from the details of later stages of the algorithm. The interested reader is referred to <ref> [7] </ref> for further details concerning a library of abstractions for N-body algorithms. 1 Clustering techniques which exploit the geometrical properties of the distribution will preserve locality better, but might lose some of the other attractive properties of ORB. Representation. We assign each BH-tree node an owner as follows.
Reference: [8] <author> S Bhatt and P. Liu. </author> <title> A framework for parallel n-body simulations. </title> <type> manuscript, </type> <year> 1994. </year>
Reference-contexts: Once all processors have received and inserted the data received into the local tree, all the locally essential trees have been built. Sender-driven communication is used not only in transferring essential data but also in all the other communication phases. This is true for the Greengard-Rokhlin algorithm as well <ref> [8] </ref>. For example, to compute the vortex element strength for each particle in the multi-filament vortex method, its two neighboring particles in the filament must be fetched through communication if they are not in the local tree.
Reference: [9] <author> P. Callahan and S. Kosaraju. </author> <title> A decomposition of multi-dimension point-sets with applications to k-nearest-neighbors and N-body potential fields. </title> <booktitle> 24th Annual ACM Symposium on Theory of Computing, </booktitle> <year> 1992. </year>
Reference-contexts: Barnes and Hut [5] applied this idea to gravitational simulations. More sophisticated schemes were developed by Greengard and Rokhlin [15] and subsequently refined by Zhao [37], Anderson [2]. Better data structures have recently been developed by Callahan and Kosaraju <ref> [9] </ref>. Several parallel implementations of the algorithms mentioned above have been developed. <p> The partition is computed recursively by dividing the original box into eight octants of equal volume until each undivided box contains exactly one body. Alternative tree decompositions have been suggested <ref> [4, 9] </ref>; the Barnes-Hut algorithm applies to these as well. Each internal node of the oct-tree represents a cluster. Once the oct-tree has been built, the moments of the internal nodes are computed in one phase up the tree, starting at the leaves.
Reference: [10] <author> S. C. Crow. </author> <title> Stability theory for a pair of trailing vortices. </title> <journal> AIAA Journal, </journal> <volume> 8(12) </volume> <pages> 2172-2179, </pages> <year> 1970. </year>
Reference-contexts: Vortex methods are also appropriate for studying the many complex flows that present coherent structures. These coherent structures frequently are closely interacting tube-like vortices. Some of the flows of practical interest that present interacting tube-like vortex structures are wing tip vortices <ref> [10, 19] </ref>, a variety of 3D jet flows of different shapes [16, 17] and turbulent flows [11]. A generic type of interaction between vortex tubes is collapse and reconnection, which is the close approach of vortex tubes that results in large vorticity and strain-rate amplification [20].
Reference: [11] <author> S. Douady, Y. Couder, and M. E. Brachet. </author> <title> Direct observation of the intermittency of intense vorticity filaments in turbulence. </title> <journal> Physical Review Letters, </journal> <volume> 67(8) </volume> <pages> 983-986, </pages> <year> 1991. </year>
Reference-contexts: These coherent structures frequently are closely interacting tube-like vortices. Some of the flows of practical interest that present interacting tube-like vortex structures are wing tip vortices [10, 19], a variety of 3D jet flows of different shapes [16, 17] and turbulent flows <ref> [11] </ref>. A generic type of interaction between vortex tubes is collapse and reconnection, which is the close approach of vortex tubes that results in large vorticity and strain-rate amplification [20]. Collapse and reconnection is an example of small scale formation in high Reynolds number flows.
Reference: [12] <author> V. M. Fernandez. </author> <title> Vortex intensification and collapse of the Lissajous-Elliptic ring: Biot-Savart simulations and visiometrics. </title> <type> Ph.D. Thesis, </type> <institution> Rut-gers University, </institution> <address> New Brunswick, New Jersey, </address> <year> 1994. </year>
Reference-contexts: size q a . models, the O (N 2 ) nature of the computational expense of the Biot-Savart direct method (where N is the number of grid points) severely limits the vortex collapse simulations, leaving the most interesting cases of collapse beyond the cases that have been examined to date <ref> [12, 13] </ref>. Recent multi-filament simulations have raised doubts about the convergence of the results [1, 3]. thickness ffi C = 0:07595 and radius a = 1.
Reference: [13] <author> V.M. Fernandez, N.J. Zabusky, </author> <title> and V.M. Gryanik. Near-singular collapse and local intensification of a "Lissajous-elliptic" vortex ring: Non-monotonic behavior and zero-approaching local energy densities. </title> <journal> Physics of Fluids A, </journal> <volume> 6(7) </volume> <pages> 2242-2244, </pages> <year> 1994. </year>
Reference-contexts: size q a . models, the O (N 2 ) nature of the computational expense of the Biot-Savart direct method (where N is the number of grid points) severely limits the vortex collapse simulations, leaving the most interesting cases of collapse beyond the cases that have been examined to date <ref> [12, 13] </ref>. Recent multi-filament simulations have raised doubts about the convergence of the results [1, 3]. thickness ffi C = 0:07595 and radius a = 1.
Reference: [14] <author> C. Greengard. </author> <title> Convergence of the vortex filament method. </title> <journal> Mathematics of Computation, </journal> <volume> 47(176) </volume> <pages> 387-398, </pages> <year> 1986. </year>
Reference-contexts: We observe in Figure 1 that, in order for convergence, it is necessary to have both a sufficiently high number of grid points N and a sufficiently small value of the smoothing parameter ffi, in agreement with the convergence theorems <ref> [6, 14] </ref>. The small values of ffi necessary for convergence make it necessary to employ millions of particles for these particular values of the ratio ffi C =ffi. 2.1 The Biot-Savart model The version of the vortex method we use was developed by Knio and Ghoniem [21].
Reference: [15] <author> L. Greengard and V. Rokhlin. </author> <title> A fast algorithm for particle simulations. </title> <journal> Journal of Computational Physics, </journal> <volume> 73, </volume> <year> 1987. </year>
Reference-contexts: This leads to an obvious divide-and-conquer algorithm in which the particles are organized in a hierarchy of clusters which allows the approximation to be applied efficiently. Barnes and Hut [5] applied this idea to gravitational simulations. More sophisticated schemes were developed by Greengard and Rokhlin <ref> [15] </ref> and subsequently refined by Zhao [37], Anderson [2]. Better data structures have recently been developed by Callahan and Kosaraju [9]. Several parallel implementations of the algorithms mentioned above have been developed.
Reference: [16] <author> H. S. Husain and F. Hussain. </author> <title> Elliptic jets. Part 3. Dynamics of preferred mode coherent structure. </title> <journal> Journal of Fluid Mechanics, </journal> <volume> 248 </volume> <pages> 315-361, </pages> <year> 1993. </year>
Reference-contexts: These coherent structures frequently are closely interacting tube-like vortices. Some of the flows of practical interest that present interacting tube-like vortex structures are wing tip vortices [10, 19], a variety of 3D jet flows of different shapes <ref> [16, 17] </ref> and turbulent flows [11]. A generic type of interaction between vortex tubes is collapse and reconnection, which is the close approach of vortex tubes that results in large vorticity and strain-rate amplification [20]. Collapse and reconnection is an example of small scale formation in high Reynolds number flows.
Reference: [17] <author> F. Hussain and H. S. Husain. </author> <title> Elliptic jets. Part 1. Characteristics of unexcited and excited jets. </title> <journal> Journal of Fluid Mechanics, </journal> <volume> 208 </volume> <pages> 257-320, </pages> <year> 1989. </year>
Reference-contexts: These coherent structures frequently are closely interacting tube-like vortices. Some of the flows of practical interest that present interacting tube-like vortex structures are wing tip vortices [10, 19], a variety of 3D jet flows of different shapes <ref> [16, 17] </ref> and turbulent flows [11]. A generic type of interaction between vortex tubes is collapse and reconnection, which is the close approach of vortex tubes that results in large vorticity and strain-rate amplification [20]. Collapse and reconnection is an example of small scale formation in high Reynolds number flows.
Reference: [18] <author> M. Warren J. Salmon and G. Winckelmans. </author> <title> Fast parallel tree codes for gravitational and fluid dynamical N-body problems. </title> <booktitle> Intl. J. Supercomputer Applications, </booktitle> <address> 8.2, </address> <year> 1994. </year>
Reference-contexts: Several parallel implementations of the algorithms mentioned above have been developed. Salmon [27] implemented the Barnes-Hut algorithm on the NCUBE and Intel iPSC, Warren and Salmon [34] reported experiments on the 512-node Intel Touchstone Delta, and later developed hashed implementations of a global tree structure which they report in <ref> [35, 18] </ref>. They have used their codes for astrophysical simulations and also for vortex dynamics. This paper builds on our CM-5 implementation [25] of the Barnes-Hut algorithm for astrophysical simulations and contrasts our approach and conclusions with the aforementioned efforts. This abstract is organized as follows. <p> In contrast with the "vortex arrow" approach <ref> [18, 23] </ref>, updating the "strength" of the vortex elements in the filament method does not require the evaluation of the velocity gradient, which involves the computation of another integral over all of the particles. Also, filaments with form of closed curves, satisfy the divergence free condition of the vorticity field. <p> Step 6 redistributes particles when workload becomes imbalanced. The ORB is incrementally updated so that workload is balanced among processors. 3.8 Comparisons with previous work We sketch the important differences between our framework and the global hashed structure of Warren and Salmon <ref> [35, 18] </ref>. Warren and Salmon [35] use a different criterion for applying approximations. They do not construct essential trees thereby saving the space overhead associated with local essential trees. Instead, they construct an explicit representation of the oct-tree. <p> Finally, it is not clear how hashed oct-trees can be extended gracefully to multi-filament simulations where each filament must maintain a cyclic ordering. Indeed, their fluid dynamics simulation involves only "free" particles (vortex arrows). While our vortex simulations are very different from those of <ref> [18] </ref> to make meaningful comparisons, the experiments for gravitational simulations are quite similar. Our performance figures compare favorably with those reported by Warren and Salmon [34, 35].
Reference: [19] <author> R. T. Johnston and J. P. Sullivan. </author> <title> A flow visualization study of the interaction between a helical vortex and a line vortex. </title> <note> Submitted to Experiments in Fluids, </note> <year> 1993. </year>
Reference-contexts: Vortex methods are also appropriate for studying the many complex flows that present coherent structures. These coherent structures frequently are closely interacting tube-like vortices. Some of the flows of practical interest that present interacting tube-like vortex structures are wing tip vortices <ref> [10, 19] </ref>, a variety of 3D jet flows of different shapes [16, 17] and turbulent flows [11]. A generic type of interaction between vortex tubes is collapse and reconnection, which is the close approach of vortex tubes that results in large vorticity and strain-rate amplification [20].
Reference: [20] <author> S. Kida and M. Takaoka. </author> <title> Vortex reconnection. Annu. </title> <journal> Rev. Fluid Mech., </journal> <volume> 26 </volume> <pages> 169-189, </pages> <year> 1994. </year>
Reference-contexts: So-called "N -body" methods have been applied to problems in astrophysics, semiconductor device simulation, molecular dynamics, plasma physics, and fluid mechanics. This paper describes the implementation of a tree code for vortex dynamics simulations, which we apply to the study of vortex collapse and reconnection <ref> [20] </ref>. We expect that the use of the fast algorithm will allow us to study the generation of small scales with adequate resolution, and make it possible to realistically simulate the flows around bodies of complex shapes encountered in engineering applications. <p> A generic type of interaction between vortex tubes is collapse and reconnection, which is the close approach of vortex tubes that results in large vorticity and strain-rate amplification <ref> [20] </ref>. Collapse and reconnection is an example of small scale formation in high Reynolds number flows. Small scale generation is very important for the energy transfer and dissipation processes in turbulent flows [26].
Reference: [21] <author> O. M. Knio and A. F. Ghoniem. </author> <title> Numerical study of a three-dimensional vortex method. </title> <journal> Journal of Computational Physics, </journal> <volume> 86 </volume> <pages> 75-106, </pages> <year> 1990. </year>
Reference-contexts: For vortex simulations the largest number of particles reported is around fifty thousand <ref> [3, 21, 36] </ref>. Larger simulations require faster methods involving fewer interactions to evaluate the field at any point. In the last decade a number of approximation algorithms have emerged. <p> The small values of ffi necessary for convergence make it necessary to employ millions of particles for these particular values of the ratio ffi C =ffi. 2.1 The Biot-Savart model The version of the vortex method we use was developed by Knio and Ghoniem <ref> [21] </ref>. The vorticity of a vortex tube is represented by a bundle of vortex filaments l (; t fl ), each of them with circulation l .
Reference: [22] <author> A. Leonard. </author> <title> Vortex methods for flow simulation. </title> <journal> Journal of Computational Physics, </journal> <volume> 37 </volume> <pages> 289-335, </pages> <year> 1980. </year>
Reference-contexts: This not only reduces considerably the computational expense, but allows better resolution for high Reynolds number flows. Biot-Savart models also allow us to perform effectively inviscid computations with no accumulation of numerical diffusion <ref> [3, 22, 23] </ref>. Vortex methods are also appropriate for studying the many complex flows that present coherent structures. These coherent structures frequently are closely interacting tube-like vortices.
Reference: [23] <author> A. Leonard. </author> <title> Computing three-dimensional incompressible flows with vortex elements. </title> <journal> Ann. Rev. Fluid Mech., </journal> <volume> 17 </volume> <pages> 523-559, </pages> <year> 1985. </year>
Reference-contexts: Biot-Savart models offer the advantage that the calculation effort concentrates in the small regions of the fluid that contain the vorticity, which are usually small compared to the domain that contains the flow <ref> [23] </ref>. This not only reduces considerably the computational expense, but allows better resolution for high Reynolds number flows. Biot-Savart models also allow us to perform effectively inviscid computations with no accumulation of numerical diffusion [3, 22, 23]. <p> This not only reduces considerably the computational expense, but allows better resolution for high Reynolds number flows. Biot-Savart models also allow us to perform effectively inviscid computations with no accumulation of numerical diffusion <ref> [3, 22, 23] </ref>. Vortex methods are also appropriate for studying the many complex flows that present coherent structures. These coherent structures frequently are closely interacting tube-like vortices. <p> In contrast with the "vortex arrow" approach <ref> [18, 23] </ref>, updating the "strength" of the vortex elements in the filament method does not require the evaluation of the velocity gradient, which involves the computation of another integral over all of the particles. Also, filaments with form of closed curves, satisfy the divergence free condition of the vorticity field.
Reference: [24] <author> P. Liu, W. Aiello, and S. Bhatt. </author> <title> An atomic model for message passing. </title> <booktitle> In 5th Annual ACM Symposium on Parallel Algorithms and Architecture, </booktitle> <year> 1993. </year>
Reference-contexts: Instead, we used a protocol which alternates sends with receives. The problem is thus reduced to ordering the messages to be sent. For example, sending messages in order of increasing destination address gives low throughput since virtual channels to the same receiver are blocked. In an earlier paper <ref> [24] </ref> we developed the atomic message model to investigate this phenomenon. Consistent with the theory, we find that sending messages in random order worked best. 3.7 Application to vortex simulation We implemented the multi-filament vortex simulation using our framework. Figure 3 gives a high-level description of the code structure.
Reference: [25] <author> P. Liu and S. Bhatt. </author> <title> Experiences with parallel n-body simulation. </title> <booktitle> In 6th Annual ACM Symposium on Parallel Algorithms and Architecture, </booktitle> <year> 1994. </year>
Reference-contexts: They have used their codes for astrophysical simulations and also for vortex dynamics. This paper builds on our CM-5 implementation <ref> [25] </ref> of the Barnes-Hut algorithm for astrophysical simulations and contrasts our approach and conclusions with the aforementioned efforts. This abstract is organized as follows. Section 2 describes the application problem in some detail, and outlines the Barnes-Hut fast summation algorithm. <p> Furthermore, ORB preserves data locality reasonably well 1 and permits simple load-balancing. While it is expensive to recompute the ORB at each time step [28], the cost of incremental load-balancing is negligible <ref> [25] </ref>. The ORB decomposition is incrementally updated in parallel as follows.
Reference: [26] <author> A.J. Majda. </author> <title> Vorticity, turbulence and acoustics in fluid flow. </title> <journal> SIAM Review, </journal> <volume> 33(3) </volume> <pages> 349-388, </pages> <month> Septem-ber </month> <year> 1991. </year>
Reference-contexts: Collapse and reconnection is an example of small scale formation in high Reynolds number flows. Small scale generation is very important for the energy transfer and dissipation processes in turbulent flows <ref> [26] </ref>. The study of small scales formation in high Reynolds number flows tends to require substantial amount of computational resources.
Reference: [27] <author> J. Salmon. </author> <title> Parallel Hierarchical N-body Methods. </title> <type> PhD thesis, </type> <institution> Caltech, </institution> <year> 1990. </year>
Reference-contexts: More sophisticated schemes were developed by Greengard and Rokhlin [15] and subsequently refined by Zhao [37], Anderson [2]. Better data structures have recently been developed by Callahan and Kosaraju [9]. Several parallel implementations of the algorithms mentioned above have been developed. Salmon <ref> [27] </ref> implemented the Barnes-Hut algorithm on the NCUBE and Intel iPSC, Warren and Salmon [34] reported experiments on the 512-node Intel Touchstone Delta, and later developed hashed implementations of a global tree structure which they report in [35, 18]. <p> Each particle has a distinct set of essential nodes which changes with time. One remark concerning distance measurements is in order. There are several ways to measure the distance between a particle and a box. Salmon <ref> [27] </ref> discusses various alternatives in some detail. For consistency, we measure distances from bodies to the perimeter of a box in the L 1 metric.
Reference: [28] <author> J. Singh. </author> <title> Parallel Hierarchical N-body Methods and their Implications for Multiprocessors. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1993. </year>
Reference-contexts: Furthermore, ORB preserves data locality reasonably well 1 and permits simple load-balancing. While it is expensive to recompute the ORB at each time step <ref> [28] </ref>, the cost of incremental load-balancing is negligible [25]. The ORB decomposition is incrementally updated in parallel as follows. <p> The next step is to make the local trees be structurally consistent with the global oct-tree. This requires adjusting the levels of all nodes which are split by ORB lines. We omit the details of the level-adjustment procedure in this paper. A similar process was developed independently in <ref> [28] </ref>; an additional complication in our case is that we build the oct-tree until each leaf contains a number, L, of bodies. Choosing L to be much larger than 1 reduces the size of the BH-tree, but makes level-adjustment somewhat tricky.
Reference: [29] <author> R.E. Tarjan. </author> <title> Data Structures and Network Algorithms. </title> <institution> Society for Industrial and Applied Mathematics, </institution> <year> 1983. </year>
Reference-contexts: The linear ordering must be maintained dynamically as particles move across processor boundaries, either because of their induced velocity or during workload remapping. We use a splay search tree <ref> [29] </ref> to store the indices of particles of the same filament. When particles enter/leave a processor domain, its filament number and index within the filament (f; i) are sent to the new processor, so that i can be inserted into the search tree for filament f in the destination.
Reference: [30] <author> Thinking Machines Corporation. </author> <title> The Connection Machine CM-5 Technical Summary, </title> <year> 1991. </year>
Reference-contexts: In the latter implementation over 53% of the time is spent constructing and manipulating the tree structure. 4 Performance Our platform is the Connection Machine CM-5 with SPARC vector units <ref> [30] </ref>. Each processing node has 32M bytes of memory and can perform floating point operations at peak rate of 128 Mflop/s [33]. We use the CMMD communication library [32].
Reference: [31] <institution> Thinking Machines Corporation. </institution> <month> CDPEAC: </month> <title> Using GCC to program in DPEAC, </title> <year> 1993. </year>
Reference-contexts: Each processing node has 32M bytes of memory and can perform floating point operations at peak rate of 128 Mflop/s [33]. We use the CMMD communication library [32]. The vector units are programmed in CDPEAC which provides an interface between C and the DPEAC assembly language for vector units <ref> [31] </ref>. The rest of the program is written in C. The vortex dynamics code along with several diagnostics are operational. Starting with the gravitational code, the effort involved adding the phase to compute vortex element strength and writing the computational CDPEAC kernel.
Reference: [32] <institution> Thinking Machines Corporation. </institution> <note> CMMD Reference Manual, </note> <year> 1993. </year>
Reference-contexts: As an extreme example, if all messages are sent before any is received, a large machine will simply crash when the number of virtual channels has been exhausted. In the CMMD message-passing library (version 3.0) each outstanding send requires a virtual channel <ref> [32] </ref> and the number of channels is limited. Instead, we used a protocol which alternates sends with receives. The problem is thus reduced to ordering the messages to be sent. <p> Each processing node has 32M bytes of memory and can perform floating point operations at peak rate of 128 Mflop/s [33]. We use the CMMD communication library <ref> [32] </ref>. The vector units are programmed in CDPEAC which provides an interface between C and the DPEAC assembly language for vector units [31]. The rest of the program is written in C. The vortex dynamics code along with several diagnostics are operational.
Reference: [33] <institution> Thinking Machines Corporation. </institution> <note> DPEAC Reference Manual, </note> <year> 1993. </year>
Reference-contexts: Each processing node has 32M bytes of memory and can perform floating point operations at peak rate of 128 Mflop/s <ref> [33] </ref>. We use the CMMD communication library [32]. The vector units are programmed in CDPEAC which provides an interface between C and the DPEAC assembly language for vector units [31]. The rest of the program is written in C. The vortex dynamics code along with several diagnostics are operational.
Reference: [34] <author> M. Warren and J. Salmon. </author> <title> Astrophysical N-body simulations using hierarchical tree data structures. </title> <booktitle> In Proceedings of Supercomputing, </booktitle> <year> 1992. </year>
Reference-contexts: Better data structures have recently been developed by Callahan and Kosaraju [9]. Several parallel implementations of the algorithms mentioned above have been developed. Salmon [27] implemented the Barnes-Hut algorithm on the NCUBE and Intel iPSC, Warren and Salmon <ref> [34] </ref> reported experiments on the 512-node Intel Touchstone Delta, and later developed hashed implementations of a global tree structure which they report in [35, 18]. They have used their codes for astrophysical simulations and also for vortex dynamics. <p> This involves computing a new separating hyperplane; we use a binary search combined with a tree traversal on the local BH-tree to determine the total weight within a parallelpiped. 3.2 Virtual global trees Unlike the original implementation of Warren and Salmon <ref> [34] </ref>, we chose to construct a representation of a distributed global oct-tree. An important consideration for us was to investigate abstractions that allow the applications programmer to declare a global data structure, a tree for example, without having to worry about the details of distributed-memory implementation. <p> Indeed, their fluid dynamics simulation involves only "free" particles (vortex arrows). While our vortex simulations are very different from those of [18] to make meaningful comparisons, the experiments for gravitational simulations are quite similar. Our performance figures compare favorably with those reported by Warren and Salmon <ref> [34, 35] </ref>. <p> In contrast, the time per iteration for a uniform distribution of 8.8 million particles on the 512-node Delta Touchstone reported by Warren and Salmon is 77 seconds <ref> [34] </ref> and 114 seconds [35] using the hashed data structure. In the latter implementation over 53% of the time is spent constructing and manipulating the tree structure. 4 Performance Our platform is the Connection Machine CM-5 with SPARC vector units [30].
Reference: [35] <author> M. Warren and J. Salmon. </author> <title> A parallel hashed oct-tree N-body algorithm. </title> <booktitle> In Proceedings of Supercomputing, </booktitle> <year> 1993. </year>
Reference-contexts: Several parallel implementations of the algorithms mentioned above have been developed. Salmon [27] implemented the Barnes-Hut algorithm on the NCUBE and Intel iPSC, Warren and Salmon [34] reported experiments on the 512-node Intel Touchstone Delta, and later developed hashed implementations of a global tree structure which they report in <ref> [35, 18] </ref>. They have used their codes for astrophysical simulations and also for vortex dynamics. This paper builds on our CM-5 implementation [25] of the Barnes-Hut algorithm for astrophysical simulations and contrasts our approach and conclusions with the aforementioned efforts. This abstract is organized as follows. <p> It is natural, therefore, to develop programming tools which reduce the time to develop high-performance application codes. This section describes our programming framework for hierarchical N -body algorithms; this framework has been used earlier for implementing the Barnes-Hut and Greengard-Rokhlin algorithms for gravitational fields. Warren and Salmon <ref> [35] </ref> have developed a different framework; we contrast the two approaches in this section. All tree codes resolve common issues of building, traversing, updating and load-balancing large trees in distributed memory. The tension between the communication overhead and computational throughput is of central concern to obtaining high performance. <p> Step 6 redistributes particles when workload becomes imbalanced. The ORB is incrementally updated so that workload is balanced among processors. 3.8 Comparisons with previous work We sketch the important differences between our framework and the global hashed structure of Warren and Salmon <ref> [35, 18] </ref>. Warren and Salmon [35] use a different criterion for applying approximations. They do not construct essential trees thereby saving the space overhead associated with local essential trees. Instead, they construct an explicit representation of the oct-tree. <p> Step 6 redistributes particles when workload becomes imbalanced. The ORB is incrementally updated so that workload is balanced among processors. 3.8 Comparisons with previous work We sketch the important differences between our framework and the global hashed structure of Warren and Salmon [35, 18]. Warren and Salmon <ref> [35] </ref> use a different criterion for applying approximations. They do not construct essential trees thereby saving the space overhead associated with local essential trees. Instead, they construct an explicit representation of the oct-tree. <p> Indeed, their fluid dynamics simulation involves only "free" particles (vortex arrows). While our vortex simulations are very different from those of [18] to make meaningful comparisons, the experiments for gravitational simulations are quite similar. Our performance figures compare favorably with those reported by Warren and Salmon <ref> [34, 35] </ref>. <p> In contrast, the time per iteration for a uniform distribution of 8.8 million particles on the 512-node Delta Touchstone reported by Warren and Salmon is 77 seconds [34] and 114 seconds <ref> [35] </ref> using the hashed data structure. In the latter implementation over 53% of the time is spent constructing and manipulating the tree structure. 4 Performance Our platform is the Connection Machine CM-5 with SPARC vector units [30].
Reference: [36] <author> G. S. Winckelmans. </author> <title> Topics in vortex methods for the computation of three- and two-dimensional incompressible unsteady flows. </title> <type> Ph.D. Thesis, </type> <institution> Cal-ifornia Institute of Technology, Pasadena, Cali-fornia, </institution> <year> 1989. </year>
Reference-contexts: For vortex simulations the largest number of particles reported is around fifty thousand <ref> [3, 21, 36] </ref>. Larger simulations require faster methods involving fewer interactions to evaluate the field at any point. In the last decade a number of approximation algorithms have emerged.
Reference: [37] <author> F. Zhao. </author> <title> An O(N ) algorithm for three dimensional N-body simulation. </title> <type> Technical report, </type> <institution> MIT, </institution> <year> 1987. </year>
Reference-contexts: Barnes and Hut [5] applied this idea to gravitational simulations. More sophisticated schemes were developed by Greengard and Rokhlin [15] and subsequently refined by Zhao <ref> [37] </ref>, Anderson [2]. Better data structures have recently been developed by Callahan and Kosaraju [9]. Several parallel implementations of the algorithms mentioned above have been developed.
References-found: 37

