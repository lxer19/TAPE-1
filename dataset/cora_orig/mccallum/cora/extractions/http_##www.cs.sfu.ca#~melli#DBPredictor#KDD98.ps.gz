URL: http://www.cs.sfu.ca/~melli/DBPredictor/KDD98.ps.gz
Refering-URL: 
Root-URL: 
Email: melli@cs.sfu.ca  
Title: On-Line Classification with a Lazy Model-Based Algorithm common approaches to classification are the eager model-based
Author: Gabor Melli 
Keyword: On-Line Classification, Lazy Model-Based Algorithms, Knowledge Discovery on Databases, SQL-based Data Mining, Numerical Discretization  
Note: Two  
Date: March 10, 1998  
Affiliation: Simon Fraser University  
Abstract: The growing access to large relational databases allows for more opportunistic data mining tasks. The focus of this paper is on tasks that require the prediction of a single event's class based on the records in a relational database. We refer to these as on-line classification tasks. This paper proposes a lazy model-based algorithm, named DBPredictor, for on-line classification tasks. This algorithm restricts its effort to the classification of the single event in question, dynamically avoids irrelevant attributes, and supports the prediction with a simple IF-THEN rule. Another shown advantage to the algorithm is its ability to be tightly-coupled with a SQL-based relational database. To achieve this tight-coupling however, a novel approach is presented that avoids the need for global dis-cretization of numerical attributes. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> AAAI. </editor> <booktitle> Thirteenth National Conference on Artificial Intelligence. </booktitle> <publisher> AAAI Press, </publisher> <year> 1996. </year>
Reference: [2] <author> R. Agrawal and J. C. Shafer. </author> <title> Parallel mining of association rules: Design, implementation, </title> <journal> and experience. IEEE Trans. Knowledge and Data Engineering, </journal> <volume> 8 </volume> <pages> 962-969, </pages> <year> 1996. </year>
Reference-contexts: Specifically, the SQL Interface Protocol (SIP) presented in [12] is used to summarize the rule's cover. A tightly-coupled approach, such as this, has been found to be desirable <ref> [2] </ref>. Furthermore, extensions to SQL's summarization capabilities such as the CUBE operator in [10] will continue to improve the performance of this type of query.
Reference: [3] <author> D. W. Aha, </author> <title> editor. Lazy Learning. </title> <publisher> Kluwer Academic, </publisher> <month> May </month> <year> 1997. </year>
Reference-contexts: Eager algorithms induce a complete classification structure (classifier) before any classification requests can be processed, while lazy algorithms forgo the learning phase and return a result tailored to the classification task at hand <ref> [4, 3] </ref>. Model-based algorithms, represent their result in a language that is richer than the language used to describe the dataset, while instance-based algorithms represent their result in the same language that is used to described the dataset [19]. <p> DBP/C4.5 DBP/IB1 C4.5/IB1 98% 75% 35% ical datasets. However, because of C4.5's significant bias against numerical attributes relative to IB1, and because of the well known strength of k-NN based algorithms in numerical domain's <ref> [3] </ref>, DBPredictor's approach to numerical attributes appears to be sound. 6 Conclusion This paper presents an algorithm, named DBPredictor, that is targetted to on-line classification tasks. These tasks require the prediction of a single event's class, based on the records stored in a relational database.
Reference: [4] <author> D. W. Aha. </author> <title> Lazy learning editorial remarks. </title> <booktitle> [3], </booktitle> <pages> pages 1-3. </pages>
Reference-contexts: Eager algorithms induce a complete classification structure (classifier) before any classification requests can be processed, while lazy algorithms forgo the learning phase and return a result tailored to the classification task at hand <ref> [4, 3] </ref>. Model-based algorithms, represent their result in a language that is richer than the language used to describe the dataset, while instance-based algorithms represent their result in the same language that is used to described the dataset [19].
Reference: [5] <author> D. W. Aha, D. Kibler, and M. K. Albert. </author> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6(1) </volume> <pages> 37-66, </pages> <year> 1991. </year>
Reference-contexts: The former approach includes decision-tree and rule induction algorithms, such as C4.5 [18] and CN2 [6]. These algorithms induce a high-level classifier (decision tree or rule list) that can then be used to classify any new event. Lazy instance-based approaches such as IB1 <ref> [5] </ref>, on the other hand, do not proceed until the event that is to be classified is presented, then the algorithm produces a prediction by quickly locating the instances that are most similar to the given event. When applied to on-line classification tasks, these two approaches encounter some shortcomings. <p> EC segment SE glass GL soybean-sml. SO hayes-roth HR tic-tac-toe TO heart HT vote VO heart-c HC with respect to irrelevant attributes and numerical attributes. The benchmark algorithms for this study were the C4.5 (r8) decision tree algorithm [18, 20] and the IB1 instance-based algorithm <ref> [5] </ref>. Table 1 lists the datasets used in this study. An attempt was made to include previously studied datasets [9, 7, 20] with a wide variety of sizes (N 2 47 20; 000 records, n 2 4 69 attributes), proportions of numerical attributes (0%; 100%) 3 , and application areas.
Reference: [6] <author> P. Clark and R. Boswell. </author> <title> Rule induction with CN2: Some recent improvements. </title> <booktitle> In Machine Learning Proceedings of the Fifth European Conference (EWSL-91), </booktitle> <pages> pages 151-163. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: The more mature classification algorithms, use either an eager model-based or a lazy instance-based approach. The former approach includes decision-tree and rule induction algorithms, such as C4.5 [18] and CN2 <ref> [6] </ref>. These algorithms induce a high-level classifier (decision tree or rule list) that can then be used to classify any new event.
Reference: [7] <author> P. Domingos. </author> <title> Unifying instance-based and rule-based induction. </title> <journal> Machine Learning, </journal> <volume> 24(2) </volume> <pages> 141-168, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: The benchmark algorithms for this study were the C4.5 (r8) decision tree algorithm [18, 20] and the IB1 instance-based algorithm [5]. Table 1 lists the datasets used in this study. An attempt was made to include previously studied datasets <ref> [9, 7, 20] </ref> with a wide variety of sizes (N 2 47 20; 000 records, n 2 4 69 attributes), proportions of numerical attributes (0%; 100%) 3 , and application areas.
Reference: [8] <author> U. M. Fayyad and K. B. Irani. </author> <title> The attribute selection problem in decision tree generation. </title> <booktitle> In Proceedings of the Tenth National Conference of Artificial Intelligence, </booktitle> <pages> pages 104-110. </pages> <publisher> AAAI Press, </publisher> <year> 1992. </year>
Reference-contexts: Previous studies have, for example, made use of impurity measures, such as entropy [17], and class separation measures, such as ORT <ref> [8] </ref> and DI [21]. In [13, 15] we evaluated several approaches to F () and found that the DI () measure along with the use of prunning to be effective.
Reference: [9] <author> J. H. Friedman, R. Kohavi, and Y. Yun. </author> <title> Lazy decision trees. </title> <booktitle> [1], </booktitle> <pages> pages 717-724. </pages>
Reference-contexts: Such an algo-rithm would focus its effort the event classification in question, and would also return a rationale for this prediction that may help the person interpret the validity of the prediction. Two recent proposals that implicitly make use of this approach include the DBPredictor [14] and the LazyDT <ref> [9] </ref> (Lazy Decision Tree) algorithms. The main difference between these two is one's use of a rules and the other's use of decision tree paths as their model. Current implementations however, are not particularly suited to on-line classification because of their memory intensive definitions and their requirement of dis-cretized datasets. <p> The benchmark algorithms for this study were the C4.5 (r8) decision tree algorithm [18, 20] and the IB1 instance-based algorithm [5]. Table 1 lists the datasets used in this study. An attempt was made to include previously studied datasets <ref> [9, 7, 20] </ref> with a wide variety of sizes (N 2 47 20; 000 records, n 2 4 69 attributes), proportions of numerical attributes (0%; 100%) 3 , and application areas. <p> This value was passed on to the next study. 5.2 Irrelevant Attributes This portion of the study compares DBPre-dictor's accuracy with that of C4.5 and IB1 5 when irrelevant attributes are present. Previous studies have already reported that lazy model-based classification algorithms can achieve equal accuracy to C4.5 <ref> [9, 15] </ref>. Table 2 presents the change in the error rates for the three algorithms when 10 irrelevant attributes 6 were added to the datasets. As expected, IB1's accuracy is relatively sensitive to irrelevant attributes.
Reference: [10] <author> J. Gray, S. Chaudhuri, A. Bosworth, A. Layman, D. Reichart, M. Venkatrao, F. Pellow, and H. Pirahesh. </author> <title> Data cube: A relational aggregation operator generalizing group-by, </title> <journal> cross-tab and subtotals. Data Mining and Knowledge Discovery, </journal> <volume> 1 </volume> <pages> 29-54, </pages> <year> 1997. </year>
Reference-contexts: Specifically, the SQL Interface Protocol (SIP) presented in [12] is used to summarize the rule's cover. A tightly-coupled approach, such as this, has been found to be desirable [2]. Furthermore, extensions to SQL's summarization capabilities such as the CUBE operator in <ref> [10] </ref> will continue to improve the performance of this type of query. A get consequent (r; (D; ~e; A c )) request is satisfied with the pure SIP: SELECT ; f (fl) FROM D WHERE GROUP BY Where = A c , f (fl) = COUNT (*), and = r:antecedent.
Reference: [11] <author> P. J. Huber. </author> <title> From large to huge: A statistician's reactions to KDD & DM. </title> <booktitle> In The Third International Conference on Knowledge Discovery & Data Mining, </booktitle> <pages> pages 304-308, </pages> <year> 1997. </year>
Reference-contexts: 1 Introduction With the ease of access to a growing amount of structured observations, particularly those stored in relational databases, classification algorithms are being used in very opportunistic ways. Opportunistic classification occurs against datasets that were not explicitly constructed for the classification task at hand <ref> [11] </ref>. A common requirement of these tasks is to predict the class for only one event, based on the records stored in a relational database. We refer these as on-line classification tasks. Consider the following example: Example 1.1.
Reference: [12] <author> G. H. John and B. Lent. </author> <title> SIPping from the data firehose. </title> <booktitle> In Proceedings, Third International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> pages 199-202. </pages> <publisher> AAAI Press, </publisher> <year> 1997. </year>
Reference-contexts: For a given rule's antecedent (r:antecedent), the procedure returns a summary of attribute A c for all the dataset records that match the antecedent. 5 The procedure proposed below, accom-plishes this summarization task with a SQL query. Specifically, the SQL Interface Protocol (SIP) presented in <ref> [12] </ref> is used to summarize the rule's cover. A tightly-coupled approach, such as this, has been found to be desirable [2]. Furthermore, extensions to SQL's summarization capabilities such as the CUBE operator in [10] will continue to improve the performance of this type of query.
Reference: [13] <author> G. Melli. </author> <title> Advances in lazy model-based induction. </title> <note> submitted to the 1998 International Conference on Machine Learning (ICML-98). </note>
Reference-contexts: Previous studies have, for example, made use of impurity measures, such as entropy [17], and class separation measures, such as ORT [8] and DI [21]. In <ref> [13, 15] </ref> we evaluated several approaches to F () and found that the DI () measure along with the use of prunning to be effective. This function measures the Euclidean distance between two class probability distributions and tests for a threshold rule cover and heuristic value.
Reference: [14] <author> G. Melli. </author> <title> Ad hoc attribute-value prediction. </title> <booktitle> [1], </booktitle> <pages> page 1396. </pages>
Reference-contexts: Such an algo-rithm would focus its effort the event classification in question, and would also return a rationale for this prediction that may help the person interpret the validity of the prediction. Two recent proposals that implicitly make use of this approach include the DBPredictor <ref> [14] </ref> and the LazyDT [9] (Lazy Decision Tree) algorithms. The main difference between these two is one's use of a rules and the other's use of decision tree paths as their model.
Reference: [15] <author> G. Melli. </author> <title> Knowledge based on-line classification. </title> <type> Master's thesis, </type> <institution> Simon Fraser University, School of Computing Science, </institution> <month> April </month> <year> 1998. </year>
Reference-contexts: Other details of this algorithm are presented in <ref> [15] </ref>. DBPredictor requires three input parameters: a partially instantiated event ~e; the attribute whose value is to be predicted (A c ); and finally a dataset D from the same domain as ~e. <p> Previous studies have, for example, made use of impurity measures, such as entropy [17], and class separation measures, such as ORT [8] and DI [21]. In <ref> [13, 15] </ref> we evaluated several approaches to F () and found that the DI () measure along with the use of prunning to be effective. This function measures the Euclidean distance between two class probability distributions and tests for a threshold rule cover and heuristic value. <p> This value was passed on to the next study. 5.2 Irrelevant Attributes This portion of the study compares DBPre-dictor's accuracy with that of C4.5 and IB1 5 when irrelevant attributes are present. Previous studies have already reported that lazy model-based classification algorithms can achieve equal accuracy to C4.5 <ref> [9, 15] </ref>. Table 2 presents the change in the error rates for the three algorithms when 10 irrelevant attributes 6 were added to the datasets. As expected, IB1's accuracy is relatively sensitive to irrelevant attributes.
Reference: [16] <author> P. M. Murphy and D. W. Aha. </author> <title> UCI repository of machine learning databases. </title> <address> Irvine, CA: </address> <institution> University of 10 California, Department of Information and Computer Science, </institution> <year> 1995. </year>
Reference-contexts: Because DBPredictor possess some information about ~e, the algorithm will generate several rules that are slightly more specialized than rule r 0 . Assume that three rules are generated at this specialization step: r 1 : (A 1 = a 1 ) ! A 4 2 <ref> [16; 64] </ref> r 3 : (A 3 = a 3 ) ! A 4 2 [30; 70] Note that as required, all three rules continue to apply to ~e. <p> An attempt was made to include previously studied datasets [9, 7, 20] with a wide variety of sizes (N 2 47 20; 000 records, n 2 4 69 attributes), proportions of numerical attributes (0%; 100%) 3 , and application areas. All datasets were retrieved from the UCI repository <ref> [16] </ref>. 3 on average the 23 datasets possessed 48% nu merical attributes 7 5.1 Tuning num ratio The first portion of the empirical study determined an appropriate value for DBPre-dictor's num ratio internal parameter.
Reference: [17] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: Previous studies have, for example, made use of impurity measures, such as entropy <ref> [17] </ref>, and class separation measures, such as ORT [8] and DI [21]. In [13, 15] we evaluated several approaches to F () and found that the DI () measure along with the use of prunning to be effective.
Reference: [18] <author> J. R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Current implementations however, are not particularly suited to on-line classification because of their memory intensive definitions and their requirement of dis-cretized datasets. The more mature classification algorithms, use either an eager model-based or a lazy instance-based approach. The former approach includes decision-tree and rule induction algorithms, such as C4.5 <ref> [18] </ref> and CN2 [6]. These algorithms induce a high-level classifier (decision tree or rule list) that can then be used to classify any new event. <p> EC segment SE glass GL soybean-sml. SO hayes-roth HR tic-tac-toe TO heart HT vote VO heart-c HC with respect to irrelevant attributes and numerical attributes. The benchmark algorithms for this study were the C4.5 (r8) decision tree algorithm <ref> [18, 20] </ref> and the IB1 instance-based algorithm [5]. Table 1 lists the datasets used in this study.
Reference: [19] <author> J. R. Quinlan. </author> <title> Combining instance-based and model-based learning. </title> <booktitle> In Proceedings of the Tenth Internal-tional Conference on Machine Learning, </booktitle> <pages> pages 236-243, </pages> <year> 1993. </year>
Reference-contexts: Model-based algorithms, represent their result in a language that is richer than the language used to describe the dataset, while instance-based algorithms represent their result in the same language that is used to described the dataset <ref> [19] </ref>. Based on this categorization, on-line classification tasks would likely benefit from a 1 lazy model-based algorithm. Such an algo-rithm would focus its effort the event classification in question, and would also return a rationale for this prediction that may help the person interpret the validity of the prediction.
Reference: [20] <author> J. R. Quinlan. </author> <title> Improved use of continuous attributes in C4.5. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 77-90, </pages> <month> March </month> <year> 1996. </year>
Reference-contexts: EC segment SE glass GL soybean-sml. SO hayes-roth HR tic-tac-toe TO heart HT vote VO heart-c HC with respect to irrelevant attributes and numerical attributes. The benchmark algorithms for this study were the C4.5 (r8) decision tree algorithm <ref> [18, 20] </ref> and the IB1 instance-based algorithm [5]. Table 1 lists the datasets used in this study. <p> The benchmark algorithms for this study were the C4.5 (r8) decision tree algorithm [18, 20] and the IB1 instance-based algorithm [5]. Table 1 lists the datasets used in this study. An attempt was made to include previously studied datasets <ref> [9, 7, 20] </ref> with a wide variety of sizes (N 2 47 20; 000 records, n 2 4 69 attributes), proportions of numerical attributes (0%; 100%) 3 , and application areas.
Reference: [21] <author> R. Uthurusamy, U. M. Fayyad, and S. Spangler. </author> <title> Learning useful rules from inconclusive data. </title> <editor> In G. Piatetsky-Shapiro and W. J. Frawley, editors, </editor> <booktitle> Knowledge Discovery in Databases, </booktitle> <pages> pages 141-157. </pages> <publisher> AAAI/MIT Press. </publisher> <pages> 11 </pages>
Reference-contexts: Previous studies have, for example, made use of impurity measures, such as entropy [17], and class separation measures, such as ORT [8] and DI <ref> [21] </ref>. In [13, 15] we evaluated several approaches to F () and found that the DI () measure along with the use of prunning to be effective. This function measures the Euclidean distance between two class probability distributions and tests for a threshold rule cover and heuristic value.
References-found: 21

