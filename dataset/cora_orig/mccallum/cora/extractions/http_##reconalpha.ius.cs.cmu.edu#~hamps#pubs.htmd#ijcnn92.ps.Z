URL: http://reconalpha.ius.cs.cmu.edu/~hamps/pubs.htmd/ijcnn92.ps.Z
Refering-URL: http://reconalpha.ius.cs.cmu.edu/~hamps/pubs.htmd/index.html
Root-URL: 
Email: hamps@speech2.cs.cmu.edu and kumar@gauss.ece.cmu.edu  
Title: Why Error Measures are Sub-Optimal for Training Neural Network Pattern Classifiers  
Author: John B. Hampshire II and B.V.K. Vijaya Kumar 
Address: Pittsburgh, PA 15213-3890  
Affiliation: Department of Electrical and Computer Engineering Carnegie Mellon University  
Note: January 16, 1992 11:45 am. Preprinted from IEEE Proceedings of IJCNN-92, vol. IV, pp. 220-227  Page 1 of 11  
Abstract: Pattern classifiers that are trained in a supervised fashion (e.g., multi-layer perceptrons, radial basis functions, etc.) are typically trained with an error measure objective function such as mean-squared error (MSE) or cross-entropy (CE). These classifiers can in theory yield (optimal) Bayesian discrimination, but in practice they often fail to do so. We explain why this happens. In so doing, we identify a number of characteristics that the optimal objective function for training classifiers must have. We show that classification figures of merit (CFM mono ) possess these optimal characteristics, whereas error measures such as MSE and CE do not. We illustrate our arguments with a simple example in which a CFM mono -trained low-order polynomial neural network approximates Bayesian discrimination on a random scalar with the fewest number of training samples and the minimum functional complexity necessary for the task. A comparable MSE-trained net yields significantly worse discrimination on the same task. fl Copyright c fl1992 by J. B. Hampshire II and B. V. K. V. Kumar: all rights reserved. This research was funded by the Air Force Office of Scientific Research (grant AFOSR-89-0551). The views and conclusions contained in this paper are the authors' and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Air Force or the U.S. Government. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Barnard. </author> <title> Performance and Generalization of the CFM Criterion Function. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2(2) </volume> <pages> 322-325, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: We should point out that trials involving the CE objective function were not feasible owing to the incompatibility of the polynomial neural net and the CE objective function: because the polynomial net's outputs can (and do) have values well outside the interval <ref> [0; 1] </ref>, even a modified form of the CE objective function accounting for such output values leads to unstable searches. Results similar to these involving the CE objective function and MLP/RBF classifiers can be found in [5]. <p> It has been (correctly) pointed out that use of the CFM mono objective function is not a sufficient condition for robust generalization <ref> [1] </ref> the neural network classifier's functional capacity (determined by its functional basis, topology, and connectivity) plays a critical role in determining whether or not generalization will be good. We agree with this assertion, but take issue with the implication that the choice of objective function has little effect on generalization.
Reference: [2] <author> E. Barnard and D. Casasent. </author> <title> A Comparison between Criterion Functions for Linear Classifiers, with an Application to Neural Nets. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 19(5) </volume> <pages> 1030-1041, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: 1 (x ; ! i )) P (! i ) = 1 P (! i ) 4 fi fl m j (x ; ! i ) = k=1 m j (x ; ! k ) 5 Equation (3) represents a multi-class generalization of a result for the two-class case in <ref> [2] </ref>. Page 4 of 11 IJCNN-92: Why Error Measures are Sub-Optimal . . . Hampshire & Kumar Original submission January 16, 1992: 11:45 am and C denotes the total number of classes with which x can be associated in this case, 3.
Reference: [3] <editor> J. L. McClelland D. E. Rumelhart et al. </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> volume 1. </volume> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: We interpret the output with the largest value as the classifier's vote for the class of its scalar input x. This polynomial classifier is depicted in figure 2, and it is trained with a modified form of the backpropagation algorithm (e.g., <ref> [3] </ref>).
Reference: [4] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, NY, </address> <year> 1973. </year>
Reference-contexts: II The Pattern Recognition Task and the Polynomial Neural Network Classifier (! 1 ; ! 2 ; . . . ! 3 note that the syntax used herein is based on that used in <ref> [4] </ref>). There are two class boundaries (B 1;2 Bayes = 4:0, B 2;3 Bayes = 4:0) for the Bayesian classifier of x. <p> Hampshire & Kumar Original submission January 16, 1992: 11:45 am is significantly higher than the Bayes rate. The high-complexity MSE-trained network, which yields the Bayes error rate for the large training sample encounters the so-called curse of dimensionality <ref> [4] </ref> for the small training set. As a result, it generalizes very poorly and yields a 7.8% error rate 3.5 times the error rate it yields for the infinitely large training sample.
Reference: [5] <author> J. B. </author> <title> Hampshire II. A Differential Theory of Statistical Pattern Recognition. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, Department of Electrical & Computer Engineering, Hammerschlag Hall, </institution> <address> Pittsburgh, PA 15213-3890, </address> <year> 1992. </year> <title> manuscript in progress. </title>
Reference-contexts: 1 See <ref> [5] </ref> for proofs. 2 We use the term error rate as a synonym for probability of error. Page 2 of 11 IJCNN-92: Why Error Measures are Sub-Optimal . . . <p> In contrast, an identical MSE-trained classifier yields a significantly higher error rate. Analogous results can be shown for other error measures (e.g., cross-entropy) and other types of neural net classifiers, such as multi-layer perceptrons (MLPs), radial basis functions (RBFs), etc. <ref> [5] </ref>. II The Pattern Recognition Task and the Polynomial Neural Network Classifier (! 1 ; ! 2 ; . . . ! 3 note that the syntax used herein is based on that used in [4]). <p> III Classification Results for Asymptotically Large and Finite Training Samples When the linear outputs fO 1 (x); O 3 (x)g in (2) are trained with the MSE objective function, the optimal values for the parameters a i1 MSE and a i0 MSE are <ref> [5] </ref> 5 a i1 MSE = m 1 (x ; ! i ) P (! i ) m 1 (x ; ! i ) P (! i ) = 1 (x j ! i ) fi m 1 (x ; ! i ) (m 1 (x ; ! i ) + <p> When the quadratic output O 2 (x) in (2) is trained with the MSE objective function, the optimal values for the parameters a 22 MSE , a 21 MSE , and a 20 MSE are <ref> [5] </ref> fi fi 2 fl fi 2 fl fi (m 1 (x ; ! 2 ) + m 1 (x ; ! 2 )) (m 2 (x ; ! 2 ) + m 2 (x ; ! 2 )) flfl = 2 (x j ! 2 ) (5) fi fi (m <p> objective function: a 22 CFM B 1;2 CFM + (a 21 CFM a 11 CFM ) B 1;2 CFM a 22 CFM B 2;3 CFM + (a 21 CFM a 31 CFM ) B 2;3 CFM a 11 CFM &lt; 0 a 22 CFM &lt; 0 where fi is arbitrary <ref> [5] </ref>. <p> Results similar to these involving the CE objective function and MLP/RBF classifiers can be found in <ref> [5] </ref>. It is clear from figure 4 that the minimum-complexity CFM mono -trained classifier achieves consistently good discrimination even for small training sample sizes. <p> Moreover, both MSE-trained classifiers exhibit significantly higher variance in their error rates for finite training samples. Based on <ref> [7, 5] </ref>, we predict that 1121 samples of x are necessary to guarantee with 95% confidence an error rate of no more than 4.0% using the CFM mono -trained classifier. <p> In this qualitative expression of the characteristics outlined in the introduction, minimum-complexity classifiers trained with classification figures-of-merit are optimal estimators of the Bayesian discriminant function. Classifiers trained with error measures are not optimal regardless of their functional capacity <ref> [7, 5] </ref>. The results of figures 3 and 4 illustrate these assertions. IV Conclusion The power of neural networks used for statistical pattern recognition lies in their ability to encode potentially complex non-linear mappings from feature vector space to the a posteriori class distributions of the feature vector. <p> On the contrary, we argue that the use of CFM mono is a necessary condition for good generalization when the available training data and functional capacity of the classifier are both limited. This paper serves to illustrate our theoretical statement of the argument, contained in <ref> [7, 5] </ref>.
Reference: [6] <author> J. B. Hampshire II and B. A. Pearlmutter. </author> <title> Equivalence Proofs for Multi-Layer Perceptron Classifiers and the Bayesian Discriminant Function. </title> <editor> In Touretzky, Elman, Sejnowski, and Hinton, editors, </editor> <booktitle> Proceedings of the 1990 Connectionist Models Summer School, </booktitle> <pages> pages 159-172, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan-Kaufmann. </publisher>
Reference: [7] <author> J. B. Hampshire II and B. V. K. Vijaya Kumar. </author> <title> Shooting Craps in Search of an Optimal Strategy for Training Connectionist Pattern Classifiers. </title> <editor> In J. Moody, S. Hanson, and R. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> vol. </volume> <pages> 4, </pages> <address> San Mateo, CA, </address> <year> 1992. </year> <note> Morgan Kauffman. Page 10 of 11 IJCNN-92: Why Error Measures are Sub-Optimal . . . Hampshire & Kumar Original submission January 16, 1992: 11:45 am </note>
Reference-contexts: Nevertheless, we will refer to the network with N 1 = 1 , N 2 = 2 , N 3 = 1 as the minimum-complexity model throughout the remainder of this paper. 4 A discussion of our information-theoretic measure of functional complexity can be found in <ref> [7] </ref>, but for the purpose of this illustration N i the order of the ith polynomial in (2) may be taken as the complexity measure for the network output O i (x). Page 3 of 11 IJCNN-92: Why Error Measures are Sub-Optimal . . . <p> Moreover, both MSE-trained classifiers exhibit significantly higher variance in their error rates for finite training samples. Based on <ref> [7, 5] </ref>, we predict that 1121 samples of x are necessary to guarantee with 95% confidence an error rate of no more than 4.0% using the CFM mono -trained classifier. <p> In this qualitative expression of the characteristics outlined in the introduction, minimum-complexity classifiers trained with classification figures-of-merit are optimal estimators of the Bayesian discriminant function. Classifiers trained with error measures are not optimal regardless of their functional capacity <ref> [7, 5] </ref>. The results of figures 3 and 4 illustrate these assertions. IV Conclusion The power of neural networks used for statistical pattern recognition lies in their ability to encode potentially complex non-linear mappings from feature vector space to the a posteriori class distributions of the feature vector. <p> In contrast, CFM mono -trained classifiers yield robust approximations to the Bayesian discriminant function with the minimum number of training examples and the simplest (i.e., minimum complexity) classifier architecture possible <ref> [7] </ref>. For this reason, classifiers trained with the CFM mono objective function generalize better than those trained with error measures in all but the cases for which training samples and classifier functional capacity are unlimited. <p> On the contrary, we argue that the use of CFM mono is a necessary condition for good generalization when the available training data and functional capacity of the classifier are both limited. This paper serves to illustrate our theoretical statement of the argument, contained in <ref> [7, 5] </ref>.
Reference: [8] <author> J. B. Hampshire II and A. H. Waibel. </author> <title> A Novel Objective Function for Improved Phoneme Recognition Using Time-Delay Neural Networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1(2) </volume> <pages> 216-228, </pages> <month> June </month> <year> 1990. </year> <booktitle> A revised and extended version of work first presented at the 1989 International Joint Conference on Neural Networks, </booktitle> <volume> vol. I, </volume> <pages> pp. 235-241. </pages>
Reference: [9] <author> G. E. Hinton. </author> <title> Connectionist Learning Procedures. </title> <editor> In J. G. Carbonell, editor, </editor> <booktitle> Machine Learning: Paradigms and Methods, </booktitle> <pages> pages 185-234. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference: [10] <author> M. D. Richard and R. P. Lippmann. </author> <title> Neural Network Classifiers Estimate Bayesian a posteriori Probabilities. </title> <journal> Neural Computation, </journal> <volume> 3(4) </volume> <pages> 461-483, </pages> <year> 1991. </year>
Reference: [11] <author> J. W. Tukey. </author> <title> Exploratory Data Analysis. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1977. </year> <note> Page 11 of 11 </note>
Reference-contexts: It is interesting to note the number of classification errors each optimized network makes on the 100 training examples: the minimum-complexity CFM mono -trained net makes no errors, the minimum-complexity MSE-trained net makes 10, and the high-complexity MSE-trained net makes 4. figure 3. The results are shown in box-plot <ref> [11, ch. 2] </ref> statistical summaries.
References-found: 11

