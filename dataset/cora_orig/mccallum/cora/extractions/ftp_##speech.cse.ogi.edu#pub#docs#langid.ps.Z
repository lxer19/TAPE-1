URL: ftp://speech.cse.ogi.edu/pub/docs/langid.ps.Z
Refering-URL: http://www.cse.ogi.edu/CSLU/publications/publications.html
Root-URL: http://www.cse.ogi.edu
Title: A SEGMENTAL APPROACH TO AUTOMATIC LANGUAGE IDENTIFICATION  
Author: Yeshwant Kumar Muthusamy 
Degree: B.Tech., Jawaharlal Nehru Technological University, Hyderabad, India, 1987 A dissertation submitted to the faculty of the Oregon Graduate Institute of Science Technology in partial fulfillment of the requirements for the degree Doctor of Philosophy in Computer Science and Engineering  
Date: October 1993  
Abstract-found: 0
Intro-found: 1
Reference: [Ass89] <author> International Phonetic Association. </author> <title> Report on the 1989 Kiel Convention. </title> <journal> Journal of the International Phonetic Association, </journal> <volume> 19(2) </volume> <pages> 67-80, </pages> <year> 1989. </year>
Reference-contexts: Secondly, since languages differ in their phonetic inventories, a multi-language fine phonetic recognizer is essential to process speech from more than one language. Such a recognizer would have to be trained on a multi-language phonetic alphabet. While the International Phonetic Alphabet (IPA) <ref> [Ass89] </ref> might serve our purpose, human transcriptions or correction of the fine phonetic labels produced by automatic labeling algorithms are error-prone and time-consuming tasks, beyond the reach of our current resources.
Reference: [Atk68] <author> K. Atkinson. </author> <title> Language identification from nonsegmental cues. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 44:378(A), </volume> <year> 1968. </year>
Reference-contexts: Also, since the focus of this review is on studies in automatic language identification, Atkinson's work on human listening experiments with English and Spanish <ref> [Atk68] </ref> is excluded.
Reference: [BC89] <author> E. Barnard and R. A. Cole. </author> <title> A neural-net training program based on conjugate-gradient optimization. </title> <type> Technical Report CSE 89-014, </type> <institution> Department of Computer Science, Oregon Graduate Institute of Science and Technology, </institution> <year> 1989. </year>
Reference-contexts: The number of hidden units was determined 56 experimentally. Figure 4.1 shows the network configuration and the input features. The inputs to the network were normalized to lie between -1.0 and 1.0. This scaling was required to minimize local-minimum problems <ref> [BC89] </ref> and to accentuate differences between the features for the seven broad phonetic categories. The normalization procedure for the segmenter features is described in detail in [Gop90]. The networks were trained using backpropagation with conjugate gradient optimization [BC89]. Each network was trained on 50 iterations through the training vectors. <p> This scaling was required to minimize local-minimum problems <ref> [BC89] </ref> and to accentuate differences between the features for the seven broad phonetic categories. The normalization procedure for the segmenter features is described in detail in [Gop90]. The networks were trained using backpropagation with conjugate gradient optimization [BC89]. Each network was trained on 50 iterations through the training vectors. The trained network was then evaluated on the vectors in the development set to measure generalization. <p> Further, detailed parametric analyses of the feature set were carried out to determine the features most useful for language identification. The neural networks described in this chapter were all trained using backpropagation with conjugate gradient optimization <ref> [BC89] </ref>.
Reference: [BCVA91] <author> E. Barnard, R. A. Cole, M. P. Vea, and F. A. Alleva. </author> <title> Pitch detection with a neural-net classifier. </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 39(2) </volume> <pages> 298-307, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: The pitch estimate is derived from a neural network pitch tracker that locates pitch periods in the filtered (0-700 Hz) waveform <ref> [BCVA91] </ref>. The spectral parameters consist of * sda700 and sda8000 : estimates of averaged spectral difference in two frequency bands (0-700 Hz and 0-8000 Hz). <p> An obstruent was considered voiced if more than half the number of frames in it were labeled as voiced by the neural network pitch tracker <ref> [BCVA91] </ref> * Ratio of voiced consonants to total number of consonants (1 value) * Average duration of the seven broad phonetic labels (7 values) * Standard deviation of the duration of the seven broad phonetic labels (7 values) * Segment-pair ratios: the ratios num (AB)=num (AX) and num (AB)=num (Y B),
Reference: [Ber93] <author> K. M. Berkling. </author> <title> Automatic language identification using statistical features derived from phoneme classifiers. </title> <note> Presented at the 1993 OGI CSE Student Research Symposium, </note> <month> May </month> <year> 1993. </year>
Reference-contexts: Such a corpus would be invaluable in pursuing a phonemic approach to automatic language identification and would serve as a common database for assessment and comparison of techniques. 6.3.2 Phoneme-based Approaches Preliminary results are in from a phoneme-based approach to automatic language identification for the English-Japanese task <ref> [Ber93, MBA + 93] </ref>.
Reference: [BHH78] <author> G. E. P. Box, W. G. Hunter, and J. S. Hunter. </author> <title> Statistics for Experimenters. </title> <publisher> John Wiley, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: An analysis of variance using the one-way layout model <ref> [BHH78, Sta91] </ref> was conducted to determine the usefulness of the 126 segment-triple features in distinguishing between the 10 languages. Of the 63 segment-triples, five produced p-values greater than 0.01. These were discarded.
Reference: [Bur92] <author> D. Burnett. </author> <title> Toward multi-language pitch tracking for telephone speech. </title> <note> Presented at the 1992 OGI CSE Student Research Symposium, </note> <month> May </month> <year> 1992. </year>
Reference-contexts: On the English-Japanese task, the ratio of occurrence of VOC-POVS proved to be the most useful feature, performing at an accuracy of 74.5%. 5.4.8 Pitch-based Features 5.4.8.1 Pitch Tracking The pitch tracker used in this research was developed by Daniel Burnett <ref> [Bur92] </ref>, and trained on multi-lingual data from the OGI Ten-language Telephone Speech Corpus. 102 A neural network-based voicing detector used frame-based PLP features to determine voiced and un-voiced regions of an utterance.
Reference: [C + 92] <author> R. A. Cole et al. </author> <title> Workshop on spoken language understanding. </title> <type> Technical Report CS/E 92-014, </type> <institution> Center for Spoken Language Understanding, Oregon Graduate Institute of Science and Technology, </institution> <year> 1992. </year>
Reference-contexts: The key to solving the problem of automatic language identification then, is the detection and exploitation of such differences between languages. 1.2 Motivation The Workshop on Spoken Language Understanding sponsored by the National Science Foundation <ref> [C + 92] </ref> has identified multi-lingual systems as one of the key research areas in the field of spoken language systems.
Reference: [CFC91] <author> J. W. Creekmore, M. A. Fanty, and R. A. Cole. </author> <title> A comparative study of five spectral representations for speaker-independent phonetic recognition. </title> <booktitle> In Proceedings of the 25th Asilomar Conference on Signals, Systems and Computers, </booktitle> <address> Pacific Grove, CA, </address> <month> November </month> <year> 1991. </year> <pages> 132 133 </pages>
Reference-contexts: Junqua [Jun89] demonstrated the superiority of PLP over standard LPC on an alphadigit recognition task. A comparison of PLP, DFT and cochleagrams [Lyo82, Sla88] on the English letter recognition task by Fanty 73 and Cole [FC91] showed the superiority of PLP over the other two representations. Creekmore et al. <ref> [CFC91] </ref> investigated five spectral representations|PLP, DFT and three representations based on LPC|for speaker-independent phonetic recognition using the TIMIT corpus. They found that the PLP outperformed the other representations with less computation and shorter learning times.
Reference: [CI82] <author> D. Cimarusti and R. B. Ives. </author> <title> Development of an automatic identification system of spoken languages: Phase 1. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing 82, </booktitle> <address> Paris, France, </address> <month> May </month> <year> 1982. </year>
Reference-contexts: Also, the lack of female speakers in the database and the use of read speech limit the generality of the results. 2.1.4 Cimarusti and Ives Cimarusti and Ives <ref> [CI82] </ref> conducted a feasibility study of a novel approach to automatic language identification that was not based on linguistic units such as phonetic segments or syllables. This approach applied pattern analysis techniques to acoustic features extracted from the speech signal. <p> 1 The corpus is distributed free of charge only to universities, not-for-profit organizations and research laboratories funded by the U.S. government. 126 6.2.2 Detailed Examination of the Broad Phonetic Approach While a majority of the approaches to automatic language identification over the past two decades have been frame-based statistical approaches <ref> [CI82, Ive86, Foi86, GMW89, Zis93] </ref>, very few researchers [LE80] have followed up on House and Neuberg's seminal work [HN77] in which they proposed that languages could be distinguished solely on the basis of sequential constraints on broad phonetic categories.
Reference: [CL92] <author> B. Chigier and H. C. Leung. </author> <title> The effects of signal representations, phonetic classification techniques and the telephone network. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing 92, </booktitle> <address> Banff, Alberta, Canada, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: First, accurate phoneme recognition from continuous speech is a difficult task, due to coartic-ulation effects that result in widely different articulatory and acoustic realizations of the same phoneme. The current state-of-the-art in speaker-independent phonetic recognition of American English is 78.0% for 39 phones <ref> [CL92] </ref> and 70.0% when all 61 TIMIT [LKS86, FDGM86] phones are considered [RF91]. Secondly, since languages differ in their phonetic inventories, a multi-language fine phonetic recognizer is essential to process speech from more than one language. Such a recognizer would have to be trained on a multi-language phonetic alphabet. <p> Creekmore et al. [CFC91] investigated five spectral representations|PLP, DFT and three representations based on LPC|for speaker-independent phonetic recognition using the TIMIT corpus. They found that the PLP outperformed the other representations with less computation and shorter learning times. Chigier and Leung <ref> [CL92] </ref> found that the PLP produced the lowest error-rates on a 39-phone recognition task using multi-layer perceptrons, for both the TIMIT and NTIMIT [JKBS90] corpuses.
Reference: [Com90] <editor> B. Comrie, editor. </editor> <booktitle> The World's Major Languages. </booktitle> <publisher> Oxford University Press, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: 56.0 PRVS 3739 63.5 64.1 64.9 INVS 13922 74.2 75.7 77.4 POVS 10745 69.7 71.8 73.5 Overall Performance 79.8 82.2 83.9 5.4 Language Classification To the best of my knowledge, all of the literature on linguistic and phonological differences between languages examines differences at the level of phonemes or phones <ref> [Cry87, Com90, Mad84] </ref>; levels that are beyond the scope of this dissertation. Therefore, visual inspection and statistical analyses of the broad phonetic segment sequences were the main tools used in the selection and development of features for language identification.
Reference: [Cry87] <author> D. </author> <title> Crystal. </title> <booktitle> The Cambridge Encyclopedia of Language, </booktitle> <pages> pages 280-339. </pages> <publisher> Cam-bridge University Press, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: The four languages belong to different language families. American English belongs to the Germanic group of Indo-European languages, Japanese is considered by many linguists to be a member of the Altaic family, while others believe it is an Isolate, i.e., belonging to no known language family <ref> [Cry87] </ref>. Mandarin Chinese is a Sino-Tibetan language, and Tamil is a Dravidian language widely spoken in Southern India and some parts of South-east Asia. 30 3.1.3 Speakers Twenty native speakers of each language 1 recorded their voices in our laboratory and were each paid $5 for their participation. <p> 56.0 PRVS 3739 63.5 64.1 64.9 INVS 13922 74.2 75.7 77.4 POVS 10745 69.7 71.8 73.5 Overall Performance 79.8 82.2 83.9 5.4 Language Classification To the best of my knowledge, all of the literature on linguistic and phonological differences between languages examines differences at the level of phonemes or phones <ref> [Cry87, Com90, Mad84] </ref>; levels that are beyond the scope of this dissertation. Therefore, visual inspection and statistical analyses of the broad phonetic segment sequences were the main tools used in the selection and development of features for language identification.
Reference: [Ead82] <author> S. J. </author> <title> Eady. Differences in the F 0 patterns of speech: Tone language versus stress language. </title> <journal> Language and Speech, </journal> <volume> 25(1) </volume> <pages> 29-42, </pages> <year> 1982. </year>
Reference-contexts: Hawaiian is known for its very limited consonant inventory. Prosodic patterns also differ significantly between languages. For example, it has been shown that fundamental frequency (F 0 ) patterns of continuous speech display different characteristics in Mandarin Chinese 1 2 (a tone language) and American English (a stress language) <ref> [Ead82] </ref>.
Reference: [FC91] <author> M. A. Fanty and R. A. Cole. </author> <title> A comparison of DFT, PLP and Cochleagram for alphabet recognition. </title> <booktitle> In Proceedings of the 25th Asilomar Conference on Signals, Systems and Computers, </booktitle> <address> Pacific Grove, CA, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: Junqua [Jun89] demonstrated the superiority of PLP over standard LPC on an alphadigit recognition task. A comparison of PLP, DFT and cochleagrams [Lyo82, Sla88] on the English letter recognition task by Fanty 73 and Cole <ref> [FC91] </ref> showed the superiority of PLP over the other two representations. Creekmore et al. [CFC91] investigated five spectral representations|PLP, DFT and three representations based on LPC|for speaker-independent phonetic recognition using the TIMIT corpus. They found that the PLP outperformed the other representations with less computation and shorter learning times.
Reference: [FCR92] <author> M. A. Fanty, R. A. Cole, and K. Roginski. </author> <title> English alphabet recognition with telephone speech. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: These features were empirically derived to capture the contextual information in the vicinity of each frame <ref> [FCR92] </ref>. Figure 5.1 shows the sampling intervals for the PLP coefficients in the context surrounding the frame to be classified. The solid boxes indicate 9 ms and 15 ms intervals over which PLP coefficients are averaged.
Reference: [FDGM86] <author> W. Fisher, G. R. Doddington, and K. Goudie-Marshall. </author> <title> The DARPA speech recognition research database: Specification and status. </title> <booktitle> In Proceedings DARPA Speech Recognition Workshop, </booktitle> <pages> pages 93-100, </pages> <month> February </month> <year> 1986. </year>
Reference-contexts: The current state-of-the-art in speaker-independent phonetic recognition of American English is 78.0% for 39 phones [CL92] and 70.0% when all 61 TIMIT <ref> [LKS86, FDGM86] </ref> phones are considered [RF91]. Secondly, since languages differ in their phonetic inventories, a multi-language fine phonetic recognizer is essential to process speech from more than one language. Such a recognizer would have to be trained on a multi-language phonetic alphabet.
Reference: [Foi86] <author> J. T. </author> <title> Foil. Language identification using noisy speech. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing 86, </booktitle> <address> Tokyo, Japan, </address> <year> 1986. </year>
Reference-contexts: Given that all of the nine production rules were based on F 0 , it would be interesting to see the performance of this system on female speakers, who tend to have higher values of F 0 than males. The database used had only male speakers. 2.1.6 Foil Foil <ref> [Foi86] </ref> was perhaps the first researcher to report on speech recorded from radio under noisy conditions (the typical signal-to-noise ratio was 5 dB). He imposed an additional constraint that language recognition be made using less than 10 seconds of speech. <p> 1 The corpus is distributed free of charge only to universities, not-for-profit organizations and research laboratories funded by the U.S. government. 126 6.2.2 Detailed Examination of the Broad Phonetic Approach While a majority of the approaches to automatic language identification over the past two decades have been frame-based statistical approaches <ref> [CI82, Ive86, Foi86, GMW89, Zis93] </ref>, very few researchers [LE80] have followed up on House and Neuberg's seminal work [HN77] in which they proposed that languages could be distinguished solely on the basis of sequential constraints on broad phonetic categories.
Reference: [FPC92] <author> M. A. Fanty, J. Pochmara, and R. A. Cole. </author> <title> An interactive environment for speech recognition research. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing 92, </booktitle> <address> Banff, Alberta, Canada, </address> <month> October </month> <year> 1992. </year> <month> 134 </month>
Reference-contexts: The seg-menter output was then corrected by trained human transcribers using an interactive display program described in <ref> [FPC92] </ref>. These broad phonetic transcriptions were used to train and evaluate later versions of the segmentation algorithm. 3.1.7 Corpus Statistics Not all speakers provided all 20 utterances. Five speakers in Tamil and one in Japanese recorded only 10 utterances each. <p> An interactive graphics program was used to display the waveform, play selected portions of the utterance, and to log information into a text file. The speech software tools used in the development of this corpus are described in detail in <ref> [FPC92] </ref>. Preliminary Verification and Evaluation. Each utterance was processed as follows: 39 * The utterance was chopped, if necessary, to remove the excess noise and/or silence flanking the speech. Care was taken to include at least 300 ms of "silence" before and after the speech.
Reference: [GJD87] <author> C. Grover, D. G. Jamieson, and M. B. Dobrovolsky. </author> <title> Intonation in English, French and German: Perception and production. </title> <journal> Language and Speech, </journal> <volume> 30(3) </volume> <pages> 277-295, </pages> <year> 1987. </year>
Reference-contexts: The intonation contours for declarative sentences are different from those for questions in many languages. For example, in English, declarative sentences have a falling contour, while questions have a rising contour <ref> [GJD87] </ref>. The two questions were included in the corpus to capture such intonational differences within languages. 3.1.5 Data Acquisition The speech was recorded using a Sennheiser HMD 224 noise-canceling microphone, low-pass filtered at 7.6 kHz and sampled at 16 kHz at 16-bit resolution.
Reference: [GMW89] <author> F.J. Goodman, A.F. Martin, and R.E. Wohlford. </author> <title> Improved automatic language identification in noisy speech. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing 89, </booktitle> <address> Glasgow, Scotland, </address> <month> May </month> <year> 1989. </year>
Reference-contexts: Also, given that the languages were from different language groups, it is likely that there are wide phonological differences between them, which might have helped in the identification process. 2.1.7 Goodman et al. Goodman et al. <ref> [GMW89] </ref> enhanced Foil's formant extraction technique for language identification by modifying and adding parameters, improving the classifier and reducing its channel sensitivity. A new formant peak-picking algorithm was devised that performed well even with very noisy speech. <p> 1 The corpus is distributed free of charge only to universities, not-for-profit organizations and research laboratories funded by the U.S. government. 126 6.2.2 Detailed Examination of the Broad Phonetic Approach While a majority of the approaches to automatic language identification over the past two decades have been frame-based statistical approaches <ref> [CI82, Ive86, Foi86, GMW89, Zis93] </ref>, very few researchers [LE80] have followed up on House and Neuberg's seminal work [HN77] in which they proposed that languages could be distinguished solely on the basis of sequential constraints on broad phonetic categories.
Reference: [Gop90] <author> M. Gopalakrishnan. </author> <title> Segmenting speech into broad phonetic categories using neural networks. </title> <type> Master's thesis, </type> <institution> Oregon Graduate Institute of Science & Technology, Beaverton, </institution> <address> OR, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: The procedure for data capture is described in Section 3.1.5. Signal Representation. A number of waveform and spectral parameters are computed in preparation for further processing. These parameters were empirically derived and were shown to be useful in the segmentation of English letters <ref> [Gop90] </ref>. The spectral parameters are generated from a 128-point discrete Fourier transform (DFT) computed on a 10 ms Hanning window. All parameters are computed every 3 ms. <p> The averaged spectral difference is computed 53 as the mean squared difference of the spectrum averaged N frames before and N frames after the frame under consideration. N was set to 8 on the basis of experiments performed in <ref> [Gop90] </ref>. The sda8000 parameter indicates changes in spectral energy from region to region. Sharp spectral changes indicate the presence of stop bursts. More gradual changes indicate vowel-nasal boundaries. <p> These features, described in Section 4.3.2.1, are designed to capture the phonetic and prosodic differences among the four languages. 54 4.2 Neural Network Segmentation The segmentation and broad classification algorithm described below is a variant of the one developed by Gopalakrishnan for isolated English letters <ref> [Gop90] </ref>. The training and test sets for segmentation are described in Section 3.1.8.1. 4.2.1 Training the Algorithm 4.2.1.1 Hand-labeling Both the training and test utterances were hand-labeled with the seven broad phonetic category labels and checked by a second labeler for correctness and consistency. The hand-labeling was semi-automatic. <p> The inputs to the network were normalized to lie between -1.0 and 1.0. This scaling was required to minimize local-minimum problems [BC89] and to accentuate differences between the features for the seven broad phonetic categories. The normalization procedure for the segmenter features is described in detail in <ref> [Gop90] </ref>. The networks were trained using backpropagation with conjugate gradient optimization [BC89]. Each network was trained on 50 iterations through the training vectors. The trained network was then evaluated on the vectors in the development set to measure generalization.
Reference: [Her90] <author> H. Hermansky. </author> <title> Perceptual linear predictive (PLP) analysis of speech. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 87 </volume> <pages> 1738-1752, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: The spectral coefficients were derived from a 7th-order PLP analysis <ref> [Her90] </ref> on the waveform. This analysis resulted in 8 coefficients (1 energy coefficient and 7 spectral coefficients) for each 3 ms frame of the utterance. <p> There were, however, two major differences: * the spectral coefficients were obtained from a PLP analysis of the waveform <ref> [Her90] </ref> instead of the DFT. * frames randomly sampled from each hand-labeled segment (Section 4.2.1.2) for training were augmented with frames sampled at the edges of segments. <p> For example, it has been shown that the human ear is most sensitive in the middle range of the auditory spectrum, and has decreasing frequency resolution above 800 Hz. The PLP analysis of speech <ref> [Her90] </ref> attempts to mimic the properties of the human auditory system by applying mathematical transformations to the short-term power spectrum produced by conventional LPC analysis, then approximating the new, auditory-like spectrum with an autoregressive all-pole model.
Reference: [HN77] <author> A. S. House and E. P. Neuberg. </author> <title> Toward automatic identification of the language of an utterance. I. Preliminary methodological considerations. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 62(3) </volume> <pages> 708-713, </pages> <year> 1977. </year>
Reference-contexts: On the other hand, it is clear that broad phonetic category information can be utilized to distinguish between languages <ref> [HN77, LE80] </ref>, allowing us to circumvent the cumbersome and difficult task of fine phonetic recognition. An additional advantage is that broad phonetic categories such as vowels, obstruents and nasals are relatively invariant across languages, eliminating the need for language-specific phonetic or phonemic alphabets. <p> The author attributes it to a lack of familiarity with the two languages resulting in selection of inappropriate reference sounds for these languages. 2.1.2 House and Neuberg In a landmark study, House and Neuberg <ref> [HN77] </ref> demonstrated the feasibility of using sequences of broad phonetic categories of speech to identify languages. They reasoned that, since accurate phoneme recognition is beyond the current state-of-the-art (true 14 in 1977; true in 1993), the information provided by broad phonetic categories (stop, fricative, vowel, silence) should be examined. <p> While Zissman used a completely different approach (HMMs and tied Gaussian probability densities), Hazen and Zue's approach, like the research described in this dissertation, was inspired by the work of House and Neuberg <ref> [HN77] </ref>, who proposed that languages can be differentiated based on sequential constraints on broad phonetic categories. The fact that two other approaches to the same problem, using the same corpus, have arrived at comparable results, is an indication of the inherent difficulty of the problem. <p> the U.S. government. 126 6.2.2 Detailed Examination of the Broad Phonetic Approach While a majority of the approaches to automatic language identification over the past two decades have been frame-based statistical approaches [CI82, Ive86, Foi86, GMW89, Zis93], very few researchers [LE80] have followed up on House and Neuberg's seminal work <ref> [HN77] </ref> in which they proposed that languages could be distinguished solely on the basis of sequential constraints on broad phonetic categories. The broad phonetic approach described in this dissertation and Hazen and Zue's work described above are two recent studies that have expanded on House and Neuberg's work.
Reference: [HZ93] <author> T. J. Hazen and V. W. Zue. </author> <title> Automatic language identification using a segment-based approach. </title> <booktitle> In Proceedings 3rd European Conference on Speech Communication and Technology (Eurospeech 93), </booktitle> <address> Berlin, Germany, </address> <month> Septem-ber </month> <year> 1993. </year>
Reference-contexts: As a result, the review does not contain any mention of the author's published work on automatic language identification using high-quality speech [MC92a] or telephone speech [MC92b]. These are described in chapters 4 and 5 respectively. Marc Zissman's work [Zis93] and Hazen and Zue's work <ref> [HZ93] </ref> using the OGI Multi-language Telephone Speech Corpus ([MCO92] and Chapter 3) are described in Chapter 6. Also, since the focus of this review is on studies in automatic language identification, Atkinson's work on human listening experiments with English and Spanish [Atk68] is excluded. <p> His results on the EnglishL 0 and L Other tasks were also comparable to our results. He did not examine the EnglishL 0 Other task. Timothy Hazen and Victor Zue at MIT have reported results on a segment-based approach to automatic language identification, designed around a formal probabilistic framework <ref> [HZ93] </ref>. Using probablistic models for the phonotactic, prosodic and acoustic properties of the different languages in the corpus, they obtained an identification accuracy of 47.7% on the development test set for the ten-language task.
Reference: [IIK90] <author> H. Irii, K. Itoh, and N. Kitawaki. </author> <title> Multilingual speech database for evaluating quality of digitized speech. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing 90, </booktitle> <pages> pages 1025-1028, </pages> <address> Kobe, Japan, </address> <year> 1990. </year>
Reference-contexts: The data was taken from a multilingual speech database distributed by NTT, Japan <ref> [IIK90] </ref>. It consisted of 16 sentences uttered twice by 4 male and 4 female speakers in each of 20 languages (American English, Arabic, Mandarin Chinese, Danish, Dutch, English, Finnish, French, German, Greek, Hindi, Hungarian, Italian, Japanese, Norwegian, 22 Polish, Portuguese, Russian, Spanish and Swedish) 1 .
Reference: [Ive86] <author> R. B. Ives. </author> <title> A minimal rule AI expert system for real-time classification of natural spoken languages. </title> <booktitle> In Proceedings 2nd Annual Artificial Intelligence and Advanced Computer Technology Conference, </booktitle> <address> Long Beach, CA, </address> <month> April-May </month> <year> 1986. </year>
Reference-contexts: Also, the relatively small number of speakers per language (five) makes it likely that the system is not truly speaker-independent. 2.1.5 Ives Using an extended database for the same languages as the previous study, Ives <ref> [Ive86] </ref> developed an expert system for real-time automatic language identification. The goal of this effort was to develop a set of rules which would minimize the time required for classification. <p> 1 The corpus is distributed free of charge only to universities, not-for-profit organizations and research laboratories funded by the U.S. government. 126 6.2.2 Detailed Examination of the Broad Phonetic Approach While a majority of the approaches to automatic language identification over the past two decades have been frame-based statistical approaches <ref> [CI82, Ive86, Foi86, GMW89, Zis93] </ref>, very few researchers [LE80] have followed up on House and Neuberg's seminal work [HN77] in which they proposed that languages could be distinguished solely on the basis of sequential constraints on broad phonetic categories.
Reference: [JFC91] <author> R. D. T. Janssen, M. A. Fanty, and R. A. Cole. </author> <title> Speaker-independent phonetic classification of continuous english letters. </title> <booktitle> In Proceedings 1991 International Joint Conference on Neural Networks, </booktitle> <address> Seattle, WA, </address> <month> July </month> <year> 1991. </year>
Reference-contexts: PLP analysis also has the advantage of producing a more compact representation than the DFT. Janssen et al. <ref> [JFC91] </ref> showed that 8 PLP coefficients per frame worked just as well as 40 DFT coefficients per frame for frame-based phonetic classification of English letters using high-quality speech. Junqua [Jun89] demonstrated the superiority of PLP over standard LPC on an alphadigit recognition task.
Reference: [JKBS90] <author> C. Jankowski, A. Kalyanswamy, S. Basson, and J. Spitz. NTIMIT: </author> <title> a phonetically balanced, continuous speech, telephone bandwidth speech database. </title> <booktitle> In 135 Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing 90, </booktitle> <address> Albuquerque, NM, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: They found that the PLP outperformed the other representations with less computation and shorter learning times. Chigier and Leung [CL92] found that the PLP produced the lowest error-rates on a 39-phone recognition task using multi-layer perceptrons, for both the TIMIT and NTIMIT <ref> [JKBS90] </ref> corpuses. In the light of such substantial evidence for its effectiveness, for both high-quality and telephone speech, PLP analysis was the spectral representation of choice for the spectral-based features used in this research.
Reference: [Jun89] <author> J. C. Junqua. </author> <title> Towards Robustness in Isolated-word Automatic Speech Recognition. </title> <type> PhD thesis, </type> <institution> University of Nancy I, </institution> <year> 1989. </year>
Reference-contexts: PLP analysis also has the advantage of producing a more compact representation than the DFT. Janssen et al. [JFC91] showed that 8 PLP coefficients per frame worked just as well as 40 DFT coefficients per frame for frame-based phonetic classification of English letters using high-quality speech. Junqua <ref> [Jun89] </ref> demonstrated the superiority of PLP over standard LPC on an alphadigit recognition task. A comparison of PLP, DFT and cochleagrams [Lyo82, Sla88] on the English letter recognition task by Fanty 73 and Cole [FC91] showed the superiority of PLP over the other two representations.
Reference: [KKWE92] <author> S. C. Kwasny, B. L. Kalman, W. Wu, and A. M. Engebretson. </author> <title> Identifying language from speech: An example of high-level, statistically-based feature extraction. </title> <booktitle> In Proceedings 14th Annual Conference of the Cognitive Science Society, </booktitle> <year> 1992. </year>
Reference-contexts: The absence of female speakers in the data set also limits 26 the generality of the results. 2.1.11 Kwasny et al. Kwasny et al. <ref> [KKWE92] </ref> reported preliminary results on a two-speaker two-language identification task using acoustic (raw speech waveform) features and neural network classifiers. The data consisted of 12.5 second samples of read speech in English and French from two bilingual speakers. Two different samples were recorded for each language for each speaker.
Reference: [LD74] <author> R. G. Leonard and G. R. Doddington. </author> <title> Automatic language identification. </title> <type> Technical Report RADC-TR-74-200, </type> <institution> Air Force Rome Air Development Center, </institution> <month> August </month> <year> 1974. </year>
Reference-contexts: The reader is directed to the references for further details. 2.1.1 The Texas Instruments Effort The first sustained effort in automatic language identification was carried out between 1973 and 1980 at Texas Instruments (TI), and is documented in a series of four reports <ref> [LD74, LD75, LD78, Leo80] </ref>. The basic philosophy underlying the TI approach was that languages differ by the frequency of occurrence of certain reference sounds or sound sequences. The sounds or sound sequences characteristic of a language occur more often in that language than in any other language under consideration. <p> Therefore, the likelihoods of the languages, given these sequences, could be computed and used to make decisions in reasonably short times. 9 Table 2.1: Studies in Automatic Language Identification 10 Study 1. The first study <ref> [LD74] </ref> concentrated on single reference sounds. The data consisted of read text from 100 adult male speakers of 5 languages, referred to simply as L 1 , L 2 , L 3 , L 4 , and L 5 .
Reference: [LD75] <author> R. G. Leonard and G. R. Doddington. </author> <title> Automatic language identification. </title> <type> Technical Report RADC-TR-75-264, </type> <institution> Air Force Rome Air Development Center, </institution> <month> October </month> <year> 1975. </year>
Reference-contexts: The reader is directed to the references for further details. 2.1.1 The Texas Instruments Effort The first sustained effort in automatic language identification was carried out between 1973 and 1980 at Texas Instruments (TI), and is documented in a series of four reports <ref> [LD74, LD75, LD78, Leo80] </ref>. The basic philosophy underlying the TI approach was that languages differ by the frequency of occurrence of certain reference sounds or sound sequences. The sounds or sound sequences characteristic of a language occur more often in that language than in any other language under consideration. <p> Pairwise identification accuracy of the 10 language pairs ranged from 60% to 100%. Overall accuracy was 64% with a nearest neighbor decision rule using the pairwise identification results. The identification decision was made using 60 seconds of speech. 11 Study 2. The second phase of the study <ref> [LD75] </ref> used the same data as above, but used sequences of several phoneme-like segments for classification. Another improvement was the use of a time-frequency scanning error measure to accept or reject hypothesized occurrences of component sound segments.
Reference: [LD78] <author> R. G. Leonard and G. R. Doddington. </author> <title> Automatic language discrimination. </title> <type> Technical Report RADC-TR-78-5, </type> <institution> Air Force Rome Air Development Center, </institution> <month> January </month> <year> 1978. </year>
Reference-contexts: The reader is directed to the references for further details. 2.1.1 The Texas Instruments Effort The first sustained effort in automatic language identification was carried out between 1973 and 1980 at Texas Instruments (TI), and is documented in a series of four reports <ref> [LD74, LD75, LD78, Leo80] </ref>. The basic philosophy underlying the TI approach was that languages differ by the frequency of occurrence of certain reference sounds or sound sequences. The sounds or sound sequences characteristic of a language occur more often in that language than in any other language under consideration. <p> A decision rule using sequences of length 5 in combination with sequences of length 1 yielded 70% accuracy on the test data, with the same threshold values as above. Study 3. Departing from the automatic selection of reference sounds used in the first two studies, the third study <ref> [LD78] </ref> used an interactive approach to the generation of reference sounds. Manual selection of reference sounds was followed by automatic isolation of the representative occurrences of these sounds from the speech data. The isolated sounds were then manually verified before further processing.
Reference: [LE80] <author> K.P. Li and T. J. Edwards. </author> <title> Statistical models for automatic language identification. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing 80, </booktitle> <address> Denver, CO, </address> <month> April </month> <year> 1980. </year>
Reference-contexts: On the other hand, it is clear that broad phonetic category information can be utilized to distinguish between languages <ref> [HN77, LE80] </ref>, allowing us to circumvent the cumbersome and difficult task of fine phonetic recognition. An additional advantage is that broad phonetic categories such as vowels, obstruents and nasals are relatively invariant across languages, eliminating the need for language-specific phonetic or phonemic alphabets. <p> Their work <ref> [LE80] </ref> represents one of the earliest efforts to develop statistical inference techniques to discriminate among languages using real speech data . <p> only to universities, not-for-profit organizations and research laboratories funded by the U.S. government. 126 6.2.2 Detailed Examination of the Broad Phonetic Approach While a majority of the approaches to automatic language identification over the past two decades have been frame-based statistical approaches [CI82, Ive86, Foi86, GMW89, Zis93], very few researchers <ref> [LE80] </ref> have followed up on House and Neuberg's seminal work [HN77] in which they proposed that languages could be distinguished solely on the basis of sequential constraints on broad phonetic categories.
Reference: [Leo80] <author> R. G. Leonard. </author> <title> Language recognition test and evaluation. </title> <type> Technical Report RADC-TR-80-83, </type> <institution> Air Force Rome Air Development Center, </institution> <month> March </month> <year> 1980. </year>
Reference-contexts: The reader is directed to the references for further details. 2.1.1 The Texas Instruments Effort The first sustained effort in automatic language identification was carried out between 1973 and 1980 at Texas Instruments (TI), and is documented in a series of four reports <ref> [LD74, LD75, LD78, Leo80] </ref>. The basic philosophy underlying the TI approach was that languages differ by the frequency of occurrence of certain reference sounds or sound sequences. The sounds or sound sequences characteristic of a language occur more often in that language than in any other language under consideration. <p> This resulted in much improved identification performance. A second set of 13 reference sounds and the same silence measure provided 80% correct classification on the test set speakers. Study 4. In the final study <ref> [Leo80] </ref>, the interactive approach to reference sound generation was extended to allow more accuracy in specifying reference sounds and more flexibility in the allowed types of reference sounds. <p> However, such manual determination of the reference sounds in the languages under consideration mandates the researchers' a priori knowledge of the languages. This could severely limit addition of languages to the identification system. This weakness is apparent in the fourth study <ref> [Leo80] </ref> in which there is a degradation in performance (from 72% to 62%) with the addition of the two languages L 7 and L 8 .
Reference: [LKS86] <author> L. Lamel, R. Kassel, and S. Seneff. </author> <title> Speech database development: Design and analysis of the acoustic-phonetic corpus. </title> <booktitle> In Proceedings DARPA Speech Recognition Workshop, </booktitle> <pages> pages 100-110, </pages> <month> February </month> <year> 1986. </year>
Reference-contexts: The current state-of-the-art in speaker-independent phonetic recognition of American English is 78.0% for 39 phones [CL92] and 70.0% when all 61 TIMIT <ref> [LKS86, FDGM86] </ref> phones are considered [RF91]. Secondly, since languages differ in their phonetic inventories, a multi-language fine phonetic recognizer is essential to process speech from more than one language. Such a recognizer would have to be trained on a multi-language phonetic alphabet.
Reference: [LRH90] <author> T. K. Leen, M. Rudnick, and D. Hammerstrom. </author> <title> Hebbian feature discovery improves classifier efficieny. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <address> San Diego, CA, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Principal Component Analysis (PCA). This technique has been shown to be useful in reducing the dimensionality of the feature space for speech recognition tasks with negligible loss in performance <ref> [LRH90] </ref>. There is no rigorous method of determining the optimal number of principal components of a given set of vectors. A good heuristic, however, is to generate a plot of the logarithm of the eigenvalues of the features.
Reference: [Lyo82] <author> R. F. Lyon. </author> <title> A computational model of filtering, detection, and compression in the cochlea. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing 82, </booktitle> <month> May </month> <year> 1982. </year> <month> 136 </month>
Reference-contexts: Junqua [Jun89] demonstrated the superiority of PLP over standard LPC on an alphadigit recognition task. A comparison of PLP, DFT and cochleagrams <ref> [Lyo82, Sla88] </ref> on the English letter recognition task by Fanty 73 and Cole [FC91] showed the superiority of PLP over the other two representations. Creekmore et al. [CFC91] investigated five spectral representations|PLP, DFT and three representations based on LPC|for speaker-independent phonetic recognition using the TIMIT corpus.
Reference: [Mad84] <author> I. Maddieson. </author> <title> Patterns of Sounds. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, Great Britain, </address> <year> 1984. </year>
Reference-contexts: 56.0 PRVS 3739 63.5 64.1 64.9 INVS 13922 74.2 75.7 77.4 POVS 10745 69.7 71.8 73.5 Overall Performance 79.8 82.2 83.9 5.4 Language Classification To the best of my knowledge, all of the literature on linguistic and phonological differences between languages examines differences at the level of phonemes or phones <ref> [Cry87, Com90, Mad84] </ref>; levels that are beyond the scope of this dissertation. Therefore, visual inspection and statistical analyses of the broad phonetic segment sequences were the main tools used in the selection and development of features for language identification.
Reference: [Mak75] <author> J. Makhoul. </author> <title> Spectral Linear Prediction: Properties and Applications. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 23 </volume> <pages> 283-296, </pages> <year> 1975. </year>
Reference-contexts: Such edge-sampling has been shown to improve frame classification performance for English phonemes [Rog91]. 5.3.1 Why Use PLP? Conventional linear predictive coding (LPC) analysis <ref> [Mak75] </ref> models the power spectrum of speech equally well at all frequencies of the analysis band, making it inconsistent with human auditory processing.
Reference: [MBA + 93] <author> Y. K. Muthusamy, K. M. Berkling, T. Arai, R. A. Cole, and E. Barnard. </author> <title> A comparison of approaches to automatic language identification using telephone speech. </title> <booktitle> In Proceedings 3rd European Conference on Speech Communication and Technology (Eurospeech 93), </booktitle> <address> Berlin, Germany, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: Such a corpus would be invaluable in pursuing a phonemic approach to automatic language identification and would serve as a common database for assessment and comparison of techniques. 6.3.2 Phoneme-based Approaches Preliminary results are in from a phoneme-based approach to automatic language identification for the English-Japanese task <ref> [Ber93, MBA + 93] </ref>.
Reference: [MC92a] <author> Y. K. Muthusamy and R. A. Cole. </author> <title> A segment-based automatic language identification system. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: As a result, the review does not contain any mention of the author's published work on automatic language identification using high-quality speech <ref> [MC92a] </ref> or telephone speech [MC92b]. These are described in chapters 4 and 5 respectively. Marc Zissman's work [Zis93] and Hazen and Zue's work [HZ93] using the OGI Multi-language Telephone Speech Corpus ([MCO92] and Chapter 3) are described in Chapter 6.
Reference: [MC92b] <author> Y. K. Muthusamy and R. A. Cole. </author> <title> Automatic segmentation and identification of ten languages using telephone speech. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing 92, </booktitle> <address> Banff, Alberta, Canada, </address> <month> Oc-tober </month> <year> 1992. </year>
Reference-contexts: As a result, the review does not contain any mention of the author's published work on automatic language identification using high-quality speech [MC92a] or telephone speech <ref> [MC92b] </ref>. These are described in chapters 4 and 5 respectively. Marc Zissman's work [Zis93] and Hazen and Zue's work [HZ93] using the OGI Multi-language Telephone Speech Corpus ([MCO92] and Chapter 3) are described in Chapter 6.
Reference: [MCO92] <author> Y. K. Muthusamy, R. A. Cole, and B. T. Oshika. </author> <title> The OGI multi-language telephone speech corpus. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing 92, </booktitle> <address> Banff, Alberta, Canada, </address> <month> October </month> <year> 1992. </year>
Reference: [NUS92] <author> S. Nakagawa, Y. Ueda, and T. Seino. </author> <title> Speaker-independent, text-independent language identification by HMM. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing 92, </booktitle> <address> Banff, Alberta, Canada, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: The one-HMM-per-language concept is an interesting one and merits further investigation. 2.1.10 Nakagawa et al. Nakagawa et al. <ref> [NUS92] </ref> examined the application of different HMM-based methods to language identification using acoustic features. They compared the performance of 4 methods: VQ (vector quantization), discrete HMM, continuous density HMM, and mixtured Gaussian distribution model.
Reference: [RF91] <author> T. Robinson and F. Fallside. </author> <title> A recurrent error propagation network speech recognition system. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 5 </volume> <pages> 259-274, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: The current state-of-the-art in speaker-independent phonetic recognition of American English is 78.0% for 39 phones [CL92] and 70.0% when all 61 TIMIT [LKS86, FDGM86] phones are considered <ref> [RF91] </ref>. Secondly, since languages differ in their phonetic inventories, a multi-language fine phonetic recognizer is essential to process speech from more than one language. Such a recognizer would have to be trained on a multi-language phonetic alphabet.
Reference: [Rog91] <author> K. Roginski. </author> <title> A neural network phonetic classifier for telephone speech. </title> <type> Master's thesis, </type> <institution> Oregon Graduate Institute of Science & Technology, Beaverton, </institution> <address> OR, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: The design and development of this corpus, and the subdivision into training, development and test sets are described in detail in Section 3.2. 5.3 Broad Phonetic Category Segmentation The broad phonetic segmentation algorithm described here is a variant of the fine phonetic segmentation algorithm developed for telephone speech in English <ref> [Rog91] </ref>. It is similar to the one described in Section 4.2 for high-quality speech, in the sense that it uses the same seven broad phonetic categories and the same procedure: frame-based neural network classification followed by a Viterbi search using duration and bigram 72 probabilities. <p> Such edge-sampling has been shown to improve frame classification performance for English phonemes <ref> [Rog91] </ref>. 5.3.1 Why Use PLP? Conventional linear predictive coding (LPC) analysis [Mak75] models the power spectrum of speech equally well at all frequencies of the analysis band, making it inconsistent with human auditory processing. <p> The energy coefficient was normalized with respect to the minimum and maximum values in each utterance using a formula similar to that in Equation 4.1. 5.3.2 Edge-sampling Roginski <ref> [Rog91] </ref> developed a neural network-based fine phonetic classifier for telephone speech for the English alphabet recognition task. He demonstrated the benefits of sampling at the edges of the hand-labeled segments in addition to sampling frames at random within the segments.
Reference: [SAG91] <author> M. Savic, E. Acosta, and S. K. Gupta. </author> <title> An automatic language identification system. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech and Signal Processing 91, </booktitle> <address> Toronto, Canada, </address> <month> May </month> <year> 1991. </year> <month> 137 </month>
Reference-contexts: The duration of the speech and the listener's linguistic background clearly influence language identification performance. These effects need to be examined in greater depth. Increasing the number of listeners would also increase the reliability of the results. 2.1.9 Savic et al. Savic et al. <ref> [SAG91] </ref> reported preliminary work on language identification using HMMs and pitch contours. The data consisted of 10 minutes of read speech in 4 languages: English, Hindi, Mandarin Chinese and Spanish, recorded in a noise-free room.
Reference: [Sla88] <author> M. Slaney. </author> <title> Lyon's Cochlear Model. </title> <type> Technical report, </type> <institution> Apple Computer Inc., </institution> <year> 1988. </year>
Reference-contexts: Junqua [Jun89] demonstrated the superiority of PLP over standard LPC on an alphadigit recognition task. A comparison of PLP, DFT and cochleagrams <ref> [Lyo82, Sla88] </ref> on the English letter recognition task by Fanty 73 and Cole [FC91] showed the superiority of PLP over the other two representations. Creekmore et al. [CFC91] investigated five spectral representations|PLP, DFT and three representations based on LPC|for speaker-independent phonetic recognition using the TIMIT corpus.
Reference: [Sta91] <institution> Statistical Sciences, Inc., </institution> <address> Seattle, Washington. </address> <note> S-PLUS User's Manual Volume 2, </note> <month> September </month> <year> 1991. </year>
Reference-contexts: They were extensively used in the exploratory analyses of the features described in this chapter. A boxplot <ref> [Sta91] </ref> is a graphical representation showing the center and spread of a data distribution, along with a display of unusually deviant data points, called outliers. is located at the median of the data. This estimates the center of the distribution. <p> An analysis of variance using the one-way layout model <ref> [BHH78, Sta91] </ref> was conducted to determine the usefulness of the 126 segment-triple features in distinguishing between the 10 languages. Of the 63 segment-triples, five produced p-values greater than 0.01. These were discarded.
Reference: [Sug91a] <author> M. Sugiyama. </author> <title> Automatic language recognition using acoustic features. </title> <type> Technical Report TR-I-0167, </type> <institution> ATR Interpreting Telephony Research Laboratories, </institution> <year> 1991. </year>
Reference-contexts: The robustness of the formant peak-picking algorithm in noise is an important result, since most communication channels are characterized by low signal-to-noise ratios. The lack of information regarding the languages used and the actual classification results precludes comparisons of these results with other work. 2.1.8 Sugiyama Sugiyama <ref> [Sug91b, Sug91a] </ref> proposed two language identification algorithms that were based on vector quantization and used acoustic features of the speech signal such as LPC coefficients, autocorrelation coefficients and delta-cepstral coefficients. The data was taken from a multilingual speech database distributed by NTT, Japan [IIK90].
Reference: [Sug91b] <author> M. Sugiyama. </author> <title> Automatic language recognition using acoustic features. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech and Signal Processing 91, </booktitle> <address> Toronto, Canada, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: The robustness of the formant peak-picking algorithm in noise is an important result, since most communication channels are characterized by low signal-to-noise ratios. The lack of information regarding the languages used and the actual classification results precludes comparisons of these results with other work. 2.1.8 Sugiyama Sugiyama <ref> [Sug91b, Sug91a] </ref> proposed two language identification algorithms that were based on vector quantization and used acoustic features of the speech signal such as LPC coefficients, autocorrelation coefficients and delta-cepstral coefficients. The data was taken from a multilingual speech database distributed by NTT, Japan [IIK90].
Reference: [Zis93] <author> M. A. Zissman. </author> <title> Automatic language identification using gaussian mixture and hidden markov models. </title> <booktitle> In Proceedings International Conference on Acoustics, Speech and Signal Processing 93, </booktitle> <address> Minneapolis, MN, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: As a result, the review does not contain any mention of the author's published work on automatic language identification using high-quality speech [MC92a] or telephone speech [MC92b]. These are described in chapters 4 and 5 respectively. Marc Zissman's work <ref> [Zis93] </ref> and Hazen and Zue's work [HZ93] using the OGI Multi-language Telephone Speech Corpus ([MCO92] and Chapter 3) are described in Chapter 6. Also, since the focus of this review is on studies in automatic language identification, Atkinson's work on human listening experiments with English and Spanish [Atk68] is excluded. <p> Marc Zissman at MIT Lincoln Laboratory has reported results of a HMM-based approach to automatic language identification <ref> [Zis93] </ref>. Using continuous observation, ergodic hidden Markov models (HMMs) with tied Gaussian observation probability densities, he obtained an identification accuracy of 46.0% on the development test set for the ten-language task. <p> 1 The corpus is distributed free of charge only to universities, not-for-profit organizations and research laboratories funded by the U.S. government. 126 6.2.2 Detailed Examination of the Broad Phonetic Approach While a majority of the approaches to automatic language identification over the past two decades have been frame-based statistical approaches <ref> [CI82, Ive86, Foi86, GMW89, Zis93] </ref>, very few researchers [LE80] have followed up on House and Neuberg's seminal work [HN77] in which they proposed that languages could be distinguished solely on the basis of sequential constraints on broad phonetic categories.
References-found: 54

