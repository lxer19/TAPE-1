URL: http://www.neci.nj.nec.com/homepages/lawrence/papers/nl-ijcai95/nl-ijcai95-letter.ps.Z
Refering-URL: http://www.neci.nj.nec.com/homepages/lawrence/papers.html
Root-URL: http://www.neci.nj.nec.com
Email: flawrence,sandiway,gilesg@research.nj.nec.com  
Title: On the Applicability of Neural Network and Machine Learning Methodologies to Natural Language Processing  
Author: Steve Lawrence Sandiway Fong, C. Lee Giles 
Address: 4 Independence Way Princeton NJ 08540  4072, Australia.  College Park, MD 20742.  
Affiliation: NEC Research Institute  Electrical and Computer Engineering, University of Queensland, St. Lucia Qld  Institute for Advanced Computer Studies, University of Maryland,  
Note: Appears in Workshop on New Approaches to Learning for Natural Language Processing, International Joint Conference on Artificial Intelligence, IJCAI-95, Montreal, Canada, pp. 18, 1996.  Also with  Also with the  
Abstract: How can we apply neural network and machine learning methodologies to natural language processing? In this paper we consider the task of training a neural network to classify natural language sentences as grammatical or un-grammatical thereby exhibiting the same kind of discriminatory power provided by the Principles and Parameters linguistic framework, or Government-and-Binding theory. We have investigated the following models: feed-forward neural networks, Frasconi-Gori-Soda and Back-Tsoi locally recurrent neural networks, Williams and Zipser and Elman recurrent neural networks, Euclidean and edit-distance nearest-neighbors, simulated annealing, and decision trees. Non-neural network machine learning methods are included primarily for comparison. Initial simulations were only partially successful by using a large temporal window as input to the models. Investigation indicated that success obtained this way did not imply that the models had learnt the grammar to a significant degree. Attempts to train networks with small temporal windows failed until we implemented several techniques aimed at avoiding local minima. We discuss the strengths and weaknesses of learning as compared to manual encoding, and we consider the similarities and differences between the various neural network and machine learning approaches. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Allen, R. B. </author> <year> (1983), </year> <title> Sequential connectionist networks for answering simple questions about a microworld, </title> <booktitle> in `5th Annual Proceedings of the Cognitive Science Society', </booktitle> <pages> pp. 489495. </pages>
Reference: <author> Back, A. </author> <year> (1992), </year> <title> New Techniques for Nonlinear System Identification: </title>
Reference-contexts: neuron k in layer l, N l is the number of neurons in layer l, w l ki is the weight connecting neuron k in layer l to neuron i in layer l 1, y l 0 = 1 (bias), and f is commonly a sigmoid function. 4 Definition 2 <ref> (Back 1992) </ref> An FIR MLP with L lay-ers excluding the input layer (0; 1; :::; L), FIR filters of order n b , and N 0 ; N 1 ; :::; N L neurons per layer, is defined as: y l k (t) (3) k (t) = i=0 ki (t) j=0
References-found: 2

