URL: http://www.tns.lcs.mit.edu/~djw/library/sigops96/baldeschwieler.ps.gz
Refering-URL: http://www.tns.lcs.mit.edu/~djw/library/sigops96/index.html
Root-URL: 
Email: feric14,brewerg@cs.berkeley.edu  rdb@cs.utexas.edu  
Title: Atlas: An Infrastructure for Global Computing  
Author: J. Eric Baldeschwieler Robert D. Blumofe Eric A. Brewer 
Address: Berkeley, California 94720  Austin, Texas 78712  
Affiliation: Computer Science Division The University of California at Berkeley  Department of Computer Sciences The University of Texas at Austin  
Abstract: In this paper, we present a proposed system architecture for global computing that we call Atlas, and we describe an early prototype that implements several of the mechanisms and policies that comprise the proposed architecture. Atlas is designed to execute parallel multithreaded programs on the networked computing resources of the world. The Atlas system is a marriage of existing technologies from Java and Cilk together with some new technologies needed to extend the system into the global domain.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ken Arnold and James Gosling. </author> <title> The Java Programming Language. </title> <publisher> Addison-Wesley, </publisher> <year> 1996. </year> <title> More information about Java and the Java virtual machine is available on the World-Wide Web at http://java.sun.com. </title>
Reference-contexts: Reasonable performance: The performance should show linear speedup for large classes of applications and should have sufficiently low overhead so as to see a benefit even with only a few machines. Atlas realizes these properties by combining existing mechanisms and policies from Java <ref> [1] </ref> and Cilk (pronounced "silk") [2, 4] together with some new mechanisms and policies that extend Atlas into a global computing infrastructure. Java is an object-oriented programming language together with a virtual-machine specification.
Reference: [2] <author> Robert D. Blumofe. </author> <title> Executing Multithreaded Programs Efficiently. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: Reasonable performance: The performance should show linear speedup for large classes of applications and should have sufficiently low overhead so as to see a benefit even with only a few machines. Atlas realizes these properties by combining existing mechanisms and policies from Java [1] and Cilk (pronounced "silk") <ref> [2, 4] </ref> together with some new mechanisms and policies that extend Atlas into a global computing infrastructure. Java is an object-oriented programming language together with a virtual-machine specification. Java programs are compiled to machine-independent byte codes that are executed by a virtual-machine interpreter running on the host machine. <p> Cilk is an algorithmic, C-based parallel multithreaded programming language together with a runtime system that employs a provably efficient thread-scheduling algorithm based on the technique of "work stealing" <ref> [2, 5] </ref> in which idle processors steal threads from randomly chosen victims. Cilk programs execute with efficient and predictable performance, guaranteed. In addition, the Cilk-NOW runtime system [2] provides automatic fault tolerance and adaptive parallelism for Cilk programs 1 executing on a network of workstations. <p> Cilk programs execute with efficient and predictable performance, guaranteed. In addition, the Cilk-NOW runtime system <ref> [2] </ref> provides automatic fault tolerance and adaptive parallelism for Cilk programs 1 executing on a network of workstations. Atlas adapts the Cilk programming model to Java; it extends Cilk's work-stealing scheduler with a hierarchy; and it borrows mechanisms from Cilk-NOW to implement adaptive parallelism and fault tolerance. <p> When a native library is used, the application is limited to run on machines that have or can dynamically link in that library, which may also limit which platforms the application can exploit. The Atlas programming model is a straightforward adaptation of the Cilk programming model <ref> [2, 4] </ref> to Java with the addition of a URL-based file system. A complete description of this programming model is beyond the scope of this paper. <p> Thus, compute servers execute locally in depth-first order, and they steal in breadth-first order. In the Atlas programming model, a procedure may synchronize only with its children, and for such programs, this work-stealing algorithm is efficient both in theory <ref> [2, 5] </ref> and in practice [2, 4]. Specifically, programs achieve linear speedup whenever the average parallelism is greater than the number of processors. Moreover, programs are guaranteed to make efficient use of stack space, and the scheduler existentially minimizes communication to within a constant factor. <p> Thus, compute servers execute locally in depth-first order, and they steal in breadth-first order. In the Atlas programming model, a procedure may synchronize only with its children, and for such programs, this work-stealing algorithm is efficient both in theory [2, 5] and in practice <ref> [2, 4] </ref>. Specifically, programs achieve linear speedup whenever the average parallelism is greater than the number of processors. Moreover, programs are guaranteed to make efficient use of stack space, and the scheduler existentially minimizes communication to within a constant factor. <p> Each subcomputation maintains its own checkpoint file, so checkpointing is fully distributed. Moreover, each subcomputation is a transaction, so that its side effects are made visible outside the subtree in an atomic action that can be undone <ref> [2] </ref>. Adaptive parallelism: The work-stealing scheduler and the tree-structured programming model allow programs to run on a set of compute servers that grows and shrinks over time. The owner of a compute server can set the policy to determine when the compute server is idle. <p> When the compute server is no longer available, all of its subcomputations are easily moved to another compute server. Moving a subcomputation requires only updating the information linking the subcomputation to its parent and child subcomputations <ref> [2] </ref>. Safety: The safety of Atlas depends on the safety of Java and of the native libraries. We take both of these for granted. <p> While some of these systems provide adaptive parallelism or fault tolerance, none provide both. In addition, none of these systems support safe or heterogeneous execution, and lacking any notion of hierarchy, the scalability of all of these systems is questionable. The Cilk-NOW system <ref> [2] </ref>, from which much of Atlas is derived, provides both adaptive parallelism and fault tolerance, but it does not support safe or heterogeneous execution, and it does not have any notion of hierarchy. It is exactly these shortcomings that motivates Atlas.
Reference: [3] <author> Robert D. Blumofe, Matteo Frigo, Christopher F. Joerg, Charles E. Leiserson, and Keith H. Randall. </author> <title> Dag-consistent distributed shared memory. </title> <booktitle> In Proceedings of the 10th International Parallel Processing Symposium (IPPS), </booktitle> <pages> pages 132-141, </pages> <address> Honolulu, Hawaii, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: The current prototype Atlas system has a number of limitations that we plan to address in future work. * The current prototype has no shared variables among threads. Fortunately, our model enables a form of weak consistency (dag consistency <ref> [3] </ref>) that we will incorporate into future versions. * The current global file system is read only.
Reference: [4] <author> Robert D. Blumofe, Christopher F. Joerg, Bradley C. Kuszmaul, Charles E. Leiserson, Keith H. Randall, and Yuli Zhou. Cilk: </author> <title> An efficient multithreaded runtime system. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), </booktitle> <pages> pages 207-216, </pages> <address> Santa Barbara, California, </address> <month> July </month> <year> 1995. </year> <title> More information about Cilk, including papers and software releases, </title> <note> is available on the World-Wide Web at http://theory.lcs.mit.edu/~cilk. </note>
Reference-contexts: Reasonable performance: The performance should show linear speedup for large classes of applications and should have sufficiently low overhead so as to see a benefit even with only a few machines. Atlas realizes these properties by combining existing mechanisms and policies from Java [1] and Cilk (pronounced "silk") <ref> [2, 4] </ref> together with some new mechanisms and policies that extend Atlas into a global computing infrastructure. Java is an object-oriented programming language together with a virtual-machine specification. Java programs are compiled to machine-independent byte codes that are executed by a virtual-machine interpreter running on the host machine. <p> When a native library is used, the application is limited to run on machines that have or can dynamically link in that library, which may also limit which platforms the application can exploit. The Atlas programming model is a straightforward adaptation of the Cilk programming model <ref> [2, 4] </ref> to Java with the addition of a URL-based file system. A complete description of this programming model is beyond the scope of this paper. <p> Thus, compute servers execute locally in depth-first order, and they steal in breadth-first order. In the Atlas programming model, a procedure may synchronize only with its children, and for such programs, this work-stealing algorithm is efficient both in theory [2, 5] and in practice <ref> [2, 4] </ref>. Specifically, programs achieve linear speedup whenever the average parallelism is greater than the number of processors. Moreover, programs are guaranteed to make efficient use of stack space, and the scheduler existentially minimizes communication to within a constant factor.
Reference: [5] <author> Robert D. Blumofe and Charles E. Leiserson. </author> <title> Scheduling multithreaded computations by work stealing. </title> <booktitle> In Proceedings of the 35th Annual Symposium on Foundations of Computer Science (FOCS), </booktitle> <pages> pages 356-368, </pages> <address> Santa Fe, New Mexico, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: Cilk is an algorithmic, C-based parallel multithreaded programming language together with a runtime system that employs a provably efficient thread-scheduling algorithm based on the technique of "work stealing" <ref> [2, 5] </ref> in which idle processors steal threads from randomly chosen victims. Cilk programs execute with efficient and predictable performance, guaranteed. In addition, the Cilk-NOW runtime system [2] provides automatic fault tolerance and adaptive parallelism for Cilk programs 1 executing on a network of workstations. <p> Thus, compute servers execute locally in depth-first order, and they steal in breadth-first order. In the Atlas programming model, a procedure may synchronize only with its children, and for such programs, this work-stealing algorithm is efficient both in theory <ref> [2, 5] </ref> and in practice [2, 4]. Specifically, programs achieve linear speedup whenever the average parallelism is greater than the number of processors. Moreover, programs are guaranteed to make efficient use of stack space, and the scheduler existentially minimizes communication to within a constant factor.
Reference: [6] <author> Nicholas Carriero and David Gelernter. </author> <title> Linda in context. </title> <journal> Communications of the ACM, </journal> <volume> 32(4) </volume> <pages> 444-458, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: The Benevolent Bandit Laboratory (BBL) [7] provides adaptive parallelism for applications running on a network of PCs and is probably the first example of a system that provides adaptive parallelism. The Piranha system [9] provides adaptive parallelism for Linda <ref> [6] </ref> programs running on a network of workstations. (The authors of Piranha appear to have coined the term "adaptive parallelism.") The Sam system [12] provides transparent fault tolerance for shared-memory parallel programs running on a network of workstations.
Reference: [7] <author> Robert E. Felderman, Eve M. Schooler, and Leonard Kleinrock. </author> <title> The Benevolent Bandit Laboratory: A testbed for distributed algorithms. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 7(2) </volume> <pages> 303-311, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: The DIB system [8] uses a heuristic form of work stealing to schedule backtrack-search applications on a network of workstations. The Benevolent Bandit Laboratory (BBL) <ref> [7] </ref> provides adaptive parallelism for applications running on a network of PCs and is probably the first example of a system that provides adaptive parallelism.
Reference: [8] <author> Raphael Finkel and Udi Manber. </author> <title> DIB|a distributed implementation of backtracking. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(2) </volume> <pages> 235-256, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: We were also able to obtain near linear speedups for POV-Ray. 6 Related work A number of systems have been developed to support parallel computation on distributed networks of workstations, but none provide all of the properties required for parallel computing on a global scale. The DIB system <ref> [8] </ref> uses a heuristic form of work stealing to schedule backtrack-search applications on a network of workstations. The Benevolent Bandit Laboratory (BBL) [7] provides adaptive parallelism for applications running on a network of PCs and is probably the first example of a system that provides adaptive parallelism.
Reference: [9] <author> David Gelernter and David Kaminsky. </author> <title> Supercomputing out of recycled garbage: Preliminary experience with Piranha. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <pages> pages 417-427, </pages> <address> Wash-ington, D.C., </address> <month> July </month> <year> 1992. </year>
Reference-contexts: The Benevolent Bandit Laboratory (BBL) [7] provides adaptive parallelism for applications running on a network of PCs and is probably the first example of a system that provides adaptive parallelism. The Piranha system <ref> [9] </ref> provides adaptive parallelism for Linda [6] programs running on a network of workstations. (The authors of Piranha appear to have coined the term "adaptive parallelism.") The Sam system [12] provides transparent fault tolerance for shared-memory parallel programs running on a network of workstations.
Reference: [10] <author> S. Levy. </author> <title> Wisecrackers. </title> <journal> Wired, </journal> <volume> 4(3), </volume> <month> March </month> <year> 1996. </year>
Reference-contexts: On the large scale, the Eclipse system [13] was used to run a Monte Carlo simulation using 192 distributed SPARC, RS6000, and i860 processors. On the global scale, the factoring of RSA-129 involved a huge number of machines spread across many countries <ref> [10] </ref>. In both cases, significant administration was required to run the application.
Reference: [11] <author> POV-Ray. </author> <note> http://www.povray.org. </note>
Reference-contexts: Although high, this overhead is the worst case|we have all of the thread creation and synchronization overhead and no work over which to amortize it. The second example is a parallelized version of the POV-Ray ray-tracing engine <ref> [11] </ref>, which has relatively large threads consisting of groups of scan lines. POV-Ray is a public domain ray tracer that has already been ported to many platforms. For this application, we converted the engine to a native library and wrote the partitioning software in Java.
Reference: [12] <author> Daniel J. Scales and Monica S. Lam. </author> <title> Transparent fault tolerance for parallel applications on networks of workstations. </title> <booktitle> In Proceedings of the USENIX 1996 Annual Winter Technical Conference, </booktitle> <address> San Diego, California, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: The Piranha system [9] provides adaptive parallelism for Linda [6] programs running on a network of workstations. (The authors of Piranha appear to have coined the term "adaptive parallelism.") The Sam system <ref> [12] </ref> provides transparent fault tolerance for shared-memory parallel programs running on a network of workstations. While some of these systems provide adaptive parallelism or fault tolerance, none provide both.
Reference: [13] <author> V. S. Sunderam and Vernon J. Rego. </author> <title> EcliPSe: A system for high performance concurrent simulation. </title> <journal> Software| Practice and Experience, </journal> <volume> 21(11) </volume> <pages> 1189-1219, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: It is exactly these shortcomings that motivates Atlas. There are at least two interesting cases of large-scale heterogeneous applications. On the large scale, the Eclipse system <ref> [13] </ref> was used to run a Monte Carlo simulation using 192 distributed SPARC, RS6000, and i860 processors. On the global scale, the factoring of RSA-129 involved a huge number of machines spread across many countries [10]. In both cases, significant administration was required to run the application.
Reference: [14] <author> A. Vehdat and T. E. Anderson, </author> <year> 1996. </year> <type> Personal communication. </type>
Reference-contexts: Atlas also provides a global file system based on URLs and local caching with coherence. The file system provides versions of the standard Unix file routines (e.g., fopen) that take URLs as file names <ref> [14] </ref>. This allows applications to access files on the home file system from anywhere. The file system uses block-based cache coherence to achieve reasonable performance, and future versions will provide ticket-based access control. 3 Architecture The Atlas system architecture consists of clients, managers, and compute servers.
References-found: 14

