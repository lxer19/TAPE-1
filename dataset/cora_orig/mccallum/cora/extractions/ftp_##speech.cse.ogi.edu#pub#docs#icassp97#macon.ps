URL: ftp://speech.cse.ogi.edu/pub/docs/icassp97/macon.ps
Refering-URL: http://www.cse.ogi.edu/CSLU/publications/publications.html
Root-URL: http://www.cse.ogi.edu
Phone: 2  3  
Title: A SINGING VOICE SYNTHESIS SYSTEM BASED ON SINUSOIDAL MODELING  
Author: Michael W. Macon fl Leslie Jensen-Link y James Oliverio Mark A. Clements E. Bryan George z 
Address: Atlanta, GA 30332-0250  Atlanta, GA 30332-0456  Instruments, Dallas, TX 75265-5474  
Affiliation: 1 School of Electrical and Computer Engineering, Georgia Institute of Technology,  Department of Music, Georgia Institute of Technology,  DSP Research and Development Center, Texas  
Abstract: Although sinusoidal models have been demonstrated to be capable of high-quality musical instrument synthesis [1], speech modification [2], and speech synthesis [3], little exploration of the application of these models to the synthesis of singing voice has been undertaken. In this paper, we propose a system framework similar to that employed in concatenation-based text-to-speech synthesizers, and describe its extension to the synthesis of singing voice. The power and flexibility of the sinusoidal model used in the waveform synthesis portion of the system [1] enables high-quality, computationally-efficient synthesis and the incorporation of musical qualities such as vibrato and spectral tilt variation. Modeling of segmental phonetic characteristics is achieved by employing a "unit selection" procedure that selects sinusoidally-modeled segments from an inventory of singing voice data collected from a human vocalist. The system, called Lyricos, is capable of synthesizing very natural-sounding singing that maintains the characteristics and perceived identity of the analyzed vocalist. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. B. George and M. J. T. Smith, </author> <title> "An analysis-by-synthesis approach to sinusoidal modeling applied to the analysis and synthesis of musical tones," </title> <journal> Journal of the Audio Engineering Society, </journal> <volume> vol. 40, </volume> <pages> pp. 497-516, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA 30332-0250 2 Department of Music, Georgia Institute of Technology, Atlanta, GA 30332-0456 3 DSP Research and Development Center, Texas Instruments, Dallas, TX 75265-5474 ABSTRACT Although sinusoidal models have been demonstrated to be capable of high-quality musical instrument synthesis <ref> [1] </ref>, speech modification [2], and speech synthesis [3], little exploration of the application of these models to the synthesis of singing voice has been undertaken. <p> In this paper, we propose a system framework similar to that employed in concatenation-based text-to-speech synthesizers, and describe its extension to the synthesis of singing voice. The power and flexibility of the sinusoidal model used in the waveform synthesis portion of the system <ref> [1] </ref> enables high-quality, computationally-efficient synthesis and the incorporation of musical qualities such as vibrato and spectral tilt variation. Modeling of segmental phonetic characteristics is achieved by employing a "unit selection" procedure that selects sinusoidally-modeled segments from an inventory of singing voice data collected from a human vocalist. <p> Sinusoidal signal models are somewhat more general representations that are capable of high-quality modeling, modification, and synthesis of both speech and music signals <ref> [1, 2, 3] </ref>. The success of previous work in speech and music synthesis motivates the application of sinusoidal modeling to the synthesis of singing voice. <p> Finally, the output waveform is synthesized using the method described below. 3. WAVEFORM SYNTHESIS The ABS/OLA sinusoidal model The waveform synthesis model used is an extension of the Analysis-by-Synthesis/Overlap-Add (ABS/OLA) sinusoidal model <ref> [1] </ref>. <p> Each signal contribution s k [n] consists of the sum of a small number of constant-frequency, constant-amplitude sinusoidal components. An iterative analysis-by-synthesis procedure is performed to find the optimal parameters to represent each signal frame <ref> [1] </ref>. Synthesis is performed by an overlap-add procedure that uses the inverse fast Fourier transform to compute each contribution s k [n], rather than sets of oscillator functions. <p> Time-scale modification of the signal is achieved by changing the synthesis frame duration, and pitch modification is performed by altering the sinusoidal components such that the fundamental frequency is modified while the speech formant structure is maintained <ref> [1] </ref>. The flexibility of this synthesis model enables the incorporation of vocal qualities such as vibrato and spectral tilt variation, adding greatly to the musical expressiveness of the synthesizer output. Vibrato/pitch drift The physiological mechanism of the pitch, amplitude, and timbral variation referred to as vibrato is somewhat in debate. <p> Algorithms presented in [3] are used to smooth discontinuities in the spectral shape, signal energy, and phase by modifying the sinusoidal components prior to synthesis. The output waveform is then synthesized using an inverse FFT and overlap-add procedure <ref> [1] </ref>. 5. SUMMARY The system described in this paper is capable of producing a natural-sounding, musically pleasing synthetic singing voice.
Reference: [2] <author> T. F. Quatieri and R. J. McAulay, </author> <title> "Shape invariant time-scale and pitch modification of speech," </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> vol. 40, </volume> <pages> pp. 497-510, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: and Computer Engineering, Georgia Institute of Technology, Atlanta, GA 30332-0250 2 Department of Music, Georgia Institute of Technology, Atlanta, GA 30332-0456 3 DSP Research and Development Center, Texas Instruments, Dallas, TX 75265-5474 ABSTRACT Although sinusoidal models have been demonstrated to be capable of high-quality musical instrument synthesis [1], speech modification <ref> [2] </ref>, and speech synthesis [3], little exploration of the application of these models to the synthesis of singing voice has been undertaken. In this paper, we propose a system framework similar to that employed in concatenation-based text-to-speech synthesizers, and describe its extension to the synthesis of singing voice. <p> Sinusoidal signal models are somewhat more general representations that are capable of high-quality modeling, modification, and synthesis of both speech and music signals <ref> [1, 2, 3] </ref>. The success of previous work in speech and music synthesis motivates the application of sinusoidal modeling to the synthesis of singing voice.
Reference: [3] <author> M. W. Macon and M. A. Clements, </author> <title> "Speech concatenation and synthesis using an overlap-add sinusoidal model," </title> <booktitle> in Proceedings of the International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <volume> vol. 1, </volume> <pages> pp. 361-364, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Institute of Technology, Atlanta, GA 30332-0250 2 Department of Music, Georgia Institute of Technology, Atlanta, GA 30332-0456 3 DSP Research and Development Center, Texas Instruments, Dallas, TX 75265-5474 ABSTRACT Although sinusoidal models have been demonstrated to be capable of high-quality musical instrument synthesis [1], speech modification [2], and speech synthesis <ref> [3] </ref>, little exploration of the application of these models to the synthesis of singing voice has been undertaken. In this paper, we propose a system framework similar to that employed in concatenation-based text-to-speech synthesizers, and describe its extension to the synthesis of singing voice. <p> Sinusoidal signal models are somewhat more general representations that are capable of high-quality modeling, modification, and synthesis of both speech and music signals <ref> [1, 2, 3] </ref>. The success of previous work in speech and music synthesis motivates the application of sinusoidal modeling to the synthesis of singing voice. <p> These units are smoothed to diminish perceptible discontinuities at the boundaries, and time-scale and pitch modification algorithms are employed to give the speech the desired prosody <ref> [3] </ref>. With an acoustic inventory of sufficient size, this approach achieves segmental quality that approaches that of human utterances, and this motivates its exploration as a framework for singing voice synthesis. 2. <p> Units are selected to represent segmental phonetic characteristics of the utterance, including coartic-ulation effects caused by the context of each phoneme. Algorithms described in <ref> [3] </ref> are applied to the modeled segments to remove disfluencies in the signal at the joined boundaries. The sinusoidal model parameters are then used to modify the pitch, duration, and spectral characteristics of the concatenated voice units as specified by the input musical score and MIDI control information. <p> Once the sequence of units has been specified using the decision tree method described above, concatenation and smoothing of the units can take place, as depicted in Figure 2. Algorithms presented in <ref> [3] </ref> are used to smooth discontinuities in the spectral shape, signal energy, and phase by modifying the sinusoidal components prior to synthesis. The output waveform is then synthesized using an inverse FFT and overlap-add procedure [1]. 5.
Reference: [4] <author> P. R. Cook, "SPASM, </author> <title> a real-time vocal tract physical model controller and Singer, the companion software synthesis system," </title> <journal> Computer Music Journal, </journal> <volume> vol. 17, </volume> <pages> pp. 30-43, </pages> <month> Spring </month> <year> 1993. </year>
Reference-contexts: These requirements significantly differentiate synthesis of singing from speech synthesis. Most previous approaches to synthesis of singing have relied on models that attempt to accurately characterize the human speech production mechanism. For example, the SPASM system developed by Cook <ref> [4] </ref> fl MWM is currently with the Oregon Graduate Institute. y LJ-L is currently with Momentum Data Systems, Inc. z This work was supported by Texas Instruments. employs an articulator-based tube representation of the vocal tract and a time-domain glottal pulse input.
Reference: [5] <author> G. Bennett and X. Rodet, </author> <title> "Synthesis of the singing voice," in Current Directions in Computer Music Research (M. </title> <editor> V. Mathews and J. R. Pierce, </editor> <booktitle> eds.), </booktitle> <pages> pp. 19-44, </pages> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: Formant synthesizers such as the CHANT system <ref> [5] </ref> rely on direct representation and control of the resonances produced by the shape of the vocal tract. Each of these techniques relies, to a degree, on accurate modeling of the dynamic characteristics of the speech production process by an approximation to the articulatory system. <p> A global setting of the vibrato rate is also possible. Addition of a slight nonperiodic drift of the pitch period (as suggested by <ref> [5] </ref>, [7], and others) also contributes to a more human-sounding result. Vocal effort scaling Another important attribute of the vocal source in singing is the variation of spectral tilt with loudness. Crescendo of the voice is accompanied by a leveling of the usual downward tilt of the source spectrum [5]. <p> by <ref> [5] </ref>, [7], and others) also contributes to a more human-sounding result. Vocal effort scaling Another important attribute of the vocal source in singing is the variation of spectral tilt with loudness. Crescendo of the voice is accompanied by a leveling of the usual downward tilt of the source spectrum [5]. Since the sinusoidal model is a frequency-domain representation, spectral tilt changes can be quite easily implemented by adjusting the slope of the sinusoidal amplitudes. Breathiness, which manifests itself as high-frequency noise in the speech spectrum, is another acoustic correlate of vocal intensity [7].
Reference: [6] <author> R. Maher and J. Beauchamp, </author> <title> "An investigation of vocal vibrato for synthesis," </title> <journal> Applied Acoustics, </journal> <volume> vol. 30, </volume> <pages> pp. 219-245, </pages> <year> 1990. </year>
Reference-contexts: Vibrato/pitch drift The physiological mechanism of the pitch, amplitude, and timbral variation referred to as vibrato is somewhat in debate. However, frequency modulation of the glottal source waveform is capable of producing many of the observed effects of vibrato <ref> [6] </ref>. As the source harmonics are swept across the vocal tract resonances, timbre and amplitude modulations as well as frequency modulation take place.
Reference: [7] <author> L. Klatt, D.H.; Klatt, </author> <title> "Analysis, synthesis, and perception of voice quality variations among female and male talkers," </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> vol. 87, </volume> <pages> pp. 820-57, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: A global setting of the vibrato rate is also possible. Addition of a slight nonperiodic drift of the pitch period (as suggested by [5], <ref> [7] </ref>, and others) also contributes to a more human-sounding result. Vocal effort scaling Another important attribute of the vocal source in singing is the variation of spectral tilt with loudness. Crescendo of the voice is accompanied by a leveling of the usual downward tilt of the source spectrum [5]. <p> Since the sinusoidal model is a frequency-domain representation, spectral tilt changes can be quite easily implemented by adjusting the slope of the sinusoidal amplitudes. Breathiness, which manifests itself as high-frequency noise in the speech spectrum, is another acoustic correlate of vocal intensity <ref> [7] </ref>. This frequency dependent noise energy can be generated within the ABS/OLA model framework by employing a phase modulation technique during synthesis [8].
Reference: [8] <author> M. W. Macon and M. A. Clements, </author> <title> "Sinusoidal modeling and modification of unvoiced speech," </title> <note> accepted for publication in IEEE Transactions on Speech and Audio Processing, </note> <year> 1997. </year>
Reference-contexts: Breathiness, which manifests itself as high-frequency noise in the speech spectrum, is another acoustic correlate of vocal intensity [7]. This frequency dependent noise energy can be generated within the ABS/OLA model framework by employing a phase modulation technique during synthesis <ref> [8] </ref>. Vocal tract length scaling In synthesis of bass voices using a voice inventory recorded from a baritone male vocalist, it was found that the voice took on an artificial "buzzy" quality, caused by extreme lowering of the fundamental frequency.
Reference: [9] <author> L. R. Rabiner and R. W. Schafer, </author> <title> Digital Processing of Speech Signals. </title> <address> Englewood Cliffs, New Jersey: </address> <publisher> Prentice-Hall, </publisher> <year> 1978. </year>
Reference-contexts: Through analysis of a simple tube model of the human vocal tract, it can be shown that the nominal formant frequencies associated with a longer vocal tract are lower than those associated with a shorter vocal tract <ref> [9] </ref>. Because of this, larger people usually have voices with a "deeper" quality; bass vocalists are typically males with vocal tracts possessing this characteristic.
Reference: [10] <author> A. J. Hunt and A. W. Black, </author> <title> "Unit selection in a concatena-tive speech synthesis system using a large speech database," </title> <booktitle> in Proc. of the Int'l Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <volume> vol. 1, </volume> <pages> pp. 373-376, </pages> <year> 1996. </year>
Reference-contexts: Although it is possible to formulate unit selection as a dynamic programming problem that finds an optimal path through a lattice of all possible units based on acoustic "costs," (e.g., <ref> [10] </ref>) the approach taken here is a simpler one designed with the constraints of the inventory in mind: best-context vowel units are selected first, and consonant units are selected in a second pass to complete the unit sequence.
References-found: 10

