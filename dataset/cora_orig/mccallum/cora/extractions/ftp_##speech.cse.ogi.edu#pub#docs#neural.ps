URL: ftp://speech.cse.ogi.edu/pub/docs/neural.ps
Refering-URL: http://www.cse.ogi.edu/CSLU/publications/publications.html
Root-URL: http://www.cse.ogi.edu
Email: yan@cse.ogi.edu barnard@cse.ogi.edu  
Phone: Phone: (503)690-1121 ext. 1637, FAX (503)690-1334  
Title: A COMPARISON OF NEURAL NET AND LINEAR CLASSIFIER AS THE PATTERN RECOGNIZER IN AUTOMATIC LANGUAGE
Author: Yonghong Yan Etienne Barnard 
Address: P.O.Box 91000, Portland, OR 97291-1000, USA  
Affiliation: Center  Oregon Graduate Institute of Science Technology  
Abstract: The goal for language identification (LID) is to quickly and accurately identify the language being spoken in a given test utterance. Recent researches has shown the importance of acoustic, phonotactic and prosodic information for language identification. How to combine these multiple information sources to give the final results is still a research issue. Traditional ways to combine multiple scores were similar to a linear classifier. In this paper, experiments were conducted to compare the performance of linear classifier based and neural network based final score combination. The results showed that approximately 15% errors of linear classifier based system were reduced by the neural network based system, which suggests that a non-linear combination of multi-information sources is necessary for language identification. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M.A. Zissman. </author> <title> Language Identification using Phoneme Recognition and Phonotactic Language Modeling. </title> <booktitle> In 1995 International Conference on Acoustic, Speech, and Signal Processing Proceedings. Vol.5, </booktitle> <pages> pages 3503-3506, </pages> <month> May </month> <year> 1995 </year>
Reference-contexts: How to combine these multiple information sources to give the final identification results is still an interesting research issue. Previously, the typical ways to combine mul tiple scores were either by some prior knowledge about the relative merit of different scores, such as in <ref> [1] </ref> or by hill-climbing optimization of a linear combination of scores [2]. These techniques thus amount to linear classification. A well-trained linear classifier can be viewed as combining the scores with different optimal weights to give the best guess.
Reference: [2] <author> T.J. Hazen and V.W. Zue. </author> <title> Recent improvements in an approach to segment-based Automatic language identification. </title> <booktitle> In Proceedings of International Conference on Spoken Language Processing, </booktitle> <address> pp.1883-1886, </address> <month> September </month> <year> 1994 </year>
Reference-contexts: Previously, the typical ways to combine mul tiple scores were either by some prior knowledge about the relative merit of different scores, such as in [1] or by hill-climbing optimization of a linear combination of scores <ref> [2] </ref>. These techniques thus amount to linear classification. A well-trained linear classifier can be viewed as combining the scores with different optimal weights to give the best guess. In this paper, comparison experiments were conducted to compare the performance of linear classifier based and neural network based final score combination.
Reference: [3] <author> Y. Yan and E. Barnard. </author> <title> An Approach to Automatic Language Identification based on Language-dependent Phone Recognition. </title> <booktitle> In 1995 International Conference on Acoustic, Speech, and Signal Processing Proceedings. Vol.5, </booktitle> <pages> pages 3511-3514, </pages> <month> May </month> <year> 1995 </year>
Reference-contexts: Recognizer 1 Recognizer 2 Recognizer M LID model 1,2,....,N Score Generator 1 LID model 1,2,....,N LID model 1,2,....,N Score Generator 2 Score Generator M LID Result Speech Signal Final Classifier 2. Overview of the LID system 2.1. General Architecture Our LID system <ref> [3] </ref> is composed of three parts: (1) language dependent phone recognizers (front end), (2) LID score generators and (3) the final classifier. A general architecture for an N-language task system is given in Figure 1. For an N-language task, M (N M ) language-dependent phone rec-ognizers were implemented. <p> The duration models are used to model the prosodic differences in different languages. The backward language model and duration model were first proposed in <ref> [3] </ref>. All the language models were optimized by the method proposed in [4]. 3. Database The Oregon Graduate Institute Multi Language Telephone Speech Corpus (OGI TS)[5] was used. It is a telephone speech database collected in USA, all the speakers are supposed to be native speakers of the languages.
Reference: [4] <author> Y. Yan and E. Barnard. </author> <title> An Approach to Language Identification with Enhanced Language Model. </title> <note> To appear EUROSPEECH-95. </note>
Reference-contexts: The duration models are used to model the prosodic differences in different languages. The backward language model and duration model were first proposed in [3]. All the language models were optimized by the method proposed in <ref> [4] </ref>. 3. Database The Oregon Graduate Institute Multi Language Telephone Speech Corpus (OGI TS)[5] was used. It is a telephone speech database collected in USA, all the speakers are supposed to be native speakers of the languages.
Reference: [5] <author> Y.K. Muthusamy, R.A.Cole and B.T.Oshika. </author> <title> The OGI multi-language telephone speech corpus. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing 92. Vol.2, </booktitle> <pages> pages 895-898, </pages> <month> October </month> <year> 1992. </year>
Reference: [6] <author> E.Barnard and R.A.Cole. </author> <title> A neural-net training program based on conjugate-gradient optimization. </title> <type> Technical Report CSE 89-014, </type> <institution> Oregon Graduate Institute, </institution> <year> 1989 </year>
Reference-contexts: A conjugate gradient optimization algorithm <ref> [6] </ref> was used to train the neural networks. conjugate gradient optimization [6]. All the experiments were conducted on both the six-language and 11-language task. Neural networks with different number of hidden nodes were evaluated; the best results were obtained by nets with 15 to 25 hidden nodes. <p> A conjugate gradient optimization algorithm <ref> [6] </ref> was used to train the neural networks. conjugate gradient optimization [6]. All the experiments were conducted on both the six-language and 11-language task. Neural networks with different number of hidden nodes were evaluated; the best results were obtained by nets with 15 to 25 hidden nodes.
Reference: [7] <author> K.P. Li, </author> <title> Experimental Improvements of a Language Id System. </title> <booktitle> In 1995 International Conference on Acoustic, Speech, and Signal Processing Proceedings. Vol.5, </booktitle> <pages> pages 3515-3518, </pages> <month> May </month> <year> 1995 </year>
Reference: [8] <author> E.S. Parris and M.J.Carey, </author> <title> Language Identification Using Multiple Knowledge Sources. </title> <booktitle> In 1995 International Conference on Acoustic, Speech, and Signal Processing Proceedings. Vol.5, </booktitle> <pages> pages 3519-3522, </pages> <month> May </month> <year> 1995 </year>
References-found: 8

