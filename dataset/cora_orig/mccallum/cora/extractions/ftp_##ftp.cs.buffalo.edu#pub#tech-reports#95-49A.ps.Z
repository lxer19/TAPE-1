URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/95-49A.ps.Z
Refering-URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/README.html
Root-URL: 
Email: wu@cs.buffalo.edu  
Title: On Parallelization of Static Scheduling Algorithms  
Author: Min-You Wu 
Address: Buffalo, NY 14260  
Affiliation: Department of Computer Science State University of New York at Buffalo  
Abstract: Most static scheduling algorithms that schedule parallel programs represented by directed acyclic graphs (DAGs) are sequential. This paper discusses the essential issues of par-allelization of static scheduling and presents two efficient parallel scheduling algorithms. The proposed algorithms have been implemented on an Intel Paragon machine, and their performance has been evaluated. These algorithms produce high-quality scheduling and are much faster than existing sequential and parallel algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> I. Ahmad and Y.K. Kwok. </author> <title> A parallel approach to multiprocessor scheduling. </title> <booktitle> In Int'l Parallel Processing Symposium, </booktitle> <pages> pages 289-293, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: Partition the graph into P equal sized sets using time domain partitioning. 2. Each PPE schedules its graph partition to generate a sub-schedule. 3. PPEs exchange information to concatenate sub-schedules. 4 scheme, which is the parallel BSA (PBSA) algorithm <ref> [1] </ref>. The BSA algorithm takes into account link contention and communication routing strategy. A PE list is constructed in a breadth-first order from the PE having the highest degree (pivot PE). This algorithm constructs a schedule incrementally by first injecting all the nodes to the pivot PE. <p> The number of communications is 2n for VPMCP1 and 2n=p for VPMCP, where p is the number of TPEs. The VPMCP algorithm is compared to VPMCP1 in Table I. The workload for testing consists of random graphs of various sizes. We use the same random graph generator in <ref> [1] </ref>. Three values of communication-computation-ratio (CCR) were selected to be 0.1, 1, and 10. The weights on the nodes and edges were generated randomly such that the average value of CCR corresponded to 0.1, 1, or 10. This set of graphs will be used in the subsequent experiment. <p> Although the latter PPE does not know the exact schedules of its former partitions, an estimation can help a node to determine its earliest start time in the latter partition. 9 In the PBSA algorithm <ref> [1] </ref>, the start time of each RPN (remote parent node) is estimated. It is done by calculating two parameters: the earliest possible start time (EPST), and the latest possible start time (LPST). <p> A hueristics is necessary. In the PBSA algorithm, a TPE with the earliest node is concatenated to the TPE of the former sub-schedule that allows the earliest execution. Then, other TPEs are concatenated to the TPEs in the former sub-schedule in a breadth-first order <ref> [1] </ref>. In the HPMCP algorithm, we assume that the start time of each node within its partition is the same. Therefore, the above algorithm cannot be applied. We simply do not perform permutation of TPEs in HPMCP.
Reference: [2] <author> I. Ahmad, Y.K. Kwok, and M.Y. Wu. </author> <title> Performance comparison of algorithms for static scheduling of DAGs to multiprocessors. </title> <booktitle> In Second Australasian Conference on Parallel and Real-time Systems, </booktitle> <month> September </month> <year> 1995. </year>
Reference-contexts: This method is called an insertion algorithm. The MCP algorithm has been compared to four other well-known scheduling algorithms under the same assumption, that are, ISH [10], ETF [8], DLS [12], and 2 LAST [3]. It has been shown that MCP performed the best <ref> [2] </ref>. The complexity of the MCP algorithm is O (n 2 logn), where n is the number of nodes in a graph. In the second step, the ties can be broken randomly to have a simplified version of MCP.
Reference: [3] <author> J. Baxter and J. H. Patel. </author> <title> The LAST algorithm: A heuristics-based static task allocation algorithm. </title> <booktitle> In Int'l Conf. on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 217-222, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: A node can be inserted to the first feasible idle time slot. This method is called an insertion algorithm. The MCP algorithm has been compared to four other well-known scheduling algorithms under the same assumption, that are, ISH [10], ETF [8], DLS [12], and 2 LAST <ref> [3] </ref>. It has been shown that MCP performed the best [2]. The complexity of the MCP algorithm is O (n 2 logn), where n is the number of nodes in a graph. In the second step, the ties can be broken randomly to have a simplified version of MCP.
Reference: [4] <author> Y.C. Chung and S. Ranka. </author> <title> Applications and performance analysis of a compile-time optimization approach for list scheduling algorithms on distributed memory multiprocessors. </title> <booktitle> In Supercomputer '92, </booktitle> <month> November </month> <year> 1992. </year>
Reference-contexts: This is an NP-complete problem in its general form [7]. Therefore, many hueristic algorithms that produce satisfactory performance have been proposed <ref> [11, 13, 5, 14, 12, 4, 9] </ref>. Although these scheduling algorithms apply to parallel programs, the algorithms themselves are sequential, and are executed on a single processor system. A sequential algorithm is slow.
Reference: [5] <author> H. El-Rewini and T. G. Lewis. </author> <title> Scheduling parallel program tasks onto arbitrary target machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <month> June </month> <year> 1990. </year>
Reference-contexts: This is an NP-complete problem in its general form [7]. Therefore, many hueristic algorithms that produce satisfactory performance have been proposed <ref> [11, 13, 5, 14, 12, 4, 9] </ref>. Although these scheduling algorithms apply to parallel programs, the algorithms themselves are sequential, and are executed on a single processor system. A sequential algorithm is slow.
Reference: [6] <author> H. El-Rewini, T. G. Lewis, and H. H. Ali. </author> <title> Task Scheduling in Parallel and Distributed Systems. </title> <publisher> Prentice Hall, </publisher> <year> 1994. </year>
Reference-contexts: time is defined as T L (n i ) = T critical level (n i ), where T critical is the length of the critical path, and level (n i ) is the length of the longest path from node n i to the end point, including node n i <ref> [6] </ref>. In fact, high-quality scheduling algorithms more or less rely on the ALAP time or level. The Modified Critical-Path (MCP) algorithm was designed to schedule a macro dataflow graph on a bounded number of PEs. The MCP Algorithm 1. Calculate the ALAP time of each node. 2.
Reference: [7] <author> M.R. Gary and D.S. Johnson. </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. W.H. </title> <publisher> Freeman and Company, </publisher> <year> 1979. </year>
Reference-contexts: This is an NP-complete problem in its general form <ref> [7] </ref>. Therefore, many hueristic algorithms that produce satisfactory performance have been proposed [11, 13, 5, 14, 12, 4, 9]. Although these scheduling algorithms apply to parallel programs, the algorithms themselves are sequential, and are executed on a single processor system. A sequential algorithm is slow.
Reference: [8] <author> J.J. Hwang, Y.C. Chow, F.D. Anger, and C.Y. Lee. </author> <title> Scheduling precedence graphs in systems with interprocessor communication times. </title> <journal> SIAM Journal of Computing, </journal> <volume> 18(2) </volume> <pages> 244-257, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: A node can be inserted to the first feasible idle time slot. This method is called an insertion algorithm. The MCP algorithm has been compared to four other well-known scheduling algorithms under the same assumption, that are, ISH [10], ETF <ref> [8] </ref>, DLS [12], and 2 LAST [3]. It has been shown that MCP performed the best [2]. The complexity of the MCP algorithm is O (n 2 logn), where n is the number of nodes in a graph.
Reference: [9] <author> A.A. Khan, C.L. McCreary, and M.S. Jones. </author> <title> A comparison of multiprocessor scheduling heuristics. </title> <booktitle> In Int'l Conf. on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 243-250, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: This is an NP-complete problem in its general form [7]. Therefore, many hueristic algorithms that produce satisfactory performance have been proposed <ref> [11, 13, 5, 14, 12, 4, 9] </ref>. Although these scheduling algorithms apply to parallel programs, the algorithms themselves are sequential, and are executed on a single processor system. A sequential algorithm is slow.
Reference: [10] <author> B. Kruatrachue and T.G. Lewis. </author> <title> Duplication scheduling heuristics (dsh): A new precedence task scheduler for parallel processor systems. </title> <type> Technical report, </type> <institution> Oregon State University, Corvallis, </institution> <year> 1987. </year>
Reference-contexts: A node can be inserted to the first feasible idle time slot. This method is called an insertion algorithm. The MCP algorithm has been compared to four other well-known scheduling algorithms under the same assumption, that are, ISH <ref> [10] </ref>, ETF [8], DLS [12], and 2 LAST [3]. It has been shown that MCP performed the best [2]. The complexity of the MCP algorithm is O (n 2 logn), where n is the number of nodes in a graph.
Reference: [11] <author> V. Sarkar. </author> <title> Partitioning and Scheduling Parallel Programs for Multiprocessors. </title> <publisher> The MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: This is an NP-complete problem in its general form [7]. Therefore, many hueristic algorithms that produce satisfactory performance have been proposed <ref> [11, 13, 5, 14, 12, 4, 9] </ref>. Although these scheduling algorithms apply to parallel programs, the algorithms themselves are sequential, and are executed on a single processor system. A sequential algorithm is slow.
Reference: [12] <author> G. C. Sih and E. A. Lee. </author> <title> A compile-time scheduling heuristic for interconnection-constrained heterogeneous processor architectures. </title> <journal> IEEE Trans. Parallel and Distributed System, </journal> <volume> 4(2) </volume> <pages> 175-187, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: This is an NP-complete problem in its general form [7]. Therefore, many hueristic algorithms that produce satisfactory performance have been proposed <ref> [11, 13, 5, 14, 12, 4, 9] </ref>. Although these scheduling algorithms apply to parallel programs, the algorithms themselves are sequential, and are executed on a single processor system. A sequential algorithm is slow. <p> A node can be inserted to the first feasible idle time slot. This method is called an insertion algorithm. The MCP algorithm has been compared to four other well-known scheduling algorithms under the same assumption, that are, ISH [10], ETF [8], DLS <ref> [12] </ref>, and 2 LAST [3]. It has been shown that MCP performed the best [2]. The complexity of the MCP algorithm is O (n 2 logn), where n is the number of nodes in a graph.
Reference: [13] <author> M. Y. Wu and D. D. Gajski. Hypertool: </author> <title> A programming aid for message-passing systems. </title> <journal> IEEE Trans. Parallel and Distributed Systems, </journal> <volume> 1(3) </volume> <pages> 330-343, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: This is an NP-complete problem in its general form [7]. Therefore, many hueristic algorithms that produce satisfactory performance have been proposed <ref> [11, 13, 5, 14, 12, 4, 9] </ref>. Although these scheduling algorithms apply to parallel programs, the algorithms themselves are sequential, and are executed on a single processor system. A sequential algorithm is slow. <p> These two requirements contradict each other in general. Usually, a high-quality scheduling algo 1 rithm is of a high complexity. The Modified Critical-Path (MCP) algorithm was introduced <ref> [13] </ref>, which offered a good quality with relatively low complexity. In this paper, we propose two par-allelized versions of MCP. We will describe the MCP algorithm in the next section. Then, we will discuss different approaches for parallel scheduling, as well as existing parallel algorithms in section 3. <p> In sections 4 and 5, we will present the VPMCP and HPMCP algorithms, respectively. A comparison of the two algorithms will be presented in section 6. 2 The MCP Algorithm A macro dataflow graph is a directed acyclic graph with a starting point and an end point <ref> [13] </ref>. A macro dataflow graph consists of a set of nodes fn 1 ; n 2 ; :::; n n g connected by a set of edges, each of which is denoted by e (n i ; n j ).
Reference: [14] <author> T. Yang and A. Gerasoulis. </author> <title> DSC: Scheduling parallel tasks on an unbounded number of processors. </title> <journal> IEEE Trans. Parallel and Distributed System, </journal> <volume> 5(9) </volume> <pages> 951-967, </pages> <month> September </month> <year> 1994. </year> <month> 23 </month>
Reference-contexts: This is an NP-complete problem in its general form [7]. Therefore, many hueristic algorithms that produce satisfactory performance have been proposed <ref> [11, 13, 5, 14, 12, 4, 9] </ref>. Although these scheduling algorithms apply to parallel programs, the algorithms themselves are sequential, and are executed on a single processor system. A sequential algorithm is slow.
References-found: 14

