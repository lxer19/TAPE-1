URL: http://www-poleia.lip6.fr/CONNEX/Theses/goutte.ps.gz
Refering-URL: http://eivind.imm.dtu.dk/staff/goutte/PUBLIS/thesis.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: TH presentee par STATISTICAL LEARNING AND REGULARISATION FOR REGRESSION Application to system identification and time
Author: ESE DE DOCTORAT DE L'UNIVERSIT E PARIS Specialite Cyril Goutte soutenue le juillet devant le jury compose de Sylvie THIRIA President Anne GU ERIN-DUGU E Rapporteur Gerard GOVAERT Rapporteur Jan LARSEN Examinateur Patrick GALLINARI Examinateur 
Date: Titre  
Address: PARIS 6.  
Affiliation: Informatique  pour obtenir le grade de DOCTEUR DE l'UNIVERSIT E  
Abstract-found: 0
Intro-found: 1
Reference: <author> Akaike, H. </author> <year> (1969). </year> <title> Fitting autoregressive models for prediction. </title> <journal> Annals of the Institute of Statistical Mathematics, </journal> <volume> 21 </volume> <pages> 243-247. </pages>
Reference: <author> Amari, S. I. </author> <year> (1967). </year> <title> A theory of adaptive pattern classifiers. </title> <journal> IEEE Transactions on Electronic Computers, </journal> <volume> 16 </volume> <pages> 279-307. </pages>
Reference: <author> Battiti, R. </author> <year> (1992). </year> <title> First- and second-order methods for learning: Between steepest descent and newton's method. </title> <journal> Neural Computation, </journal> <volume> 4(2) </volume> <pages> 141-166. </pages>
Reference: <author> Bishop, C. M. </author> <year> (1995a). </year> <title> Neural Networks for pattern recognition. </title> <publisher> Clarendon Press, Oxford. </publisher>
Reference: <author> Bishop, C. M. </author> <year> (1995b). </year> <title> Training with noise is equivalent to thikonov regularization. </title> <booktitle> NC, </booktitle> <volume> 7(1) </volume> <pages> 110-116. </pages> <note> c flC. Goutte 1996 References 17 Bottou, </note> <author> L. </author> <year> (1991). </year> <institution> Une approche theorique de l'apprentissage connexionniste : application a la reconnaissance de la parole. These de doctorat, Universite Paris XI, Orsay. </institution>
Reference: <author> Bryson, A., Denham, W., and Dreyfuss, S. </author> <year> (1963). </year> <title> Optimal programming problem with inequality constraints. I: Necessary conditions for extremal solutions. </title> <journal> AIAA journal, </journal> <volume> 1 </volume> <pages> 25-44. </pages>
Reference-contexts: Incidentally, these books adopt a statistical perspective. 1.9 The back-propagation rule is usually credited to Werbos (1974) or Rumelhart et al. (1986). However, it was discovered earlier in different contexts. Vapnik (1995) mentions its use in <ref> (Bryson et al., 1963) </ref> for solving some control problems.
Reference: <author> Fletcher, R. </author> <year> (1987). </year> <title> Practical Methods of Optimization. </title> <publisher> Wiley. </publisher>
Reference-contexts: Bottou (1991) cites Amari (1967) in the context of adaptive systems and notes that it is nothing more than proper application of the derivation rules invented by Leibnitz in the 17th century. 1.10 Optimisation methods are covered in many books such as e.g. <ref> (Fletcher, 1987) </ref> or (Press et al., 1992) for one-dimensional and multi-dimensional techniques. The quadratic approximation is necessary to handle non linear problems. However, methods presented here are also used in the linear case.
Reference: <author> Friedman, J. H. </author> <year> (1996). </year> <title> On bias, variance, 0/1 loss, and the curse-of-dimensionality. </title> <type> Technical report, </type> <institution> Department of Statistics, Stanford University. ftp://playfair.stanford.edu/pub/friedman/curse.ps.Z. </institution>
Reference: <author> Geman, S., Bienenstock, E., and Doursat, R. </author> <year> (1992). </year> <title> Neural networks and the bias/variance dilemna. </title> <journal> Neural Computation, </journal> <volume> 4(1) </volume> <pages> 1-58. </pages>
Reference: <author> Goutte, C. and Ledoux, C. </author> <year> (1995). </year> <title> Synthese des techniques de commande connex-ionniste. </title> <type> Technical Report 95/02, </type> <institution> LAFORIA. </institution>
Reference: <author> Hertz, J., Krogh, A., and Palmer, R. G. </author> <year> (1991). </year> <title> Introduction to the theory of neural computation. </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: introduction to this class of models. <ref> (Hertz et al., 1991) </ref> has been acclaimed for some time as the reference in the field. It has now been joined by the books by Bishop (1995a) and Ripley (1996).
Reference: <author> Kouam, A. </author> <year> (1993). </year> <institution> Approches connexionnistes pour la prevision des series tem-porelles. </institution> <type> PhD thesis, </type> <institution> Universite de Paris Sud. </institution>
Reference: <author> Mtller, M. </author> <year> (1993a). </year> <title> Efficient Training of Feed-Forward Neural Networks. </title> <type> PhD thesis, </type> <institution> Computer Science department, Aarhus University. </institution>
Reference-contexts: Goutte 1996 16 Parametric regression 1.11 According to Press et al. (1992), the steepest descent algorithm dates back to Cauchy. Its convergence rate is a standard consideration in numerical analysis. It is derived, e.g. by Mtller (1993a). The convergence problem for ill-conditioned Hessians is illustrated e.g. in <ref> (Mtller, 1993a) </ref>, page 16 and (Press et al., 1992), page 421. 1.12 The stochastic algorithm and the associated convergence conditions date back to Robbins and Munro (1951). This algorithm was applied to neural networks at the end of the eighties by Rumelhart et al. (1986).
Reference: <author> Mtller, M. </author> <year> (1993b). </year> <title> Supervised learning on large redundant training sets. </title> <journal> International Journal of Neural Systems, </journal> <volume> 4(1) </volume> <pages> 15-25. </pages>
Reference-contexts: A neural network perspective is offered by Mtller (1993a). The same author mentions several extensions of the conjugate gradient algorithm designed to handle stochastic training, and proposes one in <ref> (Mtller, 1993b) </ref>. 1.14 Second order methods are presented by Battiti (1992), who confirms the equivalence of BFGS and conjugate gradient. Equation (1.46) is a simplification of the update presented by Battiti (1992), page 157.
Reference: <author> Ntrgaard, P. M. </author> <year> (1996). </year> <title> System identification and control with neural networks. </title> <type> PhD thesis, </type> <institution> Department of Automation, Technical University of Denmark. </institution>
Reference: <author> Press, W. H., Teukolsky, S. A., Vetterling, W. T., and Flannery, B. P. </author> <year> (1992). </year> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, 2nd edition. </publisher>
Reference-contexts: Bottou (1991) cites Amari (1967) in the context of adaptive systems and notes that it is nothing more than proper application of the derivation rules invented by Leibnitz in the 17th century. 1.10 Optimisation methods are covered in many books such as e.g. (Fletcher, 1987) or <ref> (Press et al., 1992) </ref> for one-dimensional and multi-dimensional techniques. The quadratic approximation is necessary to handle non linear problems. However, methods presented here are also used in the linear case. Indeed, iterative minimisation methods are a common alternative to the computationally ex pensive matrix inversion in equation (1.12). c flC. <p> Its convergence rate is a standard consideration in numerical analysis. It is derived, e.g. by Mtller (1993a). The convergence problem for ill-conditioned Hessians is illustrated e.g. in (Mtller, 1993a), page 16 and <ref> (Press et al., 1992) </ref>, page 421. 1.12 The stochastic algorithm and the associated convergence conditions date back to Robbins and Munro (1951). This algorithm was applied to neural networks at the end of the eighties by Rumelhart et al. (1986).
Reference: <author> Rasmussen, C. E. </author> <year> (1993). </year> <title> Generalization in neural networks. </title> <type> Master's thesis, </type> <institution> Electronics institute, Technical University of Denmark. </institution>
Reference: <author> Ripley, B. D. </author> <year> (1996). </year> <title> Pattern Recognition and Neural Networks. </title> <publisher> Cambridge University Press. </publisher>
Reference: <author> Robbins, H. and Munro, S. </author> <year> (1951). </year> <title> A stochastic approximation method. </title> <journal> Annals of Math. Stat., </journal> <volume> 22 </volume> <pages> 400-407. </pages>
Reference: <author> Rumelhart, D., Hinton, G., and Williams, R. </author> <year> (1986). </year> <title> Learning internal representation by error propagation. </title> <editor> In Rumelhart, D. and McClellan, J., editors, </editor> <booktitle> Parallel Distributed Processing : exploring the microstructure of cognition, </booktitle> <volume> volume 1, </volume> <pages> pages 318-362. </pages> <publisher> MIT Press. </publisher>
Reference: <author> Tibshirani, R. </author> <year> (1996). </year> <title> Bias, variance and prediction error for classification rules. </title> <type> Technical report, </type> <institution> University of Toronto. </institution> <note> http://utstat.toronto.edu/pub/tibs/biasvar.ps. c flC. Goutte 1996 18 Parametric regression Tsypkin, </note> <author> Y. Z. and Nikolic, Z. J. </author> <year> (1971). </year> <title> Adaptation and learning in automatic systems, </title> <booktitle> volume 73 of Mathematics in science and engineering. </booktitle> <publisher> Academic Press, </publisher> <address> New York and London. </address>
Reference: <author> Vapnik, V. N. </author> <year> (1995). </year> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer. </publisher>
Reference: <author> Werbos, P. J. </author> <year> (1974). </year> <title> Beyond regression: New Tools for Prediction and Analysis in the Behavioral Sciences. </title> <type> PhD thesis, </type> <institution> Harvard University, </institution> <address> Cambridge, MA. </address>
Reference: <author> White, H. </author> <year> (1989). </year> <title> Learning in artificial neural networks: A statistical perspective. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 425-464. </pages>

Reference: <author> Bishop, C. M. </author> <year> (1995). </year> <title> Training with noise is equivalent to thikonov regularization. </title> <booktitle> NC, </booktitle> <volume> 7(1) </volume> <pages> 110-116. </pages>
Reference: <author> Charton, F. </author> <year> (1994). </year> <title> Discussion on the denker example. </title> <type> Personal communication. </type>
Reference: <author> Denker, J., Schwartz, D., Wittner, B., Solla, S., Howard, R., Jackel, L., and Hopfield, J. </author> <year> (1987). </year> <title> Large automatic learning, rule extraction, and generalization. </title> <journal> Complex Systems, </journal> <volume> 1(5) </volume> <pages> 877-922. </pages>
Reference: <author> Dontchev, A. L. and Zolezzi, T. </author> <year> (1992). </year> <title> Well-posed optimization problems. </title> <booktitle> Number 1543 in Lecture notes in mathematics. </booktitle> <publisher> Springer, </publisher> <address> Berlin. </address>
Reference: <author> Goutte, C. </author> <year> (1996). </year> <title> On the use of a pruning prior for neural networks. </title> <booktitle> In Neural Networks for Signal Processing VI Proceedings of the 1996 IEEE Workshop, number VI in NNSP, </booktitle> <pages> pages 52-61, </pages> <address> Piscataway, New Jersey. </address> <publisher> IEEE. </publisher>
Reference: <author> Goutte, C. and Hansen, L. K. </author> <year> (1997). </year> <title> Regularization with a pruning prior. Neural Networks. </title> <note> to appear. </note>
Reference: <author> Grandvalet, Y. </author> <year> (1995). </year> <title> Injection de bruit dans les perceptrons multicouches. </title> <type> PhD thesis, </type> <institution> Universite Technologique de Compiegne, France. </institution>
Reference: <author> Hadamard, J. </author> <year> (1902). </year> <editor> Sur les problemes aux derivees partielles et leur signification physique. Bul. </editor> <publisher> Univ. Princeton, </publisher> <pages> 13(49). </pages>
Reference: <author> Hansen, P. C. </author> <year> (1996). </year> <title> Rank-Deficient and Discrete Ill-Posed Problems. </title> <type> Doctoral Dissertation. </type> <institution> Polyteknisk Forlag, Lyngby (Denmark). c flC. </institution> <note> Goutte 1996 40 Regularisation Hassibi, </note> <author> B. and Stork, D. G. </author> <year> (1993). </year> <title> Second order derivatives for network pruning: Optimal brain surgeon. </title> <editor> In Hanson, S., Cowan, J., and Giles, C., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, volume 5 of NIPS, </booktitle> <pages> pages 164-171. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Krogh, A. and Hertz, J. A. </author> <year> (1992). </year> <title> A simple weight decay can improve generalization. </title> <editor> In Moody, J. E., Hanson, S. J., and Lippman, R. P., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, volume 4 of NIPS. </booktitle>
Reference: <author> Le Cun, Y., Denker, J. S., and Solla, S. A. </author> <year> (1990). </year> <title> Optimal brain damage. </title> <editor> In Touretzky, D. S., editor, </editor> <booktitle> Advances in Neural Information Processing Systems, number 2 in NIPS, </booktitle> <pages> pages 598-605. </pages> <publisher> Morgan-Kaufmann. </publisher>
Reference: <author> Ljung, L., Sjoberg, J., and McKelvey, T. </author> <year> (1992). </year> <title> On the use of regularization in system identification. </title> <type> Technical Report 1379, </type> <institution> Department of Electrical Engineering, Linkoping University, S-581 83 Linkoping, Sweden. </institution>
Reference: <author> Pedersen, M. W., Hansen, L. K., and Larsen, J. </author> <year> (1996). </year> <title> Pruning with generalization based weight saliencies: </title> <editor> flOBD, flOBS. In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, number 8 in NIPS. </booktitle> <publisher> MIT Press. </publisher>
Reference: <author> Plaut, D. C., Nowlan, S. J., and Hinton, G. E. </author> <year> (1986). </year> <title> Experiments on learning by backpropagation. </title> <type> Technical Report CMU-CS-86-126, </type> <institution> Carnegie Mellon University, Pittsburgh, USA. </institution>
Reference: <author> Sjoberg, J. and Ljung, L. </author> <year> (1995). </year> <title> Overtraining, regularization and searching for minimum with application to neural nets. </title> <journal> International Journal of Control. </journal>
Reference: <author> Vapnik, V. N. </author> <year> (1995). </year> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer. </publisher>

Reference: <author> Akaike, H. </author> <year> (1974). </year> <title> A new look at the statistical model identification. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 19(6) </volume> <pages> 716-723. </pages>
Reference: <author> Efron, B. E. </author> <year> (1982). </year> <title> The Jacknife, the Bootstrap and Other Resampling plans, </title> <booktitle> volume 38 of CBMS-NSF Regional Conference Series in Applied Mathematics. </booktitle> <publisher> SIAM. </publisher> <address> c flC. Goutte 1996 References 55 Goutte, C. </address> <year> (1996). </year> <title> On the use of a pruning prior for neural networks. </title> <booktitle> In NNSP96 (1996), </booktitle> <pages> pages 52-61. </pages>
Reference: <author> Goutte, C. </author> <year> (1997). </year> <title> Note on free lunches and cross-validation. Neural Computation, </title> <type> 9(6). </type> <note> to appear. </note>
Reference: <author> Hansen, L. K. and Larsen, J. </author> <year> (1996). </year> <title> Linear unlearning for cross-validation. </title> <booktitle> Advances in Computational Mathematics, </booktitle> <address> 5(2,3):269-280. </address>
Reference: <author> Hornik, K., Stinchcombe, M., and White, H. </author> <year> (1989). </year> <title> Multilayer feedforward networks are universal approximators. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 359-368. </pages>
Reference: <author> Kearns, M. </author> <year> (1996). </year> <title> A bound on the error of cross validation using the approximation and estimation rates, with consequences for the training-test split. </title> <editor> In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, number 8 in NIPS. </booktitle> <publisher> MIT Press. </publisher>
Reference: <author> Larsen, J. </author> <year> (1992). </year> <title> A generalization error estimate for nonlinear systems. </title> <editor> In Kung, S. Y., Fallside, F., and Strensen, J. A., editors, </editor> <booktitle> Neural Networks for Signal Processing Proceedings of the 1992 IEEE Workshop, number II in NNSP, </booktitle> <pages> pages 29-38, </pages> <address> Piscataway, New Jersey. </address> <publisher> IEEE. </publisher>
Reference: <author> Larsen, J. and Hansen, L. K. </author> <year> (1994). </year> <title> Generalized performance of regularized neural networks models. </title> <editor> In Vlontzos, J., Hwang, J. N., and Wilson, E., editors, </editor> <booktitle> Neural Networks for Signal Processing IV Proceedings of the 1994 IEEE Workshop, number IV in NNSP, </booktitle> <pages> pages 42-51, </pages> <address> Piscataway, New Jersey. </address> <publisher> IEEE. </publisher>
Reference: <author> Larsen, J. and Hansen, L. K. </author> <year> (1995). </year> <title> Empirical generalization assessment of neural network models. </title> <editor> In Girosi, F., editor, </editor> <booktitle> Neural Networks for Signal Processing V Proceedings of the 1995 IEEE Workshop, number V in NNSP, </booktitle> <pages> pages 42-51, </pages> <address> Piscataway, New Jersey. </address> <publisher> IEEE. </publisher>
Reference: <author> Larsen, J., Hansen, L. K., Svarer, C., and Ohlsson, M. </author> <year> (1996). </year> <title> Design and regularization of neural networks: the optimal use of a validation set. </title> <booktitle> In NNSP96 (1996), </booktitle> <pages> pages 62-71. </pages>
Reference: <author> Ljung, L., Sjoberg, J., and McKelvey, T. </author> <year> (1992). </year> <title> On the use of regularization in system identification. </title> <type> Technical Report 1379, </type> <institution> Department of Electrical Engineering, Linkoping University, S-581 83 Linkoping, Sweden. </institution>
Reference: <author> Mallows, C. </author> <year> (1973). </year> <title> Some comments on c p . Technometrics, </title> <booktitle> 15 </booktitle> <pages> 661-675. </pages>
Reference: <author> Mtller, M. </author> <year> (1993). </year> <title> A scaled conjugate gradient algorithm for fast supervised learning. </title> <booktitle> Neural Networks, </booktitle> <volume> 6(4) </volume> <pages> 525-533. </pages>
Reference: <author> Moody, J. </author> <year> (1991). </year> <title> Note on generalization, regularization and architecture selection in nonlinear learning systems. </title> <editor> In Juang, B. H., Kung, S. Y., and Kamm, C. A., editors, </editor> <booktitle> Proceedings of the first IEEE Workshop on Neural Networks for Signal Processing, number I in NNSP, </booktitle> <pages> pages 1-10, </pages> <address> Piscataway, New Jersey. </address> <publisher> IEEE. </publisher>
Reference: <author> Murata, N., Yoshizawa, S., and Amari, S. </author> <year> (1994). </year> <title> Network Information Criterion| determining the number of hidden units for an artificial neural network model. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(6) </volume> <pages> 865-872. </pages> <address> c flC. Goutte 1996 56 Hyper-parameters NNSP96 (1996). </address> <booktitle> Neural Networks for Signal Processing VI Proceedings of the 1996 IEEE Workshop, number VI in NNSP, </booktitle> <address> Piscataway, New Jersey. </address> <publisher> IEEE. </publisher>
Reference: <author> Schwartz, G. </author> <year> (1978). </year> <title> Estimating the dimension of a model. </title> <journal> The Annals of Statistics, </journal> <volume> 6(2) </volume> <pages> 461-464. </pages>
Reference: <author> Strensen, P. H., Ntrg-ard, M., Hansen, L. K., and Larsen, J. </author> <year> (1996). </year> <title> Cross-validation with luloo. </title> <booktitle> In Proceedings of 1996 International Conference on Neural Information Processing, </booktitle> <address> ICONIP'96. </address>
Reference: <author> Weigend, A. S., Huberman, B. A., and Rumelhart, D. E. </author> <year> (1990). </year> <title> Predicting the future: a connectionist approach. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1(3) </volume> <pages> 193-210. </pages>
Reference: <author> Wolpert, D. H. and Macready, W. G. </author> <year> (1995). </year> <title> The mathematics of search. </title> <type> Technical Report SFI-TR-95-02-010, </type> <institution> Santa Fe Institute. </institution>

Reference: <author> Cibas, T., Fogelman Soulie, F., Gallinari, P., and Raudys, S. </author> <year> (1994). </year> <title> Variable selection with Optimal Cell Damage. </title> <booktitle> In Proceedings of ICANN'94, </booktitle> <pages> pages 727-730. </pages>
Reference: <author> Goutte, C. </author> <year> (1997a). </year> <title> Extracting the relevant delays in time series modelling. </title> <booktitle> In Neural Networks for Signal Processing VII Proceedings of the 1997 IEEE Workshop, number VII in NNSP, </booktitle> <address> Piscataway, New Jersey. </address> <publisher> IEEE. </publisher>
Reference: <author> Goutte, C. </author> <year> (1997b). </year> <title> Lag space estimation in time series modelling. </title> <booktitle> In Proceedings of ICASSP'97. IEEE. </booktitle>
Reference: <author> He, X. and Asada, H. </author> <year> (1993). </year> <title> A new method for identifying orders of input-output models for nonlinear dynamic systems. </title> <booktitle> In American Conference on Control, </booktitle> <address> San Francisco, California. </address>
Reference: <author> Hocking, R. R. </author> <year> (1976). </year> <title> The analysis and selection of variables in linear regression. </title> <journal> Biometrics, </journal> <volume> 32 </volume> <pages> 1-49. </pages>
Reference: <author> Larsen, J. and Hansen, L. K. </author> <year> (1995). </year> <title> Empirical generalization assessment of neural network models. </title> <editor> In Girosi, F., editor, </editor> <booktitle> Neural Networks for Signal Processing V Proceedings of the 1995 IEEE Workshop, number V in NNSP, </booktitle> <pages> pages 42-51, </pages> <address> Piscataway, New Jersey. </address> <publisher> IEEE. </publisher>
Reference: <author> Leray, P. </author> <year> (1996). </year> <title> La selection de variables. </title> <type> Technical report, </type> <institution> Laforia. </institution>
Reference: <author> McLeod, A. I. </author> <year> (1994). </year> <title> Diagnostic checking of periodic autoregression models with application. </title> <journal> Journal of Time Series Analysis, </journal> <volume> 15(2) </volume> <pages> 221-233. </pages>
Reference: <author> Molina, C., Sampson, N., Fitzgerald, W. J., and Niranjan, M. </author> <year> (1996). </year> <title> Geometrical techniques for finding the embedding dimension of time series. </title> <booktitle> In Neural Networks for Signal Processing VI Proceedings of the 1996 IEEE Workshop, number VI in NNSP, </booktitle> <pages> pages 161-169, </pages> <address> Piscataway, New Jersey. </address> <publisher> IEEE. </publisher>
Reference: <author> Pi, H. and Peterson, C. </author> <year> (1994). </year> <title> Finding the embedding dimension and variable dependences in time series. </title> <journal> Neural Computation, </journal> <volume> 6(3) </volume> <pages> 509-520. </pages>

Reference: <author> Jaynes, E. T. </author> <year> (1985). </year> <title> Bayesian methods: general background. </title> <editor> In Justice, J., editor, </editor> <booktitle> Maximum Entropy and Bayesian Methods in Applied Statistics, </booktitle> <pages> pages 1-25. </pages> <publisher> Cambridge University Press. </publisher>
Reference: <author> MacKay, D. </author> <year> (1992a). </year> <title> Bayesian interpolation. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 415-447. </pages>
Reference: <author> MacKay, D. </author> <year> (1992b). </year> <title> A practical bayesian framework for backprop networks. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 448-472. </pages>
Reference: <author> MacKay, D. </author> <year> (1993). </year> <title> Hyperparameters: Optimize or integrate out? In Heidbreder, </title> <editor> G., editor, </editor> <title> Maximum entropy and Bayesian Methods. </title> <publisher> Kluwer, Dordrecht. </publisher>
Reference: <author> Neal, R. M. </author> <year> (1992). </year> <title> Bayesian training of backpropagation network by the hybrid monte carlo method. </title> <type> Technical Report CRG-TR-92-1, </type> <institution> Connectionist Research Group, Department of Computer Science, University of Toronto. </institution>
Reference: <author> Scales, J. A. and Smith, M. L. </author> <year> (1994). </year> <title> Introductory Geophysical Inverse Theory. </title> <note> Samizdat Press, available via FTP form hilbert.mines.colorado.edu. </note>
Reference: <author> Thodberg, H. H. </author> <year> (1993). </year> <title> Ace of Bayes: application of neural network with pruning. </title> <type> Technical Report 1132-E, </type> <institution> Danish meat research institute, Roskilde, Danmark. </institution>
Reference: <author> Thodberg, H. H. </author> <year> (1996). </year> <title> A review of bayesian neural networks with an application to near infrared spectroscopy. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 7(1) </volume> <pages> 56-72. </pages>
Reference: <author> Vapnik, V. N. </author> <year> (1995). </year> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer. </publisher>
Reference: <author> Williams, P. M. </author> <year> (1995). </year> <title> Bayesian regularization and pruning using a Laplace prior. </title> <journal> Neural Computation, </journal> <volume> 7(1) </volume> <pages> 117-143. </pages> <address> c flC. Goutte 1996 86 Bayesian estimation Wolpert, D. </address> <year> (1995). </year> <title> What Bayes has to say about the evidence procedure. </title> <editor> In Hei-dbreder, G., editor, </editor> <title> 1993 Maximum Entropy and Bayesian Methods Conference. </title> <publisher> Kluwer. </publisher>

Reference: <author> Goutte, C. </author> <year> (1996). </year> <title> On the use of a pruning prior for neural networks. </title> <booktitle> In Neural Networks for Signal Processing VI Proceedings of the 1996 IEEE Workshop, number VI in NNSP, </booktitle> <pages> pages 52-61, </pages> <address> Piscataway, New Jersey. </address> <publisher> IEEE. </publisher>
Reference: <author> Goutte, C. and Hansen, L. K. </author> <year> (1997). </year> <title> Regularization with a pruning prior. Neural Networks. </title> <note> to appear. </note>
Reference: <author> Hansen, L. K. and Rasmussen, C. E. </author> <year> (1994). </year> <title> Pruning from adaptive regularization. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 1223-1232. </pages>
Reference: <author> Krogh, A. and Hertz, J. A. </author> <year> (1992). </year> <title> A simple weight decay can improve generalization. </title> <editor> In Moody, J. E., Hanson, S. J., and Lippman, R. P., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, volume 4 of NIPS. </booktitle>
Reference: <author> Le Cun, Y., Denker, J. S., and Solla, S. A. </author> <year> (1990). </year> <title> Optimal brain damage. </title> <editor> In Touretzky, D. S., editor, </editor> <booktitle> Advances in Neural Information Processing Systems, number 2 in NIPS, </booktitle> <pages> pages 598-605. </pages> <publisher> Morgan-Kaufmann. </publisher>
Reference: <author> Ljung, L., Sjoberg, J., and McKelvey, T. </author> <year> (1992). </year> <title> On the use of regularization in system identification. </title> <type> Technical Report 1379, </type> <institution> Department of Electrical Engineering, Linkoping University, S-581 83 Linkoping, Sweden. </institution>
Reference: <author> Press, W. H., Teukolsky, S. A., Vetterling, W. T., and Flannery, B. P. </author> <year> (1992). </year> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, 2nd edition. </publisher>
Reference-contexts: Bottou (1991) cites Amari (1967) in the context of adaptive systems and notes that it is nothing more than proper application of the derivation rules invented by Leibnitz in the 17th century. 1.10 Optimisation methods are covered in many books such as e.g. (Fletcher, 1987) or <ref> (Press et al., 1992) </ref> for one-dimensional and multi-dimensional techniques. The quadratic approximation is necessary to handle non linear problems. However, methods presented here are also used in the linear case. Indeed, iterative minimisation methods are a common alternative to the computationally ex pensive matrix inversion in equation (1.12). c flC. <p> Its convergence rate is a standard consideration in numerical analysis. It is derived, e.g. by Mtller (1993a). The convergence problem for ill-conditioned Hessians is illustrated e.g. in (Mtller, 1993a), page 16 and <ref> (Press et al., 1992) </ref>, page 421. 1.12 The stochastic algorithm and the associated convergence conditions date back to Robbins and Munro (1951). This algorithm was applied to neural networks at the end of the eighties by Rumelhart et al. (1986).
Reference: <author> Svarer, C., Hansen, L., and Larsen, J. </author> <year> (1993). </year> <title> On design and evaluation of tapped-delay neural network architectures. </title> <editor> In et al., H. B., editor, </editor> <booktitle> IEEE International Conference on Neural Networks, ICNN, </booktitle> <pages> pages 46-51, </pages> <address> Piscataway, NJ. </address> <publisher> IEEE. </publisher>
Reference: <author> Weigend, A. S., Huberman, B. A., and Rumelhart, D. E. </author> <year> (1990). </year> <title> Predicting the future: a connectionist approach. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1(3) </volume> <pages> 193-210. </pages>

Reference: <editor> Definition A.8 Two events A and B are independent iff P (A " B) = P (A) P (B). </editor> <title> We can also define the concept of independence by using the conditional probability. </title>

Reference: <author> Goutte, C. </author> <year> (1997). </year> <title> Lag space estimation in time series modelling. </title> <booktitle> In Proceedings of ICASSP'97. IEEE. </booktitle>
Reference: <author> Hardle, W. </author> <year> (1990). </year> <title> Applied nonparametric regression. Number 19 in Econometric Society Monographs. </title> <publisher> Cambridge University Press. </publisher> <address> c flC. </address> <note> Goutte 1996 References 127 Lai, </note> <author> S. L. </author> <year> (1977). </year> <title> Large sample properties of k-nearest neighbor procedures. </title> <type> PhD thesis, </type> <institution> Department of Mathematics, UCLA, </institution> <address> Los Angeles. </address>
Reference: <author> Lowe, D. </author> <year> (1995). </year> <title> Similarity metric learning for a variable-kernel classifier. </title> <booktitle> NC, </booktitle> <volume> 7(1) </volume> <pages> 72-85. </pages>
Reference: <author> Mack, Y. P. </author> <year> (1981). </year> <title> Local properties of k-nn regression estimates. </title> <journal> SIAM Journal on Algorithm and Discrete Methods, </journal> <volume> 2 </volume> <pages> 311-323. </pages>
Reference: <author> Parzen, E. </author> <year> (1962). </year> <title> On estimation of a probability density and mode. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 33 </volume> <pages> 1065-76. </pages>
Reference: <author> Specht, D. F. </author> <year> (1991). </year> <title> A general regression neural network. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2(6) </volume> <pages> 568-576. </pages> <note> c flC. Goutte 1996 128 Non-parametric regression c flC. Goutte 1996 Bibliography Akaike, </note> <author> H. </author> <year> (1969). </year> <title> Fitting autoregressive models for prediction. </title> <journal> Annals of the Institute of Statistical Mathematics, </journal> <volume> 21 </volume> <pages> 243-247. </pages>
Reference: <author> Akaike, H. </author> <year> (1974). </year> <title> A new look at the statistical model identification. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 19(6) </volume> <pages> 716-723. </pages>
Reference: <author> Aleksic, Z. </author> <year> (1991). </year> <title> Estimating the embedding dimension. </title> <journal> Physica D, </journal> <volume> 52 </volume> <pages> 362-368. </pages>
Reference: <author> Amari, S. I. </author> <year> (1967). </year> <title> A theory of adaptive pattern classifiers. </title> <journal> IEEE Transactions on Electronic Computers, </journal> <volume> 16 </volume> <pages> 279-307. </pages>
Reference: <author> Badeva, V. and Morozov, V. </author> <year> (1991). </year> <note> Problemes incorrectement poses. Serie automa-tique. Masson, Paris. </note>
Reference: <author> Battiti, R. </author> <year> (1992). </year> <title> First- and second-order methods for learning: Between steepest descent and newton's method. </title> <journal> Neural Computation, </journal> <volume> 4(2) </volume> <pages> 141-166. </pages>
Reference: <author> Bishop, C. M. </author> <year> (1995a). </year> <title> Neural Networks for pattern recognition. </title> <publisher> Clarendon Press, Oxford. </publisher>
Reference: <author> Bishop, C. M. </author> <year> (1995b). </year> <title> Training with noise is equivalent to thikonov regularization. </title> <booktitle> NC, </booktitle> <volume> 7(1) </volume> <pages> 110-116. </pages>
Reference: <author> Bottou, L. </author> <year> (1991). </year> <institution> Une approche theorique de l'apprentissage connexionniste : application a la reconnaissance de la parole. These de doctorat, Universite Paris XI, Orsay. </institution>
Reference-contexts: This algorithm was applied to neural networks at the end of the eighties by Rumelhart et al. (1986). A good presentation including demonstrations of the convergence of the algorithm is given in <ref> (Bottou, 1991) </ref>. 1.13 The conjugate gradient algorithm is studied extensively in the linear optimisa-tion literature. A neural network perspective is offered by Mtller (1993a).
Reference: <author> Bottou, L. and Le Cun, Y. </author> <year> (1988). </year> <title> SN: A simulator for connectionist models. </title> <booktitle> In NeuroN^mes 88, </booktitle> <pages> pages 371-382, </pages> <address> N^mes, France. </address>
Reference: <author> Bryson, A., Denham, W., and Dreyfuss, S. </author> <year> (1963). </year> <title> Optimal programming problem with inequality constraints. I: Necessary conditions for extremal solutions. </title> <journal> AIAA journal, </journal> <volume> 1 </volume> <pages> 25-44. </pages>
Reference-contexts: Incidentally, these books adopt a statistical perspective. 1.9 The back-propagation rule is usually credited to Werbos (1974) or Rumelhart et al. (1986). However, it was discovered earlier in different contexts. Vapnik (1995) mentions its use in <ref> (Bryson et al., 1963) </ref> for solving some control problems.
Reference: <author> Charton, F. </author> <year> (1994). </year> <title> Discussion on the denker example. </title> <type> Personal communication. </type>
Reference: <author> Cibas, T., Fogelman Soulie, F., Gallinari, P., and Raudys, S. </author> <year> (1994). </year> <title> Variable selection with Optimal Cell Damage. </title> <booktitle> In Proceedings of ICANN'94, </booktitle> <pages> pages 727-730. </pages>
Reference: <author> Denker, J., Schwartz, D., Wittner, B., Solla, S., Howard, R., Jackel, L., and Hopfield, J. </author> <year> (1987). </year> <title> Large automatic learning, rule extraction, and generalization. </title> <journal> Complex Systems, </journal> <volume> 1(5) </volume> <pages> 877-922. </pages> <note> 129 130 Bibliography Dontchev, </note> <author> A. L. and Zolezzi, T. </author> <year> (1992). </year> <title> Well-posed optimization problems. </title> <booktitle> Number 1543 in Lecture notes in mathematics. </booktitle> <publisher> Springer, </publisher> <address> Berlin. </address>
Reference: <author> Efron, B. </author> <year> (1986). </year> <title> Why isn't everyone a bayesian? The American Statistician, 40(1) 1-11. with comments. </title>
Reference: <author> Efron, B. E. </author> <year> (1982). </year> <title> The Jacknife, the Bootstrap and Other Resampling plans, </title> <booktitle> volume 38 of CBMS-NSF Regional Conference Series in Applied Mathematics. </booktitle> <publisher> SIAM. </publisher>
Reference: <author> Fletcher, R. </author> <year> (1987). </year> <title> Practical Methods of Optimization. </title> <publisher> Wiley. </publisher>
Reference-contexts: Bottou (1991) cites Amari (1967) in the context of adaptive systems and notes that it is nothing more than proper application of the derivation rules invented by Leibnitz in the 17th century. 1.10 Optimisation methods are covered in many books such as e.g. <ref> (Fletcher, 1987) </ref> or (Press et al., 1992) for one-dimensional and multi-dimensional techniques. The quadratic approximation is necessary to handle non linear problems. However, methods presented here are also used in the linear case.
Reference: <author> Friedman, J. H. </author> <year> (1996). </year> <title> On bias, variance, 0/1 loss, and the curse-of-dimensionality. </title> <type> Technical report, </type> <institution> Department of Statistics, Stanford University. ftp://playfair.stanford.edu/pub/friedman/curse.ps.Z. </institution>
Reference: <author> Geman, S., Bienenstock, E., and Doursat, R. </author> <year> (1992). </year> <title> Neural networks and the bias/variance dilemna. </title> <journal> Neural Computation, </journal> <volume> 4(1) </volume> <pages> 1-58. </pages>
Reference: <author> Goutte, C. </author> <year> (1996). </year> <title> On the use of a pruning prior for neural networks. </title> <booktitle> In NNSP96 (1996), </booktitle> <pages> pages 52-61. </pages>
Reference: <author> Goutte, C. </author> <year> (1997a). </year> <title> Extracting the relevant delays in time series modelling. </title> <booktitle> In Neural Networks for Signal Processing VII Proceedings of the 1997 IEEE Workshop, number VII in NNSP, </booktitle> <address> Piscataway, New Jersey. </address> <publisher> IEEE. </publisher>
Reference: <author> Goutte, C. </author> <year> (1997b). </year> <title> Lag space estimation in time series modelling. </title> <booktitle> In Proceedings of ICASSP'97. IEEE. </booktitle>
Reference: <author> Goutte, C. </author> <year> (1997c). </year> <title> Note on free lunches and cross-validation. Neural Computation, </title> <type> 9(6). </type> <note> to appear. </note>
Reference: <author> Goutte, C. and Hansen, L. K. </author> <year> (1997). </year> <title> Regularization with a pruning prior. Neural Networks. </title> <note> to appear. </note>
Reference: <author> Goutte, C. and Ledoux, C. </author> <year> (1995). </year> <title> Synthese des techniques de commande connex-ionniste. </title> <type> Technical Report 95/02, </type> <institution> LAFORIA. </institution>
Reference: <author> Grandvalet, Y. </author> <year> (1995). </year> <title> Injection de bruit dans les perceptrons multicouches. </title> <type> PhD thesis, </type> <institution> Universite Technologique de Compiegne, France. </institution>
Reference: <author> Hadamard, J. </author> <year> (1902). </year> <editor> Sur les problemes aux derivees partielles et leur signification physique. Bul. </editor> <publisher> Univ. Princeton, </publisher> <pages> 13(49). </pages>
Reference: <author> Hansen, L. K. and Larsen, J. </author> <year> (1996). </year> <title> Linear unlearning for cross-validation. </title> <booktitle> Advances in Computational Mathematics, </booktitle> <address> 5(2,3):269-280. </address>
Reference: <author> Hansen, L. K. and Rasmussen, C. E. </author> <year> (1994). </year> <title> Pruning from adaptive regularization. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 1223-1232. </pages>
Reference: <author> Hansen, P. C. </author> <year> (1996). </year> <title> Rank-Deficient and Discrete Ill-Posed Problems. </title> <type> Doctoral Dissertation. </type> <institution> Polyteknisk Forlag, Lyngby (Denmark). </institution>
Reference: <author> Hardle, W. </author> <year> (1990). </year> <title> Applied nonparametric regression. Number 19 in Econometric Society Monographs. </title> <publisher> Cambridge University Press. </publisher> <address> c flC. </address> <note> Goutte 1996 Bibliography 131 Hassibi, </note> <author> B. and Stork, D. G. </author> <year> (1993). </year> <title> Second order derivatives for network pruning: Optimal brain surgeon. </title> <editor> In Hanson, S., Cowan, J., and Giles, C., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, volume 5 of NIPS, </booktitle> <pages> pages 164-171. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> He, X. and Asada, H. </author> <year> (1993). </year> <title> A new method for identifying orders of input-output models for nonlinear dynamic systems. </title> <booktitle> In American Conference on Control, </booktitle> <address> San Francisco, California. </address>
Reference: <author> Hertz, J., Krogh, A., and Palmer, R. G. </author> <year> (1991). </year> <title> Introduction to the theory of neural computation. </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: introduction to this class of models. <ref> (Hertz et al., 1991) </ref> has been acclaimed for some time as the reference in the field. It has now been joined by the books by Bishop (1995a) and Ripley (1996).
Reference: <author> Hocking, R. R. </author> <year> (1976). </year> <title> The analysis and selection of variables in linear regression. </title> <journal> Biometrics, </journal> <volume> 32 </volume> <pages> 1-49. </pages>
Reference: <author> Hornik, K., Stinchcombe, M., and White, H. </author> <year> (1989). </year> <title> Multilayer feedforward networks are universal approximators. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 359-368. </pages>
Reference: <author> Jaynes, E. T. </author> <year> (1985). </year> <title> Bayesian methods: general background. </title> <editor> In Justice, J., editor, </editor> <booktitle> Maximum Entropy and Bayesian Methods in Applied Statistics, </booktitle> <pages> pages 1-25. </pages> <publisher> Cambridge University Press. </publisher>
Reference: <author> Kouam, A. </author> <year> (1993). </year> <institution> Approches connexionnistes pour la prevision des series tem-porelles. </institution> <type> PhD thesis, </type> <institution> Universite de Paris Sud. </institution>
Reference: <author> Krogh, A. and Hertz, J. A. </author> <year> (1992). </year> <title> A simple weight decay can improve generalization. </title> <editor> In Moody, J. E., Hanson, S. J., and Lippman, R. P., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, volume 4 of NIPS. </booktitle>
Reference: <author> Lai, S. L. </author> <year> (1977). </year> <title> Large sample properties of k-nearest neighbor procedures. </title> <type> PhD thesis, </type> <institution> Department of Mathematics, UCLA, </institution> <address> Los Angeles. </address>
Reference: <author> Larsen, J. and Hansen, L. K. </author> <year> (1994). </year> <title> Generalized performance of regularized neural networks models. </title> <editor> In Vlontzos, J., Hwang, J. N., and Wilson, E., editors, </editor> <booktitle> Neural Networks for Signal Processing IV Proceedings of the 1994 IEEE Workshop, number IV in NNSP, </booktitle> <pages> pages 42-51, </pages> <address> Piscataway, New Jersey. </address> <publisher> IEEE. </publisher>
Reference: <author> Larsen, J. and Hansen, L. K. </author> <year> (1995). </year> <title> Empirical generalization assessment of neural network models. </title> <editor> In Girosi, F., editor, </editor> <booktitle> Neural Networks for Signal Processing V Proceedings of the 1995 IEEE Workshop, number V in NNSP, </booktitle> <pages> pages 42-51, </pages> <address> Piscataway, New Jersey. </address> <publisher> IEEE. </publisher>
Reference: <author> Larsen, J., Hansen, L. K., Svarer, C., and Ohlsson, M. </author> <year> (1996). </year> <title> Design and regularization of neural networks: the optimal use of a validation set. </title> <booktitle> In NNSP96 (1996), </booktitle> <pages> pages 62-71. </pages>
Reference: <author> Le Cun, Y., Denker, J. S., and Solla, S. A. </author> <year> (1990). </year> <title> Optimal brain damage. </title> <editor> In Touretzky, D. S., editor, </editor> <booktitle> Advances in Neural Information Processing Systems, number 2 in NIPS, </booktitle> <pages> pages 598-605. </pages> <publisher> Morgan-Kaufmann. </publisher>
Reference: <author> Leray, P. </author> <year> (1996). </year> <title> La selection de variables. </title> <type> Technical report, </type> <note> Laforia. c flC. Goutte 1996 132 Bibliography Ljung, </note> <author> L., Sjoberg, J., and McKelvey, T. </author> <year> (1992). </year> <title> On the use of regularization in system identification. </title> <type> Technical Report 1379, </type> <institution> Department of Electrical Engineering, Linkoping University, S-581 83 Linkoping, Sweden. </institution>
Reference: <author> Lowe, D. </author> <year> (1995). </year> <title> Similarity metric learning for a variable-kernel classifier. </title> <booktitle> NC, </booktitle> <volume> 7(1) </volume> <pages> 72-85. </pages>
Reference: <author> Mack, Y. P. </author> <year> (1981). </year> <title> Local properties of k-nn regression estimates. </title> <journal> SIAM Journal on Algorithm and Discrete Methods, </journal> <volume> 2 </volume> <pages> 311-323. </pages>
Reference: <author> MacKay, D. </author> <year> (1992a). </year> <title> Bayesian interpolation. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 415-447. </pages>
Reference: <author> MacKay, D. </author> <year> (1992b). </year> <title> A practical bayesian framework for backprop networks. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 448-472. </pages>
Reference: <author> MacKay, D. </author> <year> (1993). </year> <title> Hyperparameters: Optimize or integrate out? In Heidbreder, </title> <editor> G., editor, </editor> <title> Maximum entropy and Bayesian Methods. </title> <publisher> Kluwer, Dordrecht. </publisher>
Reference: <author> Mallows, C. </author> <year> (1973). </year> <title> Some comments on c p . Technometrics, </title> <booktitle> 15 </booktitle> <pages> 661-675. </pages>
Reference: <author> McLeod, A. I. </author> <year> (1994). </year> <title> Diagnostic checking of periodic autoregression models with application. </title> <journal> Journal of Time Series Analysis, </journal> <volume> 15(2) </volume> <pages> 221-233. </pages>
Reference: <author> Molina, C., Sampson, N., Fitzgerald, W. J., and Niranjan, M. </author> <year> (1996). </year> <title> Geometrical techniques for finding the embedding dimension of time series. </title> <booktitle> In NNSP96 (1996), </booktitle> <pages> pages 161-169. </pages>
Reference: <author> Mtller, M. </author> <year> (1993a). </year> <title> Efficient Training of Feed-Forward Neural Networks. </title> <type> PhD thesis, </type> <institution> Computer Science department, Aarhus University. </institution>
Reference-contexts: Goutte 1996 16 Parametric regression 1.11 According to Press et al. (1992), the steepest descent algorithm dates back to Cauchy. Its convergence rate is a standard consideration in numerical analysis. It is derived, e.g. by Mtller (1993a). The convergence problem for ill-conditioned Hessians is illustrated e.g. in <ref> (Mtller, 1993a) </ref>, page 16 and (Press et al., 1992), page 421. 1.12 The stochastic algorithm and the associated convergence conditions date back to Robbins and Munro (1951). This algorithm was applied to neural networks at the end of the eighties by Rumelhart et al. (1986).
Reference: <author> Mtller, M. </author> <year> (1993b). </year> <title> A scaled conjugate gradient algorithm for fast supervised learning. </title> <booktitle> Neural Networks, </booktitle> <volume> 6(4) </volume> <pages> 525-533. </pages>
Reference-contexts: A neural network perspective is offered by Mtller (1993a). The same author mentions several extensions of the conjugate gradient algorithm designed to handle stochastic training, and proposes one in <ref> (Mtller, 1993b) </ref>. 1.14 Second order methods are presented by Battiti (1992), who confirms the equivalence of BFGS and conjugate gradient. Equation (1.46) is a simplification of the update presented by Battiti (1992), page 157.
Reference: <author> Mtller, M. </author> <year> (1993c). </year> <title> Supervised learning on large redundant training sets. </title> <journal> International Journal of Neural Systems, </journal> <volume> 4(1) </volume> <pages> 15-25. </pages>
Reference: <author> Moody, J. </author> <year> (1991). </year> <title> Note on generalization, regularization and architecture selection in nonlinear learning systems. </title> <editor> In Juang, B. H., Kung, S. Y., and Kamm, C. A., editors, </editor> <booktitle> Proceedings of the first IEEE Workshop on Neural Networks for Signal Processing, number I in NNSP, </booktitle> <pages> pages 1-10, </pages> <address> Piscataway, New Jersey. </address> <publisher> IEEE. </publisher>
Reference: <author> Neal, R. M. </author> <year> (1992). </year> <title> Bayesian training of backpropagation network by the hybrid monte carlo method. </title> <type> Technical Report CRG-TR-92-1, </type> <institution> Connectionist Research Group, Department of Computer Science, University of Toronto. </institution> <month> NNSP96 </month> <year> (1996). </year> <booktitle> Neural Networks for Signal Processing VI Proceedings of the 1996 IEEE Workshop, number VI in NNSP, </booktitle> <address> Piscataway, New Jersey. </address> <publisher> IEEE. </publisher>
Reference: <author> Ntrgaard, P. M. </author> <year> (1996). </year> <title> System identification and control with neural networks. </title> <type> PhD thesis, </type> <institution> Department of Automation, Technical University of Denmark. </institution>
Reference: <author> Parzen, E. </author> <year> (1962). </year> <title> On estimation of a probability density and mode. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 33 </volume> <pages> 1065-76. </pages> <note> c flC. Goutte 1996 Bibliography 133 Pi, </note> <author> H. and Peterson, C. </author> <year> (1994). </year> <title> Finding the embedding dimension and variable dependences in time series. </title> <journal> Neural Computation, </journal> <volume> 6(3) </volume> <pages> 509-520. </pages>
Reference: <author> Plaut, D. C., Nowlan, S. J., and Hinton, G. E. </author> <year> (1986). </year> <title> Experiments on learning by backpropagation. </title> <type> Technical Report CMU-CS-86-126, </type> <institution> Carnegie Mellon University, Pittsburgh, USA. </institution>
Reference: <author> Press, W. H., Teukolsky, S. A., Vetterling, W. T., and Flannery, B. P. </author> <year> (1992). </year> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, 2nd edition. </publisher>
Reference-contexts: Bottou (1991) cites Amari (1967) in the context of adaptive systems and notes that it is nothing more than proper application of the derivation rules invented by Leibnitz in the 17th century. 1.10 Optimisation methods are covered in many books such as e.g. (Fletcher, 1987) or <ref> (Press et al., 1992) </ref> for one-dimensional and multi-dimensional techniques. The quadratic approximation is necessary to handle non linear problems. However, methods presented here are also used in the linear case. Indeed, iterative minimisation methods are a common alternative to the computationally ex pensive matrix inversion in equation (1.12). c flC. <p> Its convergence rate is a standard consideration in numerical analysis. It is derived, e.g. by Mtller (1993a). The convergence problem for ill-conditioned Hessians is illustrated e.g. in (Mtller, 1993a), page 16 and <ref> (Press et al., 1992) </ref>, page 421. 1.12 The stochastic algorithm and the associated convergence conditions date back to Robbins and Munro (1951). This algorithm was applied to neural networks at the end of the eighties by Rumelhart et al. (1986).
Reference: <author> Rasmussen, C. E. </author> <year> (1993). </year> <title> Generalization in neural networks. </title> <type> Master's thesis, </type> <institution> Electronics institute, Technical University of Denmark. </institution>
Reference: <author> Ripley, B. D. </author> <year> (1996). </year> <title> Pattern Recognition and Neural Networks. </title> <publisher> Cambridge University Press. </publisher>
Reference: <author> Robbins, H. and Munro, S. </author> <year> (1951). </year> <title> A stochastic approximation method. </title> <journal> Annals of Math. Stat., </journal> <volume> 22 </volume> <pages> 400-407. </pages>
Reference: <author> Rumelhart, D., Hinton, G., and Williams, R. </author> <year> (1986). </year> <title> Learning internal representation by error propagation. </title> <editor> In Rumelhart, D. and McClellan, J., editors, </editor> <booktitle> Parallel Distributed Processing : exploring the microstructure of cognition, </booktitle> <volume> volume 1, </volume> <pages> pages 318-362. </pages> <publisher> MIT Press. </publisher>
Reference: <author> Savit, R. and Green, M. </author> <year> (1991). </year> <title> Time series and dependent variables. </title> <journal> Physica D, </journal> <volume> 50(1) </volume> <pages> 95-116. </pages>
Reference: <author> Scales, J. A. and Smith, M. L. </author> <year> (1994). </year> <title> Introductory Geophysical Inverse Theory. </title> <note> Samizdat Press, available via FTP form hilbert.mines.colorado.edu. </note>
Reference: <author> Schwartz, G. </author> <year> (1978). </year> <title> Estimating the dimension of a model. </title> <journal> The Annals of Statistics, </journal> <volume> 6(2) </volume> <pages> 461-464. </pages>
Reference: <author> Strensen, P. H., Ntrg-ard, M., Hansen, L. K., and Larsen, J. </author> <year> (1996). </year> <title> Cross-validation with luloo. </title> <booktitle> In Proceedings of 1996 International Conference on Neural Information Processing, </booktitle> <address> ICONIP'96. </address>
Reference: <author> Specht, D. F. </author> <year> (1991). </year> <title> A general regression neural network. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2(6) </volume> <pages> 568-576. </pages>
Reference: <author> Svarer, C., Hansen, L., and Larsen, J. </author> <year> (1993). </year> <title> On design and evaluation of tapped-delay neural network architectures. </title> <editor> In et al., H. B., editor, </editor> <booktitle> IEEE International Conference on Neural Networks, ICNN, </booktitle> <pages> pages 46-51, </pages> <address> Piscataway, NJ. </address> <publisher> IEEE. </publisher>
Reference: <author> Thodberg, H. H. </author> <year> (1993). </year> <title> Ace of Bayes: application of neural network with pruning. </title> <type> Technical Report 1132-E, </type> <institution> Danish meat research institute, Roskilde, Danmark. </institution>
Reference: <author> Thodberg, H. H. </author> <year> (1996). </year> <title> A review of bayesian neural networks with an application to near infrared spectroscopy. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 7(1) </volume> <pages> 56-72. </pages>
Reference: <author> Tibshirani, R. </author> <year> (1996). </year> <title> Bias, variance and prediction error for classification rules. </title> <type> Technical report, </type> <institution> University of Toronto. </institution> <note> http://utstat.toronto.edu/pub/tibs/biasvar.ps. c flC. Goutte 1996 134 Bibliography Tsypkin, </note> <author> Y. Z. and Nikolic, Z. J. </author> <year> (1971). </year> <title> Adaptation and learning in automatic systems, </title> <booktitle> volume 73 of Mathematics in science and engineering. </booktitle> <publisher> Academic Press, </publisher> <address> New York and London. </address>
Reference: <author> Vapnik, V. N. </author> <year> (1995). </year> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer. </publisher>
Reference: <author> Weigend, A. S., Huberman, B. A., and Rumelhart, D. E. </author> <year> (1990). </year> <title> Predicting the future: a connectionist approach. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1(3) </volume> <pages> 193-210. </pages>
Reference: <author> Werbos, P. J. </author> <year> (1974). </year> <title> Beyond regression: New Tools for Prediction and Analysis in the Behavioral Sciences. </title> <type> PhD thesis, </type> <institution> Harvard University, </institution> <address> Cambridge, MA. </address>
Reference: <author> White, H. </author> <year> (1989). </year> <title> Learning in artificial neural networks: A statistical perspective. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 425-464. </pages>
Reference: <author> Williams, P. M. </author> <year> (1995). </year> <title> Bayesian regularization and pruning using a Laplace prior. </title> <journal> Neural Computation, </journal> <volume> 7(1) </volume> <pages> 117-143. </pages>
Reference: <author> Wolpert, D. H. </author> <year> (1993). </year> <title> On the use of evidence in neural networks. </title> <editor> In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, number 5 in NIPS, </booktitle> <pages> pages 539-546. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Wolpert, D. H. </author> <year> (1995). </year> <title> What Bayes has to say about the evidence procedure. </title> <editor> In Hei-dbreder, G., editor, </editor> <title> 1993 Maximum Entropy and Bayesian Methods Conference. </title> <publisher> Kluwer. </publisher>
Reference: <author> Wolpert, D. H. </author> <year> (1996). </year> <title> The lack of a priori distinctions between learning algorithms. </title> <journal> Neural Computation, </journal> <volume> 8(8) </volume> <pages> 1341-1390. </pages>
Reference: <author> Wolpert, D. H. and Macready, W. G. </author> <year> (1995). </year> <title> The mathematics of search. </title> <type> Technical Report SFI-TR-95-02-010, </type> <institution> Santa Fe Institute. </institution>
Reference: <author> Yule, G. U. </author> <year> (1927). </year> <title> On a method of investigating periodicities in disturbed series with special reference to Wolfer's sunspot numbers. </title> <journal> Philos. Trans. R. Soc. Lond. Ser. </journal> <note> A, 226:267. </note>

References-found: 178

