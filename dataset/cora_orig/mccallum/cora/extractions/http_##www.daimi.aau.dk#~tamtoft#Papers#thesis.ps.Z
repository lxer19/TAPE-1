URL: http://www.daimi.aau.dk/~tamtoft/Papers/thesis.ps.Z
Refering-URL: http://www.daimi.aau.dk/~tamtoft/papers.html
Root-URL: http://www.daimi.aau.dk
Email: internet: tamtoft@daimi.aau.dk  
Title: Sharing of Computations  
Author: Torben Amtoft 
Date: August 29, 1993  
Address: Ny Munkegade, DK-8000 Arhus C, Denmark  
Affiliation: Computer Science Department Aarhus University,  
Abstract-found: 0
Intro-found: 1
Reference: [Aho90] <author> Alfred V. Aho. </author> <title> Algorithms for finding patterns in strings. </title> <editor> In Jan van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, vol. A, chapter 5. </booktitle> <publisher> Elsevier, </publisher> <year> 1990. </year>
Reference-contexts: Finally, section 7.6 compares with related work. Some remarks concerning our treatment: 1 It is easy to see how it can be coded as level 0 rules. 2 For a survey of pattern matching algorithms, see <ref> [Aho90] </ref>. 160 * we will not formally prove that the target programs indeed im-plement the KMP/BM algorithms, as this would require a rather heavy machinery; * no attention will be paid to the complexity of the preprocessing phase; * as it is a rather common exercise to achieve the KMP algorithm
Reference: [AHU74] <author> Alfred V. Aho, John E. Hopcroft, and Jeffrey D. Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley, </publisher> <year> 1974. </year>
Reference-contexts: The program to be considered is a simulator for two-way deterministic pushdown automata (2DPDA) <ref> [AHU74, chap. 9] </ref>. It caused much surprise when Cook showed that it is always possible to simulate a 2DPDA in linear time (wrt. the length of the input tape), even if the automaton carries out an exponential number of steps. <p> The program to be considered is a simulator for two-way deterministic pushdown automata (2DPDA) <ref> [AHU74, chap. 9] </ref>. <p> Another way to detect loops would be to keep track of whether the 2DM twice performs a PUSH step pushing the same expression. 6.5 Previous work The original technique for linear time simulation of a 2DPDA, used in [Coo71] and restated as <ref> [AHU74, algorithm 9.4] </ref>, is a bottom-up approach (cf. page 20). <p> Motivated by this observation [Jon77] presents what amounts to a top-down version of the algorithm, which gives rise to essentially the same "flow-of-control" as the 2DM. In [Wat80] it is shown how something similar to <ref> [AHU74, algorithm 9.4] </ref> can be derived by means of general tabulation techniques. <p> As made explicit in <ref> [AHU74, Algorithm 9.3] </ref>, it amounts to constructing from p a DFA which given s as input enters its accepting state exactly at the end of the first occurrence of p in s. Thus at most jsj references to s are made. <p> Last, but certainly not least, one must acknowledge the work done by Olivier Danvy within the field unfortunately mostly unpublished, but being a great inspiration and covering a wide range of topics, e.g. how to get the effect of Weiner trees (cf. <ref> [AHU74, section 9.5] </ref>). 177 Chapter 8 A model for a logic language In this chapter we will formalize the idea of multilevel transition systems in a logic programming setting.
Reference: [Amt91] <author> Torben Amtoft. </author> <title> Properties of unfolding-based meta-level systems. In Partial Evaluation and Semantics-Based Program Manipulation, New Haven, Connecticut. </title> <journal> (Sigplan Notices, </journal> <volume> vol. 26, no. 9), </volume> <year> 1991. </year>
Reference-contexts: In section 4.8 we factor out some reasons why a program optimization technique may yield more than a constant (polynomial) speedup (that is, we investigate the underlying assumptions behind theorem 4.6.3 and theorem 4.6.4). The main ideas exposed in these three sections were presented at PEPM'91 <ref> [Amt91] </ref>, however in a logic programming setting. Finally, section 4.9 attempts to put the present work in perspective by describing other approaches from the literature. * In chapter 5 an abstract, non-deterministic machine (to be called an USM) implementing ultimate sharing (cf. chapter 3) is defined. <p> how those two issues are carried over to the logic world: 8.0.1 Speedup bounds in the logic world We certainly can expect something similar to theorem 4.6.3 and theorem 4.6.4 to hold in fact, these two theorems were originally stated for a model of a logic language (without explicit folding) <ref> [Amt91] </ref>. Also we can, similar to what is done in section 4.8, factor out some features whose presence enables more than a constant speedup. <p> In the literature, one is typically (contrary to our framework) not allowed to fold against a (direct or indirect) recursive predicate [KK90], [PP91a], [Sek91], [TS84]. This mirrors the view that folding corresponds to abbreviation, a view also held in <ref> [Amt91] </ref>. [TS84] and [KK90] divide the predicates into two classes: the new (corresponding to "eureka-definitions") and old, where folding is allowed against new predicates only. In the body of new predicates as well as in the body of old predicates, only old predicates can occur. <p> The crucial condition on folding is that the leftmost atom has been unfolded. Again by assigning the predicates folded against weight 0 and the others 1, the essence of this translates into our condition 8.1.2. A version of condition 8.1.2 is also stated in <ref> [Amt91] </ref>. [BCE92] gives sufficient conditions for replacement (folding being a special case) to be safe.
Reference: [Amt92a] <author> Torben Amtoft. </author> <title> Unfold/fold transformations preserving termination properties. </title> <editor> In M. Bruynooghe and M. Wirsing, editors, </editor> <booktitle> 4th International Symposium on Programming Language Implementation and Logic Programming (PLILP 92), </booktitle> <address> Leuven, Belgium, </address> <pages> pages 187-201. </pages> <publisher> Springer Verlag, LNCS no 631, </publisher> <month> August </month> <year> 1992. </year>
Reference-contexts: This further formal izes the intuition that KMP and BM are "dual". * In chapter 8 a model is set up for a logic language; special emphasis is put on giving criteria for total correctness of unfold/fold transformations. The basic ideas in this chapter have been presented at PLILP'92 <ref> [Amt92a] </ref>; the chapter itself is (apart from the introductory parts) almost identical to the technical report [Amt92b].
Reference: [Amt92b] <author> Torben Amtoft. </author> <title> Unfold/fold transformations preserving termination properties. </title> <type> Technical Report PB-410, DAIMI, </type> <institution> University of Aarhus, Denmark, </institution> <year> 1992. </year> <pages> 58 pages. </pages>
Reference-contexts: The basic ideas in this chapter have been presented at PLILP'92 [Amt92a]; the chapter itself is (apart from the introductory parts) almost identical to the technical report <ref> [Amt92b] </ref>. Section 8.1 attempts to give the main intuition behind the approach and section 8.2 compares with related work; the rest of the chapter is highly technical and perhaps ought to be an appendix instead. * Chapter 9 contains the concluding remarks.
Reference: [ANTJ89] <author> Torben Amtoft, Thomas Nikolajsen, Jesper Larsson Traff, and Neil D. Jones. </author> <title> Experiments with implementations of two theoretical constructions. </title> <booktitle> In Logic at Botik, </booktitle> <publisher> USSR (Springer LNCS no 363), </publisher> <pages> pages 119-133, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: jo j = n, such that T (ff exp ; o ) 2 n . 2 (We shall now exhibit such an automaton; note that this automaton is the "natural" way to encode an "interesting" general problem and thus cannot be considered "contrived": : : ) Proof: Much as in <ref> [ANTJ89] </ref> we come up with a parametrized construction: for each DFA (with transition function ffi) using the binary alphabet f0; 1g, a 2DPDA is constructed which given input tape of form y L a : : : a y R decides whether there exists a string of n binary digits which <p> Even though Cook's insight gives a linear time algorithm for many problems where such an algorithm is not in any way obvious, the constant factor may be prohibitively large. <ref> [ANTJ89] </ref> aims at decreasing this constant factor by partially evaluating a modified version of the top-down simulator from [Jon77] wrt. known automata. 158 Chapter 7 Deriving efficient substring matchers by partial evaluation In this chapter we show that the well-known Knuth-Morris-Pratt (KMP) and Boyer-Moore (BM) algorithms for substring matching can be
Reference: [AT89] <author> Torben Amtoft and Jesper Larsson Traff. </author> <title> Memoization and its use in lazy and incremental program generation. </title> <type> Master's thesis, </type> <institution> DIKU, University of Copenhagen, Denmark, </institution> <month> August </month> <year> 1989. </year> <note> No 89-8-1. </note>
Reference-contexts: This chapter attempts to unify and elucidate ideas from various places in the literature, including <ref> [AT89] </ref> (Jesper Traff's and mine Master's Thesis). * Chapter 4 may be considered the main chapter of the thesis, and can be summarized as follows (the chapter itself contains a more detailed overview): 1. <p> The PEM can be considered a top-down implementation of partial evaluation. The material presented in this chapter may be viewed as a generalization of the main idea behind <ref> [AT89] </ref> (Jesper Traff's and mine Master's Thesis). * Chapter 6 presents a "realistic" program which by means of a suitable instance of the USM can be made to run exponentially faster. The program to be considered is a simulator for two-way deterministic pushdown automata (2DPDA) [AHU74, chap. 9]. <p> We shall see that the effect of this clever simulation can be acquired using the general concept of ultimate sharing. This chapter is based on joint work with Jesper Traff which has been reported in TCS [AT92] (but the basic idea dates back to <ref> [AT89] </ref>). <p> In particular, a top-down version of partial evaluation can be obtained, as will be described in section 5.2. Just as (bottom-up) PE often is used for compilation purposes (by doing PE on interpreters) [JSS89], in <ref> [AT89] </ref> the idea above was used for implementing "lazy and incremental" compilation (by evaluating an interpreter using a strategy based on ultimate sharing). 35 On the other hand, it should also be clear that for most applications ultimate sharing will be a far too heavy tool (unless in a very restricted <p> Concepts resembling ultimate sharing seem to have come up rather independently in various disguises several places in the recent years: * in <ref> [AT89] </ref> it is denoted partial memoization and is worked out for an eager, first-order combinator language a system was implemented (guided by instructions from the user) and proved correct. * in [HG91] the concept (which is there christened complete laziness) is elaborated in a higher-order setting; one of the main purposes <p> Due to this trick, the system implemented in <ref> [AT89] </ref> is not unreasonably inefficient. <p> On the other hand, if the example from section 5.2.1 is run with this strategy a lot of superfluous rules will be stored, as a rule will be generated for each subexpression of the program being interpreted (this, on the other hand, may be useful for "incremental compilation" <ref> [AT89] </ref>). 143 A system implementing something rather similar to the PEM (with the above extension, but without ultimate sharing in its full generality) is described in [AT89]. The correctness proof of the system is conducted by means of a well-founded ordering, namely the lexicographical ordering of @ 3 . <p> a rule will be generated for each subexpression of the program being interpreted (this, on the other hand, may be useful for "incremental compilation" <ref> [AT89] </ref>). 143 A system implementing something rather similar to the PEM (with the above extension, but without ultimate sharing in its full generality) is described in [AT89]. The correctness proof of the system is conducted by means of a well-founded ordering, namely the lexicographical ordering of @ 3 . <p> We shall see that the effect of this clever simulation can be acquired using the general concept of ultimate sharing. This chapter is based on joint work with Jesper Traff which has been reported in [AT92] (but the basic idea dates back to <ref> [AT89] </ref>). The exposition here is substantially different, as we can build upon the general theory developed in the previous chapters. We proceed as follows: first we define a 2DPDA, write a (naive) simulator and give an example of an automaton which runs in exponential time.
Reference: [AT92] <author> Torben Amtoft and Jesper Larsson Traff. </author> <title> Partial memoiza-tion for obtaining linear time behavior of a 2DPDA. </title> <journal> Theoretical Computer Science, </journal> <volume> 98(2) </volume> <pages> 347-356, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: We shall see that the effect of this clever simulation can be acquired using the general concept of ultimate sharing. This chapter is based on joint work with Jesper Traff which has been reported in TCS <ref> [AT92] </ref> (but the basic idea dates back to [AT89]). <p> We shall see that the effect of this clever simulation can be acquired using the general concept of ultimate sharing. This chapter is based on joint work with Jesper Traff which has been reported in <ref> [AT92] </ref> (but the basic idea dates back to [AT89]). The exposition here is substantially different, as we can build upon the general theory developed in the previous chapters. <p> a constant c 0 (= c jff exp j) such that for all tapes o we have CC (ff exp ; o ) c 0 jo j By comparing with fact 6.3.1 we conclude that the machine claimed to exist in theorem 6.3.2 can have no bound on I. 2 <ref> [AT92, p. 355] </ref> makes the unjustified claim that only a constant speedup is possible in such cases. 151 6.4 A USM implementing Cook's construc- tion In this section we will prove theorem 6.3.2 by exhibiting a machine, to be called a 2DM, with the desired properties.
Reference: [Bar84] <author> H.P. Barendregt. </author> <title> The Lambda Calculus, its Syntax and Semantics. </title> <publisher> North-Holland, </publisher> <year> 1984. </year>
Reference-contexts: In section 5.2 we shall present a top-down version of PE and prove it to preserve termination properties (i.e. terminate unless the source program itself loops). 27 Chapter 3 Various Degrees of Sharing When evaluating an expression in a functional language (e.g. the - calculus <ref> [Bar84] </ref>), some subcomputations may have to be done several times, this being a cause of inefficiency. Consequently, several techniques have been devised to increase the amount of sharing. Below, we are going to list some of these techniques in order of increasing sophistication.
Reference: [BCD90] <author> A. Bossi, N. Cocco, and S. Dulli. </author> <title> A method for specializing logic programs. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12(2) </volume> <pages> 253-302, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: evaluated wrt. its first argument being 2, the following level 1 rules (specialized versions of ack) will typically be generated: ack (2,0) !ack (1,1) ack (1,0) !ack (0,1) 4 The concept of PE may be extended a bit, as e.g. in [Tak91] where "context" is taken into account, and in <ref> [BCD90] </ref> where a function can be specialized wrt. an argument satisfying some predicate. 25 If we introduce ack2, ack1 and ack0 such that ack2 (n) is an abbreviation (cf. the discussion in section 2.1.2) of ack (2,n) etc, the level 1 rules will take the form ack2 (0) !ack1 (1) ack1
Reference: [BCE92] <author> Annalisa Bossi, Nicoletta Cocco, and Sandro Etalle. </author> <title> On safe folding. </title> <editor> In M. Bruynooghe and M. Wirsing, editors, </editor> <booktitle> 4th International Symposium on Programming Language Implementation and Logic Programming (PLILP 92), </booktitle> <address> Leuven, Belgium, </address> <pages> pages 172-186. </pages> <publisher> Springer Verlag, LNCS no 631, </publisher> <month> August </month> <year> 1992. </year>
Reference-contexts: Giving conditions for total correctness of unfold/fold transformations is a rather hot topic in the logic programming community, see e.g. [TS84], [KK90], [Sek91], [GS91], [PP91a], <ref> [BCE92] </ref>. This is unlike the situation in the functional community where Kott's work (cf. section 4.9) seems rather isolated. <p> The crucial condition on folding is that the leftmost atom has been unfolded. Again by assigning the predicates folded against weight 0 and the others 1, the essence of this translates into our condition 8.1.2. A version of condition 8.1.2 is also stated in [Amt91]. <ref> [BCE92] </ref> gives sufficient conditions for replacement (folding being a special case) to be safe. <p> exploiting that we have the "equivalence" m (s (X)) j n (X) (8.6) we can replace n (X) by m (s (X)), yielding the new clause c 00 It is easily seen that transformation 1 preserves termination properties, while transformation 2 introduces an infinite loop. * In the framework of <ref> [BCE92] </ref> this behavior is explained as follows: the "dependency degree" of m wrt. c 3 is 1, as m in one step unfolds to something matching n (s (X)). <p> The equivalence (8.5) represents a "semantic delay" of 1, as m (X) in one step unfolds to n (X); whereas the equivalence (8.6) represents a semantic delay of 2, as it takes two steps for m (s (X)) to unfold to n (X). Now <ref> [BCE92, theorem 13] </ref> states that a sufficient condition for safeness is that the dependency degree is greater than or equal to the semantic delay. * In our framework, the behavior is explained as follows: by e.g. assigning 6 m weight 1 and n weight 2 condition 8.1.1 tells us that it <p> On the other hand, the equivalence (8.6) represents two unfolding steps (first m is unfolded and then n), and now it is not possible to assign weights in a way such that condition 8.1.1 is satisfied. Actually, example 5 in <ref> [BCE92] </ref> shows that often their condition amounts to the condition given in [Sek91] (and thus to our condition 8.1.1). 8.3 Fundamental concepts We now embark on exhibiting the theory, an outline of which was presented in section 8.1. The material is highly technical, but contains no "deep" or surprising results.
Reference: [BD77] <author> R.M. Burstall and John Darlington. </author> <title> A transformation system for developing recursive programs. </title> <journal> Journal of the ACM, </journal> <volume> 24(1) </volume> <pages> 44-67, </pages> <month> January </month> <year> 1977. </year>
Reference-contexts: An important conceptual difference between the perspective on transformations presented above and the perspective prevalent in the literature (e.g. <ref> [BD77] </ref>) is as follows: * in the standard framework, the source program gradually by a sequence of (hopefully) meaning preserving steps is transformed into the target program; * in our framework the behavior of the source program is "observed", and by means of the information thus gained a target program is <p> This suggests that it might be useful to memoize on "smaller units of computation" as will be done in section 3.1. 2.1.2 Unfold/fold transformations The unfold/fold framework for program transformation dates back to <ref> [BD77] </ref> and has since been the subject of much interest, primarily aimed at making the process of finding "eureka"-definitions more systematic, e.g. [NN90], [PP90], [PP91b], [PP92]. Also supercompilation [Tur86] can be seen as a variant over the concept. <p> Similarly, an exponential speedup may be achieved if one during transformation is allowed to identify some expressions (by means of where-abstractions) 115 which one is not permitted to identify during execution. A classical ex-ample of this is how one transforms the fibonacci function from being exponential into being linear <ref> [BD77] </ref>: First we introduce the function g (x) = (fib (x),fib (x+1)) this is the tupling strategy, cf. [Pet84]. <p> In this section we shall see an example of this: exploiting that the operation appending two lists is associative, a list reversing program is transformed from being quadratic into being linear (as in <ref> [BD77] </ref>). The source program is depicted below, where ++ (appending two lists) for notational convenience is an infix operator. rev ([ ]) = [ ] [ ]++ y = y We shall assume that all lists in question are finite. <p> We attempt the order of exposition to be chronological, but do not claim the list to be exhaustive. The seminal paper on unfold/fold transformations <ref> [BD77] </ref> addresses both the abovementioned issues. <p> Interestingly enough, Appendix 1 in <ref> [BD77] </ref> presupposes a call-by-value strategy: a program is given which (by such a strategy) is rather inefficient, as large data structures are built but only small parts are used. <p> It seems very hard to come up with such conditions using the framework of denotational semantics, cf. my claims p. 12. Concerning speed-up issues, <ref> [BD77, p. 48] </ref> informally reasons as follows: 1. unfolding leaves efficiency unchanged; 2. application of laws, as well as introduction of where-abstractions, are potential sources of efficiency improvements. 14 Essentially giving rise to the same flow of control as lazy evaluation. 15 As pointed out by David Sands this in general <p> introduction of looping surely does not preserve efficiency), provided "one folds with an argument which is lower in some well-founded ordering than the argument in the equation being transformed" 16 . 1 does not hold in our model as we measure complexity in terms of the number of unfoldings (while <ref> [BD77] </ref> (p. 65) measures complexity in terms of the number of arithmetic operations). 2 is just the core insight of section 4.8.3 and section 4.8.2.
Reference: [BD91] <author> Anders Bondorf and Olivier Danvy. </author> <title> Automatic autoprojec-tion of recursive equations with global variables and abstract data types. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 16(2) </volume> <pages> 151-195, </pages> <year> 1991. </year>
Reference-contexts: These problems have been attacked in several ways, e.g.: * In [Ses88] (1) is avoided by testing for cycles in the call graph (this technique being potentially very space consuming), but (2) remains a possibility. * In SIMILIX <ref> [BD91] </ref> one decreases the risk of (1) by not unfolding dynamic tests, i.e. tests whose outcome cannot be decided by the 26 static arguments alone. <p> It is worth mentioning already now that this source of speedup also occurs (and is perhaps much more common) within the world of logic programming. See section 8.0.1 for a further discussion and examples. The partial evaluator SIMILIX <ref> [BD91] </ref> treats a strict language (SCHEME), and it is a design principle that the target program must exhibit exactly the same observable runtime behavior (i.e. same side-effects, same result) as the source program. <p> This closely corresponds to the unfolding strategy used by most modern partial evaluators (eg. SIMILIX <ref> [BD91] </ref>), where the specialization algorithm unfolds until meeting a test which cannot be decided such "dynamic tests" will then be the specialization points.
Reference: [BDSK89] <author> Maurice Bruynooghe, Danny De Schreye, and Bruno Krekels. </author> <title> Compiling control. </title> <journal> Journal of Logic Programming, </journal> <volume> 6 </volume> <pages> 135-162, </pages> <year> 1989. </year>
Reference-contexts: The technique of letting the transformation "simulate" an (optimal) evaluation strategy is presented in <ref> [BDSK89] </ref> (and further elaborated in e.g. [DSMSB90]) where it is called compiling control. <p> Now, by perform-ing some steps which are not LR steps we arrive at the following program (isomorphic to the one given <ref> [BDSK89, p. 140] </ref>) where permord (A,X,Y) is "an abbreviation" of perm (X,Y),ord ([AjY]): sort ([ ],[ ]) 2 sort ([AjX],[BjY]) del (B,[AjX],Z), permord (B,Z,Y) permord (A,[ ],[ ]) 2 permord (A,[BjX],[CjY]) del (C,[BjX],Z),A &lt; C,permord (C,Z,Y) Even though efficiency has been improved, complexity is still exponential. <p> Note that exploiting laws is not a subcase of compiling control, cf. the observation <ref> [BDSK89, p. 136] </ref>: "no computation rule provides lemma generation". 181 8.0.2 Ultimate sharing in the logic world In the functional world it is always "safe" to look at a more general expression and unfold it "as far as possible" (theorem 5.1.7 provides the formal justification for this claim).
Reference: [Bel91] <author> Francoise Bellegarde. </author> <title> Program transformation and rewriting. </title> <booktitle> In 4th International Conference on Rewriting Techniques and Applications, </booktitle> <pages> pages 226-239. </pages> <booktitle> Lecture Notes in Computer Science 488, </booktitle> <year> 1991. </year>
Reference-contexts: It is proved that "call-by-need" derivations are optimal - analogous to 18 our theorem 4.4.14. In general it can only be known "after 17 The relationship between these two frameworks in general seems rather overlooked other attempts to narrow the gap include <ref> [Bel91] </ref>.
Reference: [Bir80] <author> Richard S. Bird. </author> <title> Tabulation techniques for recursive programs. </title> <journal> ACM Computing Surveys, </journal> <volume> 12(4) </volume> <pages> 403-417, </pages> <month> December </month> <year> 1980. </year>
Reference-contexts: Therefore, [Hug85] suggests to use "equality of pointers" instead (of course, then less computation will be shared). Memoization is a top-down method its bottom-up counterpart is often termed tabulation and is treated e.g. in <ref> [Bir80] </ref> and [Coh83] 1 . 1 In [Bir80] the top-down method is termed exact tabulation (as only the rules needed are generated), whereas the bottom-up method may give rise to overtabulation. <p> Therefore, [Hug85] suggests to use "equality of pointers" instead (of course, then less computation will be shared). Memoization is a top-down method its bottom-up counterpart is often termed tabulation and is treated e.g. in <ref> [Bir80] </ref> and [Coh83] 1 . 1 In [Bir80] the top-down method is termed exact tabulation (as only the rules needed are generated), whereas the bottom-up method may give rise to overtabulation.
Reference: [BKKS87] <author> H.P. Barendregt, J.R. Kennaway, J.W. Klop, </author> <title> and M.R. Sleep. Needed reduction and spine strategies for the lambda calculus. </title> <journal> Information and Computation, </journal> <volume> 75 </volume> <pages> 191-231, </pages> <year> 1987. </year> <month> 247 </month>
Reference-contexts: Notice that it is always safe to reduce a spine redex of e, in the sense that if e reduces to a head normal form then this reduction involves the reduction of all spine redices of e <ref> [BKKS87, theorem 4.9] </ref>. A reduction which reduces the topmost spine redex is termed a normal order reduction. Lazy evaluation One can work with "DAG"s instead of trees, i.e. allow a subexpression to be shared among several expressions. <p> Also, <ref> [BKKS87] </ref> investigates the issue of detecting redices in a -expression the reduction of which is needed in order to arrive at (head) normal form. 127 3. lookup rules, which model the retrieval of a previously stored result; 4. folding rules, which increase sharing by identifying nodes "with identical children".
Reference: [BM77] <author> Robert S. Boyer and J. Strother Moore. </author> <title> A fast string search-ing algorithm. </title> <journal> Communications of the ACM, </journal> 20(10) 762-772, October 1977. 
Reference-contexts: S 5 or S 4 ; but S 3 can be entered. * If the next symbol is neither a nor b, none of the S 1 : : : S 6 fit - hence S 0 is entered. 161 7.3 The BM methods Originally devised by Boyer and Moore <ref> [BM77] </ref>, numerous variants have emerged some of which are described in e.g. [KMP77] and [PS88]. The basic idea is to do the matching backwards wrt. p: with p = tree (which will be our working example), the initial configuration will be ...x......... <p> TREE If x is an e, we move the pointer one to the left and attempt to match the first e of tree, etc. The various algorithms differ in the way they treat mismatches; the original approach <ref> [BM77] </ref> is to maintain two tables, ffi 1 and ffi 2 . ffi 1 has an entry for each symbol in the alphabet (the size of which will be denoted ) which contains the rightmost occurrence (if any) of the symbol in p. ffi 2 has an entry for each position <p> We shall consider the following 4 variants of the BM method, ordered after increasing degree of sophistication (the naming used is in no way conventional): 1. The naive Boyer-Moore, to be called BMna. This is a simplification of the one given in <ref> [BM77] </ref>, exploiting ffi 1 only. 2. The original Boyer-Moore, to be called BMor. This is the one presented in [BM77], i.e. ffi 1 as well as ffi 2 is exploited. 3. The standard Boyer-Moore, to be called BMst. <p> The naive Boyer-Moore, to be called BMna. This is a simplification of the one given in <ref> [BM77] </ref>, exploiting ffi 1 only. 2. The original Boyer-Moore, to be called BMor. This is the one presented in [BM77], i.e. ffi 1 as well as ffi 2 is exploited. 3. The standard Boyer-Moore, to be called BMst. <p> The original Boyer-Moore, to be called BMor. This is the one presented in [BM77], i.e. ffi 1 as well as ffi 2 is exploited. 3. The standard Boyer-Moore, to be called BMst. This is the algo rithm hinted at <ref> [BM77, p. 771, below] </ref> and the one derived in [PS88]; it works by "combining" ffi 1 and ffi 2 into a single two-dimensional table. 4. The optimal Boyer-Moore, to be called BMop. <p> BMop does better than that, since it remembers that the fourth symbol of the subject string is an r hence it is possible to shift the pattern string four to the right. <ref> [BM77, p. 764] </ref> illustrates the behavior of BMor by means of an example, the pattern string being at-that and the subject string s being WHICH-FINALLY-HALTS.--AT-THAT-POINT BMor on that example makes 14 references to s. <p> The linearity is solely <ref> [BM77, p. 767] </ref> due to the use 4 of ffi 2 . If only ffi 1 is used (i.e. BMna is considered), fi (jpjjsj) references to s may be made (if e.g. p is of form abbbbbb: : : and a large initial segment of s contains b's only). <p> BMna is considered), fi (jpjjsj) references to s may be made (if e.g. p is of form abbbbbb: : : and a large initial segment of s contains b's only). However, BMna will suffice for most practical purposes, especially if the alphabet is large. * In <ref> [BM77] </ref> a theoretical analysis of BMor is given, and the theoretical performance is compared with the actual performance. For large alphabets the theoretical analysis is completely accurate, but for the binary alphabet actual performance is significantly worse than the one predicted by the theoretical model. <p> For large alphabets the theoretical analysis is completely accurate, but for the binary alphabet actual performance is significantly worse than the one predicted by the theoretical model. As explained by the authors <ref> [BM77, p. 770] </ref>, this is because the model does not account for the fact that some matches are guaranteed to occur. For instance, when we from the configuration ......BAA... <p> the pattern string three to the right the possibility that a mismatch occurs in position 3,4 or 5 is zero as aaabaaaaa has been aligned in such a way that its substring baa matches the subject string whereas it in the simplified theoretical model is 4 As pointed out in <ref> [BM77, p. 771] </ref> it is necessary in order to get linearity that ffi 2 also records the preceding symbol, cf. footnote 3. 165 non-zero.
Reference: [BMT92] <author> Dave Berry, Robin Milner, and David N. Turner. </author> <title> A semantics for ML concurrency primitives. </title> <booktitle> In ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 119-129, </pages> <year> 1992. </year>
Reference-contexts: in the latter 1 ; * operational semantics better capture the essence of unfolding and folding: unfolding corresponds to a transition being made in the "right" direction, folding corresponds to a transition being made 1 Recently, for instance the attempts to integrate functional programming and concurrency (see e.g. [LG88], [Nie89], <ref> [BMT92] </ref>) have renewed the interest in operational semantics for func tional languages. 178 in the "wrong" direction. By using a denotational approach, this cannot be expressed directly (cf. the claims p. 12 and p. 123).
Reference: [BvEG + 87] <author> H.P. Barendregt, M.C.J.D. van Eekelen, J.R.W. Glauert, J.R. Kennaway, M.J. Plasmeijer, </author> <title> and M.R. Sleep. Term graph rewriting. </title> <booktitle> In PARLE, </booktitle> <address> Eindhoven, The Netherlands. </address> <publisher> LNCS 259, </publisher> <pages> pages 141-158, </pages> <year> 1987. </year>
Reference-contexts: In particular, we shall often "reason on term level". A very natural question arises: is this "sound" and "complete", i.e. is "graph rewriting" and "term rewriting" able to "simulate" each other? A similar problem is addressed in <ref> [BvEG + 87] </ref>, where it is shown that under certain weak assumptions graph rewriting is a sound and complete implementation of term rewriting.
Reference: [CD89] <author> Charles Consel and Olivier Danvy. </author> <title> Partial evalution of pattern matching in strings. </title> <journal> Information Processing Letters, </journal> <volume> 30 </volume> <pages> 79-86, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: In <ref> [CD89] </ref> (this paper being the starting point of the work reported in this chapter) the KMP algorithm is derived by PE, after a naive substring matcher has been transformed into one more suitable for PE purposes.
Reference: [Cho90] <author> Christian Choffrut. </author> <title> An optimal algorithm for building the Boyer-Moore automaton. </title> <journal> EATCS Bulletin no. </journal> <volume> 40, </volume> <pages> pages 217-225, </pages> <year> 1990. </year>
Reference-contexts: On the other hand, as the alphabet grows smaller the table of BMop may grow really large, even though it is doubtful whether 2 jpj really is the tightest upper bound (for a further discussion of this issue, see <ref> [Cho90] </ref>).
Reference: [Coh83] <author> Norman H. Cohen. </author> <title> Eliminating redundant recursive calls. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(3) </volume> <pages> 265-299, </pages> <month> July </month> <year> 1983. </year>
Reference-contexts: Therefore, [Hug85] suggests to use "equality of pointers" instead (of course, then less computation will be shared). Memoization is a top-down method its bottom-up counterpart is often termed tabulation and is treated e.g. in [Bir80] and <ref> [Coh83] </ref> 1 . 1 In [Bir80] the top-down method is termed exact tabulation (as only the rules needed are generated), whereas the bottom-up method may give rise to overtabulation. In [Coh83] the top 21 Example 2.1.1 Consider the fibonacci function fib (0) !1 fib (n) !fib (n-1) + fib (n-2) for <p> Memoization is a top-down method its bottom-up counterpart is often termed tabulation and is treated e.g. in [Bir80] and <ref> [Coh83] </ref> 1 . 1 In [Bir80] the top-down method is termed exact tabulation (as only the rules needed are generated), whereas the bottom-up method may give rise to overtabulation. In [Coh83] the top 21 Example 2.1.1 Consider the fibonacci function fib (0) !1 fib (n) !fib (n-1) + fib (n-2) for n 2. a suitable representation of which will constitute the rules in R 0 .
Reference: [Coo71] <author> Stephen A. Cook. </author> <title> Linear time simulation of deterministic two-way pushdown automata. </title> <booktitle> In Information Processing 71. Proceedings of IFIP Congress 1971, </booktitle> <pages> pages 75-80. </pages> <publisher> North-Holland, </publisher> <year> 1971. </year>
Reference-contexts: The program to be considered is a simulator for two-way deterministic pushdown automata (2DPDA) [AHU74, chap. 9]. It caused much surprise when <ref> [Coo71] </ref> showed that it is always possible to simulate a 2DPDA in linear time (wrt. the length of the input tape), even if the automaton carries out an exponential number of steps in particular this result gave Donald Knuth inspiration to his fast substring matching algorithm [KMP77, p. 338]. <p> Another way to detect loops would be to keep track of whether the 2DM twice performs a PUSH step pushing the same expression. 6.5 Previous work The original technique for linear time simulation of a 2DPDA, used in <ref> [Coo71] </ref> and restated as [AHU74, algorithm 9.4], is a bottom-up approach (cf. page 20).
Reference: [DHK + 89] <author> John Darlington, Peter Harrison, Hessam Khoshnevisan, Lee McLoughlin, Nigel Perry, Helen Pull, Mike Reeve, Keith Sephton, Lyndon While, and Sue Wright. </author> <title> A functional programming environment supporting execution, partial execution and transformation. </title> <booktitle> In PARLE '89 (LNCS 365), </booktitle> <pages> pages 286-305, </pages> <year> 1989. </year>
Reference-contexts: And even though I e.g. have played a bit with the Flagship system <ref> [DHK + 89] </ref>, still (as admitted at the PLILP'92 conference!) I do not have any clear intuition about to which extent e.g. the unfold/fold methodology is useful in practical applications.
Reference: [DP88] <author> John Darlington and Helen Pull. </author> <title> A program development methodology based on a unified approach to execution and transformation. </title> <editor> In D. Bjorner, A.P. Ershov, and N.D. Jones, editors, </editor> <booktitle> Partial Evaluation and Mixed Computation, </booktitle> <pages> pages 117-131. </pages> <publisher> North-Holland, </publisher> <year> 1988. </year> <month> 248 </month>
Reference-contexts: Notice that a key point in the above model sketch is that "standard" evaluation is viewed as a special case of "symbolic" evaluation (as done in <ref> [DP88] </ref>, and as done in the PROLOG world).
Reference: [DSMSB90] <author> Danny De Schreye, Bern Martens, Gunther Sablon, and Maurice Bruynooghe. </author> <title> Compiling bottom-up and mixed derivations into top-down executable logic programs. </title> <booktitle> In Second Workshop on Meta-Programming in Logic, </booktitle> <month> April 4-6, </month> <year> 1990, </year> <title> Leuven, </title> <editor> Belgium. Ed.: M. </editor> <booktitle> Bruynooghe, </booktitle> <pages> pages 37-56, </pages> <year> 1990. </year>
Reference-contexts: The technique of letting the transformation "simulate" an (optimal) evaluation strategy is presented in [BDSK89] (and further elaborated in e.g. <ref> [DSMSB90] </ref>) where it is called compiling control. The scope of this technique is quite wide, an important special case being when the source program has been designed by means of the "generate and test" paradigm (first all candidate solutions are generated; then it is tested whether they really are solutions).
Reference: [Dyb85] <author> Hans Dybkjaer. </author> <title> Parsers and partial evaluation: An experiment. Student Project 85-7-15, </title> <institution> DIKU, University of Copen-hagen, Denmark, </institution> <month> July </month> <year> 1985. </year> <pages> 128 pages. </pages>
Reference-contexts: but different values of iseq one has to start "from scratch" afterwards (motivating why the same element may occur more than once). * A historical remark: by investigating the natural strategy BMop I discovered BMop before learning that Knuth had been 14 years ahead: : : 7.6 Related work In <ref> [Dyb85] </ref> it is investigated how to specialize Earley's general context free parser wrt. a fixed grammar; the conclusion is that a lot of handwork is necessary.
Reference: [ENRR87] <editor> H. Ehrig, M. Nagl, G. Rozenberg, and A. Rosenfeld, editors. </editor> <booktitle> Graph-Grammars and Their Application to Computer Science. Lecture Notes in Computer Science 291, </booktitle> <year> 1987. </year>
Reference-contexts: This will be modeled by a pushout (cf. definition 4.1.19), this being the standard way of expressing such "reduction within a context" (see e.g. <ref> [ENRR87] </ref>). Section 4.1 is rather long and technical, especially the proof that the pushout actually exists (section 4.1.5).
Reference: [Ers78] <author> A.P. Ershov. </author> <title> On the essence of compilation. </title> <editor> In E.J. Neuhold, editor, </editor> <booktitle> Formal Description of Programming Concepts, </booktitle> <pages> pages 391-420. </pages> <publisher> North-Holland, </publisher> <year> 1978. </year>
Reference-contexts: That would certainly be an interesting area for further research. Related approaches/remarks from the literature include * a noteworthy part of the discussion following a talk by A.P. Ershov, reported in <ref> [Ers78, p. 420] </ref>: 3 Karel Culik: In your program that computes X to the power n you assumed an input value for n equal to 5.
Reference: [FS91] <author> Gudmund S. Frandsen and Carl Sturtivant. </author> <title> What is an efficient implementation of the lambda-calculus? In John Hughes, editor, </title> <booktitle> International Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 289-312. </pages> <publisher> Springer Verlag, LNCS no 523, </publisher> <month> August </month> <year> 1991. </year>
Reference-contexts: It is well-known that lazy evaluation is suboptimal wrt. the ability for reusing (sharing) computations; it is less known that 14 also "fully lazy" evaluation is suboptimal it may even be expo-nentially bad, as shown in <ref> [FS91] </ref>. <p> Laziness is a delicate property of a function, and seemingly innocuous program transformations may lose laziness. Moreover, as pointed out in <ref> [FS91] </ref> we have Fully lazy evaluation is not fully lazy and may in fact be exponentially bad. To see this, consider as in [FS91] the family of -expressions given recursively by A 0 = x:I B n = A n I for all n where I = x:x. <p> Laziness is a delicate property of a function, and seemingly innocuous program transformations may lose laziness. Moreover, as pointed out in <ref> [FS91] </ref> we have Fully lazy evaluation is not fully lazy and may in fact be exponentially bad. To see this, consider as in [FS91] the family of -expressions given recursively by A 0 = x:I B n = A n I for all n where I = x:x. <p> Notice that in the above reduction sequence, only spine redices are reduced. By reducing something which is not the topmost spine redex (but nevertheless a spine redex), we thus gain an exponential speed-up. Now let us translate the above example to supercombinators (still following <ref> [FS91] </ref>): we arrive at L h w = w h (w w) M 0 h = I hence B n is translated to M n I.
Reference: [GAL92] <author> Georges Gonthier, Mart in Abadi, and Jean-Jacques Levy. </author> <title> The geometry of optimal lambda reduction. </title> <booktitle> In ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 15-26, </pages> <year> 1992. </year>
Reference-contexts: The properties of this strategy would surely be worth a closer study; Grue (personal communication) has not investigated the issue further. * in [Lam90] (which <ref> [GAL92] </ref> attempts to elaborate on) an algorithm for graph reduction of -expressions is given which attempts to 36 avoid duplication of computation by keeping track of how redices propagate on the other hand, "accidental sharing" (that is redices which are identical but not copies of the same original redex) is not
Reference: [GLT89] <author> Jean-Yves Girard, Yves Lafont, and Paul Taylor. </author> <title> Proofs and Types. </title> <publisher> Cambridge University Press, </publisher> <year> 1989. </year>
Reference-contexts: The main idea to use multilevel transition systems is as follows: * The original program (the source program) is represented as rules 1 Cf. the remark in <ref> [GLT89, p. 54] </ref>: "The fundamental idea of denotational semantics is to interpret reduction (a dynamic notion) by equality (a static notion)". 12 at level 0. * To execute the source program is modeled by (a sequence of ) transitions at level 1, where one at level 1 "can access" rules at
Reference: [Gre87] <author> Steve Gregory. </author> <title> Parallel Logic Programming in PARLOG - the language and its implementation. </title> <publisher> Addison-Wesley, </publisher> <year> 1987. </year>
Reference-contexts: There are two reasons for working with multisets and not with sequences (i.e. not to order the branches), a pragmatic and a mathematical one: 195 * it is rather easy to implement or-parallelism <ref> [Gre87] </ref>, as no commu-nication has to occur between the branches. On the other hand, and-parallelism [Gre87] is much harder to implement due to the need for sharing of data, hence most implementations employ the LR strategy. * If we use sequences, the Church-Rosser property will be lost. <p> There are two reasons for working with multisets and not with sequences (i.e. not to order the branches), a pragmatic and a mathematical one: 195 * it is rather easy to implement or-parallelism <ref> [Gre87] </ref>, as no commu-nication has to occur between the branches. On the other hand, and-parallelism [Gre87] is much harder to implement due to the need for sharing of data, hence most implementations employ the LR strategy. * If we use sequences, the Church-Rosser property will be lost.
Reference: [Gru] <author> Klaus Grue. Call-by-mix: </author> <title> A reduction strategy for pure lambda-calculus. </title> <institution> Circulated at DIKU, University of Copen-hagen, Denmark. </institution>
Reference-contexts: Further, it is sketched how to implement complete laziness via the underlying evaluation mechanism in a lazy language (thus, in effect all subexpressions are memoized upon). * in <ref> [Gru] </ref> a reduction strategy for the pure -calculus, termed call-by-mix, is proposed: the redex selected for reduction is the bottommost spine redex; and when doing this reduction some other tricks are employed in order to keep subexpressions shared as long as possible.
Reference: [Gru87] <author> Klaus Grue. </author> <title> An efficient formal theory. </title> <type> Technical Report 87/14, </type> <institution> DIKU, University of Copenhagen, Denmark, </institution> <year> 1987. </year> <month> 249 </month>
Reference-contexts: set of axioms in a theory T 0 , one can consider the target program as either a set of theorems in T 0 or as a set of axioms in a new theory T 1 , which one can expect to be more efficient than T 0 - cf. <ref> [Gru87] </ref>.
Reference: [GS91] <author> P. A. Gardner and J. C. Shepherdson. </author> <title> Unfold/fold transfor-mations of logic programs. </title> <editor> In J.L. Lassez and G. Plotkin, editors, </editor> <booktitle> Computational Proofs: Essays in honour of Alan Robinson. </booktitle> <year> 1991. </year>
Reference-contexts: Giving conditions for total correctness of unfold/fold transformations is a rather hot topic in the logic programming community, see e.g. [TS84], [KK90], [Sek91], <ref> [GS91] </ref>, [PP91a], [BCE92]. This is unlike the situation in the functional community where Kott's work (cf. section 4.9) seems rather isolated. <p> This is a problem to which some incorrect solutions have been proposed in the literature (and yet proved correct!), for a survey see <ref> [GS91] </ref>. 191 We want to ensure that all mappings to be defined by program clauses are reversible. <p> on unfold/fold transformations in logic languages transformation typically proceeds in a "step by step fashion"; after a goal in the body of a clause has been unfolded the clause is deleted from the program and replaced by the clause resulting from the unfolding this is the approach taken in e.g. <ref> [GS91] </ref>, [KK90], [PP91a], [Sek91], [TS84]. As 197 ? 6 6 abc,1 ab abc,2 abc,2 pointed out in [GS91], one by applying this method loses some power - to see this, consider the clause C = p (f (X)) p (X). <p> goal in the body of a clause has been unfolded the clause is deleted from the program and replaced by the clause resulting from the unfolding this is the approach taken in e.g. <ref> [GS91] </ref>, [KK90], [PP91a], [Sek91], [TS84]. As 197 ? 6 6 abc,1 ab abc,2 abc,2 pointed out in [GS91], one by applying this method loses some power - to see this, consider the clause C = p (f (X)) p (X). <p> As now not only the success set but also the failure set is preserved, negation can be handled as well. <ref> [GS91] </ref> allows folding against existing clauses (recall clauses are deleted after having been unfolded) only (not allowing a clause to be folded against itself). This greatly limits the applications, since it seems impossible to arrive at recursive definitions of eureka-predicates.
Reference: [Han92] <author> Michael Hanus. </author> <title> Improving control of logic programs by using functional logic languages. </title> <editor> In M. Bruynooghe and M. Wirs-ing, editors, </editor> <booktitle> 4th International Symposium on Programming Language Implementation and Logic Programming (PLILP 92), </booktitle> <address> Leuven, Belgium, </address> <pages> pages 1-23. </pages> <publisher> Springer Verlag, LNCS no 631, </publisher> <month> August </month> <year> 1992. </year>
Reference-contexts: actually used to produce the function this enables specializations to be reused even though the (non-used) static arguments differ. 9.2 Integrating the functional and logical model The functional and logic paradigms are not too far apart, as witnessed by several successful attempts at bridging the gap - e.g. [Red85] and <ref> [Han92] </ref>.
Reference: [Har87] <author> David Harel. </author> <title> Algorithmics The Spirit of Computing. </title> <publisher> Addison-Wesley, </publisher> <year> 1987. </year>
Reference-contexts: a very restricted form), the implementation overhead being prohibitively expensive. 3.5 Ultimate sharing and related concepts in the literature The idea of improving the efficiency of an algorithm by reusing previously computed results is, of course, by no means new this is just the philosophy behind dynamic programming (see e.g. <ref> [Har87] </ref>). However, when the algorithm to be improved is one for implementing a functional language we so to speak step up one level, since to make this particular algorithm faster (for some inputs) amounts to making many programs faster. <p> If integer operations (addition, subtraction, multiplication, division and equality test) are assigned unit cost, the model "collapses" in the sense that e.g. complete factorization of a number n can be done in time O (log n) 2 [Knu81, p. 398]. * Consider the well-known Towers of Hanoi problem (see e.g. <ref> [Har87] </ref>), which in a logic language can be solved by the program hanoi (1,A,B,C,[mv (A,B) j L]-L). hanoi (s (N),A,B,C,L-S) :- hanoi (N,A,C,B,L1-S1), hanoi (N,C,B,A,L2-S2), append (L1-S1,[mv (A,B) j L2]-S2,L-S). append (L1-L2,L2-L3,L1-L3). 112 where we use the technique of difference lists (see e.g. [SS86]) to make it possible to append lists
Reference: [Hen87] <author> Martin C. Henson. </author> <title> Elements of Functional Languages. </title> <publisher> Blackwell Scientific Publications, </publisher> <year> 1987. </year>
Reference-contexts: "more defined than" e 0 (for instance we could have e 00 = c (c (x)) and e 0 = c (g (x)) with c being a constructor symbol and g being a function 16 This is often considered the recipe for how to ensure total correctness, as e.g. in <ref> [Hen87, p. 183] </ref>. symbol). The steps are assumed to be "parallel outermost steps" (re-sembling normal order evaluation).
Reference: [HG91] <author> Carsten Kehler Holst and Carsten K. Gomard. </author> <title> Partial evaluation is fuller laziness. In Partial Evaluation and Semantics-Based Program Manipulation, New Haven, </title> <journal> Con-necticut. (Sigplan Notices, </journal> <volume> vol. 26, no. 9), </volume> <year> 1991. </year>
Reference-contexts: In some sense, fully lazy evaluation guarantees that redices present in the source program are not copied <ref> [HG91] </ref>. <p> sharing seem to have come up rather independently in various disguises several places in the recent years: * in [AT89] it is denoted partial memoization and is worked out for an eager, first-order combinator language a system was implemented (guided by instructions from the user) and proved correct. * in <ref> [HG91] </ref> the concept (which is there christened complete laziness) is elaborated in a higher-order setting; one of the main purposes is to argue that partial evaluation for higher order languages is able to achieve strictly more sharing than "full laziness" their treatment being a source of inspiration for this chapter.
Reference: [Hof92] <author> Berthold Hoffmann. </author> <title> Term rewriting with sharing and memoization. </title> <editor> In H. Kirchner and G. Levi, editors, </editor> <booktitle> Algebraic and Logic Programming, Volterra, Italy, </booktitle> <pages> pages 128-142. </pages> <publisher> Springer Verlag, LNCS no 632, </publisher> <month> September </month> <year> 1992. </year>
Reference-contexts: Then it might be possible to reason about when a concrete transformation yields a constant speedup only. <ref> [Hof92] </ref> considers jungle evaluation, a kind of term rewriting systems based on graph grammars. Instead of overwriting a node with the result of "evaluating it", as in our model, a pointer is drawn from the node to the result. <p> In our model, rule 1 corresponds to "use of level 0 rules"; rule 2 and rule 3 are implicitly present in the multilevel framework; and rule 4 correspond to the rule (4.19) proposed in section 4.8.2. <ref> [Hof92, theorem 4.13] </ref> states a correctness result, saying that an evaluation using 2,3 and 4 can be simulated using 1 and 4 this in some sense corresponds to our partial correctness theorem 4.6.3. 4.10 Possible extensions to the model In this section we briefly outline some directions in which our model
Reference: [Hol91] <author> Carsten Kehler Holst. </author> <title> Finiteness analysis. </title> <editor> In John Hughes, editor, </editor> <booktitle> International Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 473-495. </pages> <publisher> Springer Verlag, LNCS no 523, </publisher> <month> August </month> <year> 1991. </year>
Reference-contexts: However, as all specialized versions possibly needed have to be generated in advance including some which turn out to be not needed - (2) may occur and actually also (1). * In <ref> [Hol91] </ref> an analysis for ensuring that (2) cannot happen is given.
Reference: [Hon91] <author> Zhu Hong. </author> <title> How powerful are folding/unfolding transformations. </title> <type> Technical Report CSTR-91-2, </type> <institution> Department of Computer Science, Brunel University, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: For instance, if one to the usual rules for the fibonacci function fib adds the "eureka definition" &lt;fib (n),fib (n+1)&gt; ! g (n) then the computation of "critical pairs" in some sense simulates the usual unfold/fold process. In <ref> [Hon91] </ref> it is shown that unfoldings, foldings and introduction of where-abstractions do not (modulo a constant) change the "inherent complexity" of a function, where the inherent complexity is the number of steps needed to evaluate the function on an ideal parallel machine (i.e. no communication overhead etc.) for instance, the "exponential" <p> Inherent complexity In order to model "inherent complexity" as used in <ref> [Hon91] </ref> (cf. section 4.9), we allow one "step" to perform several reductions in parallel. <p> In order to simulate the results of <ref> [Hon91] </ref>, we (as one cannot use (4.31) during "execution of the source program") have to show that if 1 ` fl r : G ) c N G 0 then also 1 ` fl r : G ) c N G 00 (with G 00 "similar" to G 0 ), where
Reference: [HU79] <author> John E. Hopcroft and Jeffrey D. Ullman. </author> <title> Introduction to Automata Theory, Languages and Computation. </title> <publisher> Addison-Wesley, </publisher> <year> 1979. </year> <month> 250 </month>
Reference-contexts: This class contains <ref> [HU79, p. 261] </ref> exactly those languages which can be generated by means of LR (1) grammars, this class in turn being equal to the class of languages which can be generated by means of LR (k) grammars. 2DPDAs can even recognize some languages which are not context-free, e.g. fa n b <p> According to <ref> [HU79, p. 124] </ref>, it is not known whether there exists a context-free language which cannot be recognized by a 2DPDA. In order to write a simulator for 2DPDAs, we must find a way to code automata and tapes.
Reference: [Hug82] <author> John Hughes. </author> <title> Super combinators anew implementation method for applicative languages. </title> <booktitle> In ACM Symposium on Lisp and Functional Programming, Pittsburgh, </booktitle> <pages> pages 1-10, </pages> <year> 1982. </year>
Reference-contexts: As the expression h 4 is "hidden" in the definition of the supercombinator M, h 4 will be evaluated each time M is applied - i.e. twice. Again, one can do better: one can modify the lambda-lifting algorithm so it abstracts away the maximal free expressions, as done in <ref> [Hug82] </ref>. Then the resulting supercombinator program will be fully lazy in the sense that it has the "same" sharing properties as the original -expression when evaluated by a fully lazy implementation.
Reference: [Hug85] <author> John Hughes. </author> <title> Lazy memo-functions. </title> <booktitle> In International Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 129-146. </pages> <publisher> Springer Verlag, LNCS 201, </publisher> <year> 1985. </year>
Reference-contexts: Therefore, <ref> [Hug85] </ref> suggests to use "equality of pointers" instead (of course, then less computation will be shared).
Reference: [Jon77] <author> Neil D. Jones. </author> <title> A note on linear time simulation of deterministic two-way pushdown automata. </title> <journal> Information Processing Letters, </journal> <volume> 6(4) </volume> <pages> 110-112, </pages> <month> August </month> <year> 1977. </year>
Reference-contexts: When all (existing) terminators have been computed, the "answer" of the simulation can be looked up immediately. The above algorithm typically generates a lot of rules which are never needed for computing the final answer, cf. the discussion page 20. Motivated by this observation <ref> [Jon77] </ref> presents what amounts to a top-down version of the algorithm, which gives rise to essentially the same "flow-of-control" as the 2DM. In [Wat80] it is shown how something similar to [AHU74, algorithm 9.4] can be derived by means of general tabulation techniques. <p> Even though Cook's insight gives a linear time algorithm for many problems where such an algorithm is not in any way obvious, the constant factor may be prohibitively large. [ANTJ89] aims at decreasing this constant factor by partially evaluating a modified version of the top-down simulator from <ref> [Jon77] </ref> wrt. known automata. 158 Chapter 7 Deriving efficient substring matchers by partial evaluation In this chapter we show that the well-known Knuth-Morris-Pratt (KMP) and Boyer-Moore (BM) algorithms for substring matching can be seen as instances of a common substring matching algorithm, parametrized wrt. search strategy.
Reference: [Jon87] <editor> Simon L. Peyton Jones. </editor> <booktitle> The Implementation of Functional Programming Languages. </booktitle> <publisher> Prentice-Hall, </publisher> <year> 1987. </year>
Reference-contexts: Introduction In the recent years a lot of work has been devoted to developing tools for transforming less efficient programs into more efficient programs. These include memoization [Kho90]; unfold/fold transformations [PP91b]; graph-based implementation of lazy evaluation <ref> [Jon87] </ref> and partial evaluation [JSS89]. The efficiency improvement caused by these techniques all are due to the fact that some computations are shared, i.e. they only have to be done once. <p> Hence h 4 will be evaluated twice. However, one can do better: actually there is no need to copy +(h 4), since +(h 4) does not contain y free in <ref> [Jon87, p. 246] </ref> this observation is attributed to [Wad71]. An implementation clever enough to avoid such unnecessary copying is termed fully lazy [Jon87, p. 210]. Usually, one does not implement functional languages by means of fi-reductions of -expressions instead, one transforms into supercom-binators by lambda-lifting ([Jon87]). <p> However, one can do better: actually there is no need to copy +(h 4), since +(h 4) does not contain y free in [Jon87, p. 246] this observation is attributed to [Wad71]. An implementation clever enough to avoid such unnecessary copying is termed fully lazy <ref> [Jon87, p. 210] </ref>. Usually, one does not implement functional languages by means of fi-reductions of -expressions instead, one transforms into supercom-binators by lambda-lifting ([Jon87]). <p> In fact, the discussion in <ref> [Jon87, chap. 23] </ref> ends up with (p. 400) We conclude that it is by no means obvious how lazy a function is, and that we do not at present have any tools for reasoning about this. <p> a language with named combina-tors and pattern matching, implemented by means of graph reductions, instead of the -calculus: * One does not have to worry about free variables getting bound by fi-reduction (this problem being a main reason why the "bottommost spine redex" strategy may be considered unfeasible in practice <ref> [Jon87, p. 200] </ref>). 1 For SKI-combinators, as defined in [Tur79], the trick to obtain the effects of fully lazy evaluation is [Jon87, p. 267] to incorporate the rule S (K p) (K q) ! K (p q). 37 * If one wants to include constants (like if and plus), what we <p> not have to worry about free variables getting bound by fi-reduction (this problem being a main reason why the "bottommost spine redex" strategy may be considered unfeasible in practice [Jon87, p. 200]). 1 For SKI-combinators, as defined in [Tur79], the trick to obtain the effects of fully lazy evaluation is <ref> [Jon87, p. 267] </ref> to incorporate the rule S (K p) (K q) ! K (p q). 37 * If one wants to include constants (like if and plus), what we surely will do for modeling practical applications, the language will anyway get the flavor of a combinator language (there will be <p> place in parallel; to model call-by-value reduction; and to model garbage collection. 4.1 Graphs and graph reductions In the multilevel transition systems to be treated, the configurations will be graphs motivated by the fact that functional languages mostly (as indicated in chapter 3) are implemented by means of graph reduction <ref> [Jon87] </ref>. We will assume a set of symbols S, each s 2 S having a fixed arity Ar (s). The intuition is that if a node is labeled s then this node has Ar (s) children. Also, to each s 2 S we assign a "function arity", written Far (s).
Reference: [Jor90] <author> Jesper Jorgensen. </author> <title> Generating a pattern matching compiler by partial evaluation. </title> <editor> In Simon L. Peyton Jones, Graham Hutton, and Carsten Kehler Holst, editors, </editor> <booktitle> Functional Programming, Glasgow 1990. Workshops in Computing, </booktitle> <pages> pages 177-195. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1990. </year>
Reference-contexts: The idea of maintaining a description of what is known about the input is employed in <ref> [Jor90] </ref>, where an interpreter for a language with pattern matching is rewritten into one suitable for PE (thus a compiler can be generated).
Reference: [JSS89] <author> Neil D. Jones, Peter Sestoft, and Harald Sondergaard. </author> <title> Mix: A self-applicable partial evaluator for experiments in compiler generation. </title> <journal> Lisp and Symbolic Computation, </journal> <volume> 2(1) </volume> <pages> 9-50, </pages> <year> 1989. </year>
Reference-contexts: Introduction In the recent years a lot of work has been devoted to developing tools for transforming less efficient programs into more efficient programs. These include memoization [Kho90]; unfold/fold transformations [PP91b]; graph-based implementation of lazy evaluation [Jon87] and partial evaluation <ref> [JSS89] </ref>. The efficiency improvement caused by these techniques all are due to the fact that some computations are shared, i.e. they only have to be done once. <p> For a comprehensive treatment of central aspects of PE as well as a historical survey, see <ref> [JSS89] </ref>. In contrast to most (general) unfold/fold systems, PE is usually intended to be done automatically. As the bottom-up approach is used this implies the risk of non-termination, and in fact the great majority of existing partial evaluators may loop (even when doing PE on programs which themselves terminate). <p> In particular, a top-down version of partial evaluation can be obtained, as will be described in section 5.2. Just as (bottom-up) PE often is used for compilation purposes (by doing PE on interpreters) <ref> [JSS89] </ref>, in [AT89] the idea above was used for implementing "lazy and incremental" compilation (by evaluating an interpreter using a strategy based on ultimate sharing). 35 On the other hand, it should also be clear that for most applications ultimate sharing will be a far too heavy tool (unless in a <p> Now consider the special (but common! <ref> [JSS89] </ref>) case where an interpreter int for a language L (int taking two arguments: an L program p and some input i to p) is partially evaluated wrt. a particular L program P . <p> The correctness proof works by exploiting that an expression e terminates iff there exists a well-founded ordering OE on expressions derivable from e such that if e in one step rewrites to e 0 then e 0 OE e. In <ref> [JSS89] </ref> one finds the following remarks: Program transformation is concerned with rather radical changes to a program's structure, so the final program may have properties very different from those of the original one.
Reference: [Kah92] <author> Stefan Kahrs. Unlimp, </author> <title> uniqueness as a leitmotiv for implementation. </title> <editor> In M. Bruynooghe and M. Wirsing, editors, </editor> <booktitle> 4th International Symposium on Programming Language Implementation and Logic Programming (PLILP 92), </booktitle> <address> Leuven, Belgium, </address> <pages> pages 115-129. </pages> <publisher> Springer Verlag, LNCS no 631, </publisher> <month> August </month> <year> 1992. </year>
Reference-contexts: The trick employed is to let graphs contain not only "fan-ins" but also "fan-outs", i.e. a node may point at several other nodes, the one to actually "choose" given from the context. Thus graphs which differ only "far below" can share representation. * in <ref> [Kah92] </ref> a system implementing not only a "hashing cons" but also a "hashing apply" is described, i.e. the mapping from nodes in the heap into terms is guaranteed to be injective thus also "accidental sharing" is detected, and our distinction (section 3.3) between sharing and memoization collapses. <p> Rule 2 and 3 account for "lazy memoization" (i.e. on equality of pointers, cf. the discussion in section 2.1.1); by also employing 4 one can get "full memoization" (i.e. on structural equality) the latter is implemented in <ref> [Kah92] </ref> (cf. section 3.5). <p> Due to this trick, the system implemented in [AT89] is not unreasonably inefficient. One might also devise a clever hash function to enable quick retrieval of rules, similar to what is done in <ref> [Kah92] </ref>. 5.3 Discussion and related work An interesting variation of the PEM is to drop the requirement that the height is 2 that is, one is allowed to do a PUSH step or to do an UNFOLD step using a rule in R i , i 1 even if the height <p> As already mentioned in section 3.5, <ref> [Kah92] </ref> describes a system where all nodes in the heap are unique.
Reference: [Kho90] <author> Hessam Khoshnevisan. </author> <title> Efficient memo-table management strategies. </title> <journal> Acta Informatica, </journal> <volume> 28 </volume> <pages> 43-81, </pages> <year> 1990. </year>
Reference-contexts: Introduction In the recent years a lot of work has been devoted to developing tools for transforming less efficient programs into more efficient programs. These include memoization <ref> [Kho90] </ref>; unfold/fold transformations [PP91b]; graph-based implementation of lazy evaluation [Jon87] and partial evaluation [JSS89]. The efficiency improvement caused by these techniques all are due to the fact that some computations are shared, i.e. they only have to be done once. <p> The rules are of form f (ff) ! fi (ff and fi are constants) making it possible to share the computation of f (ff) between its various invocations. When applying the technique, two issues must be settled: * Which functions to memoize on? In <ref> [Kho90] </ref> some syntactic criteria for deciding when memoization will be useful are given, at the same time exhibiting a method for "compile time garbage collection" of obsolete rules. * Which kind of equality to use when deciding whether a function has been called with the "same" argument before? The natural choice
Reference: [KK90] <author> Tadashi Kawamura and Tadashi Kanamori. </author> <title> Preservation of stronger equivalence in unfold/fold logic program transformation. </title> <journal> Theoretical Computer Science, </journal> <volume> 75 </volume> <pages> 139-156, </pages> <year> 1990. </year> <month> 251 </month>
Reference-contexts: Giving conditions for total correctness of unfold/fold transformations is a rather hot topic in the logic programming community, see e.g. [TS84], <ref> [KK90] </ref>, [Sek91], [GS91], [PP91a], [BCE92]. This is unlike the situation in the functional community where Kott's work (cf. section 4.9) seems rather isolated. <p> unfold/fold transformations in logic languages transformation typically proceeds in a "step by step fashion"; after a goal in the body of a clause has been unfolded the clause is deleted from the program and replaced by the clause resulting from the unfolding this is the approach taken in e.g. [GS91], <ref> [KK90] </ref>, [PP91a], [Sek91], [TS84]. As 197 ? 6 6 abc,1 ab abc,2 abc,2 pointed out in [GS91], one by applying this method loses some power - to see this, consider the clause C = p (f (X)) p (X). <p> Aside from being less powerful, we also think that the step-by-step strategy conceptually is less clean than our approach - cf. the discussion p. 14. In the literature, one is typically (contrary to our framework) not allowed to fold against a (direct or indirect) recursive predicate <ref> [KK90] </ref>, [PP91a], [Sek91], [TS84]. This mirrors the view that folding corresponds to abbreviation, a view also held in [Amt91]. [TS84] and [KK90] divide the predicates into two classes: the new (corresponding to "eureka-definitions") and old, where folding is allowed against new predicates only. <p> In the literature, one is typically (contrary to our framework) not allowed to fold against a (direct or indirect) recursive predicate <ref> [KK90] </ref>, [PP91a], [Sek91], [TS84]. This mirrors the view that folding corresponds to abbreviation, a view also held in [Amt91]. [TS84] and [KK90] divide the predicates into two classes: the new (corresponding to "eureka-definitions") and old, where folding is allowed against new predicates only. In the body of new predicates as well as in the body of old predicates, only old predicates can occur.
Reference: [KMP77] <author> Donald E. Knuth, James H. Morris, Jr., and Vaughan R. Pratt. </author> <title> Fast pattern matching in strings. </title> <journal> SIAM Journal on Computing, </journal> <volume> 6(2) </volume> <pages> 323-350, </pages> <month> June </month> <year> 1977. </year>
Reference-contexts: caused much surprise when [Coo71] showed that it is always possible to simulate a 2DPDA in linear time (wrt. the length of the input tape), even if the automaton carries out an exponential number of steps in particular this result gave Donald Knuth inspiration to his fast substring matching algorithm <ref> [KMP77, p. 338] </ref>. We shall see that the effect of this clever simulation can be acquired using the general concept of ultimate sharing. This chapter is based on joint work with Jesper Traff which has been reported in [AT92] (but the basic idea dates back to [AT89]). <p> no attention will be paid to the complexity of the preprocessing phase; * as it is a rather common exercise to achieve the KMP algorithm by PE (see e.g. [Smi91] for a survey), we primarily focus upon the BM algorithm. 7.2 The KMP method This is the method described in <ref> [KMP77] </ref>. As made explicit in [AHU74, Algorithm 9.3], it amounts to constructing from p a DFA which given s as input enters its accepting state exactly at the end of the first occurrence of p in s. Thus at most jsj references to s are made. <p> * If the next symbol is neither a nor b, none of the S 1 : : : S 6 fit - hence S 0 is entered. 161 7.3 The BM methods Originally devised by Boyer and Moore [BM77], numerous variants have emerged some of which are described in e.g. <ref> [KMP77] </ref> and [PS88]. The basic idea is to do the matching backwards wrt. p: with p = tree (which will be our working example), the initial configuration will be ...x......... <p> As an example of this, consider (as in <ref> [KMP77, p. 343] </ref>) the following situation: 164 ABABABABABABABABABABABA... ^ AAAAAAACB ffi 1 enables a double shift, whereas ffi 2 enables a single shift only. However, after this single shift the situation will soon be ABABABABABABABABABABABA... ^ AAAAAAACB and now ffi 2 enables a maximal shift. * In [KMP77, p. 343] it <p> consider (as in <ref> [KMP77, p. 343] </ref>) the following situation: 164 ABABABABABABABABABABABA... ^ AAAAAAACB ffi 1 enables a double shift, whereas ffi 2 enables a single shift only. However, after this single shift the situation will soon be ABABABABABABABABABABABA... ^ AAAAAAACB and now ffi 2 enables a maximal shift. * In [KMP77, p. 343] it is shown that BMor makes at most 6jsj references to s (the constant 6 probably being much too large). The linearity is solely [BM77, p. 767] due to the use 4 of ffi 2 . If only ffi 1 is used (i.e. <p> However, in practice the space required will be O (jpj), since only a few of these entries will differ from the default action, allowing a compact representation of the tables. * The merits of BMop (versus BMst) are briefly discussed in <ref> [KMP77, p. 346] </ref>. It is trivial that BMop makes at most jsj references to s (it never looks at the same position twice, as no information is forgotten). However, only for small alphabets we may expect BMop to be significantly faster than BMst.
Reference: [Knu81] <author> Donald E. Knuth. </author> <booktitle> The Art of Computer Programming, </booktitle> <volume> volume 2. </volume> <publisher> Addison-Wesley, </publisher> <address> second edition, </address> <year> 1981. </year>
Reference-contexts: complexity theory is an area with an enormous number of pitfalls, including: * If integer operations (addition, subtraction, multiplication, division and equality test) are assigned unit cost, the model "collapses" in the sense that e.g. complete factorization of a number n can be done in time O (log n) 2 <ref> [Knu81, p. 398] </ref>. * Consider the well-known Towers of Hanoi problem (see e.g. [Har87]), which in a logic language can be solved by the program hanoi (1,A,B,C,[mv (A,B) j L]-L). hanoi (s (N),A,B,C,L-S) :- hanoi (N,A,C,B,L1-S1), hanoi (N,C,B,A,L2-S2), append (L1-S1,[mv (A,B) j L2]-S2,L-S). append (L1-L2,L2-L3,L1-L3). 112 where we use the technique
Reference: [Kot80] <author> Laurent Kott. </author> <title> A system for proving equivalences of recursive programs. </title> <booktitle> In Proceedings of 5th conference on Automated Deduction, </booktitle> <publisher> Springer LNCS 87, </publisher> <pages> pages 63-69, </pages> <year> 1980. </year>
Reference-contexts: There is no counterpart to 3 in our work: we give conditions on the "function symbol level" (i.e. on which functions are unfolded), not on the "function argument level" hence there is no need for devising a well-founded ordering relation! <ref> [Kot80] </ref> describes a framework for proving programs to be equivalent, using two techniques: 1.
Reference: [Kot82] <author> Laurent Kott. </author> <title> Unfold/fold program transformations. </title> <type> Technical Report 155, </type> <institution> INRIA, Domaine de Voluceau Rocquen-court BP105 78153 Le Chesnay Cedex, France, </institution> <year> 1982. </year>
Reference-contexts: The steps are assumed to be "parallel outermost steps" (re-sembling normal order evaluation). No proofs (or references to such) are given of the abovementioned "fact" but the content is rather close to theorem 4.6.3: : : In [Kot85] (identical to the technical report <ref> [Kot82] </ref>) the problem of assuring total correctness is addressed (but, alas, again all theorems are given without proofs or references to proofs). The language treated is a first-order functional language (without branching).
Reference: [Kot85] <author> Laurent Kott. </author> <title> Unfold/fold program transformations. </title> <editor> In Maurice Nivat and John C. Reynolds, editors, </editor> <title> Algebraic Methods in Semantics, chapter 12. </title> <publisher> Cambridge University Press, </publisher> <year> 1985. </year>
Reference-contexts: be careful not to decrease the termination domain in example 2.1.2, this would have happened if we after having unfolded g (a::x) into f (f (a::x)) immediately folds back into g (a::x) yielding the level 1 rule g (a::x) !g (a::x) Conditions to prevent this from happening are given in <ref> [Kot85] </ref>. In chapter 4 the problem is addressed anew, with the aim of giving more intuitively understandable conditions than in [Kot85]. For a logic language, the issue is treated in depth in chapter 8. 4. <p> g (a::x) into f (f (a::x)) immediately folds back into g (a::x) yielding the level 1 rule g (a::x) !g (a::x) Conditions to prevent this from happening are given in <ref> [Kot85] </ref>. In chapter 4 the problem is addressed anew, with the aim of giving more intuitively understandable conditions than in [Kot85]. For a logic language, the issue is treated in depth in chapter 8. 4. In the unfold/fold framework one usually also apart from doing unfolding and folding is allowed to perform various algebraic manipulations. <p> The steps are assumed to be "parallel outermost steps" (re-sembling normal order evaluation). No proofs (or references to such) are given of the abovementioned "fact" but the content is rather close to theorem 4.6.3: : : In <ref> [Kot85] </ref> (identical to the technical report [Kot82]) the problem of assuring total correctness is addressed (but, alas, again all theorems are given without proofs or references to proofs). The language treated is a first-order functional language (without branching).
Reference: [Lam90] <author> John Lamping. </author> <title> An algorithm for optimal lambda calculus reduction. </title> <booktitle> In ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 16-30, </pages> <year> 1990. </year>
Reference-contexts: Several clever methods exist for "a more than fully lazy" evaluation of -expressions (e.g. <ref> [Lam90] </ref>); in section 3.1 we present a parametrized evaluation strategy for supercombinator programs with the pretentious name "ultimate sharing", to be seen as a "top-down" implementation of a multilevel transition system we argue that if parameters are chosen appropriately, this strategy is able to achieve the same degree of sharing as <p> The properties of this strategy would surely be worth a closer study; Grue (personal communication) has not investigated the issue further. * in <ref> [Lam90] </ref> (which [GAL92] attempts to elaborate on) an algorithm for graph reduction of -expressions is given which attempts to 36 avoid duplication of computation by keeping track of how redices propagate on the other hand, "accidental sharing" (that is redices which are identical but not copies of the same original redex)
Reference: [Lau91] <author> John Launchbury. </author> <title> Strictness and binding-time analyses: Two for the price of one. </title> <booktitle> In Proceedings of the ACM SIG-PLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <address> Toronto, Ontario, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: But you would agree with me that it is not a mathematical consideration. 3 The point is of course that it is possible to reduce X 5 to X X X X X, while 5 X cannot be reduced in a similar way. 243 * In <ref> [Lau91, Sect. 6] </ref>, considering a first order functional language with parametric polymorphism, the observation is made that one does not gain anything by declaring arguments of polymorphic type static, since computation does not depend on such arguments. * In the system described in [RW91], to each specialized function is associated information
Reference: [LG88] <author> John M. Lucassen and David K. Gifford. </author> <title> Polymorphic effect systems. </title> <booktitle> In ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 47-57, </pages> <year> 1988. </year>
Reference-contexts: community than in the latter 1 ; * operational semantics better capture the essence of unfolding and folding: unfolding corresponds to a transition being made in the "right" direction, folding corresponds to a transition being made 1 Recently, for instance the attempts to integrate functional programming and concurrency (see e.g. <ref> [LG88] </ref>, [Nie89], [BMT92]) have renewed the interest in operational semantics for func tional languages. 178 in the "wrong" direction. By using a denotational approach, this cannot be expressed directly (cf. the claims p. 12 and p. 123).
Reference: [Llo84] <author> J.W. Lloyd. </author> <title> Foundations of Logic Programming. </title> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference-contexts: It seems clear that a configuration should be a sequence of predicate symbols, together with some information about which values the arguments to those predicates can assume. One usually represents this information as a substitution, as e.g. in <ref> [Llo84] </ref>, but below we shall argue for choosing another representation. A key point in the reasoning in section 8.1.1 is that a folding followed by an unfolding (of the same predicate symbol) should cancel each other, i.e. be equivalent to the identity.
Reference: [Mar91] <author> Luc Maranget. </author> <title> Optimal derivations in weak lambda-calculi and in orthogonal terms rewriting systems. </title> <booktitle> In ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 255-269, </pages> <year> 1991. </year>
Reference-contexts: A denotational approach is attempted, making it possible to express the inherent complexity as the "number of fixed point unfoldings needed" however, some operational reasoning nevertheless (as to be expected) sneaks into the theory. In section 4.10 we will sketch how to extend our model to encompass inherent complexity. <ref> [Mar91] </ref> investigates a class of term rewriting systems where terms are labeled thereby implicitly defining DAGS but not cyclic graphs in general. By focusing upon the derivations where all redices with same label are reduced simultaneously, graph reduction is mimiced.
Reference: [Mic68] <author> Donald Michie. </author> <title> 'Memo' functions and machine learning. </title> <journal> Nature, </journal> <volume> 218 </volume> <pages> 19-22, </pages> <month> April </month> <year> 1968. </year> <month> 252 </month>
Reference-contexts: The discussion above will be concretized in the next section. 2.1 Instances of multilevel systems We now examine three well-known techniques for program optimization and show how their behavior can be expressed in terms of multilevel systems. 2.1.1 Memoization This is a classical technique, introduced by <ref> [Mic68] </ref>. The rules are of form f (ff) ! fi (ff and fi are constants) making it possible to share the computation of f (ff) between its various invocations.
Reference: [Mil78] <author> Robin Milner. </author> <title> A theory of type polymorphism in program-ming. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 17 </volume> <pages> 348-375, </pages> <year> 1978. </year>
Reference-contexts: Remark: We do not impose any "type discipline" in our model, as our concern is the operational behavior. Then, 42 since "badly typed programs may go wrong" (paraphrasing <ref> [Mil78] </ref>), "stuck configurations" may arise. Definition 4.1.1 A graph is a triple (N; L; S) where * N is a (finite) set of nodes, N being the union of three disjoint sets: The set of active nodes, denoted A to be thought of as application nodes.
Reference: [Mil89] <author> Robin Milner. </author> <title> Communication and Concurrency. </title> <publisher> Prentice-Hall, </publisher> <year> 1989. </year>
Reference-contexts: An interesting approach to reasoning about cost is given in [San90, chap. 4], where bisimulation techniques (well-known from e.g. <ref> [Mil89] </ref>) are used for formalizing that "two expressions compute the same answer using the same amount of time" to be more precise we say that R is a cost simulation iff, whenever e 1 R e 2 , the following holds: if e 1 using k 1 steps reduces to a
Reference: [Nie89] <author> Flemming Nielson. </author> <title> The typed lambda-calculus with first-class processes. </title> <booktitle> In PARLE '89 (LNCS 366), </booktitle> <pages> pages 357-373, </pages> <year> 1989. </year>
Reference-contexts: than in the latter 1 ; * operational semantics better capture the essence of unfolding and folding: unfolding corresponds to a transition being made in the "right" direction, folding corresponds to a transition being made 1 Recently, for instance the attempts to integrate functional programming and concurrency (see e.g. [LG88], <ref> [Nie89] </ref>, [BMT92]) have renewed the interest in operational semantics for func tional languages. 178 in the "wrong" direction. By using a denotational approach, this cannot be expressed directly (cf. the claims p. 12 and p. 123).
Reference: [Nie92] <author> Flemming Nielson, </author> <title> editor. Design, Analysis and Reasoning about Tools: </title> <booktitle> Abstracts from the Second Workshop. DAIMI PB-417, </booktitle> <month> September </month> <year> 1992. </year>
Reference-contexts: For e.g. Turing machines, the trick is to encode a sequence of symbols into one single symbol 12 . However, recently Neil Jones (as reported in <ref> [Nie92, p. 17] </ref>) came up with a model, closer to "computational practice", where constants do matter.
Reference: [NN90] <author> Hanne Riis Nielson and Flemming Nielson. </author> <title> Eureka definitions for free! or disagreement points for fold/unfold transformations. </title> <editor> In Neil D. Jones, editor, </editor> <booktitle> ESOP 90, </booktitle> <address> Copenhagen, Denmark. </address> <publisher> LNCS 432, </publisher> <pages> pages 291-305, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: be useful to memoize on "smaller units of computation" as will be done in section 3.1. 2.1.2 Unfold/fold transformations The unfold/fold framework for program transformation dates back to [BD77] and has since been the subject of much interest, primarily aimed at making the process of finding "eureka"-definitions more systematic, e.g. <ref> [NN90] </ref>, [PP90], [PP91b], [PP92]. Also supercompilation [Tur86] can be seen as a variant over the concept.
Reference: [Pal89] <author> Catuscia Palamidessi. </author> <title> Algebraic properties of idempotent substitutions. </title> <type> Technical Report TR-33/89, </type> <institution> University of Pisa, </institution> <year> 1989. </year>
Reference-contexts: Of course it will be possible to repair on that, but this almost inevitably gets rather messy (as witnessed by various papers in the literature!), since substitutions are hard to reason about from an algebraic point of view (even though e.g. [Son89] and <ref> [Pal89] </ref> show that certain sets of substitutions carry some structure), in particular one has to be very careful about renaming. We therefore (as a first attempt!) shall prefer to represent data as sets of ground values.
Reference: [PB82] <author> Alberto Pettorossi and R.M. Burstall. </author> <title> Deriving very efficient algorithms for evaluating linear recurrence relations using the program transformation technique. </title> <journal> Acta Informatica, </journal> <volume> 18 </volume> <pages> 181-206, </pages> <year> 1982. </year>
Reference-contexts: Finally (section 4.8.4), we will explain the remarkable speedup achieved in <ref> [PB82] </ref> (the fibonacci function transformed from being exponential into being logarithmic) in terms of the above "factor ization". * Section 4.9 attempts to put the present work into perspective by describing (in more or less detail) other approaches from the liter ature. * Finally, in section 4.10 some ways in which <p> now we have derived (4.20) in an alternative way luckily (!) with the same result. 4.8.4 How to make really big speedups : : : One of the most remarkable successes of the unfold/fold framework is the transformation of the fibonacci function from exponential time into logarithmic time, done in <ref> [PB82] </ref>. Of course it had been known for a long time that it is possible to compute fibonacci numbers in logarithmic time, e.g. by successive squaring of matrices the virtue of [PB82] is that it is shown how to get from the inefficient program into its efficient counterpart in a systematic <p> of the unfold/fold framework is the transformation of the fibonacci function from exponential time into logarithmic time, done in <ref> [PB82] </ref>. Of course it had been known for a long time that it is possible to compute fibonacci numbers in logarithmic time, e.g. by successive squaring of matrices the virtue of [PB82] is that it is shown how to get from the inefficient program into its efficient counterpart in a systematic way. We now reconstruct the transformation in [PB82], with the aim of exposing the causes of speedup explicitly at the price of giving no hints at how one can come up <p> a long time that it is possible to compute fibonacci numbers in logarithmic time, e.g. by successive squaring of matrices the virtue of <ref> [PB82] </ref> is that it is shown how to get from the inefficient program into its efficient counterpart in a systematic way. We now reconstruct the transformation in [PB82], with the aim of exposing the causes of speedup explicitly at the price of giving no hints at how one can come up with such a transformation. <p> Clearly fib (n) = G (1,1,n). Concerning 2, we want to prove 8n; k 0 : G (a; b; n + k) = G (1; 0; k)G (a; b; n) This can be done by induction in k (in <ref> [PB82] </ref> this induction is only done implicitly): if k = 0 (4.27) reads 8n 0 : G (a; b; n) = 1 G (a; b; n) + 0 G (a; b; n + 1) which certainly holds; and if k = 1 (4.27) reads 8n 0 : G (a; b; n
Reference: [Pet84] <author> Alberto Pettorossi. </author> <title> Methodologies for Transformations and Memoing in Applicative Languages. </title> <type> PhD thesis, </type> <institution> University of Edinburgh, Department of Computer Science, </institution> <month> October </month> <year> 1984. </year>
Reference-contexts: A classical ex-ample of this is how one transforms the fibonacci function from being exponential into being linear [BD77]: First we introduce the function g (x) = (fib (x),fib (x+1)) this is the tupling strategy, cf. <ref> [Pet84] </ref>.
Reference: [Plo81] <author> Gordon D. Plotkin. </author> <title> A structural approach to operational semantics. </title> <type> Technical Report FN-19, DAIMI, </type> <institution> University of Aarhus, Denmark, </institution> <month> September </month> <year> 1981. </year>
Reference-contexts: The model is based on transitions (defined in Plotkin-style <ref> [Plo81] </ref>) between configurations; the reason for preferring a transition semantics to a denotational semantics is that the former more naturally expresses the fact that unfolding/folding etc. is operational in nature 1 .
Reference: [PP90] <author> Maurizio Proietti and Alberto Pettorossi. </author> <title> Synthesis of eureka predicates for developing logic programs. </title> <booktitle> In Lecture Notes in Computer Science 432 (ESOP 90), </booktitle> <pages> pages 306-325, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: useful to memoize on "smaller units of computation" as will be done in section 3.1. 2.1.2 Unfold/fold transformations The unfold/fold framework for program transformation dates back to [BD77] and has since been the subject of much interest, primarily aimed at making the process of finding "eureka"-definitions more systematic, e.g. [NN90], <ref> [PP90] </ref>, [PP91b], [PP92]. Also supercompilation [Tur86] can be seen as a variant over the concept. <p> However, it will always be possible to describe an unfold/fold transformation as a finite sequence of "2-level" transformations. 22 On the other hand, in e.g. [Wad90], <ref> [PP90] </ref> and [PP91b] mechanizable strategies are given which are guaranteed to terminate for certain kinds of source programs.
Reference: [PP91a] <author> Maurizio Proietti and Alberto Pettorossi. </author> <title> Semantics preserv-ing transformation rules for Prolog. In Partial Evaluation and Semantics-Based Program Manipulation, New Haven, Connecticut. </title> <journal> (Sigplan Notices, </journal> <volume> vol. 26, no. 9), </volume> <year> 1991. </year>
Reference-contexts: Giving conditions for total correctness of unfold/fold transformations is a rather hot topic in the logic programming community, see e.g. [TS84], [KK90], [Sek91], [GS91], <ref> [PP91a] </ref>, [BCE92]. This is unlike the situation in the functional community where Kott's work (cf. section 4.9) seems rather isolated. <p> By first unfolding a and then unfolding d we first get [b,d]; [c,d] and then B 1 = [b,e]; [b,f]; [c,e]; [c,f]. By first unfolding d and then unfolding a we first get [a,e]; [a,f] and then B 2 = [b,e]; [c,e]; [b,f]; [c,f]. In <ref> [PP91a] </ref> one wants to distinguish between B 1 and B 2 , and therefore unfolding of the leftmost atom only is allowed (unless extra conditions are satisfied.) It is important to make the following observation: as configurations are multisets, backtracking is automatically accounted for in the model. <p> transformations in logic languages transformation typically proceeds in a "step by step fashion"; after a goal in the body of a clause has been unfolded the clause is deleted from the program and replaced by the clause resulting from the unfolding this is the approach taken in e.g. [GS91], [KK90], <ref> [PP91a] </ref>, [Sek91], [TS84]. As 197 ? 6 6 abc,1 ab abc,2 abc,2 pointed out in [GS91], one by applying this method loses some power - to see this, consider the clause C = p (f (X)) p (X). <p> Aside from being less powerful, we also think that the step-by-step strategy conceptually is less clean than our approach - cf. the discussion p. 14. In the literature, one is typically (contrary to our framework) not allowed to fold against a (direct or indirect) recursive predicate [KK90], <ref> [PP91a] </ref>, [Sek91], [TS84]. This mirrors the view that folding corresponds to abbreviation, a view also held in [Amt91]. [TS84] and [KK90] divide the predicates into two classes: the new (corresponding to "eureka-definitions") and old, where folding is allowed against new predicates only. <p> This greatly limits the applications, since it seems impossible to arrive at recursive definitions of eureka-predicates. On the other hand, it becomes possible to give a relatively simple proof of termination preservation. In contrast to the authors mentioned so far, <ref> [PP91a] </ref> impose an order on a sequence of goals, i.e. consider PROLOG's LR strategy. The crucial condition on folding is that the leftmost atom has been unfolded. Again by assigning the predicates folded against weight 0 and the others 1, the essence of this translates into our condition 8.1.2.
Reference: [PP91b] <author> Maurizio Proietti and Alberto Pettorossi. </author> <title> Unfolding Definition Folding, in this order, for avoiding unnecessary variables in logic programs. </title> <booktitle> In Proceedings of PLILP 91, </booktitle> <address> Passau, </address> <publisher> Germany (LNCS 528), </publisher> <pages> pages 347-358, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Introduction In the recent years a lot of work has been devoted to developing tools for transforming less efficient programs into more efficient programs. These include memoization [Kho90]; unfold/fold transformations <ref> [PP91b] </ref>; graph-based implementation of lazy evaluation [Jon87] and partial evaluation [JSS89]. The efficiency improvement caused by these techniques all are due to the fact that some computations are shared, i.e. they only have to be done once. <p> to memoize on "smaller units of computation" as will be done in section 3.1. 2.1.2 Unfold/fold transformations The unfold/fold framework for program transformation dates back to [BD77] and has since been the subject of much interest, primarily aimed at making the process of finding "eureka"-definitions more systematic, e.g. [NN90], [PP90], <ref> [PP91b] </ref>, [PP92]. Also supercompilation [Tur86] can be seen as a variant over the concept. <p> However, it will always be possible to describe an unfold/fold transformation as a finite sequence of "2-level" transformations. 22 On the other hand, in e.g. [Wad90], [PP90] and <ref> [PP91b] </ref> mechanizable strategies are given which are guaranteed to terminate for certain kinds of source programs.
Reference: [PP92] <author> Maurizio Proietti and Alberto Pettorossi. </author> <title> Best-first strategies for incremental transformations of logic programs. </title> <booktitle> In Proceedings of LOPSTR 92, </booktitle> <address> Manchester, </address> <month> 2-3 July </month> <year> 1992, 1992. </year>
Reference-contexts: memoize on "smaller units of computation" as will be done in section 3.1. 2.1.2 Unfold/fold transformations The unfold/fold framework for program transformation dates back to [BD77] and has since been the subject of much interest, primarily aimed at making the process of finding "eureka"-definitions more systematic, e.g. [NN90], [PP90], [PP91b], <ref> [PP92] </ref>. Also supercompilation [Tur86] can be seen as a variant over the concept.
Reference: [PS88] <author> H. Partsch and F. A. Stomp. </author> <title> A formal derivation of Boyer and Moore's pattern matching algorithm. </title> <type> Technical Report 88-12, </type> <institution> Department of Informatics, University of Nijmegen, </institution> <month> September </month> <year> 1988. </year>
Reference-contexts: the next symbol is neither a nor b, none of the S 1 : : : S 6 fit - hence S 0 is entered. 161 7.3 The BM methods Originally devised by Boyer and Moore [BM77], numerous variants have emerged some of which are described in e.g. [KMP77] and <ref> [PS88] </ref>. The basic idea is to do the matching backwards wrt. p: with p = tree (which will be our working example), the initial configuration will be ...x......... <p> The original Boyer-Moore, to be called BMor. This is the one presented in [BM77], i.e. ffi 1 as well as ffi 2 is exploited. 3. The standard Boyer-Moore, to be called BMst. This is the algo rithm hinted at [BM77, p. 771, below] and the one derived in <ref> [PS88] </ref>; it works by "combining" ffi 1 and ffi 2 into a single two-dimensional table. 4. The optimal Boyer-Moore, to be called BMop. <p> An analogous observation is made in <ref> [PS88] </ref>, where BMst is derived by formal methods from a naive substring matcher.
Reference: [QG91] <author> Christian Queinnec and Jean-Marie Geffroy. </author> <title> Symbolic pattern matching with intelligent backtrack. Can be achieved by email to (queinnec,geffroy)@poly.polytechnique.fr, </title> <year> 1991. </year>
Reference-contexts: The idea of maintaining a description of what is known about the input is employed in [Jor90], where an interpreter for a language with pattern matching is rewritten into one suitable for PE (thus a compiler can be generated). The development in <ref> [QG91] </ref> is rather similar to ours: first a pattern language is defined, where patterns can be built up using constructors like cons, quote, or, any (with the obvious meanings) etc. also recursive patterns are allowed.
Reference: [Rao84] <author> Jean Claude Raoult. </author> <title> On graph rewritings. </title> <journal> Theoretical Computer Science, </journal> <volume> 32 </volume> <pages> 1-24, </pages> <year> 1984. </year>
Reference-contexts: We do not in any way claim our approach to be original, in particular the approach in <ref> [Rao84] </ref> is rather close to ours. <p> the following holds 1 : m (n) is the same kind of node (active/passive/virtual) as n; if n is passive then L 2 (m (n)) = L 1 (n) (hence also Ar 2 (m (n)) = Ar 1 (n)); and if n is active or passive then for all 1 <ref> [Rao84] </ref> uses the terminology that m is a morphism at n iff the last two conditions hold, i.e. if labels and successors are preserved. 45 j j j j T T T l l l T T T @ @ + @ f v r 1 r 2 i 2 f1 <p> All kinds of morphisms are stable under ?. 2 Proof: The claims concerning id G are trivial; so is the fact that iso-morphisms, homomorphisms and reductions are closed under ?. Now let us see that specializations are closed under ?: 2 Cf. the remarks <ref> [Rao84, p. 11] </ref>, to motivate why restrictions somewhat similar to condition 2 have been given. 47 1 and 3 are trivial. For 2, assume that s 2 (s 1 (a)) = s 2 (s 1 (a 0 )) with a 6= a 0 .
Reference: [Red85] <author> Uday S. Reddy. </author> <title> Narrowing as the operational semantics of functional languages. </title> <booktitle> In IEEE Logic Programming Symposium, Boston, </booktitle> <pages> pages 138-151, </pages> <year> 1985. </year>
Reference-contexts: which was actually used to produce the function this enables specializations to be reused even though the (non-used) static arguments differ. 9.2 Integrating the functional and logical model The functional and logic paradigms are not too far apart, as witnessed by several successful attempts at bridging the gap - e.g. <ref> [Red85] </ref> and [Han92].
Reference: [Red89] <author> Uday S. Reddy. </author> <title> Rewriting techniques for program synthesis. </title> <booktitle> In Proceedings of 3rd International Conference on Rewriting Techniques and Applications, Lecture Notes in Computer Science 355, </booktitle> <pages> pages 388-403, </pages> <year> 1989. </year>
Reference-contexts: So the goals of partial evaluation are in a sense more modest and, we think, achievable by simpler methods than those of program transformation in general. This intuition has, so to speak, been formalized in section 4.8. In <ref> [Red89] </ref> an alternative approach to unfold/fold transformations is presented, using concepts from the theory of term rewriting systems 17 .
Reference: [RW91] <author> Erik Ruf and Daniel Weise. </author> <title> Using types to avoid redundant specialization. In Partial Evaluation and Semantics-Based Program Manipulation, New Haven, Connecticut. </title> <journal> (Sigplan Notices, </journal> <volume> vol. 26, no. 9), </volume> <year> 1991. </year> <month> 254 </month>
Reference-contexts: in a similar way. 243 * In [Lau91, Sect. 6], considering a first order functional language with parametric polymorphism, the observation is made that one does not gain anything by declaring arguments of polymorphic type static, since computation does not depend on such arguments. * In the system described in <ref> [RW91] </ref>, to each specialized function is associated information indicating how much of the static part which was actually used to produce the function this enables specializations to be reused even though the (non-used) static arguments differ. 9.2 Integrating the functional and logical model The functional and logic paradigms are not too
Reference: [Sah91] <author> Dan Sahlin. </author> <title> An Automatic Partial Evaluator for Full Prolog. </title> <type> PhD thesis, </type> <institution> Kungliga Tekniska Hogskolan, Stockholm, SICS, Swedish Institute of Computer Science, </institution> <address> Box 1263, S-164 28 Kista, Sweden, </address> <month> March </month> <year> 1991. </year>
Reference-contexts: If the analysis reveals that (2) might happen, one will have to make some static arguments dynamic (thus giving rise to less sharing of computations). * In <ref> [Sah91] </ref> a partial evaluator for full PROLOG is given which does not violate termination properties. A number of ways to ensure this are proposed, one of which in the functional context translates into putting an upper limit on the number of specialized versions of each function.
Reference: [San90] <author> David Sands. </author> <title> Calculi for Time Analysis of Functional Programs. </title> <type> PhD thesis, </type> <institution> Imperial College, </institution> <address> London, </address> <month> September </month> <year> 1990. </year>
Reference-contexts: An interesting approach to reasoning about cost is given in <ref> [San90, chap. 4] </ref>, where bisimulation techniques (well-known from e.g. [Mil89]) are used for formalizing that "two expressions compute the same answer using the same amount of time" to be more precise we say that R is a cost simulation iff, whenever e 1 R e 2 , the following holds: if
Reference: [Sch80] <author> William L. Scherlis. </author> <title> Expression Procedures and Program Derivation. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> August </month> <year> 1980. </year> <note> Report STAN-CS-80-818. </note>
Reference-contexts: It is worth noting that chapter 8, which addresses the problem of total correctness for a logic language, is built upon a generalized version of this intuition. <ref> [Sch80] </ref> introduces (using a first-order functional language with call-by-value semantics) the notion of expression procedures, a variant over the unfold/fold framework as in our model, folding is viewed as an abbreviation.
Reference: [Sek91] <author> Hirohisa Seki. </author> <title> Unfold/fold transformation of stratified programs. </title> <journal> Theoretical Computer Science, </journal> <volume> 86(1) </volume> <pages> 107-139, </pages> <year> 1991. </year>
Reference-contexts: Giving conditions for total correctness of unfold/fold transformations is a rather hot topic in the logic programming community, see e.g. [TS84], [KK90], <ref> [Sek91] </ref>, [GS91], [PP91a], [BCE92]. This is unlike the situation in the functional community where Kott's work (cf. section 4.9) seems rather isolated. <p> in logic languages transformation typically proceeds in a "step by step fashion"; after a goal in the body of a clause has been unfolded the clause is deleted from the program and replaced by the clause resulting from the unfolding this is the approach taken in e.g. [GS91], [KK90], [PP91a], <ref> [Sek91] </ref>, [TS84]. As 197 ? 6 6 abc,1 ab abc,2 abc,2 pointed out in [GS91], one by applying this method loses some power - to see this, consider the clause C = p (f (X)) p (X). <p> Aside from being less powerful, we also think that the step-by-step strategy conceptually is less clean than our approach - cf. the discussion p. 14. In the literature, one is typically (contrary to our framework) not allowed to fold against a (direct or indirect) recursive predicate [KK90], [PP91a], <ref> [Sek91] </ref>, [TS84]. This mirrors the view that folding corresponds to abbreviation, a view also held in [Amt91]. [TS84] and [KK90] divide the predicates into two classes: the new (corresponding to "eureka-definitions") and old, where folding is allowed against new predicates only. <p> As we have seen in section 8.1.2 this condition is (too) weak, since failing branches may convert to loops. <ref> [Sek91] </ref> improves on the above, essentially by coming up with condition 8.1.1 (still when new predicates have been assigned weight 0 and old predicates weight 1). <p> Actually, example 5 in [BCE92] shows that often their condition amounts to the condition given in <ref> [Sek91] </ref> (and thus to our condition 8.1.1). 8.3 Fundamental concepts We now embark on exhibiting the theory, an outline of which was presented in section 8.1. The material is highly technical, but contains no "deep" or surprising results.
Reference: [Ses88] <author> Peter Sestoft. </author> <title> Automatic call unfolding in a partial evaluator. </title> <editor> In D. Bjorner, A.P. Ershov, and N.D. Jones, editors, </editor> <booktitle> Partial Evaluation and Mixed Computation, </booktitle> <pages> pages 485-506. </pages> <publisher> North-Holland, </publisher> <year> 1988. </year>
Reference-contexts: Two sources for non-termination exist: 1. when generating the code for a specialized function, an attempt is made to unfold infinitely often. 2. an attempt is made to specialize a function with respect to infinitely many values. These problems have been attacked in several ways, e.g.: * In <ref> [Ses88] </ref> (1) is avoided by testing for cycles in the call graph (this technique being potentially very space consuming), but (2) remains a possibility. * In SIMILIX [BD91] one decreases the risk of (1) by not unfolding dynamic tests, i.e. tests whose outcome cannot be decided by the 26 static arguments
Reference: [Smi91] <author> Donald A. Smith. </author> <title> Partial evaluation of pattern matching in constraint logic programming languages. In Partial Evaluation and Semantics-Based Program Manipulation, New Haven, Connecticut. </title> <journal> (Sigplan Notices, </journal> <volume> vol. 26, no. 9), </volume> <year> 1991. </year>
Reference-contexts: not formally prove that the target programs indeed im-plement the KMP/BM algorithms, as this would require a rather heavy machinery; * no attention will be paid to the complexity of the preprocessing phase; * as it is a rather common exercise to achieve the KMP algorithm by PE (see e.g. <ref> [Smi91] </ref> for a survey), we primarily focus upon the BM algorithm. 7.2 The KMP method This is the method described in [KMP77].
Reference: [Son89] <author> Harald Sondergaard. </author> <title> Semantics-based analysis and transformation of logic programs. </title> <type> Technical Report 89/22, </type> <institution> DIKU, University of Copenhagen, Denmark, </institution> <year> 1989. </year>
Reference-contexts: Of course it will be possible to repair on that, but this almost inevitably gets rather messy (as witnessed by various papers in the literature!), since substitutions are hard to reason about from an algebraic point of view (even though e.g. <ref> [Son89] </ref> and [Pal89] show that certain sets of substitutions carry some structure), in particular one has to be very careful about renaming. We therefore (as a first attempt!) shall prefer to represent data as sets of ground values.
Reference: [SS86] <author> Leon Sterling and Ehud Shapiro. </author> <title> The Art of Prolog. </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: well-known Towers of Hanoi problem (see e.g. [Har87]), which in a logic language can be solved by the program hanoi (1,A,B,C,[mv (A,B) j L]-L). hanoi (s (N),A,B,C,L-S) :- hanoi (N,A,C,B,L1-S1), hanoi (N,C,B,A,L2-S2), append (L1-S1,[mv (A,B) j L2]-S2,L-S). append (L1-L2,L2-L3,L1-L3). 112 where we use the technique of difference lists (see e.g. <ref> [SS86] </ref>) to make it possible to append lists by a single unification. <p> The scope of this technique is quite wide, an important special case being when the source program has been designed by means of the "generate and test" paradigm (first all candidate solutions are generated; then it is tested whether they really are solutions). As stated e.g. <ref> [SS86, p. 207] </ref> the key to improve efficiency in such cases is to "push" the tester inside the generator as "deep" as possible, making it possible to discard failed candidates before they are fully generated.
Reference: [Sta80] <author> John Staples. </author> <title> Optimal evaluations of graph-like expressions. </title> <journal> Theoretical Computer Science, </journal> <volume> 10 </volume> <pages> 297-316, </pages> <year> 1980. </year>
Reference-contexts: In this setting, it is proved that the leftmost reduction strategy is optimal. 19 Similar work in this direction includes <ref> [Sta80] </ref>, which investigates general (acyclic) graph reductions and whose Result 3 states that "sound contractions" (reducing "sound nodes") are "optimal contractions", a "sound node" loosely speaking being one the "ancestors" of which are never reduced before the node itself is the paper then gives criteria for being sound.
Reference: [Sun90] <author> Daniel M. Sunday. </author> <title> A very fast substring search algorithm. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 132-142, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Contrary to the claims of the paper, this transformation cannot (in my opinion!) be considered automatic; neither is it obvious that it preserves semantics. In <ref> [Sun90] </ref> some very efficient variations of the Boyer-Moore idea are presented (but not within the context of program derivation). The approach is characterized by two features: * After a mismatch has been found in e.g. the situation ..TEX...
Reference: [Tak91] <author> Akihiko Takano. </author> <title> Generalized partial computation for a lazy functional language. In Partial Evaluation and Semantics-Based Program Manipulation, New Haven, Connecticut. </title> <journal> (Sigplan Notices, </journal> <volume> vol. 26, no. 9), </volume> <year> 1991. </year> <month> 255 </month>
Reference-contexts: &gt; 0, n &gt; 0 If ack is partially evaluated wrt. its first argument being 2, the following level 1 rules (specialized versions of ack) will typically be generated: ack (2,0) !ack (1,1) ack (1,0) !ack (0,1) 4 The concept of PE may be extended a bit, as e.g. in <ref> [Tak91] </ref> where "context" is taken into account, and in [BCD90] where a function can be specialized wrt. an argument satisfying some predicate. 25 If we introduce ack2, ack1 and ack0 such that ack2 (n) is an abbreviation (cf. the discussion in section 2.1.2) of ack (2,n) etc, the level 1 rules
Reference: [TS84] <author> Hisao Tamaki and Taisuke Sato. </author> <title> Unfold/fold transformation of logic programs. </title> <booktitle> In Proceedings of 2nd International Logic Programming Conference, Uppsala, </booktitle> <pages> pages 127-138, </pages> <year> 1984. </year>
Reference-contexts: Giving conditions for total correctness of unfold/fold transformations is a rather hot topic in the logic programming community, see e.g. <ref> [TS84] </ref>, [KK90], [Sek91], [GS91], [PP91a], [BCE92]. This is unlike the situation in the functional community where Kott's work (cf. section 4.9) seems rather isolated. <p> In order to get a polynomial algorithm one has to do as in <ref> [TS84, p. 135] </ref>: 1. (re)define perm as follows: perm ([ ],[ ]) 2; perm ([AjX],Y) perm (X,Z), ins (A,Z,Y) ins (A,X,[AjX]) 2; ins (A,[BjX],[BjY]) ins (A,X,Y) 2. exploit the "law" (cf. section 4.8.3) that ins (A,Z,Y),ord (Y) is "equivalent" to ord (Z),ins (A,Z,Y),ord (Y). <p> logic languages transformation typically proceeds in a "step by step fashion"; after a goal in the body of a clause has been unfolded the clause is deleted from the program and replaced by the clause resulting from the unfolding this is the approach taken in e.g. [GS91], [KK90], [PP91a], [Sek91], <ref> [TS84] </ref>. As 197 ? 6 6 abc,1 ab abc,2 abc,2 pointed out in [GS91], one by applying this method loses some power - to see this, consider the clause C = p (f (X)) p (X). <p> Aside from being less powerful, we also think that the step-by-step strategy conceptually is less clean than our approach - cf. the discussion p. 14. In the literature, one is typically (contrary to our framework) not allowed to fold against a (direct or indirect) recursive predicate [KK90], [PP91a], [Sek91], <ref> [TS84] </ref>. This mirrors the view that folding corresponds to abbreviation, a view also held in [Amt91]. [TS84] and [KK90] divide the predicates into two classes: the new (corresponding to "eureka-definitions") and old, where folding is allowed against new predicates only. <p> In the literature, one is typically (contrary to our framework) not allowed to fold against a (direct or indirect) recursive predicate [KK90], [PP91a], [Sek91], <ref> [TS84] </ref>. This mirrors the view that folding corresponds to abbreviation, a view also held in [Amt91]. [TS84] and [KK90] divide the predicates into two classes: the new (corresponding to "eureka-definitions") and old, where folding is allowed against new predicates only. In the body of new predicates as well as in the body of old predicates, only old predicates can occur. <p> these back into a new predicate. * Starting with the definition of a new predicate, N O 1 : : : O n , one has to do at least one unfolding of some of the O i 's before folding back into a new predicate. 5 5 Actually, in <ref> [TS84] </ref> one is allowed to fold even if no unfolding of an O i is made, provided not all the O i 's disappear by the folding.
Reference: [Tur36] <author> Alan Turing. </author> <title> On computable numbers with an application to the Entscheidungsproblem. </title> <journal> Proc. London Math. Soc., </journal> <volume> 42 </volume> <pages> 230-265, </pages> <year> 1936. </year>
Reference-contexts: This inequality will still hold, provided we can find a constant K such that the cost of making a function call in the 12 This somewhat conflicts with Turing's philosophical motivations for the design of his machine <ref> [Tur36] </ref>, namely that there is a bound on the number of symbols a human computer can distinguish between at one glance. 113 source program is less than K times the cost of making a function call in the target program. To be concrete, consider the Ackerman function from example 2.1.3.
Reference: [Tur79] <author> D. A. Turner. </author> <title> A new implementation technique for applicative languages. </title> <journal> Software Practice and Experience, </journal> <volume> 9 </volume> <pages> 31-49, </pages> <year> 1979. </year>
Reference-contexts: by means of graph reductions, instead of the -calculus: * One does not have to worry about free variables getting bound by fi-reduction (this problem being a main reason why the "bottommost spine redex" strategy may be considered unfeasible in practice [Jon87, p. 200]). 1 For SKI-combinators, as defined in <ref> [Tur79] </ref>, the trick to obtain the effects of fully lazy evaluation is [Jon87, p. 267] to incorporate the rule S (K p) (K q) ! K (p q). 37 * If one wants to include constants (like if and plus), what we surely will do for modeling practical applications, the language
Reference: [Tur86] <author> Valentin F. Turchin. </author> <title> The concept of a supercompiler. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(3) </volume> <pages> 292-325, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: For a further discussion of the merits of the two perspectives see <ref> [Tur86, p. 293] </ref>, according to which the former is "suggested by axiomatic mathematics" and the latter is "a product of cybernetic thinking". 1.1 An overview of the thesis * In chapter 2 we elaborate on the concept of multilevel transition systems. <p> Also supercompilation <ref> [Tur86] </ref> can be seen as a variant over the concept.
Reference: [Wad71] <author> C.P. Wadsworth. </author> <title> Semantics and Pragmatics of the Lambda Calculus. </title> <type> PhD thesis, </type> <institution> Oxford University, </institution> <year> 1971. </year>
Reference-contexts: Hence h 4 will be evaluated twice. However, one can do better: actually there is no need to copy +(h 4), since +(h 4) does not contain y free in [Jon87, p. 246] this observation is attributed to <ref> [Wad71] </ref>. An implementation clever enough to avoid such unnecessary copying is termed fully lazy [Jon87, p. 210]. Usually, one does not implement functional languages by means of fi-reductions of -expressions instead, one transforms into supercom-binators by lambda-lifting ([Jon87]).
Reference: [Wad90] <author> Philip Wadler. </author> <title> Deforestation: Transforming programs to eliminate trees. </title> <journal> Theoretical Computer Science, </journal> <volume> 73 </volume> <pages> 231-248, </pages> <year> 1990. </year>
Reference-contexts: However, it will always be possible to describe an unfold/fold transformation as a finite sequence of "2-level" transformations. 22 On the other hand, in e.g. <ref> [Wad90] </ref>, [PP90] and [PP91b] mechanizable strategies are given which are guaranteed to terminate for certain kinds of source programs.
Reference: [Wat80] <author> Osamu Watanabe. </author> <title> Another application of recursion introduction. </title> <journal> Information Processing Letters, </journal> <volume> 10(3) </volume> <pages> 116-119, </pages> <year> 1980. </year>
Reference-contexts: Motivated by this observation [Jon77] presents what amounts to a top-down version of the algorithm, which gives rise to essentially the same "flow-of-control" as the 2DM. In <ref> [Wat80] </ref> it is shown how something similar to [AHU74, algorithm 9.4] can be derived by means of general tabulation techniques.
Reference: [Yos93] <author> Nobuko Yoshida. </author> <title> Optimal reduction in weak lambda calculus with shared environments. </title> <booktitle> In International Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 243-252. </pages> <publisher> ACM Press, </publisher> <year> 1993. </year> <title> 256 Index + in functional model, 48, 68 in logic model, </title> <type> 227 ++, 118 </type>
Reference-contexts: A substantial difference is that the term rewriting community often restricts attention to terminating reduction sequences. 18 Another result in this direction is presented in <ref> [Yos93] </ref>, where a weak -calculus (i.e. no reductions under a ) with explicit environments (so the Church-Rosser property holds) is 126 wards" (from a normalizing derivation) what constitutes a needed redex, but for some subclasses of term rewriting systems (including supercombi-nators without pattern matching) we know that the "leftmost-outermost" redex will
Reference: [[B]] <institution> L </institution>

References-found: 104

