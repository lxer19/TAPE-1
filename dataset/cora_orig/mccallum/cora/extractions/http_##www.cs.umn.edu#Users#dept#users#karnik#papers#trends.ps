URL: http://www.cs.umn.edu/Users/dept/users/karnik/papers/trends.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/karnik/papers/
Root-URL: http://www.cs.umn.edu
Title: Trends in Multiprocessor and Distributed Operating System Designs  
Author: Anand R. Tripathi Neeran M. Karnik 
Keyword: Key-Words: Operating Systems, Distributed Systems, Massively Parallel Systems, Multiprocessors, Multicomputers, Load Balancing, Process Synchronization  
Date: August 15, 1994  
Address: Minneapolis MN 55455  
Affiliation: Department of Computer Science University of Minnesota,  
Abstract: This paper presents an overview of the developments in operating systems technology for distributed computing systems and multiprocessor machines. We focus on those design principles that are now widely accepted as useful design paradigms. Approaches common to distributed and multiprocessor operating systems are identified. Issues discussed include process scheduling and synchronization, load balancing, virtual and shared memory management and parallel file systems. The task/thread model, and the object model of computing are also discussed. 
Abstract-found: 1
Intro-found: 1
Reference: [Almes et al. 1985] <author> G. T. Almes, A. P. Black, E. D. Lazowska, and J. D. Noe. </author> <title> The Eden System: A Technical Review. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-11:43-59, </volume> <month> January </month> <year> 1985. </year>
Reference-contexts: In Argus which is an object-based language for distributed programming, the support for object management is entirely at the language level [Liskov 1988]. Eden provided operating system level support for object naming, protection, and network transparent access <ref> [Almes et al. 1985] </ref>. Support for new object definitions and synchronization of concurrent operations on objects was supported at the language level. Amoeba associates with each object a server process; an object is accessed through a capability, which contains the address for a communication port of the server process.
Reference: [Ananda et al. 1992] <author> A. L. Ananda, B. H. Tay, and E. K. Koh. </author> <title> A Survey of Asynchronous Remote Procedure Calls. </title> <booktitle> Operating Systems Review, </booktitle> <pages> pages 92-109, </pages> <month> April </month> <year> 1992. </year>
Reference: [Anderson et al. 1989] <author> Thomas E. Anderson, Edward D. Lazowska, and Henry M. Levy. </author> <title> The Performance Implications of Thread Management Alternatives for Shared-Memory Multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 1631-1644, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Rejuvenation use occurs in situations of exceptions; in such a case a corrupted thread can be discarded and a new one can be created in its place. User-Level and Kernel-Level Threads: Several thread management libraries such as FastThreads <ref> [Anderson et al. 1989] </ref> and PCR [Weiser et al. 1989] were developed in the late 1980s to support user-level threads above traditional OS kernels that did not support threads. All scheduling and synchronization mechanisms were implemented at the user level, without involving the kernel.
Reference: [Anderson et al. 1991] <author> Thomas Anderson, Brian Bershad, Edward Lazowska, and Henry Levy. </author> <title> Scheduler Activations: </title> <booktitle> Effective Kernel Support for the User-Level Management of Parallelism . In Proceedings of the 13'th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 95-109, </pages> <year> 1991. </year>
Reference-contexts: Also, in a multiprocessor environment one cannot exploit parallelism by executing such threads in parallel on different processors. On the other hand, kernel-level threads do not suffer from these problems but they are roughly an order of magnitude more expensive in terms of creation and context switch times <ref> [Anderson et al. 1991] </ref>. The above observations regarding the advantages as well as the limitations of user-level threads motivated several system designs to support both kernel level and user level threads [Edler et al. 1988] [Marsh et al. 1991] [Anderson et al. 1991]. <p> magnitude more expensive in terms of creation and context switch times <ref> [Anderson et al. 1991] </ref>. The above observations regarding the advantages as well as the limitations of user-level threads motivated several system designs to support both kernel level and user level threads [Edler et al. 1988] [Marsh et al. 1991] [Anderson et al. 1991]. In general, in these designs the kernel provides event notifications to the user-level thread management. In Psyche the term thread refers to a user-level thread, and the term virtual processor refers to a kernel level thread [Marsh et al. 1991]. <p> To support user-level thread management it introduces a meta-system call to provide an asynchronous interface for making any system call. It also relies on kernel-to-user-space signals for call completion notifications. Anderson et al. developed a user level thread management facility on the DEC Firefly multiprocessor workstation <ref> [Anderson et al. 1991] </ref>. In their design, a scheduler activation represents a virtual processor. An activation record serves three purposes. Similar to a kernel thread it is a virtual processor for executing user threads.
Reference: [Anderson 1990] <author> Thomas Anderson. </author> <title> The Performance of Spin Lock Alternatives for Shared Memory Multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <month> Jan-uary </month> <year> 1990. </year>
Reference: [Archibald and Baer 1986] <author> James Archibald and Jean-Loup Baer. </author> <title> Cache Coherence Protocols: Evaluation Using a Multiprocessor Simulation Model . ACM Transactions on Computer Systems, </title> <address> pages 273-298, </address> <month> November </month> <year> 1986. </year> <month> 27 </month>
Reference-contexts: Most commonly used protocols are based on read-replication, which supports the single-writer/multiple-reader based synchronization. A shared item can be migrated to or replicated in the address spaces of multiple processes as long as it is being read only. This is identical to the write-invalidate approach for cache coherence <ref> [Archibald and Baer 1986] </ref>. In distributed systems and multicomputers it is implemented using directories which keep track of the current owner (the process that last modified the item) and the current list of copies. In large systems the current trends favor directory based schemes using the write invalidate approach. <p> In case of full-replication based scheme for memory coherence, a copy of the data is cre 22 ated in the address space of each process using the item. A write operation is broadcast to all copies. This is identical to the write-update protocol <ref> [Archibald and Baer 1986] </ref>. To support it in a large multicomputer or distributed system one needs a multicasting facility. One also needs atomic broadcast primitives to synchronize concurrent updates from two different nodes.
Reference: [Asbury and Scott 1989] <author> Raymond K. Asbury and David S. Scott. </author> <title> FORTRAN I/O on the iPSC/2: Is there read after write? In Proceedings of the Fourth Conference on Hypercubes, </title> <booktitle> Concurrent Computers and Applications, </booktitle> <pages> pages 129-132, </pages> <year> 1989. </year>
Reference-contexts: Thus, the file system's performance on write operations is enhanced. Shared files can be opened in several different modes, depending on the intended use. For example, the Intel iPSC's Concurrent File System provides four modes <ref> [Asbury and Scott 1989] </ref>. In one mode, each process sharing the file gets its own copy of the file pointer, and can read or write whichever parts of the file it needs to. There is no synchronization between processes.
Reference: [Athas and Seitz 1988] <author> W. Athas and C. Seitz. </author> <title> Multicomputers: Message-Passing Concurrent Computers. </title> <booktitle> IEEE Computer, </booktitle> <month> August </month> <year> 1988. </year>
Reference-contexts: For example, in the BBN's Butterfly machine remote memory access is about five times slower than local access [Mellor-Crummey and Scott 1991]. Multicomputer systems consist of a large number of low-cost microprocessors with local memory, connected by a high bandwidth interconnection network <ref> [Athas and Seitz 1988] </ref>. Such systems are generally characterized by the absence a global shared memory and are termed no-remote memory access (NORMA) architectures. Cosmic Cube [Seitz 1985] and CM-5 [Hillis and Tucker 1993] are examples in this category. In such systems interprocess communication is based exclusively on message passing.
Reference: [Bach 1984] <author> Maurice Bach. </author> <title> Multiprocessor UNIX Operating Systems. </title> <journal> Bell Laboratories Technical Journal, </journal> <pages> pages 1733-1749, </pages> <month> October </month> <year> 1984. </year>
Reference-contexts: Intel's Touchstone DELTA is one such architecture [Lillevik 1991]. Some of the initial designs of UNIX-based multiprocessors introduced the concept of asymmetric processing of operating system functions <ref> [Bach 1984] </ref>. Such architectures evolved mainly to avoid the synchronization problems associated in making the UNIX kernel code multithreaded. Only one processor is designated as the master processor for executing the system code. The current trend however is clearly in supporting symmetric multiprocessing by making UNIX implementations multithreaded and reentrant. <p> In such systems multiple threads may be executing the kernel code at the same time while running on different processors. Some multiprocessor implementations of the UNIX operating system have used semaphores for synchronizing access to shared kernel data structures <ref> [Bach 1984] </ref>. Semaphore operations are implemented as spin-lock controlled critical sections. Most of the problems in using semaphore based synchronization are related to race situations in testing a condition and then blocking a thread using the P operation [Ruane 1991]. <p> However, the use of such a mechanism in multiprocessor system introduces some potential for inefficiency. A wakeup operation may cause a number of processes to be resumed in parallel and contend for the critical section thus causing all but one of these processes to be suspended again <ref> [Bach 1984] </ref>. This is termed as the problem of thundering herd [Ruane 1991] [Campbell et al. 1991].
Reference: [Bal et al. 1992] <author> H. E. Bal, M. A. Kaashoek, and A. S. Tanenbaum. Orca: </author> <title> A Language for Parallel Programming of Distributed Systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. 18, No. 3 </volume> <pages> 190-205, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Systems such as Clouds support consistency at the level of shared segments mapped in the virtual address spaces of different objects. Other systems, such as Munin [Bennett et al. 1990] and Orca <ref> [Bal et al. 1992] </ref>, have supported consistency at the level of structured objects. A classification of consistency protocols for DSM is presented by Stumm and Zhou [1990]. These are classified along two dimensions, namely migrating/non-migrating, and replicated/non-replicated data. <p> To support it in a large multicomputer or distributed system one needs a multicasting facility. One also needs atomic broadcast primitives to synchronize concurrent updates from two different nodes. Orca, which is a distributed/parallel programming language implemented above Amoeba, uses this type of approach for managing shared objects <ref> [Bal et al. 1992] </ref>. An approach based on a central server (i.e. non-migrating and non-replicated) is not attractive because each access to the shared object incurs network communication and its associated latency. It is not able to exploit the locality of references, which a migration based scheme does.
Reference: [Barak and Litman 1985] <author> A. Barak and A. Litman. </author> <title> MOS: A Multicomputer Distributed Operating System. </title> <journal> Software Practice and Experience, </journal> <pages> pages 725-737, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: During the last decade a number of operating systems were developed for distributed computing systems. These include Mach [Rashid et al. 1988] [Loepere 1992] (and OSF/1), Amoeba [Mullender et al. 1990], Chorus [Rozier et al. 1988], MOS <ref> [Barak and Litman 1985] </ref>, Cronus [Schantz et al. 1986], Sprite [Ousterhout et al. 1988], Locus [Walker et al. 1983], Clouds [Dasgupta et al. 1991], Choices [Campbell et al. 1993], Spring [Khalidi and Nelson 1993] and Nexus [Tripathi 1989]. The design goals of some of these systems have been quite varied. <p> In local area networks a number of tools have been designed for distribution of batch jobs among idle workstations [Litzkow et al. 1988]. In multicomputer operating systems similar kinds of schemes have been used for better utilization of resources [Smith 1988] <ref> [Barak and Litman 1985] </ref> [Zacew et al. 1993]. There are four fundamental components of any dynamic load balancing architecture [Shivaratri et al. 1992]: transfer policy, selection 17 policy, location policy, and information policy. Transfer policy determines the state when a processor should participate in a load balancing activity.
Reference: [Bennett et al. 1990] <author> J. Bennett, J. Carter, and W. Zwaenepoel. Munin: </author> <title> Distributed Shared Memory Based on Type Specific Memory Coherence . In Proc. </title> <booktitle> of 1990 Conf. on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 168-176, </pages> <year> 1990. </year>
Reference-contexts: Systems such as Clouds support consistency at the level of shared segments mapped in the virtual address spaces of different objects. Other systems, such as Munin <ref> [Bennett et al. 1990] </ref> and Orca [Bal et al. 1992], have supported consistency at the level of structured objects. A classification of consistency protocols for DSM is presented by Stumm and Zhou [1990]. These are classified along two dimensions, namely migrating/non-migrating, and replicated/non-replicated data.
Reference: [Bershad 1990] <author> Brian Bershad. </author> <title> The Increasing Irrelevance of IPC Performance for Microkernel-Based Operating Systems. </title> <booktitle> In Proceedings of the Summer 1990 USENIX, </booktitle> <year> 1990. </year>
Reference: [Birman 1985] <author> Kenneth P. Birman. </author> <title> Replication and fault-tolerance in the ISIS system. </title> <booktitle> Tenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 79-86, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: The group concept is supported by the V system at the kernel level but it does not provide any global synchronization mechanisms across process groups. Isis, which is a message-passing programming environment, provides a set of high level abstractions for communication and synchronization in a group of processes <ref> [Birman 1985] </ref>. Two useful abstractions supported by it are atomic broadcast and causal broadcast. If two messages are sent to a group using the atomic broadcast primitive, then all members of the group receive the two messages in exactly the same order.
Reference: [Birman 1994] <author> Kenneth Birman. </author> <title> A Response to Cheriton and Skeen's Criticism of Causal and Totally Ordered Communication . ACM Operating Systems Review, </title> <address> pages 11-21, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: Database-style transaction mechanisms can provide a viable alternative. However, in the presence of a causally and totally ordered communication subsystem, the programming model seen by the user is significantly simplified <ref> [Birman 1994] </ref>. 3.3 Scheduling and Load Balancing In this section first we describe some of the process/thread scheduling techniques that have been used in shared memory multiprocessor systems. Next we examine the impact of synchronization requirements on scheduling.
Reference: [Black 1990] <author> David Black. </author> <title> Scheduling Support for Concurrency and Parallelism in the Mach Operating System. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 35-43, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Support is also provided for disabling preemption of a thread for some bounded sections of its code. In Mach, a higher priority thread can voluntarily relinquish the processor instead of spinning <ref> [Black 1990] </ref>. Also, if the blocked thread knows the identity of the current lock holder then it can handoff the processor to that thread. SunOS 5.O uses priority inheritance; a thread's priority is determined by the priorities of the threads that it is blocking [Eykholt et al. 1992]. <p> However this data structure can become a hot-spot of contention. For this reason, in many systems a separate local queue is maintained for each processor in addition to a global queue <ref> [Black 1990] </ref>. Relocating a thread to another processor simply involves putting its control block in that processor's local queue. 3.3.1 Scheduling on Shared Memory Multiprocessors Multiprogram Scheduling: A number of scheduling policies for multiprogrammed multiprocessors are evaluated in [Leutenegger and Vernon 1990].
Reference: [Butler and Lusk 1992] <author> Ralph Butler and Ewing Lusk. </author> <title> User Guide to the P4 Programming System. </title> <type> Technical report, </type> <institution> Argonne National Laboratory, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: For example, Express [Parasoft 1990] was based on message passing libraries for iPSC/2 and iPSC/860. There are a number of other systems, such as PVM [Sunderam 1990] and P4 <ref> [Butler and Lusk 1992] </ref>, that are user-level libraries for parallel computing on a network of computers. These systems use the facilities provided by the host operating systems to create a virtual parallel machine and provide a suitable set of primitives for programming such a machine.
Reference: [Campbell et al. 1991] <author> Mark Campbell, Richard Barton, Jim Browning, Dennis Cervenka, Ben Curry, Todd Davis, Tracy Edmonds, Russ Holt, John Slice, Tucker Smith, and Rich Wescott. </author> <title> The Parallelization of UNIX System V Release 4.0. </title> <booktitle> In Proceedings of the Winter 1991 USENIX, </booktitle> <pages> pages 307-323, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Also, there is the related issue of parallelization and scheduling of the operating system's activities. Most vendors supporting multiprocessor UNIX systems have incorporated parallelization of the operating system code. Results and experiences from some of these efforts are discussed in <ref> [Campbell et al. 1991] </ref> [Eykholt et al. 1992]. Process management primitives are required to provide support for creating and scheduling parallel activities of an application. The task-thread model, popularized by the Mach design, has now been adopted by almost every modern operating system [Rashid et al. 1988]. <p> The empirical study by Karlin et al. shows that adaptive competitive schemes that take into account the waiting time experienced during the past lock acquisition requests perform better than non-adaptive algorithms. Some system designs have introduced the concept of advisable processor locks (APL) <ref> [Campbell et al. 1991] </ref> in which each lock contains the amount of time it would be locked. Based on this information other contending processes can decide whether they should spin or sleep. <p> A wakeup operation may cause a number of processes to be resumed in parallel and contend for the critical section thus causing all but one of these processes to be suspended again [Bach 1984]. This is termed as the problem of thundering herd [Ruane 1991] <ref> [Campbell et al. 1991] </ref>. To avoid this kind of problem some of the multiprocessor implementations of UNIX have introduced the concept of WakeupOneProcess [Campbell et al. 1991], which causes only the highest priority sleeping process to be resumed. <p> This is termed as the problem of thundering herd [Ruane 1991] <ref> [Campbell et al. 1991] </ref>. To avoid this kind of problem some of the multiprocessor implementations of UNIX have introduced the concept of WakeupOneProcess [Campbell et al. 1991], which causes only the highest priority sleeping process to be resumed. Another concept adopted by some parallel UNIX designs is the introduction of mutex locks that are implicitly released on a sleep call and reacquired on wakeup [Campbell et al. 1991] [Ruane 1991]. <p> implementations of UNIX have introduced the concept of WakeupOneProcess <ref> [Campbell et al. 1991] </ref>, which causes only the highest priority sleeping process to be resumed. Another concept adopted by some parallel UNIX designs is the introduction of mutex locks that are implicitly released on a sleep call and reacquired on wakeup [Campbell et al. 1991] [Ruane 1991]. This scheme is used for deadlock avoidance. Synchronization in Message Passing Systems: In message based systems, the concepts of causality and event ordering play an important role in process synchronization [Lamport 1978].
Reference: [Campbell et al. 1993] <author> Roy Campbell, Nayeem Islam, David Raila, and Peter Madany. </author> <title> Designing and Implementing Choices: An Object-Oriented System in C++. </title> <journal> Communications of the ACM, </journal> <volume> 36(9) </volume> <pages> 117-126, </pages> <month> September </month> <year> 1993. </year> <month> 28 </month>
Reference-contexts: Object models have been adopted by various distributed operating systems and programming languages for more than a decade. Operating system designers have now realized the benefits of using object-oriented methods in designing and structuring system kernels. The Choices design is a major step in this direction <ref> [Campbell et al. 1993] </ref>. Distributed file systems are primarily used as a resource sharing service across the network with the primary goals of providing long-term information storage and network-transparent access [Levy and Silberschatz 1990]. <p> These include Mach [Rashid et al. 1988] [Loepere 1992] (and OSF/1), Amoeba [Mullender et al. 1990], Chorus [Rozier et al. 1988], MOS [Barak and Litman 1985], Cronus [Schantz et al. 1986], Sprite [Ousterhout et al. 1988], Locus [Walker et al. 1983], Clouds [Dasgupta et al. 1991], Choices <ref> [Campbell et al. 1993] </ref>, Spring [Khalidi and Nelson 1993] and Nexus [Tripathi 1989]. The design goals of some of these systems have been quite varied. The Locus and Sprite designs were primarily guided by the requirement of providing a network-transparent UNIX environment in local area networks of computers. <p> None of the above mentioned operating systems employ object-oriented design methods in their internal organization and structure. Application of object-oriented design methods to building operating systems is a relatively newer trend. Choices <ref> [Campbell et al. 1993] </ref>, Apertos [Yokote 1992], and Clouds [Dasgupta et al. 1991] represent this trend. Such systems are implemented using object-oriented methods such as classes and inheritance hierarchies. An object-oriented operating system design represents a family of operating systems, rather than a fixed single design point.
Reference: [Carter et al. 1991] <author> John Carter, John Bennett, and Willy Zwaenepoel. </author> <booktitle> Implementation and Performance of Munin . In Proceedings of the 13'th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 152-164, </pages> <year> 1991. </year>
Reference-contexts: It is not able to exploit the locality of references, which a migration based scheme does. Systems such as Munin provide multiple coherence protocols <ref> [Carter et al. 1991] </ref>. The programmer can optimize shared memory accesses by providing information about the expected access patterns of shared variables, as part of the variable declaration. Based on this information, the system can choose a suitable consistency protocol.
Reference: [Cheriton and Skeen 1993] <author> David Cheriton and Dale Skeen. </author> <booktitle> Understanding the Limitations of Causally and Totally Ordered Communication . In Proceedings of the 14'th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 44-57, </pages> <year> 1993. </year>
Reference-contexts: In case of causal broadcast, all such messages are delivered to the group members in an order that is consistent with the causality relationship among the messages. However, the utility of causal and total ordering of communication has been a matter of recent debate <ref> [Cheriton and Skeen 1993] </ref>. The contention is that the end-to-end argument [Saltzer et al. 1984] is violated in attempting to solve application-level problems (such as ordering requirements) at the communication level. Database-style transaction mechanisms can provide a viable alternative.
Reference: [Cheriton et al. 1991] <author> David Cheriton, Hendrik A. Goosen, and Patrick D. Boyle. </author> <title> Paradigm: </title>
Reference-contexts: Others provide modified compilers that detect references to shared data, and emulate the shared memory in software. In the following discussion we address software level mechanisms and related issues for implementing distributed shared memory. Paradigm is a hardware-software based hybrid scheme for massively parallel multicomputers <ref> [Cheriton et al. 1991] </ref>. Software based DSM schemes have been implemented on workstation networks and hypercube based 21 multicomputers. There are many motivations for supporting distributed shared memory. One is the simplicity of the interprocess communication abstraction in a coherent shared memory system.
References-found: 22

