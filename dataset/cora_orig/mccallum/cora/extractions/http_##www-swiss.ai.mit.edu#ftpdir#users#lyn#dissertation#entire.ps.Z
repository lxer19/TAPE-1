URL: http://www-swiss.ai.mit.edu/ftpdir/users/lyn/dissertation/entire.ps.Z
Refering-URL: http://www-swiss.ai.mit.edu/~lyn/slivers.html
Root-URL: 
Title: Slivers: Computational Modularity via Synchronized Lazy Aggregates  
Author: by Franklyn Albin Turbak Gerald Jay Sussman David K. Gifford 
Degree: (1986) Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Doctor of Philosophy at the  Signature of Author  Certified by  Matsushita Professor of Electrical Engineering Thesis Supervisor Certified by  Associate Professor of Computer Science and Engineering Thesis Supervisor Accepted by Frederic R. Morgenthaler Chairman, Departmental Committee on Graduate Students  
Note: c Massachusetts Institute of Technology  
Date: February 1994  1994  January 31, 1994  
Address: (1986)  
Affiliation: S.B., Massachusetts Institute of Technology  S.M., Massachusetts Institute of Technology  MASSACHUSETTS INSTITUTE OF TECHNOLOGY  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 0
Reference: [AA93] <author> Zena Ariola and Arvind. </author> <title> Graph rewriting systems: Capturing sharing of computation in language implementations. Computation Structure Group Memo 347, </title> <institution> MIT Laboratory for Computer Science, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: The Edgar rules have been implemented as an interpreter, but only as a proof-of-concept; we make no claim that the Edgar style of graph reduction is a good implementation technique. In this sense, Edgar is closer to work on graph rewriting semantics <ref> [B + 87, AA93] </ref>. Because implementation efficiency is not a concern, some of the concepts in Edgar are cleaner than in other graph reduction frameworks. <p> Although parallel graph reduction techniques exist [Pey87, Sme93], the focus is again on efficiency, not on semantics. Until relatively recently, side effects have been anathema to the functional programming community, so they rarely appear in graph reduction work (but see <ref> [AA93] </ref>). An notable exception to the avoidance of side effects is Bawden's work on connection graphs (CGs) [Baw86, Baw92]. Bawden introduced (CGs) to explore issues of state and linearity in programming languages.
Reference: [Ada91] <author> Stephen Adams. </author> <title> Modular Grammars for Programming Language Prototyping. </title> <type> PhD thesis, </type> <institution> Department of Electronics and Computer Science, University of Southampton, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: However, in recent years, there has been a flurry of activity on modular attribute grammars, which strive to group all the computations of a given attribute into a modular unit <ref> [DC90, Ada91, FMY92, KW92, Wat92] </ref>.
Reference: [Ame89] <author> American National Standards Institute. </author> <title> American National Standard for Information Systems Programming Language Fortran: </title> <address> S8(X3.9-198x), </address> <year> 1989. </year>
Reference-contexts: The aggregate data style has its roots in Lisp's list manipulation routines (as epitomized by mapcar) and APL's array operators. Today, the aggregate data approach is the main organizing principle for data parallel languages (e.g., Fortran 90 <ref> [Ame89] </ref>, C* [RS87], NESL [Ble92], paralations [Sab88]). <p> Gap filtering is used in situations where the location of elements is important. For example, languages with array-based or data-parallel features (e.g., <ref> [Ive87, Ame89, GJSO92, Sab88, Ble90, RS87, Ble92] </ref>) present models in which elements reside at a particular location in an data structure.
Reference: [AN89] <author> Arvind and Rishiyur S. Nikhil. </author> <title> A dataflow approach to general-purpose parallel computing. Computation Structure Group Memo 302, </title> <institution> MIT Laboratory for Computer Science, </institution> <month> July </month> <year> 1989. </year>
Reference-contexts: Today, the aggregate data approach is the main organizing principle for data parallel languages (e.g., Fortran 90 [Ame89], C* [RS87], NESL [Ble92], paralations [Sab88]). It is also a commonly used technique in many other languages, especially functional and mostly functional ones (e.g., Haskell [HJW + 92], Id <ref> [AN89] </ref>, Common Lisp [Ste90], Scheme [CR + 91], ML [MTH90]). 3.1.1 Database Example: A List Implementation For the linear database example, lists are an obvious choice for the type of aggregate data structure. Figure 3.1 shows how each of the slivers can be represented as a list-manipulation procedure. <p> For example, rather than transmitting a tree over a single channel, why not transmit a tree over a tree of channels, where each channel transmits exactly one element? This is the approach taken by languages, such as Id <ref> [AN89] </ref> and Linda [CG89], that exhibit producer/consumer parallelism. These languages support non-strict data structures 3.2. THE CHANNEL APPROACH 121 that can be manipulated by a program before their components have been computed. <p> But the eagons on the arguments further allow the arguments to be evaluate concurrently with the body of the procedure. Ecall corresponds to the default procedure application strategy for the Id programming language <ref> [AN89, Tra88] </ref>. 7.1.6 Graphical Bindings Opera's nex and nexrec constructs are "graphical" versions of let and letrec. Whereas let and letrec associate names with first-class values, nex and nexrec associate names with syntactic entities so that the general graph-structured syntactic dependencies can be expressed within the tree-structured confines of s-expressions.
Reference: [ANP89] <author> Arvind, Rishiyur S. Nikhil, and Keshav K. Pingali. I-structures: </author> <title> Data structures for parallel computing. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <pages> pages 598-632, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: THE SIGNAL PROCESSING STYLE OF PROGRAMMING [Bir89b, CM90], producer/consumer models (CLU iterators [L + 79], Id's I-structures and M-structures <ref> [ANP89, Bar92] </ref>, Linda [CG89]) and dataflow ([Den75], [WA85], [DK82]). Below, we explore both coroutining and concurrent versions of the channel approach. <p> This changes the input/ouput behavior of copy in a major way. Underconstrained control is a classic problem whenever concurrent processes communicate via shared mutable data. The solution is to introduce extra constraints in the form of synchronization. Typical synchronization mechanisms include locks [Bir89b], semaphores [Dij68], monitors [Hoa74], I-structures <ref> [ANP89] </ref>, M-structures [Bar92], and Hughes's synch construct [Hug83, Hug84]. * Termination: A cobegin does not return until all its subprocesses have returned; this kind of behavior is typical of fork/join parallelism. But getting all the subprocesses to terminate can be tricky. <p> Because all processes lose access to the synchron upon resumption, there can only be one rendezvous per synchron. The rendezvous protocol of synchrons sets it apart from other synchronization structures (e.g., semaphores [Dij68], locks [Bir89b], synchronous messages [Hoa85], I-structures <ref> [ANP89] </ref>, and M-structures [Bar92]). Synchronization typically involves some processes waiting in a suspended state for a shared synchronization entity to be released by the process that currently owns it. Traditional protocols supply explicit wait and release operations.
Reference: [Ash86] <author> E. A. Ashcroft. </author> <title> Dataflow and eduction: Data-driven and demand-driven distributed computing. </title> <editor> In J. W. deBakker, W.-P. de Roever, and G. Rozenberg, editors, </editor> <booktitle> Current Trends in Concurrency: Overviews and Tutorials, </booktitle> <pages> pages 1-50. </pages> <publisher> Springer-Verlag, </publisher> <year> 1986. </year> <booktitle> Lecture Notes in Computer Science, Number 224. </booktitle>
Reference-contexts: Arvind and Pingali describe a mechanism for simulating demand-driven evaluation in a data flow model; they use data tokens to represent demand [PA85, PA86]. Ashcroft describes a system that combines demand flow (via entities called questons) with data flow (via entities called datons) <ref> [Ash86] </ref>. The visual notations used to represent Edgar graphs were influenced by the notations used in the data flow community (e.g., [Den75, DK82]). We emphasize, however, that the execution models for Edgar and data flow are fundamentally different.
Reference: [ASS85] <author> Harold Abelson, Gerald Jay Sussman, and Julie Sussman. </author> <title> Structure and Interpretation of Computer Programs. </title> <publisher> MIT Press, </publisher> <year> 1985. </year>
Reference-contexts: Even 1.1. THE PROBLEM 23 higher-level properties like program readability, writability, and modifiability are closely tied to patterns of process evolution. Process patterns common enough to be considered idioms are more easily programmed and recognized than idiosyncratic ones. Following Abelson and Sussman <ref> [ASS85] </ref>, I refer to patterns of process evolution as computational shapes. Some examples of computational shapes are iterations in two state variables, linear recursions, and left-to-right pre-order tree walks. <p> Below, we investigate two procedures describing computations on an employee database: * (mean-age database-descriptor) returns the average age of employees in the specified database. 1 The procedure specifications have parentheses because they, like all programming examples in this dissertation, are written in Scheme, a dialect of Lisp ([CR + 91], <ref> [ASS85] </ref>). I use Scheme because its support for first-class procedures, side effects, and tail recursion permits concise expression of a wide range of programming styles. However, the computational issues I am investigating are independent of the particular language in which the examples are phrased. 2.1. <p> The imperative programs are rather similar to their functional counterparts. The main difference is that immutable formal parameters in the functional versions become mutable state variables in the imperative version. 2.1.4 Computational Shapes Following <ref> [ASS85] </ref>, I carefully distinguish procedures from the computations that they specify. 3 A procedure is just a specification for a computational process, whereas a computation is the process that dynamically unfolds when the procedure is called. <p> In addition to the obvious fact that these programs perform different calculations, the gather procedure 3 I use the term "computation" in place of the term "process" used by <ref> [ASS85] </ref>. I make this change to distinguish the standard usage of "process" in the concurrency community from the notion for computational unfoldment presented in [ASS85]. 2.1. <p> the obvious fact that these programs perform different calculations, the gather procedure 3 I use the term "computation" in place of the term "process" used by <ref> [ASS85] </ref>. I make this change to distinguish the standard usage of "process" in the concurrency community from the notion for computational unfoldment presented in [ASS85]. 2.1. <p> We'd like some method of characterizing these sorts of operational similarities and differences between computations. One way of doing this is to adopt a standard evaluation model that captures operational features. For example, <ref> [ASS85] </ref> uses an expresssion-rewriting model to analyze procedures in terms of traces of their evaluation. <p> The rightward-growing "bulge" of the trace expressions is due to the pending calls to cons, which textually encode the implicit control stack required by the computation. These examples show how the expression-rewriting model differentiates procedures according to the pattern by which their computations evolve. In keeping with <ref> [ASS85] </ref>, I will refer to these patterns of computational evolution as shapes of computation. The notion of shape is intended to capture operational features of a computation, such as time and space complexity and the relative ordering of various events. <p> The storage pitfalls of the aggregate data approach will become more problematic over time, not less so. Furthermore, the straightforward aggregate data technique fails totally in cases where the aggregate structures are conceptually infinite. Infinite data structures can be a powerful way to modularize programs (see <ref> [ASS85] </ref>). An excellent example [Hug90] is decomposing a game program into a part that generates a game tree, and a part that examines the game tree. <p> THE AGGREGATE DATA APPROACH 99 procedure for counting the number of employees who earn more than a given amount: (define (fat-cat-count threshold database) (running-sum (map-one (filter-salary threshold (generate-records database))))) Rather than manipulating lists, suppose that the subroutines manipulate Scheme streams, a form of lazy lists described in <ref> [ASS85] </ref>. <p> OF PROGRAMMING collector to reclaim the intermediate storage allocated for each record by the time the next one is processed. 2 The side effects necessary for the cdr-bashing trick aren't needed here because they are effectively hidden by the memoization employed by streams to avoid the recomputation of delayed values <ref> [ASS85] </ref>. Unfortunately, lazy data is not a silver bullet. The subtle interactions between laziness and garbage collection can make it hard to predict the storage requirements of a program. <p> But the fact that the storage can be reclaimed at any future point means that it not charged to the computation in space analysis. 3 Here's why: Under the usual environment model of Scheme evaluation (see <ref> [ASS85] </ref>), the stream argument to running-sum stream is accessible from the environment in which accum is evaluated. Even though stream is never referenced within accum, its value can't be garbage collected until accum returns. <p> While Synapse does not exhibit the full range of expressiveness implied by the sliver technique, it is rich enough for investigating the power and limitations of slivers. According to Abelson and Sussman, a language has three parts: primitives, means of combination, and means of abstraction <ref> [ASS85] </ref>. For Synapse, the primitives are a set of higher-order procedures that manipulate synchronized lazy lists and trees. These procedures are carefully implemented so that they obey all of the sliver requirements discussed in Section 5.4.2. <p> SLAGS 6.1.3 Laziness In the case of synquences, the `lazy" in "synchronized lazy aggregates" means that neither the element nor the tail of a synquence is computed until it is actually needed. (In comparison, Scheme streams have lazy tails but not lazy elements.) Additionally, both elements and tails are memoized <ref> [ASS85, Hug85] </ref> so that they are only computed once if they are computed at all. Laziness leads to two important features of synquences: 1. Synquences may be conceptually infinite. 2. An element is not computed if it never used. <p> A lazon is automatically touched if it appears in a context that requires the value of the suspended computation. 9 The touch procedure can be used to explicitly touch a lazon. Here is 9 Automatic touching distinguishes lazons from Scheme's delayed objects <ref> [ASS85] </ref>, whose suspended computations can only be explicitly touched via the force procedure. 6.1. <p> Then we explore how the features of Opera can be used to implement the Synapse slivers and slags illustrated in Chapter 6. 7.1 An Introduction to Opera Opera is a dialect of Scheme <ref> [ASS85, CR + 91] </ref> that supports various mechanisms for controlling the fine-grained operational details of programs. Figure 7.1 summarizes the syntax of the language and the primitive operations it supports. Both syntactically and semantically, 295 296 CHAPTER 7. <p> Both forms create a placeholder object that stands for the result of evaluating E body . In the case of lazons, evaluation of E body is delayed until the lazon appears in a context that requires its value. Lazons are like the delayed objects described in [Hen80] and <ref> [ASS85] </ref> except that they are implicity forced rather than explicitly forced; we shall see how this difference enhances modularity. <p> A "sufficiently smart" implementation of Opera 7.1. AN INTRODUCTION TO OPERA 311 could determine this fact through static analysis. However, we will rely only on the semantics provided in Chapter 8, which dictate that example 6 must also deadlock. A careful reader familiar with the Scheme environment model <ref> [ASS85] </ref> may wonder why a lot more examples don't deadlock. <p> Non-touching contexts, such as the operand positions of a pcall and the argument positions of a cons, do not probe any details of the lazon's body, and simply pass around the lazon instead. Lazons are similar to the delayed values described in [Hen80] and <ref> [ASS85] </ref>, except that delayed values are touched by application of an explicit force procedure. 5 In Opera, the forcing is done implicitly by a touching context. <p> The arguments will only be evaluated if they are needed within the body of the called procedure. Lcall corresponds to the call-by-need parameter passing mechanism common in functional programming languages (e.g. Haskell [HJW + 92] and Miranda [Tur85]). Like the delayed objects of [Hen80] and <ref> [ASS85] </ref>, lazons can be used to create conceptually infinite data structures and finesse certain kinds of circular dependencies. For example, the stream datatype of [ASS85] can be defined in Opera as follows: ;;; Syntactic sugar: ;;; (cons-stream E head E tail ) ;;; desugars to ;;; (cons E head (lazon E <p> Haskell [HJW + 92] and Miranda [Tur85]). Like the delayed objects of [Hen80] and <ref> [ASS85] </ref>, lazons can be used to create conceptually infinite data structures and finesse certain kinds of circular dependencies. For example, the stream datatype of [ASS85] can be defined in Opera as follows: ;;; Syntactic sugar: ;;; (cons-stream E head E tail ) ;;; desugars to ;;; (cons E head (lazon E tail )) ;;; Procedures (define head car) (define tail cdr) (define the-empty-stream '()) (define empty-stream? null?) 6 In Opera, as in Scheme, tests of <p> Opera relies on touching contexts to do this forcing. The implicit forcing of lazons supports modularity better than the explicit forcing of delayed objects. We will illustrate this point using a stream integration problem described in <ref> [ASS85] </ref>. <p> The solution suggested in <ref> [ASS85] </ref> is to delay the first argument to integral: (define (solve f y-init dt) (letrec ((y (integral (delay dy) y-init dt)) (dy (map-stream f y))) y)) But because delayed objects must be explicitly forced, this necessitates a change to integral: (define (integral delayed delayed-integrand initial-value dt) (letrec ((int (cons-stream initial-value (add-streams <p> The proc node itself has k input ports that are attached to the values of the free variables (a 1 , : : :, a k ). These inputs correspond to the environment of a closure in the traditional environment model of procedures <ref> [ASS85] </ref>. The dotted lines connected the first k input ports on the template to the input ports of the proc node are intended to suggest that the template inputs will be connected to the proc inputs upon application of the procedure. <p> This strategy has desirable consequences for garbage collection. Aggressively dropping references to the values of unused lexical variables means that those values can be garbage collected as soon as they are no longer needed. In contrast, procedures in the traditional environment model <ref> [ASS85] </ref> hold onto the values of all lexically enclosing variables. As a result, the garbage collection of many provably useless values is delayed because they are spuriously held by a procedure. <p> The "result" of the Edgar interpreter is the graph structure remaining when there are no more tasks to be performed. The basic architecture of the Edgar interpreter was inspired by Abelson and Suss-man's digital logic simulator <ref> [ASS85] </ref>. The representation of networks, the propagation of information, and the agenda-based simulation closely resemble aspects of the digital logic simulator. The main innovation of the Edgar interpreter is the dynamic nature of its networks. <p> A closure is a representation for a first-class procedure that pairs the code of the procedure with some structure that determines the meaning of free variable references that appear in the code. In the classical environment model, such as that expounded in <ref> [ASS85] </ref>, the meaning of free variables is determined by an environment structure that contains bindings for the names of all lexically enclosing variables. The problem with this approach is that the interpreter can inadvertently hold onto unreferenceable structures via the environment.
Reference: [ASU86] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers: principles, techniques, and tools. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: Most of these compile-time techniques are based on a high-level version of the loop fusion technique employed by many optimizing compilers <ref> [ASU86] </ref>. Synchronized lazy aggregates are essentially a mechanism for performing loop fusion at run-time. Due to their dynamic nature, synchronized lazy aggregates enable a level of expressiveness that cannot be matched by the static approaches. 3. <p> Deadlock can arise in the presence of synch if demand2 cannot be applied until the successful return of demand1, or vice versa. 104 CHAPTER 3. THE SIGNAL PROCESSING STYLE OF PROGRAMMING fusion technique performed by many optimizing compilers <ref> [ASU86] </ref>. As a typical example of these techniques, the Scheme expression (map f (map g l)) could be replaced by (map (lambda (x) (f (g x))) l): The later expression is preferable because no list is constructed for the ouput of g.
Reference: [Axe86] <author> Tim S. Axelrod. </author> <title> Effects of synchronization barriers on multiprocessor performance. </title> <journal> Parallel Computing, </journal> <volume> 3(2) </volume> <pages> 129-140, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: Intuitively, synchrons express a set of time constraints that are solved by Opera. The lock step model of the sliver technique implies that slivers are concurrently executing recursive procedures that participate in a barrier synchronization at every corresponding call and return. In traditional barrier synchronization <ref> [Axe86] </ref>, the number of participating processes is known in advance, and synchronization can easily be implemented by a counter. However, due to the dynamic configurability of sliver networks, the number of slivers participating in a barrier synchronization cannot generally be predicted from the text of the program.
Reference: [B + 87] <editor> H.P. Barendregt et al. </editor> <title> Toward an intermediate language based on graph rewriting. </title> <booktitle> In PARLE: Parallel Architectures and Languages Europe, </booktitle> <volume> Volume 2, </volume> <pages> pages 159 - 175. </pages> <publisher> Springer-Verlag, </publisher> <year> 1987. </year> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> Number 259. </volume> <pages> 436 BIBLIOGRAPHY </pages>
Reference-contexts: The Edgar rules have been implemented as an interpreter, but only as a proof-of-concept; we make no claim that the Edgar style of graph reduction is a good implementation technique. In this sense, Edgar is closer to work on graph rewriting semantics <ref> [B + 87, AA93] </ref>. Because implementation efficiency is not a concern, some of the concepts in Edgar are cleaner than in other graph reduction frameworks.
Reference: [Bac78] <author> John Backus. </author> <title> Can programming be liberated from the von Neuman style? A functional style and its algebra of programs. </title> <journal> Communications of the ACM, </journal> <volume> 21(8) </volume> <pages> 245-264, </pages> <month> August </month> <year> 1978. </year>
Reference-contexts: The problem with existing transformation and compilation techniques is that they either provide no guarantees or they only work on a limited class of programs. Algebraic transformation techniques <ref> [DR76, Dar82, Bac78, Bel86, Bir89a, Bir86, Bir88] </ref> perform transformations like the above map removal, but systems that automatically apply these transformations do not guarantee that all intermediate data will be removed. <p> Another valid recursive to iterative transformation is to use the cdr-bashing trick to replace (upQ init tcons synq) by (down-cons! init synq). This technique could be used to make iterative versions of append, copy, map, and map-list. A host of other transformations from the literature on program transformation <ref> [DR76, Dar82, Bac78, Bel86, Bir89a, Bir86, Bir88, Coh83, Str71, WS73, Pet84] </ref> find a convenient expression in terms of slivers. The above points indicate that, regardless of their appropriateness as an implementation language, slivers are a powerful way to design, specify, reason about, and explain programs.
Reference: [Bar84] <author> H.P Barendregt. </author> <title> The Lambda-calculus: Its Syntax and Semantics. </title> <publisher> North-Holland, </publisher> <year> 1984. </year>
Reference-contexts: In fact, attempting to assign to a variable introduced by one of these constructs is an error. Eta expansion The eta expansion phase, E exp , maps an OK expression to an OK expression by eta expanding <ref> [Bar84] </ref> every reference to a primitive procedure that does not occur in the operator position of a pcall application.
Reference: [Bar92] <author> Paul S. Barth. </author> <title> Atomic data structures for parallel computing. </title> <type> Technical Report MIT/LCS/TR-532, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: First-class Synchronization Barriers : The Id programming language employs a synchronization barrier construct as a means of controlling the non-functional features of a mostly functional language <ref> [Bar92] </ref>. The synchrons introduced in this report can be viewed as first-class synchronization barriers. Thus, one aspect of this research is exploring the gains in expressive power that can be achieved by making synchronization barriers first-class objects. 7. <p> This point is certainly not new and is elegantly argued elsewhere ([ASS85], <ref> [Bar92] </ref>). However, it is worth emphasizing that side effects are an important tool that cannot be neglected in our goal of modularizing programs. <p> THE SIGNAL PROCESSING STYLE OF PROGRAMMING [Bir89b, CM90], producer/consumer models (CLU iterators [L + 79], Id's I-structures and M-structures <ref> [ANP89, Bar92] </ref>, Linda [CG89]) and dataflow ([Den75], [WA85], [DK82]). Below, we explore both coroutining and concurrent versions of the channel approach. <p> Underconstrained control is a classic problem whenever concurrent processes communicate via shared mutable data. The solution is to introduce extra constraints in the form of synchronization. Typical synchronization mechanisms include locks [Bir89b], semaphores [Dij68], monitors [Hoa74], I-structures [ANP89], M-structures <ref> [Bar92] </ref>, and Hughes's synch construct [Hug83, Hug84]. * Termination: A cobegin does not return until all its subprocesses have returned; this kind of behavior is typical of fork/join parallelism. But getting all the subprocesses to terminate can be tricky. <p> In particular, I will be concerned with the following two issues: 1. Non-Determinism: In the presence of side-effects, multi-threaded computations may be non-deterministic. Different interleavings of mutation operators can give rise to different results. Various atomicity and synchronization techniques (e.g., <ref> [Bir89b, Dij68, Bar92] </ref>) may be required to appropriately constrain behavior. 2. Storage Requirements: Multi-threaded computations have the potential for consuming much more storage than single-threaded ones. <p> Another approach to parallel name generation would be to use down-scan with an accumulator that employed a mutable name generator. Of course, the name generator would have to be carefully constructed in order to guarantee atomicity. This is the sort of strategy that is encouraged by Id's M-structures <ref> [Bar92] </ref>. The alpha renaming network is useful for program manipulations other than alpha renaming. A deBruijn numbering program labels variables in a lambda term with a number indicating the level of the lambda that introduces them [Pey87]. <p> Because all processes lose access to the synchron upon resumption, there can only be one rendezvous per synchron. The rendezvous protocol of synchrons sets it apart from other synchronization structures (e.g., semaphores [Dij68], locks [Bir89b], synchronous messages [Hoa85], I-structures [ANP89], and M-structures <ref> [Bar92] </ref>). Synchronization typically involves some processes waiting in a suspended state for a shared synchronization entity to be released by the process that currently owns it. Traditional protocols supply explicit wait and release operations.
Reference: [Baw86] <author> Alan Bawden. </author> <title> Connection graphs. </title> <booktitle> In Symposium on Lisp and Functional Programming, </booktitle> <pages> pages 258-265. </pages> <publisher> ACM, </publisher> <month> August </month> <year> 1986. </year>
Reference-contexts: Until relatively recently, side effects have been anathema to the functional programming community, so they rarely appear in graph reduction work (but see [AA93]). An notable exception to the avoidance of side effects is Bawden's work on connection graphs (CGs) <ref> [Baw86, Baw92] </ref>. Bawden introduced (CGs) to explore issues of state and linearity in programming languages. Like Edgar, CGs are a framework in which graph rewrite rules define the semantics of interconnected nodes with labelled ports.
Reference: [Baw92] <author> Alan Bawden. </author> <title> Linear Graph Reduction: Confronting the Cost of Naming. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Mas-sachusetts Institute of Technology, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: Until relatively recently, side effects have been anathema to the functional programming community, so they rarely appear in graph reduction work (but see [AA93]). An notable exception to the avoidance of side effects is Bawden's work on connection graphs (CGs) <ref> [Baw86, Baw92] </ref>. Bawden introduced (CGs) to explore issues of state and linearity in programming languages. Like Edgar, CGs are a framework in which graph rewrite rules define the semantics of interconnected nodes with labelled ports. <p> The current version of Opera handles most standard Scheme features. The Scheme features currently not handled are: vectors, continuations, rest arguments, apply, and multiple-value returns. Eventually, I plan to add all of these to Opera. The handling of continuations will be based largely on Bawden's work (see <ref> [Baw92] </ref>). The Opera evaluator works by compiling Opera expressions into Edgar graphs that are interpreted by the Edgar interpreter (see Section 8.3 for details).
Reference: [Bel86] <author> Francoise Bellegarde. </author> <title> Rewriting systems on FP expressions that reduce the number of sequences yielded. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 6(1) </volume> <pages> 11-34, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: The problem with existing transformation and compilation techniques is that they either provide no guarantees or they only work on a limited class of programs. Algebraic transformation techniques <ref> [DR76, Dar82, Bac78, Bel86, Bir89a, Bir86, Bir88] </ref> perform transformations like the above map removal, but systems that automatically apply these transformations do not guarantee that all intermediate data will be removed. <p> Another valid recursive to iterative transformation is to use the cdr-bashing trick to replace (upQ init tcons synq) by (down-cons! init synq). This technique could be used to make iterative versions of append, copy, map, and map-list. A host of other transformations from the literature on program transformation <ref> [DR76, Dar82, Bac78, Bel86, Bir89a, Bir86, Bir88, Coh83, Str71, WS73, Pet84] </ref> find a convenient expression in terms of slivers. The above points indicate that, regardless of their appropriateness as an implementation language, slivers are a powerful way to design, specify, reason about, and explain programs. <p> The implicit buffering provided by other aggregate data approaches makes it hard to control the storage profiles of modular programs. Existing list and tree removal techniques (such as listlessness [Wad84, Wad85], deforestation [Wad88, Chi92, GLJ93], and other transformations <ref> [Bel86, Bir89a] </ref>) automatically remove some intermediate data structures, but they either are limited to restricted network topologies or do not guarantee that all intermediate structures go away. In con 6.1.
Reference: [Bir84] <author> R. S. Bird. </author> <title> Using circular programs to eliminate multiple traversals of data. </title> <journal> Acta Informatica, </journal> <pages> pages 239-250, </pages> <year> 1984. </year>
Reference-contexts: Here are some alternate interpretations: 1. The boxes are procedures that take and return aggregate data structures. 13 This is only true as long as there are no circularities in the data dependencies implied by the cables. While such "data loops" can be be useful programming techniques (e.g. <ref> [Bir84] </ref>), they can also complicate reasoning about programs by requiring programmers to think in terms of fixed points. 78 CHAPTER 2. SLIVERS 2. The boxes are demand-driven agents that request individual values from and return individual values to their neighbors. 3. <p> In this case, a down-left shard is impossible because its 2 I am assuming here that all operations performed in a tile computation are themselves strict. In the presence of lazy operators, cyclic dependencies are not only possible but often desirable (e.g., see <ref> [Bir84] </ref>, [Joh87]). Later, I will introduce a form of laziness that, if used indiscriminantly, would invalidate the claims made about the impossibility of mutually dependent tiles and the execution order of sequential tiles. However, I will carefully restrict laziness in order to preserve these claims. 4.2.
Reference: [Bir86] <author> Richard S. Bird. </author> <title> An introduction to the theory of lists. </title> <editor> In Manfred Broy, editor, </editor> <booktitle> Logic of Programming and Calculi of Discrete Design (NATO ASI Series, </booktitle> <volume> Vol. F36), </volume> <pages> pages 5-42. </pages> <publisher> Springer-Verlag, </publisher> <year> 1986. </year>
Reference-contexts: The problem with existing transformation and compilation techniques is that they either provide no guarantees or they only work on a limited class of programs. Algebraic transformation techniques <ref> [DR76, Dar82, Bac78, Bel86, Bir89a, Bir86, Bir88] </ref> perform transformations like the above map removal, but systems that automatically apply these transformations do not guarantee that all intermediate data will be removed. <p> Another valid recursive to iterative transformation is to use the cdr-bashing trick to replace (upQ init tcons synq) by (down-cons! init synq). This technique could be used to make iterative versions of append, copy, map, and map-list. A host of other transformations from the literature on program transformation <ref> [DR76, Dar82, Bac78, Bel86, Bir89a, Bir86, Bir88, Coh83, Str71, WS73, Pet84] </ref> find a convenient expression in terms of slivers. The above points indicate that, regardless of their appropriateness as an implementation language, slivers are a powerful way to design, specify, reason about, and explain programs.
Reference: [Bir88] <author> Richard S. Bird. </author> <title> Lectures on constructive functional programming. </title> <editor> In Manfred Broy, editor, </editor> <booktitle> Constructive Methods in Computing Science (NATO ASI Series, </booktitle> <volume> Vol. F55), </volume> <pages> pages 5-42. </pages> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: The problem with existing transformation and compilation techniques is that they either provide no guarantees or they only work on a limited class of programs. Algebraic transformation techniques <ref> [DR76, Dar82, Bac78, Bel86, Bir89a, Bir86, Bir88] </ref> perform transformations like the above map removal, but systems that automatically apply these transformations do not guarantee that all intermediate data will be removed. <p> Another valid recursive to iterative transformation is to use the cdr-bashing trick to replace (upQ init tcons synq) by (down-cons! init synq). This technique could be used to make iterative versions of append, copy, map, and map-list. A host of other transformations from the literature on program transformation <ref> [DR76, Dar82, Bac78, Bel86, Bir89a, Bir86, Bir88, Coh83, Str71, WS73, Pet84] </ref> find a convenient expression in terms of slivers. The above points indicate that, regardless of their appropriateness as an implementation language, slivers are a powerful way to design, specify, reason about, and explain programs.
Reference: [Bir89a] <author> R. S. Bird. </author> <title> Algebraic identities for program calculation. </title> <journal> The Computer Journal, </journal> <pages> pages 122-126, </pages> <year> 1989. </year>
Reference-contexts: The problem with existing transformation and compilation techniques is that they either provide no guarantees or they only work on a limited class of programs. Algebraic transformation techniques <ref> [DR76, Dar82, Bac78, Bel86, Bir89a, Bir86, Bir88] </ref> perform transformations like the above map removal, but systems that automatically apply these transformations do not guarantee that all intermediate data will be removed. <p> Another valid recursive to iterative transformation is to use the cdr-bashing trick to replace (upQ init tcons synq) by (down-cons! init synq). This technique could be used to make iterative versions of append, copy, map, and map-list. A host of other transformations from the literature on program transformation <ref> [DR76, Dar82, Bac78, Bel86, Bir89a, Bir86, Bir88, Coh83, Str71, WS73, Pet84] </ref> find a convenient expression in terms of slivers. The above points indicate that, regardless of their appropriateness as an implementation language, slivers are a powerful way to design, specify, reason about, and explain programs. <p> The implicit buffering provided by other aggregate data approaches makes it hard to control the storage profiles of modular programs. Existing list and tree removal techniques (such as listlessness [Wad84, Wad85], deforestation [Wad88, Chi92, GLJ93], and other transformations <ref> [Bel86, Bir89a] </ref>) automatically remove some intermediate data structures, but they either are limited to restricted network topologies or do not guarantee that all intermediate structures go away. In con 6.1.
Reference: [Bir89b] <author> Andrew Birrel. </author> <title> An introduction to programming with threads. </title> <type> SRC Report 35, </type> <institution> Digital Equipment Corporation, </institution> <month> January </month> <year> 1989. </year>
Reference-contexts: Other examples of the channel approach include: communicating threads 6 Waters argues that such cycles make programs harder to understand, and therefore should be avoided at all costs. 106 CHAPTER 3. THE SIGNAL PROCESSING STYLE OF PROGRAMMING <ref> [Bir89b, CM90] </ref>, producer/consumer models (CLU iterators [L + 79], Id's I-structures and M-structures [ANP89, Bar92], Linda [CG89]) and dataflow ([Den75], [WA85], [DK82]). Below, we explore both coroutining and concurrent versions of the channel approach. <p> The bounded queue approach is a standard means of implementing flow control between independent processes <ref> [Bir89b] </ref>. Why not simply adopt a policy in which all the channel queues are bounded? The problem with this is that there are problems that require unbounded queues. <p> This changes the input/ouput behavior of copy in a major way. Underconstrained control is a classic problem whenever concurrent processes communicate via shared mutable data. The solution is to introduce extra constraints in the form of synchronization. Typical synchronization mechanisms include locks <ref> [Bir89b] </ref>, semaphores [Dij68], monitors [Hoa74], I-structures [ANP89], M-structures [Bar92], and Hughes's synch construct [Hug83, Hug84]. * Termination: A cobegin does not return until all its subprocesses have returned; this kind of behavior is typical of fork/join parallelism. But getting all the subprocesses to terminate can be tricky. <p> In particular, I will be concerned with the following two issues: 1. Non-Determinism: In the presence of side-effects, multi-threaded computations may be non-deterministic. Different interleavings of mutation operators can give rise to different results. Various atomicity and synchronization techniques (e.g., <ref> [Bir89b, Dij68, Bar92] </ref>) may be required to appropriately constrain behavior. 2. Storage Requirements: Multi-threaded computations have the potential for consuming much more storage than single-threaded ones. <p> They provide barrier synchro 298 CHAPTER 7. OPERA: CONTROLLING OPERATIONAL BEHAVIOR nization in a system where the number of processes participating in a rendezvous is determined dynamically. Excludons are just standard lock objects <ref> [Bir89b] </ref> that support mutual exclusion via the exclusive special form. Excludons are used to enforce atomic actions by elimi nating the undesirable nondeterminism introduced by concurrency. * Non-strictness: Opera supports lazy and eager evaluation via the (lazon E body ) and (eagon E body ) special forms. <p> Because all processes lose access to the synchron upon resumption, there can only be one rendezvous per synchron. The rendezvous protocol of synchrons sets it apart from other synchronization structures (e.g., semaphores [Dij68], locks <ref> [Bir89b] </ref>, synchronous messages [Hoa85], I-structures [ANP89], and M-structures [Bar92]). Synchronization typically involves some processes waiting in a suspended state for a shared synchronization entity to be released by the process that currently owns it. Traditional protocols supply explicit wait and release operations. <p> Excludons are similar to the locking mechanisms supplied in many concurrent systems (e.g. semaphores [Dij68], locks <ref> [Bir89b] </ref>, monitors [Hoa74]). Each call to the nullary excludon constructor creates a unique, first-class excludon object. The form (exclusive E excl E body ) first evaluates E excl , which should be an excludon x, and then evaluates E body while having exclusive hold on x.
Reference: [BL93] <author> Robert D. Blumofe and Charles E. Leiserson. </author> <title> Space-efficient scheduling of mul-tithreaded computations. </title> <booktitle> In 25th Annual ACM Symposium on Theory of Computing. ACM, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: In contrast, a worst-case multi-threaded computation can populate the entire trellis at once | this leads to space exponential in the depth of the trellis. This problem can be addressed by dynamic strategies (e.g., <ref> [BL93] </ref>) or by giving the programmer fine-grained control over parallelism (e.g., [GM84]). Although they permit parallelism, multi-threaded computations do not require it. It is always possible to sequentialize a multi-threaded computation by adding additional constraints that one subcall return before another initiates.
Reference: [Ble90] <author> Guy E. Blelloch. </author> <title> Vector Models for Data-Parallel Computing. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: In the data parallel literature, "scan" refers to a partial accumulator <ref> [Ble90, Sab88, GJSO92] </ref>. In Waters's series package, though, "scan" refers to a kind of generator [Wat90]. I adopt the former meaning here. 180 CHAPTER 5. SYNCHRONIZED LAZY AGGREGATES (a) DOWN-ACCUMULTE (b) DOWN-SCAN (c) UP-SCAN (d) TRUNCATE (e) SHIFT (f ) MAP2 5.2. <p> Gap filtering is used in situations where the location of elements is important. For example, languages with array-based or data-parallel features (e.g., <ref> [Ive87, Ame89, GJSO92, Sab88, Ble90, RS87, Ble92] </ref>) present models in which elements reside at a particular location in an data structure.
Reference: [Ble92] <author> Guy E. Blelloch. NESL: </author> <title> A nested data-parallel language. </title> <type> Technical Report CMU-CS-92-103, </type> <institution> Carnegie-Mellon University Computer Science Department, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: The aggregate data style has its roots in Lisp's list manipulation routines (as epitomized by mapcar) and APL's array operators. Today, the aggregate data approach is the main organizing principle for data parallel languages (e.g., Fortran 90 [Ame89], C* [RS87], NESL <ref> [Ble92] </ref>, paralations [Sab88]). <p> Gap filtering is used in situations where the location of elements is important. For example, languages with array-based or data-parallel features (e.g., <ref> [Ive87, Ame89, GJSO92, Sab88, Ble90, RS87, Ble92] </ref>) present models in which elements reside at a particular location in an data structure.
Reference: [Bud88] <author> Timothy Budd. </author> <title> An APL Compiler. </title> <publisher> Springer-Verlag, </publisher> <year> 1988. </year> <note> BIBLIOGRAPHY 437 </note>
Reference-contexts: Because programmers cannot depend on the transformations, they must seek other methods of controlling the space behavior of their programs. APL compilation techniques <ref> [GW78, Bud88] </ref> suffer from the same problem. The listlessness [Wad84, Wad85] and deforestation [Wad88, Chi92, GLJ93] techniques pioneered by Wadler do provide guarantees, but they are rather limited in applicability. Listlessness handles a subclass of list programs, but no trees.
Reference: [CBF91] <author> Siddhartha Chatterjee, Guy E. Blelloch, and Allan Fisher. </author> <title> Size and access inference for data-parallel programs. </title> <booktitle> In Programming Language Design and Implementation '91, </booktitle> <pages> pages 130-144. </pages> <publisher> ACM, </publisher> <year> 1991. </year>
Reference-contexts: For example, the same 10.3. FUTURE WORK 431 sequence of elements can be stored in a list, an array, or as the leaves of different kinds of trees; a shapely type describes the structure holding onto the sequence [Jay]. Size and access inference for data parallel programs <ref> [CBF91] </ref> is another line of work that strongly suggests a notion of process shapes. In his work on paralations [Sab88], Sabot described an intriguing notion of communication shape. 10.3.4 Synchronization Synchrons are a powerful synchronization mechanism that may have many other uses beyond their role in synchronized lazy aggregates.
Reference: [CG89] <author> Nicholas Carriero and David Gelernter. </author> <title> Linda in context. </title> <journal> Communications of the ACM, </journal> <volume> 32(4) </volume> <pages> 444-458, </pages> <year> 1989. </year>
Reference-contexts: THE SIGNAL PROCESSING STYLE OF PROGRAMMING [Bir89b, CM90], producer/consumer models (CLU iterators [L + 79], Id's I-structures and M-structures [ANP89, Bar92], Linda <ref> [CG89] </ref>) and dataflow ([Den75], [WA85], [DK82]). Below, we explore both coroutining and concurrent versions of the channel approach. <p> For example, rather than transmitting a tree over a single channel, why not transmit a tree over a tree of channels, where each channel transmits exactly one element? This is the approach taken by languages, such as Id [AN89] and Linda <ref> [CG89] </ref>, that exhibit producer/consumer parallelism. These languages support non-strict data structures 3.2. THE CHANNEL APPROACH 121 that can be manipulated by a program before their components have been computed. Any attempt to reference an uncomputed component results in a computation that is blocked until the component is actually there.
Reference: [Chi92] <author> Wei-Ngan Chin. </author> <title> Safe fusion of functional expressions. </title> <booktitle> In Symposium on Lisp and Functional Programming, </booktitle> <pages> pages 11-20. </pages> <publisher> ACM, </publisher> <year> 1992. </year>
Reference-contexts: Because programmers cannot depend on the transformations, they must seek other methods of controlling the space behavior of their programs. APL compilation techniques [GW78, Bud88] suffer from the same problem. The listlessness [Wad84, Wad85] and deforestation <ref> [Wad88, Chi92, GLJ93] </ref> techniques pioneered by Wadler do provide guarantees, but they are rather limited in applicability. Listlessness handles a subclass of list programs, but no trees. Deforestation can eliminate both lists and trees, but only in networks that exhibit no fan-out. <p> The implicit buffering provided by other aggregate data approaches makes it hard to control the storage profiles of modular programs. Existing list and tree removal techniques (such as listlessness [Wad84, Wad85], deforestation <ref> [Wad88, Chi92, GLJ93] </ref>, and other transformations [Bel86, Bir89a]) automatically remove some intermediate data structures, but they either are limited to restricted network topologies or do not guarantee that all intermediate structures go away. In con 6.1.
Reference: [CM90] <author> Eric Cooper and J. Gregory Morrisett. </author> <title> Adding threads to standard ML. </title> <type> Technical Report CMU-CS-90-186, </type> <institution> Carnegie Mellon University Computer Science Department, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Other examples of the channel approach include: communicating threads 6 Waters argues that such cycles make programs harder to understand, and therefore should be avoided at all costs. 106 CHAPTER 3. THE SIGNAL PROCESSING STYLE OF PROGRAMMING <ref> [Bir89b, CM90] </ref>, producer/consumer models (CLU iterators [L + 79], Id's I-structures and M-structures [ANP89, Bar92], Linda [CG89]) and dataflow ([Den75], [WA85], [DK82]). Below, we explore both coroutining and concurrent versions of the channel approach. <p> This assumption was made purely to simplify the presentation. As long as slivers obey the above requirements, nothing prevents them from being implemented in terms of nested loops or mutual recursions. A rendezvous between slivers resembles interprocess synchronization in many models of concurrent processes (e.g., <ref> [Hoa85, Mil89, CM90] </ref>). However, there are several aspects that distinguish sliver synchronization from these other models. * With slivers, communication and synchronization are decoupled. Communication is achieved by referencing a data structure, while synchronization is achieved by applying wait to a synchron.
Reference: [Coh83] <author> Norman H. Cohen. </author> <title> Eliminating redundant recursive calls. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(3) </volume> <pages> 265-299, </pages> <month> July </month> <year> 1983. </year>
Reference-contexts: Another valid recursive to iterative transformation is to use the cdr-bashing trick to replace (upQ init tcons synq) by (down-cons! init synq). This technique could be used to make iterative versions of append, copy, map, and map-list. A host of other transformations from the literature on program transformation <ref> [DR76, Dar82, Bac78, Bel86, Bir89a, Bir86, Bir88, Coh83, Str71, WS73, Pet84] </ref> find a convenient expression in terms of slivers. The above points indicate that, regardless of their appropriateness as an implementation language, slivers are a powerful way to design, specify, reason about, and explain programs.
Reference: [CR + 91] <editor> William Clinger, Jonathan Rees, et al. </editor> <title> Revised 4 report on the algorithmic language Scheme. Lisp Pointers, </title> <type> 4(3), </type> <year> 1991. </year>
Reference-contexts: It is also a commonly used technique in many other languages, especially functional and mostly functional ones (e.g., Haskell [HJW + 92], Id [AN89], Common Lisp [Ste90], Scheme <ref> [CR + 91] </ref>, ML [MTH90]). 3.1.1 Database Example: A List Implementation For the linear database example, lists are an obvious choice for the type of aggregate data structure. Figure 3.1 shows how each of the slivers can be represented as a list-manipulation procedure. <p> The higher order nature of many of the slivers (i.e., they accept functional arguments) allows them to be tailored to a wide variety of problems. The choice of core procedures was influenced by the 2 This evaluation strategy is not allowed in standard Scheme <ref> [CR + 91] </ref>, which requires that the subexpres-sions be evaluated in some sequential order. 6.1. LINEAR COMPUTATIONS 223 list operations of Lisp [Ste90, CR + 91] and Haskell 3 [HJW + 92], the array operations of APL [Ive87], the vector operations of FX [GJSO92], and Waters's series operations [Wat90]. <p> The choice of core procedures was influenced by the 2 This evaluation strategy is not allowed in standard Scheme [CR + 91], which requires that the subexpres-sions be evaluated in some sequential order. 6.1. LINEAR COMPUTATIONS 223 list operations of Lisp <ref> [Ste90, CR + 91] </ref> and Haskell 3 [HJW + 92], the array operations of APL [Ive87], the vector operations of FX [GJSO92], and Waters's series operations [Wat90]. <p> Then we explore how the features of Opera can be used to implement the Synapse slivers and slags illustrated in Chapter 6. 7.1 An Introduction to Opera Opera is a dialect of Scheme <ref> [ASS85, CR + 91] </ref> that supports various mechanisms for controlling the fine-grained operational details of programs. Figure 7.1 summarizes the syntax of the language and the primitive operations it supports. Both syntactically and semantically, 295 296 CHAPTER 7. <p> If there is ever a need to explicitly force a lazon, this can be done with the touch procedure, defined as follows: (define (touch x) (if x x x)) 5 The Scheme report <ref> [CR + 91] </ref> permits, but does not require, implicit forcing of the promises created by Scheme's delay. In contrast, Opera requires implicit touching of lazons. 7.1. <p> The syntactic sugar inherited from Scheme (except for begin) is expanded according to the rules given in <ref> [CR + 91] </ref>.
Reference: [Dar82] <author> John Darlington. </author> <title> Program transformation. </title> <editor> In J. Darlington, P. Henderson, and D. A. Turner, editors, </editor> <booktitle> Functional Programming and its Applications, </booktitle> <pages> pages 177-192. </pages> <year> 1982. </year>
Reference-contexts: The problem with existing transformation and compilation techniques is that they either provide no guarantees or they only work on a limited class of programs. Algebraic transformation techniques <ref> [DR76, Dar82, Bac78, Bel86, Bir89a, Bir86, Bir88] </ref> perform transformations like the above map removal, but systems that automatically apply these transformations do not guarantee that all intermediate data will be removed. <p> Another valid recursive to iterative transformation is to use the cdr-bashing trick to replace (upQ init tcons synq) by (down-cons! init synq). This technique could be used to make iterative versions of append, copy, map, and map-list. A host of other transformations from the literature on program transformation <ref> [DR76, Dar82, Bac78, Bel86, Bir89a, Bir86, Bir88, Coh83, Str71, WS73, Pet84] </ref> find a convenient expression in terms of slivers. The above points indicate that, regardless of their appropriateness as an implementation language, slivers are a powerful way to design, specify, reason about, and explain programs.
Reference: [DC90] <author> G. D. P. Dueck and G. V. Cormack. </author> <title> Modular attribute grammars. </title> <journal> The Computer Journal, </journal> <volume> 33(2) </volume> <pages> 164-172, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: However, in recent years, there has been a flurry of activity on modular attribute grammars, which strive to group all the computations of a given attribute into a modular unit <ref> [DC90, Ada91, FMY92, KW92, Wat92] </ref>.
Reference: [Den75] <author> Jack B. Dennis. </author> <title> First version of a data flow procedure language. Computation Structure Group Memo MIT/LCS/TM-61, </title> <institution> MIT Laboratory for Computer Science, </institution> <month> May </month> <year> 1975. </year>
Reference-contexts: Devices are connected by wires that specify the data dependences between procedures. Values can be thought of as flowing along the wires in the direction of the arrows. This picture is similar to graphical depictions of dataflow programs <ref> [Den75, DK82] </ref> except that here the devices can compute only once, and the wires can transmit only a single value token before they are "used up". <p> This is represented in the tile diagram by two subcall boundaries with an OR separator. This is an ad hoc and awkward way of "sharing" several subcalls along the subcall boundary lines. This situation could be improved by decomposing the conditional into split and join operations (e.g., see <ref> [Den75] </ref>) that would enable the tile to share a single subcall boundary among several potential calls. However, this "problem" is purely an issue of visual presentation; in terms of the computational model, all that matters is that a tile computation make at most one subcall. <p> Ashcroft describes a system that combines demand flow (via entities called questons) with data flow (via entities called datons) [Ash86]. The visual notations used to represent Edgar graphs were influenced by the notations used in the data flow community (e.g., <ref> [Den75, DK82] </ref>). We emphasize, however, that the execution models for Edgar and data flow are fundamentally different. In data flow models, tokens representing values flow through mostly static networks; the same wire can be used transmit many different values.
Reference: [Dij68] <author> E. W. Dijkstra. </author> <title> Co-operating sequential processes. </title> <editor> In F. Genuys, editor, </editor> <booktitle> Programming Languages (NATO Advanced Study Institute), </booktitle> <pages> pages 43-112. </pages> <address> London: </address> <publisher> Academic Press, </publisher> <year> 1968. </year>
Reference-contexts: This changes the input/ouput behavior of copy in a major way. Underconstrained control is a classic problem whenever concurrent processes communicate via shared mutable data. The solution is to introduce extra constraints in the form of synchronization. Typical synchronization mechanisms include locks [Bir89b], semaphores <ref> [Dij68] </ref>, monitors [Hoa74], I-structures [ANP89], M-structures [Bar92], and Hughes's synch construct [Hug83, Hug84]. * Termination: A cobegin does not return until all its subprocesses have returned; this kind of behavior is typical of fork/join parallelism. But getting all the subprocesses to terminate can be tricky. <p> In particular, I will be concerned with the following two issues: 1. Non-Determinism: In the presence of side-effects, multi-threaded computations may be non-deterministic. Different interleavings of mutation operators can give rise to different results. Various atomicity and synchronization techniques (e.g., <ref> [Bir89b, Dij68, Bar92] </ref>) may be required to appropriately constrain behavior. 2. Storage Requirements: Multi-threaded computations have the potential for consuming much more storage than single-threaded ones. <p> Because all processes lose access to the synchron upon resumption, there can only be one rendezvous per synchron. The rendezvous protocol of synchrons sets it apart from other synchronization structures (e.g., semaphores <ref> [Dij68] </ref>, locks [Bir89b], synchronous messages [Hoa85], I-structures [ANP89], and M-structures [Bar92]). Synchronization typically involves some processes waiting in a suspended state for a shared synchronization entity to be released by the process that currently owns it. Traditional protocols supply explicit wait and release operations. <p> Excludons are similar to the locking mechanisms supplied in many concurrent systems (e.g. semaphores <ref> [Dij68] </ref>, locks [Bir89b], monitors [Hoa74]). Each call to the nullary excludon constructor creates a unique, first-class excludon object. The form (exclusive E excl E body ) first evaluates E excl , which should be an excludon x, and then evaluates E body while having exclusive hold on x.
Reference: [DJL88] <author> Pierre Deransart, Martin Jordan, and Bernard Lorho. </author> <title> Atribute Grammars. </title> <publisher> Springer-Verlag, </publisher> <year> 1988. </year> <booktitle> Lecture Notes in Computer Science, Number 323. </booktitle>
Reference-contexts: Though originally intended for describing the semantics of programming languages based on their grammars, today they are mainly used for syntax-directed compilation techniques (see <ref> [DJL88] </ref> for an overview). Attribute grammars are related to slivers because (1) they are a language for describing a class of tree computations (2) they support a crude notion of shape. <p> Binary tile shapes essentially specify the dependencies between different parts of a tree computation. In this respect, they resemble attribute grammars, a declarative formalism in which programs can be specified in terms of the information dependencies between the nodes of a grammar-induced tree <ref> [DJL88] </ref>. (See Section 3.3.3 for a discussion of attribute grammars.) 4.2.5 Binary Down Tiles and Non-strictness We now return to the notion of binary down tiles. These are tiles whose up-both, up-left and up-right shards are all trivial. <p> While the combination rules mentioned in the previous paragraph are helpful heuristics, the notion of subtile shape defined earlier is too coarse for handling the nuances of deadlock in a reasonable way. Presumably, something more like the circularity detection analysis of attribute grammars <ref> [DJL88] </ref> is required here, but I will not explore this avenue. For simplicity, I will assume dynamic deadlock detection throughout the rest 5.3. THE STRUCTURE OF SYNCHRONIZED LAZY AGGREGATES 189 of this document. I conclude this section with the sketch of a proof that deadlock is undecidable. <p> Ideally, it should be possible to eliminate the need for run-time overhead completely by compiling many sliver networks into efficient monolithic recursive procedures. Techniques that are particularly worth exploring in this context include series compilation [Wat91], deforestation [Wad88], partial evaluation [WCRS91], and attribute grammar evaluation <ref> [DJL88] </ref>. Series compilation is based on representing a series operator as collection of code fragments that characterize different aspects of the operator; compilation consists of gluing together corresponding fragments for a network of series operators. This approach appears promising for slivers, but the details need to be worked out. <p> The dependency analysis underlying many attribute grammar systems <ref> [DJL88] </ref> is relevant to the design of the shape calculus. It may well turn out that the shape calculus amounts to a modular version of this analysis. Jay and others are currently developing a theory of "shapely types" that factors data structures into their shapes and their elements.
Reference: [DK82] <author> Alan L. Davis and Robert M. Keller. </author> <title> Data flow program graphs. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 26-41, </pages> <month> February </month> <year> 1982. </year>
Reference-contexts: Devices are connected by wires that specify the data dependences between procedures. Values can be thought of as flowing along the wires in the direction of the arrows. This picture is similar to graphical depictions of dataflow programs <ref> [Den75, DK82] </ref> except that here the devices can compute only once, and the wires can transmit only a single value token before they are "used up". <p> THE SIGNAL PROCESSING STYLE OF PROGRAMMING [Bir89b, CM90], producer/consumer models (CLU iterators [L + 79], Id's I-structures and M-structures [ANP89, Bar92], Linda [CG89]) and dataflow ([Den75], [WA85], <ref> [DK82] </ref>). Below, we explore both coroutining and concurrent versions of the channel approach. <p> Ashcroft describes a system that combines demand flow (via entities called questons) with data flow (via entities called datons) [Ash86]. The visual notations used to represent Edgar graphs were influenced by the notations used in the data flow community (e.g., <ref> [Den75, DK82] </ref>). We emphasize, however, that the execution models for Edgar and data flow are fundamentally different. In data flow models, tokens representing values flow through mostly static networks; the same wire can be used transmit many different values.
Reference: [DR76] <author> John Darlington and R.M.Burstall. </author> <title> A system which automatically improves programs. </title> <journal> Acta Informatica, </journal> <pages> pages 41-60, </pages> <year> 1976. </year>
Reference-contexts: The problem with existing transformation and compilation techniques is that they either provide no guarantees or they only work on a limited class of programs. Algebraic transformation techniques <ref> [DR76, Dar82, Bac78, Bel86, Bir89a, Bir86, Bir88] </ref> perform transformations like the above map removal, but systems that automatically apply these transformations do not guarantee that all intermediate data will be removed. <p> Another valid recursive to iterative transformation is to use the cdr-bashing trick to replace (upQ init tcons synq) by (down-cons! init synq). This technique could be used to make iterative versions of append, copy, map, and map-list. A host of other transformations from the literature on program transformation <ref> [DR76, Dar82, Bac78, Bel86, Bir89a, Bir86, Bir88, Coh83, Str71, WS73, Pet84] </ref> find a convenient expression in terms of slivers. The above points indicate that, regardless of their appropriateness as an implementation language, slivers are a powerful way to design, specify, reason about, and explain programs.
Reference: [FMY92] <author> R. Farrow, T. J. Marlowe, and D. M. Yellin. </author> <title> Composable attribute grammars: Support for modularity in translator design and implmentation. </title> <booktitle> In Nineteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 223-234, </pages> <year> 1992. </year> <note> 438 BIBLIOGRAPHY </note>
Reference-contexts: However, in recent years, there has been a flurry of activity on modular attribute grammars, which strive to group all the computations of a given attribute into a modular unit <ref> [DC90, Ada91, FMY92, KW92, Wat92] </ref>.
Reference: [For91] <author> Alessandro Forin. </author> <title> Futures. </title> <editor> In Peter Lee, editor, </editor> <booktitle> Topics in Advanced Language Implementation, </booktitle> <pages> pages 219-241. </pages> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: The evaluation of E proceeeds concurrently with the rest of the computation. Any context requiring the actual value of the placeholder will wait until the value is available. Par is equivalent to the future construct provided by many Lisps <ref> [Hal85, Mil87, For91] </ref>. * (synch E) returns an object that suspends the computation of E. Demand1 and demand2 are procedures that demand such an object to return the value of its suspended computation, but the computation is only initiated when both demand1 and demand2 have been called on the object.
Reference: [GJ90] <author> David Gelernter and Suresh Jagannathan. </author> <title> Programming Linguistics. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: This alternate strategy is consistent with the interpretation that a returned wire indicates that the node at the source of the wire is a value node. It also corresponds to the common practice in operational semantics of classifying a subset of rewritable expressions as values <ref> [Mey, GJ90] </ref>. An additional benefit is that it greatly simplifies the handling of quote, since a compound quoted expression can be treated as a single value created before the program is executed. 6 Unfortunately, the alternate strategy for handling values complicates the Edgar rewrite rules. <p> Edgar's explicit representation of demand was inspired by the demand tokens used in the operational model for Gelernter and Jaganathan's Ideal Software Machine (ISM) <ref> [GJ90] </ref>. ISM is a model that ascribes sequential and parallel characteristics to dimensions of both space and time. The concurrency allowed by ISM (parallelism in the time dimension) is formally expressed in a semantic framework that combines aspects of Petri nets [Pet77] and graph rewriting.
Reference: [GJSO92] <author> David Gifford, Pierre Jouvelot, Mark Sheldon, and James O'Toole. </author> <title> Report on the FX-91 programming language. </title> <type> Technical Report MIT/LCS/TR-531, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: In the data parallel literature, "scan" refers to a partial accumulator <ref> [Ble90, Sab88, GJSO92] </ref>. In Waters's series package, though, "scan" refers to a kind of generator [Wat90]. I adopt the former meaning here. 180 CHAPTER 5. SYNCHRONIZED LAZY AGGREGATES (a) DOWN-ACCUMULTE (b) DOWN-SCAN (c) UP-SCAN (d) TRUNCATE (e) SHIFT (f ) MAP2 5.2. <p> Gap filtering is used in situations where the location of elements is important. For example, languages with array-based or data-parallel features (e.g., <ref> [Ive87, Ame89, GJSO92, Sab88, Ble90, RS87, Ble92] </ref>) present models in which elements reside at a particular location in an data structure. <p> LINEAR COMPUTATIONS 223 list operations of Lisp [Ste90, CR + 91] and Haskell 3 [HJW + 92], the array operations of APL [Ive87], the vector operations of FX <ref> [GJSO92] </ref>, and Waters's series operations [Wat90]. In the remainder of this section, we will employ the core procedures to illustrate various properties of sliver decomposition within Synapse. 6.1.1 Iteration vs. <p> In addition to the usual Scheme datatypes and operations, Opera supports mutable one-slot data structures called cells. These correspond to the reference structures of ML [MTH90] and FX <ref> [GJSO92] </ref>. 7.1. AN INTRODUCTION TO OPERA 299 Opera does not support Scheme's continuations. In principle, Opera should ultimately be able to support continuations, but the details of how continuations interact with Opera's fine-grained concurrency have not yet been worked out. <p> The existence of a large corpus of tree examples would help in the design of better filtering constructs. Cyclic dependencies are another area requiring further investigation. Some problems naturally decompose into components that exhibit cyclic dependencies. For example, in the FX language <ref> [GJSO92] </ref>, name resolution and type reconstruction are conceptually distinct stages of the implementation that happen to depend on each other. In practice, the implementations of these stages must be manually interwoven.
Reference: [GLJ93] <author> Andrew Gill, John Launchbury, and Simon L. Peyton Jones. </author> <title> A short cut to deforestation. </title> <booktitle> In Functional Programming and Computer Architecture, </booktitle> <year> 1993. </year>
Reference-contexts: Because programmers cannot depend on the transformations, they must seek other methods of controlling the space behavior of their programs. APL compilation techniques [GW78, Bud88] suffer from the same problem. The listlessness [Wad84, Wad85] and deforestation <ref> [Wad88, Chi92, GLJ93] </ref> techniques pioneered by Wadler do provide guarantees, but they are rather limited in applicability. Listlessness handles a subclass of list programs, but no trees. Deforestation can eliminate both lists and trees, but only in networks that exhibit no fan-out. <p> The implicit buffering provided by other aggregate data approaches makes it hard to control the storage profiles of modular programs. Existing list and tree removal techniques (such as listlessness [Wad84, Wad85], deforestation <ref> [Wad88, Chi92, GLJ93] </ref>, and other transformations [Bel86, Bir89a]) automatically remove some intermediate data structures, but they either are limited to restricted network topologies or do not guarantee that all intermediate structures go away. In con 6.1.
Reference: [GM84] <author> Richard P. Gabriel and John McCarthy. </author> <booktitle> Queue-based multi-processing Lisp. In Symposium on Lisp and Functional Programming, </booktitle> <pages> pages 25-44. </pages> <publisher> ACM, </publisher> <month> August </month> <year> 1984. </year>
Reference-contexts: In contrast, a worst-case multi-threaded computation can populate the entire trellis at once | this leads to space exponential in the depth of the trellis. This problem can be addressed by dynamic strategies (e.g., [BL93]) or by giving the programmer fine-grained control over parallelism (e.g., <ref> [GM84] </ref>). Although they permit parallelism, multi-threaded computations do not require it. It is always possible to sequentialize a multi-threaded computation by adding additional constraints that one subcall return before another initiates.
Reference: [GW78] <author> Leo J. Guibas and Douglas K. Wyatt. </author> <title> Compilation and delayed evaluation in APL. </title> <booktitle> In Conference Record of the Fifth ACM Conference on the Principles of Programming Languages, </booktitle> <pages> pages 1-8, </pages> <year> 1978. </year>
Reference-contexts: Because programmers cannot depend on the transformations, they must seek other methods of controlling the space behavior of their programs. APL compilation techniques <ref> [GW78, Bud88] </ref> suffer from the same problem. The listlessness [Wad84, Wad85] and deforestation [Wad88, Chi92, GLJ93] techniques pioneered by Wadler do provide guarantees, but they are rather limited in applicability. Listlessness handles a subclass of list programs, but no trees.
Reference: [HA87] <author> Paul Hudak and Steve Anderson. </author> <title> Pomset interpretation of parallel functional programs. </title> <booktitle> In Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 234-256, </pages> <month> September </month> <year> 1987. </year> <booktitle> Lecture Notes in Computer Science, Number 274. </booktitle>
Reference-contexts: Indeed, we will see in Chapter 7 that non-strictness is an essential technique for modularizing computations. However, because it can change timing relationships among calls and other operations, non-strictness complicates reasoning about programs (e.g., see <ref> [HA87] </ref>). For example, consider a sequential tile in which a non-strict operator links the result of one subcall to the argument of the other subcall. <p> In this way, strictness effectively manages the interleaving of operations from separate idioms. In contrast, non-strict strategies (lazy and eager evaluation) decouple the time-based ordering of the argument computations from the time of the procedure application (see <ref> [HA87] </ref>). While non-strictness is a powerful and useful language feature ([Hug90, Tra88, Hal85, Mil87]), the lack of effective barriers thwarts efforts to reason about operational details like storage requirements and operation order. <p> Edgar seems to be a natural starting point for a richer notion process equivalence that explicitly models space consumption. The hard part is defining what "modulo management operations" means in this context. Pomset models of concurrent computation <ref> [Pra86, HA87] </ref> may be helpful for defining an Edgar-based notion of process equivalence. 10.3.6 Pedagogy The computational models and tools developed for this work have important pedagogical applications.
Reference: [Hal85] <author> Robert Halstead. </author> <title> Multilisp: A language for concurrent symbolic computation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <pages> pages 501-528, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: The evaluation of E proceeeds concurrently with the rest of the computation. Any context requiring the actual value of the placeholder will wait until the value is available. Par is equivalent to the future construct provided by many Lisps <ref> [Hal85, Mil87, For91] </ref>. * (synch E) returns an object that suspends the computation of E. Demand1 and demand2 are procedures that demand such an object to return the value of its suspended computation, but the computation is only initiated when both demand1 and demand2 have been called on the object. <p> This implies that tree elements may continue to print after the top-level call to the tree-printing tile has returned! events in a binary tile. The ability of a non-strict computation to return before completing its computation is a powerful feature for expressing parallelism and speculative computation (see <ref> [Tra88, Hal85, Mil87] </ref>). Indeed, we will see in Chapter 7 that non-strictness is an essential technique for modularizing computations. However, because it can change timing relationships among calls and other operations, non-strictness complicates reasoning about programs (e.g., see [HA87]).
Reference: [Hen80] <author> Peter Henderson. </author> <title> Functional Programming: Application and Implementation. </title> <publisher> Prentice-Hall, </publisher> <year> 1980. </year>
Reference-contexts: Both forms create a placeholder object that stands for the result of evaluating E body . In the case of lazons, evaluation of E body is delayed until the lazon appears in a context that requires its value. Lazons are like the delayed objects described in <ref> [Hen80] </ref> and [ASS85] except that they are implicity forced rather than explicitly forced; we shall see how this difference enhances modularity. <p> Non-touching contexts, such as the operand positions of a pcall and the argument positions of a cons, do not probe any details of the lazon's body, and simply pass around the lazon instead. Lazons are similar to the delayed values described in <ref> [Hen80] </ref> and [ASS85], except that delayed values are touched by application of an explicit force procedure. 5 In Opera, the forcing is done implicitly by a touching context. <p> The arguments will only be evaluated if they are needed within the body of the called procedure. Lcall corresponds to the call-by-need parameter passing mechanism common in functional programming languages (e.g. Haskell [HJW + 92] and Miranda [Tur85]). Like the delayed objects of <ref> [Hen80] </ref> and [ASS85], lazons can be used to create conceptually infinite data structures and finesse certain kinds of circular dependencies.
Reference: [HJW + 92] <editor> Paul Hudak, Simon Petyon Jones, Philip Wadler, et al. </editor> <title> Report on the programming language Haskell, version 1.2. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 27(5), </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: Today, the aggregate data approach is the main organizing principle for data parallel languages (e.g., Fortran 90 [Ame89], C* [RS87], NESL [Ble92], paralations [Sab88]). It is also a commonly used technique in many other languages, especially functional and mostly functional ones (e.g., Haskell <ref> [HJW + 92] </ref>, Id [AN89], Common Lisp [Ste90], Scheme [CR + 91], ML [MTH90]). 3.1.1 Database Example: A List Implementation For the linear database example, lists are an obvious choice for the type of aggregate data structure. <p> The choice of core procedures was influenced by the 2 This evaluation strategy is not allowed in standard Scheme [CR + 91], which requires that the subexpres-sions be evaluated in some sequential order. 6.1. LINEAR COMPUTATIONS 223 list operations of Lisp [Ste90, CR + 91] and Haskell 3 <ref> [HJW + 92] </ref>, the array operations of APL [Ive87], the vector operations of FX [GJSO92], and Waters's series operations [Wat90]. In the remainder of this section, we will employ the core procedures to illustrate various properties of sliver decomposition within Synapse. 6.1.1 Iteration vs. <p> The arguments will only be evaluated if they are needed within the body of the called procedure. Lcall corresponds to the call-by-need parameter passing mechanism common in functional programming languages (e.g. Haskell <ref> [HJW + 92] </ref> and Miranda [Tur85]). Like the delayed objects of [Hen80] and [ASS85], lazons can be used to create conceptually infinite data structures and finesse certain kinds of circular dependencies.
Reference: [Hoa74] <author> C.A.R. Hoare. </author> <title> Monitors: An operating system structuring concept. </title> <journal> Communications of the ACM, </journal> <volume> 17(10) </volume> <pages> 549-557, </pages> <year> 1974. </year>
Reference-contexts: This changes the input/ouput behavior of copy in a major way. Underconstrained control is a classic problem whenever concurrent processes communicate via shared mutable data. The solution is to introduce extra constraints in the form of synchronization. Typical synchronization mechanisms include locks [Bir89b], semaphores [Dij68], monitors <ref> [Hoa74] </ref>, I-structures [ANP89], M-structures [Bar92], and Hughes's synch construct [Hug83, Hug84]. * Termination: A cobegin does not return until all its subprocesses have returned; this kind of behavior is typical of fork/join parallelism. But getting all the subprocesses to terminate can be tricky. <p> Excludons are similar to the locking mechanisms supplied in many concurrent systems (e.g. semaphores [Dij68], locks [Bir89b], monitors <ref> [Hoa74] </ref>). Each call to the nullary excludon constructor creates a unique, first-class excludon object. The form (exclusive E excl E body ) first evaluates E excl , which should be an excludon x, and then evaluates E body while having exclusive hold on x.
Reference: [Hoa85] <author> C.A.R. Hoare. </author> <title> Communicating Sequential Processes. </title> <publisher> Prentice-Hall, </publisher> <year> 1985. </year>
Reference-contexts: In the channel approach, processes are usually assumed to be independent threads of control. They may be executing concurrently, or they may be coroutining in some fashion. The channel approach is supported by numerous languages and systems. Hoare's CSP is the canonical version of this approach <ref> [Hoa85] </ref>; Unix pipes [KP84] is one of the most widely used. Other examples of the channel approach include: communicating threads 6 Waters argues that such cycles make programs harder to understand, and therefore should be avoided at all costs. 106 CHAPTER 3. <p> This is the approach adopted by CSP <ref> [Hoa85] </ref>. A rendezvous-based version of mean-age conc would be guaranteed to require only constant space. <p> Like lazy aggregates, slags are compound, potentially tree-shaped, data structures whose parts are not computed until they are required. Like synchronous communication channels, slags synchronize separate threads of control and manage inter-process storage resources. (However, unlike many other synchronization models (e.g. <ref> [Hoa85, Mil89] </ref>), slags decouple communication and synchronization.) 5.3.2 Synquences and Syndrites For simplicity, we will focus on two particular kinds of slags: synquences and syndrites. A synquence (synchronized sequence) is a synchronized lazy list while a syndrite (synchronized dendrite) is a synchronized lazy tree. <p> This assumption was made purely to simplify the presentation. As long as slivers obey the above requirements, nothing prevents them from being implemented in terms of nested loops or mutual recursions. A rendezvous between slivers resembles interprocess synchronization in many models of concurrent processes (e.g., <ref> [Hoa85, Mil89, CM90] </ref>). However, there are several aspects that distinguish sliver synchronization from these other models. * With slivers, communication and synchronization are decoupled. Communication is achieved by referencing a data structure, while synchronization is achieved by applying wait to a synchron. <p> This approach contrasts with models in which every communica tion event synchronizes sender and receiver. * Slivers engage in a multiway rendezvous that involves all the slivers in a lock step component. Most synchronous communication models support only a two-way rendezvous. While CSP <ref> [Hoa85] </ref> supports a multiway rendezvous, it is limited to communication between a single sender and multiple receivers. * Since slags carry both down and up synchrons, slivers can naturally express computations (including tree-shaped ones) that can rendezvous upon return from a call return as well as initiaion of the call. <p> Because all processes lose access to the synchron upon resumption, there can only be one rendezvous per synchron. The rendezvous protocol of synchrons sets it apart from other synchronization structures (e.g., semaphores [Dij68], locks [Bir89b], synchronous messages <ref> [Hoa85] </ref>, I-structures [ANP89], and M-structures [Bar92]). Synchronization typically involves some processes waiting in a suspended state for a shared synchronization entity to be released by the process that currently owns it. Traditional protocols supply explicit wait and release operations. <p> Edgar provides a formal model for their execution; can it help to prove them correct? * Behavioral Equivalence: I have claimed that sliver networks are operationally equivalent to monolithic programs "modulo management operations". Classical approaches to process equivalence (like CSP's trace equivalence <ref> [Hoa85] </ref>) appear too weak to handle the crucial notion of equivalent space behavior. Edgar seems to be a natural starting point for a richer notion process equivalence that explicitly models space consumption. The hard part is defining what "modulo management operations" means in this context.
Reference: [HS86a] <author> W. Daniel Hillis and Guy L. Steele Jr. </author> <title> Data parallel algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 29(12), </volume> <year> 1986. </year>
Reference-contexts: In systems that provide real parallelism (i.e., many physical processors), it is often possible to reduce overall computing resources by wasting or repeating some computation. (The data parallel techniques discussed in <ref> [HS86a] </ref> are an excellent example of this phenomenon.) In such systems, demand-driven techniques can reduce performance rather than improving it. However, in systems providing only simulated parallelism (i.e., a single processor), demand-driven techniques may still be advantageous.
Reference: [HS86b] <author> Paul Hudak and Lauren Smith. </author> <title> Para-functional programming: A paradigm for programming multiprocessor systems. </title> <booktitle> In Thirteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 243-254, </pages> <month> January </month> <year> 1986. </year> <note> BIBLIOGRAPHY 439 </note>
Reference-contexts: For example, 1 may be printed before both the 3 and 4 or after both the 3 and 4 but never between the 3 and 4. Scheme allows only eight possible orderings for the numbers displayed by (test-order): 1 See <ref> [HS86b] </ref> for work along these lines. 2 For simplicity, we will often illustrate issues of evaluation order with contrived examples that involve I/O or assignment. However, in practice, we are more interested in the relative order of purely functional operations.
Reference: [Hug82] <author> R. J. M. Hughes. Super-combinators: </author> <title> A new implementation technique for applicative languages. </title> <booktitle> In Symposium on Lisp and Functional Programming, </booktitle> <pages> pages 1-10, </pages> <month> August </month> <year> 1982. </year>
Reference-contexts: Graph reduction has long been used as an implementation technique for functional programming languages. Turner [Tur79] developed a means for compiling functional programs into graphs of three combinators (S, K, and I) and executing the resulting graphs by graph rewriting. Hughes <ref> [Hug82] </ref> introduced a method, super-combinators, for compiling a function into a specialized graph rewriting rule. Peyton Jones [Pey87] summarizes numerous issues relevant to graph reduction as an implementation technique.
Reference: [Hug83] <author> R. J. M. Hughes. </author> <title> The Design and Implementation of Programming Languages. </title> <type> PhD thesis, </type> <institution> Oxford Universiy Computing Laboratory, Programming Research Group, </institution> <month> July </month> <year> 1983. </year>
Reference-contexts: The problem has its roots in a fundamental mismatch between demand-driven evaluation and the fan-out of results from device outputs. As far as I know, only Waters [Wat91] and Hughes <ref> [Hug83, Hug84] </ref> have provided partial solutions to this problem within aggregate data approach. (In channel-based mechanisms that support fan-out, the space problem is often solved by bounded channels.) * Excessive Time Overhead: For a variety of reasons, SPS programs can require significantly more time than their monolithic counterparts. <p> In order to make sliver decomposition a practical alternative 1.3. ALTERNATE PERSPECTIVES ON THIS RESEARCH 41 to monolithic recursions, it will be necessary to develop series-like static analysis and compilation techniques for sliver programs. 5. Abstracting Over Hughes's Ideas : In his dissertation <ref> [Hug83] </ref> and an important but little-known paper [Hug84] 2 , Hughes explains why concurrency and synchronization are necessary for preserving the space characteristics of a monolithic program in a modular SPS program. <p> Hughes's Approach Hughes improved the lazy data approach by supplying mechanisms that can guarantee desirable space behavior even for networks exhibiting fan-out <ref> [Hug83, Hug84] </ref>. He observed that in programs like mean-age, a constant space computation can only be achieved if the arguments to / are somehow computed together in lock step. <p> Underconstrained control is a classic problem whenever concurrent processes communicate via shared mutable data. The solution is to introduce extra constraints in the form of synchronization. Typical synchronization mechanisms include locks [Bir89b], semaphores [Dij68], monitors [Hoa74], I-structures [ANP89], M-structures [Bar92], and Hughes's synch construct <ref> [Hug83, Hug84] </ref>. * Termination: A cobegin does not return until all its subprocesses have returned; this kind of behavior is typical of fork/join parallelism. But getting all the subprocesses to terminate can be tricky. <p> The hard part is designing a mechanism that accomplishes it. We can argue from first principles the important properties that such a mechanism must possess: * Concurrency: As Hughes has shown, any sequential evaluation strategy is insufficient for achieving desired space behavior in certain networks with fan-out <ref> [Hug83, Hug84] </ref>. The lock step model requires that the slivers are somehow executing concurrently. * Synchronization: Concurrency allows the desired behavior but it doesn't necessarily guarantee it. Some form of synchronization is required to get the effect of gluing the call boundaries together. <p> In this respect, synchrons resemble weak pairs [Mil87] and populations [RAM83], data structures whose behavior is tied to storage management. Although developed independently, synchrons are closely related to Hughes's synch construct <ref> [Hug83, Hug84] </ref>. (See Section 3.1.5 for a discussion of synch.) Both synch and synchrons involve objects that define a rendezvous point. Hughes's objects serve as a rendezvous for exactly two requests for a value. <p> Is it possible to implement synchrons more efficiently? Synchrons are an example of a growing number of data structures that interact with garbage collection. Other examples include T's populations [RAM83], MultiScheme's pairs and finalization objects [Mil87], and Hughes's synch construct <ref> [Hug83, Hug84] </ref>.
Reference: [Hug84] <author> R. J. M. Hughes. </author> <title> Parallel functional languages use less space. </title> <type> Technical report, </type> <institution> Oxford University Programming Research Group, </institution> <year> 1984. </year>
Reference-contexts: The problem has its roots in a fundamental mismatch between demand-driven evaluation and the fan-out of results from device outputs. As far as I know, only Waters [Wat91] and Hughes <ref> [Hug83, Hug84] </ref> have provided partial solutions to this problem within aggregate data approach. (In channel-based mechanisms that support fan-out, the space problem is often solved by bounded channels.) * Excessive Time Overhead: For a variety of reasons, SPS programs can require significantly more time than their monolithic counterparts. <p> The interaction between demand-driven evaluation and fan-out requires some form of concurrency to prevent spurious storage leaks (see <ref> [Hug84] </ref>). Experience with sliver decomposition suggests that concurrency is an essential technique for 38 CHAPTER 1. OVERVIEW 1.3. ALTERNATE PERSPECTIVES ON THIS RESEARCH 39 expressing programs in a modular fashion. * Synchronization: The lock step processing of sliver networks is achieved by synchrons, a novel synchronization technology. <p> ALTERNATE PERSPECTIVES ON THIS RESEARCH 41 to monolithic recursions, it will be necessary to develop series-like static analysis and compilation techniques for sliver programs. 5. Abstracting Over Hughes's Ideas : In his dissertation [Hug83] and an important but little-known paper <ref> [Hug84] </ref> 2 , Hughes explains why concurrency and synchronization are necessary for preserving the space characteristics of a monolithic program in a modular SPS program. He introduces concurrency and synchronization constructs that can be thought of as annotations for controlling the operational behavior of a functional program. <p> Hughes's Approach Hughes improved the lazy data approach by supplying mechanisms that can guarantee desirable space behavior even for networks exhibiting fan-out <ref> [Hug83, Hug84] </ref>. He observed that in programs like mean-age, a constant space computation can only be achieved if the arguments to / are somehow computed together in lock step. <p> Underconstrained control is a classic problem whenever concurrent processes communicate via shared mutable data. The solution is to introduce extra constraints in the form of synchronization. Typical synchronization mechanisms include locks [Bir89b], semaphores [Dij68], monitors [Hoa74], I-structures [ANP89], M-structures [Bar92], and Hughes's synch construct <ref> [Hug83, Hug84] </ref>. * Termination: A cobegin does not return until all its subprocesses have returned; this kind of behavior is typical of fork/join parallelism. But getting all the subprocesses to terminate can be tricky. <p> The hard part is designing a mechanism that accomplishes it. We can argue from first principles the important properties that such a mechanism must possess: * Concurrency: As Hughes has shown, any sequential evaluation strategy is insufficient for achieving desired space behavior in certain networks with fan-out <ref> [Hug83, Hug84] </ref>. The lock step model requires that the slivers are somehow executing concurrently. * Synchronization: Concurrency allows the desired behavior but it doesn't necessarily guarantee it. Some form of synchronization is required to get the effect of gluing the call boundaries together. <p> The synchronization information propagated by slags guarantees that all the slivers work in concert; one cannot race ahead or lag behind the others. In this respect, synquences resemble interprocess communication channels with bounded buffering. Waters [Wat91] and Hughes <ref> [Hug84] </ref> also extend the aggregate data approach with forms of synchronization in order to achieve this effect. <p> In this respect, synchrons resemble weak pairs [Mil87] and populations [RAM83], data structures whose behavior is tied to storage management. Although developed independently, synchrons are closely related to Hughes's synch construct <ref> [Hug83, Hug84] </ref>. (See Section 3.1.5 for a discussion of synch.) Both synch and synchrons involve objects that define a rendezvous point. Hughes's objects serve as a rendezvous for exactly two requests for a value. <p> For several years I unsuccessfully attempted to develop sliver-like modularity mechanisms in sequential models. Eventually, it became apparent that concurrency was necessary for controlling space behavior operation order in a modular way. I was later pleased to find that Hughes <ref> [Hug84] </ref> had come to a similar conclusion a decade ago! In retrospect, the notion that concurrency is important to modularity has been around for a long time; it's just not emphasized enough. <p> Is it possible to implement synchrons more efficiently? Synchrons are an example of a growing number of data structures that interact with garbage collection. Other examples include T's populations [RAM83], MultiScheme's pairs and finalization objects [Mil87], and Hughes's synch construct <ref> [Hug83, Hug84] </ref>.
Reference: [Hug85] <author> R. J. M. Hughes. </author> <title> Lazy memo-functions. </title> <booktitle> In Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 129-146, </pages> <year> 1985. </year> <booktitle> Lecture Notes in Computer Science, Number 201. </booktitle>
Reference-contexts: SLAGS 6.1.3 Laziness In the case of synquences, the `lazy" in "synchronized lazy aggregates" means that neither the element nor the tail of a synquence is computed until it is actually needed. (In comparison, Scheme streams have lazy tails but not lazy elements.) Additionally, both elements and tails are memoized <ref> [ASS85, Hug85] </ref> so that they are only computed once if they are computed at all. Laziness leads to two important features of synquences: 1. Synquences may be conceptually infinite. 2. An element is not computed if it never used.
Reference: [Hug90] <author> R. J. M. Hughes. </author> <title> Why functional programming matters. </title> <editor> In David Turner, editor, </editor> <booktitle> Research Topics in Functional Programming, </booktitle> <pages> pages 17-42. </pages> <publisher> Addison Wesley, </publisher> <year> 1990. </year>
Reference-contexts: Furthermore, the straightforward aggregate data technique fails totally in cases where the aggregate structures are conceptually infinite. Infinite data structures can be a powerful way to modularize programs (see [ASS85]). An excellent example <ref> [Hug90] </ref> is decomposing a game program into a part that generates a game tree, and a part that examines the game tree. This supports modularity because the game tree generator can be designed as an independent unit without regard to the particular ways in which it will examined.
Reference: [Ive87] <author> Kenneth E. Iverson. </author> <title> A dictionary of APL. </title> <journal> APL QUOTE QUAD, </journal> <volume> 18(1) </volume> <pages> 5-40, </pages> <month> September </month> <year> 1987. </year>
Reference-contexts: Gap filtering is used in situations where the location of elements is important. For example, languages with array-based or data-parallel features (e.g., <ref> [Ive87, Ame89, GJSO92, Sab88, Ble90, RS87, Ble92] </ref>) present models in which elements reside at a particular location in an data structure. <p> LINEAR COMPUTATIONS 223 list operations of Lisp [Ste90, CR + 91] and Haskell 3 [HJW + 92], the array operations of APL <ref> [Ive87] </ref>, the vector operations of FX [GJSO92], and Waters's series operations [Wat90]. In the remainder of this section, we will employ the core procedures to illustrate various properties of sliver decomposition within Synapse. 6.1.1 Iteration vs.
Reference: [Jay] <author> C. Barry Jay. </author> <type> Personal correspondence. </type>
Reference-contexts: For example, the same 10.3. FUTURE WORK 431 sequence of elements can be stored in a list, an array, or as the leaves of different kinds of trees; a shapely type describes the structure holding onto the sequence <ref> [Jay] </ref>. Size and access inference for data parallel programs [CBF91] is another line of work that strongly suggests a notion of process shapes.
Reference: [JG89] <author> Pierre Jouvelot and David K. Gifford. </author> <title> Communication effects for message-based concurrency. </title> <type> Technical Report MIT/LCS/TM-386, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> February </month> <year> 1989. </year>
Reference-contexts: Since receive! works by side effect, two consumers sharing the same channel will not see the same sequence of values. Copying the values to two separate channels decouples the 9 Except for a few cosmetic changes changes, these constructs are the ones described in <ref> [JG89] </ref>. 3.2.
Reference: [Joh85] <author> Thomas Johnsson. </author> <title> Lambda lifting: Transforming programs to recursive equations. </title> <booktitle> In Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 190-203, </pages> <month> September </month> <year> 1985. </year> <booktitle> Lecture Notes in Computer Science, Number 201. </booktitle>
Reference-contexts: EDGAR: EXPLICIT DEMAND GRAPH REDUCTION 8.3. COMPILING OPERA INTO EDGAR 385 at the point of call. This strategy for managing free variables as implicit arguments is a graphical version of the lambda lifting technique for transforming nested procedures into top-level ones <ref> [Joh85] </ref>. An important detail of the lambda compilation process is that the implicit parameters to a procedure are only the free variables that appear textually inside of it. A proc node only holds onto values that are likely to be used in the computation of the template.
Reference: [Joh87] <author> Thomas Johnsson. </author> <title> Attribute grammars as a functional programming paradigm. </title> <booktitle> In Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 154-173, </pages> <year> 1987. </year> <booktitle> Lecture Notes in Computer Science, Number 274. </booktitle>
Reference-contexts: Indeed, it is possible to view attribute grammars as defining recursive functions <ref> [Joh87] </ref> or procedures [Kat84]. Classical attribute grammars suffer from a lack of modularity because they distribute the specification of attribute computations across all the different types of tree nodes. <p> In this case, a down-left shard is impossible because its 2 I am assuming here that all operations performed in a tile computation are themselves strict. In the presence of lazy operators, cyclic dependencies are not only possible but often desirable (e.g., see [Bir84], <ref> [Joh87] </ref>). Later, I will introduce a form of laziness that, if used indiscriminantly, would invalidate the claims made about the impossibility of mutually dependent tiles and the execution order of sequential tiles. However, I will carefully restrict laziness in order to preserve these claims. 4.2.
Reference: [K + 86] <author> David Kranz et al. </author> <title> Orbit: An optimizing compiler for Scheme. </title> <booktitle> In Proceedings of SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <pages> pages 219-233. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1986. </year>
Reference-contexts: Assignment conversion is one aspect of name removal. The assignment conversion phase, A exp , transforms Opera expressions to OK expressions by removing all assignments | i.e., expressions of the form (set! I E). Assignment conversion is described in <ref> [K + 86] </ref>. The basic idea behind assignment conversion is to transform all assignments into mutation operations on cells (mutable one-slot data structures).
Reference: [Kat84] <author> Takuya Katayama. </author> <title> Translation of attribute grammars into procedures. </title> <journal> Transactions on Programming Languages and Systems, </journal> <volume> 6(3) </volume> <pages> 345-369, </pages> <year> 1984. </year>
Reference-contexts: Indeed, it is possible to view attribute grammars as defining recursive functions [Joh87] or procedures <ref> [Kat84] </ref>. Classical attribute grammars suffer from a lack of modularity because they distribute the specification of attribute computations across all the different types of tree nodes.
Reference: [Knu68] <author> Donald E. Knuth. </author> <title> Semantics of context-free languages. </title> <journal> Mathematical Systems Theory, </journal> <volume> 2(2) </volume> <pages> 127-145, </pages> <year> 1968. </year>
Reference-contexts: Designing such a macro would be an interesting project. 124 CHAPTER 3. THE SIGNAL PROCESSING STYLE OF PROGRAMMING 3.3.3 Attribute Grammars Attribute grammars are a formalism invented by Knuth <ref> [Knu68] </ref> for declaratively specifying the decoration of tree nodes with named attributes. Though originally intended for describing the semantics of programming languages based on their grammars, today they are mainly used for syntax-directed compilation techniques (see [DJL88] for an overview).
Reference: [Knu73] <author> Donald E. Knuth. </author> <booktitle> The Art of Computer Programming. 2nd ed. </booktitle> <volume> Vol. 1: </volume> <booktitle> Fundamental algorithms. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1973. </year> <note> 440 BIBLIOGRAPHY </note>
Reference-contexts: This is only one instance of the in-order strategy, but it has the nice property that for binary syndrites it corresponds to the traditional notion of in-order processing on binary trees <ref> [Knu73] </ref>. 2. The direction of a sequential traversal specifies the order in which a node's children are processed. If the children are assumed to have a default left-to-right ordering, then the direction can be specified as a permutation on this ordering.
Reference: [Kos84] <author> Kai Koskimes. </author> <title> A specification language for one-pass semantic analysis. </title> <booktitle> In ACM SIGPLAN '84 Symposium on Compiler Construction, </booktitle> <pages> pages 179-189, </pages> <address> Tama-City, Tokyo, </address> <month> June </month> <year> 1984. </year>
Reference-contexts: SUMMARY 125 on it. * The declarative nature of attribute grammars can make it difficult to predict how many passes the attribute computation will make over the tree. (There are some frameworks, such as one-pass attribute grammars <ref> [Kos84] </ref>, that do limit the number of passes). * Attribute grammar formalisms are usually not designed to express general tree computations; they are typically tied to parsing technology and are used mainly to specify language implementations. * Modular attribute grammar formalisms typically involve ad hoc mechanisms for specifying component connectivity.
Reference: [KP84] <author> Brian W. Kernighan and Rob Pike. </author> <title> The UNIX Programming Environment. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1984. </year>
Reference-contexts: In the channel approach, processes are usually assumed to be independent threads of control. They may be executing concurrently, or they may be coroutining in some fashion. The channel approach is supported by numerous languages and systems. Hoare's CSP is the canonical version of this approach [Hoa85]; Unix pipes <ref> [KP84] </ref> is one of the most widely used. Other examples of the channel approach include: communicating threads 6 Waters argues that such cycles make programs harder to understand, and therefore should be avoided at all costs. 106 CHAPTER 3.
Reference: [KW92] <author> U. Kastens and W. M. Waite. </author> <title> Modularity and reusability in attribute grammars. </title> <type> Technical Report CU-CS-613-92, </type> <institution> University of Colorado at Boulder, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: However, in recent years, there has been a flurry of activity on modular attribute grammars, which strive to group all the computations of a given attribute into a modular unit <ref> [DC90, Ada91, FMY92, KW92, Wat92] </ref>.
Reference: [L + 79] <author> Barbara Liskov et al. </author> <title> CLU reference manual. </title> <type> Technical Report MIT/LCS/TR-225, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> October </month> <year> 1979. </year>
Reference-contexts: Other examples of the channel approach include: communicating threads 6 Waters argues that such cycles make programs harder to understand, and therefore should be avoided at all costs. 106 CHAPTER 3. THE SIGNAL PROCESSING STYLE OF PROGRAMMING [Bir89b, CM90], producer/consumer models (CLU iterators <ref> [L + 79] </ref>, Id's I-structures and M-structures [ANP89, Bar92], Linda [CG89]) and dataflow ([Den75], [WA85], [DK82]). Below, we explore both coroutining and concurrent versions of the channel approach.
Reference: [LG88] <author> John M. Lucassen and David K. Gifford. </author> <title> Polymorphic effect systems. </title> <booktitle> In Proceedings of the ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 47-57, </pages> <year> 1988. </year>
Reference-contexts: If dynamic overheads are deemed allowable, then other forms of static analysis (in addition to the compilation techniques mentioned above) can be used to reduce these overheads. For example, using the technique of effect analysis <ref> [LG88] </ref>, it may be possible to distinguish expressions that need to be evaluated concurrently from those for which sequential evaluation is sufficient. 430 CHAPTER 10. CONCLUSION 10.3.3 Computational Shape The notions of computational shape introduced in Chapters 4 and 5 are alluring but very preliminary and informal.
Reference: [Mey] <author> Albert R. Meyer. </author> <title> A substitution model for Scheme: Formal definitions. </title> <note> In preparation. </note>
Reference-contexts: This alternate strategy is consistent with the interpretation that a returned wire indicates that the node at the source of the wire is a value node. It also corresponds to the common practice in operational semantics of classifying a subset of rewritable expressions as values <ref> [Mey, GJ90] </ref>. An additional benefit is that it greatly simplifies the handling of quote, since a compound quoted expression can be treated as a single value created before the program is executed. 6 Unfortunately, the alternate strategy for handling values complicates the Edgar rewrite rules. <p> It is possible to encode Edgar's graphical rewrite rules in a more traditional textual fashion. The substitution model described by Meyer <ref> [Mey] </ref>, which was influenced by ideas in 392 CHAPTER 8. EDGAR: EXPLICIT DEMAND GRAPH REDUCTION Edgar, is a step in this direction. Unlike most other text-based operational models, Meyer's model captures the sharing of values in data structures.
Reference: [Mil87] <author> James S. Miller. MultiScheme: </author> <title> A parallel processing system based on MIT Scheme. </title> <type> Technical Report MIT/LCS/TR-402, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> September </month> <year> 1987. </year>
Reference-contexts: The evaluation of E proceeeds concurrently with the rest of the computation. Any context requiring the actual value of the placeholder will wait until the value is available. Par is equivalent to the future construct provided by many Lisps <ref> [Hal85, Mil87, For91] </ref>. * (synch E) returns an object that suspends the computation of E. Demand1 and demand2 are procedures that demand such an object to return the value of its suspended computation, but the computation is only initiated when both demand1 and demand2 have been called on the object. <p> This implies that tree elements may continue to print after the top-level call to the tree-printing tile has returned! events in a binary tile. The ability of a non-strict computation to return before completing its computation is a powerful feature for expressing parallelism and speculative computation (see <ref> [Tra88, Hal85, Mil87] </ref>). Indeed, we will see in Chapter 7 that non-strictness is an essential technique for modularizing computations. However, because it can change timing relationships among calls and other operations, non-strictness complicates reasoning about programs (e.g., see [HA87]). <p> In the case of eagons, evaluation of E body proceeds in parallel with the rest of the computation; any context requiring the value of E body must wait until it is available. Eagons are the same as the futures used in various parallel Lisp implementations ([Hal85], <ref> [Mil87] </ref>), except that in Opera we do not assume the existence of physical parallelism. We use the name "eagon" only for symmetry with "lazon". * Other differences: Opera supplies a few additional features that add no new power to the language but simplify the expression of some programs. <p> Traditional protocols supply explicit wait and release operations. With synchrons, only the wait is explicit; the release is implicitly handled by the automatic storage manager when a global rendezvous state is achieved. In this respect, synchrons resemble weak pairs <ref> [Mil87] </ref> and populations [RAM83], data structures whose behavior is tied to storage management. Although developed independently, synchrons are closely related to Hughes's synch construct [Hug83, Hug84]. (See Section 3.1.5 for a discussion of synch.) Both synch and synchrons involve objects that define a rendezvous point. <p> Is it possible to implement synchrons more efficiently? Synchrons are an example of a growing number of data structures that interact with garbage collection. Other examples include T's populations [RAM83], MultiScheme's pairs and finalization objects <ref> [Mil87] </ref>, and Hughes's synch construct [Hug83, Hug84].
Reference: [Mil89] <author> Robin Milner. </author> <title> Communication and Concurrency. </title> <publisher> Prentice-Hall, </publisher> <year> 1989. </year>
Reference-contexts: Like lazy aggregates, slags are compound, potentially tree-shaped, data structures whose parts are not computed until they are required. Like synchronous communication channels, slags synchronize separate threads of control and manage inter-process storage resources. (However, unlike many other synchronization models (e.g. <ref> [Hoa85, Mil89] </ref>), slags decouple communication and synchronization.) 5.3.2 Synquences and Syndrites For simplicity, we will focus on two particular kinds of slags: synquences and syndrites. A synquence (synchronized sequence) is a synchronized lazy list while a syndrite (synchronized dendrite) is a synchronized lazy tree. <p> This assumption was made purely to simplify the presentation. As long as slivers obey the above requirements, nothing prevents them from being implemented in terms of nested loops or mutual recursions. A rendezvous between slivers resembles interprocess synchronization in many models of concurrent processes (e.g., <ref> [Hoa85, Mil89, CM90] </ref>). However, there are several aspects that distinguish sliver synchronization from these other models. * With slivers, communication and synchronization are decoupled. Communication is achieved by referencing a data structure, while synchronization is achieved by applying wait to a synchron.
Reference: [MTH90] <author> Robin Milner, Mads Tofte, and Robert Harper. </author> <title> The Definition of Standard ML. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: It is also a commonly used technique in many other languages, especially functional and mostly functional ones (e.g., Haskell [HJW + 92], Id [AN89], Common Lisp [Ste90], Scheme [CR + 91], ML <ref> [MTH90] </ref>). 3.1.1 Database Example: A List Implementation For the linear database example, lists are an obvious choice for the type of aggregate data structure. Figure 3.1 shows how each of the slivers can be represented as a list-manipulation procedure. <p> In addition to the usual Scheme datatypes and operations, Opera supports mutable one-slot data structures called cells. These correspond to the reference structures of ML <ref> [MTH90] </ref> and FX [GJSO92]. 7.1. AN INTRODUCTION TO OPERA 299 Opera does not support Scheme's continuations. In principle, Opera should ultimately be able to support continuations, but the details of how continuations interact with Opera's fine-grained concurrency have not yet been worked out.
Reference: [PA85] <author> Keshav Pingali and Arvind. </author> <title> Efficient demand-driven evaluation (I). </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(2) </volume> <pages> 311-333, </pages> <month> April </month> <year> 1985. </year>
Reference-contexts: However, in systems providing only simulated parallelism (i.e., a single processor), demand-driven techniques may still be advantageous. Pingali and Arvind show how demand-driven computation can be simulated within a dataflow model <ref> [PA85] </ref>, [PA86]. * Overconstrained Operation Order: The linearity of channels sometimes overconstrains the order in which operations are applied. Consider the mapping routines in Figure 3.15. These routines force the mapped function to be applied to each record in the order in which it appears in the database. <p> A handful of other systems employ explicit representation of demand. Arvind and Pingali describe a mechanism for simulating demand-driven evaluation in a data flow model; they use data tokens to represent demand <ref> [PA85, PA86] </ref>. Ashcroft describes a system that combines demand flow (via entities called questons) with data flow (via entities called datons) [Ash86]. The visual notations used to represent Edgar graphs were influenced by the notations used in the data flow community (e.g., [Den75, DK82]).
Reference: [PA86] <author> Keshav Pingali and Arvind. </author> <title> Efficient demand-driven evaluation (II). </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(1) </volume> <pages> 109-139, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: However, in systems providing only simulated parallelism (i.e., a single processor), demand-driven techniques may still be advantageous. Pingali and Arvind show how demand-driven computation can be simulated within a dataflow model [PA85], <ref> [PA86] </ref>. * Overconstrained Operation Order: The linearity of channels sometimes overconstrains the order in which operations are applied. Consider the mapping routines in Figure 3.15. These routines force the mapped function to be applied to each record in the order in which it appears in the database. <p> A handful of other systems employ explicit representation of demand. Arvind and Pingali describe a mechanism for simulating demand-driven evaluation in a data flow model; they use data tokens to represent demand <ref> [PA85, PA86] </ref>. Ashcroft describes a system that combines demand flow (via entities called questons) with data flow (via entities called datons) [Ash86]. The visual notations used to represent Edgar graphs were influenced by the notations used in the data flow community (e.g., [Den75, DK82]).
Reference: [Pet77] <author> James L. Peterson. </author> <title> Petri nets. </title> <journal> ACM Computing Surveys, </journal> <volume> 9(3) </volume> <pages> 223-250, </pages> <year> 1977. </year>
Reference-contexts: ISM is a model that ascribes sequential and parallel characteristics to dimensions of both space and time. The concurrency allowed by ISM (parallelism in the time dimension) is formally expressed in a semantic framework that combines aspects of Petri nets <ref> [Pet77] </ref> and graph rewriting. Like Petri nets, the ISM semantics uses tokens to represent concurrency and synchronization; in the case of ISM, the tokens are an explicit representation of demand.
Reference: [Pet84] <author> Alberto Pettorossi. </author> <title> A powerful strategy for deriving efficient programs by transformation. </title> <booktitle> In ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pages 273-281, </pages> <year> 1984. </year>
Reference-contexts: Another valid recursive to iterative transformation is to use the cdr-bashing trick to replace (upQ init tcons synq) by (down-cons! init synq). This technique could be used to make iterative versions of append, copy, map, and map-list. A host of other transformations from the literature on program transformation <ref> [DR76, Dar82, Bac78, Bel86, Bir89a, Bir86, Bir88, Coh83, Str71, WS73, Pet84] </ref> find a convenient expression in terms of slivers. The above points indicate that, regardless of their appropriateness as an implementation language, slivers are a powerful way to design, specify, reason about, and explain programs.
Reference: [Pey87] <editor> Simon L. Peyton Jones. </editor> <booktitle> The Implementation of Functional Programming Languages. </booktitle> <publisher> Prentice-Hall, </publisher> <year> 1987. </year>
Reference-contexts: For example, consider the following 1 Lazy data structures are not to be confused with the more general strategy of lazy evaluation. Lazy evaluation introduces many space problems of its own (such as the dragging tail problem <ref> [Pey87] </ref>) that will not be detailed here. 3.1. <p> This is the sort of strategy that is encouraged by Id's M-structures [Bar92]. The alpha renaming network is useful for program manipulations other than alpha renaming. A deBruijn numbering program labels variables in a lambda term with a number indicating the level of the lambda that introduces them <ref> [Pey87] </ref>. <p> Opera introduces two new binding forms: nex and nexrec. These are versions of let and letrec that do not evaluate the binding expressions before evaluating the body. Instead, they resemble the "graphical" lets and letrecs of a lazy functional language (e.g., see <ref> [Pey87, pages 233-234] </ref>). In addition to the usual Scheme syntactic sugar, Opera provides a few new derived special forms. There are three sequencing forms: (seq E*), (seq1 E*), and (seqn E*). <p> Turner [Tur79] developed a means for compiling functional programs into graphs of three combinators (S, K, and I) and executing the resulting graphs by graph rewriting. Hughes [Hug82] introduced a method, super-combinators, for compiling a function into a specialized graph rewriting rule. Peyton Jones <ref> [Pey87] </ref> summarizes numerous issues relevant to graph reduction as an implementation technique. While Edgar is clearly a graph reduction framework, it differs in many respects from the work done in the functional programming community. First is a difference in emphasis: Edgar is primarily a semantic framework, not an implementation technique. <p> Because implementation efficiency is not a concern, some of the concepts in Edgar are cleaner than in other graph reduction frameworks. For example, the ability for Edgar rules to "move" wires obviates the need for messy special mechanisms like overwriting application nodes and inserting indirection nodes (see <ref> [Pey87] </ref>). The fact that Edgar deals with side effects, concurrency, and synchronization clearly separates it from much of the 7 The added complexity may be due to the fact that using return tokens to represent values may just be a bad idea. <p> The traditional approach of defining a class of value graphs might simplify the presentation of the alternate strategy. This possibility needs to be explored. 390 CHAPTER 8. EDGAR: EXPLICIT DEMAND GRAPH REDUCTION other work in the field. Although parallel graph reduction techniques exist <ref> [Pey87, Sme93] </ref>, the focus is again on efficiency, not on semantics. Until relatively recently, side effects have been anathema to the functional programming community, so they rarely appear in graph reduction work (but see [AA93]).
Reference: [Plo81] <author> Gordon D. Plotkin. </author> <title> A structural approach to operational semantics. </title> <type> Technical Report DAIMI FN-19, </type> <institution> Aarhus University Computer Science Department, </institution> <month> September </month> <year> 1981. </year> <note> BIBLIOGRAPHY 441 </note>
Reference-contexts: I conclude the chapter by relating Edgar to other graph-based semantic frameworks. 8.1 The Basics of Edgar At the very highest level, Edgar follows the basic recipe for an operational semantics framework <ref> [Plo81] </ref>. In an operational semantics, program execution states are represented by some sort of structured configuration, and there are transition rules that specify how the computation can step from one configuration to the next.
Reference: [Pra86] <author> V. R. Pratt. </author> <title> Modeling concurrency with partial orders. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 15(1) </volume> <pages> 33-71, </pages> <month> February </month> <year> 1986. </year>
Reference-contexts: Edgar seems to be a natural starting point for a richer notion process equivalence that explicitly models space consumption. The hard part is defining what "modulo management operations" means in this context. Pomset models of concurrent computation <ref> [Pra86, HA87] </ref> may be helpful for defining an Edgar-based notion of process equivalence. 10.3.6 Pedagogy The computational models and tools developed for this work have important pedagogical applications.
Reference: [RAM83] <author> Jonathan Rees, Norman I. Adams, and James R. Meehan. </author> <title> The T manual (third edition). </title> <type> Technical report, </type> <institution> Yale University Department of Computer Science, </institution> <month> March </month> <year> 1983. </year>
Reference-contexts: Traditional protocols supply explicit wait and release operations. With synchrons, only the wait is explicit; the release is implicitly handled by the automatic storage manager when a global rendezvous state is achieved. In this respect, synchrons resemble weak pairs [Mil87] and populations <ref> [RAM83] </ref>, data structures whose behavior is tied to storage management. Although developed independently, synchrons are closely related to Hughes's synch construct [Hug83, Hug84]. (See Section 3.1.5 for a discussion of synch.) Both synch and synchrons involve objects that define a rendezvous point. <p> Is it possible to implement synchrons more efficiently? Synchrons are an example of a growing number of data structures that interact with garbage collection. Other examples include T's populations <ref> [RAM83] </ref>, MultiScheme's pairs and finalization objects [Mil87], and Hughes's synch construct [Hug83, Hug84].
Reference: [Ric81] <author> Charles Rich. </author> <title> Inspection methods in programming. </title> <type> Technical Report AI-TR-604, </type> <institution> MIT Artificial Intelligence Laboratory, </institution> <month> June </month> <year> 1981. </year>
Reference-contexts: In Edgar's dynamically-changing networks, a wire may only transmit a single value. Additionally, the demand-driven nature of Edgar conrasts with the data-driven emphasis of data flow. The visual notations of Edgar bear some resemblance to plan calculus representations from the Programmer's Apprentice project <ref> [Ric81] </ref>. And, in fact, Edgar was designed to support many of the same kinds of program decompositions considered in that project.
Reference: [Rob65] <author> J. A. Robinson. </author> <title> A machine-oriented logic based on the resolution principle. </title> <journal> Journal of the ACM, </journal> <volume> 12(1) </volume> <pages> 23-41, </pages> <year> 1965. </year>
Reference-contexts: The simul! procedure declares that the rendezvous of one synchron must occur simultaneously with the rendezvous of another synchron. Conceptually, simul! extends the notion of unifying logic variables <ref> [Rob65] </ref> to unifying the points in time represented by two 7.1. AN INTRODUCTION TO OPERA 305 synchrons. The simul! procedure forces its two argument synchrons to be equivalent (as determined by Opera's eq? predicate); it returns the unified synchron as its result.
Reference: [RS87] <author> J. R. Rose and Guy L. Steele Jr. </author> <title> C*: An extended C language for data parallel programming. </title> <booktitle> In Proceedings of the Second International Conference on Supercomputing, </booktitle> <volume> Vol 2, </volume> <pages> pages 2-16, </pages> <year> 1987. </year>
Reference-contexts: The aggregate data style has its roots in Lisp's list manipulation routines (as epitomized by mapcar) and APL's array operators. Today, the aggregate data approach is the main organizing principle for data parallel languages (e.g., Fortran 90 [Ame89], C* <ref> [RS87] </ref>, NESL [Ble92], paralations [Sab88]). <p> Gap filtering is used in situations where the location of elements is important. For example, languages with array-based or data-parallel features (e.g., <ref> [Ive87, Ame89, GJSO92, Sab88, Ble90, RS87, Ble92] </ref>) present models in which elements reside at a particular location in an data structure.
Reference: [RW90] <author> Charles Rich and Richard C. Waters. </author> <title> The Programmer's Apprentice. </title> <publisher> Addison-Wesley, </publisher> <year> 1990. </year>
Reference-contexts: Programming environments can help to programmers to manage idioms. One approach is to provide tools, intelligent assistants, and special-purpose languages that aid the programmer in analyzing and synthesizing programs in terms of idioms. A good example of this approach is Rich and Waters's Programmer's Apprentice project <ref> [RW90] </ref>. An alternate approach to idiom management is to devise language constructs and mechanisms for encapsulating idioms as single entities within a general-purpose programming language. This is a basic motivation for modularity and abstraction in programming languages, and is the approach taken here.
Reference: [Sab88] <author> Gary W. Sabot. </author> <title> The Paralation Model. </title> <publisher> MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: The aggregate data style has its roots in Lisp's list manipulation routines (as epitomized by mapcar) and APL's array operators. Today, the aggregate data approach is the main organizing principle for data parallel languages (e.g., Fortran 90 [Ame89], C* [RS87], NESL [Ble92], paralations <ref> [Sab88] </ref>). <p> In the data parallel literature, "scan" refers to a partial accumulator <ref> [Ble90, Sab88, GJSO92] </ref>. In Waters's series package, though, "scan" refers to a kind of generator [Wat90]. I adopt the former meaning here. 180 CHAPTER 5. SYNCHRONIZED LAZY AGGREGATES (a) DOWN-ACCUMULTE (b) DOWN-SCAN (c) UP-SCAN (d) TRUNCATE (e) SHIFT (f ) MAP2 5.2. <p> Gap filtering is used in situations where the location of elements is important. For example, languages with array-based or data-parallel features (e.g., <ref> [Ive87, Ame89, GJSO92, Sab88, Ble90, RS87, Ble92] </ref>) present models in which elements reside at a particular location in an data structure. <p> Size and access inference for data parallel programs [CBF91] is another line of work that strongly suggests a notion of process shapes. In his work on paralations <ref> [Sab88] </ref>, Sabot described an intriguing notion of communication shape. 10.3.4 Synchronization Synchrons are a powerful synchronization mechanism that may have many other uses beyond their role in synchronized lazy aggregates. For example, it might be possible to use them as the basis for a general temporal constraint solver.
Reference: [Sme93] <author> Jacobus Edith Willem (Sjaak) Smetsers. </author> <title> Graph Rewriting and Functional Languages. </title> <type> PhD thesis, </type> <institution> Katholieke Universiteit Nijmegan, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: The traditional approach of defining a class of value graphs might simplify the presentation of the alternate strategy. This possibility needs to be explored. 390 CHAPTER 8. EDGAR: EXPLICIT DEMAND GRAPH REDUCTION other work in the field. Although parallel graph reduction techniques exist <ref> [Pey87, Sme93] </ref>, the focus is again on efficiency, not on semantics. Until relatively recently, side effects have been anathema to the functional programming community, so they rarely appear in graph reduction work (but see [AA93]).
Reference: [Ste77] <author> Guy L. Steele Jr. </author> <title> Debunking the "expensive procedure call" myth, or procedure call implementations considered harmful, or LAMBDA, the ultimate Goto. </title> <type> Technical Report AIM-443, </type> <institution> MIT Artificial Intelligence Laboratory, </institution> <month> October </month> <year> 1977. </year>
Reference-contexts: For example, Figure 1.6 (a) is an abstract depiction of a general linear recursion. Such a computation breaks cleanly into a down (call) phase and and up (return) phase. An iterative linear computation is a special case in which each call is a non-returning tail call <ref> [Ste77] </ref>; it exhibits no up phase, because all returns events effectively occur at the same time (as indicated by the dotted lines in Figure 1.6 (b)). These notions extend to tree computations; Figure 1.7 is a gallery of some shapes for binary computation trees. <p> Despite the fact that the functional and imperative versions of mean-age are written in different styles, they specify computations with similar operational characteristics. In both cases, even though the internal loop procedure is syntactically recursive, the tail-recursive property of Scheme ([CR + 91], <ref> [Ste77] </ref>) guarantees that loop executes iteratively, just as if it had been written as a do, for, or while loop in other languages. And both loop procedures iterate over the same three state variables, though in one case they are explicit arguments and in the other case they are implicit.
Reference: [Ste90] <author> Guy L. Steele Jr. </author> <title> Common Lisp: The Language. </title> <publisher> Digital Press, </publisher> <year> 1990. </year>
Reference-contexts: It is also a commonly used technique in many other languages, especially functional and mostly functional ones (e.g., Haskell [HJW + 92], Id [AN89], Common Lisp <ref> [Ste90] </ref>, Scheme [CR + 91], ML [MTH90]). 3.1.1 Database Example: A List Implementation For the linear database example, lists are an obvious choice for the type of aggregate data structure. Figure 3.1 shows how each of the slivers can be represented as a list-manipulation procedure. <p> Indeed, the notion of gluing together corresponding fragments of different slivers is at the heart of Waters's series compiler [Wat91]. 3.3.2 Looping Macros Many versions of Lisp have supported complex macros that capture certain looping idioms. 12 For example, in Common Lisp <ref> [Ste90] </ref>, the mean-age procedure can be written as follows: (defun mean-age (database) (loop for record = (first-record database) then (first-record (next-record record)) until (end-of-database? record) sum (record-get record 'age) into total count record into length finally (return (/ total length)))) Here, for, until, sum, count, and finally all introduce clauses into <p> The choice of core procedures was influenced by the 2 This evaluation strategy is not allowed in standard Scheme [CR + 91], which requires that the subexpres-sions be evaluated in some sequential order. 6.1. LINEAR COMPUTATIONS 223 list operations of Lisp <ref> [Ste90, CR + 91] </ref> and Haskell 3 [HJW + 92], the array operations of APL [Ive87], the vector operations of FX [GJSO92], and Waters's series operations [Wat90].
Reference: [Str71] <author> H. R. </author> <title> Strong. Translating recursion equations into flow charts. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 5 </volume> <pages> 254-285, </pages> <year> 1971. </year>
Reference-contexts: Another valid recursive to iterative transformation is to use the cdr-bashing trick to replace (upQ init tcons synq) by (down-cons! init synq). This technique could be used to make iterative versions of append, copy, map, and map-list. A host of other transformations from the literature on program transformation <ref> [DR76, Dar82, Bac78, Bel86, Bir89a, Bir86, Bir88, Coh83, Str71, WS73, Pet84] </ref> find a convenient expression in terms of slivers. The above points indicate that, regardless of their appropriateness as an implementation language, slivers are a powerful way to design, specify, reason about, and explain programs.
Reference: [Tra88] <author> Kenneth R. Traub. </author> <title> Sequential implementation of lenient programming languages. </title> <type> Technical Report MIT/LCS/TR-417, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> October </month> <year> 1988. </year>
Reference-contexts: This implies that tree elements may continue to print after the top-level call to the tree-printing tile has returned! events in a binary tile. The ability of a non-strict computation to return before completing its computation is a powerful feature for expressing parallelism and speculative computation (see <ref> [Tra88, Hal85, Mil87] </ref>). Indeed, we will see in Chapter 7 that non-strictness is an essential technique for modularizing computations. However, because it can change timing relationships among calls and other operations, non-strictness complicates reasoning about programs (e.g., see [HA87]). <p> But the eagons on the arguments further allow the arguments to be evaluate concurrently with the body of the procedure. Ecall corresponds to the default procedure application strategy for the Id programming language <ref> [AN89, Tra88] </ref>. 7.1.6 Graphical Bindings Opera's nex and nexrec constructs are "graphical" versions of let and letrec. Whereas let and letrec associate names with first-class values, nex and nexrec associate names with syntactic entities so that the general graph-structured syntactic dependencies can be expressed within the tree-structured confines of s-expressions.
Reference: [Tur79] <author> D. A. Turner. </author> <title> A new implementation for applicative languages. </title> <journal> Software - Practice and Experience, </journal> <volume> 9 </volume> <pages> 31-49, </pages> <year> 1979. </year>
Reference-contexts: However, my implementation actually uses the every-value is-returned strategy. 8.5 Related Work The development of Edgar was heavily influenced by previous work in operational semantics and graph reduction. Graph reduction has long been used as an implementation technique for functional programming languages. Turner <ref> [Tur79] </ref> developed a means for compiling functional programs into graphs of three combinators (S, K, and I) and executing the resulting graphs by graph rewriting. Hughes [Hug82] introduced a method, super-combinators, for compiling a function into a specialized graph rewriting rule.
Reference: [Tur85] <author> D. A. Turner. Miranda: </author> <title> A non-strict functional language with polymorphic types. </title> <booktitle> In Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 1-16, </pages> <year> 1985. </year> <booktitle> Lecture Notes in Computer Science, Number 201. </booktitle>
Reference-contexts: The arguments will only be evaluated if they are needed within the body of the called procedure. Lcall corresponds to the call-by-need parameter passing mechanism common in functional programming languages (e.g. Haskell [HJW + 92] and Miranda <ref> [Tur85] </ref>). Like the delayed objects of [Hen80] and [ASS85], lazons can be used to create conceptually infinite data structures and finesse certain kinds of circular dependencies.
Reference: [Tur94] <author> Franklyn Turbak. Slivers: </author> <title> Computational modularity via synchronized lazy aggregates. </title> <type> Technical Report AI-TR-1466, </type> <institution> MIT Artificial Intelligence Laboratory, </institution> <year> 1994. </year> <note> Revised version of doctoral dissertation. In prepartion. </note>
Reference-contexts: In the presence of nodes that model side effects, the value represented by a wire may change over time. See <ref> [Tur94] </ref> for details. 8.1. THE BASICS OF EDGAR 351 The snapshot depicted in Figure 8.1 is an intermediate configuration in a simple numer ical computation. <p> The complete collection of Edgar rules is described in <ref> [Tur94] </ref>. 8.2.1 Procedures The handling of procedures and procedure calls is shown in Figure 8.11. A pcall node is a general procedure application node. The [pcall-request] rule propagates demand for a pcall node to all of its input wires.
Reference: [WA85] <author> William W. Wadge and Edward A. Ashcroft. </author> <title> Lucid, the Dataflow Programming Language. </title> <publisher> Academic Press, </publisher> <year> 1985. </year> <note> 442 BIBLIOGRAPHY </note>
Reference-contexts: THE SIGNAL PROCESSING STYLE OF PROGRAMMING [Bir89b, CM90], producer/consumer models (CLU iterators [L + 79], Id's I-structures and M-structures [ANP89, Bar92], Linda [CG89]) and dataflow ([Den75], <ref> [WA85] </ref>, [DK82]). Below, we explore both coroutining and concurrent versions of the channel approach.
Reference: [Wad84] <author> Philip Wadler. </author> <title> Listlessness is better than laziness: Lazy evaluation and garbage collection at compile-time. </title> <booktitle> In ACM Symposium On Lisp and Functional Programming, </booktitle> <pages> pages 45-52, </pages> <year> 1984. </year>
Reference-contexts: Because programmers cannot depend on the transformations, they must seek other methods of controlling the space behavior of their programs. APL compilation techniques [GW78, Bud88] suffer from the same problem. The listlessness <ref> [Wad84, Wad85] </ref> and deforestation [Wad88, Chi92, GLJ93] techniques pioneered by Wadler do provide guarantees, but they are rather limited in applicability. Listlessness handles a subclass of list programs, but no trees. Deforestation can eliminate both lists and trees, but only in networks that exhibit no fan-out. <p> Specifying explicit buffers may seem annoying, but it is the cost of the space consumption guarantees provided by sliver decomposition. The implicit buffering provided by other aggregate data approaches makes it hard to control the storage profiles of modular programs. Existing list and tree removal techniques (such as listlessness <ref> [Wad84, Wad85] </ref>, deforestation [Wad88, Chi92, GLJ93], and other transformations [Bel86, Bir89a]) automatically remove some intermediate data structures, but they either are limited to restricted network topologies or do not guarantee that all intermediate structures go away. In con 6.1.
Reference: [Wad85] <author> Philip Wadler. </author> <title> Listlessness is better than laziness II: Composing listless functions. </title> <booktitle> In Programs As Data Objects, </booktitle> <pages> pages 282-305. </pages> <publisher> Springer-Verlag, </publisher> <year> 1985. </year> <note> Lecture Notes in Computer Science 217. </note>
Reference-contexts: Because programmers cannot depend on the transformations, they must seek other methods of controlling the space behavior of their programs. APL compilation techniques [GW78, Bud88] suffer from the same problem. The listlessness <ref> [Wad84, Wad85] </ref> and deforestation [Wad88, Chi92, GLJ93] techniques pioneered by Wadler do provide guarantees, but they are rather limited in applicability. Listlessness handles a subclass of list programs, but no trees. Deforestation can eliminate both lists and trees, but only in networks that exhibit no fan-out. <p> Specifying explicit buffers may seem annoying, but it is the cost of the space consumption guarantees provided by sliver decomposition. The implicit buffering provided by other aggregate data approaches makes it hard to control the storage profiles of modular programs. Existing list and tree removal techniques (such as listlessness <ref> [Wad84, Wad85] </ref>, deforestation [Wad88, Chi92, GLJ93], and other transformations [Bel86, Bir89a]) automatically remove some intermediate data structures, but they either are limited to restricted network topologies or do not guarantee that all intermediate structures go away. In con 6.1.
Reference: [Wad88] <author> Philip Wadler. </author> <title> Deforestation: Transforming programs to eliminate trees. </title> <booktitle> In 2nd European Symposium on Programming, </booktitle> <pages> pages 344-358, </pages> <year> 1988. </year> <booktitle> Lecture Notes in Computer Science, Number 300. </booktitle>
Reference-contexts: Because programmers cannot depend on the transformations, they must seek other methods of controlling the space behavior of their programs. APL compilation techniques [GW78, Bud88] suffer from the same problem. The listlessness [Wad84, Wad85] and deforestation <ref> [Wad88, Chi92, GLJ93] </ref> techniques pioneered by Wadler do provide guarantees, but they are rather limited in applicability. Listlessness handles a subclass of list programs, but no trees. Deforestation can eliminate both lists and trees, but only in networks that exhibit no fan-out. <p> The implicit buffering provided by other aggregate data approaches makes it hard to control the storage profiles of modular programs. Existing list and tree removal techniques (such as listlessness [Wad84, Wad85], deforestation <ref> [Wad88, Chi92, GLJ93] </ref>, and other transformations [Bel86, Bir89a]) automatically remove some intermediate data structures, but they either are limited to restricted network topologies or do not guarantee that all intermediate structures go away. In con 6.1. <p> The static approach holds more promise for increased efficiency. Ideally, it should be possible to eliminate the need for run-time overhead completely by compiling many sliver networks into efficient monolithic recursive procedures. Techniques that are particularly worth exploring in this context include series compilation [Wat91], deforestation <ref> [Wad88] </ref>, partial evaluation [WCRS91], and attribute grammar evaluation [DJL88]. Series compilation is based on representing a series operator as collection of code fragments that characterize different aspects of the operator; compilation consists of gluing together corresponding fragments for a network of series operators.
Reference: [Wat] <author> Richard C. Waters. </author> <title> To NReverse when consing a list or by pointer manipulation to avoid it; that is the question. </title> <note> Lisp Pointers (to appear). </note>
Reference-contexts: Note how the use of down-cons! ensures that nth-down-scan generates an 8 Waters has recently shown that the reversal technique and the cdr-bashing technique are practically indistinguishable in terms of efficiency <ref> [Wat] </ref>. In light of this result, he argues for using the simpler one | i.e., the reversal approach. However, in a language like Synapse, where the cdr-bashing technique can be expressed as a modular component, cdr-bashing may actually the "simpler" approach. 234 CHAPTER 6.
Reference: [Wat78] <author> Richard C. Waters. </author> <title> Automatic analysis of the logical structure of programs. </title> <type> Technical Report AI-TR-492, </type> <institution> MIT Artificial Intelligence Laboratory, </institution> <month> December </month> <year> 1978. </year>
Reference-contexts: Sliver decomposition extends lazy aggregates with the synchronization of channel-based approaches. 4. Generalizing Series : Much of this research was inspired by Waters's extensive work on loop decomposition <ref> [Wat78, Wat79, Wat84, Wat87, Wat90, Wat91] </ref>. Waters designed a well-engineered mechanism for expressing loops in terms of networks of linear iterative operators manipulating a kind of synchronized lazy data structure known as series. <p> Slags are a particular implementation of the cables that appear in sliver diagrams. Intuitively, a slag is a data structure that represents the values of a program variable over time. Waters uses the term temporal abstraction <ref> [Wat78] </ref> to refer to this notion. In fact, his series data structure [Wat90, Wat91] is a particular instance of the more general synchronized lazy aggregates. <p> At the very least, slivers should be used to duplicate results from attribute grammar research within the aggregate data style. A more ambitious undertaking would be a detailed study of existing tree programs along the lines of Waters's study of iterative programs <ref> [Wat78, Wat79] </ref>. Compilers and interpreters are particularly relevant candidates for study. One of the main pragmatic motivations for slivers was to be able to describe compilers and interpreters in a modular, but still efficient, fashion.
Reference: [Wat79] <author> Richard C. Waters. </author> <title> A method for analyzing loop programs. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-5(3):237-247, </volume> <month> May </month> <year> 1979. </year>
Reference-contexts: Sliver decomposition extends lazy aggregates with the synchronization of channel-based approaches. 4. Generalizing Series : Much of this research was inspired by Waters's extensive work on loop decomposition <ref> [Wat78, Wat79, Wat84, Wat87, Wat90, Wat91] </ref>. Waters designed a well-engineered mechanism for expressing loops in terms of networks of linear iterative operators manipulating a kind of synchronized lazy data structure known as series. <p> These kinds of idioms (generate, map, filter, accumulate) arise repeatedly in manipulation of linear data. For example, Waters found that 90% of the code in the Fortran Scientific Subroutine Package could be expressed wholly in terms of these idioms <ref> [Wat79] </ref>. A major problem with the above procedures is that the idioms are not localized in the program text. For example, the database enumeration idiom is spread out across each procedure body in the calls to first-record, next-record, and end-of-database?. Sim 52 CHAPTER 2. <p> At the very least, slivers should be used to duplicate results from attribute grammar research within the aggregate data style. A more ambitious undertaking would be a detailed study of existing tree programs along the lines of Waters's study of iterative programs <ref> [Wat78, Wat79] </ref>. Compilers and interpreters are particularly relevant candidates for study. One of the main pragmatic motivations for slivers was to be able to describe compilers and interpreters in a modular, but still efficient, fashion.
Reference: [Wat84] <author> Richard C. Waters. </author> <title> Expressional loops. </title> <booktitle> In ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 1-10, </pages> <year> 1984. </year>
Reference-contexts: Sliver decomposition extends lazy aggregates with the synchronization of channel-based approaches. 4. Generalizing Series : Much of this research was inspired by Waters's extensive work on loop decomposition <ref> [Wat78, Wat79, Wat84, Wat87, Wat90, Wat91] </ref>. Waters designed a well-engineered mechanism for expressing loops in terms of networks of linear iterative operators manipulating a kind of synchronized lazy data structure known as series.
Reference: [Wat87] <author> Richard C. Waters. </author> <title> Efficient interpretation of synchronizable series expressions. </title> <booktitle> In ACM SIGPLAN '87 Symposium on Interpreters and Interpretive Techniques, </booktitle> <volume> volume SE-5, </volume> <pages> pages 74-85, </pages> <year> 1987. </year>
Reference-contexts: Sliver decomposition extends lazy aggregates with the synchronization of channel-based approaches. 4. Generalizing Series : Much of this research was inspired by Waters's extensive work on loop decomposition <ref> [Wat78, Wat79, Wat84, Wat87, Wat90, Wat91] </ref>. Waters designed a well-engineered mechanism for expressing loops in terms of networks of linear iterative operators manipulating a kind of synchronized lazy data structure known as series.
Reference: [Wat90] <author> Richard C. Waters. </author> <title> Series. </title> <editor> In Guy L. Steele Jr., editor, </editor> <title> Common Lisp: </title> <booktitle> The Language, </booktitle> <pages> pages 923-955. </pages> <publisher> Digital Press, </publisher> <year> 1990. </year>
Reference-contexts: Sliver decomposition extends lazy aggregates with the synchronization of channel-based approaches. 4. Generalizing Series : Much of this research was inspired by Waters's extensive work on loop decomposition <ref> [Wat78, Wat79, Wat84, Wat87, Wat90, Wat91] </ref>. Waters designed a well-engineered mechanism for expressing loops in terms of networks of linear iterative operators manipulating a kind of synchronized lazy data structure known as series. <p> Listlessness handles a subclass of list programs, but no trees. Deforestation can eliminate both lists and trees, but only in networks that exhibit no fan-out. The most impressive of the transformation approaches is Waters's series technique <ref> [Wat91, Wat90] </ref>. Series is a linear datatype that corresponds to a sequence of the values assumed by a state variable during an iteration. The series compiler can transform a large class of series networks, including those with fan-in, fan-out, and filtering, into efficient loops. <p> In the data parallel literature, "scan" refers to a partial accumulator [Ble90, Sab88, GJSO92]. In Waters's series package, though, "scan" refers to a kind of generator <ref> [Wat90] </ref>. I adopt the former meaning here. 180 CHAPTER 5. SYNCHRONIZED LAZY AGGREGATES (a) DOWN-ACCUMULTE (b) DOWN-SCAN (c) UP-SCAN (d) TRUNCATE (e) SHIFT (f ) MAP2 5.2. SLIVER DECOMPOSITION 181 function fun over the elements of two consumpts to give a single product. <p> Slags are a particular implementation of the cables that appear in sliver diagrams. Intuitively, a slag is a data structure that represents the values of a program variable over time. Waters uses the term temporal abstraction [Wat78] to refer to this notion. In fact, his series data structure <ref> [Wat90, Wat91] </ref> is a particular instance of the more general synchronized lazy aggregates. Whereas a series represents the successive values of the state variable of a loop, synchronized lazy aggregates can represent the conceptual tree of values taken on by an identifier within an arbitrary recursive procedure. <p> LINEAR COMPUTATIONS 223 list operations of Lisp [Ste90, CR + 91] and Haskell 3 [HJW + 92], the array operations of APL [Ive87], the vector operations of FX [GJSO92], and Waters's series operations <ref> [Wat90] </ref>. In the remainder of this section, we will employ the core procedures to illustrate various properties of sliver decomposition within Synapse. 6.1.1 Iteration vs. Recursion Using the slivers in Figures 6.1 and 6.2, it is easy to define both iterative and recursive procedures in a modular fashion. <p> While it is fairly easy to express a wide range of linear computations in terms of existing synquence slivers, there is still much functionality missing. More linear primitives, along the lines of those developed for series <ref> [Wat90] </ref>, need to be added. Syndrites are in a considerably more embryonic state than synquences. The current set of syndrite slivers is very weak.
Reference: [Wat91] <author> Richard C. Waters. </author> <title> Automatic transformation of series expressions into loops. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(1) </volume> <pages> 52-98, </pages> <month> Jan-uary </month> <year> 1991. </year>
Reference-contexts: The problem has its roots in a fundamental mismatch between demand-driven evaluation and the fan-out of results from device outputs. As far as I know, only Waters <ref> [Wat91] </ref> and Hughes [Hug83, Hug84] have provided partial solutions to this problem within aggregate data approach. (In channel-based mechanisms that support fan-out, the space problem is often solved by bounded channels.) * Excessive Time Overhead: For a variety of reasons, SPS programs can require significantly more time than their monolithic counterparts. <p> Sliver decomposition extends lazy aggregates with the synchronization of channel-based approaches. 4. Generalizing Series : Much of this research was inspired by Waters's extensive work on loop decomposition <ref> [Wat78, Wat79, Wat84, Wat87, Wat90, Wat91] </ref>. Waters designed a well-engineered mechanism for expressing loops in terms of networks of linear iterative operators manipulating a kind of synchronized lazy data structure known as series. <p> Listlessness handles a subclass of list programs, but no trees. Deforestation can eliminate both lists and trees, but only in networks that exhibit no fan-out. The most impressive of the transformation approaches is Waters's series technique <ref> [Wat91, Wat90] </ref>. Series is a linear datatype that corresponds to a sequence of the values assumed by a state variable during an iteration. The series compiler can transform a large class of series networks, including those with fan-in, fan-out, and filtering, into efficient loops. <p> While this approach may not be reasonable for programmers, it can be good idea for compilers. Indeed, the notion of gluing together corresponding fragments of different slivers is at the heart of Waters's series compiler <ref> [Wat91] </ref>. 3.3.2 Looping Macros Many versions of Lisp have supported complex macros that capture certain looping idioms. 12 For example, in Common Lisp [Ste90], the mean-age procedure can be written as follows: (defun mean-age (database) (loop for record = (first-record database) then (first-record (next-record record)) until (end-of-database? record) sum (record-get record <p> Slags are a particular implementation of the cables that appear in sliver diagrams. Intuitively, a slag is a data structure that represents the values of a program variable over time. Waters uses the term temporal abstraction [Wat78] to refer to this notion. In fact, his series data structure <ref> [Wat90, Wat91] </ref> is a particular instance of the more general synchronized lazy aggregates. Whereas a series represents the successive values of the state variable of a loop, synchronized lazy aggregates can represent the conceptual tree of values taken on by an identifier within an arbitrary recursive procedure. <p> And in fact, in straightforward implementations of slivers, synquence manipulation would incur sufficient overhead to discourage programmers from actually writing the list utilities as indicated in Figure 6.10. However, as I will argue later, it is likely that techniques similar to those used by Waters in his series package <ref> [Wat91] </ref> can be used to compile such definitions into efficient code. More important, the kinds of decompositions shown in Figure 6.10 have many expressiveness advantages that transcend issues of efficiency: * Synquences are a common currency in which to express a wide range of linear iterations and recursions. <p> The synchronization information propagated by slags guarantees that all the slivers work in concert; one cannot race ahead or lag behind the others. In this respect, synquences resemble interprocess communication channels with bounded buffering. Waters <ref> [Wat91] </ref> and Hughes [Hug84] also extend the aggregate data approach with forms of synchronization in order to achieve this effect. <p> The static approach holds more promise for increased efficiency. Ideally, it should be possible to eliminate the need for run-time overhead completely by compiling many sliver networks into efficient monolithic recursive procedures. Techniques that are particularly worth exploring in this context include series compilation <ref> [Wat91] </ref>, deforestation [Wad88], partial evaluation [WCRS91], and attribute grammar evaluation [DJL88]. Series compilation is based on representing a series operator as collection of code fragments that characterize different aspects of the operator; compilation consists of gluing together corresponding fragments for a network of series operators.
Reference: [Wat92] <author> D. A. Watt. </author> <title> Modular description of programming languages. </title> <journal> The Computer Journal, </journal> <volume> 35:A009-A0028, </volume> <year> 1992. </year>
Reference-contexts: However, in recent years, there has been a flurry of activity on modular attribute grammars, which strive to group all the computations of a given attribute into a modular unit <ref> [DC90, Ada91, FMY92, KW92, Wat92] </ref>.
Reference: [WCRS91] <author> Daniel Weise, Roland Conybeare, Erik Ruf, and Scott Seligman. </author> <title> Automatic online partial evaluation. </title> <booktitle> In Proceedings of the Conference on Functional Programming Languages and Computer Architecture. </booktitle> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1991. </year>
Reference-contexts: The static approach holds more promise for increased efficiency. Ideally, it should be possible to eliminate the need for run-time overhead completely by compiling many sliver networks into efficient monolithic recursive procedures. Techniques that are particularly worth exploring in this context include series compilation [Wat91], deforestation [Wad88], partial evaluation <ref> [WCRS91] </ref>, and attribute grammar evaluation [DJL88]. Series compilation is based on representing a series operator as collection of code fragments that characterize different aspects of the operator; compilation consists of gluing together corresponding fragments for a network of series operators.
Reference: [WS73] <author> S. A. Walker and H. R. </author> <title> Strong. Characterizations of flowchartable recursions. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 7 </volume> <pages> 404-447, </pages> <year> 1973. </year>
Reference-contexts: Another valid recursive to iterative transformation is to use the cdr-bashing trick to replace (upQ init tcons synq) by (down-cons! init synq). This technique could be used to make iterative versions of append, copy, map, and map-list. A host of other transformations from the literature on program transformation <ref> [DR76, Dar82, Bac78, Bel86, Bir89a, Bir86, Bir88, Coh83, Str71, WS73, Pet84] </ref> find a convenient expression in terms of slivers. The above points indicate that, regardless of their appropriateness as an implementation language, slivers are a powerful way to design, specify, reason about, and explain programs.
References-found: 111

