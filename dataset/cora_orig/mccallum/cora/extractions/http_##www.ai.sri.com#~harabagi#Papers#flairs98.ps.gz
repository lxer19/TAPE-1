URL: http://www.ai.sri.com/~harabagi/Papers/flairs98.ps.gz
Refering-URL: http://www.ai.sri.com/~harabagi/
Root-URL: 
Email: harabagi@ai.sri.com  
Title: WordNet-Based Inference of Textual Cohesion and Coherence  
Author: Sanda M. Harabagiu 
Address: 333 Ravenswood Ave. Menlo Park, CA 94025  
Affiliation: Artificial Intelligence Center SRI International  
Abstract: This paper 1 presents a computational method for the recognition of the cohesive and coherence structures of texts. A large lexical knowledge base built on top of WordNet provides with the lexico-semantic information that needs to be mined. A path-finding algorithm returns the cohesive structure of a text with results that outperform previous approaches. The lexical paths contained in the cohesive structures are used to (1) build patterns of association between cue phrases and coherence relations and (2) to find the lexical characteristics of coherence categories. Finally, the textual coherence structure is recognized by giving priority to the coherence constrains induced by cue phrases. The paper presents also the performance of building the coherence structure for several texts. 
Abstract-found: 1
Intro-found: 1
Reference: <author> B.J. Grosz and C.L. Sidner. </author> <title> Attention, intentions and the structure of discourse. </title> <journal> Computational Linguistics, </journal> <volume> 12(2) </volume> <pages> 175-204, </pages> <year> 1986. </year>
Reference: <author> M.A.K. Halliday and R. Hasan. </author> <title> Cohesion in English. </title> <publisher> Longman, </publisher> <address> London, </address> <year> 1976. </year>
Reference-contexts: Their approach found well over 90% of the intuitive lexical relations from a set of five examples presented in (Morris 1988), and was able to retrieve 14 out of the 16 nonsystematic lexical chains given as examples in <ref> (Halliday and Hassan 1976) </ref> (thus an 87% recall). These promising results prompted the consideration of using WordNet for the detection of lexical cohesion relations from the large corpus provided by Treebank (Marcus et al.1993).
Reference: <author> S.M. Harabagiu. </author> <title> WordNet-based inference of textual context, cohesion and coherence. </title> <type> PhD thesis, </type> <institution> University of Southern California, </institution> <address> Los Angeles, CA, </address> <year> 1997. </year>
Reference-contexts: Some of these relations are: agent, object, instrument, beneficiary, location, state, reason, theme or manner. Such relations were acquired from the corpus of gloss definitions provided by WordNet, and called gloss relations. This is part of the methodology of mapping the gloss definitions into semantic networks, presented in <ref> (Harabagiu 1997) </ref>. A path-finding algorithm A path-finding algorithm was designed to find semantic paths between the words of a text. It consists of four steps, that successively perform searches in WordNet and consolidate the cohesion structures. <p> For example, for the text presented in (Morris and Hirst 1991), we found 38 lexical paths as opposed to their 9 lexical chains. Our results, fully detailed in <ref> (Harabagiu 1997) </ref> show an increase in the recall with 44%. The precision is enforced as well, since the paths have to comply with the constraints of the local contexts. Even for the paths that correspond to their lexical chains, the inter-relationships between the words were more dense. <p> The only automatic coherence builder for English we are aware of is Marcu's Rhetorical parser, therefore we assessed the correctness of our algorithm by measuring the agreement with the rhetorical structure built by Marcu for the same text. The experiments are detailed in <ref> (Harabagiu 1997) </ref> and show that we obtained almost 80% identical coherence structures. The knowledge inferred by the coherence structures of texts was also used for solving coreference in texts and ported significant improvements in precision, fully detailed in (Harabagiu 1997). <p> The experiments are detailed in <ref> (Harabagiu 1997) </ref> and show that we obtained almost 80% identical coherence structures. The knowledge inferred by the coherence structures of texts was also used for solving coreference in texts and ported significant improvements in precision, fully detailed in (Harabagiu 1997).
Reference: <author> J. Hirshberg and D Litman. </author> <title> Empirical studies on the disambiguation of cue phrases. </title> <journal> Computational Linguistics, </journal> <volume> 19(3) </volume> <pages> 501-530, </pages> <year> 1993. </year>
Reference-contexts: However, the majority of the cue phrases are ambiguous, in the sense that they have also alternative meanings, where the word doesn't contribute to the discourse level semantics, but rather to the semantic meaning of the sentences. Building on the previous work encompassing the studies presented in <ref> (Hirshberg and Litman 1993) </ref>, (Siegel and McKeown 1994), (Grosz and Sidner 1996), the approach used in (Marcu 1997), extends the problem of cue phrase disambiguation by distinguishing the discourse sense of a cue phrase into finer meanings, corresponding to the rhetorical relations it indicates.
Reference: <author> E.H. Hovy. </author> <title> Automated discourse generation using discourse structure relations. </title> <journal> Artificial Intelligence, </journal> <volume> 63 </volume> <pages> 341-385, </pages> <year> 1993. </year>
Reference-contexts: We 1 Copyright 1998, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. develop a methodology of discovering coherence patterns from the lexical cohesion of texts, using an initial set of coherence rules inspired by the Hovy's taxonomy of discourse relations <ref> (Hovy 1993) </ref>. Its performance is compared to Marcu's recent Rhetorical Parser (Marcu 1997). The WordNet lexical database WordNet (Miller 1995) is a machine-readable dictionary designed at Princeton, following psycholinguistic principles. Unlike standard alphabetical dictionaries which organize vocabularies using morphological similarities, WordNet organizes lexical information in terms of word meanings. <p> Our approach has many similarities with Marcu's method because we focus on the recognition of a basic set of relations derived from the top of the taxonomy obtained by Hovy in <ref> (Hovy 1993) </ref>. The cardinality of the set of potential discourse markers we considered is far smaller than the one used by Marcu. We have been considering only 29 cue phrases, as opposed to Marcu's study of 450 discourse markers.
Reference: <author> A. Kehler. </author> <title> Interpreting Cohesion Forms in the Context of Discourse Inference. </title> <type> PhD thesis, </type> <institution> Harvard University, </institution> <address> Cambridge, MA, </address> <year> 1995. </year>
Reference-contexts: We consider a taxonomy of coherence relations, initially reported in (Maier and Hovy 1992) that is mapped into the coherence categories devised in <ref> (Kehler 1995) </ref>. These coherence categories are characterized by properties that can be recognized from the information brought forward by the lexico-semantic paths. Lists of cohesive constraints, as indicated by properties of the sequences derived from the WordNet paths, are derived and help recognize each coherence relation.
Reference: <author> E. Maier and E. Hovy. </author> <title> Organizing discourse structure relations using metafunctions. </title> <editor> In H. Horacek, editor, </editor> <title> New Concepts in Natural Language Generation: Planning, </title> <booktitle> Realization and Systems, </booktitle> <pages> pages 178-201. </pages> <publisher> Pinter, </publisher> <address> London, </address> <year> 1992. </year>
Reference-contexts: Here, we describe the effect of knowledge gathered from a large linguistic database on the recognition of coherence relations and on the general structure of the discourse. We consider a taxonomy of coherence relations, initially reported in <ref> (Maier and Hovy 1992) </ref> that is mapped into the coherence categories devised in (Kehler 1995). These coherence categories are characterized by properties that can be recognized from the information brought forward by the lexico-semantic paths.
Reference: <author> W.C. Mann and S. Thompson. </author> <title> Rhetorical structure theory. </title> <booktitle> Text, </booktitle> <volume> 8 </volume> <pages> 243-281, </pages> <year> 1988. </year>
Reference-contexts: The only other automatic method of building the rhetorical tree of a text we are aware of is the method implemented in (Marcu 1996). Marcu reformulates the definition of the structure of a text as devised by the RST <ref> (Mann and Thompson 1988) </ref> relations. He considers a formal treatment of the coherence structure by relying on a shallow discourse analyzer based on cue phrase information. The inspection of the coherence and cohesion constraints imposed by the structure of semantic paths triggers the discovery of Resemblance and Cause-Effect relations.
Reference: <author> D. Marcu. </author> <title> Building up rhethorical structure trees. </title> <booktitle> In Proceedings of the 13th National Conference on Artificial Intelligence (AAAI-96), </booktitle> <pages> pages 1069-1074, </pages> <address> Port-land, OR, </address> <year> 1996. </year>
Reference-contexts: We favor this organization of the textual coherence structure to the hierarchi cal organization. The only other automatic method of building the rhetorical tree of a text we are aware of is the method implemented in <ref> (Marcu 1996) </ref>. Marcu reformulates the definition of the structure of a text as devised by the RST (Mann and Thompson 1988) relations. He considers a formal treatment of the coherence structure by relying on a shallow discourse analyzer based on cue phrase information.
Reference: <author> D. Marcu. </author> <title> The rhethorical parsing of natural language texts. </title> <booktitle> In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, </booktitle> <year> 1997. </year>
Reference-contexts: All rights reserved. develop a methodology of discovering coherence patterns from the lexical cohesion of texts, using an initial set of coherence rules inspired by the Hovy's taxonomy of discourse relations (Hovy 1993). Its performance is compared to Marcu's recent Rhetorical Parser <ref> (Marcu 1997) </ref>. The WordNet lexical database WordNet (Miller 1995) is a machine-readable dictionary designed at Princeton, following psycholinguistic principles. Unlike standard alphabetical dictionaries which organize vocabularies using morphological similarities, WordNet organizes lexical information in terms of word meanings. <p> Building on the previous work encompassing the studies presented in (Hirshberg and Litman 1993), (Siegel and McKeown 1994), (Grosz and Sidner 1996), the approach used in <ref> (Marcu 1997) </ref>, extends the problem of cue phrase disambiguation by distinguishing the discourse sense of a cue phrase into finer meanings, corresponding to the rhetorical relations it indicates. <p> Three independent judges identified the meanings of the cue phrases and validated the results of the disambiguation procedure. The results show that 86.45% of the discourse senses of the cue phrases were discovered with a precision of 72.91%, a result which is close to what Marcu obtained <ref> (Marcu 1997) </ref> with a surface-based algorithm. Text Coherence It is well established that the structure of a text contains more than the collection of the sentence structures; its meaning is determined by the logical relations between sentences.
Reference: <author> M. Marcus, B. Santorini and M.A. Marcinkiewicz. </author> <title> Building a large annotated corpus of English: The Penn Treebank. </title> <journal> Computational Linguistics, </journal> <volume> 19(2) </volume> <pages> 313-330, </pages> <year> 1993. </year>
Reference-contexts: These promising results prompted the consideration of using WordNet for the detection of lexical cohesion relations from the large corpus provided by Treebank <ref> (Marcus et al.1993) </ref>. In the process, we discovered interesting associations with the approach of Morris and Hirst, as well as complex divergences. <p> Whenever at least two of the humans tagged a text passage with the same coherence relation as the the one in the automatic structure, we considered a hit, in other cases a miss. Table 1 illus trates the precision and correctness obtained for five texts from Treebank <ref> (Marcus et al.1993) </ref>. The number n 1 stands for the number of coherence relations identified manually, n 2 represents the number of relations identified by the algorithm and n 3 is the number of coherence relations correctly identified.
Reference: <author> G.A. Miller. </author> <title> WordNet: A Lexical Database. </title> <journal> Communication of the ACM, </journal> <volume> vol 38: No11, </volume> <pages> pages 39-41, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: If cohesion is a term for sticking together, then coherence is a term for making sense. In this paper, we revisit the notion of lexical cohesion, and present its contribution to the evaluation of text coherence. We make use of the vast lexical knowledge rendered by WordNet <ref> (Miller 1995) </ref> to build lexical paths spanning the words of texts. Lexical cohesion, resulting from novel techniques of searching the WordNet thesaurus, is shown to contribute to an automatic approach of discourse coherence analysis. We 1 Copyright 1998, American Association for Artificial Intelligence (www.aaai.org). <p> All rights reserved. develop a methodology of discovering coherence patterns from the lexical cohesion of texts, using an initial set of coherence rules inspired by the Hovy's taxonomy of discourse relations (Hovy 1993). Its performance is compared to Marcu's recent Rhetorical Parser (Marcu 1997). The WordNet lexical database WordNet <ref> (Miller 1995) </ref> is a machine-readable dictionary designed at Princeton, following psycholinguistic principles. Unlike standard alphabetical dictionaries which organize vocabularies using morphological similarities, WordNet organizes lexical information in terms of word meanings. <p> Words having multiple semantic meanings belong to as many synsets as their meanings, which are ordered along their frequency of occurance in real texts. Words and their underlying concepts are linked in WordNet through thirteen types of lexico-semantic relations, presented in <ref> (Miller 1995) </ref>. Most of the 391,885 relations encoded in WordNet are represented by is-a relations that create hierarchies of nouns and verbs. Some meronym (is part, is member, has stuff) relations between noun concepts are also represented.
Reference: <author> J. Morris. </author> <title> Lexical cohesion, the thesaurus, and the structure of text. </title> <type> Master's thesis, </type> <institution> University of Toronto, Toronto, Canada, </institution> <year> 1988. </year>
Reference-contexts: Lexical paths as forms of cohesion The first algorithm that searched for lexical cohesion relations in texts was devised by Morris and Hirst. Their approach found well over 90% of the intuitive lexical relations from a set of five examples presented in <ref> (Morris 1988) </ref>, and was able to retrieve 14 out of the 16 nonsystematic lexical chains given as examples in (Halliday and Hassan 1976) (thus an 87% recall).
Reference: <author> J. Morris and G. Hirst. </author> <title> Lexical cohesion computed by thesaural relations as an indicator of the structure of text. </title> <journal> Computational Linguistics, </journal> <volume> 17 </volume> <pages> 21-48, </pages> <year> 1991. </year>
Reference-contexts: This property of sentences of "sticking together" to function as a whole, as defined in (Hal-liday and Hassan 1976) and <ref> (Morris and Hirst 1991) </ref> is known as cohesion. A sequence of sentences in a text must also display logical connections, accounting for the coherence of the text. If cohesion is a term for sticking together, then coherence is a term for making sense. <p> In contrast, the path-finding algorithm provides with a wealth of lexical cohesion relations, most of them uncovered by the algorithm of Morris and Hirst. For example, for the text presented in <ref> (Morris and Hirst 1991) </ref>, we found 38 lexical paths as opposed to their 9 lexical chains. Our results, fully detailed in (Harabagiu 1997) show an increase in the recall with 44%. The precision is enforced as well, since the paths have to comply with the constraints of the local contexts.
Reference: <author> E.V. Siegel and K.R. McKeown. </author> <title> Emergent linguistic rules from inducing decision trees: Disambiguating discourse clue words. </title> <booktitle> In Proceedings of the 12th National Conference on Artificial Intelligence (AAAI-96), </booktitle> <pages> pages 820-826, </pages> <address> Seattle, WA, </address> <year> 1994. </year>
Reference-contexts: Building on the previous work encompassing the studies presented in (Hirshberg and Litman 1993), <ref> (Siegel and McKeown 1994) </ref>, (Grosz and Sidner 1996), the approach used in (Marcu 1997), extends the problem of cue phrase disambiguation by distinguishing the discourse sense of a cue phrase into finer meanings, corresponding to the rhetorical relations it indicates.
References-found: 15

