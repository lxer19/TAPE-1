URL: http://www.cs.wisc.edu/~fischer/ftp/pub/sohi/theses/austin.ps.gz
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/sohi/theses/
Root-URL: http://www.cs.wisc.edu
Title: HARDWARE AND SOFTWARE MECHANISMS FOR REDUCING LOAD LATENCY  
Author: By Todd Michael Austin 
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy (Computer Sciences) at the  
Date: 1996  
Address: WISCONSIN MADISON  
Affiliation: UNIVERSITY OF  
Abstract-found: 0
Intro-found: 1
Reference: [AHH88] <author> A. Agarwal, J. Hennessy, and M. Horowitz. </author> <title> Cache performance of operating systems and multiprogramming. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(4) </volume> <pages> 393-431, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: Techniques that work to reduce the frequency of cache misses usually attack the problem of reducing conflict misses. Approaches along these lines include set-associative caches [KJLH89, Hea86, Smi82a], column-associative caches <ref> [AP93, AHH88] </ref>, stride tolerant address mappings [Sez93, IL89, CL89], static [Kes91, DS91] or dynamic [BLRC94, LBF92, Kes91] page coloring, program restructuring [LRW91, Wu92, PH90, Fer76], and reference exclusion [McF92, CD89, ASW + 93, Hsu94, Con92].
Reference: [AP93] <author> A. Agarwal and S. D. Pudar. </author> <title> Column-associative caches: A technique for reducing the miss rate of direct-mapped caches. </title> <booktitle> Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <volume> 21(2) </volume> <pages> 179-190, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Techniques that work to reduce the frequency of cache misses usually attack the problem of reducing conflict misses. Approaches along these lines include set-associative caches [KJLH89, Hea86, Smi82a], column-associative caches <ref> [AP93, AHH88] </ref>, stride tolerant address mappings [Sez93, IL89, CL89], static [Kes91, DS91] or dynamic [BLRC94, LBF92, Kes91] page coloring, program restructuring [LRW91, Wu92, PH90, Fer76], and reference exclusion [McF92, CD89, ASW + 93, Hsu94, Con92]. <p> Reducing the frequency of cache misses also works to reduce the performance impact of cache misses; approaches along these lines include set-associative caches [KJLH89, Hea86], column-associative caches <ref> [AP93] </ref>, stride tolerant address mappings [Gao93, SL93, Sez93], page coloring [Kes91, DS91, BLRC94], and program restructuring to improve data [CMT94, LRW91] or instruction cache performance [Wu92, PH90, McF89].
Reference: [APS95] <author> T. M. Austin, D. N. Pnevmatikatos, and G. S. Sohi. </author> <title> Streamlining data cache access with fast address calculation. </title> <booktitle> Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <volume> 23(2) </volume> <pages> 369-380, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Prediction performance with 16 byte blocks was also examined (detailed results can be found in an earlier paper on fast address calculation <ref> [APS95] </ref>). Overall the prediction failure rate decreased slightly when the block size increased, since misaligned pointers benefited from more full addition capability in the fast address calculation mechanism. As Figure 3.5 shows, software support was extremely successful at decreasing the failure rate of effective address predictions.
Reference: [AS92] <author> T. M. Austin and G. S. Sohi. </author> <title> Dynamic dependency analysis of ordinary programs. </title> <booktitle> In Conference Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 342-351. </pages> <institution> Association for Computing Machinery, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: The scheduler attempts to place independent instructions between loads and their first use, keeping pipeline resources utilized until loads complete. To make good schedules, the scheduler needs independent work, which is finite and usually quite small in the basic blocks of control intensive codes, e.g., many integer codes <ref> [AS92] </ref>. Global scheduling techniques [Fis81, MLC + 92, ME92] have been developed as a way to mitigate this effect; however, these techniques often suffer from ambiguous dependencies, unpredictable latencies, and safety issues that limit the extent of their effectiveness.
Reference: [ASU86] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1986. </year>
Reference-contexts: For general pointer accesses, most load offsets are small. In fact, for a number of programs analyzed, e.g., GCC, zero was the most common offset used. Zero offsets are primarily the product of array subscript operations where strength reduction <ref> [ASU86] </ref> of the subscript expression succeeded, pointer dereferences to basic types (e.g., integers), and pointer dereferences to the first element of a structured (record) variable. Non-zero offsets arise from primarily three sources: structure offsets, some array accesses, and array index constants.
Reference: [ASW + 93] <author> S. G. Abraham, R. A. Sugumar, D. Windheiser, B. R. Rau, and R. Gupta. </author> <title> Predictability of load/store instruction latencies. </title> <booktitle> Proceedings of the 26th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 139-152, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Approaches along these lines include set-associative caches [KJLH89, Hea86, Smi82a], column-associative caches [AP93, AHH88], stride tolerant address mappings [Sez93, IL89, CL89], static [Kes91, DS91] or dynamic [BLRC94, LBF92, Kes91] page coloring, program restructuring [LRW91, Wu92, PH90, Fer76], and reference exclusion <ref> [McF92, CD89, ASW + 93, Hsu94, Con92] </ref>.
Reference: [AVS93] <author> T. M. Austin, T.N. Vijaykumar, and G. S. Sohi. </author> <title> Knapsack: A zero-cycle memory hierarchy component. </title> <type> Technical Report TR 1189, </type> <institution> Computer Sciences Department, UW-Madison, Madison, WI, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: At cache misses, memory operand specifiers within instructions were replaced with direct stack cache addresses. When the partially decoded instructions were executed, operands in the stack cache could be accessed as quickly as registers. The knapsack memory <ref> [AVS93] </ref> component included support for zero-cycle loads. Software support was used to place data into the power-of-two aligned knapsack region, providing zero-cycle access to these variables when made with the architecturally-defined knapsack pointer. This optimization was limited primarily to global data.
Reference: [BC91] <author> J.-L. Baer and T.-F. Chen. </author> <title> An effective on-chip preloading scheme to reduce data access penalty. </title> <booktitle> Supercomputing '91, </booktitle> <pages> pages 176-186, </pages> <year> 1991. </year>
Reference-contexts: the speedups are better than those found on the 32 register architecture due to the excellent performance of the many extra stack and global accesses. 4.5 Related Work The application of early issue as a means of reducing load latency has been gainfully applied in a number of previous works <ref> [BC91, EV93, GM93] </ref>. All use an address predictor mechanism, which is a variant of the load delta table [EV93], to generate addresses early in the pipeline, allowing loads to be initiated earlier than the execute stage.
Reference: [BCT92] <author> P. Briggs, K. D. Cooper, and L. Torczon. </author> <title> Rematerialization. </title> <booktitle> Proceedings of the ACM SIG-PLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <volume> 27(7) </volume> <pages> 311-321, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Register allocation is a well developed area and continues to progress. Current research centers on increasing the number of candidates for register allocation, e.g., register 6 allocation for subscripted variables [CCK90], and increasing the utilization of a finite collection of registers through techniques such as live-range splitting <ref> [BCT92] </ref> and load/store range analysis [KH92b]. Unfortunately, many program variables are still forced into memory, due to the limited size and addressability of register files. Cache misses are often a significant component of load latency, especially in numeric codes where data locality is low.
Reference: [BEH91] <author> D. G. Bradlee, S. J. Eggers, and R. R. Henry. </author> <title> Integrating register allocation and instruction scheduling for RISCs. </title> <booktitle> Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <volume> 19(2) </volume> <pages> 122-131, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: While the current phase ordering provided the best performance for this framework, modifications to the algorithms to control phase interactions would likely result in better performance. Phase interactions between register allocation and instruction scheduling algorithms have been effectively controlled with heuristic methods <ref> [BEH91] </ref>. * The current optimization framework places all variables at compile time. While inexpensive and simple to implement, the approach lacks the ability to adapt to a particular run of the program. A challenging area of future exploration is run-time variable placement.
Reference: [BF92] <author> B. K. Bray and M. J. Flynn. </author> <title> Translation hint buffers to reduce access time of physically-addressed instruction caches. </title> <booktitle> Proceedings of the 25th Annual International Symposium on Microarchitecture, </booktitle> <volume> 23(1) </volume> <pages> 206-209, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: On either of these events, the previous translation is invalidated and another address translation of the PC is initiated. The translation hit buffer (THB) <ref> [BF92] </ref> further extends this idea to include a prediction of the next translation as well.
Reference: [BHIL94] <author> J. Borkenhagen, G. Handlogten, J. Irish, and S. Levenstein. </author> <title> AS/400 64-bit PowerPC-compatible processor implementation. </title> <booktitle> ICCD, </booktitle> <year> 1994. </year>
Reference-contexts: Already, some processor designs have turned to alternative TLB organizations with better latency and bandwidth characteristics; for example, Hal's SPARC64 [Gwe95] and IBM's AS/400 64-bit PowerPC <ref> [BHIL94] </ref> processor both implement multi-level TLBs. Many processors implement multi-level TLBs for instruction fetch translation as well [CBJ92]. This chapter extends the work in high-bandwidth address translation design by introducing four designs with better latency and area characteristics than a multi-ported TLB. <p> area overhead of this design is concentrated in the implementation of the L1 TLB, which for small sizes and few ports should be much smaller than the L2 TLB. 63 At least two commercial processors have explored the use of multi-level TLBs; Hal's SPARC64 [Gwe95] and IBM's AS/400 64-bit PowerPC <ref> [BHIL94] </ref> processors both implement multi-level TLBs to meet the latency and bandwidth needs of their respective designs. Multi-level TLB designs have long been used for reducing the latency of instruction fetch translations [CBJ92]. 5.3.4 Piggyback Ports Piggyback ports, shown in Figure 5.3, exploit spatial locality in simultaneous address translation requests.
Reference: [BKW90] <author> A. Borg, R. E. Kessler, and D. W. Wall. </author> <title> Generation and analysis of very long address traces. </title> <booktitle> Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <volume> 18(2) </volume> <pages> 270-279, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Cache misses are often a significant component of load latency, especially in numeric codes where data locality is low. Much work has been done to reduce both the latency and frequency of data cache misses. Approaches that work to reduce miss latencies include multi-level caches <ref> [JW94, BKW90, WBL89] </ref>, victim caches [Jou90], and cache line prefetching [RL92, CBM + 92, MLG92]. Non-blocking caches [FJ94, CB92, Con92, SF91] also help to reduce the impact of cache misses by letting other cache accesses complete while misses are serviced. <p> It is also possible to reduce the latency of cache misses using techniques that include multi-level caches <ref> [JW94, BKW90, WBL89] </ref>, victim caches [Jou90], and prefetching [RL92, CBM + 92, MLG92].
Reference: [BLRC94] <author> B. N Bershad, D. Lee, T. H. Romer, and J. B. Chen. </author> <title> Avoiding conflict misses dynamically in large direct-mapped caches. </title> <booktitle> Conference Proceedings of the Sixth International Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1994. </year> <month> 116 </month>
Reference-contexts: Techniques that work to reduce the frequency of cache misses usually attack the problem of reducing conflict misses. Approaches along these lines include set-associative caches [KJLH89, Hea86, Smi82a], column-associative caches [AP93, AHH88], stride tolerant address mappings [Sez93, IL89, CL89], static [Kes91, DS91] or dynamic <ref> [BLRC94, LBF92, Kes91] </ref> page coloring, program restructuring [LRW91, Wu92, PH90, Fer76], and reference exclusion [McF92, CD89, ASW + 93, Hsu94, Con92]. <p> Reducing the frequency of cache misses also works to reduce the performance impact of cache misses; approaches along these lines include set-associative caches [KJLH89, Hea86], column-associative caches [AP93], stride tolerant address mappings [Gao93, SL93, Sez93], page coloring <ref> [Kes91, DS91, BLRC94] </ref>, and program restructuring to improve data [CMT94, LRW91] or instruction cache performance [Wu92, PH90, McF89]. In this chapter, a novel software-based variable placement optimization, called cache-conscious data placement, is introduced as a technique for reducing the frequency of data cache misses. <p> TLB miss and page fault rates. The approaches presented in this paper would certainly extend to lower levels of the memory hierarchy as well; this is a potential area for future exploration. A number of peripheral works employ data relocation to improve data cache performance. Page coloring <ref> [BLRC94, LBF92, Kes91] </ref> techniques have leveraged the memory mapping capability of virtual memory to reduce conflicts in physically indexed caches. User-programmable cache set mappings [DS91] have been proposed to provide the same benefits. <p> Any implementation would likely benefit from novel hardware and/or software support for profiling and placing variables. Run-time placement techniques have already been shown to work well for virtual memory page placement <ref> [BLRC94] </ref>. * The current placement algorithms targeted performance improvements for a user-specified fixed-size target cache geometry. Performance improvements on other cache geometries were not as impressive and in many cases stability was lost.
Reference: [BRG + 89] <author> D. Black, R. Rashid, D. Golub, C. Hill, and R. Baron. </author> <title> Translation lookaside buffer consistency: A software approach. </title> <booktitle> Proceedings of the Third International Conference on Architectural Support for Programming Languages Operating Systems, </booktitle> <pages> pages 113-122, </pages> <year> 1989. </year>
Reference-contexts: Since the L1 TLB is small, it may be possible to use a more effective replacement policy (e.g., LRU replacement in the L1 TLB vs. random replacement in the L2 TLB), which should improve the hit rate of the L1 TLB. If the processor supports hardware-based TLB consistency operations <ref> [BRG + 89] </ref>, multi-level inclusion should be enforced in the L1 TLB during L2 TLB replacements or invalidations, i.e., the entries in the L1 TLB should be a subset of the entries in the L2 TLB.
Reference: [BZ93] <author> D. A. Barrett and B. G. Zorn. </author> <title> Using lifetime predictors to improve memory allocation performance. </title> <booktitle> Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <volume> 28(6) </volume> <pages> 187-196, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Heap variables are named when they are created (e.g., at calls to malloc ()) using the address of the call site to malloc () combined (with XOR-folding) with a few return addresses from the stack. (Similar heap naming schemes were employed by Lebeck and Wood [LW94] and Barrett and Zorn <ref> [BZ93] </ref>.) This naming approach does a reasonably good job of satisfying the constraints listed above. Since the addresses of calls sites and function returns do not change between runs of a program (provided the program is not recompiled), heap variable names do not change between runs. <p> The Scout operating system [MMO + 94] employs data placement to reduce data cache conflict between active protocol stacks. The approach used to name heap variables was adopted from <ref> [BZ93] </ref>. The history of the development of this work is relevant related work. The initial placement strategy tried to fill the padding space created by variable alignment with other variables. <p> A P -quantile profile is a profile with variable-size buckets which track the start and end points of where each 1=P of the samples lie. One disadvantage of P -quantile profiles is that they require two passes over the sample stream, however, P -quantile estimators <ref> [BZ93, JC85] </ref> have been shown to work well with only a single pass. Alternatively, profiles could support progressively increasing buckets sizes like those employed in the Paradyn parallel performance measurement tools [MCC + 95]. 115
Reference: [CB92] <author> T.-F. Chen and J.-L. Baer. </author> <title> Reducing memory latency via non-blocking and prefetching caches. </title> <booktitle> Conference Proceedings of the Fifth International Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <volume> 27(9) </volume> <pages> 51-61, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Much work has been done to reduce both the latency and frequency of data cache misses. Approaches that work to reduce miss latencies include multi-level caches [JW94, BKW90, WBL89], victim caches [Jou90], and cache line prefetching [RL92, CBM + 92, MLG92]. Non-blocking caches <ref> [FJ94, CB92, Con92, SF91] </ref> also help to reduce the impact of cache misses by letting other cache accesses complete while misses are serviced. Techniques that work to reduce the frequency of cache misses usually attack the problem of reducing conflict misses.
Reference: [CBJ92] <author> J. B. Chen, A. Borg, and N. P. Jouppi. </author> <title> A simulation based study of TLB performance. </title> <booktitle> Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <volume> 19(2) </volume> <pages> 114-123, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Already, some processor designs have turned to alternative TLB organizations with better latency and bandwidth characteristics; for example, Hal's SPARC64 [Gwe95] and IBM's AS/400 64-bit PowerPC [BHIL94] processor both implement multi-level TLBs. Many processors implement multi-level TLBs for instruction fetch translation as well <ref> [CBJ92] </ref>. This chapter extends the work in high-bandwidth address translation design by introducing four designs with better latency and area characteristics than a multi-ported TLB. Using detailed timing simulation, the performance of the proposed high-bandwidth designs was compared to the performance of a TLB with unlimited bandwidth. <p> Instruction fetch translation is well served by a single-ported instruction TLB or by a small micro-TLB implemented over a unified instruction and data TLB <ref> [CBJ92] </ref>. The remainder of this chapter is organized as follows. Section 5.2 describes a performance model for address translation and qualitatively explores the impact of address translation latency and bandwidth on system performance. <p> Multi-level TLB designs have long been used for reducing the latency of instruction fetch translations <ref> [CBJ92] </ref>. 5.3.4 Piggyback Ports Piggyback ports, shown in Figure 5.3, exploit spatial locality in simultaneous address translation requests. When simultaneous requests arrive at a TLB port, requests with identical virtual page addresses may be satisfied by the same TLB access.
Reference: [CBM + 92] <author> W. Y. Chen, R. A. Bringmann, S. A. Mahlke, R. E. Hank, and J. E. Sicolo. </author> <title> An efficient architecture for loop based data preloading. </title> <booktitle> Proceedings of the 25th Annual International Symposium on Microarchitecture, </booktitle> <volume> 23(1) </volume> <pages> 92-101, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: Much work has been done to reduce both the latency and frequency of data cache misses. Approaches that work to reduce miss latencies include multi-level caches [JW94, BKW90, WBL89], victim caches [Jou90], and cache line prefetching <ref> [RL92, CBM + 92, MLG92] </ref>. Non-blocking caches [FJ94, CB92, Con92, SF91] also help to reduce the impact of cache misses by letting other cache accesses complete while misses are serviced. Techniques that work to reduce the frequency of cache misses usually attack the problem of reducing conflict misses. <p> It is also possible to reduce the latency of cache misses using techniques that include multi-level caches [JW94, BKW90, WBL89], victim caches [Jou90], and prefetching <ref> [RL92, CBM + 92, MLG92] </ref>.
Reference: [CCH + 87] <author> F. Chow, S. Correll, M. Himelstein, E. Killian, and L. Weber. </author> <title> How many addressing modes are enough. </title> <booktitle> Conference Proceedings of the Second International Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 117-121, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: Global pointer addressing is used to access small global (static) variables. The SimpleScalar (and MIPS) approach to global pointer addressing uses a reserved immutable register, called the global pointer, plus a constant offset to access variables in the global region of the program's data segment <ref> [CCH + 87] </ref>. The linker constructs the global region such that all variables referenced by name are grouped together near the target address of the global pointer. As shown in Table 3.1, global pointer addressing is prevalent in some programs, but not all. <p> The addressing mode field specifies either a register+constant or register+register addressing (if supported in the ISA). The base register type is one of the following: SP, GP, or Other. SP and GP loads use the stack or global pointer <ref> [CCH + 87] </ref> as a base register, respectively. Loads 38 marked "Other" use a register other than the stack or global pointer as a base register. The offset field specifies the offset of the load if it is contained as an immediate value in the instruction. <p> Figure 5.4 illustrates the basis for this approach. Loads and stores access memory through register pointers: global accesses through the global pointer <ref> [CCH + 87] </ref>, stack accesses through the stack pointer, and all other references through general purpose register pointers. Pointers are created whenever a variable is referenced, its address is taken, or when dynamic storage is allocated.
Reference: [CCK90] <author> D. Callahan, S. Carr, and K. Kennedy. </author> <title> Improving register allocation for subscripted variables. </title> <booktitle> Proceedings of the ACM SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <volume> 25(6) </volume> <pages> 53-65, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Register allocation is a well developed area and continues to progress. Current research centers on increasing the number of candidates for register allocation, e.g., register 6 allocation for subscripted variables <ref> [CCK90] </ref>, and increasing the utilization of a finite collection of registers through techniques such as live-range splitting [BCT92] and load/store range analysis [KH92b]. Unfortunately, many program variables are still forced into memory, due to the limited size and addressability of register files.
Reference: [CD89] <author> C.-H. Chi and H. Dietz. </author> <title> Unified management of registers and cache using liveness and cache bypass. </title> <booktitle> Proceedings of the ACM SIGPLAN '89 Conference on Programming Language Design and Implementation, </booktitle> <volume> 24(7) </volume> <pages> 344-355, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: Approaches along these lines include set-associative caches [KJLH89, Hea86, Smi82a], column-associative caches [AP93, AHH88], stride tolerant address mappings [Sez93, IL89, CL89], static [Kes91, DS91] or dynamic [BLRC94, LBF92, Kes91] page coloring, program restructuring [LRW91, Wu92, PH90, Fer76], and reference exclusion <ref> [McF92, CD89, ASW + 93, Hsu94, Con92] </ref>.
Reference: [Che87] <author> R. Cheng. </author> <title> Virtual address caches in UNIX. </title> <booktitle> Proceedings of the Summer 1987 USENIX Technical Conference, </booktitle> <pages> pages 217-224, </pages> <year> 1987. </year>
Reference-contexts: In a multiprocessing environment, cache coherence operations must first be reverse-translated to remote virtual addresses before data can be located in the remote cache. Many solutions have been devised to eliminate synonyms, including alignment restrictions on shared data <ref> [Che87] </ref>, selective invalidation [WBL89], and single address space operating systems [KCE92]. However, these approaches have yet to come into widespread use due to performance and/or implementation impacts on application and system software.
Reference: [CK92] <author> T. Chiueh and R. H. Katz. </author> <title> Eliminating the address translation bottleneck for physical address cache. </title> <booktitle> Proceedings of the Fifth International Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <volume> 27(9) </volume> <pages> 137-148, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: The translation hit buffer (THB) [BF92] further extends this idea to include a prediction of the next translation as well. Pretranslation can be viewed as an extension of Chiueh and Katz's branch address cache (BAC) <ref> [CK92] </ref>, which was applied as a mechanism to reduce access latency of physically indexed caches. (A similar mechanism was proposed in an earlier paper [HHL + 90].) The proposed pretranslation design includes a number of modifications to the original BAC mechanism.
Reference: [CK93] <author> R. F. Cmelik and D. Keppel. Shade: </author> <title> A fast instruction set simulator for execution profiling. </title> <type> Technical Report UWCSE 93-06-06, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <address> Seattle, WA, </address> <year> 1993. </year>
Reference-contexts: A simulator-based profiler, while simple to extend for event monitoring, results in poor execution performance. In a production environment, performance could be significantly improved using some form of monitoring that supports direct execution of the profiled program. Suitable 85 approaches along these lines include dynamic compilation <ref> [CK93] </ref> and executable editing [LS95]. Efficient binding of memory reference addresses to variable profiles is implemented though the use of the instance map. The instance map shadows the entire virtual address space, providing a pointer to a profile for each allocated address in the virtual address space.
Reference: [CL89] <author> C.-L. Chen and C.-K. Liao. </author> <title> Analysis of vector access performance on skewed interleaved memory. </title> <booktitle> Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <volume> 17(3) </volume> <pages> 387-394, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Techniques that work to reduce the frequency of cache misses usually attack the problem of reducing conflict misses. Approaches along these lines include set-associative caches [KJLH89, Hea86, Smi82a], column-associative caches [AP93, AHH88], stride tolerant address mappings <ref> [Sez93, IL89, CL89] </ref>, static [Kes91, DS91] or dynamic [BLRC94, LBF92, Kes91] page coloring, program restructuring [LRW91, Wu92, PH90, Fer76], and reference exclusion [McF92, CD89, ASW + 93, Hsu94, Con92]. <p> Page coloring [BLRC94, LBF92, Kes91] techniques have leveraged the memory mapping capability of virtual memory to reduce conflicts in physically indexed caches. User-programmable cache set mappings [DS91] have been proposed to provide the same benefits. Compiler-directed dimension extension <ref> [CL89] </ref> works to relocate data within large arrays, giving opportunity to improve data cache performance when a large array conflicts with itself. Data placement optimizations have been used to reduce false sharing in shared memory multiprocessors [JE95].
Reference: [CMMP95] <author> T. Conte, K. Menezes, P. Mills, and B. Patel. </author> <title> Optimization of instruction fetch mechanisms for high issue rates. </title> <booktitle> Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <volume> 23(2) </volume> <pages> 333-344, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: The branch predictor was changed to a 2-level adaptive predictor with an 8 bit global history pattern indexing a 4096 entry pattern history table (i.e. the GAp method from [YP93]) with 2-bit saturating counters. In addition, a limited variant of the collapsing buffer (described in <ref> [CMMP95] </ref>) was added to increase fetch bandwidth.
Reference: [CMT94] <author> S. Carr, K. S. McKinley, and C.-W. Tseng. </author> <title> Compiler optimizations for improving data locality. </title> <booktitle> Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <volume> 28(5) </volume> <pages> 252-262, </pages> <month> October </month> <year> 1994. </year> <month> 117 </month>
Reference-contexts: Reducing the frequency of cache misses also works to reduce the performance impact of cache misses; approaches along these lines include set-associative caches [KJLH89, Hea86], column-associative caches [AP93], stride tolerant address mappings [Gao93, SL93, Sez93], page coloring [Kes91, DS91, BLRC94], and program restructuring to improve data <ref> [CMT94, LRW91] </ref> or instruction cache performance [Wu92, PH90, McF89]. In this chapter, a novel software-based variable placement optimization, called cache-conscious data placement, is introduced as a technique for reducing the frequency of data cache misses.
Reference: [Con92] <author> T. M. Conte. </author> <title> Tradeoffs in processor/memory interfaces for superscalar processors. </title> <booktitle> Proceedings of the 25th Annual International Symposium on Microarchitecture, </booktitle> <volume> 23(1) </volume> <pages> 202-205, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: Much work has been done to reduce both the latency and frequency of data cache misses. Approaches that work to reduce miss latencies include multi-level caches [JW94, BKW90, WBL89], victim caches [Jou90], and cache line prefetching [RL92, CBM + 92, MLG92]. Non-blocking caches <ref> [FJ94, CB92, Con92, SF91] </ref> also help to reduce the impact of cache misses by letting other cache accesses complete while misses are serviced. Techniques that work to reduce the frequency of cache misses usually attack the problem of reducing conflict misses. <p> Approaches along these lines include set-associative caches [KJLH89, Hea86, Smi82a], column-associative caches [AP93, AHH88], stride tolerant address mappings [Sez93, IL89, CL89], static [Kes91, DS91] or dynamic [BLRC94, LBF92, Kes91] page coloring, program restructuring [LRW91, Wu92, PH90, Fer76], and reference exclusion <ref> [McF92, CD89, ASW + 93, Hsu94, Con92] </ref>.
Reference: [Dav95] <author> B. Davidson. </author> <title> The Microsoft LEGO system. </title> <booktitle> UW-Madison Programming Languages Seminar presentation, </booktitle> <month> September </month> <year> 1995. </year>
Reference-contexts: Techniques such as basic block re-ordering <ref> [Dav95, Wu92, PH90] </ref>, function grouping [Wu92, PH90, Fer76, Fer74, HG71], and text ordering based on control structure [McF91] have all been shown to significantly improve instruction cache performance.
Reference: [DM82] <author> D. R. Ditzel and H. R. McLellan. </author> <title> Register allocation for free: the C machine stack cache. </title> <booktitle> In 1st International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 48-56, </pages> <address> Palo Alto, CA, </address> <month> March </month> <year> 1982. </year>
Reference-contexts: Hardware-based early-issue of loads, on the other hand, benefits from run-time information and the ability to dynamically mask faults, making it possible to hoist loads past stores and branches that would otherwise form barriers for the compiler. The C Machine <ref> [DM82] </ref> used a novel approach to implement zero-cycle access to stack frame variables. At cache misses, memory operand specifiers within instructions were replaced with direct stack cache addresses. When the partially decoded instructions were executed, operands in the stack cache could be accessed as quickly as registers.
Reference: [DS91] <author> F. Dahlgren and P. Stenstrom. </author> <title> On reconfigurable on-chip data caches. </title> <booktitle> Proceedings of the 24th Annual International Symposium on Microarchitecture, </booktitle> <volume> 22(1) </volume> <pages> 189-198, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Techniques that work to reduce the frequency of cache misses usually attack the problem of reducing conflict misses. Approaches along these lines include set-associative caches [KJLH89, Hea86, Smi82a], column-associative caches [AP93, AHH88], stride tolerant address mappings [Sez93, IL89, CL89], static <ref> [Kes91, DS91] </ref> or dynamic [BLRC94, LBF92, Kes91] page coloring, program restructuring [LRW91, Wu92, PH90, Fer76], and reference exclusion [McF92, CD89, ASW + 93, Hsu94, Con92]. <p> Reducing the frequency of cache misses also works to reduce the performance impact of cache misses; approaches along these lines include set-associative caches [KJLH89, Hea86], column-associative caches [AP93], stride tolerant address mappings [Gao93, SL93, Sez93], page coloring <ref> [Kes91, DS91, BLRC94] </ref>, and program restructuring to improve data [CMT94, LRW91] or instruction cache performance [Wu92, PH90, McF89]. In this chapter, a novel software-based variable placement optimization, called cache-conscious data placement, is introduced as a technique for reducing the frequency of data cache misses. <p> A number of peripheral works employ data relocation to improve data cache performance. Page coloring [BLRC94, LBF92, Kes91] techniques have leveraged the memory mapping capability of virtual memory to reduce conflicts in physically indexed caches. User-programmable cache set mappings <ref> [DS91] </ref> have been proposed to provide the same benefits. Compiler-directed dimension extension [CL89] works to relocate data within large arrays, giving opportunity to improve data cache performance when a large array conflicts with itself. Data placement optimizations have been used to reduce false sharing in shared memory multiprocessors [JE95].
Reference: [ECP96] <author> M. Evers, P.-Y. Chang, and Y. N. Patt. </author> <title> Using hybrid branch predictors to improve branch prediction accuracy in the presence of context switches. </title> <booktitle> Proceedings of the 23nd Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Stateful address predictors, like the load delta table [EV93], have been shown to work well on array accesses. It seems likely that combining the two approaches could produce more accurate, albeit more expensive, predictor designs. Similar hybrid approaches have worked well for branch prediction <ref> [ECP96] </ref>. 7.2.3 Leveraging Address Prediction Information A common mechanism used repeatedly in this thesis is address prediction. Fast address calculation employs a stateless set index predictor, as do zero-cycle loads. Pretranslation predicts the next virtual page address a base register will access.
Reference: [EV93] <author> R. J. Eickemeyer and S. Vassiliadis. </author> <title> A load-instruction unit for pipelined processors. </title> <journal> IBM J. Res. Develop., </journal> <volume> 37(4) </volume> <pages> 547-564, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: the speedups are better than those found on the 32 register architecture due to the excellent performance of the many extra stack and global accesses. 4.5 Related Work The application of early issue as a means of reducing load latency has been gainfully applied in a number of previous works <ref> [BC91, EV93, GM93] </ref>. All use an address predictor mechanism, which is a variant of the load delta table [EV93], to generate addresses early in the pipeline, allowing loads to be initiated earlier than the execute stage. <p> All use an address predictor mechanism, which is a variant of the load delta table <ref> [EV93] </ref>, to generate addresses early in the pipeline, allowing loads to be initiated earlier than the execute stage. The load delta table tracks both the previous address accessed 53 by a particular load and one or more computed stride values used to predict a load's next effective address. <p> During the lifetime of a pointer, it is dereferenced at loads and stores, and manipulated using integer arithmetic. Over the lifetime of the pointer, it may be dereferenced and manipulated many times. Studies have shown, e.g., Eickemeyer and Vassiliadis' work <ref> [EV93] </ref>, that when pointers are manipulated, it is often the case that small constant values are added to or subtracted from the pointer. The end result, which this design exploits, is that successive dereferences of a pointer often yield accesses to the same virtual memory page. <p> These accesses are the result of array accesses that the compiler cannot strength-reduce. Stateful address predictors, like the load delta table <ref> [EV93] </ref>, have been shown to work well on array accesses. It seems likely that combining the two approaches could produce more accurate, albeit more expensive, predictor designs.
Reference: [Fer74] <author> D. Ferrari. </author> <title> Improving locality by critical working sets. </title> <journal> Communications of the ACM, </journal> <volume> 17(11) </volume> <pages> 614-620, </pages> <month> November </month> <year> 1974. </year>
Reference-contexts: Techniques such as basic block re-ordering [Dav95, Wu92, PH90], function grouping <ref> [Wu92, PH90, Fer76, Fer74, HG71] </ref>, and text ordering based on control structure [McF91] have all been shown to significantly improve instruction cache performance.
Reference: [Fer76] <author> D. Ferrari. </author> <title> The improvement of program behavior. </title> <journal> Computer, </journal> <volume> 9(11) </volume> <pages> 39-47, </pages> <month> November </month> <year> 1976. </year>
Reference-contexts: Approaches along these lines include set-associative caches [KJLH89, Hea86, Smi82a], column-associative caches [AP93, AHH88], stride tolerant address mappings [Sez93, IL89, CL89], static [Kes91, DS91] or dynamic [BLRC94, LBF92, Kes91] page coloring, program restructuring <ref> [LRW91, Wu92, PH90, Fer76] </ref>, and reference exclusion [McF92, CD89, ASW + 93, Hsu94, Con92]. <p> Techniques such as basic block re-ordering [Dav95, Wu92, PH90], function grouping <ref> [Wu92, PH90, Fer76, Fer74, HG71] </ref>, and text ordering based on control structure [McF91] have all been shown to significantly improve instruction cache performance.
Reference: [Fis81] <author> J. A. Fisher. </author> <title> Trace scheduling: A technique for global microcode compaction. </title> <journal> IEEE Transaction on Computers, </journal> <volume> C-30(7):478-490, </volume> <month> July </month> <year> 1981. </year>
Reference-contexts: To make good schedules, the scheduler needs independent work, which is finite and usually quite small in the basic blocks of control intensive codes, e.g., many integer codes [AS92]. Global scheduling techniques <ref> [Fis81, MLC + 92, ME92] </ref> have been developed as a way to mitigate this effect; however, these techniques often suffer from ambiguous dependencies, unpredictable latencies, and safety issues that limit the extent of their effectiveness.
Reference: [FJ94] <author> K. I. Farkas and N. P. Jouppi. </author> <title> Complexity/performance tradeoffs with non-blocking loads. </title> <booktitle> Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <volume> 22(2) </volume> <pages> 211-222, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Execution models with high levels 4 of latency tolerating capability include those that support such techniques as out-of-order issue [HP90], non-blocking loads <ref> [FJ94] </ref>, and speculative execution [HP90]. The extent to which a program accesses memory (i.e., the dynamic frequency of loads) also affects the degree to which load latency impacts program performance. Programs that access memory often will need better load performance for good overall performance. <p> Much work has been done to reduce both the latency and frequency of data cache misses. Approaches that work to reduce miss latencies include multi-level caches [JW94, BKW90, WBL89], victim caches [Jou90], and cache line prefetching [RL92, CBM + 92, MLG92]. Non-blocking caches <ref> [FJ94, CB92, Con92, SF91] </ref> also help to reduce the impact of cache misses by letting other cache accesses complete while misses are serviced. Techniques that work to reduce the frequency of cache misses usually attack the problem of reducing conflict misses. <p> Much effort has been invested in reducing the impact of cache misses on program performance. As with any other latency, cache miss latency can be tolerated using compile-time techniques such as instruction scheduling [KE93, PS90, GM86], or run-time techniques including out-of-order issue, decoupled execution [Smi82b], or non-blocking loads <ref> [FJ94] </ref>. It is also possible to reduce the latency of cache misses using techniques that include multi-level caches [JW94, BKW90, WBL89], victim caches [Jou90], and prefetching [RL92, CBM + 92, MLG92].
Reference: [FP91] <author> M. Farrens and A. Park. </author> <title> Dynamic base register caching: A technique for reducing address bus width. </title> <booktitle> Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <volume> 19(3) </volume> <pages> 128-137, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: A better alternative for these designs may be to adapt the BRIC as a means for caching register values. Previous studies, e.g., <ref> [FP91] </ref>, have found a significant amount of temporal locality in base and index register accesses. A small cache, on the order of 4 to 8 entries should provide the necessary bandwidth to register values without increasing the number of ports on the existing integer register file. <p> The register specifier-indexed BRIC, used in the two decode stage design, also performed well for all programs. Performance for a even a four entry register specifier-indexed BRIC is very good. This result is to be expected since register file accesses have a significant amount of temporal locality <ref> [FP91] </ref>. The column labeled No GP/SP shows speedups for an 8 entry address-indexed BRIC without separate registers available to cache the global and stack pointer.
Reference: [Fra93] <author> M. Franklin. </author> <title> A New Paradigm for Superscalar Processing. </title> <type> PhD thesis, </type> <institution> UW-Madison, Madison, WI, </institution> <note> To appear 1993. </note>
Reference-contexts: When processor progress is stalled due to a load delay (or other instruction delays), the dynamic scheduler selects another instruction from a window of available instructions or another independent thread of control. Examples of execution models that perform dynamic scheduling include Multiscalar <ref> [Fra93] </ref>, decoupled [Smi82b], dataflow [Vee86], and multi-threaded [LGN92, Smi81]. The best way by far to reduce load latency is to make memory access time zero by moving the accessed storage into a register. Register allocation is a well developed area and continues to progress.
Reference: [Gao93] <author> Q. S. Gao. </author> <title> The chinese remainder theorem and the prime memory system. </title> <booktitle> Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <volume> 21(2) </volume> <pages> 337-340, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Reducing the frequency of cache misses also works to reduce the performance impact of cache misses; approaches along these lines include set-associative caches [KJLH89, Hea86], column-associative caches [AP93], stride tolerant address mappings <ref> [Gao93, SL93, Sez93] </ref>, page coloring [Kes91, DS91, BLRC94], and program restructuring to improve data [CMT94, LRW91] or instruction cache performance [Wu92, PH90, McF89]. In this chapter, a novel software-based variable placement optimization, called cache-conscious data placement, is introduced as a technique for reducing the frequency of data cache misses.
Reference: [GGK + 83] <author> A. Gottlieb, R. Grishman, C. P. Kruskal, K. P. McAuliffe, L. Rudolph, and M. Snir. </author> <title> The nyu ultracomputer designing a mimd, shared memory parallel machine. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 32(2) </volume> <pages> 175-189, </pages> <month> February </month> <year> 1983. </year>
Reference-contexts: If both requests are executing under the same protection domain, the other fields of the translation request, i.e., protection and page status information, may be shared as well. The approach is similar to read combining in multiprocessor interconnection networks <ref> [LS94, GGK + 83] </ref>. Piggyback ports have minimal impact on translation latency. Once a request is submitted to the TLB, all other requesters can compare their virtual addresses in parallel with TLB access.
Reference: [GM86] <author> P. B. Gibbons and S. S. Muchnick. </author> <title> Efficient instruction scheduling for pipelined processors. </title> <booktitle> Proceedings of the SIGPLAN 1986 Symposium on Compiler Construction, </booktitle> <volume> 21(7), </volume> <month> July </month> <year> 1986. </year>
Reference-contexts: The techniques can be broadly bisected into two camps: latency tolerating techniques and latency reducing techniques. Tolerating techniques work by inserting independent instructions into the pipeline delay slots created by load latencies. Local instruction scheduling <ref> [KE93, PS90, GM86] </ref> is a commonly used compile-time technique to tolerate load latency (it is employed in this work as well). The scheduler attempts to place independent instructions between loads and their first use, keeping pipeline resources utilized until loads complete. <p> This observation is further supported by the possibility that the load delta table may perform better on codes where fast address calculation performs poorly (e.g., poorly structured numeric codes). Instruction scheduling <ref> [KE93, PS90, GM86] </ref> is essentially the software dual of the early-issue mechanism, as both work to increase distance between loads and their first use. There are tradeoffs to using either approach. <p> For these codes, cache miss optimizations will have a much greater impact on program performance. Much effort has been invested in reducing the impact of cache misses on program performance. As with any other latency, cache miss latency can be tolerated using compile-time techniques such as instruction scheduling <ref> [KE93, PS90, GM86] </ref>, or run-time techniques including out-of-order issue, decoupled execution [Smi82b], or non-blocking loads [FJ94]. It is also possible to reduce the latency of cache misses using techniques that include multi-level caches [JW94, BKW90, WBL89], victim caches [Jou90], and prefetching [RL92, CBM + 92, MLG92].
Reference: [GM93] <author> M. Golden and T. Mudge. </author> <title> Hardware support for hiding cache latency. </title> <institution> CSE-TR-152-93, University of Michigan, Dept. of Electrical Engineering and Computer Science, </institution> <month> February </month> <year> 1993. </year> <month> 118 </month>
Reference-contexts: increase in cache accesses due to speculation, the impact of store buffer stalls was surprisingly small, typically less than a 1% degradation in the speedups attained with unlimited cache store bandwidth. (The results in Figure 3.6 include the performance impact of store buffer stalls.) 3.7 Related Work Golden and Mudge <ref> [GM93] </ref> explored the use of a load target buffer (LTB) as a means of reducing load latencies. An LTB, loosely based on a branch target buffer, uses the address of a load instruction to predict the effective address early in the pipeline. <p> the speedups are better than those found on the 32 register architecture due to the excellent performance of the many extra stack and global accesses. 4.5 Related Work The application of early issue as a means of reducing load latency has been gainfully applied in a number of previous works <ref> [BC91, EV93, GM93] </ref>. All use an address predictor mechanism, which is a variant of the load delta table [EV93], to generate addresses early in the pipeline, allowing loads to be initiated earlier than the execute stage.
Reference: [GM94] <author> M. Golden and T. Mudge. </author> <title> A comparison of two pipeline organizations. </title> <booktitle> Proceedings of the 27th Annual International Symposium on Microarchitecture, </booktitle> <volume> 25(1) </volume> <pages> 153-161, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Similar approaches have been shown to work well for "AGI" (or late execute) style pipelines <ref> [GM94] </ref>. 7.2.5 Exposing Address Translation to the Compiler The pretranslation technique described in Chapter 5 performed well, but overall worse than a same-sized L1 TLB. The primary reason for this difference lies in the mechanism by which each design reuses translations.
Reference: [Gwe94a] <author> L. Gwennap. </author> <title> Digital leads the pack with 21164. </title> <type> Microprocessor Report, 8(12) </type> <pages> 1-10, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: time. 4.2.2 Implementation with Multiple Decode Stages The increased complexity of instruction decode created by wide issue and faster clock speeds has forced many recent designs to increase the number of pipeline stages between instruction fetch and execute. (Stages collectively referred to as decode stages.) For example, the DEC 21164 <ref> [Gwe94a] </ref> has three decode stages and the MIPS R10000 [Gwe94b] has two. Adding more decode stages increases the mis-predicted branch penalty, however, architects have compensated for this penalty by increasing branch prediction accuracy through such means as larger branch target buffers or more effective predictors, 41 e.g., two-level adaptive [YP93].
Reference: [Gwe94b] <author> L. Gwennap. </author> <title> MIPS R10000 uses decoupled architecture. </title> <type> Microprocessor Report, 8(14) </type> <pages> 18-22, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: complexity of instruction decode created by wide issue and faster clock speeds has forced many recent designs to increase the number of pipeline stages between instruction fetch and execute. (Stages collectively referred to as decode stages.) For example, the DEC 21164 [Gwe94a] has three decode stages and the MIPS R10000 <ref> [Gwe94b] </ref> has two. Adding more decode stages increases the mis-predicted branch penalty, however, architects have compensated for this penalty by increasing branch prediction accuracy through such means as larger branch target buffers or more effective predictors, 41 e.g., two-level adaptive [YP93].
Reference: [Gwe95] <author> L. Gwennap. </author> <title> Hal reveals multichip SPARC processor. </title> <type> Microprocessor Report, 9(3) </type> <pages> 1-11, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: While this design meets the latency and bandwidth requirements of many current designs, continued demands may soon render it impractical, forcing tomorrow's designs to find alternative translation mechanisms. Already, some processor designs have turned to alternative TLB organizations with better latency and bandwidth characteristics; for example, Hal's SPARC64 <ref> [Gwe95] </ref> and IBM's AS/400 64-bit PowerPC [BHIL94] processor both implement multi-level TLBs. Many processors implement multi-level TLBs for instruction fetch translation as well [CBJ92]. This chapter extends the work in high-bandwidth address translation design by introducing four designs with better latency and area characteristics than a multi-ported TLB. <p> The additional area overhead of this design is concentrated in the implementation of the L1 TLB, which for small sizes and few ports should be much smaller than the L2 TLB. 63 At least two commercial processors have explored the use of multi-level TLBs; Hal's SPARC64 <ref> [Gwe95] </ref> and IBM's AS/400 64-bit PowerPC [BHIL94] processors both implement multi-level TLBs to meet the latency and bandwidth needs of their respective designs.
Reference: [Hea86] <author> M. Hill and et al. </author> <title> Design decisions in SPUR. </title> <journal> IEEE Computer, </journal> 19(11) 8-22, November 1986. 
Reference-contexts: Techniques that work to reduce the frequency of cache misses usually attack the problem of reducing conflict misses. Approaches along these lines include set-associative caches <ref> [KJLH89, Hea86, Smi82a] </ref>, column-associative caches [AP93, AHH88], stride tolerant address mappings [Sez93, IL89, CL89], static [Kes91, DS91] or dynamic [BLRC94, LBF92, Kes91] page coloring, program restructuring [LRW91, Wu92, PH90, Fer76], and reference exclusion [McF92, CD89, ASW + 93, Hsu94, Con92]. <p> Reducing the frequency of cache misses also works to reduce the performance impact of cache misses; approaches along these lines include set-associative caches <ref> [KJLH89, Hea86] </ref>, column-associative caches [AP93], stride tolerant address mappings [Gao93, SL93, Sez93], page coloring [Kes91, DS91, BLRC94], and program restructuring to improve data [CMT94, LRW91] or instruction cache performance [Wu92, PH90, McF89].
Reference: [HG71] <author> D. J. Hatfield and J Gerald. </author> <title> Program restructuring for virtual memory. </title> <journal> IBM Systems Journal, </journal> <volume> 10(3) </volume> <pages> 168-192, </pages> <year> 1971. </year>
Reference-contexts: Techniques such as basic block re-ordering [Dav95, Wu92, PH90], function grouping <ref> [Wu92, PH90, Fer76, Fer74, HG71] </ref>, and text ordering based on control structure [McF91] have all been shown to significantly improve instruction cache performance.
Reference: [HHL + 90] <author> K. Hua, A. Hunt, L. Liu, J-K. Peir, D. Pruett, and J. </author> <title> Temple. Early resolution of address translation in cache design. </title> <booktitle> Proceedings of the 1990 IEEE International Conference on Computer Design, </booktitle> <pages> pages 408-412, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Pretranslation can be viewed as an extension of Chiueh and Katz's branch address cache (BAC) [CK92], which was applied as a mechanism to reduce access latency of physically indexed caches. (A similar mechanism was proposed in an earlier paper <ref> [HHL + 90] </ref>.) The proposed pretranslation design includes a number of modifications to the original BAC mechanism. High-bandwidth address translation is accommodated by attaching the virtual page address to register values.
Reference: [HP90] <author> J. L. Hennessy and D. A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers Inc., </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: Execution models with high levels 4 of latency tolerating capability include those that support such techniques as out-of-order issue <ref> [HP90] </ref>, non-blocking loads [FJ94], and speculative execution [HP90]. The extent to which a program accesses memory (i.e., the dynamic frequency of loads) also affects the degree to which load latency impacts program performance. Programs that access memory often will need better load performance for good overall performance. <p> Execution models with high levels 4 of latency tolerating capability include those that support such techniques as out-of-order issue <ref> [HP90] </ref>, non-blocking loads [FJ94], and speculative execution [HP90]. The extent to which a program accesses memory (i.e., the dynamic frequency of loads) also affects the degree to which load latency impacts program performance. Programs that access memory often will need better load performance for good overall performance. <p> The entire 32 bit effective address is not ready until late into the cache access cycle, just in time for the address tag check. The idea of exploiting the two-dimensional structure of memory is being used in several other contexts, such as paged mode DRAM access <ref> [HP90] </ref>. Katevenis and Tzartzanis [KT91] proposed a technique for reducing pipeline branch penalties by rearranging instructions so that both possible targets of a conditional branch are stored in a single instruction cache line. The high bandwidth of the cache is used to fetch both targets of a branch instruction. <p> Address translation is a vital mechanism in modern computer systems. The process provides the operating system with the mapping and protection mechanisms necessary to manage multiple large and private address spaces in a single, limited size physical memory <ref> [HP90] </ref>. In practice, most microprocessors implement low-latency address translation with a translation lookaside buffer (TLB). <p> A similar approach is employed in carry-select adders <ref> [HP90] </ref>. 112 7.2.2 Combining Stateful and Stateless Address Predictors Fast address calculation has a very predictable and easy to identify failure mode, i.e., loads that use register+register mode addressing constitute nearly all of the prediction failures. These accesses are the result of array accesses that the compiler cannot strength-reduce.
Reference: [HS89] <author> M. D. Hill and A. J. Smith. </author> <title> Evaluating associativity in CPU caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12) </volume> <pages> 1612-1630, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: With variable placement to control the contents of data cache blocks, it becomes possible to influence the performance of the data cache. To see how this is possible, consider how changing a variables placement affects a data cache miss from each of the three miss classes <ref> [HS89] </ref>: Conflict Misses: Conflict misses occur when the number of frequently referenced blocks mapping to the same cache set is greater than the associativity of the cache. Blocks that do not fit into the cache set will displace other blocks each time they are referenced.
Reference: [Hsu94] <author> P. Y.-T. Hsu. </author> <title> Designing the TFP microprocessor. </title> <journal> IEEE Micro, </journal> <volume> 14(2) </volume> <pages> 23-33, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Approaches along these lines include set-associative caches [KJLH89, Hea86, Smi82a], column-associative caches [AP93, AHH88], stride tolerant address mappings [Sez93, IL89, CL89], static [Kes91, DS91] or dynamic [BLRC94, LBF92, Kes91] page coloring, program restructuring [LRW91, Wu92, PH90, Fer76], and reference exclusion <ref> [McF92, CD89, ASW + 93, Hsu94, Con92] </ref>. <p> It also removes the load-use hazard that occurs in traditional five stage pipelines, instead introducing an address-use hazard. The address-use hazard stalls the pipeline for one cycle if the computation of the base register value is immediately followed by a dependent load or store. The R8000 (TFP) processor <ref> [Hsu94] </ref> uses a similar approach. Zero-cycle loads can be viewed as essentially a variation on this pipeline design employing early issue rather than late execute.
Reference: [IL89] <author> D. T. Harper III and D. A. Linebarger. </author> <title> A dynamic storage scheme for conflict-free vector access. </title> <booktitle> Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <volume> 17(3) </volume> <pages> 72-77, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Techniques that work to reduce the frequency of cache misses usually attack the problem of reducing conflict misses. Approaches along these lines include set-associative caches [KJLH89, Hea86, Smi82a], column-associative caches [AP93, AHH88], stride tolerant address mappings <ref> [Sez93, IL89, CL89] </ref>, static [Kes91, DS91] or dynamic [BLRC94, LBF92, Kes91] page coloring, program restructuring [LRW91, Wu92, PH90, Fer76], and reference exclusion [McF92, CD89, ASW + 93, Hsu94, Con92].
Reference: [JC85] <author> R. Jain and I. Chlamtac. </author> <title> The P 2 algorithm for dynamic calculation of quantiles and histograms without storing observations. </title> <journal> Communications of the ACM, </journal> <volume> 28(10) </volume> <pages> 1076-1085, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: A P -quantile profile is a profile with variable-size buckets which track the start and end points of where each 1=P of the samples lie. One disadvantage of P -quantile profiles is that they require two passes over the sample stream, however, P -quantile estimators <ref> [BZ93, JC85] </ref> have been shown to work well with only a single pass. Alternatively, profiles could support progressively increasing buckets sizes like those employed in the Paradyn parallel performance measurement tools [MCC + 95]. 115
Reference: [JE95] <author> T. E. Jeremiassen and S. J. Eggers. </author> <title> Reducing false sharing on shared memory multiprocessors through compile-time data transformations. </title> <booktitle> Proceedings of the Symposium on Principals and Practice of Parallel Programming, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Compiler-directed dimension extension [CL89] works to relocate data within large arrays, giving opportunity to improve data cache performance when a large array conflicts with itself. Data placement optimizations have been used to reduce false sharing in shared memory multiprocessors <ref> [JE95] </ref>. Compiler-directed variable partitioning has been proposed as an approach to reduce inter-variable interactions [Mue95] for the purpose of improving the predictability of cache access latencies in real-time systems. The Scout operating system [MMO + 94] employs data placement to reduce data cache conflict between active protocol stacks.
Reference: [Jol91] <author> R. Jolly. A 9-ns 1.4 gigabyte/s, </author> <title> 17-ported CMOS register file. </title> <journal> IEEE J. of Solid-State Circuits, </journal> <volume> 25 </volume> <pages> 1407-1412, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: In addition, this design has a large area due to the many wires and comparators needed to implement each port. (In CMOS technology, the area of a multi-ported device is proportional to the square of the number of ports <ref> [Jol91] </ref>.) Independent of access latency and implementation area considerations, this design provides the best bandwidth and hit rate of all the designs, hence, it provides a convenient standard for gauging the performance of the other proposed designs. 5.3.2 Interleaved TLB An interleaved TLB, shown in Figure 5.2b, employs an interconnect to
Reference: [Jou89] <author> N. P. Jouppi. </author> <title> Architecture and organizational tradeoffs in the design of the MultiTitan CPU. </title> <booktitle> Proceedings of the 16st Annual International Symposium on Computer Architecture, </booktitle> <volume> 17(3) </volume> <pages> 281-289, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: The knapsack memory [AVS93] component included support for zero-cycle loads. Software support was used to place data into the power-of-two aligned knapsack region, providing zero-cycle access to these variables when made with the architecturally-defined knapsack pointer. This optimization was limited primarily to global data. Jouppi <ref> [Jou89] </ref> proposed a pipeline that performed ALU operations and memory access in the same stage. The pipeline employs a separate address generation pipeline stage, pushing the execution of ALU instructions and cache access to the same pipeline stage. This organization increases the mispredicted branch penalty by one cycle.
Reference: [Jou90] <author> N. P. Jouppi. </author> <title> Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers. </title> <booktitle> Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <volume> 18(2) </volume> <pages> 364-373, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Much work has been done to reduce both the latency and frequency of data cache misses. Approaches that work to reduce miss latencies include multi-level caches [JW94, BKW90, WBL89], victim caches <ref> [Jou90] </ref>, and cache line prefetching [RL92, CBM + 92, MLG92]. Non-blocking caches [FJ94, CB92, Con92, SF91] also help to reduce the impact of cache misses by letting other cache accesses complete while misses are serviced. <p> It is also possible to reduce the latency of cache misses using techniques that include multi-level caches [JW94, BKW90, WBL89], victim caches <ref> [Jou90] </ref>, and prefetching [RL92, CBM + 92, MLG92].
Reference: [Jou93] <author> N. P. Jouppi. </author> <title> Cache write policies and performance. </title> <booktitle> Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <volume> 21(2) </volume> <pages> 191-201, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Using this strategy, stores will complete earlier in the pipeline as well, reducing the frequency of stalled loads. Of course, this design must ensure that misspeculated stores can be undone. For designs employing a store buffer <ref> [Jou93] </ref> and a two-cycle store sequence, this may not pose a problem. In the first cycle, cache tags are probed to see if the access hits in the cache, in the second (possibly much later) cycle, the store is made to the cache.
Reference: [JW94] <author> N. P. Jouppi and S. J.E. Wilton. </author> <title> Tradeoffs in two-level on-chip caching. </title> <booktitle> Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <volume> 22(2) </volume> <pages> 34-45, </pages> <month> April </month> <year> 1994. </year> <month> 119 </month>
Reference-contexts: Cache misses are often a significant component of load latency, especially in numeric codes where data locality is low. Much work has been done to reduce both the latency and frequency of data cache misses. Approaches that work to reduce miss latencies include multi-level caches <ref> [JW94, BKW90, WBL89] </ref>, victim caches [Jou90], and cache line prefetching [RL92, CBM + 92, MLG92]. Non-blocking caches [FJ94, CB92, Con92, SF91] also help to reduce the impact of cache misses by letting other cache accesses complete while misses are serviced. <p> Techniques for delivering high-bandwidth memory access are well developed, both in the literature and in practice. The common approaches are multi-ported [SF91], interleaved [Rau91], and multilevel <ref> [JW94] </ref> memory structures. Multi-ported TLBs are already widely used; this work develops and evaluates interleaved and multi-level TLBs as well. In addition, piggyback ports are introduced as a technique to exploit the high level of spatial locality in simultaneous translation requests. <p> It is also possible to reduce the latency of cache misses using techniques that include multi-level caches <ref> [JW94, BKW90, WBL89] </ref>, victim caches [Jou90], and prefetching [RL92, CBM + 92, MLG92].
Reference: [KCE92] <author> E. J. Koldinger, J. S. Chase, and S. J. Eggers. </author> <title> Architectural support for single address space operating systems. </title> <booktitle> Proceedings of the Fifth International Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <volume> 27(9) </volume> <pages> 175-186, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: In a multiprocessing environment, cache coherence operations must first be reverse-translated to remote virtual addresses before data can be located in the remote cache. Many solutions have been devised to eliminate synonyms, including alignment restrictions on shared data [Che87], selective invalidation [WBL89], and single address space operating systems <ref> [KCE92] </ref>. However, these approaches have yet to come into widespread use due to performance and/or implementation impacts on application and system software. Moreover, these solutions do not solve the second problem that arises with virtual-address caches, efficient implementation 60 of protection. <p> One solution is to integrate protection information into cache blocks [WEG + 86]. However, the page-granularity of protection information makes managing these fields both complicated and expensive. Another solution is to implement a TLB minus the physical page address information, called a protection lookaside buffer (PLB) <ref> [KCE92] </ref>. This TLB-like structure, however, still requires high-bandwidth and low-latency access (although, latency requirements are somewhat relaxed). In light of these drawbacks, virtual-address caches have seen little use in real systems.
Reference: [KE93] <author> D. R. Kerns and S. J. Eggers. </author> <title> Balanced scheduling: Instruction scheduling when memory latency is unknown. </title> <booktitle> Proceedings of the 1992 ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <volume> 28(6) </volume> <pages> 278-289, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: The techniques can be broadly bisected into two camps: latency tolerating techniques and latency reducing techniques. Tolerating techniques work by inserting independent instructions into the pipeline delay slots created by load latencies. Local instruction scheduling <ref> [KE93, PS90, GM86] </ref> is a commonly used compile-time technique to tolerate load latency (it is employed in this work as well). The scheduler attempts to place independent instructions between loads and their first use, keeping pipeline resources utilized until loads complete. <p> This observation is further supported by the possibility that the load delta table may perform better on codes where fast address calculation performs poorly (e.g., poorly structured numeric codes). Instruction scheduling <ref> [KE93, PS90, GM86] </ref> is essentially the software dual of the early-issue mechanism, as both work to increase distance between loads and their first use. There are tradeoffs to using either approach. <p> For these codes, cache miss optimizations will have a much greater impact on program performance. Much effort has been invested in reducing the impact of cache misses on program performance. As with any other latency, cache miss latency can be tolerated using compile-time techniques such as instruction scheduling <ref> [KE93, PS90, GM86] </ref>, or run-time techniques including out-of-order issue, decoupled execution [Smi82b], or non-blocking loads [FJ94]. It is also possible to reduce the latency of cache misses using techniques that include multi-level caches [JW94, BKW90, WBL89], victim caches [Jou90], and prefetching [RL92, CBM + 92, MLG92].
Reference: [Kes91] <author> R. E. Kessler. </author> <title> Analysis of Multi-Megabyte Secondary CPU Cache Memories. </title> <type> Tr 1032, </type> <institution> Computer Sciences Department, UW-Madison, Madison, WI, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: Techniques that work to reduce the frequency of cache misses usually attack the problem of reducing conflict misses. Approaches along these lines include set-associative caches [KJLH89, Hea86, Smi82a], column-associative caches [AP93, AHH88], stride tolerant address mappings [Sez93, IL89, CL89], static <ref> [Kes91, DS91] </ref> or dynamic [BLRC94, LBF92, Kes91] page coloring, program restructuring [LRW91, Wu92, PH90, Fer76], and reference exclusion [McF92, CD89, ASW + 93, Hsu94, Con92]. <p> Techniques that work to reduce the frequency of cache misses usually attack the problem of reducing conflict misses. Approaches along these lines include set-associative caches [KJLH89, Hea86, Smi82a], column-associative caches [AP93, AHH88], stride tolerant address mappings [Sez93, IL89, CL89], static [Kes91, DS91] or dynamic <ref> [BLRC94, LBF92, Kes91] </ref> page coloring, program restructuring [LRW91, Wu92, PH90, Fer76], and reference exclusion [McF92, CD89, ASW + 93, Hsu94, Con92]. <p> Reducing the frequency of cache misses also works to reduce the performance impact of cache misses; approaches along these lines include set-associative caches [KJLH89, Hea86], column-associative caches [AP93], stride tolerant address mappings [Gao93, SL93, Sez93], page coloring <ref> [Kes91, DS91, BLRC94] </ref>, and program restructuring to improve data [CMT94, LRW91] or instruction cache performance [Wu92, PH90, McF89]. In this chapter, a novel software-based variable placement optimization, called cache-conscious data placement, is introduced as a technique for reducing the frequency of data cache misses. <p> TLB miss and page fault rates. The approaches presented in this paper would certainly extend to lower levels of the memory hierarchy as well; this is a potential area for future exploration. A number of peripheral works employ data relocation to improve data cache performance. Page coloring <ref> [BLRC94, LBF92, Kes91] </ref> techniques have leveraged the memory mapping capability of virtual memory to reduce conflicts in physically indexed caches. User-programmable cache set mappings [DS91] have been proposed to provide the same benefits.
Reference: [KH92a] <author> G. Kane and J. Heinrich. </author> <title> MIPS RISC Architecture. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1992. </year>
Reference-contexts: Figure 2.1 illustrates the structure of the experimental framework. 2.1 Compiler Tools All experiments were performed with programs compiled for the SimpleScalar architecture. The Sim-pleScalar architecture is a superset of the MIPS-I instruction set <ref> [KH92a] </ref> with the following notable differences: * There are no architected delay slots for loads, stores, or control transfers. * Loads and stores support additional addressing modes: indexed, auto-increment, and auto decrement, for all data types. * SQRT implements single- and double-precision floating point square roots. * The architecture employs a
Reference: [KH92b] <author> P. Kolte and M. J. Harrold. </author> <title> Load/store range analysis for global register allocation. </title> <booktitle> Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <volume> 28(6) </volume> <pages> 268-277, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Current research centers on increasing the number of candidates for register allocation, e.g., register 6 allocation for subscripted variables [CCK90], and increasing the utilization of a finite collection of registers through techniques such as live-range splitting [BCT92] and load/store range analysis <ref> [KH92b] </ref>. Unfortunately, many program variables are still forced into memory, due to the limited size and addressability of register files. Cache misses are often a significant component of load latency, especially in numeric codes where data locality is low.
Reference: [KJLH89] <author> R. E. Kessler, R. Jooss, A. Lebeck, and M. D. Hill. </author> <title> Inexpensive implementations of set-associativity. </title> <booktitle> Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <volume> 17(3) </volume> <pages> 131-139, </pages> <year> 1989. </year>
Reference-contexts: Techniques that work to reduce the frequency of cache misses usually attack the problem of reducing conflict misses. Approaches along these lines include set-associative caches <ref> [KJLH89, Hea86, Smi82a] </ref>, column-associative caches [AP93, AHH88], stride tolerant address mappings [Sez93, IL89, CL89], static [Kes91, DS91] or dynamic [BLRC94, LBF92, Kes91] page coloring, program restructuring [LRW91, Wu92, PH90, Fer76], and reference exclusion [McF92, CD89, ASW + 93, Hsu94, Con92]. <p> evaluations consider both bit selection, which uses a 61 62 portion of the virtual page address to select the bank, as well as an XOR-folding scheme, which randomizes the bank assignment by XOR'ing together portions of the virtual page address. (XOR-folding functions have been shown to provide better bank distribution <ref> [KJLH89] </ref>.) By its construction, an interleaved TLB cannot be fully-associative, since any particular page can reside in only one bank. It must have at least as many sets as banks. <p> Reducing the frequency of cache misses also works to reduce the performance impact of cache misses; approaches along these lines include set-associative caches <ref> [KJLH89, Hea86] </ref>, column-associative caches [AP93], stride tolerant address mappings [Gao93, SL93, Sez93], page coloring [Kes91, DS91, BLRC94], and program restructuring to improve data [CMT94, LRW91] or instruction cache performance [Wu92, PH90, McF89].
Reference: [KT91] <author> M. Katevenis and N. Tzartzanis. </author> <title> Reducing the branch penalty by rearranging instructions in a double-width memory. </title> <booktitle> Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <volume> 19(2) </volume> <pages> 15-27, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: The idea of exploiting the two-dimensional structure of memory is being used in several other contexts, such as paged mode DRAM access [HP90]. Katevenis and Tzartzanis <ref> [KT91] </ref> proposed a technique for reducing pipeline branch penalties by rearranging instructions so that both possible targets of a conditional branch are stored in a single instruction cache line. The high bandwidth of the cache is used to fetch both targets of a branch instruction.
Reference: [LBF92] <author> W. L. Lynch, B. K. Bray, and M. J. Flynn. </author> <title> The effect of page allocation on caches. </title> <booktitle> Proceedings of the 25th Annual International Symposium on Microarchitecture, </booktitle> <volume> 23(1) </volume> <pages> 222-225, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: Techniques that work to reduce the frequency of cache misses usually attack the problem of reducing conflict misses. Approaches along these lines include set-associative caches [KJLH89, Hea86, Smi82a], column-associative caches [AP93, AHH88], stride tolerant address mappings [Sez93, IL89, CL89], static [Kes91, DS91] or dynamic <ref> [BLRC94, LBF92, Kes91] </ref> page coloring, program restructuring [LRW91, Wu92, PH90, Fer76], and reference exclusion [McF92, CD89, ASW + 93, Hsu94, Con92]. <p> TLB miss and page fault rates. The approaches presented in this paper would certainly extend to lower levels of the memory hierarchy as well; this is a potential area for future exploration. A number of peripheral works employ data relocation to improve data cache performance. Page coloring <ref> [BLRC94, LBF92, Kes91] </ref> techniques have leveraged the memory mapping capability of virtual memory to reduce conflicts in physically indexed caches. User-programmable cache set mappings [DS91] have been proposed to provide the same benefits.
Reference: [LE89] <author> H. Levy and R. Eckhouse. </author> <title> Computer Programming and Architecture, The VAX. </title> <publisher> Digital Press, </publisher> <year> 1989. </year>
Reference-contexts: If virtual memory state changes are infrequent, it may be sufficient to simply flush the pretranslation cache whenever changes occur. The VAX IPA register used a similar technique to reuse translations during instruction fetch <ref> [LE89] </ref>. The current PC physical address translation is stored in the Instruction Physical Address (IPA) register, and this translation is used to access the cache until: 1) the PC crosses a page boundary, or 2) a 66 branch is taken.
Reference: [LGN92] <author> P. Lenir, R. Govindarajan, and S.S. Nemawarkar. </author> <title> Exploiting instruction-level parallelism: The multithreaded approach. </title> <booktitle> Proceedings of the 25th Annual International Symposium on Microarchitecture, </booktitle> <volume> 23(1) </volume> <pages> 189-192, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: When processor progress is stalled due to a load delay (or other instruction delays), the dynamic scheduler selects another instruction from a window of available instructions or another independent thread of control. Examples of execution models that perform dynamic scheduling include Multiscalar [Fra93], decoupled [Smi82b], dataflow [Vee86], and multi-threaded <ref> [LGN92, Smi81] </ref>. The best way by far to reduce load latency is to make memory access time zero by moving the accessed storage into a register. Register allocation is a well developed area and continues to progress.
Reference: [LRW91] <author> M. S. Lam, E. E. Rothberg, and M. E. Wolf. </author> <title> The cache performance and optimizations of blocked algorithms. </title> <booktitle> Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 63-74, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Approaches along these lines include set-associative caches [KJLH89, Hea86, Smi82a], column-associative caches [AP93, AHH88], stride tolerant address mappings [Sez93, IL89, CL89], static [Kes91, DS91] or dynamic [BLRC94, LBF92, Kes91] page coloring, program restructuring <ref> [LRW91, Wu92, PH90, Fer76] </ref>, and reference exclusion [McF92, CD89, ASW + 93, Hsu94, Con92]. <p> Reducing the frequency of cache misses also works to reduce the performance impact of cache misses; approaches along these lines include set-associative caches [KJLH89, Hea86], column-associative caches [AP93], stride tolerant address mappings [Gao93, SL93, Sez93], page coloring [Kes91, DS91, BLRC94], and program restructuring to improve data <ref> [CMT94, LRW91] </ref> or instruction cache performance [Wu92, PH90, McF89]. In this chapter, a novel software-based variable placement optimization, called cache-conscious data placement, is introduced as a technique for reducing the frequency of data cache misses.
Reference: [LS94] <author> A. Lebeck and G. Sohi. </author> <title> Request combining in multiprocessors with arbitrary interconnection networks. </title> <journal> IEEE TPDS, </journal> <month> November </month> <year> 1994. </year>
Reference-contexts: If both requests are executing under the same protection domain, the other fields of the translation request, i.e., protection and page status information, may be shared as well. The approach is similar to read combining in multiprocessor interconnection networks <ref> [LS94, GGK + 83] </ref>. Piggyback ports have minimal impact on translation latency. Once a request is submitted to the TLB, all other requesters can compare their virtual addresses in parallel with TLB access.
Reference: [LS95] <author> J. R. Larus and E. Schnarr. EEL: </author> <title> Machine-independent executable editing. </title> <booktitle> Proceedings of the ACM SIGPLAN '95 Conference on Programming Language Design and Implementation, </booktitle> <volume> 30(6) </volume> <pages> 291-300, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: In a production environment, performance could be significantly improved using some form of monitoring that supports direct execution of the profiled program. Suitable 85 approaches along these lines include dynamic compilation [CK93] and executable editing <ref> [LS95] </ref>. Efficient binding of memory reference addresses to variable profiles is implemented though the use of the instance map. The instance map shadows the entire virtual address space, providing a pointer to a profile for each allocated address in the virtual address space.
Reference: [LW94] <author> A. R. Lebeck and D. A. Wood. </author> <title> Cache profiling and the spec benchmarks: A case study. </title> <journal> IEEE Computer, </journal> <volume> 27(10) </volume> <pages> 15-26, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Heap variables are named when they are created (e.g., at calls to malloc ()) using the address of the call site to malloc () combined (with XOR-folding) with a few return addresses from the stack. (Similar heap naming schemes were employed by Lebeck and Wood <ref> [LW94] </ref> and Barrett and Zorn [BZ93].) This naming approach does a reasonably good job of satisfying the constraints listed above.
Reference: [MCC + 95] <author> B. P. Miller, M. D. Callaghan, J. M. Cargille, J. K. Hollingsworth, R. B. Irvin, K. L. Karavanic, K. Kunchithapadam, and T. Newhall. </author> <title> The Paradyn parallel performance measurement tools. </title> <journal> IEEE Computer, </journal> <volume> 28(11), </volume> <month> November </month> <year> 1995. </year>
Reference-contexts: Alternatively, profiles could support progressively increasing buckets sizes like those employed in the Paradyn parallel performance measurement tools <ref> [MCC + 95] </ref>. 115
Reference: [McF89] <author> S. McFarling. </author> <title> Program optimization for instruction caches. </title> <booktitle> Third International Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <volume> 24 </volume> <pages> 183-191, </pages> <month> April </month> <year> 1989. </year> <month> 120 </month>
Reference-contexts: frequency of cache misses also works to reduce the performance impact of cache misses; approaches along these lines include set-associative caches [KJLH89, Hea86], column-associative caches [AP93], stride tolerant address mappings [Gao93, SL93, Sez93], page coloring [Kes91, DS91, BLRC94], and program restructuring to improve data [CMT94, LRW91] or instruction cache performance <ref> [Wu92, PH90, McF89] </ref>. In this chapter, a novel software-based variable placement optimization, called cache-conscious data placement, is introduced as a technique for reducing the frequency of data cache misses. To apply the approach, a program is first profiled to characterize how its variables are used.
Reference: [McF91] <author> S. McFarling. </author> <title> Procedure merging with instruction caches. </title> <booktitle> Proceedings of the ACM SIG-PLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <volume> 26(6) </volume> <pages> 71-79, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Techniques such as basic block re-ordering [Dav95, Wu92, PH90], function grouping [Wu92, PH90, Fer76, Fer74, HG71], and text ordering based on control structure <ref> [McF91] </ref> have all been shown to significantly improve instruction cache performance. Like this work, the approaches usually rely on profile information to guide heuristic algorithms in placing instructions to minimize instruction cache conflicts, and maximize cache line utilization and block prefetch.
Reference: [McF92] <author> S. McFarling. </author> <title> Cache replacement with dynamic exclusion. </title> <booktitle> Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <volume> 20(2) </volume> <pages> 191-200, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Approaches along these lines include set-associative caches [KJLH89, Hea86, Smi82a], column-associative caches [AP93, AHH88], stride tolerant address mappings [Sez93, IL89, CL89], static [Kes91, DS91] or dynamic [BLRC94, LBF92, Kes91] page coloring, program restructuring [LRW91, Wu92, PH90, Fer76], and reference exclusion <ref> [McF92, CD89, ASW + 93, Hsu94, Con92] </ref>.
Reference: [ME92] <author> S.-M. Moon and K. Ebcioglu. </author> <title> An efficient resource-constrained global scheduling technique for superscalar and VLIW processors. </title> <booktitle> Proceedings of the 25th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 55-71, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: To make good schedules, the scheduler needs independent work, which is finite and usually quite small in the basic blocks of control intensive codes, e.g., many integer codes [AS92]. Global scheduling techniques <ref> [Fis81, MLC + 92, ME92] </ref> have been developed as a way to mitigate this effect; however, these techniques often suffer from ambiguous dependencies, unpredictable latencies, and safety issues that limit the extent of their effectiveness.
Reference: [MLC + 92] <author> S. A. Mahlke, D. C. Lin, W. Y. Chen, R. E. Hank, and R. A. Bringmann. </author> <title> Effective compiler support for predicated execution using the hyperblock. </title> <booktitle> In Conference Record of the 25th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 45-54, </pages> <address> Portland, OR, </address> <month> December </month> <year> 1992. </year> <institution> Association for Computing Machinery. </institution>
Reference-contexts: To make good schedules, the scheduler needs independent work, which is finite and usually quite small in the basic blocks of control intensive codes, e.g., many integer codes [AS92]. Global scheduling techniques <ref> [Fis81, MLC + 92, ME92] </ref> have been developed as a way to mitigate this effect; however, these techniques often suffer from ambiguous dependencies, unpredictable latencies, and safety issues that limit the extent of their effectiveness.
Reference: [MLG92] <author> T. C. Mowry, M. S. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> Conference Proceedings of the Fifth International Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <volume> 27(9) </volume> <pages> 62-73, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Much work has been done to reduce both the latency and frequency of data cache misses. Approaches that work to reduce miss latencies include multi-level caches [JW94, BKW90, WBL89], victim caches [Jou90], and cache line prefetching <ref> [RL92, CBM + 92, MLG92] </ref>. Non-blocking caches [FJ94, CB92, Con92, SF91] also help to reduce the impact of cache misses by letting other cache accesses complete while misses are serviced. Techniques that work to reduce the frequency of cache misses usually attack the problem of reducing conflict misses. <p> It is also possible to reduce the latency of cache misses using techniques that include multi-level caches [JW94, BKW90, WBL89], victim caches [Jou90], and prefetching <ref> [RL92, CBM + 92, MLG92] </ref>.
Reference: [MMO + 94] <author> A. B. Montz, D. Mosberger, S. W. O'Malley, L. L. Peterson, T. A. Proebsting, and J. H. Hartman. </author> <title> Scout: A communications-oriented operating system. </title> <type> Technical Report TR 94-20, </type> <institution> Department of Computer Science, University of Arizona, </institution> <address> Tucson, AZ, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: Data placement optimizations have been used to reduce false sharing in shared memory multiprocessors [JE95]. Compiler-directed variable partitioning has been proposed as an approach to reduce inter-variable interactions [Mue95] for the purpose of improving the predictability of cache access latencies in real-time systems. The Scout operating system <ref> [MMO + 94] </ref> employs data placement to reduce data cache conflict between active protocol stacks. The approach used to name heap variables was adopted from [BZ93]. The history of the development of this work is relevant related work.
Reference: [Mue95] <author> F. Mueller. </author> <title> Compiler support for software-based cache partitioning. </title> <booktitle> ACM SIGPLAN Workshop on Languages, Compilers, and Tools for Real-Time Systems, </booktitle> <volume> 30(11) </volume> <pages> 125-133, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Data placement optimizations have been used to reduce false sharing in shared memory multiprocessors [JE95]. Compiler-directed variable partitioning has been proposed as an approach to reduce inter-variable interactions <ref> [Mue95] </ref> for the purpose of improving the predictability of cache access latencies in real-time systems. The Scout operating system [MMO + 94] employs data placement to reduce data cache conflict between active protocol stacks. The approach used to name heap variables was adopted from [BZ93].
Reference: [PH90] <author> K. Pettis and R. C. Hansen. </author> <title> Profile guided code positioning. </title> <booktitle> Proceedings of the ACM SIG-PLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <volume> 25(6) </volume> <pages> 16-27, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Approaches along these lines include set-associative caches [KJLH89, Hea86, Smi82a], column-associative caches [AP93, AHH88], stride tolerant address mappings [Sez93, IL89, CL89], static [Kes91, DS91] or dynamic [BLRC94, LBF92, Kes91] page coloring, program restructuring <ref> [LRW91, Wu92, PH90, Fer76] </ref>, and reference exclusion [McF92, CD89, ASW + 93, Hsu94, Con92]. <p> frequency of cache misses also works to reduce the performance impact of cache misses; approaches along these lines include set-associative caches [KJLH89, Hea86], column-associative caches [AP93], stride tolerant address mappings [Gao93, SL93, Sez93], page coloring [Kes91, DS91, BLRC94], and program restructuring to improve data [CMT94, LRW91] or instruction cache performance <ref> [Wu92, PH90, McF89] </ref>. In this chapter, a novel software-based variable placement optimization, called cache-conscious data placement, is introduced as a technique for reducing the frequency of data cache misses. To apply the approach, a program is first profiled to characterize how its variables are used. <p> Techniques such as basic block re-ordering <ref> [Dav95, Wu92, PH90] </ref>, function grouping [Wu92, PH90, Fer76, Fer74, HG71], and text ordering based on control structure [McF91] have all been shown to significantly improve instruction cache performance. <p> Techniques such as basic block re-ordering [Dav95, Wu92, PH90], function grouping <ref> [Wu92, PH90, Fer76, Fer74, HG71] </ref>, and text ordering based on control structure [McF91] have all been shown to significantly improve instruction cache performance.
Reference: [PS90] <author> K. V. Palem and B. B. Simmons. </author> <title> Scheduling time-critical instructions on RISC machines. </title> <journal> SIGPLAN Notices, </journal> <volume> 25(6) </volume> <pages> 270-279, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The techniques can be broadly bisected into two camps: latency tolerating techniques and latency reducing techniques. Tolerating techniques work by inserting independent instructions into the pipeline delay slots created by load latencies. Local instruction scheduling <ref> [KE93, PS90, GM86] </ref> is a commonly used compile-time technique to tolerate load latency (it is employed in this work as well). The scheduler attempts to place independent instructions between loads and their first use, keeping pipeline resources utilized until loads complete. <p> This observation is further supported by the possibility that the load delta table may perform better on codes where fast address calculation performs poorly (e.g., poorly structured numeric codes). Instruction scheduling <ref> [KE93, PS90, GM86] </ref> is essentially the software dual of the early-issue mechanism, as both work to increase distance between loads and their first use. There are tradeoffs to using either approach. <p> For these codes, cache miss optimizations will have a much greater impact on program performance. Much effort has been invested in reducing the impact of cache misses on program performance. As with any other latency, cache miss latency can be tolerated using compile-time techniques such as instruction scheduling <ref> [KE93, PS90, GM86] </ref>, or run-time techniques including out-of-order issue, decoupled execution [Smi82b], or non-blocking loads [FJ94]. It is also possible to reduce the latency of cache misses using techniques that include multi-level caches [JW94, BKW90, WBL89], victim caches [Jou90], and prefetching [RL92, CBM + 92, MLG92].
Reference: [Rau91] <author> B. R. Rau. </author> <title> Pseudo-randomly interleaved memory. </title> <booktitle> Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <volume> 19(3) </volume> <pages> 74-83, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Techniques for delivering high-bandwidth memory access are well developed, both in the literature and in practice. The common approaches are multi-ported [SF91], interleaved <ref> [Rau91] </ref>, and multilevel [JW94] memory structures. Multi-ported TLBs are already widely used; this work develops and evaluates interleaved and multi-level TLBs as well. In addition, piggyback ports are introduced as a technique to exploit the high level of spatial locality in simultaneous translation requests.
Reference: [RL92] <author> A. Rogers and K. Li. </author> <title> Software support for speculative loads. </title> <booktitle> Conference Proceedings of the Fifth International Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 38-50, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Much work has been done to reduce both the latency and frequency of data cache misses. Approaches that work to reduce miss latencies include multi-level caches [JW94, BKW90, WBL89], victim caches [Jou90], and cache line prefetching <ref> [RL92, CBM + 92, MLG92] </ref>. Non-blocking caches [FJ94, CB92, Con92, SF91] also help to reduce the impact of cache misses by letting other cache accesses complete while misses are serviced. Techniques that work to reduce the frequency of cache misses usually attack the problem of reducing conflict misses. <p> It is also possible to reduce the latency of cache misses using techniques that include multi-level caches [JW94, BKW90, WBL89], victim caches [Jou90], and prefetching <ref> [RL92, CBM + 92, MLG92] </ref>.
Reference: [Sez93] <author> A. Seznec. </author> <title> A case for two-way skewed-associative caches. </title> <booktitle> Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <volume> 21(2) </volume> <pages> 169-178, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Techniques that work to reduce the frequency of cache misses usually attack the problem of reducing conflict misses. Approaches along these lines include set-associative caches [KJLH89, Hea86, Smi82a], column-associative caches [AP93, AHH88], stride tolerant address mappings <ref> [Sez93, IL89, CL89] </ref>, static [Kes91, DS91] or dynamic [BLRC94, LBF92, Kes91] page coloring, program restructuring [LRW91, Wu92, PH90, Fer76], and reference exclusion [McF92, CD89, ASW + 93, Hsu94, Con92]. <p> Reducing the frequency of cache misses also works to reduce the performance impact of cache misses; approaches along these lines include set-associative caches [KJLH89, Hea86], column-associative caches [AP93], stride tolerant address mappings <ref> [Gao93, SL93, Sez93] </ref>, page coloring [Kes91, DS91, BLRC94], and program restructuring to improve data [CMT94, LRW91] or instruction cache performance [Wu92, PH90, McF89]. In this chapter, a novel software-based variable placement optimization, called cache-conscious data placement, is introduced as a technique for reducing the frequency of data cache misses.
Reference: [SF91] <author> G. S. Sohi and M. Franklin. </author> <title> High-bandwidth data memory systems for superscalar processors. </title> <booktitle> Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 53-62, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Much work has been done to reduce both the latency and frequency of data cache misses. Approaches that work to reduce miss latencies include multi-level caches [JW94, BKW90, WBL89], victim caches [Jou90], and cache line prefetching [RL92, CBM + 92, MLG92]. Non-blocking caches <ref> [FJ94, CB92, Con92, SF91] </ref> also help to reduce the impact of cache misses by letting other cache accesses complete while misses are serviced. Techniques that work to reduce the frequency of cache misses usually attack the problem of reducing conflict misses. <p> The proposed designs fall into two categories: designs that extend traditional high-bandwidth memory design to the domain of address translation, and designs crafted specifically for high-bandwidth address translation. Techniques for delivering high-bandwidth memory access are well developed, both in the literature and in practice. The common approaches are multi-ported <ref> [SF91] </ref>, interleaved [Rau91], and multilevel [JW94] memory structures. Multi-ported TLBs are already widely used; this work develops and evaluates interleaved and multi-level TLBs as well. In addition, piggyback ports are introduced as a technique to exploit the high level of spatial locality in simultaneous translation requests.
Reference: [SL93] <author> A. Seznec and J. Lenfant. </author> <title> Odd memory systems may be quite interesting. </title> <booktitle> Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <volume> 21(2) </volume> <pages> 341-350, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Reducing the frequency of cache misses also works to reduce the performance impact of cache misses; approaches along these lines include set-associative caches [KJLH89, Hea86], column-associative caches [AP93], stride tolerant address mappings <ref> [Gao93, SL93, Sez93] </ref>, page coloring [Kes91, DS91, BLRC94], and program restructuring to improve data [CMT94, LRW91] or instruction cache performance [Wu92, PH90, McF89]. In this chapter, a novel software-based variable placement optimization, called cache-conscious data placement, is introduced as a technique for reducing the frequency of data cache misses.
Reference: [Sla94] <author> M. Slater. </author> <title> AMD's K5 designed to outrun Pentium. </title> <type> Microprocessor Report, 8(14) </type> <pages> 1-11, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Steven proposes the use of an OR function for all effective address computation. Steven's approach was only intended as a method for speeding up stack accesses, all other accesses require additional instructions to explicitly compute effective addresses. The performance of this pipeline organization was not evaluated. AMD's K5 processor <ref> [Sla94] </ref> overlaps a portion of effective address computation with cache access. The lower 11 bits of the effective address is computed in the cycle prior to cache access.
Reference: [Smi81] <author> B. J. Smith. </author> <title> Architecture and applications of the HEP multiprocessor. </title> <booktitle> SPIE, </booktitle> <pages> pages 241-248, </pages> <year> 1981. </year> <month> 121 </month>
Reference-contexts: When processor progress is stalled due to a load delay (or other instruction delays), the dynamic scheduler selects another instruction from a window of available instructions or another independent thread of control. Examples of execution models that perform dynamic scheduling include Multiscalar [Fra93], decoupled [Smi82b], dataflow [Vee86], and multi-threaded <ref> [LGN92, Smi81] </ref>. The best way by far to reduce load latency is to make memory access time zero by moving the accessed storage into a register. Register allocation is a well developed area and continues to progress.
Reference: [Smi82a] <author> A. J. Smith. </author> <title> Cache memories. </title> <journal> Computing Surveys, </journal> <volume> 14(3) </volume> <pages> 473-530, </pages> <month> September </month> <year> 1982. </year>
Reference-contexts: Techniques that work to reduce the frequency of cache misses usually attack the problem of reducing conflict misses. Approaches along these lines include set-associative caches <ref> [KJLH89, Hea86, Smi82a] </ref>, column-associative caches [AP93, AHH88], stride tolerant address mappings [Sez93, IL89, CL89], static [Kes91, DS91] or dynamic [BLRC94, LBF92, Kes91] page coloring, program restructuring [LRW91, Wu92, PH90, Fer76], and reference exclusion [McF92, CD89, ASW + 93, Hsu94, Con92].
Reference: [Smi82b] <author> J. E. Smith. </author> <title> Decoupled access/execute architectures. </title> <booktitle> Proceedings of the 9th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 112-119, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: When processor progress is stalled due to a load delay (or other instruction delays), the dynamic scheduler selects another instruction from a window of available instructions or another independent thread of control. Examples of execution models that perform dynamic scheduling include Multiscalar [Fra93], decoupled <ref> [Smi82b] </ref>, dataflow [Vee86], and multi-threaded [LGN92, Smi81]. The best way by far to reduce load latency is to make memory access time zero by moving the accessed storage into a register. Register allocation is a well developed area and continues to progress. <p> Much effort has been invested in reducing the impact of cache misses on program performance. As with any other latency, cache miss latency can be tolerated using compile-time techniques such as instruction scheduling [KE93, PS90, GM86], or run-time techniques including out-of-order issue, decoupled execution <ref> [Smi82b] </ref>, or non-blocking loads [FJ94]. It is also possible to reduce the latency of cache misses using techniques that include multi-level caches [JW94, BKW90, WBL89], victim caches [Jou90], and prefetching [RL92, CBM + 92, MLG92].
Reference: [SPE91] <institution> SPEC newsletter. Fairfax, Virginia, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: MPEG play is an MPEG compressed video decoder displaying a 79 frame compressed video file. TFFT performs real and complex FFTs on a randomly generated data set. YACR-2 is a VLSI channel router routing a channel with 230 terminals. The remaining benchmarks are from the SPEC92 benchmark suite <ref> [SPE91] </ref>. 14 Benchmark Language Input Options/Modifications Compress C in Elvis C unix.c %s/for/forever/g, %s/./& /g Eqntott C int pri 3.eqn Espresso C cps.in GCC C 1stmt.i Ghostscript C fast-addr.ps -dNOPAUSE -sDEVICE=ppm -c quit Grep C 3x inputs.txt -E -f regex.in Perl C tests.pl MPEG play C coil.mpg Sc C loada1 Xlisp
Reference: [SPE95] <institution> SPEC newsletter. Fairfax, Virginia, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: The top group are integer codes, the bottom group are floating point codes. Compress, Go, Perl, and Vortex are from the SPEC '95 benchmark suite <ref> [SPE95] </ref>. Elvis is a VI-compatible text editor performing textual replacements in batch mode. Ghostscript is a postscript viewer rendering a page with text and graphics to a PPM-format graphics file. Grep performs regular expression matches in a large text file.
Reference: [Ste88] <author> G. Steven. </author> <title> A novel effective address calculation mechanism for RISC microprocessors. </title> <journal> Computer Architecture News, </journal> <volume> 16(4) </volume> <pages> 150-156, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: Second, it is more accurate at predicting effective addresses 34 because it predicts addresses using the operands of the effective address calculation, rather than the address of the load. In addition, fast address calculation employs compile-time optimization to further improve performance. An earlier paper by Steven <ref> [Ste88] </ref> goes as far as proposing a four stage pipeline that eliminates the address generation stage and executes both memory accesses and ALU instructions in the same stage. Steven proposes the use of an OR function for all effective address computation.
Reference: [TH94] <author> M. Talluri and M. D. Hill. </author> <title> Surpassing the TLB performance of superpages with less operating system support. </title> <booktitle> Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <volume> 29(11) </volume> <pages> 171-182, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: The piggybacked designs all perform better, with the PB2 and I4/PB designs experiencing virtually no degradation in average IPC. 5.4.4 Performance with Increased Page Size A recent trend in TLB design has been to increase page sizes <ref> [TH94] </ref>. This trend is prompted by workloads with large data sets and/or little locality. Increased page size has a number of effects on the performance of the designs. <p> Changing the bank selection function will affect the distribution of accesses to the TLB banks. If the system supports variable page sizes <ref> [TH94] </ref>, there are implications on the proposed designs. For the interleaved design, bank selection must be a function of only the virtual page address; as a result, changes in the page size could require a change in the bank selection function.
Reference: [Vee86] <author> A. H. Veen. </author> <title> Dataflow machine architecture. </title> <journal> Computing Surveys, </journal> <volume> 18(4) </volume> <pages> 365-396, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: When processor progress is stalled due to a load delay (or other instruction delays), the dynamic scheduler selects another instruction from a window of available instructions or another independent thread of control. Examples of execution models that perform dynamic scheduling include Multiscalar [Fra93], decoupled [Smi82b], dataflow <ref> [Vee86] </ref>, and multi-threaded [LGN92, Smi81]. The best way by far to reduce load latency is to make memory access time zero by moving the accessed storage into a register. Register allocation is a well developed area and continues to progress.
Reference: [WBL89] <author> W.-H. Wang, J.-L. Baer, and H. M. Levy. </author> <title> Organization and performance of a two-level virtual-real cache hierarchy. </title> <booktitle> Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <volume> 17(3) </volume> <pages> 140-148, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Cache misses are often a significant component of load latency, especially in numeric codes where data locality is low. Much work has been done to reduce both the latency and frequency of data cache misses. Approaches that work to reduce miss latencies include multi-level caches <ref> [JW94, BKW90, WBL89] </ref>, victim caches [Jou90], and cache line prefetching [RL92, CBM + 92, MLG92]. Non-blocking caches [FJ94, CB92, Con92, SF91] also help to reduce the impact of cache misses by letting other cache accesses complete while misses are serviced. <p> In a multiprocessing environment, cache coherence operations must first be reverse-translated to remote virtual addresses before data can be located in the remote cache. Many solutions have been devised to eliminate synonyms, including alignment restrictions on shared data [Che87], selective invalidation <ref> [WBL89] </ref>, and single address space operating systems [KCE92]. However, these approaches have yet to come into widespread use due to performance and/or implementation impacts on application and system software. Moreover, these solutions do not solve the second problem that arises with virtual-address caches, efficient implementation 60 of protection. <p> It is also possible to reduce the latency of cache misses using techniques that include multi-level caches <ref> [JW94, BKW90, WBL89] </ref>, victim caches [Jou90], and prefetching [RL92, CBM + 92, MLG92].
Reference: [WE88] <author> N. Weste and K. Eshraghian. </author> <title> Principles of CMOS VLSI Design: A Systems Perspective. </title> <publisher> Addison-Wesley Publishing Company, Inc., </publisher> <year> 1988. </year>
Reference-contexts: A multi-ported TLB provides multiple access paths to all cells of the TLB, allowing multiple translations in a single cycle. The relatively small size of current TLBs along with the layout of the highly-associative storage lends itself well to multi-porting at the cells <ref> [WE88] </ref>. Although a multi-ported TLB design provides an excellent hit rate at each access port, its latency and area increase sharply as the number of ports or entries is increased. <p> Since every entry of the TLB is accessible from each port of the device, this design provides a good hit rate for each port (low M T LB ). However, the capacitance and resistance load on each access path increases as the number of ports or entries is increased <ref> [WE88] </ref>, resulting in longer access latency (t T LBhit ).
Reference: [WEG + 86] <author> D. A. Wood, S. J. Eggers, G. Gibson, M. D. Hill, J. M. Pendleton, S. A. Ritchie, G. S. Tay-lor, R. H. Katz, and D. A. Patterson. </author> <title> An in-cache address translation mechanism. </title> <booktitle> Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <volume> 14(2) </volume> <pages> 358-365, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: As a result, their implementation has been naturally integrated into the TLB. If the TLB is eliminated through use of a virtual-address cache, the problem of implementing protection still remains. One solution is to integrate protection information into cache blocks <ref> [WEG + 86] </ref>. However, the page-granularity of protection information makes managing these fields both complicated and expensive. Another solution is to implement a TLB minus the physical page address information, called a protection lookaside buffer (PLB) [KCE92].
Reference: [WJ94] <author> S. J.E. Wilton and N. P. Jouppi. </author> <title> An enhanced access and cycle time model for on-chip caches. </title> <type> Tech report 93/5, </type> <institution> DEC Western Research Lab, </institution> <year> 1994. </year>
Reference-contexts: To accomplish this task, an organizational property of on-chip data caches is exploited. To minimize access time, on-chip caches are organized as wide two-dimensional arrays of memory cells (as shown in Figure 3.3). Each row of the cache array typically contains one or more data blocks 20 <ref> [WRP92, WJ94] </ref>. To access a word in the cache, the set index portion of the effective address is used to read an entire cache row from the data array and a tag value from the tag array.
Reference: [WRP92] <author> T. Wada, S. Rajan, and S. A. Pyzybylski. </author> <title> An analytical access time model for on-chip cache memories. </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> 27(8) </volume> <pages> 1147-1156, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: To accomplish this task, an organizational property of on-chip data caches is exploited. To minimize access time, on-chip caches are organized as wide two-dimensional arrays of memory cells (as shown in Figure 3.3). Each row of the cache array typically contains one or more data blocks 20 <ref> [WRP92, WJ94] </ref>. To access a word in the cache, the set index portion of the effective address is used to read an entire cache row from the data array and a tag value from the tag array.
Reference: [Wu92] <author> Y. Wu. </author> <title> Ordering functions for improving memory reference locality in a shared memory multiprocessor system. </title> <booktitle> Proceedings of the 25th Annual International Symposium on Microarchitecture, </booktitle> <volume> 23(1) </volume> <pages> 218-221, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: Approaches along these lines include set-associative caches [KJLH89, Hea86, Smi82a], column-associative caches [AP93, AHH88], stride tolerant address mappings [Sez93, IL89, CL89], static [Kes91, DS91] or dynamic [BLRC94, LBF92, Kes91] page coloring, program restructuring <ref> [LRW91, Wu92, PH90, Fer76] </ref>, and reference exclusion [McF92, CD89, ASW + 93, Hsu94, Con92]. <p> frequency of cache misses also works to reduce the performance impact of cache misses; approaches along these lines include set-associative caches [KJLH89, Hea86], column-associative caches [AP93], stride tolerant address mappings [Gao93, SL93, Sez93], page coloring [Kes91, DS91, BLRC94], and program restructuring to improve data [CMT94, LRW91] or instruction cache performance <ref> [Wu92, PH90, McF89] </ref>. In this chapter, a novel software-based variable placement optimization, called cache-conscious data placement, is introduced as a technique for reducing the frequency of data cache misses. To apply the approach, a program is first profiled to characterize how its variables are used. <p> Techniques such as basic block re-ordering <ref> [Dav95, Wu92, PH90] </ref>, function grouping [Wu92, PH90, Fer76, Fer74, HG71], and text ordering based on control structure [McF91] have all been shown to significantly improve instruction cache performance. <p> Techniques such as basic block re-ordering [Dav95, Wu92, PH90], function grouping <ref> [Wu92, PH90, Fer76, Fer74, HG71] </ref>, and text ordering based on control structure [McF91] have all been shown to significantly improve instruction cache performance.
Reference: [YP93] <author> T.-Y. Yeh and Y. N. Patt. </author> <title> A comparison of dynamic branch predictors that use two levels of branch history. </title> <booktitle> Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 257-266, </pages> <month> May </month> <year> 1993. </year> <title> Computer Architecture News, </title> <type> 21(2), </type> <month> May </month> <year> 1993. </year> <month> 122 </month>
Reference-contexts: Adding more decode stages increases the mis-predicted branch penalty, however, architects have compensated for this penalty by increasing branch prediction accuracy through such means as larger branch target buffers or more effective predictors, 41 e.g., two-level adaptive <ref> [YP93] </ref>. Given extra decode stages, the task of implementing zero-cycle loads becomes markedly easier. Figure 4.3 shows one approach to providing support for zero-cycle loads on an in-order issue pipeline with two decode stages. Register access is delayed to the first decode stage of the pipeline. <p> The branch predictor was changed to a 2-level adaptive predictor with an 8 bit global history pattern indexing a 4096 entry pattern history table (i.e. the GAp method from <ref> [YP93] </ref>) with 2-bit saturating counters. In addition, a limited variant of the collapsing buffer (described in [CMMP95]) was added to increase fetch bandwidth.
References-found: 108

