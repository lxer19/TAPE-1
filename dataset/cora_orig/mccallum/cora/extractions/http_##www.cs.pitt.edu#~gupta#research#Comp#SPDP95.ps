URL: http://www.cs.pitt.edu/~gupta/research/Comp/SPDP95.ps
Refering-URL: http://www.cs.pitt.edu/~gupta/research/dm.html
Root-URL: 
Email: fgupta,bodikg@cs.pitt.edu  
Title: Adaptive Loop Transformations for Scientific Programs  
Author: Rajiv Gupta and Rastislav Bodik 
Address: Pittsburgh Pittsburgh, PA 15260  
Affiliation: Dept. of Computer Science University of  
Abstract: To facilitate the efficient execution of scientific programs, parallelizing compilers apply a wide range of loop transformations. In some situations it may not be possible to determine the applicability and usefulness of the transformations at compile-time. In this paper we develop adaptive versions of various loop transformations which lend themselves to efficient application at run-time. Instead of explicitly applying a transformation at compile-time, we generate adaptive code which is able to behave like the transformed code, if so desired, at run-time. Therefore an adaptive program can be viewed as expressing multiple ways of executing a program. The selection of the particular execution is based upon run-time information. Adaptive programs offer a more practical and powerful alternative to mul-tiversion programs. Keywords loop transformations, data locality, compiler directed scheduling, load balancing. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Byler, J.R.B. Davies, C. Huson, B. Leasure, and M. Wolfe, </author> <title> "Multiple Version Loops," </title> <booktitle> International Conf. on Parallel Processing, </booktitle> <pages> pages 312-318, </pages> <month> August </month> <year> 1987. </year>
Reference-contexts: The explicit application of transformations at run-time is expensive since it would require the generation of code at run-time. One approach for avoiding explicit transformation of programs at run-time is termed as static specialization. Procedure cloning [3], developed for traditional optimizations, and mul-tiversion loops <ref> [1] </ref>, developed for loop parallelization, are examples of this approach. Byler et al. [1] create multiple versions of loops at compile-time, and the appropriate version is selected based upon run-time information. <p> One approach for avoiding explicit transformation of programs at run-time is termed as static specialization. Procedure cloning [3], developed for traditional optimizations, and mul-tiversion loops <ref> [1] </ref>, developed for loop parallelization, are examples of this approach. Byler et al. [1] create multiple versions of loops at compile-time, and the appropriate version is selected based upon run-time information. The drawback of using multiversion programs is that it results in significant code growth since a copy of the loop is required corresponding to each loop version.
Reference: [2] <author> S. Carr, K.S. McKinley, and C-W. Tseng, </author> <title> "Compiler Optimizations for Improving Data Locality," </title> <booktitle> Proc. Sixth ASPLOS, </booktitle> <pages> pages 252-262, </pages> <year> 1994. </year>
Reference-contexts: Loop Fission and Fusion: These transformations alter the order in which the statements in the loop (s) are executed. Since altering the execution order of statements also changes the memory reference behavior, one possible use for these transformations is for the improvement of the data locality of a program <ref> [5, 2, 9] </ref>. Loop fission can also be used to decompose a serial loop into multiple loops some of which may be parallel. The fusing of two loops, in effect, causes the execution of iterations from the two original loops to be interleaved. <p> Consider the example in Figure 7 (a) and assume that arrays are stored in row-major form. To obtain good data locality for array A, the j loop should be the inner loop. On the other hand, array B requires the reverse loop nest order. Carr et al <ref> [2] </ref> compute the ideal order of a loop nest and, if possible, statically interchange the loops in the nest to follow this order. <p> Therefore the predicate in adaptive form is (:LA ^ true) _ (LA ^ i n shift) = (:LA) _ (LA ^ i n shif t). Loop Reversal: Machines with memory hierar-chies benefit from transformations such as loop interchange <ref> [2, 5] </ref> and blocking [7]. Adaptive loop reversal is another transformation for improving data locality. The transformation simply rearranges the order of the loop bounds as shown in Figure 10. In this example we assume that array A is stored in row-major form.
Reference: [3] <author> K. Cooper, M. Hall, and K. Kennedy, </author> <title> "A Methodology for Procedure Cloning," </title> <journal> Computer Languages, </journal> <volume> Vol. 19, No. 2, </volume> <pages> pages 105-117, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: The explicit application of transformations at run-time is expensive since it would require the generation of code at run-time. One approach for avoiding explicit transformation of programs at run-time is termed as static specialization. Procedure cloning <ref> [3] </ref>, developed for traditional optimizations, and mul-tiversion loops [1], developed for loop parallelization, are examples of this approach. Byler et al. [1] create multiple versions of loops at compile-time, and the appropriate version is selected based upon run-time information.
Reference: [4] <author> P. Y. Hsu, </author> <title> Highly Concurrent Scalar Processing, </title> <type> PhD Thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, </institution> <year> 1986. </year>
Reference: [5] <author> K. Kennedy and K.S. McKinley, </author> <title> "Maximizing Loop Parallelism and Improving Data Locality via Loop Fusion and Distribution," </title> <booktitle> Proc. 6th Annual Workshop on Lang. and Compilers for Parallel Comp., </booktitle> <publisher> LNCS 768 Springer Verlag, </publisher> <pages> pages 301-320, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: Loop Fission and Fusion: These transformations alter the order in which the statements in the loop (s) are executed. Since altering the execution order of statements also changes the memory reference behavior, one possible use for these transformations is for the improvement of the data locality of a program <ref> [5, 2, 9] </ref>. Loop fission can also be used to decompose a serial loop into multiple loops some of which may be parallel. The fusing of two loops, in effect, causes the execution of iterations from the two original loops to be interleaved. <p> Therefore the predicate in adaptive form is (:LA ^ true) _ (LA ^ i n shift) = (:LA) _ (LA ^ i n shif t). Loop Reversal: Machines with memory hierar-chies benefit from transformations such as loop interchange <ref> [2, 5] </ref> and blocking [7]. Adaptive loop reversal is another transformation for improving data locality. The transformation simply rearranges the order of the loop bounds as shown in Figure 10. In this example we assume that array A is stored in row-major form.
Reference: [6] <author> D. Keppel, S. Eggers, and R. Henry, </author> <title> "A Case for Run-time Code Generation," </title> <type> Tech. Report 91-11-04, </type> <institution> DCSE, Univ. of Washington, </institution> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: Thus, it would be highly desirable to develop an approach that enables us to benefit from the application of transformations at run-time. Previous research has developed efficient run-time optimization techniques in context of traditional sequential code optimizations <ref> [6, 14] </ref>. In this paper we address the problem of run-time application of a wide range of loop transformations. The explicit application of transformations at run-time is expensive since it would require the generation of code at run-time.
Reference: [7] <author> M. Lam, E. Rothberg, and M. E. Wolf, </author> <title> "The cache performance and optimizations of blocked algorithms," </title> <booktitle> Proc. Fourth ASPLOS, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: Therefore the predicate in adaptive form is (:LA ^ true) _ (LA ^ i n shift) = (:LA) _ (LA ^ i n shif t). Loop Reversal: Machines with memory hierar-chies benefit from transformations such as loop interchange [2, 5] and blocking <ref> [7] </ref>. Adaptive loop reversal is another transformation for improving data locality. The transformation simply rearranges the order of the loop bounds as shown in Figure 10. In this example we assume that array A is stored in row-major form.
Reference: [8] <author> S.-T. Leung and J. Zahorjan, </author> <title> "Improving the Performance of Run-time Parallelization," </title> <booktitle> ACM SIGPLAN Symp. on PPoPP, </booktitle> <pages> pages 83-91, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Recently, a significant amount of research has been carried out to perform run-time dependence analysis fl Supported in part by the National Science Foundation through a Presidential Young Investigator Award CCR-9157371 to the Univ. of Pittsburgh. for use in the run-time parallelization of programs <ref> [8, 10, 11, 12] </ref>. This approach has been found to be beneficial since often the cost of run-time analysis is small compared to the payoff of uncovering parallelism. Although significant progress has been made in run-time detection of parallel loops, run-time application of complex loop transformations has not been considered.
Reference: [9] <author> E.P. Marakatos and T.J. LeBlanc, </author> <title> "Using Processor Affinity in Loop Scheduling on Shared-Memory Multiprocessors," </title> <journal> IEEE Trans. on Parallel and Distributed Processing, </journal> <volume> Vol. 5, No. 4, </volume> <pages> pages 379-400, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Loop Fission and Fusion: These transformations alter the order in which the statements in the loop (s) are executed. Since altering the execution order of statements also changes the memory reference behavior, one possible use for these transformations is for the improvement of the data locality of a program <ref> [5, 2, 9] </ref>. Loop fission can also be used to decompose a serial loop into multiple loops some of which may be parallel. The fusing of two loops, in effect, causes the execution of iterations from the two original loops to be interleaved.
Reference: [10] <author> S. Midkiff and D. Padua, </author> <title> "Compiler Algorithms for Synchronization," </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. C-36, No. 12, </volume> <month> Dec. </month> <year> 1987. </year>
Reference-contexts: Recently, a significant amount of research has been carried out to perform run-time dependence analysis fl Supported in part by the National Science Foundation through a Presidential Young Investigator Award CCR-9157371 to the Univ. of Pittsburgh. for use in the run-time parallelization of programs <ref> [8, 10, 11, 12] </ref>. This approach has been found to be beneficial since often the cost of run-time analysis is small compared to the payoff of uncovering parallelism. Although significant progress has been made in run-time detection of parallel loops, run-time application of complex loop transformations has not been considered.
Reference: [11] <author> J. Saltz, R. Mirchandaney, and K. Crowley, </author> <title> "Run-time Parallelization and Scheduling of Loops," </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. 40, No. 5, </volume> <pages> pages 603-611, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Recently, a significant amount of research has been carried out to perform run-time dependence analysis fl Supported in part by the National Science Foundation through a Presidential Young Investigator Award CCR-9157371 to the Univ. of Pittsburgh. for use in the run-time parallelization of programs <ref> [8, 10, 11, 12] </ref>. This approach has been found to be beneficial since often the cost of run-time analysis is small compared to the payoff of uncovering parallelism. Although significant progress has been made in run-time detection of parallel loops, run-time application of complex loop transformations has not been considered.
Reference: [12] <author> P. Peterson and D. Padua, </author> <title> "Dynamic Dependence Analysis: A Novel Method for Data Dependence Evaluation," </title> <booktitle> Proc. Fifth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <publisher> LNCS 757 Springer Ver-lag, </publisher> <pages> pages 64-81, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Recently, a significant amount of research has been carried out to perform run-time dependence analysis fl Supported in part by the National Science Foundation through a Presidential Young Investigator Award CCR-9157371 to the Univ. of Pittsburgh. for use in the run-time parallelization of programs <ref> [8, 10, 11, 12] </ref>. This approach has been found to be beneficial since often the cost of run-time analysis is small compared to the payoff of uncovering parallelism. Although significant progress has been made in run-time detection of parallel loops, run-time application of complex loop transformations has not been considered.
Reference: [13] <author> F. Irigoin and R. Triolet, </author> <title> "Supernode Partitioning," </title> <booktitle> Proc. 15th Annual ACM Symp. on POPL, </booktitle> <pages> pages 319-329, </pages> <month> Jan. </month> <year> 1988. </year>
Reference: [14] <author> M. Leone and P. Lee, </author> <title> "Lightweight Run-Time Code Generation," </title> <booktitle> Proc. ACM SIGPLAN PEPM Workshop, </booktitle> <pages> pages 97-106, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Thus, it would be highly desirable to develop an approach that enables us to benefit from the application of transformations at run-time. Previous research has developed efficient run-time optimization techniques in context of traditional sequential code optimizations <ref> [6, 14] </ref>. In this paper we address the problem of run-time application of a wide range of loop transformations. The explicit application of transformations at run-time is expensive since it would require the generation of code at run-time.
Reference: [15] <author> T. Watts, M.L. Soffa, and R. Gupta, </author> <title> "Techniques for Integrating Parallelizing Transformations and Compiler Based Scheduling Methods," </title> <booktitle> Proc. Supercomputing'92, </booktitle> <pages> pages 830-839, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Integrated Selective Fusion and Dynamic Scheduling: It is well known that loop transformations and loop scheduling techniques must work in tandem to obtain good performance. This observation has resulted in techniques for integrated transformation and scheduling <ref> [15] </ref>. Next we illustrate the application of selective loop fusion for achieving better data locality and load balancing during dynamic scheduling of loops.
References-found: 15

