URL: http://www.cs.ucsd.edu/users/crosin/thesis.final.ps.gz
Refering-URL: http://www.cs.ucsd.edu/users/crosin/
Root-URL: http://www.cs.ucsd.edu
Title: Coevolutionary Search Among Adversaries  
Author: Christopher Darrell Rosin Professor John D. Batali Professor J. Andrew McCammon 
Degree: A dissertation submitted in partial satisfaction of the requirements for the degree Doctor of Philosophy in Computer Science by  Committee in charge: Professor Richard K. Belew, Chairperson Professor Ramamohan Paturi Professor Garrison W. Cottrell  
Date: 1997  
Affiliation: UNIVERSITY OF CALIFORNIA, SAN DIEGO  
Abstract-found: 0
Intro-found: 1
Reference: [1] <institution> Computer go ladder. </institution> <note> Available online at http://www.cgl.ucsf.edu/go/ladder.html. </note>
Reference-contexts: IV.C.3 Go as an AI Problem Although a number of Go programs are in development by various researchers, recent published work on computer Go is fairly rare [56, 85, 46, 20, 77]. More information about existing Go programs and issues in computer Go is available from the Internet <ref> [11, 1, 2] </ref>; refer to these for further information about topics discussed below. Beyond providing an open-ended adversarial domain, Go has several additional features that make it attractive as a test problem in artificial intelligence [20].
Reference: [2] <institution> Computer go mailing list. </institution> <note> Information available online at http://cs.anu.edu.au/ Lex.Weaver/COMPUTER-GO/. </note>
Reference-contexts: IV.C.3 Go as an AI Problem Although a number of Go programs are in development by various researchers, recent published work on computer Go is fairly rare [56, 85, 46, 20, 77]. More information about existing Go programs and issues in computer Go is available from the Internet <ref> [11, 1, 2] </ref>; refer to these for further information about topics discussed below. Beyond providing an open-ended adversarial domain, Go has several additional features that make it attractive as a test problem in artificial intelligence [20].
Reference: [3] <author> D.H. Ackley. ccr: </author> <title> A network of worlds for research. </title> <booktitle> In Artificial Life V, </booktitle> <year> 1996. </year>
Reference-contexts: In such close-ended domains, computational approaches only need to produce a finite sequence of improvements, until this ultimate solution is found. More complex problem domains in artificial intelligence, such as board games [95], robotics [88], mathematics [55], and artificial life simulations <ref> [3] </ref>, often do not possess such ultimate, perfect solutions of reasonable complexity. Instead, there exist open-ended sequences of useful solutions of increasing performance and complexity. Such problem domains offer an opportunity to demonstrate open-ended intelligent systems capable of climbing these sequences to solutions of arbitrary performance and complexity.
Reference: [4] <author> J.S. Albus. </author> <title> Mechanisms of planning and problem solving in the brain. </title> <journal> Mathematical Biosciences, </journal> <volume> 45 </volume> <pages> 247-293, </pages> <year> 1979. </year>
Reference-contexts: The drug infusion rate u is determined by a CMAC controller that has current blood pressure and rate of blood pressure change as inputs. A CMAC controller <ref> [4] </ref> divides its input state space with a grid, and has a separate weight for each cell in the grid. 2 A particular input activates N cells that are close to it in state space; N is a parameter that determines the extent of generalization.
Reference: [5] <author> L.V. Allis, H.J. Van Den Herik, and P.H. Huntjens. </author> <title> Go-Moku solved by new search techniques. </title> <journal> Computational Intelligence, </journal> <volume> 12(1), </volume> <year> 1996. </year>
Reference-contexts: The removal of the burden of the endgame from coevolution should make search easier, and allow more accurate strategy evaluations throughout a coevolutionary run. Exact endgame search techniques are well-developed, due to their application in computing complete solutions to games <ref> [5, 6] </ref>. Methods have been developed that allow extremely fast identification of won positions, many moves away from the actual end of the game. Solutions to Qubic and Go-Moku depended on such techniques [5, 6]. <p> Exact endgame search techniques are well-developed, due to their application in computing complete solutions to games <ref> [5, 6] </ref>. Methods have been developed that allow extremely fast identification of won positions, many moves away from the actual end of the game. Solutions to Qubic and Go-Moku depended on such techniques [5, 6]. These games may be good test domains for the integration of coevolution with endgame tree search. 203 V.D.3 Informative Parasites Coevolutionary search is guided by the information provided by competition against parasites. Effective search requires that parasites be informative.
Reference: [6] <author> L.V. Allis, M. Van Der Meulen, and H. Jaap Van Den Herik. </author> <title> Proof-number search. </title> <journal> Artificial Intelligence, </journal> <volume> 66(1), </volume> <year> 1994. </year>
Reference-contexts: The removal of the burden of the endgame from coevolution should make search easier, and allow more accurate strategy evaluations throughout a coevolutionary run. Exact endgame search techniques are well-developed, due to their application in computing complete solutions to games <ref> [5, 6] </ref>. Methods have been developed that allow extremely fast identification of won positions, many moves away from the actual end of the game. Solutions to Qubic and Go-Moku depended on such techniques [5, 6]. <p> Exact endgame search techniques are well-developed, due to their application in computing complete solutions to games <ref> [5, 6] </ref>. Methods have been developed that allow extremely fast identification of won positions, many moves away from the actual end of the game. Solutions to Qubic and Go-Moku depended on such techniques [5, 6]. These games may be good test domains for the integration of coevolution with endgame tree search. 203 V.D.3 Informative Parasites Coevolutionary search is guided by the information provided by competition against parasites. Effective search requires that parasites be informative.
Reference: [7] <author> E. Amaldi and V. Kann. </author> <title> The complexity and approximability of finding maximum feasible subsystems of linear relations. </title> <journal> Theoretical Computer Science, </journal> <volume> 147 </volume> <pages> 181-210, </pages> <year> 1995. </year>
Reference-contexts: This practical problem-specific efficiency is probably the best we can hope for; even simple versions of these strategy learning problems are instances of the maximum satisfying linear subsystem problem, which is NP-complete and hard to approximate <ref> [7] </ref>. We describe the strategy learning algorithms below. Since they are highly specific to this particular model, we do not provide extensive details. Finding Inhibitors In this model, the design of an inhibitor effective against a single mutant is very fast and easy.
Reference: [8] <author> P. J. Angeline and J. B. Pollack. </author> <title> Competitive environments evolve better solutions for complex tasks. </title> <editor> In Stephanie Forrest, editor, </editor> <booktitle> Proceedings of the Fifth International Conference on Genetic Algorithms. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: In such problems, dynamic programming cannot be usefully applied because it has no temporal structure to work with. We seek a more general approach to adversarial problems. Several authors have discussed general issues surrounding the proper choice of opponents for the learning of robust game strategies <ref> [32, 91, 8] </ref>. The common theme is that diverse strong opposition is needed to learn good strategies. A single expert opposing program will often be unavailable, and even if it is available it may have a simple flaw that can be exploited by an otherwise weak strategy. <p> Hillis coevolved sorting networks and difficult sequences to sort [44]. Coevolution produced better solutions than random testing. We revisit this application in Section IV.B. Coevolution can be naturally applied to finding strategies for zero-sum games. Angeline and Pollack applied a coevolutionary form of genetic programming to tic-tac-toe <ref> [8] </ref>, with moderate success. We have solved this simple game with a different representation and several extensions to competitive coevolution (Chapter III). Sims has had great success coevolving simulated 3-D robots to participate in a simple zero-sum competition in a simulated environment [88]. <p> We experimentally test these methods below, separately and in combination with each other. III.C.1 Competitive Fitness Sharing The usual way to assign fitness based on success in a set of competitions is to sum the score (often a binary value indicating success/failure) across all competitions <ref> [8, 44, 12] </ref>. This measure will be called "simple fitness" here. In "fitness sharing" a sharing function is defined to take into account similarity (according to some metric) among individuals [41].
Reference: [9] <author> D. Angluin. </author> <title> Negative results for equivalence queries. </title> <journal> Machine Learning, </journal> <volume> 5(2), </volume> <year> 1990. </year>
Reference-contexts: This limits the total number of steps of the competitive algorithm to l. Counterexamples are not sufficient to learn all games, though. Angluin has shown that for some classes in concept learning, an adversary may choose uninformative counterexamples so that the target cannot be exactly learned in polynomial time <ref> [9] </ref>. Below, we show the existence of a game for which counterexamples do not ensure polynomial learnability, with either worst-case strategy learning algorithms or randomized strategy learning algorithms. Example 3 (Generalized Guessing Games) Let H consist of all natural numbers in the range 0 : : : n 1.
Reference: [10] <author> M. Anthony, G. Brightwell, D. Cohen, and J. Shawe-Taylor. </author> <title> On exact specification by examples. </title> <booktitle> In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, </booktitle> <year> 1992. </year>
Reference-contexts: Define the specification number k for G to be the size of the smallest teaching set. These definitions follow the corresponding ones for concept learning <ref> [10, 42] </ref>. Since we are considering exact learning, we seek bounds on learning time that are polynomial in k, as well as in lg (jHj) and lg (jX j). The following lemma 18 shows that the dependence on k is necessary. <p> If no reasonably-sized teaching set exists, then optimal individuals cannot be practically distinguished, and coevolution cannot be expected to find optimal individuals. In general, small teaching sets seem to exist for many problems <ref> [42, 10] </ref>. For competitive coevolution, this suggests: 1. We should encourage some sort of niching, so that such sets of individuals may arise and be stable in a population. 2.
Reference: [11] <institution> American Go Association. AGA computer go. </institution> <note> Available online at http://www.usgo.org/computer/, 1997. </note>
Reference-contexts: IV.C.3 Go as an AI Problem Although a number of Go programs are in development by various researchers, recent published work on computer Go is fairly rare [56, 85, 46, 20, 77]. More information about existing Go programs and issues in computer Go is available from the Internet <ref> [11, 1, 2] </ref>; refer to these for further information about topics discussed below. Beyond providing an open-ended adversarial domain, Go has several additional features that make it attractive as a test problem in artificial intelligence [20].
Reference: [12] <author> R. Axelrod. </author> <title> Evolution of strategies in the Iterated Prisoner's Dilemma. </title> <editor> In Lawrence Davis, editor, </editor> <title> Genetic Algorithms and Simulated Annealing. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1989. </year> <pages> 207 208 </pages>
Reference-contexts: In such cases, there only needs to be one population, whose members compete amongst themselves. For example, this symmetry holds in the Prisoners' Dilemma <ref> [12] </ref>. 7 Part of the motivation for using population-based search methods to solve adversarial problems comes from biology, as described above. We envision better solutions driving the evolution of more difficult test cases, and more difficult test cases driving the evolution of better solutions. <p> This dual use seems to make competitive coevolution a particularly efficient heuristic approach to adversarial problems. The original work on coevolution in a GA context was done by Axelrod <ref> [12] </ref> on the Prisoners' Dilemma. Since then, a number of coevolutionary experiments have been done on this problem. The goals of these experiments are typically in modelling the emergence of cooperation, which is fairly orthogonal to the concerns of adversarial problems. <p> We experimentally test these methods below, separately and in combination with each other. III.C.1 Competitive Fitness Sharing The usual way to assign fitness based on success in a set of competitions is to sum the score (often a binary value indicating success/failure) across all competitions <ref> [8, 44, 12] </ref>. This measure will be called "simple fitness" here. In "fitness sharing" a sharing function is defined to take into account similarity (according to some metric) among individuals [41].
Reference: [13] <editor> R.T. Bakker. The deer flees, the wolf pursues: incongruencies in predator-prey coevolution. In D.J. Futuyma and M. Slatkin, editors, Coevolution. </editor> <publisher> Sinauer, </publisher> <year> 1983. </year>
Reference-contexts: The table below shows brain size (normalized for body size) increases in these two groups of species over a 60 million year time span, based on fossils from the northern hemisphere [47]. Similar increases have been observed for running speed (derived from bone size measurements) <ref> [13] </ref>. We have some confidence that the brain size increase is due to the pressures of competition; there were no significant predators in South America during most of this time span, and no such increase in brain size was seen. <p> One example of this is running speed in mammalian predator and prey species. Using anatomical indicators of running speed, a fairly steady increase in running speed has been observed in North American ungulates and their predators over the last 60 million years <ref> [13] </ref>. Although there may be costs involved, we generally expect faster running speed to be better for competition between predators and prey. The long-term increase in running speed may have required a wide variety of changes 192 at the genetic level at various times.
Reference: [14] <author> E.B. Baum, D. Boneh, and C. Garrett. </author> <title> Where genetic algorithms excel. </title> <note> This is an expanded and improved version of "On Genetic Algorithms" which appeared in the Proceedings of the Eighth Annual Conference on Computational Learning Theory, ACM., </note> <year> 1995. </year>
Reference-contexts: Hillis' work (see below) on search for 16-input networks constrains all nets to start with the "butterfly," a particular sequence of 32 compare-exchanges. Since many of the best human-designed 16-input networks begin with the butterfly, this is a reasonable way to reduce the search space. Baum <ref> [14] </ref> and Juille [49] point out that the butterfly also greatly reduces the required testing problem. Only 151 distinct unsorted sequences remain after passing all 2 16 zero-one sequences through the initial 16-input butterfly, so specification number for sorting nets biased by the butterfly is 151.
Reference: [15] <author> M.A. Bedau and N.H. Packard. </author> <title> Measurement of evolutionary activity, </title> <booktitle> teleology, and life. In Artificial Life II. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: Because of the close correspondence between the set of positions and the genes in our representation, this measure is similar to measures of gene usage <ref> [15] </ref>. Table III.13 shows significant usage for the seven variants, on 2DTTT. The significant usage threshold was set at 320, the number of games that 4 hosts play against all their parasites. Table III.14 shows significant differences in usage for 2DTTT.
Reference: [16] <author> R.K. Belew and M. Mitchell. </author> <title> Adaptive Individuals in Evolving Populations: Models and Algorithms. </title> <publisher> Addison-Wesley, </publisher> <year> 1996. </year>
Reference-contexts: This is a form of self-correction to timing that may occur during coevolution. It resembles the way in which the Baldwin effect can exploit individuals' plasticity (eg. to learn during their lifetimes) to convert difficult evolutionary problems into tractable ones <ref> [16] </ref>. Drift is likely to be an important factor in tuning parameters. The phantom parasite explicitly causes selection of drifting, pedagogical individuals in cases of extreme unbalance.
Reference: [17] <editor> R.K. Belew and M.D. Vose, editors. </editor> <booktitle> Foundations of Genetic Algorithms 4. </booktitle> <publisher> Mor-gan Kaufmann, </publisher> <year> 1997. </year>
Reference-contexts: Over successive generations, selection chooses the best novel individuals generated by crossover and mutation, so that fitness tends to improve. Extensive work on genetic algorithms discusses many variations on this basic outline, and a wide variety of applications (for example <ref> [33, 17] </ref>). The GA has a natural coevolutionary extension to adversarial problems, described below. I.B.3 Biological Motivation The genetic algorithm is inspired and informed by properties of biological evolution. Similarly, coevolutionary methods for approaching adversarial problems are inspired by competitive coevolution in nature.
Reference: [18] <author> E.R. Berlekamp, J.H. Conway, and R.K. Guy. </author> <title> Winning Ways for Your Mathematical Plays. </title> <publisher> Academic Press, </publisher> <year> 1982. </year>
Reference-contexts: Much of computational learning theory explores concept classes that may be learned in such models. Chapter II discusses the application of computational learning theory to adversarial problems. For simple adversarial problems, analytic solutions may be possible with specialized techniques such as combinatorial game theory <ref> [18] </ref>. Also, for reasonably small games, minimax versions of dynamic programming and tree search can be used to compute exact solutions. In general, larger problems require heuristic methods. Heuristic forms of dynamic programming exist. These are used, for example, to train neural networks via reinforcement learning. <p> Players alternate removing an arbitrary number of stones from a single pile. The player to take the last stone wins. The configuration used here starts with 4 piles, containing 3, 4, 5, and 4 stones. This configuration allows the first player to force a win with optimal play <ref> [18] </ref>. A simple one-gene-per-position representation is used for Nim. The positions are placed on the genome in lexicographic order: (0,0,0,0), (0,0,0,1), : : :, (3,4,5,4), where each quadruple gives the number of stones in each of the four piles.
Reference: [19] <author> N.H. Bshouty, R. Cleve, S. Kannan, and C. Tamon. </author> <title> Oracles and queries that are sufficient for exact learning. </title> <booktitle> In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, COLT 94. ACM, </booktitle> <year> 1994. </year>
Reference-contexts: Plutowski [74] minimizes the number of examples used for learning, but does no search over examples: instead, examples are selected from a fixed set on the basis of a criterion that tells how informative they will be. Query learning <ref> [19] </ref> is a theoretical model that explores inductive learning problems in which the learner may obtain examples from oracles more powerful than a random example source. <p> This result shows that, with such a powerful counterexample oracle, any teachable class of hypotheses can be learned. It has previously been shown that any finite concept class may be efficiently learned with simple counterexamples and powerful computational abilities, by querying with hypotheses outside of H <ref> [19] </ref>. Our result is distinct from this, because it relies only on hypotheses from H. Note that fixing k 0 is crucial. If the counterexample oracle were allowed to provide as many counterexamples as passed hypotheses, it could simply provide a separate counterexample to each hypothesis. <p> For convenience, assume that it is a first-player strategy h p : 8x g (h p ; x) = 1. The goal of approximate learning is to find a small mixed strategy for the first player that wins most of the time. We use a sort of "majority" algorithm <ref> [19] </ref> that samples first-player strategies which defeat a set of second-player opponents. Then, it finds a second-player strategy that defeats a significant fraction of these. If no such second-player strategy exists, we are done. Otherwise, it probably eliminates a significant fraction of remaining first-player strategies.
Reference: [20] <author> J. Burmeister and J. Wiles. </author> <title> The challenge of go as a domain for ai research: A comparison between go and chess. </title> <booktitle> In Proceedings of the Third Australian and New Zealand Conference on Intelligent Information Systems. </booktitle> <institution> IEEE Western Australia Section, </institution> <year> 1995. </year>
Reference-contexts: IV.C.3 Go as an AI Problem Although a number of Go programs are in development by various researchers, recent published work on computer Go is fairly rare <ref> [56, 85, 46, 20, 77] </ref>. More information about existing Go programs and issues in computer Go is available from the Internet [11, 1, 2]; refer to these for further information about topics discussed below. <p> Beyond providing an open-ended adversarial domain, Go has several additional features that make it attractive as a test problem in artificial intelligence <ref> [20] </ref>. It has proven quite difficult for computers; current programs play at a low intermediate level that does not come close to the mastery with which computers play other games such as chess [94]. <p> Since almost every open intersection is a legal move each turn in Go, the number of choices to explore for each move is larger than in most board games for which search has been successful (for example, an average of roughly 200 for Go as opposed to 35 for chess <ref> [20] </ref>). Furthermore, a single move can have subtle effects on the position that are difficult to compare with the effects of 144 other move choices. Finally, there are no known methods to rapidly obtain accurate evaluations of positions in Go, making it impossible to do efficient extensive search.
Reference: [21] <author> K.C. Chou, A.G. Tomasselli, </author> <title> I.M. Reardon, and R.L. Heinrikson. Predicting human immunodeficiency virus protease cleavage sites in proteins by a discriminant function method. </title> <journal> Proteins, </journal> <volume> 24(1) </volume> <pages> 51-72, </pages> <year> 1996. </year>
Reference-contexts: Inhibitors and mutants are evaluated with a simple volume-based geometric match. As detailed below, we obtain an "average volume" of the contents of each subsite for the substrates that are known to bind with the native protease <ref> [21] </ref>. We then penalize deviations from this average volume. This is a simple evaluation of geometric fit.
Reference: [22] <author> V. Chvatal. </author> <title> A greedy heuristic for the set-covering problem. </title> <journal> Mathematics of Operations Research, </journal> <volume> 4 </volume> <pages> 233-235, </pages> <year> 1979. </year>
Reference-contexts: Using L 0 2 repeatedly, a set-covering approximation can be used to create a second-player strategy learning algorithm that returns sets of size at most k 0 = k log jHj <ref> [22] </ref>. <p> Since specification number may be larger than one, the algorithm needs to be able to find sets of mutants that cover all prior inhibitors. A greedy set-covering approximation <ref> [22] </ref> is used for this purpose: new mutants are chosen to cover as many remaining uncovered inhibitors as possible. This proceeds until all inhibitors are covered. This algorithm is computationally expensive, often requiring several hours on a 100MHz PowerPC processor.
Reference: [23] <author> D. Cliff and G. F. Miller. </author> <title> Tracking the red queen: Measurements of adaptive progress in co-evolutionary simulations. </title> <editor> In F. Moran, A. Moreno, J. J. Merelo, and P. Chacon, editors, </editor> <booktitle> Advances in Artificial Life. Third European Conference on Artificial Life Proceedings. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: Both controllers and structure of these robots were evolved, and a variety of complex successful strategies emerged. Several researchers have had success applying competitive coevolution to versions of the pursuer-evader differential game <ref> [23, 78] </ref>. Recently, a very simple form of coevolution has been successfully applied to backgammon [75]. Sebald and Schlenzig discuss a different approach to adversarial problems, and its application to controller design [86]. This method uses a nested evolutionary programming procedure. <p> The idea of measuring coverage of prior opponents has been discussed in the context of empirical game learning systems <ref> [23] </ref>. II.D.2 Using Worst-case Strategy Learning Algorithms This competitive algorithm performs as well as possible with worst-case strategy learning algorithms. Theorem 3 Assume a game G has maximum transitive chain length `. The covering competitive algorithm learns a perfect strategy for G in at most `(k 0 + 1) strategies. <p> A black dot indicates that the particular first-population strategy won the game between the row and column individuals. For display purposes, we usually sample generations at a fixed interval to obtain a 200 by 200 bitmap. Similar diagrams have been used to show the outcome of pursuer-evader coevolution <ref> [23] </ref>. In the ideal arms race, each individual would defeat the opponents from all prior generations. This would correspond to a black triangle in the lower left, and a white triangle in the upper right.
Reference: [24] <author> D. Cohn, L. Atlas, and R. Ladner. </author> <title> Improving generalization with active learning. </title> <journal> Machine Learning, </journal> <volume> 15(2) </volume> <pages> 201-221, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Here, we are more concerned with heuristic methods for large, difficult problems. Some of the general theory, however, may be usefully applied to adversarial problems; methods in Chapter II have some connections to this theory. In machine learning, some of the issues in active learning <ref> [24, 25, 74] </ref> are similar to those here. Active learning tries to select informative examples from which to learn. For example, Cohn [24] makes the assumption that examples are available without labels. Labels may be obtained via some expensive method, so the number of labels requested is to be minimized. <p> In machine learning, some of the issues in active learning [24, 25, 74] are similar to those here. Active learning tries to select informative examples from which to learn. For example, Cohn <ref> [24] </ref> makes the assumption that examples are available without labels. Labels may be obtained via some expensive method, so the number of labels requested is to be minimized.
Reference: [25] <author> D.A. Cohn, Z. Ghahramani, and M.I. Jordan. </author> <title> Active learning with statistical models. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 129-145, </pages> <year> 1996. </year> <month> 209 </month>
Reference-contexts: Here, we are more concerned with heuristic methods for large, difficult problems. Some of the general theory, however, may be usefully applied to adversarial problems; methods in Chapter II have some connections to this theory. In machine learning, some of the issues in active learning <ref> [24, 25, 74] </ref> are similar to those here. Active learning tries to select informative examples from which to learn. For example, Cohn [24] makes the assumption that examples are available without labels. Labels may be obtained via some expensive method, so the number of labels requested is to be minimized. <p> No search over examples is done, though; the learner simply watches incoming examples from some distribution, and chooses which to label. In related work, Cohn shows how to choose new examples optimally in a problem-dependent way for certain simple concept representations <ref> [25] </ref>. Plutowski [74] minimizes the number of examples used for learning, but does no search over examples: instead, examples are selected from a fixed set on the basis of a criterion that tells how informative they will be.
Reference: [26] <author> J.H. Condra and E.A. Emini. </author> <title> Preventing hiv-1 drug resistance. </title> <booktitle> Science & Medicine, </booktitle> <month> January/February </month> <year> 1997. </year>
Reference-contexts: It is based on simple properties of the geometry of the HIV protease active site. Inhibitor binding is evaluated using the idea that molecules tend to bind when they have complementary shapes that fit together well. Most existing protease inhibitors are peptidemimetics <ref> [26] </ref>. We also consider this class of inhibitors, represented as hexapeptides (sequences of six amino acids). When bound to HIV protease, the position of such inhibitors tends to be well-conserved. Each of the six amino acids fits into a roughly defined pocket in the active site of the protease. <p> It seems that most simple mathematical coevolutionary models do not include the possibility of a search that is so difficult that pedagogical stepping stones are required to make progress [38]. Concepts related to pedagogy and search difficulty have been discussed in the context of virus evolution <ref> [26] </ref>. For complex organisms, however, it may be difficult to understand the evolutionary landscape in sufficient detail to show the need for pedagogical stepping stones. This may make computational experiments valuable in providing understandable coevolutionary systems with sufficient complexity to make pedagogy a significant factor. <p> It is not the case that resistance mutations do not exist to such combination therapies. Instead, such therapies create a difficult evolutionary problem for HIV: multiple mutations are required for effective resistance. Since such multiple mutations are rare, HIV replication can be suppressed for a very long time <ref> [26] </ref>. In patients that have already received some subset of the drugs included in the combination, prior to starting combination therapy, resistant forms of HIV may already be present that overcame the prior stepping stone.
Reference: [27] <author> J.H. Condra, William A. Schleif, et al. </author> <title> In vivo emergence of hiv-1 variants resistant to multiple protease inhibitors. </title> <journal> Nature, </journal> <volume> 374 </volume> <pages> 569-571, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: Mutants of HIV protease that are resistant to an inhibitor have a modified shape that prevents binding by the inhibitor, while retaining the required cleavage ability of the protease. In clinical tests, mutants have arisen that are cross-resistant to a panel of inhibitors <ref> [27] </ref>. This application depends on a computational model that can predict whether or not a given inhibitor will be effective against a particular mutant form of HIV protease. For example, molecular docking software predicts geometry and energy of binding between molecules [65].
Reference: [28] <author> P. Darwen and X. Yao. </author> <title> Automatic modularisation by speciation. </title> <booktitle> In Proceedings of the Third IEEE International Conference on Evolutionary Computation (ICEC'96), </booktitle> <year> 1996. </year>
Reference-contexts: Recently, several authors have used a tournament format based on [90] with competitive coevolution. This format yields fitness with properties similar to those given by competitive fitness sharing. This has been successfully applied to the Prisoners' Dilemma <ref> [28] </ref> and the game of Dots-and-Boxes [98]. 58 III.C.2 Shared Sampling It is desirable to reduce the computational effort expended in competition by testing each individual in the host population against only a limited sample of parasites from the other population.
Reference: [29] <author> R. Dawkins and J.R. Krebs. </author> <title> Arms races between and within species. </title> <journal> Proceedings of the Royal Society of London B, </journal> <volume> 205 </volume> <pages> 489-511, </pages> <month> September </month> <year> 1979. </year>
Reference-contexts: of solving adversarial problems, the most encouraging feature of biological coevolution is the evolutionary arms race, in which competing species are engaged in a feedback loop; an adaptive change by one creates 6 a new challenge for the other, leading to an adaptive change on its part and so on <ref> [29] </ref>. A classic example of an evolutionary arms race occurs between carnivores and their herbivorous prey. The table below shows brain size (normalized for body size) increases in these two groups of species over a 60 million year time span, based on fossils from the northern hemisphere [47]. <p> V.B Arms Races The original concept of the biological arms race included the idea of reciprocal adaptations, and the idea of generally improved competitive ability for tightly 190 interacting species <ref> [29] </ref>. But it didn't include a complete definition that would allow a test for the existence of a continued arms race over arbitrarily long time scales.
Reference: [30] <author> T. Durham. </author> <title> A program with a touch of Zen. In Computing Horizons. </title> <publisher> Addison-Wesley, </publisher> <year> 1985. </year>
Reference-contexts: Cellular automata are based on local pattern-matching primitives, but can compute larger-scale features through repeated action of these primitives sequentially in time. Hand 148 designed cellular automata rules have been used in a program written by Allan Scarff, reportedly with some success <ref> [30] </ref>. We performed preliminary experiments that used a very simple representation of the form described above, with little added computational capability or Go knowledge [80]. We do not describe these experiments in detail here, since they used a different form of coevolution from the other applications described in this chapter.
Reference: [31] <author> M. Enzenberger. </author> <title> The integration of a priori knowledge into a go playing neural network. </title> <address> ftp://ftp.cgl.ucsf.edu/pub/pett/go/ladder/NeuroGo.ps.gz, </address> <year> 1996. </year>
Reference-contexts: Such novel approaches have hope of improving on the state of the art sooner than in other games such as chess where conventional methods already work so well. Reinforcement learning has been used to train neural networks to play Go <ref> [85, 31] </ref>. While this method has been very successful in other games such as backgam 145 mon [95], it has had only limited success in Go. Part of the reason for this may be that learning-based Go programs have been simple experimental systems. <p> When we do this, we try to provide only simple, exact features, leaving heuristic judgements to be made by coevolution. Existing learning-based approaches to Go use neural network evaluation functions <ref> [85, 31] </ref>. To play using an evaluation function, all possible moves must be evaluated each turn. This is computationally expensive, so we instead evolve control functions that directly choose a move. Beyond this difference, our representation shares many capabilities with these neural network representations. <p> A flag specifies whether the variable should be initialized to zero before doing the addition. These counting, thresholding, and weighting operations give strategies some of the computational capabilities of neural networks <ref> [85, 31] </ref>. For actions which modify string variables, this becomes slightly more complicated. For a particular string, all matches of the condition are checked, where the central point in the condition can be any of the member points in the string.
Reference: [32] <author> S.L. Epstein. </author> <title> Toward an ideal trainer. </title> <journal> Machine Learning, </journal> <volume> 15(3) </volume> <pages> 251-277, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: In such problems, dynamic programming cannot be usefully applied because it has no temporal structure to work with. We seek a more general approach to adversarial problems. Several authors have discussed general issues surrounding the proper choice of opponents for the learning of robust game strategies <ref> [32, 91, 8] </ref>. The common theme is that diverse strong opposition is needed to learn good strategies. A single expert opposing program will often be unavailable, and even if it is available it may have a simple flaw that can be exploited by an otherwise weak strategy. <p> This is a fairly strong assumption; we make it for several reasons. Strategy learning is an optimization problem with a fixed, efficiently-computable cost function. This type of optimization problem has been well studied. Several researchers have had empirical success in strategy learning against fixed opponents <ref> [32, 85, 91] </ref>. The focus of the theory presented here is on the difficulties in game learning that arise even though a strategy learning algorithm exists. <p> Some of our early experiments in competitive coevolution displayed repeated convergence on specialists that were poor test cases overall, with little arms race progress seen. The use of diverse, strong opponents has been discussed in the context of game-playing <ref> [32, 91] </ref>. Here, we want diverse parasites at every stage, appropriate to the current level of the hosts. The parasites need to be diverse in such a way that they provide appropriate testing for all hosts in the current population.
Reference: [33] <editor> L.J. Eshelman, editor. </editor> <booktitle> Proceedings of the Sixth International Conference on Genetic Algorithms. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: Over successive generations, selection chooses the best novel individuals generated by crossover and mutation, so that fitness tends to improve. Extensive work on genetic algorithms discusses many variations on this basic outline, and a wide variety of applications (for example <ref> [33, 17] </ref>). The GA has a natural coevolutionary extension to adversarial problems, described below. I.B.3 Biological Motivation The genetic algorithm is inspired and informed by properties of biological evolution. Similarly, coevolutionary methods for approaching adversarial problems are inspired by competitive coevolution in nature.
Reference: [34] <author> A.S. Fraenkel, M.R. Garey, D.S. Johnson, T. Schafer, and Y. Yesha. </author> <title> The complexity of checkers on an n x n board preliminary report. </title> <booktitle> In FOCS 19, </booktitle> <year> 1978. </year>
Reference-contexts: Given our simple framework, however, Example 1 is adequate to show long transitive chains may exist within powerful classes of strategies for Go. This example could be easily extended to N by N checkers <ref> [34] </ref>. This suggests that we should go beyond worst-case strategy learning algorithms, to eliminate the need for dependence on `. II.B.3 Randomized Strategy Learning Algorithms In practice, it seems unlikely that simple, natural strategy learning algorithms will be "adversarial", producing uninformative near-worst-case strategies.
Reference: [35] <author> R.J. Freund and W.J. Wilson. </author> <title> Statistical Methods. </title> <publisher> Academic Press, </publisher> <year> 1997. </year>
Reference-contexts: (20) * * 10 100% 5.02 1.12 Table III.1: Number of Game-Pairs to Solve 2DTTT (in Millions) (i) (ii) (iii) (iv) (v) (vi) (vii) (ii) X - (iv) X - (vi) X - Table III.2: Significant Differences (p &lt; 0:05) in Number of Games to Solve 2DTTT post hoc tests <ref> [35] </ref>. This test is used for all significance tests in this chapter. In each case, some significant difference exists among the samples, so we will only report the pairwise significant differences. For 2DTTT, significant differences are shown in Table III.2.
Reference: [36] <author> Y. Freund and R.E. Schapire. </author> <title> Game theory, on-line prediction and boosting. </title> <booktitle> In Proceedings of the Ninth Annual Conference on Computational Learning Theory, </booktitle> <year> 1996. </year>
Reference-contexts: The methods here are related to algorithms of interest in concept learning due to their connection to boosting <ref> [36] </ref>. The algorithms used in concept learning 45 update a distribution over strategies by increasing the weight on strategies that compete successfully; this is similar to the competitive methods described here.
Reference: [37] <author> D. Fudenberg and D.K. Levine. </author> <title> Theory of Learning in Games. </title> <note> published online at http://levine.sscnet.ucla.edu/Papers/CONTENTS.HTM, 1997. </note>
Reference-contexts: Since this is usually infeasible, there is no way to directly apply optimization algorithms. 3 I.B Related Work I.B.1 Related Problems and Methods Classical game theory [96], the theory of differential games [64], and economic game learning theory <ref> [37] </ref> address similar problems to those described here. The focus in this literature is typically on problems simple enough to be understood analytically. When general-purpose methods for solving games are explored, they are typically efficient only for very small games.
Reference: [38] <author> D.J. Futuyma and M. Slatkin. </author> <title> Coevolution. </title> <publisher> Sinauer, </publisher> <year> 1983. </year>
Reference-contexts: Although the computational experiments in Chapters III and IV are not focused on biological modelling, it is still worth going back to explore coevolutionary concepts from these experiments that may be relevant to biological understanding. The biological literature on coevolution is well developed <ref> [38] </ref>. Existing work explores a wide variety of evolutionary phenomena that can occur, and includes simple mathematical models. Computational experiments may be able to contribute to this field by providing examples of coevolution that are concrete, and that include a complex search component. <p> It seems that most simple mathematical coevolutionary models do not include the possibility of a search that is so difficult that pedagogical stepping stones are required to make progress <ref> [38] </ref>. Concepts related to pedagogy and search difficulty have been discussed in the context of virus evolution [26]. For complex organisms, however, it may be difficult to understand the evolutionary landscape in sufficient detail to show the need for pedagogical stepping stones.
Reference: [39] <author> A. Giordana and F. Neri. </author> <title> Search-intensive concept induction. </title> <journal> Evolutionary Computation, </journal> <volume> 3(4), </volume> <year> 1995. </year>
Reference-contexts: Related to this is the "emergent fitness sharing" technique found to promote generalization in an immune system model [90]. and the "universal suffrage operator" that encourages diverse coverage of examples in a GA-based concept induction system <ref> [39] </ref>. Our method is similar to these techniques. In the "competitive fitness sharing" (henceforth, "fitness sharing" or "shared fitness") method we propose here, each parasite is treated as an independent resource to be shared by those hosts in the population that can defeat it.
Reference: [40] <author> R.R. Glesener and D. Tilman. </author> <title> Sexuality and the components of environmental uncertainty. </title> <publisher> American Naturalist, </publisher> <year> 1978. </year>
Reference-contexts: A survey of the use of sexual recombination by animals in a variety of ecosystems reveals that it is more prevalent when the ecosystem is biologically complex, with a wide variety of coevolutionary interactions <ref> [40] </ref>. It is less common in ecosystems with fewer predatory and pathogenic species. It is believed that the coevolutionary pressures in complex environments demand high evolutionary rates of participating species. Species that evolve more slowly due to a lack of sexual recombination are not competitively viable [89].
Reference: [41] <author> D.E. Goldberg and J. Richardson. </author> <title> Genetic algorithms with sharing for multimodal function optimization. </title> <editor> In John J. Grefenstette, editor, </editor> <booktitle> Proceedings of the Second International Conference on Genetic Algorithms. </booktitle> <editor> L. </editor> <publisher> Erlbaum Assoc., </publisher> <year> 1987. </year> <month> 210 </month>
Reference-contexts: This measure will be called "simple fitness" here. In "fitness sharing" a sharing function is defined to take into account similarity (according to some metric) among individuals <ref> [41] </ref>. An individual's simple fitness is divided by the sum of its similarities with each other individual in the population, rewarding unusual individuals.
Reference: [42] <author> S.A Goldman and M.J. Kearns. </author> <title> On the complexity of teaching. </title> <editor> In M.K. War-muth and Leslie Valiant, editors, </editor> <booktitle> Proceedings of the Fourth Annual Workshop on Computational Learning Theory. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: Define the specification number k for G to be the size of the smallest teaching set. These definitions follow the corresponding ones for concept learning <ref> [10, 42] </ref>. Since we are considering exact learning, we seek bounds on learning time that are polynomial in k, as well as in lg (jHj) and lg (jX j). The following lemma 18 shows that the dependence on k is necessary. <p> The other side, without an optimal strategy, can only hope to defeat non-optimal opponents. A teaching set is a set of individuals that work together by having, for each possible non-optimal opponent, at least one individual in the set capable of defeating that opponent <ref> [42] </ref>. If no reasonably-sized teaching set exists, then optimal individuals cannot be practically distinguished, and coevolution cannot be expected to find optimal individuals. In general, small teaching sets seem to exist for many problems [42, 10]. For competitive coevolution, this suggests: 1. <p> If no reasonably-sized teaching set exists, then optimal individuals cannot be practically distinguished, and coevolution cannot be expected to find optimal individuals. In general, small teaching sets seem to exist for many problems <ref> [42, 10] </ref>. For competitive coevolution, this suggests: 1. We should encourage some sort of niching, so that such sets of individuals may arise and be stable in a population. 2.
Reference: [43] <author> W.E. Hart, T.E. Kammeyer, and R.K. Belew. </author> <title> The role of development in genetic algorithms. </title> <editor> In D. Whitley and M. Vose, editors, </editor> <booktitle> Foundations of Genetic Algorithms 3, </booktitle> <pages> pages 315-332. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: The offspring is chosen to lie at a random point in this expanded box. To aid search in finding edges of the constraint region, patients that are generated outside the constraint region via crossover and mutation are "repaired" <ref> [43] </ref>. A binary search is done along the line between the new point (outside the constraint region) and the old point (inside the constraint region). An arbitrary parent is chosen as the old point when repairing the results of crossover. <p> The computational experiments in Chapters III and IV do not offer sufficiently complex representations to consider the evolution of developmental systems. But there have been experiments combining genetic algorithms with models of development <ref> [43] </ref>, so there is some hope that computational experiments could eventually shed light on this hypothesis. V.C Pedagogy, Drift, and Resistance Section III.G.6 discussed the idea of pedagogy in coevolution.
Reference: [44] <author> W. D. Hillis. </author> <title> Co-evolving parasites improve simulated evolution as an optimization procedure. </title> <editor> In C. Langton et al., editor, </editor> <booktitle> Artificial Life II. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: There are, however, an exponentially large number of sequences, making exhaustive testing expensive when searching for large networks. The testing problem may be substantially reduced by searching for a smaller set of difficult-to-sort test sequences that still serve as an adequate test suite <ref> [44] </ref>. As a rather different example, consider the search for strategies in a game. Here, the problem is to discover a strategy that defeats any possible opponent, without testing exhaustively against the large space of possible opponents. <p> Since then, a number of coevolutionary experiments have been done on this problem. The goals of these experiments are typically in modelling the emergence of cooperation, which is fairly orthogonal to the concerns of adversarial problems. Hillis coevolved sorting networks and difficult sequences to sort <ref> [44] </ref>. Coevolution produced better solutions than random testing. We revisit this application in Section IV.B. Coevolution can be naturally applied to finding strategies for zero-sum games. Angeline and Pollack applied a coevolutionary form of genetic programming to tic-tac-toe [8], with moderate success. <p> We need to be able to test whether a candidate implementation of the function is correct on a chosen point in the domain. Given this, we can search for an implementation that is correct over the whole domain. For example, Hillis <ref> [44] </ref> used coevolution to simultaneously search for sorting networks and test sequences, to obtain an implementation of sorting that was suitable for hardware 9 (see Section IV.B for further experiments on sorting networks). <p> In this chapter we use the term host to refer to the individual whose fitness is under consideration, and parasites to refer to the individuals that are testing the host (this terminology differs somewhat from Hillis' <ref> [44] </ref> and from biological terminology). In all experiments done here, two evolutionarily distinct populations (i.e. no exchange of genetic material between populations) are used, with competitions taking place between populations. One population contains candidate solutions to the adversarial problem, and the other contains test cases. <p> We experimentally test these methods below, separately and in combination with each other. III.C.1 Competitive Fitness Sharing The usual way to assign fitness based on success in a set of competitions is to sum the score (often a binary value indicating success/failure) across all competitions <ref> [8, 44, 12] </ref>. This measure will be called "simple fitness" here. In "fitness sharing" a sharing function is defined to take into account similarity (according to some metric) among individuals [41]. <p> This is far smaller than the specification number of 65519 for unbiased 16-input networks. Although it would be possible to manually restrict testing of butterfly-biased networks to a complete set of 151 sequences, coevolutionary experiments such as those performed by Hillis <ref> [44] </ref> are still valuable in this domain. Below, experiments with such biased networks serve as a basis of comparison against coevolutionary experiments on unbiased sorting networks, where specification number is much larger. Also, the ease of manual testing is very specific to butterfly-biased networks. <p> So, the general study of coevolution on biased classes of networks is worthwhile. Hillis Several authors have explored the use of genetic algorithms and related evolutionary methods for sorting network design. The original work along these lines is due to Hillis <ref> [44] </ref>, and it is the most relevant work here. Hillis used an evolutionary algorithm to design 16-input sorting networks 129 [44]. This size has been a focus of efforts to produce nets with a minimal number of compare-exchange gates. <p> Hillis Several authors have explored the use of genetic algorithms and related evolutionary methods for sorting network design. The original work along these lines is due to Hillis <ref> [44] </ref>, and it is the most relevant work here. Hillis used an evolutionary algorithm to design 16-input sorting networks 129 [44]. This size has been a focus of efforts to produce nets with a minimal number of compare-exchange gates. Hillis constrained networks to begin with the butterfly, and evolved the remainder of the network. Experiments used a diploid representation that allowed for a variable number of compare-exchange gates. <p> Non-coevolutionary were said to run for up to 135 (a) Biased (b) Unbiased 100 generations 350 generations Figure IV.7: Sorting Network Arms Races 5000 generations with a population of 65536 <ref> [44] </ref>. In coevolutionary runs, with 10-20 parasites per test, this would be about 3-6 billion total competitions. Hillis states that coevolutionary runs were faster than noncoevolutionary runs, but it is unclear whether this was due to reduced testing per sorting network, or a smaller number of generations. <p> A second type of spatial distribution places individuals on a grid (which may be one-dimensional, two-dimensional, or higher-dimensional); mating and reproductive competition take place among individuals that are nearby on the grid. Spatially distributed populations have also been used with competitive coevolution, in Hillis' experiments on sorting network design <ref> [44] </ref>. In this work, both solutions and test cases are distributed on a two-dimensional grid, and competition takes place between solutions and test cases at the same spatial location on the grid. An island version of coevolution would also be possible.
Reference: [45] <author> J.H. Holland. </author> <title> Adaptation in natural and artificial systems : an introductory analysis with applications to biology, control, </title> <booktitle> and artificial intelligence. </booktitle> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: A third binary variable is used for output to specify the move as described above. Five more variables may be used in arbitrary ways by evolved strategies. Rules are represented as condition-action pairs, in the style of classifier systems <ref> [45] </ref>. Conditions consist of 0, 1, or "don't care" for each bit in the points of a 3x3 neighborhood around the central point. Such a neighborhood is shown in Figure IV.8. Actions consist of 0, 1, or "don't modify" for each bit in the central point.
Reference: [46] <institution> Japan Computer Shogi Association and Japan Computer Go Association. </institution> <note> Game Programming Workshop in Japan '95, </note> <year> 1995. </year>
Reference-contexts: IV.C.3 Go as an AI Problem Although a number of Go programs are in development by various researchers, recent published work on computer Go is fairly rare <ref> [56, 85, 46, 20, 77] </ref>. More information about existing Go programs and issues in computer Go is available from the Internet [11, 1, 2]; refer to these for further information about topics discussed below.
Reference: [47] <author> H.J. Jerison. </author> <title> Evolution of the Brain and Intelligence. </title> <publisher> Academic Press, </publisher> <year> 1973. </year>
Reference-contexts: A classic example of an evolutionary arms race occurs between carnivores and their herbivorous prey. The table below shows brain size (normalized for body size) increases in these two groups of species over a 60 million year time span, based on fossils from the northern hemisphere <ref> [47] </ref>. Similar increases have been observed for running speed (derived from bone size measurements) [13]. <p> In biological coevolution, there are a variety of possible responses to a lack of balance. The dominated side can become extinct: herbivores may have gotten far enough ahead (in running speed and brain size) to be a major factor in the extinction of creodonts, carnivores of the early Tertiary <ref> [47] </ref>. If the side that is dominating the competition relies on exploitation of the opposition, it may have to back off from this domination to preserve the opposition for its own survival. This may contribute, for example, to the evolution of reduced virulence in parasites [62].
Reference: [48] <author> M. Jerrum, L. Valiant, and V. Vazirani. </author> <title> Random generation of combinatorial structures from a uniform distribution. </title> <journal> Theoretical Computer Science, </journal> <volume> 43(2), </volume> <year> 1986. </year>
Reference-contexts: II.E.2 Complexity Accounting for k The problem solved by the strategy learning algorithm is in N P (a formula containing several copies of f , with different opponents inserted, must be satisfied). Satisfying the (p; q)-randomization criterion is not much more difficult: the method of almost uniform generation <ref> [48] </ref> can be used to do this in randomized polynomial time with an N P oracle, as follows. <p> This can be done with an N P oracle in randomized time polynomial in the input size and log ( 1 * ) <ref> [48] </ref>. We may reasonably meet the (p; q)-randomization criterion by, for example, setting * = 1 2 to obtain q = 1 and p = 2 3 . <p> Almost uniform generation chooses members y of A with probability u (y), where 1 1+* u (y) u (1 + *)u (y). This can be done in time polynomial in n and log ( 1 * ), with an N P oracle <ref> [48] </ref>. Steps 2 and 3 of the learning algorithm may be done with the aid of an N P oracle. Values for the constants are given below.
Reference: [49] <author> H. Juille. </author> <title> Evolution of non-deterministic incremental algorithms as a new approach for search in state spaces. </title> <editor> In Larry J. Eshelman, editor, </editor> <booktitle> Proceedings of the Sixth International Conference on Genetic Algorithms. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: Hillis' work (see below) on search for 16-input networks constrains all nets to start with the "butterfly," a particular sequence of 32 compare-exchanges. Since many of the best human-designed 16-input networks begin with the butterfly, this is a reasonable way to reduce the search space. Baum [14] and Juille <ref> [49] </ref> point out that the butterfly also greatly reduces the required testing problem. Only 151 distinct unsorted sequences remain after passing all 2 16 zero-one sequences through the initial 16-input butterfly, so specification number for sorting nets biased by the butterfly is 151. <p> The spatial distribution may be of value in maintaining diverse niches in such situations. This is discussed further in Chapter V. Other Experimental Work Juille used a problem-specific distributed search algorithm to find sorting networks without using the initial bias of 32 compare-exchanges used by Hillis <ref> [49] </ref>. The algorithm used exhaustive testing in a specialized way. Compare-exchange gates were added sequentially to an initially empty network. As partial networks were built, the set of remaining unsorted test cases was tracked; once the set becomes 131 empty the sorting network is correct. <p> This simple method may not provide a high-performance search for optimally short networks (see Juille's method for a performance system <ref> [49] </ref>), but it should prevent networks from becoming excessively long. In the effort to produce shorter sorting networks, evolution may meaningfully proceed for many generations after the first perfect network is found.
Reference: [50] <author> T.E. Kammeyer, R.K. Belew, and S.G. Williamson. </author> <title> Evolving compare-exchange networks using grammars. </title> <journal> Artificial Life, </journal> <volume> 2(2), </volume> <year> 1995. </year>
Reference-contexts: Alternative choices of bias on the space of candidate solutions in this problem allow us to explore the behavior of coevolution under varying conditions of theoretical testing difficulty. Sorting networks are sorting algorithms of a particular simple form, suitable 126 for hardware implementation <ref> [52, 50] </ref>. Sorting networks consist of a sequence of compare-exchange gates that operate on a fixed-length sequence of numbers. Each compare-exchange examines two particular elements (for example, the third and fifth elements in the sequence) and swaps them if they are out of order. <p> Below, experiments with such biased networks serve as a basis of comparison against coevolutionary experiments on unbiased sorting networks, where specification number is much larger. Also, the ease of manual testing is very specific to butterfly-biased networks. A variety of useful biases are possible (for example, developmental biases <ref> [50] </ref>), not all of which possess small test sets that are easy to analytically derive. So, the general study of coevolution on biased classes of networks is worthwhile. Hillis Several authors have explored the use of genetic algorithms and related evolutionary methods for sorting network design. <p> While quite successful, it is difficult to compare results from this largely problem-specific technique with those obtained by more general search algorithms and coevolutionary methods. Kammeyer and Belew used a grammar-based developmental representation to evolve specifications that could generate sorting networks of arbitrarily large size <ref> [50] </ref>. During evolution, small networks were generated from candidate grammars and tested exhaustively. The best specifications resulting from evolution were checked manually to prove that they could generate correct networks for any number of inputs. This work succeeded in producing novel families of correct networks.
Reference: [51] <author> M.J. Kearns and U.V. Vazirani. </author> <title> An Introduction to Computational Learning Theory. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Chernoff bounds may be used to upper bound the probability of an excessive number of successes <ref> [51] </ref>. Using a Chernoff bound, the probability that we obtain more than a fraction ffi 0 (1 + *) + ff of successes is at most e 2m (ff 2 ) .
Reference: [52] <author> D.E. Knuth. </author> <title> The Art of Computer Programming, volume 3: Sorting and Searching. </title> <publisher> Addison Wesley, </publisher> <year> 1973. </year>
Reference-contexts: Alternative choices of bias on the space of candidate solutions in this problem allow us to explore the behavior of coevolution under varying conditions of theoretical testing difficulty. Sorting networks are sorting algorithms of a particular simple form, suitable 126 for hardware implementation <ref> [52, 50] </ref>. Sorting networks consist of a sequence of compare-exchange gates that operate on a fixed-length sequence of numbers. Each compare-exchange examines two particular elements (for example, the third and fifth elements in the sequence) and swaps them if they are out of order. <p> Prior theoretical work on sorting networks allows us to determine specification number for the sorting network design problem. A completely naive test of candidate networks might use all n! permutations of n distinct inputs. The zero-one principle <ref> [52] </ref> shows that this is unnecessary: any sorting network that sorts all 2 n sequences of zeroes and ones will sort any sequence correctly. <p> Initial experiments used randomly chosen tests from this space. This succeeded in producing correct sorting networks, the shortest of which were 65 compare-exchanges long. This is several longer than Green's network, the shortest known on 16 inputs with 60 compare-exchanges <ref> [52] </ref>. In an effort to obtain better results, Hillis tried to do more effective testing by using coevolution. Test cases consisted of 10-20 test sequences, and competition was restricted to be between test cases and sorting networks at the same location on the grid.
Reference: [53] <author> J.R. Koza. </author> <title> Genetic programming : on the programming of computers by means of natural selection. </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: If, for example, genetic operators are often destructive, or if fitness improvements are rare, brood selection may be able to efficiently test the large numbers of individuals that must be generated each generation. 1 Genetic programming is the application of genetic algorithms to the evolution of programs <ref> [53] </ref>. 64 In competitive coevolution, the first condition is always satisfied: we are always testing hosts against a sample of parasites. It is easy to use a smaller sample of parasites for brood testing. The natural fit of brood selection helps motivate its consideration here.
Reference: [54] <author> M. Land and R.K. Belew. </author> <title> Towards a self-replicating language for computation. </title> <editor> In J.R. McDonnell, R.G. Reynolds, and D.B. Fogel, editors, </editor> <booktitle> Evolutionary Programming IV. </booktitle> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: To study this emergence of complexity in the coevolutionary Go experiment, we need a concrete measure of strategy complexity. Simple measures of complexity may only capture superficial features of strategies, rather than truly useful complexity <ref> [54] </ref>. For example, one measure of complexity that seems appropriate in this representation is the number of rules in a strategy. But rules do not need to be used during computation, so a large number of rules may be present in a strategy without many being used.
Reference: [55] <author> D.B. Lenat. </author> <title> AM: Discovery in mathematics as heuristic search. </title> <editor> In R. Davis and D.B. Lenat, editors, </editor> <booktitle> Knowledge-Based Systems in Artificial Intelligence, </booktitle> <pages> pages 1-225. </pages> <publisher> McGraw-Hill, </publisher> <year> 1982. </year> <month> 211 </month>
Reference-contexts: In such close-ended domains, computational approaches only need to produce a finite sequence of improvements, until this ultimate solution is found. More complex problem domains in artificial intelligence, such as board games [95], robotics [88], mathematics <ref> [55] </ref>, and artificial life simulations [3], often do not possess such ultimate, perfect solutions of reasonable complexity. Instead, there exist open-ended sequences of useful solutions of increasing performance and complexity.
Reference: [56] <author> D. Levy. </author> <title> Computer Games. </title> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: Once placed, stones are never moved. Each turn, almost every empty intersection on the grid is a legal move. See <ref> [56] </ref> for details about the game. The transitive chain is constructed from an exponentially large number of strategies, each of which has a unique numeric label. <p> IV.C.3 Go as an AI Problem Although a number of Go programs are in development by various researchers, recent published work on computer Go is fairly rare <ref> [56, 85, 46, 20, 77] </ref>. More information about existing Go programs and issues in computer Go is available from the Internet [11, 1, 2]; refer to these for further information about topics discussed below.
Reference: [57] <author> K. Lindgren and M.G. Nordahl. </author> <title> Artificial food webs. </title> <booktitle> In Artificial Life III, </booktitle> <year> 1994. </year>
Reference-contexts: First, we give two revealing viewpoints of competitive coevolution. Then, we will describe the methods, and analyze them mathematically and experimentally. III.B.2 The Infinite Population View Consider an evolutionary model similar to one used in <ref> [57] </ref>. Genotypes occupy a fraction of a population, so that these fractions sum to 1. Since these 54 fractions can be any real number between 0 and 1, the population size is essentially infinite. Only a finite number of distinct genotypes are present in the population.
Reference: [58] <author> R.J. Lipton and N.E. Young. </author> <title> Simple strategies for large zero-sum games with applications to complexity theory. </title> <booktitle> In Proceedings of the Twenty-Sixth Annual ACM Symposium on the Theory of Computing. STOC '94. ACM, </booktitle> <year> 1994. </year>
Reference-contexts: But this is simply the canonical 2 P -complete decision problem QSAT 2 [70], so game learning is 2 P -complete. Similar problems in game theory have been related to the complexity class 2 P <ref> [58, 71] </ref>. Lemma 2 shows that, using our definition of a competitive algorithm, no competitive algorithm exists that solves every game in time polynomial in lg (jHj), lg (jX j), and k with worst-case strategy learning algorithms. <p> Assume g returns 1 or 0 indicating whether the first player or second player was the winner, respectively, in time polynomial in n 1 and n 2 . The goal is to find a "small" mixed strategy (in the same sense used by Lipton and Young <ref> [58] </ref>), that randomizes uniformly over a polynomially large multiset of pure strategies and obtains a value for the game that is close to optimal. <p> An optimal mixed strategy may require an exponentially large representation. Lipton has shown, however, that small mixed strategies that randomize over polynomially many pure strategies may achieve a payoff close to optimal <ref> [58] </ref>. In particular, let z = n 2 2fi 2 , for some chosen fi. <p> This improves a computational complexity result in <ref> [58] </ref>, where it is shown that approximate game learning is in 2 P .
Reference: [59] <author> N. Littlestone. </author> <title> Learning quickly when irrelevant attributes abound: a new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2(4), </volume> <year> 1988. </year>
Reference-contexts: It is even true independent of the specification number, which can get large for this class of games if second-player strategies are unrestricted. Also, this is accomplished without keeping track of all previous first and second player strategies. This is similar to mistake-bounded concept learning <ref> [59] </ref>. Since the mistake bound should be fairly small, one can even imagine doing final learning, with such a strategy learning algorithm, against expert human opponents. This mistake bound works for this special type of representation and strategy learning algorithm.
Reference: [60] <author> W. Maass. </author> <title> On-line learning with an oblivious environment and the power of randomization. </title> <booktitle> In Proceedings of the Fourth Annual Workshop on Computational Learning Theory, </booktitle> <year> 1991. </year>
Reference-contexts: Since we allow the strategy learning algorithm unlimited access to the game, any game with a solution has a first-player strategy learning algorithm that immediately produces a perfect strategy. Another typical approach for going beyond worst-case algorithms is to use randomized learning algorithms <ref> [60] </ref>. We consider randomized strategy learning al 22 gorithms that, when passed a set A of opponents, choose from a distribution over the set of strategies (or strategy sets) that defeat A.
Reference: [61] <author> S.W. Mahfoud. </author> <title> Population size and genetic drift in fitness sharing. </title> <editor> In L.D. Whitley and M.D. Vose, editors, </editor> <booktitle> Foundations of Genetic Algorithms 3. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: Probabilities will be derived here assuming that fitness proportional selection is used. The result should be comparable under tournament selection, but the analysis is more difficult. A similar, far more extensive analysis has been done for fixed fitness functions with conventional fitness sharing <ref> [61] </ref>. First, assume that competitive fitness sharing is being used. The total fitness received by the host population is O, since each defeatable parasite contributes a total of 1 fitness to the host population, divided evenly among those hosts able to defeat it.
Reference: [62] <author> R.M. </author> <title> May and R.M. </title> <editor> Anderson. Parasite-host coevolution. In D.J. Futuyma and M. Slatkin, editors, Coevolution. </editor> <publisher> Sinauer, </publisher> <year> 1983. </year>
Reference-contexts: If the side that is dominating the competition relies on exploitation of the opposition, it may have to back off from this domination to preserve the opposition for its own survival. This may contribute, for example, to the evolution of reduced virulence in parasites <ref> [62] </ref>. Finally, there may be a feedback loop that restores balance by increasing selection pressure on the dominated species and reducing selection pressure on the dominating species [79]. This difference in evolutionary rates tends to bring the species back into parity.
Reference: [63] <author> J. McInerney. </author> <title> Biologically Influenced Algorithms and Parallelism in Non-Linear Optimization. </title> <type> PhD thesis, </type> <institution> University of California, </institution> <address> San Diego, </address> <year> 1992. </year>
Reference-contexts: V.D.1 Further Methods for Competitive Coevolution Spatially Distributed Coevolution The spatial distribution of populations is likely to be a useful technique for coevolutionary heuristics. Spatially distributed populations have been successfully used in non-coevolutionary genetic algorithms to efficiently parallelize evolution, and maintain multiple solution niches during search <ref> [63] </ref>. One type of distributed evolution breaks a population up into separate sub-population "islands", with small amounts of migration among islands maintaining communication between subpopulations.
Reference: [64] <author> A. Mehlmann. </author> <title> Applied Differential Games. </title> <publisher> Plenum, </publisher> <year> 1988. </year>
Reference-contexts: Since this is usually infeasible, there is no way to directly apply optimization algorithms. 3 I.B Related Work I.B.1 Related Problems and Methods Classical game theory [96], the theory of differential games <ref> [64] </ref>, and economic game learning theory [37] address similar problems to those described here. The focus in this literature is typically on problems simple enough to be understood analytically. When general-purpose methods for solving games are explored, they are typically efficient only for very small games.
Reference: [65] <author> G.M. Morris, D.S. Goodsell, R. Huey, and A.J. Olson. </author> <title> Distributed automated docking of flexible ligands to proteins parallel applications of autodock 2.4. </title> <journal> Journal of Computer-Aided Molecular Design, </journal> <volume> 10 </volume> <pages> 293-304, </pages> <year> 1996. </year>
Reference-contexts: This application depends on a computational model that can predict whether or not a given inhibitor will be effective against a particular mutant form of HIV protease. For example, molecular docking software predicts geometry and energy of binding between molecules <ref> [65] </ref>. This could be used to evaluate the binding of an inhibitor to a particular mutant. 6 Proteins are long sequences of amino acids that fold into a specific 3-D shape. <p> Beyond the simple model, we plan to use molecular docking software to obtain more accurate binding evaluations for use in competitive coevolution. Autodock <ref> [65] </ref> is an accurate molecular docking tool developed in Arthur Olson's group at the Scripps Research Institute. This software calculates several energy terms to evaluate the energy of a candidate docking. <p> Extensive development of Autodock has produced a system that has been shown to make accurate predictions of both binding strength and geometry in systems like HIV protease <ref> [65] </ref>. Although it is currently somewhat slow to use for all competition evaluations during competitive coevolution (a single docking run takes about five minutes on a contemporary workstation), problem-specific optimizations may be possible. For example, the inhibitor must bind to the active site for results to be meaningful.
Reference: [66] <author> Martin Mueller. </author> <note> Explorer. Available online at http://nobi.ethz.ch/martin/explorer.html. </note>
Reference-contexts: Evolved first-player strategies from the hall of fame were tested against the Swiss Explorer Go Program (version 3.3, 1995) <ref> [66, 67] </ref>. This is a relatively strong Go program that often achieves respectable performance in international computer Go competitions.
Reference: [67] <author> Martin Mueller. </author> <title> Computer Go as a Sum of Local Games: An Application of Combinatorial Game Theory. </title> <type> PhD thesis, </type> <institution> Institute for Theoretical Computer Science, ETH Zurich, </institution> <year> 1995. </year>
Reference-contexts: Evolved first-player strategies from the hall of fame were tested against the Swiss Explorer Go Program (version 3.3, 1995) <ref> [66, 67] </ref>. This is a relatively strong Go program that often achieves respectable performance in international computer Go competitions.
Reference: [68] <author> S. Nolfi, D. Floreano, O. Miglino, and F. Mondada. </author> <title> How to evolve autonomous robots: </title> <booktitle> different approaches in evolutionary robotics. In Artificial Life IV. </booktitle> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: It should be possible to integrate these during coevolution. This has been done in a GA context with simple staging methods that use computational simulation for evaluations early in a run, and more accurate real 186 world experiments for later evaluations <ref> [68] </ref>. In general, less costly models should be used to carefully choose competitions to evaluate with more accurate, more expensive methods. <p> A threshold of acceptable performance defines the point at which optimization has solved the problem. For example, we might use optimization to design a controller to navigate a robot through an environment, using a simulation of the environment <ref> [68] </ref>. The objective function optimized could be the time required to navigate through the environment, with a threshold giving acceptable time. The simulation of a physical environment is necessarily approximate.
Reference: [69] <author> C.K. Oei, D.E. Goldberg, and S.J. Chang. </author> <title> Tournament selection, niching, and the preservation of diversity. </title> <type> Technical Report IlliGAL Report 91011, </type> <institution> Urbana: University of Illinois, Illinois Genetic Algorithms Laboratory, </institution> <year> 1991. </year>
Reference-contexts: New sample members are chosen to have maximal competitive shared fitness within the current sample. This is equivalent to choosing the sample from the population via truncation selection using competitive fitness sharing with continuously updated sharing <ref> [69] </ref>. In this way, each successive member of the sample is chosen to be one that competed well against individuals that the other members of the current sample did not compete well against. <p> The use of standard fitness sharing with tournament selection can lead to fluctuations in niche sizes. The method of continuously updated sharing has been developed to overcome this problem <ref> [69] </ref>, This method involves calculating shared fitness within the next generation's population, currently being constructed, rather than in the original population. This resembles the method of shared sampling used here. <p> Otherwise, fitness loss from failure to defeat the phantom parasite might prevent optimal individuals from being consistently chosen as elitists. 78 on Nim. It was not found to be helpful. The fluctuations discussed in <ref> [69] </ref> may be less of a problem due to the presence of a large number of niches, and the fact that competitive fitness sharing sums fitness over many components. <p> To help prevent such oscillations from becoming problematic, we use continuously updated sharing in the sorting network population to give perfect and imperfect networks fairly stable niche sizes within each generation. This method measures shared fitness within the new generation being constructed <ref> [69] </ref>, and was described in section III.E.2. In our experiments, a sorting network is represented directly by an ordered list of arbitrary compare-exchanges on the 16 elements. In biased experiments, this list is appended to the butterfly before testing.
Reference: [70] <author> C.H. Papadimitriou. </author> <title> Computational Complexity. </title> <publisher> Addison-Wesley, </publisher> <year> 1994. </year> <month> 212 </month>
Reference-contexts: A decision-problem version of game learning can be formulated: is the game a first-player win? That is, if game outcome is given by f (h; x), decide whether 9h8xf (h; x). But this is simply the canonical 2 P -complete decision problem QSAT 2 <ref> [70] </ref>, so game learning is 2 P -complete. Similar problems in game theory have been related to the complexity class 2 P [58, 71].
Reference: [71] <author> C.H. Papadimitriou and M. Yannakakis. </author> <title> On complexity as bounded rationality. </title> <booktitle> In Proceedings of the Twenty-Sixth Annual ACM Symposium on the Theory of Computing. STOC '94. ACM, </booktitle> <year> 1994. </year>
Reference-contexts: But this is simply the canonical 2 P -complete decision problem QSAT 2 [70], so game learning is 2 P -complete. Similar problems in game theory have been related to the complexity class 2 P <ref> [58, 71] </ref>. Lemma 2 shows that, using our definition of a competitive algorithm, no competitive algorithm exists that solves every game in time polynomial in lg (jHj), lg (jX j), and k with worst-case strategy learning algorithms.
Reference: [72] <author> I. Parberry. </author> <title> Single-exception sorting networks and the computational complexity of optimal sorting network verification. </title> <journal> Mathematical Systems Theory, </journal> <volume> 23(2) </volume> <pages> 81-93, </pages> <year> 1990. </year>
Reference-contexts: We can use a result of Parberry to obtain a matching lower bound on specification number, for reasonably inclusive classes of candidate sorting networks <ref> [72] </ref>. Parberry has given a general construction for single-exception networks that correctly sort every unsorted input except for one particular sequence. Given an arbitrary n and an unsorted sequence s of zeroes and ones of length n, the construction yields a single-exception network that sorts every sequence but s.
Reference: [73] <author> J. Paredis. </author> <title> Coevolutionary computation. </title> <booktitle> Artificial Life, </booktitle> <pages> 2(355-375), </pages> <year> 1995. </year>
Reference-contexts: We discuss this application further in Section IV.A. Paredis uses a steady-state coevolutionary genetic algorithm, that preserves individuals and accumulates fitness over long time scales <ref> [73] </ref>. This helps give coevolution some memory, similar to the method discussed in Section III.C.3. Paredis uses competitive coevolution to evolve solutions while actively modifying a distribution over a fixed set of test cases. <p> In addition, Paredis discusses "symbiotic" (or "cooperative") coevolution, in which a problem is decomposed into components that may be improved separately in distinct cooperating populations. This can play a variety of roles in decomposable optimization and machine learning tasks <ref> [76, 73] </ref>. The goals of these cooperative systems are quite different from the goals of competitive methods for adversarial problems. I.D Categories of Applications To computationally solve an adversarial problem, we generally must be able to evaluate a candidate solution on any chosen test case.
Reference: [74] <author> M. Plutowki and H. White. </author> <title> Selecting concise training sets from clean data. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4(2) </volume> <pages> 305-318, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: Here, we are more concerned with heuristic methods for large, difficult problems. Some of the general theory, however, may be usefully applied to adversarial problems; methods in Chapter II have some connections to this theory. In machine learning, some of the issues in active learning <ref> [24, 25, 74] </ref> are similar to those here. Active learning tries to select informative examples from which to learn. For example, Cohn [24] makes the assumption that examples are available without labels. Labels may be obtained via some expensive method, so the number of labels requested is to be minimized. <p> No search over examples is done, though; the learner simply watches incoming examples from some distribution, and chooses which to label. In related work, Cohn shows how to choose new examples optimally in a problem-dependent way for certain simple concept representations [25]. Plutowski <ref> [74] </ref> minimizes the number of examples used for learning, but does no search over examples: instead, examples are selected from a fixed set on the basis of a criterion that tells how informative they will be.
Reference: [75] <author> J. Pollack, A. Blair, and M. Land. </author> <title> Coevolution of a backgammon player. </title> <booktitle> In Artificial Life V, </booktitle> <year> 1996. </year>
Reference-contexts: Both controllers and structure of these robots were evolved, and a variety of complex successful strategies emerged. Several researchers have had success applying competitive coevolution to versions of the pursuer-evader differential game [23, 78]. Recently, a very simple form of coevolution has been successfully applied to backgammon <ref> [75] </ref>. Sebald and Schlenzig discuss a different approach to adversarial problems, and its application to controller design [86]. This method uses a nested evolutionary programming procedure. <p> For example, it is not clear that Samuel's 4 Similar results were obtained when this condition was checked explicitly by sampling the outcome of several games between new and old strategies <ref> [75] </ref>. 16 bootstrapping procedure described above will converge rapidly on good strategies, even if new successful Beta strategies are continually found. As another example, we may ask whether competitive coevolution is certain to make long-term progress if search is usually locally successful in defeating current opposition. <p> Such bounds will be most meaningful in the context of complex games if H and X are restricted to strategies that are compactly representable in some particular scheme. For example, strategies might be represented as neural net evaluation functions <ref> [85, 95, 97, 75] </ref>. The framework can also be used to consider the problem of learning to defeat particular classes of simple opponents, rather than arbitrary all-powerful opponents. <p> This is essentially the competitive algorithm used in Samuel's checkers learning system, and is very similar to that used in a recent backgammon learning system <ref> [75] </ref>. The main problem with this competitive algorithm is that intransitivity may exist, and a particular learner may simply keep choosing strategies in a cycle. Intran-sitivity has been observed when using this competitive algorithm for backgammon 23 [75]. <p> and is very similar to that used in a recent backgammon learning system <ref> [75] </ref>. The main problem with this competitive algorithm is that intransitivity may exist, and a particular learner may simply keep choosing strategies in a cycle. Intran-sitivity has been observed when using this competitive algorithm for backgammon 23 [75]. Even a randomized learner may get stuck in such a cycle for a long time. The following example demonstrates this. Example 2 (Small Game Trees) This example considers games represented by game trees.
Reference: [76] <author> M.A. Potter. </author> <title> The Design and Analysis of a Computational Model of Cooperative Coevolution. </title> <type> PhD thesis, </type> <institution> George Mason University, </institution> <year> 1997. </year>
Reference-contexts: In addition, Paredis discusses "symbiotic" (or "cooperative") coevolution, in which a problem is decomposed into components that may be improved separately in distinct cooperating populations. This can play a variety of roles in decomposable optimization and machine learning tasks <ref> [76, 73] </ref>. The goals of these cooperative systems are quite different from the goals of competitive methods for adversarial problems. I.D Categories of Applications To computationally solve an adversarial problem, we generally must be able to evaluate a candidate solution on any chosen test case.
Reference: [77] <author> W. Reitman and B. Wilcox. </author> <title> The structure and performance of the INTERIM.2 Go program. </title> <booktitle> In Proceedings of Sixth International Joint Conference on Artificial Intelligence. </booktitle> <address> IJCAII, </address> <year> 1979. </year>
Reference-contexts: IV.C.3 Go as an AI Problem Although a number of Go programs are in development by various researchers, recent published work on computer Go is fairly rare <ref> [56, 85, 46, 20, 77] </ref>. More information about existing Go programs and issues in computer Go is available from the Internet [11, 1, 2]; refer to these for further information about topics discussed below.
Reference: [78] <author> Craig W. Reynolds. </author> <title> Competition, coevolution, and the game of tag. </title> <editor> In Rodney A. Brooks and Pattie Maes, editors, </editor> <booktitle> Artificial Life IV. </booktitle> <publisher> MIT Press, </publisher> <year> 1994. </year> <title> Used competitive co-evolution successfully to evolve strategies for the game of tag, treated as a differential game on a plane. </title>
Reference-contexts: Both controllers and structure of these robots were evolved, and a variety of complex successful strategies emerged. Several researchers have had success applying competitive coevolution to versions of the pursuer-evader differential game <ref> [23, 78] </ref>. Recently, a very simple form of coevolution has been successfully applied to backgammon [75]. Sebald and Schlenzig discuss a different approach to adversarial problems, and its application to controller design [86]. This method uses a nested evolutionary programming procedure.
Reference: [79] <author> M.L. Rosenzweig. </author> <title> Evolution of the predator isocline. </title> <journal> Evolution, </journal> <volume> 27 </volume> <pages> 84-94, </pages> <year> 1973. </year>
Reference-contexts: This may contribute, for example, to the evolution of reduced virulence in parasites [62]. Finally, there may be a feedback loop that restores balance by increasing selection pressure on the dominated species and reducing selection pressure on the dominating species <ref> [79] </ref>. This difference in evolutionary rates tends to bring the species back into parity. Such a feedback loop may not be able to operate smoothly if the fitness of competitors has a complex, noncontinuous dependence on genotypes.
Reference: [80] <author> C.D. Rosin and R.K. Belew. </author> <title> Methods for competitive co-evolution: Finding opponents worth beating. </title> <editor> In L.J. Eshelman, editor, </editor> <booktitle> Proceedings of the Sixth International Conference on Genetic Algorithms. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: Here, the problem is to discover a strategy that defeats any possible opponent, without testing exhaustively against the large space of possible opponents. Again, by searching for strong opponents, we may achieve our goal with substantially less computational 1 2 effort <ref> [80] </ref>. We may give a general definition of this sort of problem: given f (x; t), where f is an efficiently computable function that determines whether candidate solution x is correct on test case t, find a solution correct on all test cases. <p> A preliminary version of Chapter II was published in the 1996 COLT proceedings [81]. Much of Chapter III will appear in the Evolutionary Computation Journal [82]. Small portions of Chapter III and Section IV.C appeared in an ICGA conference paper <ref> [80] </ref>. Chapter II Theory of Coevolutionary Methods Competitive coevolution searches for solutions and test cases that defeat earlier test cases and solutions. The hope is that this approach can bootstrap its way to robust solutions and difficult test cases. <p> Other factors may enter into the control of drift as well. For example, in an early set of experiments on 2DTTT, random sampling of opponents was found to require fewer generations to solve the game than full round-robin testing <ref> [80] </ref>. This is surprising: random sampling may save work overall, but it yields noisier fitness information that should lead to a larger number of generations required. It appears now that full round-robin testing was less effective because it did not permit drift to as great an extent. <p> Hand 148 designed cellular automata rules have been used in a program written by Allan Scarff, reportedly with some success [30]. We performed preliminary experiments that used a very simple representation of the form described above, with little added computational capability or Go knowledge <ref> [80] </ref>. We do not describe these experiments in detail here, since they used a different form of coevolution from the other applications described in this chapter. Instead, we describe representational requirements that are revealed by limitations observed in these experiments.
Reference: [81] <author> C.D. Rosin and R.K. Belew. </author> <title> A competitive approach to game learning. </title> <booktitle> In Proceedings of the Ninth Annual Conference on Computational Learning Theory, </booktitle> <year> 1996. </year>
Reference-contexts: Chapter V suggests possible implications of these computational experiments for the understanding of biological coevolution, and 10 concludes with a summary and directions for future research. A preliminary version of Chapter II was published in the 1996 COLT proceedings <ref> [81] </ref>. Much of Chapter III will appear in the Evolutionary Computation Journal [82]. Small portions of Chapter III and Section IV.C appeared in an ICGA conference paper [80]. Chapter II Theory of Coevolutionary Methods Competitive coevolution searches for solutions and test cases that defeat earlier test cases and solutions.
Reference: [82] <author> C.D. Rosin and R.K. Belew. </author> <title> New methods for competitive coevolution. </title> <journal> Evolutionary Computation, </journal> <volume> 5(1), </volume> <year> 1997. </year>
Reference-contexts: A preliminary version of Chapter II was published in the 1996 COLT proceedings [81]. Much of Chapter III will appear in the Evolutionary Computation Journal <ref> [82] </ref>. Small portions of Chapter III and Section IV.C appeared in an ICGA conference paper [80]. Chapter II Theory of Coevolutionary Methods Competitive coevolution searches for solutions and test cases that defeat earlier test cases and solutions.
Reference: [83] <author> C.D. Rosin, R.S. Halliday, W.E. Hart, and R.K. Belew. </author> <title> A comparison of global and local search methods in drug docking. </title> <booktitle> In Proceedings of the Seventh International Conference on Genetic Algorithms, </booktitle> <year> 1997. </year>
Reference-contexts: This software calculates several energy terms to evaluate the energy of a candidate docking. It assumes a fixed protein position, and uses a genetic algorithm-local search hybrid to optimize the position and internal rotation angles of a small drug molecule <ref> [83] </ref>. Extensive development of Autodock has produced a system that has been shown to make accurate predictions of both binding strength and geometry in systems like HIV protease [65].
Reference: [84] <author> A. Samuel. </author> <title> Some studies in machine learning using the game of checkers. In E.A. </title> <editor> Feigenbaum and J. Feldman, editors, </editor> <booktitle> Computers and Thought. </booktitle> <publisher> McGraw-Hill, </publisher> <year> 1963. </year> <month> 213 </month>
Reference-contexts: As a concrete example of how this framework might be applied, consider Samuel's original work on learning evaluation functions for checkers from self-play <ref> [84] </ref>. Games were played between a fixed Beta player and a learning Alpha player. Alpha would learn from these games via Samuel's reinforcement learning algorithm; this corresponds to the strategy learning algorithm. When Alpha was finally able to defeat Beta, Beta was replaced by Alpha.
Reference: [85] <author> N. Schraudolph, P. Dayan, and T.J. Sejnowski. </author> <title> Temporal difference learning of position evaluation in the game of go. </title> <booktitle> In Advances in Neural Information Processing Systems 6, </booktitle> <year> 1994. </year>
Reference-contexts: This is a fairly strong assumption; we make it for several reasons. Strategy learning is an optimization problem with a fixed, efficiently-computable cost function. This type of optimization problem has been well studied. Several researchers have had empirical success in strategy learning against fixed opponents <ref> [32, 85, 91] </ref>. The focus of the theory presented here is on the difficulties in game learning that arise even though a strategy learning algorithm exists. <p> Such bounds will be most meaningful in the context of complex games if H and X are restricted to strategies that are compactly representable in some particular scheme. For example, strategies might be represented as neural net evaluation functions <ref> [85, 95, 97, 75] </ref>. The framework can also be used to consider the problem of learning to defeat particular classes of simple opponents, rather than arbitrary all-powerful opponents. <p> IV.C.3 Go as an AI Problem Although a number of Go programs are in development by various researchers, recent published work on computer Go is fairly rare <ref> [56, 85, 46, 20, 77] </ref>. More information about existing Go programs and issues in computer Go is available from the Internet [11, 1, 2]; refer to these for further information about topics discussed below. <p> Such novel approaches have hope of improving on the state of the art sooner than in other games such as chess where conventional methods already work so well. Reinforcement learning has been used to train neural networks to play Go <ref> [85, 31] </ref>. While this method has been very successful in other games such as backgam 145 mon [95], it has had only limited success in Go. Part of the reason for this may be that learning-based Go programs have been simple experimental systems. <p> When we do this, we try to provide only simple, exact features, leaving heuristic judgements to be made by coevolution. Existing learning-based approaches to Go use neural network evaluation functions <ref> [85, 31] </ref>. To play using an evaluation function, all possible moves must be evaluated each turn. This is computationally expensive, so we instead evolve control functions that directly choose a move. Beyond this difference, our representation shares many capabilities with these neural network representations. <p> After coevolution, resulting strategies were tested against Wally, a weak public-domain Go-playing program that has been used by others to test learned Go strategies <ref> [85] </ref>. The best evolved strategies from the 7x7 experiment were able to consistently defeat Wally, and the best evolved strategies from the 9x9 experiment were usually able to defeat Wally. <p> A flag specifies whether the variable should be initialized to zero before doing the addition. These counting, thresholding, and weighting operations give strategies some of the computational capabilities of neural networks <ref> [85, 31] </ref>. For actions which modify string variables, this becomes slightly more complicated. For a particular string, all matches of the condition are checked, where the central point in the condition can be any of the member points in the string.
Reference: [86] <author> A.V. Sebald and J. Schlenzig. </author> <title> Minimax design of neural net controllers for highly uncertain plants. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(1) </volume> <pages> 73-82, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Several researchers have had success applying competitive coevolution to versions of the pursuer-evader differential game [23, 78]. Recently, a very simple form of coevolution has been successfully applied to backgammon [75]. Sebald and Schlenzig discuss a different approach to adversarial problems, and its application to controller design <ref> [86] </ref>. This method uses a nested evolutionary programming procedure. The outer loop searches for controllers, while the inner loop 8 produces a fitness evaluation by searching for the worst test case for each controller. We discuss this application further in Section IV.A. <p> Simulation is used to evaluate the behavior of particular controllers on particular plants. We consider the design of drug-delivery controllers for the control of patient blood pressure during surgery. After presenting prior work on an appropriate model and its use in controller design <ref> [86, 87] </ref>, we use coevolution to study the existence of perfect controllers under the model. A model of blood pressure response to sodium nitroprusside infusion has been developed based on experimental data [87]. <p> It depends on several parameters 113 describing a specific patient, and on an externally supplied level of drug infusion each timestep. The model has been used in the design of CMAC controllers that map blood pressure to drug infusion level in real-time during the course of simulated surgery <ref> [86] </ref>. Given a patient and a candidate CMAC controller, the model can evaluate the performance of the controller on the patient. This evaluation forms the basis for an adversarial problem in which we seek a controller that successfully controls all possible patients. <p> This evaluation forms the basis for an adversarial problem in which we seek a controller that successfully controls all possible patients. Below, we discuss prior work on this model and its use in controller design (see <ref> [86] </ref> for full details). A coevolutionary experiment that closely adheres to this prior work is then described. The form of the model was chosen to allow adequate flexibility in simulating the behavior of real patients [87]. <p> The coefficients and d describe the patient. All parameters are continuous, except for the integer d. Development of controllers for actual use in surgery would require guarantees of robustness across a wide range of surgical scenarios. 1 As a first step, experiments described here focus on one particular scenario <ref> [86] </ref>. The initial condition is X [0] = 100 mmHg, and the simulation is iterated for 700 timesteps. During a simulation, the patient is said to "die" if mean arterial pressure ever drops below 50 mmHg, or if it rises above 120 mmHg after timestep 200. <p> Since we know in advance that we wish to use a large hash table size, there is no reason to include this size as a parameter for these experiments. 115 IV.A.2 Minimax Evolution The blood pressure controller design problem was originally formulated as a minimax optimization problem <ref> [86] </ref>. Total squared error from the target blood pressure of 70 mmHg is summed over all timesteps of the simulation to score a particular controller on a particular patient. If the patient "dies," as described above, a large penalty is assigned to the controller. <p> The minimax goal is to find the controller for which the worst-case patient (specific to that controller) gives the smallest error. Sebald and Schlenzig use a minimax form of evolutionary programming to approach this problem <ref> [86] </ref>. An evolving population searches for controllers. Each controller in this population is evaluated by a separate complete evolutionary search for patients that are difficult for that controller. This inner loop searches for the patient that maximizes the error assigned to the controller. <p> In preliminary runs that used different parameter settings and suffered from balance problems, a controller would arise that could not be defeated by any patients produced by subsequent coevolution. In the manner of <ref> [86] </ref> as described above, a separate test was done to evolve patients that maximize error against one of these successful controllers. This test produced no patients that died under this controller's care; from this test, the controller appeared to be perfect.
Reference: [87] <author> A.V. Sebald, G. Schnurer, M. Parti, N.T. Smith, </author> <title> and M.L. Quinn. A dynamic empirical model of the human response to sodium nitroprusside during cardiac surgery. </title> <booktitle> In Proceedings, 10th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, </booktitle> <volume> volume 2, </volume> <pages> pages 517-518, </pages> <year> 1988. </year>
Reference-contexts: Simulation is used to evaluate the behavior of particular controllers on particular plants. We consider the design of drug-delivery controllers for the control of patient blood pressure during surgery. After presenting prior work on an appropriate model and its use in controller design <ref> [86, 87] </ref>, we use coevolution to study the existence of perfect controllers under the model. A model of blood pressure response to sodium nitroprusside infusion has been developed based on experimental data [87]. <p> After presenting prior work on an appropriate model and its use in controller design [86, 87], we use coevolution to study the existence of perfect controllers under the model. A model of blood pressure response to sodium nitroprusside infusion has been developed based on experimental data <ref> [87] </ref>. It depends on several parameters 113 describing a specific patient, and on an externally supplied level of drug infusion each timestep. The model has been used in the design of CMAC controllers that map blood pressure to drug infusion level in real-time during the course of simulated surgery [86]. <p> A coevolutionary experiment that closely adheres to this prior work is then described. The form of the model was chosen to allow adequate flexibility in simulating the behavior of real patients <ref> [87] </ref>.
Reference: [88] <author> Karl Sims. </author> <title> Evolving 3d morphology and behavior by competition. </title> <editor> In Rodney A. Brooks and Pattie Maes, editors, </editor> <booktitle> Artificial Life IV. </booktitle> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: We have solved this simple game with a different representation and several extensions to competitive coevolution (Chapter III). Sims has had great success coevolving simulated 3-D robots to participate in a simple zero-sum competition in a simulated environment <ref> [88] </ref>. Both controllers and structure of these robots were evolved, and a variety of complex successful strategies emerged. Several researchers have had success applying competitive coevolution to versions of the pursuer-evader differential game [23, 78]. Recently, a very simple form of coevolution has been successfully applied to backgammon [75]. <p> In such close-ended domains, computational approaches only need to produce a finite sequence of improvements, until this ultimate solution is found. More complex problem domains in artificial intelligence, such as board games [95], robotics <ref> [88] </ref>, mathematics [55], and artificial life simulations [3], often do not possess such ultimate, perfect solutions of reasonable complexity. Instead, there exist open-ended sequences of useful solutions of increasing performance and complexity.
Reference: [89] <author> J. Maynard Smith. </author> <title> The Evolution of Sex. </title> <publisher> Cambridge University Press, </publisher> <year> 1978. </year>
Reference-contexts: It is less common in ecosystems with fewer predatory and pathogenic species. It is believed that the coevolutionary pressures in complex environments demand high evolutionary rates of participating species. Species that evolve more slowly due to a lack of sexual recombination are not competitively viable <ref> [89] </ref>. So, it seems that pressure for coevolutionary success can promote change in the basic mechanisms on which evolution depends.
Reference: [90] <author> R.E. Smith, S. Forrest, </author> <title> and A.S. Perelson. Searching for diverse, cooperative populations with genetic algorithms. </title> <journal> Evolutionary Computation, </journal> <volume> 1(2), </volume> <year> 1993. </year>
Reference-contexts: An individual's simple fitness is divided by the sum of its similarities with each other individual in the population, rewarding unusual individuals. Related to this is the "emergent fitness sharing" technique found to promote generalization in an immune system model <ref> [90] </ref>. and the "universal suffrage operator" that encourages diverse coverage of examples in a GA-based concept induction system [39]. Our method is similar to these techniques. <p> When many such niches exist they are necessarily small and subject to extremely low numbers due to sampling errors. Under simple fitness, there is a greater danger of extinction under these conditions than with competitive fitness sharing. Recently, several authors have used a tournament format based on <ref> [90] </ref> with competitive coevolution. This format yields fitness with properties similar to those given by competitive fitness sharing.
Reference: [91] <author> Chuen-Tsai Sun, Ying-Hong Liao, Jing-Yi Lu, and Fu-May Zheng. </author> <title> Genetic algorithm learning in game playing with multiple coaches. </title> <booktitle> In Proceedings of the First IEEE Conference on Evolutionary Computation. IEEE, </booktitle> <year> 1994. </year>
Reference-contexts: In such problems, dynamic programming cannot be usefully applied because it has no temporal structure to work with. We seek a more general approach to adversarial problems. Several authors have discussed general issues surrounding the proper choice of opponents for the learning of robust game strategies <ref> [32, 91, 8] </ref>. The common theme is that diverse strong opposition is needed to learn good strategies. A single expert opposing program will often be unavailable, and even if it is available it may have a simple flaw that can be exploited by an otherwise weak strategy. <p> This is a fairly strong assumption; we make it for several reasons. Strategy learning is an optimization problem with a fixed, efficiently-computable cost function. This type of optimization problem has been well studied. Several researchers have had empirical success in strategy learning against fixed opponents <ref> [32, 85, 91] </ref>. The focus of the theory presented here is on the difficulties in game learning that arise even though a strategy learning algorithm exists. <p> Some of our early experiments in competitive coevolution displayed repeated convergence on specialists that were poor test cases overall, with little arms race progress seen. The use of diverse, strong opponents has been discussed in the context of game-playing <ref> [32, 91] </ref>. Here, we want diverse parasites at every stage, appropriate to the current level of the hosts. The parasites need to be diverse in such a way that they provide appropriate testing for all hosts in the current population.
Reference: [92] <author> P.D. Surry and N.J. Radcliffe. </author> <title> Real representations. </title> <editor> In R.K. Belew and M.D. Vose, editors, </editor> <booktitle> Foundations of Genetic Algorithms 4. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1997. </year>
Reference-contexts: Simply rejecting such offspring results in premature convergence, and a lack of exploration near the boundaries of the constraint region where the best patients are found. To allow a continuous exploration of the space of patients, "ff-box" crossover is used <ref> [92] </ref>. Figure IV.2 shows this form of crossover in two dimensions. The two patients A and B being crossed define the corners of box 1. If offspring points are always chosen within this box, the population rapidly converges.
Reference: [93] <author> W.A. Tackett and A. Carmi. </author> <title> The unique implications of brood selection for genetic programming. </title> <booktitle> In Proceedings of the First IEEE Conference on Evolutionary Computation. IEEE, </booktitle> <year> 1994. </year>
Reference-contexts: The "hall of fame" encourages arms races by saving good individuals from prior generations. The "phantom parasite" is used to maintain balance between competitors in an arms race. Brood selection, a method from genetic programming <ref> [93] </ref>, avoids excessive testing of poor offspring. We 53 provide several different motivations for these methods, and mathematical insights into their use. Experimental comparisons are done, and a detailed analysis of these experiments is done in terms of testing issues, diversity, extinction, arms race progress measurements, and drift. <p> In a stable population, almost all of these offspring will die long before reproducing. So, there is competitive pressure for survival among the members of such a brood. Since many die very young, survival may be determined by a limited test with a significant element of chance. Tackett <ref> [93] </ref> discusses this concept of brood selection in the context of genetic programming. 1 During reproduction, each pair of mating parents produces a relatively large number of offspring. A limited fitness test is applied to these offspring to determine which one makes it into the next population.
Reference: [94] <author> C.J. Tan. </author> <title> Deep Blue: </title> <booktitle> computer chess and massively parallel systems. In Conference Proceedings of the 1995 International Conference on Supercomputing. ACM, </booktitle> <year> 1995. </year>
Reference-contexts: It has proven quite difficult for computers; current programs play at a low intermediate level that does not come close to the mastery with which computers play other games such as chess <ref> [94] </ref>. The typical approach to computer game play involves massive search using a fairly simple evaluation function to compare moves. This approach has not worked well at all in Go.
Reference: [95] <author> G. Tesauro. </author> <title> Temporal difference learning and TD-Gammon. </title> <journal> Communications of the ACM, </journal> <volume> 38(3) </volume> <pages> 58-68, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: In general, larger problems require heuristic methods. Heuristic forms of dynamic programming exist. These are used, for example, to train neural networks via reinforcement learning. This method has been successfully applied to backgammon <ref> [95] </ref> and othello [97]. Dynamic programming may apply when the complexity of the problem lies in its extension in time. <p> In some empirical work on game learning, the competitive algorithm is not explicitly defined. For example, Tesauro's backgammon learning system <ref> [95] </ref> uses reinforcement learning without explicitly checking whether new strategies defeat old ones, as a competitive algorithm would. Given the continual improvement Tesauro observed in his strategies, it is likely that this condition was met implicitly. 4 So, our framework might be used to explain the performance of this system. <p> Such bounds will be most meaningful in the context of complex games if H and X are restricted to strategies that are compactly representable in some particular scheme. For example, strategies might be represented as neural net evaluation functions <ref> [85, 95, 97, 75] </ref>. The framework can also be used to consider the problem of learning to defeat particular classes of simple opponents, rather than arbitrary all-powerful opponents. <p> In such close-ended domains, computational approaches only need to produce a finite sequence of improvements, until this ultimate solution is found. More complex problem domains in artificial intelligence, such as board games <ref> [95] </ref>, robotics [88], mathematics [55], and artificial life simulations [3], often do not possess such ultimate, perfect solutions of reasonable complexity. Instead, there exist open-ended sequences of useful solutions of increasing performance and complexity. <p> Reinforcement learning has been used to train neural networks to play Go [85, 31]. While this method has been very successful in other games such as backgam 145 mon <ref> [95] </ref>, it has had only limited success in Go. Part of the reason for this may be that learning-based Go programs have been simple experimental systems. While appropriate for research, these systems lack the extensive work that has provided knowledge-intensive focused search and other capabilities of conventional Go programs. <p> Each competition explores a single line of play down to the end of the game. Playing to the end of the game removes inaccuracy due to misevaluation of terminal positions; other game-playing techniques such as rollouts in backgammon rely on this to obtain accurate evaluations of positions <ref> [95] </ref>. Entire strategies are evaluated based on a sample of a few such lines of play. The coevolutionary process improves strategies based on information obtained from these lines of play, so that later search focuses on lines that include strong moves and responses.
Reference: [96] <author> J. von Neumann and O. Morgenstern. </author> <title> Theory of Games and Economic Behavior. </title> <publisher> Princeton University Press, </publisher> <year> 1944. </year>
Reference-contexts: Since this is usually infeasible, there is no way to directly apply optimization algorithms. 3 I.B Related Work I.B.1 Related Problems and Methods Classical game theory <ref> [96] </ref>, the theory of differential games [64], and economic game learning theory [37] address similar problems to those described here. The focus in this literature is typically on problems simple enough to be understood analytically. <p> For any zero-sum two-player game, the optimal mixed strategy for a player is the one that achieves the best possible payoff against the worst-case opposing strategy. The minimax theorem <ref> [96] </ref> shows that the worst payoff for the optimal first-player mixed strategy is the same as the worst payoff for the optimal second-player mixed strategy. This payoff gives the value V for the game.
Reference: [97] <author> S. Walker, R. Lister, and T. Downs. </author> <title> Temporal difference, non-determinism, and noise: a case study on the `Othello' board game. </title> <editor> In M. Marinaro and P.G. Morasso, editors, </editor> <booktitle> ICANN '94. Proceedings of the International Conference on Artificial Neural Networks. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1994. </year> <month> 214 </month>
Reference-contexts: In general, larger problems require heuristic methods. Heuristic forms of dynamic programming exist. These are used, for example, to train neural networks via reinforcement learning. This method has been successfully applied to backgammon [95] and othello <ref> [97] </ref>. Dynamic programming may apply when the complexity of the problem lies in its extension in time. In such problems, dynamic programming can begin by building evaluations that are accurate for states arising later in time, and work backwards to obtain accuracy for states arising earlier in time. <p> Such bounds will be most meaningful in the context of complex games if H and X are restricted to strategies that are compactly representable in some particular scheme. For example, strategies might be represented as neural net evaluation functions <ref> [85, 95, 97, 75] </ref>. The framework can also be used to consider the problem of learning to defeat particular classes of simple opponents, rather than arbitrary all-powerful opponents.
Reference: [98] <author> L. Weaver and T. Bossomaier. </author> <title> Evolution of neural networks to play the game of dots-and-boxes. </title> <booktitle> Poster session presented at Artificial Life V, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Recently, several authors have used a tournament format based on [90] with competitive coevolution. This format yields fitness with properties similar to those given by competitive fitness sharing. This has been successfully applied to the Prisoners' Dilemma [28] and the game of Dots-and-Boxes <ref> [98] </ref>. 58 III.C.2 Shared Sampling It is desirable to reduce the computational effort expended in competition by testing each individual in the host population against only a limited sample of parasites from the other population.
Reference: [99] <author> A.S. Wu and R.K. Lindsay. </author> <title> Empirical studies of the genetic algorithm with non-coding segments. </title> <journal> Evolutionary Computation, </journal> <volume> 3(2), </volume> <year> 1995. </year>
Reference-contexts: But rules do not need to be used during computation, so a large number of rules may be present in a strategy without many being used. Unused "junk" rules may affect the evolution of strategies <ref> [99] </ref>, but their presence does not reveal increased computational sophistication. To provide more accurate measures of computational sophistication, we first decide which rules are likely to impact the actual moves made by strategies during play. <p> In the first 100 generations, most rules in these long lists are junk rules that are unused by computation. These junk rules may serve an evolutionary role in protecting other, important rules from destruction by genetic operators <ref> [99] </ref>, but they do not reveal computational sophistication. The number of pruned rules in each population displays a gradual rise over the first 200 generations.
References-found: 99

