URL: http://www.cs.pitt.edu/~gupta/research/Comp/lcpc97.ps
Refering-URL: http://www.cs.pitt.edu/~gupta/research/dm.html
Root-URL: 
Title: An Array Data Flow Analysis based Communication Optimizer  
Author: Xin Yuan, Rajiv Gupta, and Rami Melhem 
Address: Pittsburgh, Pittsburgh, PA 15260  
Affiliation: Dept. of Computer Science, Univ. of  
Abstract: We present an efficient array data flow analysis based global communication optimizer which manages the analysis cost by partitioning the data flow problems into subproblems and solving the subproblems one at a time in a demand driven manner. In comparison to traditional array data flow based techniques, our scheme greatly reduces the memory requirement and manages the analysis time more effectively. The optimizer performs message vectorization, global redundant communication elimination and global communication scheduling. Our experience with the optimizer suggests that array data flow analysis for communication optimization is efficient and effective.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> S. P. Amarasinghe, J. M. Anderson, M. S. Lam and C. W. Tseng, </author> <title> "The SUIF Compiler for Scalable Parallel Machines," </title> <booktitle> Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: Although partitioning of data flow problems requires multiple traversals of the program graph, it does not significantly affect the analysis cost since the set operations dominate the analysis time. Our optimizer is implemented on top of the Stanford SUIF compiler <ref> [1] </ref>. Experiments were conducted to evaluate the performance of the optimizer. To our knowledge, this is the first implementation of a full scale array data flow analysis based communication optimizer. The rest of the paper is organized as follows. Section 2 describes the program representation.
Reference: 2. <author> S. P. Amarasinghe and M. S. </author> <title> Lam "Communication Optimization and Code Generation for Distributed Memory Machine." </title> <booktitle> In Proceedings ACM SIGPLAN'93 Conference on Programming Languages Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Communication optimization is crucial for obtaining good performance for programs compiled to execute on distributed memory systems. Traditionally, data dependence analysis has been used to perform communication optimizations within a single loop nest <ref> [2, 10, 14] </ref>. Recently, data flow analysis techniques have been developed to obtain information for global communication optimizations [4, 7, 9, 12, 13]. One approach, which will be referred to as the array dependence approach, refines data flow analysis for scalar with data dependence analysis [4, 12, 13].
Reference: 3. <author> P. Banerjee, J. A. Chandy, M. Gupta, E. W. Hodges IV, J. G. Holm, A. Lain, D. J. Palermo, S. Ramaswamy, and E. Su. </author> <title> "The PARADIGM Compiler for Distributed-Memory Multicomputers." </title> <journal> in IEEE Computer, </journal> <volume> Vol. 28, No. 10, </volume> <pages> pages 37-47, </pages> <month> Oc-tober </month> <year> 1995. </year>
Reference: 4. <author> S. Chakrabarti, M. Gupta and J. </author> <title> Choi "Global Communication Analysis and Optimization." </title> <booktitle> In Programming Language Design and Implementation(PLDI), </booktitle> <year> 1996, </year> <pages> pages 68-78. </pages>
Reference-contexts: Traditionally, data dependence analysis has been used to perform communication optimizations within a single loop nest [2, 10, 14]. Recently, data flow analysis techniques have been developed to obtain information for global communication optimizations <ref> [4, 7, 9, 12, 13] </ref>. One approach, which will be referred to as the array dependence approach, refines data flow analysis for scalar with data dependence analysis [4, 12, 13]. Another approach, which we will refer to as the array dataflow approach, performs global array data flow analysis [7, 9]. <p> Recently, data flow analysis techniques have been developed to obtain information for global communication optimizations [4, 7, 9, 12, 13]. One approach, which will be referred to as the array dependence approach, refines data flow analysis for scalar with data dependence analysis <ref> [4, 12, 13] </ref>. Another approach, which we will refer to as the array dataflow approach, performs global array data flow analysis [7, 9]. The array dataflow approach can obtain more accurate data flow information at a higher analysis cost than the array dependence approach. <p> In this paper, we present an array data flow analysis based communication optimizer. The optimizer performs message vectorization, redundant communication elimination and global message scheduling <ref> [4] </ref>. To address the analysis cost ? Partially supported by NSF Presidential Young Investigator Award CCR-9157371 problem, the data flow analysis for the optimizations is partitioned into phases. <p> propagation of a DOWN request is similar to that of an UP request except that a DOWN propagation stops at interval boundaries. 4.5 Global Message Scheduling After redundant communication elimination, our optimizer further reduces the number of messages using a global message scheduling algorithm proposed by Chakrabarti et al. in <ref> [4] </ref>. The idea of this optimization is to combine messages that are of the same communication pattern into a single message to reduce number of messages in a program. In order to perform message scheduling, the optimizer first determines the earliest and latest points for each communication. <p> The latest point for a communication is the place of the SCD after redundant communication elimination. Note that after message vectorization, SCDs are placed in the outermost loops that can perform the communications. The earliest point for a SCD can be found by propagating the SCD backward. As in <ref> [4] </ref>, we assume that communication for a SCD is performed at a single point. Hence, the backward propagation will stop after an assignment statement, a loop header or a branch statement where part of the SCD is killed. <p> Since the propagation of SCDs stops at a loop header node, only the UP propagation is needed. Once the earliest and latest points for each communication are known, the greedy algorithm in <ref> [4] </ref> is used to do the communication scheduling. 5 Experimental Results Our optimizer is implemented on top of the Stanford SUIF compiler.
Reference: 5. <author> J.F. Collard, d. Barthou and P. </author> <title> Feautrier "Fuzzy Array Dataflow analysis." </title> <booktitle> In 5th ACM SIGPLAN Symposium on Principle & Practice of Parallel Programming, </booktitle> <month> July </month> <year> 1995, </year> <institution> Santa Barbara, </institution> <address> CA. </address>
Reference: 6. <author> E. Duesterwald, R. Gupta and M. L. </author> <booktitle> Soffa "Demand-driven Computation of Inter-procedural Data Flow" In Symposium on Principles of Programming Languages, </booktitle> <address> Jan. 1995, San Francisco, CA. </address>
Reference: 7. <author> C. Gong, R. Gupta and R. </author> <title> Melhem "Compilation Techniques for Optimizing Communication on Distributed-Memory Systems" In International Conference on Parallel Processing, </title> <booktitle> Vol II, </booktitle> <pages> pages 39-46, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: Traditionally, data dependence analysis has been used to perform communication optimizations within a single loop nest [2, 10, 14]. Recently, data flow analysis techniques have been developed to obtain information for global communication optimizations <ref> [4, 7, 9, 12, 13] </ref>. One approach, which will be referred to as the array dependence approach, refines data flow analysis for scalar with data dependence analysis [4, 12, 13]. Another approach, which we will refer to as the array dataflow approach, performs global array data flow analysis [7, 9]. <p> One approach, which will be referred to as the array dependence approach, refines data flow analysis for scalar with data dependence analysis [4, 12, 13]. Another approach, which we will refer to as the array dataflow approach, performs global array data flow analysis <ref> [7, 9] </ref>. The array dataflow approach can obtain more accurate data flow information at a higher analysis cost than the array dependence approach. The high analysis cost in the array dataflow approach results from the complexity of the data flow descriptor [7, 9] and that operations on the descriptors often result <p> array dataflow approach, performs global array data flow analysis <ref> [7, 9] </ref>. The array dataflow approach can obtain more accurate data flow information at a higher analysis cost than the array dependence approach. The high analysis cost in the array dataflow approach results from the complexity of the data flow descriptor [7, 9] and that operations on the descriptors often result in lists of descriptors. In traditional data flow analysis, to obtain data flow information at one point, data flow solutions at all points must be computed. This requires large memory space and does not allow control over the analysis time.
Reference: 8. <author> M. Gupta and E. </author> <title> Schonberg "A Framework for Exploiting Data Availability to Optimize Communication." </title> <booktitle> In 6th International Workshop on Languages and Compilers for Parallel Computing, </booktitle> <volume> LNCS 768, </volume> <pages> pp 216-233, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: This information is used in the next step for redundant communication elimination. 3. Redundant communication elimination. The optimizer performs redundant communication elimination using demand driven version of availability communication analysis <ref> [8] </ref>, which computes communications that are available before each statement. A communication in a statement is redundant if it can be subsumed by available communications at the statement. Our optimizer also eliminates partially redundant communication. 4. Message scheduling. <p> These rules form a demand driven version of the interval analysis in <ref> [8] </ref>. These rules propagate each communication to its earliest (latest) possible points in backward (forward) propagation, that is, the earliest (latest) points in the program that the communication can be placed (alive). Each optimization performed by the optimizer computes the required data flow information using a subset of these rules. <p> When a DOWN propagation reaches the loop header of the interval in which it originates, its propagation will terminate as opposed to UP propagations where SCDs are expanded when a loop header is reached. The UP and DOWN propagations correspond to the two phases of the interval analysis in <ref> [8] </ref>. Fig. 2. General propagation rules Fig. 2 depicts the rules for propagating SCDs backward. Rules to propagate SCDs forward are similar. In the figure, (S, U P jDOW N ) denotes the UP or DOWN propagation of communication S. <p> Second, communications are propagated forward within each interval to perform availability communication analysis and redundant communication elimination. This is the redundant communication elimination step. Notice that propagating SCDs within their intervals is much more efficient than propagating SCDs through the whole program as in <ref> [8] </ref>. Third, the optimizer performs communication scheduling. In this phase, SCDs are propagated backward to find their earliest points that can be used to place the communications. <p> Actions in forward propagation Using the interval analysis technique <ref> [8] </ref>, two passes are needed to obtain the data flow solutions in an interval. Initially, UP propagations are performed. Once the UP propagations reach interval headers, summaries of the SCDs are calculated and DOWN propagations of the summaries are triggered.
Reference: 9. <author> M. Gupta, E. Schonberg and H. </author> <title> Srinivasan "A Unified Framework for Optimizing Communication in Data-parallel Programs." </title> <journal> In IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> Vol. 7, No. 7, </volume> <pages> pages 689-704, </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: Traditionally, data dependence analysis has been used to perform communication optimizations within a single loop nest [2, 10, 14]. Recently, data flow analysis techniques have been developed to obtain information for global communication optimizations <ref> [4, 7, 9, 12, 13] </ref>. One approach, which will be referred to as the array dependence approach, refines data flow analysis for scalar with data dependence analysis [4, 12, 13]. Another approach, which we will refer to as the array dataflow approach, performs global array data flow analysis [7, 9]. <p> One approach, which will be referred to as the array dependence approach, refines data flow analysis for scalar with data dependence analysis [4, 12, 13]. Another approach, which we will refer to as the array dataflow approach, performs global array data flow analysis <ref> [7, 9] </ref>. The array dataflow approach can obtain more accurate data flow information at a higher analysis cost than the array dependence approach. The high analysis cost in the array dataflow approach results from the complexity of the data flow descriptor [7, 9] and that operations on the descriptors often result <p> array dataflow approach, performs global array data flow analysis <ref> [7, 9] </ref>. The array dataflow approach can obtain more accurate data flow information at a higher analysis cost than the array dependence approach. The high analysis cost in the array dataflow approach results from the complexity of the data flow descriptor [7, 9] and that operations on the descriptors often result in lists of descriptors. In traditional data flow analysis, to obtain data flow information at one point, data flow solutions at all points must be computed. This requires large memory space and does not allow control over the analysis time. <p> This requires large memory space and does not allow control over the analysis time. Thus, only a very simplified version of the array dataflow approach has been previously implemented <ref> [9] </ref> and it was uncertain whether the array dataflow approach is practical for large programs. In this paper, we present an array data flow analysis based communication optimizer. The optimizer performs message vectorization, redundant communication elimination and global message scheduling [4]. <p> Our analysis is based upon a variant of Tarjan's intervals [15]. The analysis requires that there are no critical edges which are edges that connect a node with multiple outgoing edges to a node with multiple incoming edges. The critical edges can be eliminated by edge splitting transformation <ref> [9] </ref>. Fig. 1 shows an example code and its corresponding interval flow graph.
Reference: 10. <author> S. Hiranandani, K. Kennedy and C. </author> <title> Tseng "Compiling Fortran D for MIMD Distributed-memory Machines." </title> <journal> Comm. of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Communication optimization is crucial for obtaining good performance for programs compiled to execute on distributed memory systems. Traditionally, data dependence analysis has been used to perform communication optimizations within a single loop nest <ref> [2, 10, 14] </ref>. Recently, data flow analysis techniques have been developed to obtain information for global communication optimizations [4, 7, 9, 12, 13]. One approach, which will be referred to as the array dependence approach, refines data flow analysis for scalar with data dependence analysis [4, 12, 13].
Reference: 11. <author> High Performance Fortran Forum. </author> <title> "High Performance Fortran Language specification." </title> <type> version 1.0 Technique Report CRPC-TR92225, </type> <institution> Rice University, </institution> <year> 1993. </year>
Reference-contexts: The abstract processor space is similar to a template in High Performance Fortran (HPF) <ref> [11] </ref>, which is a grid over which different arrays are aligned. In the rest of the paper, when we refer to communication, we mean communication on the virtual processor grid. A Section Communication Descriptor (SCD), denoted as &lt; N; D; M; Q &gt;, is composed of three parts.
Reference: 12. <author> K. Kennedy and N. </author> <title> Nedeljkovic "Combining dependence and data-flow analyses to optimize communication." </title> <booktitle> In Proceedings of the 9th International Parallel Processing Symposium, </booktitle> <address> Santa Barbara, CA, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: Traditionally, data dependence analysis has been used to perform communication optimizations within a single loop nest [2, 10, 14]. Recently, data flow analysis techniques have been developed to obtain information for global communication optimizations <ref> [4, 7, 9, 12, 13] </ref>. One approach, which will be referred to as the array dependence approach, refines data flow analysis for scalar with data dependence analysis [4, 12, 13]. Another approach, which we will refer to as the array dataflow approach, performs global array data flow analysis [7, 9]. <p> Recently, data flow analysis techniques have been developed to obtain information for global communication optimizations [4, 7, 9, 12, 13]. One approach, which will be referred to as the array dependence approach, refines data flow analysis for scalar with data dependence analysis <ref> [4, 12, 13] </ref>. Another approach, which we will refer to as the array dataflow approach, performs global array data flow analysis [7, 9]. The array dataflow approach can obtain more accurate data flow information at a higher analysis cost than the array dependence approach.
Reference: 13. <author> K. Kennedy and A. Sethi, </author> <title> "A Constraint Based Communication Placement Framework." Technique Report CRPC-TR95515-S, Center for Research on Parallel Computation, </title> <institution> Rice University. </institution> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: Traditionally, data dependence analysis has been used to perform communication optimizations within a single loop nest [2, 10, 14]. Recently, data flow analysis techniques have been developed to obtain information for global communication optimizations <ref> [4, 7, 9, 12, 13] </ref>. One approach, which will be referred to as the array dependence approach, refines data flow analysis for scalar with data dependence analysis [4, 12, 13]. Another approach, which we will refer to as the array dataflow approach, performs global array data flow analysis [7, 9]. <p> Recently, data flow analysis techniques have been developed to obtain information for global communication optimizations [4, 7, 9, 12, 13]. One approach, which will be referred to as the array dependence approach, refines data flow analysis for scalar with data dependence analysis <ref> [4, 12, 13] </ref>. Another approach, which we will refer to as the array dataflow approach, performs global array data flow analysis [7, 9]. The array dataflow approach can obtain more accurate data flow information at a higher analysis cost than the array dependence approach.
Reference: 14. <author> J. Li and M. Chen, </author> <title> "Compiling communication efficient programs for massively parallel machines." </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 361-376, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Communication optimization is crucial for obtaining good performance for programs compiled to execute on distributed memory systems. Traditionally, data dependence analysis has been used to perform communication optimizations within a single loop nest <ref> [2, 10, 14] </ref>. Recently, data flow analysis techniques have been developed to obtain information for global communication optimizations [4, 7, 9, 12, 13]. One approach, which will be referred to as the array dependence approach, refines data flow analysis for scalar with data dependence analysis [4, 12, 13].
Reference: 15. <author> R.E. </author> <title> Tarjan "Testing flow graph reducibility." </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 9 </volume> <pages> 355-365, </pages> <year> 1974. </year>
Reference-contexts: The optimizer performs optimizations on each subroutine. A subroutine is represented as an interval flow graph G = (N; E), with nodes N and edges E. Our analysis is based upon a variant of Tarjan's intervals <ref> [15] </ref>. The analysis requires that there are no critical edges which are edges that connect a node with multiple outgoing edges to a node with multiple incoming edges. The critical edges can be eliminated by edge splitting transformation [9].
Reference: 16. <author> X. Yuan, R. Gupta and R. </author> <title> Melhem "Demand-driven Data Flow Analysis for Communication Optimization." Workshop on Challenging in Compiling for Scalable Parallel Systems, </title> <address> New Orleans, Louisiana, </address> <month> Oct. </month> <year> 1996. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: Within each phase, the data flow problems are partitioned into subproblems and the subproblems are solved one at a time using a demand driven algorithm <ref> [16] </ref>. Each subproblem contains the data flow effect of a single communication. Our scheme offers many advantages over traditional data flow analysis techniques for communication optimizations. First, since subproblems are solved one at a time, same memory space can be reused repeatedly. Second, the analysis time can be easily managed. <p> An example program and its interval flow graph 3 Section Communication Descriptor (SCD) In this section, we introduce the Section Communication Descriptor (SCD), which is used in the optimizer to represent communications. This descriptor augments the descriptor we designed previously <ref> [16] </ref> in two ways: (1) it has the ability of describing communications that occur specifically in certain iterations of a loop, and (2) it is more powerful in describing communication relations. <p> (&lt; N 1 ; D 1 ; M 1 ; Q 1 &gt;, &lt; N 2 ; D 2 ; M 2 ; Q 2 &gt;), otherwise. 4 The Optimizer The optimizer performs message vectorization, redundant communication elimination and communication scheduling using algorithms based upon the demand driven analysis in <ref> [16] </ref>. The optimization steps include: 1. Initial SCD calculation. Here the optimizer calculates the communication requirement for each statement that contains remote memory references. Communications required by each statement are called initial SCDs for the statement and are placed preceding the statement. 2. Message vectorization and available communication summary calculation. <p> 1 0 0 ; M w = 0 2 1 G l = 1 0 0 ; G r = 1 0 0 &lt; N = w; D = (i; j); M =&lt; (2 fl j; i + 1); (i; j); ?&gt;; Q =?&gt; 4.2 General SCD Propagation Rules In <ref> [16] </ref>, we designed a set of general rules to propagate SCDs in an interval flow graph. These rules form a demand driven version of the interval analysis in [8].
References-found: 16

