URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR92247.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: A GLOBAL CONVERGENCE THEORY FOR GENERAL TRUST-REGION-BASED ALGORITHMS FOR EQUALITY CONSTRAINED OPTIMIZATION  
Author: J. E. DENNIS, JR. MAHMOUD EL-ALEM AND MARIA C. MACIEL 
Keyword: Key Words: Constrained Optimization, Global Convergence, Trust Regions, Equality Constrained, Nonlinear Programming, Conjugate Gradient, Inexact Newton Method.  
Note: AMS subject classifications. 65K05, 49D37.  
Abstract: This work presents a global convergence theory for a broad class of trust-region algorithms for the smooth nonlinear programming problem with equality constraints. The main result generalizes Powell's 1975 result for unconstrained trust-region algorithms. The trial step is characterized by very mild conditions on its normal and tangential components. The normal component need not be computed accurately. The theory requires a quasi-normal component to satisfy a fraction of Cauchy decrease condition on the quadratic model of the linearized constraints. The tangential component then must satisfy a fraction of Cauchy decrease condition on a quadratic model of the Lagrangian function in the translated tangent space of the constraints determined by the quasi-normal component. The Lagrange multipliers estimates and the Hessian estimates are assumed only to be bounded. The other main characteristic of this class of algorithms is that the step is evaluated by using the augmented Lagrangian as a merit function and the penalty parameter is updated using the El-Alem scheme. The properties of the step together with the way that the penalty parameter is chosen are sufficient to establish global convergence. As an example, an algorithm is presented which can be viewed as a generalization of the Steihaug-Toint dogleg algorithm for the unconstrained case. It is based on a quadratic programming algorithm that uses a step in a quasi-normal direction to the tangent space of the constraints and then does feasible conjugate reduced-gradient steps to solve the reduced quadratic program. This algorithm should cope quite well with large problems for which effective preconditioners are known. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. BISCHOF, A. CARLE, G. CORLISS, A. GRIEWANK, and P. HOVLAND. </author> <title> Adifor generating derivative codes from fortran programs. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 11-29, </pages> <year> 1992. </year>
Reference-contexts: Therefore s dog c = ffs i + (1 ff)s i+1 with ff 2 <ref> [0; 1] </ref>: It is easy to see that krC T c s cp and c s i+1 + C c k krC T c + C c k: By convexity, krC T c + C c k krC T c + C c k: Thus, kC c k 2 kC c <p> See Dennis and Lewis [6]. In either case, the uniform boundedness of f k g follows from the problem assumptions. The exact Hessian matrix perhaps can be gotten by using automatic differentiation or an adjoint integration approach. See Bischof et al: <ref> [1] </ref>. However, an approximation to the Hessian of the Lagrangian can be used. Also, for example, setting H k to a fixed matrix (e: g: H k = 0) for all k is valid.
Reference: [2] <author> R. H. BYRD, R. B. SCHNABEL, and G. A. SHULTZ. </author> <title> A trust region algorithm for nonlinearly constrained optimization. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 24 </volume> <pages> 1152-1170, </pages> <year> 1987. </year>
Reference-contexts: The other main characteristic of this class of algorithms is that the step is evaluated for acceptance by using the augmented Lagrangian function with penalty parameter updated by the scheme proposed by El-Alem [9]. Conditions 1, and 3 are satisfied by the algorithms of Byrd, Schnabel, and Shultz <ref> [2] </ref>, Celis, Dennis, and Tapia [4], Byrd and Omojokun [21], and Powell and Yuan [23]. Byrd, Schnabel, and Shultz and Byrd and Omojokun require a normal, rather than just a quasi-normal s n c , in 2. <p> They are the tangent-space approach, and the full-space approach. We describe them briefly in the next section. More details can be found in Maciel [17]. See also Byrd, Schnabel and Shultz <ref> [2] </ref>, Celis, Dennis and Tapia [4], Omojokun [21], Powell and Yuan [23], and Vardi [31] and [32]. 4.1. The tangent-space approach. <p> These algorithms have the trust region capability of dealing quite well with zero or negative curvature in the tangent space of constraints. Thus, nonexistence of an SQP step at the current iterate is readily handled. To choose s n c , Byrd, Schnabel and Shultz <ref> [2] </ref> and Vardi [31],[32] suggest relaxing the linearized constraints by replacing C c by ffC c where ff 2 (0; 1], is chosen to ensure that the above trust-region subproblem is feasible. Thus, s n c = ffrC c (rC T c rC c ) 1 C c . <p> The problem assumptions. We start by stating the assumptions under which global convergence is proved for Algorithm 6.1. Assumptions A1 - A5 (see below) are used by Byrd, Schnabel, and Shultz <ref> [2] </ref>, El-Alem [8], [9], [10] and Powell and Yuan [23] and their particular choices of Lagrange multiplier vectors satisfy A6. Let the sequence of iterates fx k g generated by the algorithm satisfy: A1.
References-found: 2

