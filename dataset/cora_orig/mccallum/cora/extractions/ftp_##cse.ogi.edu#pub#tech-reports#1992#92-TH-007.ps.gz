URL: ftp://cse.ogi.edu/pub/tech-reports/1992/92-TH-007.ps.gz
Refering-URL: http://www.cse.ogi.edu/~wardk/
Root-URL: http://www.cse.ogi.edu
Title: A Speech Act Model of Air Traffic Control Dialogue  
Author: by Karen Ward B. 
Degree: 1978 A thesis presented to the faculty of the Oregon Graduate Institute of Science Technology in partial fulfillment of the requirements for the degree of Master of Science in Computer Science and Engineering  
Date: July 1992  
Affiliation: Sci., University of Oregon,  
Abstract-found: 0
Intro-found: 1
Reference: [Allen 89] <author> James Allen, Stephane Guez, Louis Hoebel, Elizabeth Hinkelman, Keri Jackson, Alice Kyburg, and David Truam, </author> <title> The Discourse System Project, </title> <type> Technical Report 317, </type> <institution> University of Rochester (1989). </institution>
Reference-contexts: Allen has proposed a system architecture designed to integrate models of reference and focus, discourse structure, and speech acts <ref> [Allen 89] </ref>; his Discourse System is designed for text, however, not spoken language. 4.3 Representation 4.3.1 Model of Agent Interaction 46 As belief space blfs about B blf ( ) act blf ( ) blf ( ) Bs belief space blfs about A blf ( ) act utterance blf ( )
Reference: [Allen 91a] <author> James F. Allen, </author> <title> Discourse Structure in the TRAINS Project, </title> <booktitle> Proceedings of the Fourth DARPA Workshop on Speech and Natural Language (Feb. </booktitle> <year> 1991). </year>
Reference-contexts: Austin estimated that there roughly one thousand speech act verbs in English, and he proposed a preliminary taxonomy based on an intuitive classification of related verbs. Searle later proposed a hierarchical taxonomy based on similarities among the speech act properties [Searle 85]. Allen <ref> [Allen 91a] </ref> presents an alternate taxonomy of intention-based speech acts, categorized as understanding, information, or coordination acts. Whether any of these efforts succeed in capturing the expressive richness of the English language is a question perhaps best left to language philosophers.
Reference: [Allen 91b] <author> James Allen, </author> <title> Time and Time Again: The Many Ways to Represent Time, </title> <journal> International Journal of Intelligent Systems, </journal> <volume> Vol. 6, </volume> <pages> pp. </pages> <month> 341-359 </month> <year> (1991). </year>
Reference-contexts: As mentioned earlier, it does not capture notions of time; a more general representation would add a time interval argument to indicate when the agent believed the action would or did occur (see, for example, <ref> [Allen 91b] </ref>). It also does not capture events that take place without being triggered by the actions of some agent. This limitation is acceptable for this representation because its purpose is to model dialogue and not to model general events.
Reference: [Austin 62] <author> J. L. Austin, </author> <title> How to Do Things with Words, </title> <publisher> Oxford University Press, </publisher> <address> London (1962). </address>
Reference-contexts: Several people have suggested heuristics for recognizing verbs that can describe speech acts (e.g., <ref> [Austin 62] </ref>, [Stubbs 83]). Austin estimated that there roughly one thousand speech act verbs in English, and he proposed a preliminary taxonomy based on an intuitive classification of related verbs. Searle later proposed a hierarchical taxonomy based on similarities among the speech act properties [Searle 85].
Reference: [Clark 81] <author> Herbert H. Clark and Catherine R. Marshall, </author> <title> Definite Reference and Mutual Knowledge, pp. 10-63 in Elements of Discourse Understanding, </title> <editor> ed. Ivan A. Sag, </editor> <publisher> Cambridge University Press, </publisher> <month> Cam-bridge </month> <year> (1981). </year>
Reference: [Clark 86] <author> Herbert H. Clark and Deanna Wilkes-Gibbs, </author> <title> Referring as a Collaborative Process, </title> <journal> Cognition, </journal> <volume> Vol. 22, </volume> <pages> pp. </pages> <month> 1-39 </month> <year> (1986). </year>
Reference-contexts: But what is being changed by a conversation? In a series of studies, Clark and his colleagues proposed and developed a theory of conversation as collaborative process in which conversants work together to build up a mutual model of the conversation ([Clark 81], [Schober 89], [Clark 89], <ref> [Clark 86] </ref>). In Clarks view, many of the characteristics of real-world dialogue can be explained in terms of mutuality of knowledge.
Reference: [Clark 89] <author> Herbert H. Clark and Edward F. Schaefer, </author> <title> Contributing to Discourse, </title> <journal> Cognitive Science, </journal> <volume> Vol. 13, </volume> <pages> pp. </pages> <month> 259-294 </month> <year> (1989). </year>
Reference-contexts: But what is being changed by a conversation? In a series of studies, Clark and his colleagues proposed and developed a theory of conversation as collaborative process in which conversants work together to build up a mutual model of the conversation ([Clark 81], [Schober 89], <ref> [Clark 89] </ref>, [Clark 86]). In Clarks view, many of the characteristics of real-world dialogue can be explained in terms of mutuality of knowledge. <p> a criterion sufficient for current purposes (<ref> [Clark 89] </ref>, pg. 163). Clark modeled conversation, then, as a series of contribution-acceptance pairs. After each contribution by one conversant (A), the other conversant (B) accepts the contribution by displaying evidence of understanding. This evidence might consist of one of the following (taken from [Clark 89], pg. 267): 1. Continued attention. By continuing to listen, B indicates that As presenta tion has been understood to Bs satisfaction. 15 2. Initiation of the relevant next contribution. B shows that As contribution has been understood by starting in on the next relevant contribution. 3. Acknowledgment. <p> Clark also suggested a hierarchy of evidence of trouble in understanding (from <ref> [Clark 89] </ref> pg. 268): State 0: B didnt notice that A was attempting to communicate. State 1: B noticed that A was attempting to communicate, but B wasnt in State 2. For instance, B may have heard A say something without catching all the words. <p> The half-duplex modality also affects the structure of dialogue. For instance, it tends to discourage the installment-type exchanges noted by Clark <ref> [Clark 89] </ref>, where complex information say, an address is presented in several parts with the speaker waiting for acknowledgment after each piece of the contribution. In ATC communications, however, we see the opposite pattern: multiple items of information are usually packaged into one transmission. <p> The problem, of course, is that the controller assumes that the instructions are understood and accepted by the pilot unless the pilot objects. This accords well with Clarks observation that we accept silence as evidence of understanding and acceptance <ref> [Clark 89] </ref>. On a crowded radio channel, however, the lack of response could signify that the pilot was unable to gain control of the frequency. The emphasis on verbal communications in ATC is changing, however. <p> Notice that the controller does not reply to the pilots acknowledgment, nor does the pilot expect a reply. This is in accordance both with recommended ATC procedure and with Clarks rules for evidence of understanding <ref> [Clark 89] </ref>. The initial acknowledgment was made in the form of the fairly strong display of understanding. It should be followed by some weaker evidence of understanding, either initiation of the next relevant contribution (in this case, there was none) or with continued attention (silence). <p> The pilots response good-bye is not approved phraseology. It is, however, a relevant next contribution <ref> [Clark 89] </ref>, and as such strongly signals understanding and acceptance of the controllers contribution. 69 5.3.4.3 Representation of Complex Transmissions How should complex transmissions such as utterance 131 be represented? This analysis presents them in terms of multiple speech acts (e.g., utterance 131 in Table 46).
Reference: [Cohen 79] <author> Philip R. Cohen and C. Raymond Perrault, </author> <title> Elements of a Plan-Based Theory of Speech Acts, </title> <journal> Cognitive Science, </journal> <volume> Vol. 3(3), </volume> <pages> pp. </pages> <month> 177-212 </month> <year> (1979). </year> <month> 76 </month>
Reference-contexts: This approach accords well with findings that the structure of discourse about a particular task closely follows the structure of the task itself (for example, [Oviatt 88], <ref> [Cohen 79] </ref>, [Grosz 86]).
Reference: [Cohen 84] <author> Philip R. Cohen, </author> <title> The Pragmatics of Referring and the Modality of Communication, </title> <journal> Computational Linguistics, </journal> <volume> Vol. 10(2), </volume> <pages> pp. </pages> <month> 97-146 (Apr </month> <year> 1984). </year>
Reference-contexts: For example, how do subdialogues for correction or clarification fit into a planning model? What about back-channel responses, the uh-huhs and head nods that punctuate casual dialogue? How is conversational turn-taking coordinated? The plan-based analyses of Cohen <ref> [Cohen 84] </ref> or Litman [Litman 87], for instance, explain these phenomena only with difficulty. 2.2.4 The Collaborative View of Conversation Speech act theory views communicative acts verbal or nonverbal as attempts to bring about change.
Reference: [Cook 90] <author> Guy Cook, </author> <title> Transcribing Infinity: Problems of Context Presentation, </title> <journal> Journal of Pragmatics, </journal> <volume> Vol. 14, </volume> <pages> pp. </pages> <month> 1-24 </month> <year> (1990). </year>
Reference-contexts: For most of these, there is no accepted method of describing or representing the relevant features, nor even a theoretical basis for predicting when a given feature might be relevant <ref> [Cook 90] </ref>.
Reference: [Doddington 85] <author> George R. Doddington, </author> <title> Speaker Recognition - Identifying People by their Voices, </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> Vol. 73(11), </volume> <pages> pp. </pages> <month> 1651-1664 (Nov. </month> <year> 1985). </year>
Reference-contexts: For instance, the differences between the way two different speakers pronounce the same word may be far greater than the difference between two different words uttered by the same speaker, or even by one speaker saying the same word under different conditions <ref> [Doddington 85] </ref>. From a speech recognition standpoint, then, the biggest concern is controlling or compensating for acoustic variability.
Reference: [Doring 83] <author> B. Dring and A. Knauper, </author> <title> A Simulation Study with a Combined Network and Production Systems Model of Pilot Behavior on an ILS-approach, </title> <journal> Automatica, </journal> <volume> Vol. </volume> <pages> 19(6) pp. </pages> <month> 741-747 </month> <year> (1983). </year>
Reference: [Dunham 91] <author> Scott Dunham, </author> <type> personal communication, </type> <month> June 10, </month> <year> 1991. </year>
Reference-contexts: Finally, there is a certain amount of chitchat that takes place on occasion. It is not unheard of for enroute controllers to provide pilots with score updates during the World Series, for example <ref> [Dunham 91] </ref>. Although the volume of such social discourse is still far smaller than in most other domains, it does occur. Despite these exceptions, ATC communications represent an unusually constrained and focused subset of English. <p> To understand such exchanges, it will be necessary to model a utterances effect on all hearers. The effect will not in general be the same for all hearers, of course. A pilot and retired controller comments <ref> [Dunham 91] </ref>: With no visual display of traffic in the cockpit, the only way to keep track of whats going on is via the radio.
Reference: [FAA 89] <author> FAA, </author> <title> Air Traffic Control, </title> <month> (7110.65F) </month> <year> (1989). </year>
Reference-contexts: The information in this section is drawn primarily from three sources: Air Traffic Control <ref> [FAA 89] </ref>, the air traffic controllers primary manual; Airmans Information Manual [FAA 91], the pilots major source of information on air traffic control procedures; A Users View of the Air Traffic Control (ATC) System [Rosenbaum 88], an unusually thorough explanation of the air traffic control system written for the non-pilot. 3.1.1
Reference: [FAA 91] <author> FAA, </author> <note> Airmans Information Manual (1991). </note>
Reference-contexts: The information in this section is drawn primarily from three sources: Air Traffic Control [FAA 89], the air traffic controllers primary manual; Airmans Information Manual <ref> [FAA 91] </ref>, the pilots major source of information on air traffic control procedures; A Users View of the Air Traffic Control (ATC) System [Rosenbaum 88], an unusually thorough explanation of the air traffic control system written for the non-pilot. 3.1.1 Controllers An air traffic controllers primary responsibility is to ensure the <p> For instance, five and nine are notoriously confusable over the radio, so they are pronounced fife and niner ([FAA 89], <ref> [FAA 91] </ref>). 1. English is the international language of aviation and is used at major airports throughout much of the world. 31 The number of words in the core vocabulary is quite small. <p> For example, pilots routinely report their altimeter reading when entering a controllers airspace even though the Mode C transponder also reports the aircraft altitude automatically. This is done because altimeters are frequently imprecise ([FAA 89], <ref> [FAA 91] </ref>) and the aircraft altitude is critical in maintaining aircraft separation. Radar or computer failures can cause erroneous or missing data on the controllers display, making confirmation of critical information even more important [Fischetti 86].
Reference: [Fanty 92] <author> Mark Fanty, Ronald A. Cole, and Krist Roginski, </author> <title> English Alphabet Recognition with Telephone Speech, </title> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <editor> ed. John E. Moody, Steven J. Hanson, and Richard P. Lippmann, </editor> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA (1992). </address>
Reference-contexts: Hall, for instance, found that the grammar of the Local Controllers prescribed vocabulary is built around just four verbs [Hall 88]. The core vocabulary also incorporates a high proportion of letters and numbers, a speech recognition task that has been particularly well-studied (e.g., <ref> [Fanty 92] </ref>). The prescribed vocabulary is somewhat misleading, however; the range of language actually used in pilot-controller communications is much more extensive and complex than one might think from reading the ATC manual (see, for example, the dialogues recorded in [Ward 90b]). This happens for several reasons.
Reference: [Fink 86] <author> Pamela K. Fink and Alan W. Biermann, </author> <title> The Correction of Ill-Formed Input using History-Based Expectation with Applications to Speech Understanding, </title> <journal> Computational Linguistics, </journal> <volume> Vol. </volume> <month> 12(1) (Jan. </month> <year> 1986). </year>
Reference-contexts: One approach has been to develop dynamic grammars, which change as the dialogue progresses. Fink and Biermann made an early attempt to implement a system that could recognize and exploit patterns in the discourse <ref> [Fink 86] </ref>. When a pattern was detected, the system formed expectations about what was likely to be said next. These expectations were used to bias the recognizers grammar toward the expected next sentence. This design was noteworthy in that the system acquired these patterns dynamically by monitoring the discourse.
Reference: [Fischetti 86] <author> Mark A. Fischetti, Tekla S. Perry, Paul Wallich, Glenn Zorpette, John Voelcker, and John Horgan, </author> <title> Our Burdened Skies, </title> <journal> IEEE Spectrum (Nov. </journal> <year> 1986). </year> <month> 77 </month>
Reference-contexts: Congested frequencies are a particular problem around heavily used airports <ref> [Fischetti 86] </ref>. Pilots sometimes have difficulty getting through to the controller in a timely fashion. As the channel becomes saturated with calls, the controller may begin issuing the next instruction without waiting for an acknowledgment of the previous one. <p> This is done because altimeters are frequently imprecise ([FAA 89], [FAA 91]) and the aircraft altitude is critical in maintaining aircraft separation. Radar or computer failures can cause erroneous or missing data on the controllers display, making confirmation of critical information even more important <ref> [Fischetti 86] </ref>. Searle and other writers (e.g. [Grimshaw 80]) have wrestled with the issue of insincerity in speech. Searles definitions of speech acts include sincerity requirements [Searle 69]. For instance, to recognize a request the hearer must believe that the speaker honestly wants the proposed action to occur.
Reference: [Goguen 83] <author> Joseph A. Goguen and Charlotte Linde, </author> <title> Linguistic Methodology for the Analysis of Aviation Accidents, </title> <type> NASA contractor report 3741 (7110.65F), </type> <institution> National Aeronautics and Space Administration (1983). </institution>
Reference: [Goodine 91] <author> David Goodine, Stephanie Seneff, Lynette Hirschman, and Michael Phillips, </author> <title> Full Integration of Speech and Language Understanding in the MIT Spoken Language System, </title> <booktitle> Second European Conference on Speech Communication and Technology (Sept. </booktitle> <year> 1991). </year>
Reference-contexts: The SUMMIT speech recognition system and the TINA language understanding system can be run in several configurations. In the most tightly coupled configuration, TINAs parser is called interactively during the recognizers search phase to prune impossible theories from the search space <ref> [Goodine 91] </ref>. The TINA language understanding system currently includes only a fairly traditional context-free grammar and does not yet incorporate dialogue-level knowledge.
Reference: [Grimshaw 80] <author> Allen D. Grimshaw, Mishearings, Misunderstandings, </author> <title> and Other Nonsuccesses in Talk: A Plea for Redress of Speaker-Oriented Bias, </title> <journal> Sociological Inquiry, </journal> <volume> Vol. 50(3/4), </volume> <pages> pp. </pages> <month> 31-74 </month> <year> (1980). </year>
Reference-contexts: Radar or computer failures can cause erroneous or missing data on the controllers display, making confirmation of critical information even more important [Fischetti 86]. Searle and other writers (e.g. <ref> [Grimshaw 80] </ref>) have wrestled with the issue of insincerity in speech. Searles definitions of speech acts include sincerity requirements [Searle 69]. For instance, to recognize a request the hearer must believe that the speaker honestly wants the proposed action to occur.
Reference: [Grosz 86] <author> Barbara J. Grosz and Candace L. Sidner, </author> <title> Attention, Intentions, and the Structure of Discourse, </title> <booktitle> Computational Linguistics Vol. </booktitle> <pages> 12(3) pp. </pages> <month> 175-204 </month> <year> (1986). </year>
Reference-contexts: This approach accords well with findings that the structure of discourse about a particular task closely follows the structure of the task itself (for example, [Oviatt 88], [Cohen 79], <ref> [Grosz 86] </ref>).
Reference: [Hall 88] <author> Robert Francis Hall, </author> <title> Voice Recognition and Artificial Intelligence in an Air Traffic Control Environment, AFIT/CI/NR 88-171 (1988). </title> <type> Masters Thesis. </type>
Reference-contexts: English is the international language of aviation and is used at major airports throughout much of the world. 31 The number of words in the core vocabulary is quite small. Hall, for instance, found that the grammar of the Local Controllers prescribed vocabulary is built around just four verbs <ref> [Hall 88] </ref>. The core vocabulary also incorporates a high proportion of letters and numbers, a speech recognition task that has been particularly well-studied (e.g., [Fanty 92]). <p> Hall used a PC-based system to build a speaker-dependent speech recognition application for the tower positions: local controller, ground controller, and clearance delivery <ref> [Hall 88] </ref>. His efforts were constrained by the decision to use an off-the-shelf PC-based speech recognition system, however. He developed a static case-based grammar implemented in the proprietary software of the speech recognizer that he was using.
Reference: [Harold 91] <author> Lloyd Harold, </author> <title> FAA Academy, </title> <type> personal communication, </type> <month> Jan. 29, </month> <year> 1991. </year>
Reference-contexts: The computers provide a simulation of the airspace around the tower including such factors as wind, traffic, pilots responses to ATC, etc. and alters the projection accordingly. An instructor at the FAA Academy describes this life-sized voice-controlled video game as being very realistic <ref> [Harold 91] </ref>.
Reference: [Hauptmann 88] <author> Alexander G. Hauptmann, Sheryl R. Young, and Wayne H. Ward, </author> <title> Using Dialog-Level Knowledge Sources to Improve Speech Recognition, </title> <booktitle> AAAI 88, </booktitle> <pages> pp. </pages> <month> 729-733 </month> <year> (1988). </year>
Reference-contexts: Instead, Fink and Biermann applied the expectations to the recognizer output as part of a post-processing error correction strategy. The MINDS project was more successful at implementing a true dynamic grammar ([Young 89b], [Young 89a], <ref> [Hauptmann 88] </ref>). This system tracked the dialogue state and modified the grammar to reect the current dialogue context and focus, user goals, and problem solving strategy. As an utterance was processed, the system constructed a grammar for the speech recognizer to use in interpreting the next utterance.
Reference: [Heppenheim 90] <author> T. A. Heppenheimer, </author> <title> Air Traffic Control, </title> <booktitle> Aviation Week & Space Technology, </booktitle> <pages> pp. </pages> <address> S1-S11 (Dec. 3, </address> <year> 1990). </year>
Reference-contexts: This system provides a data link between the ATC system and the cockpit. Pilots and controllers will be able to exchange routine information such as weather data and altitude confirmations without using the radio <ref> [Heppenheim 90] </ref>. This is expected to reduce the frequency congestion, but at the cost of a reduction in the pilots situational awareness. 3.2.3 Task, Context, and Role Task, context, and the relative roles of the conversants have tremendous effect on the structure and content of dialogue.
Reference: [Hovy 91] <author> Eduard H. Hovy, </author> <title> Pragmatics and Natural Language Generation, </title> <journal> Artificial Intelligence Vol. </journal> <volume> 43, </volume> <pages> pp. </pages> <month> 153-197 </month> <year> (1990). </year> <month> 78 </month>
Reference-contexts: These affect both the words we choose <ref> [Hovy 91] </ref> and the interpretation we place on what we hear [Stubbs 83]. For instance, the primary difference between a suggestion and an order lies in whether the speaker has the authority to order the hearer to 22 perform the action [Searle 69].
Reference: [Jelinek 90] <author> Federick Jelinek, </author> <title> Self-Organized Language Modeling for Speech Recognition, pp. 450-506 in Readings in Speech Recognition, </title> <editor> ed. Alex Waibel and Kai-Fu Lee, </editor> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA (1990). </address>
Reference: [Lee 90] <author> Kai-Fu Lee, Hsiao-Wuen Hon, and Raj Reddy, </author> <title> An Overview of the SPHINX Speech Recognition System, </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> Vol. 38(1), </volume> <pages> pp. </pages> <month> 35-45 (Jan. </month> <year> 1990). </year>
Reference-contexts: Frame-based approaches to parsing and semantic analysis have been more successful in handling unconstrained speech. CMUs Phoenix system uses slot-level grammars built around frame-based semantics to implement a phrase-driven exible parser ([Ward 91], [Young 91]). The output of the SPHINX speech recognition system <ref> [Lee 90] </ref> is passed to a parser which applies grammatical restraints at the phrase level. The phrases fill in slots in semantic frames, which are then analyzed further to construct a database query.
Reference: [Litman 87] <author> Diane J. Litman and James F. Allen, </author> <title> A Plan Recognition Model for Subdialogues in Conversations, </title> <journal> Cognitive Science, </journal> <volume> Vol. 11, </volume> <pages> pp. </pages> <month> 163-200 </month> <year> (1987). </year>
Reference-contexts: For example, how do subdialogues for correction or clarification fit into a planning model? What about back-channel responses, the uh-huhs and head nods that punctuate casual dialogue? How is conversational turn-taking coordinated? The plan-based analyses of Cohen [Cohen 84] or Litman <ref> [Litman 87] </ref>, for instance, explain these phenomena only with difficulty. 2.2.4 The Collaborative View of Conversation Speech act theory views communicative acts verbal or nonverbal as attempts to bring about change.
Reference: [Malcolm 90] <author> Janet Malcolm, </author> <booktitle> The Morality of Journalism, </booktitle> <address> The New York Review, p. </address> <month> 20 (Mar. 1, </month> <year> 1990). </year>
Reference: [Novick 88] <author> David G. Novick, </author> <title> Control of Mixed-Initiative Discourse through Meta-locutionary Acts: A Computational Model (Technical Report No. </title> <institution> CIS-TR-88-18). Department of Computer and Information Science, University of Oregon (1988). </institution>
Reference-contexts: Mutuality of knowledge considerations can motivate and explain the information that conversants exchange. Thus, in this model, conversation is viewed as an attempt to establish and build upon mutual knowledge using speech acts. This synthesis was first proposed by Novick <ref> [Novick 88] </ref> to explain conversational control acts.
Reference: [Novick 90] <author> David G. Novick, </author> <title> Modeling Belief and Action in a Multi-Agent System, pp. 34-41 in AI, Simulation and Planning in High Autonomy Systems, </title> <editor> ed. Bernard Zeigler and Jerzy Rozenblit, </editor> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA (1990). </address>
Reference-contexts: The issue of multiple conversants has been largely ignored in dialogue modelling efforts to date; a notable exception is Novicks saso system, which simulates multi-agent conversations at the speech act level <ref> [Novick 90] </ref>. Novick does not define the belief structures necessary to represent mutuality of belief among multiple agents, however. A suitable representation, then, should meet the following goals: It should support reasoning about the beliefs and intentions of the con versants. <p> These will be tested against a broader sample of ATC dialogue using saso, a rule-based shell for modeling multi-agent interactions <ref> [Novick 90] </ref>. 75
Reference: [Novick 92] <author> David G. Novick, Ron Cole, Benjamin Corliss, </author> <title> The Effect of Context on the Intelligibility of Air Traffic Control Communication, </title> <note> in preparation (1992). </note>
Reference-contexts: Despite the amount of information contained, this transmission is delivered at the same rapid-fire pace as the rest of the dialogue. One would expect this to be difficult or impossible to follow, and in fact non-pilots do find this utterance to be nearly unintelligible <ref> [Novick 92] </ref>. How does the pilot understand it? One explanation is that although the pilot probably cannot anticipate exactly what instructions the controller will give, the pilot can strongly expect to receive the instructions in the order given.
Reference: [Oviatt 88] <author> Sharon L. Oviatt and Philip R. Cohen, </author> <title> Discourse Structure and Performance Efficiency in Interactive and Noninteractive Spoken Modalities, </title> <note> Technical Note 454, SRI International (Nov. 1988). 79 </note>
Reference-contexts: A speaker plans utterances to accomplish certain goals; a hearer interprets those utterances in light of the inferred intentions goals of the speaker. This approach accords well with findings that the structure of discourse about a particular task closely follows the structure of the task itself (for example, <ref> [Oviatt 88] </ref>, [Cohen 79], [Grosz 86]).
Reference: [Perrault 80] <author> C. R. Perrault and J. F. Allen, </author> <title> A Plan-Based Analysis of Indirect Speech Acts, </title> <journal> American Journal of Computational Linguistics, </journal> <volume> Vol. </volume> <pages> 6 (3-4), pp. </pages> <month> 167-182 (July-December </month> <year> 1980). </year>
Reference: [Power 79] <author> Richard Power, </author> <title> The Organization of Purposeful Dialogues, </title> <journal> Linguistics, </journal> <volume> Vol. 17, </volume> <pages> pp. </pages> <month> 107-152 </month> <year> (1979). </year>
Reference-contexts: Language, then, is viewed as just another tool to be used in accomplishing the goal, and utterance planning becomes incorporated into the larger task planning <ref> [Power 79] </ref>. 14 But task planning, although an important part of explaining dialogue structure, does not seem sufficient for explaining certain phenomena observed in actual dialogue.
Reference: [Price 91] <editor> Patti Price (Ed.), </editor> <booktitle> Proceedings of the Fourth DARPA Workshop on Speech and Natural Language (1991). </booktitle>
Reference-contexts: 4 Chapter 2 Dialogue Models for Spoken Language Understanding Systems Spoken language understanding technology is expanding from the highly constrained systems that marked the first successes in the field [Reddy 76] to more ambitious systems designed to function in real-world settings (for example, the systems participating in DARPAs ATIS project <ref> [Price 91] </ref>). The long-term goal, of course, is a system capable of matching human performance in unconstrained, speaker-independent dialogue. For this, acoustic models alone are insufficient. Humans rely on a wealth of non-acoustic language information to aid in interpreting the speech signal: syntax and semantics, prosody and pragmatics. <p> the search space; they are reasonably straightforward to construct when the input is understood well enough so that the word occurrence probabilities can be estimated; because they do not incorporate notions of grammar, semantics or pragmatics, N-gram models have proven robust in handling the ungrammatical constructions that typify unconstrained speech <ref> [Price 91] </ref>. The lack of higher-level language modeling leads to several serious problems, however. First, the recognizer frequently returns an ungrammatical or nonsensical result. <p> This strategy assumes that unparsable utterance fragments represent restarts or repeats and may be ignored, an assumption which may not generalize well. The Phoenix system has performed well on the DARPA ATIS task, however <ref> [Price 91] </ref>, and this approach is clearly a useful one. 7 2.1.2 Integrating Language Models with Speech Recognition Although frame-based language models represent an improvement over simple N-gram word models, the standard architecture still suffers from an inherent limitation: higher-level language knowledge is used only after the fact to correct and
Reference: [Rosenbaum 88] <author> Rosenbaum, S. L. </author> <year> (1988). </year> <title> A Users View of the Air Traffic Control (ATC) System, Internal Memorandum 46321-881130-01.IM, </title> <institution> AT&T Bell Laboratories (Nov. </institution> <year> 1988). </year>
Reference-contexts: The information in this section is drawn primarily from three sources: Air Traffic Control [FAA 89], the air traffic controllers primary manual; Airmans Information Manual [FAA 91], the pilots major source of information on air traffic control procedures; A Users View of the Air Traffic Control (ATC) System <ref> [Rosenbaum 88] </ref>, an unusually thorough explanation of the air traffic control system written for the non-pilot. 3.1.1 Controllers An air traffic controllers primary responsibility is to ensure the separation of aircraft. Each controller is responsible for coordinating traffic within a particular piece of the airspace.
Reference: [Reddy 76] <author> D. Raj Reddy, </author> <title> Speech Recognition by Machine: A Review, </title> <booktitle> IEEE Proceedings Vol. </booktitle> <volume> 64(4), </volume> <pages> pp. </pages> <month> 502-531 (April </month> <year> 1976). </year>
Reference-contexts: Chapter 6 contains a summary and conclusions. 4 Chapter 2 Dialogue Models for Spoken Language Understanding Systems Spoken language understanding technology is expanding from the highly constrained systems that marked the first successes in the field <ref> [Reddy 76] </ref> to more ambitious systems designed to function in real-world settings (for example, the systems participating in DARPAs ATIS project [Price 91]). The long-term goal, of course, is a system capable of matching human performance in unconstrained, speaker-independent dialogue. For this, acoustic models alone are insufficient. <p> This has several implications for choosing a domain for speech understanding research. 19 2.3.1.1 Vocabulary In simplified terms, a speech recognition system works by attempting to find the best match between its acoustic input and its vocabulary <ref> [Reddy 76] </ref>. As the number of alternatives grows, the search space quickly becomes unmanageably large. For speech recognition performance, then, a good domain is one with either a small vocabulary overall, or one with a vocabulary that has few legal alternatives at each point.
Reference: [Saville-Troike 85] <author> Muriel Saville-Troike, </author> <title> The Place of Silence in an Integrated Theory of Communication, pp.3-18 in Perspectives on Silence, </title> <editor> ed. Debo-rah Tannen and Muriel Saville-Troike, </editor> <publisher> Ablex Publishing Co., </publisher> <address> Nor-wood, New Jersey (1985). </address>
Reference-contexts: In the analysis which follows, we will see that what isnt said is often as significant as what is said. We can take this a step further by observing that silence is itself a potent carrier of meaning <ref> [Saville-Troike 85] </ref>. We communicate not only with speech and sound, but with gesture and pause. We observe, then, a many-to-many mapping between the literal form of a com-municatory action and our intuitive notion of what the speaker intended in performing it.
Reference: [Schober 89] <author> Michael F. Schober and Herbert H. Clark, </author> <title> Understanding by Addressees and Overhearers, </title> <journal> Cognitive Psychology Vol. </journal> <volume> 21, </volume> <pages> pp. </pages> <month> 211-232 </month> <year> (1989). </year>
Reference-contexts: But what is being changed by a conversation? In a series of studies, Clark and his colleagues proposed and developed a theory of conversation as collaborative process in which conversants work together to build up a mutual model of the conversation ([Clark 81], <ref> [Schober 89] </ref>, [Clark 89], [Clark 86]). In Clarks view, many of the characteristics of real-world dialogue can be explained in terms of mutuality of knowledge. <p> He spoke up when he heard what sounded like a dangerous situation developing. This accords well with Schober and Clarks findings that it is more difficult for overhearers to understand a conversation <ref> [Schober 89] </ref>. 3.2.5 Safety and Sincerity Safety issues have a strong effect on ATC communications. This is seen particularly clearly in the emphasis on explicit confirmation of information and instructions, and in the relative lack of insincere and non-literal speech.
Reference: [Schwartz 92] <author> Randal L. Schwartz, </author> <type> personal communication, </type> <month> July 2, </month> <year> 1992. </year>
Reference-contexts: They often do not do so. A local pilot reports that Seattle Center (ARTCC) often confirms ATIS information just before handoff <ref> [Schwartz 92] </ref>; if this is common procedure, then Portland Approach may not expect to confirm ATIS with an incoming IFR ight.
Reference: [Schwartz 90] <author> Richard Schwartz and Yen-Lu Chow, </author> <title> The N-Best Algorithm: An Efficient and Exact Procedure for Finding the N Most Likely Sentence Hypotheses, </title> <booktitle> ICASSP 90 (1990). </booktitle>
Reference-contexts: The most common approach has been to modify the rec-ognizer to return several possible word strings instead of a single assignment. This ranked list of the N-best possibilities is then evaluated by a separate language component (see, for example, <ref> [Schwartz 90] </ref> or [Soong 90]). Traditional language grammars fare poorly in spoken language systems. Unconstrained speech is very different from the grammatical prose that we were encouraged to use in English class.
Reference: [Searle 69] <author> John R. Searle, </author> <title> Speech Acts: An Essay in the Philosophy of Language, </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge (1969). </address> <month> 80 </month>
Reference-contexts: It is not enough to transcribe the words that may or may not 10 have been said. For reasoning about communication, we need a representation that captures the speakers intention and the hearers understanding. Speech act theory suggests such a representation ([Austin 62], <ref> [Searle 69] </ref>). Speech act theory treats language as a tool the speaker uses to bring about changes in the world. <p> A spoken language understanding system, then, needs to be able to derive and represent the speech acts that underlie the observed locutionary acts. Searle proposed that speech acts could be recognized and defined by a set of rules <ref> [Searle 69] </ref>. In Searles terminology, propositional content rules indicate the literal 11 The literal meaning of the utterance refers to some future action of the hearer. <p> These affect both the words we choose [Hovy 91] and the interpretation we place on what we hear [Stubbs 83]. For instance, the primary difference between a suggestion and an order lies in whether the speaker has the authority to order the hearer to 22 perform the action <ref> [Searle 69] </ref>. <p> Radar or computer failures can cause erroneous or missing data on the controllers display, making confirmation of critical information even more important [Fischetti 86]. Searle and other writers (e.g. [Grimshaw 80]) have wrestled with the issue of insincerity in speech. Searles definitions of speech acts include sincerity requirements <ref> [Searle 69] </ref>. For instance, to recognize a request the hearer must believe that the speaker honestly wants the proposed action to occur.
Reference: [Searle 75] <author> John R. Searle, </author> <title> Indirect Speech Acts, pp. 59-82 in Syntax and Semantics, Volume 3: Speech Acts, </title> <editor> ed. J. L. </editor> <publisher> Morgan, Academic Press, </publisher> <address> New York (1975). </address>
Reference: [Searle 85] <author> John R. Searle and Daniel Vanderveken, </author> <title> Foundations of Illocution-ary Logic, </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge (1985). </address>
Reference-contexts: A question naturally arises from this: how universal are these 1. In his later work, Searle refined the rule categories somewhat <ref> [Searle 85] </ref>. In particular, his 1985 version includes additional categories designed to capture degree of strength (e.g., the intuitive differ ence between request and beg). For the purposes of this work, the original categories are sufficient. Propositional content Preparatory conditions Sincerity conditions Essential feature Rule Properties of Request Table 1. <p> Austin estimated that there roughly one thousand speech act verbs in English, and he proposed a preliminary taxonomy based on an intuitive classification of related verbs. Searle later proposed a hierarchical taxonomy based on similarities among the speech act properties <ref> [Searle 85] </ref>. Allen [Allen 91a] presents an alternate taxonomy of intention-based speech acts, categorized as understanding, information, or coordination acts. Whether any of these efforts succeed in capturing the expressive richness of the English language is a question perhaps best left to language philosophers.
Reference: [Soong 90] <author> Frank K. Soong and Eng-Fong Huang, </author> <title> A Tree-Trellis Based Fast Search for Finding the N Best Sentence Hypotheses in Continuous Speech Recognition, </title> <booktitle> Proceedings of the 1990 International COnference on Spoken Language Processing (1990). </booktitle>
Reference-contexts: The most common approach has been to modify the rec-ognizer to return several possible word strings instead of a single assignment. This ranked list of the N-best possibilities is then evaluated by a separate language component (see, for example, [Schwartz 90] or <ref> [Soong 90] </ref>). Traditional language grammars fare poorly in spoken language systems. Unconstrained speech is very different from the grammatical prose that we were encouraged to use in English class.
Reference: [Steedman 90] <author> Mark Steedman, </author> <title> Structure and Intonation in Spoken Language Understanding, </title> <type> Technical Report MS-CIS-90-23, </type> <institution> University of Pennsylvania (April 1990). </institution>
Reference-contexts: While traditional linguistic notions of syntax and semantics are not sufficient for determining the intent of an utterance, they are certainly necessary. Furthermore, there are many other pragmatic issues that are not addressed here, such as common-sense knowledge, intonation and pause as modifiers of meaning <ref> [Steedman 90] </ref>, inference generation. Each of these problems are difficult in isolation; to attain human performance in understanding unconstrained dialogue, however, we need to have at least rudimentary methods of handling all of these linguistic phenomena.
Reference: [Stubbs 83] <author> Michael Stubbs, </author> <title> Discourse Analysis, </title> <publisher> Chicago Press, </publisher> <address> Chicago, IL (1983). </address>
Reference-contexts: Several people have suggested heuristics for recognizing verbs that can describe speech acts (e.g., [Austin 62], <ref> [Stubbs 83] </ref>). Austin estimated that there roughly one thousand speech act verbs in English, and he proposed a preliminary taxonomy based on an intuitive classification of related verbs. Searle later proposed a hierarchical taxonomy based on similarities among the speech act properties [Searle 85]. <p> Only in the context of a particular domain can one determine whether a particular shade of meaning is significant. 13 2.2.3 Using Speech Acts to Model Dialogue A dialogue a conversational exchange between two or more persons exhibits structure above the utterance level. As Stubbs points out <ref> [Stubbs 83] </ref>, we can readily distinguish between random sentences and actual dialogue, or grasp a joke that depends on faulty discourse order: A: Yes, I can. B: Can you see into the future? It is this structure that lends a conversation its coherence. <p> These affect both the words we choose [Hovy 91] and the interpretation we place on what we hear <ref> [Stubbs 83] </ref>. For instance, the primary difference between a suggestion and an order lies in whether the speaker has the authority to order the hearer to 22 perform the action [Searle 69]. <p> To minimize these effects, it is common to design experiments involving strangers or to observe people interacting in situations where their roles are formalized or well-defined (for example, Stubbs analysis of classroom dialogue <ref> [Stubbs 83] </ref>). 2.3.3 Summary An ideal domain, then, would have these characteristics: Vocabulary: perplexity low confusability low small core vocabulary few out-of-vocabulary words letters and numbers semantic ambiguity minimized Noise: signal quality high background noise low and consistent few nonverbal noises little overlapping talk Data Collection: short, repetitive, well-defined tasks easy
Reference: [Summers 88] <author> W. Van Summers, David B. Pisoni, Robert H. Bernacki, Robert I. Pedlow, and Michael A. </author> <title> Stokes, Effects of Noise on Speech Production: Acoustic and Perceptual Analyses, </title> <journal> J. Acoust. Soc. Am., </journal> <volume> Vol. 84(3), </volume> <pages> pp. </pages> <month> 917-928 (Sept. </month> <year> 1988). </year>
Reference-contexts: It may be difficult for the recognizer to determine where the noise ends and the speech signal begins, for instance, or to distinguish a door slam from a word. Also, people make distinct prosodic, acoustic, and phonetic changes in their speech when speaking in the presence of noise <ref> [Summers 88] </ref>. Although people instinctively change their speech production to increase their intelligibility, ironically these changes may be great enough to confuse a recognizer, especially if the noise levels and thus the speakers compensating productions vary greatly.
Reference: [Ward 90a] <author> Karen Ward, David Novick, and Carolyn Sousa, </author> <title> Air Traffic Control Communications at Detroit Metro - Wayne County Airport, </title> <type> Technical Report No. CS/E 90-024, </type> <institution> Oregon Graduate Institute (Dec. </institution> <year> 1990). </year>
Reference: [Ward 90b] <author> Karen Ward, David Novick, and Carolyn Sousa, </author> <title> Air Traffic Control Communications at Portland International Airport, </title> <type> Technical Report No. CS/E 90-025, </type> <institution> Oregon Graduate Institute (Dec. </institution> <year> 1990). </year> <month> 81 </month>
Reference-contexts: State 3: B understood what A meant. 2.2.5 Example The exchange reproduced in Figure 1 illustrates several types of evidence of understanding and of trouble in understanding. The exchange is drawn from a longer dialogue found in <ref> [Ward 90b] </ref>. As the conversation begins, the pilot (Ford 645) has attempted to contact the controller (Approach) for permission to make a sight-seeing ight over downtown Port-land. <p> The prescribed vocabulary is somewhat misleading, however; the range of language actually used in pilot-controller communications is much more extensive and complex than one might think from reading the ATC manual (see, for example, the dialogues recorded in <ref> [Ward 90b] </ref>). This happens for several reasons. First, although controllers are rigorously trained in the standard phraseology, pilots especially general aviation pilots are not. We see both a greater variation in their phraseology and a greater degree of dis-uency in their speech. <p> Also, standard phrases are augmented as necessary to accomplish the task at hand, resulting in a more English-like vocabulary when unusual situations arise. This example is taken from a conversation that took place at Detroit Metro Wayne County Airport <ref> [Ward 90b] </ref>. The control 32 ler is asking the pilot of Northwest 752 to allow a smaller commuter ight to take off before him. <p> A third type of variation from the standard phraseology is slang, as illustrated by this controllers use of three holer to indicate a 727 (from <ref> [Ward 90b] </ref>): (239) Local Control: Continental two oh one, follow the Northwest seven twenty seven approaching from your left, hell be the second three holer. Finally, there is a certain amount of chitchat that takes place on occasion. <p> These conversations are not entirely separate, though; pilots routinely monitor the frequency in order to build up a situational awareness of the activities around them. Thus they may, on occasion, respond or refer to dialogue that was not directed to them, as in this exchange from <ref> [Ward 90b] </ref>: (66) Ground Control: Northwest two oh nine, uh, you still got your radar on? (67) NW 209: Thats a negative, Northwest two zero nine. (68) Ground Control: OK. (.) (69) NW 255: Two fifty five does. <p> The protocols used in this study were made from a recording taped from the radio at a site approximately two nautical miles from the airport on July 1, 1990, between 10:45 and 11:15 AM <ref> [Ward 90b] </ref>. From these a suitable dialogue was selected for explication. From this sample dialogue I developed a model sufficient to represent and motivate the exchanges that were observed. <p> Expectations for both pilot and controller are heavily dependent on time and on the conversants mental models of how long something is likely to take or of when something should occur. For instance, consider this exchange <ref> [Ward 90b] </ref>: (57) Horizon 64: Horizon sixty four cleared for the approach? (58) Approach: Horizon sixty four youre: one zero miles from Laker, maintain four thousand five hundred till established on the localizer cleared ILS two eight right approach, maintain one seven zero knots until Laker. <p> It takes time to for a controller to transmit and confirm instructions to a pilot and more time for the pilot to implement the requested action, and during that time the situation may change, as in this example <ref> [Ward 90b] </ref>: (33) Approach: Alaska two oh five, nine from Laker, maintain three thousand till established on the localizer cleared the ILS two eight right approach, maintain speed (of) one eight zero until Laker. (34) Alaska 205: Roger Alaska two oh five, uh, cleared approach three thousand till established on the <p> This chapter discusses the analysis at a more general level, focussing on the characteristic patterns seen in the dialogue and on the representation issues that arose during the analysis. 5.2 The Dialogue The dialogue used in this example was selected from a larger corpus of air traffic control dialogue <ref> [Ward 90b] </ref>; the dialogue is shown in its entirety in Figure 3. It represents a complete conversation between an approach controller at Portland International Airport and the pilot of a commercial ight approaching the airport to land. <p> Utterance numbers are from the original transcript <ref> [Ward 90b] </ref>. See Appendix C for context. 56 The pilot establishes communications with the approach controller. In this dialogue, this takes place in utterances 108 and 109. The controller directs the aircraft to the approach gate by issuing a series of vectors, seen here in utterances 110 through 128. <p> The controller hands off the ight to the tower controller (utterances 157 and 158). The ILS approach task was selected because it represents a common task and a common form of interaction between pilot and controller. The 30 minutes of dialogue transcribed in <ref> [Ward 90b] </ref> include seven complete examples of the ILS approach task and portions of five more. Thus there are several samples available for analysis, and more can be readily obtained.
Reference: [Ward 91] <author> Wayne Ward, </author> <title> Current Status of the CMU ATIS System, </title> <booktitle> in Fourth DARPA Workshop on Speech and Natural Language, </booktitle> <editor> ed. Patti Price, </editor> <year> (1991). </year>
Reference-contexts: For example, the utterance Show me I want to see all ights to Denver after two pm would be initially parsed into a frame like this: [list]: I want to see [ights]: all ights [arrive_loc]: to Denver [depart_time_range]: after two pm (example taken from <ref> [Ward 91] </ref>). This strategy assumes that unparsable utterance fragments represent restarts or repeats and may be ignored, an assumption which may not generalize well. <p> This layered grammar technique was designed to 8 permit the system to respond efficiently when the next utterance matched expectations while still exhibiting graceful degradation when an utterance was unexpected. Although intuitively appealing, this approach apparently was unacceptably inefficient; in their more recent work (e.g., <ref> [Ward 91] </ref>,[Young 91]), Ward and Young have abandoned dynamic grammars in favor of a more traditional post-processing approach. A more successful effort to integrate higher-level language models into the speech recognition process comes from MIT.
Reference: [Wesson 77] <author> Robert B. Wesson, </author> <title> Problem-Solving with Simulation in the World of an Air Traffic Controller, </title> <institution> University of Texas at Austin (1977). Dissertation. </institution>
Reference-contexts: Thus, his system did not attempt to build up an understanding of the context, nor did it have any way of capturing the pilots contributions to the conversation. Wesson modelled problem-solving for enroute controllers and achieved performance rivalling that of trained controllers for the given domain <ref> [Wesson 77] </ref>. His system successfully addressed the problem of modelling change and planning in a dynamic system. 38 He did not address communications, though, and recognized the lack of speech recognition as a barrier to his system (he tested his system with hand-entered data).
Reference: [Young 89a] <author> Sheryl R. Young, Alexander G. Hauptmann, Wayne H. Ward, Edward T. Smith, and Philip Werner, </author> <title> High Level Knowledge Sources in Usable Speech Recognition Systems, </title> <journal> Communications of the ACM Vol. </journal> <month> 32(2) </month> <year> (1989). </year>
Reference-contexts: Instead, Fink and Biermann applied the expectations to the recognizer output as part of a post-processing error correction strategy. The MINDS project was more successful at implementing a true dynamic grammar ([Young 89b], <ref> [Young 89a] </ref>, [Hauptmann 88]). This system tracked the dialogue state and modified the grammar to reect the current dialogue context and focus, user goals, and problem solving strategy. As an utterance was processed, the system constructed a grammar for the speech recognizer to use in interpreting the next utterance.
Reference: [Young 89b] <author> Sheryl R. Young, Wayne H. Ward, and Alexander G. Hauptmann, </author> <title> Layering Predictions: Flexible Use of Dialog Expectation in Speech Recognition, </title> <booktitle> IJCAI 89, </booktitle> <year> (1989). </year>
Reference: [Young 91] <author> Sheryl Young, </author> <title> Using Semantics to Correct Parser Output for ATIS Utterances, </title> <booktitle> in Fourth DARPA Workshop on Speech and Natural Language, </booktitle> <editor> ed. </editor> <booktitle> Patti Price (1991). </booktitle> <pages> 82 </pages>
Reference-contexts: Frame-based approaches to parsing and semantic analysis have been more successful in handling unconstrained speech. CMUs Phoenix system uses slot-level grammars built around frame-based semantics to implement a phrase-driven exible parser ([Ward 91], <ref> [Young 91] </ref>). The output of the SPHINX speech recognition system [Lee 90] is passed to a parser which applies grammatical restraints at the phrase level. The phrases fill in slots in semantic frames, which are then analyzed further to construct a database query.
References-found: 58

