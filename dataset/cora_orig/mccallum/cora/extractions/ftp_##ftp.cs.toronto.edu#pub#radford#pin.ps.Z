URL: ftp://ftp.cs.toronto.edu/pub/radford/pin.ps.Z
Refering-URL: http://www.cs.toronto.edu/~radford/pin.abstract.html
Root-URL: 
Title: Priors for Infinite Networks  
Author: Radford M. Neal 
Note: E-mail: radford@cs.toronto.edu  
Abstract: Technical Report CRG-TR-94-1 Department of Computer Science University of Toronto 10 King's College Road Toronto, Canada M5S 1A4 Abstract Bayesian inference begins with a prior distribution for model parameters that is meant to capture prior beliefs about the relationship being modeled. For multilayer perceptron networks, where the parameters are the connection weights, the prior lacks any direct meaning | what matters is the prior over functions computed by the network that is implied by this prior over weights. In this paper, I show that priors over weights can be defined in such a way that the corresponding priors over functions reach reasonable limits as the number of hidden units in the network goes to infinity. When using such priors, there is thus no need to limit the size of the network in order to avoid "overfitting". The infinite network limit also provides insight into the properties of different priors. A Gaussian prior for hidden-to-output weights results in a Gaussian process prior for functions, which can be smooth, Brownian, or fractional Brownian, depending on the hidden unit activation function and the prior for input-to-hidden weights. Quite different effects can be obtained using priors based on non-Gaussian stable distributions. In networks with more than one hidden layer, a combination of Gaussian and non-Gaussian priors appears most interesting. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Peitgen, H.-O. and Saupe, D., </author> <title> editors (1988) The Science of Fractal Images, </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Distributions over functions of this sort, in which the joint distribution of the values of the function at any finite number of points is multivariate Gaussian, are known as Gaussian processes; they arise in many contexts, including spatial statistics (Ripley 1981), computer vision (Szeliski 1989), and computer graphics <ref> (Peitgen and Saupe 1988) </ref>. The prior covariances between the values of output k for different values of the inputs are in general not zero, which is what allows learning to occur. <p> Functions with intermediate properties are obtained when 1 &lt; &lt; 2; functions "rougher" than Brownian motion are obtained when 0 &lt; &lt; 1. One way to achieve these effects would be to change the hidden unit activation function from tanh (z) to sign (z)jzj (1)=2 <ref> (Peitgen and Saupe 1988, Sections 1.4.1 and 1.6.11) </ref>. However, the unbounded derivatives of this activation function would pose problems for gradient-based learning methods.
Reference: <author> Buntine, W. L. and Weigend, A. S. </author> <title> (1991) "Bayesian back-propagation", </title> <journal> Complex Systems, </journal> <volume> vol. 5, </volume> <pages> pp. 603-643. </pages>
Reference: <author> Falconer, K. </author> <title> (1990) Fractal Geometry: </title> <booktitle> Mathematical Foundations and Applications, </booktitle> <address> Chichester: </address> <publisher> John Wiley. </publisher>
Reference-contexts: , they have a Brownian nature characterized by D (xs=2; x+s=2) being proportional to s. 2.4 Fractional Brownian priors It is natural to wonder whether a prior on the weights and biases going into hidden units can be found for which the resulting prior over functions has fractional Brownian properties <ref> (Falconer 1990, Section 16.2) </ref>, characterized by D (x (p) ; x (q) ) ~ jx (p) x (q) j (12) As above, values of = 2 and = 1 correspond to smooth and Brownian functions.
Reference: <author> Feller, W. </author> <title> (1966) An Introduction to Probability Theory and its Applications, </title> <booktitle> Volume II, </booktitle> <address> New York: </address> <publisher> John Wiley. </publisher>
Reference-contexts: Such priors can indeed be constructed, using prior distributions for the weights from hidden to output units that do not have finite variance. 13 3.1 Limits for priors with infinite variance The theory of stable distributions <ref> (Feller, 1966, Section VI.1) </ref> provides the basis for analysing the convergence of priors in which hidden-to-output weights have infinite variance. <p> Distributions with tails that (roughly speaking) have densities that decline as z (ff+1) , with 0 &lt; ff &lt; 2 are in the normal domain of attraction of the symmetric stable distributions of index ff <ref> (Feller, 1966, Sections IX.8 and XVII.5) </ref>. We can define a prior on network weights in such as fashion that the resulting prior on the value of a network output for a particular input converges to a non-Gaussian symmetric stable distribution as the number of hidden units, H, goes to infinity.
Reference: <author> Funahashi, K. </author> <title> (1989) "On the approximate realization of continuous mappings by neural networks", </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 2, </volume> <pages> pp. 183-192. </pages>
Reference-contexts: Hence our actual prior belief will usually be that the model is simply wrong. I propose to address these problems by focusing on the limit as the number of hidden units in the network approaches infinity. Several workers <ref> (e.g. Funahashi 1989) </ref> have shown that in this limit a network with one layer of hidden units can approximate any continuous function defined on a compact domain arbitrarily closely. An infinite network will thus be a reasonable "non-parametric" model for many problems.
Reference: <author> MacKay, D. J. C. </author> <title> (1991) Bayesian Methods for Adaptive Models, </title> <type> Ph.D thesis, </type> <institution> California Institute of Technology. </institution>
Reference-contexts: We may then wish to treat these values as unknown hyperparameters, giving them higher-level prior distributions that are rather broad. One benefit of such a hierarchical model is that the degree of "regularization" that is appropriate for the task can be determined automatically from the data <ref> (MacKay 1991, 1992) </ref>.
Reference: <author> MacKay, D. J. C. </author> <title> (1992) "A practical Bayesian framework for backpropagation networks", </title> <journal> Neural Computation, </journal> <volume> vol. 4, </volume> <pages> pp. 448-472. </pages>
Reference: <author> Neal, R. M. </author> <title> (1992) "Bayesian training of backpropagation networks by the hybrid Monte Carlo method", </title> <type> Technical Report CRG-TR-92-1, </type> <institution> Dept. of Computer Science, University of Toronto. </institution>
Reference-contexts: Achieving this is not trivial. Methods based on making a Gaussian approximation to the posterior (MacKay 1991, 1992; Buntine and Weigend 1991) may break down as the number of hidden units becomes large. Markov chain Monte Carlo methods <ref> (Neal 1992, 1993) </ref> produce the correct answer eventually, but may sometimes fail to reach the true posterior distribution in a reasonable length of time.
Reference: <author> Neal, R. M. </author> <title> (1993) "Bayesian learning via stochastic dynamics", </title> <editor> in C. L. Giles, S. </editor> <publisher> J. </publisher>
Reference: <editor> Hanson, and J. D. Cowan (editors), </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pp. 475-482, </pages> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Ripley, B. D. </author> <title> (1981) Spatial Statistics, </title> <address> New York: </address> <publisher> John Wiley. </publisher>
Reference-contexts: Distributions over functions of this sort, in which the joint distribution of the values of the function at any finite number of points is multivariate Gaussian, are known as Gaussian processes; they arise in many contexts, including spatial statistics <ref> (Ripley 1981) </ref>, computer vision (Szeliski 1989), and computer graphics (Peitgen and Saupe 1988). The prior covariances between the values of output k for different values of the inputs are in general not zero, which is what allows learning to occur.
Reference: <author> Szeliski, R. </author> <title> (1989) Bayesian Modeling of Uncertainty in Low-level Vision, </title> <address> Boston: </address> <publisher> Kluwer. </publisher> <pages> 22 </pages>
Reference-contexts: Distributions over functions of this sort, in which the joint distribution of the values of the function at any finite number of points is multivariate Gaussian, are known as Gaussian processes; they arise in many contexts, including spatial statistics (Ripley 1981), computer vision <ref> (Szeliski 1989) </ref>, and computer graphics (Peitgen and Saupe 1988). The prior covariances between the values of output k for different values of the inputs are in general not zero, which is what allows learning to occur.
References-found: 12

