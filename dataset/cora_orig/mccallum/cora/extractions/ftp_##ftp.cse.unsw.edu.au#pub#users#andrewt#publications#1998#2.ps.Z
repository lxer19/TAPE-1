URL: ftp://ftp.cse.unsw.edu.au/pub/users/andrewt/publications/1998/2.ps.Z
Refering-URL: http://www.cse.unsw.edu.au/school/publications/1998/SCSE_publications.html
Root-URL: 
Email: bbriedis@cse.unsw.edu.au tom@cse.unsw.edu.au  
Title: Using the Grow-And-Prune Network to Solve Problems of Large Dimensionality  
Author: B.J. Briedis and T.D. Gedeon 
Address: Sydney NSW 2052 AUSTRALIA  
Affiliation: School of Computer Science Engineering The University of New South Wales  
Abstract: This paper investigates a technique for creating sparsely connected feed-forward neural networks which may be capable of producing networks that have very large input and output layers. The architecture appears to be particularly suited to tasks that involve sparse training data as it is able to take advantage of the sparseness to further reduce training time. Some initial results are presented based on tests on the 16 bit compression problem.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Fiesler. </author> <title> Comparative bibliography of onto-genic neural networks. </title> <booktitle> In Proceedings of the International Conference on Artificial Neural Networks (ICANN 94), </booktitle> <pages> pages 793-796, </pages> <year> 1994. </year>
Reference-contexts: There has been little research into training networks which are sparse throughout the course of their training. A classification of networks whose structures do adapt during the course of their training is to be found in a paper by Fiesler <ref> [1] </ref>. Few of these networks, however, appear to be designed in order to allow supervised learning to be applied to very high-dimensional problems. One interesting case of a sparse neural network being applied to a high-dimensional problem is the area of phoneme probability estimation [8].
Reference: [2] <author> D. Harris and T. D. Gedeon. </author> <title> Adaptive insertion of units in feed-forward networks. </title> <booktitle> In Proceedings 4th International Conference on Neural Networks and their Applications, </booktitle> <year> 1991. </year>
Reference-contexts: This reduces the likelihood of training becoming trapped in local minima as the error surface is frequently changed. It often improves generalisation and can also reduce training time <ref> [2, 10] </ref>. The GAP method combines the techniques of growing and pruning to form sparse networks. 3. The GAP network The GAP network starts with a small number of fully connected units which are trained using some reasonable training method (e.g. back-propagation or RPROP [7]).
Reference: [3] <author> D. Harris and T. D. Gedeon. </author> <title> Network reduction techniques. </title> <booktitle> In Proceedings International Conference Neural Networks Methodologies and Applications, </booktitle> <volume> volume 1, </volume> <pages> pages 119-126, </pages> <year> 1991. </year>
Reference-contexts: Instead of subdividing large problems they seek to store only the most significant information. Sparse networks are often created by training a fully connected network and then pruning some of its connections. This is commonly done either to improve network generalisation or to extract rules <ref> [3, 6] </ref>. There are two problems with this approach. Firstly, training a large fully connected network is a lengthy process. Secondly, there is no guarantee that the sparse network that results is close to having an optimal structure.
Reference: [4] <author> E. Karnin. </author> <title> A simple procedure for pruning back-propagation trained neural networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1(2) </volume> <pages> 239-242, </pages> <year> 1990. </year>
Reference-contexts: The 16 bit compression problem is a useful benchmark problem for evaluating GAP architectures, as it is quick to run and the results are easy to interpret. 6. Further Work Pruning methods more sophisticated than magnitude-based pruning, such as optimal brain damage [5] and Karnin's method <ref> [4] </ref>, might yield significant improvements in GAP networks, as they are subject to continuous heavy pruning. The adoption of a cascade of neurons could also be worthwhile [10]. A further possibility is to reintroduce a few deleted connections throughout training, perhaps on a random basis.
Reference: [5] <author> Y. Le Cun, J. Denker, and S. Solla. </author> <title> Optimal brain damage. </title> <editor> In D. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 598-605. </pages> <publisher> Morgan Kauffman, </publisher> <year> 1990. </year>
Reference-contexts: The 16 bit compression problem is a useful benchmark problem for evaluating GAP architectures, as it is quick to run and the results are easy to interpret. 6. Further Work Pruning methods more sophisticated than magnitude-based pruning, such as optimal brain damage <ref> [5] </ref> and Karnin's method [4], might yield significant improvements in GAP networks, as they are subject to continuous heavy pruning. The adoption of a cascade of neurons could also be worthwhile [10]. A further possibility is to reintroduce a few deleted connections throughout training, perhaps on a random basis.
Reference: [6] <author> R. Reed. </author> <title> Pruning algorithms, a survey. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4(5) </volume> <pages> 740-747, </pages> <year> 1993. </year>
Reference-contexts: Instead of subdividing large problems they seek to store only the most significant information. Sparse networks are often created by training a fully connected network and then pruning some of its connections. This is commonly done either to improve network generalisation or to extract rules <ref> [3, 6] </ref>. There are two problems with this approach. Firstly, training a large fully connected network is a lengthy process. Secondly, there is no guarantee that the sparse network that results is close to having an optimal structure.
Reference: [7] <author> M. Riedmiller and H. Braun. </author> <title> A direct adaptive method for faster backpropagation learning: The RPROP algorithm. </title> <booktitle> In Proceedings of the IEEE International Conference on Neural Networks (ICNN), </booktitle> <pages> pages 586-591, </pages> <year> 1993. </year>
Reference-contexts: The GAP method combines the techniques of growing and pruning to form sparse networks. 3. The GAP network The GAP network starts with a small number of fully connected units which are trained using some reasonable training method (e.g. back-propagation or RPROP <ref> [7] </ref>). Once training has levelled out, a number of network connections are removed simultaneously using a pruning algorithm. The pruning reduces the number of interconnections to a floor (call it n) that stays fixed for the duration of the entire training. <p> Several connections were removed at the same time with no training being done between deletions. No neurons were removed during training. The RPROP training method was selected as it generally performs better than back propagation and it lacks the learning rate parameter <ref> [7] </ref>. The weights and update values (used in RPROP) were not changed when training was recommenced after an adjustment of the network structure. The activation curve used was that shown in Equation 1. No noise was employed, nor was thresholding.
Reference: [8] <author> N. Strom. </author> <title> Phoneme probability estimation with dynamic sparsely connected artificial neural networks. </title> <journal> The Free Speech Journal, </journal> <volume> 1(5), </volume> <year> 1997. </year>
Reference-contexts: Unfortunately networks of this sort are severely restricted in the amount of information they may contain. Better use may be made of the limited number of connections by allowing nodes to instead be sparsely connected (for example see <ref> [8] </ref>). A large number of nodes may be present in a network, with the average fan-in and fan-out of the units being small. <p> Few of these networks, however, appear to be designed in order to allow supervised learning to be applied to very high-dimensional problems. One interesting case of a sparse neural network being applied to a high-dimensional problem is the area of phoneme probability estimation <ref> [8] </ref>. In this paper the author found a randomly connected network performed better than a fully connected with a larger number of connections. One technique which is often used to improve network performance is to add hidden nodes during the training of a neural network.
Reference: [9] <author> J. P. Sutton. </author> <title> Neurobiological and computational aspects of modularity. </title> <booktitle> In Proceedings of the Ninth Australian Conference on Neural Networks (ACNN'98), </booktitle> <month> February </month> <year> 1998. </year>
Reference-contexts: In addition hidden units may be grouped in various ways so as to reduce the number of interconnections. One interesting suggestion is to use a fractal structure as a network architecture <ref> [9] </ref>. It is in general not obvious how to best subdivide a high dimensional problem. This is a concern as inappropriately dividing of a problem can adversely affect the performance of a system.
Reference: [10] <author> N. Treadgold and T. D. Gedeon. </author> <title> A cascade network algorithm employing progressive RPROP. In Biological and Artificial Computation: From Neuroscience to Technology. </title> <booktitle> International Work-Conference on Artificial and Natural Neural Networks (IWANN'97), </booktitle> <pages> pages 733-742, </pages> <year> 1997. </year>
Reference-contexts: This reduces the likelihood of training becoming trapped in local minima as the error surface is frequently changed. It often improves generalisation and can also reduce training time <ref> [2, 10] </ref>. The GAP method combines the techniques of growing and pruning to form sparse networks. 3. The GAP network The GAP network starts with a small number of fully connected units which are trained using some reasonable training method (e.g. back-propagation or RPROP [7]). <p> Further Work Pruning methods more sophisticated than magnitude-based pruning, such as optimal brain damage [5] and Karnin's method [4], might yield significant improvements in GAP networks, as they are subject to continuous heavy pruning. The adoption of a cascade of neurons could also be worthwhile <ref> [10] </ref>. A further possibility is to reintroduce a few deleted connections throughout training, perhaps on a random basis. The most important research still to be done is to test the GAP method on a range of large real-world problems.
Reference: [11] <author> W. X. Wen. SGNNN: </author> <title> Self-generating network of neural networks. </title> <booktitle> In Proceedings of the Ninth Australian Conference on Neural Networks (ACNN'98), </booktitle> <month> February </month> <year> 1998. </year>
Reference-contexts: This division may be done in a number of ways: by input pattern, by input node or by output node (see <ref> [11] </ref> in these conference proceedings for more details). In addition hidden units may be grouped in various ways so as to reduce the number of interconnections. One interesting suggestion is to use a fractal structure as a network architecture [9].
References-found: 11

