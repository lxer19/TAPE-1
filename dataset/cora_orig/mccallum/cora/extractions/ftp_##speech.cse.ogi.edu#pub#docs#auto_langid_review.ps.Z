URL: ftp://speech.cse.ogi.edu/pub/docs/auto_langid_review.ps.Z
Refering-URL: http://www.cse.ogi.edu/CSLU/publications/publications.html
Root-URL: http://www.cse.ogi.edu
Title: Automatic Language Identification: A Review/Tutorial  
Author: Yeshwant K. Muthusamyy, Etienne Barnardz and Ronald A. Colez 
Note: Center for Spoken Language Understanding  
Address: Texas Instruments, Inc.  
Affiliation: Systems and Information Sciences Laboratory  Oregon Graduate Institute of Science and Technology  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> K. Atkinson. </author> <title> Language identification from nonsegmental cues. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 44:378(A), </volume> <year> 1968. </year>
Reference-contexts: the salient acoustic and other characteristics that can be useful for language ID. 1 It can be obtained from the Linguistic Data Consortium, University of Pennsylvania, Philadelphia, PA. 4 5.2 Perceptual Experiments Before the advent of OGI TS, there were only two attempts to study human performance on language identification <ref> [1, 32] </ref>. These studies, performed on different corpora, were very limited in their scope and in the number of speakers used. A series of perceptual experiments were conducted using 1-, 2-, 4- and 6-second excerpts of speech excised from the spontaneous speech utterances in the original ten-language OGI corpus [28].
Reference: [2] <author> D. Cimarusti and R. B. Ives. </author> <title> Development of an automatic identification system of spoken languages: Phase 1. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing 82, </booktitle> <address> Paris, France, </address> <month> May </month> <year> 1982. </year>
Reference-contexts: The number of languages has varied from three to twenty. The approaches to language identification have used "reference sounds" in each language [16, 17, 18, 19], segment-and syllable-based Markov models [21], pitch contours [5, 31], formant vectors [5, 6], acoustic features <ref> [2] </ref>, broad phonetic and prosodic features [25], and just raw waveform features [12]. A variety of classification methods have been tried, including HMMs [9, 21], expert systems [11], clustering algorithms [5, 17, 32], quadratic classifiers [5] and artificial neural networks [12].
Reference: [3] <author> S. J. </author> <title> Eady. Differences in the F 0 patterns of speech: Tone language versus stress language. </title> <journal> Language and Speech, </journal> <volume> 25(1) </volume> <pages> 29-42, </pages> <year> 1982. </year>
Reference-contexts: Hawaiian is known for its very limited consonant inventory. Prosodic patterns also differ significantly between languages. For example, it has been shown that fundamental frequency (F 0 ) patterns of continuous speech display different characteristics in Mandarin Chinese (a tone language) and American English (a stress language) <ref> [3] </ref>. The key to solving the problem of automatic language identification then, is the detection and exploitation of such differences between languages. Of course, if we had a system or set of systems that could "understand" each language, it would also be identifying the correct one in the process.
Reference: [4] <author> W. Fisher, G. R. Doddington, and K. Goudie-Marshall. </author> <title> The DARPA speech recognition research database: Specification and status. </title> <booktitle> In Proceedings DARPA Speech Recognition Workshop, </booktitle> <pages> pages 93-100, </pages> <month> February </month> <year> 1986. </year>
Reference-contexts: isolation, meaningful comparisons across studies is not possible, for the following reasons: * Many of the studies represented classified or sensitive research, so experimental details (e.g., languages used) were often not described [5, 6, 16, 17, 18, 19, 21]. * There was no common, public-domain database (like the TIMIT corpus <ref> [4, 13] </ref> for continuous speech recognition) with which to evaluate different approaches to language ID.
Reference: [5] <author> J. T. </author> <title> Foil. Language identification using noisy speech. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing 86, </booktitle> <address> Tokyo, Japan, </address> <year> 1986. </year>
Reference-contexts: The number of languages has varied from three to twenty. The approaches to language identification have used "reference sounds" in each language [16, 17, 18, 19], segment-and syllable-based Markov models [21], pitch contours <ref> [5, 31] </ref>, formant vectors [5, 6], acoustic features [2], broad phonetic and prosodic features [25], and just raw waveform features [12]. A variety of classification methods have been tried, including HMMs [9, 21], expert systems [11], clustering algorithms [5, 17, 32], quadratic classifiers [5] and artificial neural networks [12]. <p> The number of languages has varied from three to twenty. The approaches to language identification have used "reference sounds" in each language [16, 17, 18, 19], segment-and syllable-based Markov models [21], pitch contours [5, 31], formant vectors <ref> [5, 6] </ref>, acoustic features [2], broad phonetic and prosodic features [25], and just raw waveform features [12]. A variety of classification methods have been tried, including HMMs [9, 21], expert systems [11], clustering algorithms [5, 17, 32], quadratic classifiers [5] and artificial neural networks [12]. <p> A variety of classification methods have been tried, including HMMs [9, 21], expert systems [11], clustering algorithms <ref> [5, 17, 32] </ref>, quadratic classifiers [5] and artificial neural networks [12]. A detailed review of these studies can be found in [22]. The literature does not present a coherent picture. <p> A variety of classification methods have been tried, including HMMs [9, 21], expert systems [11], clustering algorithms [5, 17, 32], quadratic classifiers <ref> [5] </ref> and artificial neural networks [12]. A detailed review of these studies can be found in [22]. The literature does not present a coherent picture. <p> While the performance figures of some of the studies might look impressive in isolation, meaningful comparisons across studies is not possible, for the following reasons: * Many of the studies represented classified or sensitive research, so experimental details (e.g., languages used) were often not described <ref> [5, 6, 16, 17, 18, 19, 21] </ref>. * There was no common, public-domain database (like the TIMIT corpus [4, 13] for continuous speech recognition) with which to evaluate different approaches to language ID.
Reference: [6] <author> F.J. Goodman, A.F. Martin, and R.E. Wohlford. </author> <title> Improved automatic language identification in noisy speech. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing 89, </booktitle> <address> Glasgow, Scotland, </address> <month> May </month> <year> 1989. </year>
Reference-contexts: The number of languages has varied from three to twenty. The approaches to language identification have used "reference sounds" in each language [16, 17, 18, 19], segment-and syllable-based Markov models [21], pitch contours [5, 31], formant vectors <ref> [5, 6] </ref>, acoustic features [2], broad phonetic and prosodic features [25], and just raw waveform features [12]. A variety of classification methods have been tried, including HMMs [9, 21], expert systems [11], clustering algorithms [5, 17, 32], quadratic classifiers [5] and artificial neural networks [12]. <p> While the performance figures of some of the studies might look impressive in isolation, meaningful comparisons across studies is not possible, for the following reasons: * Many of the studies represented classified or sensitive research, so experimental details (e.g., languages used) were often not described <ref> [5, 6, 16, 17, 18, 19, 21] </ref>. * There was no common, public-domain database (like the TIMIT corpus [4, 13] for continuous speech recognition) with which to evaluate different approaches to language ID.
Reference: [7] <author> T. J. Hazen and V. W. Zue. </author> <title> Automatic language identification using a segment-based approach. </title> <booktitle> In Proceedings 3rd European Conference on Speech Communication and Technology (Eurospeech 93), </booktitle> <month> September </month> <year> 1993. </year> <month> 14 </month>
Reference-contexts: In addition, the last two years have seen a substantial increase in papers on language ID in major speech conferences and symposia such as ICASSP, Eurospeech and SRS (Speech Research Symposium) <ref> [7, 15, 20, 23, 24, 28, 33, 34] </ref>, with complete sessions devoted to language ID in each of them. <p> This insight has, however, not contributed much to the success of current systems. Similarly, the incorporation of explicit prosodic information was not as useful in early language-identification systems of the current generation <ref> [7, 22] </ref> as the designers may have hoped. <p> There is, nonetheless, much reason to think that prosodic differences will contribute significantly to language identification in the future, and recent research has begun to fulfill this promise [10]. 10 Hazen and Zue <ref> [7] </ref> incorporated pitch information by multiplying their acoustic and phonotac--tic probabilities by a third factor which captured the probability densities of pitch distributions in the various languages. (The first derivative of the pitch is treated similarly.) They have reported systems with both broad-category and phonetic acoustic models. <p> He also extracted features indicative of speech rate and syllabic timing. Again, these prosodic features were found to be marginally useful|much less so than the durational and phonotactic features that he also employed. Segmental duration has been much more useful in characterizing language differences. In <ref> [7] </ref> and [26], the distributions of the durations in each broad category were modeled as additional factors in the computation of language likelihoods, and this was seen to be quite useful in both cases. <p> The "distance" of a language is the average of the S smallest speaker distances 12 Method Whole 10-sec Desc Speaker-identification based 78 59 [20] Acoustic model per language 53 50 [33] Phonotactics per broad category; pitch; timing; duration 66 48 [26] Acoustics per broad category; phonotactics; pitch; duration 57 46 <ref> [7] </ref> Table 1: Percentage of test utterances correctly classified in 10-language-recognition task with various approaches developed before 1994. "Whole" and "10-sec" refer to results obtained with whole stories and 10-second segments, respectively. "Desc" cites the papers in which the systems are described Method Whole 10-sec Desc Speaker-identification based 78 63 [20]
Reference: [8] <author> T. J. Hazen and V. W. Zue. </author> <title> Recent improvements in an approach to segment-based auto-matic language identification. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing 94, </booktitle> <address> Yokohama, Japan, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: 1994. "Whole" and "10-sec" refer to results obtained with whole stories and 10-second segments, respectively. "Desc" cites the papers in which the systems are described Method Whole 10-sec Desc Speaker-identification based 78 63 [20] Acoustic model per phone; phonotactics 79 70 [34] Acoustics per phone; phonotactics; pitch; duration 69 64 <ref> [8] </ref> Table 2: Results for 11-language recognition with various more recent approaches all systems. Also, it has been found that factors not explicitly mentioned above can have a substantial impact on performance factors such as noise compensation, channel equalization, etc.
Reference: [9] <author> A. S. House and E. P. Neuberg. </author> <title> Toward automatic identification of the language of an utterance. I. Preliminary methodological considerations. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 62(3) </volume> <pages> 708-713, </pages> <year> 1977. </year>
Reference-contexts: A variety of classification methods have been tried, including HMMs <ref> [9, 21] </ref>, expert systems [11], clustering algorithms [5, 17, 32], quadratic classifiers [5] and artificial neural networks [12]. A detailed review of these studies can be found in [22]. The literature does not present a coherent picture.
Reference: [10] <author> S. E. Hutchins and A. Thyme-Gobbel. </author> <title> Experiments using prosody for language identification. </title> <booktitle> In Proceedings Speech Research Symposium XIV, </booktitle> <address> Baltimore, Maryland, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: There is, nonetheless, much reason to think that prosodic differences will contribute significantly to language identification in the future, and recent research has begun to fulfill this promise <ref> [10] </ref>. 10 Hazen and Zue [7] incorporated pitch information by multiplying their acoustic and phonotac--tic probabilities by a third factor which captured the probability densities of pitch distributions in the various languages. (The first derivative of the pitch is treated similarly.) They have reported systems with both broad-category and phonetic acoustic <p> In [7] and [26], the distributions of the durations in each broad category were modeled as additional factors in the computation of language likelihoods, and this was seen to be quite useful in both cases. Perhaps the most successful work on incorporating prosodic information <ref> [10] </ref> proceeds by first segmenting an utterance into syllables based on amplitude and pitch information. Various statistics based on the rhythmic and tonal characteristics are then computed. Information related to rhythm is encapsulated in terms of syllable timing and duration, and descriptors of amplitude patterns.
Reference: [11] <author> R. B. Ives. </author> <title> A minimal rule AI expert system for real-time classification of natural spoken languages. </title> <booktitle> In Proceedings 2nd Annual Artificial Intelligence and Advanced Computer Technology Conference, </booktitle> <address> Long Beach, CA, </address> <month> April-May </month> <year> 1986. </year>
Reference-contexts: A variety of classification methods have been tried, including HMMs [9, 21], expert systems <ref> [11] </ref>, clustering algorithms [5, 17, 32], quadratic classifiers [5] and artificial neural networks [12]. A detailed review of these studies can be found in [22]. The literature does not present a coherent picture.
Reference: [12] <author> S. C. Kwasny, B. L. Kalman, W. Wu, and A. M. Engebretson. </author> <title> Identifying language from speech: An example of high-level, statistically-based feature extraction. </title> <booktitle> In Proceedings 14th Annual Conference of the Cognitive Science Society, </booktitle> <year> 1992. </year>
Reference-contexts: The approaches to language identification have used "reference sounds" in each language [16, 17, 18, 19], segment-and syllable-based Markov models [21], pitch contours [5, 31], formant vectors [5, 6], acoustic features [2], broad phonetic and prosodic features [25], and just raw waveform features <ref> [12] </ref>. A variety of classification methods have been tried, including HMMs [9, 21], expert systems [11], clustering algorithms [5, 17, 32], quadratic classifiers [5] and artificial neural networks [12]. A detailed review of these studies can be found in [22]. The literature does not present a coherent picture. <p> pitch contours [5, 31], formant vectors [5, 6], acoustic features [2], broad phonetic and prosodic features [25], and just raw waveform features <ref> [12] </ref>. A variety of classification methods have been tried, including HMMs [9, 21], expert systems [11], clustering algorithms [5, 17, 32], quadratic classifiers [5] and artificial neural networks [12]. A detailed review of these studies can be found in [22]. The literature does not present a coherent picture.
Reference: [13] <author> L. Lamel, R. Kassel, and S. Seneff. </author> <title> Speech database development: Design and analysis of the acoustic-phonetic corpus. </title> <booktitle> In Proceedings DARPA Speech Recognition Workshop, </booktitle> <pages> pages 100-110, </pages> <month> February </month> <year> 1986. </year>
Reference-contexts: isolation, meaningful comparisons across studies is not possible, for the following reasons: * Many of the studies represented classified or sensitive research, so experimental details (e.g., languages used) were often not described [5, 6, 16, 17, 18, 19, 21]. * There was no common, public-domain database (like the TIMIT corpus <ref> [4, 13] </ref> for continuous speech recognition) with which to evaluate different approaches to language ID.
Reference: [14] <author> L. F. Lamel and J-L. S. Gauvain. </author> <title> Language identification using phone-based acoustic likelihoods. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing 94, pages I-293-I-296, </booktitle> <address> Adelaide, Australia, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Training now consists of two stages. First, a stochastic model is trained for each of the fundamental units, exactly as in Section 6.1. Various combinations of models and units have been used: HMMs with broad-phonetic classes [33] or phone classes <ref> [14, 15, 33, 34] </ref>, and neural networks with phone classes [23, 24]. These trained models are then used to estimate the stochastic grammar appropriate for each language.
Reference: [15] <author> L. F. Lamel and J-L. S. Gauvain. </author> <title> Identifying non-linguistic speech features. </title> <booktitle> In Proceedings 3rd European Conference on Speech Communication and Technology (Eurospeech 93), </booktitle> <pages> pages 23-30, </pages> <address> Berlin, Germany, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: In addition, the last two years have seen a substantial increase in papers on language ID in major speech conferences and symposia such as ICASSP, Eurospeech and SRS (Speech Research Symposium) <ref> [7, 15, 20, 23, 24, 28, 33, 34] </ref>, with complete sessions devoted to language ID in each of them. <p> Training now consists of two stages. First, a stochastic model is trained for each of the fundamental units, exactly as in Section 6.1. Various combinations of models and units have been used: HMMs with broad-phonetic classes [33] or phone classes <ref> [14, 15, 33, 34] </ref>, and neural networks with phone classes [23, 24]. These trained models are then used to estimate the stochastic grammar appropriate for each language.
Reference: [16] <author> R. G. Leonard. </author> <title> Language recognition test and evaluation. </title> <type> Technical Report RADC-TR-80-83, </type> <institution> Air Force Rome Air Development Center, </institution> <month> March </month> <year> 1980. </year>
Reference-contexts: The number of languages has varied from three to twenty. The approaches to language identification have used "reference sounds" in each language <ref> [16, 17, 18, 19] </ref>, segment-and syllable-based Markov models [21], pitch contours [5, 31], formant vectors [5, 6], acoustic features [2], broad phonetic and prosodic features [25], and just raw waveform features [12]. <p> While the performance figures of some of the studies might look impressive in isolation, meaningful comparisons across studies is not possible, for the following reasons: * Many of the studies represented classified or sensitive research, so experimental details (e.g., languages used) were often not described <ref> [5, 6, 16, 17, 18, 19, 21] </ref>. * There was no common, public-domain database (like the TIMIT corpus [4, 13] for continuous speech recognition) with which to evaluate different approaches to language ID.
Reference: [17] <author> R. G. Leonard and G. R. Doddington. </author> <title> Automatic language identification. </title> <type> Technical Report RADC-TR-74-200, </type> <institution> Air Force Rome Air Development Center, </institution> <month> August </month> <year> 1974. </year>
Reference-contexts: The number of languages has varied from three to twenty. The approaches to language identification have used "reference sounds" in each language <ref> [16, 17, 18, 19] </ref>, segment-and syllable-based Markov models [21], pitch contours [5, 31], formant vectors [5, 6], acoustic features [2], broad phonetic and prosodic features [25], and just raw waveform features [12]. <p> A variety of classification methods have been tried, including HMMs [9, 21], expert systems [11], clustering algorithms <ref> [5, 17, 32] </ref>, quadratic classifiers [5] and artificial neural networks [12]. A detailed review of these studies can be found in [22]. The literature does not present a coherent picture. <p> While the performance figures of some of the studies might look impressive in isolation, meaningful comparisons across studies is not possible, for the following reasons: * Many of the studies represented classified or sensitive research, so experimental details (e.g., languages used) were often not described <ref> [5, 6, 16, 17, 18, 19, 21] </ref>. * There was no common, public-domain database (like the TIMIT corpus [4, 13] for continuous speech recognition) with which to evaluate different approaches to language ID.
Reference: [18] <author> R. G. Leonard and G. R. Doddington. </author> <title> Automatic language discrimination. </title> <type> Technical Report RADC-TR-78-5, </type> <institution> Air Force Rome Air Development Center, </institution> <month> January </month> <year> 1978. </year>
Reference-contexts: The number of languages has varied from three to twenty. The approaches to language identification have used "reference sounds" in each language <ref> [16, 17, 18, 19] </ref>, segment-and syllable-based Markov models [21], pitch contours [5, 31], formant vectors [5, 6], acoustic features [2], broad phonetic and prosodic features [25], and just raw waveform features [12]. <p> While the performance figures of some of the studies might look impressive in isolation, meaningful comparisons across studies is not possible, for the following reasons: * Many of the studies represented classified or sensitive research, so experimental details (e.g., languages used) were often not described <ref> [5, 6, 16, 17, 18, 19, 21] </ref>. * There was no common, public-domain database (like the TIMIT corpus [4, 13] for continuous speech recognition) with which to evaluate different approaches to language ID.
Reference: [19] <author> R. G. Leonard and G. R. Doddington. </author> <title> Automatic language identification. </title> <type> Technical Report RADC-TR-75-264, </type> <institution> Air Force Rome Air Development Center, </institution> <month> October </month> <year> 1975. </year>
Reference-contexts: The number of languages has varied from three to twenty. The approaches to language identification have used "reference sounds" in each language <ref> [16, 17, 18, 19] </ref>, segment-and syllable-based Markov models [21], pitch contours [5, 31], formant vectors [5, 6], acoustic features [2], broad phonetic and prosodic features [25], and just raw waveform features [12]. <p> While the performance figures of some of the studies might look impressive in isolation, meaningful comparisons across studies is not possible, for the following reasons: * Many of the studies represented classified or sensitive research, so experimental details (e.g., languages used) were often not described <ref> [5, 6, 16, 17, 18, 19, 21] </ref>. * There was no common, public-domain database (like the TIMIT corpus [4, 13] for continuous speech recognition) with which to evaluate different approaches to language ID.
Reference: [20] <author> K. P. Li. </author> <title> Automatic language identification using syllabic features. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing 94, pages I-297-I-300, </booktitle> <address> Adelaide, Australia, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: In addition, the last two years have seen a substantial increase in papers on language ID in major speech conferences and symposia such as ICASSP, Eurospeech and SRS (Speech Research Symposium) <ref> [7, 15, 20, 23, 24, 28, 33, 34] </ref>, with complete sessions devoted to language ID in each of them. <p> Finally, the correlation between pitch and amplitude is also described by several measures. Using just these prosodic features, Hutchins and Thyme-Gobbel have obtained results comparable to those reported by other groups using many additional sources of information. 6.4 Language-based Speaker Similarity Li <ref> [20] </ref> has achieved much success by importing ideas from speaker identification into language identification. <p> S is a parameter optimized for performance; it varies from 1 to 25 in the experiments reported in <ref> [20] </ref>. This process is schematized in Fig. 5. 6.5 Performance Results Although all of these systems have been tested on the OGI TS corpus, not all groups have performed exactly the same tests. <p> The "distance" between the utterance and each of the speakers is determined as the sum of the errors in these matches. The "distance" of a language is the average of the S smallest speaker distances 12 Method Whole 10-sec Desc Speaker-identification based 78 59 <ref> [20] </ref> Acoustic model per language 53 50 [33] Phonotactics per broad category; pitch; timing; duration 66 48 [26] Acoustics per broad category; phonotactics; pitch; duration 57 46 [7] Table 1: Percentage of test utterances correctly classified in 10-language-recognition task with various approaches developed before 1994. "Whole" and "10-sec" refer to results <p> [7] Table 1: Percentage of test utterances correctly classified in 10-language-recognition task with various approaches developed before 1994. "Whole" and "10-sec" refer to results obtained with whole stories and 10-second segments, respectively. "Desc" cites the papers in which the systems are described Method Whole 10-sec Desc Speaker-identification based 78 63 <ref> [20] </ref> Acoustic model per phone; phonotactics 79 70 [34] Acoustics per phone; phonotactics; pitch; duration 69 64 [8] Table 2: Results for 11-language recognition with various more recent approaches all systems. <p> Systems designed from quite dissimilar principles (e.g. based on phonotactics [34] versus speaker similarity <ref> [20] </ref>) perform comparably on the task described here. 7 Conclusions After languishing for almost two decades, the field of language ID has been rejuvenated with the advent of a public-domain multilingual corpus of speech.
Reference: [21] <author> K.P. Li and T. J. Edwards. </author> <title> Statistical models for automatic language identification. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing 80, </booktitle> <address> Denver, CO, </address> <month> April </month> <year> 1980. </year>
Reference-contexts: The number of languages has varied from three to twenty. The approaches to language identification have used "reference sounds" in each language [16, 17, 18, 19], segment-and syllable-based Markov models <ref> [21] </ref>, pitch contours [5, 31], formant vectors [5, 6], acoustic features [2], broad phonetic and prosodic features [25], and just raw waveform features [12]. <p> A variety of classification methods have been tried, including HMMs <ref> [9, 21] </ref>, expert systems [11], clustering algorithms [5, 17, 32], quadratic classifiers [5] and artificial neural networks [12]. A detailed review of these studies can be found in [22]. The literature does not present a coherent picture. <p> While the performance figures of some of the studies might look impressive in isolation, meaningful comparisons across studies is not possible, for the following reasons: * Many of the studies represented classified or sensitive research, so experimental details (e.g., languages used) were often not described <ref> [5, 6, 16, 17, 18, 19, 21] </ref>. * There was no common, public-domain database (like the TIMIT corpus [4, 13] for continuous speech recognition) with which to evaluate different approaches to language ID.
Reference: [22] <author> Y. K. Muthusamy. </author> <title> A Segmental Approach to Automatic Language Identification. </title> <type> PhD thesis, </type> <institution> Oregon Graduate Institute of Science & Technology, </institution> <year> 1993. </year> <month> 15 </month>
Reference-contexts: A variety of classification methods have been tried, including HMMs [9, 21], expert systems [11], clustering algorithms [5, 17, 32], quadratic classifiers [5] and artificial neural networks [12]. A detailed review of these studies can be found in <ref> [22] </ref>. The literature does not present a coherent picture. <p> The utterances ranged in duration from 1 second to 50 seconds, with an average duration of 13.4 seconds. Hindi is a recent addition to the corpus. The design, collection and development of the original ten-language corpus is described in detail in <ref> [22, 27] </ref>. OGI TS has been placed in the public domain 1 . 4.3 Impact of OGI TS: NIST Evaluations The advent of OGI TS has sparked renewed interest in language ID. <p> This insight has, however, not contributed much to the success of current systems. Similarly, the incorporation of explicit prosodic information was not as useful in early language-identification systems of the current generation <ref> [7, 22] </ref> as the designers may have hoped. <p> Hazen and Zue find that the prosodic factor is useful, but not by much, in both cases. Muthusamy <ref> [22] </ref> considered more complex prosodic models, which take into account the pitch variation within and across the different segments marked by a broad-category classifier. He also extracted features indicative of speech rate and syllabic timing.
Reference: [23] <author> Y. K. Muthusamy, T. Arai, K. M. Berkling, R. A. Cole, and E. Barnard. </author> <title> Two approaches to automatic language identification with telephone speech. </title> <booktitle> In Speech Research Symposium XIII, </booktitle> <pages> pages 443-449, </pages> <address> Baltimore, Maryland, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: In addition, the last two years have seen a substantial increase in papers on language ID in major speech conferences and symposia such as ICASSP, Eurospeech and SRS (Speech Research Symposium) <ref> [7, 15, 20, 23, 24, 28, 33, 34] </ref>, with complete sessions devoted to language ID in each of them. <p> First, a stochastic model is trained for each of the fundamental units, exactly as in Section 6.1. Various combinations of models and units have been used: HMMs with broad-phonetic classes [33] or phone classes [14, 15, 33, 34], and neural networks with phone classes <ref> [23, 24] </ref>. These trained models are then used to estimate the stochastic grammar appropriate for each language. Grammars equivalent to a bigram grammar have generally been employed; that is, these models capture the likelihood that each phoneme is followed by any other phoneme.
Reference: [24] <author> Y. K. Muthusamy, K. M. Berkling, T. Arai, R. A. Cole, and E. Barnard. </author> <title> A comparison of approaches to automatic language identification using telephone speech. </title> <booktitle> In Proceedings 3rd European Conference on Speech Communication and Technology (Eurospeech 93), </booktitle> <address> Berlin, Germany, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: In addition, the last two years have seen a substantial increase in papers on language ID in major speech conferences and symposia such as ICASSP, Eurospeech and SRS (Speech Research Symposium) <ref> [7, 15, 20, 23, 24, 28, 33, 34] </ref>, with complete sessions devoted to language ID in each of them. <p> First, a stochastic model is trained for each of the fundamental units, exactly as in Section 6.1. Various combinations of models and units have been used: HMMs with broad-phonetic classes [33] or phone classes [14, 15, 33, 34], and neural networks with phone classes <ref> [23, 24] </ref>. These trained models are then used to estimate the stochastic grammar appropriate for each language. Grammars equivalent to a bigram grammar have generally been employed; that is, these models capture the likelihood that each phoneme is followed by any other phoneme. <p> This has been confirmed in experiments more directly aimed at comparing phonetic and broad-category based approaches <ref> [24, 35] </ref>.
Reference: [25] <author> Y. K. Muthusamy and R. A. Cole. </author> <title> A segment-based automatic language identification system. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: The number of languages has varied from three to twenty. The approaches to language identification have used "reference sounds" in each language [16, 17, 18, 19], segment-and syllable-based Markov models [21], pitch contours [5, 31], formant vectors [5, 6], acoustic features [2], broad phonetic and prosodic features <ref> [25] </ref>, and just raw waveform features [12]. A variety of classification methods have been tried, including HMMs [9, 21], expert systems [11], clustering algorithms [5, 17, 32], quadratic classifiers [5] and artificial neural networks [12]. A detailed review of these studies can be found in [22].
Reference: [26] <author> Y. K. Muthusamy and R. A. Cole. </author> <title> Automatic segmentation and identification of ten languages using telephone speech. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing 92, </booktitle> <address> Banff, Alberta, Canada, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: He also extracted features indicative of speech rate and syllabic timing. Again, these prosodic features were found to be marginally useful|much less so than the durational and phonotactic features that he also employed. Segmental duration has been much more useful in characterizing language differences. In [7] and <ref> [26] </ref>, the distributions of the durations in each broad category were modeled as additional factors in the computation of language likelihoods, and this was seen to be quite useful in both cases. <p> The "distance" of a language is the average of the S smallest speaker distances 12 Method Whole 10-sec Desc Speaker-identification based 78 59 [20] Acoustic model per language 53 50 [33] Phonotactics per broad category; pitch; timing; duration 66 48 <ref> [26] </ref> Acoustics per broad category; phonotactics; pitch; duration 57 46 [7] Table 1: Percentage of test utterances correctly classified in 10-language-recognition task with various approaches developed before 1994. "Whole" and "10-sec" refer to results obtained with whole stories and 10-second segments, respectively. "Desc" cites the papers in which the systems are
Reference: [27] <author> Y. K. Muthusamy, R. A. Cole, and B. T. Oshika. </author> <title> The OGI multi-language telephone speech corpus. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing 92, </booktitle> <address> Banff, Alberta, Canada, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: The utterances ranged in duration from 1 second to 50 seconds, with an average duration of 13.4 seconds. Hindi is a recent addition to the corpus. The design, collection and development of the original ten-language corpus is described in detail in <ref> [22, 27] </ref>. OGI TS has been placed in the public domain 1 . 4.3 Impact of OGI TS: NIST Evaluations The advent of OGI TS has sparked renewed interest in language ID. <p> It is nevertheless possible to obtain at least some impression of the utility of different approaches by looking at comparable results that have been published. The first set of results refer to the original OGI corpus, which consisted of data in 10 languages <ref> [27, 30] </ref>. Results are given for the "stories" part of the corpus, consisting mostly of extemporaneous speech, which is widely used as a test set.
Reference: [28] <author> Y. K. Muthusamy, Neena Jain, and Ronald A. Cole. </author> <title> Perceptual benchmarks for automatic language identification. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech and Signal Processing 94, </booktitle> <address> Adelaide, Australia, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: In addition, the last two years have seen a substantial increase in papers on language ID in major speech conferences and symposia such as ICASSP, Eurospeech and SRS (Speech Research Symposium) <ref> [7, 15, 20, 23, 24, 28, 33, 34] </ref>, with complete sessions devoted to language ID in each of them. <p> These studies, performed on different corpora, were very limited in their scope and in the number of speakers used. A series of perceptual experiments were conducted using 1-, 2-, 4- and 6-second excerpts of speech excised from the spontaneous speech utterances in the original ten-language OGI corpus <ref> [28] </ref>. These experiments used an interactive graphical interface that played out excerpts of speech at random from the 10 languages, and maintained a log of listener responses. The listeners were given feedback on every trial so that they were constantly being trained as the experiment progressed.
Reference: [29] <author> S. Nakagawa, Y. Ueda, and T. Seino. </author> <title> Speaker-independent, text-independent language identification by HMM. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing 92, </booktitle> <address> Banff, Alberta, Canada, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: One approach is to model an entire language by a single stochastic model, as shown schematically in Figure 2 (a). For instance, an ergodic HMM (i.e. an HMM with all states connected to all other states |see as input <ref> [33, 29] </ref>. To identify the language of an unknown utterance, it is decoded with each of these models in turn. The language of the model which thus predicts the utterance with the highest likelihood is taken as the language of the utterance.
Reference: [30] <author> D. S. Pallett and A. F. Martin. </author> <title> Language identification: Testing protocols and evaluation procedures. </title> <booktitle> In Speech Research Symposium XIII, </booktitle> <pages> pages 428-442, </pages> <address> Baltimore, Maryland, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: It is nevertheless possible to obtain at least some impression of the utility of different approaches by looking at comparable results that have been published. The first set of results refer to the original OGI corpus, which consisted of data in 10 languages <ref> [27, 30] </ref>. Results are given for the "stories" part of the corpus, consisting mostly of extemporaneous speech, which is widely used as a test set.
Reference: [31] <author> M. Savic, E. Acosta, and S. K. Gupta. </author> <title> An automatic language identification system. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech and Signal Processing 91, </booktitle> <address> Toronto, Canada, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: The number of languages has varied from three to twenty. The approaches to language identification have used "reference sounds" in each language [16, 17, 18, 19], segment-and syllable-based Markov models [21], pitch contours <ref> [5, 31] </ref>, formant vectors [5, 6], acoustic features [2], broad phonetic and prosodic features [25], and just raw waveform features [12]. A variety of classification methods have been tried, including HMMs [9, 21], expert systems [11], clustering algorithms [5, 17, 32], quadratic classifiers [5] and artificial neural networks [12].
Reference: [32] <author> M. Sugiyama. </author> <title> Automatic language recognition using acoustic features. </title> <type> Technical Report TR-I-0167, </type> <institution> ATR Interpreting Telephony Research Laboratories, </institution> <year> 1991. </year>
Reference-contexts: A variety of classification methods have been tried, including HMMs [9, 21], expert systems [11], clustering algorithms <ref> [5, 17, 32] </ref>, quadratic classifiers [5] and artificial neural networks [12]. A detailed review of these studies can be found in [22]. The literature does not present a coherent picture. <p> the salient acoustic and other characteristics that can be useful for language ID. 1 It can be obtained from the Linguistic Data Consortium, University of Pennsylvania, Philadelphia, PA. 4 5.2 Perceptual Experiments Before the advent of OGI TS, there were only two attempts to study human performance on language identification <ref> [1, 32] </ref>. These studies, performed on different corpora, were very limited in their scope and in the number of speakers used. A series of perceptual experiments were conducted using 1-, 2-, 4- and 6-second excerpts of speech excised from the spontaneous speech utterances in the original ten-language OGI corpus [28].
Reference: [33] <author> M. A. Zissman. </author> <title> Automatic language identification using gaussian mixture and hidden markov models. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech and Signal Processing 93, </booktitle> <address> Minneapolis, MN, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: In addition, the last two years have seen a substantial increase in papers on language ID in major speech conferences and symposia such as ICASSP, Eurospeech and SRS (Speech Research Symposium) <ref> [7, 15, 20, 23, 24, 28, 33, 34] </ref>, with complete sessions devoted to language ID in each of them. <p> One approach is to model an entire language by a single stochastic model, as shown schematically in Figure 2 (a). For instance, an ergodic HMM (i.e. an HMM with all states connected to all other states |see as input <ref> [33, 29] </ref>. To identify the language of an unknown utterance, it is decoded with each of these models in turn. The language of the model which thus predicts the utterance with the highest likelihood is taken as the language of the utterance. <p> Training now consists of two stages. First, a stochastic model is trained for each of the fundamental units, exactly as in Section 6.1. Various combinations of models and units have been used: HMMs with broad-phonetic classes <ref> [33] </ref> or phone classes [14, 15, 33, 34], and neural networks with phone classes [23, 24]. These trained models are then used to estimate the stochastic grammar appropriate for each language. <p> Training now consists of two stages. First, a stochastic model is trained for each of the fundamental units, exactly as in Section 6.1. Various combinations of models and units have been used: HMMs with broad-phonetic classes [33] or phone classes <ref> [14, 15, 33, 34] </ref>, and neural networks with phone classes [23, 24]. These trained models are then used to estimate the stochastic grammar appropriate for each language. <p> The "distance" of a language is the average of the S smallest speaker distances 12 Method Whole 10-sec Desc Speaker-identification based 78 59 [20] Acoustic model per language 53 50 <ref> [33] </ref> Phonotactics per broad category; pitch; timing; duration 66 48 [26] Acoustics per broad category; phonotactics; pitch; duration 57 46 [7] Table 1: Percentage of test utterances correctly classified in 10-language-recognition task with various approaches developed before 1994. "Whole" and "10-sec" refer to results obtained with whole stories and 10-second segments,
Reference: [34] <author> M. A. Zissman and E. Singer. </author> <title> Automatic language identification of telephone speech messages using phoneme recognition and N-gram modeling. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing 94, pages I-305-I-308, </booktitle> <address> Adelaide, Australia, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: In addition, the last two years have seen a substantial increase in papers on language ID in major speech conferences and symposia such as ICASSP, Eurospeech and SRS (Speech Research Symposium) <ref> [7, 15, 20, 23, 24, 28, 33, 34] </ref>, with complete sessions devoted to language ID in each of them. <p> Training now consists of two stages. First, a stochastic model is trained for each of the fundamental units, exactly as in Section 6.1. Various combinations of models and units have been used: HMMs with broad-phonetic classes [33] or phone classes <ref> [14, 15, 33, 34] </ref>, and neural networks with phone classes [23, 24]. These trained models are then used to estimate the stochastic grammar appropriate for each language. <p> This eliminates the need for training corpora in each of the target languages. One foregoes, however, the acoustic scores that were included in the discrimination before. The most successful language ID system on the OGI TS corpus to date <ref> [34] </ref> generalizes this idea somewhat: rather than using a single acoustic recognizer and N language models for N target languages, it uses as many acoustic recognizers as possible (M = 6 languages in OGI TS have been labeled phonemically to date), and computes N language models for each of these. <p> classified in 10-language-recognition task with various approaches developed before 1994. "Whole" and "10-sec" refer to results obtained with whole stories and 10-second segments, respectively. "Desc" cites the papers in which the systems are described Method Whole 10-sec Desc Speaker-identification based 78 63 [20] Acoustic model per phone; phonotactics 79 70 <ref> [34] </ref> Acoustics per phone; phonotactics; pitch; duration 69 64 [8] Table 2: Results for 11-language recognition with various more recent approaches all systems. Also, it has been found that factors not explicitly mentioned above can have a substantial impact on performance factors such as noise compensation, channel equalization, etc. <p> Systems designed from quite dissimilar principles (e.g. based on phonotactics <ref> [34] </ref> versus speaker similarity [20]) perform comparably on the task described here. 7 Conclusions After languishing for almost two decades, the field of language ID has been rejuvenated with the advent of a public-domain multilingual corpus of speech.
Reference: [35] <author> M. A. Zissman and E. Singer. </author> <title> Language identification using phonetic class recognition and N-gram analysis. </title> <booktitle> In Speech Research Symposium XIII, </booktitle> <pages> pages 400-409, </pages> <address> Baltimore, Maryland, </address> <month> June </month> <year> 1993. </year> <month> 16 </month>
Reference-contexts: This has been confirmed in experiments more directly aimed at comparing phonetic and broad-category based approaches <ref> [24, 35] </ref>.
References-found: 35

