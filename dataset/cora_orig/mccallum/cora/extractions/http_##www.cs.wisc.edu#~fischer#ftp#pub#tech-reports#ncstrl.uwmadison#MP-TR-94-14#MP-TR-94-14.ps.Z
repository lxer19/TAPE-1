URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/MP-TR-94-14/MP-TR-94-14.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/MP-TR-94-14/
Root-URL: http://www.cs.wisc.edu
Title: CANCER DIAGNOSIS AND PROGNOSIS VIA LINEAR-PROGRAMMING-BASED MACHINE LEARNING  
Author: By W. Nick Street 
Degree: A thesis submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy (Computer Sciences) at the  
Date: 1994  
Address: WISCONSIN MADISON  
Affiliation: UNIVERSITY OF  
Abstract-found: 0
Intro-found: 0
Reference: [1] <author> D. W. Aha, D. Kibler, and M. K. Albert. </author> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 37-66, </pages> <year> 1991. </year>
Reference-contexts: For compatibility, this test was run using the greedy feature-selection method. * k-nearest neighbors: The nearest neighbor procedure <ref> [1, 35] </ref> is another effective and intuitive method for generalization. In this application, the recurrence time for the k recurrent cases closest to the given test point in Euclidean space were averaged to give a prediction.
Reference: [2] <author> M. L. Astion and P. Wilding. </author> <title> Application of neural networks to the interpretation of laboratory data in cancer diagnosis. </title> <journal> Clinical Chemistry, </journal> <volume> 38 </volume> <pages> 34-38, </pages> <year> 1992. </year>
Reference-contexts: In breast cancer, different researchers have applied neural networks to diagnosing from mammograms [100], ultrasound images [34], and pathological markers <ref> [2] </ref>. Still, few of these systems have received general clinical usage. Machine learning methods have also been applied to prognosis. Burke [16] uses a traditional backpropagation network to predict 10-year survival in breast cancer patients. A self-organizing neural architecture was used by Schenone et al. [81] to predict recurrence time.
Reference: [3] <author> D. Ballard and C. Brown. </author> <title> Computer Vision. </title> <publisher> Prentice-Hall, Inc, </publisher> <address> Engle-wood Cliffs, NJ, </address> <year> 1982. </year>
Reference-contexts: However, properly segmenting an image into its constituent parts is an important and largely unsolved problem in itself. This is made worse by the fact that the cells are very heterogeneous, with widely varying sizes, shapes, and image intensity properties. We tried several different approaches, including thresholding, region growing <ref> [3] </ref> and contour following [18]. <p> In our case E image measures the gray-level discontinuity along the snake. To quantify this discontinuity we convolve the area of the image corresponding to the snake point with a Sobel <ref> [3] </ref> edge detector and observe the resulting edge magnitude. This term is customized by taking advantage of the fact that cell nuclei are generally darker than the surrounding material. <p> Area Nuclear area is measured by counting the number of pixels on the interior of the snake and adding one-half of the pixels in the perimeter, to correct for the error caused by digitization. 4. Compactness Perimeter and area are combined <ref> [3] </ref> to give a measure of the compactness of the cell nuclei using the formula perimeter 2 =area. This dimensionless number is minimized for a circle and increases with the irregularity of the boundary.
Reference: [4] <author> P. H. Bartels, G. F. Bahr, M. Bibbo, and G. L. Wied. </author> <title> Objective cell image analysis. </title> <journal> Journal of Histochemistry and Cytochemistry, </journal> <volume> 30(4) </volume> <pages> 280-354, </pages> <year> 1973. </year>
Reference-contexts: Patients who choose not to have the biopsy done are followed for a year at one-month intervals to check for changes in the tumor. 2.7 Discussion and extensions While the methodology of applying machine learning techniques to features extracted from medical images is certainly not new <ref> [4, 89] </ref>, this work is unique in several respects. First is the precise quantification of nuclear shape, using several different features. This should make the Xcyt system more generally applicable to new situations where different representations of shape are relevant to outcome.
Reference: [5] <author> W. G. Baxt. </author> <title> Use of an artificial neural network for data analysis in clinical decision making: The diagnosis of acute coronary occlusion. </title> <journal> Neural Computation, </journal> <volume> 2(4) </volume> <pages> 480-489, </pages> <year> 1990. </year>
Reference-contexts: the space of possible feature sets [24, 44, 60], where only a small manageable set of variable subsets are examined. 1.2.2 Medical applications Machine learning approaches particularly ANN's have been used for a number of disparate clinical diagnosis tasks; diagnosing, for example, skin lesions [101], appendicitis [26], and myocardial infarction <ref> [5] </ref>. In breast cancer, different researchers have applied neural networks to diagnosing from mammograms [100], ultrasound images [34], and pathological markers [2]. Still, few of these systems have received general clinical usage. Machine learning methods have also been applied to prognosis.
Reference: [6] <author> O. H. Beahrs, D. E. Henson, R. V. P. Hutter, and B. J. Kennedy. </author> <title> Manual for Staging of Cancer. </title> <booktitle> J.B. Lippincott, Philadelphia, 4th edition, </booktitle> <year> 1992. </year>
Reference-contexts: Since the 1950's, the standard method for prognosis has been the TNM (tumor size, lymph node, metas-tasis) staging system <ref> [6, 36] </ref>. Patients with distant spread of the disease (metas-tasis) at time of surgery have the worst prognosis. Among other patients, both increased tumor size and an increased number of cancerous lymph nodes (near the tumor) have been shown to be negative prognostic factors [19]. <p> Several researchers, beginning with Black et al. [12], have shown evidence that cellular features observed at the time of diagnosis can be used to predict whether or not the disease will recur following surgery. However, with the widespread use of the TNM (tumor size, lymph node, metastasis) staging system <ref> [6, 36] </ref>, nuclear grade is now rarely used as a prognostic indicator.
Reference: [7] <author> K. P. Bennett. </author> <title> Decision tree construction via linear programming. </title> <booktitle> In Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society Conference, </booktitle> <pages> pages 97-101, </pages> <year> 1992. </year> <pages> 87 88 </pages>
Reference-contexts: While successful [93], the diagnostic results still depended on the ability to subjectively assign values to input features, and were therefore difficult to replicate. Further, the classifier employed was relatively complex, employing four pairs of planes in 9-space. Subsequent work by Mangasarian with K. P. Ben-nett <ref> [7, 10] </ref>, including the development of the MSM-Tree (MSM-T) algorithm (described in Chapter 2), resulted in improved diagnostic results. <p> of these 30 features, separated by benign and malignant cases, appears in Appendix A. 2.4 Application to breast cancer diagnosis 2.4.1 Classification method: Multisurface Method-Tree (MSM-T) The classification procedure used to separate benign from malignant samples is a variant on the Multisurface Method (MSM) [53, 54] known as MSM-Tree (MSM-T) <ref> [7, 10] </ref>. This method uses a linear programming [22] model to iteratively place a series of separating planes in the feature space of the examples. If the two sets of points are linearly separable, the first plane will be placed between them. <p> This chapter describes the results of such an approach applied to the MSM-T decision-tree building program <ref> [7, 10] </ref>. As described in Chapter 2, each step of the MSM-T algorithm creates a separating plane that minimizes the average distance of misclassified points from the plane.
Reference: [8] <author> K. P. Bennett. </author> <title> Machine Learning via Mathematical Programming. </title> <type> PhD thesis, </type> <institution> University of Wisconsin-Madison, Computer Sciences Department, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: MSM-T has been shown to learn concepts as well or better than more traditional learning methods such as C4.5 [71, 72] and CART [14]. It also has an advantage over artificial neural network (ANN) methods such as backpropagation [78] in that the training proceeds much faster <ref> [8] </ref>. The linear programming in the current version of MSM-T is implemented using the MINOS numerical optimization package [64]. 2.4.2 Exhaustive feature selection The 30-dimensional diagnosis data of benign and malignant points are linearly separable.
Reference: [9] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Neural network training via linear programming. </title> <editor> In P. M. Pardalos, editor, </editor> <booktitle> Advances in Optimization and Parallel Computing, </booktitle> <pages> pages 56-67. </pages> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1992. </year>
Reference-contexts: The MSM procedure uses a linear programming model to place successive pairs of separating planes in the feature space of the input examples, building a piecewise-linear separating surface. The procedure can also be considered a neural network training algorithm <ref> [9] </ref>. While successful [93], the diagnostic results still depended on the ability to subjectively assign values to input features, and were therefore difficult to replicate. Further, the classifier employed was relatively complex, employing four pairs of planes in 9-space. Subsequent work by Mangasarian with K. P.
Reference: [10] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Robust linear programming discrimination of two linearly inseparable sets. </title> <journal> Optimization Methods and Software, </journal> <volume> 1 </volume> <pages> 23-34, </pages> <year> 1992. </year>
Reference-contexts: While successful [93], the diagnostic results still depended on the ability to subjectively assign values to input features, and were therefore difficult to replicate. Further, the classifier employed was relatively complex, employing four pairs of planes in 9-space. Subsequent work by Mangasarian with K. P. Ben-nett <ref> [7, 10] </ref>, including the development of the MSM-Tree (MSM-T) algorithm (described in Chapter 2), resulted in improved diagnostic results. <p> of these 30 features, separated by benign and malignant cases, appears in Appendix A. 2.4 Application to breast cancer diagnosis 2.4.1 Classification method: Multisurface Method-Tree (MSM-T) The classification procedure used to separate benign from malignant samples is a variant on the Multisurface Method (MSM) [53, 54] known as MSM-Tree (MSM-T) <ref> [7, 10] </ref>. This method uses a linear programming [22] model to iteratively place a series of separating planes in the feature space of the examples. If the two sets of points are linearly separable, the first plane will be placed between them. <p> This chapter describes the results of such an approach applied to the MSM-T decision-tree building program <ref> [7, 10] </ref>. As described in Chapter 2, each step of the MSM-T algorithm creates a separating plane that minimizes the average distance of misclassified points from the plane.
Reference: [11] <author> J. Berkson. </author> <title> The calculation of survival rates. </title> <editor> In W. Walters, H. K. Gray, and J. T. Priestley, editors, Carcinoma and Other Malignant Lesions of the Stomach. W. B. </editor> <publisher> Saunders, </publisher> <address> Philadelphia, </address> <year> 1942. </year>
Reference-contexts: Since these probabilities are not directly available due to the right-censoring of the samples, a standard approximation known as the Kaplan-Meier or product limit plot can be used [42]. Product limit curves can be seen as a special case of the traditional life-table estimate <ref> [11] </ref>, in which a large number of samples are grouped into bins based on survival time.
Reference: [12] <author> M. M. Black, S. R. Opler, and F. D. Speer. </author> <title> Survival in breast cancer cases in relation to the structure of the primary tumor and regional lymph nodes. </title> <journal> Surgery, Gynecology and Obstetrics, </journal> <volume> 100 </volume> <pages> 543-551, </pages> <year> 1955. </year>
Reference-contexts: Among other patients, both increased tumor size and an increased number of cancerous lymph nodes (near the tumor) have been shown to be negative prognostic factors [19]. Nuclear features observed at the time of diagnosis have also been shown to be correlated with prognosis <ref> [12, 68] </ref>. Prognosis in the TNM model is expressed as survival curves, that is, plots 3 of time vs. probability of survival. Values of tumor size and lymph node status are discretized into a small number of intervals, and bins are created by plotting them against each other. <p> Chapter 3 Breast cancer prognosis: the recurrence surface approximation We next address the more difficult question of the long-term prognosis of patients with cancer. Several researchers, beginning with Black et al. <ref> [12] </ref>, have shown evidence that cellular features observed at the time of diagnosis can be used to predict whether or not the disease will recur following surgery.
Reference: [13] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. </author> <title> Occam's razor. </title> <journal> Information Processing Letters, </journal> <volume> 24 </volume> <pages> 377-380, </pages> <year> 1987. </year>
Reference-contexts: Similarly, decision trees learned by the algorithms CART [14] or C4.5 [70, 73] are pruned to remove potentially extraneous decision branches. While biasing a learning system toward simpler concepts makes the accuracy more reliably measurable <ref> [13] </ref> and often improves generalization on real-world data, it should not be interpreted as a universally desirable bias [80, 98].
Reference: [14] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, Inc., </publisher> <address> Pacific Grove, CA, </address> <year> 1984. </year>
Reference-contexts: Another popular concept representation which is more relevant to our work is that of decision trees. This paradigm has been used extensively in work from both machine learning [70, 73, 87] and statistics <ref> [14] </ref>. Each of these methods carries with it a particular inductive bias; that is, a tendency to form one type of concept over another. <p> Connectionist methods such as optimal brain damage [48] and cascade correlation [28] prevent overfitting by searching for an appropriate neural network architecture. Similarly, decision trees learned by the algorithms CART <ref> [14] </ref> or C4.5 [70, 73] are pruned to remove potentially extraneous decision branches. While biasing a learning system toward simpler concepts makes the accuracy more reliably measurable [13] and often improves generalization on real-world data, it should not be interpreted as a universally desirable bias [80, 98]. <p> The resulting planes can then be used in the manner of a decision tree to classify new points. MSM-T has been shown to learn concepts as well or better than more traditional learning methods such as C4.5 [71, 72] and CART <ref> [14] </ref>. It also has an advantage over artificial neural network (ANN) methods such as backpropagation [78] in that the training proceeds much faster [8].
Reference: [15] <author> J. Buckley and I. James. </author> <title> Linear regression with censored data. </title> <journal> Biometrika, </journal> <volume> 66 </volume> <pages> 429-436, </pages> <year> 1979. </year>
Reference-contexts: This regression method assumes that hazard functions for different patients are all proportional, and therefore that the survival functions are powers of one another. A prediction for time to recur (TTR) can then be obtained by finding the median of the expected survival function. By contrast, Buckley and James <ref> [15] </ref> devised a statistical method which directly estimates TTR.
Reference: [16] <author> H. B. Burke. </author> <title> Artificial neural networks for cancer research: Outcome prediction. </title> <journal> Seminars in Surgical Oncology, </journal> <volume> 10 </volume> <pages> 73-79, </pages> <year> 1994. </year>
Reference-contexts: In breast cancer, different researchers have applied neural networks to diagnosing from mammograms [100], ultrasound images [34], and pathological markers [2]. Still, few of these systems have received general clinical usage. Machine learning methods have also been applied to prognosis. Burke <ref> [16] </ref> uses a traditional backpropagation network to predict 10-year survival in breast cancer patients. A self-organizing neural architecture was used by Schenone et al. [81] to predict recurrence time. <p> In particular, recurrence or survival data is right censored, i.e., the right endpoint (recurrence time) is sometimes unknown. Previous work at Wisconsin [91, 96] and elsewhere <ref> [16] </ref> has framed prognosis as a classification problem by choosing a particular endpoint, such as two years or five years. The more sophisticated work of Ravdin et al. is discussed in Chapter 4. Problems involving censored data are common to several fields; see for instance [41, 49].
Reference: [17] <author> P. J. Burt. </author> <title> Fast filter transforms for image processing. </title> <journal> Computational Graphics, Image Processing, </journal> <volume> 16 </volume> <pages> 20-51, </pages> <year> 1981. </year> <month> 89 </month>
Reference-contexts: However, it is currently limited by the local nature of the convergence algorithm, requiring a close initial approximation. One solution to this limitation would be to smooth the image to successive levels of reduced detail, as in the Gaussian pyramid scheme 25 <ref> [17] </ref>. The initialized snake could be translated to the lowest, most highly filtered level and run to convergence on the filtered cell nucleus. The resulting snake would then be translated back and used as the initialization at the next more detailed level.
Reference: [18] <author> J. D. Cappellitti and A. Rosenfeld. </author> <title> Three-dimensional boundary following. Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> 48 </volume> <pages> 80-92, </pages> <year> 1989. </year>
Reference-contexts: This is made worse by the fact that the cells are very heterogeneous, with widely varying sizes, shapes, and image intensity properties. We tried several different approaches, including thresholding, region growing [3] and contour following <ref> [18] </ref>. Various commercial and public domain image analysis systems utilize these methods. 1 Our chosen solution [85, 86, 95] is a semi-automatic segmentation procedure [43] known as "snakes." Beginning with a user-defined approximate boundary as an initialization (see Figure 2.1) the snake locates the actual boundary of the cell nucleus.
Reference: [19] <author> C. L. Carter, C. Allen, and D. E. Henson. </author> <title> Relation of tumor size, lymph node status, and survival in 24,740 breast cancer cases. </title> <journal> Cancer, </journal> <volume> 63 </volume> <pages> 181-187, </pages> <year> 1989. </year>
Reference-contexts: Patients with distant spread of the disease (metas-tasis) at time of surgery have the worst prognosis. Among other patients, both increased tumor size and an increased number of cancerous lymph nodes (near the tumor) have been shown to be negative prognostic factors <ref> [19] </ref>. Nuclear features observed at the time of diagnosis have also been shown to be correlated with prognosis [12, 68]. Prognosis in the TNM model is expressed as survival curves, that is, plots 3 of time vs. probability of survival. <p> However, with the widespread use of the TNM (tumor size, lymph node, metastasis) staging system [6, 36], nuclear grade is now rarely used as a prognostic indicator. For instance, the breast database from the Surveillance, Epidemiology, and End Results (SEER) Program of the National Cancer Institute <ref> [19] </ref>, which contains follow-up data for over 24,000 breast cancer patients, includes a three-valued input representing nuclear grade for only about 17% of the cases. This chapter introduces a new, linear programming-based prognosis prediction method. This method is widely applicable to problems involving recurrence or survival data.
Reference: [20] <author> A. Charnes. </author> <title> Some fundamental theorems of perceptron theory and their geometry. </title> <editor> In J. T. Tou and R. H. Wilcox, editors, </editor> <booktitle> Computer and Information Sciences. </booktitle> <publisher> Spartan Books, </publisher> <address> Washington, D. C., </address> <year> 1964. </year>
Reference-contexts: Highleyman [37] first found that showing linear separability is equivalent to to finding a nonnegative solution to a set of linear equalities. Both Charnes <ref> [20] </ref> and Mangasarian [52] developed linear programs which construct planes to separate linearly separable point sets. Mangasarian [52] also showed how to separate sets by a nonlinear surface using linear programming whenever the surface parameters appeared linearly, e.g. a quadratic or polynomial surface.
Reference: [21] <author> D. R. Cox. </author> <title> Regression models and life-tables. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> B 34 </volume> <pages> 187-202, </pages> <year> 1972. </year>
Reference-contexts: More relevant are parametric regression methods, in which the distributional parameters are themselves functions of the available input features. See for example Lee [49] and the references therein. Non-parametric statistical techniques include the Cox proportional hazards model <ref> [21] </ref> which estimates the hazard function, h (t), of time, that is, the instantaneous risk at time t. This regression method assumes that hazard functions for different patients are all proportional, and therefore that the survival functions are powers of one another.
Reference: [22] <author> G. B. Dantzig. </author> <title> Linear Programming and Extensions. </title> <publisher> Princeton University Press, </publisher> <address> Princeton NJ, </address> <year> 1963. </year>
Reference-contexts: This method has also been used to verify the value of new prognostic features [75]. 1.2.3 Linear programming approaches Mathematical optimization approaches, in particular linear programming <ref> [22] </ref>, have long been used in problems of pattern separation. Highleyman [37] first found that showing linear separability is equivalent to to finding a nonnegative solution to a set of linear equalities. Both Charnes [20] and Mangasarian [52] developed linear programs which construct planes to separate linearly separable point sets. <p> This method uses a linear programming <ref> [22] </ref> model to iteratively place a series of separating planes in the feature space of the examples. If the two sets of points are linearly separable, the first plane will be placed between them.
Reference: [23] <author> M. De Laurentiis and P. M. Ravdin. </author> <title> A technique for using neural network analysis to perform survival analysis of censored data. </title> <journal> Cancer Letters, </journal> <volume> 77 </volume> <pages> 127-138, </pages> <year> 1994. </year>
Reference-contexts: Here a separation methodology is used in the space of f eatures fi time, with the result again being a plane which can be used to predict recurrence. The data manipulation used here builds upon the work of Ravdin and colleagues <ref> [23, 74, 75] </ref>, who use a neural network to learn survival curves. Before constructing the predictive surface, the training examples must be pre-processed in order to successfully frame the prognosis problem as one of separation.
Reference: [24] <author> N. R. Draper and H. Smith. </author> <title> Applied Regression Analysis. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1966. </year>
Reference-contexts: If framed as an integer programming problem, the search can be approximated with methods such as branch and bound [65]. Most methods, including those used in statistical regression, focus on a greedy, 6 directed search through the space of possible feature sets <ref> [24, 44, 60] </ref>, where only a small manageable set of variable subsets are examined. 1.2.2 Medical applications Machine learning approaches particularly ANN's have been used for a number of disparate clinical diagnosis tasks; diagnosing, for example, skin lesions [101], appendicitis [26], and myocardial infarction [5]. <p> The tolerance bands used in training are shown above and below the learned curve. This approach to function estimation can be seen as the inverse of the statistical practice of constructing confidence bands after approximating the function <ref> [24, 30, 88] </ref>. We also note that in the field of approximation theory, similar ideas have been used by letting equality constraints of the problem be replaced by inequalities [56]. 80 6.2 Simulation results The banded approximation idea was first tested with a simple curve fitting application, using simulated data.
Reference: [25] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern classification and scene analysis. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: In particular, feed-forward ANN's trained with algorithms such as backpropagation [78] have been shown to be an extremely flexible and powerful approach to function approximation [39, 50, 79]. Historically, the field of statistical pattern recognition, as summarized in Duda and Hart <ref> [25] </ref>, contributes much to our understanding of such problems. Methods such as linear discriminant analysis, logistic regression, and nearest-neighbor classification, as described in [25], are widely used. Another popular concept representation which is more relevant to our work is that of decision trees. <p> Historically, the field of statistical pattern recognition, as summarized in Duda and Hart <ref> [25] </ref>, contributes much to our understanding of such problems. Methods such as linear discriminant analysis, logistic regression, and nearest-neighbor classification, as described in [25], are widely used. Another popular concept representation which is more relevant to our work is that of decision trees. This paradigm has been used extensively in work from both machine learning [70, 73, 87] and statistics [14].
Reference: [26] <author> R. C. Eberhart, R. W. Dobbins, and L. V. Hutton. </author> <title> Neural network paradigm comparisons for appendicitis diagnosis. </title> <booktitle> In Proceedings of the Fourth Annual IEEE Symposium on Computer-Based Medical Systems, </booktitle> <pages> pages 298-304, </pages> <year> 1991. </year>
Reference-contexts: 6 directed search through the space of possible feature sets [24, 44, 60], where only a small manageable set of variable subsets are examined. 1.2.2 Medical applications Machine learning approaches particularly ANN's have been used for a number of disparate clinical diagnosis tasks; diagnosing, for example, skin lesions [101], appendicitis <ref> [26] </ref>, and myocardial infarction [5]. In breast cancer, different researchers have applied neural networks to diagnosing from mammograms [100], ultrasound images [34], and pathological markers [2]. Still, few of these systems have received general clinical usage. Machine learning methods have also been applied to prognosis.
Reference: [27] <author> B. Efron. </author> <title> The Jackknife, the Bootstrap and Other Resampling Plans. </title> <booktitle> Number 38 in CBMS-NSF Regional Conference Series in Applied Mathematics. </booktitle> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1982. </year> <month> 90 </month>
Reference-contexts: A 5 more tractable re-sampling method is cross-validation [84], which successively removes, say, 10% of the data for testing, again using each point in exactly one test set. Other statistical techniques, such as jackknifing and bootstrap <ref> [27] </ref>, are also available. Generalization performance on real-world data sets is often improved if some sort of smoothing procedure is applied to the learned concept, preventing the method from memorizing the training data at the expense of accuracy on unseen examples.
Reference: [28] <author> S. E. Fahlman and C. Lebiere. </author> <title> The cascade-correlation learning architecture. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 524-532, </pages> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For instance, a tuning or validation set (a portion of the training examples set aside to simulate unseen data) can be used to stop training a backpropagation network before it reaches optimality on the training set. Connectionist methods such as optimal brain damage [48] and cascade correlation <ref> [28] </ref> prevent overfitting by searching for an appropriate neural network architecture. Similarly, decision trees learned by the algorithms CART [14] or C4.5 [70, 73] are pruned to remove potentially extraneous decision branches.
Reference: [29] <author> W. J. Frable. Thin-needle aspiration biopsy. </author> <title> In Major Problems in Pathology 14. </title> <publisher> W.B. Saunders Co., </publisher> <address> Philadelphia, </address> <year> 1983. </year>
Reference-contexts: However, diagnosis with this procedure has met with mixed success; see for instance the summary of Frable <ref> [29] </ref>. Many different features are thought to be correlated with malignancy, and may interact with one another. Thus the process remains highly subjective, depending upon the skill and experience of the physician. The review of Giard and Hermans [33] particularly emphasized the need for performance standards in FNA diagnosis. <p> to being an effective research tool, the UNIX version of Xcyt is also being used in clinical decision making, and is in current use for breast cancer diagnosis on real patients at the University of Wisconsin Hospitals. 2.2 Image analysis (snakes) Many studies, some of which were summarized by Frable <ref> [29] </ref>, have shown that various features of cell nuclei can be used to distinguish benign from malignant breast tumors. The first step in automatic diagnosis is to isolate and precisely 8 9 specify the nuclei in a captured digital image.
Reference: [30] <author> R. Fraiman and G. Perez Iribarren. </author> <title> Conservative confidence bands for non-parametric regression. </title> <editor> In G. Roussas, editor, </editor> <booktitle> Nonparametric Functional Estimation and Related Topics, </booktitle> <pages> pages 45-66. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year>
Reference-contexts: The tolerance bands used in training are shown above and below the learned curve. This approach to function estimation can be seen as the inverse of the statistical practice of constructing confidence bands after approximating the function <ref> [24, 30, 88] </ref>. We also note that in the field of approximation theory, similar ideas have been used by letting equality constraints of the problem be replaced by inequalities [56]. 80 6.2 Simulation results The banded approximation idea was first tested with a simple curve fitting application, using simulated data.
Reference: [31] <author> J. E. Freund. </author> <title> Mathematical Statistics. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, fifth edition, </address> <year> 1992. </year>
Reference-contexts: Survival curves can also be constructed in a parametric fashion by fitting a known distribution to the observed cases. For instance, the survival probability can be estimated with an exponential distribution: s (t) = e t . The parameter can then be estimated in a standard maximum likelihood fashion <ref> [31] </ref>. More relevant are parametric regression methods, in which the distributional parameters are themselves functions of the available input features. See for example Lee [49] and the references therein.
Reference: [32] <author> S. Geman, E. Bienestock, and R. Doursat. </author> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-58, </pages> <year> 1992. </year>
Reference-contexts: This effect was most noticeable for the smaller training sets, as the improvement ranged from 12.5% for the 10 point set to 2% for 50 points. The diminished return of banded approximation for larger training sets makes sense in terms of the bias/variance tradeoff <ref> [32] </ref>. Since the same type of surface is being constructed either with or without the tolerance band, the methods have equal predictive power and hence the expected bias part of the error remains the same.
Reference: [33] <author> R. W. M. Giard and J. Hermans. </author> <title> The value of aspiration cytologic examination of the breast. A statistical review of the medical literature. </title> <journal> Cancer, </journal> <volume> 69 </volume> <pages> 2104-2110, </pages> <year> 1992. </year>
Reference-contexts: Many different features are thought to be correlated with malignancy, and may interact with one another. Thus the process remains highly subjective, depending upon the skill and experience of the physician. The review of Giard and Hermans <ref> [33] </ref> particularly emphasized the need for performance standards in FNA diagnosis.
Reference: [34] <author> V. Golberg, A. Manduca, D. L. Ewert, J. J. Gisvold, and J. F. Greenleaf. </author> <title> Improvement in specificity of ultrasonography for diagnosis of breast tumors by means of artificial intelligence. </title> <journal> Medical Physics, </journal> <volume> 19 </volume> <pages> 1475-1481, </pages> <year> 1992. </year>
Reference-contexts: In breast cancer, different researchers have applied neural networks to diagnosing from mammograms [100], ultrasound images <ref> [34] </ref>, and pathological markers [2]. Still, few of these systems have received general clinical usage. Machine learning methods have also been applied to prognosis. Burke [16] uses a traditional backpropagation network to predict 10-year survival in breast cancer patients.
Reference: [35] <author> P. Hart. </author> <title> The condensed nearest neighbor rule. </title> <journal> Transactions on Information Theory, </journal> <volume> IT-14:515-516, </volume> <year> 1967. </year>
Reference-contexts: For compatibility, this test was run using the greedy feature-selection method. * k-nearest neighbors: The nearest neighbor procedure <ref> [1, 35] </ref> is another effective and intuitive method for generalization. In this application, the recurrence time for the k recurrent cases closest to the given test point in Euclidean space were averaged to give a prediction.
Reference: [36] <author> P. Hermanek and L. H. Sobin, </author> <title> editors. TNM Classification of Malignant Tumors. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, 4th edition, </address> <year> 1987. </year> <month> 91 </month>
Reference-contexts: Since the 1950's, the standard method for prognosis has been the TNM (tumor size, lymph node, metas-tasis) staging system <ref> [6, 36] </ref>. Patients with distant spread of the disease (metas-tasis) at time of surgery have the worst prognosis. Among other patients, both increased tumor size and an increased number of cancerous lymph nodes (near the tumor) have been shown to be negative prognostic factors [19]. <p> Several researchers, beginning with Black et al. [12], have shown evidence that cellular features observed at the time of diagnosis can be used to predict whether or not the disease will recur following surgery. However, with the widespread use of the TNM (tumor size, lymph node, metastasis) staging system <ref> [6, 36] </ref>, nuclear grade is now rarely used as a prognostic indicator.
Reference: [37] <author> W. Highleyman. </author> <title> A note on linear separability. </title> <journal> IRE Transactions on Electronic Computers, </journal> <pages> pages 777-778, </pages> <year> 1961. </year> <title> Correspondence Section. </title>
Reference-contexts: This method has also been used to verify the value of new prognostic features [75]. 1.2.3 Linear programming approaches Mathematical optimization approaches, in particular linear programming [22], have long been used in problems of pattern separation. Highleyman <ref> [37] </ref> first found that showing linear separability is equivalent to to finding a nonnegative solution to a set of linear equalities. Both Charnes [20] and Mangasarian [52] developed linear programs which construct planes to separate linearly separable point sets.
Reference: [38] <author> G. E. Hinton. </author> <title> Learning distributed representations of concepts. </title> <booktitle> In Proceedings of the Eighth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 1-12, </pages> <address> Hillsdale, 1986. </address> <publisher> Erlbaum. </publisher>
Reference-contexts: Thus we view feature selection as a means of finding the optimal number of parameters, in the spirit of neural network algorithms such as pruning [82], weight decay <ref> [38] </ref> and optimal brain damage [48]. Essential to the feature-selection algorithm is the fact that the features have all been normalized to lie in approximately the same range.
Reference: [39] <author> K. Hornik, M. Stinchcombe, and H. White. </author> <title> Multilayer feedforward networks are universal approximators. </title> <booktitle> Neural Networks, </booktitle> <volume> 2(5) </volume> <pages> 359-366, </pages> <year> 1989. </year>
Reference-contexts: In recent years, one of the most popular of these approaches has been artificial neural networks (ANN's), or connectionist methods. In particular, feed-forward ANN's trained with algorithms such as backpropagation [78] have been shown to be an extremely flexible and powerful approach to function approximation <ref> [39, 50, 79] </ref>. Historically, the field of statistical pattern recognition, as summarized in Duda and Hart [25], contributes much to our understanding of such problems. Methods such as linear discriminant analysis, logistic regression, and nearest-neighbor classification, as described in [25], are widely used.
Reference: [40] <author> I. T. Jolliffe. </author> <title> Principal Component Analysis. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1986. </year>
Reference-contexts: While our feature subsets were generated randomly, the sets could chosen algorithmically, either as a manual pre-processing step or with a principal components analysis <ref> [40] </ref>. In this manner the level 0 predictors could operate on sets of features which "belong together." Of all the RSA extensions described in this chapter, the implicit RSA demonstrated the most improvement in predictive power.
Reference: [41] <author> R. G. Miller Jr. </author> <title> Survival Analysis. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: The more sophisticated work of Ravdin et al. is discussed in Chapter 4. Problems involving censored data are common to several fields; see for instance <ref> [41, 49] </ref>. In engineering, one might be interested in the survival characteristics of electronic components, while sociologists might consider what factors lead to long-lasting marriages. However, the application of machine learning methods to these problems has been rare.
Reference: [42] <author> E. L. Kaplan and P. Meier. </author> <title> Nonparametric estimation from incomplete observations. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 53 </volume> <pages> 457-481, </pages> <year> 1958. </year>
Reference-contexts: A survival curve plots the probability of survival against time, as time increases from zero. Since these probabilities are not directly available due to the right-censoring of the samples, a standard approximation known as the Kaplan-Meier or product limit plot can be used <ref> [42] </ref>. Product limit curves can be seen as a special case of the traditional life-table estimate [11], in which a large number of samples are grouped into bins based on survival time.
Reference: [43] <author> M. Kass, A. Witkin, and D. Terzopoulos. Snakes: </author> <title> Active contour models. </title> <journal> International Journal of Computer Vision, </journal> <volume> 1(4) </volume> <pages> 321-331, </pages> <year> 1988. </year>
Reference-contexts: We tried several different approaches, including thresholding, region growing [3] and contour following [18]. Various commercial and public domain image analysis systems utilize these methods. 1 Our chosen solution [85, 86, 95] is a semi-automatic segmentation procedure <ref> [43] </ref> known as "snakes." Beginning with a user-defined approximate boundary as an initialization (see Figure 2.1) the snake locates the actual boundary of the cell nucleus. A snake is a deformable spline which seeks to minimize an energy function defined over the arclength of a closed curve.
Reference: [44] <author> K. Kira and L. Rendell. </author> <title> The feature selection problem: Traditional methods and a new algorithm. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 129-134, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: If framed as an integer programming problem, the search can be approximated with methods such as branch and bound [65]. Most methods, including those used in statistical regression, focus on a greedy, 6 directed search through the space of possible feature sets <ref> [24, 44, 60] </ref>, where only a small manageable set of variable subsets are examined. 1.2.2 Medical applications Machine learning approaches particularly ANN's have been used for a number of disparate clinical diagnosis tasks; diagnosing, for example, skin lesions [101], appendicitis [26], and myocardial infarction [5].
Reference: [45] <author> J. Kittler. </author> <title> Feature selection and extraction. In Young & Fu, editor, Handbook of Pattern Recognition and Image Processing. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1986. </year>
Reference-contexts: Setting missing values to the mean is a common practice which is particularly appropriate here. Since we are building linear models, a missing value has no effect on the prediction for that case. The feature-selection procedure is a variation of the heuristic sequential backward elimination method <ref> [45] </ref>, a top-down, greedy search through the space of feature sets. The procedure begins by setting aside a tuning or validation set, that is a surrogate testing set, in our case a randomly selected 10% of the training cases.
Reference: [46] <author> P. Lachenbruch and P. Mickey. </author> <title> Estimation of error rates in discriminant analysis. </title> <journal> Technometrics, </journal> <volume> 10 </volume> <pages> 1-11, </pages> <year> 1968. </year> <month> 92 </month>
Reference-contexts: One effective estimation method is leave-one-out testing <ref> [46] </ref>, wherein the generalizer is trained with all but one of the samples and tested on the "left-out" sample. This procedure is repeated m times until each training point has been used for testing. The result is a very accurate and nearly unbiased, but computationally expensive, accuracy estimate.
Reference: [47] <author> K. Lang, A. Waibel, and G. Hinton. </author> <title> A time-delay neural network architecture for isolated word recognition. </title> <booktitle> Neural Networks, </booktitle> <pages> 3(23-43), </pages> <year> 1990. </year>
Reference-contexts: Some methods, such as the feature-selection algorithm used in previous chapters and early stopping in backpropagation, utilize a tuning or validation set <ref> [47] </ref>, which is a part of the training data that is not explicitly used by the training algorithm. Rather, this set is used to tune the parameters of the training method, for instance, to choose the set of features for training.
Reference: [48] <author> Y. Le Cun, J. S. Denker, and S. A. Solla. </author> <title> Optimal brain damage. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 598-605, </pages> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For instance, a tuning or validation set (a portion of the training examples set aside to simulate unseen data) can be used to stop training a backpropagation network before it reaches optimality on the training set. Connectionist methods such as optimal brain damage <ref> [48] </ref> and cascade correlation [28] prevent overfitting by searching for an appropriate neural network architecture. Similarly, decision trees learned by the algorithms CART [14] or C4.5 [70, 73] are pruned to remove potentially extraneous decision branches. <p> Thus we view feature selection as a means of finding the optimal number of parameters, in the spirit of neural network algorithms such as pruning [82], weight decay [38] and optimal brain damage <ref> [48] </ref>. Essential to the feature-selection algorithm is the fact that the features have all been normalized to lie in approximately the same range.
Reference: [49] <author> E. T. Lee. </author> <title> Statistical Methods for Survival Data Analysis. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: While a patient can be classified 'recur' if the disease is observed, there is no real cutoff point at which the patient can be considered a non-recurrent case. The data are therefore censored <ref> [49] </ref>, in that we know a time to recur (TTR) for only a subset of patients; for the others, we know only the time of their last check-up, or disease free survival time (DFS). <p> The more sophisticated work of Ravdin et al. is discussed in Chapter 4. Problems involving censored data are common to several fields; see for instance <ref> [41, 49] </ref>. In engineering, one might be interested in the survival characteristics of electronic components, while sociologists might consider what factors lead to long-lasting marriages. However, the application of machine learning methods to these problems has been rare. <p> The parameter can then be estimated in a standard maximum likelihood fashion [31]. More relevant are parametric regression methods, in which the distributional parameters are themselves functions of the available input features. See for example Lee <ref> [49] </ref> and the references therein. Non-parametric statistical techniques include the Cox proportional hazards model [21] which estimates the hazard function, h (t), of time, that is, the instantaneous risk at time t.
Reference: [50] <author> R. Lippmann. </author> <title> An introduction to computing with neural nets. </title> <journal> IEEE ASSP Magazine, </journal> <pages> pages 4-22, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: In recent years, one of the most popular of these approaches has been artificial neural networks (ANN's), or connectionist methods. In particular, feed-forward ANN's trained with algorithms such as backpropagation [78] have been shown to be an extremely flexible and powerful approach to function approximation <ref> [39, 50, 79] </ref>. Historically, the field of statistical pattern recognition, as summarized in Duda and Hart [25], contributes much to our understanding of such problems. Methods such as linear discriminant analysis, logistic regression, and nearest-neighbor classification, as described in [25], are widely used.
Reference: [51] <author> B. B. Mandelbrot. </author> <title> The Fractal Geometry of Nature, chapter 5. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <address> New York, </address> <year> 1977. </year>
Reference-contexts: The lengths of perpendicular segments on the right of the major axis are compared to those on the left. 9. Fractal Dimension The fractal dimension of a cell is approximated using the "coastline approximation" described by Mandelbrot <ref> [51] </ref>. The perimeter of the nucleus 17 is measured using increasingly larger 'rulers.' As the ruler size increases, decreasing the precision of the measurement, the observed perimeter decreases. See Figure 2.7.
Reference: [52] <author> O. L. Mangasarian. </author> <title> Linear and nonlinear separation of patterns by linear programming. </title> <journal> Operations Research, </journal> <volume> 13 </volume> <pages> 444-452, </pages> <year> 1965. </year>
Reference-contexts: Highleyman [37] first found that showing linear separability is equivalent to to finding a nonnegative solution to a set of linear equalities. Both Charnes [20] and Mangasarian <ref> [52] </ref> developed linear programs which construct planes to separate linearly separable point sets. Mangasarian [52] also showed how to separate sets by a nonlinear surface using linear programming whenever the surface parameters appeared linearly, e.g. a quadratic or polynomial surface. <p> Highleyman [37] first found that showing linear separability is equivalent to to finding a nonnegative solution to a set of linear equalities. Both Charnes [20] and Mangasarian <ref> [52] </ref> developed linear programs which construct planes to separate linearly separable point sets. Mangasarian [52] also showed how to separate sets by a nonlinear surface using linear programming whenever the surface parameters appeared linearly, e.g. a quadratic or polynomial surface.
Reference: [53] <author> O. L. Mangasarian. </author> <title> Multi-surface method of pattern separation. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-14:801-807, </volume> <year> 1968. </year>
Reference-contexts: H. Wolberg (Surgery and Human Oncology) and O. L. Mangasar-ian (Computer Sciences). The Multisurface Method (MSM) <ref> [53, 54] </ref> of pattern separation was first applied to a collection of cases represented by nine subjectively evaluated cytological features [57, 58, 92]. <p> as p n . 18 each of these 30 features, separated by benign and malignant cases, appears in Appendix A. 2.4 Application to breast cancer diagnosis 2.4.1 Classification method: Multisurface Method-Tree (MSM-T) The classification procedure used to separate benign from malignant samples is a variant on the Multisurface Method (MSM) <ref> [53, 54] </ref> known as MSM-Tree (MSM-T) [7, 10]. This method uses a linear programming [22] model to iteratively place a series of separating planes in the feature space of the examples. If the two sets of points are linearly separable, the first plane will be placed between them.
Reference: [54] <author> O. L. Mangasarian. </author> <title> Mathematical programming in neural networks. </title> <journal> ORSA Journal on Computing, </journal> <volume> 5 </volume> <pages> 349-360, </pages> <year> 1993. </year>
Reference-contexts: H. Wolberg (Surgery and Human Oncology) and O. L. Mangasar-ian (Computer Sciences). The Multisurface Method (MSM) <ref> [53, 54] </ref> of pattern separation was first applied to a collection of cases represented by nine subjectively evaluated cytological features [57, 58, 92]. <p> as p n . 18 each of these 30 features, separated by benign and malignant cases, appears in Appendix A. 2.4 Application to breast cancer diagnosis 2.4.1 Classification method: Multisurface Method-Tree (MSM-T) The classification procedure used to separate benign from malignant samples is a variant on the Multisurface Method (MSM) <ref> [53, 54] </ref> known as MSM-Tree (MSM-T) [7, 10]. This method uses a linear programming [22] model to iteratively place a series of separating planes in the feature space of the examples. If the two sets of points are linearly separable, the first plane will be placed between them.
Reference: [55] <author> O. L. Mangasarian and R. R. Meyer. </author> <title> Nonlinear perturbation of linear programs. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 17 </volume> <pages> 745-752, </pages> <year> 1979. </year>
Reference-contexts: To reflect this, the v term in the objective is weighed by an appropriately small positive parameter ffi, forcing underestimated recurrent points closer to the surface. Based on a perturbation theorem <ref> [55] </ref>, for a sufficiently small positive ffi, that is 0 &lt; ffi ffi for some ffi, the objective minimizes the weighted term conditionally, i.e., of those possible variable values which minimize the first two terms of the objective, those values which minimize the third term are chosen.
Reference: [56] <author> O. L. Mangasarian and L. L. Schumaker. </author> <title> Splines via optimal control. In Approximation with Special Emphasis on Spline Functions, </title> <address> pages 119-156. </address> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1969. </year>
Reference-contexts: We also note that in the field of approximation theory, similar ideas have been used by letting equality constraints of the problem be replaced by inequalities <ref> [56] </ref>. 80 6.2 Simulation results The banded approximation idea was first tested with a simple curve fitting application, using simulated data. The use of simulated data allows the predicted curve to be compared directly to the known function that generated the input points.
Reference: [57] <author> O. L. Mangasarian, R. Setiono, and W. H. Wolberg. </author> <title> Pattern recognition via linear programming: Theory and application to medical diagnosis. </title> <booktitle> 93 In Proceedings of the Workshop on Large-Scale Numerical Optimization, </booktitle> <pages> pages 22-31, </pages> <address> Philadelphia, Pennsylvania, </address> <year> 1990. </year> <note> SIAM. </note>
Reference-contexts: H. Wolberg (Surgery and Human Oncology) and O. L. Mangasar-ian (Computer Sciences). The Multisurface Method (MSM) [53, 54] of pattern separation was first applied to a collection of cases represented by nine subjectively evaluated cytological features <ref> [57, 58, 92] </ref>. The MSM procedure uses a linear programming model to place successive pairs of separating planes in the feature space of the input examples, building a piecewise-linear separating surface. The procedure can also be considered a neural network training algorithm [9].
Reference: [58] <author> O. L. Mangasarian and W. H. Wolberg. </author> <title> Cancer diagnosis via linear programming. </title> <journal> SIAM News, </journal> <volume> 23:1 & 18, </volume> <year> 1990. </year>
Reference-contexts: H. Wolberg (Surgery and Human Oncology) and O. L. Mangasar-ian (Computer Sciences). The Multisurface Method (MSM) [53, 54] of pattern separation was first applied to a collection of cases represented by nine subjectively evaluated cytological features <ref> [57, 58, 92] </ref>. The MSM procedure uses a linear programming model to place successive pairs of separating planes in the feature space of the input examples, building a piecewise-linear separating surface. The procedure can also be considered a neural network training algorithm [9].
Reference: [59] <author> E. Marshall. </author> <title> Search for a killer: Focus shifts from fat to hormones in special report on breast cancer. </title> <journal> Science, </journal> <volume> 259 </volume> <pages> 618-621, </pages> <year> 1993. </year>
Reference-contexts: begin with an overview of the application area which unifies this research: the diagnosis and prognosis of breast cancer. 1.1 Breast cancer Despite a great deal of public education and scientific research, breast cancer continues to be the most common and the second most deadly form of cancer among women <ref> [59] </ref>. An increased incidence of the disease has offset improving survival rates, resulting in a nearly constant mortality from breast cancer over the past twenty years [61].
Reference: [60] <author> J. Michel, G. Mirchandani, and S. Wald. </author> <title> Prognosis with neural networks using statistically based feature sets. </title> <booktitle> In Fifth Annual IEEE Symposium on Computer-Based Medical Systems, </booktitle> <pages> pages 695-702, </pages> <address> Los Alamitos, CA, June 1992. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: If framed as an integer programming problem, the search can be approximated with methods such as branch and bound [65]. Most methods, including those used in statistical regression, focus on a greedy, 6 directed search through the space of possible feature sets <ref> [24, 44, 60] </ref>, where only a small manageable set of variable subsets are examined. 1.2.2 Medical applications Machine learning approaches particularly ANN's have been used for a number of disparate clinical diagnosis tasks; diagnosing, for example, skin lesions [101], appendicitis [26], and myocardial infarction [5].
Reference: [61] <author> B. A. Miller, E. J. Feuer, and B. F. Hankey. </author> <title> Recent incidence trends for breast cancer in women and relevance of early detection: An update. </title> <journal> CA Cancer Journal for Clinicians, </journal> <volume> 43(1) </volume> <pages> 27-41, </pages> <year> 1993. </year>
Reference-contexts: An increased incidence of the disease has offset improving survival rates, resulting in a nearly constant mortality from breast cancer over the past twenty years <ref> [61] </ref>. Two major areas of breast cancer research will be addressed in this work: early detection through timely diagnosis, and appropriate planning of treatment through accurate prognosis. 1 2 1.1.1 Diagnosis The diagnosis of breast tumors has traditionally been performed by a full biopsy, an invasive surgical procedure.
Reference: [62] <author> M. Mitze, </author> <year> 1992. </year> <type> Personal communication. </type>
Reference-contexts: This we attribute to the small amount of information available in an objectively graded, three-valued feature. 3.5.3 Gutenberg data We also evaluated the RSA procedure on a third data set from the group at Gutenberg, Germany <ref> [62] </ref>.
Reference: [63] <author> S. Mukhopadhyay, A. Roy, L. S. Kim, and S. Govil. </author> <title> A polynomial time algorithm for generating neural networks for pattern classification: Its stability properties and some test results. </title> <journal> Neural Computation, </journal> <volume> 5 </volume> <pages> 317-330, </pages> <year> 1993. </year>
Reference-contexts: More recently, Roy et al. [77] and Mukhopadhyay et al. <ref> [63] </ref> used linear programming to generate convex "covers" which are combined to classify general patterns. 1.3 Review of previous work at Wisconsin The application of machine learning techniques to problems in breast cancer diagnosis at the University of Wisconsin began in 1989 with the collaborative work of W. H.
Reference: [64] <author> B.A. Murtagh and M.A. Saunders. </author> <title> MINOS 5.0 user's guide. </title> <type> Technical Report SOL 83.20, </type> <institution> Stanford University, </institution> <month> December </month> <year> 1983. </year> <note> MINOS 5.4 Release Notes, </note> <month> December </month> <year> 1992. </year>
Reference-contexts: It also has an advantage over artificial neural network (ANN) methods such as backpropagation [78] in that the training proceeds much faster [8]. The linear programming in the current version of MSM-T is implemented using the MINOS numerical optimization package <ref> [64] </ref>. 2.4.2 Exhaustive feature selection The 30-dimensional diagnosis data of benign and malignant points are linearly separable. Thus finding one MSM-T plane in this 30-dimensional feature space 19 is already an example of overfitting and leads to poor generalization. <p> The objective averages the errors over their respective classes. (Note: e is a vector of 1's of appropriate dimension.) As with MSM-T, the linear program is implemented using the MINOS numerical optimization software <ref> [64] </ref>. Because recurrences take place at some unknown time prior to their detection, we do not consider underestimated recurrent points to be as serious of an error as overestimated ones.
Reference: [65] <author> P. M. Narendra and K. Fukunaga. </author> <title> A branch and bound algorithm for feature subset selection. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-26(9):917-922, </volume> <month> September </month> <year> 1977. </year>
Reference-contexts: However, finding the optimal feature set is a combinatorial search problem. If framed as an integer programming problem, the search can be approximated with methods such as branch and bound <ref> [65] </ref>.
Reference: [66] <author> V. Pareto. Manuel d'Economie Politique. Giard, Paris, </author> <year> 1909. </year> <month> 94 </month>
Reference-contexts: The possible solutions to such a problem are known as efficient or Pareto optimal <ref> [66] </ref> points. At a Pareto optimal point, none of the f i (x) can be further lowered without increasing the value of some other f j (x).
Reference: [67] <author> E. Parzen. </author> <title> On estimation of a probability density and mode. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 35 </volume> <pages> 1065-1076, </pages> <year> 1962. </year>
Reference-contexts: A graphical depiction of this process is shown in 21 onto the normal to the separating plane. Separate histograms are then formed for the benign and malignant points along the normal. 22 Two probability density functions (relative to the classifier) were then approximated using a Parzen window <ref> [67, 69] </ref> or kernel technique. In general, kernel techniques assign each sample a probability mass centered on the point and falling away gradually in both directions. <p> Histograms are then built to count the number of cases in each class which were predicted to recur at all possible times. For clearer presentation, we again use a Parzen window kernel technique <ref> [67] </ref> as in the diagnosis probability density estimations in Chapter 2. Results for endpoints of two years and five years are shown in Figures 3.6 and 3.7. years versus the RSA feature-based prediction of time to recur xw + fl.
Reference: [68] <author> K. J. Pienta and D. S. Coffey. </author> <title> Correlation of nuclear morphometry with progression of breast cancer. </title> <journal> Cancer, </journal> <volume> 68 </volume> <pages> 2012-2016, </pages> <year> 1991. </year>
Reference-contexts: Among other patients, both increased tumor size and an increased number of cancerous lymph nodes (near the tumor) have been shown to be negative prognostic factors [19]. Nuclear features observed at the time of diagnosis have also been shown to be correlated with prognosis <ref> [12, 68] </ref>. Prognosis in the TNM model is expressed as survival curves, that is, plots 3 of time vs. probability of survival. Values of tumor size and lymph node status are discretized into a small number of intervals, and bins are created by plotting them against each other.
Reference: [69] <author> W. L. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling. </author> <title> Numerical Recipes. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference-contexts: A graphical depiction of this process is shown in 21 onto the normal to the separating plane. Separate histograms are then formed for the benign and malignant points along the normal. 22 Two probability density functions (relative to the classifier) were then approximated using a Parzen window <ref> [67, 69] </ref> or kernel technique. In general, kernel techniques assign each sample a probability mass centered on the point and falling away gradually in both directions. <p> In this experiment, we added time 2 ; 1 time ; and 1 time 2 . This adds several complications: * The predicted TTR now requires finding the root of a nonlinear set of equations. This was handled with a safe Newton-Raphson method, as implemented in <ref> [69] </ref>. However, the resulting root may or may not be unique. This was handled in the following experiments by beginning the search for roots near the median recurrence time and working outward to both extremes.
Reference: [70] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: Methods such as linear discriminant analysis, logistic regression, and nearest-neighbor classification, as described in [25], are widely used. Another popular concept representation which is more relevant to our work is that of decision trees. This paradigm has been used extensively in work from both machine learning <ref> [70, 73, 87] </ref> and statistics [14]. Each of these methods carries with it a particular inductive bias; that is, a tendency to form one type of concept over another. <p> Connectionist methods such as optimal brain damage [48] and cascade correlation [28] prevent overfitting by searching for an appropriate neural network architecture. Similarly, decision trees learned by the algorithms CART [14] or C4.5 <ref> [70, 73] </ref> are pruned to remove potentially extraneous decision branches. While biasing a learning system toward simpler concepts makes the accuracy more reliably measurable [13] and often improves generalization on real-world data, it should not be interpreted as a universally desirable bias [80, 98].
Reference: [71] <author> J. R. Quinlan. </author> <title> Decision trees as probabilistic classifiers. </title> <booktitle> In Proceedings of Fourth International Workshop on Machine Learning, </booktitle> <address> Los Altos, CA, 1987. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The resulting planes can then be used in the manner of a decision tree to classify new points. MSM-T has been shown to learn concepts as well or better than more traditional learning methods such as C4.5 <ref> [71, 72] </ref> and CART [14]. It also has an advantage over artificial neural network (ANN) methods such as backpropagation [78] in that the training proceeds much faster [8].
Reference: [72] <author> J. R. Quinlan. </author> <title> Simplifying decision trees. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 27, </volume> <year> 1987. </year>
Reference-contexts: The resulting planes can then be used in the manner of a decision tree to classify new points. MSM-T has been shown to learn concepts as well or better than more traditional learning methods such as C4.5 <ref> [71, 72] </ref> and CART [14]. It also has an advantage over artificial neural network (ANN) methods such as backpropagation [78] in that the training proceeds much faster [8].
Reference: [73] <author> J. Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kauf-mann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: Methods such as linear discriminant analysis, logistic regression, and nearest-neighbor classification, as described in [25], are widely used. Another popular concept representation which is more relevant to our work is that of decision trees. This paradigm has been used extensively in work from both machine learning <ref> [70, 73, 87] </ref> and statistics [14]. Each of these methods carries with it a particular inductive bias; that is, a tendency to form one type of concept over another. <p> Connectionist methods such as optimal brain damage [48] and cascade correlation [28] prevent overfitting by searching for an appropriate neural network architecture. Similarly, decision trees learned by the algorithms CART [14] or C4.5 <ref> [70, 73] </ref> are pruned to remove potentially extraneous decision branches. While biasing a learning system toward simpler concepts makes the accuracy more reliably measurable [13] and often improves generalization on real-world data, it should not be interpreted as a universally desirable bias [80, 98]. <p> The mock generalization results used 50% of the training data for tuning, and weighted the training and tuning sets equally. Tests were performed both with and without a pruning procedure which is implemented in MSM-T to remove spurious subtrees. This pruning is borrowed from the C4.5 decision tree algorithm <ref> [73] </ref>, and involves a statistical test of significance for each decision node. Mock generalization improved the MSM-T testing results both with and without the pruning procedure. The most noticeable improvement is in the reduction of the variance of the estimated correctness.
Reference: [74] <author> P. M. Ravdin and G. M. Clark. </author> <title> A practical application of neural network analysis for predicting outcome of individual breast cancer patients. </title> <journal> Breast Cancer Research and Treatment, </journal> <volume> 22 </volume> <pages> 285-293, </pages> <year> 1992. </year>
Reference-contexts: Machine learning methods have also been applied to prognosis. Burke [16] uses a traditional backpropagation network to predict 10-year survival in breast cancer patients. A self-organizing neural architecture was used by Schenone et al. [81] to predict recurrence time. Ravdin and Clark <ref> [74] </ref> include time as one of the input features of an ANN to predict a survival curve for each patient (see Chapter 4). <p> Here a separation methodology is used in the space of f eatures fi time, with the result again being a plane which can be used to predict recurrence. The data manipulation used here builds upon the work of Ravdin and colleagues <ref> [23, 74, 75] </ref>, who use a neural network to learn survival curves. Before constructing the predictive surface, the training examples must be pre-processed in order to successfully frame the prognosis problem as one of separation. <p> In this manner the level 0 predictors could operate on sets of features which "belong together." Of all the RSA extensions described in this chapter, the implicit RSA demonstrated the most improvement in predictive power. One possible extension of this approach, again similar to Ravdin and Clark <ref> [74] </ref>, is to screen the generated examples to reduce the large majority of non-recurrent cases for small values of time and the similar preponderance of recurrent cases for later times.
Reference: [75] <author> P. M. Ravdin, A. K. Tandon, D. C. Allred, G. M. Clark, S. A. W. Fuqua, S. H. Hilsenbeck, G. C. Chamness, and C. K. Osborne. </author> <title> Cathepsin D by western blotting and immunohistochemistry: Failure to confirm correlations with prognosis in node-negative breast cancer. </title> <journal> Journal of Clinical Oncology, </journal> <volume> 12 </volume> <pages> 467-474, </pages> <year> 1994. </year>
Reference-contexts: Ravdin and Clark [74] include time as one of the input features of an ANN to predict a survival curve for each patient (see Chapter 4). This method has also been used to verify the value of new prognostic features <ref> [75] </ref>. 1.2.3 Linear programming approaches Mathematical optimization approaches, in particular linear programming [22], have long been used in problems of pattern separation. Highleyman [37] first found that showing linear separability is equivalent to to finding a nonnegative solution to a set of linear equalities. <p> Here a separation methodology is used in the space of f eatures fi time, with the result again being a plane which can be used to predict recurrence. The data manipulation used here builds upon the work of Ravdin and colleagues <ref> [23, 74, 75] </ref>, who use a neural network to learn survival curves. Before constructing the predictive surface, the training examples must be pre-processed in order to successfully frame the prognosis problem as one of separation.
Reference: [76] <author> A. Robel. </author> <title> The dynamic pattern selection algorithm: Effective training and controlled generalization of backpropagation neural networks. </title> <type> Technical Report 93/23, </type> <institution> Technische Universitat Berlin, </institution> <year> 1994. </year> <month> 95 </month>
Reference-contexts: Since little information from the tuning set is being utilized, it is reasonable to maximize the size of the training set. Other methods which take fuller advantage of the tuning set <ref> [76] </ref> split the data evenly between training and tuning. Mock generalization shares more with these latter approaches, as it attempts to use all the information available in both training and tuning sets. The proper size is further influenced by the fact that we are working with relatively small data sets.
Reference: [77] <author> Asim Roy, Lark Sang Kim, and Somnath Mukhopadhyay. </author> <title> A polynomial time algorithm for the construction and training of a class of multilayer perceptrons. </title> <booktitle> Neural Networks, </booktitle> <volume> 6 </volume> <pages> 535-545, </pages> <year> 1993. </year>
Reference-contexts: More recently, Roy et al. <ref> [77] </ref> and Mukhopadhyay et al. [63] used linear programming to generate convex "covers" which are combined to classify general patterns. 1.3 Review of previous work at Wisconsin The application of machine learning techniques to problems in breast cancer diagnosis at the University of Wisconsin began in 1989 with the collaborative work
Reference: [78] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart and J. L. Mc-Clelland, editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> volume 1, chapter 8. </volume> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: There exist many different approaches for learning the function f , and many different ways to represent it. In recent years, one of the most popular of these approaches has been artificial neural networks (ANN's), or connectionist methods. In particular, feed-forward ANN's trained with algorithms such as backpropagation <ref> [78] </ref> have been shown to be an extremely flexible and powerful approach to function approximation [39, 50, 79]. Historically, the field of statistical pattern recognition, as summarized in Duda and Hart [25], contributes much to our understanding of such problems. <p> MSM-T has been shown to learn concepts as well or better than more traditional learning methods such as C4.5 [71, 72] and CART [14]. It also has an advantage over artificial neural network (ANN) methods such as backpropagation <ref> [78] </ref> in that the training proceeds much faster [8]. The linear programming in the current version of MSM-T is implemented using the MINOS numerical optimization package [64]. 2.4.2 Exhaustive feature selection The 30-dimensional diagnosis data of benign and malignant points are linearly separable. <p> See Equation 3.2. * Modified backpropagation: We also evaluated an artificial neural network (ANN) using a modified version of back-propagation <ref> [78] </ref>. The output unit for our ANN used the identity function as its response rather than the familiar sigmoid, allowing any real-valued output. The error function was also changed to the one-sided errors as used in the RSA; learning took place only on underestimated non-recurrent cases and overestimated recurrent cases.
Reference: [79] <author> D. E. Rumelhart and J. L. McClelland. </author> <title> Parallel Distributed Processing. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: In recent years, one of the most popular of these approaches has been artificial neural networks (ANN's), or connectionist methods. In particular, feed-forward ANN's trained with algorithms such as backpropagation [78] have been shown to be an extremely flexible and powerful approach to function approximation <ref> [39, 50, 79] </ref>. Historically, the field of statistical pattern recognition, as summarized in Duda and Hart [25], contributes much to our understanding of such problems. Methods such as linear discriminant analysis, logistic regression, and nearest-neighbor classification, as described in [25], are widely used.
Reference: [80] <author> C. Schaffer. </author> <title> Overfitting avoidance as bias. </title> <journal> Machine Learning, </journal> <volume> 10 </volume> <pages> 153-178, </pages> <year> 1993. </year>
Reference-contexts: While biasing a learning system toward simpler concepts makes the accuracy more reliably measurable [13] and often improves generalization on real-world data, it should not be interpreted as a universally desirable bias <ref> [80, 98] </ref>.
Reference: [81] <author> A. Schenone, L. Andreucci, V. Sanguinetti, and P. </author> <title> Morasso. Neural networks for prognosis in breast cancer. </title> <journal> Physica Medica, </journal> <volume> IX(Supplement </volume> 1):175-178, June 1993. 
Reference-contexts: Still, few of these systems have received general clinical usage. Machine learning methods have also been applied to prognosis. Burke [16] uses a traditional backpropagation network to predict 10-year survival in breast cancer patients. A self-organizing neural architecture was used by Schenone et al. <ref> [81] </ref> to predict recurrence time. Ravdin and Clark [74] include time as one of the input features of an ANN to predict a survival curve for each patient (see Chapter 4).
Reference: [82] <author> J. Sietsma and R. J. F. Dow. </author> <title> Neural net pruning why and how. </title> <booktitle> In IEEE International Conference on Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 325-333, </pages> <address> New York, 1988. </address> <publisher> IEEE Press. </publisher>
Reference-contexts: This sections describes a more elegant and automatic solution, which incorporates feature selection into the learning algorithm as a means of parameter elimination. Thus we view feature selection as a means of finding the optimal number of parameters, in the spirit of neural network algorithms such as pruning <ref> [82] </ref>, weight decay [38] and optimal brain damage [48]. Essential to the feature-selection algorithm is the fact that the features have all been normalized to lie in approximately the same range.
Reference: [83] <author> R. E. Steuer. </author> <title> Multiple Criteria Optimization: Theory, Computation, and Application. </title> <publisher> John Wiley and Sons, </publisher> <year> 1986. </year>
Reference-contexts: since = 0 reverts the problem to the original MSM-T and the values &gt; 0:5 are symmetric, with the roles of the training and tuning sets reversed. 5.2 Relationship to Pareto optimality Considering the tuning and training errors separately in the objective function suggests the field of multi-objective mathematical programming <ref> [83] </ref>. This area 72 of optimization considers problems of the form vector min x2S 6 6 6 6 f 1 (x) . . . 3 7 7 7 5 Here, multiple, possibly competing objectives are to be optimized.
Reference: [84] <author> M. Stone. </author> <title> Cross-validatory choice and assessment of statistical predictions. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> 36 </volume> <pages> 111-147, </pages> <year> 1974. </year>
Reference-contexts: This procedure is repeated m times until each training point has been used for testing. The result is a very accurate and nearly unbiased, but computationally expensive, accuracy estimate. A 5 more tractable re-sampling method is cross-validation <ref> [84] </ref>, which successively removes, say, 10% of the data for testing, again using each point in exactly one test set. Other statistical techniques, such as jackknifing and bootstrap [27], are also available.
Reference: [85] <author> W. N. </author> <title> Street. Toward automated cancer diagnosis: An interactive system for cell feature extraction. </title> <type> Technical Report 1052, </type> <institution> Computer Sciences Department, University of Wisconsin, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: We tried several different approaches, including thresholding, region growing [3] and contour following [18]. Various commercial and public domain image analysis systems utilize these methods. 1 Our chosen solution <ref> [85, 86, 95] </ref> is a semi-automatic segmentation procedure [43] known as "snakes." Beginning with a user-defined approximate boundary as an initialization (see Figure 2.1) the snake locates the actual boundary of the cell nucleus.
Reference: [86] <author> W. N. Street, W. H. Wolberg, and O. L. Mangasarian. </author> <title> Nuclear feature extraction for breast tumor diagnosis. </title> <booktitle> In IS&T/SPIE 1993 International 96 Symposium on Electronic Imaging: Science and Technology, volume 1905, </booktitle> <pages> pages 861-870, </pages> <address> San Jose, California, </address> <year> 1993. </year>
Reference-contexts: We tried several different approaches, including thresholding, region growing [3] and contour following [18]. Various commercial and public domain image analysis systems utilize these methods. 1 Our chosen solution <ref> [85, 86, 95] </ref> is a semi-automatic segmentation procedure [43] known as "snakes." Beginning with a user-defined approximate boundary as an initialization (see Figure 2.1) the snake locates the actual boundary of the cell nucleus. <p> energy function, the snakes have been adapted to this particular application in two ways: the directional edge detector and the elliptical curvature assumption. 2.3 Extraction of digital nuclear features In order to evaluate the size, shape and texture of the cell nuclei, we arrived at a set of ten features <ref> [86, 94] </ref> which are computed for each cell. Some of these replicate or approximate features previously evaluated subjectively by Wolberg [92], while others are unique to this work. All of the size and shape features were verified using idealized phantom cells [97]. The computed features are as follows. 1.
Reference: [87] <author> P. E. Utgoff. </author> <title> Incremental induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 4 </volume> <pages> 161-186, </pages> <year> 1989. </year>
Reference-contexts: Methods such as linear discriminant analysis, logistic regression, and nearest-neighbor classification, as described in [25], are widely used. Another popular concept representation which is more relevant to our work is that of decision trees. This paradigm has been used extensively in work from both machine learning <ref> [70, 73, 87] </ref> and statistics [14]. Each of these methods carries with it a particular inductive bias; that is, a tendency to form one type of concept over another.
Reference: [88] <author> G. Wahba. </author> <title> Bayesian confidence intervals for the cross-validated smoothing spline. </title> <journal> Journal of the Royal Statistical Society (Series B), </journal> <volume> 45 </volume> <pages> 133-150, </pages> <year> 1983. </year>
Reference-contexts: The tolerance bands used in training are shown above and below the learned curve. This approach to function estimation can be seen as the inverse of the statistical practice of constructing confidence bands after approximating the function <ref> [24, 30, 88] </ref>. We also note that in the field of approximation theory, similar ideas have been used by letting equality constraints of the problem be replaced by inequalities [56]. 80 6.2 Simulation results The banded approximation idea was first tested with a simple curve fitting application, using simulated data.
Reference: [89] <author> G. L. Wied, P. H. Bartels, M. Bibbo, and H. E. Dytch. </author> <title> Image analysis in quantitative cytopathology and histopathology. </title> <booktitle> Human Pathology, </booktitle> <volume> 20(6) </volume> <pages> 549-571, </pages> <year> 1989. </year>
Reference-contexts: Patients who choose not to have the biopsy done are followed for a year at one-month intervals to check for changes in the tumor. 2.7 Discussion and extensions While the methodology of applying machine learning techniques to features extracted from medical images is certainly not new <ref> [4, 89] </ref>, this work is unique in several respects. First is the precise quantification of nuclear shape, using several different features. This should make the Xcyt system more generally applicable to new situations where different representations of shape are relevant to outcome.
Reference: [90] <author> D. J. Williams and M. Shah. </author> <title> A fast algorithm for active contours. </title> <booktitle> In Proceedings of the Third International Conference on Computer Vision, </booktitle> <pages> pages 592-595, </pages> <address> Osaka, Japan, </address> <month> December </month> <year> 1990. </year>
Reference-contexts: The continuity term does not determine shape, but does prevent snake points from bunching together near areas of sharpest gray scale contrast. 12 In order to control computation time, the optimal local value of the energy function is approximated using a greedy algorithm due to Williams and Shah <ref> [90] </ref>. If the function value at a particular snake point can be lowered by moving the point to an adjacent pixel, then it is moved, thus possibly affecting the energy computation at other points.
Reference: [91] <author> W. H. Wolberg, K. P. Bennett, and O. L. Mangasarian. </author> <title> Breast cancer diagnosis and prognostic determination from cell analysis. </title> <type> Manuscript, </type> <institution> Departments of Surgery and Human Oncology and Computer Sciences, University of Wisconsin, </institution> <year> 1992. </year>
Reference-contexts: Subsequent work by Mangasarian with K. P. Ben-nett [7, 10], including the development of the MSM-Tree (MSM-T) algorithm (described in Chapter 2), resulted in improved diagnostic results. The problem of cancer prognosis was also cast as a classification problem <ref> [91, 96] </ref>, separating those patients who experienced recurrence in less than two years from those known to have been disease free for at least two years. Chapter 2 Breast cancer diagnosis: Xcyt 2.1 Introduction The first problem we address is that of diagnosing breast masses as benign or malignant. <p> In particular, recurrence or survival data is right censored, i.e., the right endpoint (recurrence time) is sometimes unknown. Previous work at Wisconsin <ref> [91, 96] </ref> and elsewhere [16] has framed prognosis as a classification problem by choosing a particular endpoint, such as two years or five years. The more sophisticated work of Ravdin et al. is discussed in Chapter 4.
Reference: [92] <author> W. H. Wolberg and O. L. Mangasarian. </author> <title> Multisurface method of pattern separation for medical diagnosis applied to breast cytology. </title> <booktitle> Proceedings of the National Academy of Sciences, U.S.A., </booktitle> <volume> 87 </volume> <pages> 9193-9196, </pages> <year> 1990. </year>
Reference-contexts: H. Wolberg (Surgery and Human Oncology) and O. L. Mangasar-ian (Computer Sciences). The Multisurface Method (MSM) [53, 54] of pattern separation was first applied to a collection of cases represented by nine subjectively evaluated cytological features <ref> [57, 58, 92] </ref>. The MSM procedure uses a linear programming model to place successive pairs of separating planes in the feature space of the input examples, building a piecewise-linear separating surface. The procedure can also be considered a neural network training algorithm [9]. <p> Some of these replicate or approximate features previously evaluated subjectively by Wolberg <ref> [92] </ref>, while others are unique to this work. All of the size and shape features were verified using idealized phantom cells [97]. The computed features are as follows. 1.
Reference: [93] <author> W. H. Wolberg and O. L. Mangasarian. </author> <title> Computer-designed expert systems for breast cytology diagnosis. </title> <journal> Analytical and Quantitative Cytology and Histology, </journal> <volume> 15(1) </volume> <pages> 67-74, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: The MSM procedure uses a linear programming model to place successive pairs of separating planes in the feature space of the input examples, building a piecewise-linear separating surface. The procedure can also be considered a neural network training algorithm [9]. While successful <ref> [93] </ref>, the diagnostic results still depended on the ability to subjectively assign values to input features, and were therefore difficult to replicate. Further, the classifier employed was relatively complex, employing four pairs of planes in 9-space. Subsequent work by Mangasarian with K. P.
Reference: [94] <author> W. H. Wolberg, W. N. Street, D. H. Heisey, and O. L. Mangasarian. </author> <title> Computer-derived nuclear features distinguish malignant from benign breast cytology. Cancer, </title> <note> submitted, 1994. 97 </note>
Reference-contexts: energy function, the snakes have been adapted to this particular application in two ways: the directional edge detector and the elliptical curvature assumption. 2.3 Extraction of digital nuclear features In order to evaluate the size, shape and texture of the cell nuclei, we arrived at a set of ten features <ref> [86, 94] </ref> which are computed for each cell. Some of these replicate or approximate features previously evaluated subjectively by Wolberg [92], while others are unique to this work. All of the size and shape features were verified using idealized phantom cells [97]. The computed features are as follows. 1.
Reference: [95] <author> W. H. Wolberg, W. N. Street, and O. L. Mangasarian. </author> <title> Breast cytology diagnosis via digital image analysis. </title> <journal> Analytical and Quantitative Cytology and Histology, </journal> <volume> 15(6) </volume> <pages> 396-404, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: We tried several different approaches, including thresholding, region growing [3] and contour following [18]. Various commercial and public domain image analysis systems utilize these methods. 1 Our chosen solution <ref> [85, 86, 95] </ref> is a semi-automatic segmentation procedure [43] known as "snakes." Beginning with a user-defined approximate boundary as an initialization (see Figure 2.1) the snake locates the actual boundary of the cell nucleus.
Reference: [96] <author> W. H. Wolberg, W. N. Street, and O. L. Mangasarian. </author> <title> Computerized breast cancer diagnosis and prognosis from fine-needle aspirates. Analytical and Quantitative Cytology and Histology, </title> <note> submitted, </note> <year> 1994. </year>
Reference-contexts: Subsequent work by Mangasarian with K. P. Ben-nett [7, 10], including the development of the MSM-Tree (MSM-T) algorithm (described in Chapter 2), resulted in improved diagnostic results. The problem of cancer prognosis was also cast as a classification problem <ref> [91, 96] </ref>, separating those patients who experienced recurrence in less than two years from those known to have been disease free for at least two years. Chapter 2 Breast cancer diagnosis: Xcyt 2.1 Introduction The first problem we address is that of diagnosing breast masses as benign or malignant. <p> This variation lies along two dimensions: the nature of the cells selected by a particular operator, and how demanding the operator is regarding the converged snakes. We have examined the extent of this variation <ref> [96] </ref> and found that computation of shape features can vary by up to 10% among different users. One way to avoid this type of variation is to calibrate the system for each new user. <p> In particular, recurrence or survival data is right censored, i.e., the right endpoint (recurrence time) is sometimes unknown. Previous work at Wisconsin <ref> [91, 96] </ref> and elsewhere [16] has framed prognosis as a classification problem by choosing a particular endpoint, such as two years or five years. The more sophisticated work of Ravdin et al. is discussed in Chapter 4.
Reference: [97] <author> W. H. Wolberg, W. N. Street, and O. L. Mangasarian. </author> <title> Machine learning techniques to diagnose breast cancer from image-processed nuclear features of fine needle aspirates. </title> <journal> Cancer Letters, </journal> <volume> 77 </volume> <pages> 163-171, </pages> <year> 1994. </year>
Reference-contexts: Some of these replicate or approximate features previously evaluated subjectively by Wolberg [92], while others are unique to this work. All of the size and shape features were verified using idealized phantom cells <ref> [97] </ref>. The computed features are as follows. 1. Radius Radius is computed by averaging the length of radial line segments, that is, lines from the center of mass of the snake to each of the snake points. See Figure 2.3. 2.
Reference: [98] <author> D. H. Wolpert. </author> <title> On overfitting as bias. </title> <type> Technical Report TR 92-03-5001, </type> <institution> The Santa Fe Institute, </institution> <year> 1992. </year>
Reference-contexts: While biasing a learning system toward simpler concepts makes the accuracy more reliably measurable [13] and often improves generalization on real-world data, it should not be interpreted as a universally desirable bias <ref> [80, 98] </ref>.
Reference: [99] <author> D. H. Wolpert. </author> <title> Stacked generalization. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 241-259, </pages> <year> 1992. </year>
Reference-contexts: A new RSA is then trained in this augmented 52 53 space. * Stacked RSA: This method is an adaptation of Wolpert's Stacked Generalization algorithm <ref> [99] </ref> and uses cross-validation to estimate the errors of the RSA procedure and attempts to correct them with a second-level predictor. <p> Reported results are the average of five 10-fold cross-validation tests. Results from the original RSA are included for reference. 61 4.2 Stacked RSA (SRSA) This experiment evaluates several variations of Wolpert's Stacked Generalization method <ref> [99] </ref>. The idea is to use a second-level generalizer (level 1, in Wolpert's terminology) to learn the errors being made by one or more initial generalizers (at level 0).
Reference: [100] <author> Y. Wu, M. L. Giger, K. Doi, C. J. Vyborny, R. A. Schmidt, and C. E. Metz. </author> <title> Artificial neural networks in mammography: Application to decision making in the diagnosis of breast cancer. </title> <journal> Radiology, </journal> <volume> 187 </volume> <pages> 81-87, </pages> <year> 1993. </year>
Reference-contexts: In breast cancer, different researchers have applied neural networks to diagnosing from mammograms <ref> [100] </ref>, ultrasound images [34], and pathological markers [2]. Still, few of these systems have received general clinical usage. Machine learning methods have also been applied to prognosis. Burke [16] uses a traditional backpropagation network to predict 10-year survival in breast cancer patients.
Reference: [101] <author> Y. O. Yoon, R. W. Brobst, P. R. Bergstresser, and L. L. Peterson. </author> <title> A desktop neural network for dermatology diagnosis. </title> <journal> Journal of Neural Network Computation, </journal> <pages> pages 43-52, </pages> <month> Summer </month> <year> 1989. </year>
Reference-contexts: a greedy, 6 directed search through the space of possible feature sets [24, 44, 60], where only a small manageable set of variable subsets are examined. 1.2.2 Medical applications Machine learning approaches particularly ANN's have been used for a number of disparate clinical diagnosis tasks; diagnosing, for example, skin lesions <ref> [101] </ref>, appendicitis [26], and myocardial infarction [5]. In breast cancer, different researchers have applied neural networks to diagnosing from mammograms [100], ultrasound images [34], and pathological markers [2]. Still, few of these systems have received general clinical usage. Machine learning methods have also been applied to prognosis.
References-found: 101

