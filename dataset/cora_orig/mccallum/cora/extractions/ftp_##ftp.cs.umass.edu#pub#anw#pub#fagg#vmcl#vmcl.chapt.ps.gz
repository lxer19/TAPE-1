URL: ftp://ftp.cs.umass.edu/pub/anw/pub/fagg/vmcl/vmcl.chapt.ps.gz
Refering-URL: http://www-anw.cs.umass.edu/~fagg/papers.html
Root-URL: 
Abstract-found: 0
Intro-found: 1
Reference: <author> Amari, S., & Arbib, M. A. </author> <year> (1977). </year> <title> Competition and Cooperation in Neural Nets. </title> <editor> In J. Metzler (Eds.), </editor> <publisher> Systems Neuroscience Academic Press. </publisher>
Reference-contexts: The inhibitory unit ensures that when the system has reached an equilibrium point, at most one motor program has become selected. This style of distributed Winner-Take-All computation is due to <ref> (Amari & Arbib, 1977) </ref>. The noise signal is important at this point for providing a diversity in the search for the correct mapping. In addition, it helps to prevent the system from becoming stuck onto a saddle point, where it cannot decide between one of two equally-active motor selection units.
Reference: <author> Arbib, M. A. </author> <year> (1989). </year> <title> The Metaphorical Brain 2: Neural Networks and Beyond. </title> <address> New York: </address> <note> Wiley Fagg and Arbib: Visual-Motor Conditional Learning 2 9 Interscience. </note>
Reference-contexts: Finally, the noise vectors, noise and motor_noise (which are injected into the voting and motor selection units, respectively), are initialized (1.11). Network Dynamics The following RUN_MODULEs implement a single timestep of the simulation. Each of the neurons are implemented as leaky-integrators <ref> (Arbib, 1989) </ref>. RUN_MODULE (FEATURE) - diff (feature_mem, u_feature, -feature_mem - threshold_f + (2.1) mat_mult_col_vec (W_in_feature, inputs)); feature = NSLsat (feature_mem, 0, 1, 0, 1); (2.2) - The FEATURE module computes the membrane potential (feature_mem) and the firing rate (fe at u re ) of the feature detector units.
Reference: <author> Barto, A. G., Sutton, R. S., & Anderson, C. W. </author> <year> (1983). </year> <title> Neuron-like Adaptive Elements That Can Solve Difficult Learning Control Problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> SMC-5, </volume> <pages> 834-46. </pages>
Reference-contexts: Moreover, rather than always selecting the motor program that has the highest incoming feature support, the system is enabled by the noise to choose other possibilities. This keeps the system from prematurely committing to an incorrect solution, maintaining diversity during the search process <ref> (Barto, Sutton, & Anderson, 1983) </ref>. Thus, the amount of time dedicated to the search process can be significantly decreased. <p> This method works, in general, because this incorrect column will always be active in conjunction with several other columns that do vote appropriately and are always able to overrule its vote. This scheme is similar to that used by <ref> (Barto, et al., 1983) </ref> in that one or more elements may correct for errors made by another element. In their case, however, the correction is made sequentially through time, rather than in parallel.
Reference: <author> Grossberg, S. </author> <year> (1976). </year> <title> A Theory of Visual Coding, Memory, and Development: Part 1. Parallel Development and Coding of Neural Feature Detectors. </title> <journal> Biological Cybernetics, </journal> <volume> 23, </volume> <pages> 121-134. </pages>
Reference-contexts: The Fagg and Arbib: Visual-Motor Conditional Learning 1 8 feature detecting algorithm is related to the competitive learning of von der Malsburg (von der Malsburg, 1973) and Grossberg <ref> (Grossberg, 1976) </ref> (discussed further in (Rumelhart & Zipser, 1986)). Individual columns learn to become feature detectors for specific subpatterns of the visual stimulus. However, a column does not recognize a pattern to the exclusion of other patterns. Instead, several columns participate in the recognition at once.
Reference: <author> Hinton, G. E., & Sejnowski, T. J. </author> <year> (1986). </year> <title> Learning and Relearning in Boltzmann Machines. </title> <editor> In J. </editor> <publisher> L. </publisher>
Reference-contexts: A complete model will also reproduce the temporal activity of various neurons in the premotor cortex: anticipatory units, signal-related units, set-related units, and movement-related units. 2. Model Much of neural network research has concentrated upon supervised learning techniques, such as the generalized delta rule (backpropagation, <ref> (Rumelhart, Hinton, & Williams, 1986) </ref>). In our modelling efforts, we have chosen to explore other algorithms within an architecture that can be related (at least at a high level) to the biological architecture, while perhaps also offering greater computational capability. <p> Noise processes have been used as an active element of several neural models. Noise is used in Boltzmann machines as a device for escaping local minima and as a way of breaking symmetry between two possible solution paths <ref> (Hinton & Sejnowski, 1986) </ref>. Although the problem of local minima is not a concern in this work, the problem of choosing between two equally desirable solutions is a considerable one.
Reference: <editor> McClelland & D. E. Rumelhart (Eds.), </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <volume> Volume 1: </volume> <pages> Foundations (pp. 282-317). </pages> <publisher> Bradford Book/The MIT Press. </publisher>
Reference: <author> Mitz, A. R., Godshalk, M., & Wise, S. P. </author> <year> (1991). </year> <title> Learning-Dependent Neuronal Activity in the Premotor Cortex. </title> <journal> Journal of Neuroscience, </journal> <volume> 11(6), </volume> <pages> 1855-72. </pages>
Reference-contexts: 1. Introduction Mitz, Godshalk and Wise <ref> (Mitz, Godshalk, & Wise, 1991) </ref> examine learning-dependent activity in the premotor cortex of two rhesus monkeys required to move a lever in a particular direction in response to a specific visual stimulus. Figure 1 shows the protocol and expected response for one such trial.
Reference: <author> Rumelhart, D. E., Hinton, G. E., & Williams, R. J. </author> <year> (1986). </year> <title> Learning Internal Representations by Error Propagation. </title> <editor> In D. Rumelhart & J. McClelland (Eds.), </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition (pp. </booktitle> <pages> 318-62). </pages> <publisher> Bradford Books/MIT Press. </publisher>
Reference-contexts: A complete model will also reproduce the temporal activity of various neurons in the premotor cortex: anticipatory units, signal-related units, set-related units, and movement-related units. 2. Model Much of neural network research has concentrated upon supervised learning techniques, such as the generalized delta rule (backpropagation, <ref> (Rumelhart, Hinton, & Williams, 1986) </ref>). In our modelling efforts, we have chosen to explore other algorithms within an architecture that can be related (at least at a high level) to the biological architecture, while perhaps also offering greater computational capability. <p> The Fagg and Arbib: Visual-Motor Conditional Learning 1 8 feature detecting algorithm is related to the competitive learning of von der Malsburg (von der Malsburg, 1973) and Grossberg (Grossberg, 1976) (discussed further in <ref> (Rumelhart & Zipser, 1986) </ref>). Individual columns learn to become feature detectors for specific subpatterns of the visual stimulus. However, a column does not recognize a pattern to the exclusion of other patterns. Instead, several columns participate in the recognition at once.
Reference: <author> Rumelhart, D. E., & Zipser, D. </author> <year> (1986). </year> <title> Feature Discovery by Competitive Learning. </title> <editor> In J. L. McClelland & D. E. Rumelhart (Eds.), </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition Volume 1: </booktitle> <pages> Foundations (pp. 151-193). </pages> <publisher> Bradford Books/The MIT Press. </publisher>
Reference-contexts: A complete model will also reproduce the temporal activity of various neurons in the premotor cortex: anticipatory units, signal-related units, set-related units, and movement-related units. 2. Model Much of neural network research has concentrated upon supervised learning techniques, such as the generalized delta rule (backpropagation, <ref> (Rumelhart, Hinton, & Williams, 1986) </ref>). In our modelling efforts, we have chosen to explore other algorithms within an architecture that can be related (at least at a high level) to the biological architecture, while perhaps also offering greater computational capability. <p> The Fagg and Arbib: Visual-Motor Conditional Learning 1 8 feature detecting algorithm is related to the competitive learning of von der Malsburg (von der Malsburg, 1973) and Grossberg (Grossberg, 1976) (discussed further in <ref> (Rumelhart & Zipser, 1986) </ref>). Individual columns learn to become feature detectors for specific subpatterns of the visual stimulus. However, a column does not recognize a pattern to the exclusion of other patterns. Instead, several columns participate in the recognition at once.
Reference: <author> Weitzenfeld, A. </author> <year> (1991). </year> <title> NSL - Neural Simulation Language Version 2.1 (TR No. </title> <institution> CNE-91-05). Center for Neural Engineering, University of Southern California, </institution> <address> Los Angeles, CA. </address> <note> Fagg and Arbib: Visual-Motor Conditional Learning 3 0 </note>
Reference-contexts: In addition, we would like to thank Rob Redekopp for his aid in performing some of the backpropagation experiments. Our model has been implemented in the Neural Simulation Language (NSL) <ref> (Weitzenfeld, 1991) </ref>, and executes on Sun workstations. Both NSL and this model are available via anonymous ftp from the Brain Simulation Laboratory at USC (yorick.usc.edu). For further information, email may be sent to the author: ahfagg@robotics.usc.edu.
References-found: 10

