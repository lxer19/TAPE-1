URL: http://www.cs.rice.edu:80/~mpal/papers/Frontiers95.ps
Refering-URL: http://www.cs.rice.edu:80/~mpal/papers/index.html
Root-URL: 
Title: Compiler Support for Out-of-Core Arrays on Parallel Machines  
Author: Michael Paleczny Ken Kennedy Charles Koelbel 
Address: Houston, TX 77005-1892  
Affiliation: Rice University, Department of Computer Science  
Abstract: We are investigating a compiler-based approach to the above problem. In general, our compiler techniques attempt to choreograph I/O for an application based on high-level programmer annotations similar to Fortran D's DECOMPOSITION, ALIGN, and DISTRIBUTE statements. The central problem is to generate "deferred routines" which delay computations until all the data they require have been read into main memory. We present the results for two applications, LU factorization and red-black relaxation, on 1 to 32 nodes of an Intel Paragon after hand application of these compiler techniques. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Walid Abu-Sufah. </author> <title> Improving the Performance of Virtual Memory Computers. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1979. </year>
Reference-contexts: This allows our work to focus on efficient deferred routines and I/O optimization. Previous compiler-related work has focused on transformations to improve virtual memory performance. Abu-Sufah <ref> [1] </ref> examined the application of loop distribution followed by loop fusion to improve locality. Trivedi [11] examined the potential benefits of programmer-inserted prefetching directives for matrix multiplication on the STAR computer and compiler support for demand prepaging on sequential machines [12].
Reference: [2] <author> J. R. Allen and K. Kennedy. </author> <title> Vector register allocation. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(10) </volume> <pages> 1290-1317, </pages> <month> Oc-tober </month> <year> 1992. </year>
Reference-contexts: This requires that sufficient memory is available in-core for two tiles. If insufficient memory is available for a profitable transformation the compiler can provide feedback to the programmer. This transformation is similar to alignment of vector operations to allow reuse <ref> [2] </ref>. Next, the compiler can optimize the inserted I/O statements. Dependence analysis results can determine when it is safe to overlap computation and I/O. A cost model must also be applied to determine that this is profitable, as the overlapping may require more main memory.
Reference: [3] <author> Rajesh Bordawekar, Juan Miguel del Rosario, and Alok Choudhary. </author> <title> Design and evaluation of primitives for parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 452-461, </pages> <address> Portland, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: When more processors are used, the compiler can keep additional tiles in-core until all the data is kept in memory. Input and output of these tiles is choreographed by the compiler. Initial and final data accesses can be remapped if necessary using either run-time routines <ref> [3] </ref> or compiler-generated I/O if the external distribution is known. 4 Compiling for out-of-core execution Our strategy for out-of-core compilation consists of three phases: program analysis, I/O insertion and optimization, and parallelization and communication optimization. <p> When the initial or final data is not compatible with the temporary storage distribution, or the programmer requests a redistribution during execution, the data must be remapped. This can be done with run-time routines extended for out-of-core data <ref> [3] </ref> or by explicit I/O interleaved with computation when the source and destination distribution are known at compile time. 4.3 Parallelization and communication optimization Finally, the parallelization phase compiles the transformed program for parallel machines. <p> We are extending this method to support the automatic construction of out-of-core parallel programs with compiler overlapped I/O and computation. Bordawekar, del Rosario, and Choudhary <ref> [3] </ref> designed a library of user-accessible primitives which are configured at runtime to the desired memory and disk distributions of the data. We believe such an I/O library can be used effectively by our approach. Thakur, Bordawekar and Choudhary [10] are working on compiler methods for out-of-core HPF programs.
Reference: [4] <author> Steve Carr and Ken Kennedy. </author> <title> Compiler blockability of numerical algorithms. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <address> Minneapolis, MN, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: Abu-Sufah [1] examined the application of loop distribution followed by loop fusion to improve locality. Trivedi [11] examined the potential benefits of programmer-inserted prefetching directives for matrix multiplication on the STAR computer and compiler support for demand prepaging on sequential machines [12]. A growing body of work <ref> [4, 9] </ref> examines similar concerns for cache memories. Cormen [5] developed efficient out-of-core permutation algorithms and examined their I/O requirements within a data-parallel virtual memory system (VM-DP).
Reference: [5] <author> T. H. Cormen. </author> <title> Virtual memory for data-parallel computing. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <year> 1992. </year>
Reference-contexts: Trivedi [11] examined the potential benefits of programmer-inserted prefetching directives for matrix multiplication on the STAR computer and compiler support for demand prepaging on sequential machines [12]. A growing body of work [4, 9] examines similar concerns for cache memories. Cormen <ref> [5] </ref> developed efficient out-of-core permutation algorithms and examined their I/O requirements within a data-parallel virtual memory system (VM-DP).
Reference: [6] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of inter-procedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Interprocedural data-flow analysis propagates the user I/O annotations in the same way as the Fortran D data mapping directives are handled. Code sections that access out-of-core data are identified as the sections that use the annotated arrays, and their data use is summarized using Regular Section Descriptors (RSDs) <ref> [6] </ref>. Again, this analysis is similar to the communications analysis performed by the Fortran D compiler.
Reference: [7] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Our approach is to develop compiler techniques that choreograph I/O for an application, specifically, the temporary storage and retrieval of out-of-core data during execution. A programmer will declare the desired organization of input and output with statements similar to Fortran D's DECOMPOSITION, ALIGN, and DISTRIBUTE <ref> [7] </ref>. These annotations allow a high-level description of the relationship between an array and its use in the computation. The compiler will use this information and static program analysis to segment the computation, construct appropriate I/O statements, and insert them in the program. <p> This ratio determines the amount of I/O which will be performed, but does not affect the amount of computation. 7 Related work Hiranandani, Kennedy and Tseng <ref> [7] </ref> described a compilation method for Fortran D programs based Memory Number of Processors Size 1 2 4 8 16 32 16 MB 24.4% 22.4% 23.8% 24.4% 19.3% 13.4% Table 2: Percent improvement in execution time between synchronous and asynchronous I/O for LU factorization. ( fl result unavailable) Matrix Total Execution
Reference: [8] <author> Ed Kushner. </author> <title> Optimizing I/O performance for the Paragon (tm) supercomputer. Intel On-Line, </title> <address> http://abacus.training.ssd.intel.com/InfoSelect/ bulletinV01N02.html, 1(2), </address> <month> August </month> <year> 1994. </year>
Reference-contexts: This is consistent with Intel's rec-ommendation <ref> [8] </ref>: "In general, the recommendation is to have one I/O node for every ten compute nodes." 6.2 LU factorization Our results for LU factorization are shown in Figure 4. One set of results is from nodes containing 16 MB of memory, the other from nodes with 32 MB of memory.
Reference: [9] <author> Todd C. Mowry. </author> <title> Tolerating Latency Through Software-Controlled Data Prefetching. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering, Stanford University, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Abu-Sufah [1] examined the application of loop distribution followed by loop fusion to improve locality. Trivedi [11] examined the potential benefits of programmer-inserted prefetching directives for matrix multiplication on the STAR computer and compiler support for demand prepaging on sequential machines [12]. A growing body of work <ref> [4, 9] </ref> examines similar concerns for cache memories. Cormen [5] developed efficient out-of-core permutation algorithms and examined their I/O requirements within a data-parallel virtual memory system (VM-DP).
Reference: [10] <author> Rajeev Thakur, Rajesh Bordawekar, and Alok Choudhary. </author> <title> Compiler and runtime support for out-of-core HPF programs. </title> <booktitle> In Proceedings of the 1994 ACM International Conference on Supercomputing, </booktitle> <pages> pages 382-391, </pages> <address> Manch-ester, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: Bordawekar, del Rosario, and Choudhary [3] designed a library of user-accessible primitives which are configured at runtime to the desired memory and disk distributions of the data. We believe such an I/O library can be used effectively by our approach. Thakur, Bordawekar and Choudhary <ref> [10] </ref> are working on compiler methods for out-of-core HPF programs. This has many similarities to our work, however, we believe our use of programmer I/O directives provides useful information to the compiler, the discovery of which is still a significant research problem.

Reference: [12] <author> Kishor S. Trivedi. </author> <title> On the paging performance of array algorithms. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-26(10):938-947, </volume> <month> October </month> <year> 1977. </year>
Reference-contexts: Abu-Sufah [1] examined the application of loop distribution followed by loop fusion to improve locality. Trivedi [11] examined the potential benefits of programmer-inserted prefetching directives for matrix multiplication on the STAR computer and compiler support for demand prepaging on sequential machines <ref> [12] </ref>. A growing body of work [4, 9] examines similar concerns for cache memories. Cormen [5] developed efficient out-of-core permutation algorithms and examined their I/O requirements within a data-parallel virtual memory system (VM-DP).
Reference: [13] <author> David Womble, David Greenberg, Stephen Wheat, and Rolf Riessen. </author> <title> Beyond core: Making parallel computer I/O practical. </title> <booktitle> In DAGS93, </booktitle> <address> Hanover, NH, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: In addition, increasing the size of a data set typically increases the amount of computation. This makes parallel machines attractive for out-of-core problems. Unfortunately, parallel programming is also complex on current machines. Work at Sandia National Lab <ref> [13] </ref> on parallel out-of-core programs shows that low-level I/O optimization is also important, but requires significant programmer effort. <p> Related work is discussed in Section 7 and our conclusions are presented in Section 8. 2 I/O system Our model of the I/O subsystem pairs each processor with a disk as was done in <ref> [13] </ref>. A processor can access its local disk directly; remote disks require cooperation from their owning processor. We implement this on the Paragon by creating a separate file for each processor within a parallel file system. <p> This decision reduces the speedup of computation which affects the execution times we report later. 5.2 Transforming LU factorization The second test case is LU factorization with pivoting, a method for solving dense linear systems. Our program is based on one provided by David Womble at Sandia National Laboratory <ref> [13, 14] </ref>. Since his program is out-of-core and parallel, we started by serial izing the computation then used the compiler to rediscover the original implementation. The sequential code is shown in Figure 6.
Reference: [14] <author> David E. </author> <type> Womble. </type> <institution> Sandia National Laboratories, </institution> <month> July </month> <year> 1993. </year> <title> Private communication. </title>
Reference-contexts: This decision reduces the speedup of computation which affects the execution times we report later. 5.2 Transforming LU factorization The second test case is LU factorization with pivoting, a method for solving dense linear systems. Our program is based on one provided by David Womble at Sandia National Laboratory <ref> [13, 14] </ref>. Since his program is out-of-core and parallel, we started by serial izing the computation then used the compiler to rediscover the original implementation. The sequential code is shown in Figure 6.
References-found: 13

