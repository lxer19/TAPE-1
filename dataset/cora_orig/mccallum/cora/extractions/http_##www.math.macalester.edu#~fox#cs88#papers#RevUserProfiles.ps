URL: http://www.math.macalester.edu/~fox/cs88/papers/RevUserProfiles.ps
Refering-URL: http://www.math.macalester.edu/~fox/cs88/readings.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: -dbillsus, pazzani-@ics.uci.edu  
Title: Revising User Profiles: The Search for Interesting Web Sites  
Author: Daniel Billsus and Michael Pazzani 
Address: Irvine, California 92717  
Affiliation: Department of Information and Computer Science University of California, Irvine  
Abstract: We describe Syskill & Webert, a software agent that learns to rate pages on the World Wide Web (WWW), deciding what pages might interest a user. The user rates explored pages on a three point scale, and Syskill & Webert learns a user profile by an a-lyzing the information on each page. We focus on an extension to Syskill & Webert that lets a user provide the system with an initial profile of his interests in order to increase the classification accuracy without seeing many rated pages. We represent this user pr o-file in a probabilistic way, which allows us to revise the profile as more training data is becoming avai l-able using conjugate priors, a common technique from Bayesian statistics for probability revision. U n-seen pages are classified using a simple Bayesian classifier that uses the revised probabilities. We co m-pare our approach to learning algorithms that do not make use of such background knowledge, and find that a user defined profile can significantly increase the classification accuracy. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Domingos, P., and Pazzani, M. </author> <year> 1996. </year> <title> Beyond Indepen d-ence: Conditions for the Optimality of the Simple Bay e-sian Classifier. </title> <booktitle> To appear in the Proceedings of the Inte r-national Conference on Machine Learning. </booktitle>
Reference: <author> Duda, R. O., and Hart, P. E., </author> <year> 1973. </year> <title> Pattern Classification and Scene Analysis. </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: The default version of Syskill & Webert uses a simple Bayesian class i-fier <ref> (Duda & Hart, 1973) </ref> to determine this probability. Note that these ratings and predictions are specific to one user and do not reflect on how other users might rate the pages. <p> Using the Simple Bayesian Classifier The probability tables that represent the current profile of the users interests can be directly used in Syskill & W e-berts default classification algorithm, the simple Bayesian classifier (SBC). The SBC <ref> (Duda & Hart, 1973) </ref> is a pro b-abilistic method for classification. It can be used to dete r a b a b + equivalent sample size user' s probability estimate mine the probability that an example i belongs to class C j given feature values of the example.
Reference: <author> Heckerman, D. </author> <year> 1995. </year> <title> A Tutorial on Learning with Bay e-sian Networks, </title> <type> Technical Report, </type> <institution> MSR-TR-95-06, Micr o-soft Corporation. </institution>
Reference-contexts: The probability tables that represent the current profile of the users interests can be directly used in Syskill & Weberts default classification algorithm, the simple Bay e-sian classifier. Conjugate Priors Conjugate priors are a traditional technique from Bay e-sian statistics to update probabilities from data <ref> (Heckerman, 1995) </ref>. Using this approach, a probability density function p ( q ) is associated with a random variable Q whose value q has to be learned. The probability density function p ( q ) is gradually updated to reflect the observed data.
Reference: <author> Lang, K. </author> <year> 1995. </year> <title> NewsWeeder: Learning to filter Netnews. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> 331-339. </pages> <address> Lake Tahoe, Calif.: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Second, learning algorithms are applied to the feature vectors. Feature Selection Not all words that appear in an HTML document are used as features. We use an information-based approach, sim i-lar to that used by an early version of the NewsWeeder program <ref> (Lang, 1995) </ref> to determine which words to use as features. Intuitively, one would like words that occur fr e-quently in pages on the hotlist, but infrequently on pages on the coldlist (or vice versa).
Reference: <author> Mauldin, M. L., and Leavitt, J. R. </author> <year> 1994. </year> <title> Web Agent R elated Research at the Center for Machine Translation. </title> <booktitle> Proceedings of the ACM Special Interest Group on Ne t-worked Information Discovery and Retrieval (SIGNIDR-94). </booktitle>
Reference-contexts: Syskill & Webert uses a user profile to identify inte resting web pages in two ways. First, it can annotate any HTML page with information on whether the user would be interested in visiting each page linked from that page. Second, Syskill & Webert can construct a LYCOS <ref> (Mauldin & Leavitt, 1994) </ref> query and retrieve pages that might match a users interest, and then annotate this result of the LYCOS search. Figure 1 shows a Web page on i n-dependent rock bands that has been annotated by Syskill and Webert.
Reference: <author> Pazzani, M., Muramatsu J., and Billsus, D. </author> <year> 1996. </year> <title> Syskill & Webert: Identifying interesting web sites. </title> <booktitle> To appear in the Proceedings of the National Conference on Artificial Intelligence, </booktitle> <address> Portland, OR. </address>
Reference-contexts: Initial experiments with our system <ref> (Pazzani, Muramatsu, Billsus, 1996) </ref> have shown that although it is much more accurate than chance at predicting whether a user would be interested in a page, the accuracy does not increase substantially as the user rates more and more pages. <p> In the experiments discussed in this paper, we use the 96 most informative words, because previous web page classification experiments with a simple Bayesian classifier resulted in the highest average accuracy over all tested domains for this value of k <ref> (Pazzani, Muramatsu, Billsus, 1996) </ref>. Table 1 shows some of the most inform a-tive words obtained from a collection of 140 HTML doc u-ments on independent rock bands as an example for fe a tures selected by expected information gain. <p> This problem can be addressed with an initial user defined profile. Learning Algorithms Once the HTML source for a given topic has been co n-verted to positive and negative examples represented as feature vectors, it is possible to run many learning alg o-rithms on the data. In previous experiments <ref> (Pazzani, M u-ramatsu, Billsus, 1996) </ref> we have investigated the accuracy of 5 machine learning algorithms (simple Bayesian class i-fier, nearest neighbor, PEBLS, decision trees (ID3), and neural nets). <p> SBC-UFeatures in turn performs better than all approaches that do not make use of the features provided by the user, such as SBC-IFeatures and all the standard machine learning algorithms that we tested in previous experiments <ref> (Pazzani, Muramatsu, Bil l-sus, 1996) </ref>. The difference between the accuracy of the algorithms that make use of the probabilities provided by the user and SBC-UFeatures is most significant when there are only few training examples.
Reference: <author> Quinlan, J. R. </author> <year> 1986. </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference-contexts: Intuitively, one would like words that occur fr e-quently in pages on the hotlist, but infrequently on pages on the coldlist (or vice versa). This is accomplished by finding the expected information gain (E (W,S)) <ref> (e.g., Quinlan, 1986) </ref> that the presence or absence of a word ( W) gives toward the classification of elements of a set of pages (S): E W S I S p W present I S W present p W absent I S W absent ( , ) ( ) [ (
Reference: <author> Winkler, R. </author> <year> 1967. </year> <title> The assessment of prior distributions in Bayesian analysis. </title> <journal> American Statistical Association Jou r-nal, </journal> <volume> 62 </volume> <pages> 776-800. </pages>
Reference-contexts: In Syskill & Webert we are using the initial profile given by the user to compute prior beta distributions for the probabilities p (word i present | class j sess a beta distribution is the equivalentsample-size method <ref> (Winkler, 1967) </ref>. The method is based on the idea to define the equivalent sample size as the number of o b-servations we would have had to have seen starting from complete ignorance in order to have the same confidence in the values of q .
References-found: 8

