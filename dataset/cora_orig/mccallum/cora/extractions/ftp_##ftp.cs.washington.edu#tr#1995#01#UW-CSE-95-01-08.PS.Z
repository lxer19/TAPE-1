URL: ftp://ftp.cs.washington.edu/tr/1995/01/UW-CSE-95-01-08.PS.Z
Refering-URL: http://www.cs.washington.edu/homes/zahorjan/homepage/listof.htm
Root-URL: 
Title: Extending the Applicability and Improving the Performance of Runtime Parallelization  
Author: Shun-Tak Leung and John Zahorjan 
Affiliation: Department of Computer Science and Engineering University of Washington  
Abstract: Technical Report 95-01-08 January 1995 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dimitri P. Bertsekas and John N. Tsitsiklis. </author> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, </address> <year> 1989. </year>
Reference-contexts: A less obvious case is when an antidependence between two iterations always implies a flow dependence. In this case, a schedule respecting the flow dependences automatically respects the antidependences as well. An example is using successive overrelaxation (SOR) <ref> [1, 5] </ref> to solve a sparse linear system with a symmetric coefficient matrix 1 . Such linear systems may arise from finite elements analysis in structural engineering or numerical solution of elliptic partial differential equations using finite difference [5]. <p> Our source loop is one iteration of the successive overrelaxation (SOR) algorithm for solving a sparse linear system <ref> [1] </ref>. ([6] list a number of applications taken from the Perfect Benchmarks that defy static analysis, and to which the inspector-executor approach is applicable.) The dependences in the SOR application depend on the sparsity structure of the coefficient matrix, thus making compile-time parallelization impossible.
Reference: [2] <author> Henry III Burkhardt, Steven Frank, Bruce Knobe, and James Rothnie. </author> <title> Overview of the KSR1 computer system. </title> <type> Technical Report KSR-TR-9202001, </type> <institution> Kendall Square Research, </institution> <address> Boston, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: Figure 6 shows relevant characteristics of the matrices we used. The experiments were run on a Kendall Square Research KSR1 shared-memory multiprocessor <ref> [2] </ref> running OSF/1. All programs were written in C using KSR1's pthreads and the source loop was manually transformed into different inspectors and executors. We employed a runtime restructuring technique we have developed [4] to reduce the adverse cache effects of runtime parallelization under both approaches.
Reference: [3] <author> Shun-Tak Leung and John Zahorjan. </author> <title> Improving the performance of runtime parallelization. </title> <booktitle> In Proceedings of Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 83-91, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: In this paper we examine the second approach, the inspector-executor method. Like much of the previous work in this area <ref> [3, 9] </ref>, we focus on implementations for shared address space multiprocessors. do i = 1, n enddo (a) Source Loop: Restricted Form do i = 1, n enddo (b) Source Loop: General Form Let us call the sequential loop to be parallelized the source loop.
Reference: [4] <author> Shun-Tak Leung and John Zahorjan. </author> <title> Restructuring arrays for efficient parallel loop execution. </title> <type> Technical Report 94-02-01, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <year> 1994. </year>
Reference-contexts: The experiments were run on a Kendall Square Research KSR1 shared-memory multiprocessor [2] running OSF/1. All programs were written in C using KSR1's pthreads and the source loop was manually transformed into different inspectors and executors. We employed a runtime restructuring technique we have developed <ref> [4] </ref> to reduce the adverse cache effects of runtime parallelization under both approaches. Timings are for 32 processors on one ring. For approach A, we compared the array copying costs with iteration execution times.
Reference: [5] <author> William H. Press, Brian P. Flannery, Saul A. Teukolsky, and William T. Vetterling. </author> <title> Numerical recipes: </title> <booktitle> the art of scientific computing. </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference-contexts: A less obvious case is when an antidependence between two iterations always implies a flow dependence. In this case, a schedule respecting the flow dependences automatically respects the antidependences as well. An example is using successive overrelaxation (SOR) <ref> [1, 5] </ref> to solve a sparse linear system with a symmetric coefficient matrix 1 . Such linear systems may arise from finite elements analysis in structural engineering or numerical solution of elliptic partial differential equations using finite difference [5]. <p> An example is using successive overrelaxation (SOR) [1, 5] to solve a sparse linear system with a symmetric coefficient matrix 1 . Such linear systems may arise from finite elements analysis in structural engineering or numerical solution of elliptic partial differential equations using finite difference <ref> [5] </ref>. In addition to depth, the schedules computed under the two approaches also affect executor cache behaviors differently in ways that are highly loop-specific.
Reference: [6] <author> Lawrence Rauchwerger and David Padua. </author> <title> The LRPD test: Speculative run-time parallelization of loops with privatization and reduction parallelization. </title> <type> Technical Report CSRD-TR-1390, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana-Champaign, IL. </institution>
Reference-contexts: Some loops, however, may contain parallelism not detectable in this way. For example, in sparse matrix computations, array subscripts often involve indirection arrays and thus defy static analysis <ref> [6, 9] </ref>. There are two basic approaches that can be taken to parallelizing such loops: speculative execution as a DOALL [6, 7]; and the inspector-executor method [8, 9]. <p> Some loops, however, may contain parallelism not detectable in this way. For example, in sparse matrix computations, array subscripts often involve indirection arrays and thus defy static analysis [6, 9]. There are two basic approaches that can be taken to parallelizing such loops: speculative execution as a DOALL <ref> [6, 7] </ref>; and the inspector-executor method [8, 9]. While the former yields good results when the fl This material is based upon work supported by the National Science Foundation (Grants CCR-9123308 and CCR-9200832), the Washington Technology Center, and Digital Equipment Corporation Systems Research Center and External Research Program.
Reference: [7] <author> Lawrence Rauchwerger and David Padua. </author> <title> The privatizing DOALL test: A run-time technique for DOALL loop identification and array privatization. </title> <booktitle> In Proceedings of International Conference on Supercomputing, </booktitle> <year> 1994. </year>
Reference-contexts: Some loops, however, may contain parallelism not detectable in this way. For example, in sparse matrix computations, array subscripts often involve indirection arrays and thus defy static analysis [6, 9]. There are two basic approaches that can be taken to parallelizing such loops: speculative execution as a DOALL <ref> [6, 7] </ref>; and the inspector-executor method [8, 9]. While the former yields good results when the fl This material is based upon work supported by the National Science Foundation (Grants CCR-9123308 and CCR-9200832), the Washington Technology Center, and Digital Equipment Corporation Systems Research Center and External Research Program.
Reference: [8] <author> Joel Saltz, Harry Berryman, and Janet Wu. </author> <title> Multiprocessors and runtime compilation. </title> <booktitle> In Proceedings of International Workshop on Compilers for Parallel Computers, </booktitle> <address> Paris, </address> <year> 1990. </year>
Reference-contexts: For example, in sparse matrix computations, array subscripts often involve indirection arrays and thus defy static analysis [6, 9]. There are two basic approaches that can be taken to parallelizing such loops: speculative execution as a DOALL [6, 7]; and the inspector-executor method <ref> [8, 9] </ref>. While the former yields good results when the fl This material is based upon work supported by the National Science Foundation (Grants CCR-9123308 and CCR-9200832), the Washington Technology Center, and Digital Equipment Corporation Systems Research Center and External Research Program. <p> Figure 1 (a) (adapted from <ref> [8] </ref>) shows the form of the source loops addressed in previous work [8, 9]. (Since we are concerned only about loop-carried dependences, statements not causing such dependences are omitted. <p> Figure 1 (a) (adapted from [8]) shows the form of the source loops addressed in previous work <ref> [8, 9] </ref>. (Since we are concerned only about loop-carried dependences, statements not causing such dependences are omitted. <p> N4 Matrix 0 2 4 Inspector Time (s) Type A Inspector Type B Inspector 0 20 40 60 80 100 # Executor Invocations 0 10 20 Execution Time (s) Matrix M1, f (i)=i Approach A Approach B Often the source loop is executed many times with the same dependence pattern <ref> [8, 9] </ref>. In these cases, the inspector is executed once and each executor invocation reuses the same schedule. Figure 11 illustrates how the overall execution time depends on the number of executor invocations. In this example, the type B executor is faster than the corresponding type A executor.
Reference: [9] <author> Joel H. Saltz, Ravi Mirchandaney, and Kay Crowley. </author> <title> Runtime parallelization and scheduling of loops. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 40(5) </volume> <pages> 603-612, </pages> <month> May </month> <year> 1991. </year> <month> 11 </month>
Reference-contexts: Some loops, however, may contain parallelism not detectable in this way. For example, in sparse matrix computations, array subscripts often involve indirection arrays and thus defy static analysis <ref> [6, 9] </ref>. There are two basic approaches that can be taken to parallelizing such loops: speculative execution as a DOALL [6, 7]; and the inspector-executor method [8, 9]. <p> For example, in sparse matrix computations, array subscripts often involve indirection arrays and thus defy static analysis [6, 9]. There are two basic approaches that can be taken to parallelizing such loops: speculative execution as a DOALL [6, 7]; and the inspector-executor method <ref> [8, 9] </ref>. While the former yields good results when the fl This material is based upon work supported by the National Science Foundation (Grants CCR-9123308 and CCR-9200832), the Washington Technology Center, and Digital Equipment Corporation Systems Research Center and External Research Program. <p> In this paper we examine the second approach, the inspector-executor method. Like much of the previous work in this area <ref> [3, 9] </ref>, we focus on implementations for shared address space multiprocessors. do i = 1, n enddo (a) Source Loop: Restricted Form do i = 1, n enddo (b) Source Loop: General Form Let us call the sequential loop to be parallelized the source loop. <p> Figure 1 (a) (adapted from [8]) shows the form of the source loops addressed in previous work <ref> [8, 9] </ref>. (Since we are concerned only about loop-carried dependences, statements not causing such dependences are omitted. <p> This restriction means that the loop has no loop-carried output dependences, and that x [i] is written by iteration i (only). These two observations make possible the inspector and executor algorithms in <ref> [9] </ref>, shown here in Figure 2. To parallelize the source loop, the compiler generates two pieces of code: an inspector and an executor. At run time, the inspector examines loop-carried dependences and computes a parallel schedule. <p> N4 Matrix 0 2 4 Inspector Time (s) Type A Inspector Type B Inspector 0 20 40 60 80 100 # Executor Invocations 0 10 20 Execution Time (s) Matrix M1, f (i)=i Approach A Approach B Often the source loop is executed many times with the same dependence pattern <ref> [8, 9] </ref>. In these cases, the inspector is executed once and each executor invocation reuses the same schedule. Figure 11 illustrates how the overall execution time depends on the number of executor invocations. In this example, the type B executor is faster than the corresponding type A executor.
References-found: 9

