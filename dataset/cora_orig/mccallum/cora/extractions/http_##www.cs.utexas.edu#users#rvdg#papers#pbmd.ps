URL: http://www.cs.utexas.edu/users/rvdg/papers/pbmd.ps
Refering-URL: http://www.cs.utexas.edu/users/rvdg/SL_library/library.html
Root-URL: 
Title: Parallel Matrix Distributions:  
Author: and Robert van de Geijn 
Date: Oct. 10, 1995  
Address: Austin, TX 78712  Austin, TX 78712  
Affiliation: Texas Institute for Computational and Applied Mathematics The University of Texas at Austin  Department of Computer Sciences and Texas Institute for Computational and Applied Mathematics The University of Texas at Austin  
Abstract: Have we been doing it all wrong? Abstract The basic premise of this report is that traditional matrix distributions for distributing matrices on distributed memory parallel architectures are in practice too restrictive. The primary problem lies with the fact that such distributions start with the matrix, not with the underlying physical problem. Through a series of examples, we show how this hampers convenient interfaces between applications and libraries. In some instances, we show how it hampers performance in general. We propose a new data distribution, Physically Based Matrix Distributions, which appear to show promise for solving the encountered problems. Some traditionally used distributions are shown to be a special, but often unnatural, case of this more general class of distributions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> The NAS Parallel Benchmarks. David Bailey, John Barton, Thomas Lasinski and Horst Simon (editors). </author> <type> NASA Technical Memorandum 103863, </type> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: A particularly important instance of this is the case where x and y are distributed identically. Applications include N-body problems (computation of the force on the particles due to other particles) [14] and iterative methods (conjugate gradient-like iterations require inner-products of x and y) <ref> [1] </ref>. 2.2.1 Simple case To start our explanation of what distributions meet the conditions of the above principle, we will assume x and y are partitioned into p approximately equal subvectors: x = B B x 0 . . . 1 C A 0 B @ y 1 y p1 C <p> For simplicity, we will assume m = n. 7 3.1.1 Application: NAS Parallel CG Benchmark The NAS parallel CG benchmark <ref> [1, 2] </ref> can be roughly described as a problem that uses an inverse power iteration to find the smallest eigenvalue of a randomly sparse symmetric positive definite matrix. To solve the associated linear system, a simple, unpreconditioned, conjugate gradient method is used. <p> The bulk of the computation is in the sparse matrix-vector multiply and inner-products of the conjugate gradient iteration. The constraint is thus that both x and y must be identically distributed across all processors, to facilitate the inner-products. 3.1.2 Parallel Matrix-Vector Multiplication A typical implementation <ref> [1, 14] </ref> will use a blocked matrix distribution as in Fig. 1. The issues for the other traditional distributions are essentially the same.
Reference: [2] <author> D. H. Bailey, E. Barszcz, L. Dagum, and H. D. Simon. </author> <title> NAS Parallel Benchmark Results, </title> <booktitle> Proceedings of SHPCC94. </booktitle>
Reference-contexts: For simplicity, we will assume m = n. 7 3.1.1 Application: NAS Parallel CG Benchmark The NAS parallel CG benchmark <ref> [1, 2] </ref> can be roughly described as a problem that uses an inverse power iteration to find the smallest eigenvalue of a randomly sparse symmetric positive definite matrix. To solve the associated linear system, a simple, unpreconditioned, conjugate gradient method is used.
Reference: [3] <editor> C.A Brebbia, J. C. F. Telles and L. C. Wrobel 1984, </editor> <title> Boundary Element Techniques, Theory and applications in Engineering, </title> <publisher> Springer-Verlag. </publisher>
Reference-contexts: This was due to the fact that more elements of the matrix in blocks on the diagonal of the matrix must be computed through singular integrals. In order to perform singular integrals (either by the Duffy triangular coordinate method [7] or the local polar coordinate method <ref> [3] </ref>), the original element must be divided into several subtriangles; and then the integrals are performed separately in all subtriangles.
Reference: [4] <author> J. Choi, J. J. Dongarra, R. Pozo, and D. W. Walker, </author> <title> "Scalapack: A Scalable Linear Algebra Library for Distributed Memory Concurrent Computers, </title> <booktitle> Proceedings of the Fourth Symposium on the Frontiers of Massively Parallel Computation. </booktitle> <publisher> IEEE Comput. Soc. Press, </publisher> <year> 1992, </year> <pages> pp. 120-127. </pages>
Reference-contexts: Perhaps the longest studied problem has been that of distributing matrices to processors. For most dense linear algebra problems, so-called two dimensional data distributions have been shown to be required to obtain scalable high performance <ref> [4, 9, 15] </ref>. However, some disturbing observations made in this paper seem to indicate the final solution has not yet been found. The most immediate observation is that the distribution of matrices is typically decoupled from the partitioning and distribution of the underlying physical problem. <p> We will discuss the latter application. 3.2.2 Parallel linear solver implementation The standard matrix distributions used by parallel dense linear solver packages are two-dimensional (block) wrapped distributions <ref> [4, 9, 15, 18] </ref>. The benefit of blocking is that it allows the parallel implementation to be more conveniently implemented using level-3 BLAS [8] matrix-matrix operations) which reduce memory traffic on each processor, thereby yielding higher performance.
Reference: [5] <author> J. Choi, J. Dongarra, and D. Walker, </author> <title> "The Design of a Parallel Dense Linear Algebra Software Library: Reduction to Hessenberg, Tridiagonal, and Bidiagonal Form," </title> <address> UT, CS-95-275, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: Take, for instance, traditional methods for solving the symmetric algebraic eigenvalue problem: The first phase involves a reduction to tridiagonal form, which requires a two-dimensional data distribution for scalability <ref> [5] </ref>. However, subsequently, the tridiagonal eigenvalue problem must be solved. While traditional matrix distributions leave the tridiagonal on a small number of processors, PBMD leaves it distributed among all processors. Matrices can come in many forms, and need not be explicitly formed.
Reference: [6] <author> Tom Cwik, Robert van de Geijn, and Jean Patterson, </author> <title> "Application of Massively Parallel Computation to Integral Equ Models of Electromagnetic Scattering," </title> <journal> Journal of the Optical Society of America A, </journal> <volume> Vol. 11, No. 4, </volume> <month> April </month> <year> 1994, </year> <pages> pp. 1538-1545 </pages>
Reference-contexts: matrix that represents the accumulation of all row pivots required for stability (we assume the factorization uses partial pivoting.) 8 3.2.1 Application: boundary element problems in acoustics One of the primary applications of parallel dense linear solvers for very large problems come from boundary integral formulations in electromagnetics and acoustics <ref> [6, 13] </ref>. We will discuss the latter application. 3.2.2 Parallel linear solver implementation The standard matrix distributions used by parallel dense linear solver packages are two-dimensional (block) wrapped distributions [4, 9, 15, 18].
Reference: [7] <author> L. Demkowicz, A. Karafiat and J.T. </author> <title> Oden 1992 Comp. </title> <journal> Meths. Appl. Mech. Engrg. </journal> <volume> 101, </volume> <month> 251-282. </month> <title> Solution of elastic scattering problems in linear acoustics using h-p boundary element method. </title>
Reference-contexts: This was due to the fact that more elements of the matrix in blocks on the diagonal of the matrix must be computed through singular integrals. In order to perform singular integrals (either by the Duffy triangular coordinate method <ref> [7] </ref> or the local polar coordinate method [3]), the original element must be divided into several subtriangles; and then the integrals are performed separately in all subtriangles.
Reference: [8] <author> J. J. Dongarra, J. Du Croz, S. Hammarling, and I. Duff, </author> <title> "A Set of Level 3 Basic Linear Algebra Subprograms," </title> <journal> TOMS, </journal> <volume> Vol. 16, No. 1, </volume> <pages> ppages 1-17, </pages> <year> 1990. </year>
Reference-contexts: The benefit of blocking is that it allows the parallel implementation to be more conveniently implemented using level-3 BLAS <ref> [8] </ref> matrix-matrix operations) which reduce memory traffic on each processor, thereby yielding higher performance.
Reference: [9] <author> Jack. J. Dongarra, Robert A. van de Geijn, and David W. Walker, </author> <title> "Scalability Issues Affecting the Design of a Dense Linear Algebra Library," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 22, No. 3, </volume> <month> Sept. </month> <year> 1994, </year> <pages> pp. 523-537. </pages>
Reference-contexts: Perhaps the longest studied problem has been that of distributing matrices to processors. For most dense linear algebra problems, so-called two dimensional data distributions have been shown to be required to obtain scalable high performance <ref> [4, 9, 15] </ref>. However, some disturbing observations made in this paper seem to indicate the final solution has not yet been found. The most immediate observation is that the distribution of matrices is typically decoupled from the partitioning and distribution of the underlying physical problem. <p> We will discuss the latter application. 3.2.2 Parallel linear solver implementation The standard matrix distributions used by parallel dense linear solver packages are two-dimensional (block) wrapped distributions <ref> [4, 9, 15, 18] </ref>. The benefit of blocking is that it allows the parallel implementation to be more conveniently implemented using level-3 BLAS [8] matrix-matrix operations) which reduce memory traffic on each processor, thereby yielding higher performance.
Reference: [10] <author> L. Demkowicz, J. T. Oden, W. Rachowicz and O, </author> <title> Hardy "Toward A Universal hp Adaptive Finite Element Strategy, Part 1. Constrained Approximation and Data Structure" , Comput. Methods. </title> <journal> Appl. Mech. and Engg., </journal> <volume> 77(1989), </volume> <month> pp.79-112 </month>
Reference: [11] <author> A. Edelman, </author> <title> "Large Dense Numerical Linear Algebra in 1993: The Parallel Computing Influence". </title> <journal> Journal of Supercomputing Applications. </journal> <volume> 7 (1993), </volume> <pages> pp. 113-128. </pages>
Reference-contexts: We are not the first to indicate that making the linear operator represented by a matrix the center of the universe leads to a very limited view of the world. Indeed, Alan Edelman quite bluntly states <ref> [11] </ref> The "All large dense matrices are structured" hypothesis: This point of view states that nature is not so perverse as to throw n 2 numbers at us haphazardly.
Reference: [12] <editor> G. Fox, et al., </editor> <booktitle> Solving Problems on Concurrent Processors: </booktitle> <volume> Volume 1, </volume> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference: [13] <author> P. Geng, J. T. Oden and R. A. van de Geijn 1995, </author> <title> Massively Parallel Computation for Acoustical Scattering Problems using Boundary Element Methods to appear in Journal of Sound and Vibration. </title>
Reference-contexts: matrix that represents the accumulation of all row pivots required for stability (we assume the factorization uses partial pivoting.) 8 3.2.1 Application: boundary element problems in acoustics One of the primary applications of parallel dense linear solvers for very large problems come from boundary integral formulations in electromagnetics and acoustics <ref> [6, 13] </ref>. We will discuss the latter application. 3.2.2 Parallel linear solver implementation The standard matrix distributions used by parallel dense linear solver packages are two-dimensional (block) wrapped distributions [4, 9, 15, 18]. <p> This non-uniqueness problem is overcome by combining the original integral formulation with a hypersingular integral formulation (the Burton-Miller method). The whole integral formulation then is approximated by the Galerkin method. The hyper-singular integrals are avoided through a special transformation on the weak formulation <ref> [13] </ref>. The Burton-Miller formulation induces significant extra work in generating the dense matrices characteristic of integral formulations. Generating blocks of the matrix on the diagonal proved substantially more expensive than blocks away from the diagonal.
Reference: [14] <author> B. Hendrickson, R. Leland, and S. Plimpton, </author> <title> A Parallel Algorithm for Matrix-Vector Multiplication, </title> <type> Tech. Rep. SAND 92-2765, </type> <institution> Sandia National Laboratories, </institution> <address> Albuquerque, NM, </address> <month> March </month> <year> 1993. </year>
Reference-contexts: A particularly important instance of this is the case where x and y are distributed identically. Applications include N-body problems (computation of the force on the particles due to other particles) <ref> [14] </ref> and iterative methods (conjugate gradient-like iterations require inner-products of x and y) [1]. 2.2.1 Simple case To start our explanation of what distributions meet the conditions of the above principle, we will assume x and y are partitioned into p approximately equal subvectors: x = B B x 0 . <p> The bulk of the computation is in the sparse matrix-vector multiply and inner-products of the conjugate gradient iteration. The constraint is thus that both x and y must be identically distributed across all processors, to facilitate the inner-products. 3.1.2 Parallel Matrix-Vector Multiplication A typical implementation <ref> [1, 14] </ref> will use a blocked matrix distribution as in Fig. 1. The issues for the other traditional distributions are essentially the same. <p> Assuming x is distributed in column-major order, the columns of A assigned to column j of processors is determined by the elements of subvectors x (j1)r ; : : : x jr1 . 3.1.3 Problems The distribution leads to two problems <ref> [14, 16, 17] </ref>: * After the matrix-vector multiplication, the elements of y will invariably be distributed different than x.
Reference: [15] <author> B. A. Hendrickson and D. E. Womble, </author> <title> "The Torus-Wrap Mapping for Dense Matrix Calculations on Massively Parallel Computers," </title> <journal> SIAM J. Sci. Comput., check issue number 14 </journal>
Reference-contexts: Perhaps the longest studied problem has been that of distributing matrices to processors. For most dense linear algebra problems, so-called two dimensional data distributions have been shown to be required to obtain scalable high performance <ref> [4, 9, 15] </ref>. However, some disturbing observations made in this paper seem to indicate the final solution has not yet been found. The most immediate observation is that the distribution of matrices is typically decoupled from the partitioning and distribution of the underlying physical problem. <p> We will discuss the latter application. 3.2.2 Parallel linear solver implementation The standard matrix distributions used by parallel dense linear solver packages are two-dimensional (block) wrapped distributions <ref> [4, 9, 15, 18] </ref>. The benefit of blocking is that it allows the parallel implementation to be more conveniently implemented using level-3 BLAS [8] matrix-matrix operations) which reduce memory traffic on each processor, thereby yielding higher performance.
Reference: [16] <author> J. G. Lewis, D. G. Payne, and R. A. van de Geijn, </author> <title> "Matrix-Vector Multiplication and Conjugate Gradient Algorithms on Distributed Memory Computers," </title> <booktitle> in Proceedings of the Scalable High Performance Computing Conference 1994. </booktitle>
Reference-contexts: Assuming x is distributed in column-major order, the columns of A assigned to column j of processors is determined by the elements of subvectors x (j1)r ; : : : x jr1 . 3.1.3 Problems The distribution leads to two problems <ref> [14, 16, 17] </ref>: * After the matrix-vector multiplication, the elements of y will invariably be distributed different than x. <p> However, nonzero elements can be expected to be concentrated around the diagonal and off-diagonal bands. These regions of the matrix are not distributed evenly among processors. 3.1.4 Benefits of PBMD In <ref> [16, 17] </ref>, we show how a PBMD induced by distributing x and y identically overcomes all of the concerns mentioned above: * No additional communication is required since the parallel implementation can be arranged to leave the result y distributed like x. * The diagonal is distributed equally to all processors.
Reference: [17] <author> J.G. Lewis and R.A. van de Geijn, </author> <title> Distributed Memory Matrix-Vector Multiplication and Conjugate Gradient Algorithms, </title> <booktitle> in the proceedings of Supercomputing '93, </booktitle> <address> Portland, OR, </address> <month> November 15-19, </month> <year> 1993. </year>
Reference-contexts: Assuming x is distributed in column-major order, the columns of A assigned to column j of processors is determined by the elements of subvectors x (j1)r ; : : : x jr1 . 3.1.3 Problems The distribution leads to two problems <ref> [14, 16, 17] </ref>: * After the matrix-vector multiplication, the elements of y will invariably be distributed different than x. <p> However, nonzero elements can be expected to be concentrated around the diagonal and off-diagonal bands. These regions of the matrix are not distributed evenly among processors. 3.1.4 Benefits of PBMD In <ref> [16, 17] </ref>, we show how a PBMD induced by distributing x and y identically overcomes all of the concerns mentioned above: * No additional communication is required since the parallel implementation can be arranged to leave the result y distributed like x. * The diagonal is distributed equally to all processors.
Reference: [18] <author> W. Lichtenstein and S. L. Johnsson, </author> <title> "Block-Cyclic Dense Linear Algebra", </title> <institution> Harvard University, Center for Research in Computing Technology, TR-04-92, </institution> <month> Jan., </month> <year> 1992. </year>
Reference-contexts: We will discuss the latter application. 3.2.2 Parallel linear solver implementation The standard matrix distributions used by parallel dense linear solver packages are two-dimensional (block) wrapped distributions <ref> [4, 9, 15, 18] </ref>. The benefit of blocking is that it allows the parallel implementation to be more conveniently implemented using level-3 BLAS [8] matrix-matrix operations) which reduce memory traffic on each processor, thereby yielding higher performance.
Reference: [19] <author> J. T. Oden, A. Patra, Y. Feng, </author> <title> "Parallel Domain Decomposition Solver For Adaptive hp Methods" submitted to SIAM Journal for Numerical Methods. </title>
Reference-contexts: The most sophisticated of these use highly adaptive hp meshes, wherein both the local element size and the polynomial order are dynamically chosen for maximum efficiency <ref> [19] </ref>. These problems lead to highly irregular sparse linear systems. However, the sparsity typically exhibits itself as locally dense blocks. The sparsity pattern is dictated by both the connectivity of the graph associated with the discretization mesh and the local polynomial distributions.
Reference: [20] <author> E. Rothberg and R. Schreiber, </author> <title> "Improved load distribution in parallel sparse Cholesky factorization." </title> <booktitle> in Proceedings of Supercomputing 94 , pp. </booktitle> <pages> 783-792. </pages>
Reference-contexts: These methods create a physically based ordering that reduces the amount of fill-in that occurs during the factorization stage, thereby reducing required computation. Given an ordering, one effective general purpose implementations of sparse factorization is given by Rothberg and Schreiber <ref> [20, 21] </ref>. In their implementation, they use a "supernodal" method that allows them to take advantage of dense blocks in the sparse matrix.
Reference: [21] <author> E. Rothberg and R. Schreiber, </author> <title> "Efficient parallel sparse Cholesky factorization," </title> <booktitle> in Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing (R. </booktitle> <editor> Schreiber, et al. eds.), </editor> <publisher> SIAM, </publisher> <year> 1994, </year> <pages> pp. 407-412. 15 </pages>
Reference-contexts: These methods create a physically based ordering that reduces the amount of fill-in that occurs during the factorization stage, thereby reducing required computation. Given an ordering, one effective general purpose implementations of sparse factorization is given by Rothberg and Schreiber <ref> [20, 21] </ref>. In their implementation, they use a "supernodal" method that allows them to take advantage of dense blocks in the sparse matrix.
References-found: 21

