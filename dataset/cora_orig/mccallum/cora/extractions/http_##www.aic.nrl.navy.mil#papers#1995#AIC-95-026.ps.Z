URL: http://www.aic.nrl.navy.mil/papers/1995/AIC-95-026.ps.Z
Refering-URL: http://www.ifi.unit.no/iccbr95/papers.html
Root-URL: 
Email: (dietrich.wettschereck@gmd.de)  (aha@aic.nrl.navy.mil)  
Phone: 2  
Title: Weighting Features  
Author: Dietrich Wettschereck and David W. Aha 
Address: 53754 Sankt Augustin, Germany  Washington, DC 20375 USA  
Affiliation: 1 German National Research Center for Computer Science,  Navy Center for Applied Research in AI, Naval Research Laboratory,  
Abstract: Many case-based reasoning algorithms retrieve cases using a derivative of the k-nearest neighbor (k-NN) classifier, whose similarity function is sensitive to irrelevant, interacting, and noisy features. Many proposed methods for reducing this sensitivity parameterize k-NN's similarity function with feature weights. We focus on methods that automatically assign weight settings using little or no domain-specific knowledge. Our goal is to predict the relative capabilities of these methods for specific dataset characteristics. We introduce a five-dimensional framework that categorizes automated weight-setting methods, empirically compare methods along one of these dimensions, summarize our results with four hypotheses, and describe additional evidence that supports them. Our investigation revealed that most methods correctly assign low weights to completely irrelevant features, and methods that use performance feedback demonstrate three advantages over other methods (i.e., they require less pre-processing, better tolerate interacting features, and in crease learning rate).
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W. </author> <year> (1990). </year> <title> A study of instance-based learning algorithms for supervised learning tasks: Mathematical, empirical, </title> <type> and psychological evaluations (TR 90-42). </type> <institution> Irvine, CA: University of California, Department of Information and Computer Science. </institution>
Reference-contexts: For incorrect classifications, the weights of mismatching features are incremented while the weights of matching features are decremented. Salzberg reported that different values of f worked better for different datasets. Wettschereck and Dietterich (1995) argued that EACH's weighting method is insensitive to skewed concept distributions, a problem that IB4 <ref> (Aha, 1990) </ref> addresses. It computes weights using w f = max CumulativeWeight f WeightNormalizer f 0:5; 0 ; (4) where CumulativeWeight f is expected to asymptote to WeightNormalizer f 2 for seemingly irrelevant features.
Reference: <author> Aha, D. W. </author> <year> (1991). </year> <title> Incremental constructive induction: An instance-based approach. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning (pp. </booktitle> <pages> 117-121). </pages> <address> Evanston, IL: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Dimension 5: Knowledge This is the most important dimension for distinguishing feature weighting methods. Domain-specific knowledge can be used, for example, to constrain the case representation (Stanfill & Waltz, 1986), guide feature transformation <ref> (Aha, 1991) </ref>, or assign case-specific weight settings (Cain et al., 1991). We focus on automated algorithms that do not receive much task-specific knowledge.
Reference: <author> Aha, D. W., & Bankert, R. L. </author> <year> (1994). </year> <title> Feature selection for case-based classification of cloud types: An empirical comparison. </title> <editor> In D. W. Aha (Ed.) </editor> <booktitle> Case-Based Reasoning: Papers from the 1994 Workshop (TR WS-94-01). </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Aha, D. W., & Goldstone, R. L. </author> <year> (1992). </year> <title> Concept learning and flexible weighting. </title> <booktitle> In Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 534-539). </pages> <address> Bloomington, </address> <publisher> IN: Lawrence Erlbaum. </publisher>
Reference-contexts: Case specific weighting provides great flexibility in assessing feature relevance, which is required to model subject data accurately <ref> (Aha & Goldstone, 1992) </ref>. Dimension 5: Knowledge This is the most important dimension for distinguishing feature weighting methods. Domain-specific knowledge can be used, for example, to constrain the case representation (Stanfill & Waltz, 1986), guide feature transformation (Aha, 1991), or assign case-specific weight settings (Cain et al., 1991).
Reference: <author> Cain, T., Pazzani, M. J., & Silverstein, G. </author> <year> (1991). </year> <title> Using domain knowledge to influence similarity judgement. </title> <booktitle> In Proceedings of the Case-Based Reasoning Workshop (pp. </booktitle> <pages> 191-202). </pages> <address> Washington, DC: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Dimension 5: Knowledge This is the most important dimension for distinguishing feature weighting methods. Domain-specific knowledge can be used, for example, to constrain the case representation (Stanfill & Waltz, 1986), guide feature transformation (Aha, 1991), or assign case-specific weight settings <ref> (Cain et al., 1991) </ref>. We focus on automated algorithms that do not receive much task-specific knowledge.
Reference: <author> Cardie, C. </author> <year> (1993). </year> <title> Using decision trees to improve case-based learning. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning (pp. </booktitle> <pages> 25-32). </pages> <address> Amherst, MA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Creecy, R. H., Masand, B. M., Smith, S. J., & Waltz, D. L. </author> <year> (1992). </year> <title> Trading MIPS and memory for knowledge engineering. </title> <journal> Communications of the ACM, </journal> <volume> 35, </volume> <pages> 48-64. </pages>
Reference-contexts: Conditional probabilities: This group consists of two methods that assign feature weights using simple conditional probabilities, discretize numeric features, and binarize symbolic features <ref> (Creecy et al., 1992) </ref>.
Reference: <author> Daelemans, W., van den Bosch, A. </author> <year> (1992). </year> <title> Generalization performance of backpropagation learning on a syllabification task. </title> <booktitle> In Proceedings of TWLT3: Connectionism and Natural Language Processing (pp. </booktitle> <pages> 27-37). </pages> <address> Enschede, The Netherlands: </address> <note> Unpublished. </note>
Reference: <author> Devijver, P. A., & Kittler, J. </author> <year> (1982). </year> <title> Pattern recognition: A statistical approach. </title> <address> En-glewood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference-contexts: Dimensions For Distinguishing Feature Weighting Methods Dimension Possible Values Feedback fAvailable, Not Availableg Weight Space fContinuous; Binaryg Representation fGiven; Transformedg Generality fGlobal; Localg Knowledge fPoor; Intensiveg feature selection, which constrains the search space to binary values. Feature selection has a long history in pattern recognition <ref> (e.g., Devijver & Kittler, 1982) </ref>. Several researchers have recently reported accuracy and/or speed improvements for k-NN variants in CBR systems (e.g., Cardie, 1993; Moore & Lee, 1994; Skalak, 1994; Aha & Bankert, 1994). Feature selection methods can reduce the task's dimensionality when they eliminate irrelevant features.
Reference: <author> Domingos, P. </author> <title> Context-sensitive feature selection for lazy learners. </title> <note> To appear in Artificial Intelligence Review.. </note>
Reference: <author> Fayyad, U. M., & Irani, K. B. </author> <year> (1993). </year> <title> Multi-interval discretization of continuous-valued attributes for classification learning. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 1022-1029). </pages> <address> Chambery, France: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: There exist 16 equally-sized intervals. Table 6. Average Accuracies for the sinusoidal Task Relative to k-NN Feature Weight Learning Algorithm Discretization Feedback Method Ignorant Method Method k-NN k-NN V SM CCF VDM MI none 74.20.7 88.6 <ref> (Fayyad & Irani, 1993) </ref> -65.1 -65.1 -69.6 Manually Set 81.6 82.8 88.5 Fig. 2. Learning curves for MI and Relief-f for two tasks. k-NN V SM significantly outperformed MI for up to four parity features, they performed equally poorly for larger numbers of interacting features.
Reference: <author> Kelly, J. D., Jr., & Davis, L. </author> <year> (1991). </year> <title> A hybrid genetic algorithm for classification. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 645-650). </pages> <address> Sydney, Australia: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We eval-uate a hill-climbing variant of RELIEF-F, Kononenko's (1994) extension of RELIEF, in our experiments. Continuous optimizers: These feedback methods iteratively update feature weights using randomly-selected training cases. For example, GA-WKNN <ref> (Kelly and Davis, 1991) </ref> uses a genetic algorithm to update feature weights, where search is guided by five genetic operators and fitness is based on both training accuracy and recency. GA-WKNN attained lower error rates than k-NN for three datasets.
Reference: <author> Kira, K., & Rendell, L. A. </author> <year> (1992). </year> <title> A practical approach to feature selection. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning (pp. </booktitle> <pages> 249-256). </pages> <address> Aberdeen, Scotland: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kohavi, R., Langley, P., & Yun, Y. </author> <year> (1995). </year> <title> Heuristic search for feature weights in instance-based learning. </title> <type> Unpublished manuscript. </type>
Reference: <author> Kononenko, I. </author> <year> (1994). </year> <title> Estimating attributes: Analysis and extensions of RELIEF. </title> <booktitle> In Proceedings of the 1994 European Conference on Machine Learning (pp. </booktitle> <pages> 171-182). </pages> <address> Catania, Italy: </address> <publisher> Springer Verlag. </publisher>
Reference: <author> Lowe, D. </author> <year> (1995). </year> <title> Similarity metric learning for a variable-kernal classifier. </title> <journal> Neural Computation, </journal> <volume> 7, </volume> <pages> 72-85. </pages>
Reference: <author> Mohri, T., & Tanaka, H. </author> <year> (1994). </year> <title> An optimal weighting criterion of case indexing for both numeric and symbolic attributes. </title> <editor> In D. W. Aha (Ed.), </editor> <booktitle> Case-Based Reasoning: Papers from the 1994 Workshop (TR WS-94-01). </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: of k-NN have been reported to improve its retrieval accuracy on some tasks (e.g., Aha, 1990; Kelly & Davis, 1991; Wettschereck, 1994), their relative merits are not known; previous comparisons focussed on specific algorithm pairings (e.g., Wettschereck & Dietterich, 1995; Kohavi et al., 1995) or present only case study results <ref> (Mohri & Tanaka, 1994) </ref>. We introduce a framework for feature-weighting methods in Section 2 and empirically compare a specific subset of them in Section 3. Case studies are not particularly informative. Therefore, in Section 4 we present hypotheses for explaining these results and investigate each empirically. <p> This algorithm tends to classify too many cases according to the majority class <ref> (Mohri & Tanaka, 1994) </ref>. The cross-category feature importance (CCF) method averages across classes. It computes weight settings using w f = c2C Since Mohri and Tanaka (1994) reported good results for CCF, we included it in our experiments.
Reference: <author> Moore, A. W., & Lee, M. S. </author> <year> (1994). </year> <title> Efficient algorithms for minimizing cross validation error. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning (pp. </booktitle> <pages> 190-198). </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Murphy, P. </author> <year> (1995). </year> <title> UCI Repository of machine learning databases [Machine-readable data repository @ics.uci.edu]. </title> <address> Irvine, CA: </address> <institution> University of California, Department of Information and Computer Science. </institution>
Reference-contexts: Thus, it has both interacting and irrelevant features. The remaining datasets are obtainable from the UCI Repository <ref> (Murphy, 1995) </ref>. Whereas LED's relevant features are roughly equally relevant, Waveform's vary in their relevance. Waveform+19 (LED+17) is identical to Waveform (LED) with the addition of 19 (17) continuous irrelevant features. The Cleveland dataset contains some redundant features, while the NETtalk dataset has no irrelevant or redundant features (Wettschereck, 1994).
Reference: <author> Ricci, F., & Avesani, P. </author> <year> (1995). </year> <title> Learning a local similarity metric for case-based reasoning. </title> <booktitle> To appear in Proceedings of the First International Conference on Case-Based Reasoning. </booktitle> <address> Sesimbra, Portugal: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Salzberg, S. L. </author> <year> (1991). </year> <title> A nearest hyperrectangle learning method. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 251-276. </pages>
Reference-contexts: Daelemans and van den Bosch (1992) reported that MI significantly improved k-NN's accuracy for one task. Wettschereck and Dietterich (1995) reported that MI significantly increased EACH's <ref> (Salzberg, 1991) </ref> accuracy. In our experiments, we examined a variant of this approach that discretizes continuous features using Fayyad and Irani's (1993) algorithm. Table 2.
Reference: <author> Shannon, C. E. </author> <year> (1948). </year> <title> A mathematical theory of communication. </title> <journal> Bell Systems Technology Journal, </journal> <volume> 27, </volume> <pages> 379-423. </pages>
Reference-contexts: Thus, two cases are similar if they have feature values whose respective projections on the training library have similar class distributions. We included the VDM in our experiments. Mutual information: This third ignorant approach assigns feature weights using the mutual information (MI) <ref> (Shannon, 1948) </ref> between the values of a feature and the class of the training cases.
Reference: <author> Skalak, D. </author> <year> (1994). </year> <title> Prototype and feature selection by sampling and random mutation hill climbing algorithms. </title> <booktitle> In Proceedings of the Eleventh International Machine Learning Conference (pp. </booktitle> <pages> 293-301). </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Stanfill, C., & Waltz, D. </author> <year> (1986). </year> <title> Toward memory-based reasoning. </title> <journal> Communications of the ACM, </journal> <volume> 29, </volume> <pages> 1213-1228. </pages>
Reference-contexts: Case specific weighting provides great flexibility in assessing feature relevance, which is required to model subject data accurately (Aha & Goldstone, 1992). Dimension 5: Knowledge This is the most important dimension for distinguishing feature weighting methods. Domain-specific knowledge can be used, for example, to constrain the case representation <ref> (Stanfill & Waltz, 1986) </ref>, guide feature transformation (Aha, 1991), or assign case-specific weight settings (Cain et al., 1991). We focus on automated algorithms that do not receive much task-specific knowledge. <p> The cross-category feature importance (CCF) method averages across classes. It computes weight settings using w f = c2C Since Mohri and Tanaka (1994) reported good results for CCF, we included it in our experiments. Class projection: The more sophisticated value-difference metric (VDM) <ref> (Stanfill & Waltz, 1986) </ref> does not binarize symbolic features.
Reference: <author> Wettschereck, D. </author> <year> (1994). </year> <title> A study of distance-based machine learning algorithms. </title> <type> Doctoral dissertation, </type> <institution> Department of Computer Science, Oregon State University, </institution> <address> Cor-vallis, OR. </address>
Reference-contexts: Whereas LED's relevant features are roughly equally relevant, Waveform's vary in their relevance. Waveform+19 (LED+17) is identical to Waveform (LED) with the addition of 19 (17) continuous irrelevant features. The Cleveland dataset contains some redundant features, while the NETtalk dataset has no irrelevant or redundant features <ref> (Wettschereck, 1994) </ref>. Each dataset was randomly partitioned 25 times into disjoint training and test sets. Table 4 lists the algorithms' average test set accuracy relative to k-NN, whose standard error is also listed. Significant differences are highlighted using boldface (i.e., two-tailed t-tests, confidence level 0.05).
Reference: <author> Wettschereck, D., Aha, D. W. & Mohri, </author> <title> T (1995). A review and comparative evaluation of feature weighting methods for lazy learning algorithms (TR AIC-95-012). </title> <address> Washington, DC: </address> <institution> Naval Research Laboratory, </institution> <note> Navy Center for Applied Research in Artificial Intelligence. </note>
Reference-contexts: The derivative of this error with respect to each feature weight is used to guide the search. The VSM performed as well or better than several other algorithms on two datasets yet required far less training time than some other algorithms. We experimented with k-NN V SM <ref> (Wettschereck et al., 1995) </ref>, a variant of the VSM that isolates its feature-weighting method. k-NN V SM computes the distances between all pairs of training cases using Equation 1, assigns the same value to each weight (i.e., 8 f fw f = 1g), then optimizes the value of k, and finally <p> Third, gauss-band has interacting features. It extends sinusoidal with three additional features; two of these define four Gaussian distributions, while the fifth determines whether the class is determined by the first or second pair of features. The fourth dataset is a parity problem on 11 binary 3 See <ref> (Wettschereck et al., 1995) </ref> for a more extensive evaluation. Table 4.
Reference: <author> Wettschereck, D., & Dietterich, T. G. </author> <year> (1995). </year> <title> An experimental comparison of the nearest neighbor and nearest hyperrectangle algorithms. </title> <journal> Machine Learning, </journal> <volume> 19, </volume> <month> 5-28. </month> <title> This paper was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: The derivative of this error with respect to each feature weight is used to guide the search. The VSM performed as well or better than several other algorithms on two datasets yet required far less training time than some other algorithms. We experimented with k-NN V SM <ref> (Wettschereck et al., 1995) </ref>, a variant of the VSM that isolates its feature-weighting method. k-NN V SM computes the distances between all pairs of training cases using Equation 1, assigns the same value to each weight (i.e., 8 f fw f = 1g), then optimizes the value of k, and finally <p> Third, gauss-band has interacting features. It extends sinusoidal with three additional features; two of these define four Gaussian distributions, while the fifth determines whether the class is determined by the first or second pair of features. The fourth dataset is a parity problem on 11 binary 3 See <ref> (Wettschereck et al., 1995) </ref> for a more extensive evaluation. Table 4.
References-found: 27

