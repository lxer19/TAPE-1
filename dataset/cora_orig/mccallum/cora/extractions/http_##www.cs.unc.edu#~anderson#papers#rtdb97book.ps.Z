URL: http://www.cs.unc.edu/~anderson/papers/rtdb97book.ps.Z
Refering-URL: http://www.cs.unc.edu/~anderson/papers.html
Root-URL: http://www.cs.unc.edu
Title: IMPLEMENTING HARD REAL-TIME TRANSACTIONS ON MULTIPROCESSORS  
Author: James H. Anderson, Rohit Jain, and Srikanth Ramamurthy 
Affiliation: Department of Computer Science, University of North Carolina at Chapel Hill  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> J. Anderson, R. Jain, S. Ramamurthy, </author> <title> "Wait-Free Object-Sharing Schemes for Real-Time Uniprocessors and Multiprocessors", </title> <type> manuscript, </type> <year> 1997. </year>
Reference-contexts: However, these overhead terms arise for different reasons, and overhead terms when using our approach are often much smaller than those of lock-based schemes <ref> [1] </ref>. Our transaction implementation is described in detail in subsequent sections. First, in Section 2, we review previous work on implementing hard real-time transactions on multiprocessors. We then present an overview of some of the key mechanisms used in our work in Section 3. <p> Of the 2 T worst-case execution cost, a factor of T represents the overhead associated with helping. (It is possible to reduce this overhead term using priority ceiling information as explained in <ref> [1] </ref>.) This overhead term is similar to blocking factors that arise in scheduling conditions when using the 2 The "optimistic" counterpart of a wait-free object is a lock-free object [5]. In a lock-free object implementation, an operation may be interfered with, in which case it must be retried. <p> The fact that object-access requirements do not have to be predeclared makes it easier to support mode changes and dynamically-generated transactions. The notion of incremental helping can be extended for application on multiprocessors. We call the resulting scheme cyclic helping <ref> [1, 3] </ref>. With cyclic helping, the processors are thought of as if they were part of a logical ring. Tasks are helped through the use of a "help counter", which cycles around the ring. <p> On a ring of size P , the time to perform a transaction is therefore at most 2 P T , where T is the largest sequential transaction cost. (Tighter worst-case bounds are given in <ref> [1] </ref>.) Unlike the MPCP and DPCP, information about object-access requirements is not required for implementing cyclic helping. (However, the tighter bounds in [1] do make use of such information.) It can often be advantageous to incorporate priority information into an object-sharing scheme. <p> the time to perform a transaction is therefore at most 2 P T , where T is the largest sequential transaction cost. (Tighter worst-case bounds are given in <ref> [1] </ref>.) Unlike the MPCP and DPCP, information about object-access requirements is not required for implementing cyclic helping. (However, the tighter bounds in [1] do make use of such information.) It can often be advantageous to incorporate priority information into an object-sharing scheme. We have developed a wait-free scheme called priority helping that does this [1, 3]. <p> We have developed a wait-free scheme called priority helping that does this <ref> [1, 3] </ref>. Priority helping is similar to cyclic helping, except that the help counter is always advanced to the processor with the highest-priority pending transaction. <p> These routines are called within user-supplied transaction code to read or write words of the MEM array; e.g., "Write (1; Read (10))" performs the assignment "MEM <ref> [1] </ref> := MEM [10]". Figure 4 shows a simple example transaction, which enqueues an item onto a shared queue. This transaction would be executed by calling Exec (Enqueue ). <p> While the help counter points to the virtual processor, any read-only transaction can be performed without helping other transactions. Also, the cyclic helping scheme actually can be implemented in a manner that requires a transaction to perform only one traversal of the helping ring instead of two <ref> [1] </ref>. Details of these optimizations are omitted here due to space limitations. The explanation of our transaction implementation would seem to imply that one centralized help counter is always required. This is not the case.
Reference: [2] <author> J. Anderson, S. Ramamurthy, M. Moir, K. Jeffay, </author> <title> "Lock-Free Trans. for Real-Time Systems", </title> <booktitle> Proc. First Int'l Workshop on Real-Time Databases: Issues and Applications, </booktitle> <year> 1996, </year> <pages> 107-114. </pages>
Reference-contexts: With either the MPCP or the DPCP, tasks are susceptible to very large blocking factors, and correspondingly low processor utilizations. In addition, these 1 On uniprocessors, the number of transaction restarts over an interval of time can be bounded (assuming memory-resident data) <ref> [2] </ref>. However, bounding the effects of repeated restarts due to conflicts across processors in a multiprocessor system does not seem practical. <p> The implementation has been obtained by combining a multiprocessor-based cyclic-helping scheme with a lock-free transaction implementation for real-time uniprocessors presented previously <ref> [2] </ref>. Modifications for implementing a priority-helping scheme could be similarly applied. Our implementation is defined by four procedures, Exec, Help, Read, and Write, which are given in Figure 3. Variable declarations that are used in these procedures are given in Figure 2.
Reference: [3] <author> J. Anderson, S. Ramamurthy, R. </author> <title> Jain "Implementing Wait-Free Objects on Priority-Based Systems", </title> <booktitle> Proc. 16th ACM Symp. on Prin. of Distr. </booktitle> <address> Comp., </address> <note> to appear. </note>
Reference-contexts: In a sense, a helping-based wait-free scheme is a "pessimistic" notion of nonblocking user-level synchronization that is similar to a lock-based scheme, with helping taking the place of blocking. (This is illustrated quite well by Figure 1, which is considered below.) 2 In a recent paper <ref> [3] </ref>, we showed that the cost of helping can be greatly reduced on priority-based real-time uniprocessor systems by using a technique called incremental helping. The general idea of incremental helping is illustrated in Figure 1. <p> The fact that object-access requirements do not have to be predeclared makes it easier to support mode changes and dynamically-generated transactions. The notion of incremental helping can be extended for application on multiprocessors. We call the resulting scheme cyclic helping <ref> [1, 3] </ref>. With cyclic helping, the processors are thought of as if they were part of a logical ring. Tasks are helped through the use of a "help counter", which cycles around the ring. <p> We have developed a wait-free scheme called priority helping that does this <ref> [1, 3] </ref>. Priority helping is similar to cyclic helping, except that the help counter is always advanced to the processor with the highest-priority pending transaction. <p> CCAS is useful because the compare-only value can be used to ensure that a "late" CCAS operation by a task that has been preempted and then resumed has no effect. Fortunately, CCAS is easy to implement even if CAS2 is not available. In a recent paper <ref> [3] </ref>, we showed that CCAS can be implemented using only three high-level language statements if CAS (a commonly-available instruction) is available. We continue our description of the Exec and Help procedures by considering the shared variables that are referenced in these procedures.
Reference: [4] <author> R. Bettati, </author> <title> End-to-End Scheduling to Meet Deadlines in Distributed Systems, </title> <type> Ph.D. Thesis, </type> <institution> Computer Science Dept., Univ. of Illinois, </institution> <year> 1994. </year>
Reference-contexts: In effect, such suspensions break a task into a sequence of subtasks, each of which may experience a priority inversion. Researchers at the University of Illinois have proposed an "end-to-end" approach for sharing resources in multiprocessors, in which tasks are converted into a sequence of periodic subtasks <ref> [4, 11] </ref>. Each subtask can perform either local computation or a single-processor transaction. Subtasks are scheduled on a per-processor basis, using uniprocessor scheduling schemes. Resources are accessed through the use of the PCP or a similar scheme. Like the DPCP, the end-to-end approach requires a binding of resources to processors.
Reference: [5] <author> M. Herlihy, </author> <title> "A Methodology for Implementing Highly Concurrent Data Objects", </title> <journal> ACM Trans. on Prog. Langs. and Sys., </journal> <volume> 15(5), </volume> <year> 1993, </year> <pages> 745-770. </pages>
Reference-contexts: Before a task is allowed to do this, however, it must first help any previously-announced transaction (on its processor) to complete execution. This scheme requires only one announce variable per processor. In contrast, previous constructions for asynchronous systems require one announce variable per task <ref> [5] </ref>. Also, with incremental helping, each task helps at most one other task, while in helping schemes for asynchronous systems, each task helps all other tasks in the worst case. <p> represents the overhead associated with helping. (It is possible to reduce this overhead term using priority ceiling information as explained in [1].) This overhead term is similar to blocking factors that arise in scheduling conditions when using the 2 The "optimistic" counterpart of a wait-free object is a lock-free object <ref> [5] </ref>. In a lock-free object implementation, an operation may be interfered with, in which case it must be retried. Repeated retries may be needed before an operation is successful. Implementing Hard Real-Time Transactions on Multiprocessors 5 announces its transaction.
Reference: [6] <author> V. Lortz, </author> <title> An Object-Oriented Real-Time Database System for Multiprocessors, </title> <type> Ph.D. Thesis, </type> <institution> Computer Science Dept., Univ. of Michigan, </institution> <year> 1994. </year>
Reference-contexts: Lortz and Shin have implemented a hard real-time database system called MDARTS (Multiprocessor Database Architecture for Real-Time Systems), in which both RPC transactions and concurrent shared-memory-based transactions are supported <ref> [6] </ref>. This was the first (and perhaps only) actual implementation of a hard real-time multiprocessor database server with reasonable performance. However, in MDARTS, critical sections are implemented by disabling preemptions (to deal with conflicts within a processor) and by using queue-based spin locks (to deal with conflicts across processors).
Reference: [7] <author> R. Rajkumar, </author> <title> "Real-Time Synchronization Protocols for Shared Memory Multiprocessors", </title> <booktitle> Proc. Int'l Conf. on Distr. Comp. Sys., </booktitle> <year> 1990, </year> <pages> 116-123. </pages>
Reference-contexts: Rajkumar et al. proposed two such mechanisms by extending the uniprocessor priority ceiling protocol (PCP) [8, 10]. The resulting protocols are called the distributed PCP (DPCP) and the multiprocessor PCP (MPCP), respectively <ref> [7, 8, 9] </ref>. Both protocols assume a model in which a task can perform one or more transactions. In the DPCP, global resources are guarded by synchronization processors.
Reference: [8] <author> Raghunathan Rajkumar, </author> <title> Synchronization In Real-Time Systems A Priority Inheritance Approach, </title> <publisher> Kluwer Academic Publications, </publisher> <year> 1991. </year>
Reference-contexts: Rajkumar et al. proposed two such mechanisms by extending the uniprocessor priority ceiling protocol (PCP) <ref> [8, 10] </ref>. The resulting protocols are called the distributed PCP (DPCP) and the multiprocessor PCP (MPCP), respectively [7, 8, 9]. Both protocols assume a model in which a task can perform one or more transactions. In the DPCP, global resources are guarded by synchronization processors. <p> Rajkumar et al. proposed two such mechanisms by extending the uniprocessor priority ceiling protocol (PCP) [8, 10]. The resulting protocols are called the distributed PCP (DPCP) and the multiprocessor PCP (MPCP), respectively <ref> [7, 8, 9] </ref>. Both protocols assume a model in which a task can perform one or more transactions. In the DPCP, global resources are guarded by synchronization processors. <p> T 2 detects that T 3 's transaction is complete, so it announces and executes its own transaction, and relinquishes the processor to T 3 . T 3 detects that its transaction has been completed, so it returns. priority inheritance protocol (PIP) <ref> [8, 10] </ref>. Like the PIP, information about which tasks access which objects is not required for implementing the helping scheme. (PCP-like schemes do require access information.) This is because conflicts are recorded dynamically through the use of the announce variable.
Reference: [9] <author> R. Rajkumar, L. Sha, J. Lehoczky, </author> <title> "Real-Time Synchronization Protocols for Multiprocessors", </title> <booktitle> Proc. IEEE Real-Time Sys. Symp., </booktitle> <year> 1988, </year> <pages> 259-269. </pages>
Reference-contexts: Rajkumar et al. proposed two such mechanisms by extending the uniprocessor priority ceiling protocol (PCP) [8, 10]. The resulting protocols are called the distributed PCP (DPCP) and the multiprocessor PCP (MPCP), respectively <ref> [7, 8, 9] </ref>. Both protocols assume a model in which a task can perform one or more transactions. In the DPCP, global resources are guarded by synchronization processors.
Reference: [10] <author> L. Sha, R. Rajkumar, J. Lehoczky, </author> <title> "Priority Inheritance Protocols: An Approach to Real-Time System Synchronization", </title> <journal> IEEE Trans. on Computers, </journal> <volume> 39(9), </volume> <year> 1990, </year> <pages> 1175-1185. </pages>
Reference-contexts: Rajkumar et al. proposed two such mechanisms by extending the uniprocessor priority ceiling protocol (PCP) <ref> [8, 10] </ref>. The resulting protocols are called the distributed PCP (DPCP) and the multiprocessor PCP (MPCP), respectively [7, 8, 9]. Both protocols assume a model in which a task can perform one or more transactions. In the DPCP, global resources are guarded by synchronization processors. <p> T 2 detects that T 3 's transaction is complete, so it announces and executes its own transaction, and relinquishes the processor to T 3 . T 3 detects that its transaction has been completed, so it returns. priority inheritance protocol (PIP) <ref> [8, 10] </ref>. Like the PIP, information about which tasks access which objects is not required for implementing the helping scheme. (PCP-like schemes do require access information.) This is because conflicts are recorded dynamically through the use of the announce variable. <p> These routines are called within user-supplied transaction code to read or write words of the MEM array; e.g., "Write (1; Read (10))" performs the assignment "MEM [1] := MEM <ref> [10] </ref>". Figure 4 shows a simple example transaction, which enqueues an item onto a shared queue. This transaction would be executed by calling Exec (Enqueue ).
Reference: [11] <author> J. Sun, R. Bettati, J. W.-S. Liu, </author> <title> "Using End-to-End Scheduling Approach to Schedule Tasks with Shared Resources in Multiprocessor Systems", </title> <booktitle> Proc. 11th IEEE Workshop on Real-Time Op. Sys. & Software, </booktitle> <year> 1994. </year>
Reference-contexts: In effect, such suspensions break a task into a sequence of subtasks, each of which may experience a priority inversion. Researchers at the University of Illinois have proposed an "end-to-end" approach for sharing resources in multiprocessors, in which tasks are converted into a sequence of periodic subtasks <ref> [4, 11] </ref>. Each subtask can perform either local computation or a single-processor transaction. Subtasks are scheduled on a per-processor basis, using uniprocessor scheduling schemes. Resources are accessed through the use of the PCP or a similar scheme. Like the DPCP, the end-to-end approach requires a binding of resources to processors.
References-found: 11

