URL: http://www.cs.nyu.edu/vijayk/papers/eff-coop.ps
Refering-URL: http://www.cs.nyu.edu/vijayk/papers.html
Root-URL: http://www.cs.nyu.edu
Email: fachien,feng,vijayk,jplevyakg@cs.uiuc.edu  
Title: Techniques for Efficient Execution of Fine-Grained Concurrent Programs  
Author: Andrew A. Chien, Wuchun Feng, Vijay Karamcheti and John Plevyak 
Affiliation: Department of Computer Science University of Illinois at Urbana-Champaign  
Abstract: Concurrent object-oriented programming languages are an attractive approach for programming massively-parallel machines. However, exploiting object-level concurrency is problematic as the linkage and communication overhead can overwhelm the benefits of the fine-grained concurrency. Our approach achieves efficient execution by tuning the grain size, matching the execution grain size to that efficiently supportable by the architecture. To verify the feasibility of grain-size tuning, we study the invocation locality of a collection of object-oriented programs. The results suggest that local constraints on placement combined with code specialization can produce a significant increase in execution grain size. We describe several compile-time analyses which identify opportunities to increase grain size. These analyses identify static relationships between objects and enable transformations to reduce invocation cost. Some initial measurements are presented.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, B. Lim, D. Kranz, and J. Kubiatowicz. </author> <month> April: </month> <title> A processor architecture for multiprocessing. </title> <booktitle> In International Symposium on Computer Architecture, </booktitle> <year> 1990. </year>
Reference-contexts: The first approach is typical to data-parallel programming languages, while our approach and the stream approach can work with more general heterogenous data structures. Grain-size tuning is an issue even in shared memory machines <ref> [1] </ref> since large grains are required to achieve reasonable execution efficiency. 6 Summary Our work focuses on making the execution of fine-grained concurrent object-oriented programs efficient. The key to our approach is to transform the execution grain size of programs to match the underlying hardware.
Reference: [2] <author> G. Agha. </author> <title> Actors: A Model of Concurrent Computation in Distributed Systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: 1 Introduction Concurrent object-oriented languages based on the Actor model <ref> [2] </ref> have received a great deal of attention as an approach for scalable programming of massively-parallel machines because concurrency control and modularity are naturally and conveniently captured in objects. Two critical implementation inefficiencies have prevented concurrent object-oriented languages from realizing their potential.
Reference: [3] <author> P. America. Pool-T: </author> <title> A parallel object-oriented language. </title> <editor> In A. Yonezawa and M. Tokoro, editors, </editor> <booktitle> Object-Oriented Concurrent Programming, </booktitle> <pages> pages 199-220. </pages> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: Summing the invocations to the preferred neighbors for each object, and normalizing by the num 3 We have examined CST [12], CA [8], ABCL [18], POOL <ref> [3] </ref>, Rosette [16], and Presto [4] programs. ber of invocations, yields the overall invocation locality over the program. This aggregate measure of communication to preferred neighbors approximates the reduction in communication which can be obtained by specializing the invocation sequence between an object and its preferred neighbors.
Reference: [4] <author> B. Bershad, E. Lazowska, and H. Levy. </author> <title> Presto: A system for object-oriented parallel programming. </title> <journal> Software Practice and Experience, </journal> <volume> 18(8), </volume> <year> 1988. </year>
Reference-contexts: Summing the invocations to the preferred neighbors for each object, and normalizing by the num 3 We have examined CST [12], CA [8], ABCL [18], POOL [3], Rosette [16], and Presto <ref> [4] </ref> programs. ber of invocations, yields the overall invocation locality over the program. This aggregate measure of communication to preferred neighbors approximates the reduction in communication which can be obtained by specializing the invocation sequence between an object and its preferred neighbors.
Reference: [5] <author> A. Black, N. Hutchinson, E. Jul, H. Levy, and L. Carter. </author> <title> Distribution and abstract types in Emerald. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-13(1):65-76, </volume> <month> January </month> <year> 1987. </year>
Reference-contexts: In fine-grained object-oriented languages, this overhead can easily dominate the overall cost of the computation. Our approach focuses on developing program analysis methods which identify data locality and transformation 1 Emerald <ref> [5] </ref>, Rosette [16], and Concurrent C++[15] all distinguish between active (first-class, mobile) and passive (private to an active object) objects. techniques which co-locate or merge objects to take advantage of this locality. An Example is given in Figure 2.
Reference: [6] <author> T. Blank. </author> <title> The Maspar MP-1 architecture. </title> <booktitle> In Proceedings of COMPCON, </booktitle> <pages> pages 20-4. </pages> <publisher> IEEE, </publisher> <year> 1990. </year>
Reference-contexts: Two bodies of work, both of which simultaneously optimize data placement and execution grain size, are similar to ours. First, the compiler for the MasPar MP-1 <ref> [6] </ref> lumps together operations on a number of array elements and allocates these chunks of work to individual processors. Second, efficient execution of concurrent logic languages has been obtained by grouping successive elements of a stream [14].
Reference: [7] <author> C. Chambers and D. Ungar. </author> <title> Iterative type analysis and extended message splitting. </title> <booktitle> In Proceedings of the SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 150-60, </pages> <year> 1990. </year>
Reference-contexts: Thus, the creation of the child object of class B can be pushed inside the initializer for class A. Because the container object of class A may be used for several different types, it must be type-split <ref> [7] </ref>, and new specialized versions of the container (parent) class code must be created. As a result, each type-split version of the container (parent) class becomes a possible site for the optimizations discussed in Section 3.1.1. <p> microseconds in the highly tuned J-machine. 6 If the amount of work is conserved, the grain size can be found by dividing the work by the number of messages. 5 Related Work Unlike the approaches for specializing invocations in sequential object-oriented languages which reduce the cost of type-dependent polymorphic dispatches <ref> [9, 7] </ref>, our approach specializes invocations so as to reduce communication and overhead due to message passing. Two bodies of work, both of which simultaneously optimize data placement and execution grain size, are similar to ours.
Reference: [8] <author> A. A. Chien and W. J. Dally. </author> <title> Concurrent Aggregates (CA). </title> <booktitle> In Proceedings of Second Symposium on Principles and Practice of Parallel Programming. ACM, </booktitle> <month> March </month> <year> 1990. </year>
Reference-contexts: Summing the invocations to the preferred neighbors for each object, and normalizing by the num 3 We have examined CST [12], CA <ref> [8] </ref>, ABCL [18], POOL [3], Rosette [16], and Presto [4] programs. ber of invocations, yields the overall invocation locality over the program.
Reference: [9] <author> L. P. Deutsch and A. M. Schiffman. </author> <title> Efficient implementation of the Smalltalk-80 system. </title> <booktitle> In Eleventh Symposium on Principles of Programming Languages, </booktitle> <pages> pages 297-302. </pages> <publisher> ACM, </publisher> <year> 1984. </year>
Reference-contexts: microseconds in the highly tuned J-machine. 6 If the amount of work is conserved, the grain size can be found by dividing the work by the number of messages. 5 Related Work Unlike the approaches for specializing invocations in sequential object-oriented languages which reduce the cost of type-dependent polymorphic dispatches <ref> [9, 7] </ref>, our approach specializes invocations so as to reduce communication and overhead due to message passing. Two bodies of work, both of which simultaneously optimize data placement and execution grain size, are similar to ours.
Reference: [10] <author> L. Hendren and A. Nicolau. </author> <title> Parallelizing programs with recursive data structures. </title> <journal> IEEE Transactions on Parallel and Distributed Computing, </journal> <volume> 1(1) </volume> <pages> 35-47, </pages> <year> 1990. </year>
Reference-contexts: a recursive call on f then insert (f,R) end Extension Calls for each c in C do if c contains an allocation of t then insert (c,E) end 3.2 Recursive Data Structures Exploiting class-level structure in object-oriented programs allows us to identify recursive data structures such as lists and trees <ref> [10] </ref> and transform their grain size. Typical implementations of recursive data structures in Actor languages localize the interesting control-flow information in the methods for one class.
Reference: [11] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiler Optimizations for Fortran D on MIMD Distributed-Memory Machines. </title> <booktitle> In Supercomputing '91, </booktitle> <pages> pages 86-100, </pages> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: In addition, to optimize locality, it must be possible to change the relative location of objects. Traditional approaches <ref> [11] </ref> define regular mappings from the index space of arrays to the set of memories. These direct mappings fully define the data placement in the machine and can be exploited to specialize code to that particular mapping.
Reference: [12] <author> W. Horwat, A. Chien, and W. Dally. </author> <title> Experience with CST: </title> <booktitle> Programming and implementation. In Proceedings of the SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 101-9. </pages> <booktitle> ACM SIGPLAN, </booktitle> <publisher> ACM Press, </publisher> <year> 1989. </year>
Reference-contexts: Summing the invocations to the preferred neighbors for each object, and normalizing by the num 3 We have examined CST <ref> [12] </ref>, CA [8], ABCL [18], POOL [3], Rosette [16], and Presto [4] programs. ber of invocations, yields the overall invocation locality over the program.
Reference: [13] <author> E. Myers. </author> <title> A precise interprocedural data flow algorithm. </title> <booktitle> In Seventh Symposium on Principles of Programming Languages, </booktitle> <pages> pages 219-30, </pages> <year> 1980. </year>
Reference-contexts: Due to the known difficulty of data-flow and aliasing analysis in the presence of pointers <ref> [17, 13] </ref>, the static analysis techniques 4 This number is conservative for CA programs in general because the use of aggregates and their randomized interface dissipates invocation locality. Locality can be enhanced by reducing the randomness of the interface.
Reference: [14] <author> V. Saraswat, K. Kahn, and J. Levy. </author> <title> Janus: A step towards distributed constraint programming. </title> <booktitle> In Proceedings of the North American Conference on Logic Programming, </booktitle> <address> Austin, Texas, </address> <month> October </month> <year> 1990. </year>
Reference-contexts: First, the compiler for the MasPar MP-1 [6] lumps together operations on a number of array elements and allocates these chunks of work to individual processors. Second, efficient execution of concurrent logic languages has been obtained by grouping successive elements of a stream <ref> [14] </ref>. The first approach is typical to data-parallel programming languages, while our approach and the stream approach can work with more general heterogenous data structures.
Reference: [15] <author> K. Smith and R. Smith II. </author> <title> The Experimental Systems Project at the Microelectronics and Computer Technology Corporation. </title> <booktitle> In Proceedings of the Fourth Conference on Hypercube Computers, </booktitle> <year> 1989. </year>
Reference: [16] <author> C. Tomlinson, M. Scheevel, and V. Singh. </author> <title> Report on Rosette 1.0. MCC Internal Report, Object-Based Concurrent Systems Project, </title> <month> December </month> <year> 1989. </year>
Reference-contexts: In fine-grained object-oriented languages, this overhead can easily dominate the overall cost of the computation. Our approach focuses on developing program analysis methods which identify data locality and transformation 1 Emerald [5], Rosette <ref> [16] </ref>, and Concurrent C++[15] all distinguish between active (first-class, mobile) and passive (private to an active object) objects. techniques which co-locate or merge objects to take advantage of this locality. An Example is given in Figure 2. <p> Summing the invocations to the preferred neighbors for each object, and normalizing by the num 3 We have examined CST [12], CA [8], ABCL [18], POOL [3], Rosette <ref> [16] </ref>, and Presto [4] programs. ber of invocations, yields the overall invocation locality over the program. This aggregate measure of communication to preferred neighbors approximates the reduction in communication which can be obtained by specializing the invocation sequence between an object and its preferred neighbors.
Reference: [17] <author> W. E. Weihl. </author> <title> Interprocedural data flow analysis in the presence of pointers, procedure variables, and label variables. </title> <booktitle> In Seventh Symposium on Principles of Programming Languages, </booktitle> <pages> pages 83-94, </pages> <year> 1980. </year>
Reference-contexts: Due to the known difficulty of data-flow and aliasing analysis in the presence of pointers <ref> [17, 13] </ref>, the static analysis techniques 4 This number is conservative for CA programs in general because the use of aggregates and their randomized interface dissipates invocation locality. Locality can be enhanced by reducing the randomness of the interface.
Reference: [18] <author> A. Yonezawa, </author> <title> editor. ABCL: An Object-Oriented Concurrent System. </title> <publisher> MIT Press, </publisher> <year> 1990. </year> <note> ISBN 0-262-24029-7. </note>
Reference-contexts: Summing the invocations to the preferred neighbors for each object, and normalizing by the num 3 We have examined CST [12], CA [8], ABCL <ref> [18] </ref>, POOL [3], Rosette [16], and Presto [4] programs. ber of invocations, yields the overall invocation locality over the program. This aggregate measure of communication to preferred neighbors approximates the reduction in communication which can be obtained by specializing the invocation sequence between an object and its preferred neighbors.
References-found: 18

