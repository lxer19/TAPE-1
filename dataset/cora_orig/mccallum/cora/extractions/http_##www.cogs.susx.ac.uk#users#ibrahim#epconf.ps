URL: http://www.cogs.susx.ac.uk/users/ibrahim/epconf.ps
Refering-URL: http://www.cs.bham.ac.uk/~wbl/biblio/gp-bibliography.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Email: ibrahim@cogs.susx.ac.uk  
Title: Evolution of Learning Rules for Hard Learning Problems  
Author: Ibrahim KUSCU 
Keyword: Supervised Learning, Genetic Programming, Three Monk's Problems, Parity Prob lems.  
Address: Brighton BN1 9QH  
Affiliation: Cognitive and Computing Sciences University of Sussex  
Abstract: Recent experiments with a genetic-based encoding schema are presented as a potentially useful tool in discovering learning rules by means of evolution. The representation strategy is similar to that used in genetic programming (GP) but it employs only a fixed set of functions to solve a variety of problems. In this paper, three Monk's and parity problems are tested. The results indicate the usefulness of the encoding schema in discovering learning rules for hard learning problems. The problems and future research directions are discussed within the context of GP practices. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D.J. Chalmers. </author> <title> Evolution of learning: an experiment in genetic connectionism. </title> <editor> In Touretzky et al, </editor> <title> 8 editor, Connectionist Models. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Combined with genetic algorithms it can successfully produce an evolution of learning rules. Rather than searching for a general learning algorithm (as in <ref> [1] </ref>), the aim is to see whether or not evolution would produce a specific learning rule for the problem in hand. The representation schema is very similar to the one used in genetic programming (GP) [6] .
Reference: [2] <author> A. Clark and C. Thornton. </author> <title> Trading spaces: Computation, representation and the limits of uninformed learning. Behavioral and Brain Sciences, </title> <publisher> Forthcoming. </publisher>
Reference-contexts: This means that there is a direct correlation between particular input values and particular output values. However, sometimes the rule may not refer to particular values of variables. Rather it may refer to possible relationships between input values. It has been shown <ref> [2] </ref> that learning behaviors based on training sets that involve a relationship among values of the input variables can be extremely difficult (named as type-2 learning problems).
Reference: [3] <author> S. Thrun et al. </author> <title> The monk's problems a performance comparison of different learning algorithms. </title> <type> Technical Report CMU-CS-91-197, </type> <institution> School of Computer Science, Carnegie-Mellon University., USA, </institution> <year> 1991. </year>
Reference-contexts: Then the experiments and the results will be presented. Finally, I will conclude with a discussion and future research possibilities using the genetic-based encoding schema. 2 Three Monk's Problems The three Monk's problems are used to compare the performance of different symbolic and non-symbolic learning techniques <ref> [3] </ref> including AQ17-DCI, AQ17-FCLS, AQ14-NT, AQ15-GA, Assistant Professional, mFOIL, ID5R-hat, TDIDT, ID3, AQR, CN2, CLASSWEB, ECOB-VEB, PRISM, Backpropagation and Cascade Correlation. Monk's problems involve classification of robots which are described by six different attributes. <p> RESULTS Original Coding Binary Coding Problems Training Testing Training Testing MONK 1 91 88 MONK 2 74 68 79 69 MONK 3 93 98 93.5 97 Table 1: Best performances in percentages The results obtained are better than some of the learning algorithms used in the comparison experiment of Thrun <ref> [3] </ref>. The performance on monk-1 and monk-3 is at the level of competing with most of the algorithms. Although the performance on monk-2 is very low, this is not surprising and similar to the results obtained by Thrun.
Reference: [4] <author> D. Goldberg. </author> <title> Genetic Algorithms in Search, Optimization and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <address> Massachusettes, </address> <year> 1989. </year>
Reference-contexts: The typical structure of an expression (flI1 fl ((flI2 fl +1) fl 0)) would look like as in Figure 1. 4.2 Genetic Aspects The Schema Theorem developed by Holland [5] based on genetic search has proven to be useful in many applications involving large, complex and deceptive search spaces <ref> [4] </ref>. So genetic search is most likely to allow fast, robust evolution of genotypes encoding for potential learning rules as mathematical expressions. Using Genetic Algorithms (GA) the model is implemented in LISP. The top level structure of the system exhibits the @R @ + @R *I2* 1 following: 1.
Reference: [5] <author> J. Holland. </author> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor, MI, USA, </address> <year> 1975. </year>
Reference-contexts: More details will be given about this later. The typical structure of an expression (flI1 fl ((flI2 fl +1) fl 0)) would look like as in Figure 1. 4.2 Genetic Aspects The Schema Theorem developed by Holland <ref> [5] </ref> based on genetic search has proven to be useful in many applications involving large, complex and deceptive search spaces [4]. So genetic search is most likely to allow fast, robust evolution of genotypes encoding for potential learning rules as mathematical expressions.
Reference: [6] <author> J. Koza. </author> <title> Genetic Programming:On the programming of computers by means of natural selection. </title> <publisher> MIT press, </publisher> <address> Cambridge, MA, </address> <year> 1992. </year>
Reference-contexts: Rather than searching for a general learning algorithm (as in [1]), the aim is to see whether or not evolution would produce a specific learning rule for the problem in hand. The representation schema is very similar to the one used in genetic programming (GP) <ref> [6] </ref> . However, introducing prior knowledge into the representation of initial solutions using problem-specific functions is minimal, if any at all. In this strategy potential learning rules are encoded as random mathematical expressions at variable lengths. The expressions are made up of random numbers and random variables. <p> The number of input variables increases from 6 to 17 since each possible value of the attributes is represented as 3 digit binary numbers where each digit represents the presence of a specific value of the attributes. 3 Genetic Programming In genetic programming paradigm <ref> [6] </ref> problems of artificial intelligence (AI) are viewed as the discovery of computer programs which produce desired outputs for particular inputs. A computer program could be an expression, formula, plan, control strategy, decision tree or a model depending on the sort of AI problem. Koza [6] claims that solving AI problems <p> Programming In genetic programming paradigm <ref> [6] </ref> problems of artificial intelligence (AI) are viewed as the discovery of computer programs which produce desired outputs for particular inputs. A computer program could be an expression, formula, plan, control strategy, decision tree or a model depending on the sort of AI problem. Koza [6] claims that solving AI problems requires searching the space of possible computer programs for the better fitting individual computer program. GP is a method of searching for this better fitting individual computer program based on Darwinian selection and genetic operations. <p> The selection of the functions and terminals are guided by the sufficiency property which states that "the set of terminals and the set of primitive functions be capable of expressing a solution to the problem" <ref> [6] </ref> p.86. Since there is not a universal set of functions which is capable of solving every problem, the need for reducing the set of primitives to a minimally sufficient set seems justified. However, how to choose a minimally sufficient set remains an open question.
Reference: [7] <author> I. Kuscu. </author> <title> Evolution of learning rules for supervised tasks i: Simple learning problems. </title> <type> Technical Report CSRP-394, Uni. </type> <institution> of Sussex, COGS, </institution> <year> 1995. </year>
Reference-contexts: In one of the studies [11] well-known learning algorithms such as ID3, backpropagation, and classifier systems were tested on a type-2 problem and all showed poor results. fl This research is funded by Middle East Technical University, Ankara, Turkiye In a previous paper <ref> [7] </ref> an encoding schema has been presented and tested on several simple supervised tasks. Combined with genetic algorithms it can successfully produce an evolution of learning rules. <p> F' = f %, fl; +; ; SQUASHING g In another paper, <ref> [7] </ref> I have shown that arithmetical functions together with SQUASHING function can define OR, AND and XOR and compared to these functions they are relatively more general. <p> The results of the experiments in <ref> [7] </ref> and being able to discover or re-represent solutions to monk-1 and monk-3 problems in this paper provided evidence in support of the first hypothesis. Failing to find a successful solution for monk-2 seems a poor support for the second hypothesis.
Reference: [8] <author> I. Kuscu. </author> <title> Incrementally learning the rules for supervised tasks: Monk's problems. </title> <type> Technical Report CSRP-396, Uni. </type> <institution> of Sussex, COGS, </institution> <year> 1995. </year>
Reference-contexts: The performance on monk-1 and monk-3 is at the level of competing with most of the algorithms. Although the performance on monk-2 is very low, this is not surprising and similar to the results obtained by Thrun. Moreover, in a recent extension of the experiments <ref> [8] </ref> where the representation is improved and the performance in learning, especially on the monk-2 problem, is increased. The results emphasise how the encoding can enable us to evolve learning rules for these problems with fixed, general and non-problem-specific set of functions. <p> The second issue is that more complex and larger problems might require solutions which are represented hierarchically. In fact, this is exactly what I have found through my recent experiments <ref> [8] </ref>. The new representation provides a direct hierarchical coding for the possible solutions to Monk's and parity problems and improves the possibility of finding solutions as well as the speed of it. <p> In recent experiments, the representation is allowed to be random expressions organised in layers so that it can code for larger and more complex solutions <ref> [8] </ref>. By incrementally building up expressions on the way towards the solution it has been found that (1) in every run a solution can be reached, (2) the solution is reached faster, and (3) the power of representation is improved to code for more complex and larger problems.
Reference: [9] <author> U. O'Reilly and F. Oppacher. </author> <title> An experimental perspective on genetic programming. </title> <editor> In R. Manner and B. Manderick, editors, </editor> <booktitle> Proc of 2nd Intl Conf on Parallel Problem Solving from Nature, Amster-dam, 1992. </booktitle> <publisher> North Holland. </publisher>
Reference-contexts: Although Koza claims that GP is most proper for those problems which require hierarchical representation, there is strong evidence in my experiments and in <ref> [9] </ref> that this aspect of the GP might be quite limited. In recent experiments, the representation is allowed to be random expressions organised in layers so that it can code for larger and more complex solutions [8].
Reference: [10] <author> D. Rumelhart, G. Hinton, and R. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. Rumelhart, J. McClelland, </editor> <title> and the PDP Research Group, editors, Parallel Distributed Processing: Explorations in the Micro-structures of Cognition. Vols I and II. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1986. </year>
Reference-contexts: Several functions have been tested in this mapping including the logistic activation function used by <ref> [10] </ref>.
Reference: [11] <author> C. Thornton. </author> <title> Supervised learning of conditional approach: a case study. </title> <type> Technical Report 291, </type> <institution> COGS, University of Sussex, </institution> <year> 1993. </year>
Reference-contexts: Rather it may refer to possible relationships between input values. It has been shown [2] that learning behaviors based on training sets that involve a relationship among values of the input variables can be extremely difficult (named as type-2 learning problems). In one of the studies <ref> [11] </ref> well-known learning algorithms such as ID3, backpropagation, and classifier systems were tested on a type-2 problem and all showed poor results. fl This research is funded by Middle East Technical University, Ankara, Turkiye In a previous paper [7] an encoding schema has been presented and tested on several simple supervised
Reference: [12] <author> D. Whitley. </author> <title> The genitor algorithm and why rank based-based allocation of reproductive trials is best. </title> <editor> In J.D. Schaffer, editor, </editor> <booktitle> Proceedings of Third International Conference on Genetic Algorithms, </booktitle> <pages> pages 116-123. </pages> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, CA, </address> <year> 1989. </year> <month> 9 </month>
Reference-contexts: The expressions are ranked after each generation according to their success. Those who are higher in the rank (higher scoring ones) are said to be most fitting expressions. 4.2.2 Selection In the model, parent selection technique for reproduction is normalizing by using an exponential function taken from Whitley's <ref> [12] </ref> rank-based selection technique. The function generates integer numbers from 1 to population size. The generation of numbers exhibits characteristics of a non-linear function where there is more tendency to produce smaller numbers (since higher scoring expressions are on top of the rank).
References-found: 12

