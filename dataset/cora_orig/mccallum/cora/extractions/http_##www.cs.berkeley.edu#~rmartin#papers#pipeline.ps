URL: http://www.cs.berkeley.edu/~rmartin/papers/pipeline.ps
Refering-URL: http://www.cs.berkeley.edu/~rmartin/abstracts.html
Root-URL: 
Title: Towards a Theory of Optimal Communication Pipelines  
Author: Randolph Y. Wang Arvind Krishnamurthy Richard P. Martin Thomas E. Anderson David E. Culler 
Abstract: In this paper, we study how to minimize the latency of a message through a network that consists of a number of store-and-forward stages. This research is especially relevant for today's low overhead communication subsystems that employ dedicated processing elements for protocol processing. We develop an abstract pipeline model that reveals a crucial performance tradeoff. We subsequently exploit this tradeoff and present a series of fragmentation algorithms designed to minimize message latency. We provide an experimental methodology that enables the construction of customized pipeline algorithms that can adapt to the specific pipeline characteristics and application workloads. By applying this methodology to the Myrinet-GAM system, we have improved its latency by up to 51%. We also study the effectiveness of this technique for other realistic cases. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anderson, T., Culler, D., Patterson, D., </author> <title> and the NOW team. A Case for NOW (Networks of Workstations). </title> <journal> IEEE Micro (Feb. </journal> <year> 1995), </year> <pages> 54-64. </pages>
Reference-contexts: In practice, when we apply the fragmentation model summarized by (12), k must be an integer in <ref> [1; B] </ref>. Due to the shape of the latency function T (k) (as shown in functions to (12) and choose the integer solution that minimizes T . We make several observations of this model. <p> We can also monitor changes of the pipeline parameters, such as those due to contention, and fine-tune the fragmentation at run time resulting in a customized fragmentation algorithm that adapts to user packet sizes and network characteristics. 5.2 Experimental Platform Our main experimental platform is the Berkeley NOW <ref> [1, 5] </ref>, which is a cluster of UltraSPARC Model 170 workstations running at 167 MHz. The host operating system is Solaris 2.5. The workstations are connected by Myrinet [3]. Each workstation is equipped with a Myricom M2F network interface card on the SBUS.
Reference: [2] <author> Anderson, T., Dahlin, M., Neefe, J., Patterson, D., Roselli, D., and Wang, R. </author> <title> Serverless Network File Systems. </title> <journal> ACM Transactions on Computer Systems 14, </journal> <month> 1 (Feb. </month> <year> 1996), </year> <pages> 41-79. </pages>
Reference-contexts: 1 Introduction The goal of this research is to answer a simple question: how do we minimize the latency of a message through a network that consists of a number of store-and-forward stages? This question arose during our effort to improve the performance of a distributed file system <ref> [2] </ref> on a high-speed local area network [3]. Two important characteristics of the communication pattern distinguish the file system from the supercomputer applications which traditionally run on these networks. The first is the synchronous nature of the communication.
Reference: [3] <author> Boden, N., Cohen, D., Felderman, R., Kulawik, A., Seitz, C., Seizovic, J., and Su, W. </author> <title> Myrinet A Gigabit-per-Second Local-Area Network. </title> <journal> IEEE MICRO (Feb. </journal> <year> 1995), </year> <pages> 29-36. </pages>
Reference-contexts: this research is to answer a simple question: how do we minimize the latency of a message through a network that consists of a number of store-and-forward stages? This question arose during our effort to improve the performance of a distributed file system [2] on a high-speed local area network <ref> [3] </ref>. Two important characteristics of the communication pattern distinguish the file system from the supercomputer applications which traditionally run on these networks. The first is the synchronous nature of the communication. <p> The host operating system is Solaris 2.5. The workstations are connected by Myrinet <ref> [3] </ref>. Each workstation is equipped with a Myricom M2F network interface card on the SBUS. Figure 6 shows the block diagram of the interface card.
Reference: [4] <author> Chun, B., Mainwaring, A., and Culler, D. </author> <title> Virtual Network Transport Protocols for Myrinet. </title> <booktitle> In Proc. of 1997 Hot Interconnects V (August 1997). </booktitle>
Reference-contexts: New communication software [21, 20] has significantly reduced host processing overhead, which in turn has made it possible to use finer-grained fragmentation to increase the parallelism in the communication subsystem. Among such communication subsystems, however, different approaches exist. For example, Active Messages (AM2) <ref> [4] </ref> do not fragment medium messages, while Fast Messages (FM) [14] use 128 byte fragments. In order to evaluate the soundness of these design choices, one needs a systematic approach.
Reference: [5] <author> Culler, D. E., Arpaci-Dusseau, A., Chun, B., Lumetta, S., Mainwaring, A., Martin, R., Yoshikawa, C., and Wong, F. </author> <title> Parallel Computing on the Berkeley NOW. </title> <booktitle> In 9th Joint Symposium on Parallel Processing (1997). </booktitle>
Reference-contexts: We can also monitor changes of the pipeline parameters, such as those due to contention, and fine-tune the fragmentation at run time resulting in a customized fragmentation algorithm that adapts to user packet sizes and network characteristics. 5.2 Experimental Platform Our main experimental platform is the Berkeley NOW <ref> [1, 5] </ref>, which is a cluster of UltraSPARC Model 170 workstations running at 167 MHz. The host operating system is Solaris 2.5. The workstations are connected by Myrinet [3]. Each workstation is equipped with a Myricom M2F network interface card on the SBUS.
Reference: [6] <author> Feeley, M. J., Morgan, W. E., Pighin, F. P., Karlin, A. R., Levy, H. M., and Thekkath, C. A. </author> <title> Implementing Global Memory Management in a Workstation Cluster. </title> <booktitle> In Proc. of the 15th ACM Symposium on Operating Systems Principles (December 1995), </booktitle> <pages> pp. 201-212. </pages>
Reference-contexts: These communication characteristics are not only common in other high performance distributed file systems [12, 19], they are also shared by distributed shared memory systems <ref> [10, 18, 6, 9] </ref> and database applications.
Reference: [7] <author> Jacobson, V. </author> <title> pathchar ATool to Infer Characteristics of Internet Paths. </title> <address> http://www.msri.org/sched/empennage /jacobson.html, </address> <year> 1997. </year>
Reference-contexts: However, their proposed changes to the internet architecture allow the application of our technique to the IP internet: the use of transparent fragmentation where each hop performs reassembly and the recording of path information in each packet allow high level protocols to intelligently choose fragment sizes. In pathchar <ref> [7] </ref>, Jacobson discusses a technique of measuring the latency and bandwidth characteristics of the individual hops on an internet path. By gradually increasing the "time-to-live" field of an IP packet, pathchar isolates the latency contributed by each additional hop and uncovers its characteristics.
Reference: [8] <author> Jamrozik, H. A., Feeley, M. J., Voelker, G. M., II, J. E., Karlin, A. R., Levy, H. M., and Vernon, M. K. </author> <title> Reducing Network Latency Using Subpages in a Global Memory Environment. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VII) (Oct. </booktitle> <year> 1996), </year> <pages> pp. 258-267. </pages>
Reference-contexts: In order to evaluate the soundness of these design choices, one needs a systematic approach. The lack of an analytical model and the heavy reliance on simulation and empirical experience in some previous research efforts such as <ref> [8] </ref> have made it hard to generalize their results to systems other than their own. In this paper, we develop a framework that may lead to a complete theory of optimal communication pipelines. We demonstrate several important optimality criteria. <p> diagnostic mechanism similar to the IP "time-to-live" field in a communication pipeline can allow the black-box measurement of detailed pipeline parameters. 12 GMS relies on simulation to find the optimal fragment size for sending an 8KB message through a pipeline that consists of an AN2 network and DEC Alpha workstations <ref> [8] </ref>.
Reference: [9] <author> Johnson, K. L., Kaashoek, M. F., and Wallach, D. A. </author> <title> CRL: High Performance All-Software Distributed Shared Memory. </title> <booktitle> In Proc. of the 15th ACM Symposium on Operating Systems Principles (December 1995), </booktitle> <pages> pp. 213-228. </pages>
Reference-contexts: These communication characteristics are not only common in other high performance distributed file systems [12, 19], they are also shared by distributed shared memory systems <ref> [10, 18, 6, 9] </ref> and database applications.
Reference: [10] <author> Keleher, P., Cox, A. L., Dwarkadas, S., and Zwaenepoel, W. TreadMarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <booktitle> In Proc. of the 1994 Winter Usenix Conference (January 1994), </booktitle> <pages> pp. 115-132. </pages>
Reference-contexts: These communication characteristics are not only common in other high performance distributed file systems [12, 19], they are also shared by distributed shared memory systems <ref> [10, 18, 6, 9] </ref> and database applications.
Reference: [11] <author> Kent, C. A., and Mogul, J. C. </author> <title> Fragmentation considered harmful. </title> <booktitle> In Proc. of Frontiers in Computer Communications Technology, ACM SIGCOMM (August 1987). </booktitle>
Reference-contexts: Kent and Mogul discussed this problem in <ref> [11] </ref> and argued that the higher level protocol must make an effort to use packets whose size matches the minimum of the maximum fragment allowed on the route. This is a compromise for the legacy systems on the internet and is not optimal.
Reference: [12] <author> Lee, E. K., and Thekkath, C. E. </author> <title> Petal: Distributed Virtual Disks. </title> <booktitle> In Seventh International Conference on Architectural Support for Programming Languages and Operating Systems (October 1996), </booktitle> <pages> pp. 84-92. </pages>
Reference-contexts: The file system, on the other hand, reads and writes file blocks (4KB or 8KB medium messages), whose performance characteristics, as we will see, are governed by a model that is not well understood. These communication characteristics are not only common in other high performance distributed file systems <ref> [12, 19] </ref>, they are also shared by distributed shared memory systems [10, 18, 6, 9] and database applications.
Reference: [13] <author> Martin, R. P., Vahdat, A. M., Culler, D. E., and An-derson, T. E. </author> <title> Effects of Communication Latency, Overhead, and Bandwidth in a Cluster Architecture. </title> <booktitle> In Proceedings of the Twenty-Fourth International Symposium on Computer Architecture (May 1997), </booktitle> <pages> pp. 85-97. </pages>
Reference-contexts: Two important characteristics of the communication pattern distinguish the file system from the supercomputer applications which traditionally run on these networks. The first is the synchronous nature of the communication. While many supercomputer applications tend to communicate asynchronously to mask communication latency <ref> [13] </ref>, a read miss in the file system cache, for example, blocks the application until the entire file block arrives. The second distinguishing characteristic is message size. <p> We provide a methodology that systematically uncovers communication pipeline parameters and constructs customized pipeline algorithms. We present empirical studies comparing theory to practice. In one example, we show that the discrepancy between the model prediction and the implementation measurement on Myrinet-GAM <ref> [13] </ref> averages 5.9%. By applying the fixed-sized fragmentation to this example system, we have achieved a performance improvement of up to 51%. We extend the study to important hypothetical pipelines including disks. The remainder of the paper is organized as follows. <p> The base communication subsystem we use in the study is Generic Active Messages (GAM) <ref> [13] </ref>, a version of Active Messages [20] enhanced to support cluster applications. One notable feature of this communication layer is its low overhead, which has enabled fine-grained fragmentation that would be infeasible on systems with higher overhead.
Reference: [14] <author> Pakin, S., Lauria, M., and Chien, A. </author> <title> High Performance Messaging on Workstations: Illinois Fast Messages (FM) for Myrinet. </title> <booktitle> In Proc. of Supercomputing '95 (November 1995). </booktitle>
Reference-contexts: Among such communication subsystems, however, different approaches exist. For example, Active Messages (AM2) [4] do not fragment medium messages, while Fast Messages (FM) <ref> [14] </ref> use 128 byte fragments. In order to evaluate the soundness of these design choices, one needs a systematic approach. <p> We compare the performance of our fixed-sized fragmentation algorithm with that of the original GAM implementation and that of FM <ref> [14] </ref>, both of which always use a static fragmentation strategy (128B for FM and 4KB for the original GAM). Figure 9 shows the result. FM suffers from the high overhead incurred by the small fragments for large packets.
Reference: [15] <author> Postel, J. </author> <title> Internet protocol. Request for Comments 791, </title> <journal> Information Sciences Institute, </journal> <month> Sept. </month> <year> 1981. </year>
Reference-contexts: Although traditional communication layers such as TCP/IP have examined the use of fragmentation in the context of managing congestion, buffer overflow, and errors in the wide area <ref> [15, 16] </ref>, they have not systematically addressed the issue of optimizing latency by properly fragmenting these medium messages as the high host overhead in these systems has made fine-grained fragmentation infeasible. Recent developments in high performance local area network technology have necessitated revisiting the message fragmentation issues. <p> From the experiments in this section, we conclude that the optimal algorithms are unlikely to obtain significant improvements over the simple combination of fixed-sized fragmentation and hierarchical fragmentation. 6 Related Work Internet protocols have long used fragmentation to manage packet buffering, congestion control, and packet losses <ref> [15, 16] </ref>. Due to the high overheads of these protocols, it is generally better to use large packets to maximize bandwidth. However, most datagram networks impose a maximum fragment size.
Reference: [16] <author> Postel, J. </author> <title> Transmission control protocol. Request for Comments 793, </title> <journal> Information Sciences Institute, </journal> <month> Sept. </month> <year> 1981. </year> <month> 13 </month>
Reference-contexts: Although traditional communication layers such as TCP/IP have examined the use of fragmentation in the context of managing congestion, buffer overflow, and errors in the wide area <ref> [15, 16] </ref>, they have not systematically addressed the issue of optimizing latency by properly fragmenting these medium messages as the high host overhead in these systems has made fine-grained fragmentation infeasible. Recent developments in high performance local area network technology have necessitated revisiting the message fragmentation issues. <p> From the experiments in this section, we conclude that the optimal algorithms are unlikely to obtain significant improvements over the simple combination of fixed-sized fragmentation and hierarchical fragmentation. 6 Related Work Internet protocols have long used fragmentation to manage packet buffering, congestion control, and packet losses <ref> [15, 16] </ref>. Due to the high overheads of these protocols, it is generally better to use large packets to maximize bandwidth. However, most datagram networks impose a maximum fragment size.
Reference: [17] <author> Prylli, L., and Tourancheau, B. </author> <title> New protocol design for high performance networking. </title> <type> Tech. Rep. 97-22, </type> <institution> LIP-ENS Lyon, 69364 Lyon, France, </institution> <year> 1997. </year>
Reference-contexts: The bottleneck shifts from the "Req-CPU" stage to the "Wire" stage as the fragment size increases. The Myrinet-BIP system <ref> [17] </ref> is the only other system that we are aware of that systematically adapts the fragment size to the user packet size.
Reference: [18] <author> Scales, D. J., and Lam, M. S. </author> <title> The Design and Evaluation of a Shared Object System for Distributed Memory Machines. </title> <booktitle> In Proc. of the First Symposium on Operating Systems Design and Implementation (November 1994), </booktitle> <pages> pp. 101-114. </pages>
Reference-contexts: These communication characteristics are not only common in other high performance distributed file systems [12, 19], they are also shared by distributed shared memory systems <ref> [10, 18, 6, 9] </ref> and database applications.

References-found: 18

