URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/scottd/www/aaai98-search.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/scottd/www/workstuff.html
Root-URL: 
Title: Applying Online Search Techniques to Continuous-State Reinforcement Learning key to the success of the local
Author: Scott Davies Andrew Y. Ng Andrew Moore 
Note: The  
Address: Pittsburgh, PA 15213  Cambridge, MA 02139  
Affiliation: School of Computer Science Carnegie-Mellon University  Artificial Intelligence Lab Massachusetts Institute of Technology  
Abstract: In this paper, we describe methods for efficiently computing better solutions to control problems in continuous state spaces. We provide algorithms that exploit online search to boost the power of very approximate value functions discovered by traditional reinforcement learning techniques. We examine local searches, where the agent performs a finite-depth lookahead search, and global searches, where the agent performs a search for a trajectory all the way from the current state to a goal state. The key to the success of the global methods lies in using aggressive state-space search techniques such as uniform-cost search and A fl , tamed into a tractable form by exploiting neighborhood relations and trajectory constraints that arise from continuous-space dynamic control. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Atkeson, C. G. </author> <year> 1989. </year> <title> Using Local Models to Control Movement. </title> <booktitle> In Proceedings of Neural Information Processing Systems Conference. </booktitle>
Reference: <author> Barto, A. G.; Bradtke, S. J.; and Singh, S. P. </author> <year> 1994. </year> <title> Real-time Learning and Control using Asynchronous Dynamic Programming. </title> <journal> AI Journal, </journal> <note> to appear (also published as UMass Amherst Technical Report 91-57 in 1991). </note>
Reference: <author> Barto, A. G.; Sutton, R. S.; and Anderson, C. W. </author> <year> 1983. </year> <title> Neuronlike Adaptive elements that that can learn difficult Control Problems. </title> <journal> IEEE Trans. on Systems Man and Cybernetics 13(5) </journal> <pages> 835-846. </pages>
Reference-contexts: The goal is to raise the hand at least one link's height above the shoulder (Sutton 1997). State consists of joint angles and angular velocities at the shoulder and elbow. Actions are positive or negative torque. * move-cart-pole (4 dimensional): A cart-and-pole system <ref> (Barto et al. 1983) </ref> starting with the pole upright is to be moved some distance to a goal state, keeping the pole upright (harder than the stabilization problem). It terminates with a huge penalty (10 6 ) if the pole falls over.
Reference: <author> Bertsekas, D. P. </author> <year> 1995. </year> <title> Dynamic Programming and optimal control, volume 1. </title> <publisher> Athena Scientific. </publisher>
Reference-contexts: The optimal value function is shown by dots. The shorter the time to goal, the larger the black dot. rithm above, executes the first action on that trajectory, and then does a new search from the resulting state. If B is the "parallel backup operator" <ref> (Bertsekas 1995) </ref> so that BV (s) = max a2A R (s; a) + flV (ffi (s; a)), then executing the full jAj d search is formally equivalent to executing the greedy policy with respect to the value function B d1 V .
Reference: <author> Boone, G. </author> <year> 1997. </year> <title> Minimum-Time Control of the Acrobot. </title> <booktitle> In International Conference on Robotics and Automation. </booktitle>
Reference-contexts: CLS: Constrained Local Search To make deeper searches computationally cheaper, we might consider only a subset of all possible trajectories of depth d. Especially for dynamic control, often an optimal trajectory repeatedly selects and then holds a certain action for some time, such as suggested by <ref> (Boone 1997) </ref>. Therefore, a natural subset of the jAj d possible trajectories are trajectories that switch their actions rarely.
Reference: <author> Boyan, J. A.; Moore, A. W.; and Sutton, R. S., eds. </author> <year> 1995. </year> <title> Proceedings of the Workshop on Value Function Approximation. </title> <booktitle> Machine Learning Conference: </booktitle> <address> CMU-CS-95-206. </address> <note> Web: http://www.cs.cmu.edu/~reinf/ml95/. </note>
Reference: <author> Burghes, D., and Graham, A. </author> <year> 1980. </year> <title> Introduction to Control Theory including Optimal Control. </title> <publisher> Ellis Horwood. </publisher>
Reference-contexts: The Copyright c fl1998, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. paper consists of a progression of improvements to conventional action-selection from value functions, along the way using techniques from value function approximation (Davies, 1997), real-time search (Korf, 1990), constrained trajectories <ref> (Burghes and Graham 1980) </ref>, and robot motion planning (Latombe 1991, Boyan et al. 1995, Boone 1997). All of the algorithms perform search online to find a good trajectory from some current state. Briefly, the progression is as follows: * LS: Local Search.
Reference: <author> Davies, S. </author> <year> 1997. </year> <title> Multidimensional Triangulation and Interpolation for Reinforcement Learning. </title> <booktitle> In Neural Information Processing Systems 9, 1996. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We restrict our attention to deterministic domains. The Copyright c fl1998, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. paper consists of a progression of improvements to conventional action-selection from value functions, along the way using techniques from value function approximation <ref> (Davies, 1997) </ref>, real-time search (Korf, 1990), constrained trajectories (Burghes and Graham 1980), and robot motion planning (Latombe 1991, Boyan et al. 1995, Boone 1997). All of the algorithms perform search online to find a good trajectory from some current state. Briefly, the progression is as follows: * LS: Local Search. <p> Learns a forward-dynamics model, and uses it to generate approximate value functions for the LS and CLS approaches. In this paper, the approximate value functions are obtained by k-dimensional simplex interpolation combined with value iteration <ref> (Davies 1997) </ref>, but the approaches are applicable for accelerating any model based reinforcement learning algorithm that produces approximate value functions, such as an LQR solution to a linearized problem or a neural net value function computed with TD. <p> An example of IGS on the mountain-parking domain is shown in Figure 6. The value function was approximated with a simplex-based interpolation <ref> (Davies 1997) </ref> on a coarse 7 by 7 grid, with all other parameters the same as in Figure 5. Much less of state space is searched than by UGS. In Figure 7, the value function was approximated more accurately with a simplex-based interpolation on a 21 by 21 grid. <p> For now, we consider only the case where we are given a model of the world, and leave the model-learning case to the next section. In this case, the value functions used during search (except by the Uniformed Global Search) are calculated using the simplex-interpolation algorithm described in <ref> (Davies 1997) </ref>; once generated, they need not be updated during the search process. Local Search Here, we look at the effects of different parameter settings for Local Search. We first consider move-cart-pole.
Reference: <author> Friedman, J. H.; Bentley, J. L.; and Finkel, R. A. </author> <year> 1977. </year> <title> An Algorithm for Finding Best Matches in Logarithmic Expected Time. </title> <journal> ACM Trans. on Mathematical Software 3(3) </journal> <pages> 209-226. </pages>
Reference-contexts: This does not preclude the use of online search techniques; as a toy example, Figure 9 shows cumulative reward learning curves for mountain-parking. For each action, a kd-tree implementation of 1-nearest-neighbor <ref> (Friedman et al. 1977) </ref> is used to learn the state transitions, and to encourage exploration, states sufficiently far from points stored in both trees are optimistically assumed to be mountain-parking with model learning. (Shallow gradients are good.) zero-cost absorbing states.
Reference: <author> Korf, R. E. </author> <year> 1990. </year> <title> Real-Time Heuristic Search. </title> <booktitle> Artifical Intelligence 42. </booktitle>
Reference-contexts: We restrict our attention to deterministic domains. The Copyright c fl1998, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. paper consists of a progression of improvements to conventional action-selection from value functions, along the way using techniques from value function approximation (Davies, 1997), real-time search <ref> (Korf, 1990) </ref>, constrained trajectories (Burghes and Graham 1980), and robot motion planning (Latombe 1991, Boyan et al. 1995, Boone 1997). All of the algorithms perform search online to find a good trajectory from some current state. Briefly, the progression is as follows: * LS: Local Search. <p> This approach, a form of receding horizon control, has most famously been applied to minimax game playing programs (Russell and Norvig 1995) and has also been used in single-agent systems on discrete domains (e.g. <ref> (Korf 1990) </ref>). In game-playing scenarios it has also been used in conjunction with automatically learned value functions, such as in Samuel's celebrated checkers program (Samuel 1959) and Tesauro's backgammon player (Tesauro and Galperin, 1997). <p> However, it might be better to use points along such trajectories to further update the value function in order to concentrate computational time and value function approxi-mator accuracy on the most relevant parts of the state space. The resulting algorithm would be reminiscent of Korf's RT A fl <ref> (Korf 1990) </ref> and Barto's RT DP (Barto et al. 1995). The trajectories found by the algorithms described in this paper use a small discrete set of actions, and do not always switch between these actions in a completely locally optimal manner.
Reference: <author> Latombe, J. </author> <year> 1991. </year> <title> Robot Motion Planning. </title> <publisher> Kluwer. </publisher>
Reference-contexts: All rights reserved. paper consists of a progression of improvements to conventional action-selection from value functions, along the way using techniques from value function approximation (Davies, 1997), real-time search (Korf, 1990), constrained trajectories (Burghes and Graham 1980), and robot motion planning <ref> (Latombe 1991, Boyan et al. 1995, Boone 1997) </ref>. All of the algorithms perform search online to find a good trajectory from some current state. Briefly, the progression is as follows: * LS: Local Search. <p> We assume the set of goal states is known. Why not continue growing a search tree until it finds a goal state? The answer is clear|the combinatorial explosion would be devastating. In order to deal with this problem, we borrow a technique from robot motion planning <ref> (Latombe 1991) </ref>. We first divide the state space up into a fine uniform grid. A sparse representation is used so that only grid cells that are visited take up memory 1 .
Reference: <author> Moore, A. W., and Atkeson, C. G. </author> <year> 1995. </year> <title> The Parti-game Algorithm for Variable Resolution Reinforcement Learning in Multidimensional State-spaces. </title> <booktitle> Machine Learning 21. </booktitle>
Reference-contexts: Experiments We tested our algorithms on the following domains 2 : * mountain-parking (2 dimensional): As described in the Introduction. This is slightly more difficult than the normal mountain-car problem, as we require a velocity near 0 at the top of the hill <ref> (Moore and Atkeson 1995) </ref>. State consists of x-position and ve locity. Actions are accelerate forward or backward. * acrobot (4 dimensional): An acrobot is a two-link planar robot acting in the vertical plane under gravity with only one weak actuator at its elbow joint. <p> Informed Global Search (IGS) is often much more tractable than Uninformed Global Search (UGS), even with relatively crudely approximated value func tions. 2. However, more accurate (yet computationally tractable) value function approximators may be needed than the simplex-grid-based approximators used here. 3. Variable resolution methods (e.g. extensions to <ref> (Moore and Atkeson 1995) </ref>) would probably be needed for the Global Search's state-space partitions rather than the uniform grids used here. The algorithms tested in this paper calculated the approximate value functions used by their search procedures independently of any particular trajectories that were subsequently searched or executed.
Reference: <author> Nilsson, N. J. </author> <year> 1971. </year> <booktitle> Problem-solving Methods in Artificial Intelligence. </booktitle> <publisher> McGraw Hill. </publisher>
Reference-contexts: A graph showing such a search for the mountain-parking domain is depicted in Figure 5. IGS: Informed Global Search We can modify Uninformed Global Search (UGS) by using an approximated value function to guide the search expansions in the style of A fl search <ref> (Nilsson 1971) </ref>, as written out in detail below. The search proceeds from the most promising-looking states first, where the "promise" of a state is the cost to get to the state (along previously searched trajectories) plus the remaining-cost-to-go as estimated with the value function.
Reference: <author> Russell, S., and Norvig, P. </author> <year> 1995. </year> <title> Artificial Intelligence A Modern Approach. </title> <publisher> Prentice Hall. </publisher>
Reference-contexts: For example, in discounted problems, if the largest absolute error in V is ", the largest absolute error in B d1 V is fl d1 ". This approach, a form of receding horizon control, has most famously been applied to minimax game playing programs <ref> (Russell and Norvig 1995) </ref> and has also been used in single-agent systems on discrete domains (e.g. (Korf 1990)). In game-playing scenarios it has also been used in conjunction with automatically learned value functions, such as in Samuel's celebrated checkers program (Samuel 1959) and Tesauro's backgammon player (Tesauro and Galperin, 1997). <p> On the other hand, it also typically means that the heuristic function used here may sometimes overestimate the cost required to get from some points to the goal, which can sometimes lead to suboptimal solutions that is, the approximated value function is not necessarily an optimistic or admissible heuristic <ref> (Russell and Norvig 1995) </ref>. <p> This is a phenomenon that often occurs in A fl -like searches when one's heuristic evaluation function is not strictly optimistic <ref> (Russell and Norvig 1995) </ref>. This is not a problem for UGS, which is effectively using the maximally optimistic "constant 0" evaluation function. <p> In domains where the action space is actually continuous, it would be useful to use a local trajectory optimization routine such as that used in (Atkeson, 1994) in order to fine-tune the discovered trajectories. Lastly, algorithms to learn reasonably accurate yet consistently "optimistic" <ref> (Russell and Norvig 1995) </ref> value functions might be helpful for Informed Global Search.
Reference: <author> Samuel, A. L. </author> <year> 1959. </year> <title> Some Studies in Machine Learning using the Game of Checkers. </title> <journal> IBM Journal on Research and Development 3. </journal> <note> Reprinted in E. </note> <editor> A. Feigenbaum and J. Feldman, editors, </editor> <booktitle> Computers and Thought, </booktitle> <publisher> McGraw-Hill, </publisher> <year> 1963. </year>
Reference-contexts: In game-playing scenarios it has also been used in conjunction with automatically learned value functions, such as in Samuel's celebrated checkers program <ref> (Samuel 1959) </ref> and Tesauro's backgammon player (Tesauro and Galperin, 1997). CLS: Constrained Local Search To make deeper searches computationally cheaper, we might consider only a subset of all possible trajectories of depth d.
Reference: <author> Sutton, R. S. </author> <year> 1996. </year> <title> Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding. </title> <editor> In Touretzky, D.; Mozer, M.; and Hasselmo, M., eds., </editor> <booktitle> Neural Information Processing Systems 8. </booktitle>
Reference-contexts: The point at which this least-cost trajectory first enters a grid element is used as the grid element's "representative state," and 1 This has a flavor not dissimilar to the hashed sparse coarse encodings of <ref> (Sutton 1996) </ref>. example. Velocity on x-axis, car position on y axis. Large black dot is starting state; the small dots are grid elements' "representative states." acts as the starting point for the local search. The rationale for the pruning is an assumed similarity among points in the same grid element.
Reference: <author> Tesauro, G., and Galperin, G. R. </author> <year> 1997. </year> <title> On-line Policy Improvement using Monte-Carlo Search. </title> <editor> In Mozer, M. C.; Jordan, M. I.; and Petsche, T., eds., </editor> <booktitle> Advances in Neural Information Processing Systems 9. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In game-playing scenarios it has also been used in conjunction with automatically learned value functions, such as in Samuel's celebrated checkers program (Samuel 1959) and Tesauro's backgammon player <ref> (Tesauro and Galperin, 1997) </ref>. CLS: Constrained Local Search To make deeper searches computationally cheaper, we might consider only a subset of all possible trajectories of depth d.
References-found: 17

