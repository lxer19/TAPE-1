URL: http://www.almaden.ibm.com/cs/people/ragrawal/papers/sigmod93.ps
Refering-URL: http://www.cs.bham.ac.uk/~anp/bibtex/kdd.bib.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Mining Association Rules between Sets of Items in Large Databases  
Author: Rakesh Agrawal Tomasz Imielinski Arun Swami 
Address: 650 Harry Road, San Jose, CA 95120  
Affiliation: IBM Almaden Research Center  
Abstract: We are given a large database of customer transactions. Each transaction consists of items purchased by a customer in a visit. We present an efficient algorithm that generates all significant association rules between items in the database. The algorithm incorporates buffer management and novel estimation and pruning techniques. We also present results of applying this algorithm to sales data obtained from a large retailing company, which shows the effectiveness of the algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Rakesh Agrawal, Tomasz Imielinski, and Arun Swami, </author> <title> "Database Mining: A Performance Perspective", </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <note> Special Issue on Learning and Discovery in Knowledge-Based Databases, (to appear). </note>
Reference-contexts: For this data set, the algorithm exhibited excellent performance. The estimation procedure exhibited high accuracy and the pruning techniques were able to prune out a very large fraction of itemsets without measuring them. The work reported in this paper has been done in the context of the Quest project <ref> [1] </ref> at the IBM Al-maden Research Center. In Quest, we are exploring the various aspects of the database mining problem.
Reference: [2] <author> Rakesh Agrawal, Sakti Ghosh, Tomasz Imielinski, Bala Iyer, and Arun Swami, </author> <title> "An Interval Classifier for Database Mining Applications", </title> <address> VLDB-92 , Vancouver, British Columbia, </address> <year> 1992, </year> <pages> 560-573. </pages>
Reference-contexts: In Quest, we are exploring the various aspects of the database mining problem. Besides the problem of discovering association rules, some other problems that we have looked into include the enhancement of the database capability with classification queries <ref> [2] </ref> and queries over large sequences. We believe that database mining is an important new application area for databases, combining commercial interest with intriguing research questions. Acknowledgments. We thank Mike Monnelly for his help in obtaining the data used in the performance experiments.
Reference: [3] <author> Dina Bitton, </author> <title> "Bridging the Gap Between Database Theory and Practice", Cadre Technologies, </title> <address> Menlo Park, </address> <year> 1992. </year>
Reference-contexts: There has been work in the database community on inferring functional dependencies from data, and efficient inference algorithms have been presented in <ref> [3] </ref> [8]. Functional dependencies are very specific predicate rules while our rules are propositional in nature. Contrary to our framework, the algorithms in [3] [8] consider strict satisfaction of rules. <p> There has been work in the database community on inferring functional dependencies from data, and efficient inference algorithms have been presented in <ref> [3] </ref> [8]. Functional dependencies are very specific predicate rules while our rules are propositional in nature. Contrary to our framework, the algorithms in [3] [8] consider strict satisfaction of rules. Due to the strict satisfaction, these algorithms take advantage of the implications between rules and do not consider rules that are logically implied by the rules already discovered.
Reference: [4] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone, </author> <title> Classification and Regression Trees, </title> <publisher> Wadsworth, </publisher> <address> Belmont, </address> <year> 1984. </year>
Reference-contexts: There has been considerable work in discovering classification rules: Given examples that belong to one of the pre-specified classes, discover rules for classifying them. Classic work in this area include <ref> [4] </ref> [9]. The algorithm we propose in this paper is targeted at discovering qualitative rules. However, the rules we discover are not classification rules. We have no pre-specified classes. Rather, we find all the rules that describe association between sets of items.
Reference: [5] <author> B. Falkenhainer and R. Michalski, </author> <title> "Integrating Quantitative and Qualitative Discovery: The ABACUS System", </title> <journal> Machine Learning, </journal> <volume> 1(4): </volume> <pages> 367-401. </pages>
Reference-contexts: Because too many formulas might fit the given data, the domain knowledge is generally used to provide the bias toward the formulas that are appropriate for the domain. Examples of some well-known systems in this category include ABACUS <ref> [5] </ref>, Bacon [7], and COPER [6]. Business databases reflect the uncontrolled real world, where many different causes overlap and many patterns are likely to co-exist [10]. Rules in such data are likely to have some uncertainty.
Reference: [6] <author> M. Kokar, </author> <title> "Discovering Functional Formulas through Changing Representation Base", </title> <booktitle> Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <year> 1986, </year> <pages> 455-459. </pages>
Reference-contexts: Because too many formulas might fit the given data, the domain knowledge is generally used to provide the bias toward the formulas that are appropriate for the domain. Examples of some well-known systems in this category include ABACUS [5], Bacon [7], and COPER <ref> [6] </ref>. Business databases reflect the uncontrolled real world, where many different causes overlap and many patterns are likely to co-exist [10]. Rules in such data are likely to have some uncertainty.
Reference: [7] <author> P. Langley, H. Simon, G. Bradshaw, and J. Zytkow, </author> <title> Scientific Discovery: Computational Explorations of the Creative Process, </title> <publisher> The MIT Press, </publisher> <address> Cam-bridge, Mass., </address> <year> 1987. </year>
Reference-contexts: Because too many formulas might fit the given data, the domain knowledge is generally used to provide the bias toward the formulas that are appropriate for the domain. Examples of some well-known systems in this category include ABACUS [5], Bacon <ref> [7] </ref>, and COPER [6]. Business databases reflect the uncontrolled real world, where many different causes overlap and many patterns are likely to co-exist [10]. Rules in such data are likely to have some uncertainty.
Reference: [8] <author> Heikki Mannila and Kari-Jouku Raiha, </author> <title> "Dependency Inference", </title> <address> VLDB-87, Brighton, England, </address> <year> 1987, </year> <pages> 155-158. </pages>
Reference-contexts: There has been work in the database community on inferring functional dependencies from data, and efficient inference algorithms have been presented in [3] <ref> [8] </ref>. Functional dependencies are very specific predicate rules while our rules are propositional in nature. Contrary to our framework, the algorithms in [3] [8] consider strict satisfaction of rules. <p> There has been work in the database community on inferring functional dependencies from data, and efficient inference algorithms have been presented in [3] <ref> [8] </ref>. Functional dependencies are very specific predicate rules while our rules are propositional in nature. Contrary to our framework, the algorithms in [3] [8] consider strict satisfaction of rules. Due to the strict satisfaction, these algorithms take advantage of the implications between rules and do not consider rules that are logically implied by the rules already discovered.
Reference: [9] <author> J. Ross Quinlan, </author> <title> "Induction of Decision Trees", </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <year> 1986, </year> <pages> 81-106. </pages>
Reference-contexts: There has been considerable work in discovering classification rules: Given examples that belong to one of the pre-specified classes, discover rules for classifying them. Classic work in this area include [4] <ref> [9] </ref>. The algorithm we propose in this paper is targeted at discovering qualitative rules. However, the rules we discover are not classification rules. We have no pre-specified classes. Rather, we find all the rules that describe association between sets of items.
Reference: [10] <author> G. Piatetsky-Shapiro, </author> <title> Discovery, Analysis, and Presentation of Strong Rules, </title> <booktitle> In [11], </booktitle> <pages> 229-248. </pages>
Reference-contexts: Examples of some well-known systems in this category include ABACUS [5], Bacon [7], and COPER [6]. Business databases reflect the uncontrolled real world, where many different causes overlap and many patterns are likely to co-exist <ref> [10] </ref>. Rules in such data are likely to have some uncertainty. The qualitative rule discovery programs are targeted at such business data and they generally use little or no domain knowledge. <p> However, the rules we discover are not classification rules. We have no pre-specified classes. Rather, we find all the rules that describe association between sets of items. An algorithm, called the KID3 algorithm, has been presented in <ref> [10] </ref> that can be used to discover the kind of association rules we have considered. The KID3 algorithm is fairly straightforward. Attributes are not restricted to be binary in this algorithm.
Reference: [11] <editor> G. Piatetsky-Shapiro (Editor), </editor> <title> Knowledge Discovery in Databases, </title> <publisher> AAAI/MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: It should be noted however that the remaining tuple optimization is a much cheaper optimization. 5 Related Work Discovering rules from data has been a topic of active research in AI. In <ref> [11] </ref>, the rule discovery programs have been categorized into those that find quantitative rules and those that find qualitative laws.
Reference: [12] <author> L.G. Valiant, </author> <title> "A Theory of Learnable", </title> <journal> CACM, </journal> <volume> 27, </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: Our algorithm is linear in number of transactions in the database. The work of Valiant <ref> [12] </ref> [13] deals with learning boolean formulae. Our rules can be viewed as boolean implications. However, his learnability theory deals mainly with worst case bounds under any possible probabilistic distribution.
Reference: [13] <author> L.G. Valiant, </author> <title> "Learning Disjunctions and Conjunctions", </title> <booktitle> IJCAI-85, </booktitle> <address> Los Angeles, </address> <year> 1985, </year> <pages> 560-565. </pages>
Reference-contexts: Our algorithm is linear in number of transactions in the database. The work of Valiant [12] <ref> [13] </ref> deals with learning boolean formulae. Our rules can be viewed as boolean implications. However, his learnability theory deals mainly with worst case bounds under any possible probabilistic distribution.
Reference: [14] <author> Yi-Hua Wu and Shulin Wang, </author> <title> Discovering Functional Relationships from Observational Data, </title> <booktitle> In [11], </booktitle> <pages> 55-70. </pages>
Reference-contexts: The purpose of quantitative rule discovery programs is to automate the discovery of numeric laws of the type commonly found in scientific data, such as Boyle's 8 law P V = c. The problem is stated as follows <ref> [14] </ref>: Given m variables x 1 ; x 2 ; . . . ; x m and k groups of observational data d 1 ; d 2 ; . . . ; d k , where each d i is a set of m values | one for each variable, find
References-found: 14

