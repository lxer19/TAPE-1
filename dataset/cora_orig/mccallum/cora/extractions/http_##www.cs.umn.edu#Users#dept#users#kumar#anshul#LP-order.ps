URL: http://www.cs.umn.edu/Users/dept/users/kumar/anshul/LP-order.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/anshul/
Root-URL: http://www.cs.umn.edu
Title: Graph Partitioning Based Sparse Matrix Orderings for Interior-Point Algorithms  
Author: Anshul Gupta P. O. Box 
Keyword: LIMITED DISTRIBUTION NOTICE  
Affiliation: Computer Science/Mathematics IBM Research  IBM Research Division T.J. Watson Research Center  Yorktown  IBM Research Division Almaden Austin China Haifa Tokyo Watson Zurich  
Address: 20467 (90480)  Heights, New York 10598  
Note: RC  
Pubnum: Report  
Email: anshul@watson.ibm.com  
Date: May 21, 1996  
Abstract: This report has been submitted for publication outside of IBM and will probably be copyrighted if accepted for publication. It has been issued as a Research Report for early dissemination of its contents. In view of the transfer of copyright to the outside publisher, its distribution outside of IBM prior to publication should be limited to peer communications and specific requests. After outside publication, requests should be filled only by reprints or legally obtained copies of the article (e.g., payment of royalties). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Cleve Ashcraft and Joseph W.-H. Liu. </author> <title> Robust ordering of sparse matrices using multisection. </title> <type> Technical Report CS 96-01, </type> <institution> Department of Computer Science, York University, </institution> <address> Ontario, Canada, </address> <year> 1996. </year>
Reference-contexts: This paper shows that MD based heuristics can be quite unsuitable for many linear programming problems. Recent work by the author [14], Hendrickson and Rothberg [19], Ashcraft and Liu <ref> [1] </ref>, and Karypis and Kumar [23, 22] suggests that GP based heuristics are capable of producing better quality orderings than MD based heuristics for finite-element problems while staying within a small constant factor of the run time of MD based heuristics. <p> On the other hand, node-separators of size two (i.e., a; b or k; l) are available with almost the same degree of imbalance between the two partitions. Another approach <ref> [2, 1, 19] </ref> is to find a node-separator within the coarse graph and refine it into a node-separator of the original graph in order to overcome the drawbacks of working with an edge-separator. However, even this approach is also not completely free of pitfalls. <p> The other two graph-partitioning based ordering softwares that we are 10 11 aware of, are Metis [23] and DDSEP <ref> [1] </ref>. Both of these were designed primarily for finite-element problems.
Reference: [2] <author> Cleve Ashcraft and Joseph W.-H. Liu. </author> <title> Generalized nested dissection: Some recent progress. </title> <booktitle> In Proceedings of Fifth SIAM Conference on Applied Linear Algebra, </booktitle> <address> Snowbird, Utah, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: On the other hand, node-separators of size two (i.e., a; b or k; l) are available with almost the same degree of imbalance between the two partitions. Another approach <ref> [2, 1, 19] </ref> is to find a node-separator within the coarse graph and refine it into a node-separator of the original graph in order to overcome the drawbacks of working with an edge-separator. However, even this approach is also not completely free of pitfalls. <p> After each uncoarsening step, we refine the node-separator; i.e., in the final steps of uncoarsening, the subject of refinement is changed from the edge-separator to a node-separator. For refining edge-separators, we use the popular Fiduccia-Mattheyses [7] heuristic, which is a linear time version of the Kernighan-Lin [25] heuristic. In <ref> [2] </ref>, Ashcraft and Liu extended the Fiduccia-Mattheyses framework to refine node-separators as well.
Reference: [3] <author> T. Bui and C. Jones. </author> <title> A heuristic for reducing fill in sparse matrix factorization. </title> <booktitle> In Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 445-452, </pages> <year> 1993. </year>
Reference-contexts: This is the basis of many efficient parallel algorithms for sparse matrix factorization [16, 21, 11]. A key step in our ordering algorithm is finding a small node bisector of a graph. Recent research <ref> [3, 18, 22, 14] </ref> has shown multilevel algorithms to be fast and effective in computing graph partitions. A typical multilevel graph partitioning algorithm has four components, namely coarsening, initial partitioning, uncoarsening, and refining. In [14], we describe in detail the general heuristic to partition a graph into k parts.
Reference: [4] <author> G. B. Dantzig. </author> <title> Linear Programming and Extensions. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1963. </year> <month> 17 </month>
Reference-contexts: In addition to the traditional Simplex method <ref> [4] </ref>, OSL includes subroutines that implement all three popular algorithmic approaches for the interior-point (barrier) methods, namely, the primal barrier method [13], the primal-dual method [31, 29], and the predictor-corrector method [32, 30].
Reference: [5] <author> Timothy A. Davis, Patrick Amestoy, and Iain S. Duff. </author> <title> An approximate minimum degree ordering al-gorithm. </title> <type> Technical Report TR-94-039, </type> <institution> Computer and Information Sciences Department, University of Florida, </institution> <address> Gainesville, FL, </address> <year> 1994. </year>
Reference-contexts: The Multiple Minimum Degree (MMD) algorithm by George and Liu [9, 10] and the Approximate Minimum Degree (AMD) algorithm by Davis, Amestoy, and Duff <ref> [5] </ref> represent the state of the art in MD based heuristics. Until now, with the exception of Meszaros [33] and Rothberg and Hendrickson [35], most researchers have focused on ordering sparse matrices arising in finite-element applications and these applications have guided the development of the ordering heuristics. <p> Section 3 shows that replacing an MD based ordering by the WGPP ordering reduced the total solution time of a suite of 52 medium to large LP problems by a factor of 2.2. Both MMD and AMD algorithms rely heavily on the concepts of mass elimination and indistinguishable nodes <ref> [10, 5] </ref> in order to achieve their run time efficiency. These concepts work very well for finite-element graphs, but may not be very effective for many relatively unstructured LP problems. <p> In our implementation, the size of the terminal subgraphs ranges from a few dozen nodes to a few hundred nodes depending on the size of the original graph. We use the AMD variation (due to Davis, Amestoy, and Duff <ref> [5] </ref>) of the minimum degree heuristic for ordering these small subgraphs. After the nodes of the two subgraphs at any level of recursion have been labeled, the nodes of the separator at that level are labeled and the algorithm returns to the previous level, if any. <p> Of these, only Metis is available at the time of writing this paper, and it does not work for a majority of LP problems. 3.1 Comparison with MMD and AMD In Table 1, we compare three codes, namely MMD by George and Liu [9], AMD by Davis, Amestoy, and Duff <ref> [5] </ref>, and WGPP [15] described in this paper on a dozen large LP problems. The table reports the run times of the three ordering codes on an IBM RS6000/590 work-station. All three were compiled with the highest level of optimization (-O3 option) of XLF 3.2 Fortran compiler. <p> I am also thankful to George Karypis and Vipin Kumar for useful discussions during the earlier part of this work and to Edward Rothberg for his comments on an earlier draft of this paper. The sparse matrix ordering subroutine of WGPP uses the AMD <ref> [5] </ref> code written by Tim Davis, Patrick Amestoy, and Iain Duff to label small subgraphs with a few hundred nodes.
Reference: [6] <author> Iain S. Duff and Torbjorn Wiberg. </author> <title> Remarks on implementations of O(n 1=2 o ) assignment algorithms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 14 </volume> <pages> 267-287, </pages> <year> 1988. </year>
Reference-contexts: A conventional approach is to [9, 23] refine the edge-separator between the two subgraphs after each step of uncoarsening so that few edges connect nodes of different subgraphs in the final partitioning of the original graph. Then an algorithm for finding a minimum cover <ref> [6, 34] </ref> is used to compute a node-separator from the edge-separator. This approach relies heavily on the assumption that the size of a node-separator is proportional to the size of the edge-separator containing it.
Reference: [7] <author> C. M. Fiduccia and R. M. Mattheyses. </author> <title> A linear time heuristic for improving network partitions. </title> <booktitle> In Proceedings of the 19th IEEE Design Automation Conference, </booktitle> <pages> pages 175-181, </pages> <year> 1982. </year>
Reference-contexts: After each uncoarsening step, we refine the node-separator; i.e., in the final steps of uncoarsening, the subject of refinement is changed from the edge-separator to a node-separator. For refining edge-separators, we use the popular Fiduccia-Mattheyses <ref> [7] </ref> heuristic, which is a linear time version of the Kernighan-Lin [25] heuristic. In [2], Ashcraft and Liu extended the Fiduccia-Mattheyses framework to refine node-separators as well.
Reference: [8] <author> Alan George. </author> <title> Nested dissection of a regular finite-element mesh. </title> <journal> SIAM Journal on Numerical Ananlysis, </journal> <volume> 10 </volume> <pages> 345-363, </pages> <year> 1973. </year>
Reference-contexts: The overall approach of our ordering algorithm follows the fundamental technique of generalized nested dissection <ref> [8] </ref>. <p> Intuitively, smaller node-separators mean less fill and less work during factorization. For finite-element graphs with certain nice properties, it can be proved that this technique yields orderings within a constant factor of the optimal <ref> [28, 27, 8, 9] </ref>. Such bounds cannot be proved for the unstructured LP matrices. However, as we show in Section 3, with suitable modifications, the strategy works quite well in practice even for matrices with arbitrary sparsity patterns.
Reference: [9] <author> Alan George and Joseph W.-H. Liu. </author> <title> Computer Solution of Large Sparse Positive Definite Systems. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year>
Reference-contexts: IP algorithms are iterative in nature and the most time-consuming computational step in each iteration is solving a (typically) sparse symmetric positive definite system of linear equations of the form (AD 2 A T )y = b. Solving this system using direct methods involves four steps <ref> [9] </ref>, namely, ordering, symbolic factorization, numerical factorization, and triangular solutions. Sparse matrix factorization derives its efficiency from avoiding arithmetic operations with zeros, which constitute the bulk of the matrix. During the process of factorization, some of the original zero entries in the matrix are overwritten by a nonzero value. <p> The initial success of MD based heuristics prompted intense research [10] to improve their run time and quality and they have been the methods of choice among practitioners. The Multiple Minimum Degree (MMD) algorithm by George and Liu <ref> [9, 10] </ref> and the Approximate Minimum Degree (AMD) algorithm by Davis, Amestoy, and Duff [5] represent the state of the art in MD based heuristics. <p> Intuitively, smaller node-separators mean less fill and less work during factorization. For finite-element graphs with certain nice properties, it can be proved that this technique yields orderings within a constant factor of the optimal <ref> [28, 27, 8, 9] </ref>. Such bounds cannot be proved for the unstructured LP matrices. However, as we show in Section 3, with suitable modifications, the strategy works quite well in practice even for matrices with arbitrary sparsity patterns. <p> The ultimate goal of the graph bisection phase of ordering is to find a small node-separator. Current GP based ordering algorithms follow two different approaches to finding a small node-separator. A conventional approach is to <ref> [9, 23] </ref> refine the edge-separator between the two subgraphs after each step of uncoarsening so that few edges connect nodes of different subgraphs in the final partitioning of the original graph. Then an algorithm for finding a minimum cover [6, 34] is used to compute a node-separator from the edge-separator. <p> Of these, only Metis is available at the time of writing this paper, and it does not work for a majority of LP problems. 3.1 Comparison with MMD and AMD In Table 1, we compare three codes, namely MMD by George and Liu <ref> [9] </ref>, AMD by Davis, Amestoy, and Duff [5], and WGPP [15] described in this paper on a dozen large LP problems. The table reports the run times of the three ordering codes on an IBM RS6000/590 work-station. <p> /NNZ W GPP 1.00 Table 4: Some statistics on the number of nonzeros in the triangular factor with OSL and WGPP orderings on the problems in Table 2. reduction (increase) in fill-in results in a large reduction (increase) in the operation count, and hence the run time, of numerical factorization <ref> [9] </ref>. Table 4 shows that WGPP reduced the fill-in in almost as many problems as the number of problems in which it increased the fill-in. However, most increases were much smaller than most decreases.
Reference: [10] <author> Alan George and Joseph W.-H. Liu. </author> <title> The evolution of the minimum degree ordering algorithm. </title> <journal> SIAM Review, </journal> <volume> 31(1) </volume> <pages> 1-19, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: GP based heuristics regard the symmetric sparse matrix as the adjacency matrix of a graph and follow a divide-and-conquer strategy to label the nodes of the graph by partitioning it into smaller subgraphs. The initial success of MD based heuristics prompted intense research <ref> [10] </ref> to improve their run time and quality and they have been the methods of choice among practitioners. <p> The initial success of MD based heuristics prompted intense research [10] to improve their run time and quality and they have been the methods of choice among practitioners. The Multiple Minimum Degree (MMD) algorithm by George and Liu <ref> [9, 10] </ref> and the Approximate Minimum Degree (AMD) algorithm by Davis, Amestoy, and Duff [5] represent the state of the art in MD based heuristics. <p> Section 3 shows that replacing an MD based ordering by the WGPP ordering reduced the total solution time of a suite of 52 medium to large LP problems by a factor of 2.2. Both MMD and AMD algorithms rely heavily on the concepts of mass elimination and indistinguishable nodes <ref> [10, 5] </ref> in order to achieve their run time efficiency. These concepts work very well for finite-element graphs, but may not be very effective for many relatively unstructured LP problems.
Reference: [11] <author> Alan George, Joseph W.-H. Liu, and Esmond G.-Y. Ng. </author> <title> Communication reduction in parallel sparse Cholesky factorization on a hypercube. </title> <editor> In M. T. Heath, editor, </editor> <booktitle> Hypercube Multiprocessors 1987, </booktitle> <pages> pages 576-586. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1987. </year>
Reference-contexts: In addition, graph bisection yields two submatrices of the original matrix that can be factored independently in parallel. This is the basis of many efficient parallel algorithms for sparse matrix factorization <ref> [16, 21, 11] </ref>. A key step in our ordering algorithm is finding a small node bisector of a graph. Recent research [3, 18, 22, 14] has shown multilevel algorithms to be fast and effective in computing graph partitions.
Reference: [12] <author> Madhurima Ghose and Edward Rothberg. </author> <title> A parallel implementation of the multiple minimum degree ordering heuristic. </title> <type> Technical report, </type> <institution> Old Dominion University, </institution> <address> Norfolk, VA, </address> <year> 1994. </year>
Reference-contexts: In addition to generating better quality orderings faster, GP based methods are more suitable for distributed-memory parallel computers than MD based methods. The only attempt to perform a minimum degree ordering in parallel that we are aware of <ref> [12] </ref> was not successful in reducing the ordering time over a serial implementation. On the other hand, there is a strong theoretical and experimental evidence of efficient parallel algorithms for graph partitioning and sparse matrix orderings based on graph partitioning [24].
Reference: [13] <author> P. E. Gill, W. Murray, M. A. Saunders, J. A. Tomlin, and M. H. Wright. </author> <title> On projected Newton barrier methods for linear programming and an equivalence to Karmarkar's projective method. </title> <journal> Mathematical Programming, </journal> <volume> 36 </volume> <pages> 183-209, </pages> <year> 1986. </year>
Reference-contexts: 1 Introduction Over the past decade, interior-point (IP) or barrier methods <ref> [20, 13, 31, 29, 32, 30] </ref> have become increasingly popular for solving medium- to large-size linear programming (LP) problems. <p> In addition to the traditional Simplex method [4], OSL includes subroutines that implement all three popular algorithmic approaches for the interior-point (barrier) methods, namely, the primal barrier method <ref> [13] </ref>, the primal-dual method [31, 29], and the predictor-corrector method [32, 30].
Reference: [14] <author> Anshul Gupta. </author> <title> Fast and effective algorithms for graph partitioning and sparse matrix reordering. </title> <type> Technical Report RC 20496 (90799), </type> <institution> IBM T. J. Watson Research Center, </institution> <address> Yorktown Heights, NY, </address> <month> July 10, </month> <year> 1996. </year> <note> Available on the WWW at the IBM Research CyberJournal site at http://www.research.ibm.com:8080/. </note>
Reference-contexts: As a result, the linear programming community has been using these well established ordering heuristics that were not originally developed for their applications. This paper shows that MD based heuristics can be quite unsuitable for many linear programming problems. Recent work by the author <ref> [14] </ref>, Hendrickson and Rothberg [19], Ashcraft and Liu [1], and Karypis and Kumar [23, 22] suggests that GP based heuristics are capable of producing better quality orderings than MD based heuristics for finite-element problems while staying within a small constant factor of the run time of MD based heuristics. <p> The algorithms presented in this paper are included in the graph partitioning and sparse matrix ordering package (WGPP) [15] 1 developed by the author. In <ref> [14] </ref>, we present experimental results of using WGPP for graph partitioning and for ordering sparse matrices arising in different applications. In this paper, we show that WGPP offers significant advantages over MD based heuristics for the matrices arising in IP computations. <p> This is the basis of many efficient parallel algorithms for sparse matrix factorization [16, 21, 11]. A key step in our ordering algorithm is finding a small node bisector of a graph. Recent research <ref> [3, 18, 22, 14] </ref> has shown multilevel algorithms to be fast and effective in computing graph partitions. A typical multilevel graph partitioning algorithm has four components, namely coarsening, initial partitioning, uncoarsening, and refining. In [14], we describe in detail the general heuristic to partition a graph into k parts. <p> Recent research [3, 18, 22, 14] has shown multilevel algorithms to be fast and effective in computing graph partitions. A typical multilevel graph partitioning algorithm has four components, namely coarsening, initial partitioning, uncoarsening, and refining. In <ref> [14] </ref>, we describe in detail the general heuristic to partition a graph into k parts. <p> To save coarsening time in the actual implementation, WGPP switches to using the modified edge weights only if heavy- and heaviest-edge matchings fail to reduce the size of the graph sufficiently. 2.1.2 Initial partitioning In <ref> [14] </ref>, we show that the initial partitioning phase can be completely eliminated, thereby simplifying and speeding up the overall partitioning algorithm. <p> The set of tagged and untagged nodes now form the two partitions of the graph. In <ref> [14] </ref>, we discuss that graph growing is the bottom-up equivalent of coarsening. We continue to coarsen the graph until it contains only two nodes, which constitute a natural initial partition. 2.1.3 Uncoarsening and refinement The uncoarsening and refining components of graph partitioning work together.
Reference: [15] <author> Anshul Gupta. WGPP: </author> <title> Watson graph partitioning (and sparse matrix ordering) package: Users manual. </title> <type> Technical Report RC 20453 (90427), </type> <institution> IBM T. J. Watson Research Center, </institution> <address> Yorktown Heights, NY, </address> <month> May 6, </month> <year> 1996. </year>
Reference-contexts: We show that, with our modifications, the benefits of GP based ordering over MD based ordering are even more pronounced for LP problems than for finite-element 2 problems. The algorithms presented in this paper are included in the graph partitioning and sparse matrix ordering package (WGPP) <ref> [15] </ref> 1 developed by the author. In [14], we present experimental results of using WGPP for graph partitioning and for ordering sparse matrices arising in different applications. In this paper, we show that WGPP offers significant advantages over MD based heuristics for the matrices arising in IP computations. <p> Metis is available at the time of writing this paper, and it does not work for a majority of LP problems. 3.1 Comparison with MMD and AMD In Table 1, we compare three codes, namely MMD by George and Liu [9], AMD by Davis, Amestoy, and Duff [5], and WGPP <ref> [15] </ref> described in this paper on a dozen large LP problems. The table reports the run times of the three ordering codes on an IBM RS6000/590 work-station. All three were compiled with the highest level of optimization (-O3 option) of XLF 3.2 Fortran compiler. <p> WGPP contains subroutines that replace OSL's internal minimum-degree based ordering subroutines (see <ref> [15] </ref> for details) if an application program is linked with both WGPP and OSL libraries. 2 In Table 2, we compare OSL's run times for solving a large randomly chosen suite of LP problems using IP methods with and without linking WGPP. <p> We describe a GP based ordering heuristic for LP problems that is a part of the more comprehensive WGPP library <ref> [15] </ref> developed at IBM Research for graph partitioning and ordering different types of sparse matrices. By means of experiments over a large suite of real-life LP problems, we demonstrate the superiority of WGPP ordering over other state-of-the-art ordering softwares in use, in terms of both ordering time and quality.
Reference: [16] <author> Anshul Gupta, George Karypis, and Vipin Kumar. </author> <title> Highly scalable parallel algorithms for sparse matrix factorization. </title> <type> Technical Report 94-63, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <year> 1994. </year> <note> To appear in IEEE Transactions on Parallel and Distributed Systems, 1997. Postscript file available via anonymous FTP from the site ftp://ftp.cs.umn.edu/users/kumar. </note>
Reference-contexts: GP based ordering also aids the parallelization of the factorization and triangular solution phases of a direct solver. Gupta, Karypis, and Kumar <ref> [16, 21] </ref> have proposed a highly scalable parallel formulation of sparse Cholesky factorization that derives a significant part of its parallelism from the underlying partitioning of the graph of the sparse matrix. <p> In addition, graph bisection yields two submatrices of the original matrix that can be factored independently in parallel. This is the basis of many efficient parallel algorithms for sparse matrix factorization <ref> [16, 21, 11] </ref>. A key step in our ordering algorithm is finding a small node bisector of a graph. Recent research [3, 18, 22, 14] has shown multilevel algorithms to be fast and effective in computing graph partitions. <p> GP based ordering also aids in effective parallelization of some of the key computations in an interior-point algorithm <ref> [16, 17, 21] </ref>. 5 Acknowledgements I am grateful to John Forrest for providing the test problems and for his help in creating the experimental set up by exposing OSL's internal ordering routines.
Reference: [17] <author> Anshul Gupta and Vipin Kumar. </author> <title> Parallel algorithms for forward and back substitution in direct solution of sparse linear systems. </title> <booktitle> In Supercomputing '95 Proceedings, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: Gupta, Karypis, and Kumar [16, 21] have proposed a highly scalable parallel formulation of sparse Cholesky factorization that derives a significant part of its parallelism from the underlying partitioning of the graph of the sparse matrix. In <ref> [17] </ref>, Gupta and Kumar present efficient parallel algorithms for solving lower- and upper-triangular systems resulting from sparse factorization. In both parallel factorization and triangular solutions, a part of the parallelism would be lost if an MD based heuristic is used to preorder the sparse matrix. <p> GP based ordering also aids in effective parallelization of some of the key computations in an interior-point algorithm <ref> [16, 17, 21] </ref>. 5 Acknowledgements I am grateful to John Forrest for providing the test problems and for his help in creating the experimental set up by exposing OSL's internal ordering routines.
Reference: [18] <author> Bruce Hendrickson and Robert Leland. </author> <title> A multilevel algorithm for partitioning graphs. </title> <booktitle> In Supercomputing '95 Proceedings, </booktitle> <year> 1995. </year> <note> Also available a Technical Report SAND93-1301, </note> <institution> Sandia National Laboratories, </institution> <address> Albuquerque, NM. </address>
Reference-contexts: This is the basis of many efficient parallel algorithms for sparse matrix factorization [16, 21, 11]. A key step in our ordering algorithm is finding a small node bisector of a graph. Recent research <ref> [3, 18, 22, 14] </ref> has shown multilevel algorithms to be fast and effective in computing graph partitions. A typical multilevel graph partitioning algorithm has four components, namely coarsening, initial partitioning, uncoarsening, and refining. In [14], we describe in detail the general heuristic to partition a graph into k parts. <p> Given a weighted graph after any stage of coarsening, there are several choices of matchings for the next coarsening step. A simple matching scheme <ref> [18] </ref> known as random matching (RM) randomly chooses pairs of connected unmatched nodes to include in the matching. In [22], Karypis and Kumar describe a heuristic known as heavy-edge matching (HEM).
Reference: [19] <author> Bruce Hendrickson and Edward Rothberg. </author> <title> Improving the runtime and quality of nested dissection ordering. </title> <type> Technical Report SAND96-0868J, </type> <institution> Sandia National Laboratories, </institution> <address> Albuquerque, NM, </address> <year> 1996. </year>
Reference-contexts: As a result, the linear programming community has been using these well established ordering heuristics that were not originally developed for their applications. This paper shows that MD based heuristics can be quite unsuitable for many linear programming problems. Recent work by the author [14], Hendrickson and Rothberg <ref> [19] </ref>, Ashcraft and Liu [1], and Karypis and Kumar [23, 22] suggests that GP based heuristics are capable of producing better quality orderings than MD based heuristics for finite-element problems while staying within a small constant factor of the run time of MD based heuristics. <p> On the other hand, node-separators of size two (i.e., a; b or k; l) are available with almost the same degree of imbalance between the two partitions. Another approach <ref> [2, 1, 19] </ref> is to find a node-separator within the coarse graph and refine it into a node-separator of the original graph in order to overcome the drawbacks of working with an edge-separator. However, even this approach is also not completely free of pitfalls.
Reference: [20] <author> Narendra Karmarkar. </author> <title> A new polynomial-time algorithm for linear programming. </title> <journal> Combinatorica, </journal> <volume> 4(8) </volume> <pages> 373-395, </pages> <year> 1984. </year>
Reference-contexts: 1 Introduction Over the past decade, interior-point (IP) or barrier methods <ref> [20, 13, 31, 29, 32, 30] </ref> have become increasingly popular for solving medium- to large-size linear programming (LP) problems.
Reference: [21] <author> George Karypis, Anshul Gupta, and Vipin Kumar. </author> <title> Parallel formulation of interior point algorithms. </title> <type> Technical Report 94-20, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <month> April </month> <year> 1994. </year> <note> A short version appears in Supercomputing '94 Proceedings. </note>
Reference-contexts: GP based ordering also aids the parallelization of the factorization and triangular solution phases of a direct solver. Gupta, Karypis, and Kumar <ref> [16, 21] </ref> have proposed a highly scalable parallel formulation of sparse Cholesky factorization that derives a significant part of its parallelism from the underlying partitioning of the graph of the sparse matrix. <p> In addition, graph bisection yields two submatrices of the original matrix that can be factored independently in parallel. This is the basis of many efficient parallel algorithms for sparse matrix factorization <ref> [16, 21, 11] </ref>. A key step in our ordering algorithm is finding a small node bisector of a graph. Recent research [3, 18, 22, 14] has shown multilevel algorithms to be fast and effective in computing graph partitions. <p> GP based ordering also aids in effective parallelization of some of the key computations in an interior-point algorithm <ref> [16, 17, 21] </ref>. 5 Acknowledgements I am grateful to John Forrest for providing the test problems and for his help in creating the experimental set up by exposing OSL's internal ordering routines.
Reference: [22] <author> George Karypis and Vipin Kumar. </author> <title> A fast and high quality multilevel scheme for partitioning irregular graphs. </title> <type> Technical Report TR 95-035, </type> <institution> Department of Computer Science, University of Minnesota, </institution> <year> 1995. </year>
Reference-contexts: This paper shows that MD based heuristics can be quite unsuitable for many linear programming problems. Recent work by the author [14], Hendrickson and Rothberg [19], Ashcraft and Liu [1], and Karypis and Kumar <ref> [23, 22] </ref> suggests that GP based heuristics are capable of producing better quality orderings than MD based heuristics for finite-element problems while staying within a small constant factor of the run time of MD based heuristics. <p> This is the basis of many efficient parallel algorithms for sparse matrix factorization [16, 21, 11]. A key step in our ordering algorithm is finding a small node bisector of a graph. Recent research <ref> [3, 18, 22, 14] </ref> has shown multilevel algorithms to be fast and effective in computing graph partitions. A typical multilevel graph partitioning algorithm has four components, namely coarsening, initial partitioning, uncoarsening, and refining. In [14], we describe in detail the general heuristic to partition a graph into k parts. <p> Given a weighted graph after any stage of coarsening, there are several choices of matchings for the next coarsening step. A simple matching scheme [18] known as random matching (RM) randomly chooses pairs of connected unmatched nodes to include in the matching. In <ref> [22] </ref>, Karypis and Kumar describe a heuristic known as heavy-edge matching (HEM). HEM not only helps reduce the run time of the refinement component of graph partitioning, but also tends to generate partitions with small separators. <p> One of the effective heuristics for initial partitioning is graph growing <ref> [22] </ref>.
Reference: [23] <author> George Karypis and Vipin Kumar. METIS: </author> <title> Unstructured graph partitioning and sparse matrix ordering system. </title> <type> Technical report, </type> <institution> Department of Computer Science, University of Minnesota, </institution> <year> 1995. </year>
Reference-contexts: This paper shows that MD based heuristics can be quite unsuitable for many linear programming problems. Recent work by the author [14], Hendrickson and Rothberg [19], Ashcraft and Liu [1], and Karypis and Kumar <ref> [23, 22] </ref> suggests that GP based heuristics are capable of producing better quality orderings than MD based heuristics for finite-element problems while staying within a small constant factor of the run time of MD based heuristics. <p> The ultimate goal of the graph bisection phase of ordering is to find a small node-separator. Current GP based ordering algorithms follow two different approaches to finding a small node-separator. A conventional approach is to <ref> [9, 23] </ref> refine the edge-separator between the two subgraphs after each step of uncoarsening so that few edges connect nodes of different subgraphs in the final partitioning of the original graph. Then an algorithm for finding a minimum cover [6, 34] is used to compute a node-separator from the edge-separator. <p> The other two graph-partitioning based ordering softwares that we are 10 11 aware of, are Metis <ref> [23] </ref> and DDSEP [1]. Both of these were designed primarily for finite-element problems.
Reference: [24] <author> George Karypis and Vipin Kumar. </author> <title> Parallel multilevel graph partitioning. </title> <type> Technical Report TR 95-036, </type> <institution> Department of Computer Science, University of Minnesota, </institution> <year> 1995. </year>
Reference-contexts: On the other hand, there is a strong theoretical and experimental evidence of efficient parallel algorithms for graph partitioning and sparse matrix orderings based on graph partitioning <ref> [24] </ref>. GP based ordering also aids the parallelization of the factorization and triangular solution phases of a direct solver.
Reference: [25] <author> B. W. Kernighan and S. Lin. </author> <title> An efficient heuristic procedure for partitioning graphs. </title> <journal> The Bell System Technical Journal, </journal> <year> 1970. </year>
Reference-contexts: After each uncoarsening step, we refine the node-separator; i.e., in the final steps of uncoarsening, the subject of refinement is changed from the edge-separator to a node-separator. For refining edge-separators, we use the popular Fiduccia-Mattheyses [7] heuristic, which is a linear time version of the Kernighan-Lin <ref> [25] </ref> heuristic. In [2], Ashcraft and Liu extended the Fiduccia-Mattheyses framework to refine node-separators as well.
Reference: [26] <institution> Optimization Subroutine Library. Guide and Reference. IBM Corporation, </institution> <note> Release 2.1, Fifth Edition, February 1995. Publication number SC23-0519-04. </note>
Reference-contexts: Thus, for the class of problems for which minimum degree is both fast and better than the graph partitioning based ordering, WGPP returns the permutation corresponding to the former. 3.2 Impact on IBM Optimization Subroutine Library The IBM Optimization Subroutine Library (OSL) <ref> [26] </ref> is a collection of high-performance mathematical subroutines for use by application programs that solve optimization problems.
Reference: [27] <author> R. J. Lipton, D. J. Rose, and R. E. Tarjan. </author> <title> Generalized nested dissection. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 16 </volume> <pages> 346-358, </pages> <year> 1979. </year>
Reference-contexts: Intuitively, smaller node-separators mean less fill and less work during factorization. For finite-element graphs with certain nice properties, it can be proved that this technique yields orderings within a constant factor of the optimal <ref> [28, 27, 8, 9] </ref>. Such bounds cannot be proved for the unstructured LP matrices. However, as we show in Section 3, with suitable modifications, the strategy works quite well in practice even for matrices with arbitrary sparsity patterns.
Reference: [28] <author> R. J. Lipton and R. E. Tarjan. </author> <title> A separator theorem for planar graphs. </title> <journal> SIAM Journal on Applied Mathematics, </journal> <volume> 36 </volume> <pages> 177-189, </pages> <year> 1979. </year>
Reference-contexts: Intuitively, smaller node-separators mean less fill and less work during factorization. For finite-element graphs with certain nice properties, it can be proved that this technique yields orderings within a constant factor of the optimal <ref> [28, 27, 8, 9] </ref>. Such bounds cannot be proved for the unstructured LP matrices. However, as we show in Section 3, with suitable modifications, the strategy works quite well in practice even for matrices with arbitrary sparsity patterns.
Reference: [29] <author> Irvin J. Lustig, Roy E. Marsten, and David F. Shanno. </author> <title> Computational experience with a primal-dual interior-point method for linear programming. </title> <journal> Linear Algebra and Its Applications, </journal> <volume> 152 </volume> <pages> 191-222, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction Over the past decade, interior-point (IP) or barrier methods <ref> [20, 13, 31, 29, 32, 30] </ref> have become increasingly popular for solving medium- to large-size linear programming (LP) problems. <p> In addition to the traditional Simplex method [4], OSL includes subroutines that implement all three popular algorithmic approaches for the interior-point (barrier) methods, namely, the primal barrier method [13], the primal-dual method <ref> [31, 29] </ref>, and the predictor-corrector method [32, 30].
Reference: [30] <author> Irvin J. Lustig, Roy E. Marsten, and David F. Shanno. </author> <title> On implementing Mehrotra's predictor-corrector interior-point method for linear programming. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 2(3) </volume> <pages> 435-449, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Over the past decade, interior-point (IP) or barrier methods <ref> [20, 13, 31, 29, 32, 30] </ref> have become increasingly popular for solving medium- to large-size linear programming (LP) problems. <p> In addition to the traditional Simplex method [4], OSL includes subroutines that implement all three popular algorithmic approaches for the interior-point (barrier) methods, namely, the primal barrier method [13], the primal-dual method [31, 29], and the predictor-corrector method <ref> [32, 30] </ref>.
Reference: [31] <author> N. Megiddo. </author> <title> Pathways to the optimal set in linear programming. </title> <editor> In N. Megiddo, editor, </editor> <booktitle> Progress in Mathematical Programming, </booktitle> <pages> pages 131-158. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: 1 Introduction Over the past decade, interior-point (IP) or barrier methods <ref> [20, 13, 31, 29, 32, 30] </ref> have become increasingly popular for solving medium- to large-size linear programming (LP) problems. <p> In addition to the traditional Simplex method [4], OSL includes subroutines that implement all three popular algorithmic approaches for the interior-point (barrier) methods, namely, the primal barrier method [13], the primal-dual method <ref> [31, 29] </ref>, and the predictor-corrector method [32, 30].
Reference: [32] <author> Sanjay Mehrotra. </author> <title> On the implementation of a primal-dual interior point method. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 2(4) </volume> <pages> 575-601, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Over the past decade, interior-point (IP) or barrier methods <ref> [20, 13, 31, 29, 32, 30] </ref> have become increasingly popular for solving medium- to large-size linear programming (LP) problems. <p> In addition to the traditional Simplex method [4], OSL includes subroutines that implement all three popular algorithmic approaches for the interior-point (barrier) methods, namely, the primal barrier method [13], the primal-dual method [31, 29], and the predictor-corrector method <ref> [32, 30] </ref>.
Reference: [33] <author> Csaba Meszaros. </author> <title> The inexact minimum local fill-in ordering algorithm. </title> <type> Technical Report WP 95-7, </type> <institution> Computer and Automation Research Institute, Hungarian Academy of Sciences, </institution> <year> 1995. </year>
Reference-contexts: The Multiple Minimum Degree (MMD) algorithm by George and Liu [9, 10] and the Approximate Minimum Degree (AMD) algorithm by Davis, Amestoy, and Duff [5] represent the state of the art in MD based heuristics. Until now, with the exception of Meszaros <ref> [33] </ref> and Rothberg and Hendrickson [35], most researchers have focused on ordering sparse matrices arising in finite-element applications and these applications have guided the development of the ordering heuristics. Extensive use of the interior-point methods for solving LP problems is relatively recent.
Reference: [34] <author> Alex Pothen and C.-J. Fan. </author> <title> Computing the block triangular form of a sparse matrix. </title> <journal> ACM Transactions on Mathematical Software, </journal> <year> 1990. </year>
Reference-contexts: A conventional approach is to [9, 23] refine the edge-separator between the two subgraphs after each step of uncoarsening so that few edges connect nodes of different subgraphs in the final partitioning of the original graph. Then an algorithm for finding a minimum cover <ref> [6, 34] </ref> is used to compute a node-separator from the edge-separator. This approach relies heavily on the assumption that the size of a node-separator is proportional to the size of the edge-separator containing it.
Reference: [35] <author> Edward Rothberg and Bruce Hendrickson. </author> <title> Sparse matrix ordering methods for interior point linear programming. </title> <type> Technical Report SAND96-0475J, </type> <institution> Sandia National Laboratories, </institution> <address> Albuquerque, NM, </address> <year> 1996. </year>
Reference-contexts: The Multiple Minimum Degree (MMD) algorithm by George and Liu [9, 10] and the Approximate Minimum Degree (AMD) algorithm by Davis, Amestoy, and Duff [5] represent the state of the art in MD based heuristics. Until now, with the exception of Meszaros [33] and Rothberg and Hendrickson <ref> [35] </ref>, most researchers have focused on ordering sparse matrices arising in finite-element applications and these applications have guided the development of the ordering heuristics. Extensive use of the interior-point methods for solving LP problems is relatively recent. <p> In this paper, we present a suite of algorithms that enhance the GP based ordering approach to generate robust orderings of sparse matrices arising in LP problem In <ref> [35] </ref>, Rothberg and Hendrickson independently presented evidence of the superiority of GP based ordering methods for LP problems using somewhat different graph partitioning methods.
Reference: [36] <author> M. Yannakakis. </author> <title> Computing the minimum fill-in is NP-complete. </title> <journal> SIAM Journal on Algebraic Discrete Methods, </journal> <volume> 2 </volume> <pages> 77-79, </pages> <year> 1981. </year> <month> 19 </month>
Reference-contexts: In addition to IP methods, computing a fill-reducing ordering or permutation of the rows and columns of a sparse matrix is an important problem in any application that uses a direct method for solving large sparse systems of linear equations. Finding an optimal ordering is an NP-complete problem <ref> [36] </ref> and heuristics must be used to obtain an acceptable non-optimal solution. Improving the run time and quality of ordering heuristics has been a subject of research for almost three decades.
References-found: 36

