URL: http://www.cs.unc.edu/~anderson/diss/moirdiss.ps.Z
Refering-URL: http://www.cs.unc.edu/~anderson/
Root-URL: http://www.cs.unc.edu
Title: Efficient Object Sharing in Shared-Memory Multiprocessors  
Author: by Mark Moir Prof. James Anderson, Adviser Prof. Maurice Herlihy, Reader Prof. Don Stanat, Reader 
Degree: A dissertation submitted to the faculty of the  in partial fulfillment of the requirements for the degree of Doctor of Philosophy in the  Approved by:  
Date: 1996  
Address: Chapel Hill  Chapel Hill  
Affiliation: University of North Carolina at  Department of Computer Science.  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Y. Afek, H. Attiya, D. Dolev, E. Gafni, M. Merritt, and N. Shavit, </author> <title> "Atomic Snapshots of Shared Memory", </title> <booktitle> Proceedings of the Ninth Annual Symposium on Principles of Distributed Computing, </booktitle> <year> 1990, </year> <pages> pp. 1-14. </pages>
Reference-contexts: These implementations include constructions of complex registers from simpler registers [22, 25, 39, 40, 58, 59, 66, 68, 78, 79, 81, 85, 88, 93]; atomic snapshots that allow multiple variables to be read atomically, <ref> [1, 4, 5, 16, 17, 32, 50, 55] </ref>, algorithms for maintaining timestamps [31, 33, 37, 52], and mechanisms for implementing any object whose operations satisfy certain algebraic requirements [6, 7, 13]. <p> To ensure that a SC operation does not overwrite the contents of the current buffer, the SC operations of each process p alternate between two buffers, BUF [p; 0] and BUF <ref> [p; 1] </ref>. <p> To 7 Recall that private variables in all figures are assumed to retain their values between procedure calls. 57 see why this ensures that the current buffer is not overwritten, observe that, if BUF [p; 0] is the current buffer, then process p will attempt a SC operation using BUF <ref> [p; 1] </ref> before it modifies BUF [p; 0] again. If p's SC succeeds, then BUF [p; 0] is no longer the current buffer. If p's SC fails, then some other process has performed a successful SC, which also implies that BUF [p; 0] is no longer the current buffer. <p> This work is performed by special Read and Write procedures, which are called by the sequential operation in order to read or write the MEM array. As a result, our constructions are not completely transparent to the sequential object designer. For example, instead of writing "MEM <ref> [1] </ref> := MEM [10]", the designer would write "Write (1; Read (10))". As a more concrete example, consider Figure 5.3, which contains the actual code we used to implement a queue using our constructions. <p> For process q, q@1 ^ q:h = 1 holds, so q is about to execute set first zero (X <ref> [1] </ref>). As X [1][1] is the first clear bit in X [1], q:1 will establish q@2 ^ q:h = 1 ^ q:v = 1, and will therefore acquire name 5. (b) Process p has released name 1 and process q has acquired name 5. executing set first zero (X [1]). <p> For process q, q@1 ^ q:h = 1 holds, so q is about to execute set first zero (X <ref> [1] </ref>). As X [1][1] is the first clear bit in X [1], q:1 will establish q@2 ^ q:h = 1 ^ q:v = 1, and will therefore acquire name 5. (b) Process p has released name 1 and process q has acquired name 5. executing set first zero (X [1]). <p> (X <ref> [1] </ref>). As X [1][1] is the first clear bit in X [1], q:1 will establish q@2 ^ q:h = 1 ^ q:v = 1, and will therefore acquire name 5. (b) Process p has released name 1 and process q has acquired name 5. executing set first zero (X [1]). Because each process tests the available names in segments, and because processes may release and acquire names concurrently, it may seem possible for a process to reach the last segment when none of the names in that segment are available. <p> [p]:bit))) Figure A.2: Definitions used in the correctness proof for the algorithm in Figures 5.5 and 5.6 (continued from Figure A.1). 174 invariant 0 p:curr :pid &lt; N ^ p:curr :tag 2 f0; 1g (I23) invariant 0 p:ptrs:help &lt; N ^ 0 BUF [p; 0]:help &lt; N ^ 0 BUF <ref> [p; 1] </ref>:help &lt; N (I24) invariant 0 p:try &lt; N (I25) invariant p@f2::5g ^ still valid (p) ) p:curr = X (I26) invariant p@f33::40g ) p:dcnt = 0 (I27) invariant p@6 ) :still valid (p) (I28) invariant p@27 ) 0 p:tmp N (I29) invariant p@f1::6; 26::27g ^ p:from = 29 )
Reference: [2] <author> Y. Afek, D. Dauber, and D. Touitou, </author> <title> "Wait-free Made Fast (Extended Abstract)", </title> <booktitle> Proceedings of the 27th Annual ACM Symposium on Theory of Computing, </booktitle> <year> 1995, </year> <pages> pp. 538-547. </pages>
Reference-contexts: a shared-memory multiprocessor that supports instructions (like compare-and-swap) with unbounded consensus number, any object can be implemented in a wait-free manner for any number of processes. 2.2.3 Universal Constructions In recent years, several groups of researchers have presented methods for automatically "transforming" sequential object implementations into wait-free or lock-free ones <ref> [2, 18, 41, 43, 44, 56, 77, 84] </ref>. These methods are called universal constructions. A universal 24 construction relieves the object designer of the need to reason about concurrency, thereby greatly simplifying the task of providing a correct lock-free or wait-free implementation for a particular shared object. <p> Herlihy's wait-free construction guarantees that this cannot happen, but does so at considerable expense. In particular, the time complexity of performing an operation is (N ), where N is the total number of processes, even if no other process attempts to perform an operation concurrently. Afek, Dauber, and Touitou <ref> [2] </ref> presented a wait-free, universal construction that attempts to achieve a middle ground: the time complexity of 29 their construction depends on the number of processes that concurrently access the object, rather than the total number of processes. <p> This is particularly problematic in applications that have different object access behaviors at different times, because it requires the backoff mechanism to be tuned dynamically. Wait-free implementations whose time complexity depends on contention, as proposed by Afek, Dauber, and Touitou <ref> [2] </ref>, have the potential overcome these problems. However, efficient constructions with this property remain to be seen. 166 * Existing wait-free universal constructions (including ours) do not benefit from parallel execution of operations. This could prove to be a severe disadvantage, because it limits scalability.
Reference: [3] <author> A. Afek, D. Dolev, E. Gafni, M. Merritt, and N. Shavit, </author> <title> "A Bounded, First-In, First-Enabled Solution to the `-Exclusion Problem", </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 16(3), </volume> <year> 1994, </year> <pages> pp. 939-953. </pages>
Reference-contexts: To see why two bits are needed to detect whether q's operation is complete, consider the scenario in Figure 5.7. In this figure, process p performs two operations. In the first, p's SC is successful, and p replaces RET [5] with RET <ref> [3] </ref> as the current return block at line 11. During p's first operation, q starts an operation. However, q starts this operation too late to be helped by p. Before p's execution of line 11, q determines that RET [5] is the current return block (line 4). <p> With Contention Contention-Free Instructions Used [34] 1 fi (1) Large Critical Sections [35] 1 fi (1) Large Critical Sections [30] 1 fi (N 2 ) Safe Bits <ref> [3] </ref> 1 fi (N ) Atomic Read and Write [26] 1 fi (N ) Atomic Read and Write Thm. 9 fi (k log (N=k)) fi (1) Fetch-and-Add, Test-and-Set Thm. 10 fi (c) fi (1) Fetch-and-Add, Test-and-Set Thm. 13 fi (k log (N=k)) fi (1) Fetch-and-Add, Test-and-Set Thm. 14 fi (c) fi
Reference: [4] <author> J. Anderson, </author> <title> "Composite Registers", </title> <journal> Distributed Computing, </journal> <volume> 6(3), </volume> <year> 1993, </year> <pages> pp. 141-154. </pages>
Reference-contexts: These implementations include constructions of complex registers from simpler registers [22, 25, 39, 40, 58, 59, 66, 68, 78, 79, 81, 85, 88, 93]; atomic snapshots that allow multiple variables to be read atomically, <ref> [1, 4, 5, 16, 17, 32, 50, 55] </ref>, algorithms for maintaining timestamps [31, 33, 37, 52], and mechanisms for implementing any object whose operations satisfy certain algebraic requirements [6, 7, 13].
Reference: [5] <author> J. Anderson, </author> <title> "Multi-Writer Composite Registers", </title> <journal> Distributed Computing, </journal> <volume> 7(4), </volume> <year> 1994, </year> <pages> pp. 175-195. </pages>
Reference-contexts: These implementations include constructions of complex registers from simpler registers [22, 25, 39, 40, 58, 59, 66, 68, 78, 79, 81, 85, 88, 93]; atomic snapshots that allow multiple variables to be read atomically, <ref> [1, 4, 5, 16, 17, 32, 50, 55] </ref>, algorithms for maintaining timestamps [31, 33, 37, 52], and mechanisms for implementing any object whose operations satisfy certain algebraic requirements [6, 7, 13]. <p> To see why two bits are needed to detect whether q's operation is complete, consider the scenario in Figure 5.7. In this figure, process p performs two operations. In the first, p's SC is successful, and p replaces RET <ref> [5] </ref> with RET [3] as the current return block at line 11. During p's first operation, q starts an operation. However, q starts this operation too late to be helped by p. Before p's execution of line 11, q determines that RET [5] is the current return block (line 4). <p> p's SC is successful, and p replaces RET <ref> [5] </ref> with RET [3] as the current return block at line 11. During p's first operation, q starts an operation. However, q starts this operation too late to be helped by p. Before p's execution of line 11, q determines that RET [5] is the current return block (line 4). Now, p starts a second operation. Because p previously replaced RET [5] as the current return block, RET [5] is now p's private copy, so p's second operation uses RET [5] to record the operations it helps. <p> During p's first operation, q starts an operation. However, q starts this operation too late to be helped by p. Before p's execution of line 11, q determines that RET <ref> [5] </ref> is the current return block (line 4). Now, p starts a second operation. Because p previously replaced RET [5] as the current return block, RET [5] is now p's private copy, so p's second operation uses RET [5] to record the operations it helps. When p executes line 25, it changes q's applied bit to indicate that it has applied q's operation. <p> However, q starts this operation too late to be helped by p. Before p's execution of line 11, q determines that RET <ref> [5] </ref> is the current return block (line 4). Now, p starts a second operation. Because p previously replaced RET [5] as the current return block, RET [5] is now p's private copy, so p's second operation uses RET [5] to record the operations it helps. When p executes line 25, it changes q's applied bit to indicate that it has applied q's operation. <p> Before p's execution of line 11, q determines that RET <ref> [5] </ref> is the current return block (line 4). Now, p starts a second operation. Because p previously replaced RET [5] as the current return block, RET [5] is now p's private copy, so p's second operation uses RET [5] to record the operations it helps. When p executes line 25, it changes q's applied bit to indicate that it has applied q's operation. Note that, at this stage, q's operation has only been applied to p's private object copy, and p has not yet performed its SC. <p> Note that, at this stage, q's operation has only been applied to p's private object copy, and p has not yet performed its SC. However, if q reads the applied bit of RET <ref> [5] </ref> (which it previously determined to be the current RET block) at line 4, then q incorrectly concludes that its operation has been applied to the object, and terminates prematurely. <p> Our decision to evaluate our k-exclusion algorithms by their remote-reference counts | that is, to distinguish between local and remote accesses of shared memory | is motivated by recent work on local-spin spin locks <ref> [5, 12, 38, 72, 97, 98] </ref>. In such locks, the impact of the processor-to-memory bottleneck is minimized by structuring programs so that processes busy wait only on locally-accessible shared variables.
Reference: [6] <author> J. Anderson and B. Groselj, </author> <title> "Beyond Atomic Registers: Bounded Wait-Free Implementations of Nontrivial Objects", </title> <booktitle> Science of Computer Programming, </booktitle> <year> 1992, </year> <pages> pp. 192-237. </pages>
Reference-contexts: 39, 40, 58, 59, 66, 68, 78, 79, 81, 85, 88, 93]; atomic snapshots that allow multiple variables to be read atomically, [1, 4, 5, 16, 17, 32, 50, 55], algorithms for maintaining timestamps [31, 33, 37, 52], and mechanisms for implementing any object whose operations satisfy certain algebraic requirements <ref> [6, 7, 13] </ref>.
Reference: [7] <author> J. Anderson and M. Moir. </author> <title> "Towards A Necessary and Sufficient Condition for Wait-Free Synchronization", </title> <booktitle> Proceedings of the Seventh International Workshop on Distributed Algorithms, </booktitle> <year> 1993, </year> <pages> pp. 39-53. </pages>
Reference-contexts: 39, 40, 58, 59, 66, 68, 78, 79, 81, 85, 88, 93]; atomic snapshots that allow multiple variables to be read atomically, [1, 4, 5, 16, 17, 32, 50, 55], algorithms for maintaining timestamps [31, 33, 37, 52], and mechanisms for implementing any object whose operations satisfy certain algebraic requirements <ref> [6, 7, 13] </ref>. <p> For example, a construction is given in <ref> [7] </ref> that implements any object such that, for each pair of operations on the object, either the two operations commute with each other, or one overwrites the other (i.e., the effects of executing both operations is the same as executing just one of them).
Reference: [8] <author> J. Anderson and M. Moir, </author> <title> "Using k-Exclusion to Implement Resilient, Scalable Shared Objects", </title> <booktitle> Proceedings of the 13th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <year> 1994, </year> <pages> pp. 141-150. </pages>
Reference-contexts: In Chapter 7, we study the renaming problem. Finally, conclusions and a discussion of future directions for this research appear in Chapter 8. Some of the lengthier proofs from Chapters 5 through 7 appear in appendices. (This dissertation includes work that is based, with permission, on previously published work <ref> [8, 9, 10, 74, 75] </ref>.) Chapter 2 Related Work 2.1 Mutual Exclusion Algorithms In the mutual exclusion, each of a set of concurrent processes repeatedly executes a noncritical section of code and a critical section of code. <p> This is achieved by designing algorithms in which all busy waiting is performed on locally-accessible shared variables that are statically allocated to processes. The algorithms presented here are much simpler and more efficient than the distributed shared-memory algorithms we presented in <ref> [8] </ref>. Our approach here is the same as that of Section 6.3. In particular, we inductively reduce the problem of implementing (N; k)-exclusion to that of implementing (k + 1; k)- exclusion. We present two algorithms for (k + 1; k)-exclusion, both of which have constant remote-reference counts.
Reference: [9] <author> J. Anderson and M. Moir, </author> <title> "Universal Constructions for Multi-Object Operations", </title> <booktitle> Proceedings of the 14th Annual ACM Symposium on Principles of Distributed Computing , 1995, </booktitle> <pages> pp. 184-194. </pages>
Reference-contexts: In Chapter 7, we study the renaming problem. Finally, conclusions and a discussion of future directions for this research appear in Chapter 8. Some of the lengthier proofs from Chapters 5 through 7 appear in appendices. (This dissertation includes work that is based, with permission, on previously published work <ref> [8, 9, 10, 74, 75] </ref>.) Chapter 2 Related Work 2.1 Mutual Exclusion Algorithms In the mutual exclusion, each of a set of concurrent processes repeatedly executes a noncritical section of code and a critical section of code. <p> Thus, they are not the same as the multi-word operations considered in <ref> [9, 18, 54, 84] </ref>, which access multiple variables that are stored in separate, and not necessarily contiguous, memory words. The multi-word operations we consider admit simpler and more efficient implementations than those considered in [9, 18, 54, 84]. 1 The SC operation terminates in fi (1) time after the most recent <p> Thus, they are not the same as the multi-word operations considered in <ref> [9, 18, 54, 84] </ref>, which access multiple variables that are stored in separate, and not necessarily contiguous, memory words. The multi-word operations we consider admit simpler and more efficient implementations than those considered in [9, 18, 54, 84]. 1 The SC operation terminates in fi (1) time after the most recent spurious FSC. <p> We would like to extend our constructions to take advantage of such parallel execution where possible. For example, in our shared queue implementations, an enqueue operation might unnecessarily interfere with a dequeue operation. In <ref> [9] </ref>, we addressed similar concerns when implementing wait-free operations on multiple objects. In Chapter 6, we presented a technique for improving the time and space efficiency of wait-free, universal constructions, as well as the space efficiency of lock-free ones. This technique is based on efficient k-assignment algorithms. <p> This could prove to be a severe disadvantage, because it limits scalability. Therefore, designing universal constructions that do exploit parallelism is an important research direction. We have made some progress towards this goal elsewhere <ref> [9] </ref> by designing constructions for implementing multiple objects and supporting multi-object operations that execute in parallel where possible. Drawing on the techniques presented in [9] and in Chapter 5, it should be possible to implement general, wait-free "transactions" (arbitrary operations) that execute in parallel if they do not overlap. 167 Appendix <p> Therefore, designing universal constructions that do exploit parallelism is an important research direction. We have made some progress towards this goal elsewhere <ref> [9] </ref> by designing constructions for implementing multiple objects and supporting multi-object operations that execute in parallel where possible. Drawing on the techniques presented in [9] and in Chapter 5, it should be possible to implement general, wait-free "transactions" (arbitrary operations) that execute in parallel if they do not overlap. 167 Appendix A Correctness Proofs for Algorithms in Chapter 5 In this appendix, we provide a formal correctness proof for the wait-free construction for large objects
Reference: [10] <author> J. Anderson and M. Moir, </author> <title> "Universal Constructions for Large Objects", </title> <booktitle> Proceedings of the Ninth International Workshop on Distributed Algorithms, </booktitle> <year> 1995, </year> <pages> pp. 168-182. </pages>
Reference-contexts: In Chapter 7, we study the renaming problem. Finally, conclusions and a discussion of future directions for this research appear in Chapter 8. Some of the lengthier proofs from Chapters 5 through 7 appear in appendices. (This dissertation includes work that is based, with permission, on previously published work <ref> [8, 9, 10, 74, 75] </ref>.) Chapter 2 Related Work 2.1 Mutual Exclusion Algorithms In the mutual exclusion, each of a set of concurrent processes repeatedly executes a noncritical section of code and a critical section of code. <p> Elsewhere <ref> [10] </ref>, we present a similar construction, in which the LL operation has the "normal" semantics: it always returns the current value of the implemented variable, even if the subsequent SC operation is sure to fail. <p> This work is performed by special Read and Write procedures, which are called by the sequential operation in order to read or write the MEM array. As a result, our constructions are not completely transparent to the sequential object designer. For example, instead of writing "MEM [1] := MEM <ref> [10] </ref>", the designer would write "Write (1; Read (10))". As a more concrete example, consider Figure 5.3, which contains the actual code we used to implement a queue using our constructions. As seen in the figure, this code is very similar to the "normal" sequential code for a queue. <p> a process p is faulty in a history t 0 s 0 s 1 ! if and only if for some i 0, process p is outside of its noncritical section 2 All previously published performance evaluations of resilient objects that we know of assume a one-process-per-processor model of computation <ref> [10, 44] </ref>.
Reference: [11] <author> R. Anderson and H. Woll, </author> <title> "Wait-Free Parallel Algorithms for the Union-Find Problem", </title> <booktitle> Proceedings of the 23rd ACM Symposium on Theory of Computing , 1991, </booktitle> <pages> pp. 370-380. 261 </pages>
Reference-contexts: Implementations of various types of queues have been presented by Lamport [64], by Herlihy and Wing [47], by Israeli and Rappoport [53], by Wing and Gong [94, 95], and by Michael and Scott [73]. Anderson and Woll <ref> [11] </ref> and Lanin and Shasha [67] present implementations for various set operations. 31 Valois presents lock-free implementations for various data structures, including queues, lists, trees, and dictionaries [90, 91, 92].
Reference: [12] <author> T. Anderson, </author> <title> "The Performance of Spin Lock Alternatives for Shared-Memory Multiprocessors", </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1), </volume> <year> 1990, </year> <pages> pp. 6-16. </pages>
Reference-contexts: Of course, significant effort has also been put into designing "efficient" mutual exclusion algorithms. Many have focused on the worst-case time complexity of acquiring a lock when no other processes require it, while others have studied the performance of various mutual exclusion algorithms under increasing levels of contention <ref> [12, 38, 72, 97] </ref>. (The level of contention is the number of processes that simultaneously request a lock.) Finally, efforts have been made to determine the impact of the available hardware and instructions on the design of mutual exclusion algorithms. <p> While another process holds the lock, these attempts fail, so processes that are requesting the lock must retry. Recent performance studies <ref> [12, 38, 72, 97] </ref> show that, as the number of concurrent processes in a system grows, excessive traffic on the processor-to-memory interconnect can quickly become a bottleneck that limits performance. <p> Recently, various researchers have presented mutual exclusion algorithms for 18 shared-memory multiprocessors that are designed to be scalable (by avoiding excessive interconnect traffic) while providing progress guarantees to processes that attempt to acquire the lock. Some of the best-known are due to Anderson <ref> [12] </ref>, to Graunke and Thakkar [38], to Mellor-Crummey and Scott [72], and to Yang and Anderson [97]. All of these mutual exclusion algorithms achieve scalable performance through the use of "local spinning". <p> The graph compares a simple test-and-set lock with mutual exclusion algorithms by Peterson and Fischer [82], by Lamport [65], by Styer [87], by Yang and Anderson [97], by Mellor-Crummey and Scott [72], and by Anderson <ref> [12] </ref>. <p> Our decision to evaluate our k-exclusion algorithms by their remote-reference counts | that is, to distinguish between local and remote accesses of shared memory | is motivated by recent work on local-spin spin locks <ref> [5, 12, 38, 72, 97, 98] </ref>. In such locks, the impact of the processor-to-memory bottleneck is minimized by structuring programs so that processes busy wait only on locally-accessible shared variables. <p> In practice, a shared variable can be made locally-accessible by storing it in a local cache line or in a local par tition of distributed shared memory. Performance studies presented in <ref> [12, 38, 72, 97, 98] </ref> show that minimizing remote memory accesses is important for scalable performance in the design of synchronization algorithms. 95 Although the k-assignment problem may seem to be much harder than the k-exclusion problem, we show that if one allows reasonable synchronization primitives, then any k-exclusion algorithm can <p> We begin by explaining the key insight on which all of our k-exclusion algorithms are based. On first thought, it may seem that the k-exclusion problem could be efficiently solved by simply modifying a queue-based spin lock <ref> [12, 38, 72] </ref> so that a process waits in the queue only if k other processes are already in their critical sections. Before giving our first algorithm, we explain why this simple approach is problematic. Consider the simple (unrealistic) queue-based (N; k)-exclusion algorithm in Figure 6.2. <p> The number of participating processes is varied, and the priority queue operations are equally divided among these processes. Previous experiments involving scalable synchronization constructs have assumed that each process runs on a dedicated processor <ref> [12, 38, 72, 97] </ref>. However, in practice it can be desirable to run more than one process on each processor. In our experiments, we consider scenarios in which processes share processors by multiprogramming. <p> Results of Anderson <ref> [12] </ref> and Cypher [28] imply that no constant-time, wait-free implementation of fetch-and-add from compare-and-swap or LL/SC exists. However, the lower bound implied by these results is quite low, leaving a significant gap between the lower bound and the best known implementation.
Reference: [13] <author> J. Aspnes and M. Herlihy, </author> <title> "Wait-Free Data Structures in the Asynchronous PRAM Model", </title> <booktitle> Proceedings of the Second Annual ACM Symposium on Parallel Architectures and Algorithms , 1990, </booktitle> <pages> pp. 340-349. </pages>
Reference-contexts: 39, 40, 58, 59, 66, 68, 78, 79, 81, 85, 88, 93]; atomic snapshots that allow multiple variables to be read atomically, [1, 4, 5, 16, 17, 32, 50, 55], algorithms for maintaining timestamps [31, 33, 37, 52], and mechanisms for implementing any object whose operations satisfy certain algebraic requirements <ref> [6, 7, 13] </ref>.
Reference: [14] <author> H. Attiya, A. Bar-Noy, D. Dolev, D. Koller, D. Peleg, and R. Reischuk, </author> <title> "Achievable Cases in an Asynchronous Environment", </title> <booktitle> Proceedings of the 28th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1987, </year> <pages> pp. 337-346. </pages>
Reference-contexts: In this chapter, we study the renaming problem in greater depth. In particular, we investigate the impact of the available instruction set on the problem of renaming. Previous research on renaming has focused on the one-time renaming problem <ref> [14, 19, 23, 74] </ref>, in which each of k processes is required to choose a distinct value, called a name, that ranges over f0; :::; M 1g. Each process is assumed to have a unique process identifier ranging over f0; :::; N 1g. <p> The renaming problem has been studied previously for both message-passing <ref> [14] </ref> and shared-memory multiprocessing systems [19, 23, 26]. We present several wait-free algorithms for both one-time and long-lived renaming on shared-memory multiprocessing systems. Previous wait-free renaming algorithms have time complexity that is dependent on the size of the original name space.
Reference: [15] <author> H. Attiya, A. Bar-Noy, D. Dolev, D. Koller, D. Peleg, and R. Reischuk, </author> <title> "Renaming in an Asynchronous Environment", </title> <journal> Journal of the ACM 37(3), </journal> <year> 1990, </year> <pages> pp. 524-548. </pages>
Reference-contexts: Progress must be guaranteed in the face of undetectable process halting failures. Specifically, if at most k 1 processes fail undetectably, then any nonfailed process that wants to enter its critical section eventually reaches it. In an important variant of the k-exclusion problem, originally posed by Attiya et al. <ref> [15] </ref>, there is an added requirement that each process entering its critical section must obtain a unique "name" from a fixed set of k names. Following the terminology of Burns and Peterson [26], we call this variant of the k-exclusion problem the k-assignment problem.
Reference: [16] <author> H. Attiya, M. Herlihy, and O. Rachman, </author> <title> "Efficient Atomic Snapshots Using Lattice Agreement", </title> <booktitle> Proceedings of the 6th International Workshop on Distributed Algorithms, </booktitle> <year> 1992, </year> <pages> pp. 35-53. </pages>
Reference-contexts: These implementations include constructions of complex registers from simpler registers [22, 25, 39, 40, 58, 59, 66, 68, 78, 79, 81, 85, 88, 93]; atomic snapshots that allow multiple variables to be read atomically, <ref> [1, 4, 5, 16, 17, 32, 50, 55] </ref>, algorithms for maintaining timestamps [31, 33, 37, 52], and mechanisms for implementing any object whose operations satisfy certain algebraic requirements [6, 7, 13].
Reference: [17] <author> H. Attiya and O. Rachman, </author> <title> "Atomic Snapshots in O(n log n) Operations", </title> <booktitle> Proceedings of the 12th Annual ACM Symposium on the Principles of Distributed Computing, </booktitle> <year> 1993, </year> <pages> pp. 29-40. </pages>
Reference-contexts: These implementations include constructions of complex registers from simpler registers [22, 25, 39, 40, 58, 59, 66, 68, 78, 79, 81, 85, 88, 93]; atomic snapshots that allow multiple variables to be read atomically, <ref> [1, 4, 5, 16, 17, 32, 50, 55] </ref>, algorithms for maintaining timestamps [31, 33, 37, 52], and mechanisms for implementing any object whose operations satisfy certain algebraic requirements [6, 7, 13].
Reference: [18] <author> G. Barnes, </author> <title> "A Method for Implementing Lock-Free Shared Data Structures", </title> <booktitle> Proceedings of the Fifth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1993, </year> <pages> pp. 261-270. </pages>
Reference-contexts: a shared-memory multiprocessor that supports instructions (like compare-and-swap) with unbounded consensus number, any object can be implemented in a wait-free manner for any number of processes. 2.2.3 Universal Constructions In recent years, several groups of researchers have presented methods for automatically "transforming" sequential object implementations into wait-free or lock-free ones <ref> [2, 18, 41, 43, 44, 56, 77, 84] </ref>. These methods are called universal constructions. A universal 24 construction relieves the object designer of the need to reason about concurrency, thereby greatly simplifying the task of providing a correct lock-free or wait-free implementation for a particular shared object. <p> Several groups of researchers have presented constructions that are designed to overcome one or more of these disadvantages. These efforts are described briefly below. Barnes <ref> [18] </ref> recognized the importance of allowing operations to execute in parallel where possible. He presented a mechanism in which an object is protected by a number of locks. <p> Thus, they are not the same as the multi-word operations considered in <ref> [9, 18, 54, 84] </ref>, which access multiple variables that are stored in separate, and not necessarily contiguous, memory words. The multi-word operations we consider admit simpler and more efficient implementations than those considered in [9, 18, 54, 84]. 1 The SC operation terminates in fi (1) time after the most recent <p> Thus, they are not the same as the multi-word operations considered in <ref> [9, 18, 54, 84] </ref>, which access multiple variables that are stored in separate, and not necessarily contiguous, memory words. The multi-word operations we consider admit simpler and more efficient implementations than those considered in [9, 18, 54, 84]. 1 The SC operation terminates in fi (1) time after the most recent spurious FSC.
Reference: [19] <author> A. Bar-Noy and D. Dolev, </author> <title> "Shared Memory versus Message-Passing in an Asynchronous Distributed Environment", </title> <booktitle> Proceedings of the 8th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <year> 1989, </year> <pages> pp. 307-318. </pages>
Reference-contexts: In this chapter, we study the renaming problem in greater depth. In particular, we investigate the impact of the available instruction set on the problem of renaming. Previous research on renaming has focused on the one-time renaming problem <ref> [14, 19, 23, 74] </ref>, in which each of k processes is required to choose a distinct value, called a name, that ranges over f0; :::; M 1g. Each process is assumed to have a unique process identifier ranging over f0; :::; N 1g. <p> The renaming problem has been studied previously for both message-passing [14] and shared-memory multiprocessing systems <ref> [19, 23, 26] </ref>. We present several wait-free algorithms for both one-time and long-lived renaming on shared-memory multiprocessing systems. Previous wait-free renaming algorithms have time complexity that is dependent on the size of the original name space. <p> It has been shown that if M &lt; 2k 1, then M -renaming cannot be implemented in a wait-free manner using only atomic reads and writes [46]. Wait-free, read/write algorithms for onetime renaming that yield an optimal name space of size M = 2k 1 have been proposed in <ref> [19, 23] </ref>. However, in these algorithms, the time complexity of choosing a name is dependent on N , the size of the original name space. <p> We are grateful to Hagit Attiya for pointing this out to us. 135 Reference M Time Complexity Space Complexity Fast? Long-Lived? [26] 2k 1 fi (N k 2 ) fi (N 2 ) No Yes <ref> [19] </ref> 2k 1 fi (N 4 k ) fi (N ) No No Thm. 19 k (k + 1)=2 fi (k) fi (k 2 ) Yes No [24] k (k + 1)=2 fi (k 3 ) fi (k 4 min (3 k ; N )) Yes Yes Thm. 22 2k 1
Reference: [20] <institution> BBN Advanced Computers, Inside the TC2000 Computer, </institution> <month> February, </month> <year> 1990. </year>
Reference-contexts: Note that for b = 1, set first zero is equivalent to test and set. The set first zero operation for b &gt; 1 can be implemented, for example, using the atomff0andset operation available on the BBN TC2000 multiprocessor <ref> [20] </ref>. The clr bit (X; i) operation clears the ith bit of the b-bit shared variable X . For b = 1, clr bit is a simple write operation. <p> The first algorithm uses set first zero and clr bit to access shared, b-bit variables and has time complexity fi (k=b). As discussed earlier, these operations can be implemented, for example, using operations available on the BBN TC2000 <ref> [20] </ref>. The second algorithm in this section has time complexity fi (log k) | a significant improvement over the first algorithm. To achieve this improvement, this algorithm uses the bounded decrement operation.
Reference: [21] <author> B. Berhsad. </author> <title> "Practical Considerations for Non-Blocking Concurrent Objects", </title> <booktitle> Proceedings of the 13th International Conference on Distributed Computing Systems, </booktitle> <year> 1993, </year> <pages> pp. 264-274. </pages>
Reference: [22] <author> B. Bloom, </author> <title> "Constructing Two-Writer Atomic Registers", </title> <journal> IEEE Transactions on Computers , 37(12), </journal> <month> December </month> <year> 1988, </year> <pages> pp. 1506-1514. </pages> <booktitle> Also appeared in Proceedings of the Sixth Annual Symposium on Principles of Distributed Computing, </booktitle> <year> 1987, </year> <pages> pp. 249-259. </pages>
Reference: [23] <author> E. Borowsky and E. Gafni, </author> <title> "Immediate Atomic Snapshots and Fast Renaming", </title> <booktitle> Proceedings of the 12th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <year> 1993, </year> <pages> pp. 41-50. </pages>
Reference-contexts: In this chapter, we study the renaming problem in greater depth. In particular, we investigate the impact of the available instruction set on the problem of renaming. Previous research on renaming has focused on the one-time renaming problem <ref> [14, 19, 23, 74] </ref>, in which each of k processes is required to choose a distinct value, called a name, that ranges over f0; :::; M 1g. Each process is assumed to have a unique process identifier ranging over f0; :::; N 1g. <p> The renaming problem has been studied previously for both message-passing [14] and shared-memory multiprocessing systems <ref> [19, 23, 26] </ref>. We present several wait-free algorithms for both one-time and long-lived renaming on shared-memory multiprocessing systems. Previous wait-free renaming algorithms have time complexity that is dependent on the size of the original name space. <p> It has been shown that if M &lt; 2k 1, then M -renaming cannot be implemented in a wait-free manner using only atomic reads and writes [46]. Wait-free, read/write algorithms for onetime renaming that yield an optimal name space of size M = 2k 1 have been proposed in <ref> [19, 23] </ref>. However, in these algorithms, the time complexity of choosing a name is dependent on N , the size of the original name space.
Reference: [24] <author> H. Buhrman, J. Garay, J. Hoepman, and M. Moir, </author> <title> "Long-Lived Renaming Made Fast", </title> <booktitle> Proceedings of the 14th Annual ACM Symposium on Principles of Distributed Computing , 1995, </booktitle> <pages> pp. 194-203. </pages>
Reference-contexts: 135 Reference M Time Complexity Space Complexity Fast? Long-Lived? [26] 2k 1 fi (N k 2 ) fi (N 2 ) No Yes [19] 2k 1 fi (N 4 k ) fi (N ) No No Thm. 19 k (k + 1)=2 fi (k) fi (k 2 ) Yes No <ref> [24] </ref> k (k + 1)=2 fi (k 3 ) fi (k 4 min (3 k ; N )) Yes Yes Thm. 22 2k 1 fi (k 4 ) fi (k 4 ) Yes Yes Table 7.1: A summary of read/write, wait-free M -renaming algorithms.
Reference: [25] <author> J. Burns and G. Peterson, </author> <title> "Constructing Multi-Reader Atomic Values from NonAtomic Values", </title> <booktitle> Proceedings of the Sixth Annual Symposium on Principles of Distributed Computing, </booktitle> <year> 1987, </year> <pages> pp. 222-231. </pages>
Reference: [26] <author> J. Burns and G. Peterson, </author> <title> "The Ambiguity of Choosing", </title> <booktitle> Proceedings of the Eighth Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <year> 1989, </year> <pages> pp. 145-157. 262 </pages>
Reference-contexts: In an important variant of the k-exclusion problem, originally posed by Attiya et al. [15], there is an added requirement that each process entering its critical section must obtain a unique "name" from a fixed set of k names. Following the terminology of Burns and Peterson <ref> [26] </ref>, we call this variant of the k-exclusion problem the k-assignment problem. In this chapter, we present several algorithms for solving the k-exclusion and k-assignment problems on shared-memory multiprocessors. <p> With Contention Contention-Free Instructions Used [34] 1 fi (1) Large Critical Sections [35] 1 fi (1) Large Critical Sections [30] 1 fi (N 2 ) Safe Bits [3] 1 fi (N ) Atomic Read and Write <ref> [26] </ref> 1 fi (N ) Atomic Read and Write Thm. 9 fi (k log (N=k)) fi (1) Fetch-and-Add, Test-and-Set Thm. 10 fi (c) fi (1) Fetch-and-Add, Test-and-Set Thm. 13 fi (k log (N=k)) fi (1) Fetch-and-Add, Test-and-Set Thm. 14 fi (c) fi (1) Fetch-and-Add, Test-and-Set Thm. 17 fi (k log (N=k)) <p> The renaming problem has been studied previously for both message-passing [14] and shared-memory multiprocessing systems <ref> [19, 23, 26] </ref>. We present several wait-free algorithms for both one-time and long-lived renaming on shared-memory multiprocessing systems. Previous wait-free renaming algorithms have time complexity that is dependent on the size of the original name space. <p> However, in these algorithms, the time complexity of choosing a name is dependent on N , the size of the original name space. Similarly, the only previous algorithm that solves long-lived renaming to a name space of size 2k 1, due to Burns and Peterson <ref> [26] </ref>, 1 has time complexity that is dependent on the size of the original name space. We present the most efficient algorithm to date for fast, long-lived k (k + 1)=2-renaming. <p> We are grateful to Hagit Attiya for pointing this out to us. 135 Reference M Time Complexity Space Complexity Fast? Long-Lived? <ref> [26] </ref> 2k 1 fi (N k 2 ) fi (N 2 ) No Yes [19] 2k 1 fi (N 4 k ) fi (N ) No No Thm. 19 k (k + 1)=2 fi (k) fi (k 2 ) Yes No [24] k (k + 1)=2 fi (k 3 ) fi <p> In particular, by combining our fast, long-lived renaming algorithm with the (non-fast) `-assignment algorithm (with ` = 2k 1) of Burns and Peterson <ref> [26] </ref>, fast, long-lived renaming can be achieved with a name space of size 2k 1. As explained earlier, if at most k processes concurrently access Burns and Peterson's algorithm, then their algorithm is wait-free. <p> As explained earlier, if at most k processes concurrently access Burns and Peterson's algorithm, then their algorithm is wait-free. The worst-case time complexity of acquiring and releasing a name once is fi (N k 2 ) [80]. Thus, we have the following result. By results of Burns and Peterson <ref> [26] </ref>, and of Herlihy and Shavit [46], this algorithm is optimal with respect to the size of the name space.
Reference: [27] <author> K. Chandy and J. Misra, </author> <title> Parallel Program Design: A Foundation, </title> <publisher> Addison-Wesley, </publisher> <year> 1988. </year>
Reference-contexts: Assertional proof techniques <ref> [27] </ref> are used to prove correct the more difficult algorithms in this dissertation. We prove that an assertion I is an invariant by showing that it holds inductively or that it follows from established invariants.
Reference: [28] <author> R. Cypher, </author> <title> "The Communication Requirements of Mutual Exclusion", </title> <booktitle> Proceedings of the 7th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1995, </year> <pages> pp. 147-156. </pages>
Reference-contexts: Results of Anderson [12] and Cypher <ref> [28] </ref> imply that no constant-time, wait-free implementation of fetch-and-add from compare-and-swap or LL/SC exists. However, the lower bound implied by these results is quite low, leaving a significant gap between the lower bound and the best known implementation.
Reference: [29] <author> E. Dijkstra, </author> <title> "Solution to a Problem in Concurrent Programming Control", </title> <journal> Communications of the ACM, </journal> <volume> 8(9), </volume> <year> 1965, </year> <note> p. 569. </note>
Reference-contexts: In order to avoid corruption of a shared object, it is usually necessary to synchronize concurrent accesses to the object. Various approaches have been proposed for dealing with the problem of coordinating concurrent accesses to a shared object. Among these 2 approaches, the use of mutual exclusion <ref> [29] </ref> is the most commonly accepted. In order to modify a shared object using mutual exclusion, a process first acquires a lock associated with that object, performs its operation, and then releases the lock. <p> These code sections must ensure that no two distinct processes execute within their critical sections concurrently. This problem was first posed by Dijkstra in 1965 <ref> [29] </ref>, and an enormous amount of research effort has been devoted to studying this problem since. In this section, we give a brief overview of the recent history of research on mutual exclusion algorithms for shared-memory multiprocessors. The simplest mutual exclusion algorithm is known as a test-and-set lock. <p> These results show that our k-exclusion algorithms are fast and scalable. The k-exclusion problem was posed by Fischer et al. [34] as a generalization of the well-known mutual exclusion problem <ref> [29] </ref>. In the k-exclusion problem, the objective is to 1 As mentioned in Chapter 2, a spin lock is a mutual exclusion algorithm that uses spinning to wait when another process holds the lock.
Reference: [30] <author> D. Dolev, E. Gafni, and N. Shavit, </author> <title> "Towards a Non-atomic Era: l-Exclusion as a Test Case", </title> <booktitle> Proceedings of the 20th ACM Symposium on Theory of Computing, </booktitle> <year> 1988, </year> <pages> pp. 78-92. </pages>
Reference-contexts: Observe that all previously published algorithms have unbounded remote-reference counts under contention, and most have high remote-reference counts even 94 Remote-Reference Complexity Ref. With Contention Contention-Free Instructions Used [34] 1 fi (1) Large Critical Sections [35] 1 fi (1) Large Critical Sections <ref> [30] </ref> 1 fi (N 2 ) Safe Bits [3] 1 fi (N ) Atomic Read and Write [26] 1 fi (N ) Atomic Read and Write Thm. 9 fi (k log (N=k)) fi (1) Fetch-and-Add, Test-and-Set Thm. 10 fi (c) fi (1) Fetch-and-Add, Test-and-Set Thm. 13 fi (k log (N=k)) fi
Reference: [31] <author> D. Dolev and N. Shavit, </author> <title> "Bounded Concurrent Timestamp Systems are Constructible!", </title> <booktitle> Proceedings of the 21st Annual ACM Symposium on Theory of Computing , 1989, </booktitle> <pages> pp. 454-465. </pages>
Reference-contexts: These implementations include constructions of complex registers from simpler registers [22, 25, 39, 40, 58, 59, 66, 68, 78, 79, 81, 85, 88, 93]; atomic snapshots that allow multiple variables to be read atomically, [1, 4, 5, 16, 17, 32, 50, 55], algorithms for maintaining timestamps <ref> [31, 33, 37, 52] </ref>, and mechanisms for implementing any object whose operations satisfy certain algebraic requirements [6, 7, 13].
Reference: [32] <author> C. Dwork, M. Herlihy, S. Plotkin, and O. Waarts, </author> <title> "Time Lapse Snapshots", </title> <booktitle> Proceedings of the Israel Symposium on the Theory of Computing and Systems, </booktitle> <year> 1992, </year> <pages> pp. 154-170. </pages>
Reference-contexts: These implementations include constructions of complex registers from simpler registers [22, 25, 39, 40, 58, 59, 66, 68, 78, 79, 81, 85, 88, 93]; atomic snapshots that allow multiple variables to be read atomically, <ref> [1, 4, 5, 16, 17, 32, 50, 55] </ref>, algorithms for maintaining timestamps [31, 33, 37, 52], and mechanisms for implementing any object whose operations satisfy certain algebraic requirements [6, 7, 13].
Reference: [33] <author> C. Dwork and O. Waarts, </author> <title> "Simple and Efficient Bounded and Concurrent Timestamp-ing or Bounded Concurrent Timestamp Systems are Comprehensible!", </title> <booktitle> Proceedings of the 24th Annual ACM Symposium on Theory of Computing, </booktitle> <year> 1992, </year> <pages> pp. 655-666. </pages>
Reference-contexts: These implementations include constructions of complex registers from simpler registers [22, 25, 39, 40, 58, 59, 66, 68, 78, 79, 81, 85, 88, 93]; atomic snapshots that allow multiple variables to be read atomically, [1, 4, 5, 16, 17, 32, 50, 55], algorithms for maintaining timestamps <ref> [31, 33, 37, 52] </ref>, and mechanisms for implementing any object whose operations satisfy certain algebraic requirements [6, 7, 13].
Reference: [34] <author> M. Fischer, N. Lynch, J. Burns, and A. Borodin, </author> <title> "Resource Allocation with Immunity to Process Failure", </title> <booktitle> Proceedings of the 20th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1979, </year> <pages> pp. 234-254. </pages>
Reference-contexts: We present performance results that show that, in multi-programmed systems, these object implementations can perform much better than either wait-free or spin-lock-based object implementations. These results show that our k-exclusion algorithms are fast and scalable. The k-exclusion problem was posed by Fischer et al. <ref> [34] </ref> as a generalization of the well-known mutual exclusion problem [29]. In the k-exclusion problem, the objective is to 1 As mentioned in Chapter 2, a spin lock is a mutual exclusion algorithm that uses spinning to wait when another process holds the lock. <p> This notion is formalized in Section 6.1.) Table 6.1 also specifies the set of instructions used by each algorithm. Observe that all previously published algorithms have unbounded remote-reference counts under contention, and most have high remote-reference counts even 94 Remote-Reference Complexity Ref. With Contention Contention-Free Instructions Used <ref> [34] </ref> 1 fi (1) Large Critical Sections [35] 1 fi (1) Large Critical Sections [30] 1 fi (N 2 ) Safe Bits [3] 1 fi (N ) Atomic Read and Write [26] 1 fi (N ) Atomic Read and Write Thm. 9 fi (k log (N=k)) fi (1) Fetch-and-Add, Test-and-Set Thm. <p> The algorithms of Theorem 9 through Theorem 18 all use atomic reads and writes in addition to the instructions listed. in the absence of contention. In addition, the algorithms of <ref> [34] </ref> and [35] assume the existence of large mutually exclusive critical sections that are executed atomically.
Reference: [35] <author> M. Fischer, N. Lynch, J. Burns, and A. Borodin, </author> <title> "Distributed FIFO Allocation of Identical Resources Using Small Shared Space", </title> <journal> ACM Transactions on Programming Languages and Systems , 11(1), </journal> <year> 1989, </year> <pages> pp. 90-114. </pages>
Reference-contexts: Observe that all previously published algorithms have unbounded remote-reference counts under contention, and most have high remote-reference counts even 94 Remote-Reference Complexity Ref. With Contention Contention-Free Instructions Used [34] 1 fi (1) Large Critical Sections <ref> [35] </ref> 1 fi (1) Large Critical Sections [30] 1 fi (N 2 ) Safe Bits [3] 1 fi (N ) Atomic Read and Write [26] 1 fi (N ) Atomic Read and Write Thm. 9 fi (k log (N=k)) fi (1) Fetch-and-Add, Test-and-Set Thm. 10 fi (c) fi (1) Fetch-and-Add, Test-and-Set <p> The algorithms of Theorem 9 through Theorem 18 all use atomic reads and writes in addition to the instructions listed. in the absence of contention. In addition, the algorithms of [34] and <ref> [35] </ref> assume the existence of large mutually exclusive critical sections that are executed atomically.
Reference: [36] <author> M. Fischer, N. Lynch, and M. Paterson, </author> <title> "Impossibility of Distributed Consensus with One Faulty Process", </title> <journal> Journal of the ACM, </journal> <year> 1985, </year> <pages> pp. 374-382. </pages>
Reference-contexts: Loui and Abu-Amara [70] showed that the consensus problem cannot be solved in a wait-free manner for N &gt; 1 in a shared-memory multiprocessor that provides only simple read and write instructions. (A similar result was previously proved for message passing systems by Fischer, Lynch, and Patterson <ref> [36] </ref>.) 23 Herlihy later extended these results to other shared objects by classifying each object according to its consensus number [41, 43].
Reference: [37] <author> R. Gawlick, N. Lynch, and N. Shavit, </author> <title> "Concurrent Timestamping Made Simple", </title> <booktitle> Proceedings of the Israel Symposium on the Theory of Computing and Systems, </booktitle> <year> 1992, </year> <pages> pp. 171-183. </pages>
Reference-contexts: These implementations include constructions of complex registers from simpler registers [22, 25, 39, 40, 58, 59, 66, 68, 78, 79, 81, 85, 88, 93]; atomic snapshots that allow multiple variables to be read atomically, [1, 4, 5, 16, 17, 32, 50, 55], algorithms for maintaining timestamps <ref> [31, 33, 37, 52] </ref>, and mechanisms for implementing any object whose operations satisfy certain algebraic requirements [6, 7, 13].
Reference: [38] <author> G. Graunke and S. Thakkar, </author> <title> "Synchronization Algorithms for Shared-Memory Multiprocessors", </title> <booktitle> IEEE Computer 23, </booktitle> <year> 1990, </year> <pages> pp. 60-69. </pages>
Reference-contexts: Of course, significant effort has also been put into designing "efficient" mutual exclusion algorithms. Many have focused on the worst-case time complexity of acquiring a lock when no other processes require it, while others have studied the performance of various mutual exclusion algorithms under increasing levels of contention <ref> [12, 38, 72, 97] </ref>. (The level of contention is the number of processes that simultaneously request a lock.) Finally, efforts have been made to determine the impact of the available hardware and instructions on the design of mutual exclusion algorithms. <p> While another process holds the lock, these attempts fail, so processes that are requesting the lock must retry. Recent performance studies <ref> [12, 38, 72, 97] </ref> show that, as the number of concurrent processes in a system grows, excessive traffic on the processor-to-memory interconnect can quickly become a bottleneck that limits performance. <p> Recently, various researchers have presented mutual exclusion algorithms for 18 shared-memory multiprocessors that are designed to be scalable (by avoiding excessive interconnect traffic) while providing progress guarantees to processes that attempt to acquire the lock. Some of the best-known are due to Anderson [12], to Graunke and Thakkar <ref> [38] </ref>, to Mellor-Crummey and Scott [72], and to Yang and Anderson [97]. All of these mutual exclusion algorithms achieve scalable performance through the use of "local spinning". A spinning algorithm is one that busy-waits by repeatedly testing shared variables (i.e., no shared variables are written while busy-waiting). <p> Our decision to evaluate our k-exclusion algorithms by their remote-reference counts | that is, to distinguish between local and remote accesses of shared memory | is motivated by recent work on local-spin spin locks <ref> [5, 12, 38, 72, 97, 98] </ref>. In such locks, the impact of the processor-to-memory bottleneck is minimized by structuring programs so that processes busy wait only on locally-accessible shared variables. <p> In practice, a shared variable can be made locally-accessible by storing it in a local cache line or in a local par tition of distributed shared memory. Performance studies presented in <ref> [12, 38, 72, 97, 98] </ref> show that minimizing remote memory accesses is important for scalable performance in the design of synchronization algorithms. 95 Although the k-assignment problem may seem to be much harder than the k-exclusion problem, we show that if one allows reasonable synchronization primitives, then any k-exclusion algorithm can <p> We begin by explaining the key insight on which all of our k-exclusion algorithms are based. On first thought, it may seem that the k-exclusion problem could be efficiently solved by simply modifying a queue-based spin lock <ref> [12, 38, 72] </ref> so that a process waits in the queue only if k other processes are already in their critical sections. Before giving our first algorithm, we explain why this simple approach is problematic. Consider the simple (unrealistic) queue-based (N; k)-exclusion algorithm in Figure 6.2. <p> The number of participating processes is varied, and the priority queue operations are equally divided among these processes. Previous experiments involving scalable synchronization constructs have assumed that each process runs on a dedicated processor <ref> [12, 38, 72, 97] </ref>. However, in practice it can be desirable to run more than one process on each processor. In our experiments, we consider scenarios in which processes share processors by multiprogramming.
Reference: [39] <author> S. Haldar and P. Subramanian, </author> <title> "Space-Optimum Conflict-Free Construction of 1-Writer 1-Reader Multivalued Atomic Variable", </title> <booktitle> Proceedings of the 8th International Workshop on Distributed Algorithms , 1994, </booktitle> <pages> pp. 116-128. </pages>
Reference: [40] <author> S. Haldar and K. Vidyasankar, </author> <title> "Space-Efficient Construction of Buffer-Optimal 1-Writer 1-Reader Multivalued Atomic Variable", </title> <booktitle> Proceedings of the 15th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <year> 1996, </year> <note> p. 178. </note>
Reference: [41] <author> M. Herlihy, </author> <title> "Impossibility and Universality Results for Wait-Free Synchronization", </title> <booktitle> Proceedings of the Seventh Annual ACM Symposium on Principles of Distributed Computing , 1988, </booktitle> <pages> pp. 276-290. 263 </pages>
Reference-contexts: To allow programmers of concurrent applications to easily use new lock-free and wait-free shared objects, a mechanism that automatically generates such an implementation from its sequential implementation is desirable. Towards this end, Herlihy proposed universal constructions <ref> [41] </ref>. A universal construction is a mechanism that produces a lock-free or wait-free implementation of a shared object, given code for a sequential implementation of that object. In a seminal paper, Herlihy presented the first attempt at practical lock-free and wait-free universal constructions [42, 44]. <p> N &gt; 1 in a shared-memory multiprocessor that provides only simple read and write instructions. (A similar result was previously proved for message passing systems by Fischer, Lynch, and Patterson [36].) 23 Herlihy later extended these results to other shared objects by classifying each object according to its consensus number <ref> [41, 43] </ref>. The consensus number of an object is the maximum number of processes for which a wait-free consensus algorithm exists that relies only on that object and read and write instructions. Herlihy showed that for each N 1, there exists an object with consensus number N . <p> a shared-memory multiprocessor that supports instructions (like compare-and-swap) with unbounded consensus number, any object can be implemented in a wait-free manner for any number of processes. 2.2.3 Universal Constructions In recent years, several groups of researchers have presented methods for automatically "transforming" sequential object implementations into wait-free or lock-free ones <ref> [2, 18, 41, 43, 44, 56, 77, 84] </ref>. These methods are called universal constructions. A universal 24 construction relieves the object designer of the need to reason about concurrency, thereby greatly simplifying the task of providing a correct lock-free or wait-free implementation for a particular shared object. <p> Herlihy's lock-free and wait-free constructions for small objects, as well as his lock-free construction for large objects, are described in Section 1.2; in the following paragraphs, we briefly describe other universal constructions. The first universal construction is due to Herlihy <ref> [41] </ref>. This construction is used to prove the universality of consensus, and is therefore based on general consensus objects. The construction works by appending operations onto a list in an order that is consistent with the partial order over operation invocations. <p> This construction is similar in spirit to Herlihy's original construction <ref> [41] </ref>, although it uses bounded space. It also has overhead that is too high for the construction to be considered practical. Later, Herlihy [42] made an important step towards practical universal constructions by presenting the first universal construction that is based on a "real" machine instruction, namely compare-and-swap.
Reference: [42] <author> M. Herlihy, </author> <title> "A Methodology for Implementing Highly Concurrent Data Structures", </title> <booktitle> Proceedings of the Second Annual ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <year> 1990, </year> <pages> pp. 197-206. </pages>
Reference-contexts: A universal construction is a mechanism that produces a lock-free or wait-free implementation of a shared object, given code for a sequential implementation of that object. In a seminal paper, Herlihy presented the first attempt at practical lock-free and wait-free universal constructions <ref> [42, 44] </ref>. While this line of research provides an excellent starting point for our work, this and other universal constructions [56, 77] for lock-free and wait-free shared objects have entailed significant time and space overhead, precluding their use in practical settings. <p> This construction is similar in spirit to Herlihy's original construction [41], although it uses bounded space. It also has overhead that is too high for the construction to be considered practical. Later, Herlihy <ref> [42] </ref> made an important step towards practical universal constructions by presenting the first universal construction that is based on a "real" machine instruction, namely compare-and-swap. Although implementable on existing shared-memory multiprocessors, this construction entails high overhead, and is therefore impractical.
Reference: [43] <author> M. Herlihy, </author> <title> "Wait-Free Synchronization", </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(1), </volume> <year> 1991, </year> <pages> pp. 124-149. </pages>
Reference-contexts: N &gt; 1 in a shared-memory multiprocessor that provides only simple read and write instructions. (A similar result was previously proved for message passing systems by Fischer, Lynch, and Patterson [36].) 23 Herlihy later extended these results to other shared objects by classifying each object according to its consensus number <ref> [41, 43] </ref>. The consensus number of an object is the maximum number of processes for which a wait-free consensus algorithm exists that relies only on that object and read and write instructions. Herlihy showed that for each N 1, there exists an object with consensus number N . <p> a shared-memory multiprocessor that supports instructions (like compare-and-swap) with unbounded consensus number, any object can be implemented in a wait-free manner for any number of processes. 2.2.3 Universal Constructions In recent years, several groups of researchers have presented methods for automatically "transforming" sequential object implementations into wait-free or lock-free ones <ref> [2, 18, 41, 43, 44, 56, 77, 84] </ref>. These methods are called universal constructions. A universal 24 construction relieves the object designer of the need to reason about concurrency, thereby greatly simplifying the task of providing a correct lock-free or wait-free implementation for a particular shared object. <p> This construction requires an unbounded number of variables; Herlihy later presents a version that uses a bounded number of variables, although the values of some of these variables grow, albeit slowly, without bound <ref> [43] </ref>. In a subsequent paper, Jayanti and Toueg correct some minor errors and show that bounded variables suffice [56]. Despite all of these improvements, these constructions all have high time and space complexity, and cannot be considered practical. <p> The constructions in this chapter are based on LL, VL, and SC operations for a multi-word shared variable. As shown in the previous chapter, these operations can be efficiently implemented from similar one-word primitives. This chapter extends recent research on universal lock-free and wait-free constructions of shared objects <ref> [43, 44] </ref>. Such constructions can be used to implement any object in a lock-free or a wait-free manner, and thus can be used as the basis for a general methodol 59 ogy for constructing highly-concurrent objects.
Reference: [44] <author> M. Herlihy, </author> <title> "A Methodology for Implementing Highly Concurrent Data Objects", </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 15(5), </volume> <year> 1993, </year> <pages> pp. 745-770. </pages>
Reference-contexts: A major component of the research presented in this dissertation focuses on shared object implementations that avoid the use of locks and thereby greatly reduce, or even eliminate, waiting. Previous efforts to eliminate waiting from shared object implementations have focused on lock-free and wait-free shared object implementations <ref> [44, 66] </ref>. A shared object implementation is lock-free if, for every operation by each process p, some operation is guaranteed to complete after a finite number of steps of process p. <p> A universal construction is a mechanism that produces a lock-free or wait-free implementation of a shared object, given code for a sequential implementation of that object. In a seminal paper, Herlihy presented the first attempt at practical lock-free and wait-free universal constructions <ref> [42, 44] </ref>. While this line of research provides an excellent starting point for our work, this and other universal constructions [56, 77] for lock-free and wait-free shared objects have entailed significant time and space overhead, precluding their use in practical settings. <p> Herlihy shows that this ensures that a process can fail to perform its own operation at most twice before the operation is performed by another process <ref> [44] </ref>. Several subtle difficulties arise from the need to ensure that each operation takes effect on the object exactly once, and that, when an operation is performed by a process other than the process that invoked it, the correct return value for that operation is communicated to the invoking process. <p> a shared-memory multiprocessor that supports instructions (like compare-and-swap) with unbounded consensus number, any object can be implemented in a wait-free manner for any number of processes. 2.2.3 Universal Constructions In recent years, several groups of researchers have presented methods for automatically "transforming" sequential object implementations into wait-free or lock-free ones <ref> [2, 18, 41, 43, 44, 56, 77, 84] </ref>. These methods are called universal constructions. A universal 24 construction relieves the object designer of the need to reason about concurrency, thereby greatly simplifying the task of providing a correct lock-free or wait-free implementation for a particular shared object. <p> A universal 24 construction relieves the object designer of the need to reason about concurrency, thereby greatly simplifying the task of providing a correct lock-free or wait-free implementation for a particular shared object. Much of the research presented in this dissertation is based on universal constructions due to Herlihy <ref> [44] </ref>. Herlihy's lock-free and wait-free constructions for small objects, as well as his lock-free construction for large objects, are described in Section 1.2; in the following paragraphs, we briefly describe other universal constructions. The first universal construction is due to Herlihy [41]. <p> Although implementable on existing shared-memory multiprocessors, this construction entails high overhead, and is therefore impractical. Herlihy made the first serious attempt at practical universal constructions in <ref> [44] </ref>. He presented three constructions: lock-free and wait-free constructions for "small" shared objects, and a lock-free construction for "large" shared objects. These constructions are based on the LL/SC instruction pair, and are described in more detail in Section 1.2. <p> The constructions in this chapter are based on LL, VL, and SC operations for a multi-word shared variable. As shown in the previous chapter, these operations can be efficiently implemented from similar one-word primitives. This chapter extends recent research on universal lock-free and wait-free constructions of shared objects <ref> [43, 44] </ref>. Such constructions can be used to implement any object in a lock-free or a wait-free manner, and thus can be used as the basis for a general methodol 59 ogy for constructing highly-concurrent objects. <p> In this chapter, we address these shortcoming by presenting more efficient universal constructions that can usually be used to implement large objects with low space overhead. We take as our starting point the lock-free and wait-free universal constructions for small objects presented by Herlihy in <ref> [44] </ref>. In these constructions, operations are implemented using "retry loops". <p> This mechanism ensures that if a process is repeatedly unsuccessful in modifying the shared object pointer, then it is eventually helped by another process (in fact, Herlihy shows that it is helped after at most two loop iterations <ref> [44] </ref>). As Herlihy points out, these constructions perform poorly if used to implement large objects. To overcome this problem, he presents a lock-free construction in which a 60 large object is fragmented into blocks linked by pointers. <p> It is interesting to note that our wait-free construction outperforms our lock-free one. We believe that this is because the cost of recopying blocks in the event that a SC fails dominates the cost of helping. 86 87 In Herlihy's performance experiments on small objects <ref> [44] </ref>, exponential backoff played an important role in improving performance. Exponential backoff is implemented by introducing a random delay after each failed SC operation. The length of this delay is chosen from a uniform, random distribution between zero and a maximum delay. <p> We should point out that we deliberately chose the queue to show the advantages of our constructions over Herlihy's. We also implemented a skew heap | the object considered by Herlihy in <ref> [44] </ref>. As a first step, we implemented a dynamic memory allocation mechanism on top of our large object construction. This provides a more convenient interface for objects (including skew heaps) that are naturally represented as nodes that are dynamically allocated and released. <p> a process p is faulty in a history t 0 s 0 s 1 ! if and only if for some i 0, process p is outside of its noncritical section 2 All previously published performance evaluations of resilient objects that we know of assume a one-process-per-processor model of computation <ref> [10, 44] </ref>. <p> All of our experiments have the same structure, so before we present any specific details, we give a brief overview. On both machines, we implemented a shared priority queue using the local-spin queue lock of Mellor-Crummey and Scott [72], and Herlihy's universal wait-free object construction <ref> [44] </ref>. To test the performance of our algorithms we used the "inductive fast path" method of Figure 6.6 with each level implemented using the appropriate algorithm (Figure 6.3 on the cache-coherent multiprocessor, and Figure 6.8 on the distributed shared-memory multiprocessor).
Reference: [45] <author> M. Herlihy and J. Moss, </author> <title> "Transactional Memory: Architectural Support for Lock-Free Data Structures", </title> <booktitle> Proceedings of the 20th International Symposium in Computer Architecture, </booktitle> <year> 1993, </year> <pages> pp. 289-300. </pages>
Reference-contexts: Barnes did not present a wait-free construction. Shavit and Touitou presented a method that they called software transactional memory [84], which is based on transactional memory | a hardware mechanism proposed by Herlihy and Moss <ref> [45] </ref>. Transactional memory allows programmers to write transactions (code fragments that operate on shared memory) that either execute atomically or fail without modifying the memory.
Reference: [46] <author> M. Herlihy and N. Shavit, </author> <title> "The Asynchronous Computability Theorem for t-Resilient Tasks", </title> <booktitle> Proceedings of the 25th ACM Symposium on Theory of Computing, </booktitle> <year> 1993, </year> <pages> pp. 111-120. </pages>
Reference-contexts: We first present renaming algorithms that use only atomic read and write instructions. It has been shown that if M &lt; 2k 1, then M -renaming cannot be implemented in a wait-free manner using only atomic reads and writes <ref> [46] </ref>. Wait-free, read/write algorithms for onetime renaming that yield an optimal name space of size M = 2k 1 have been proposed in [19, 23]. However, in these algorithms, the time complexity of choosing a name is dependent on N , the size of the original name space. <p> The worst-case time complexity of acquiring and releasing a name once is fi (N k 2 ) [80]. Thus, we have the following result. By results of Burns and Peterson [26], and of Herlihy and Shavit <ref> [46] </ref>, this algorithm is optimal with respect to the size of the name space. <p> By using read-modify-write operations, these algorithms significantly improve upon the performance of the algorithms in the previous section. Furthermore, these algorithms yield a name space of size k, which is clearly optimal (the lower bound results of Herlihy and Shavit <ref> [46] </ref> do not apply to algorithms that employ read-modify-write operations). The first algorithm uses set first zero and clr bit to access shared, b-bit variables and has time complexity fi (k=b). As discussed earlier, these operations can be implemented, for example, using operations available on the BBN TC2000 [20].
Reference: [47] <author> M. Herlihy and J. Wing, </author> <title> "Axioms for Concurrent Objects", </title> <booktitle> Proceedings of the 14th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pp. 13-26, </pages> <year> 1987. </year>
Reference-contexts: Other researchers have considered wait-free and lock-free implementations using instructions that are stronger than simple reads and writes. Implementations of various types of queues have been presented by Lamport [64], by Herlihy and Wing <ref> [47] </ref>, by Israeli and Rappoport [53], by Wing and Gong [94, 95], and by Michael and Scott [73].
Reference: [48] <author> M. Herlihy and J. Wing, </author> <title> "Linearizability: A Correctness Condition for Concurrent Objects", </title> <journal> ACM Transactions on Programming Languages and Systems , 12(3), </journal> <year> 1990, </year> <pages> pp. 463-492. </pages>
Reference-contexts: Thus, the correctness condition for such implementations is necessarily more complicated than for mutual-exclusion-based implementations (where the correctness of the sequential operations implies the correctness of the implementation). Like most researchers in the area of wait-free and lock-free shared object implementations, we use linearizability <ref> [48] </ref> as a correctness condition for our constructions. As described more precisely in the next chapter, linearizability requires that processes invoking operations on the implemented object cannot distinguish between the wait-free or lock-free implementation of that object, and one that ensures that each operation is executed sequentially. <p> The semantics of a shared object is satisfied if each invocation on an object appears to the invoking processes to be executed instantaneously at some distinct point during the invocation's interval. The formal correctness condition used to ensure this is linearizability <ref> [48] </ref>.
Reference: [49] <author> C. A. R. Hoare, </author> <title> "An Axiomatic Basis for Computer Programming", </title> <journal> Communications of the ACM 12, </journal> <year> 1969, </year> <pages> pp. </pages> <month> 576-580,583. </month>
Reference-contexts: Symbols in parentheses have the same binding power. We occasionally use parentheses to override these binding rules. We sometimes use Hoare triples <ref> [49] </ref> to denote the effects of a statement execution. Chapter 4 Support for Algorithms and Related Results In this chapter, we present several implementations involving universal primitives. These implementations allow the LL/SC-based constructions presented in the following chapters to be applied with greater flexibility.
Reference: [50] <author> J.-H. Hoepman and J. Tromp, </author> <title> "Binary Snapshots", </title> <booktitle> Proceedings of the Seventh International Workshop on Distributed Algorithms, </booktitle> <year> 1993, </year> <pages> pp. 18-25. </pages>
Reference-contexts: These implementations include constructions of complex registers from simpler registers [22, 25, 39, 40, 58, 59, 66, 68, 78, 79, 81, 85, 88, 93]; atomic snapshots that allow multiple variables to be read atomically, <ref> [1, 4, 5, 16, 17, 32, 50, 55] </ref>, algorithms for maintaining timestamps [31, 33, 37, 52], and mechanisms for implementing any object whose operations satisfy certain algebraic requirements [6, 7, 13].
Reference: [51] <author> M. Inoue, W. Chen, T. Masuzawa, and N. Tokura, </author> <title> "Linear-Time Snapshot Using Multi-Writer Multi-Reader Registers", </title> <booktitle> Proceedings of the 8th International Workshop on Distributed Algorithms , 1994, </booktitle> <pages> pp. 130-140. </pages>
Reference: [52] <author> A. Israeli and M. Li, </author> <title> "Bounded Time-Stamps", </title> <booktitle> Proceedings of the 28th IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1987, </year> <pages> pp. 371-382. </pages>
Reference-contexts: These implementations include constructions of complex registers from simpler registers [22, 25, 39, 40, 58, 59, 66, 68, 78, 79, 81, 85, 88, 93]; atomic snapshots that allow multiple variables to be read atomically, [1, 4, 5, 16, 17, 32, 50, 55], algorithms for maintaining timestamps <ref> [31, 33, 37, 52] </ref>, and mechanisms for implementing any object whose operations satisfy certain algebraic requirements [6, 7, 13].
Reference: [53] <author> A. Israeli and L. Rappoport, </author> <title> "Efficient Wait-Free Implementation of a Concurrent Priority Queue", </title> <booktitle> Proceedings of the 7th International Workshop on Distributed Algorithms , 1993, </booktitle> <pages> pp. 1-16. </pages>
Reference-contexts: Other researchers have considered wait-free and lock-free implementations using instructions that are stronger than simple reads and writes. Implementations of various types of queues have been presented by Lamport [64], by Herlihy and Wing [47], by Israeli and Rappoport <ref> [53] </ref>, by Wing and Gong [94, 95], and by Michael and Scott [73]. Anderson and Woll [11] and Lanin and Shasha [67] present implementations for various set operations. 31 Valois presents lock-free implementations for various data structures, including queues, lists, trees, and dictionaries [90, 91, 92].
Reference: [54] <author> A. Israeli and L. Rappoport, </author> <title> "Disjoint-Access-Parallel Implementations of Strong Shared Memory Primitives", </title> <booktitle> Proceedings of the 13th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <year> 1994, </year> <pages> pp. 151-160. </pages>
Reference-contexts: Thus, they are not the same as the multi-word operations considered in <ref> [9, 18, 54, 84] </ref>, which access multiple variables that are stored in separate, and not necessarily contiguous, memory words. The multi-word operations we consider admit simpler and more efficient implementations than those considered in [9, 18, 54, 84]. 1 The SC operation terminates in fi (1) time after the most recent <p> Thus, they are not the same as the multi-word operations considered in <ref> [9, 18, 54, 84] </ref>, which access multiple variables that are stored in separate, and not necessarily contiguous, memory words. The multi-word operations we consider admit simpler and more efficient implementations than those considered in [9, 18, 54, 84]. 1 The SC operation terminates in fi (1) time after the most recent spurious FSC. <p> In contrast, our implementations of these primitives are time-optimal. The best previous wait-free implementation of LL, VL, and SC using Read and CAS, recently presented by Israeli and Rappoport in <ref> [54] </ref>, requires fi (N ) time per operation.
Reference: [55] <author> A. Israeli, A. Shaham, and A. Shirazi, </author> <title> "Linear-Time Snapshots for Unbalanced Systems", </title> <booktitle> Proceedings of the Seventh International Workshop on Distributed Algorithms, </booktitle> <year> 1993, </year> <pages> pp. 26-38. </pages>
Reference-contexts: These implementations include constructions of complex registers from simpler registers [22, 25, 39, 40, 58, 59, 66, 68, 78, 79, 81, 85, 88, 93]; atomic snapshots that allow multiple variables to be read atomically, <ref> [1, 4, 5, 16, 17, 32, 50, 55] </ref>, algorithms for maintaining timestamps [31, 33, 37, 52], and mechanisms for implementing any object whose operations satisfy certain algebraic requirements [6, 7, 13].
Reference: [56] <author> P. Jayanti and S. Toueg, </author> <title> "Some Results on the Impossibility, Universality, and Decid-ability of Consensus", </title> <booktitle> Proceedings of the 6th International Workshop on Distributed Algorithms, </booktitle> <year> 1992, </year> <pages> pp. 69-84. 264 </pages>
Reference-contexts: In a seminal paper, Herlihy presented the first attempt at practical lock-free and wait-free universal constructions [42, 44]. While this line of research provides an excellent starting point for our work, this and other universal constructions <ref> [56, 77] </ref> for lock-free and wait-free shared objects have entailed significant time and space overhead, precluding their use in practical settings. <p> a shared-memory multiprocessor that supports instructions (like compare-and-swap) with unbounded consensus number, any object can be implemented in a wait-free manner for any number of processes. 2.2.3 Universal Constructions In recent years, several groups of researchers have presented methods for automatically "transforming" sequential object implementations into wait-free or lock-free ones <ref> [2, 18, 41, 43, 44, 56, 77, 84] </ref>. These methods are called universal constructions. A universal 24 construction relieves the object designer of the need to reason about concurrency, thereby greatly simplifying the task of providing a correct lock-free or wait-free implementation for a particular shared object. <p> In a subsequent paper, Jayanti and Toueg correct some minor errors and show that bounded variables suffice <ref> [56] </ref>. Despite all of these improvements, these constructions all have high time and space complexity, and cannot be considered practical. Plotkin [77] proposed a new universal synchronization primitive called a "sticky bit", and presents a universal construction that is based on this primitive.
Reference: [57] <author> T. Johnson and K. Harathi, </author> <title> "Interruptible Critical Sections", </title> <type> Technical Report, </type> <institution> TR94-007, Dept. Of Computer Science, University of Florida at Gainesville, </institution> <year> 1993. </year>
Reference: [58] <author> L. Kirousis, E. Kranakis, and P. Vitanyi, </author> <title> "Atomic Multireader Register", </title> <booktitle> Proceedings of the Second International Workshop on Distributed Computing , 1987, </booktitle> <pages> pp. 278-296. </pages>
Reference: [59] <author> L. Kirousis, P. Spirakis, and P. Tsigas, </author> <title> "Reading Many Variables in One Atomic Operation: Solutions with Linear or Sublinear Complexity", </title> <booktitle> Proceedings of the Fifth International Workshop on Distributed Algorithms , 1991, </booktitle> <pages> pp. 229-241. </pages>
Reference: [60] <author> D. Knuth, </author> <title> "Additional Comments on a Problem in Concurrent Programming Control", </title> <journal> Communications of the ACM, </journal> <volume> 9(5) 1966, </volume> <pages> pp. 321-322. </pages>
Reference-contexts: Another process that needs to modify the object must wait until the lock is released. 1.1 Mutual Exclusion Algorithms Substantial research effort has been devoted to the design of algorithms for mutual exclusion. Some researchers have focused on ensuring starvation freedom <ref> [60] </ref>, so that a process that tries to acquire a lock eventually succeeds in doing so. Others have sought stronger guarantees that ensure, for example, that processes acquire the lock in (approximately) the same order that they request it [62].
Reference: [61] <author> A. LaMarca, </author> <title> "A Performance Evaluation of Lock-Free Synchronization Protocols", </title> <booktitle> Proceedings of the 13th Annual ACM Symposium on Principles of Distributed Computing , 1994, </booktitle> <pages> pp. 130-140. </pages>
Reference-contexts: However, these new constructions are not expected to perform well in practice, because they make heavy use of the LL/SC synchronization instructions. The LL/SC instructions are usually quite expensive, relative to simple read and write instructions <ref> [61] </ref>. Finally, these constructions do not exploit parallelism. 30 2.2.4 Specific Objects Finally, many researchers have presented lock-free and wait-free implementations for specific shared objects. Such implementations can potentially take advantage of the semantics of the object under consideration to improve performance.
Reference: [62] <author> L. Lamport, </author> <title> "A New Solution of Dijkstra's Concurrent Programming Problem", </title> <journal> Communications of the ACM, </journal> <volume> 17(8), </volume> <year> 1974, </year> <pages> pp. 453-455. </pages>
Reference-contexts: Some researchers have focused on ensuring starvation freedom [60], so that a process that tries to acquire a lock eventually succeeds in doing so. Others have sought stronger guarantees that ensure, for example, that processes acquire the lock in (approximately) the same order that they request it <ref> [62] </ref>. Of course, significant effort has also been put into designing "efficient" mutual exclusion algorithms.
Reference: [63] <author> L. Lamport, </author> <title> "How to Make a Multiprocessor Computer that Correctly Executes Mul-tiprocess Programs", </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9), </volume> <year> 1979, </year> <pages> pp. 690-691. </pages>
Reference-contexts: us. (An exception arises in Chapter 6, where, for performance reasons, we present different algorithms for machines with cache-coherence mechanisms and for distributed shared memory machines on which different parts of shared memory can be accessed at different speeds.) However, we do assume that the shared memory provides sequential consistency <ref> [63] </ref>, which, roughly speaking, ensures that all memory operations appear to occur in the same total order to all processors. We model computations on shared memory multiprocessors using states and histories.
Reference: [64] <author> L. Lamport, </author> <title> "Specifying Concurrent Program Modules", </title> <journal> ACM Transactions on Programming Languages and Systems , 5(2), </journal> <year> 1983, </year> <pages> pp. 190-222. </pages>
Reference-contexts: Other researchers have considered wait-free and lock-free implementations using instructions that are stronger than simple reads and writes. Implementations of various types of queues have been presented by Lamport <ref> [64] </ref>, by Herlihy and Wing [47], by Israeli and Rappoport [53], by Wing and Gong [94, 95], and by Michael and Scott [73].
Reference: [65] <author> L. Lamport, </author> <title> "A Fast Mutual Exclusion Algorithm", </title> <journal> ACM Transactions on Computer Systems , 5(1), </journal> <year> 1987, </year> <pages> pp. 1-11. </pages>
Reference-contexts: This figure summarizes results of performance experiments run on the Sequent Symmetry, a shared-memory multiprocessor with cache-coherence; these results were originally reported by Yang and Anderson in [97]. The graph compares a simple test-and-set lock with mutual exclusion algorithms by Peterson and Fischer [82], by Lamport <ref> [65] </ref>, by Styer [87], by Yang and Anderson [97], by Mellor-Crummey and Scott [72], and by Anderson [12]. <p> The first is fast, but not long-lived, and the second is long-lived, but not fast. All of these algorithms employ a novel technique that uses "building blocks" based on the "fast path" mechanism employed by Lamport's fast mutual exclusion algorithm <ref> [65] </ref>. After presenting our fast, long-lived k (k + 1)=2-renaming algorithm, we show how it can be combined with previous, non-fast algorithms to achieve fast, long-lived renaming to the optimal name space size of 2k 1. <p> The code fragment shown in Figure 7.2 shows how the building block can be implemented using atomic read and write operations. The technique employed is essentially that of the "fast path" mechanism used in Lamport's fast mutual exclusion algorithm <ref> [65] </ref>. A process that stops corresponds to a process successfully "taking the fast path" in Lamport's algorithm. <p> Thus, if fewer than k processes concurrently use a particular renaming algorithm, then the worst-case time complexity of acquiring and releasing a name is lower than the time complexity stated in our theorems. This is an important practical advantage because contention should be low in most well-designed applications <ref> [65] </ref>. Our study of read/write algorithms for long-lived renaming has culminated in an 163 algorithm that is fast and achieves an optimal name space size of 2k 1, while having time complexity that is independent of the size of the original name space.
Reference: [66] <author> L. Lamport, </author> <title> "On Interprocess Communication, Parts I and II", </title> <booktitle> Distributed Computing 1, </booktitle> <year> 1986, </year> <pages> pp. 77-101. </pages>
Reference-contexts: A major component of the research presented in this dissertation focuses on shared object implementations that avoid the use of locks and thereby greatly reduce, or even eliminate, waiting. Previous efforts to eliminate waiting from shared object implementations have focused on lock-free and wait-free shared object implementations <ref> [44, 66] </ref>. A shared object implementation is lock-free if, for every operation by each process p, some operation is guaranteed to complete after a finite number of steps of process p.
Reference: [67] <author> V. Lanin and D. Shasha, </author> <title> "Concurrent Set Manipulation without Locking", </title> <booktitle> Proceedings of the 7th Annual ACM Symposium on Principles of Database Systems , 1988, </booktitle> <pages> pp. 211-220. </pages>
Reference-contexts: Implementations of various types of queues have been presented by Lamport [64], by Herlihy and Wing [47], by Israeli and Rappoport [53], by Wing and Gong [94, 95], and by Michael and Scott [73]. Anderson and Woll [11] and Lanin and Shasha <ref> [67] </ref> present implementations for various set operations. 31 Valois presents lock-free implementations for various data structures, including queues, lists, trees, and dictionaries [90, 91, 92]. Finally, Massalin and Pu have implemented an entire operating system using lock-free data structures such as lists, queues, and stacks [71].
Reference: [68] <author> M. Li, J. Tromp, and P. Vitanyi, </author> <title> "How to Construct Wait-Free Variables", </title> <booktitle> Proceedings of International Colloquium on Automata, Languages, and Programming, </booktitle> <year> 1989, </year> <pages> pp. 488-505. </pages>
Reference: [69] <author> B.-H. Lim and A. Agrawal, </author> <title> "Waiting Algorithms for Synchronization", </title> <journal> ACM Transactions on Computer Systems , 11(3), </journal> <year> 1993, </year> <pages> pp. 253-294. </pages>
Reference-contexts: Thus, the use of idle-waiting can lead to poor performance if waiting times are usually short. For a detailed description of these trade-offs, and for a study of heuristics for deciding between idle waiting and busy waiting, see <ref> [69] </ref>. 21 However, as a result of this relaxed condition, these algorithms admit the possibility of starvation. Furthermore, implementing the relaxed FIFO ordering and ensuring that a process is not preempted while in its critical section requires the use of special kernel support.
Reference: [70] <author> M. Loui, H. Abu-Amara, </author> <title> "Memory Requirements for Agreement Among Unreliable Asynchronous Processes", </title> <booktitle> Advances in Computing Research 4, </booktitle> <year> 1987, </year> <pages> pp. 163-183. </pages>
Reference-contexts: Loui and Abu-Amara <ref> [70] </ref> showed that the consensus problem cannot be solved in a wait-free manner for N &gt; 1 in a shared-memory multiprocessor that provides only simple read and write instructions. (A similar result was previously proved for message passing systems by Fischer, Lynch, and Patterson [36].) 23 Herlihy later extended these results
Reference: [71] <author> H. Massalin and C. Pu, </author> <title> "A Lock-Free Multiprocessor OS Kernel", </title> <type> Technical Report CUCS-005-91, </type> <institution> Columbia University, </institution> <year> 1991. </year>
Reference-contexts: Finally, Massalin and Pu have implemented an entire operating system using lock-free data structures such as lists, queues, and stacks <ref> [71] </ref>. Chapter 3 Preliminaries In this chapter, we briefly describe our model of computation, and introduce some conventions, definitions, and notation that are common to the following chapters. All of the results in this dissertation concern asynchronous, shared-memory multiprocessors.
Reference: [72] <author> J. Mellor-Crummey and M. Scott, </author> <title> "Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors", </title> <journal> ACM Transactions on Computer Systems , 9(1), </journal> <year> 1991, </year> <pages> pp. 21-65. </pages>
Reference-contexts: Of course, significant effort has also been put into designing "efficient" mutual exclusion algorithms. Many have focused on the worst-case time complexity of acquiring a lock when no other processes require it, while others have studied the performance of various mutual exclusion algorithms under increasing levels of contention <ref> [12, 38, 72, 97] </ref>. (The level of contention is the number of processes that simultaneously request a lock.) Finally, efforts have been made to determine the impact of the available hardware and instructions on the design of mutual exclusion algorithms. <p> While another process holds the lock, these attempts fail, so processes that are requesting the lock must retry. Recent performance studies <ref> [12, 38, 72, 97] </ref> show that, as the number of concurrent processes in a system grows, excessive traffic on the processor-to-memory interconnect can quickly become a bottleneck that limits performance. <p> Some of the best-known are due to Anderson [12], to Graunke and Thakkar [38], to Mellor-Crummey and Scott <ref> [72] </ref>, and to Yang and Anderson [97]. All of these mutual exclusion algorithms achieve scalable performance through the use of "local spinning". A spinning algorithm is one that busy-waits by repeatedly testing shared variables (i.e., no shared variables are written while busy-waiting). <p> The graph compares a simple test-and-set lock with mutual exclusion algorithms by Peterson and Fischer [82], by Lamport [65], by Styer [87], by Yang and Anderson [97], by Mellor-Crummey and Scott <ref> [72] </ref>, and by Anderson [12]. <p> Our decision to evaluate our k-exclusion algorithms by their remote-reference counts | that is, to distinguish between local and remote accesses of shared memory | is motivated by recent work on local-spin spin locks <ref> [5, 12, 38, 72, 97, 98] </ref>. In such locks, the impact of the processor-to-memory bottleneck is minimized by structuring programs so that processes busy wait only on locally-accessible shared variables. <p> In practice, a shared variable can be made locally-accessible by storing it in a local cache line or in a local par tition of distributed shared memory. Performance studies presented in <ref> [12, 38, 72, 97, 98] </ref> show that minimizing remote memory accesses is important for scalable performance in the design of synchronization algorithms. 95 Although the k-assignment problem may seem to be much harder than the k-exclusion problem, we show that if one allows reasonable synchronization primitives, then any k-exclusion algorithm can <p> We begin by explaining the key insight on which all of our k-exclusion algorithms are based. On first thought, it may seem that the k-exclusion problem could be efficiently solved by simply modifying a queue-based spin lock <ref> [12, 38, 72] </ref> so that a process waits in the queue only if k other processes are already in their critical sections. Before giving our first algorithm, we explain why this simple approach is problematic. Consider the simple (unrealistic) queue-based (N; k)-exclusion algorithm in Figure 6.2. <p> All of our experiments have the same structure, so before we present any specific details, we give a brief overview. On both machines, we implemented a shared priority queue using the local-spin queue lock of Mellor-Crummey and Scott <ref> [72] </ref>, and Herlihy's universal wait-free object construction [44]. To test the performance of our algorithms we used the "inductive fast path" method of Figure 6.6 with each level implemented using the appropriate algorithm (Figure 6.3 on the cache-coherent multiprocessor, and Figure 6.8 on the distributed shared-memory multiprocessor). <p> The number of participating processes is varied, and the priority queue operations are equally divided among these processes. Previous experiments involving scalable synchronization constructs have assumed that each process runs on a dedicated processor <ref> [12, 38, 72, 97] </ref>. However, in practice it can be desirable to run more than one process on each processor. In our experiments, we consider scenarios in which processes share processors by multiprogramming.
Reference: [73] <author> M. Michael and M. Scott, </author> <title> "Simple, Fast, and Practical Non-Blocking and Blocking Concurrent Queue Algorithms", </title> <booktitle> Proceedings of the 15th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <year> 1996, </year> <pages> pp. 267-276. 265 </pages>
Reference-contexts: Implementations of various types of queues have been presented by Lamport [64], by Herlihy and Wing [47], by Israeli and Rappoport [53], by Wing and Gong [94, 95], and by Michael and Scott <ref> [73] </ref>. Anderson and Woll [11] and Lanin and Shasha [67] present implementations for various set operations. 31 Valois presents lock-free implementations for various data structures, including queues, lists, trees, and dictionaries [90, 91, 92].
Reference: [74] <author> M. Moir and J. Anderson, </author> <title> "Fast, Long-Lived Renaming", </title> <booktitle> Proceedings of the 8th International Workshop on Distributed Algorithms, </booktitle> <year> 1994, </year> <pages> pp. 141-155. </pages>
Reference-contexts: In Chapter 7, we study the renaming problem. Finally, conclusions and a discussion of future directions for this research appear in Chapter 8. Some of the lengthier proofs from Chapters 5 through 7 appear in appendices. (This dissertation includes work that is based, with permission, on previously published work <ref> [8, 9, 10, 74, 75] </ref>.) Chapter 2 Related Work 2.1 Mutual Exclusion Algorithms In the mutual exclusion, each of a set of concurrent processes repeatedly executes a noncritical section of code and a critical section of code. <p> In this chapter, we study the renaming problem in greater depth. In particular, we investigate the impact of the available instruction set on the problem of renaming. Previous research on renaming has focused on the one-time renaming problem <ref> [14, 19, 23, 74] </ref>, in which each of k processes is required to choose a distinct value, called a name, that ranges over f0; :::; M 1g. Each process is assumed to have a unique process identifier ranging over f0; :::; N 1g.
Reference: [75] <author> M. Moir and J. Anderson, </author> <title> "Wait-Free Algorithms for Fast, Long-Lived Renaming", </title> <booktitle> Science of Computer Programming 25, </booktitle> <year> 1995, </year> <pages> pp. 1-39. </pages>
Reference-contexts: In Chapter 7, we study the renaming problem. Finally, conclusions and a discussion of future directions for this research appear in Chapter 8. Some of the lengthier proofs from Chapters 5 through 7 appear in appendices. (This dissertation includes work that is based, with permission, on previously published work <ref> [8, 9, 10, 74, 75] </ref>.) Chapter 2 Related Work 2.1 Mutual Exclusion Algorithms In the mutual exclusion, each of a set of concurrent processes repeatedly executes a noncritical section of code and a critical section of code. <p> Correctness proofs for algorithms in this chapter appear in Appendix C. (Because the results of Section 7.3.2 subsume the results of Sections 7.2 and 7.3.1, and because full correctness proofs for those results are presented elsewhere <ref> [75] </ref>, these proofs are not repeated here.) 7.1 Definitions In the one-time M -renaming problem, each of k processes, with distinct process identifiers ranging over f0; :::; N 1g, chooses a distinct value ranging over f0; :::; M 1g. We assume that 1 &lt; k M &lt; N . <p> At most one process stops at each building block, so a process that stops at a building block receives a unique name. However, a process may also obtain a name by taking k 1 steps in the grid. In <ref> [75] </ref>, we show that distinct processes that take k 1 steps in the grid acquire distinct names. We also prove that each process acquires a name from f0; :::; k (k + 1)=2 1g with worst-case time complexity fi (k). Thus, we have the following result. <p> We now give an informal description of the first algorithm, which is proved correct in <ref> [75] </ref>. <p> In fact, this is unnecessary, because the fact that these operations are concurrent is sufficient to ensure that either p or q will not receive a value of stop from the building block. In <ref> [75] </ref>, we prove that, for distinct processes p and q, if p@8 ^ q@8 holds, then 146 p and q hold distinct names from f0; :::; k (k + 1)=2 1g. We also prove that the worst-case time complexity of acquiring and releasing a name once is fi (N k).
Reference: [76] <author> M. Moir and J. Garay, </author> <title> "Fast, Long-Lived Renaming Improved and Simplified", </title> <booktitle> to appear in the proceedings of the 10th International Workshop on Distributed Algorithms, 1996. A brief announcement appeared in Proceedings of the 15th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <year> 1996, </year> <note> p. 152. </note>
Reference: [77] <author> S. Plotkin, </author> <title> "Sticky Bits and Universality of Consensus", </title> <booktitle> Proceedings of the 8th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <year> 1989, </year> <pages> pp. 159-175. </pages>
Reference-contexts: In a seminal paper, Herlihy presented the first attempt at practical lock-free and wait-free universal constructions [42, 44]. While this line of research provides an excellent starting point for our work, this and other universal constructions <ref> [56, 77] </ref> for lock-free and wait-free shared objects have entailed significant time and space overhead, precluding their use in practical settings. <p> a shared-memory multiprocessor that supports instructions (like compare-and-swap) with unbounded consensus number, any object can be implemented in a wait-free manner for any number of processes. 2.2.3 Universal Constructions In recent years, several groups of researchers have presented methods for automatically "transforming" sequential object implementations into wait-free or lock-free ones <ref> [2, 18, 41, 43, 44, 56, 77, 84] </ref>. These methods are called universal constructions. A universal 24 construction relieves the object designer of the need to reason about concurrency, thereby greatly simplifying the task of providing a correct lock-free or wait-free implementation for a particular shared object. <p> In a subsequent paper, Jayanti and Toueg correct some minor errors and show that bounded variables suffice [56]. Despite all of these improvements, these constructions all have high time and space complexity, and cannot be considered practical. Plotkin <ref> [77] </ref> proposed a new universal synchronization primitive called a "sticky bit", and presents a universal construction that is based on this primitive.
Reference: [78] <author> R. Newman-Wolfe, </author> <title> "A Protocol for Wait-Free, Atomic, Multi-Reader Shared Variables", </title> <booktitle> Proceedings of the Sixth Annual Symposium on Principles of Distributed Computing , 1987, </booktitle> <pages> pp. 232-248. </pages>
Reference: [79] <author> G. Peterson, </author> <title> "Concurrent Reading While Writing", </title> <journal> ACM Transactions on Programming Languages and Systems 5, </journal> <year> 1983, </year> <pages> pp. 46-55. </pages>
Reference: [80] <author> G. Peterson, </author> <type> personal communication, </type> <month> November </month> <year> 1995. </year>
Reference-contexts: As explained earlier, if at most k processes concurrently access Burns and Peterson's algorithm, then their algorithm is wait-free. The worst-case time complexity of acquiring and releasing a name once is fi (N k 2 ) <ref> [80] </ref>. Thus, we have the following result. By results of Burns and Peterson [26], and of Herlihy and Shavit [46], this algorithm is optimal with respect to the size of the name space.
Reference: [81] <author> G. Peterson and J. Burns, </author> <title> "Concurrent Reading While Writing II: The Multi-Writer Case", </title> <booktitle> Proceedings of the 28th Annual Symposium on Foundations of Computer Science, </booktitle> <year> 1987. </year>
Reference: [82] <author> G. Peterson and M. Fischer, </author> <title> "Economical Solutions for the Critical Section Problem in a Distributed System", </title> <booktitle> Proceedings of the 9th ACM Symposium on Theory of Computing , 1977, </booktitle> <pages> pp. 91-97. </pages>
Reference-contexts: This figure summarizes results of performance experiments run on the Sequent Symmetry, a shared-memory multiprocessor with cache-coherence; these results were originally reported by Yang and Anderson in [97]. The graph compares a simple test-and-set lock with mutual exclusion algorithms by Peterson and Fischer <ref> [82] </ref>, by Lamport [65], by Styer [87], by Yang and Anderson [97], by Mellor-Crummey and Scott [72], and by Anderson [12].
Reference: [83] <author> L. Sha, R. Rajkumar, and J. Lehoczky, </author> <title> "Priority Inheritance Protocols: An Approach to Real-Time System Synchronization", </title> <journal> IEEE Transactions on Computers , 39(9), </journal> <year> 1990, </year> <pages> pp. 1175-1185. </pages>
Reference-contexts: In such systems, processes (or tasks) are required to complete execution by a specified deadline; priority inversion can introduce lengthy delays, thereby causing tasks to miss their deadlines. Common solutions to this problem (for example, the priority inheritance protocol <ref> [83] </ref>) entail additional overhead and complicated operating system support. For all of the reasons discussed above, it is highly desirable to reduce or eliminate 5 waiting in concurrent applications.
Reference: [84] <author> N. Shavit and D. Touitou, </author> <title> "Software Transactional Memory", </title> <booktitle> Proceedings of the 14th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <year> 1995, </year> <pages> pp. 204-213. </pages>
Reference-contexts: a shared-memory multiprocessor that supports instructions (like compare-and-swap) with unbounded consensus number, any object can be implemented in a wait-free manner for any number of processes. 2.2.3 Universal Constructions In recent years, several groups of researchers have presented methods for automatically "transforming" sequential object implementations into wait-free or lock-free ones <ref> [2, 18, 41, 43, 44, 56, 77, 84] </ref>. These methods are called universal constructions. A universal 24 construction relieves the object designer of the need to reason about concurrency, thereby greatly simplifying the task of providing a correct lock-free or wait-free implementation for a particular shared object. <p> Nonetheless, Barnes's construction improves over Herlihy's by avoiding copying the entire object, and by allowing operations to execute in parallel where possible. Barnes did not present a wait-free construction. Shavit and Touitou presented a method that they called software transactional memory <ref> [84] </ref>, which is based on transactional memory | a hardware mechanism proposed by Herlihy and Moss [45]. Transactional memory allows programmers to write transactions (code fragments that operate on shared memory) that either execute atomically or fail without modifying the memory. <p> According to simulated performance studies conducted by Shavit and Touitou, this results in improved performance over Barnes's method. One limitation of software transactional memory (as presented in <ref> [84] </ref>) is that it does not support dynamic transactions. (A transaction is static if the locations it accesses are known in advance, and dynamic otherwise.) As a result, it is not particularly well suited to implementing general shared object operations. <p> Thus, they are not the same as the multi-word operations considered in <ref> [9, 18, 54, 84] </ref>, which access multiple variables that are stored in separate, and not necessarily contiguous, memory words. The multi-word operations we consider admit simpler and more efficient implementations than those considered in [9, 18, 54, 84]. 1 The SC operation terminates in fi (1) time after the most recent <p> Thus, they are not the same as the multi-word operations considered in <ref> [9, 18, 54, 84] </ref>, which access multiple variables that are stored in separate, and not necessarily contiguous, memory words. The multi-word operations we consider admit simpler and more efficient implementations than those considered in [9, 18, 54, 84]. 1 The SC operation terminates in fi (1) time after the most recent spurious FSC.
Reference: [85] <author> A. Singh, J. Anderson, and M. Gouda, </author> <title> "The Elusive Atomic Register", </title> <journal> Journal of the ACM , 41(2), </journal> <year> 1994, </year> <pages> pp. 311-339. </pages>
Reference: [86] <author> J. Singh, W. Weber, and A. Gupta, </author> <title> "SPLASH: Stanford Parallel Applications for Shared-Memory", </title> <journal> ACM SIGARCH Computer Architecture News, </journal> <volume> 22(4), </volume> <year> 1992, </year> <pages> pp. 5-44. </pages>
Reference: [87] <author> E. Styer, </author> <title> "Improving Fast Mutual Exclusion", </title> <booktitle> Proceedings of the Eleventh Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <year> 1992, </year> <pages> pp. 159-168. </pages>
Reference-contexts: This figure summarizes results of performance experiments run on the Sequent Symmetry, a shared-memory multiprocessor with cache-coherence; these results were originally reported by Yang and Anderson in [97]. The graph compares a simple test-and-set lock with mutual exclusion algorithms by Peterson and Fischer [82], by Lamport [65], by Styer <ref> [87] </ref>, by Yang and Anderson [97], by Mellor-Crummey and Scott [72], and by Anderson [12].
Reference: [88] <author> J. Tromp, </author> <title> "How to Construct an Atomic Variable", </title> <booktitle> Proceedings of the Third International Workshop on Distributed Algorithms , 1989, </booktitle> <pages> pp. 292-302. </pages>
Reference: [89] <author> J. Turek, D. Shasha, and S. Prakash, </author> <title> "Locking Without Blocking: Making Lock Based Concurrent Data Structure Algorithms Non-Blocking", </title> <booktitle> Proceedings of the 11th Symposium on Principles of Database Systems, </booktitle> <year> 1992, </year> <pages> pp. 212-222. </pages>
Reference-contexts: This allows a process that needs to acquire a lock that is already held by another process to "help" the latter process to perform its operation and to release the lock. (A similar approach is proposed by Turek, Shasha, and Prakash <ref> [89] </ref>.) Barnes's construction uses the LL/SC instruction pair to ensure that each step of an operation is performed only once, despite the possibility of multiple processes attempting to perform that step concurrently. Because of the relative expense of executing these synchronization instructions, this tech 27 nique introduces significant overhead.
Reference: [90] <author> J. Valois, </author> <title> "Implementing Lock-Free Queues", </title> <booktitle> Proceedings of the Seventh International Conference on Parallel and Distributed Computing Systems, </booktitle> <year> 1994, </year> <pages> pp. 64-69. 266 </pages>
Reference-contexts: Anderson and Woll [11] and Lanin and Shasha [67] present implementations for various set operations. 31 Valois presents lock-free implementations for various data structures, including queues, lists, trees, and dictionaries <ref> [90, 91, 92] </ref>. Finally, Massalin and Pu have implemented an entire operating system using lock-free data structures such as lists, queues, and stacks [71].
Reference: [91] <author> J. Valois, </author> <title> "Lock-Free Linked Lists Using Compare-and-Swap", </title> <booktitle> Proceedings of the 14th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <year> 1995, </year> <pages> pp. 214-222. </pages>
Reference-contexts: Anderson and Woll [11] and Lanin and Shasha [67] present implementations for various set operations. 31 Valois presents lock-free implementations for various data structures, including queues, lists, trees, and dictionaries <ref> [90, 91, 92] </ref>. Finally, Massalin and Pu have implemented an entire operating system using lock-free data structures such as lists, queues, and stacks [71].
Reference: [92] <author> J. Valois, </author> <title> "Lock-Free Data Structures ", Ph.D. </title> <type> Thesis, </type> <institution> Rensselaer Polytechnic Institute, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: Anderson and Woll [11] and Lanin and Shasha [67] present implementations for various set operations. 31 Valois presents lock-free implementations for various data structures, including queues, lists, trees, and dictionaries <ref> [90, 91, 92] </ref>. Finally, Massalin and Pu have implemented an entire operating system using lock-free data structures such as lists, queues, and stacks [71].
Reference: [93] <author> P. Vitanyi and B. Awerbuch, </author> <title> "Atomic Shared Register Access by Asynchronous Hardware", </title> <booktitle> Proceedings of the 27th IEEE Symposium on the Foundations of Computer Science, </booktitle> <year> 1986, </year> <pages> pp. 233-243. </pages>
Reference: [94] <author> J. Wing and C. Gong, </author> <title> "A Library of Concurrent Objects and their Proofs of Correctness", </title> <type> Technical Report CMU-CS-90-151, </type> <institution> Carnegie Mellon University, </institution> <year> 1990. </year>
Reference-contexts: Other researchers have considered wait-free and lock-free implementations using instructions that are stronger than simple reads and writes. Implementations of various types of queues have been presented by Lamport [64], by Herlihy and Wing [47], by Israeli and Rappoport [53], by Wing and Gong <ref> [94, 95] </ref>, and by Michael and Scott [73]. Anderson and Woll [11] and Lanin and Shasha [67] present implementations for various set operations. 31 Valois presents lock-free implementations for various data structures, including queues, lists, trees, and dictionaries [90, 91, 92].
Reference: [95] <author> J. Wing and C. Gong, </author> <title> "Testing and Verifying Concurrent Objects", </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 17(2), </volume> <year> 1993, </year> <pages> pp. 164-182. </pages>
Reference-contexts: Other researchers have considered wait-free and lock-free implementations using instructions that are stronger than simple reads and writes. Implementations of various types of queues have been presented by Lamport [64], by Herlihy and Wing [47], by Israeli and Rappoport [53], by Wing and Gong <ref> [94, 95] </ref>, and by Michael and Scott [73]. Anderson and Woll [11] and Lanin and Shasha [67] present implementations for various set operations. 31 Valois presents lock-free implementations for various data structures, including queues, lists, trees, and dictionaries [90, 91, 92].
Reference: [96] <author> R. Wisniewski, L. Kontothanassis, and M. Scott, </author> <title> "Scalable Spin Locks for Multipro-grammed Systems", </title> <booktitle> Proceedings of the 8th International Parallel Processing Symposium, </booktitle> <year> 1994, </year> <pages> pp. 583-589. </pages>
Reference-contexts: Those processes might in turn be preempted while they are waiting, making matters even worse. This problem seems to be inherent in any FIFO mutual exclusion algorithm. Recently, Wisniewski et al. <ref> [96] </ref> have designed mutual exclusion algorithms with the relaxed condition that running processes acquire the lock in FIFO order.
Reference: [97] <author> J.-H. Yang and J. Anderson, </author> <title> "Fast, Scalable Synchronization with Minimal Hardware Support", </title> <booktitle> Proceedings of the 12th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <year> 1993, </year> <pages> pp. 171-182. </pages>
Reference-contexts: Of course, significant effort has also been put into designing "efficient" mutual exclusion algorithms. Many have focused on the worst-case time complexity of acquiring a lock when no other processes require it, while others have studied the performance of various mutual exclusion algorithms under increasing levels of contention <ref> [12, 38, 72, 97] </ref>. (The level of contention is the number of processes that simultaneously request a lock.) Finally, efforts have been made to determine the impact of the available hardware and instructions on the design of mutual exclusion algorithms. <p> While another process holds the lock, these attempts fail, so processes that are requesting the lock must retry. Recent performance studies <ref> [12, 38, 72, 97] </ref> show that, as the number of concurrent processes in a system grows, excessive traffic on the processor-to-memory interconnect can quickly become a bottleneck that limits performance. <p> Some of the best-known are due to Anderson [12], to Graunke and Thakkar [38], to Mellor-Crummey and Scott [72], and to Yang and Anderson <ref> [97] </ref>. All of these mutual exclusion algorithms achieve scalable performance through the use of "local spinning". A spinning algorithm is one that busy-waits by repeatedly testing shared variables (i.e., no shared variables are written while busy-waiting). <p> To see the impact of local spinning on the performance of various mutual exclusion algorithms, consider Figure 2.1. This figure summarizes results of performance experiments run on the Sequent Symmetry, a shared-memory multiprocessor with cache-coherence; these results were originally reported by Yang and Anderson in <ref> [97] </ref>. The graph compares a simple test-and-set lock with mutual exclusion algorithms by Peterson and Fischer [82], by Lamport [65], by Styer [87], by Yang and Anderson [97], by Mellor-Crummey and Scott [72], and by Anderson [12]. <p> of performance experiments run on the Sequent Symmetry, a shared-memory multiprocessor with cache-coherence; these results were originally reported by Yang and Anderson in <ref> [97] </ref>. The graph compares a simple test-and-set lock with mutual exclusion algorithms by Peterson and Fischer [82], by Lamport [65], by Styer [87], by Yang and Anderson [97], by Mellor-Crummey and Scott [72], and by Anderson [12]. <p> Our decision to evaluate our k-exclusion algorithms by their remote-reference counts | that is, to distinguish between local and remote accesses of shared memory | is motivated by recent work on local-spin spin locks <ref> [5, 12, 38, 72, 97, 98] </ref>. In such locks, the impact of the processor-to-memory bottleneck is minimized by structuring programs so that processes busy wait only on locally-accessible shared variables. <p> In practice, a shared variable can be made locally-accessible by storing it in a local cache line or in a local par tition of distributed shared memory. Performance studies presented in <ref> [12, 38, 72, 97, 98] </ref> show that minimizing remote memory accesses is important for scalable performance in the design of synchronization algorithms. 95 Although the k-assignment problem may seem to be much harder than the k-exclusion problem, we show that if one allows reasonable synchronization primitives, then any k-exclusion algorithm can <p> The number of participating processes is varied, and the priority queue operations are equally divided among these processes. Previous experiments involving scalable synchronization constructs have assumed that each process runs on a dedicated processor <ref> [12, 38, 72, 97] </ref>. However, in practice it can be desirable to run more than one process on each processor. In our experiments, we consider scenarios in which processes share processors by multiprogramming.
Reference: [98] <author> J.-H. Yang and J. Anderson, </author> <title> "A Fast, Scalable Mutual Exclusion Algorithm", </title> <booktitle> Distributed Computing 9, </booktitle> <year> 1995, </year> <pages> pp. 51-60. </pages>
Reference-contexts: Our decision to evaluate our k-exclusion algorithms by their remote-reference counts | that is, to distinguish between local and remote accesses of shared memory | is motivated by recent work on local-spin spin locks <ref> [5, 12, 38, 72, 97, 98] </ref>. In such locks, the impact of the processor-to-memory bottleneck is minimized by structuring programs so that processes busy wait only on locally-accessible shared variables. <p> In practice, a shared variable can be made locally-accessible by storing it in a local cache line or in a local par tition of distributed shared memory. Performance studies presented in <ref> [12, 38, 72, 97, 98] </ref> show that minimizing remote memory accesses is important for scalable performance in the design of synchronization algorithms. 95 Although the k-assignment problem may seem to be much harder than the k-exclusion problem, we show that if one allows reasonable synchronization primitives, then any k-exclusion algorithm can
References-found: 98

