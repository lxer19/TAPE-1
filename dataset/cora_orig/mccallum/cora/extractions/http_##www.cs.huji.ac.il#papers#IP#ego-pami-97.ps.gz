URL: http://www.cs.huji.ac.il/papers/IP/ego-pami-97.ps.gz
Refering-URL: http://www.cs.huji.ac.il/~peleg/index.html
Root-URL: http://www.cs.huji.ac.il
Title: Recovery of Ego-Motion Using Region Alignment decomposition of image motion into a 2D parametric motion
Author: Michal Irani Benny Rousso Shmuel Peleg 
Note: The  
Abstract: A method for computing the 3D camera motion (the ego-motion) in a static scene is described, where initially a detected 2D motion between two frames is used to align corresponding image regions. We prove that such a 2D registration removes all effects of camera rotation, even for those image regions that remain misaligned. The resulting residual parallax displacement field between the two region-aligned images is an epipolar field centered at the FOE (Focus-of-Expansion). The 3D camera translation is recovered from the epipolar field. The 3D camera rotation is recovered from the computed 3D translation and the detected 2D motion. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Adiv. </author> <title> Determining three-dimensional motion and structure from optical flow generated by several moving objects. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 7(4) </volume> <pages> 384-401, </pages> <month> July </month> <year> 1985. </year>
Reference: [2] <author> G. Adiv. </author> <title> Inherent ambiguities in recovering 3D motion and structure from a noisy flow field. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 11 </volume> <pages> 477-489, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: This is opposed to most methods which try to compute the ego-motion from the flow field, and require an accurate flow field in order to resolve the rotation-translation ambiguity <ref> [2] </ref>. Furthermore, the residual flow field can be estimated more accurately than general flow, as it is globally constrained to lie on an epipolar field. Once the FOE is estimated, and given camera calibration information, the 3D camera translation (T X ; T Y ; T Z ) is recovered. <p> These inaccuracies lead to large errors in the interpretation the image motion in terms of its rotational and translational components <ref> [2] </ref>. 2D parametric estimation, on the other hand, is expressed in terms of few parameters (e.g., 8), yet has a substantially larger region of support in the image. Therefore, the "flow" estimation of a 2D parametric motion is highly constrained and well conditioned.
Reference: [3] <author> S. Ayer and H. Sawhney. </author> <title> Layered representation of motion video using robust maximum-likelihood estimation of mixture models and mdl encoding. </title> <booktitle> In International Conference on Computer Vision, </booktitle> <pages> pages 777-784, </pages> <address> Cambridge, MA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: III. Computing a 2D Parametric Motion We use the method described in [16] to detect a 2D parametric transformation of an image region. This method is briefly described in this section. Other methods for computing a 2D parametric region motion [7], <ref> [3] </ref> can be used as well. Let R be an image region that has a single 2D parametric transformation q between two frames, I (x; y; t) and a) b) Fig. 3.
Reference: [4] <author> Moshe Ben-Ezra, Shmuel Peleg, and Benny Rousso. </author> <title> Motion segmentation using convergence properties. </title> <booktitle> In ARPA IU Workshop, </booktitle> <month> November </month> <year> 1994. </year> <note> In these proceedings. </note>
Reference-contexts: However, a region of support R of an image segment with a single 2D parametric motion is not known a-priori. To allow for automatic detection and locking onto a single 2D parametric image motion, a robust version of this scheme is applied [16], <ref> [4] </ref>. The robust version of the algorithm incorporates two additional mechanisms to the above described scheme: 1. Outlier Rejection: The local misalignments at each iteration provide weights for the weighted-least-squares regression process of the next iteration. 4 2.
Reference: [5] <author> J.R. Bergen, P. Anandan, K.J. Hanna, and R. Hingorani. </author> <title> Hierarchical model-based motion estimation. </title> <booktitle> In European Conference on Computer Vision, </booktitle> <pages> pages 237-252, </pages> <address> Santa Margarita Ligure, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: After iterating certain number of times within a pyramid level, the process continues at the next finer level [6], <ref> [5] </ref>, [16]. When the above technique is applied to a region R, the reference and the inspection images are registered so that the image region R is aligned. However, a region of support R of an image segment with a single 2D parametric motion is not known a-priori.
Reference: [6] <author> J.R. Bergen, P.J. Burt, R. Hingorani, and S. Peleg. </author> <title> A three-frame algorithm for estimating two-component image motion. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 14 </volume> <pages> 886-895, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: After iterating certain number of times within a pyramid level, the process continues at the next finer level <ref> [6] </ref>, [5], [16]. When the above technique is applied to a region R, the reference and the inspection images are registered so that the image region R is aligned. However, a region of support R of an image segment with a single 2D parametric motion is not known a-priori.
Reference: [7] <author> M.J. Black and P. Anandan. </author> <title> The robust estimation of multiple motions: Parametric and piecewise-smooth flow fields. CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> 63(1) </volume> <pages> 75-104, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: III. Computing a 2D Parametric Motion We use the method described in [16] to detect a 2D parametric transformation of an image region. This method is briefly described in this section. Other methods for computing a 2D parametric region motion <ref> [7] </ref>, [3] can be used as well. Let R be an image region that has a single 2D parametric transformation q between two frames, I (x; y; t) and a) b) Fig. 3.
Reference: [8] <author> R. Chipolla, Y. Okamoto, and Y. Kuno. </author> <title> Robust structure from motion using motion paralax. </title> <booktitle> In International Conference on Computer Vision, </booktitle> <pages> pages 374-382, </pages> <address> Berlin, </address> <month> May </month> <year> 1993. </year>
Reference: [9] <author> K. Daniilidis and H.-H. Nagel. </author> <title> The coupling of rotation and translation in motion estimation of planar surfaces. </title> <booktitle> In IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 188-193, </pages> <month> June </month> <year> 1993. </year>
Reference: [10] <author> K. Hanna. </author> <title> Direct multi-resolution estimation of ego-motion and structure from motion. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 156-162, </pages> <address> Princeton, NJ, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: T Y ; T Z ) = (1:68 cm ; 0:16 cm ; 12 cm ) and ( X ; Y ; Z ) = Once the 3D motion parameters of the camera are computed, the 3D scene structure can be reconstructed using a scheme similar to that suggested in <ref> [10] </ref>. Correspondences between small image patches (currently 5 fi 5 pixels) are computed only along the radial lines emerging from the FOE (taking the rotations into account). The depth map is computed from the magnitude of these displacements.
Reference: [11] <author> D.J. Heeger and A. Jepson. </author> <title> Simple method for computing 3d motion and depth. </title> <booktitle> In International Conference on Computer Vision, </booktitle> <pages> pages 96-100, </pages> <year> 1990. </year>
Reference: [12] <author> B.K.P. Horn. </author> <title> Relative orientation. </title> <journal> International Journal of Computer Vision, </journal> <volume> 4(1) </volume> <pages> 58-78, </pages> <month> June </month> <year> 1990. </year>
Reference: [13] <author> B.K.P. Horn and E.J. Weldon. </author> <title> Direct methods for recovering motion. </title> <journal> International Journal of Computer Vision, </journal> <volume> 2(1) </volume> <pages> 51-76, </pages> <month> June </month> <year> 1988. </year>
Reference: [14] <author> M. Irani and P. Anandan. </author> <title> Parallax geometry of pairs of points for 3D scene analysis. </title> <booktitle> In European Conference on Computer Vision, pages I:17-30, </booktitle> <address> Cambridge, UK, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: The depth map is computed from the magnitude of these displacements. In Fig. 2.d, the computed inverse depth map of the scene ( 1 Z (x;y) ) is displayed. Similar approaches to 3D shape re covery have since been suggested by [25], [18], [26], <ref> [14] </ref>. Fig. ?? shows an example where the ego-motion estimation was used to electronically stabilize (i.e., remove camera jitter) from a sequence obtained by a hand held camera. III.
Reference: [15] <author> M. Irani, B. Rousso, and S. Peleg. </author> <title> Detecting and tracking multiple moving objects using temporal integration. </title> <booktitle> In European Conference on Computer Vision, </booktitle> <pages> pages 282-287, </pages> <address> Santa Mar-garita Ligure, </address> <month> May </month> <year> 1992. </year>
Reference: [16] <author> M. Irani, B. Rousso, and S. Peleg. </author> <title> Computing occluding and transparent motions. </title> <journal> International Journal of Computer Vision, </journal> <volume> 12(1) </volume> <pages> 5-16, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Fig. ?? shows an example where the ego-motion estimation was used to electronically stabilize (i.e., remove camera jitter) from a sequence obtained by a hand held camera. III. Computing a 2D Parametric Motion We use the method described in <ref> [16] </ref> to detect a 2D parametric transformation of an image region. This method is briefly described in this section. Other methods for computing a 2D parametric region motion [7], [3] can be used as well. <p> After iterating certain number of times within a pyramid level, the process continues at the next finer level [6], [5], <ref> [16] </ref>. When the above technique is applied to a region R, the reference and the inspection images are registered so that the image region R is aligned. However, a region of support R of an image segment with a single 2D parametric motion is not known a-priori. <p> However, a region of support R of an image segment with a single 2D parametric motion is not known a-priori. To allow for automatic detection and locking onto a single 2D parametric image motion, a robust version of this scheme is applied <ref> [16] </ref>, [4]. The robust version of the algorithm incorporates two additional mechanisms to the above described scheme: 1. Outlier Rejection: The local misalignments at each iteration provide weights for the weighted-least-squares regression process of the next iteration. 4 2.
Reference: [17] <author> M. Irani, B. Rousso, and S. Peleg. </author> <title> Recovery of ego-motion using image stabilization. </title> <booktitle> In IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 454-460, </pages> <address> Seattle, </address> <month> June </month> <year> 1994. </year>
Reference: [18] <author> R. Kumar, P. Anandan, and K. Hanna. </author> <title> Direct recovery of shape from multiple views: a parallax based approach. </title> <booktitle> In Proc 12th ICPR, </booktitle> <year> 1994. </year>
Reference-contexts: The depth map is computed from the magnitude of these displacements. In Fig. 2.d, the computed inverse depth map of the scene ( 1 Z (x;y) ) is displayed. Similar approaches to 3D shape re covery have since been suggested by [25], <ref> [18] </ref>, [26], [14]. Fig. ?? shows an example where the ego-motion estimation was used to electronically stabilize (i.e., remove camera jitter) from a sequence obtained by a hand held camera. III.
Reference: [19] <author> J.M. Lawn and R. Cipolla. </author> <title> Epipolar estimation using affine motion-parallax. </title> <booktitle> In BMVC93, </booktitle> <year> 1993. </year>
Reference: [20] <author> D.T. Lawton and J.H. Rieger. </author> <title> The use of difference fields in processing sensor motion. </title> <booktitle> In ARPA IU Workshop, </booktitle> <pages> pages 78-83, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: To locate the FOE, the parallax optical flow between the registered frames is computed, and the FOE is located using a search method similar to that described in <ref> [20] </ref>. Candidates for the FOE are sampled over a half sphere and projected onto the image plane. For each such candidate, a global error measure is computed from local deviations of the flow field from the radial lines emerging from the candidate FOE.
Reference: [21] <author> C.H. Lee. </author> <title> Structure and motion from two perspective views via planar patch. </title> <booktitle> In International Conference on Computer Vision, </booktitle> <pages> pages 158-164, </pages> <year> 1988. </year>
Reference: [22] <author> H.C. Longuet-Higgins. </author> <title> Visual ambiguity of a moving plane. </title> <journal> Proceedings of The Royal Society of London B, </journal> <volume> 223 </volume> <pages> 165-175, </pages> <year> 1984. </year>
Reference: [23] <author> S. Negahdaripour and S. Lee. </author> <title> Motion recovery from image sequences using first-order optical flow information. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 132-139, </pages> <address> Princeton, NJ, </address> <month> Oc-tober </month> <year> 1991. </year>
Reference: [24] <author> F. Lustman O.D. Faugeras and G. Toscani. </author> <title> Motion and structure from motion from point and line matching. </title> <booktitle> In Proc. 1st International Conference on Computer Vision, </booktitle> <pages> pages 25-34, </pages> <address> London, </address> <year> 1987. </year>
Reference: [25] <author> Harpreet Sawhney. </author> <title> 3d geometry from planar parallax. </title> <booktitle> In IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: The depth map is computed from the magnitude of these displacements. In Fig. 2.d, the computed inverse depth map of the scene ( 1 Z (x;y) ) is displayed. Similar approaches to 3D shape re covery have since been suggested by <ref> [25] </ref>, [18], [26], [14]. Fig. ?? shows an example where the ego-motion estimation was used to electronically stabilize (i.e., remove camera jitter) from a sequence obtained by a hand held camera. III.
Reference: [26] <author> A. Shashua and N. Navab. </author> <title> Relative affine structure: Theory and application to 3d reconstruction from perspective views. </title> <booktitle> In IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 483-489, </pages> <address> Seattle, Wa., </address> <month> June </month> <year> 1994. </year> <month> 6 </month>
Reference-contexts: The depth map is computed from the magnitude of these displacements. In Fig. 2.d, the computed inverse depth map of the scene ( 1 Z (x;y) ) is displayed. Similar approaches to 3D shape re covery have since been suggested by [25], [18], <ref> [26] </ref>, [14]. Fig. ?? shows an example where the ego-motion estimation was used to electronically stabilize (i.e., remove camera jitter) from a sequence obtained by a hand held camera. III.
References-found: 26

