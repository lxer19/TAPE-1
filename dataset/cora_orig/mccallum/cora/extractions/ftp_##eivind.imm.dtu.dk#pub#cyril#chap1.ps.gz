URL: ftp://eivind.imm.dtu.dk/pub/cyril/chap1.ps.gz
Refering-URL: http://eivind.imm.dtu.dk/staff/goutte/PUBLIS/thesis.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Parametric regression 1.1 Learning problem model f bw b w in turn is an estimator
Author: G (w) E (e f w e f w p (yjx) p dxdy (.) 
Note: b is the estimator of provided by the model. In the framework of this chapter, it is provided by a specific  risk  
Abstract: Let us present briefly the learning problem we will address in this chapter and the following. The ultimate goal is the modelling of a mapping f : x 7! y from multidimensional input x to output y. The output can be multi-dimensional, but we will mostly address situations where it is a one dimensional real value. Furthermore, we should take into account the fact that we scarcely ever observe the actual true mapping y = f (x). This is due to perturbations such as e.g. observational noise. We will rather have a joint probability p (x; y). We expect this probability to be peaked for values of x and y corresponding to the mapping. We focus on automatic learning by example. A set D = of data sampled from the joint distribution p (x; y) = p (yjx) p (x) is collected. With the help of this set, we try to identify a model of the data, parameterised by a set of 1.2 Learning and optimisation The fit of the model to the system in a given point x is measured using a criterion representing the distance from the model prediction b y to the system, e (y; f w (x)). This is the local risk . The performance of the model is measured by the expected This quantity represents the ability to yield good performance for all the possible situations (i.e. (x; y) pairs) and is thus called generalisation error . The optimal set 1 parameters w: f w : x 7! b y.
Abstract-found: 1
Intro-found: 1
Reference: <author> Akaike, H. </author> <year> (1969). </year> <title> Fitting autoregressive models for prediction. </title> <journal> Annals of the Institute of Statistical Mathematics, </journal> <volume> 21 </volume> <pages> 243-247. </pages>
Reference: <author> Amari, S. I. </author> <year> (1967). </year> <title> A theory of adaptive pattern classifiers. </title> <journal> IEEE Transactions on Electronic Computers, </journal> <volume> 16 </volume> <pages> 279-307. </pages>
Reference: <author> Battiti, R. </author> <year> (1992). </year> <title> First- and second-order methods for learning: Between steepest descent and newton's method. </title> <journal> Neural Computation, </journal> <volume> 4(2) </volume> <pages> 141-166. </pages>
Reference: <author> Bishop, C. M. </author> <year> (1995a). </year> <title> Neural Networks for pattern recognition. </title> <publisher> Clarendon Press, Oxford. </publisher>
Reference: <author> Bishop, C. M. </author> <year> (1995b). </year> <title> Training with noise is equivalent to thikonov regularization. </title> <booktitle> NC, </booktitle> <volume> 7(1) </volume> <pages> 110-116. </pages> <note> c flC. Goutte 1996 References 17 Bottou, </note> <author> L. </author> <year> (1991). </year> <institution> Une approche theorique de l'apprentissage connexionniste : application a la reconnaissance de la parole. These de doctorat, Universite Paris XI, Orsay. </institution>
Reference: <author> Bryson, A., Denham, W., and Dreyfuss, S. </author> <year> (1963). </year> <title> Optimal programming problem with inequality constraints. I: Necessary conditions for extremal solutions. </title> <journal> AIAA journal, </journal> <volume> 1 </volume> <pages> 25-44. </pages>
Reference-contexts: Incidentally, these books adopt a statistical perspective. 1.9 The back-propagation rule is usually credited to Werbos (1974) or Rumelhart et al. (1986). However, it was discovered earlier in different contexts. Vapnik (1995) mentions its use in <ref> (Bryson et al., 1963) </ref> for solving some control problems.
Reference: <author> Fletcher, R. </author> <year> (1987). </year> <title> Practical Methods of Optimization. </title> <publisher> Wiley. </publisher>
Reference-contexts: Bottou (1991) cites Amari (1967) in the context of adaptive systems and notes that it is nothing more than proper application of the derivation rules invented by Leibnitz in the 17th century. 1.10 Optimisation methods are covered in many books such as e.g. <ref> (Fletcher, 1987) </ref> or (Press et al., 1992) for one-dimensional and multi-dimensional techniques. The quadratic approximation is necessary to handle non linear problems. However, methods presented here are also used in the linear case.
Reference: <author> Friedman, J. H. </author> <year> (1996). </year> <title> On bias, variance, 0/1 loss, and the curse-of-dimensionality. </title> <type> Technical report, </type> <institution> Department of Statistics, Stanford University. ftp://playfair.stanford.edu/pub/friedman/curse.ps.Z. </institution>
Reference: <author> Geman, S., Bienenstock, E., and Doursat, R. </author> <year> (1992). </year> <title> Neural networks and the bias/variance dilemna. </title> <journal> Neural Computation, </journal> <volume> 4(1) </volume> <pages> 1-58. </pages>
Reference: <author> Goutte, C. and Ledoux, C. </author> <year> (1995). </year> <title> Synthese des techniques de commande connex-ionniste. </title> <type> Technical Report 95/02, </type> <institution> LAFORIA. </institution>
Reference: <author> Hertz, J., Krogh, A., and Palmer, R. G. </author> <year> (1991). </year> <title> Introduction to the theory of neural computation. </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: introduction to this class of models. <ref> (Hertz et al., 1991) </ref> has been acclaimed for some time as the reference in the field. It has now been joined by the books by Bishop (1995a) and Ripley (1996).
Reference: <author> Kouam, A. </author> <year> (1993). </year> <institution> Approches connexionnistes pour la prevision des series tem-porelles. </institution> <type> PhD thesis, </type> <institution> Universite de Paris Sud. </institution>
Reference: <author> Mtller, M. </author> <year> (1993a). </year> <title> Efficient Training of Feed-Forward Neural Networks. </title> <type> PhD thesis, </type> <institution> Computer Science department, Aarhus University. </institution>
Reference-contexts: Goutte 1996 16 Parametric regression 1.11 According to Press et al. (1992), the steepest descent algorithm dates back to Cauchy. Its convergence rate is a standard consideration in numerical analysis. It is derived, e.g. by Mtller (1993a). The convergence problem for ill-conditioned Hessians is illustrated e.g. in <ref> (Mtller, 1993a) </ref>, page 16 and (Press et al., 1992), page 421. 1.12 The stochastic algorithm and the associated convergence conditions date back to Robbins and Munro (1951). This algorithm was applied to neural networks at the end of the eighties by Rumelhart et al. (1986).
Reference: <author> Mtller, M. </author> <year> (1993b). </year> <title> Supervised learning on large redundant training sets. </title> <journal> International Journal of Neural Systems, </journal> <volume> 4(1) </volume> <pages> 15-25. </pages>
Reference-contexts: A neural network perspective is offered by Mtller (1993a). The same author mentions several extensions of the conjugate gradient algorithm designed to handle stochastic training, and proposes one in <ref> (Mtller, 1993b) </ref>. 1.14 Second order methods are presented by Battiti (1992), who confirms the equivalence of BFGS and conjugate gradient. Equation (1.46) is a simplification of the update presented by Battiti (1992), page 157.
Reference: <author> Ntrgaard, P. M. </author> <year> (1996). </year> <title> System identification and control with neural networks. </title> <type> PhD thesis, </type> <institution> Department of Automation, Technical University of Denmark. </institution>
Reference: <author> Press, W. H., Teukolsky, S. A., Vetterling, W. T., and Flannery, B. P. </author> <year> (1992). </year> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, 2nd edition. </publisher>
Reference-contexts: Bottou (1991) cites Amari (1967) in the context of adaptive systems and notes that it is nothing more than proper application of the derivation rules invented by Leibnitz in the 17th century. 1.10 Optimisation methods are covered in many books such as e.g. (Fletcher, 1987) or <ref> (Press et al., 1992) </ref> for one-dimensional and multi-dimensional techniques. The quadratic approximation is necessary to handle non linear problems. However, methods presented here are also used in the linear case. Indeed, iterative minimisation methods are a common alternative to the computationally ex pensive matrix inversion in equation (1.12). c flC. <p> Its convergence rate is a standard consideration in numerical analysis. It is derived, e.g. by Mtller (1993a). The convergence problem for ill-conditioned Hessians is illustrated e.g. in (Mtller, 1993a), page 16 and <ref> (Press et al., 1992) </ref>, page 421. 1.12 The stochastic algorithm and the associated convergence conditions date back to Robbins and Munro (1951). This algorithm was applied to neural networks at the end of the eighties by Rumelhart et al. (1986).
Reference: <author> Rasmussen, C. E. </author> <year> (1993). </year> <title> Generalization in neural networks. </title> <type> Master's thesis, </type> <institution> Electronics institute, Technical University of Denmark. </institution>
Reference: <author> Ripley, B. D. </author> <year> (1996). </year> <title> Pattern Recognition and Neural Networks. </title> <publisher> Cambridge University Press. </publisher>
Reference: <author> Robbins, H. and Munro, S. </author> <year> (1951). </year> <title> A stochastic approximation method. </title> <journal> Annals of Math. Stat., </journal> <volume> 22 </volume> <pages> 400-407. </pages>
Reference: <author> Rumelhart, D., Hinton, G., and Williams, R. </author> <year> (1986). </year> <title> Learning internal representation by error propagation. </title> <editor> In Rumelhart, D. and McClellan, J., editors, </editor> <booktitle> Parallel Distributed Processing : exploring the microstructure of cognition, </booktitle> <volume> volume 1, </volume> <pages> pages 318-362. </pages> <publisher> MIT Press. </publisher>
Reference: <author> Tibshirani, R. </author> <year> (1996). </year> <title> Bias, variance and prediction error for classification rules. </title> <type> Technical report, </type> <institution> University of Toronto. </institution> <note> http://utstat.toronto.edu/pub/tibs/biasvar.ps. c flC. Goutte 1996 18 Parametric regression Tsypkin, </note> <author> Y. Z. and Nikolic, Z. J. </author> <year> (1971). </year> <title> Adaptation and learning in automatic systems, </title> <booktitle> volume 73 of Mathematics in science and engineering. </booktitle> <publisher> Academic Press, </publisher> <address> New York and London. </address>
Reference: <author> Vapnik, V. N. </author> <year> (1995). </year> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer. </publisher>
Reference: <author> Werbos, P. J. </author> <year> (1974). </year> <title> Beyond regression: New Tools for Prediction and Analysis in the Behavioral Sciences. </title> <type> PhD thesis, </type> <institution> Harvard University, </institution> <address> Cambridge, MA. </address>
Reference: <author> White, H. </author> <year> (1989). </year> <title> Learning in artificial neural networks: A statistical perspective. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 425-464. </pages>
Reference: <author> Wolpert, D. H. </author> <year> (1996). </year> <title> The lack of a priori distinctions between learning algorithms. </title> <journal> Neural Computation, </journal> <volume> 8(8) </volume> <pages> 1341-1390. </pages> <address> c flC. </address> <month> Goutte </month> <year> 1996 </year>
References-found: 25

