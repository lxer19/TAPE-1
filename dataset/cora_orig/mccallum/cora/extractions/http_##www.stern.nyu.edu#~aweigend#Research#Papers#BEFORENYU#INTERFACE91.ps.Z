URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/BEFORENYU/INTERFACE91.ps.Z
Refering-URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/BEFORENYU/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Abstract-found: 0
Intro-found: 1
Reference: <author> Akaike, Hirotugo. </author> <title> Statistical predictor identification. </title> <journal> Ann. Institute of Statistical Mathematics, </journal> <volume> 22 </volume> <pages> 203-217, </pages> <year> 1970. </year>
Reference-contexts: For linear regression, it is sometimes possible to correct for the usually over-optimistic estimate. An example is to multiply the fitting error with (N + k)=(N k), where N is the number of data points and k is the number of parameters of the model <ref> (Akaike, 1970) </ref>. It is not at all clear to what degree such approximations hold for nonlinear models, such as connectionist networks.
Reference: <author> Baldi, Pierre and Chauvin, Yves. </author> <title> Temporal evolution of generalization during learning in linear networks. </title> <note> Submitted to Neural Computation, </note> <year> 1991. </year>
Reference: <author> Baldi, Pierre and Hornik, Kurt. </author> <title> Back-propagation and unsupervised learning in linear networks. </title> <editor> In Chauvin, Y. and Rumelhart, D. E., editors, </editor> <title> Backpropagation and Connectionist Theory. </title> <publisher> Lawrence Erlbaum, </publisher> <year> 1992. </year>
Reference: <author> Friedman, Jerome H. </author> <title> Multivariate adaptive regression splines. </title> <journal> The Annals of Statistics, </journal> <volume> 19 </volume> <pages> 1-141 (with discussion), </pages> <year> 1991. </year>
Reference: <author> Geman, Stuart, Bienenstock, Elie, and Doursat, Rene. </author> <title> Neural networks and the bias/variance dilemma. </title> <note> Submitted to Neural Computation, </note> <year> 1991. </year>
Reference: <author> Lapedes, Alan S. and Farber, Robert M. </author> <title> Nonlinear signal processing using neural networks: prediction and system modelling. </title> <type> Technical Report LA-UR-87-2662, </type> <institution> Los Alamos National Laboratory, </institution> <year> 1987. </year>
Reference: <author> Lewis, Peter A. W. and Stevens, J. G. </author> <title> Nonlinear modeling of time series using multivariate adaptive regression splines (MARS). </title> <journal> Submitted to Journal of the American Statistical Association, </journal> <year> 1991. </year>
Reference: <author> Nowlan, Steven J. </author> <title> Soft Competitive Adaptation: Neural Network Learning Algorithms based on Fitting Statistical Mixtures. </title> <type> PhD thesis, </type> <institution> CMU (Computer Science), </institution> <year> 1991. </year>
Reference: <author> Priestley, Maurice B. </author> <title> Non-linear and Non-stationary Time Series Analysis. </title> <publisher> Academic Press, </publisher> <year> 1988. </year>
Reference: <author> Rumelhart, David E., Hinton, Geoffrey E., and Williams, Ronald J. </author> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E. and McClelland, J. L., editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <pages> pages 318-362. </pages> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: There is a u-shaped minimum between the extremes of having a too simple network that produces horrendous errors and a network with small errors on the training data that has enormous complexity. This sum is minimized through back-propagation <ref> (Rumelhart et al., 1986) </ref>. 1.1 ARCHITECTURE Fig. 1 shows the architecture (the pattern of connectivity or topology) of a feed-forward network with one hidden layer. (For the time series we analyzed, one hidden layer sufficed.) The abbreviation d-n-1 denotes the following network: * The d input units are given the past
Reference: <author> Subba Rao, T. and Gabr, M. M. </author> <title> An Introduction to Bispectral Analysis and Bilinear Time Series Models, </title> <booktitle> volume 24 of Lecture Notes in Statistics. </booktitle> <publisher> Springer, </publisher> <year> 1984. </year>
Reference: <author> Stokbro, Kurt. </author> <title> Predicting chaos with weighted maps. </title> <type> Technical Report 91/10 S, </type> <institution> Nordita, Copenhagen, </institution> <year> 1991. </year>
Reference: <author> Tong, Howell and Lim, K. S. </author> <title> Threshold autoregression, limit cycles and cyclical data. </title> <journal> Journal Royal Statistical Society B, </journal> <volume> 42 </volume> <pages> 245-292, </pages> <year> 1980. </year>
Reference: <author> Tong, Howell. </author> <title> Non-linear Time Series: a Dynamical System Approach. </title> <publisher> Oxford University Press, </publisher> <year> 1990. </year>
Reference: <author> Weigend, Andreas S., Huberman, Bernardo A., and Rumel-hart, David E. </author> <title> Predicting the future: a connectionist approach. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1 </volume> <pages> 193-209, </pages> <year> 1990. </year>
Reference: <author> Weigend, Andreas S., Huberman, Bernardo A., and Rumel-hart, David E. </author> <title> Predicting sunspots and exchange rates with connectionist networks. </title> <editor> In Casdagli, M. and Eu-bank, S. G., editors, </editor> <title> Nonlinear Modeling and Forecasting. </title> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
References-found: 16

