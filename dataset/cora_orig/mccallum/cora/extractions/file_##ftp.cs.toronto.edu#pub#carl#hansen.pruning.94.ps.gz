URL: file://ftp.cs.toronto.edu/pub/carl/hansen.pruning.94.ps.gz
Refering-URL: http://www.cs.toronto.edu/~carl/pub.html
Root-URL: 
Email: Emails: lars@eiffel.ei.dth.dk carl@cs.toronto.edu  
Title: Pruning from Adaptive Regularization  
Author: Lars Kai Hansen and Carl Edward Rasmussen 
Date: January 12, 1994  
Address: Denmark, B 349 DK-2800 Lyngby. Denmark  
Affiliation: connect, Electronics Institute Technical University of  
Abstract: Inspired by the recent upsurge of interest in Bayesian methods we consider adaptive regularization. A generalization based scheme for adaptation of regularization parameters is introduced and compared to Bayesian regularization. We show that pruning arises naturally within both adaptive regularization schemes. As model example we have chosen the simplest possible: estimating the mean of a random variable with known variance. Marked similarities are found between the two methods in that they both involve a "noise limit", below which they regularize with infinite weight decay, i.e., they prune. However, pruning is not always beneficial. We show explicitly that both methods in some cases may increase the generalization error. This corresponds to situations where the underlying assumptions of the regularizer are poorly matched to the environment.
Abstract-found: 1
Intro-found: 1
Reference: <author> Akaike, H. </author> <year> (1969). </year> <title> Fitting autoregressive models for prediction. </title> <journal> Ann. Inst. Stat. Mat., </journal> <volume> 21 </volume> <pages> 243-247. </pages>
Reference: <author> Buntine, W. and Weigend, A. </author> <year> (1991). </year> <title> Bayesian back-propagation. </title> <journal> Complex Systems, </journal> <volume> 5 </volume> <pages> 603-643. </pages>
Reference-contexts: We note that for a teacher distribution that has significant mass at small teachers like the P 0 ( ~w) ~ 1=j ~wj distribution corresponding to the prior of the Bayesian scheme <ref> (Buntine and Weigend, 1991) </ref> the brave generalization student is 8 adaptive regularization schemes used to estimate the mean of a Gaussian variable.
Reference: <author> Hansen, L. </author> <year> (1993). </year> <title> Stochastic linear learning: Exact test and training error averages. </title> <booktitle> Neural Networks, </booktitle> <volume> 6 </volume> <pages> 393-396. </pages>
Reference-contexts: We iterate: this is no big surprise since that student precisely minimizes the generalization error with the correct (implicit) prior. Our experience with adaptive regularization is generally positive. A detailed account of multivariate applications, where pruning is achieved by utilizing individual weight regularization will be given elsewhere <ref> (Rasmussen and Hansen, 1993) </ref>. The idea of using the estimated test error as a cost function for determination of regularization parameters is quite general.
Reference: <author> Le Cun, Y., Denker, J., and Solla, S. </author> <year> (1990). </year> <title> Optimal brain damage. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 598-605, </pages> <address> San Mateo. (Denver 1989), </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> MacKay, D. </author> <year> (1992a). </year> <title> Bayesian interpolation. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 415-447. </pages>
Reference-contexts: a range of ff 2 we find that w PM shows a qualitative pruning behavior similar to that of w MP . 5 Generalization error minimization While the evidence is an interesting statistical quantity, it is not obvious what the relation is between maximizing the evidence and minimizing test error <ref> (MacKay, 1992a) </ref>. Since the latter often is the basic modeling objective, we propose here to base the optimization of ff on the expected generalization error, cf. eq. (2).
Reference: <author> MacKay, D. </author> <year> (1992b). </year> <title> A practical framework for backpropagation networks. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 448-472. </pages>
Reference: <author> Moody, J. </author> <year> (1991). </year> <title> Note on generalization, regularization and architecture selection in nonlinear systems. </title> <editor> In Kung, S., Juang, B., and Kamm, C., editors, </editor> <booktitle> IEEE International Conference on Neural Networks, </booktitle> <pages> pages 1-10. </pages> <publisher> IEEE. </publisher>
Reference-contexts: Pruning is a popular tool for reducing model capacity and pruning schemes have been successfully applied to layered neural networks (Le Cun et al., 1990; Thodberg, 1991; Svarer et al., 1993). While pruning is a discrete decision process, regularization introduces soft constraints such as weight decay <ref> (Moody, 1991) </ref>. A common feature of these techniques is the need for control parameters: stop criteria for pruning and weight decays 1 for regularization. In (Svarer et al., 1993) a statistical stop criterion was developed for pruning of networks for regression problems. <p> Since the latter often is the basic modeling objective, we propose here to base the optimization of ff on the expected generalization error, cf. eq. (2). A similar approach was mentioned in <ref> (Moody, 1991) </ref>. 5 In order to be able to compare directly with the Bayesian approach, we use a regularized least squares procedure to find the student weight E T = 2 2 m=1 1 ffw 2 ; (15) Minimizing w.r.t. w, we recover (9) w G = N + ff 2
Reference: <author> Rasmussen, C. and Hansen, L. </author> <year> (1993). </year> <note> In preparation. </note>
Reference-contexts: We iterate: this is no big surprise since that student precisely minimizes the generalization error with the correct (implicit) prior. Our experience with adaptive regularization is generally positive. A detailed account of multivariate applications, where pruning is achieved by utilizing individual weight regularization will be given elsewhere <ref> (Rasmussen and Hansen, 1993) </ref>. The idea of using the estimated test error as a cost function for determination of regularization parameters is quite general.
Reference: <author> Solla, S. </author> <year> (1992). </year> <title> Capacity control in classsifiers for pattern recognizers. </title> <editor> In Kung, S.Y. et al., editor, </editor> <booktitle> IEEE International Conference on Neural Networks, </booktitle> <pages> pages 255-266, </pages> <address> Piscataway, NJ. </address> <publisher> IEEE. </publisher>
Reference: <author> Svarer, C., Hansen, L., and Larsen, J. </author> <year> (1993). </year> <title> On design and evaluation of tapped-delay neural network architectures. </title> <editor> In Berenji, H.R. et al., editor, </editor> <booktitle> IEEE International Conference on Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 46-51, </pages> <address> Piscataway, NJ. (San Francisco 1993), </address> <publisher> IEEE. </publisher>
Reference-contexts: While pruning is a discrete decision process, regularization introduces soft constraints such as weight decay (Moody, 1991). A common feature of these techniques is the need for control parameters: stop criteria for pruning and weight decays 1 for regularization. In <ref> (Svarer et al., 1993) </ref> a statistical stop criterion was developed for pruning of networks for regression problems. <p> The idea of using the estimated test error as a cost function for determination of regularization parameters is quite general. We are currently pursuing this in the context of time series processing with feed-forward networks for which the corresponding generalization error estimate is well-known see eg. <ref> (Svarer et al., 1993) </ref> In conclusion we have shown that the use of adaptive regularization for the simple task of estimating the mean of a Gaussian variable of known variance, leads to non-trivial decision processes.
Reference: <author> Thodberg, H. </author> <year> (1991). </year> <title> Improving generalization on neural networks through pruning. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1 </volume> <pages> 317-326. 10 </pages>
References-found: 11

