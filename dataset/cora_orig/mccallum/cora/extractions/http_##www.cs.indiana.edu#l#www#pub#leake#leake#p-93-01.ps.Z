URL: http://www.cs.indiana.edu/l/www/pub/leake/leake/p-93-01.ps.Z
Refering-URL: http://www.cs.indiana.edu/l/www/pub/leake/leake/
Root-URL: http://www.cs.indiana.edu
Title: Focusing Construction and Selection of Abductive Hypotheses  
Author: David B. Leake 
Address: Bloomington, IN 47405, U.S.A.  
Affiliation: Department of Computer Science Indiana University  
Note: Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, 1993, pp. 24-29  
Abstract: Many abductive understanding systems explain novel situations by a chaining process that is neutral to explainer needs beyond generating some plausible explanation for the event being explained. This paper examines the relationship of standard models of abductive understanding to the case-based explanation model. In case-based explanation, construction and selection of abductive hypotheses are focused by specific explanations of prior episodes and by goal-based criteria reflecting current information needs. The case-based method is inspired by observations of human explanation of anomalous events during everyday understanding, and this paper focuses on the method's contributions to the problems of building good explanations in everyday domains. We identify five central issues, compare how those issues are addressed in traditional and case-based explanation models, and discuss motivations for using the case-based approach to facilitate generation of plausible and useful explanations in domains that are complex and imperfectly un derstood.
Abstract-found: 1
Intro-found: 1
Reference: [ Ajjanagadde, 1991 ] <author> V. Ajjanagadde. </author> <title> Incorporating background knowledge and structured explananda in abductive reasoning: A framework. </title> <institution> Wilhelm Schickard-Institute fur Informatik WSI 91-6, Universitat Tubingen, </institution> <year> 1991. </year>
Reference-contexts: However, plausibility alone is not sufficient to distinguish between candidate explanations|many different valid explanations can be generated for any event. One possible response is to attempt to pursue explanations from all perspectives <ref> [ Ajjanagadde, 1991 ] </ref> , but this aggravates the already expensive explanation task. Instead, case-based explanation takes the view that considerations of information needs should determine the aspects of an event to explain.
Reference: [ Charniak and Goldman, 1991 ] <author> E. Charniak and R. Gold-man. </author> <title> A probabilistic model of plan recognition. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 160-165, </pages> <address> Anaheim, CA, </address> <month> July </month> <year> 1991. </year> <note> AAAI. </note>
Reference: [ Charniak, 1986 ] <author> E. Charniak. </author> <title> A neat theory of marker passing. </title> <booktitle> In Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <pages> pages 584-588, </pages> <address> Philadelphia, PA, </address> <month> August </month> <year> 1986. </year> <note> AAAI. </note>
Reference-contexts: A well-known problem for these methods is the cost of explanation construction, due to the combinatorial explosion of alternatives to consider. Methods have been proposed to control chaining cost (e.g., <ref> [ Charniak, 1986; Hobbs et al., 1990 ] </ref> ), but despite the benefits of these methods, the difficulty of efficiently generating explanations remains acute in rich domains. Case-based explanation addresses this problem by adapting prior explanations to new situations, to avoid the cost of chaining from scratch. <p> In abductive reasoning systems, the dominant method for judging plausibility of explanations is to favor explanations that are in some sense structurally "minimal" (e.g. <ref> [ Charniak, 1986; Kautz and Allen, 1986 ] </ref> ). These methods stress factors such as the number of abductive hypotheses rather than their content. Instead, the case-based model relies primarily on the content of the assumptions and explanatory chain.
Reference: [ deKleer and Williams, 1989 ] <author> J. deKleer and B. Williams. </author> <title> Diagnosis with behavioral modes. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1324-1330, </pages> <address> Detroit, MI, </address> <month> August </month> <year> 1989. </year> <month> IJ-CAI. </month>
Reference-contexts: This approach differs from models of abduction in which all explanation construction is assumed to precede any evaluation, and is in the same spirit as recent research on choosing which explanations to pursue according to validity estimates (e.g., <ref> [ deKleer and Williams, 1989; Ng and Mooney, 1990 ] </ref> .
Reference: [ Dietterich and Flann, 1988 ] <author> T. Dietterich and N. Flann. </author> <title> An inductive approach to solving the imperfect theory problem. </title> <booktitle> In Proceedings of the 1988 AAAI Spring Symposium on Explanation-based Learning, </booktitle> <address> Stanford, CA., 1988. </address> <publisher> AAAI. </publisher>
Reference-contexts: Previous research on overcoming imperfect theory problems focuses on methods for repairing an imperfect domain theory when problems arise (e.g., <ref> [ Dietterich and Flann, 1988; Rajamoney, 1988 ] </ref> ); our focus is how to generate reasonable explanations despite possible imperfections in the domain theory. 3.2 The nature of explanations The need of everyday explainers to use imperfect information is reflected in how case-based explanation views the nature of explanations.
Reference: [ Freitag and Friedrich, 1991 ] <author> H. Freitag and G. Friedrich. </author> <title> Goal-driven structural focusing in model-based diagnosis. </title> <booktitle> In Working Notes of the Second International Workshop on Principles of Diagnosis, </booktitle> <address> Milano, Italy, </address> <year> 1991. </year>
Reference-contexts: However, the influence of overarching goals has begun to be investigated in research on abductive diagnosis (e.g., for controlling large-scale diagnosis <ref> [ Freitag and Friedrich, 1991 ] </ref> and for integrating diagnosis and response [ Rymon et al., 1991 ] ).
Reference: [ Hobbs et al., 1990 ] <author> J. Hobbs, M. Stickel, D. Appelt, and P. Martin. </author> <title> Interpretation as abduction. </title> <type> Technical Report 499, </type> <institution> SRI International, </institution> <year> 1990. </year>
Reference-contexts: nature of explanations, (3) the plausibility evaluation process, (4) the role of anomalies in focusing an un-derstander's explanation, and (5) the influence of over-arching goals on explanation. 3.1 Building explanations Explanation construction methods: In abduc-tive understanding systems, standard theorem-proving chaining techniques are generally the mechanism for generating candidate explanations (e.g., <ref> [ Hobbs et al., 1990; Kautz and Allen, 1986 ] </ref> ). A well-known problem for these methods is the cost of explanation construction, due to the combinatorial explosion of alternatives to consider. <p> A well-known problem for these methods is the cost of explanation construction, due to the combinatorial explosion of alternatives to consider. Methods have been proposed to control chaining cost (e.g., <ref> [ Charniak, 1986; Hobbs et al., 1990 ] </ref> ), but despite the benefits of these methods, the difficulty of efficiently generating explanations remains acute in rich domains. Case-based explanation addresses this problem by adapting prior explanations to new situations, to avoid the cost of chaining from scratch.
Reference: [ Kahneman et al., 1982 ] <author> D. Kahneman, P. Slovic, and A. Tversky. </author> <title> Judgement under uncertainty: Heuristics and biases. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1982. </year>
Reference-contexts: Similarity-based methods are not guaranteed to parallel correct probabilities and can sometimes lead to errors, but they are heuristics that people appear to use to estimate likelihoods when probabilities are unavailable <ref> [ Kahneman et al., 1982 ] </ref> .
Reference: [ Kass and Leake, 1988 ] <author> A. Kass and D. Leake. </author> <title> Case-based reasoning applied to constructing explanations. </title> <editor> In J. Kolodner, editor, </editor> <booktitle> Proceedings of the Case-Based Reasoning Workshop, </booktitle> <pages> pages 190-208, </pages> <address> Palo Alto, </address> <year> 1988. </year> <title> DARPA, </title> <publisher> Morgan Kaufmann, Inc. </publisher>
Reference: [ Kass, 1990 ] <author> A. Kass. </author> <title> Developing Creative Hypotheses by Adapting Explanations. </title> <type> PhD thesis, </type> <institution> Yale University, 1990. Northwestern University Institute for the Learning Sciences, </institution> <type> Technical Report 6. </type>
Reference-contexts: Specific aspects of the model were further refined in SWALE's descendents ABE <ref> [ Kass, 1990 ] </ref> and ACCEPTER [ Leake, 1992 ] . These systems use case-based reasoning to focus explanation of real-world events. Rather than assuming perfect knowledge, their explanation effort applies limited background knowledge to the incomplete information provided in simple news stories. <p> Generate problem characterizations for any problems. * Explanation adaptation: If problems were found, use the evaluator's problem characterization to select adaptation strategies <ref> [ Kass, 1990 ] </ref> for modifying the explanation to repair the problems. Apply the strategies and return to the explanation evalu ation phase to evaluate the new explanation. <p> That substantiation is beyond the scope of this paper but has been provided elsewhere: problem characterization and retrieval issues are addressed in [ Leake, 1991b; Leake, 1992 ] , explanation evaluation issues are addressed in [ Leake, 1991a; Leake, 1992 ] , and adaptation issues are addressed in <ref> [ Kass, 1990 ] </ref> . 3 Perspective on the case-based model To delineate the ramifications of the case-based model and its relationship to other methods for abductive understanding, our central focus is to examine and put into perspective the tenets of the case-based model concerning five key issues: (1) how to <p> When those explanations are applied to new situations it adapts them to fit the new situation, using adaptation strategies that include not only generalization but also deletion, addition and replacement of components of the explanation <ref> [ Kass, 1990 ] </ref> . Consequently, case-based explanation can apply prior explanations to a wider range of circumstances that explanation-based generalization. <p> This process depends on having effective strategies for guiding adaptation, and a library of such strategies is proposed in <ref> [ Kass, 1990 ] </ref> . We note that during the adaptation process, case-based explanation uses incremental evaluation of hy potheses to decide how to proceed further.
Reference: [ Kautz and Allen, 1986 ] <author> H. Kautz and J. Allen. </author> <title> Generalized plan recognition. </title> <booktitle> In Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <pages> pages 32-37, </pages> <address> Philadel-phia, PA, </address> <month> August </month> <year> 1986. </year> <note> AAAI. </note>
Reference-contexts: nature of explanations, (3) the plausibility evaluation process, (4) the role of anomalies in focusing an un-derstander's explanation, and (5) the influence of over-arching goals on explanation. 3.1 Building explanations Explanation construction methods: In abduc-tive understanding systems, standard theorem-proving chaining techniques are generally the mechanism for generating candidate explanations (e.g., <ref> [ Hobbs et al., 1990; Kautz and Allen, 1986 ] </ref> ). A well-known problem for these methods is the cost of explanation construction, due to the combinatorial explosion of alternatives to consider. <p> In abductive reasoning systems, the dominant method for judging plausibility of explanations is to favor explanations that are in some sense structurally "minimal" (e.g. <ref> [ Charniak, 1986; Kautz and Allen, 1986 ] </ref> ). These methods stress factors such as the number of abductive hypotheses rather than their content. Instead, the case-based model relies primarily on the content of the assumptions and explanatory chain.
Reference: [ Keller, 1988 ] <author> R. Keller. </author> <title> Defining operationality for explanation-based learning. </title> <journal> Artificial Intelligence, </journal> <volume> 35(2) </volume> <pages> 227-241, </pages> <year> 1988. </year>
Reference-contexts: (EBL) has addressed the question of what constitutes a useful explanation. 3 However, although EBL has been applied to a wide range of tasks (such as object recognition, problem solving, and search control), within these tasks explanations have been used for a single purpose: forming rules for concept recognition. (See <ref> [ Keller, 1988 ] </ref> for a discussion of how diverse EBL systems can be placed within the concept recognition framework.) Consequently, all these systems reflect the concept recognition task by making two basic assumptions about the form of explanations.
Reference: [ Leake, 1991a ] <author> D. Leake. </author> <title> Goal-based explanation evaluation. </title> <journal> Cognitive Science, </journal> <volume> 15(4) </volume> <pages> 509-545, </pages> <year> 1991. </year>
Reference-contexts: and by another crucial problem: that the goodness of explanations depends not only on their validity but also on whether they provide the information fl I would like to thank Ashwin Ram for helpful discussions of these issues and the IJCAI reviewers for their useful comments. that the explainer needs <ref> [ Leake, 1991a; Leake, 1992; Ram and Leake, 1991 ] </ref> . The need for explanations to reflect explainer goals both complicates explanation selection and potentially increases the number of candidate hypotheses that must be generated. <p> This paper examines the significance of the case-based explanation process (e.g., [ Kass and Leake, 1988; Leake, 1992; Schank, 1986; Schank and Leake, 1989 ] ) and its theory of explanation evaluation (e.g., <ref> [ Leake, 1991a; Leake, 1992 ] </ref> ) as methods for focusing construction and selection of abductive hypotheses. <p> That substantiation is beyond the scope of this paper but has been provided elsewhere: problem characterization and retrieval issues are addressed in [ Leake, 1991b; Leake, 1992 ] , explanation evaluation issues are addressed in <ref> [ Leake, 1991a; Leake, 1992 ] </ref> , and adaptation issues are addressed in [ Kass, 1990 ] . 3 Perspective on the case-based model To delineate the ramifications of the case-based model and its relationship to other methods for abductive understanding, our central focus is to examine and put into perspective <p> For example, subjects attempting to absolve themselves of blame will favor different explanations from those without that goal [ Snyder et al., 1983 ] . In general, in any multi-task system the only way to assure useful explanations is to explicitly evaluate their goodness according to current system goals <ref> [ Leake, 1991a; Leake, 1992 ] </ref> . Although goal-based explanation selection is important regardless of the means used for explanation construction, the importance of goals beyond facilitating routine understanding has received little attention in research on other models of abductive understanding. <p> For example, even if an explanation shows that a disease can be predicted with absolute certainty, based on a set of environmental factors, that explanation will be worthless for developing a vaccine for the disease unless the explanation shows how those environmental factors cause the disease. In <ref> [ Leake, 1991a; Leake, 1992 ] </ref> we discuss ten disparate sets of requirements for good explanations that arise from different uses for explanations. A subset of these requirements has been implemented in ACCEPTER's explanation evaluation process.
Reference: [ Leake, 1991b ] <author> D. Leake. </author> <title> An indexing vocabulary for case-based explanation. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 10-15, </pages> <address> Ana-heim, CA, </address> <month> July </month> <year> 1991. </year> <note> AAAI. </note>
Reference-contexts: In order to establish this algorithm as a viable alternative to standard abductive methods, the effectiveness of each phase of the process must be substantiated. That substantiation is beyond the scope of this paper but has been provided elsewhere: problem characterization and retrieval issues are addressed in <ref> [ Leake, 1991b; Leake, 1992 ] </ref> , explanation evaluation issues are addressed in [ Leake, 1991a; Leake, 1992 ] , and adaptation issues are addressed in [ Kass, 1990 ] . 3 Perspective on the case-based model To delineate the ramifications of the case-based model and its relationship to other methods <p> In the case-based model, explanation retrieval is focused according to the explainer's information needs to resolve the anomalies that prompted explanation. ACCEPTER directs explanation search by retrieving explanations indexed as relevant to the anomalies prompting explanation <ref> [ Leake, 1991b; Leake, 1992 ] </ref> , and includes a processing phase that explicitly evaluates the relevance of candidate explanations to the anomaly being explained [ Leake, 1992 ] . <p> In <ref> [ Leake, 1991b; Leake, 1992 ] </ref> we describe an indexing vocabulary for organizing a memory of explanations to facilitate this focused explanation retrieval. 3.5 The role of overarching goals The previous section shows that explanation must reflect system information needs in order to generate explanations that are useful for resolving anomalies.
Reference: [ Leake, 1992 ] <author> D. Leake. </author> <title> Evaluating Explanations: A Content Theory. </title> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, NJ, </address> <year> 1992. </year>
Reference-contexts: and by another crucial problem: that the goodness of explanations depends not only on their validity but also on whether they provide the information fl I would like to thank Ashwin Ram for helpful discussions of these issues and the IJCAI reviewers for their useful comments. that the explainer needs <ref> [ Leake, 1991a; Leake, 1992; Ram and Leake, 1991 ] </ref> . The need for explanations to reflect explainer goals both complicates explanation selection and potentially increases the number of candidate hypotheses that must be generated. <p> This paper examines the significance of the case-based explanation process (e.g., [ Kass and Leake, 1988; Leake, 1992; Schank, 1986; Schank and Leake, 1989 ] ) and its theory of explanation evaluation (e.g., <ref> [ Leake, 1991a; Leake, 1992 ] </ref> ) as methods for focusing construction and selection of abductive hypotheses. <p> Specific aspects of the model were further refined in SWALE's descendents ABE [ Kass, 1990 ] and ACCEPTER <ref> [ Leake, 1992 ] </ref> . These systems use case-based reasoning to focus explanation of real-world events. Rather than assuming perfect knowledge, their explanation effort applies limited background knowledge to the incomplete information provided in simple news stories. <p> In order to establish this algorithm as a viable alternative to standard abductive methods, the effectiveness of each phase of the process must be substantiated. That substantiation is beyond the scope of this paper but has been provided elsewhere: problem characterization and retrieval issues are addressed in <ref> [ Leake, 1991b; Leake, 1992 ] </ref> , explanation evaluation issues are addressed in [ Leake, 1991a; Leake, 1992 ] , and adaptation issues are addressed in [ Kass, 1990 ] . 3 Perspective on the case-based model To delineate the ramifications of the case-based model and its relationship to other methods <p> That substantiation is beyond the scope of this paper but has been provided elsewhere: problem characterization and retrieval issues are addressed in [ Leake, 1991b; Leake, 1992 ] , explanation evaluation issues are addressed in <ref> [ Leake, 1991a; Leake, 1992 ] </ref> , and adaptation issues are addressed in [ Kass, 1990 ] . 3 Perspective on the case-based model To delineate the ramifications of the case-based model and its relationship to other methods for abductive understanding, our central focus is to examine and put into perspective <p> plausible inference networks [ Pearl, 1988 ] .) Consequently, decisions about the plausibility of an XP in a given situation depend both on its abductive assump tions (as in standard models) and on how well the in-ternal derivation of the belief-support chain of the XP applies in the current situation <ref> [ Leake, 1992 ] </ref> . 3.3 Plausibility evaluation In case-based explanation, the first criterion for selecting likely explanations is experience in similar situations: Explanations of new situations are considered most plausible if they have applied in similar prior situations. <p> A full description of our model's plausibility evaluation can be found in <ref> [ Leake, 1992 ] </ref> . 3.4 The role of anomalies in focusing explanation Standard abductive understanding systems take a neutral view of the events they explain: given an event to explain, they either accept any chain accounting for the event or always seek explanations focusing on a fixed aspect of the <p> In the case-based model, explanation retrieval is focused according to the explainer's information needs to resolve the anomalies that prompted explanation. ACCEPTER directs explanation search by retrieving explanations indexed as relevant to the anomalies prompting explanation <ref> [ Leake, 1991b; Leake, 1992 ] </ref> , and includes a processing phase that explicitly evaluates the relevance of candidate explanations to the anomaly being explained [ Leake, 1992 ] . <p> ACCEPTER directs explanation search by retrieving explanations indexed as relevant to the anomalies prompting explanation [ Leake, 1991b; Leake, 1992 ] , and includes a processing phase that explicitly evaluates the relevance of candidate explanations to the anomaly being explained <ref> [ Leake, 1992 ] </ref> . Although standard abductive understanding systems do not attempt to select the aspects of an event on which to focus, at first glance it appears that their basic backwards chaining mechanism would be sufficient to allow explanation to be focused on resolving anomalies. <p> In <ref> [ Leake, 1991b; Leake, 1992 ] </ref> we describe an indexing vocabulary for organizing a memory of explanations to facilitate this focused explanation retrieval. 3.5 The role of overarching goals The previous section shows that explanation must reflect system information needs in order to generate explanations that are useful for resolving anomalies. <p> For example, subjects attempting to absolve themselves of blame will favor different explanations from those without that goal [ Snyder et al., 1983 ] . In general, in any multi-task system the only way to assure useful explanations is to explicitly evaluate their goodness according to current system goals <ref> [ Leake, 1991a; Leake, 1992 ] </ref> . Although goal-based explanation selection is important regardless of the means used for explanation construction, the importance of goals beyond facilitating routine understanding has received little attention in research on other models of abductive understanding. <p> For example, even if an explanation shows that a disease can be predicted with absolute certainty, based on a set of environmental factors, that explanation will be worthless for developing a vaccine for the disease unless the explanation shows how those environmental factors cause the disease. In <ref> [ Leake, 1991a; Leake, 1992 ] </ref> we discuss ten disparate sets of requirements for good explanations that arise from different uses for explanations. A subset of these requirements has been implemented in ACCEPTER's explanation evaluation process.
Reference: [ Mooney, 1990 ] <author> R. Mooney. </author> <title> A General Explanation-based Learning Mechanism and its Application to Narrative Understanding. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Ma-teo, </address> <year> 1990. </year>
Reference-contexts: Case-based explanation addresses this problem by adapting prior explanations to new situations, to avoid the cost of chaining from scratch. The case-based approach to re-using explanations contrasts with another method that learns to facilitate explanation, explanation-based schema acquisition (e.g., <ref> [ Mooney, 1990 ] </ref> ). When confronted with novel situations, explanation-based schema acquisition builds 1 SWALE, ACCEPTER and ABE all start with libraries of stored explanations. <p> systems take a neutral view of the events they explain: given an event to explain, they either accept any chain accounting for the event or always seek explanations focusing on a fixed aspect of the event (e.g., always trying to explain in terms of the goals that an action satisfies <ref> [ Mooney, 1990 ] </ref> ). In such systems, decisions of which explanation to select are based entirely on plausibility of the candidates (e.g., [ Charniak, 1986; Charniak and Goldman, 1991; Hobbs et al., 1990; Kautz and Allen, 1986; Ng and Mooney, 1990 ] ).
Reference: [ Ng and Mooney, 1990 ] <author> H. Ng and R. Mooney. </author> <title> On the role of coherence in abductive explanation. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 337-342, </pages> <address> Boston, MA, </address> <month> July </month> <year> 1990. </year> <note> AAAI. </note>
Reference-contexts: This approach differs from models of abduction in which all explanation construction is assumed to precede any evaluation, and is in the same spirit as recent research on choosing which explanations to pursue according to validity estimates (e.g., <ref> [ deKleer and Williams, 1989; Ng and Mooney, 1990 ] </ref> .
Reference: [ Pearl, 1988 ] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kauf-mann Publishers, Inc., </publisher> <address> San Mateo, </address> <year> 1988. </year>
Reference-contexts: chain of reasoning that accounts for why antecedents of the XP provide support for belief in the consequent of the XP, but the reasoning chain is not considered to prove that the consequent must necessarily hold. (In this respect XPs are in the same spirit as Pearl's plausible inference networks <ref> [ Pearl, 1988 ] </ref> .) Consequently, decisions about the plausibility of an XP in a given situation depend both on its abductive assump tions (as in standard models) and on how well the in-ternal derivation of the belief-support chain of the XP applies in the current situation [ Leake, 1992 ]
Reference: [ Rajamoney, 1988 ] <author> S. Rajamoney. </author> <title> Experimentation-based theory revision. </title> <booktitle> In Proceedings of the 1988 AAAI Spring Symposium on Explanation-based Learning. AAAI, </booktitle> <year> 1988. </year>
Reference-contexts: Previous research on overcoming imperfect theory problems focuses on methods for repairing an imperfect domain theory when problems arise (e.g., <ref> [ Dietterich and Flann, 1988; Rajamoney, 1988 ] </ref> ); our focus is how to generate reasonable explanations despite possible imperfections in the domain theory. 3.2 The nature of explanations The need of everyday explainers to use imperfect information is reflected in how case-based explanation views the nature of explanations.
Reference: [ Ram and Leake, 1991 ] <author> A. Ram and D. Leake. </author> <title> Evaluation of explanatory hypotheses. </title> <booktitle> In Proceedings of the Thirteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 867-871, </pages> <address> Chicago, IL, August 1991. </address> <publisher> Lawrence Erlbaum. </publisher>
Reference-contexts: and by another crucial problem: that the goodness of explanations depends not only on their validity but also on whether they provide the information fl I would like to thank Ashwin Ram for helpful discussions of these issues and the IJCAI reviewers for their useful comments. that the explainer needs <ref> [ Leake, 1991a; Leake, 1992; Ram and Leake, 1991 ] </ref> . The need for explanations to reflect explainer goals both complicates explanation selection and potentially increases the number of candidate hypotheses that must be generated.
Reference: [ Read and Cesa, 1991 ] <author> S. Read and I. Cesa. </author> <title> This reminds me of the time when : : : : Expectation failures in reminding and explanation. </title> <journal> Journal of Experimental Social Psychology, </journal> <volume> 27 </volume> <pages> 1-25, </pages> <year> 1991. </year>
Reference-contexts: Case-based explanation builds new explanations by retrieving stored explanations for previous episodes and adapting them to fit current circumstances and needs. The case-based process models some aspects of human explanation <ref> [ Read and Cesa, 1991 ] </ref> , and, as we describe in the following sections, facilitates generation of plausible and useful explanations despite the problems of incomplete information and imperfect domain theories that mark everyday explanation. <p> Later psychological experiments support the psychological validity of the reminding-based explanation process and the tendency of people to favor explanations that are based on prior explanations of similar episodes <ref> [ Read and Cesa, 1991 ] </ref> .
Reference: [ Rymon et al., 1991 ] <author> R. Rymon, B. Webber, and J. Clarke. </author> <title> Towards goal-directed diagnosis. </title> <booktitle> In Working Notes of the Second International Workshop on Principles of Diagnosis, </booktitle> <address> Milano, Italy, </address> <year> 1991. </year>
Reference-contexts: However, the influence of overarching goals has begun to be investigated in research on abductive diagnosis (e.g., for controlling large-scale diagnosis [ Freitag and Friedrich, 1991 ] and for integrating diagnosis and response <ref> [ Rymon et al., 1991 ] </ref> ).
Reference: [ Schank and Leake, 1989 ] <author> R.C. Schank and D. Leake. </author> <title> Creativity and learning in a case-based explainer. </title> <journal> Artificial Intelligence, </journal> <volume> 40(1-3):353-385, </volume> <year> 1989. </year>
Reference: [ Schank, 1986 ] <author> R.C. Schank. </author> <title> Explanation Patterns: Understanding Mechanically and Creatively. </title> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, NJ, </address> <year> 1986. </year>
Reference-contexts: The case-based approach, however, explicitly treats explanations as plausible reasoning chains that may be imperfect. In the case-based model, explanations are represented as explanation patterns (XPs) <ref> [ Schank, 1986 ] </ref> .
Reference: [ Snyder et al., 1983 ] <author> C. Snyder, R. Higgens, and R. Stucky. Excuses: </author> <title> Masquerades in Search of Grace. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: However, the second explanation is a better explanation for protecting the ATM in the future. Psychological evidence shows that people favor different explanations for an event if they have different goals. For example, subjects attempting to absolve themselves of blame will favor different explanations from those without that goal <ref> [ Snyder et al., 1983 ] </ref> . In general, in any multi-task system the only way to assure useful explanations is to explicitly evaluate their goodness according to current system goals [ Leake, 1991a; Leake, 1992 ] .
References-found: 25

