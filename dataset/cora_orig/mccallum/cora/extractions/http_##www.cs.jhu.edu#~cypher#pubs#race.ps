URL: http://www.cs.jhu.edu/~cypher/pubs/race.ps
Refering-URL: http://www.cs.jhu.edu/~cypher/pubs/pubs.html
Root-URL: http://www.cs.jhu.edu
Email: cypher@cs.jhu.edu leu@almaden.ibm.com  
Title: Efficient Race Detection for Message-Passing Programs with Nonblocking Sends and Receives  
Author: Robert Cypher Eric Leu 
Address: 650 Harry Road Baltimore, MD 21218 San Jose, CA 95120  
Affiliation: Department of Computer Science IBM Almaden Research Center Johns Hopkins University  
Abstract: This paper presents an algorithm for performing on-the-fly race detection for parallel message-passing programs. The algorithm reads a trace of the communication events in a message-passing parallel program and either finds a specific race condition or reports that the traced program is race-free. It supports a rich message-passing model, including blocking and non-blocking sends and receives, synchronous and asynchronous sends, receive selectivity by source and/or tag value, and arbitrary amounts of system buffering of messages. It runs in polynomial time and is very efficient for most types of executions. A key feature of the race detection algorithm is its use of several new types of logical clocks for determining ordering relations. It is likely that these logical clocks will also be useful in other settings. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Callahan, K. Kennedy and J. Subhlok, </author> <title> "Analysis of Event Synchronization in a Parallel Programming Tool", </title> <booktitle> in Proc. ACM Symp. on Principles and Practice of Parallel Programming, </booktitle> <pages> pp. 21-30, </pages> <year> 1990. </year>
Reference-contexts: However, the work by Netzer [24] is for shared memory programs, while the work by Netzer and Miller [26] applies only to message-passing programs with blocking sends and receives. Finally, many researchers have studied race conditions in parallel programs that use shared memory <ref> [1, 2, 9, 11, 14, 17, 22, 24, 25] </ref>. In this paper we describe an algorithm for detecting race conditions in parallel programs. It is a sequential algorithm that reads a trace of the communication events in a message-passing parallel program and determines if the execution contains a race condition.
Reference: [2] <author> J-D. Choi and S. Min, </author> <title> "Race Frontier: Reproducing Data Races in Parallel-Program Debugging", </title> <booktitle> in Proc. ACM Symp. on Principles and Practice of Parallel Programming, </booktitle> <pages> pp. 145-154, </pages> <year> 1991. </year>
Reference-contexts: However, the work by Netzer [24] is for shared memory programs, while the work by Netzer and Miller [26] applies only to message-passing programs with blocking sends and receives. Finally, many researchers have studied race conditions in parallel programs that use shared memory <ref> [1, 2, 9, 11, 14, 17, 22, 24, 25] </ref>. In this paper we describe an algorithm for detecting race conditions in parallel programs. It is a sequential algorithm that reads a trace of the communication events in a message-passing parallel program and determines if the execution contains a race condition.
Reference: [3] <author> T. Cormen, C. Leiserson and R. Rivest, </author> <title> "Introduction to Algorithms", </title> <publisher> The MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1990. </year>
Reference: [4] <author> R. Curtis and L. Wittie, "BugNet: </author> <title> A Debugging System for Parallel Programming Environments", </title> <booktitle> in Proc. 3rd Intl. Conf. on Distributed Computing Systems, </booktitle> <pages> pp. 394-399, </pages> <month> Oct. </month> <year> 1982. </year>
Reference-contexts: Other researchers have studied race conditions in parallel programs. Damodaran-Kamal and Francioni created a tool for detecting race conditions in message-passing programs that use PVM [8]. However, they assume that only blocking sends and receives are used. Researchers have also considered race conditions in the context of execution replay <ref> [4, 15, 19, 20, 24, 26] </ref>, where the goal is to record enough information to reproduce an execution. In general, execution replay aids debugging by allowing bugs to be reproduced, but it does not help detect races in programs which should be race-free.
Reference: [5] <author> R. Cypher, </author> <title> "Message-Passing Models for Blocking and Nonblocking Communication", </title> <booktitle> DIMACS Workshop on Models, Architectures, and Technologies for Parallel Computation, </booktitle> <pages> pp. </pages> <month> 274-279, </month> <type> Tech. Rep. 93-87, </type> <institution> DIMACS, </institution> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: In addition to the standard blocking sends and receives, many message-passing environments now provide nonblocking send and/or receive commands [6, 10, 13, 16, 23, 27]. The nonblocking commands support the overlapping of communication and computation, and they can be used to avoid deadlock <ref> [5] </ref>. However, because nonblocking commands allow a single process to have many outstanding sends and receives at a single time, their use greatly increases the potential for race conditions. As a result, the detec tion of race conditions in parallel programs that use nonblocking sends and receives is particularly important.
Reference: [6] <author> R. Cypher and E. Leu, </author> <title> "The Semantics of Blocking and Nonblocking Send and Receive Primitives", </title> <booktitle> in Proc. 8th IEEE Intl. Parallel Processing Symp., </booktitle> <pages> pp. 729-735, </pages> <year> 1994. </year>
Reference-contexts: Second, race conditions complicate debugging because their nondeterministic nature can prevent incorrect program behavior from being repeated. In addition to the standard blocking sends and receives, many message-passing environments now provide nonblocking send and/or receive commands <ref> [6, 10, 13, 16, 23, 27] </ref>. The nonblocking commands support the overlapping of communication and computation, and they can be used to avoid deadlock [5]. <p> In this paper we will assume weakly-ordered communication, which was defined by Cypher and Leu <ref> [6] </ref> and has been adopted in the Message-Passing Interface (MPI) standard [23]. Informally, given any receive command, weakly-ordered communication requires that only the first send that is still unmatched and is compatible with the given receive can be matched to that receive. <p> Similarly, given any send command, weakly-ordered communication requires that only the first receive that is still unmatched and is compatible with the given send can be matched to that send. 2.2 Formal Model All of the results in this paper are based on a formal model of message-passing semantics <ref> [6] </ref>. We will now give a brief overview of that model. We consider the execution of a message-passing program that consists of p static processes.
Reference: [7] <author> R. Cypher and E. Leu, </author> <title> "Repeatable and Portable Message-Passing Programs", </title> <booktitle> in Proc. ACM Symp. on Principles of Distributed Computing, </booktitle> <pages> pp. 22-31, </pages> <year> 1994. </year>
Reference-contexts: This notion is defined formally <ref> [7] </ref> by considering every possible PS s and PR r which were not matched to one another, and considering every possible intermediate state of the program execution. If at any possible intermediate state s can be matched to r without violating the model's semantics, there is a race condition. <p> Unfortunately, there could be an exponential number of possible intermediate states, so the formal definition does not immediately provide a polynomial time race detection algorithm. Fortunately, a characterization of race conditions has been created that does lead to a polynomial time algorithm <ref> [7] </ref>. This characterization will be given in Theorem 2.2 below. 2.5 Known Results The following theorem captures a property of weakly-ordered communication [7]. <p> Fortunately, a characterization of race conditions has been created that does lead to a polynomial time algorithm <ref> [7] </ref>. This characterization will be given in Theorem 2.2 below. 2.5 Known Results The following theorem captures a property of weakly-ordered communication [7]. Theorem 2.1 Given any matches (s 1 ; r 1 ) and (s 2 ; r 2 ), if s 1 precedes s 2 and r 2 precedes r 1 , then s 1 and r 2 are not compatible. <p> Theorem 2.2 gives a precise characterization of those programs that have race conditions <ref> [7] </ref>. Theorem 2.2 A program execution hE; M i has a race condition iff there exists a match (s 1 ; r 1 ) and a PS s 2 which conflict. <p> We have shown that any execution (with locally deterministic events) is portable and repeatable if and only if it is free from race conditions and it does not have any system buffer dependencies <ref> [7] </ref>. Thus we will use the race detection algorithm presented here, in conjunction with a test for system buffer dependencies, to determine the portability and repeatability of the execution.
Reference: [8] <author> S. Damodaran-Kamal and J. Francioni, "Nonde-terminacy: </author> <title> Testing and Debugging in Message Passing Parallel Programs", </title> <booktitle> in Proc. ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pp. 118-128, </pages> <year> 1993. </year>
Reference-contexts: As a result, the detec tion of race conditions in parallel programs that use nonblocking sends and receives is particularly important. Other researchers have studied race conditions in parallel programs. Damodaran-Kamal and Francioni created a tool for detecting race conditions in message-passing programs that use PVM <ref> [8] </ref>. However, they assume that only blocking sends and receives are used. Researchers have also considered race conditions in the context of execution replay [4, 15, 19, 20, 24, 26], where the goal is to record enough information to reproduce an execution.
Reference: [9] <author> A. Dinning and E. Schonberg, </author> <title> "An Empirical Comparison of Monitoring Algorithms for Access Anomaly Detection", </title> <booktitle> in Proc. ACM Symp. on Principles and Practice of Parallel Programming, </booktitle> <pages> pp. 1-10, </pages> <year> 1990. </year>
Reference-contexts: However, the work by Netzer [24] is for shared memory programs, while the work by Netzer and Miller [26] applies only to message-passing programs with blocking sends and receives. Finally, many researchers have studied race conditions in parallel programs that use shared memory <ref> [1, 2, 9, 11, 14, 17, 22, 24, 25] </ref>. In this paper we describe an algorithm for detecting race conditions in parallel programs. It is a sequential algorithm that reads a trace of the communication events in a message-passing parallel program and determines if the execution contains a race condition.
Reference: [10] <author> J. Dongarra, R. Hempel, A. Hey and D. Walker, </author> <title> "A Proposal for a User-Level, Message-Passing Interface in a Distributed Memory Environment", </title> <type> ORNL Tech. Rep., </type> <institution> ORNL/TM-12231, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Second, race conditions complicate debugging because their nondeterministic nature can prevent incorrect program behavior from being repeated. In addition to the standard blocking sends and receives, many message-passing environments now provide nonblocking send and/or receive commands <ref> [6, 10, 13, 16, 23, 27] </ref>. The nonblocking commands support the overlapping of communication and computation, and they can be used to avoid deadlock [5].
Reference: [11] <author> P. Emrath, S. Ghosh and D. Padua, </author> <title> "Event Synchronization Analysis for Debugging Parallel Programs", </title> <booktitle> in Proc. Supercomputing '89, </booktitle> <pages> pp. 580-588, </pages> <month> Nov. </month> <year> 1989. </year>
Reference-contexts: However, the work by Netzer [24] is for shared memory programs, while the work by Netzer and Miller [26] applies only to message-passing programs with blocking sends and receives. Finally, many researchers have studied race conditions in parallel programs that use shared memory <ref> [1, 2, 9, 11, 14, 17, 22, 24, 25] </ref>. In this paper we describe an algorithm for detecting race conditions in parallel programs. It is a sequential algorithm that reads a trace of the communication events in a message-passing parallel program and determines if the execution contains a race condition.
Reference: [12] <author> C. Fidge, </author> <title> "Logical Time in Distributed Computing Systems", </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 28-33, </pages> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: An important feature of our race detection algorithm is its use of several new types of logical clocks for determining ordering relations. These logical clocks, which are fundamentally different from those previously proposed <ref> [12, 14, 18, 21, 26] </ref>, are the first to capture the ordering information present when nonblocking sends and receives are used. As a result, it is likely that they will also be useful in other settings. The remainder of this paper is organized as follows. <p> This bottleneck can be overcome with the use of logical clocks, as will be described in the next section. 3 Logical Clocks A logical clock is a value that is associated with each element of a partially ordered set in order to speed the comparison of items in the set <ref> [12, 14, 18, 21, 26] </ref>. <p> how logical clocks can be created for events and matches in the context of on the-fly race detection. 3.1 Type A Clocks The first type of logical clock we will consider is a natural extension of the vector clocks that were created for message-passing systems with blocking sends and receives <ref> [12, 21] </ref>. These clocks, which we call Type A clocks, are defined below. Given any vectors u and v, let sup (u; v) denote the component-wise maximum of u and v. <p> As a result of Theorem 3.3, Type A and C clocks can be used for race detection. Note that Type C clocks are fundamentally different from all previously defined logical clocks <ref> [12, 14, 18, 21, 26] </ref>, as they must be used in conjunction with another type of clock in order to test the "happened-before" relation. 3.4 Type D Clocks Type C clocks still have the disadvantage that C (s;r) can depend on events and matches that happen much later than (s; r).
Reference: [13] <author> D. Frye, R. Bryant, H. Ho, R. Lawrence and M. Snir, </author> <title> "An External User Interface for Scalable Parallel Systems", </title> <type> Tech. Rep., </type> <institution> IBM Highly Parallel Supercomputing Systems Lab., </institution> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: Second, race conditions complicate debugging because their nondeterministic nature can prevent incorrect program behavior from being repeated. In addition to the standard blocking sends and receives, many message-passing environments now provide nonblocking send and/or receive commands <ref> [6, 10, 13, 16, 23, 27] </ref>. The nonblocking commands support the overlapping of communication and computation, and they can be used to avoid deadlock [5].
Reference: [14] <author> D. Helmbold, C. McDowell and J-Z. Wang, </author> <title> "Determining Possible Event Orders by Analyzing Sequential Traces", </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 4(7), </volume> <pages> pp. 827-840, </pages> <year> 1993. </year>
Reference-contexts: However, the work by Netzer [24] is for shared memory programs, while the work by Netzer and Miller [26] applies only to message-passing programs with blocking sends and receives. Finally, many researchers have studied race conditions in parallel programs that use shared memory <ref> [1, 2, 9, 11, 14, 17, 22, 24, 25] </ref>. In this paper we describe an algorithm for detecting race conditions in parallel programs. It is a sequential algorithm that reads a trace of the communication events in a message-passing parallel program and determines if the execution contains a race condition. <p> An important feature of our race detection algorithm is its use of several new types of logical clocks for determining ordering relations. These logical clocks, which are fundamentally different from those previously proposed <ref> [12, 14, 18, 21, 26] </ref>, are the first to capture the ordering information present when nonblocking sends and receives are used. As a result, it is likely that they will also be useful in other settings. The remainder of this paper is organized as follows. <p> This bottleneck can be overcome with the use of logical clocks, as will be described in the next section. 3 Logical Clocks A logical clock is a value that is associated with each element of a partially ordered set in order to speed the comparison of items in the set <ref> [12, 14, 18, 21, 26] </ref>. <p> As a result of Theorem 3.3, Type A and C clocks can be used for race detection. Note that Type C clocks are fundamentally different from all previously defined logical clocks <ref> [12, 14, 18, 21, 26] </ref>, as they must be used in conjunction with another type of clock in order to test the "happened-before" relation. 3.4 Type D Clocks Type C clocks still have the disadvantage that C (s;r) can depend on events and matches that happen much later than (s; r).
Reference: [15] <author> M. Hurfin, N. Plouzeau and M. Raynal, "ERE-BUS: </author> <title> A Debugger for Asynchronous Distributed Systems", </title> <booktitle> In Proc. 3rd Intl. IEEE Workshop on Future Trends in Distributed Computing Systems, </booktitle> <pages> pp. 93-98, </pages> <month> Apr. </month> <year> 1992. </year>
Reference-contexts: Other researchers have studied race conditions in parallel programs. Damodaran-Kamal and Francioni created a tool for detecting race conditions in message-passing programs that use PVM [8]. However, they assume that only blocking sends and receives are used. Researchers have also considered race conditions in the context of execution replay <ref> [4, 15, 19, 20, 24, 26] </ref>, where the goal is to record enough information to reproduce an execution. In general, execution replay aids debugging by allowing bugs to be reproduced, but it does not help detect races in programs which should be race-free.
Reference: [16] <author> Intel Corp., </author> <title> "Intel iPSC/860 Programmer's Reference Manual". </title>
Reference-contexts: Second, race conditions complicate debugging because their nondeterministic nature can prevent incorrect program behavior from being repeated. In addition to the standard blocking sends and receives, many message-passing environments now provide nonblocking send and/or receive commands <ref> [6, 10, 13, 16, 23, 27] </ref>. The nonblocking commands support the overlapping of communication and computation, and they can be used to avoid deadlock [5].
Reference: [17] <author> Y-K. Jun and K. Koh, </author> <title> "On-the-fly Detection of Access Anomalies in Nested Parallel Loops", </title> <booktitle> in Proc. ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pp. 107-117, </pages> <year> 1993. </year>
Reference-contexts: However, the work by Netzer [24] is for shared memory programs, while the work by Netzer and Miller [26] applies only to message-passing programs with blocking sends and receives. Finally, many researchers have studied race conditions in parallel programs that use shared memory <ref> [1, 2, 9, 11, 14, 17, 22, 24, 25] </ref>. In this paper we describe an algorithm for detecting race conditions in parallel programs. It is a sequential algorithm that reads a trace of the communication events in a message-passing parallel program and determines if the execution contains a race condition.
Reference: [18] <author> L. Lamport, </author> <title> "Time, Clocks, and the Ordering of Events in a Distributed System", </title> <journal> CACM, </journal> <volume> 21(7), </volume> <pages> pp. 558-565, </pages> <year> 1978. </year>
Reference-contexts: An important feature of our race detection algorithm is its use of several new types of logical clocks for determining ordering relations. These logical clocks, which are fundamentally different from those previously proposed <ref> [12, 14, 18, 21, 26] </ref>, are the first to capture the ordering information present when nonblocking sends and receives are used. As a result, it is likely that they will also be useful in other settings. The remainder of this paper is organized as follows. <p> Rule 4 captures the message-ordering properties of weakly ordered communication. Finally, relation R + h is the "happened-before" relation for message-passing with weakly-ordered communication, and is analogous to Lamport's "happened-before" relation <ref> [18] </ref>. However, unlike Lamport's "happened-before" relation, R + h is a partial order of the events and the matches, rather than simply events. <p> This bottleneck can be overcome with the use of logical clocks, as will be described in the next section. 3 Logical Clocks A logical clock is a value that is associated with each element of a partially ordered set in order to speed the comparison of items in the set <ref> [12, 14, 18, 21, 26] </ref>. <p> As a result of Theorem 3.3, Type A and C clocks can be used for race detection. Note that Type C clocks are fundamentally different from all previously defined logical clocks <ref> [12, 14, 18, 21, 26] </ref>, as they must be used in conjunction with another type of clock in order to test the "happened-before" relation. 3.4 Type D Clocks Type C clocks still have the disadvantage that C (s;r) can depend on events and matches that happen much later than (s; r).
Reference: [19] <author> T. Leblanc and J. Mellor-Crummey, </author> <title> "Debugging Parallel Programs with Instant Replay", </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(4), </volume> <pages> pp. 471-482, </pages> <year> 1987. </year>
Reference-contexts: Other researchers have studied race conditions in parallel programs. Damodaran-Kamal and Francioni created a tool for detecting race conditions in message-passing programs that use PVM [8]. However, they assume that only blocking sends and receives are used. Researchers have also considered race conditions in the context of execution replay <ref> [4, 15, 19, 20, 24, 26] </ref>, where the goal is to record enough information to reproduce an execution. In general, execution replay aids debugging by allowing bugs to be reproduced, but it does not help detect races in programs which should be race-free.
Reference: [20] <author> E. Leu, A. Schiper and A. Zramdini, </author> <title> "Efficient Execution Replay Technique for Distributed Memory Architectures", </title> <booktitle> in Proc. 2nd Euro-pean Distributed Memory Computing Conference, </booktitle> <publisher> LNCS 487, Springer-Verlag, </publisher> <pages> pp. 315-324, </pages> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: Other researchers have studied race conditions in parallel programs. Damodaran-Kamal and Francioni created a tool for detecting race conditions in message-passing programs that use PVM [8]. However, they assume that only blocking sends and receives are used. Researchers have also considered race conditions in the context of execution replay <ref> [4, 15, 19, 20, 24, 26] </ref>, where the goal is to record enough information to reproduce an execution. In general, execution replay aids debugging by allowing bugs to be reproduced, but it does not help detect races in programs which should be race-free.
Reference: [21] <author> F. Mattern, </author> <title> "Virtual Time and Global States of Distributed Systems", in Parallel and Distributed Algorithms, </title> <editor> M. Cosnard and P. Quinton, eds., </editor> <publisher> North-Holland, Amsterdam, </publisher> <pages> pp. 215-226, </pages> <year> 1989. </year>
Reference-contexts: An important feature of our race detection algorithm is its use of several new types of logical clocks for determining ordering relations. These logical clocks, which are fundamentally different from those previously proposed <ref> [12, 14, 18, 21, 26] </ref>, are the first to capture the ordering information present when nonblocking sends and receives are used. As a result, it is likely that they will also be useful in other settings. The remainder of this paper is organized as follows. <p> This bottleneck can be overcome with the use of logical clocks, as will be described in the next section. 3 Logical Clocks A logical clock is a value that is associated with each element of a partially ordered set in order to speed the comparison of items in the set <ref> [12, 14, 18, 21, 26] </ref>. <p> how logical clocks can be created for events and matches in the context of on the-fly race detection. 3.1 Type A Clocks The first type of logical clock we will consider is a natural extension of the vector clocks that were created for message-passing systems with blocking sends and receives <ref> [12, 21] </ref>. These clocks, which we call Type A clocks, are defined below. Given any vectors u and v, let sup (u; v) denote the component-wise maximum of u and v. <p> As a result of Theorem 3.3, Type A and C clocks can be used for race detection. Note that Type C clocks are fundamentally different from all previously defined logical clocks <ref> [12, 14, 18, 21, 26] </ref>, as they must be used in conjunction with another type of clock in order to test the "happened-before" relation. 3.4 Type D Clocks Type C clocks still have the disadvantage that C (s;r) can depend on events and matches that happen much later than (s; r).
Reference: [22] <author> J. Mellor-Crummey, </author> <title> "Compile-Time Support for Efficient Data Race Detection in Shared-Memory Parallel Programs", </title> <booktitle> in Proc. ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pp. 129-139, </pages> <year> 1993. </year>
Reference-contexts: However, the work by Netzer [24] is for shared memory programs, while the work by Netzer and Miller [26] applies only to message-passing programs with blocking sends and receives. Finally, many researchers have studied race conditions in parallel programs that use shared memory <ref> [1, 2, 9, 11, 14, 17, 22, 24, 25] </ref>. In this paper we describe an algorithm for detecting race conditions in parallel programs. It is a sequential algorithm that reads a trace of the communication events in a message-passing parallel program and determines if the execution contains a race condition.
Reference: [23] <author> Message Passing Interface Forum, </author> <title> "Document for a Standard Message-Passing Interface (Draft)", </title> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: Second, race conditions complicate debugging because their nondeterministic nature can prevent incorrect program behavior from being repeated. In addition to the standard blocking sends and receives, many message-passing environments now provide nonblocking send and/or receive commands <ref> [6, 10, 13, 16, 23, 27] </ref>. The nonblocking commands support the overlapping of communication and computation, and they can be used to avoid deadlock [5]. <p> In this paper we will assume weakly-ordered communication, which was defined by Cypher and Leu [6] and has been adopted in the Message-Passing Interface (MPI) standard <ref> [23] </ref>. Informally, given any receive command, weakly-ordered communication requires that only the first send that is still unmatched and is compatible with the given receive can be matched to that receive.
Reference: [24] <author> R. Netzer, </author> <title> "Optimal Tracing and Replay for Debugging Shared-Memory Parallel Programs", </title> <booktitle> in Proc. ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pp. 1-11, </pages> <year> 1993. </year>
Reference-contexts: Other researchers have studied race conditions in parallel programs. Damodaran-Kamal and Francioni created a tool for detecting race conditions in message-passing programs that use PVM [8]. However, they assume that only blocking sends and receives are used. Researchers have also considered race conditions in the context of execution replay <ref> [4, 15, 19, 20, 24, 26] </ref>, where the goal is to record enough information to reproduce an execution. In general, execution replay aids debugging by allowing bugs to be reproduced, but it does not help detect races in programs which should be race-free. <p> In general, execution replay aids debugging by allowing bugs to be reproduced, but it does not help detect races in programs which should be race-free. An exception is the execution replay work by Netzer <ref> [24] </ref> and by Netzer and Miller [26], as they detect races in order to reduce the amount of information that is recorded. However, the work by Netzer [24] is for shared memory programs, while the work by Netzer and Miller [26] applies only to message-passing programs with blocking sends and receives. <p> An exception is the execution replay work by Netzer <ref> [24] </ref> and by Netzer and Miller [26], as they detect races in order to reduce the amount of information that is recorded. However, the work by Netzer [24] is for shared memory programs, while the work by Netzer and Miller [26] applies only to message-passing programs with blocking sends and receives. Finally, many researchers have studied race conditions in parallel programs that use shared memory [1, 2, 9, 11, 14, 17, 22, 24, 25]. <p> However, the work by Netzer [24] is for shared memory programs, while the work by Netzer and Miller [26] applies only to message-passing programs with blocking sends and receives. Finally, many researchers have studied race conditions in parallel programs that use shared memory <ref> [1, 2, 9, 11, 14, 17, 22, 24, 25] </ref>. In this paper we describe an algorithm for detecting race conditions in parallel programs. It is a sequential algorithm that reads a trace of the communication events in a message-passing parallel program and determines if the execution contains a race condition.
Reference: [25] <author> R. Netzer and B. Miller, </author> <title> "Improving the Accuracy of Data Race Detection", </title> <booktitle> in Proc. ACM Symp. on Principles and Practice of Parallel Programming, </booktitle> <pages> pp. 133-144, </pages> <year> 1991. </year>
Reference-contexts: However, the work by Netzer [24] is for shared memory programs, while the work by Netzer and Miller [26] applies only to message-passing programs with blocking sends and receives. Finally, many researchers have studied race conditions in parallel programs that use shared memory <ref> [1, 2, 9, 11, 14, 17, 22, 24, 25] </ref>. In this paper we describe an algorithm for detecting race conditions in parallel programs. It is a sequential algorithm that reads a trace of the communication events in a message-passing parallel program and determines if the execution contains a race condition.
Reference: [26] <author> R. Netzer and B. Miller, </author> <title> "Optimal Tracing and Replay for Debugging Message-Passing Parallel Programs", </title> <booktitle> in Proc. Supercomputing '92, </booktitle> <pages> pp. 502-511, </pages> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: Other researchers have studied race conditions in parallel programs. Damodaran-Kamal and Francioni created a tool for detecting race conditions in message-passing programs that use PVM [8]. However, they assume that only blocking sends and receives are used. Researchers have also considered race conditions in the context of execution replay <ref> [4, 15, 19, 20, 24, 26] </ref>, where the goal is to record enough information to reproduce an execution. In general, execution replay aids debugging by allowing bugs to be reproduced, but it does not help detect races in programs which should be race-free. <p> In general, execution replay aids debugging by allowing bugs to be reproduced, but it does not help detect races in programs which should be race-free. An exception is the execution replay work by Netzer [24] and by Netzer and Miller <ref> [26] </ref>, as they detect races in order to reduce the amount of information that is recorded. However, the work by Netzer [24] is for shared memory programs, while the work by Netzer and Miller [26] applies only to message-passing programs with blocking sends and receives. <p> An exception is the execution replay work by Netzer [24] and by Netzer and Miller <ref> [26] </ref>, as they detect races in order to reduce the amount of information that is recorded. However, the work by Netzer [24] is for shared memory programs, while the work by Netzer and Miller [26] applies only to message-passing programs with blocking sends and receives. Finally, many researchers have studied race conditions in parallel programs that use shared memory [1, 2, 9, 11, 14, 17, 22, 24, 25]. In this paper we describe an algorithm for detecting race conditions in parallel programs. <p> An important feature of our race detection algorithm is its use of several new types of logical clocks for determining ordering relations. These logical clocks, which are fundamentally different from those previously proposed <ref> [12, 14, 18, 21, 26] </ref>, are the first to capture the ordering information present when nonblocking sends and receives are used. As a result, it is likely that they will also be useful in other settings. The remainder of this paper is organized as follows. <p> This bottleneck can be overcome with the use of logical clocks, as will be described in the next section. 3 Logical Clocks A logical clock is a value that is associated with each element of a partially ordered set in order to speed the comparison of items in the set <ref> [12, 14, 18, 21, 26] </ref>. <p> As a result of Theorem 3.3, Type A and C clocks can be used for race detection. Note that Type C clocks are fundamentally different from all previously defined logical clocks <ref> [12, 14, 18, 21, 26] </ref>, as they must be used in conjunction with another type of clock in order to test the "happened-before" relation. 3.4 Type D Clocks Type C clocks still have the disadvantage that C (s;r) can depend on events and matches that happen much later than (s; r).
Reference: [27] <institution> Thinking Machines Corp., </institution> <note> "CMMD Reference Manual, version 3.0", </note> <month> May </month> <year> 1993. </year>
Reference-contexts: Second, race conditions complicate debugging because their nondeterministic nature can prevent incorrect program behavior from being repeated. In addition to the standard blocking sends and receives, many message-passing environments now provide nonblocking send and/or receive commands <ref> [6, 10, 13, 16, 23, 27] </ref>. The nonblocking commands support the overlapping of communication and computation, and they can be used to avoid deadlock [5].
References-found: 27

