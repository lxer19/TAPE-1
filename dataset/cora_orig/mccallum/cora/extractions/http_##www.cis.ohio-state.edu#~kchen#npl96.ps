URL: http://www.cis.ohio-state.edu/~kchen/npl96.ps
Refering-URL: http://www.cis.ohio-state.edu/~kchen/on-line.html
Root-URL: 
Title: Text-Dependent Speaker Identification Based on Input/Output HMMs: An Empirical Study  
Author: Ke Chen, Dahong Xie and Huisheng Chi 
Keyword: Speaker Identification, Input/Output HMM, EM algorithm, temporal processing  
Note: Neural Processing Letters, Vol. 3, No. 2, 1996, pp.  
Address: 100871, China  
Affiliation: National Lab of Machine Perception and Center for Information Science Peking University, Beijing  
Email: Email: chen@cis.pku.edu.cn  
Date: 81-89.  
Abstract: In this paper, we explore the Input/Output HMM (IOHMM) architecture for a substantial problem, that of text-dependent speaker identification. For subnetworks modeled with generalized linear models, we extend the IRLS algorithm to the M-step of the corresponding EM algorithm. Experimental results show that the improved EM algorithm yields significantly faster training than the original one. In comparison with the multilayer perceptron, the dynamic programming technique and hidden Markov models, we empirically demonstrate that the IOHMM architecture is a promising way to text-dependent speaker identification.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G.R. Doddington, </author> <title> Speaker recognitionidentifying people by their voices, </title> <booktitle> Proceedings of IEEE, </booktitle> <volume> Vol. 73, pp.1651-1664, </volume> <year> 1986. </year>
Reference-contexts: 1 Introduction Speaker identification task is to classify an unlabeled voice token as belonging to one of a set of N reference speakers <ref> [1] </ref>. It is a very hard problem since a speaker's voice changes in time. There have been extensive studies in this field based upon conventional techniques of speech signal processing [2]. Recently, the neural computing techniques have been investigated to improve the classification performance [3].
Reference: [2] <author> T. Matsui and S. Furui, </author> <title> Speaker recognition technology, </title> <journal> NTT Review, </journal> <volume> 7(2), </volume> <pages> pp. 40-48, </pages> <year> 1995. </year>
Reference-contexts: It is a very hard problem since a speaker's voice changes in time. There have been extensive studies in this field based upon conventional techniques of speech signal processing <ref> [2] </ref>. Recently, the neural computing techniques have been investigated to improve the classification performance [3]. It is well known that the temporal information or sequence effect plays a crucial role in speech processing.
Reference: [3] <author> Y. Bennani and P. Gallinari, </author> <title> Connectionist approaches for automatic speaker recognition, </title> <booktitle> Proc. ESCA Workshop on Auto matic Speaker Recognition, </booktitle> <pages> pp. 95-102, </pages> <year> 1994. </year>
Reference-contexts: It is a very hard problem since a speaker's voice changes in time. There have been extensive studies in this field based upon conventional techniques of speech signal processing [2]. Recently, the neural computing techniques have been investigated to improve the classification performance <ref> [3] </ref>. It is well known that the temporal information or sequence effect plays a crucial role in speech processing. In previous researches, a dynamic programming technique called Dynamic Time Warping (DTW) [4] was proposed to handle the sequence effects with a template matching method in speech processing. <p> For the purpose of comparison, we also investigate some classic techniques in text-dependent speaker identification, i.e. HMMs [8], the DTW [4] and the MLP <ref> [3] </ref>, on the same training and testing sets. For HMMs, we adopt the discrete HMM technique and the codebook consists of 256 codewords. We also investigated three kinds of HMMs, i.e. ergodic, left-right without jump and left-right with jumps [12].
Reference: [4] <author> H. Sakoe and S. Chiba, </author> <title> Dynamic programming algorithm optimization for speech word recognition, </title> <journal> IEEE Trans. Acoustics, Speech and Signal Processing, </journal> <volume> Vol. ASSP-26, No. 1, </volume> <pages> pp. 43-49, </pages> <year> 1978. </year>
Reference-contexts: Recently, the neural computing techniques have been investigated to improve the classification performance [3]. It is well known that the temporal information or sequence effect plays a crucial role in speech processing. In previous researches, a dynamic programming technique called Dynamic Time Warping (DTW) <ref> [4] </ref> was proposed to handle the sequence effects with a template matching method in speech processing. But the performance of the DTW is unsatisfactory in speaker identification since a speaker's voice greatly changes in different time and environments so that the testing data may be rather different from the templates. <p> For the purpose of comparison, we also investigate some classic techniques in text-dependent speaker identification, i.e. HMMs [8], the DTW <ref> [4] </ref> and the MLP [3], on the same training and testing sets. For HMMs, we adopt the discrete HMM technique and the codebook consists of 256 codewords. We also investigated three kinds of HMMs, i.e. ergodic, left-right without jump and left-right with jumps [12].
Reference: [5] <author> Y. Bengio, P. Simard and P. Frasconi, </author> <title> Learning long-term dependencies with gradient descent is difficult IEEE Trans. </title> <booktitle> Neural Networks, </booktitle> <volume> 5(2), </volume> <pages> pp. 157-166, </pages> <year> 1994. </year>
Reference-contexts: In neural network community, some temporal processing techniques have been developed such as recurrent networks and time-delay techniques etc. Unfortunately, most of those techniques either cannot capture long-term temporal information <ref> [5] </ref> or suffer from high computational burdens [6]. In the recent research of neural computing, Bengio et al proposed a recurrent architecture having a modular structure based upon the Mixture of Experts (ME) architecture and Hidden Markov Model (HMM).
Reference: [6] <author> K. Chen, D. Xie and H. Chi, </author> <title> Speaker identification using time-delay HMEs, </title> <journal> International Journal of Neural Systems, </journal> <volume> 7(1), </volume> <month> March </month> <year> 1996. </year> <note> (in press) </note>
Reference-contexts: In neural network community, some temporal processing techniques have been developed such as recurrent networks and time-delay techniques etc. Unfortunately, most of those techniques either cannot capture long-term temporal information [5] or suffer from high computational burdens <ref> [6] </ref>. In the recent research of neural computing, Bengio et al proposed a recurrent architecture having a modular structure based upon the Mixture of Experts (ME) architecture and Hidden Markov Model (HMM). Expectation-Maximization (EM) algorithm with the supervised learning paradigm is also employed in the model for training. <p> In speaker identification, the classification is a specific multiway classification that the output is the binary vector with a single non-zero component. Therefore, we use the generalized Bernoulli distribution proposed in <ref> [6] </ref> as the probabilistic model of output networks. In [6], we have shown that the generalized Bernoulli distribution is a member of the exponential family. <p> In speaker identification, the classification is a specific multiway classification that the output is the binary vector with a single non-zero component. Therefore, we use the generalized Bernoulli distribution proposed in <ref> [6] </ref> as the probabilistic model of output networks. In [6], we have shown that the generalized Bernoulli distribution is a member of the exponential family.
Reference: [7] <author> Y. Bengio and P. Frasconi, </author> <title> An Input/Output HMM architecture, </title> <booktitle> in Advances in Neural Information Systems 7, </booktitle> <editor> J.D. Cowan et al eds., </editor> <publisher> MIT press, </publisher> <year> 1995. </year>
Reference-contexts: Unlike standard HMMs which only learn the output sequence distribution and trained by an unsupervised EM algorithm, the model can be used to learn map input sequences to output sequences using the same processing style as recurrent neural networks. So it is called Input/Output HMM (IOHMM) <ref> [7] </ref>. An utterance of speaker may be viewed as a time-series. In text-dependent speaker identification, the text in both training and testing is the same or is known. <p> Accordingly, we extend the IRLS algorithm in [9] to the M-step of the EM algorithm for the IOHMM instead of the original gradient ascent method in <ref> [7] </ref>. Experiments demonstrate that the improved EM algorithm yields significantly faster training than the original one. The remainder of the paper is organized as follows. Section 2 briefly reviews the IOHMM architecture and introduces the IRLS algorithm to the EM algorithm in the IOHMM for training. <p> Moreover, admissible state transitions will be specified by a directed graph G whose vertices correspond to the model's states and the set of successors of state j in S j . Bengio et al model such a system as the recurrent architecture <ref> [7] </ref> illustrated in Fig. 1. The architecture consists of a set of state networks N j ; j = 1; ; n and a set of output networks O j ; j = 1; ; n. <p> The actual form of the output distribution, denoted f Y (y t ; ~ t ), will be chosen according to the given task. 2 2.2 The Improved EM Algorithm For the recurrent architecture, Bengio et al have proposed an EM algorithm using a supervised paradigm for parameter estimation <ref> [7] </ref>. Consider the training data are a set of P pairs of input/output sequences of length T p : D = f (u 1 (p); y 1 (p)); p = 1; ; P g. Let fi denote the set of all parameters in the architecture. <p> ) = P where ff j;t1 = P (y t1 1 ) = f Y (y t1 ; ~ j;t1 ) k and t ; x t = iju T X ' ki;t+1 fi k;t+1 : In the M-step, in general, it can be completed by the gradient ascent method <ref> [7] </ref>. Unfortunately, it suffers from rather slow training. In practice, distributions in the exponential family can cover most of problems. The IRLS algorithm is an iterative algorithm for computing the maximum likelihood estimates of the parameters of a GLIM [9][10]. <p> In detail, 4 or 5 epoches (about 5 minutes) are merely needed to reach the steady state using the improved EM algorithm, while more than 800 epoches (about 5 hours) are usually necessary to reach the steady state using the original EM algorithm in <ref> [7] </ref>. For the purpose of comparison, we also investigate some classic techniques in text-dependent speaker identification, i.e. HMMs [8], the DTW [4] and the MLP [3], on the same training and testing sets. For HMMs, we adopt the discrete HMM technique and the codebook consists of 256 codewords.
Reference: [8] <author> S. Furui, </author> <title> An overview of speaker recognition technology, </title> <booktitle> Proc. ESCA Workshop on Automatic Speaker Recognition, </booktitle> <pages> pp. 1-9, </pages> <year> 1994. </year>
Reference-contexts: For the purpose of comparison, we also investigate some classic techniques in text-dependent speaker identification, i.e. HMMs <ref> [8] </ref>, the DTW [4] and the MLP [3], on the same training and testing sets. For HMMs, we adopt the discrete HMM technique and the codebook consists of 256 codewords. We also investigated three kinds of HMMs, i.e. ergodic, left-right without jump and left-right with jumps [12].
Reference: [9] <author> M.I. Jordan and R.A. Jacobs, </author> <title> Hierarchical mixture of experts and EM algorithm, </title> <journal> Neural Computation, </journal> <volume> 6(2), </volume> <pages> pp. 181-214, </pages> <year> 1994. </year>
Reference-contexts: the other hand, we also show that the M-step of the EM algorithm in the IOHMM is still an Iterative Reweighted Least Square (IRLS) problem <ref> [9] </ref>[10] when the statistical structure of its subnetworks can be modeled by generalized linear model (GLIM) theory [10]. Accordingly, we extend the IRLS algorithm in [9] to the M-step of the EM algorithm for the IOHMM instead of the original gradient ascent method in [7]. Experiments demonstrate that the improved EM algorithm yields significantly faster training than the original one. The remainder of the paper is organized as follows. <p> Based upon all aforementioned results, we could claim empirically that the IOHMM is better than those classic techniques in text-dependent speaker identification. 4 Conclusions We have described an application of the Input/Output HMM to text-dependent speaker identification. We have also extended the IRLS algorithm <ref> [9] </ref> to the M-step of the EM algorithm in the IOHMM when the statistical structure of subsets could be modeled by the GLIM. Experimental results show that the IOHMM is a promising architecture which can deal with temporal information in text-dependent speaker identification.
Reference: [10] <author> P. McCullagh and J.A. Nelder, </author> <title> Generalized Linear Models, </title> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1989. </year>
Reference-contexts: On the other hand, we also show that the M-step of the EM algorithm in the IOHMM is still an Iterative Reweighted Least Square (IRLS) problem [9]<ref> [10] </ref> when the statistical structure of its subnetworks can be modeled by generalized linear model (GLIM) theory [10]. Accordingly, we extend the IRLS algorithm in [9] to the M-step of the EM algorithm for the IOHMM instead of the original gradient ascent method in [7]. Experiments demonstrate that the improved EM algorithm yields significantly faster training than the original one. <p> The tth component of e (o) is e (o) ir;t = (y r;t ir;t )=f 0 ( ir;t ) and f () is the link function <ref> [10] </ref> of f Y (y t ; ~ i;t ). U is the matrix consisting of all training data and its rows are the feature vectors of each utterances. <p> W (o) a diagonal matrix whose tth diagonal element is w ir;t = [f 0 ( ir;t )] 2 =V ar (y r;t ) and V ar () is the variance function <ref> [10] </ref> of f Y (y t ; ~ i;t ). In speaker identification, the classification is a specific multiway classification that the output is the binary vector with a single non-zero component. Therefore, we use the generalized Bernoulli distribution proposed in [6] as the probabilistic model of output networks.
Reference: [11] <author> K. Zwicker, </author> <title> Subdivision of the audible frequency range into critical bands, </title> <journal> J. Acoust. Soc. Amer., </journal> <volume> 35(2), </volume> <pages> pp. 248-252, </pages> <year> 1961. </year>
Reference-contexts: pre-emphasis filter H (z) = 1 0:95z 1 , 3) 16-order linear predictive (LP) analysis, 4) 256-point LP based FFT formed every 25.6ms using a Hamming analysis window without overlapping, 5) dividing channels from 0 Hz to 5.0125 KHz into 24 channels according to the knowledge on critical bands in <ref> [11] </ref>, subtraction of the average from the components and normalization of the pattern vectors as follows: In each channel, the energy is accumulated and denoted as E i ; (i = 1; 2; ; 24).

References-found: 11

