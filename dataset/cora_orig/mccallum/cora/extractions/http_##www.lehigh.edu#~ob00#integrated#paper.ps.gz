URL: http://www.lehigh.edu/~ob00/integrated/paper.ps.gz
Refering-URL: http://www.lehigh.edu/~ob00/integrated.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Knowledge Integration and Rule Extraction in Neural Networks  
Author: Olcay Boz 
Date: 09 10 1995  
Affiliation: Lehigh University EECS Department  
Abstract-found: 0
Intro-found: 0
Reference: <author> Andrews, R., Diederich, J., and Tickle, A.B.,.(1995), </author> <title> A survey and critique of techniques for extracting rules from trained artificial neural networks., </title> <note> Knowledge-Based Systems. </note> <author> (ftp://ftp.fit.qut.edu.au/pub/NRC/AUSIM/doc/QUTNRC-95-01-02.ps.Z) Baum, E., Lang, K., </author> <year> (1991), </year> <title> Constructing hidden units using examples and queries., </title> <editor> in Lippman, R., Moody, J., Touretzky, D., editors, </editor> <booktitle> Advances in neural Information Processing Systems, </booktitle> <volume> vol. 3, </volume> <pages> pp. 904-910, </pages> <address> San Mateo, CA, </address> <publisher> Morgan Kaufmann. Caudill, Maureen, </publisher> <year> (1990), </year> <title> "Using Neural Nets: Hybrid Expert Networks.", </title> <journal> AI Expert, </journal> <month> November </month> <year> 1990, </year> <pages> pp. 49 - 54. </pages>

Reference: <author> Fu, L.M., </author> <year> (1992), </year> <title> A Neural Network Model for learning Rule-Based Systems., </title> <publisher> IEEE. </publisher>
Reference-contexts: KBCNN (Fu, 1993) developed by LiMin Fu is a learning model which uses domain knowledge for creating a neural net and creating rules from the same neural net after training. KBCNN is an extension of an earlier system, Knowledgetron, <ref> (Fu, 1992) </ref> developed by Fu. Fus system shares the same advantages and disadvantages of KBANN. 5.5 Rule Extraction Neural networks are known as black boxes. Getting an explanation about the reasoning of the neural net is not very easy. <p> KBCNN is an extension of an earlier system, Knowledgetron, <ref> (Fu, 1992) </ref> developed by Fu. Initial Domain theory is in propositional horn clause format. <p> is an element of S n ) exceed the threshold on the unit; with each element n of the S n set, form a rule: `if p and NOT n, then the concept designated by the unit'. 8.8 KT KT Algorithm was also developed by Fu (Fu, 1994), (Fu, 1993), <ref> (Fu, 1992) </ref>. Although KT needed a special trainer at first with the addition of some heuristics to the search algorithm it can be used with plain backpropagation learning. KT uses a basic principle of biological neural networks.
Reference: <author> Fu, L.M., </author> <year> (1993), </year> <title> Knowledge-Based Connectionism for Revising Domain Theories., </title> <journal> in IEEE transactions on Systems, Man and Cybernetics, </journal> <volume> vol. 23, no. 1, </volume> <month> January/February </month> <year> 1993. </year>
Reference-contexts: KBANN may, depending on the initial knowledge domain theory, create very deep networks. Another disadvantage of KBANN is that, it uses only binary valued inputs. In real world applications inputs do not get only binary values, they also get integer, real and nominal (ordered or unordered) values. KBCNN <ref> (Fu, 1993) </ref> developed by LiMin Fu is a learning model which uses domain knowledge for creating a neural net and creating rules from the same neural net after training. KBCNN is an extension of an earlier system, Knowledgetron, (Fu, 1992) developed by Fu. <p> But newly added nodes do not have a symbolic meaning (they are not created from domain knowledge). Even if the rules are extracted from TopGen these newly added nodes will decrease the understandability of the rules. 7.3 KBCNN - Knowledge Based Conceptual Neural Network 23 KBCNN <ref> (Fu, 1993) </ref> developed by LiMin Fu is a learning model which uses domain knowledge for creating a neural net and creating rules from the same neural net after training. KBCNN is an extension of an earlier system, Knowledgetron, (Fu, 1992) developed by Fu. <p> and n is an element of S n ) exceed the threshold on the unit; with each element n of the S n set, form a rule: `if p and NOT n, then the concept designated by the unit'. 8.8 KT KT Algorithm was also developed by Fu (Fu, 1994), <ref> (Fu, 1993) </ref>, (Fu, 1992). Although KT needed a special trainer at first with the addition of some heuristics to the search algorithm it can be used with plain backpropagation learning. KT uses a basic principle of biological neural networks.
Reference: <author> Fu, L.M., </author> <year> (1994), </year> <title> Rule Creation From Neural Networks., </title> <journal> in IEEE transactions on systems, man and cybernetics, </journal> <volume> vol. 24, no. 8, </volume> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: all negative-attributes and n is an element of S n ) exceed the threshold on the unit; with each element n of the S n set, form a rule: `if p and NOT n, then the concept designated by the unit'. 8.8 KT KT Algorithm was also developed by Fu <ref> (Fu, 1994) </ref>, (Fu, 1993), (Fu, 1992). Although KT needed a special trainer at first with the addition of some heuristics to the search algorithm it can be used with plain backpropagation learning. KT uses a basic principle of biological neural networks.
Reference: <author> Gallant, S.I., </author> <year> (1988), </year> <title> Connectionist Expert Systems., </title> <journal> Communications of the ACM, February 1988, </journal> <volume> vol. 31, no 2. 43 Gallant, </volume> <pages> S.I., </pages> <year> (1993), </year> <title> Neural Network learning and Expert Systems., </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts. </address>
Reference-contexts: A connectionist expert system <ref> (Gallant 1988, 1993) </ref> is another variation of the coupled models. In connectionist expert systems, neural network is used to represent knowledge locally as weights between symbolic nodes. Each node has a symbolic meaning in these systems. <p> Connectionist expert systems represent relationships between pieces of knowledge with weighted links between symbolic nodes. One of the most popular of all the hybrid algorithms is Gallants MACIE which is a connectionist expert system. 6.1 MACIE (Matrix Controlled Inference Engine) - A connectionist expert system MACIE <ref> (Gallant, 1988) </ref> (Gallant, 1993) is a connectionist expert system which can be used in an interactive fashion and is able to give explanations and able to seek values for 14 the unknown inputs by asking the values to the user.
Reference: <author> Hilario, M., </author> <year> (1995), </year> <title> An Overview of Strategies for Neurosymbolic Integration., </title> <type> Technical Report, </type> <institution> CUI - University of Geneva, UNIGE-AI-95-04, </institution> <note> 1995. </note> <author> (ftp://cui.unige.ch/AI/tech-reports/unige-ai-95-04.ps) Hinton,, G.E., </author> <year> (1986), </year> <title> Learning Distributed Representations of Concepts., </title> <booktitle> In Proceedings of the Eighth Annual Conference of The Cognitive Science Society, </booktitle> <pages> pp. 1-12, </pages> <address> Amherst, MA. </address> <publisher> Erlbaum. </publisher>
Reference: <author> Jacobs, R.A., and Jordan, M. </author> <year> (1991), </year> <title> "A competitive modular connectionist architecture.", </title> <editor> in R.P. Lippmann, J.E. Moody and D.S. Teuretzky (eds), </editor> <booktitle> Neural Information Processing systems 3, (Denver 1990), </booktitle> <pages> pp 767-773, </pages> <address> New York, </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> McMillan, C., Mozer, M.C., and Smolensky, P., </author> <year> (1991), </year> <title> The connectionist scientist game: rule extraction and refinement in a neural network., </title> <booktitle> Proc. of the Thirteenth Annual Conference of the Cognitive Science Society, </booktitle> <address> (Hillsdale NJ). </address>
Reference-contexts: Therefore the algorithm is exponential. Fus solution to this problem by restricting the size of the solution search space by placing a ceiling on the number of antecedents may create adverse effects on real life problems. 8.9 RuleNet RuleNet <ref> (McMillan et al., 1991) </ref> is one of the earliest examples of rule extraction in neural networks. The basic idea is to : train a neural network. extract rules by using connection strength in the network. 38 inject the rules back to the neural network.
Reference: <author> Medsker, </author> <title> L.R., (1994), Hybrid Neural Network and Expert Systems., </title> <publisher> Kluwer Academic Publishers. </publisher>
Reference: <author> Medsker, </author> <title> L.R., Belley, D.L., (1992), "Models and Guidelines For Integrating Expert Systems and Neural Networks.", in "Hybrid architectures for intelligent system", Abraham Kandel and Gideon Langholz (eds.). </title>
Reference: <author> Mitchell, T. and Thrun, S., </author> <year> (1993), </year> <title> "Explanation-Based Learning: A Comparison of Symbolic and Neural Network Approaches.", </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <editor> P. Utgoff (ed.), </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <note> (http://www.informatik.uni-bonn.de/~thrun/mitchell.EBL-ml93.ps.Z) Nowlan, </note> <author> S. J., and Hinton, G.E., </author> <year> (1992), </year> <title> Simplifying neural networks by soft weight sharing., </title> <booktitle> In advances in Neural Information Processing Systems, </booktitle> <volume> vol. 5, </volume> <editor> J. Moody, </editor> <publisher> S. </publisher>
Reference: <editor> Hanson, & R. Lippmann (Eds.), </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>

Reference: <author> Pop, E., Hayward, R., and Diederich, J., </author> <year> (1994), </year> <title> RULENEG: Extracting Rules From a Trained ANN by Stepwise negation., </title> <type> technical report, </type> <month> QUT NRC (December </month> <year> 1994). </year>
Reference-contexts: The consequences of the rules have also intermediate concepts (hidden units) which do not have symbolic meanings assigned. This makes the rules even less understandable. Like Towels technique inputs have to be binary in this algorithm. Another rule extraction technique, RuleNeg, was developed by E. Pop <ref> (Pop, Hayward and Diederich, 1994) </ref>. An adaptation of the PAC algorithm developed by Valiant (1984). In this method, rule extraction is viewed as a learning task. The output created by the network is the target of the learning algorithm. The method extracts conjunctive rules only. <p> In their previous work (Ramachandran and Pratt, 1992) Ramachandran and Pratt developed Information Measure Based Skeletosation (IMBS) algorithm. IMBS finds hyperplanes whose IM values are 0. In this way the corresponding hidden units can be removed from the network. 8.5 RULENEG RULENEG is developed by E. Pop <ref> (Pop, Hayward and Diederich, 1994) </ref>. An adaptation of the PAC algorithm developed by Valiant (1984). In this method rule extraction is viewed as a learning task. The output created by the network is the target of the learning algorithm. The method extracts conjunctive rules only.
Reference: <author> Pratt, L.Y., </author> <year> (1993), </year> <title> Transferring Previously Learned Back-Propagation Neural Networks to New Learning Tasks., </title> <type> Technical Report ML-TR-37, </type> <institution> Rutgers University, Department of Computer Science, </institution> <month> May, </month> <type> Ph.D. Thesis. </type> <note> (ftp://franklinite.mines.edu/pub/pratt-papers/pratt.thesis.ps.Z) Pratt, L.Y., and Christensen, A.N., </note> <year> (1994), </year> <title> Relaxing the hyperplane assumption in the analysis and modification of back-propagation networks., </title> <editor> In Robert Trappl, ed., </editor> <booktitle> Cybernetics and Systems '94 . World Scientific, Singapore, </booktitle> <pages> Pages 1711-1718. </pages> <editor> (ftp://franklinite.mines.edu/pub/pratt-papers/pratt-christensen-csr94.ps.Z) Quinlan, J.R., </editor> <year> (1983), </year> <title> Learning Efficient Classification Procedures and Their Application to Chess End Games., </title> <booktitle> In Machine Learning, </booktitle> <pages> pp. 463-482, </pages> <address> Palo Alto, CA: </address> <publisher> Tioga Publishing. </publisher>
Reference-contexts: Time for creating decision trees increases exponentially with the increasing number of training patterns. This is not the case for neural networks. Decision boundaries in neural networks are represented by hyperplanes. They divide the feature space into classification regions (Pratt and Cristensen, 1994) <ref> (Pratt, 1993) </ref>. Hidden units in the neural networks represent what hyperplanes in feature space represent. The classification labels in feature space represent the same things with the output units in neural networks. Same things can be said for decision trees and the hyperplanes.
Reference: <author> Quinlan, J.R., </author> <year> (1986), </year> <title> Induction of Decision Trees., </title> <booktitle> Machine Learning, </booktitle> <pages> 1 81-106. </pages>
Reference-contexts: The rules are extracted by using Fus KT algorithm which is explained in detail in the rule extraction section.. 7.4 Symbolic Knowledge Refinement Systems There are similar symbolic knowledge refining systems. Some of these use decision trees and some use production rules. EITHER (Ourston and Mooney, 1994).uses ID3 <ref> (Quinlan, 1986) </ref> to make minimal corrections to domain theories. The second system is Labyrinth-k (Thompson, Langley and Iba, 1991) allows the insertion of domain knowledge. These systems are also able to use initial knowledge that is both incorrect and incomplete as the basis of learning. <p> Possible remedies are : Early termination, post pruning, rule pruning. 8.16.1.2 C4.5 C4.5 is a descendent of ID3 system developed by Ross Quinlan <ref> (Quinlan, 1986) </ref> (Quinlan, 1993). This is the system which has the greatest impact on machine learning research in the last years. It is a supervised learning system and constructs decision trees from examples.
Reference: <author> Quinlan, J.R., </author> <year> (1993), </year> <title> C4.5 : Programs For Machine Learning., </title> <publisher> Morgan Kauffman Publishers, </publisher> <address> San Mateo, California. </address>
Reference-contexts: There is no rule simplification and rule pruning step for increasing the generality and understandability of the rules. Rules extracted are not mutually exclusive. An input record may satisfy more than one rule or none. C4.5 <ref> (Quinlan, 1993) </ref> solves this problem by sorting the rules according to the class and the first rule that match an input record determines the class for that record. NofM algorithm makes the assumption that each hidden unit corresponds to a meaningful concept. <p> Possible remedies are : Early termination, post pruning, rule pruning. 8.16.1.2 C4.5 C4.5 is a descendent of ID3 system developed by Ross Quinlan (Quinlan, 1986) <ref> (Quinlan, 1993) </ref>. This is the system which has the greatest impact on machine learning research in the last years. It is a supervised learning system and constructs decision trees from examples. The search space for a particular problem is all the possible trees that can be constructed with the samples.

Reference: <author> Shavlik, J.W., </author> <year> (1994), </year> <title> A Framework For Combining Symbolic and Neural Learning., in Artificial Intelligence and Neural Networks, Steps towards principled integration, edited by Vasant Honavar, </title> <editor> Leonard Uhr, </editor> <publisher> Academic Press. </publisher> <editor> (ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group/shavlik.tr92.ps) Thompson, K., Langley, P., and Iba, W., </editor> <year> (1991), </year> <title> Using Background Knowledge In Concept Formation., </title> <booktitle> In: Proceedings Of The Eighth International Machine Learning Workshop, </booktitle> <address> Evanston, IL, </address> <pages> pp. 554-558. </pages>
Reference-contexts: Since the domain knowledge is worth some number of examples, there is no need to have as many examples as inductive learning systems require. Some of the existing knowledge refinement techniques using neural nets are explained below: 9 KBANN (Knowledge Based Artificial Neural Network) <ref> (Towell and Shavlik, 1994) </ref> (Towell and Shavlik, 1993) (Towell, 1992) creates knowledge based neural networks whose topological structure is equivalent to dependency structure of the rules in the domain theory. Rules used by KBANN for domain knowledge and for extracted knowledge are if-then rules. <p> This produces if u 2 is false and u 5 is true then conclude that u 7 is true. 7. Knowledge Refinement Techniques MACIE 18 7.1 KBANN KBANN (Knowledge Based Artificial Neural Network) <ref> (Towell and Shavlik, 1994) </ref> (Towell and Shavlik, 1993) creates knowledge based neural networks whose topological structure is equivalent to dependency structure of the rules in the domain theory. Rules used by KBANN are if-then rules. <p> If the domain theory is not good enough the network should alter the theory significantly. But without being able add new rules or get rid off existing rules, the domain theory can only be updated on the antecedent level. TopGen (TOPology GENerator) developed by Opitz (Opitz and Shavlik, 1993), <ref> (Opitz and Shavlik, 1994) </ref>, (Opitz, 1995) uses domain theory and the training data to expand the neural net. This is done by adding hidden nodes to the network. Nodes are added to the places where errors are high. TopGen uses two validation sets.
Reference: <author> Thrun, S., </author> <year> (1993), </year> <title> Extracting Provably Correct Rules from Artificial Neural Networks., </title> <type> Technical Report IAI-TR-93-5. </type> <institution> University of Bonn. </institution>


Reference: <author> Valiant, L.G. </author> <year> (1984), </year> <title> A Theory of The Learnable., </title> <journal> Communications of the ACM, </journal> <volume> 27 </volume> <pages> 1134-1142. </pages>
References-found: 19

