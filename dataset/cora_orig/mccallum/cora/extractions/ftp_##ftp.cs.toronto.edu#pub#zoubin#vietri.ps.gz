URL: ftp://ftp.cs.toronto.edu/pub/zoubin/vietri.ps.gz
Refering-URL: http://www.cs.utoronto.ca/~zoubin/
Root-URL: 
Email: zoubin@cs.toronto.edu  
Title: Learning Dynamic Bayesian Networks  
Author: Zoubin Ghahramani 
Note: Table of Contents  
Date: October, 1997  
Web: http://www.cs.utoronto.ca/ zoubin/  
Address: Toronto, ON M5S 3H5, Canada  
Affiliation: Department of Computer Science University of Toronto  
Abstract: Bayesian networks are directed acyclic graphs that represent dependencies between variables in a probabilistic model. Many time series models, including the hidden Markov models (HMMs) used in speech recognition and Kalman filter models used in filtering and control applications, can be viewed as examples of dynamic Bayesian networks. We first provide a brief tutorial on learning and Bayesian networks. We then present some dynamic Bayesian networks that can capture much richer structure than HMMs and Kalman filters, including spatial and temporal multiresolution structure, distributed hidden state representations, and multiple switching linear regimes. While exact probabilistic inference is intractable in these networks, one can obtain tractable variational approximations which call as subroutines the forward-backward and Kalman filter recursions. These approximations can be used to learn the model parameters by maximizing a lower bound on the likelihood. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> B. D. O. Anderson and J. B. Moore. </author> <title> Optimal Filtering. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1979. </year>
Reference-contexts: Thorough treatments of Kalman filtering and smoothing can be found in <ref> [1, 18] </ref>. 12 observations fY 1 ; : : : Y t g.
Reference: 2. <author> P. Baldi, Y. Chauvin, T. Hunkapiller, and M.A. McClure. </author> <title> Hidden Markov mod-els of biological primary sequence information. </title> <booktitle> Proc. </booktitle> <institution> Nat. Acad. Sci. (USA), </institution> <month> 91(3) </month> <pages> 1059-1063, </pages> <year> 1994. </year>
Reference-contexts: Like state-space models, HMMs can be augmented to allow for input variables [7, 4, 36]. The system then models the conditional distribution of a sequence of output observations given a sequence of input observations. HMMs have been applied extensively to problems in speech recognition [28], computational biology <ref> [32, 2] </ref>, and fault detection [48]. 4 Learning and Inference A Bayesian approach to learning starts with some a priori knowledge about the model structure|the set of arcs in the Bayesian network|and model parameters.
Reference: 3. <author> L.E. Baum, T. Petrie, G. Soules, and N. Weiss. </author> <title> A maximization technique occur-ring in the statistical analysis of probabilistic functions of Markov chains. </title> <journal> The Annals of Mathematical Statistics, </journal> <volume> 41 </volume> <pages> 164-171, </pages> <year> 1970. </year>
Reference-contexts: Section 3 demonstrates the use of Bayesian networks for modeling time series, including some well-known examples such as the Kalman filer and the hidden Markov model. Section 4 focuses on the problem of learning the parameters of a Bayesian network using the Expectation-Maximization (EM) algorithm <ref> [3, 10] </ref>. Section 5 describes some richer models appropriate for time series with nonlinear or mul- tiresolution structure. Inference in such models may be computationally intractable. <p> The Expectation-Maximization (EM) algorithm <ref> [3, 10] </ref> alternates between maximizing F with respect to Q and , respectively, holding the other fixed. <p> Since the state variables are hidden we cannot compute (32) directly. The EM algorithm, which in the case of HMMs is known as the Baum-Welch algorithm <ref> [3] </ref>, allows us to circumvent this problem by computing the expectation of (32) under the posterior distribution of the hidden states given the observations. This expectation can be expressed as a function of hS t i and hS t S 0 t1 i (1 t T ).
Reference: 4. <author> Y. Bengio and P. Frasconi. </author> <title> An input-output HMM architecture. </title> <editor> In G. Tesauro, D. S. Touretzky, and T. K. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 427-434. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1995. </year>
Reference-contexts: For real- valued observation vectors, P (Y t jS t ) can be modeled in many different forms, such as a Gaussian, mixture of Gaussians, or a neural network. Like state-space models, HMMs can be augmented to allow for input variables <ref> [7, 4, 36] </ref>. The system then models the conditional distribution of a sequence of output observations given a sequence of input observations.
Reference: 5. <author> J. Besag. </author> <title> Spatial interaction and the statistical analysis of lattice systems. </title> <journal> J. Royal Stat. Soc. B, </journal> <volume> 36 </volume> <pages> 192-326, </pages> <year> 1974. </year>
Reference-contexts: this paper are based on it) and refer the reader to relevant 3 That is, D is a child of both the previous and following nodes in the path. 4 Undirected graphical models (Markov networks) are another important tool for representing probability distributions, and have a different set of semantics <ref> [5, 13] </ref>. We will deal exclusively with directed graphical models in this paper. 4 texts [41, 24, 19] for details. Assume we observe some evidence: the value of some variables in the network.
Reference: 6. <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International Group, </booktitle> <address> Belmont, CA, </address> <year> 1984. </year>
Reference-contexts: This model therefore generalizes the "hierarchical mixture of experts" [27] and other related decision tree models such as CART <ref> [6] </ref> and MARS [12] by giving the decisions Markovian dynamics. Tree structured HMMs provide a useful starting point for modeling time series with both temporal and spatial structure at multiple resolutions.
Reference: 7. <author> T. W. Cacciatore and S. J. Nowlan. </author> <title> Mixtures of controllers for jump linear and non-linear plants. </title> <editor> In J. D. Cowan, G. Tesauro, and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 719-726. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA, </address> <year> 1994. </year>
Reference-contexts: For real- valued observation vectors, P (Y t jS t ) can be modeled in many different forms, such as a Gaussian, mixture of Gaussians, or a neural network. Like state-space models, HMMs can be augmented to allow for input variables <ref> [7, 4, 36] </ref>. The system then models the conditional distribution of a sequence of output observations given a sequence of input observations. <p> This model can be seen as an extension of the "mixture of experts" architecture for modular learning in neural networks <ref> [22, 7, 36] </ref>. Each state-space model is a linear expert with Gaussian output noise and linear-Gaussian dynamics. The switch state "gates" the outputs of the M state-space models, and therefore plays the role of a gating network with Markovian dynamics [7, 36]. Fig. 7. <p> Each state-space model is a linear expert with Gaussian output noise and linear-Gaussian dynamics. The switch state "gates" the outputs of the M state-space models, and therefore plays the role of a gating network with Markovian dynamics <ref> [7, 36] </ref>. Fig. 7. Bayesian network representation for switching state-space models.
Reference: 8. <author> C. K. Carter and R. Kohn. </author> <title> Markov chain Monte Carlo in conditionally Gaussian state space models. </title> <institution> Australian Graduate School of Management, University of New South Wales, </institution> <year> 1996. </year>
Reference-contexts: needed to estimate hS (m) (m) (n)0 hS t1 S t i are collected using the states visited and the probabilities estimated during this sampling process and are used in the approximate E step of EM. 11 Monte Carlo methods for learning in dynamic Bayesian networks have been explored by <ref> [9, 30, 8, 17] </ref>. 6.2 Variational Methods Another approach to approximating a probability distribution P is to define a parametrized distribution Q and vary its parameters so as to minimize the distance between Q and P .
Reference: 9. <author> T. Dean and K. </author> <title> Kanazawa. A model for reasoning about persitence and causation. </title> <journal> Computational Intelligence, </journal> <volume> 5(3) </volume> <pages> 142-150, </pages> <year> 1989. </year>
Reference-contexts: needed to estimate hS (m) (m) (n)0 hS t1 S t i are collected using the states visited and the probabilities estimated during this sampling process and are used in the approximate E step of EM. 11 Monte Carlo methods for learning in dynamic Bayesian networks have been explored by <ref> [9, 30, 8, 17] </ref>. 6.2 Variational Methods Another approach to approximating a probability distribution P is to define a parametrized distribution Q and vary its parameters so as to minimize the distance between Q and P .
Reference: 10. <author> A.P. Dempster, N.M. Laird, and D.B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> J. Royal Statistical Society Series B, </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: Section 3 demonstrates the use of Bayesian networks for modeling time series, including some well-known examples such as the Kalman filer and the hidden Markov model. Section 4 focuses on the problem of learning the parameters of a Bayesian network using the Expectation-Maximization (EM) algorithm <ref> [3, 10] </ref>. Section 5 describes some richer models appropriate for time series with nonlinear or mul- tiresolution structure. Inference in such models may be computationally intractable. <p> The Expectation-Maximization (EM) algorithm <ref> [3, 10] </ref> alternates between maximizing F with respect to Q and , respectively, holding the other fixed. <p> The maximum in the M step is obtained by maximizing the the first term in (15), since the entropy of Q does not depend on : M step: k+1 arg max X This is the expression most often associated with the EM algorithm <ref> [10] </ref>, but it obscures the elegant interpretation of EM as coordinate ascent in F . Since F = L at the beginning of each M step, and since the E step does not change , we are guaranteed not to decrease the likelihood after each combined EM step.
Reference: 11. <author> V. Digalakis, J. R. Rohlicek, and M. Ostendorf. </author> <title> ML estimation of a Stochastic Linear System with the EM Algorithm and its Application to Speech Recognition. </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> 1(4) </volume> <pages> 431-442, </pages> <year> 1993. </year>
Reference-contexts: Z f (X) P (XjY; k ) dX: (20) Then, the M step for C is C X Y t hX t i 0 X hX t X 0 ! 1 Similar M steps can be derived for all the other parameters by taking derivatives of the expected log probability <ref> [47, 11, 15] </ref>. 7 In general we require all terms of the kind hX t i, hX t X 0 t i and hX t X 0 t1 i.
Reference: 12. <author> J. H. Friedman. </author> <title> Multivariate adaptive regression splines. </title> <journal> The Annals of Statistics, </journal> <volume> 19 </volume> <pages> 1-141, </pages> <year> 1991. </year>
Reference-contexts: This model therefore generalizes the "hierarchical mixture of experts" [27] and other related decision tree models such as CART [6] and MARS <ref> [12] </ref> by giving the decisions Markovian dynamics. Tree structured HMMs provide a useful starting point for modeling time series with both temporal and spatial structure at multiple resolutions.
Reference: 13. <author> S. Geman and D. Geman. </author> <title> Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 6 </volume> <pages> 721-741, </pages> <year> 1984. </year>
Reference-contexts: this paper are based on it) and refer the reader to relevant 3 That is, D is a child of both the previous and following nodes in the path. 4 Undirected graphical models (Markov networks) are another important tool for representing probability distributions, and have a different set of semantics <ref> [5, 13] </ref>. We will deal exclusively with directed graphical models in this paper. 4 texts [41, 24, 19] for details. Assume we observe some evidence: the value of some variables in the network. <p> The sequence of states resulting from each pass of Gibbs sampling defines a Markov chain over the state space of the model. This Markov chain is guaranteed to converge to the posterior probabilities of the states given the observations <ref> [13] </ref> as long as none of the probabilities in the model is exactly zero 10 . Thus, after some suitable time, samples from the Markov chain can be taken as approximate samples from the posterior probabilities.
Reference: 14. <author> Z. Ghahramani. </author> <title> Factorial learning and the EM algorithm. </title> <editor> In G. Tesauro, D.S. Touretzky, and T.K. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 617-624. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1995. </year>
Reference-contexts: This static mixture model, without inclusion of the time index and the Markov dynamics, is a factorial parameterization of the standard mixture of Gaussians model that has interest in its own right <ref> [52, 20, 14] </ref>. The model we have just presented extends this by allowing Markov dynamics in the discrete state variables underlying the mixture. 17 Fig. 5.
Reference: 15. <author> Z. Ghahramani and G. E. Hinton. </author> <title> Parameter estimation for linear dynamical sys-tems. </title> <institution> Technical Report CRG-TR-96-2 [ftp://ftp.cs.toronto.edu/pub/zoubin/tr96-2.ps.gz] , Department of Computer Science, University of Toronto, </institution> <year> 1996. </year> <month> 29 </month>
Reference-contexts: Z f (X) P (XjY; k ) dX: (20) Then, the M step for C is C X Y t hX t i 0 X hX t X 0 ! 1 Similar M steps can be derived for all the other parameters by taking derivatives of the expected log probability <ref> [47, 11, 15] </ref>. 7 In general we require all terms of the kind hX t i, hX t X 0 t i and hX t X 0 t1 i.
Reference: 16. <author> Z. Ghahramani and G. E. Hinton. </author> <title> Switching state-space models. </title> <type> Technical Report CRG-TR-96-3 [ftp://ftp.cs.toronto.edu/pub/ zoubin/switch.ps.gz], </type> <institution> Department of Computer Science, University of Toronto, </institution> <year> 1996. </year>
Reference-contexts: To model time series with continuous but nonlinear dynamics, it is possible to combine the real-valued hidden state of linear-Gaussian state-space models and the discrete state of HMMs. One natural way to do this is the switching state-space model <ref> [16] </ref>. In switching state-space models, the sequence of observations fY t g is modeled using a hidden state space comprising M real-valued state vectors, X (m) one discrete state vector S t . <p> We then recompute responsibilities by running the forward-backward algorithm on the switch process using the predicted error of each SSM. This procedure is iterated until the responsibilities converge. Details of this structured variational approximation for switching state-space models are provided in <ref> [16] </ref>. 6.5 Convex duality The framework for obtaining lower bounds on log likelihoods is a special case of more general variational methods based on convex duality.
Reference: 17. <author> Z. Ghahramani and M. I. Jordan. </author> <title> Factorial hidden Markov models. </title> <booktitle> Machine Learning, </booktitle> <year> 1997. </year>
Reference-contexts: The first of these represents the hidden state of an HMM using a set of distinct state variables. We can this HMM with a distributed state representation, a factorial hidden Markov model <ref> [17] </ref>. 5.1 Example 3: Factorial HMMs We generalize the HMM by representing the state using a collection of discrete state variables S t = S t ; : : :S t ; : : : ; S t ; (45) each of which can take on K (m) values. <p> This can be further improved upon by using the fact that the state transitions are defined via M matrices of size K fi K rather than a single K M fi K M matrix, resulting in a recursive algorithm using O (T M K M+1 ) operations (see <ref> [17] </ref>, appendix B). Unfortunately, this time complexity cannot be improved upon. Given the observation at time t, the K-valued state variables become coupled in an M th order interaction. It is not possible to sum over each variable independently. <p> needed to estimate hS (m) (m) (n)0 hS t1 S t i are collected using the states visited and the probabilities estimated during this sampling process and are used in the approximate E step of EM. 11 Monte Carlo methods for learning in dynamic Bayesian networks have been explored by <ref> [9, 30, 8, 17] </ref>. 6.2 Variational Methods Another approach to approximating a probability distribution P is to define a parametrized distribution Q and vary its parameters so as to minimize the distance between Q and P . <p> 'fg is the softmax operator, which maps a vector A into a vector B of the same size, with elements B i = expfA i g X expfA j g ; (61) and log P (m) denotes the elementwise logarithm of the transition matrix P (m) (see appendix C in <ref> [17] </ref> for details of the derivation). The first term of (59) is the projection of the error in reconstructing the observation onto the weights of state vector m|the more a particular setting of a state vector can reduce this error, the larger its associated variational mean. <p> Conver <p>- gence to a global minimum of the KL divergence is not required, and in general this procedure will converge to a local minimum. Once the fixed point equations have converged, the expectations required for the E step can be obtained as a simple function of the parameters <ref> [17] </ref>. 6.4 Example: Structured approximation for factorial HMMs The approximation presented in the previous section factors the posterior prob <p>- ability into a product of statistically independent distributions over the state variables.
Reference: 18. <author> G.C. Goodwin and K.S. </author> <title> Sin. Adaptive filtering prediction and control. </title> <address> PrenticeHall, </address> <year> 1984. </year>
Reference-contexts: Thorough treatments of Kalman filtering and smoothing can be found in <ref> [1, 18] </ref>. 12 observations fY 1 ; : : : Y t g.
Reference: 19. <author> D. Heckerman. </author> <title> A tutorial on learning with Bayesian networks. </title> <institution> Technical Report MSR-TR-95-06 [ftp://ftp.research.microsoft.com/pub/tr/TR-95-06.PS] , Microsoft Research, </institution> <year> 1996. </year>
Reference-contexts: We will deal exclusively with directed graphical models in this paper. 4 texts <ref> [41, 24, 19] </ref> for details. Assume we observe some evidence: the value of some variables in the network. The goal of belief propagation is to update the marginal probabilities of all the variables in the network to incorporate this new evidence. <p> The approach of exploiting such tractable substructures was first suggested in the machine learning literature by Saul and Jordan <ref> (1996) </ref>. We write the structured variational approximation as Q (fS t gj) = Z Q m=1 (m) T Y Q (S t jS t1 ; ); (62) where Z Q is a normalization constant ensuring that Q sums to one.
Reference: 20. <author> G. E. Hinton and R. S. Zemel. Autoencoders, </author> <title> minimum description length, and Helmholtz free energy. </title> <editor> In J.D. Cowan, G. Tesauro, and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6. </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA, </address> <year> 1994. </year>
Reference-contexts: This static mixture model, without inclusion of the time index and the Markov dynamics, is a factorial parameterization of the standard mixture of Gaussians model that has interest in its own right <ref> [52, 20, 14] </ref>. The model we have just presented extends this by allowing Markov dynamics in the discrete state variables underlying the mixture. 17 Fig. 5.
Reference: 21. <author> T. S. Jaakkola. </author> <title> Variational methods for Inference and estimation in graphical mod-els. </title> <type> Technical Report Ph.D. Thesis, </type> <institution> Department of Brain and Cognitive Sciences, MIT, </institution> <address> Cambridge, MA, </address> <year> 1997. </year>
Reference-contexts: Second, we have not dealt with networks in which there are complex nonlinear interactions. Methods from convex duality can, in principle, be used to solve these problems. We present only a brief tutorial here and refer the reader to <ref> [21] </ref> for examples of how this approach can be used to define upper bounds and deal with certain nonlinearities. A convex function f (x) is characterized by the property that the set of points f (x; y) : y f (x)g is convex.
Reference: 22. <author> R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. </author> <title> Adaptive mixture of local experts. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 79-87, </pages> <year> 1991. </year>
Reference-contexts: This model can be seen as an extension of the "mixture of experts" architecture for modular learning in neural networks <ref> [22, 7, 36] </ref>. Each state-space model is a linear expert with Gaussian output noise and linear-Gaussian dynamics. The switch state "gates" the outputs of the M state-space models, and therefore plays the role of a gating network with Markovian dynamics [7, 36]. Fig. 7.
Reference: 23. <author> E. T. Jaynes. </author> <title> Probability Theory: </title> <booktitle> The Logic of Science. </booktitle> <year> 1995. </year>
Reference-contexts: Furthermore, there is added uncertainty resulting from the limited size of our data set and any mismatch between our model and the true process. Probability theory provides a powerful tool for expressing both randomness and uncertainty in our model <ref> [23] </ref>. We can express the uncertainty in our prediction of the future outcome Y t+1 via a probability density P (Y t+1 jY 1 ; : : : ; Y t ).
Reference: 24. <author> F. V. Jensen. </author> <title> Introduction to Bayesian Networks. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: We will deal exclusively with directed graphical models in this paper. 4 texts <ref> [41, 24, 19] </ref> for details. Assume we observe some evidence: the value of some variables in the network. The goal of belief propagation is to update the marginal probabilities of all the variables in the network to incorporate this new evidence.
Reference: 25. <author> F. V. Jensen, S. L. Lauritzen, and K. G. Olesen. </author> <title> Bayesian updating in recur-sive graphical models by local computations. </title> <journal> Computational Statistics Quarterly, </journal> <volume> 4 </volume> <pages> 269-282, </pages> <year> 1990. </year>
Reference-contexts: For multiply connected networks, in which there can be more than one undirected path between any two nodes, there exists a more general algorithm known as the junction tree algorithm <ref> [33, 25] </ref>.
Reference: 26. <author> M. I. Jordan, Z. Ghahramani, and L. K. Saul. </author> <title> Hidden Markov decision trees. </title> <editor> In M.C. Mozer, M.I Jordan, and T. Petsche, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 9. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1997. </year>
Reference-contexts: Tree structured HMMs provide a useful starting point for modeling time series with both temporal and spatial structure at multiple resolutions. We have explored this generalization of factorial HMMs in <ref> [26] </ref>. 5.3 Example 5: Switching State space models Both factorial HMMs and tree-structured HMMs use discrete hidden state representations. To model time series with continuous but nonlinear dynamics, it is possible to combine the real-valued hidden state of linear-Gaussian state-space models and the discrete state of HMMs. <p> The other choice is to retain the arcs within a time step and eliminate the arcs between consecutive time steps. Both of these approximations, along with an approximation based on the Viterbi, algorithm are pursued in <ref> [26] </ref>. For switching state-space models, the natural approximation is to uncouple the M state-space models (SSMs) from the discrete Markov process controlling the switch variable.
Reference: 27. <author> M. I. Jordan and R.A. Jacobs. </author> <title> Hierarchical mixtures of experts and the EM algo-rithm. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 181-214, </pages> <year> 1994. </year>
Reference-contexts: At the next time step, a similar procedure is used to generate data from the model, except that now each decision in the tree is dependent on the decision taken at that node in the previous time step. This model therefore generalizes the "hierarchical mixture of experts" <ref> [27] </ref> and other related decision tree models such as CART [6] and MARS [12] by giving the decisions Markovian dynamics. Tree structured HMMs provide a useful starting point for modeling time series with both temporal and spatial structure at multiple resolutions.
Reference: 28. <author> B. H. Juang and L. R. Rabiner. </author> <title> Hidden Markov models for speech recognition. </title> <journal> Technometrics, </journal> <volume> 33 </volume> <pages> 251-272, </pages> <year> 1991. </year>
Reference-contexts: Like state-space models, HMMs can be augmented to allow for input variables [7, 4, 36]. The system then models the conditional distribution of a sequence of output observations given a sequence of input observations. HMMs have been applied extensively to problems in speech recognition <ref> [28] </ref>, computational biology [32, 2], and fault detection [48]. 4 Learning and Inference A Bayesian approach to learning starts with some a priori knowledge about the model structure|the set of arcs in the Bayesian network|and model parameters.
Reference: 29. <author> R. E. Kalman and R. S. Bucy. </author> <title> New results in linear filtering and prediction. </title> <journal> Journal of Basic Engineering (ASME), </journal> <volume> 83D:95-108, </volume> <year> 1961. </year>
Reference-contexts: It consists of two parts: a forward recursion which uses the observations from Y 1 to Y t , known as the Kalman filter <ref> [29] </ref>, and a backward recursion which uses the observations from Y T to Y t+1 [43]. 8 We have already seen that in order to compute the marginal probability of a variable in a Bayesian network one must take into account both the evidence above and below the variable.
Reference: 30. <author> K. Kanazawa, D. Koller, and S. J. Russell. </author> <title> Stochastic simulation algorithms for dynamic probabilistic networks. </title> <editor> In P. Besnard and S. Hanks, editors, </editor> <booktitle> Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference., </booktitle> <pages> pages 346-351. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA, </address> <year> 1995. </year>
Reference-contexts: needed to estimate hS (m) (m) (n)0 hS t1 S t i are collected using the states visited and the probabilities estimated during this sampling process and are used in the approximate E step of EM. 11 Monte Carlo methods for learning in dynamic Bayesian networks have been explored by <ref> [9, 30, 8, 17] </ref>. 6.2 Variational Methods Another approach to approximating a probability distribution P is to define a parametrized distribution Q and vary its parameters so as to minimize the distance between Q and P .
Reference: 31. <author> J. H. Kim and J. Peal. </author> <title> A computational model for causal and diagnostic reasoning in inference systems. </title> <booktitle> In Proceedings of the 8th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 190-193. </pages> <year> 1983. </year>
Reference-contexts: The absence of arcs in a Bayesian networks implies conditional independence relations which can be exploited to obtain efficient algorithms for computing marginal and conditional probabilities. For singly connected networks, in which the underlying undirected graph has no loops, there exists a general algorithm called belief propagation <ref> [31, 41] </ref>. For multiply connected networks, in which there can be more than one undirected path between any two nodes, there exists a more general algorithm known as the junction tree algorithm [33, 25].
Reference: 32. <author> A. Krogh, M. Brown, I. S. Mian, K. Sjolander, and D. Haussler. </author> <title> Hidden Markov models in computational biology: Applications to protein modeling. </title> <journal> Journal of Molecular Biology, </journal> <volume> 235 </volume> <pages> 1501-1531, </pages> <year> 1994. </year>
Reference-contexts: Like state-space models, HMMs can be augmented to allow for input variables [7, 4, 36]. The system then models the conditional distribution of a sequence of output observations given a sequence of input observations. HMMs have been applied extensively to problems in speech recognition [28], computational biology <ref> [32, 2] </ref>, and fault detection [48]. 4 Learning and Inference A Bayesian approach to learning starts with some a priori knowledge about the model structure|the set of arcs in the Bayesian network|and model parameters.
Reference: 33. <author> S. L. Lauritzen and D. J. Spiegelhalter. </author> <title> Local computations with probabilities on graphical structures and their application to expert systems. </title> <journal> J. Royal Statistical Society B, </journal> <pages> pages 157-224, </pages> <year> 1988. </year>
Reference-contexts: For multiply connected networks, in which there can be more than one undirected path between any two nodes, there exists a more general algorithm known as the junction tree algorithm <ref> [33, 25] </ref>.
Reference: 34. <author> L. Ljung and T. Soderstrom. </author> <title> Theory and Practice of Recursive Identification. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1983. </year> <month> 30 </month>
Reference-contexts: It is useful to define the quantities X t t as the mean vector and covariance matrix of X t , respectively, given 7 The parameters of a linear-Gaussian state-space model can also be estimated using methods from on-line recursive identification <ref> [34] </ref>. 8 The forward and backward recursions together are also known as the Rauch-Tung- Streibel (RTS) smoother. Thorough treatments of Kalman filtering and smoothing can be found in [1, 18]. 12 observations fY 1 ; : : : Y t g.
Reference: 35. <author> D. J. C. MacKay. </author> <title> Probable networks and plausible predictions|a review of prac--tical Bayesian methods for supervised neural networks. Network: </title> <booktitle> Computation in Neural Systems, </booktitle> <volume> 6 </volume> <pages> 469-505, </pages> <year> 1995. </year>
Reference-contexts: : : ; Y (N) g, each of which can be a vector or time series of vectors, then the likelihood of the data set is: P (Dj; M) = i=1 6 Two approximate methods for integrating over the posterior in the case of neural network models are described in <ref> [35] </ref> and [38]. 9 For notational convenience we henceforth drop the implicit conditioning on the model structure, M.
Reference: 36. <author> M. Meila and M. I. Jordan. </author> <title> Learning fine motion by Markov mixtures of experts. </title> <editor> In D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8. </booktitle> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: For real- valued observation vectors, P (Y t jS t ) can be modeled in many different forms, such as a Gaussian, mixture of Gaussians, or a neural network. Like state-space models, HMMs can be augmented to allow for input variables <ref> [7, 4, 36] </ref>. The system then models the conditional distribution of a sequence of output observations given a sequence of input observations. <p> This model can be seen as an extension of the "mixture of experts" architecture for modular learning in neural networks <ref> [22, 7, 36] </ref>. Each state-space model is a linear expert with Gaussian output noise and linear-Gaussian dynamics. The switch state "gates" the outputs of the M state-space models, and therefore plays the role of a gating network with Markovian dynamics [7, 36]. Fig. 7. <p> Each state-space model is a linear expert with Gaussian output noise and linear-Gaussian dynamics. The switch state "gates" the outputs of the M state-space models, and therefore plays the role of a gating network with Markovian dynamics <ref> [7, 36] </ref>. Fig. 7. Bayesian network representation for switching state-space models.
Reference: 37. <author> R. M. Neal. </author> <title> Probabilistic inference using Markov chain monte carlo methods. </title> <type> Technical Report CRG-TR-93-1, </type> <institution> Department of Computer Science, University of Toronto, </institution> <year> 1993. </year>
Reference-contexts: However, it is often easy to set up a Markov chain that will converge to samples from the posterior. One of the simplest methods to achieve this is Gibbs sampling (for a review of Gibbs sampling and other Markov chain Monte Carlo methods, see <ref> [37] </ref>). For a given observation sequence fY t g, Gibbs sampling starts with a random setting of the hidden states fS t g.
Reference: 38. <author> R. M. Neal. </author> <title> Bayesian Learning for Neural Networks. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: ; Y (N) g, each of which can be a vector or time series of vectors, then the likelihood of the data set is: P (Dj; M) = i=1 6 Two approximate methods for integrating over the posterior in the case of neural network models are described in [35] and <ref> [38] </ref>. 9 For notational convenience we henceforth drop the implicit conditioning on the model structure, M.
Reference: 39. <author> R. M. Neal and G. E. Hinton. </author> <title> A new view of the EM algorithm that justifies in-cremental and other variants. </title> <type> Technical report, </type> <institution> Department of Computer Science, University of Toronto, </institution> <year> 1993. </year>
Reference-contexts: global configuration (X; Y ) to be log P (X; Y j), then some readers may notice that the lower bound F (Q; ) L () is the negative of a quantity known in statistical physics as the free energy: the expected energy under Q minus the entropy of Q <ref> [39] </ref>. The Expectation-Maximization (EM) algorithm [3, 10] alternates between maximizing F with respect to Q and , respectively, holding the other fixed.
Reference: 40. <author> G. Parisi. </author> <title> Statistical Field Theory. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, CA, </address> <year> 1988. </year>
Reference-contexts: Q (S t j t ) = k=1 t;k t;k where S (m) K X S t;k = 1: A completely factorized approximation of this kind is often used in statistical physics, where it provides the basis for simple yet powerful mean field approxi <br>- mations to statistical mechanical systems <ref> [40] </ref>. (a) (b) Fig. 8. (a) The completely factorized variational approximation assuming that all the state variables are independent (conditional on the observation sequence). (b) A structured variational approximation assuming that the state variables retain their Markov structure within each chain, but are independent across chains. 23 To make the bound
Reference: 41. <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: if along every undirected path between a node in A and a node in B there is a node D such that: (1) D has converging arrows 3 and neither D nor its descendents are in C, or (2) D does not have converging arrow and D is in C <ref> [41] </ref>. From visual inspection of the graphical model it is therefore easy to infer many independence relations without explicitly grinding through Bayes rule. <p> The absence of arcs in a Bayesian networks implies conditional independence relations which can be exploited to obtain efficient algorithms for computing marginal and conditional probabilities. For singly connected networks, in which the underlying undirected graph has no loops, there exists a general algorithm called belief propagation <ref> [31, 41] </ref>. For multiply connected networks, in which there can be more than one undirected path between any two nodes, there exists a more general algorithm known as the junction tree algorithm [33, 25]. <p> We will deal exclusively with directed graphical models in this paper. 4 texts <ref> [41, 24, 19] </ref> for details. Assume we observe some evidence: the value of some variables in the network. The goal of belief propagation is to update the marginal probabilities of all the variables in the network to incorporate this new evidence.
Reference: 42. <author> L. R. Rabiner and B. H. Juang. </author> <title> An Introduction to hidden Markov models. </title> <journal> IEEE Acoustics, Speech & Signal Processing Magazine, </journal> <volume> 3 </volume> <pages> 4-16, </pages> <year> 1986. </year>
Reference-contexts: The second term, hS t S 0 t1 i, is a matrix containing the joint probability that the HMM was in each of the K 2 pairs of states at times t 1 and t. In the HMM notation of <ref> [42] </ref>, hS t i corresponds to fl t and 9 When learning from a data set containing multiple sequences, this quantity has to be computed separately for each sequence. <p> Occasionally, it is also useful to compute the 15 single most probable state sequence. The solution to this problem is given by the Viterbi algorithm [51], which is also very similar to the forward-backward algorithm except that some of the summations are replaced by maximizations (see <ref> [42] </ref> for a tutorial on HMMs, especially as applied to speech recognition). 5 Beyond Tractable Models Linear-Gaussian state-space models and hidden Markov models provide an interesting starting point for designing dynamic Bayesian networks. However, they suffer from important limitations when it comes to modeling real world time series.
Reference: 43. <author> H. E. Rauch. </author> <title> Solutions to the linear smoothing problem. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 8 </volume> <pages> 371-372, </pages> <year> 1963. </year>
Reference-contexts: It consists of two parts: a forward recursion which uses the observations from Y 1 to Y t , known as the Kalman filter [29], and a backward recursion which uses the observations from Y T to Y t+1 <ref> [43] </ref>. 8 We have already seen that in order to compute the marginal probability of a variable in a Bayesian network one must take into account both the evidence above and below the variable.
Reference: 44. <author> R. Rockafellar. </author> <title> Convex Analysis. </title> <publisher> Princeton University Press, </publisher> <year> 1970. </year>
Reference: 45. <author> L. K. Saul and M. I. Jordan. </author> <title> Mixed memory Markov models. </title> <editor> In D. Madigan and P. Smyth, editors, </editor> <booktitle> Proceedings of the 1997 Conference on Artificial Intelligence and Statistics. </booktitle> <address> Ft. Lauderdale, FL, </address> <year> 1997. </year>
Reference-contexts: This assumption can be relaxed in many ways by introducing coupling between the state variables in a single time step <ref> [45] </ref>. One interesting way to couple the variables is to order them, such that S (m) t depends on S (n) t for 1 n &lt; m.
Reference: 46. <author> L.K. Saul and M. I. Jordan. </author> <title> Exploiting tractable substructures in Intractable networks. </title> <editor> In D.S. Touretzky, M.C. Mozer, and M.E. Hasselmo, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8. </booktitle> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference: 47. <author> R. H. Shumway and D. S. Stoffer. </author> <title> An approach to time series smoothing and forecasting using the EM algorithm. </title> <journal> J. Time Series Analysis, </journal> <volume> 3(4) </volume> <pages> 253-264, </pages> <year> 1982. </year>
Reference-contexts: Z f (X) P (XjY; k ) dX: (20) Then, the M step for C is C X Y t hX t i 0 X hX t X 0 ! 1 Similar M steps can be derived for all the other parameters by taking derivatives of the expected log probability <ref> [47, 11, 15] </ref>. 7 In general we require all terms of the kind hX t i, hX t X 0 t i and hX t X 0 t1 i. <p> Again, equation (27) shifts the mean by an amount proportional to the prediction error X T t1 . We can also recursively compute the covariance across two time steps <ref> [47] </ref> V T t J 0 t+1;t AV t t1 which is initialized V T T;T 1 = (I K T C)AV T1 T 1 .
Reference: 48. <author> P. Smyth. </author> <title> Hidden Markov models for fault detection in dynamic systems. </title> <journal> Pattern Recognition, </journal> <volume> 27(1) </volume> <pages> 149-164, </pages> <year> 1994. </year>
Reference-contexts: The system then models the conditional distribution of a sequence of output observations given a sequence of input observations. HMMs have been applied extensively to problems in speech recognition [28], computational biology [32, 2], and fault detection <ref> [48] </ref>. 4 Learning and Inference A Bayesian approach to learning starts with some a priori knowledge about the model structure|the set of arcs in the Bayesian network|and model parameters.
Reference: 49. <author> P. Smyth, D. Heckerman, and M. I. Jordan. </author> <title> Probabilistic independence networks for hidden Markov probability models. </title> <journal> Neural Computation, </journal> <volume> 9 </volume> <pages> 227-269, </pages> <year> 1997. </year>
Reference-contexts: matrix we obtain P ij / t=2 = t=2 hS t;i S t1;j i t=2 hS t1;j i The necessary expectations are computed using the forward-backward algorithm. 4.6 The forward-backward algorithm The forward-backward algorithm is simply belief propagation applied to the Bayesian network corresponding to a hidden Markov model (see <ref> [49] </ref> for a recent treatment).
Reference: 50. <author> M. A. Tanner and W. H. Wong. </author> <title> The calculation of posterior distributions by data augmentation (with discussion). </title> <journal> Journal of the American Statistical Association, </journal> <volume> 82 </volume> <pages> 528-550, </pages> <year> 1987. </year>
Reference-contexts: to ensure convergence 11 A more Bayesian treatment of the learning problem, in which the parameters are also considered hidden random variables, can be handled by Gibbs sampling by replacing the "M step" with sampling from the conditional distribution of the parameters given the other hidden variables (for example, see <ref> [50] </ref>). 22 some of the dependencies in P . Given this structure, we are free to vary the parameters of Q so as to obtain the tightest possible bound by minimizing (56).
Reference: 51. <author> A. J. </author> <title> Viterbi. Error bounds for convolutional codes and an asymptotically optimal decoding algorithm. </title> <journal> IEEE Trans. Informat. Theory, </journal> <volume> IT-13:260-269, </volume> <year> 1967. </year>
Reference-contexts: Occasionally, it is also useful to compute the 15 single most probable state sequence. The solution to this problem is given by the Viterbi algorithm <ref> [51] </ref>, which is also very similar to the forward-backward algorithm except that some of the summations are replaced by maximizations (see [42] for a tutorial on HMMs, especially as applied to speech recognition). 5 Beyond Tractable Models Linear-Gaussian state-space models and hidden Markov models provide an interesting starting point for designing
Reference: 52. <author> R. S. Zemel. </author> <title> A minimum description length framework for unsupervised learning. </title> <type> Ph.D. Thesis, </type> <institution> Dept. of Computer Science, University of Toronto, Toronto, Canada, </institution> <year> 1993. </year>
Reference-contexts: This static mixture model, without inclusion of the time index and the Markov dynamics, is a factorial parameterization of the standard mixture of Gaussians model that has interest in its own right <ref> [52, 20, 14] </ref>. The model we have just presented extends this by allowing Markov dynamics in the discrete state variables underlying the mixture. 17 Fig. 5.
References-found: 52

