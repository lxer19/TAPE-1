URL: http://www.cs.umass.edu/~dprecup/publications/Sutton-Precup-Singh-ICML98.ps
Refering-URL: http://www-anw.cs.umass.edu/Publications/recent.html
Root-URL: 
Email: rich@cs.umass.edu  dprecup@cs.umass.edu  baveja@cs.colorado.edu  
Title: Intra-Option Learning about Temporally Abstract Actions  
Author: Richard S. Sutton Doina Precup Satinder Singh 
Address: Amherst, MA 01003-4610  Amherst, MA 01003-4610  Boulder, CO 80309-0430  
Affiliation: Department of Computer Science University of Massachusetts  Department of Computer Science University of Massachusetts  Department of Computer Science University of Colorado  
Abstract: Several researchers have proposed modeling temporally abstract actions in reinforcement learning by the combination of a policy and a termination condition, which we refer to as an option. Value functions over options and models of options can be learned using methods designed for semi-Markov decision processes (SMDPs). However, all these methods require an option to be executed to termination. In this paper we explore methods that learn about an option from small fragments of experience consistent with that option, even if the option itself is not executed. We call these methods intra-option learning methods because they learn from experience within an option. Intra-option methods are sometimes much more efficient than SMDP methods because they can use off-policy temporal-difference mechanisms to learn simultaneously about all the options consistent with an experience, not just the few that were actually executed. In this paper we present intra-option learning methods for learning value functions over options and for learning multi-time models of the consequences of options. We present computational examples in which these new methods learn much faster than SMDP methods and learn effectively when SMDP methods cannot learn at all. We also sketch a convergence proof for intra option value learning.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bradtke, S. J. & Duff, M. O. </author> <year> (1995). </year> <title> Reinforcement learning methods for continuous-time Markov decision problems. </title> <booktitle> In Advances in Neural Information Processing Systems 7 (pp. </booktitle> <volume> 393400). </volume> <publisher> MIT Press. </publisher>
Reference-contexts: After the execution of option o is started in state s, we next jump to the state s 0 in which it terminates. Based on this experience, an estimate Q (s; o) of the optimal option-value function is updated. For example, the SMDP version of one-step Q-learning <ref> (Bradtke and Duff, 1995) </ref>, which we call one-step SMDP Q-learning, updates after each option termination by Q (s; o) Q (s; o) + ff r + fl k max Q (s 0 ; o 0 ) Q (s; o) ; where k is the number of time steps elapsing between s <p> In this case, SMDP methods can be ap plied, since all the options are actually executed. We experimented with two SMDP methods: one-step SMDP Q-learning <ref> (Bradtke and Duff, 1995) </ref> and a hierarchical form of Q-learning called macro Q-learning (McGovern, Sutton and Fagg, 1997).
Reference: <author> Dayan, P. & Hinton, G. E. </author> <year> (1993). </year> <title> Feudal reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems 5 (pp. </booktitle> <volume> 271278). </volume> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Dietterich, T. G. </author> <year> (1998). </year> <title> The MAXQ method for hierarchical reinforcement learning. </title> <booktitle> In Proceedings of the Fifteenth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Huber, M. & Grupen, R. A. </author> <year> (1997). </year> <title> A feedback control structure for on-line learning tasks. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <pages> 22(3-4), 303315. </pages>
Reference: <author> Jaakkola, T., Jordan, M. & Singh, S. </author> <year> (1994). </year> <title> On the convergence of stochastic iterative dynamic programming algorithms. </title> <journal> Neural Computation, </journal> <volume> 6(6), </volume> <pages> 11851201. </pages>
Reference: <author> Kaelbling, L. P. </author> <year> (1993). </year> <title> Hierarchical learning in stochastic domains: Preliminary results. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning (pp. </booktitle> <volume> 167173). </volume> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kalmar, Z., Szepesvari, C. & L orincz, A. </author> <year> (1997). </year> <title> Module based reinforcement learning for a real robot. </title> <booktitle> In Proceedings of the Sixth European Workshop on Learning Robots (pp. </booktitle> <pages> 2232). </pages>
Reference: <author> Lin, L.-J. </author> <year> (1993). </year> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University. </institution>
Reference: <author> Mahadevan, S., Marchallek, N., Das, T. K. & Gosavi, A. </author> <year> (1997). </year> <title> Self-improving factory simulation using continuous-time average-reward reinforcement learning. </title> <booktitle> In Proceedings of the Fourteenth International Conference on Machine Learning (pp. </booktitle> <volume> 202 210). </volume> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> McGovern, A., Sutton, R. S. & Fagg, A. H. </author> <year> (1997). </year> <title> Roles of macro-actions in accelerating reinforcement learning. </title> <note> In Grace Hopper Celebration of Women in Computing (pp. 1317). </note>
Reference-contexts: In this case, SMDP methods can be ap plied, since all the options are actually executed. We experimented with two SMDP methods: one-step SMDP Q-learning (Bradtke and Duff, 1995) and a hierarchical form of Q-learning called macro Q-learning <ref> (McGovern, Sutton and Fagg, 1997) </ref>. The difference between the two methods is that, when taking a multi-step option, SMDP Q-learning only updates the value of that option, whereas macro Q-learning also updates the values of the one-step options (actions) that were taken along the way.
Reference: <author> Parr, R. </author> <title> (in preparation). Hierarchical Control and learning for Markov decision processes. </title> <type> PhD thesis, </type> <institution> Berkeley University. </institution> <note> Chapter 3. </note>
Reference: <author> Parr, R. & Russel, S. </author> <year> (1998). </year> <title> Reinforcement learning with hierarchies of machines. </title> <booktitle> In Advances in Neural Information Processing Systems 10. </booktitle> <publisher> MIT Press. </publisher>
Reference: <author> Precup, D., Sutton, R. S. & Singh, S. </author> <year> (1997). </year> <title> Planning with closed-loop macro actions. </title> <booktitle> In Working Notes of the AAAI Fall Symposium '97 on Model-directed Autonomous Systems (pp. </booktitle> <pages> 7076). </pages>
Reference-contexts: Such models are used in planning methods <ref> (e.g., Precup, Sutton, and Singh, 1997, 1998a,b) </ref>. The most straightforward approach to learning the model of an option is to execute the option to termination many times in each state s, recording the resultant next states s 0 , cumulative discounted rewards r, and elapsed times k.
Reference: <author> Precup, D., Sutton, R. S. & Singh, S. </author> <year> (1998a). </year> <title> Multi-time models for temporally abstract planning. </title> <booktitle> In Advances in Neural Information Processing Systems 10. </booktitle> <publisher> MIT Press. </publisher>
Reference: <author> Precup, D., Sutton, R. S. & Singh, S. </author> <year> (1998b). </year> <title> Theoretical results on reinforcement learning with temporally abstract options. </title> <booktitle> In Machine Learning: ECML98. 10th European Conference on Machine Learning, </booktitle> <address> Chem-nitz, Germany, </address> <month> April </month> <year> 1998. </year> <journal> Proceedings (pp. </journal> <volume> 382 393). </volume> <publisher> Springer Verlag. </publisher>
Reference: <author> Puterman, M. L. </author> <year> (1994). </year> <title> Markov Decision Processes: Discrete Stochastic Dynamic Programming. </title> <publisher> Wiley. </publisher>
Reference-contexts: Options are closely related to the actions in a special kind of decision problem known as a semi-Markov decision process, or SMDP <ref> (e.g., see Puterman, 1994) </ref>. Any fixed set of options for a given MDP defines a new SMDP overlaid on the MDP. The appropriate form of model for options, analogous to the r a s and p a ss 0 defined earlier for actions, is known from existing SMDP theory.
Reference: <author> Singh, S. P. </author> <year> (1992a). </year> <title> Reinforcement learning with a hierarchy of abstract models. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence (pp. </booktitle> <address> 202207). </address> <publisher> MIT/AAAI Press. </publisher>
Reference: <author> Singh, S. P. </author> <year> (1992b). </year> <title> Scaling reinforcement learning by learning variable temporal resolution models. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning (pp. </booktitle> <volume> 406415). </volume> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sutton, R. S. </author> <year> (1995). </year> <title> TD models: Modeling the world as a mixture of time scales. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning (pp. </booktitle> <volume> 531539). </volume> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sutton, R. S. & Barto, A. G. </author> <year> (1998). </year> <title> Reinforcement Learning: An Introduction. </title> <publisher> MIT Press. </publisher>
Reference-contexts: This framework is appealing because of its general goal formulation, applicability to stochastic environments, and ability to use sample or simulation models <ref> (e.g., see Sutton and Barto, 1998) </ref>. Extensions of MDPs to semi-Markov decision processes (SMDPs) provide a way to model temporally abstract actions, as we summarize in Sections 3 and 4 below. <p> In particular, if the options are Markov and we are willing to look inside them, then we can use special temporal-difference methods to learn usefully about an option before the option terminates. This is the main idea behind intra-option methods. Intra-option methods are examples of off-policy learning methods <ref> (Sutton and Barto, 1998) </ref> in that they learn about the consequences of one policy while actually behaving according to another, potentially different policy. Intra-option methods can be used to learn simultaneously about many different options from the same experience.
Reference: <author> Sutton, R. S., Precup, D. & Singh, S. </author> <title> (in preparation). Between MDPs and Semi-MDPs: learning, planning, and representing knowledge at multiple temporal scales. </title> <journal> Journal of AI Research. </journal>
Reference: <author> Thrun, S. & Schwartz, A. </author> <year> (1995). </year> <title> Finding structure in reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems 7 (pp. </booktitle> <volume> 385392). </volume> <publisher> MIT Press. </publisher>
References-found: 22

