URL: http://www.eecs.berkeley.edu/~xniu/publications/oonski94.ps
Refering-URL: http://www.eecs.berkeley.edu/~xniu/
Root-URL: 
Title: Sparse Matrix Libraries in C++ for High Performance Architectures  
Author: Jack Dongarra xz Andrew Lumsdaine Xinhui Niu Roldan Pozo Karin Remington 
Affiliation: Oak Ridge National Laboratory University of Tennessee University of Notre Dame Mathematical Sciences Section Dept. of Computer Science Dept. of Computer Science Engineering  
Abstract: We describe an object oriented sparse matrix library in C++ designed for portability and performance across a wide class of machine architectures. Besides simplifying the subroutine interface, the object oriented design allows the same driving code to be used for various sparse matrix formats, thus addressing many of the difficulties encountered with the typical approach to sparse matrix libraries. We also discuss the the design of a C++ library for implementing various iterative methods for solving linear systems of equations. Performance results indicate that the C++ codes are competitive with optimized Fortran. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Barrett et al. </author> <title> Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods. </title> <publisher> SIAM Press, </publisher> <address> Philadelphia, </address> <year> 1994. </year>
Reference-contexts: In such cases only an upper (or lower) triangular portion of the matrix is stored. The trade-off is slightly more complicated kernel operations with a somewhat different pattern of data access. Details of each data storage format are given in <ref> [1] </ref> and [5]. 2.1 Sparse Matrix Operations Our library contains the common computational kernels required for solving linear systems with many direct and iterative methods. <p> Sparse matrices can also be initialized from conventional data and index vectors, thus allowing a universal interface to import data from C or Fortran modules. 3 Iterative Solvers One motivation for this work is that high level matrix algorithms, such as those found in <ref> [1] </ref>, can be easily implemented in C++. For example, consider the preconditioned conjugate gradient algorithm, used to solve Ax = b, with preconditioner M . The comparison between the pseudo-code and the C++ listing appears in Figure 1. <p> This code fragment works for all of the supported sparse storage classes and makes use of data and architecture specific computational kernels (such as the proposed Level 3 Sparse BLAS [4]). Various iterative methods, as described in Bar-rett et al. <ref> [1] </ref>, have been incorporated into the design of IML++, an Iterative Methods Library in C++.
Reference: [2] <author> J. Dongarra, J. Du Croz, I. S. Duff, and S. Ham-marling. </author> <title> A set of level 3 basic linear algebra subprograms. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 16 </volume> <pages> 1-17, </pages> <year> 1990. </year>
Reference-contexts: The internal data structures of these kernels are compatible with the proposed Level 3 Sparse BLAS, thus providing the user with a large software base of Fortran module and application libraries. Just as the dense Level 3 BLAS <ref> [2] </ref> have allowed for higher performance kernels on hierarchical memory architectures, the Sparse BLAS allow vendors to provide optimized routines taking advantage of indirect addressing hardware, registers, pipelining, caches, memory management, and parallelism on their particular architecture.
Reference: [3] <author> I. Duff, R. Grimes, and J. Lewis. </author> <title> Sparse matrix test problems. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 15 </volume> <pages> 1-14, </pages> <year> 1989. </year>
Reference-contexts: It assumes no ordering among nonzero values within each row, but rows are stored in consecutive order. Compressed Column Storage (CCS): Also commonly referred to as the Harwell-Boeing sparse matrix format <ref> [3] </ref>. Similar to CRS, except columns, rather than rows, are stored contiguously. Note that the CCS ordering of A is the same as the CRS of A T . Compressed Diagonal Storage (CDS): Designed primarily for matrices with relatively constant bandwidth, the sub- and super diagonals are stored contiguously. <p> is a (block) diagonal matrix, A and A 0 are sparse matrices, and op (A) is either A or A T . 2.2 Matrix Construction and I/O In dealing with issues of I/O, the C++ library is presently designed to support reading and writing to Harwell-Boeing format sparse matrix files <ref> [3] </ref>. These files are inherently in compressed column storage; however, since sparse matrices in the library can be transformed between various data formats, this is not a severe limitation. File input is embedded as another form of a sparse matrix constructor.
Reference: [4] <author> I. Duff, M. Marrone, and G. Radicati. </author> <title> A proposal for user level sparse BLAS. </title> <type> Technical report, </type> <institution> CERFACS TR/PA/92/85, </institution> <year> 1992. </year>
Reference-contexts: Here the operators such as * and += have been overloaded to work with matrix and vectors formats. This code fragment works for all of the supported sparse storage classes and makes use of data and architecture specific computational kernels (such as the proposed Level 3 Sparse BLAS <ref> [4] </ref>). Various iterative methods, as described in Bar-rett et al. [1], have been incorporated into the design of IML++, an Iterative Methods Library in C++.
Reference: [5] <author> M. A. Heroux. </author> <title> A proposal for a sparse BLAS toolkit. </title> <type> Technical report, </type> <institution> CERFACS TR/PA/92/90, </institution> <year> 1992. </year>
Reference-contexts: In such cases only an upper (or lower) triangular portion of the matrix is stored. The trade-off is slightly more complicated kernel operations with a somewhat different pattern of data access. Details of each data storage format are given in [1] and <ref> [5] </ref>. 2.1 Sparse Matrix Operations Our library contains the common computational kernels required for solving linear systems with many direct and iterative methods.
Reference: [6] <author> J. A. Meijerink and H. A. van der Vorst. </author> <title> An iterative solution method for linear systems of which the coefficient matrix is a symmetric M - matrix. </title> <journal> Math. Comp., </journal> <volume> 31 </volume> <pages> 148-162, </pages> <year> 1977. </year>
Reference-contexts: Although iterative methods have provided much of the motivation for SparseLib++, many of the same operations and design issues apply to direct methods as well. In particular, some of the most popular preconditioners, such as Incomplete LU Factorization (ILU) <ref> [6] </ref>, have components quite similar to direct methods. 4 Efficiency 4.1 Performance To get some measure of the efficiency of our C++ class designs, we tested the performance of our library modules against the public-domain Fortran sparse matrix package SPARSKIT [7].
Reference: [7] <author> Y. Saad. Sparskit: </author> <title> A basic toolkit for sparse matrix computations. </title> <type> Technical report, </type> <institution> NASA Ames Research Center TR 90-20, </institution> <year> 1990. </year>
Reference-contexts: the most popular preconditioners, such as Incomplete LU Factorization (ILU) [6], have components quite similar to direct methods. 4 Efficiency 4.1 Performance To get some measure of the efficiency of our C++ class designs, we tested the performance of our library modules against the public-domain Fortran sparse matrix package SPARSKIT <ref> [7] </ref>. The SPARSKIT package was designed as a "tool kit", with one of its basic goals being to facilitate the transfer of data among researchers in sparse matrix computations, and peak efficiency across machines was not of primary concern.
References-found: 7

