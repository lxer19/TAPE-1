URL: ftp://ftpipr.ira.uka.de/pub/papers/1995/iros95pw.ps.gz
Refering-URL: ftp://ftpipr.ira.uka.de/.public_html/papersna.html
Root-URL: 
Title: Multiple Sensorprocessing for High-Precision Navigation and Environmental Modeling with a mobile Robot  
Author: P. Weckesser, R. Dillmann, M. Elbs and S. Hampel Prof. Dr. U. Rembold, Prof. Dr. R. Dillmann 
Address: 76128 Karlsruhe, Germany  
Affiliation: Institute for Real-Time Computer Systems Robotics  Department for Computer Science, University of Karlsruhe  
Abstract: In this paper an approach to real-time position correction and environmental modeling based on odom-etry, ultrasonic sensing, structured light sensing and active stereo vision (bin- and trinocular) is presented. Odometry provides the robot with a position estimation and with the help of a model of the environment sensor perceptions can be matched to predictions. Ultrasonic sensing is capable of collision avoidance and obstacle detection and so enables navigation in simply structured environments. Model-based image processing allows detecting and classifying natural landmarks in the stereo images uniquely. With only one observation the robot's position and orientation relative to the observed landmark is found precisely. This sensing strategy is used when high precision is necessary for the performance of the navigation task. Finally techniques are described that allow an automatic mapping of an unknown or only partially known environment. 
Abstract-found: 1
Intro-found: 1
Reference: [AAK71] <author> Y.I Abdel-Aziz and H.M. Karara. </author> <title> Direct linear transformation into object space coordinates in close-range photogrammetry. </title> <booktitle> In Symposium on Close-Range Photogram-metry, </booktitle> <institution> Universty of Illinoios at Urbana-champaign, </institution> <month> January </month> <year> 1971. </year>
Reference-contexts: Instead of extracting the camera parameters directly from the camera geometry it is possible with the knowledge of at least 6 corresponding scene (S) and image (I) points to compute the dlt-matrices (M ) (direct linear transformation) for the left and right camera <ref> [AAK71] </ref>. With the dlt-matrix a homogeneous formulation (equation 1) of the perspective transformation is possible. The 12 parameters of the dlt-matrix m i;j contain all 11 camera parameters [Foe90].
Reference: [Agg89] <author> J.K. Aggarwal, </author> <title> editor. Multisensor Fusion for Computer Vision. </title> <booktitle> NATO ASI Series. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1989. </year>
Reference-contexts: Based on this strategy reactive visual sensing, structured light as well as high-level image processing techniques like landmark recognition and automatic mapping of the environment are described. Future work will focus on the fusion of different types of sensor information by using extended Kalman-filtering and online sensor-planning <ref> [Agg89] </ref>. On the other hand there will be a focus on the application of mobile service robots that can supply an operator with all the processed sensor measurements in order to achieve a very reliable teleoperation system with a high degree of autonomy (shared control).
Reference: [Alo90] <author> J. Aloimonos. </author> <title> Purposive and qualitativ vision. </title> <booktitle> In Proc. of Image Understanding Workshop, </booktitle> <pages> pages 816-828, </pages> <year> 1990. </year>
Reference-contexts: cameras the scene-reconstruction of corresponding ob jects can be computed [WH94]. 2. structured light: obstacles can be detected using triangulation techniques with a light source (laser) and a camera (section 2.2). 3. predictive and purposive vision: for a given scene point the image point can be computed (sec tion 5.2, <ref> [Alo90] </ref>). 2.2 Obstacle detection using structured light A very reliable approach to obtain three-dimensional information out of two-dimensional pictures is triangulation with structured light. A wide range of applications has been developed to achieve high accuracy and to expand the field of view [VO90].
Reference: [Bey91] <author> H. Beyer. </author> <title> An introduction to photogram-metric camera calibration. </title> <type> Technical report, </type> <institution> Institute of Geodesy and Photogram-metry, ETH, Zuerich, Schweiz, </institution> <year> 1991. </year>
Reference-contexts: The accuracy of corner detection determines the quality of calibration. In order to improve this poor accuracy in the detection of reference points for the calibration pho-togrammetric methods are applied <ref> [Bey91] </ref>. An automatic calibration technique that detects the reference points in the images with sub-pixel accuracy was developed [WH94]. With this technique for the selection of reference points the accuracy of calibration was improved enormously. The whole calibration procedure is fully automized.
Reference: [DM94] <author> J. Dold and H.-G. </author> <title> Mass. An application of epipolar line intersection in a hybrid close range photogrammetric system. </title> <editor> In Prof. J.F. Fryer, editor, </editor> <booktitle> Close Range Techniques and Machine Vision, </booktitle> <pages> pages 65-70. </pages> <publisher> ISPRS Commision V, </publisher> <year> 1994. </year>
Reference-contexts: With KASTOR being able to perform a real-time edge detection, an edge-based matching algorithm is applied. The high quality of calibration enables a highly precise computation of corresponding endpoints of edges with the use of the epipolar constraint. In <ref> [DM94] </ref> it was shown that corresponding image points can unequivocally be computed if three or more calibrated cameras are used. With a binocular stereo system that is not provided with any knowledge of the environment the matching is very difficult.
Reference: [Foe90] <editor> R. Foehr. Photogrammetrische Erfas-sung raumlicher Informationen aus Video-bildern, </editor> <booktitle> volume 7 of Fortschritte der Robotik. </booktitle> <editor> W. Ameling and M. Weck, </editor> <year> 1990. </year>
Reference-contexts: With the dlt-matrix a homogeneous formulation (equation 1) of the perspective transformation is possible. The 12 parameters of the dlt-matrix m i;j contain all 11 camera parameters <ref> [Foe90] </ref>.
Reference: [Gen94] <author> V. Gengenbach. </author> <title> Einsatz von Ruckkopplugen in der Bildverarbeitung bei einem Hand-Auge-System zur automa-tischen Demontage. </title> <type> PhD thesis, </type> <institution> University of Karlsruhe, </institution> <year> 1994. </year>
Reference-contexts: In the following the camera model is introduced which is used for the calibration (figure 2). The camera is characterized by its image plane (ccd-chip) and the position of the optical center C. Altogether there are 12 camera parameters in this camera model <ref> [Gen94] </ref>. Instead of extracting the camera parameters directly from the camera geometry it is possible with the knowledge of at least 6 corresponding scene (S) and image (I) points to compute the dlt-matrices (M ) (direct linear transformation) for the left and right camera [AAK71].
Reference: [LDW95] <author> I.S. Lin, R. Dillmann, and F. Wallner. </author> <title> An advanced telerobotic control system for mobile robots. </title> <editor> In Prof. U. Rembold, editor, IAS, </editor> <year> 1995. </year>
Reference-contexts: This data-rate is sufficient as the whole sensor information is processed on-board. Only parametric information about the environment and commands to the robot are transmitted via radio. An operator can teleoperate PRIAMOS and KASTOR with a six degrees of freedom space-mouse. <ref> [LDW95] </ref> 2 The active vision system KASTOR KASTOR [WW94] consists of two cameras, mounted on a platform with motor-controlled tilt and turn. Zoom and focus as well as the vergence of each camera unit are equipped with motors.
Reference: [Sch92] <author> C. Schmid. </author> <title> Auto-calibration of cameras by direct observation of objects. </title> <type> Master's thesis, </type> <institution> University of Karlsruhe, Institute for Real-Time Control Systems and Robotics, University of Grenoble, LIFIA, </institution> <year> 1992. </year>
Reference-contexts: In <ref> [Sch92] </ref> a cube is used as reference object. The corners of the cube can be located with a corner operator or by the intersection of the edges of the cube. Experimental results have shown that the accuracy that can be reached by this method is not very high (2 pixels).
Reference: [VO90] <author> P. Vuylsteke and A. </author> <title> Oosterlinck. Range image acquisition with a single binary-encoded light pattern. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 12(2) </volume> <pages> 148-164, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: A wide range of applications has been developed to achieve high accuracy and to expand the field of view <ref> [VO90] </ref>. There is a wide range of commercial systems, mostly designed to operate in known environments with well defined light sources. The system introduced here has to operate on changing floor coverings and under changing light conditions.
Reference: [WD94] <author> P. Weckesser and R. Dillmann. </author> <title> Accuracy of scene reconstruction with an active stereo vision system using motorized zoom lenses. In Prof. D.P. Casasent, editor, Intelligent Robots and Computer Vision XIII: 3D Vision, Product Inspection, and Active Vision, </title> <booktitle> pages 470 - 481. SPIE, </booktitle> <year> 1994. </year>
Reference-contexts: Then the calibration procedure can be performed for this camera setup. In any case sub-pixel accuracy for the detection of the reference points is achieved <ref> [WD94] </ref>. As a result it is possible to change the optical parameters and come back to a pre-calibrated position due to the high precision of the mechanical construction of the system [Wil93].
Reference: [WGD95] <author> F. Wallner, R. Graf, and R. Dillmann. </author> <title> Real-time map refinement by fusing sonar and active stereo-vision. </title> <booktitle> In IEEE International Conference on Robotics and Automation, </booktitle> <address> Nagoya, </address> <year> 1995. </year>
Reference-contexts: PRIAMOS is equipped with four Mecanum wheels, which enable holonome travelling with three degrees of freedom. All these degrees of freedom can be combined. A multisensor system supports the vehicle with odometric, sonar, and visual information. The sensor system and its use is extensively described in <ref> [WGD95] </ref>. The odometry sensors provide an position and orientation-estimation and the sonar sensors are mainly used for collision avoidance, obstacle detection and mapping of simply structured environments. The image-processing and the camera-head control system are basic compo nents of the optical sensor system.
Reference: [WH94] <author> P. Weckesser and G. Hetzel. </author> <title> Photogram-metric calibration methods for an active stereo vision system. </title> <editor> In Prof. A. Borkowsky and Prof. J. Crowley, editors, </editor> <booktitle> Intelligent Robotic Systems (IRS), </booktitle> <pages> pages 430-436, </pages> <year> 1994. </year>
Reference-contexts: The accuracy of corner detection determines the quality of calibration. In order to improve this poor accuracy in the detection of reference points for the calibration pho-togrammetric methods are applied [Bey91]. An automatic calibration technique that detects the reference points in the images with sub-pixel accuracy was developed <ref> [WH94] </ref>. With this technique for the selection of reference points the accuracy of calibration was improved enormously. The whole calibration procedure is fully automized. <p> The accuracy of the calibration used will always be in the subpixel dimension. The described calibration technique and equation 1 are used for the following purposes: 1. stereo-reconstruction: by using two or more cameras the scene-reconstruction of corresponding ob jects can be computed <ref> [WH94] </ref>. 2. structured light: obstacles can be detected using triangulation techniques with a light source (laser) and a camera (section 2.2). 3. predictive and purposive vision: for a given scene point the image point can be computed (sec tion 5.2, [Alo90]). 2.2 Obstacle detection using structured light A very reliable approach
Reference: [Wil93] <author> R. Wilson. </author> <title> Modeling and calibration of au-tomized zoom lenses. </title> <type> PhD thesis, </type> <address> CMU, Pittsburgh, USA, </address> <year> 1993. </year>
Reference-contexts: In any case sub-pixel accuracy for the detection of the reference points is achieved [WD94]. As a result it is possible to change the optical parameters and come back to a pre-calibrated position due to the high precision of the mechanical construction of the system <ref> [Wil93] </ref>. Because of tolerances in the lenses the best results can be achieved if the positions are always adjusted from the same side. The accuracy of the calibration used will always be in the subpixel dimension.
Reference: [WW94] <author> P. Weckesser and F. Wallner. </author> <title> Calibrating the active vision system KASTOR for real-time robot navigation. </title> <editor> In Prof. J.F. Fryer, editor, </editor> <booktitle> Close Range Techniques and Machine Vision, </booktitle> <pages> pages 430-436. </pages> <publisher> ISPRS Com-mision V, </publisher> <year> 1994. </year>
Reference-contexts: This data-rate is sufficient as the whole sensor information is processed on-board. Only parametric information about the environment and commands to the robot are transmitted via radio. An operator can teleoperate PRIAMOS and KASTOR with a six degrees of freedom space-mouse. [LDW95] 2 The active vision system KASTOR KASTOR <ref> [WW94] </ref> consists of two cameras, mounted on a platform with motor-controlled tilt and turn. Zoom and focus as well as the vergence of each camera unit are equipped with motors. A third passive camera is added to the system in order to improve the reliability of the vision system.
Reference: [WWD95] <author> P. Weckesser, F. Wallner, and R. Dillmann. </author> <title> Position correction of a mobile robot using predictive vision. </title> <editor> In U. Rembold and R. Dillamnn, editors, </editor> <booktitle> International conference on Intelligent Autonomous Systems. </booktitle> <address> IAS-4, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: From this results a position correction that allows the navigation of the robot through a narrow door. This is decribed in detail in <ref> [WWD95] </ref>. 5.2.1 Vision-based position correction In the following the task of position correction is illustrated in another example. The robot received a position estimation that enables it to make a planned perception of the area where a pillar is expected.
Reference: [ZF92] <author> Z. Zhang and O. Faugeras. </author> <title> 3D Dynamic Scene Analysis. </title> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: Figure 8 shows the indoor environment PRI-AMOS operates in. Significant natural structures in this environment are used as landmrks to support the robot's navigation. Several different objects are used as landmarks (doors, pillars, : : : ) to perform a position correction <ref> [ZF92] </ref> during longer navigation tasks. 5.1 Generic models of landmarks In order to recognize a landmark an adequate model of this landmark has to exist in the robot's internal map. Again a basically edge-based description for the landmarks is used. The landmarks are generically modeled out of edges.
References-found: 17

