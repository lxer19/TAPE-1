URL: http://www.sdsc.edu/~tbailey/papers/ismb96.ps
Refering-URL: 
Root-URL: 
Title: The megaprior heuristic for discovering protein sequence patterns  
Author: Timothy L. Bailey and Michael Gribskov 
Keyword: sequence modeling; Dirichlet priors; expectation maximization; machine learning; protein motifs; hidden Markov models; unsupervised learning; sequence alignment, multiple  
Address: P.O. Box 85608 San Diego, California 92186-9784  
Affiliation: San Diego Supercomputer Center  
Note: From: Proceedings of the Fourth International Conference on Intelligent Systems for Molecular Biology  AAAI Press  
Email: ftbailey, gribskovg@sdsc.edu  
Date: June, 1996  
Abstract: Several computer algorithms for discovering patterns in groups of protein sequences are in use that are based on fitting the parameters of a statistical model to a group of related sequences. These include hidden Markov model (HMM) algorithms for multiple sequence alignment, and the MEME and Gibbs sampler algorithms for discovering motifs. These algorithms are sometimes prone to producing models that are incorrect because two or more patterns have been combined. The statistical model produced in this situation is a convex combination (weighted average) of two or more different models. This paper presents a solution to the problem of convex combinations in the form of a heuristic based on using extremely low variance Dirichlet mixture priors as part of the statistical model. This heuristic, which we call the megaprior heuristic, increases the strength (i.e., decreases the variance) of the prior in proportion to the size of the sequence dataset. This causes each column in the final model to strongly resemble the mean of a single component of the prior, regardless of the size of the dataset. We describe the cause of the convex combination problem, analyze it mathematically, motivate and describe the implementation of the megaprior heuristic, and show how it can effectively eliminate the problem of convex combinations in protein sequence pattern discovery. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Timothy L. Bailey and Charles Elkan. </author> <title> Unsupervised learning of multiple motifs in biopolymers using EM. </title> <booktitle> Machine Learning, </booktitle> <address> 21(1-2):51-80, </address> <month> October </month> <year> 1995. </year>
Reference: <author> Timothy L. Bailey and Charles Elkan. </author> <title> The value of prior knowledge in discovering motifs with MEME. </title> <booktitle> In Proceedings of the Third International Conference on Intelligent Systems for Molecular Biology, </booktitle> <pages> pages 21-29. </pages> <publisher> AAAI Press, </publisher> <year> 1995. </year>
Reference: <author> Timothy L. Bailey. </author> <title> Separating mixtures using megapriors. </title> <type> Technical Report CS96-471, </type> <institution> http://www.sdsc.edu/~/tbailey/papers.html, Department of Computer Science, University of Cal-ifornia, </institution> <address> San Diego, </address> <month> January </month> <year> 1996. </year>
Reference: <author> Amos Bairoch. </author> <title> The PROSITE database, its status in 1995. </title> <journal> Nucleic Acids Research, </journal> <volume> 24(1) </volume> <pages> 189-196, </pages> <year> 1995. </year>
Reference: <author> P. Baldi, Y. Chauvin, T. Hunkapiller, and M. A. Mc-clure. </author> <title> Hidden Markov models of biological primary sequence information. </title> <booktitle> Proc. </booktitle> <institution> Natl. Acad. Sci. USA, </institution> <month> 91(3) </month> <pages> 1059-1063, </pages> <year> 1994. </year>
Reference: <author> Michael Brown, Richard Hughey, Anders Krogh, I. Saira Mian, Kimmen Sjolander, and David Haus-sler. </author> <title> Using Dirichlet mixture priors to derive hidden Markov models for protein families. </title> <booktitle> In Intelligent Systems for Molecular Biology, </booktitle> <pages> pages 47-55. </pages> <publisher> AAAI Press, </publisher> <year> 1993. </year>
Reference: <author> A. P. Dempster, N. M. Laird, and D. B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> 39(1) </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: Learning the parameters of a multi-component model is difficult due to local optima in the likelihood surface. To minimize this problem, MEME learns the informative components of the motif model one-at-a-time. Using the expectation maximization algorithm (EM) <ref> (Dempster et al. 1977) </ref>, MEME repeatedly fits a two-component mixture model to the subsequences generated from the data. We would like the algorithm to converge to the model shown in Fig. 3 (or a similar one modeling the all "b" component).
Reference: <author> Sean R. Eddy. </author> <title> Multiple alignment using hidden Markov models. </title> <booktitle> In Proceedings of the Third International Conference on Intelligent Systems for Molecular Biology, </booktitle> <pages> pages 114-120. </pages> <publisher> AAAI Press, </publisher> <year> 1995. </year>
Reference: <author> Michael Gribskov, Roland Luthy, and David Eisen-berg. </author> <title> Profile analysis. </title> <booktitle> Methods in Enzymology, </booktitle> <volume> 183 </volume> <pages> 146-159, </pages> <year> 1990. </year>
Reference: <author> A. Krogh, M. Brown, I. S. Mian, K. Sjolander, and D. Haussler. </author> <title> Hidden Markov models in computational biology: Applications to protein modeling. </title> <journal> Journal of Molecular Biology, </journal> <volume> 235(5) </volume> <pages> 1501-1531, </pages> <year> 1994. </year>
Reference: <author> Charles E. Lawrence, Stephen F. Altschul, </author> <title> Mark S. </title>
Reference: <author> Boguski, Jun S. Liu, Andrew F. Neuwald, and John C. Wootton. </author> <title> Detecting subtle sequence signals: </title>
References-found: 12

