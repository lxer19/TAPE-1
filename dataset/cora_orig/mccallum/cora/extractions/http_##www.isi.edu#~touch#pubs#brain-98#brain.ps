URL: http://www.isi.edu/~touch/pubs/brain-98/brain.ps
Refering-URL: http://www.isi.edu/~touch/pubs/brain-98/
Root-URL: http://www.isi.edu
Email: (touch@isi.edu)  
Title: application of high-performance switches. Such switches form the backbone of ATM networks, and the backplane
Author: Joe Touch 
Affiliation: USC Information Sciences Institute  
Date: May 2, 1997  
Note: 1: Introduction High-speed network research has been focusing on the  2: Processing requirements A typical alternative to these intra-router/switch processors is to use hosts as in-link processors (Figure 2). This  
Abstract: Current networking research efforts, such as Active Networks and high-performance router development, rely on emerging high-speed switching technology, but can also require programmable processing resources that switches lack. Here we present our vision of a board for realizing this resource for switches, such as ATM. The BRAIN board is a switch- and link- independent processor that provides a pipelined CPU as well as a prototyping area for custom hardware. It takes advantage of recent commodity workstation bus architecture together with existing network interfaces to provide a port processor independent of the underlying networking technology. Here we present our vision of BRAIN, a Board for Realizing Active Intelligence in Networks. Our goal is to design and implement a number of BRAIN boards, to distribute them to the networking research community, and to coordinate their use to enable high-performance programmable networking research. The BRAIN board is a programmable processor intended to augment switching fabrics, although it does not rely on a particular link or network technology. Instead, BRAIN takes advantage of recent advances in commodity workstations, together with available network interface cards, to provide a processing resource that is not limited by host backplane bandwidth contention. This document describes our vision of the BRAIN board. First we discuss the processing requirements of network switches. We then present our design goals, and describe the BRAIN board architecture. We compare the BRAIN to alternate designs, and finally discuss our target for implementing and using the BRAIN boards. Active Networks programs require programmable processing resources inside the network, typically at routers [17]. By contrast, current router designs minimize processor interaction with packets, using dedicated hardware to perform common-case routing [11], [16]. Even emerging router designs based on general-purpose CPUs, e.g., BBNs Multigigabit Router, assume the CPU is dedicated to the common-case algorithm to achieve high performance [2]. Switch-based designs eliminate programmable processing, providing only programmable signalling and configuration management. In each case, a general-purpose programmable processing resource is lacking. Programmable processing can be added inside a router or switch by placing a CPU near each input or output link (Figure 1). This requires the modification of the switch hardware, which is typical proprietary. A notable recent exception is the NSF-sponsored Gigabit Network Technology Distribution Program at the Univ. of Washington in St. Louis, wherein the switch architecture is openly documented and public [9]. The GNT program will implement and distribute a number (40) of switchkits, including a high-speed ATM switch and some PCI host interfaces. However, even in the GNT case, custom, architecture-specific hardware is required to support line-rate processing of cell- or packet-level data. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Advanced Micro Devices AMD Mach 465 part information, &lt;http://www.vantis.com/products/overview/c17466d.html&gt; </institution>
Reference-contexts: A separate on-board memory supports scratchpad workspace, program storage, and additional slots that can be used for value-added memory, such as CAMs. The PLD socket supports a high-level programmable device, e.g., an AMD Mach 465 <ref> [1] </ref>. Earlier versions of this chip were used to implement single-clock IP checksum operations; this version will allow complex logic functions to be implemented in hardware, rather than in an often cumbersome sequence of software operations [20]. FIGURE 4.
Reference: [2] <institution> BBN Multigigabit Router, </institution> <note> information &lt;http:// www.bbn.com/&gt; </note>
Reference-contexts: By contrast, current router designs minimize processor interaction with packets, using dedicated hardware to perform common-case routing [11], [16]. Even emerging router designs based on general-purpose CPUs, e.g., BBNs Multigigabit Router, assume the CPU is dedicated to the common-case algorithm to achieve high performance <ref> [2] </ref>. Switch-based designs eliminate programmable processing, providing only programmable signalling and configuration management. In each case, a general-purpose programmable processing resource is lacking. Programmable processing can be added inside a router or switch by placing a CPU near each input or output link (Figure 1).
Reference: [3] <author> Bhattacharjee, B., Calvert, K., Zegura, E., </author> <title> An Architecture for Active Networking, </title> <type> Technical Report CIT-CC-96-20, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <year> 1996. </year> <title> NIC PCI #1 Net in CPU RAM NIC PCI#2 Net out BRAIN Local PCI System PCI cardedge slot slot 5 </title>
Reference-contexts: FIGURE 1. Intra-router/switch processing vs. host based processing FIGURE 2. PC-based processing Emerging network capabilities such as encryption, or Active Networks packet data functions such as transcod-ing or data fusion, tax the capabilities of a host-based solution <ref> [3] </ref>, [4], [19]. For processing-bound functions, multiprocessor hosts may suffice. However, when the entire packet data must be processed inside the host, communication bandwidth can be insufficient.
Reference: [4] <author> Cromp, R., Campbell, W., and Short, Jr., N. </author> <title> (An intelligent information fusion system for handling the archiving and querying of terabyte-sized spatial databases, </title> <booktitle> International Space Year Conference on Earth and Space Science Information Systems, </booktitle> <address> Pasadena, CA, </address> <month> Feb. </month> <year> 1992. </year>
Reference-contexts: FIGURE 1. Intra-router/switch processing vs. host based processing FIGURE 2. PC-based processing Emerging network capabilities such as encryption, or Active Networks packet data functions such as transcod-ing or data fusion, tax the capabilities of a host-based solution [3], <ref> [4] </ref>, [19]. For processing-bound functions, multiprocessor hosts may suffice. However, when the entire packet data must be processed inside the host, communication bandwidth can be insufficient. <p> In RAM PCI chipset PCI chipset Router Host BW 600 200 Mb 2-cross PCI GNT Myrinet 1-cross PCI ATM (others) (BRAIN) 3 general pipelined processing resource, which can be used for network processing, or can also support other pipelined I/O processing, e.g., for data ingest processing prior to disk storage <ref> [4] </ref>. 3.1: Internal architecture Internally, the BRAIN board is a pipelined processor (Figure 4). It uses separate PCI interfaces for data input and output, and relies on a multi-Harvard central processor, currently indicated 1 as the Texas Instruments TMSC40 DSP [18].
Reference: [5] <institution> Cyclone Microsystems PCI-914 Intelligent I/O Controller board information, &lt;http://www.cyclone.com/pci914.htm&gt; </institution>
Reference-contexts: In the latter case, the BRAIN board can be used for data ingest between a network interface and permanent storage, or rendering graphics prior to display. 4: Prior work There is only one other known available dual-PCI controller board, the Cyclone Microsystems PCI-914 Intelligent I/O Controller <ref> [5] </ref>. The IIOC contains an Intel i960 processor which uses a single-Harvard architecture, and so does not support simultaneous input and output. Its primary PCI is a cardedge, but its secondary PCI supports only IQ Modules, an open but non-standard PCI interface.
Reference: [6] <editor> Dalton, C., Watson, G., et al., Afterburner, </editor> <booktitle> IEEE Network, V7 N4, </booktitle> <month> July </month> <year> 1993, </year> <pages> pp. 36-43. </pages>
Reference-contexts: Its primary PCI is a cardedge, but its secondary PCI supports only IQ Modules, an open but non-standard PCI interface. As a result, the Cyclone board is not suited to general use with generic existing PCI NIC cards. There have been many notable prior outboard network processor boards <ref> [6] </ref>, [7], [10], [12]. In most cases, these boards relieve the host CPU of handling interrupts, scheduling data transfers, or packet processing, i.e., they reduce the processing load on the CPU.
Reference: [7] <author> Davie, B., </author> <title> The Architecture and Implementation of a High-Speed Host Interface, </title> <journal> IEEE JSAC, V11 N2, </journal> <month> Feb. </month> <year> 1993, </year> <pages> pp. 228-239. </pages>
Reference-contexts: Its primary PCI is a cardedge, but its secondary PCI supports only IQ Modules, an open but non-standard PCI interface. As a result, the Cyclone board is not suited to general use with generic existing PCI NIC cards. There have been many notable prior outboard network processor boards [6], <ref> [7] </ref>, [10], [12]. In most cases, these boards relieve the host CPU of handling interrupts, scheduling data transfers, or packet processing, i.e., they reduce the processing load on the CPU.
Reference: [8] <institution> Dual-processor motherboard specification (Intel), </institution> <note> see &lt;http://www.intel.com/design/pro/datashts/242016.htm&gt; </note>
Reference-contexts: There are two possible configurations, which will be compared as part of the proposed work. In the first, the input and output PCI interfaces of the BRAIN board plug directly into a dual-PCI host (Figure 5). In a dual-PCI host, a bridge chip isolates the two PCIs <ref> [8] </ref>. Network input passes over PCI #1 into the BRAIN board, and out onto PCI #2 to the network output. Because the address spaces of the input and output side of the BRAIN board are distinct, traffic never crosses the bridge chip.
Reference: [9] <institution> Gigabit Network Technology (GNT) Distribution Program, information &lt;http://www.arl.wustl.edu/~jst/gigatech/Res-Dist.html&gt; </institution>
Reference-contexts: This requires the modification of the switch hardware, which is typical proprietary. A notable recent exception is the NSF-sponsored Gigabit Network Technology Distribution Program at the Univ. of Washington in St. Louis, wherein the switch architecture is openly documented and public <ref> [9] </ref>. The GNT program will implement and distribute a number (40) of switchkits, including a high-speed ATM switch and some PCI host interfaces. However, even in the GNT case, custom, architecture-specific hardware is required to support line-rate processing of cell- or packet-level data.
Reference: [10] <author> Kalmanek, C., et al., </author> <title> Xunet 2: Lessons from an Early Wide-Area ATM Testbed, </title> <journal> IEEE/ACM Transactions on Networking, V5 N2, </journal> <month> Feb. </month> <year> 1997, </year> <month> pp40-55. </month>
Reference-contexts: As a result, the Cyclone board is not suited to general use with generic existing PCI NIC cards. There have been many notable prior outboard network processor boards [6], [7], <ref> [10] </ref>, [12]. In most cases, these boards relieve the host CPU of handling interrupts, scheduling data transfers, or packet processing, i.e., they reduce the processing load on the CPU.
Reference: [11] <author> Minshall, G., Lyon, T., and Huston, L., </author> <title> IP Switching and Gigabit Routers, </title> <journal> IEEE Communications Magazine, </journal> <month> Jan. </month> <year> 1997. </year>
Reference-contexts: 1: Introduction High-speed network research has been focusing on the application of high-performance switches. Such switches form the backbone of ATM networks, and the backplane of recent routers <ref> [11] </ref>. Alone, switches are insufficient for either routers or emerging programmable networking systems, such as Active Networks (AN) [17]. These systems require a high-performance processor that can keep pace with gigabit link rates. Here we present our vision of BRAIN, a Board for Realizing Active Intelligence in Networks. <p> By contrast, current router designs minimize processor interaction with packets, using dedicated hardware to perform common-case routing <ref> [11] </ref>, [16]. Even emerging router designs based on general-purpose CPUs, e.g., BBNs Multigigabit Router, assume the CPU is dedicated to the common-case algorithm to achieve high performance [2]. Switch-based designs eliminate programmable processing, providing only programmable signalling and configuration management. In each case, a general-purpose programmable processing resource is lacking.
Reference: [12] <author> Mockapetris, P., </author> <title> Communication Environments for Local Networks, </title> <type> Ph.D. Dissertation, </type> <institution> UC Irvine, </institution> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: As a result, the Cyclone board is not suited to general use with generic existing PCI NIC cards. There have been many notable prior outboard network processor boards [6], [7], [10], <ref> [12] </ref>. In most cases, these boards relieve the host CPU of handling interrupts, scheduling data transfers, or packet processing, i.e., they reduce the processing load on the CPU.
Reference: [13] <institution> Myricom home pages, &lt;http://www.myri.com/&gt; </institution>
Reference-contexts: The 32-bit PCI backplane in current hosts supports bandwidths of 1.056 Gbps, supporting OC-3 (155 Mbps) links, but cannot support a fully-loaded OC-12 (622 Mbps) or either GNT (1.2 Gbps) or Myrinet links (640 Mbps-1.28 Gbps <ref> [13] </ref>), because the PCI bus must support the full line rate in both directions simultaneously (Figure 3, 2-cross). The BRAIN board supports full PCI bandwidth for link processing, supporting a most of the link bandwidths of these new technologies (Figure 3, 1-cross).
Reference: [14] <author> Parulka, G. </author> <title> ATM Tap Project, </title> <type> Wash U. </type> <institution> in St. Louis, pending project. </institution>
Reference-contexts: This analysis also assumes 100% utilization of the PCI bus, and that no other peripherals use that bus, if this assumption is false the available bandwidth is further limited. A recent example uses a host and host-interface to monitor a 1.2 Gbps (OC-24) link <ref> [14] </ref>. In this system, only packet headers and cell counts can be logged, due to the bandwidth limits of the host. FIGURE 3. Configuration bandwidth limits 2.1: The GNT Program The Univ. of Washington in St.
Reference: [15] <institution> PCI Local Bus Specification, Rev. 2.0, PCI Special Interest Group, Hillsboro, </institution> <address> OR, </address> <year> 1993. </year>
Reference-contexts: The signal properties of PCI support this configuration inside a single host easily, which is why the newly-available dual-PCI configuration enables this design <ref> [15] </ref>. FIGURE 5. BRAIN uses dual PCI, existing NICs In the second possible configuration, the BRAIN board is controlled via a single-PCI host, and uses two on-board slots, one for each NIC, and the local PCI is connected to the card edge to the host PCI (Figure 6).
Reference: [16] <author> Rekhter, Y., et al., </author> <title> Tag Switching Architecture Overview, </title> <note> (working draft), </note> <month> Jan. </month> <year> 1997. </year>
Reference-contexts: By contrast, current router designs minimize processor interaction with packets, using dedicated hardware to perform common-case routing [11], <ref> [16] </ref>. Even emerging router designs based on general-purpose CPUs, e.g., BBNs Multigigabit Router, assume the CPU is dedicated to the common-case algorithm to achieve high performance [2]. Switch-based designs eliminate programmable processing, providing only programmable signalling and configuration management. In each case, a general-purpose programmable processing resource is lacking.
Reference: [17] <author> Tennenhouse, D.L., et al., </author> <title> A Survey of Active Network Research, </title> <journal> IEEE Communications Magazine, </journal> <month> Jan </month> <year> 1997, </year> <pages> pp. 80-86. </pages>
Reference-contexts: 1: Introduction High-speed network research has been focusing on the application of high-performance switches. Such switches form the backbone of ATM networks, and the backplane of recent routers [11]. Alone, switches are insufficient for either routers or emerging programmable networking systems, such as Active Networks (AN) <ref> [17] </ref>. These systems require a high-performance processor that can keep pace with gigabit link rates. Here we present our vision of BRAIN, a Board for Realizing Active Intelligence in Networks. <p> We then present our design goals, and describe the BRAIN board architecture. We compare the BRAIN to alternate designs, and finally discuss our target for implementing and using the BRAIN boards. 2: Processing requirements Active Networks programs require programmable processing resources inside the network, typically at routers <ref> [17] </ref>. By contrast, current router designs minimize processor interaction with packets, using dedicated hardware to perform common-case routing [11], [16]. Even emerging router designs based on general-purpose CPUs, e.g., BBNs Multigigabit Router, assume the CPU is dedicated to the common-case algorithm to achieve high performance [2].
Reference: [18] <institution> Texas Instruments TI TMS320C40 part information, &lt;http:/ </institution> <note> /WWW.TI.COM/sc/docs/dsps/products/c4x/&gt; </note>
Reference-contexts: It uses separate PCI interfaces for data input and output, and relies on a multi-Harvard central processor, currently indicated 1 as the Texas Instruments TMSC40 DSP <ref> [18] </ref>. The processor supports two 32-bit full-rate DMA channels, as well as multiple low-speed asynchronous DMA channels which are used here to access an on-board PLD development area and a local PCI bus for disk access, etc.
Reference: [19] <editor> Touch, J., et al., </editor> <booktitle> ISIs High-Performance Networking Research, submission to the GNT Workshop, </booktitle> <address> St. Louis MO, </address> <month> June </month> <year> 1996 </year> <month> &lt;http://emperor.arl.wustl.edu/~jst/gigatech/ whitepapers/touch.ps&gt; </month>
Reference-contexts: FIGURE 1. Intra-router/switch processing vs. host based processing FIGURE 2. PC-based processing Emerging network capabilities such as encryption, or Active Networks packet data functions such as transcod-ing or data fusion, tax the capabilities of a host-based solution [3], [4], <ref> [19] </ref>. For processing-bound functions, multiprocessor hosts may suffice. However, when the entire packet data must be processed inside the host, communication bandwidth can be insufficient.
Reference: [20] <author> Touch J., Parham, B., </author> <title> Implementing the Internet Checksum in Hardware, </title> <journal> RFC-1936, </journal> <volume> ISI, </volume> <month> April, </month> <year> 1996. </year> <note> &lt;ftp:// ftp.isi.edu/in-notes/rfc1936.txt&gt; </note>
Reference-contexts: The PLD socket supports a high-level programmable device, e.g., an AMD Mach 465 [1]. Earlier versions of this chip were used to implement single-clock IP checksum operations; this version will allow complex logic functions to be implemented in hardware, rather than in an often cumbersome sequence of software operations <ref> [20] </ref>. FIGURE 4. Internal design 3.2: Interface architecture The BRAIN board uses an existing PCI-based host as a controller. There are two possible configurations, which will be compared as part of the proposed work.
References-found: 20

