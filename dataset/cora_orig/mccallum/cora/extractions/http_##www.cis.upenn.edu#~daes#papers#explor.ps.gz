URL: http://www.cis.upenn.edu/~daes/papers/explor.ps.gz
Refering-URL: http://www.cis.upenn.edu/~daes/papers.html
Root-URL: 
Email: Email: daes@linc.cis.upenn.edu  Email: lgg@linc.cis.upenn.edu  
Phone: Phone: 1-215-573-6286 Fax: 1-215-573-9247  
Title: Characterizing the benefits of model-based vs. direct-control learning in exploration  
Author: Dale Schuurmans Lloyd Greenwald zx 
Keyword: associative reinforcement learning, PAC learning, generalization in reinforcement learning  
Note: Also: NEC Research Institute, Princeton, NJ Mailing address: Dale Schuurmans, Institute for Research in  
Address: Philadelphia, PA 19104  3401 Walnut Street, Suite 400A, Philadelphia, PA 19104-6228.  Fellow  
Affiliation: Institute for Research in Cognitive Science  Department of Computer and Information Science University of Pennsylvania  Cognitive Science, Univer- sity of Pennsylvania,  Also: Veterans Health Administration Medical Informatics  
Abstract: We consider the problem of learning a good control function from reinforcement. Although much studied, the issue of how (or even whether) to use prior knowledge to scale reinforcement learning up to real-world problems remains an important open issue. We investigate the inherent data-complexity of reinforcement learning when prior knowledge is expressed either as a set of possible reward models (model-based learning) or as a set of possible controllers (direct-control learning). Our key observation is that the distinction between these two approaches arises solely from the partial evaluation nature of the task (i.e. that each training instance evaluates only a subset of the controllers), and does not arise from temporal credit assignment or exploration/exploitation concerns per se. This allows us to compare these two learning approaches in a very simple batch setting. Our results show that it is not always beneficial to pursue a purely model-based approach, even though this can yield significant benefits at times. We show that in our simple setting there is a hybrid learning strategy which combines the benefits of both model-based and direct-control learning, and dominates the performance of both. This work contributes preliminary steps towards understanding how prior domain knowledge can be properly exploited to improve the data-efficiency of (a simple version of) reinforcement learning, and reveals the subtleties and difficulties that must be faced in general. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Ackley and M. Littman. </author> <title> Generalization and scaling in reinforcement learning. </title> <booktitle> In NIPS-2, </booktitle> <pages> pages 550557, </pages> <year> 1990. </year>
Reference-contexts: Although this seems like a severely limiting assumption, note that it does not eliminate the distinction between model-based and direct-control learning (which is our central concern here). In fact, there is a precedent for considering control learning scenarios which make this assumptionspecifically in work on associative reinforcement learning <ref> [2, 12, 1] </ref> and neural networks for learning one shot set- point control [18]. This is also a reasonable model of information retrieval tasks which involve reinforcement signals obtained from relevance feedback. <p> This problem has been studied under our independent state assumption in the guise of associative reinforcement learning <ref> [13, 12, 1, 2, 30, 31] </ref>, learning in behavior based robotics [15], and neural networks for learning direct inverse control [5, 11, 14, 17].
Reference: [2] <author> A. Barto and P. Anandan. </author> <title> Pattern-recognizing stochastic learning automata. </title> <journal> IEEE Trans. Sys. Man Cyb., </journal> <volume> 15(3):360375, </volume> <year> 1985. </year>
Reference-contexts: In a RL problem, by contrast, each training instance hs; a; ri evaluates only a subset of the candidate control functions c : S ! A; namely, those that would have taken action a in state s <ref> [2, 13] </ref>. The reward r says nothing directly about what would have been obtained if a controller took a different action a 0 in situation s. <p> Although this seems like a severely limiting assumption, note that it does not eliminate the distinction between model-based and direct-control learning (which is our central concern here). In fact, there is a precedent for considering control learning scenarios which make this assumptionspecifically in work on associative reinforcement learning <ref> [2, 12, 1] </ref> and neural networks for learning one shot set- point control [18]. This is also a reasonable model of information retrieval tasks which involve reinforcement signals obtained from relevance feedback. <p> This problem has been studied under our independent state assumption in the guise of associative reinforcement learning <ref> [13, 12, 1, 2, 30, 31] </ref>, learning in behavior based robotics [15], and neural networks for learning direct inverse control [5, 11, 14, 17]. <p> But this cannot work in general. If the actions are not sampled uniformly at random, then an inferior action can easily demonstrate a higher expected reward. Barto and Anandan <ref> [2] </ref> reduce the two-action case to supervised learning by making the assumption that m (s; :a) = 1 m (s; a). But this just amounts to making explicit assumptions about the reward model.
Reference: [3] <author> S. Ben-David, N. Cesa-Bianchi, D. Haussler, and P. </author> <title> Long. Characterizations of learnability for classes of f0; ::; ng-valued functions. </title> <institution> J. Comput. Sys. Sci., 50:7486, </institution> <year> 1995. </year>
Reference-contexts: strength of the prior knowledge encoded by C?) Here, to measure the complexity of C we note that, in the two action case, it suffices to use the standard notion of VC- dimension. 1 (For the multi-action case we need to resort to the generalized notions of VC-dimension developed by <ref> [3, 22] </ref>.) Given this observation we can note that, as in standard PAC learning, there is a simple generic learning strategy that achieves near optimal data-efficiency. Procedure DC-Learn. Proceed in a series of rounds. <p> But this just amounts to making explicit assumptions about the reward model. We address such a model-based approach to learning in the next section. 7 C with probability at least 1 ffi (using the standard PAC bounds from VC-theory, and its generalizations <ref> [4, 3] </ref>). Return any controller that has always been successful every time it has been tried. The idea behind this procedure is simply to accumulate evidence across the entire space of possible controllers to reliably distinguish the good candidates from bad.
Reference: [4] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. </author> <title> Learnability and the VapnikChervonenkis dimension. </title> <journal> Journal of the ACM, </journal> <volume> 36(4):929965, </volume> <year> 1989. </year>
Reference-contexts: But this just amounts to making explicit assumptions about the reward model. We address such a model-based approach to learning in the next section. 7 C with probability at least 1 ffi (using the standard PAC bounds from VC-theory, and its generalizations <ref> [4, 3] </ref>). Return any controller that has always been successful every time it has been tried. The idea behind this procedure is simply to accumulate evidence across the entire space of possible controllers to reliably distinguish the good candidates from bad.
Reference: [5] <author> D. DeMers and K. Kreutz-Delgado. </author> <title> Inverse kinematics of dextrous manipulators. </title> <editor> In O. Omidvar and P. van der Smagt, editors, </editor> <booktitle> Neural Systems for Robotics. </booktitle> <publisher> Academic Press, </publisher> <year> 1997. </year>
Reference-contexts: This problem has been studied under our independent state assumption in the guise of associative reinforcement learning [13, 12, 1, 2, 30, 31], learning in behavior based robotics [15], and neural networks for learning direct inverse control <ref> [5, 11, 14, 17] </ref>. Although there is a large and interesting literature on this approach, we will investigate a particularly simple version of the problem in order to gain a concrete understanding of its nature.
Reference: [6] <author> T. Dietterich. </author> <title> Machine learning research: Four current directions. </title> <journal> AI Magazine, </journal> <volume> 18(4):97136, </volume> <year> 1997. </year>
Reference-contexts: The real importance of this assumption is that it allows us to investigate model-based versus direct-control learning without having to deal with the issues of temporal credit assignment [26, 27] and the effect of state-dynamics on exploration <ref> [24, 28, 13, 6] </ref>. Formally then, we assume the successive states are drawn independently according to some stationary distribution P S , and therefore characterize the world by a state distribution P S and the reward model m w .
Reference: [7] <author> A. Ehrenfeucht, D. Haussler, M. J. Kearns, and L. Valiant. </author> <title> A general lower bound on the number of examples needed for learning. Information and Computation, </title> <address> 82:247261, </address> <year> 1989. </year>
Reference-contexts: is simply to fix a set of shattered states s 1 ; :::; s d and choose a difficult distribution such that every state must be seen to guarantee a good controller, and yet one state is left out with significant probability if the expected sample size is too small <ref> [7, 25] </ref>. Although this generic direct-control learning procedure does not seem practical, it has interesting advantages over some well-known learning algorithms in the literature. For example, consider the IEKDNF algorithm of [12], which learns controllers c : f0; 1g n ! f0; 1g that can be expressed as k-DNF formulae. <p> Proposition 4. Any learning procedure requires at least * (d + ln 1 training examples to correctly solve the batch MB-learning problem. (Follows from <ref> [7] </ref>.) Clearly this is not a serious proposal for a procedure that one would actually use in real reinforcement learning applications.
Reference: [8] <author> C.-N. Fietcher. </author> <title> Efficient reinforcement learning. </title> <booktitle> In COLT-94, </booktitle> <year> 1994. </year>
Reference-contexts: Some immediate directions are to (1) extend the theoretical model to stochastic rewards, (2) investigate concrete algorithmic issues in specific tasks, and most importantly (3) to introduce dependence between actions and states and extend our learning strategies and theoretical analysis to deal with state-transition dynamics and temporal credit assignment issues <ref> [8] </ref>.
Reference: [9] <author> S. Goldman and M. Kearns. </author> <title> On the complexity of teaching. </title> <booktitle> In COLT-91, </booktitle> <pages> pages 303314, </pages> <year> 1991. </year>
Reference-contexts: We do this by appealing to the notion of universal identification sequence developed in <ref> [9, 10] </ref> which measures the difficulty of identifying an arbitrary boolean function from a given class of functions. <p> Similarly, a random (uniform) search strategy takes jAj trials in expectation to find a single successful action. 6 This is because the data-complexity of exactly identifying the reward model is lower bounded by the teaching dimension of the class of models M <ref> [9] </ref>. So, for example, the teaching dimension of the last column in the matrix is n 1 (easily determined using the technique of [9]). <p> successful action. 6 This is because the data-complexity of exactly identifying the reward model is lower bounded by the teaching dimension of the class of models M <ref> [9] </ref>. So, for example, the teaching dimension of the last column in the matrix is n 1 (easily determined using the technique of [9]). This is a lower bound on the smallest number of actions that need to be observed in order to guarantee that this column is distinguished from all others in the matrix. 12 model-based learning or naive direct-control learning are not sensible approaches to this problem.
Reference: [10] <author> S. Goldman, M. Kearns, and R. Schapire. </author> <title> Exact identification of circuits using fixed points of amplification functions. </title> <booktitle> In FOCS-90, </booktitle> <pages> pages 193202, </pages> <year> 1990. </year>
Reference-contexts: We do this by appealing to the notion of universal identification sequence developed in <ref> [9, 10] </ref> which measures the difficulty of identifying an arbitrary boolean function from a given class of functions.
Reference: [11] <author> M. Jordan and D. Rumelhart. </author> <title> Forward models: Supervised learning with a distal teacher. </title> <booktitle> Cognitive Science, </booktitle> <address> 16(3):307354, </address> <year> 1992. </year> <month> 15 </month>
Reference-contexts: This problem has been studied under our independent state assumption in the guise of associative reinforcement learning [13, 12, 1, 2, 30, 31], learning in behavior based robotics [15], and neural networks for learning direct inverse control <ref> [5, 11, 14, 17] </ref>. Although there is a large and interesting literature on this approach, we will investigate a particularly simple version of the problem in order to gain a concrete understanding of its nature. <p> This model-based approach to learning control (under the independent state assumption) is often implicitly addressed in work on associative reinforcement learning [21, 13] and in learning forward models for control <ref> [19, 20, 11] </ref>. 3 As before, to develop a concrete understanding of this problem and to facilitate comparisons with the previous approach, we consider a particularly simple version of the task.
Reference: [12] <author> L. Kaelbling. </author> <title> Associative reinforcement learning: Functions in k-DNF. </title> <booktitle> Machine Learning, </booktitle> <address> 15:279, </address> <year> 1994. </year>
Reference-contexts: Although this seems like a severely limiting assumption, note that it does not eliminate the distinction between model-based and direct-control learning (which is our central concern here). In fact, there is a precedent for considering control learning scenarios which make this assumptionspecifically in work on associative reinforcement learning <ref> [2, 12, 1] </ref> and neural networks for learning one shot set- point control [18]. This is also a reasonable model of information retrieval tasks which involve reinforcement signals obtained from relevance feedback. <p> This problem has been studied under our independent state assumption in the guise of associative reinforcement learning <ref> [13, 12, 1, 2, 30, 31] </ref>, learning in behavior based robotics [15], and neural networks for learning direct inverse control [5, 11, 14, 17]. <p> Although this generic direct-control learning procedure does not seem practical, it has interesting advantages over some well-known learning algorithms in the literature. For example, consider the IEKDNF algorithm of <ref> [12] </ref>, which learns controllers c : f0; 1g n ! f0; 1g that can be expressed as k-DNF formulae. The class of k-DNF controllers clearly has finite VC-dimension, so the procedure DC-learn is guaranteed to reliably learn a near-optimal controller using a reasonable sample size. However, the IEKDNF procedure in [12] <p> <ref> [12] </ref>, which learns controllers c : f0; 1g n ! f0; 1g that can be expressed as k-DNF formulae. The class of k-DNF controllers clearly has finite VC-dimension, so the procedure DC-learn is guaranteed to reliably learn a near-optimal controller using a reasonable sample size. However, the IEKDNF procedure in [12] is not. In fact, there are simple cases where IEKDNF is guaranteed to converge to a bad controller, even when a perfect 8 controller exists in the assumed class C kDNF . <p> Assume the distribution P S puts probability 1=3 on state h0; 1i and 2=3 on h1; 0i, and the reward model is such that m w (s; 1) = 1 and m w (s; 0) = 0 for all states s. Then, using the notation of <ref> [12] </ref>, we let er (x i ; a) denote the expected reward of a specific controller c which takes actions c (hx 1 ; x 2 i) = a if x i = 1 and :a if x i = 0.
Reference: [13] <author> L. Kaelbling, M. Littman, and A. Moore. </author> <title> Reinforcement learning: A survey. </title> <type> JAIR, </type> <institution> 4:237285, </institution> <year> 1996. </year>
Reference-contexts: In a RL problem, by contrast, each training instance hs; a; ri evaluates only a subset of the candidate control functions c : S ! A; namely, those that would have taken action a in state s <ref> [2, 13] </ref>. The reward r says nothing directly about what would have been obtained if a controller took a different action a 0 in situation s. <p> The real importance of this assumption is that it allows us to investigate model-based versus direct-control learning without having to deal with the issues of temporal credit assignment [26, 27] and the effect of state-dynamics on exploration <ref> [24, 28, 13, 6] </ref>. Formally then, we assume the successive states are drawn independently according to some stationary distribution P S , and therefore characterize the world by a state distribution P S and the reward model m w . <p> This problem has been studied under our independent state assumption in the guise of associative reinforcement learning <ref> [13, 12, 1, 2, 30, 31] </ref>, learning in behavior based robotics [15], and neural networks for learning direct inverse control [5, 11, 14, 17]. <p> This model-based approach to learning control (under the independent state assumption) is often implicitly addressed in work on associative reinforcement learning <ref> [21, 13] </ref> and in learning forward models for control [19, 20, 11]. 3 As before, to develop a concrete understanding of this problem and to facilitate comparisons with the previous approach, we consider a particularly simple version of the task.
Reference: [14] <author> J. Kinderman and A. Linden. </author> <title> Inversion of neural networks by gradient descent. </title> <booktitle> Parallel Computing, </booktitle> <address> 14:277286, </address> <year> 1990. </year>
Reference-contexts: This problem has been studied under our independent state assumption in the guise of associative reinforcement learning [13, 12, 1, 2, 30, 31], learning in behavior based robotics [15], and neural networks for learning direct inverse control <ref> [5, 11, 14, 17] </ref>. Although there is a large and interesting literature on this approach, we will investigate a particularly simple version of the problem in order to gain a concrete understanding of its nature.
Reference: [15] <author> P. Maes and R. Brooks. </author> <title> Learning to coordinate behaviors. </title> <booktitle> In AAAI-90, </booktitle> <pages> pages 796802, </pages> <year> 1990. </year>
Reference-contexts: This problem has been studied under our independent state assumption in the guise of associative reinforcement learning [13, 12, 1, 2, 30, 31], learning in behavior based robotics <ref> [15] </ref>, and neural networks for learning direct inverse control [5, 11, 14, 17]. Although there is a large and interesting literature on this approach, we will investigate a particularly simple version of the problem in order to gain a concrete understanding of its nature.
Reference: [16] <author> S. Mahadevan and L. Kaelbling. </author> <title> The NSF workshop on reinforcement learning: Summary and observations. </title> <journal> AI Magazine, </journal> <volume> 17, </volume> <year> 1996. </year>
Reference-contexts: However, the issue of how (or even whether) to use domain knowledge and prior constraints in reinforcement learning is still an open research issueif not the central one in this area <ref> [23, 16] </ref>. In this paper we contribute to the understanding of the role prior knowledge plays in reinforcement and control learning problems, and how we might best exploit it.
Reference: [17] <author> B. </author> <title> Mel. Further explorations in visually guided reaching: Making MURPHY smarter. </title> <booktitle> In NIPS-1, </booktitle> <pages> pages 348355, </pages> <year> 1989. </year>
Reference-contexts: This problem has been studied under our independent state assumption in the guise of associative reinforcement learning [13, 12, 1, 2, 30, 31], learning in behavior based robotics [15], and neural networks for learning direct inverse control <ref> [5, 11, 14, 17] </ref>. Although there is a large and interesting literature on this approach, we will investigate a particularly simple version of the problem in order to gain a concrete understanding of its nature.
Reference: [18] <author> W. Miller, R. Sutton, and P. Werbos, </author> <title> editors. Neural Networks for Control. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: In fact, there is a precedent for considering control learning scenarios which make this assumptionspecifically in work on associative reinforcement learning [2, 12, 1] and neural networks for learning one shot set- point control <ref> [18] </ref>. This is also a reasonable model of information retrieval tasks which involve reinforcement signals obtained from relevance feedback.
Reference: [19] <author> A. Moore. </author> <title> Acquisition of dynamic control knowledge for a robotic manipulator. </title> <booktitle> In ICML-90, </booktitle> <pages> pages 244252, </pages> <year> 1990. </year>
Reference-contexts: This model-based approach to learning control (under the independent state assumption) is often implicitly addressed in work on associative reinforcement learning [21, 13] and in learning forward models for control <ref> [19, 20, 11] </ref>. 3 As before, to develop a concrete understanding of this problem and to facilitate comparisons with the previous approach, we consider a particularly simple version of the task. <p> However, even in this respect, convergence to a suboptimal controller is damaging. A correct batch learning procedure like DC-learn, after a short training phase, would always be guaranteed to achieve better expected returns than IEKDNF could ever achieve on this problem. 3 Note that Moore <ref> [19, 20] </ref> does not consider a restricted class of models per se, but his work makes a strong model-based assumption that similar actions in similar states will yield similar outcomes.
Reference: [20] <author> A. Moore. </author> <title> Fast, robust adaptive control by learning only forward models. </title> <booktitle> In NIPS-4, </booktitle> <pages> pages 571578, </pages> <year> 1992. </year>
Reference-contexts: This model-based approach to learning control (under the independent state assumption) is often implicitly addressed in work on associative reinforcement learning [21, 13] and in learning forward models for control <ref> [19, 20, 11] </ref>. 3 As before, to develop a concrete understanding of this problem and to facilitate comparisons with the previous approach, we consider a particularly simple version of the task. <p> However, even in this respect, convergence to a suboptimal controller is damaging. A correct batch learning procedure like DC-learn, after a short training phase, would always be guaranteed to achieve better expected returns than IEKDNF could ever achieve on this problem. 3 Note that Moore <ref> [19, 20] </ref> does not consider a restricted class of models per se, but his work makes a strong model-based assumption that similar actions in similar states will yield similar outcomes.
Reference: [21] <author> P. Munroe. </author> <title> A dual back-propagation scheme for scalar reward learning. In Cogn. </title> <journal> Sci. Soc. Conf. </journal> <volume> 87, </volume> <pages> pages 165176, </pages> <year> 1987. </year>
Reference-contexts: This model-based approach to learning control (under the independent state assumption) is often implicitly addressed in work on associative reinforcement learning <ref> [21, 13] </ref> and in learning forward models for control [19, 20, 11]. 3 As before, to develop a concrete understanding of this problem and to facilitate comparisons with the previous approach, we consider a particularly simple version of the task.
Reference: [22] <author> B. Natarajan. </author> <title> Probably approximate learning of sets and functions. </title> <journal> SIAM J. Comput., </journal> <volume> 20(2):328351, </volume> <year> 1991. </year>
Reference-contexts: strength of the prior knowledge encoded by C?) Here, to measure the complexity of C we note that, in the two action case, it suffices to use the standard notion of VC- dimension. 1 (For the multi-action case we need to resort to the generalized notions of VC-dimension developed by <ref> [3, 22] </ref>.) Given this observation we can note that, as in standard PAC learning, there is a simple generic learning strategy that achieves near optimal data-efficiency. Procedure DC-Learn. Proceed in a series of rounds.
Reference: [23] <author> S. Russell and P. Norvig. </author> <title> Artificial Intelligence: A Modern Approach. </title> <publisher> Prentice-Hall, </publisher> <year> 1995. </year>
Reference-contexts: However, the issue of how (or even whether) to use domain knowledge and prior constraints in reinforcement learning is still an open research issueif not the central one in this area <ref> [23, 16] </ref>. In this paper we contribute to the understanding of the role prior knowledge plays in reinforcement and control learning problems, and how we might best exploit it.
Reference: [24] <author> R. Schapire. </author> <title> The Design and Analysis of Efficient Learning Algorithms. </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: The real importance of this assumption is that it allows us to investigate model-based versus direct-control learning without having to deal with the issues of temporal credit assignment [26, 27] and the effect of state-dynamics on exploration <ref> [24, 28, 13, 6] </ref>. Formally then, we assume the successive states are drawn independently according to some stationary distribution P S , and therefore characterize the world by a state distribution P S and the reward model m w .
Reference: [25] <author> D. Schuurmans and R. Greiner. </author> <title> Sequential PAC learning. </title> <booktitle> In COLT-95, </booktitle> <pages> pages 377384, </pages> <year> 1995. </year>
Reference-contexts: is simply to fix a set of shattered states s 1 ; :::; s d and choose a difficult distribution such that every state must be seen to guarantee a good controller, and yet one state is left out with significant probability if the expected sample size is too small <ref> [7, 25] </ref>. Although this generic direct-control learning procedure does not seem practical, it has interesting advantages over some well-known learning algorithms in the literature. For example, consider the IEKDNF algorithm of [12], which learns controllers c : f0; 1g n ! f0; 1g that can be expressed as k-DNF formulae.
Reference: [26] <author> R. Sutton. </author> <title> Learning to predict by the method of temporal differences. </title> <booktitle> Machine Learning, </booktitle> <address> 3:944, </address> <year> 1988. </year>
Reference-contexts: This is also a reasonable model of information retrieval tasks which involve reinforcement signals obtained from relevance feedback. The real importance of this assumption is that it allows us to investigate model-based versus direct-control learning without having to deal with the issues of temporal credit assignment <ref> [26, 27] </ref> and the effect of state-dynamics on exploration [24, 28, 13, 6]. Formally then, we assume the successive states are drawn independently according to some stationary distribution P S , and therefore characterize the world by a state distribution P S and the reward model m w .
Reference: [27] <author> R. Sutton. </author> <title> TD models: modeling the world at a mixture of time scales. </title> <booktitle> In ICML-95, </booktitle> <pages> pages 531539, </pages> <year> 1995. </year>
Reference-contexts: This is also a reasonable model of information retrieval tasks which involve reinforcement signals obtained from relevance feedback. The real importance of this assumption is that it allows us to investigate model-based versus direct-control learning without having to deal with the issues of temporal credit assignment <ref> [26, 27] </ref> and the effect of state-dynamics on exploration [24, 28, 13, 6]. Formally then, we assume the successive states are drawn independently according to some stationary distribution P S , and therefore characterize the world by a state distribution P S and the reward model m w .
Reference: [28] <author> C. Watkins. </author> <title> Models of Delayed Reinforcement Learning. </title> <type> PhD thesis, </type> <institution> Psychology Department, Cambridge University, </institution> <year> 1989. </year>
Reference-contexts: One of the main issues that faces reinforcement and control learning is scaling up. Although several general purpose learning algorithms such as Q-learning <ref> [28, 29] </ref> have been developed for reinforcement learning problems, getting these algorithms to solve significant real world problems ultimately boils down to effectively exploiting prior knowledge and constraints, just as in regular prediction learning. <p> The real importance of this assumption is that it allows us to investigate model-based versus direct-control learning without having to deal with the issues of temporal credit assignment [26, 27] and the effect of state-dynamics on exploration <ref> [24, 28, 13, 6] </ref>. Formally then, we assume the successive states are drawn independently according to some stationary distribution P S , and therefore characterize the world by a state distribution P S and the reward model m w .
Reference: [29] <author> C. Watkins and P. </author> <title> Dayan. </title> <journal> Q-learning. Machine Learning Journal, </journal> <volume> 8(3/4), </volume> <year> 1992. </year>
Reference-contexts: One of the main issues that faces reinforcement and control learning is scaling up. Although several general purpose learning algorithms such as Q-learning <ref> [28, 29] </ref> have been developed for reinforcement learning problems, getting these algorithms to solve significant real world problems ultimately boils down to effectively exploiting prior knowledge and constraints, just as in regular prediction learning.
Reference: [30] <author> R. Williams. </author> <title> On the use of backpropagation in associative reinforcement learning. </title> <booktitle> In ICNN-88, </booktitle> <pages> pages 263270, </pages> <year> 1988. </year>
Reference-contexts: This problem has been studied under our independent state assumption in the guise of associative reinforcement learning <ref> [13, 12, 1, 2, 30, 31] </ref>, learning in behavior based robotics [15], and neural networks for learning direct inverse control [5, 11, 14, 17].
Reference: [31] <author> R. Williams. </author> <title> Simple statistical gradient-following algorithms for connectionist reinforcement learning. </title> <booktitle> Machine Learning, </booktitle> <address> 8(3):229256, </address> <year> 1992. </year>
Reference-contexts: This problem has been studied under our independent state assumption in the guise of associative reinforcement learning <ref> [13, 12, 1, 2, 30, 31] </ref>, learning in behavior based robotics [15], and neural networks for learning direct inverse control [5, 11, 14, 17].
Reference: [32] <author> J. Wyatt. </author> <title> Exploration and inference in learning from reinforcement. </title> <type> PhD thesis, </type> <institution> University of Edinburgh, Dept. of A.I., </institution> <year> 1997. </year> <month> 16 </month>
Reference-contexts: We note that this distinction is made purely in terms of the data-complexity of learning, and is not necessarily tied to the issue of computational efficiency (as commonly suggested, for example in <ref> [32] </ref>). Finally, we consider the question of whether there exist hybrid learning strategies which are neither purely model-based nor controller-based, but which can 3 strictly dominate the performance of both.
References-found: 32

