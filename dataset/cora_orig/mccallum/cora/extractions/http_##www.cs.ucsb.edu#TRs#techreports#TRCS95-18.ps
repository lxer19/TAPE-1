URL: http://www.cs.ucsb.edu/TRs/techreports/TRCS95-18.ps
Refering-URL: http://www.cs.ucsb.edu/TRs/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: fibarra,pedro,marting@cs.ucsb.edu  
Title: On the Complexity of Commutativity Analysis  
Author: Oscar Ibarra, Pedro Diniz and Martin Rinard 
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California, Santa Barbara  
Abstract: Two operations commute if they generate the same result regardless of the order in which they execute. Commutativity is an important property | commuting operations enable significant optimizations in the fields of parallel computing, optimizing compilers, parallelizing compilers and database concurrency control. Algorithms that statically decide if operations commute can be an important component of systems in these fields because they enable the automatic application of these optimizations. In this paper we define the commutativity decision problem and establish its complexity for a variety of basic instructions and control constructs. Although deciding commuta-tivity is, in general, undecidable or computationally intractable, we believe that efficient algorithms exist that can solve many of the cases that arise in practice.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aho, J. Hopcroft and J. Ullman. </author> <title> Data Structures and Algorithms. </title> <publisher> Addison-Wesley, </publisher> <year> 1982. </year>
Reference-contexts: If neither method writes an instance variable the other method either reads or writes, the method are independent. Clearly this algorithm can be implemented in polynomial time with respect to both the method's length and the number of instance variables using set representation techniques <ref> [1] </ref>. 4.3 Reduction A common operation in many applications is to reduce many values into one accumulator variable by applying a commutative and associative operator such as + or fl.
Reference: [2] <author> A. Aho, R. Sethi and J. Ullman. </author> <booktitle> Compilers: Principles, Techniques and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference: [3] <author> J. Barnes and P. Hut. </author> <title> A hierarchical O(NlogN) force-calculation algorithm. </title> <booktitle> Nature, </booktitle> <pages> pages 446-449, </pages> <month> December </month> <year> 1976. </year>
Reference-contexts: These applications include the Barnes-Hut <ref> [3] </ref> hierarchical N-body algorithm and the molecular dynamics code Water 1 . In the Barnes-Hut application, the algorithm maintains and performs multiple traversals on a spatial pointer-based tree data structure.
Reference: [4] <author> M. Berry et al. </author> <title> The PERFECT Club Benchmars: Effective Performance Evaluation of Supercomputers. </title> <type> Technical Report CSRD-827, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, </institution> <month> May, </month> <year> 1989. </year>
Reference-contexts: In the Barnes-Hut application, the algorithm maintains and performs multiple traversals on a spatial pointer-based tree data structure. Each such traversal reads data from nodes in the tree and 1 A FORTRAN language variant of the Water code used in our analysis can be found in the PERFECT Benchmark <ref> [4] </ref> set of applications under the name of MDG. 10 generates commuting updates to the body nodes at the leaves. The Water application evaluates forces and potentials in a system of water molecules in the liquid state. The molecules are organized in an array data structure.
Reference: [5] <author> U. Banerjee, R. Eigenmann, A. Nicolau, and D. Padua. </author> <title> Automatic program parallelization. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2) </volume> <pages> 211-243, </pages> <month> February </month> <year> 1993. </year>
Reference: [6] <author> W. Blume and R. </author> <title> Eigenmann Symbolic Range Propagation. </title> <booktitle> In Proceedings of the Ninth IEEE Int. Parallel Processing Symposium, </booktitle> <pages> pp. 357-363, </pages> <month> April, </month> <year> 1995 </year>
Reference-contexts: Despite this worst-case scenario we, do not expect the analysis to exhibit this exponential behavior in practice. Other research that uses related symbolic analysis techniques supports this hypothesis <ref> [6] </ref>. 4.4.1 Straight-line Codes For straight-line codes the algorithm constructs the instance variable expressions by symbolically executing each statement. At each point in the program each variable is bound to an expression denoting its value at that point.
Reference: [7] <author> P. Cousot and R. Cousot. </author> <title> Abstract Interpretation: A unified lattice model for static analysis of programs by construction or approximation of fixpoints. </title> <booktitle> In Proceedings of the Fourth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Los Angeles, CA, </address> <month> January </month> <year> 1977. </year>
Reference: [8] <author> P. Diniz. and M. Rinard. </author> <title> Exploiting Commutating Operations in Parallelizing Serial Programs Department of Computer Science, </title> <type> Technical Report TRCS95-11, </type> <month> January, </month> <year> 1995 </year>
Reference: [9] <author> A. Fisher and A. Ghuloum. </author> <title> Parallelizing complex scans and reductions. </title> <booktitle> In Proceedings of the SIGPLAN '94 Conference on Program Language Design and Implementation, </booktitle> <address> Orlando, FL, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: The algorithm builds the table by recursively traversing the outer conditional expressions to identify the minimal conjunctions of basic terms that select each maximal conditional-free subexpression as the value of the original expression. It is possible to further simplify the table using logic minimization techniques as proposed in <ref> [9] </ref>. To compare two expressions for equality the algorithm performs a simple recursive isomorphism test. The algorithm checks that the condition tables have the same indexes and that corresponding subexpressions in the table are isomorphic. 4.4.3 Array Variables It is possible to integrate array variables in the symbolic execution framework.
Reference: [10] <author> E. Gurari and O. Ibarra. </author> <title> The Complexity of the Equivalence Problem for Simple Programs. </title> <journal> In Journal of the ACM, </journal> <volume> 28(3) </volume> <pages> 535-560, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: Commutativity over zero input is polynomial-time decidable for C-programs with no nesting of loops, where t denotes a "forward" label not in the scope of any do-loop (do-statements however can be labeled) <ref> [10] </ref>. 3.3 PSPACE-Hard Problems If in result #10 we allow "forward" labels to be inside the do-loops, then the problem becomes PSPACE-hard. In fact, we can show the following by coding the computation of a deterministic linear-bounded automaton. Result #11.
Reference: [11] <author> M. Garey and D. Johnson. </author> <title> Computer and Intractability. A Guide to the Theory of NP-Completeness. </title> <publisher> Freeman, </publisher> <address> San Francisco, </address> <year> 1979. </year>
Reference-contexts: In this paper we focus on the theoretical aspects of commutativity analysis. We identify classes of programs for which commutativ-ity analysis is undecidable, PSPACE-hard, NP-hard, polynomial, and probabilistically polynomial-time decidable (see <ref> [11] </ref> for definitions and motivations). For some cases we also show the class of programs to be complete for the corresponding complexity class. The results presented here rely on known complexity results from the area of theoretical computer science. They serve two purposes.
Reference: [12] <author> L. Hendren, J. Hummel, and A. Nicolau. </author> <title> Abstractions for recursive pointer data structures: improving the analysis and transformation of imperative programs. </title> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Program Language Design and Implementation, </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference: [13] <author> O. Ibarra and S. Moran. </author> <title> Probabilistic Algorithms for Deciding Equivalence of Straight-Line Programs. </title> <journal> In Journal of the ACM, </journal> <volume> 30(1) </volume> <pages> 217-228, </pages> <month> January </month> <year> 1983. </year>
Reference-contexts: Result #15. Let C = fx 1; x y + z; x y z; x y fl zg. Over D = finite subset of Z with at least two elements, commutativity with ZERO is NP-hard for C-programs <ref> [13] </ref>. Note that the non-commutativity problem for the above programs is clearly in NP. We also have the following result. Result #16. Let C = fx x fl c; x x=2g and input domain D = N . <p> Result #18. Let C = fx 1; x y + z; x y z; x y fl zg. Over D = Z or D = R, the commutativity problem for C-programs is decidable in probabilistic polynomial time <ref> [13] </ref>. One can show that, if in result #15, D has exactly one element, then the commutativity problem is probabilistically decidable in polynomial time [13]. 7 4 Practical Algorithms for Commutativity Analysis Despite the undecidable/intractable results presented in the previous section, we believe it is possible to develop algorithms that can <p> Over D = Z or D = R, the commutativity problem for C-programs is decidable in probabilistic polynomial time <ref> [13] </ref>. One can show that, if in result #15, D has exactly one element, then the commutativity problem is probabilistically decidable in polynomial time [13]. 7 4 Practical Algorithms for Commutativity Analysis Despite the undecidable/intractable results presented in the previous section, we believe it is possible to develop algorithms that can recognize many of the commuting operations that occur in practice.
Reference: [14] <author> O. Ibarra and B. Leininger. </author> <title> On the Simplification and Equivalence Problems for Straight-Line Programs. </title> <journal> In Journal of the ACM, </journal> <volume> 30(3) </volume> <pages> 641-656, </pages> <month> July </month> <year> 1983. </year>
Reference-contexts: The proofs are similar to the ones in <ref> [14] </ref> for proving the undecidability of program equivalence. Result #1. Let C = fx 1; x x + y; x x=yg and input domain D = Z. It is undecidable to determine, given a C-program P with three input variables and nine auxiliary variables, whether it commutes with ONE.
Reference: [15] <author> O. Ibarra and B. Leininger. </author> <title> On the Zero-Inequivalence Problem for Loop Programs. </title> <journal> In Journal of Computer and System Sciences , 26(1) </journal> <pages> 47-64, </pages> <month> February </month> <year> 1983. </year>
Reference-contexts: Result #14. Let C = fx 1; x x + y; x x yg. Over D = N , commutativity with ZERO is NP-hard for C-programs with one input variable and two auxiliary variables <ref> [15] </ref>. One can strengthen this result by requiring the use of only one auxiliary variable, but the instruction set is now C = fx 1; x 2x; x x + 1; x x + y; x x y; x y xg.
Reference: [16] <author> O. Ibarra, B. Leininger, and S. Moran. </author> <title> On the Complexity of Simple Arithmetic Expressions. </title> <booktitle> In Theoretical Computer Science , 19 </booktitle> <pages> 17-28, </pages> <year> 1982. </year>
Reference-contexts: It is NP-hard to decide, given two C-programs P 1 and P 2 and a positive integer m, whether P 1 and P 2 commute for all non-negative integer values of x &lt; m <ref> [16] </ref>. In this result, the complexity is with respect to the maximum of the length of P 1 , length of P 2 , and length of the binary representation of m. <p> Let C = fx x fl c; x x=2g and input domain D = N . There is a polynomial-time algorithm to decide, given two C-programs P 1 and P 2 whether P 1 and P 2 commute. <ref> [16] </ref> 3.5 Probabilistic Polynomial-Time Problems If in result #15, D is the set of all integers or the set of all rational numbers, commutativity is probabilistically decidable in polynomial time.
Reference: [17] <author> O. Ibarra, B. Leininger, and L. Rosier. </author> <title> A Note on the Complexity of Program Evaluation. </title> <journal> In Mathematical Systems Theory, </journal> <volume> 17 </volume> <pages> 85-96, </pages> <year> 1984. </year>
Reference-contexts: The lower bound holds even for five-variable programs <ref> [17] </ref>. If we replace x x + y and x x y by simpler constructs, we can show the following: Result #10. Let C = fx 0; x x + 1; x x 1; x y; if x = 0 then goto t; goto t; do x endg. <p> Commutativity with ONE over zero input is PSPACE-hard for C-programs with no nesting of loops. This result also holds for C = fx 0; x x+1; x x 1; if x = 0 then y 1; do x endg <ref> [17] </ref>. Note that if C = fx 0; x x + 1; x x 1; x y; if x = 0 then y z; do x endg commutativity over zero input for C-programs with no nesting of loops is in PSPACE.
Reference: [18] <author> O. Ibarra and B. Leininger. </author> <title> The Complexity of the Equivalence Problem for Simple Loop-Free Programs. </title> <journal> In SIAM Journal on Computing, </journal> <volume> 11(1) </volume> <pages> 15-27, </pages> <month> February </month> <year> 1982. </year>
Reference-contexts: Result #12. Let C = fx 2x; x x=2; x x + yg and input domain D = Z. The commutativity problem for C-programs with one input variable and one auxiliary variable is NP-hard. The result also holds when x x + y is replaced by x x y <ref> [18] </ref>. Commutativity, nevertheless, is decidable. <p> It can be shown that the non-commutativity problem for C + -programs is in NP <ref> [18] </ref>. Thus commutativity can be decided in exponential time. Next, we have Result #13. Let C = fx 0; x x=2; x x yg and input domain D = Z. Commu-tativity with ZERO for C-programs with one input variable and two auxiliary variables is NP-hard [18]. 6 The above result is <p> + -programs is in NP <ref> [18] </ref>. Thus commutativity can be decided in exponential time. Next, we have Result #13. Let C = fx 0; x x=2; x x yg and input domain D = Z. Commu-tativity with ZERO for C-programs with one input variable and two auxiliary variables is NP-hard [18]. 6 The above result is the best possible since it can be shown that commutativity with ZERO is decidable in polynomial time for C-programs with two program variables (both variables may also be input variables) for C = fx 0; x c; x c; x xflc; x x=c; x x+c; <p> is the best possible since it can be shown that commutativity with ZERO is decidable in polynomial time for C-programs with two program variables (both variables may also be input variables) for C = fx 0; x c; x c; x xflc; x x=c; x x+c; x xc; x xyg <ref> [18] </ref>. Result #14. Let C = fx 1; x x + y; x x yg. Over D = N , commutativity with ZERO is NP-hard for C-programs with one input variable and two auxiliary variables [15].
Reference: [19] <author> M. Lam. </author> <title> Software Pipelining. </title> <booktitle> In Proceedings of the SIGPLAN '88 Conference on Program Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1988. </year>
Reference: [20] <author> M. Rinard and P. Diniz. </author> <title> Automatically Parallelizing Serial Programs using Commutativity Analysis. </title> <note> (submitted for publication) </note>
Reference-contexts: In the context of parallel computing commuting operations enable concurrent execution because they can execute in any order without changing the final result [23, 21]. Parallelizing compilers that recognize commuting operations can exploit this property to automatically generate parallel code for computations that consist only of commuting operations <ref> [20] </ref>. In the area of databases exploiting commuting operations can improve the performance of concurrency control algorithms by increasing the amount of concurrency in the transaction schedule [24]. This broad range of applications motivates the design of static analysis techniques capable of automatically detecting commuting operations | commutativity analysis.
Reference: [21] <author> J. Solworth and B. Reagan. </author> <title> Arbitrary order operations on trees. </title> <booktitle> In Languages and Compilers for Parallel Computing, Fourth International Workshop, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: In the context of parallel computing commuting operations enable concurrent execution because they can execute in any order without changing the final result <ref> [23, 21] </ref>. Parallelizing compilers that recognize commuting operations can exploit this property to automatically generate parallel code for computations that consist only of commuting operations [20].
Reference: [22] <author> R. Sethi. </author> <title> Testing for the Church-Rooser Property. </title> <journal> In Journal of the ACM, </journal> <volume> 21(4) </volume> <pages> 671-679, </pages> <month> October </month> <year> 1974. </year>
Reference-contexts: Knowledge of commuting operations is of practical significance. In the context of optimizing compilers commuting program transformations can be used to reduce the search space for the optimal program transformation sequence, hence reducing the algorithmic complexity of the compiler optimization algorithms <ref> [22] </ref>. In the context of parallel computing commuting operations enable concurrent execution because they can execute in any order without changing the final result [23, 21]. Parallelizing compilers that recognize commuting operations can exploit this property to automatically generate parallel code for computations that consist only of commuting operations [20].
Reference: [23] <author> G. Steele. </author> <title> Making asynchronous parallelism safe for the world. </title> <booktitle> In Proceedings of the Seventeenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 218-231, </pages> <address> San Francisco, CA, </address> <month> January </month> <year> 1990. </year>
Reference-contexts: In the context of parallel computing commuting operations enable concurrent execution because they can execute in any order without changing the final result <ref> [23, 21] </ref>. Parallelizing compilers that recognize commuting operations can exploit this property to automatically generate parallel code for computations that consist only of commuting operations [20].
Reference: [24] <author> W. Weihl. </author> <title> Commutativity-based concurrency control for abstract data types. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(12) </volume> <pages> 1488-1505, </pages> <month> December </month> <year> 1988. </year>
Reference-contexts: In the area of databases exploiting commuting operations can improve the performance of concurrency control algorithms by increasing the amount of concurrency in the transaction schedule <ref> [24] </ref>. This broad range of applications motivates the design of static analysis techniques capable of automatically detecting commuting operations | commutativity analysis. In this paper we focus on the theoretical aspects of commutativity analysis.
Reference: [25] <author> M. Wolf and M. Lam. </author> <title> A Data Locality Optimizing Algorithm. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Program Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference: [26] <author> M. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year> <month> 12 </month>
References-found: 26

