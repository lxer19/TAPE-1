URL: http://www.cs.jhu.edu/~salzberg/ijcai93.ps
Refering-URL: http://www.cs.jhu.edu/~salzberg/cs661.html
Root-URL: 
Email: lastname@cs.jhu.edu  
Phone: (410) 516-8296  
Title: Induction of Oblique Decision Trees  
Author: David Heath and Simon Kasif and Steven Salzberg 
Address: Baltimore, MD 21218  
Affiliation: Department of Computer Science The Johns Hopkins University  
Abstract: This paper introduces a randomized technique for partitioning examples using oblique hyperplanes. Standard decision tree techniques, such as ID3 and its descendants, partition a set of points with axis-parallel hyper-planes. Our method, by contrast, attempts to find hyperplanes at any orientation. The purpose of this more general technique is to find smaller but equally accurate decision trees than those created by other methods. We have tested our algorithm on both real and simulated data, and found that in some cases it produces surprisingly small trees without losing predictive accuracy. Small trees allow us, in turn, to obtain simple qualitative descriptions of each problem domain.
Abstract-found: 1
Intro-found: 1
Reference: [ Baum and Haussler, 1989 ] <author> E. Baum and D. Haussler. </author> <title> What size net gives valid generalization? Neural Computation, </title> <booktitle> 1 </booktitle> <pages> 141-160, </pages> <year> 1989. </year>
Reference: [ Blum and Rivest, 1988 ] <author> A. Blum and R. L. Rivest. </author> <title> Training a 3-node neural network is np-complete. </title> <booktitle> In Proceedings of the First ADM Workshop on the Computational Learning Theory, </booktitle> <pages> pages 9-18, </pages> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference: [ Breiman et al., 1984 ] <author> L. Breiman, J. Friedman, R. Ol-shen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <year> 1984. </year>
Reference: [ Duda and Hart, 1973 ] <author> R. Duda and P. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference-contexts: Note that this theorem says that finding even one optimal hyperplane is hard; clearly, then, building a whole tree is even harder. It is worth noting that there do exist energy measures for which finding the best hyperplane is possible in polynomial time. See <ref> [Duda and Hart, 1973] </ref> for a description of different techniques of generating hyperplanes in the absence of linear separability. 2.1 Finding approximate solutions It is clear that by abandoning the restriction to axis-parallel hyperplanes, we have made the problem of building decision trees considerably harder.
Reference: [ Heath, 1992 ] <author> D. Heath. </author> <title> A geometric framework for machine learning. </title> <type> PhD thesis, </type> <institution> The Johns Hopkins University, Baltimore, MD, </institution> <year> 1992. </year>
Reference-contexts: Given two point sets K 1 and K 2 and a value k, the problem of determining if there is a hyperplane such that e k is NP-complete. For a proof of this theorem, see <ref> [Heath, 1992] </ref>. Note that this theorem says that finding even one optimal hyperplane is hard; clearly, then, building a whole tree is even harder. It is worth noting that there do exist energy measures for which finding the best hyperplane is possible in polynomial time. <p> To force SADT to generate a relatively pure split, we define the sum-minority error measure to be min (u 1 ; v 1 ) + min (u 2 ; v 2 ). For a description of this and other error measures, see <ref> [Heath, 1992] </ref>. 3.2 Taking advantage of randomization We believe randomization in learning algorithms can easily be used to advantage. <p> See [Mingers, 1989] for an overview of several decision tree pruning methods. The results we present in our paper are for unpruned trees. It turns out that some pruning methods work well on our oblique trees. A comparison of pruning strategies for SADT can be found in <ref> [Heath, 1992] </ref>. 7 Predicting using smallest trees One advantage of randomization is that SADT can generate many different trees to model a dataset, and then use some additional criterion to decide which tree to use for classification. <p> If construction of a good classifier is more important than producing a concise representation, then one might use SADT to generate a set of trees, and then use the majority vote for classification. We have experimented with this and other ideas <ref> [Heath, 1992] </ref>, but these results are not included in the interest of brevity. To summarize, SADT was able, for the datasets we used in our experiments, to generate quite small trees. We consider this to be its main advantage over other decision tree algorithms.
Reference: [ Hyafile and Rivest, 1976 ] <author> L. Hyafile and R. Rivest. </author> <title> Constructing optimal binary decision trees is np-complete. </title> <journal> Information Processing Letters, </journal> <volume> 5(1) </volume> <pages> 15-17, </pages> <year> 1976. </year>
Reference: [ Judd, 1988 ] <author> J. S. Judd. </author> <title> On the complexity of loading shallow neural networks. </title> <journal> Journal of Complexity, </journal> <volume> 4 </volume> <pages> 177-192, </pages> <year> 1988. </year>
Reference: [ Lin and Vitter, 1991 ] <author> J. H. Lin and J. S. Vitter. </author> <title> Complexity results on learning by neural nets. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 211-230, </pages> <year> 1991. </year>
Reference: [ Mangasarian et al., 1990 ] <author> O. Mangasarian, R. Setiono, and W. Wolberg. </author> <title> Pattern recognition via linear programming: Theory and application to medical diagnosis. </title> <booktitle> SIAM Workshop on Optimization, </booktitle> <year> 1990. </year>
Reference: [ Mingers, 1989 ] <author> J. Mingers. </author> <title> An empirical comparison of pruning methods for decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 4 </volume> <pages> 227-243, </pages> <year> 1989. </year>
Reference-contexts: See, for example, [Breiman, et al., 1984]. The approach commonly used for standard decision trees is to first generate a tree that correctly classifies every training example, and use another procedure, pruning, to reduce the size of the tree. See <ref> [Mingers, 1989] </ref> for an overview of several decision tree pruning methods. The results we present in our paper are for unpruned trees. It turns out that some pruning methods work well on our oblique trees.
Reference: [ Odewahn et al., 1992 ] <author> S. C. Odewahn, E. B. Stockwell, R. L. Pennington, R. M. Humphreys, and W. A. Zu-mach. </author> <title> Automated star/galaxy discrimination with neural networks. </title> <journal> Astronomical Journal, </journal> <volume> 103(1) </volume> <pages> 318-331, </pages> <year> 1992. </year>
Reference: [ Quinlan, ] <author> J.R. Quinlan. </author> <title> Simplifying decisoin trees. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 27 </volume> <pages> 221-234. </pages>
Reference: [ Quinlan, 1986 ] <author> J.R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference: [ Salzberg, 1991 ] <author> S. Salzberg. </author> <title> Distance metrics for instance-based learning. </title> <booktitle> In Methodologies for Intelligent Systems: 6th International Symposium, ISMIS '91, </booktitle> <pages> pages 399-408, </pages> <address> New York, 1991. </address> <publisher> Springer-Verlag. </publisher>
Reference: [ Utgoff and Brodley, ] <author> P. Utgoff and C. Brodley. </author> <title> An incremental method for find multivariate splits for decision trees. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 56-65, </pages> <address> Los Altos, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Utgoff, 1989 ] <author> P. Utgoff. </author> <title> Incremental induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 4(2) </volume> <pages> 161-186, </pages> <year> 1989. </year>
Reference: [ Wiess and Kapouleas, 1989 ] <author> S. Wiess and I. Kapouleas. </author> <title> An empirical comparison of pattern recognition, neural nets, and machine learning classification methods. </title> <booktitle> In Proceedings of the Eleventh IJCAI, </booktitle> <address> Detroit, MI, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
References-found: 17

