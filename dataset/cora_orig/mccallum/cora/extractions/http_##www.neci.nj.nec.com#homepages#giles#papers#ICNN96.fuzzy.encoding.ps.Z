URL: http://www.neci.nj.nec.com/homepages/giles/papers/ICNN96.fuzzy.encoding.ps.Z
Refering-URL: http://www.neci.nj.nec.com/homepages/giles/papers/
Root-URL: 
Email: e-mail: fomlinc,karvel,gilesg@research.nj.nec.com  
Phone: Phone: (609) 951-f2691,2642,2622g FAX: (609) 951-2682  
Title: Representation of Fuzzy Finite State Automata in Continuous Recurrent Neural Networks  
Author: Christian W. Omlin a Karvel K. Thornber a C. Lee Giles a;b 
Address: Princeton, NJ 08540  College Park, MD 20742  
Affiliation: a NEC Research Institute,  b UMIACS, U. of Maryland,  
Abstract: Based on previous work on encoding deterministic finite-state automata (DFAs) in discrete-time, second-order recurrent neural networks with sigmoidal discriminant functions, we propose an algorithm that constructs an augmented recurrent neural network that encodes fuzzy finite-state automata (FFAs). Given an arbitrary FFA, we apply an algorithm which transforms the FFA into an equivalent deterministic acceptor which computes the fuzzy string membership function. The neural network can be constructed such that it recognizes strings of fuzzy regular languages with arbitrary accuracy.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Alquezar and A. Sanfeliu, </author> <title> "An algebraic framework to represent finite state machines in single-layer recurrent neural networks," </title> <journal> Neural Computation, </journal> <volume> vol. 7, no. 5, </volume> <editor> p. </editor> <volume> 931, </volume> <year> 1995. </year>
Reference-contexts: A large class of problems where the current state depends on both the current input and the previous state can be modeled by finite-state automata or their equivalent grammars. It has been shown that recurrent neural networks can represent deterministic finite-state automata (DFAs) <ref> [1, 5, 6, 11] </ref>. Thus, it is only natural to investigate whether recurrent neural networks can also represent fuzzy finite-state automata (FFAs) and thus be used to implement recognizers of fuzzy regular languages. <p> Finite State Automata and Recurrent Neural Networks Recurrent neural networks have been shown to be at least computationally equivalent to Turing machines [16]. Their computational power and training ability make them useful tools for modeling nonlinear dynamical systems. DFAs can be represented in many discrete-time, recurrent network architectures <ref> [1, 5, 6] </ref>. We choose for convenience networks with second-order weights W ijk . <p> A fuzzy finite state automaton (FFA) f M is a 6-tuple f M =&lt; ; Q; Z; R; ffi; ! &gt; where is the input alphabet, Q is a set of fuzzy states, Z is a finite output alphabet, R is the fuzzy initial state, ffi : fi Q fi <ref> [0; 1] </ref> ! Q is the fuzzy transition map and ! : Q ! Z is the output map. Fig. 1: Recurrent Network Architecture for Fuzzy Finite State Automata: The architecture consists of two parts: Recurrent state neurons with second-order weights implement the finite-state dynamics of the deterministic acceptor. <p> greatly simplifies the encoding of FFAs in recurrent networks with continuous discriminant functions: Theorem 3.2 Given a regular fuzzy grammar e G, there exists a deterministic finite state automaton M with output alphabet Z f : is a production weightg [ f0g which computes the membership function : fl ! <ref> [0; 1] </ref> of the language L ( e G). The constructive proof can be found in [17]. <p> Network Architecture for Fuzzy Automata Theorem 3.2 enables us to transform any FFA into a deterministic automaton which computes the same membership function : fl ! <ref> [0; 1] </ref>. We just need to demonstrate how to implement the computation of with continuous discriminant functions. For that purpose, we augment the network architecture used for encoding DFAs with additional weights which connect the recurrent state neurons to a linear output neuron.
Reference: [2] <author> J. Bezdek, </author> <title> ed., </title> <journal> IEEE Transactions on Neural Networks Special Issue on Fuzzy Logic and Neural Networks, </journal> <volume> vol. 3. </volume> <booktitle> IEEE Neural Networks Council, </booktitle> <year> 1992. </year>
Reference-contexts: 1. Introduction There has been an increased interest in combining artificial neural networks and fuzzy systems (see <ref> [2] </ref> for a collection of papers). Fuzzy logic [21] provides a mathematical foundation for approximate reasoning; fuzzy logic controllers have proven very successful in a variety of applications. The parameters of adaptive fuzzy systems have clear physical meanings which facilitates the choice of their initial values.
Reference: [3] <author> G. Cybenko, </author> <title> "Approximation by superpositions of a sigmoidal function," Mathematics of Control, Signals, </title> <journal> and Systems, </journal> <volume> vol. 2, </volume> <pages> pp. 303-314, </pages> <year> 1989. </year>
Reference-contexts: Artificial neural networks have become valuable computational tools in their own right for tasks such as pattern recognition, control, and forecasting. Fuzzy systems and multilayer perceptrons are computationally equivalent, i.e. they are both universal approximators <ref> [3, 19] </ref>. Recurrent neural networks have been shown to be computationally equivalent with Turing machines [16]; whether or not recurrent fuzzy systems are also Turing equivalent remains an open question. While the methodologies underlying fuzzy systems and neural networks are quite different, their functional forms are often similar.
Reference: [4] <author> D. Dubois and H. Prade, </author> <title> Fuzzy sets and systems: </title> <journal> theory and applications, </journal> <volume> vol. </volume> <booktitle> 144 of Mathematics in Science and Engineering, </booktitle> <pages> pp. 220-226. </pages> <publisher> Academic Press, </publisher> <year> 1980. </year>
Reference-contexts: Any fuzzy automaton as described in definition 3.3 is equivalent to a restricted fuzzy automaton <ref> [4] </ref>. 1 There exists a correspondence between FFAs and fuzzy regular grammars [4]: Theorem 3.1 For a given fuzzy grammar e G, there exists a fuzzy automaton f M such that L ( e G) = L ( f M ). <p> Any fuzzy automaton as described in definition 3.3 is equivalent to a restricted fuzzy automaton <ref> [4] </ref>. 1 There exists a correspondence between FFAs and fuzzy regular grammars [4]: Theorem 3.1 For a given fuzzy grammar e G, there exists a fuzzy automaton f M such that L ( e G) = L ( f M ). Our goal is to use only continuous (sigmoidal and linear) discriminant functions for the neural network implementation of FFAs.
Reference: [5] <author> P. Frasconi, M. Gori, M. Maggini, and G. </author> <title> Soda, "Representation of finite state automata in recurrent radial basis function networks," </title> <booktitle> Machine Learning, </booktitle> <year> 1995. </year> <note> In press. </note>
Reference-contexts: A large class of problems where the current state depends on both the current input and the previous state can be modeled by finite-state automata or their equivalent grammars. It has been shown that recurrent neural networks can represent deterministic finite-state automata (DFAs) <ref> [1, 5, 6, 11] </ref>. Thus, it is only natural to investigate whether recurrent neural networks can also represent fuzzy finite-state automata (FFAs) and thus be used to implement recognizers of fuzzy regular languages. <p> Finite State Automata and Recurrent Neural Networks Recurrent neural networks have been shown to be at least computationally equivalent to Turing machines [16]. Their computational power and training ability make them useful tools for modeling nonlinear dynamical systems. DFAs can be represented in many discrete-time, recurrent network architectures <ref> [1, 5, 6] </ref>. We choose for convenience networks with second-order weights W ijk .
Reference: [6] <author> C. Giles, C. Miller, D. Chen, H. Chen, G. Sun, and Y. Lee, </author> <title> "Learning and extracting finite state automata with second-order recurrent neural networks," </title> <journal> Neural Computation, </journal> <volume> vol. 4, no. 3, </volume> <editor> p. </editor> <volume> 380, </volume> <year> 1992. </year>
Reference-contexts: A large class of problems where the current state depends on both the current input and the previous state can be modeled by finite-state automata or their equivalent grammars. It has been shown that recurrent neural networks can represent deterministic finite-state automata (DFAs) <ref> [1, 5, 6, 11] </ref>. Thus, it is only natural to investigate whether recurrent neural networks can also represent fuzzy finite-state automata (FFAs) and thus be used to implement recognizers of fuzzy regular languages. <p> Finite State Automata and Recurrent Neural Networks Recurrent neural networks have been shown to be at least computationally equivalent to Turing machines [16]. Their computational power and training ability make them useful tools for modeling nonlinear dynamical systems. DFAs can be represented in many discrete-time, recurrent network architectures <ref> [1, 5, 6] </ref>. We choose for convenience networks with second-order weights W ijk .
Reference: [7] <author> V. Gorrini and H. Bersini, </author> <title> "Recurrent fuzzy systems," </title> <booktitle> in Proceedings of the Third IEEE Conference on Fuzzy Systems, </booktitle> <volume> vol. I, </volume> <pages> pp. 193-198, </pages> <year> 1994. </year>
Reference-contexts: The development of powerful learning algorithms for neural networks has been beneficial to the field of fuzzy systems which adopted some learning algorithms; e.g. there exists a backpropagation training algorithms for fuzzy logic systems which are similar to the training algorithms for neural networks <ref> [7] </ref>. A large class of problems where the current state depends on both the current input and the previous state can be modeled by finite-state automata or their equivalent grammars. It has been shown that recurrent neural networks can represent deterministic finite-state automata (DFAs) [1, 5, 6, 11].
Reference: [8] <author> J. Grantner and M. Patyra, </author> <title> "Synthesis and analysis of fuzzy logic finite state machine models," </title> <booktitle> in Proceedings of the Third IEEE Conference on Fuzzy Systems, </booktitle> <volume> vol. I, </volume> <pages> pp. 205-210, </pages> <year> 1994. </year>
Reference-contexts: The fundamentals of FFAs have been in discussed in [13] without presenting a systematic method for machine synthesis. Neural network implementations of fuzzy automata have been proposed in the literature <ref> [8, 9, 18] </ref>. A general synthesis method for synchronous fuzzy sequential circuits has been discussed in [20].
Reference: [9] <author> S. Lee and E. Lee, </author> <title> "Fuzzy neural networks," </title> <journal> Mathematical Biosciences, </journal> <volume> vol. 23, </volume> <pages> pp. 151-177, </pages> <year> 1975. </year>
Reference-contexts: The fundamentals of FFAs have been in discussed in [13] without presenting a systematic method for machine synthesis. Neural network implementations of fuzzy automata have been proposed in the literature <ref> [8, 9, 18] </ref>. A general synthesis method for synchronous fuzzy sequential circuits has been discussed in [20].
Reference: [10] <author> S. Mensch and H. Lipp, </author> <title> "Fuzzy specification of finite state machines," </title> <booktitle> in Proceedings of the European Design Automation Conference, </booktitle> <pages> pp. 622-626, </pages> <year> 1990. </year>
Reference-contexts: Notice that we do not claim that such a representation can be learned. Fuzzy grammars have been found to be useful in a variety of applications such as in the analysis of Xrays [12], in digital circuit design <ref> [10] </ref>, and in the design of intelligent human-computer interfaces [14]. The fundamentals of FFAs have been in discussed in [13] without presenting a systematic method for machine synthesis. Neural network implementations of fuzzy automata have been proposed in the literature [8, 9, 18].
Reference: [11] <author> C. Omlin and C. Giles, </author> <title> "Stable encoding of large finite-state automata in recurrent neural networks with sigmoid discriminants," </title> <booktitle> Neural Computation, </booktitle> <year> 1996. </year> <note> Accepted for publication. </note>
Reference-contexts: A large class of problems where the current state depends on both the current input and the previous state can be modeled by finite-state automata or their equivalent grammars. It has been shown that recurrent neural networks can represent deterministic finite-state automata (DFAs) <ref> [1, 5, 6, 11] </ref>. Thus, it is only natural to investigate whether recurrent neural networks can also represent fuzzy finite-state automata (FFAs) and thus be used to implement recognizers of fuzzy regular languages. <p> I t We have recently proven that DFAs can be encoded in discrete-time, second-order recurrent neural networks with sigmoidal discriminant functions such that the DFA and constructed network accept the same regular language <ref> [11] </ref>.
Reference: [12] <author> A. Pathak and S. Pal, </author> <title> "Fuzzy grammars in syntactic recognition of skeletal maturity from x-rays," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> vol. 16, no. 5, </volume> <pages> pp. 657-667, </pages> <year> 1986. </year>
Reference-contexts: Notice that we do not claim that such a representation can be learned. Fuzzy grammars have been found to be useful in a variety of applications such as in the analysis of Xrays <ref> [12] </ref>, in digital circuit design [10], and in the design of intelligent human-computer interfaces [14]. The fundamentals of FFAs have been in discussed in [13] without presenting a systematic method for machine synthesis. Neural network implementations of fuzzy automata have been proposed in the literature [8, 9, 18].
Reference: [13] <author> E. Santos, </author> <title> "Maximin automata," </title> <journal> Information and Control, </journal> <volume> vol. 13, </volume> <pages> pp. 363-377, </pages> <year> 1968. </year>
Reference-contexts: Fuzzy grammars have been found to be useful in a variety of applications such as in the analysis of Xrays [12], in digital circuit design [10], and in the design of intelligent human-computer interfaces [14]. The fundamentals of FFAs have been in discussed in <ref> [13] </ref> without presenting a systematic method for machine synthesis. Neural network implementations of fuzzy automata have been proposed in the literature [8, 9, 18]. A general synthesis method for synchronous fuzzy sequential circuits has been discussed in [20].
Reference: [14] <author> H. Senay, </author> <title> "Fuzzy command grammars for intelligent interface design," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> vol. 22, no. 5, </volume> <pages> pp. 1124-1131, </pages> <year> 1992. </year>
Reference-contexts: Notice that we do not claim that such a representation can be learned. Fuzzy grammars have been found to be useful in a variety of applications such as in the analysis of Xrays [12], in digital circuit design [10], and in the design of intelligent human-computer interfaces <ref> [14] </ref>. The fundamentals of FFAs have been in discussed in [13] without presenting a systematic method for machine synthesis. Neural network implementations of fuzzy automata have been proposed in the literature [8, 9, 18]. A general synthesis method for synchronous fuzzy sequential circuits has been discussed in [20].
Reference: [15] <author> J. Si and A. Michel, </author> <title> "Analysis and synthesis of a class of discrete-time neural networks with multilevel threshold neurons," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 6, no. 1, </volume> <editor> p. </editor> <volume> 105, </volume> <year> 1995. </year>
Reference-contexts: A general synthesis method for synchronous fuzzy sequential circuits has been discussed in [20]. A synthesis method for a class of discrete-time neural networks with multilevel threshold neurons with applications to gray level image processing has been proposed in <ref> [15] </ref>. fl Published in Proceedings of IEEE International Conference on Neural Networks, p. 1023, IEEE Press, 1996. Copyright IEEE. 2. Finite State Automata and Recurrent Neural Networks Recurrent neural networks have been shown to be at least computationally equivalent to Turing machines [16]. <p> This suggests the use of continuous multilevel threshold neurons <ref> [15] </ref> which also have the potential for stable internal DFA state representations. Whether training such networks is feasible remains an open question.
Reference: [16] <author> H. Siegelmann and E. Sontag, </author> <title> "On the computational power of neural nets," </title> <journal> Journal of Computer and System Sciences, </journal> <volume> vol. 50, no. 1, </volume> <pages> pp. 132-150, </pages> <year> 1995. </year>
Reference-contexts: Fuzzy systems and multilayer perceptrons are computationally equivalent, i.e. they are both universal approximators [3, 19]. Recurrent neural networks have been shown to be computationally equivalent with Turing machines <ref> [16] </ref>; whether or not recurrent fuzzy systems are also Turing equivalent remains an open question. While the methodologies underlying fuzzy systems and neural networks are quite different, their functional forms are often similar. <p> Copyright IEEE. 2. Finite State Automata and Recurrent Neural Networks Recurrent neural networks have been shown to be at least computationally equivalent to Turing machines <ref> [16] </ref>. Their computational power and training ability make them useful tools for modeling nonlinear dynamical systems. DFAs can be represented in many discrete-time, recurrent network architectures [1, 5, 6]. We choose for convenience networks with second-order weights W ijk .
Reference: [17] <author> M. Thomason and P. Marinos, </author> <title> "Deterministic acceptors of regular fuzzy languages," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> no. 3, </volume> <pages> pp. 228-230, </pages> <year> 1974. </year>
Reference-contexts: The constructive proof can be found in <ref> [17] </ref>.
Reference: [18] <author> F. Unal and E. Khan, </author> <title> "A fuzzy finite state machine implementation based on a neural fuzzy system," </title> <booktitle> in Proceedings of the Third International Conference on Fuzzy Systems, </booktitle> <volume> vol. 3, </volume> <pages> pp. 1749-1754, </pages> <year> 1994. </year>
Reference-contexts: The fundamentals of FFAs have been in discussed in [13] without presenting a systematic method for machine synthesis. Neural network implementations of fuzzy automata have been proposed in the literature <ref> [8, 9, 18] </ref>. A general synthesis method for synchronous fuzzy sequential circuits has been discussed in [20].
Reference: [19] <author> L.-X. Wang, </author> <title> "Fuzzy systems are universal approximators," </title> <booktitle> in Proceedings of the First International Conference on Fuzzy Systems, </booktitle> <pages> pp. 1163-1170, </pages> <year> 1992. </year>
Reference-contexts: Artificial neural networks have become valuable computational tools in their own right for tasks such as pattern recognition, control, and forecasting. Fuzzy systems and multilayer perceptrons are computationally equivalent, i.e. they are both universal approximators <ref> [3, 19] </ref>. Recurrent neural networks have been shown to be computationally equivalent with Turing machines [16]; whether or not recurrent fuzzy systems are also Turing equivalent remains an open question. While the methodologies underlying fuzzy systems and neural networks are quite different, their functional forms are often similar.
Reference: [20] <author> T. Watanabe, M. Matsumoto, and M. Enokida, </author> <title> "Synthesis of synchronous fuzzy sequential circuits," </title> <booktitle> in Proceedings of the Third IFSA World Congress, </booktitle> <pages> pp. 288-291, </pages> <year> 1989. </year>
Reference-contexts: The fundamentals of FFAs have been in discussed in [13] without presenting a systematic method for machine synthesis. Neural network implementations of fuzzy automata have been proposed in the literature [8, 9, 18]. A general synthesis method for synchronous fuzzy sequential circuits has been discussed in <ref> [20] </ref>. A synthesis method for a class of discrete-time neural networks with multilevel threshold neurons with applications to gray level image processing has been proposed in [15]. fl Published in Proceedings of IEEE International Conference on Neural Networks, p. 1023, IEEE Press, 1996. Copyright IEEE. 2.
Reference: [21] <author> L. Zadeh, </author> <title> "Fuzzy sets," </title> <journal> Information and Control, </journal> <volume> vol. 8, </volume> <pages> pp. 338-353, </pages> <year> 1965. </year>
Reference-contexts: 1. Introduction There has been an increased interest in combining artificial neural networks and fuzzy systems (see [2] for a collection of papers). Fuzzy logic <ref> [21] </ref> provides a mathematical foundation for approximate reasoning; fuzzy logic controllers have proven very successful in a variety of applications. The parameters of adaptive fuzzy systems have clear physical meanings which facilitates the choice of their initial values.
References-found: 21

