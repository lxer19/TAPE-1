URL: ftp://ftp.cs.virginia.edu/pub/techreports/CS-93-40.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/techreports/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Portable Run-Time Support for Dynamic Object-Oriented Parallel Processing  
Author: Andrew S. Grimshaw,Jon B. Weissman,W. Timothy Strayer 
Note: This work partially funded by NSF grants ASC-9201822 and CDA 8922545-01, and NASA NGT-50970.  
Abstract: Technical Report No. CS-93-40 July 14, 1993 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Agerwala and Arvind, </author> <title> Data Flow Systems, </title> <journal> IEEE Computer, </journal> <volume> vol. 15, no. 2, </volume> <pages> pp. 10-13, </pages> <month> February, </month> <year> 1982. </year>
Reference: [2] <author> R. F. Babb, </author> <title> Parallel Processing with Large-Grain Data Flow Techniques, </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 55-61, </pages> <month> July, </month> <year> 1984. </year>
Reference: [3] <author> B. Beck, </author> <title> Shared Memory Parallel Programming in C++, </title> <journal> IEEE Software, </journal> <pages> 7(4) pp. 38-48, </pages> <month> July, </month> <year> 1990. </year>
Reference: [4] <author> B. N. Bershad, E. D. Lazowska, and H. M. Levy, </author> <title> Presto: A System for Object-Oriented Parallel Programming, </title> <journal> Software - Practice and Experience, </journal> <volume> 18(8), </volume> <pages> pp. 713-732, </pages> <month> August, </month> <year> 1988. </year>
Reference: [5] <author> A. Beguelin et al., </author> <title> HeNCE: Graphical Development Tools for Network-Based Concurrent Computing, </title> <booktitle> Proceedings SHPCC-92, </booktitle> <pages> pp. 129-136, </pages> <address> Williamsburg, VA, </address> <month> May, </month> <year> 1992. </year>
Reference: [6] <author> F. Bodin, et. al., </author> <title> Distributed pC++: Basic Ideas for an Object Parallel Language, </title> <booktitle> Proceedings Object-Oriented Numerics Conference, </booktitle> <pages> pp. </pages> <address> 1-24,Sunriver, Oregon, </address> <month> April 25-27, </month> <year> 1993. </year>
Reference: [7] <author> J. C. Browne, T. Lee, and J. Werth, </author> <title> Experimental Evaluation of a Reusability-Oriented Parallel Programming Environment, </title> <journal> IEEE Transactions on Software Engineering, pp. </journal> <volume> 111-120, vol. 16, no. 2, </volume> <month> Feb., </month> <year> 1990. </year>
Reference: [8] <author> T. L. Casavant, and J. G. Kuhl, </author> <title> A Taxonomy of Scheduling in General-Purpose Distributed Computing Systems, </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> vol. 14, </volume> <pages> pp. 141-154, </pages> <month> February, </month> <year> 1988. </year>
Reference-contexts: Their model uses system state information to describe the way in which the load in the system is distributed among its components. FALCON falls into the classification schemes of <ref> [8] </ref> [19] as having the following characteristics: Distributed: The scheduling decision is distributed. Each decision is reached independently. Load Sharing: FALCON transparently distributes the workload by transferring new tasks from nodes that are heavily loaded to nodes that are lightly loaded.
Reference: [9] <author> J. Dennis, </author> <title> First Version of a Data Flow Procedure Language, </title> <publisher> MIT TR-673, </publisher> <month> May, </month> <year> 1975. </year>
Reference: [10] <author> D.L. Eager, E. D. Lazowska, and J. Zahorjan, </author> <title> Adaptive Load Sharing in Homogeneous Distributed Systems, </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> vol. 12, </volume> <pages> pp. 662-675, </pages> <month> May, </month> <year> 1986. </year>
Reference-contexts: There is a rich literature on scheduling in distributed systems [8]<ref> [10] </ref>[19]. The general scheduling problem is NP-hard. Thus, much of the work in scheduling is based upon heuristics. Our scheduler, FALCON (Fully Automatic Load Coordinator for Networks) [16] is heuristic and based upon the work of Eager, Lazowska and Zahorjan [10] who developed a model for sender-initiated adaptive load sharing for homogeneous distributed systems. Their model uses system state information to describe the way in which the load in the system is distributed among its components.
Reference: [11] <author> A. S. Grimshaw, </author> <title> The Mentat Computation Model - Data-Driven Support for Dynamic Object-Oriented Parallel Processing, </title> <institution> Computer Science Technical Report, CS-93-30, University of Virginia, </institution> <month> May, </month> <year> 1993. </year>
Reference-contexts: This is called intra-object parallelism encapsulation; the caller only sees the member function invocation and is unaware that the function is implemented in parallel. 2.1. The Macro Dataow Model of Computation Macro dataow is Mentats underlying model of computation. The macro dataow (MDF <ref> [11] </ref>) model is a medium grain, data-driven computation model inspired by dataow [1][9][24][28]. <p> Performance of Mentat RTS on Sun SparcStation 2 Network. 29 computation instances, adding arcs, and constructing future lists, is quite small (190 uS) when compared to the message transport costs. This is good news, and justifies the use of dynamic graphs <ref> [11] </ref>. That we have not captured all of the overhead terms is also clear - the remainder is in the two block predicate calls (1150 microseconds) and in numerous small ancillary functions. Finally, regular object reuse results in a large time savings, 20 mS versus 150 mSec.
Reference: [12] <author> A. S. Grimshaw, E. A. West, and W.R. Pearson, </author> <title> No Pain and Gain! - Experiences with Mentat on Biological Application, </title> <journal> Concurrency: Practice & Experience, </journal> <pages> pp. 309-328, </pages> <note> Vol. 5, issue 4, </note> <month> July, </month> <year> 1993. </year>
Reference: [13] <author> A.S. Grimshaw, J.B.Weissman, E.A. West, and E. Loyot, </author> <title> Meta Systems: An Approach Combining Parallel Processing And Heterogeneous Distributed Computing Systems, </title> <institution> TR-92-43, Department of Computer Science, University of Virginia, </institution> <month> December, </month> <year> 1992. </year>
Reference-contexts: Our objective is to integrate these hosts into a system that provides the illusion of one large virtual machine to users. As part of the Mentat metasystem testbed project <ref> [13] </ref> we are extending the 31 Mentat run-time system into a heterogeneous environment. Issues that must be addressed include data alignment, data coercion, and scheduling, in particular, automatic problem decomposition and placement [27].
Reference: [14] <author> A. S. Grimshaw, </author> <title> Easy to Use Object-Oriented Parallel Programming with Mentat, </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 39-51, </pages> <month> May, </month> <year> 1993. </year>
Reference-contexts: This simplifies the task of writing parallel programs. The high-performance aspects of Mentat have been demonstrated in [12][15]. For a general overview of Mentat and of the Mentat parallel processing philosophy see <ref> [14] </ref>. In this paper we present the Mentat run-time system which supports the Mentat Programming Language.
Reference: [15] <author> A. S. Grimshaw, W. T. Strayer, and P. Narayan, </author> <title> Dynamic Object-Oriented Parallel Processing, </title> <booktitle> IEEE Parallel & Distributed Technology: Systems & Applications, </booktitle> <pages> pp. 33-47, </pages> <month> May, </month> <year> 1993. </year>
Reference: [16] <author> A. S. Grimshaw and V. E. Vivas, </author> <title> FALCON: A Distributed Scheduler for MIMD Architectures, </title> <booktitle> Proceedings of the Symposium on Experiences with Distributed and Multiprocessor Systems, </booktitle> <pages> pp. 149-163, </pages> <address> Atlanta, GA, </address> <month> March, </month> <year> 1991. </year>
Reference-contexts: The last two, binding and status information services, are primarily bookkeeping, e.g., 19 reporting to the user the name of the objects running on a particular processor. Here we will confine our attention to scheduling and instantiation. A more detailed description can be found in <ref> [16] </ref>. The basic scheduling problem is to assign Mentat objects to processors in such a manner that total execution time of the application is minimized. The problem is complicated by three facts. <p> The general scheduling problem is NP-hard. Thus, much of the work in scheduling is based upon heuristics. Our scheduler, FALCON (Fully Automatic Load Coordinator for Networks) <ref> [16] </ref> is heuristic and based upon the work of Eager, Lazowska and Zahorjan [10] who developed a model for sender-initiated adaptive load sharing for homogeneous distributed systems. Their model uses system state information to describe the way in which the load in the system is distributed among its components.
Reference: [17] <author> A. S. Grimshaw, </author> <title> The Mentat Run-Time System: Support for Medium Grain Parallel Computation, </title> <booktitle> Pro 32 ceedings of the Fifth Distributed Memory Computing Conference, </booktitle> <pages> pp. 1064-1073, </pages> <address> Charleston, SC., </address> <month> April 9-12, </month> <year> 1990. </year>
Reference-contexts: The IM returns a bound Mentat object name. The named object has been selected by the IM to service the regular object member function. The TMU extracts from the token database the tokens for the computation and 5. An earlier method described in <ref> [17] </ref> was not scalable, it has been replaced. 23 forwards them to the named object. Thus, each token is transported two hops, once to the TMU, and once from the TMU to the object.
Reference: [18] <author> A. S. Grimshaw, D. Mack, and T. Strayer, MMPS: </author> <title> Portable Message Passing Support for Parallel Computing, </title> <booktitle> Proceedings of the Fifth Distributed Memory Computing Conference, </booktitle> <pages> pp. 784-789, </pages> <address> Charleston, SC., </address> <month> April 9-12, </month> <year> 1990. </year>
Reference-contexts: The interface in turn is 9 implemented as machine-independent modules. At the bottom level are the architecture and operating system specific modules where all platform-specific code has been isolated. The platform-specific modules include the communication system MMPS (the Modular Message Passing System <ref> [18] </ref>), and the object loader used by the instantiation manager. By isolating platform-dependent code we simplify both software maintenance and the task of porting Mentat to new platforms.
Reference: [19] <author> A. Hac, </author> <title> Load Balancing in Distributed Systems: A Summary, Performance Evaluation Review, </title> <journal> ACM, </journal> <volume> vol. 16, </volume> <pages> pp. 17-25, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: Their model uses system state information to describe the way in which the load in the system is distributed among its components. FALCON falls into the classification schemes of [8] <ref> [19] </ref> as having the following characteristics: Distributed: The scheduling decision is distributed. Each decision is reached independently. Load Sharing: FALCON transparently distributes the workload by transferring new tasks from nodes that are heavily loaded to nodes that are lightly loaded.
Reference: [20] <author> R. H. Halstead Jr., </author> <title> Multilisp: A Language for Concurrent Symbolic Computation, </title> <journal> ACM Transactions on Programming Languages and Systems, pp. </journal> <volume> 501-538, vol. 7, no. 4, </volume> <month> October, </month> <year> 1985. </year>
Reference-contexts: In this case the graph has grown, rather than contracted as in Figure 2. The point is that in both cases only As future needs modification; neither E, nor any other actor need be notified of the change. 2. MDF futures should not be confused with Multilisp futures <ref> [20] </ref>. Es future A D F As future 5 2.2. Mentat Classes The most important extension to C++ is the keyword mentat as a prefix to class definitions, as shown on line 1 of Figure 4.
Reference: [21] <author> Intel Corporation, </author> <title> iPSC/2 USERS GUIDE, Intel Scientific Computers, </title> <address> Beaverton, OR, </address> <month> March, </month> <year> 1988. </year>
Reference-contexts: This virtual machine also abstracts the platform-specific details so that the MPL code is portable across various MIMD architectures. The run-time system is currently running on several systems: the Intel iPSC/2 and iPSC/860 using NX/2 <ref> [21] </ref>, and networks of Sun, Silicon Graphics, and IBM RS 6000 workstations using UDP packets and Unix sockets. 3.1. The Virtual Machine The virtual macro dataow machine, as shown in Figure 5, provides a service interface to the compiled MPL code.
Reference: [22] <author> J. K. Lee and D. Gannon, </author> <title> Object Oriented Parallel Programming Experiments and Results, </title> <booktitle> Proceedings of Supercomputing 91, </booktitle> <pages> pp. 273-282, </pages> <address> Albuquerque, NM, </address> <year> 1991. </year>
Reference: [23] <author> S. K. Smith, et al., </author> <title> Experimental Systems Project at MCC, </title> <type> MCC Technical Report Number: </type> <institution> ACA-ESP-089-89, </institution> <month> March 2, </month> <year> 1989. </year>
Reference: [24] <author> V. P. Srini, </author> <title> An Architectural Comparison of Dataow Systems, </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 68-88, </pages> <month> March, </month> <year> 1986. </year>
Reference: [25] <author> B. Stroustrup, </author> <booktitle> What is Object-Oriented Programming? IEEE Software, </booktitle> <pages> pp. 10-20, </pages> <month> May, </month> <year> 1988. </year>
Reference: [26] <author> J. B. Weissman, A. S. Grimshaw, and R. Ferraro, </author> <title> Parallel Object-Oriented Computation Applied to a Finite Element Problem, </title> <note> to appear in Scientific Computing, </note> <year> 1993. </year>
Reference: [27] <author> J. B. Weissman and A. S. Grimshaw, </author> <title> Multigranular Scheduling of Data Parallel Programs, </title> <institution> Computer Science Technical Report, CS-93-38, University of Virginia, </institution> <month> July, </month> <year> 1993. </year>
Reference-contexts: As part of the Mentat metasystem testbed project [13] we are extending the 31 Mentat run-time system into a heterogeneous environment. Issues that must be addressed include data alignment, data coercion, and scheduling, in particular, automatic problem decomposition and placement <ref> [27] </ref>. Acknowledgments This research was performed in part using the Intel iPSC/860 Gamma operated by Caltech on behalf of the Concurrent Supercomputing Consortium.
Reference: [28] <author> A. H. Veen, </author> <title> Dataow Machine Architecture, </title> <journal> ACM Computing Surveys, pp. </journal> <volume> 365-396, vol. 18, no. 4, </volume> <month> December, </month> <year> 1986. </year>
References-found: 28

