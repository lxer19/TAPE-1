URL: http://www.cs.berkeley.edu/~johnw/papers/icnn94.ps.Z
Refering-URL: http://www.cs.berkeley.edu/~johnw/publications.html
Root-URL: 
Title: A Supercomputer for Neural Computation  
Author: Krste Asanovic, James Beck, Jerome Feldman, Nelson Morgan, and John Wawrzynek 
Abstract: The requirement to train large neural networks quickly has prompted the design of a new massively parallel supercomputer using custom VLSI. This design features 128 processing nodes, communicating over a mesh network connected directly to the processor chip. Studies show peak performance in the range of 160 billion arithmetic operations per second. This paper presents the case for custom hardware that combines neural network-specific features with a general programmable machine architecture, and briefly describes the design in progress. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Asanovic, J. Beck, T. Callahan, J. Feldman, B. Irissou, B. Kingsbury, P. Kohn, J. Lazzaro, N. Morgan, D. Stoutamire, and J. Wawrzynek, </author> <title> CNS-1 Architecture Specification A Connectionist Network Supercomputer, </title> <type> ICSI Technical Report, </type> <institution> TR-93-021, </institution> <month> April </month> <year> 1993. </year>
Reference: [2] <author> K. Asanovic and N. Morgan, </author> <title> Experimental Determination of Precision Requirements for Back-Propagation Training of Artificial Neural Networks, </title> <booktitle> In Proceedings 2nd International Conference on Microelectronics for Neural Networks, </booktitle> <address> Munich, </address> <month> October </month> <year> 1991. </year>
Reference: [3] <author> T. von Eicken, D. E. Culler, S. C. Goldstein and K. E. Schauser, </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation, </title> <booktitle> Proc. Tenth International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: The instruction fetch bandwidth is supported by an on-chip instruction cache. Torrent also includes a hardware router to handle communications in the CNS-1 mesh. The communications path is tightly integrated with both the scalar and vector processors, and supports a low overhead transfer protocol based on active messages <ref> [3] </ref>. The appeal of active messages for CNS-1 is that a message arriving at its destination triggers execution of a local event handler, allowing very fast response for short messages.
Reference: [4] <author> D. Hammerstrom, </author> <title> A VLSI architecture for High-Performance, Low-Cost, On-Chip Learning, </title> <booktitle> In Proc. International Joint Conference on Neural Networks, </booktitle> <pages> pages II-537-543, </pages> <year> 1990. </year>
Reference: [5] <author> J. Hennessy and D. Patterson, </author> <title> Computer Architecture: A Quantitative Approach, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1990. </year>
Reference: [6] <author> J. Lazzaro, J. Wawrzynek, M. Mahowald, M. Sivilotti and D. Gillespie, </author> <title> Silicon Auditory Processor as Computer Peripherals, </title> <journal> IEEE Journal on Neural Networks, </journal> <month> May </month> <year> 1993. </year>
Reference: [7] <author> N. Morgan, J. Beck, P. Kohn, J. Bilmes, E. Allman and J. Beer, </author> <title> The Ring Array Processor (RAP): A Multiprocessing Peripheral for Connectionist Applications, </title> <journal> Journal of Parallel and Distributed Processing, Special Issue on Neural Networks, v14, </journal> <volume> pp.248-259, </volume> <year> 1992. </year>
Reference: [8] <author> S. Mueller and B. Gomes, </author> <title> A Performance Analysis of CNS-1 on Sparse Connectionist Networks, </title> <type> ICSI Technical Report, </type> <institution> TR-94-009, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: The ability to customize these handlers will be important for neural nets with sparse interconnections and activations, where the balance between computation and communication is particularly critical <ref> [8] </ref>. B. Hydrant I/O Chip Communication between the networked Torrent processors and the outside world takes place through a custom network adapter chip, named Hydrant. Attached along one edge of the communication mesh, Hydrants convert the high speed CNS-1 protocols to a more general parallel interface.
Reference: [9] <author> S. Omohundro, </author> <title> The Sather Programming Language, </title> <journal> Dr. Dobb's Journal, </journal> <volume> Volume 18, Issue 11, </volume> <editor> p. </editor> <volume> 42, </volume> <month> October </month> <year> 1993. </year>
Reference: [10] <author> U. Ramacher, J. Beichter, W. Raab, J. Anlauf, N. Bruls, M. Hachmann, and M. </author> <title> Wesseling, Design of a 1st Generation Neurocomputer, In VLSI Design of Neural Networks, </title> <publisher> Kluwer Academic, </publisher> <year> 1991. </year>
References-found: 10

