URL: ftp://ftp.cs.uidaho.edu/pub/foster/papers/soule-thesis.ps.gz
Refering-URL: http://www.cs.bham.ac.uk/~wbl/biblio/gp-bibliography.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Code Growth in Genetic Programming  
Author: by Terence Soule James A. Foster 
Degree: A Dissertation Presented in Partial Fulfillment of the Requirements for the Degree of Doctor of Philosophy with a Major in Computer Science in the  
Note: Major Professor:  
Date: May 1998  May 15, 1998  
Affiliation: College of Graduate Studies University of Idaho  
Abstract-found: 0
Intro-found: 1
Reference: [AIK96] <author> David Andre, Forrest H. Bennett III, and John R. Koza. </author> <title> Discovery by genetic programming of a cellular automata rule that is better than any known rule for the majority classification problem. </title> <editor> In John R. Koza, David E. Goldberg, David B. Fogel, and Rick R. Riolo, editors, </editor> <booktitle> Genetic Programming 1996: Proceedings of the First Annual Conference, </booktitle> <pages> pages 3-11. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Because crossover uses previously tested programs the search is guided by previous successes, making it very effective. GP has produced solutions on par with the best solutions developed using other methods (including hand written solutions) on problems including spacecraft attitude maneuvers and general classification problems <ref> [AIK96, How96] </ref>. Despite GP's early successes it has not scaled well to larger problems. A major factor in this failure has been code growth. Code growth can be described as the tendency of programs generated using GP to grow much larger than is functionally necessary.
Reference: [ALM + 92] <author> Sanjeev Arora, Carsten Lund, Rajeev Motwani, Madhu Sudan, and Mario Szegedy. </author> <title> Proof verification and the hardness of approximation problems. </title> <booktitle> In Proceedings 33rd IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 14-23. </pages> <publisher> IEEE, </publisher> <year> 1992. </year>
Reference-contexts: The GP parameters for the maximum clique problem are summarized in Table 3. This is a particularly difficult problem, since no polynomial time, deterministic algorithm exists for finding a good approximate solution unless P = NP (which is very unlikely), as proven by Arora et al. <ref> [ALM + 92] </ref>.
Reference: [Ang97] <author> Peter J. Angeline. </author> <title> Subtree crossover: Building block engine or macromuta-tion. </title> <editor> In John R. Koza, Kalyanmoy Deb, Marco Dorigo, David B. Fogel, Max Garzon, Hitoshi Iba, and Rick R. Riolo, editors, </editor> <booktitle> Genetic Programming 1997: Proceedings of the Second Annual Conference, </booktitle> <pages> pages 9-17. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1997. </year>
Reference-contexts: In fact, this limitation of crossover has been a major factor in the current debate between the use of crossover versus the use of subtree mutation <ref> [Ang97, LS97] </ref>. In subtree mutation, which is sometimes used as a replacement for crossover, a subtree is removed and replaced by a randomly generated subtree. The less obvious affects of the current population on future evolution are also much less widely recognized.
Reference: [Ash97] <author> Dan Ashlock. </author> <title> Gp-automata for dividing the dollar. </title> <editor> In John R. Koza, Kalyanmoy Deb, Marco Dorigo, David B. Fogel, Max Garzon, Hitoshi Iba, and Rick R. Riolo, editors, </editor> <booktitle> Genetic Programming 1997: Proceedings of the Second Annual Conference, </booktitle> <pages> pages 18-26. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kauf-mann, </publisher> <year> 1997. </year> <month> 63 </month>
Reference-contexts: GP has been applied to a variety of problems which is too wide to reasonably enumerate; a few of the more interesting applications include: robot and satellite guidance [Iba97, FN97, How96], data mining [RE96], neural net design [GWP96], circuit design [Tho96, KAIMA96, KIAMA96], and economic strategy development <ref> [Ash97, Len97] </ref>. GP's flexibility derives from its very simple nature. Any problem whose potential solutions can be adequately measured and compared is a potential candidate for GP. In lieu of biological reproduction GP uses a crossover operation which exchanges sections of code between programs.
Reference: [Bli96] <author> Tobias Blickle. </author> <title> Evolving compact solutions in genetic programming: A case study. </title> <editor> In Hans-Michael Voigt, Werner Ebeling, Ingo Rechenberg, and Hans-Paul Schwefel, editors, </editor> <booktitle> Parallel Problem Solving from Nature IV Proceedings of the International Conference on Evolutionary Computing. </booktitle> <address> Heidelberg: </address> <publisher> Springer Verlag, </publisher> <year> 1996. </year>
Reference-contexts: Commonly the penalty is a simple linear function of the solution size, but other more, and less, subtle approaches have been used. Of these, variable penalty functions, which respond to the fitness and size of individuals within the population appear to be the most robust <ref> [IdGS94, ZM95, Bli96] </ref>. Parsimony pressure is also often scaled to assure the deletion of large programs without changing the fitness of smaller programs [Koz92]. Programs exceeding a certain size and/or depth are discarded, which is equivalent to penalizing them with a zero fitness. <p> Other, more recent, experiments have shown much better results with parsimony pressure <ref> [Bli96, SFD96a] </ref>, but the reasons for the differences in performance have not been adequately explained. Our goal is to better understand how parsimony pressure affects evolving populations and to be able to predict when it will or will not function effectively.
Reference: [BT94] <author> Tobias Blickle and Lothar Thiele. </author> <title> Genetic programming and redundancy. </title> <editor> In Jorn Hopf, editor, </editor> <booktitle> Genetic Algorithms within the Framework of Evolutionary Computation, </booktitle> <pages> pages 33 - 38, </pages> <year> 1994. </year>
Reference-contexts: Additionally, it appears to have an impact on the search process which is currently not well understood. 3 Code Growth The phenomenon of code growth as illustrated in Figure 6 is well documented in the GP literature <ref> [Koz92, BT94, NB95, MM95, SFD96a] </ref>. It is also well documented that most of this growth is in code which does not contribute to the individual's fitness. <p> However, each is distinct. It is also worth noting that these theories are not mutually contradictory. Thus, any or all of them could apply to GP. 3.3.1 The Destructive Hypothesis In roughly equivalent theories, Nordin and Banzhaf [NB95], McPhee and Miller [MM95], and Blicke and Thiele <ref> [BT94] </ref> have argued that code growth occurs to protect programs against the potentially destructive effects of operations other than selection- for standard GP this means crossover. <p> When crossover is non-destructive the amount of code growth is significantly reduced. However, the more general formulations of the destructive hypothesis, notably those by McPhee and Miller [MM95] and by Blicke and Thiele <ref> [BT94] </ref>, suggest that any, primarily destructive operation, can lead to code growth. In this section the effect of the point mutation on code growth is examined. Under point mutation there is a fixed probability (p m ) that a program will be chosen for mutation.
Reference: [Chr76] <author> N. Christofides. </author> <title> Worst-case analysis of a new heuristic for the traveling salesman problem. </title> <type> Technical report, </type> <institution> Carnegie-Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1976. </year>
Reference-contexts: Both are NP complete, but they differ in the degree to which they can be approximated. TSP can be approximated in polynomial time to within a factor of * of optimal, for arbitrary *, iff P = NP (see Papadimitriou [Pap94]). However, Christofides <ref> [Chr76] </ref> has shown that a polynomial time approximation of TSP can come within a constant factor (3/2) of the actual solution. Thus, TSP is a problem for which arbitrarily good approximations are impossible, though a constant ratio is possible. But for MC even crude approximations are impossible.
Reference: [EA96] <author> Joshua M. Epstein and Robert Axtell. </author> <title> Growing Artificial Societies. </title> <publisher> The MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: The survival, and more importantly the reproduction, of programs can be based directly on their behavior without assigning a numerical fitness value to that behavior. This approach has been used in the growth and evolution of artificial societies and some artificial life experiments, but is rarely seen in GP <ref> [EA96] </ref>. However, survival is not the only force influencing the system. Growth due to removal bias is not based on survival. Rather the growth is caused by the bias created by the lack of symmetry in the importance of the code removed during crossover versus the code added during crossover.
Reference: [FN97] <author> Dario Floreano and Stefano Nolfi. </author> <title> God save the red queen! competition in co-evolutionary robotics. </title> <editor> In John R. Koza, Kalyanmoy Deb, Marco Dorigo, David B. Fogel, Max Garzon, Hitoshi Iba, and Rick R. Riolo, editors, </editor> <booktitle> Genetic Programming 1997: Proceedings of the Second Annual Conference, </booktitle> <pages> pages 398-406. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1997. </year>
Reference-contexts: GP has been applied to a variety of problems which is too wide to reasonably enumerate; a few of the more interesting applications include: robot and satellite guidance <ref> [Iba97, FN97, How96] </ref>, data mining [RE96], neural net design [GWP96], circuit design [Tho96, KAIMA96, KIAMA96], and economic strategy development [Ash97, Len97]. GP's flexibility derives from its very simple nature. Any problem whose potential solutions can be adequately measured and compared is a potential candidate for GP.
Reference: [Gol89] <author> David E. Goldberg. </author> <title> Genetic Algorithms in Search, Optimization, and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: most common form of fitness based selection, the probability of an individual with fitness f i being selected for survival from a population of N individuals with fitnesses (f 1 ; :::; f j ; :::; f N ) is equal to f i = P N j=0 f j <ref> [Gol89, Koz92] </ref>. The analogy is a roulette wheel in which each individual gets a section whose width is proportional to the individual's fitness. Fitter individuals get a wider section and a better chance of survival. <p> In contrast to roulette wheel selection, tournament selection is based on relative fitness values (rank) rather than absolute fitness values. In tournament selection n individuals are randomly chosen from the total population of N individuals <ref> [Gol89, Koz92] </ref>. Of those 6 n the individual with the highest fitness is included in the next generations population. The n individuals act as a tournament in which the fittest individual survives. <p> Typical values for n range from two to five. Another rank based approach is stochastic remainder <ref> [Gol89, Koz92] </ref>. The population is sorted by fitness and each individual is assigned a rank from 0 to N . Each individual has 2 2 fl rank=N copies selected for the next generation. Fractional values represent the probability of being selected.
Reference: [GR96] <author> Chris Gathercole and Peter Ross. </author> <title> An adverse interaction between crossover and restricted tree depth in genetic programming. </title> <editor> In John R. Koza, David E. Goldberg, David B. Fogel, and Rick R. Riolo, editors, </editor> <booktitle> Genetic Programming 1996: Proceedings of the First Annual Conference, </booktitle> <pages> pages 291-296. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Although parsimony pressure in general does lead to smaller programs it is also very easy to over-correct and degrade the search performance [Koz92, NB95]. A scaled function avoids this problem, but may 19 make searches difficult near the size limits <ref> [GR96, LP97a] </ref>. Where parsimony pressure attempts to use evolutionary forces to limit code growth, operator modification attempts to remove, or at least mitigate, the forces causing code growth.
Reference: [GR97] <author> Chris Gathercole and Peter Ross. </author> <title> Tackling the boolean even n parity problem with genetic programming and limited-error fitness. </title> <editor> In John R. Koza, Kalyanmoy Deb, Marco Dorigo, David B. Fogel, Max Garzon, Hitoshi Iba, and Rick R. Riolo, editors, </editor> <booktitle> Genetic Programming 1997: Proceedings of the Second Annual Conference, </booktitle> <pages> pages 119-127. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1997. </year>
Reference: [GWP96] <author> Frederic Gruau, Darrell Whitley, and Larry Pyeatt. </author> <title> A comparison between cellular encoding and direct encoding for genetic neural networks. </title> <editor> In John R. Koza, David E. Goldberg, David B. Fogel, and Rick R. Riolo, editors, </editor> <booktitle> Genetic Programming 1996: Proceedings of the First Annual Conference, </booktitle> <pages> pages 81-89. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: GP has been applied to a variety of problems which is too wide to reasonably enumerate; a few of the more interesting applications include: robot and satellite guidance [Iba97, FN97, How96], data mining [RE96], neural net design <ref> [GWP96] </ref>, circuit design [Tho96, KAIMA96, KIAMA96], and economic strategy development [Ash97, Len97]. GP's flexibility derives from its very simple nature. Any problem whose potential solutions can be adequately measured and compared is a potential candidate for GP.
Reference: [Has96] <author> J. Hastad. </author> <title> Clique is indeed hard. </title> <type> Manuscript, </type> <note> improving his paper from STOC 1996, 1996. 64 </note>
Reference-contexts: This is a particularly difficult problem, since no polynomial time, deterministic algorithm exists for finding a good approximate solution unless P = NP (which is very unlikely), as proven by Arora et al. [ALM + 92]. H astad <ref> [Has96] </ref> has recently strengthened this result by proving that for any constant * there is a polynomial time approximation of MC with accuracy n * iff P = NP, where the "accuracy" of an approximation is the ratio of the actual size of the maximum clique to the one returned by
Reference: [HFF97] <author> Dale C. Hooper, Nicholas S. Flann, and Stephanie R. Fuller. </author> <title> Recombina-tive hill-climbing: A stronger search method for genetic programming. </title> <editor> In John R. Koza, Kalyanmoy Deb, Marco Dorigo, David B. Fogel, Max Gar-zon, Hitoshi Iba, and Rick R. Riolo, editors, </editor> <booktitle> Genetic Programming 1997: Proceedings of the Second Annual Conference, </booktitle> <pages> pages 174-179. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1997. </year>
Reference-contexts: Notable operator modification approaches include varying the rate of crossover (and mutation) [RB96], varying the selection of crossover points (such as the use of explicitly defined introns) [NFB95], and negating destructive crossover events (a form of hill climbing) <ref> [SF97a, OO95, HFF97] </ref>. Each of these approaches has the goal of reducing the evolutionary importance of inviable code. Code modification involves changing the structure of the code either during or after evolution usually by editing out the inoperative code. <p> In the second form of non-destructive crossover an offspring is only kept if its fitness equals or exceeds its parent's fitness. These are similar to hill climbing crossover used by several other researchers to improve performance <ref> [OO95, HFF97] </ref>. With either form of non-destructive crossover there is no longer a possibility that crossover may disrupt a successful individual and cause it to be removed from the population. Thus, the evolutionary advantage of protective code no longer exists. <p> This is the less rigorous form of non-destructive crossover and is similar to the crossover hill climbing used by O'Reilly and Oppacher [OO95] and Hooper et al. <ref> [HFF97] </ref>. However, unlike O'Reilly and Oppacher's version only one attempt was made to produce more successful offspring. If this attempt fails the parent is kept instead of being replaced by a randomly created individual, as was done in O'Reilly and Oppacher's modified crossover operation.
Reference: [How96] <author> Brian Howley. </author> <title> Genetic programming of near-minimum-time spacecraft attitude maneuvers. </title> <editor> In John R. Koza, David E. Goldberg, David B. Fogel, and Rick R. Riolo, editors, </editor> <booktitle> Genetic Programming 1996: Proceedings of the First Annual Conference, </booktitle> <pages> pages 98-106. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: GP has been applied to a variety of problems which is too wide to reasonably enumerate; a few of the more interesting applications include: robot and satellite guidance <ref> [Iba97, FN97, How96] </ref>, data mining [RE96], neural net design [GWP96], circuit design [Tho96, KAIMA96, KIAMA96], and economic strategy development [Ash97, Len97]. GP's flexibility derives from its very simple nature. Any problem whose potential solutions can be adequately measured and compared is a potential candidate for GP. <p> Because crossover uses previously tested programs the search is guided by previous successes, making it very effective. GP has produced solutions on par with the best solutions developed using other methods (including hand written solutions) on problems including spacecraft attitude maneuvers and general classification problems <ref> [AIK96, How96] </ref>. Despite GP's early successes it has not scaled well to larger problems. A major factor in this failure has been code growth. Code growth can be described as the tendency of programs generated using GP to grow much larger than is functionally necessary.
Reference: [HS97] <author> Kim Harris and Peter Smith. </author> <title> Exploring alternative operators and search strategies in genetic programming. </title> <editor> In John R. Koza, Kalyanmoy Deb, Marco Dorigo, David B. Fogel, Max Garzon, Hitoshi Iba, and Rick R. Riolo, editors, </editor> <booktitle> Genetic Programming 1997: Proceedings of the Second Annual Conference, </booktitle> <pages> pages 147-155. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1997. </year>
Reference: [Iba97] <editor> Hitoshi Iba. </editor> <title> Multiple-agent learning for a robot navigation task by genetic programming. </title> <editor> In John R. Koza, Kalyanmoy Deb, Marco Dorigo, David B. Fogel, Max Garzon, Hitoshi Iba, and Rick R. Riolo, editors, </editor> <booktitle> Genetic Programming 1997: Proceedings of the Second Annual Conference, </booktitle> <pages> pages 195-200. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1997. </year>
Reference-contexts: GP has been applied to a variety of problems which is too wide to reasonably enumerate; a few of the more interesting applications include: robot and satellite guidance <ref> [Iba97, FN97, How96] </ref>, data mining [RE96], neural net design [GWP96], circuit design [Tho96, KAIMA96, KIAMA96], and economic strategy development [Ash97, Len97]. GP's flexibility derives from its very simple nature. Any problem whose potential solutions can be adequately measured and compared is a potential candidate for GP.
Reference: [IdGS94] <author> Hitoshi Iba, Hugo de Garis, and Taisuke Sato. </author> <title> Genetic programming using a minimum description length principle. </title> <editor> In Jr. Kenneth E. Kinnear, editor, </editor> <booktitle> Advances in Genetic Programming, </booktitle> <pages> pages 265-284. </pages> <address> Cambridge, MA: </address> <publisher> The MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Commonly the penalty is a simple linear function of the solution size, but other more, and less, subtle approaches have been used. Of these, variable penalty functions, which respond to the fitness and size of individuals within the population appear to be the most robust <ref> [IdGS94, ZM95, Bli96] </ref>. Parsimony pressure is also often scaled to assure the deletion of large programs without changing the fitness of smaller programs [Koz92]. Programs exceeding a certain size and/or depth are discarded, which is equivalent to penalizing them with a zero fitness.
Reference: [KAIMA96] <author> John R. Koza, David Andre, Forrest H. Bennett III, and Keane Martin A. </author> <title> Use of automatically defined functions and architecture-altering operations in automated circuit synthesis with genetic programming. </title> <editor> In John R. Koza, David E. Goldberg, David B. Fogel, and Rick R. Riolo, editors, </editor> <booktitle> Genetic Programming 1996: Proceedings of the First Annual Conference, </booktitle> <pages> pages 132-140. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: GP has been applied to a variety of problems which is too wide to reasonably enumerate; a few of the more interesting applications include: robot and satellite guidance [Iba97, FN97, How96], data mining [RE96], neural net design [GWP96], circuit design <ref> [Tho96, KAIMA96, KIAMA96] </ref>, and economic strategy development [Ash97, Len97]. GP's flexibility derives from its very simple nature. Any problem whose potential solutions can be adequately measured and compared is a potential candidate for GP.
Reference: [KIAMA96] <author> John R. Koza, Forrest H. Bennett III, David Andre, and Keane Martin A. </author> <title> Automated wywiwyg design of both the topology and component values of electrical circuits using genetic programming. </title> <editor> In John R. Koza, David E. Goldberg, David B. Fogel, and Rick R. Riolo, editors, </editor> <booktitle> Genetic Programming 1996: Proceedings of the First Annual Conference, </booktitle> <pages> pages 123-131. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: GP has been applied to a variety of problems which is too wide to reasonably enumerate; a few of the more interesting applications include: robot and satellite guidance [Iba97, FN97, How96], data mining [RE96], neural net design [GWP96], circuit design <ref> [Tho96, KAIMA96, KIAMA96] </ref>, and economic strategy development [Ash97, Len97]. GP's flexibility derives from its very simple nature. Any problem whose potential solutions can be adequately measured and compared is a potential candidate for GP.
Reference: [Koz92] <author> John R. Koza. </author> <title> Genetic Programming: On the Programming of Computers by Means of Natural Selection. </title> <address> Cambridge, MA: </address> <publisher> The MIT Press, </publisher> <year> 1992. </year> <month> 65 </month>
Reference-contexts: 1 Introduction Genetic programming (GP) is a stochastic process for automatically generating computer programs. It is loosely based on the process of natural evolution, using analogies of sexual reproduction and natural selection to guide the development of programs <ref> [Koz92] </ref>. GP has been applied to a variety of problems which is too wide to reasonably enumerate; a few of the more interesting applications include: robot and satellite guidance [Iba97, FN97, How96], data mining [RE96], neural net design [GWP96], circuit design [Tho96, KAIMA96, KIAMA96], and economic strategy development [Ash97, Len97]. <p> The basic GP languages are designed so that as long as internal nodes consist of functional 4 Table 1: Operations for the maze navigation problem. Terminals forward,back,left,right,no-op Functions if,while,progn Predicates wall ahead,no wall ahead operations and leaf nodes consist of terminal operators the resulting program is syntactically correct <ref> [Koz92] </ref>. More complicated GP languages with typed operators can be used, although they require limitations on which operators can be linked together in the syntax tree. For our navigation problem only a simple language is required. Its elements are summarized in Table 1. <p> most common form of fitness based selection, the probability of an individual with fitness f i being selected for survival from a population of N individuals with fitnesses (f 1 ; :::; f j ; :::; f N ) is equal to f i = P N j=0 f j <ref> [Gol89, Koz92] </ref>. The analogy is a roulette wheel in which each individual gets a section whose width is proportional to the individual's fitness. Fitter individuals get a wider section and a better chance of survival. <p> In contrast to roulette wheel selection, tournament selection is based on relative fitness values (rank) rather than absolute fitness values. In tournament selection n individuals are randomly chosen from the total population of N individuals <ref> [Gol89, Koz92] </ref>. Of those 6 n the individual with the highest fitness is included in the next generations population. The n individuals act as a tournament in which the fittest individual survives. <p> Typical values for n range from two to five. Another rank based approach is stochastic remainder <ref> [Gol89, Koz92] </ref>. The population is sorted by fitness and each individual is assigned a rank from 0 to N . Each individual has 2 2 fl rank=N copies selected for the next generation. Fractional values represent the probability of being selected. <p> The purpose of this type of mutation is to avoid having a particular type of node vanish entirely from the population. However, this is not a serious concern in GP <ref> [Koz92] </ref> and 8 single node mutation is no longer in common use. The second common type of mutation is subtree mutation. Like crossover subtree mutation removes a randomly chosen subtree from one individual. However, the replacement subtree is randomly generated instead of being donated by a different individual. <p> Additionally, it appears to have an impact on the search process which is currently not well understood. 3 Code Growth The phenomenon of code growth as illustrated in Figure 6 is well documented in the GP literature <ref> [Koz92, BT94, NB95, MM95, SFD96a] </ref>. It is also well documented that most of this growth is in code which does not contribute to the individual's fitness. <p> Of these, variable penalty functions, which respond to the fitness and size of individuals within the population appear to be the most robust [IdGS94, ZM95, Bli96]. Parsimony pressure is also often scaled to assure the deletion of large programs without changing the fitness of smaller programs <ref> [Koz92] </ref>. Programs exceeding a certain size and/or depth are discarded, which is equivalent to penalizing them with a zero fitness. This form of parsimony pressure is sometimes referred to as capping because it places caps on the allowed size (or depth) of the programs. <p> This form of parsimony pressure is sometimes referred to as capping because it places caps on the allowed size (or depth) of the programs. Although parsimony pressure in general does lead to smaller programs it is also very easy to over-correct and degrade the search performance <ref> [Koz92, NB95] </ref>. A scaled function avoids this problem, but may 19 make searches difficult near the size limits [GR96, LP97a]. Where parsimony pressure attempts to use evolutionary forces to limit code growth, operator modification attempts to remove, or at least mitigate, the forces causing code growth. <p> and show that it is a partial indicator of success or failure under parsimony pressure. 46 5.5.1 The Effect of Parsimony Pressure Parsimony pressure is becoming a very common method for restraining code growth in GP, despite that fact that early experiments with its use often lead to poor performance <ref> [NB95, Koz92] </ref>. Other, more recent, experiments have shown much better results with parsimony pressure [Bli96, SFD96a], but the reasons for the differences in performance have not been adequately explained. <p> However, by generation 19 all of the performance differences are significant (Student's two-tailed t test, p &lt; 0.01). Table 5 includes the average performance for all three test cases at all five levels of parsimony pressure. These results are similar to other negative results with parsimony pressure <ref> [Koz92, NB95] </ref>. Program size is controlled, but performance suffers. Both the benefits and the costs of parsimony pressure are clearly correlated with the amount of parsimony pressure being applied. An initial rise in performance is seen in all five trials of Figure 24.
Reference: [Koz94] <author> John R. Koza. </author> <title> Genetic Programming II: Automatic Discovery of Reusable Programs. </title> <address> Cambridge, MA: </address> <publisher> The MIT Press, </publisher> <year> 1994. </year>
Reference: [Len97] <author> Terje Lensberg. </author> <title> A genetic programming experiment on investment behavior under knightian uncertainty. </title> <editor> In John R. Koza, Kalyanmoy Deb, Marco Dorigo, David B. Fogel, Max Garzon, Hitoshi Iba, and Rick R. Riolo, editors, </editor> <booktitle> Genetic Programming 1997: Proceedings of the Second Annual Conference, </booktitle> <pages> pages 231-239. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1997. </year>
Reference-contexts: GP has been applied to a variety of problems which is too wide to reasonably enumerate; a few of the more interesting applications include: robot and satellite guidance [Iba97, FN97, How96], data mining [RE96], neural net design [GWP96], circuit design [Tho96, KAIMA96, KIAMA96], and economic strategy development <ref> [Ash97, Len97] </ref>. GP's flexibility derives from its very simple nature. Any problem whose potential solutions can be adequately measured and compared is a potential candidate for GP. In lieu of biological reproduction GP uses a crossover operation which exchanges sections of code between programs.
Reference: [LP97a] <author> W. B. Langdon and R. Poli. </author> <title> An analysis of the max problem in genetic programming. </title> <editor> In John R. Koza, Kalyanmoy Deb, Marco Dorigo, David B. Fogel, Max Garzon, Hitoshi Iba, and Rick R. Riolo, editors, </editor> <booktitle> Genetic Programming 1997: Proceedings of the Second Annual Conference, </booktitle> <pages> pages 222-230. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1997. </year>
Reference-contexts: Although parsimony pressure in general does lead to smaller programs it is also very easy to over-correct and degrade the search performance [Koz92, NB95]. A scaled function avoids this problem, but may 19 make searches difficult near the size limits <ref> [GR96, LP97a] </ref>. Where parsimony pressure attempts to use evolutionary forces to limit code growth, operator modification attempts to remove, or at least mitigate, the forces causing code growth.
Reference: [LP97b] <author> W. B. Langdon and R. Poli. </author> <title> Fitness causes bloat. </title> <type> Technical Report CSRP-97-09, </type> <institution> University of Birmingham, Birmingham, UK, </institution> <year> 1997. </year>
Reference-contexts: Thus, there must be an additional factor that pushes these two operations towards larger programs. Langdon and Poli have demonstrated that for at least one problem the inclusion of fitness <ref> [LP97b] </ref> causes code growth. Random selection did not cause code growth. However, as has already been noted, the growth does not directly contribute to fitness. <p> Intuitively, removing a section of code from a reasonably functional program and replacing it with code from a different program would rarely be expected to increase the first program's performance. Several studies have confirmed this intuition showing that few crossovers increase fitness and many lower fitness <ref> [NB95, LP97b] </ref>. Thus, on average crossover is a neutral or destructive operation. However, it is 14 also clear that a crossover which only affects inviable code cannot be destructive. <p> Tests using non-destructive crossover on the maze navigation problem significantly decreased code growth, lending strong support for the destructive hypothesis [SF97a]. 3.3.2 The Solution Distribution Langdon and Poli have argued that code growth is partially caused by the distribution of semantically equivalent solutions in the solution space <ref> [LP97b] </ref>. In most variable length representations a particular solution can be represented by many semantically equivalent, but syntactically different, programs. The existence of inviable and inoperative code guarantees that for any given program size there are many more larger versions of the solution than there are smaller ones.
Reference: [LS97] <author> Sean Luke and Lee Spector. </author> <title> A comparison of crossover and mutation in genetic programming. </title> <editor> In John R. Koza, Kalyanmoy Deb, Marco Dorigo, David B. Fogel, Max Garzon, Hitoshi Iba, and Rick R. Riolo, editors, </editor> <booktitle> Genetic Programming 1997: Proceedings of the Second Annual Conference, </booktitle> <pages> pages 240-245. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1997. </year>
Reference-contexts: In fact, this limitation of crossover has been a major factor in the current debate between the use of crossover versus the use of subtree mutation <ref> [Ang97, LS97] </ref>. In subtree mutation, which is sometimes used as a replacement for crossover, a subtree is removed and replaced by a randomly generated subtree. The less obvious affects of the current population on future evolution are also much less widely recognized.
Reference: [MM95] <author> Nicholas Freitag McPhee and Justin Darwin Miller. </author> <title> Accurate replication in genetic programming. </title> <editor> In Larry J. Eshelman, editor, </editor> <booktitle> Proceedings of the Sixth International Conference on Genetic Algorithms, </booktitle> <pages> pages 303-309. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: Additionally, it appears to have an impact on the search process which is currently not well understood. 3 Code Growth The phenomenon of code growth as illustrated in Figure 6 is well documented in the GP literature <ref> [Koz92, BT94, NB95, MM95, SFD96a] </ref>. It is also well documented that most of this growth is in code which does not contribute to the individual's fitness. <p> However, each is distinct. It is also worth noting that these theories are not mutually contradictory. Thus, any or all of them could apply to GP. 3.3.1 The Destructive Hypothesis In roughly equivalent theories, Nordin and Banzhaf [NB95], McPhee and Miller <ref> [MM95] </ref>, and Blicke and Thiele [BT94] have argued that code growth occurs to protect programs against the potentially destructive effects of operations other than selection- for standard GP this means crossover. <p> When crossover is non-destructive the amount of code growth is significantly reduced. However, the more general formulations of the destructive hypothesis, notably those by McPhee and Miller <ref> [MM95] </ref> and by Blicke and Thiele [BT94], suggest that any, primarily destructive operation, can lead to code growth. In this section the effect of the point mutation on code growth is examined.
Reference: [NB95] <author> Peter Nordin and Wolfgang Banzhaf. </author> <title> Complexity compression and evolution. </title> <editor> In Larry J. Eshelman, editor, </editor> <booktitle> Proceedings of the Sixth International Conference on Genetic Algorithms, </booktitle> <pages> pages 310-317. </pages> <address> San Francisco, CA: </address> <publisher> Mor-gan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: Additionally, it appears to have an impact on the search process which is currently not well understood. 3 Code Growth The phenomenon of code growth as illustrated in Figure 6 is well documented in the GP literature <ref> [Koz92, BT94, NB95, MM95, SFD96a] </ref>. It is also well documented that most of this growth is in code which does not contribute to the individual's fitness. <p> However, each is distinct. It is also worth noting that these theories are not mutually contradictory. Thus, any or all of them could apply to GP. 3.3.1 The Destructive Hypothesis In roughly equivalent theories, Nordin and Banzhaf <ref> [NB95] </ref>, McPhee and Miller [MM95], and Blicke and Thiele [BT94] have argued that code growth occurs to protect programs against the potentially destructive effects of operations other than selection- for standard GP this means crossover. <p> Intuitively, removing a section of code from a reasonably functional program and replacing it with code from a different program would rarely be expected to increase the first program's performance. Several studies have confirmed this intuition showing that few crossovers increase fitness and many lower fitness <ref> [NB95, LP97b] </ref>. Thus, on average crossover is a neutral or destructive operation. However, it is 14 also clear that a crossover which only affects inviable code cannot be destructive. <p> This form of parsimony pressure is sometimes referred to as capping because it places caps on the allowed size (or depth) of the programs. Although parsimony pressure in general does lead to smaller programs it is also very easy to over-correct and degrade the search performance <ref> [Koz92, NB95] </ref>. A scaled function avoids this problem, but may 19 make searches difficult near the size limits [GR96, LP97a]. Where parsimony pressure attempts to use evolutionary forces to limit code growth, operator modification attempts to remove, or at least mitigate, the forces causing code growth. <p> and show that it is a partial indicator of success or failure under parsimony pressure. 46 5.5.1 The Effect of Parsimony Pressure Parsimony pressure is becoming a very common method for restraining code growth in GP, despite that fact that early experiments with its use often lead to poor performance <ref> [NB95, Koz92] </ref>. Other, more recent, experiments have shown much better results with parsimony pressure [Bli96, SFD96a], but the reasons for the differences in performance have not been adequately explained. <p> However, by generation 19 all of the performance differences are significant (Student's two-tailed t test, p &lt; 0.01). Table 5 includes the average performance for all three test cases at all five levels of parsimony pressure. These results are similar to other negative results with parsimony pressure <ref> [Koz92, NB95] </ref>. Program size is controlled, but performance suffers. Both the benefits and the costs of parsimony pressure are clearly correlated with the amount of parsimony pressure being applied. An initial rise in performance is seen in all five trials of Figure 24.
Reference: [NFB95] <author> Peter Nordin, Frank Francone, and Wolfgang Banzhaf. </author> <title> Explicitly defined introns and destructive crossover in genetic programming. </title> <type> Technical Report SysReport 3/95, </type> <institution> University of Dortmund, Germany, </institution> <year> 1995. </year>
Reference-contexts: Notable operator modification approaches include varying the rate of crossover (and mutation) [RB96], varying the selection of crossover points (such as the use of explicitly defined introns) <ref> [NFB95] </ref>, and negating destructive crossover events (a form of hill climbing) [SF97a, OO95, HFF97]. Each of these approaches has the goal of reducing the evolutionary importance of inviable code. Code modification involves changing the structure of the code either during or after evolution usually by editing out the inoperative code.
Reference: [OO95] <author> Una-May O'Reilly and Frank Oppacher. </author> <title> Hybridized crossover-based search techniques for program discovery. </title> <type> Technical Report 95-02-007, </type> <institution> Santa Fe Institute, </institution> <address> Santa Fe, New Mexico, </address> <year> 1995. </year>
Reference-contexts: Notable operator modification approaches include varying the rate of crossover (and mutation) [RB96], varying the selection of crossover points (such as the use of explicitly defined introns) [NFB95], and negating destructive crossover events (a form of hill climbing) <ref> [SF97a, OO95, HFF97] </ref>. Each of these approaches has the goal of reducing the evolutionary importance of inviable code. Code modification involves changing the structure of the code either during or after evolution usually by editing out the inoperative code. <p> In the second form of non-destructive crossover an offspring is only kept if its fitness equals or exceeds its parent's fitness. These are similar to hill climbing crossover used by several other researchers to improve performance <ref> [OO95, HFF97] </ref>. With either form of non-destructive crossover there is no longer a possibility that crossover may disrupt a successful individual and cause it to be removed from the population. Thus, the evolutionary advantage of protective code no longer exists. <p> An offspring is incorporated into the new population only if its fitness equals or exceeds that of its parent, otherwise the parent is kept. This is the less rigorous form of non-destructive crossover and is similar to the crossover hill climbing used by O'Reilly and Oppacher <ref> [OO95] </ref> and Hooper et al. [HFF97]. However, unlike O'Reilly and Oppacher's version only one attempt was made to produce more successful offspring. If this attempt fails the parent is kept instead of being replaced by a randomly created individual, as was done in O'Reilly and Oppacher's modified crossover operation.
Reference: [Pap94] <author> Christos H. Papadimitriou. </author> <title> Computational Complexity. </title> <publisher> Addison-Wesley, </publisher> <year> 1994. </year> <month> 66 </month>
Reference-contexts: Both are NP complete, but they differ in the degree to which they can be approximated. TSP can be approximated in polynomial time to within a factor of * of optimal, for arbitrary *, iff P = NP (see Papadimitriou <ref> [Pap94] </ref>). However, Christofides [Chr76] has shown that a polynomial time approximation of TSP can come within a constant factor (3/2) of the actual solution. Thus, TSP is a problem for which arbitrarily good approximations are impossible, though a constant ratio is possible. But for MC even crude approximations are impossible.
Reference: [RB95] <author> Justinian P. Rosca and Dana H. Ballard. </author> <title> Causality in genetic programming. </title> <editor> In Larry J. Eshelman, editor, </editor> <booktitle> Proceedings of the Sixth International Conference on Genetic Algorithms, </booktitle> <pages> pages 256-263. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: For example, the average depth of a subtree removed from a complete, binary tree of depth D is 2 D 2 D 1 <ref> [RB95] </ref>, whereas for a minimal, binary tree it is D 2 +D 4D+2 [SF97a]. Even two subtrees of the same size within a tree may have very different average subtree sizes if their shapes are different. To limit this variation within a tree we define the property of pseudo self-similarity. <p> Or tree shapes may evolve to improve evolution. For example, Rosca and Ballard have suggested that denser trees may be beneficial because they have relatively smaller crossover branches and therefore could be less disrupted by crossover <ref> [RB95] </ref>. The densities of the program trees evolved for different problems may determine if density is problem dependent. However, it is likely that any differences in density caused by the solution requirements would only appear in the viable or operative code. <p> What is surprising is that the trees are progressively less dense, indicating that the increase in depth must be occurring relatively faster than the increase in size. It is unclear why this trend towards sparser trees, which contradicts the hypothesis proposed by Rosca and Ballard <ref> [RB95] </ref> that denser trees should be favored, occurs. One possible explanation is that there is a fixed contour in the program space which represents an equilibrium between size and depth and the programs are tending towards this line.
Reference: [RB96] <author> Justinian P. Rosca and Dana H. Ballard. </author> <title> Complexity drift in evolutionary computation with tree representations. </title> <type> Technical Report NRL5, </type> <institution> University of Rochester, Rochester, </institution> <address> New York, </address> <year> 1996. </year>
Reference-contexts: Where parsimony pressure attempts to use evolutionary forces to limit code growth, operator modification attempts to remove, or at least mitigate, the forces causing code growth. Notable operator modification approaches include varying the rate of crossover (and mutation) <ref> [RB96] </ref>, varying the selection of crossover points (such as the use of explicitly defined introns) [NFB95], and negating destructive crossover events (a form of hill climbing) [SF97a, OO95, HFF97]. Each of these approaches has the goal of reducing the evolutionary importance of inviable code. <p> As previously noted in a full tree (a tree with a density of 1) the average depth of a randomly selected node, such as used in crossover, is 2 D 2 D 1 which approaches 2 in the limit of increasing depth <ref> [RB96] </ref>. Thus, for full trees, as they grow larger and deeper crossover is concentrated at the branch tips. In contrast, in minimal trees the average depth of a randomly selected branch is D 2 +D 4D+2 which approaches D 4 in the limit of increasing depth [SF97a]. <p> Thus, for minimal trees the trend is towards removing relatively large branches. The notion of programs evolving to minimize the effects of crossover suggest that fuller trees should be favored <ref> [RB96] </ref>. Another feature of the difference in crossover depths is that sparser trees should lead to greater fluctuations in size between parents and offspring. This trend should produce larger growth as the protective benefits of the larger trees improve their selection probabilities. vertical axis is logarithmic, base 2.
Reference: [RE96] <author> Tae Wan Ryu and Cristoph F. Eick. </author> <title> Masson: Discovering comminalities in collection of objects using genetic programming. </title> <editor> In John R. Koza, David E. Goldberg, David B. Fogel, and Rick R. Riolo, editors, </editor> <booktitle> Genetic Programming 1996: Proceedings of the First Annual Conference, </booktitle> <pages> pages 200-208. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: GP has been applied to a variety of problems which is too wide to reasonably enumerate; a few of the more interesting applications include: robot and satellite guidance [Iba97, FN97, How96], data mining <ref> [RE96] </ref>, neural net design [GWP96], circuit design [Tho96, KAIMA96, KIAMA96], and economic strategy development [Ash97, Len97]. GP's flexibility derives from its very simple nature. Any problem whose potential solutions can be adequately measured and compared is a potential candidate for GP.
Reference: [SF97a] <author> Terence Soule and James A. Foster. </author> <title> Code size and depth flows in genetic programming. </title> <editor> In John R. Koza, Kalyanmoy Deb, Marco Dorigo, David B. Fogel, Max Garzon, Hitoshi Iba, and Rick R. Riolo, editors, </editor> <booktitle> Genetic Programming 1997: Proceedings of the Second Annual Conference, </booktitle> <pages> pages 313-320. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1997. </year>
Reference-contexts: Thus, survival does not depend on avoiding the destructive effects of crossover and there is no evolutionary benefit to code growth. Tests using non-destructive crossover on the maze navigation problem significantly decreased code growth, lending strong support for the destructive hypothesis <ref> [SF97a] </ref>. 3.3.2 The Solution Distribution Langdon and Poli have argued that code growth is partially caused by the distribution of semantically equivalent solutions in the solution space [LP97b]. In most variable length representations a particular solution can be represented by many semantically equivalent, but syntactically different, programs. <p> For example, the average depth of a subtree removed from a complete, binary tree of depth D is 2 D 2 D 1 [RB95], whereas for a minimal, binary tree it is D 2 +D 4D+2 <ref> [SF97a] </ref>. Even two subtrees of the same size within a tree may have very different average subtree sizes if their shapes are different. To limit this variation within a tree we define the property of pseudo self-similarity. <p> Notable operator modification approaches include varying the rate of crossover (and mutation) [RB96], varying the selection of crossover points (such as the use of explicitly defined introns) [NFB95], and negating destructive crossover events (a form of hill climbing) <ref> [SF97a, OO95, HFF97] </ref>. Each of these approaches has the goal of reducing the evolutionary importance of inviable code. Code modification involves changing the structure of the code either during or after evolution usually by editing out the inoperative code. <p> Additionally, an increase in the amount of viable code may be observed because it will not need to "hide" from crossover. Previous work with the maze navigation problem has shown that non-destructive crossover does lead to greatly diminished growth <ref> [SF97a] </ref>. The continuation of these experiments on other test problems confirms that the destructive hypothesis is valid. * Can operators other than crossover produce similar growth? The most general interpretation of the destructive hypothesis suggests that any, primarily destructive, operator will produce code growth. <p> Thus, for full trees, as they grow larger and deeper crossover is concentrated at the branch tips. In contrast, in minimal trees the average depth of a randomly selected branch is D 2 +D 4D+2 which approaches D 4 in the limit of increasing depth <ref> [SF97a] </ref>. Thus, for minimal trees the trend is towards removing relatively large branches. The notion of programs evolving to minimize the effects of crossover suggest that fuller trees should be favored [RB96].
Reference: [SF97b] <author> Terence Soule and James A. Foster. </author> <title> Genetic algorithm hardness measures applied to the maximum clique problem. </title> <editor> In Thomas Back, editor, </editor> <booktitle> Proceeed-ings of the Seventh Annual Conference on Genetic Algorithms, </booktitle> <pages> pages 81-87. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1997. </year>
Reference-contexts: Thus, MC has the advantage of extreme, but scalable, difficulty. We have done considerable work as part of a separate research project to characterize 25 the difficultly of various graph types and to compare GP and genetic algorithm (GA) performance on maximum clique <ref> [SFD96b, SF97b] </ref>. This work was extremely helpful in deciding which graphs to use as test instances and in interpreting results. A large suite of test graphs exists for MC thanks to the efforts of the Center for Discrete Mathematics and Theoretical Computer Science (DIMACS) at Rutgers University. <p> Larger cliques will contain more vertices so they will have a higher fitness than smaller cliques. This fitness function has already proven to be relatively effective when used with genetic algorithms solving the maximum clique problem <ref> [SFD96b, SF97b] </ref>. 4.2.2 Experimental Parameters A number of parameters, such as population size, crossover rate, etc. are involved in any GP experiment. In order to draw justifiable conclusions from different experiments it is necessary to keep as many of these parameters as similar as possible.
Reference: [SF97c] <author> Terence Soule and James A. Foster. </author> <title> Removal bias: a new cause of code growth in tree based evolutionary programming. </title> <booktitle> Submitted to the Conference on Evolutionary Computation, </booktitle> <year> 1997. </year>
Reference-contexts: However, the existence of more, semantically equivalent, larger programs is not sufficient to produce growth. Those larger programs must actually be easier to find by the GP search. Although it is possible that this is true it is not obviously so. 3.3.3 Removal Bias As we reported in <ref> [SF97c] </ref> a less restrictive version of non-destructive crossover (offspring are kept if their fitness is equal to their parent's) produces growth levels higher than the more restrictive version of non-destructive crossover but still lower than normal crossover.
Reference: [SFD96a] <author> Terence Soule, James A. Foster, and John Dickinson. </author> <title> Code growth in genetic programming. </title> <editor> In John R. Koza, David E. Goldberg, David B. Fogel, and Rick R. Riolo, editors, </editor> <booktitle> Genetic Programming 1996: Proceedings of the First Annual Conference, </booktitle> <pages> pages 215-223. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Additionally, it appears to have an impact on the search process which is currently not well understood. 3 Code Growth The phenomenon of code growth as illustrated in Figure 6 is well documented in the GP literature <ref> [Koz92, BT94, NB95, MM95, SFD96a] </ref>. It is also well documented that most of this growth is in code which does not contribute to the individual's fitness. <p> Furthermore, in many cases editing is relatively ineffective at reducing code growth. Some evidence suggests that this is because an incomplete editor will allow some non-contributing code to slip through. The evolutionary process then magnifies that code in particular and the benefits are much smaller than expected <ref> [SFD96a] </ref>. In programs which include loops, finding all non-contributing code is an unsolvable problem, so the issue of incomplete editors is unavoidable. <p> Other, more recent, experiments have shown much better results with parsimony pressure <ref> [Bli96, SFD96a] </ref>, but the reasons for the differences in performance have not been adequately explained. Our goal is to better understand how parsimony pressure affects evolving populations and to be able to predict when it will or will not function effectively.
Reference: [SFD96b] <author> Terence Soule, James A. Foster, and John Dickinson. </author> <title> Using genetic programming to approximate maximum clique. </title> <editor> In John R. Koza, David E. Goldberg, David B. Fogel, and Rick R. Riolo, editors, </editor> <booktitle> Genetic Programming 1996: Proceedings of the First Annual Conference, </booktitle> <pages> pages 400-405. </pages> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Thus, MC has the advantage of extreme, but scalable, difficulty. We have done considerable work as part of a separate research project to characterize 25 the difficultly of various graph types and to compare GP and genetic algorithm (GA) performance on maximum clique <ref> [SFD96b, SF97b] </ref>. This work was extremely helpful in deciding which graphs to use as test instances and in interpreting results. A large suite of test graphs exists for MC thanks to the efforts of the Center for Discrete Mathematics and Theoretical Computer Science (DIMACS) at Rutgers University. <p> Larger cliques will contain more vertices so they will have a higher fitness than smaller cliques. This fitness function has already proven to be relatively effective when used with genetic algorithms solving the maximum clique problem <ref> [SFD96b, SF97b] </ref>. 4.2.2 Experimental Parameters A number of parameters, such as population size, crossover rate, etc. are involved in any GP experiment. In order to draw justifiable conclusions from different experiments it is necessary to keep as many of these parameters as similar as possible.
Reference: [Tho96] <author> Adrian Thompson. </author> <title> Silicon evolution. </title> <editor> In John R. Koza, David E. Goldberg, David B. Fogel, and Rick R. Riolo, editors, </editor> <booktitle> Genetic Programming 1996: Proceedings of the First Annual Conference, </booktitle> <pages> pages 444-452. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: GP has been applied to a variety of problems which is too wide to reasonably enumerate; a few of the more interesting applications include: robot and satellite guidance [Iba97, FN97, How96], data mining [RE96], neural net design [GWP96], circuit design <ref> [Tho96, KAIMA96, KIAMA96] </ref>, and economic strategy development [Ash97, Len97]. GP's flexibility derives from its very simple nature. Any problem whose potential solutions can be adequately measured and compared is a potential candidate for GP.
Reference: [ZM95] <author> Byoung-Tak Zhang and Heinz Muhlenbein. </author> <title> Balancing accuracy and par-simony in genetic programming. </title> <journal> Evolutionary Computation, </journal> <volume> 3(1) </volume> <pages> 17-38, </pages> <year> 1995. </year>
Reference-contexts: Commonly the penalty is a simple linear function of the solution size, but other more, and less, subtle approaches have been used. Of these, variable penalty functions, which respond to the fitness and size of individuals within the population appear to be the most robust <ref> [IdGS94, ZM95, Bli96] </ref>. Parsimony pressure is also often scaled to assure the deletion of large programs without changing the fitness of smaller programs [Koz92]. Programs exceeding a certain size and/or depth are discarded, which is equivalent to penalizing them with a zero fitness.
References-found: 42

