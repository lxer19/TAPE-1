URL: ftp://arch.cs.ucdavis.edu/papers/AP-isca98.ps.Z
Refering-URL: http://arch.cs.ucdavis.edu/RAD/
Root-URL: http://www.cs.ucdavis.edu
Title: Active Pages: A Computation Model for Intelligent Memory  
Author: Mark Oskin, Frederic T. Chong, and Timothy Sherwood 
Affiliation: Department of Computer Science University of California at Davis  
Note: Appears in the 1998 International Symposium on Computer Architecture, Barcelona.  
Abstract: Microprocessors and memory systems suffer from a growing gap in performance. We introduce Active Pages, a computation model which addresses this gap by shifting data-intensive computations to the memory system. An Active Page consists of a page of data and a set of associated functions which can operate upon that data. We describe an implementation of Active Pages on RADram (Reconfigurable Architecture DRAM), a memory system based upon the integration of DRAM and reconfigurable logic. Results from the SimpleScalar simulator [BA97] demonstrate up to 1000X speedups on several applications using the RADram system versus conventional memory systems. We also explore the sensitivity of our results to implementations in other memory technologies. 
Abstract-found: 1
Intro-found: 1
Reference: [A + 96] <editor> R. Amerson et al. </editor> <booktitle> Teramac configurable custom computing. In Symp on FPGAs for Custom Computing Machines, </booktitle> <pages> pages 32-38, </pages> <address> Napa Valley, CA, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: This alternative is promising, but we have seen that different applications can exploit significantly different computations in the memory system. Our results have shown that integrating reconfigurable logic is highly effective. Reconfigurable computing has shown considerable success at special-purpose applications <ref> [A + 96] </ref> [B + 96], but has had difficulty competing with microprocessors on more general-purpose tasks such as floating-point arithmetic. Some groups focus upon building reconfigurable processors [HW97] [WH96] [RS94] [WC96], but face an even more difficult competition with commodity microprocessors.
Reference: [AA95] <author> P. M. Athanas and A. L. Abbott. </author> <title> Real-time image processing on a custom computing platform. </title> <journal> IEEE Computer, </journal> <volume> 28(2) </volume> <pages> 16-24, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: The count is run on the same database in both the RADram system and on a conventional implementation. Image Processing Image processing and signal processing have been traditional strengths of FPGA's and custom processor technologies [R + 93] <ref> [AA95] </ref> [K + 96]. We implemented an image median filtering [RW92] application on RADram. Median filtering is a non-linear method which reduces the noise contained in an image without blurring the high-frequency components of the image signal. The RADram implementation divides the image by row blocks among various Active Pages.
Reference: [Ash90] <author> Peter J. Ashenden. </author> <title> The VHDL cookbook, 1st ed. </title> <institution> Dept of CS, U of Adelaide, </institution> <address> S Australia, </address> <month> July </month> <year> 1990. </year>
Reference-contexts: With large matrices, the RADram system has enough Active Pages executing to keep the processor computing at peak floating-point speeds. 6 Synthesized Logic In order to estimate performance and area of RADram logic configurations, each function of an application's Active Pages was hand-coded in a high-level circuit-description language, VHDL <ref> [Ash90] </ref>, and circuits synthesized to completely routed designs in contemporary FPGA technology. This provided a means to verify the timing of the simulated circuit implementation, as well as information on circuit area, which helped guide the RADram design.
Reference: [B + 96] <editor> D. Buell et al. </editor> <booktitle> Splash 2: FPGAs in a Custom Computing Machine. IEEE Computer Society, </booktitle> <year> 1996. </year>
Reference-contexts: This alternative is promising, but we have seen that different applications can exploit significantly different computations in the memory system. Our results have shown that integrating reconfigurable logic is highly effective. Reconfigurable computing has shown considerable success at special-purpose applications [A + 96] <ref> [B + 96] </ref>, but has had difficulty competing with microprocessors on more general-purpose tasks such as floating-point arithmetic. Some groups focus upon building reconfigurable processors [HW97] [WH96] [RS94] [WC96], but face an even more difficult competition with commodity microprocessors.
Reference: [B + 97a] <author> N. Bowman et al. </author> <title> Evaluation of existing architectures in IRAM systems. </title> <booktitle> In Workshop on Mixing Logic and DRAM, </booktitle> <address> Denver, CO, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: This results in dramatically improved DRAM bandwidth and latency to the processor core, but conventional processors are not designed to exploit these improvements <ref> [B + 97a] </ref>. An interesting alternative is to integrate specialized logic into DRAM to perform operations such as Read-Modify-Write [B + 97b]. This alternative is promising, but we have seen that different applications can exploit significantly different computations in the memory system.
Reference: [B + 97b] <author> A. Brown et al. </author> <title> Using MML to simulate multiple dual-ported SRAMs: Parallel routing lookups in an ATM switch controller. </title> <booktitle> In Workshop on Mixing Logic and DRAM, </booktitle> <address> Den-ver, CO, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: This results in dramatically improved DRAM bandwidth and latency to the processor core, but conventional processors are not designed to exploit these improvements [B + 97a]. An interesting alternative is to integrate specialized logic into DRAM to perform operations such as Read-Modify-Write <ref> [B + 97b] </ref>. This alternative is promising, but we have seen that different applications can exploit significantly different computations in the memory system. Our results have shown that integrating reconfigurable logic is highly effective.
Reference: [BA97] <author> D. Burger and T. Austin. </author> <title> The SimpleScalar tool set, </title> <journal> v2.0. Comp Arch News, </journal> <volume> 25(3), </volume> <month> June </month> <year> 1997. </year>
Reference-contexts: Second, a set of applications were chosen which represented various algorithmic domains. Finally, these applications were written and optimized for both the RADram and conventional memory system architectures. As a base for a simulation environment we started with the SimpleScalar v2.0 tool set <ref> [BA97] </ref>. This tool set provides the mechanisms to compile, debug and simulate applications compiled to a RISC architecture. The SimpleScalar RISC architecture is loosely based upon the MIPS R3000 instruction set architecture. The SimpleScalar environment was extended by replacing the simulated conventional memory hierarchy with an Active-Page memory system.
Reference: [Bat74] <author> K. E. Batcher. </author> <title> STARAN parallel processor system hardware. </title> <booktitle> AFIPS Conf Proceedings, </booktitle> <pages> pages 405-410, </pages> <year> 1974. </year>
Reference-contexts: DeHon described limited integration of reconfigurable logic and DRAM in an early memo [DeH95], but did not evaluate it further. Our philosophy is reminiscent of scatter-gather engines from a long line of supercomputers [HT72] [SH90] [CG86] <ref> [Bat74] </ref> [EJ73] [HS86] [L + 92]. Hockney and Jesshope [HJ88] give a good history of such machines. Our approach, however, supports a much wider variety of data manipulations and computations than these machines.
Reference: [Bee96] <author> Nelson H. F. Beebe. </author> <title> A bibliography of publications about the Linux operating system. </title> <type> Technical report, </type> <institution> Ctr for Scientific Comp, Dept of Math, U of Utah, </institution> <address> Salt Lake City, UT, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Our future work will address these issues both formally and practically by clarifying the policy of interaction between an operating system and the Active Page memory system, and by simulation of a modified operating system kernel such as Linux <ref> [Bee96] </ref>. In addition to operating system studies, multi-threaded application support will be investigated. Future work shall address inter-page and inter-chip communication issues. Before mechanisms are formalized for inter-page communication, a detailed evaluation of inter-page communication requirements is required.
Reference: [BGK96] <author> D. Burger, J. Goodman, and A. Kagi. </author> <title> Quantifying memory bandwidth limitations in future microprocessors. </title> <booktitle> In ISCA, </booktitle> <address> Philadelphia, PA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Microprocessor performance continues to follow phenomenal growth curves which drive the computing industry. Unfortunately, memory-system performance is falling behind. Processor-centric optimizations to bridge this processor-memory gap include prefetching, speculation, out-of-order execution, and mul-tithreading [WM95]. Several of these approaches can lead to memory-bandwidth problems <ref> [BGK96] </ref>. We introduce Active Pages, a model of computation which partitions applications between a processor and an intelligent memory system. Our goal is to keep processors running at peak speeds by off-loading data manipulation to logic placed in the memory system.
Reference: [CG86] <author> A. Charlesworth and J. Gustafson. </author> <title> Introducing replicated VLSI to supercomputing: </title> <booktitle> The FPS-164/MAX scientific computer. IEEE Computer, </booktitle> <month> March </month> <year> 1986. </year>
Reference-contexts: DeHon described limited integration of reconfigurable logic and DRAM in an early memo [DeH95], but did not evaluate it further. Our philosophy is reminiscent of scatter-gather engines from a long line of supercomputers [HT72] [SH90] <ref> [CG86] </ref> [Bat74] [EJ73] [HS86] [L + 92]. Hockney and Jesshope [HJ88] give a good history of such machines. Our approach, however, supports a much wider variety of data manipulations and computations than these machines.
Reference: [CLR96] <author> T. Cormen, C. Leiserson, and R. Rivest. </author> <title> Introduction To Algorithms. </title> <publisher> MIT Press, </publisher> <address> Cambridge MA, </address> <year> 1996. </year>
Reference-contexts: This wave-front computation runs in O (n log (n)) time on the RADram system. The RADram system implements the LCS computation by dividing the algorithm into two steps. The first step is the computation of the LCS result matrix itself. The second step is the backtracking <ref> [CLR96] </ref> required to find the largest common subsequence. The RADram system executes the first step entirely within the reconfigurable logic inside the memory system. Backtracking executes entirely within the processor. 5.2 Processor-Centric Partitioning Active Pages are intended for simple, application-specific operations, leaving more complex computations to general-purpose microprocessors.
Reference: [D + 92] <author> I. Duff et al. </author> <title> User's guide for the Harwell-Boeing sparse matrix collection. </title> <type> Technical Report TR/PA/92/86, CER-FACS, 42 Ave G. Coriolis, </type> <address> 31057 Toulouse Cedex, France, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: Sparse-Matrix Multiply A wide range of real-world problems can be represented as sparse matrices. We examine both a common scientific benchmark and a more challenging compiler optimization problem. Our scientific benchmark involves the multiplication of matrices representing finite-element computations taken from the Harwell-Boeing benchmark suite <ref> [D + 92] </ref>. Our compiler optimization problem involves using the Simplex method [NM65] to perform optimal register allocation [GW96]. A key computation in both these applications is sparse vector-vector dot-product. Conventional implementations of this operation are severely limited by processor-memory bandwidth.
Reference: [DeH95] <author> A. DeHon. </author> <title> Notes on integrating reconfigurable logic with DRAM arrays. </title> <type> Transit Note 120, </type> <institution> MIT, AI Lab. 545 Tech Sq. </institution> <address> Cambridge MA 02139, </address> <month> March </month> <year> 1995. </year>
Reference-contexts: Our approach avoids these difficulties by exploiting the strengths of both microprocessors and reconfigurable logic. We focus upon data manipulation to make the memory system perform better for the processor. DeHon described limited integration of reconfigurable logic and DRAM in an early memo <ref> [DeH95] </ref>, but did not evaluate it further. Our philosophy is reminiscent of scatter-gather engines from a long line of supercomputers [HT72] [SH90] [CG86] [Bat74] [EJ73] [HS86] [L + 92]. Hockney and Jesshope [HJ88] give a good history of such machines.
Reference: [DeH96a] <author> Andre DeHon. </author> <title> DPGA utilization and application. </title> <booktitle> In Proc of the Int Symp on Field Programmable Gate Arrays. </booktitle> <address> ACM/SIGDA, </address> <month> February </month> <year> 1996. </year>
Reference-contexts: Of particular concern is the high cost of swapping Active Pages to and from disk. Current FPGA technologies take 100s of milliseconds to reconfigure. New technologies, however, promise to reduce these times by several orders of magnitude <ref> [DeH96a] </ref>. Our future work will address these issues both formally and practically by clarifying the policy of interaction between an operating system and the Active Page memory system, and by simulation of a modified operating system kernel such as Linux [Bee96].
Reference: [DeH96b] <author> Andre DeHon. </author> <title> Reconfigurable Architectures for General-Purpose Computing. </title> <type> PhD thesis, </type> <institution> MIT, </institution> <year> 1996. </year>
Reference-contexts: If we devote half of the area of such a chip to logic, we expect the DRAM process to support approximately 32M transistors, which is enough to provide 256 LEs to each 512K sub-array of the remaining 0.5-gigabits of memory on the chip. DeHon <ref> [DeH96b] </ref> gives several estimates of FPGA area. We adopt a processor-mediated approach to inter-page communication which assumes infrequent communication. When an Active-Page function reaches a memory reference that can not be satisfied by its local page, it blocks and raises a processor interrupt.
Reference: [EJ73] <author> A. Evensen and J.Troy. </author> <title> Introduction to the architecture of a 288-element PEPE. </title> <booktitle> In Proc. 1973 Sagamore Conf. on Par Processing, </booktitle> <pages> pages 162-169, </pages> <year> 1973. </year>
Reference-contexts: DeHon described limited integration of reconfigurable logic and DRAM in an early memo [DeH95], but did not evaluate it further. Our philosophy is reminiscent of scatter-gather engines from a long line of supercomputers [HT72] [SH90] [CG86] [Bat74] <ref> [EJ73] </ref> [HS86] [L + 92]. Hockney and Jesshope [HJ88] give a good history of such machines. Our approach, however, supports a much wider variety of data manipulations and computations than these machines.
Reference: [Gus97] <author> D. Gusfield. </author> <title> Algorithms on Strings, Trees, and Sequences. </title> <publisher> Cambridge University Press, </publisher> <year> 1997. </year>
Reference-contexts: Largest Common Subsequence This algorithm is representative of a broad class of string algorithms which form the basis for modern biological research. At the heart of the computer algorithm to reconstruct DNA sequences are string algorithms such as largest common subsequence, global alignment, and local alignment <ref> [Gus97] </ref>. The largest common subsequence (LCS) computation is typically done using a dynamic programming construction. This construction runs in O (n 2 ) time and space for sequences of length n. One can view the construction as a set of computations over a plane.
Reference: [GVNG94] <author> D. Gajski, F. Vahid, S. Narayan, and J. Gong. </author> <title> Specification and Design of Embedded Systems. </title> <publisher> Prentice Hall, Inc, </publisher> <address> Englewood Cliffs, New Jersey 07632, </address> <year> 1994. </year>
Reference-contexts: Ideally, a compiler would take high-level source code and divide the computation into processor code and Active-Page functions, optimizing for memory bandwidth, synchronization, and parallelism to reduce execution time. This partitioning problem is very similar to that encountered in hardware-software co-design systems <ref> [GVNG94] </ref> which must divide code into pieces which run on general purpose processors and pieces which are implemented by ASICs (Application-Specific Integrated Circuits).
Reference: [GW96] <author> D. Goodwin and K. Wilken. </author> <title> Optimal and near-optimal global register allocation using 0-1 integer programming. </title> <journal> Software-Practice & Experience, </journal> <volume> 26(8) </volume> <pages> 929-965, </pages> <year> 1996. </year>
Reference-contexts: We examine both a common scientific benchmark and a more challenging compiler optimization problem. Our scientific benchmark involves the multiplication of matrices representing finite-element computations taken from the Harwell-Boeing benchmark suite [D + 92]. Our compiler optimization problem involves using the Simplex method [NM65] to perform optimal register allocation <ref> [GW96] </ref>. A key computation in both these applications is sparse vector-vector dot-product. Conventional implementations of this operation are severely limited by processor-memory bandwidth. Sparse vector FLOPS on a conventional system are often an order of magnitude lower than those for dense vectors.
Reference: [H + 96] <author> M. Hall et al. </author> <title> Maximizing multiprocessor performance with the SUIF compiler. </title> <booktitle> Computer, </booktitle> <month> December </month> <year> 1996. </year>
Reference-contexts: These systems estimate the performance of each line of code on alternative technologies, account for communication between components, and use integer programming or simulated annealing to minimize execution time and cost. Active Pages could use a similar approach, but would also need to borrow from par-allelizing compiler technology <ref> [H + 96] </ref> to produce data layouts and schedule computation within the memory system. Integration of Active Pages with a real operating system poses new challenges. Active Pages are similar to both memory pages and parallel processors.
Reference: [HJ88] <author> R. W. Hockney and C. R. Jesshope. </author> <title> Parallel Computers: Architecture, Programming, and Algorithms. </title> <publisher> Adam Hilger Ltd., </publisher> <address> Bristol, UK, </address> <note> second edition, </note> <year> 1988. </year>
Reference-contexts: DeHon described limited integration of reconfigurable logic and DRAM in an early memo [DeH95], but did not evaluate it further. Our philosophy is reminiscent of scatter-gather engines from a long line of supercomputers [HT72] [SH90] [CG86] [Bat74] [EJ73] [HS86] [L + 92]. Hockney and Jesshope <ref> [HJ88] </ref> give a good history of such machines. Our approach, however, supports a much wider variety of data manipulations and computations than these machines.
Reference: [HS86] <author> W. D. Hillis and G. L. Steele. </author> <title> The Connection Machine. </title> <publisher> M.I.T. Press, </publisher> <year> 1986. </year>
Reference-contexts: DeHon described limited integration of reconfigurable logic and DRAM in an early memo [DeH95], but did not evaluate it further. Our philosophy is reminiscent of scatter-gather engines from a long line of supercomputers [HT72] [SH90] [CG86] [Bat74] [EJ73] <ref> [HS86] </ref> [L + 92]. Hockney and Jesshope [HJ88] give a good history of such machines. Our approach, however, supports a much wider variety of data manipulations and computations than these machines.
Reference: [HT72] <author> R. Hintz and D. Tate. </author> <title> Control data STAR-100 processor design. </title> <booktitle> In COMPCON, </booktitle> <pages> pages 1-4, </pages> <year> 1972. </year>
Reference-contexts: We focus upon data manipulation to make the memory system perform better for the processor. DeHon described limited integration of reconfigurable logic and DRAM in an early memo [DeH95], but did not evaluate it further. Our philosophy is reminiscent of scatter-gather engines from a long line of supercomputers <ref> [HT72] </ref> [SH90] [CG86] [Bat74] [EJ73] [HS86] [L + 92]. Hockney and Jesshope [HJ88] give a good history of such machines. Our approach, however, supports a much wider variety of data manipulations and computations than these machines.
Reference: [HW97] <author> J. Hauser and J. Wawrzynek. </author> <title> Garp a MIPS processor with a reconfigurable coprocessor. </title> <booktitle> In Symp on FPGAs for Custom Computing Machines, </booktitle> <address> Napa Valley, CA, </address> <month> April </month> <year> 1997. </year>
Reference-contexts: Our results have shown that integrating reconfigurable logic is highly effective. Reconfigurable computing has shown considerable success at special-purpose applications [A + 96] [B + 96], but has had difficulty competing with microprocessors on more general-purpose tasks such as floating-point arithmetic. Some groups focus upon building reconfigurable processors <ref> [HW97] </ref> [WH96] [RS94] [WC96], but face an even more difficult competition with commodity microprocessors. Our approach avoids these difficulties by exploiting the strengths of both microprocessors and reconfigurable logic. We focus upon data manipulation to make the memory system perform better for the processor.
Reference: [I + 97] <author> K. Itoh et al. </author> <title> Limitations and challenges of multigigabit DRAM chip design. </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> 32(5) </volume> <pages> 624-634, </pages> <year> 1997. </year>
Reference-contexts: RADram is an architecture based upon the integration of the next generation of FPGA (Field-Programmable Gate Array) and DRAM technology. To minimize latency and reduce power consumption, large DRAMs are divided into subarrays, each with its own subset of address bits and decoders <ref> [I + 97] </ref>. RADram exploits this structure by associating a block of reconfigurable logic with each subarray. <p> a good sub-array size to minimize power and latency is 512 Kbytes Parameter Reference Variation CPU Clock 1 GHz - L1 I-Cache 64K - L1 D-Cache 64K 32K-256K L2 Cache 1M 256K-4M Reconf Logic 100 MHz 10-500 MHz Cache Miss 50 ns 0-600 ns Table 1: Summary of RADram parameters <ref> [I + 97] </ref>. The RADram system associates 256 LEs (Logic Elements, a standard block of logic in FPGAs which is based upon a 4-element Look Up Table or 4-LUT) to each of these sub-arrays. This allows efficient support for Active-Page sizes which are multiples of 512 kbytes.
Reference: [K + 96] <author> W. King et al. </author> <title> Using MORPH in an industrial machine vision system. </title> <editor> In K. L. Pocek and J. Arnold, editors, </editor> <booktitle> Proceedings of IEEE Workshop on FPGAs for Custom Computing Machines, </booktitle> <pages> pages 18-26, </pages> <address> Napa, CA, </address> <month> april </month> <year> 1996. </year>
Reference-contexts: The count is run on the same database in both the RADram system and on a conventional implementation. Image Processing Image processing and signal processing have been traditional strengths of FPGA's and custom processor technologies [R + 93] [AA95] <ref> [K + 96] </ref>. We implemented an image median filtering [RW92] application on RADram. Median filtering is a non-linear method which reduces the noise contained in an image without blurring the high-frequency components of the image signal. The RADram implementation divides the image by row blocks among various Active Pages.
Reference: [L + 92] <author> Charles E. Leiserson et al. </author> <title> The network architecture of the connection machine CM-5. </title> <booktitle> In Symposium on Parallel Architectures and Algorithms, </booktitle> <pages> pages 272-285, </pages> <address> San Diego, Cal-ifornia, </address> <month> June </month> <year> 1992. </year> <note> ACM. </note>
Reference-contexts: DeHon described limited integration of reconfigurable logic and DRAM in an early memo [DeH95], but did not evaluate it further. Our philosophy is reminiscent of scatter-gather engines from a long line of supercomputers [HT72] [SH90] [CG86] [Bat74] [EJ73] [HS86] <ref> [L + 92] </ref>. Hockney and Jesshope [HJ88] give a good history of such machines. Our approach, however, supports a much wider variety of data manipulations and computations than these machines.
Reference: [M + 96] <author> J. Mitchell et al. </author> <title> MPEG Video Compression Standard. </title> <publisher> Chapman & Hall, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: While future work will explore more MPEG routines, current work has focused upon application of the correction matrices within the P and B frames <ref> [M + 96] </ref>. Future implementation of the MPEG algorithm will partition additional components between the processor and RADram memory system.
Reference: [NM65] <author> J. Nelder and R. Mead. </author> <title> A simplex method for function minimization. </title> <journal> Computer Journal, </journal> <volume> 7 </volume> <pages> 308-313, </pages> <year> 1965. </year>
Reference-contexts: We examine both a common scientific benchmark and a more challenging compiler optimization problem. Our scientific benchmark involves the multiplication of matrices representing finite-element computations taken from the Harwell-Boeing benchmark suite [D + 92]. Our compiler optimization problem involves using the Simplex method <ref> [NM65] </ref> to perform optimal register allocation [GW96]. A key computation in both these applications is sparse vector-vector dot-product. Conventional implementations of this operation are severely limited by processor-memory bandwidth. Sparse vector FLOPS on a conventional system are often an order of magnitude lower than those for dense vectors.
Reference: [Pat95] <author> David Patterson. </author> <title> Microprocessors in 2020. </title> <publisher> Scientific American, </publisher> <month> September </month> <year> 1995. </year>
Reference-contexts: This work is supported in part by an NSF CAREER award to Fred Chong, by Altera, and by grants from the UC Davis Academic Senate. More info at http://arch.cs.ucdavis.edu/RAD yield, higher parallelism, and better integration with commodity microprocessors when compared to architectures such as IRAM <ref> [Pat95] </ref>. Since memory technologies are a moving target, we measure the sensitivity of our results to the speed of Active Page implementations. This allows us to generalize to currently available technologies such as DRAM macrocells in ASIC (Application-Specific Integrated Circuit) technologies. <p> Additionally, a memory bus capable of transferring 32 bits of data between memory and cache every 10 ns is assumed. Why Reconfigurable Logic? The potential of gi--gabit densities in DRAM has prompted research and development in a variety of implementation options for intelligent memory. IRAM <ref> [Pat95] </ref>, an integration of processor core and DRAM, is a well-known option studied at Berkeley. RADram, however, is likely to have better yield, higher parallelism, and better integration with commodity processors than IRAM. The primary advantage of RADram memory devices is that they will be inexpensive to fabricate.
Reference: [Prz97] <author> Steven Przybylski. </author> <title> Embedded DRAMs: Today and toward system-level integration. </title> <type> Technical report, </type> <institution> Verdande Group, Inc., </institution> <address> 3281 Lynn Oaks Drive, San Jose, CA, </address> <month> September </month> <year> 1997. </year>
Reference-contexts: The primary advantage of RADram memory devices is that they will be inexpensive to fabricate. Processor chips cost ten times as much as memory chips because their complexity makes their yield, or percentage of working chips, much lower <ref> [Prz97] </ref>. DRAMs are fabricated with redundant memory cells that can replace defective cells through laser modification after chip production. The uniform nature of reconfigurable logic allows for similar measures in RADram chips. In contrast, IRAM chip designers will have to work hard to avoid yields similar to processor chips. <p> Processors have also been fabricated in DRAM chips. Current DRAM in logic chips has poor density. Logic in DRAM chips has poor speed and density. Merged DRAM-logic processes, which can fabricate both kinds of structures well, are becoming available <ref> [Prz97] </ref>. Our study, however, is conservative and assumes a DRAM process with associated penalties in logic speed and density. Power Power consumption is a major concern for DRAM chips because increased chip temperatures result in higher charge leakage from storage cells.
Reference: [R + 93] <author> D. Ross et al. </author> <title> An FPGA-based hardware accelerator for image processing. </title> <editor> In W. Moore and W. Luk, editors, More FPGAs: </editor> <booktitle> Proc of the 1993 Int workshop on field-programmable logic and applications, </booktitle> <pages> pages 299-306, </pages> <address> Oxford, England, </address> <year> 1993. </year>
Reference-contexts: The count is run on the same database in both the RADram system and on a conventional implementation. Image Processing Image processing and signal processing have been traditional strengths of FPGA's and custom processor technologies <ref> [R + 93] </ref> [AA95] [K + 96]. We implemented an image median filtering [RW92] application on RADram. Median filtering is a non-linear method which reduces the noise contained in an image without blurring the high-frequency components of the image signal.
Reference: [RS94] <author> R. Razdan and M. Smith. </author> <title> A high-performance microarchi-tecture with hardware-programmable functional units. </title> <booktitle> In Int Symp on Microarchitecture, </booktitle> <pages> pages 172-180, </pages> <address> San Jose, CA, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: Reconfigurable computing has shown considerable success at special-purpose applications [A + 96] [B + 96], but has had difficulty competing with microprocessors on more general-purpose tasks such as floating-point arithmetic. Some groups focus upon building reconfigurable processors [HW97] [WH96] <ref> [RS94] </ref> [WC96], but face an even more difficult competition with commodity microprocessors. Our approach avoids these difficulties by exploiting the strengths of both microprocessors and reconfigurable logic. We focus upon data manipulation to make the memory system perform better for the processor.
Reference: [RW92] <author> G. Rafael and R. Woods. </author> <title> Digital Image Processing. </title> <publisher> Addison-Wesley, </publisher> <year> 1992. </year>
Reference-contexts: The count is run on the same database in both the RADram system and on a conventional implementation. Image Processing Image processing and signal processing have been traditional strengths of FPGA's and custom processor technologies [R + 93] [AA95] [K + 96]. We implemented an image median filtering <ref> [RW92] </ref> application on RADram. Median filtering is a non-linear method which reduces the noise contained in an image without blurring the high-frequency components of the image signal. The RADram implementation divides the image by row blocks among various Active Pages.
Reference: [Sem94] <institution> Semiconductor Industry Association. The national technology roadmap for semiconductors. </institution> <note> http://www.sematech.org/public/roadmap/, 1994. </note>
Reference-contexts: This allows efficient support for Active-Page sizes which are multiples of 512 kbytes. Each LE requires about 1K transistors of area on a logic chip. The Semiconductor Industry Association (SIA) roadmap <ref> [Sem94] </ref> projects mass production of 1-gigabit DRAM chips by the year 2001.
Reference: [SH90] <author> N. Sammur and M. Hagan. </author> <title> Mapping signal processing algorithms on parallel architectures. </title> <journal> J of Par and Distr Comp, </journal> <volume> 8(2) </volume> <pages> 180-185, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: We focus upon data manipulation to make the memory system perform better for the processor. DeHon described limited integration of reconfigurable logic and DRAM in an early memo [DeH95], but did not evaluate it further. Our philosophy is reminiscent of scatter-gather engines from a long line of supercomputers [HT72] <ref> [SH90] </ref> [CG86] [Bat74] [EJ73] [HS86] [L + 92]. Hockney and Jesshope [HJ88] give a good history of such machines. Our approach, however, supports a much wider variety of data manipulations and computations than these machines.
Reference: [SKS97] <author> A. Silberschatz, H. Korth, and S. Sudarshan. </author> <title> Database System Concepts. </title> <publisher> McGraw-Hill, </publisher> <year> 1997. </year>
Reference-contexts: The count operation is implemented by a binary comparison circuit. These three operations are indicative of a broad range of array operations which the RADram system can effectively compute. Further examples from the STL library include: accumulate, partial sum, random shu*e, rotate, and adjacent difference. Database Query Several methods <ref> [SKS97] </ref> exist to speed up database searches, if the searches involve indexed fields. Indexing produces a second table within the database which permits the database engine to quickly locate fields in logarithmic or constant time. However, indexing is often not practical for highly-varied queries or under tight storage constraints.
Reference: [WC96] <author> R. Wittig and P. Chow. OneChip: </author> <title> An FPGA processor with reconfigurable logic. </title> <booktitle> In Symposium on FPGAs for Custom Computing Machines, </booktitle> <pages> pages 126-135, </pages> <address> Napa Valley, California, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: Reconfigurable computing has shown considerable success at special-purpose applications [A + 96] [B + 96], but has had difficulty competing with microprocessors on more general-purpose tasks such as floating-point arithmetic. Some groups focus upon building reconfigurable processors [HW97] [WH96] [RS94] <ref> [WC96] </ref>, but face an even more difficult competition with commodity microprocessors. Our approach avoids these difficulties by exploiting the strengths of both microprocessors and reconfigurable logic. We focus upon data manipulation to make the memory system perform better for the processor.
Reference: [WH96] <author> M. Wirthlin and B. Hutchings. </author> <title> A dynamic instruction set computer. </title> <booktitle> In Symposium on FPGAs for Custom Computing Machines, </booktitle> <pages> pages 99-107, </pages> <address> Napa Valley, California, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: Our results have shown that integrating reconfigurable logic is highly effective. Reconfigurable computing has shown considerable success at special-purpose applications [A + 96] [B + 96], but has had difficulty competing with microprocessors on more general-purpose tasks such as floating-point arithmetic. Some groups focus upon building reconfigurable processors [HW97] <ref> [WH96] </ref> [RS94] [WC96], but face an even more difficult competition with commodity microprocessors. Our approach avoids these difficulties by exploiting the strengths of both microprocessors and reconfigurable logic. We focus upon data manipulation to make the memory system perform better for the processor.
Reference: [WM95] <author> W. Wulf and S. McKee. </author> <title> Hitting the memory wall: Implications of the obvious. </title> <journal> Computer Architecture News, </journal> <volume> 23(1), </volume> <month> March </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Microprocessor performance continues to follow phenomenal growth curves which drive the computing industry. Unfortunately, memory-system performance is falling behind. Processor-centric optimizations to bridge this processor-memory gap include prefetching, speculation, out-of-order execution, and mul-tithreading <ref> [WM95] </ref>. Several of these approaches can lead to memory-bandwidth problems [BGK96]. We introduce Active Pages, a model of computation which partitions applications between a processor and an intelligent memory system.
References-found: 41

