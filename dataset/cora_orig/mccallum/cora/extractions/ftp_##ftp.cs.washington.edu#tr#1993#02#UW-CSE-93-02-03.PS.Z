URL: ftp://ftp.cs.washington.edu/tr/1993/02/UW-CSE-93-02-03.PS.Z
Refering-URL: http://www.cs.washington.edu/homes/levy/opal/opalpapers.html
Root-URL: 
Title: User-Level Threads and Interprocess Communication  
Author: Michael J. Feeley, Jeffrey S. Chase, and Edward D. Lazowska 
Date: 93-02-03  
Address: Seattle, WA 98195  
Affiliation: Department of Computer Science and Engineering, FR-35 University of Washington  
Pubnum: Technical Report  
Abstract: User-level threads have performance and flexibility advantages over both Unix-like processes and kernel threads. However, the performance of user-level threads may suffer in multipro-grammed environments, or when threads block in the kernel (e.g., for I/O). These problems can be particularly severe in tasks that communicate frequently using IPC (e.g., multithreaded servers), due to interactions between the user-level thread scheduler and the operating system IPC primitives. Efficient IPC typically involves processor handoff that blocks the caller and unblocks a thread in the callee; when combined with user-level threads, this can cause problems for both caller and callee, particularly if the caller thread should subsequently block. In this paper we describe a new user-level thread package, called OThreads, designed to support blocking and efficient IPC for a system based on traditional kernel threads. We discuss the extent to which these problems can be solved at the user level without kernel changes such as scheduler activations. Our conclusion is that problems caused by application-controlled blocking and IPC can be resolved in the user-level thread package, but that problems due to multiprogramming workload and unanticipated blocking such as page faults require kernel changes such as scheduler activations.
Abstract-found: 1
Intro-found: 1
Reference: [Anderson et al. 91] <author> Anderson, T. E., Bershad, B. N., Lazowska, E. D., and Levy, H. M. </author> <title> Scheduler activations: Effective kernel support for the user-level management of parallelism. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating System Principles, </booktitle> <pages> pages 95-109. </pages> <publisher> ACM SIGOPS, </publisher> <month> October </month> <year> 1991. </year>
Reference-contexts: Anderson points out this two-level scheduling problem and proposes that scheduler activations replace kernel threads as the execution vessel provided by the kernel <ref> [Anderson et al. 91] </ref>. Scheduler activations provide a mechanism for the kernel to upcall the user-level thread scheduler to inform it of kernel scheduling changes that effect it | such as when one of its threads blocks or unblocks.
Reference: [Bershad et al. 88] <author> Bershad, B. N., Lazowska, E. D., and Levy, H. M. </author> <title> Presto: A system for object oriented parallel programming. </title> <journal> Software | Practice and Experience, </journal> <volume> 18(8) </volume> <pages> 713-732, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Each is specialized to a particular operating environment and each suffers two-level scheduling problems when used in an environment of cooperating domains. 2.3.1 Presto and Amber Presto is a user-level thread package designed to support fine-grain parallel programming, but not communication <ref> [Bershad et al. 88] </ref>. When a Presto application is started, it is assigned some number of processors for its exclusive use. Presto creates this number of scheduler threads, which spin on the thread ready queue waiting for user-level threads to run.
Reference: [Bershad et al. 89] <author> Bershad, B. N., Anderson, T. E., Lazowska, E. D., and Levy, H. M. </author> <title> Lightweight remote procedure call. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating System Principles, </booktitle> <pages> pages 102-113. </pages> <publisher> ACM SIGOPS, </publisher> <month> December </month> <year> 1989. </year> <month> 18 </month>
Reference-contexts: Making these calls fast requires donating IPC , which hands-off the processor from caller to callee as part of the call. This minimizes the cost of the control transfer between domains, avoiding the generic scheduling operation that would otherwise be required. This is the approach taken by LRPC <ref> [Bershad et al. 89] </ref>, Mach [Draves 90] and Windows NT [Custer 93]. The threads that run in Opal domains are user-level threads; each domain has a user-level scheduler 3 idle server domains. that manages the threads in that domain.
Reference: [Bershad et al. 92] <author> Bershad, B. N., Redell, D. D., and Ellis, J. R. </author> <title> Fast mutual exclusion for unipro cessors. </title> <booktitle> In Proceedings of the Fifth Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 223-233. </pages> <booktitle> ACM SIGPLAN, </booktitle> <month> September </month> <year> 1992. </year>
Reference-contexts: This strategy ensures that a spinner will not preempt the thread that holds the lock; it works well on a uniprocessor, but increases spinlock latency on a multiprocessor. Restartable atomic sequences are another effective strategy for dealing with spinlock preemption on uniprocessors <ref> [Bershad et al. 92] </ref>. Another way that communication can be supported in CThreads is to set the creation limit equal to the number of processors and use asynchronous IPC, as is done in NewThreads. <p> This provides a simple way to implement a multithreaded server | common to the Opal environment | without needing any locks in the server to guarantee mutual exclusion. This can have a positive effect on performance on architectures, like the MIPS R3000, that lack hardware synchronization instructions <ref> [Bershad et al. 92] </ref>. 6 Conclusion We built a new user-level thread package called OThreads that supports IPC and blocking for an operating system with traditional kernel threads, e.g., OSF/1, Mach or Windows NT.
Reference: [Chase et al. 89] <author> Chase, J. S., Amador, F. G., Lazowska, E. D., Levy, H. M., and Littlefield, R. J. </author> <title> The Amber system: Parallel programming on a network of multiprocessors. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 147-158, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Amber is a thread package based on Presto for supporting distributed parallel programming <ref> [Chase et al. 89, Feeley et al. 91] </ref>. In both Presto and Amber, the number of kernel threads is never greater than the number of processors allocated to it | no two level scheduling problems. Nevertheless, there are two key problems that make Presto and Amber unsuitable for our environment.
Reference: [Chase et al. 92a] <author> Chase, J. S., Levy, H. M., Baker-Harvey, M., and Lazowska, E. D. </author> <title> How to use a 64-bit virtual address space. </title> <type> Technical Report 92-03-02, </type> <institution> University of Washing-ton, Department of Computer Science and Engineering, </institution> <month> March </month> <year> 1992. </year> <title> Shortened version published as Opal: A Single Address Space System for 64-Bit Architectures, </title> <booktitle> Third IEEE Workshop on Workstation Operating Systems (WWOS-III), </booktitle> <month> April </month> <year> 1992. </year>
Reference-contexts: Section 5 discusses why user-level threads are needed in this environment. 2.1 Cooperating-Domain Environment OThreads was developed as part of the Opal project, whose purpose is to explore new operating system models for wide-address (e.g., 64-bit) architectures <ref> [Chase et al. 92a, Chase et al. 92b] </ref>. Opal is an operating system environment in which all applications execute in a shared, potentially persistent, virtual address space.
Reference: [Chase et al. 92b] <author> Chase, J. S., Levy, H. M., Lazowska, E. D., and Baker-Harvey, M. </author> <title> Lightweight shared objects in a 64-bit operating system. </title> <booktitle> In Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, </booktitle> <month> October </month> <year> 1992. </year> <institution> University of Washington CSE Technical Report 92-03-09. </institution>
Reference-contexts: Section 5 discusses why user-level threads are needed in this environment. 2.1 Cooperating-Domain Environment OThreads was developed as part of the Opal project, whose purpose is to explore new operating system models for wide-address (e.g., 64-bit) architectures <ref> [Chase et al. 92a, Chase et al. 92b] </ref>. Opal is an operating system environment in which all applications execute in a shared, potentially persistent, virtual address space.
Reference: [Cooper & Draves 88] <author> Cooper, E. C. and Draves, R. P. </author> <title> C threads. </title> <type> Technical Report CMU-CS-88 154, </type> <institution> Department of Computer Science, Carnegie-Mellon University, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: well for distributed IPC but increases the latency of local communication because it disallows the use of donating IPC. * As with Presto and Amber, idle domains consume processors by spinning on the ready queue. 2.3.3 CThreads CThreads is the user-level thread package used by many Mach servers and applications <ref> [Cooper & Draves 88] </ref>. CThreads was designed to support calls to Mach message primitives from a user-level thread, but it does this without trying to avoid two-level scheduling problems.
Reference: [Custer 93] <author> Custer, H. </author> <title> Inside Windows NT. </title> <publisher> Microsoft Press, </publisher> <address> Redmond, Washington, </address> <year> 1993. </year>
Reference-contexts: This minimizes the cost of the control transfer between domains, avoiding the generic scheduling operation that would otherwise be required. This is the approach taken by LRPC [Bershad et al. 89], Mach [Draves 90] and Windows NT <ref> [Custer 93] </ref>. The threads that run in Opal domains are user-level threads; each domain has a user-level scheduler 3 idle server domains. that manages the threads in that domain.
Reference: [Draves 90] <author> Draves, R. </author> <title> A revised ipc interface. </title> <booktitle> In USENIX Workshop Proceedings, MACH, </booktitle> <pages> pages 101-121, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: This minimizes the cost of the control transfer between domains, avoiding the generic scheduling operation that would otherwise be required. This is the approach taken by LRPC [Bershad et al. 89], Mach <ref> [Draves 90] </ref> and Windows NT [Custer 93]. The threads that run in Opal domains are user-level threads; each domain has a user-level scheduler 3 idle server domains. that manages the threads in that domain.
Reference: [Feeley et al. 91] <author> Feeley, M. J., Bershad, B. N., Chase, J. S., and Levy, H. M. </author> <title> Dynamic node reconfiguration in a parallel-distributed environment. </title> <booktitle> In Proceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 114-121, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Amber is a thread package based on Presto for supporting distributed parallel programming <ref> [Chase et al. 89, Feeley et al. 91] </ref>. In both Presto and Amber, the number of kernel threads is never greater than the number of processors allocated to it | no two level scheduling problems. Nevertheless, there are two key problems that make Presto and Amber unsuitable for our environment.
Reference: [Felten & McNamee 92a] <author> Felten, E. W. and McNamee, D. </author> <title> Improving the performance of message passing applications by multithreading. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference SHPCC-92, </booktitle> <pages> pages 84-89, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: This means that each of the many idle domains in our environment would be consuming valuable processor cycles. 6 2.3.2 NewThreads NewThreads is a user-level thread package similar to Presto, designed to support distributed IPC <ref> [Felten & McNamee 92a, Felten & McNamee 92b] </ref>. As with Presto and Amber, a NewThreads application is assigned some number of processors and when idle has that number of spinblockers spinning on the ready queue waiting for work.
Reference: [Felten & McNamee 92b] <author> Felten, E. W. and McNamee, D. </author> <title> Newthreads 2.0 user's guide. </title> <type> Technical report, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: This means that each of the many idle domains in our environment would be consuming valuable processor cycles. 6 2.3.2 NewThreads NewThreads is a user-level thread package similar to Presto, designed to support distributed IPC <ref> [Felten & McNamee 92a, Felten & McNamee 92b] </ref>. As with Presto and Amber, a NewThreads application is assigned some number of processors and when idle has that number of spinblockers spinning on the ready queue waiting for work.
Reference: [Marsh et al. 91] <author> Marsh, B. D., Scott, M. L., LeBlanc, T. J., and Markatos, E. P. </author> <title> First-class user level threads. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating System Principles, </booktitle> <pages> pages 110-121. </pages> <publisher> ACM SIGOPS, </publisher> <month> October </month> <year> 1991. </year>
Reference-contexts: The kernel ensures that the number of scheduler activations never exceeds the number of processors. This problem was also identified by researchers working on the Psyche parallel operating system. Pscyhe uses kernel threads, but upcalls the user-level to inform it of scheduling changes <ref> [Marsh et al. 91] </ref>. Anderson's solution requires changes to the operating system kernel, as does the Pscyhe approach. Changing the kernel is difficult; a production quality implementation must be general, reliable and efficient. As a result, though research prototypes exist, systems with scheduler activations are not presently available.
Reference: [Tucker & Gupta 89] <author> Tucker, A. and Gupta, A. </author> <title> Process control and scheduling issues for multipro grammed shared-memory multiprocessors. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating System Principles, </booktitle> <pages> pages 159-166. </pages> <publisher> ACM SIGOPS, </publisher> <month> December </month> <year> 1989. </year>
Reference-contexts: In the face of a heavily multiprogrammed workload, in particular, maintaining this global invariant on kernel threads requires coordination between domains that is not part of the current OThreads implementation. We believe, however, that a scheme similar to <ref> [Tucker & Gupta 89] </ref> would solve this problem. Our proposed solution assumes that every domain is running OThreads and that domains trust each other to behave properly as we describe below.
Reference: [Zahorjan et al. 91] <author> Zahorjan, J., Lazowska, E. D., and Eager, D. L. </author> <title> The effect of scheduling discipline on spin overhead in shared memory multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(2) </volume> <pages> 180-198, </pages> <month> April </month> <year> 1991. </year> <month> 19 </month>
Reference-contexts: The system is unable to correctly schedule these threads because doing so requires information known only to the user-level scheduler. Lacking this information, the system might preempt a thread at an inopportune time (e.g., when it is holding a spinlock <ref> [Zahorjan et al. 91] </ref>) or it might leave an important thread suspended while choosing to run an unimportant one. We are primarily interested in two-level scheduling that occurs when an application makes a blocking system call or when it communicates using IPC.
References-found: 16

