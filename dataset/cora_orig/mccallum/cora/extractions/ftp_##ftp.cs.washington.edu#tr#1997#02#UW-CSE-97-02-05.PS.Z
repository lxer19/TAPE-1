URL: ftp://ftp.cs.washington.edu/tr/1997/02/UW-CSE-97-02-05.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-title.html
Root-URL: 
Title: An Executable Taxonomy of On-Line Modeling Algorithms 1  
Author: Suzanne Bunton 
Keyword: data compression, universal coding, on-line stochastic model ing, statistical inference, finite-state automata  
Affiliation: Department of Computer Science and Engineering University of Washington  
Pubnum: Technical Report UW-CSE-97-02-05  
Abstract: This paper gives an overview of our decomposition of a group of existing and novel on-line sequence modeling algorithms into component parts. Our decomposition, and its implementation, show that these algorithms can be implemented as a cross product of predominantly independent sets. The result is all of the following: a test bed for executing controlled experiments with algorithm components, a framework that unifies existing techniques and defines novel techniques, and a taxonomy for describing on-line sequence modeling algorithms precisely and completely in a way that enables meaningful comparison. 1 A version of this paper appears in Proceedings of the DCC, March 1997. This report contains 
Abstract-found: 1
Intro-found: 1
Reference: [Bun96] <author> S. Bunton. </author> <title> On-Line Stochastic Processes in Data Compression. </title> <type> PhD thesis, </type> <institution> University of Washington, </institution> <month> December </month> <year> 1996. </year> <month> 9 </month>
Reference-contexts: Full descriptions of the algorithms, and definitions of the technical terms used in this overview, are given in Chapters 2-6 of <ref> [Bun96] </ref>, excerpts of which appear as the companion papers [Bun97b, Bun97a]. 1 Design Philosophy The executable cross product and nomenclature described here are used to produce and describe the experimental results of [Bun97b, Bun97a], which evaluate different state selection and probability estimation techniques, respectively. <p> names to each feature option, to form a language for precisely describing on-line statistical algorithms. * The symbols accompanying the English names give a terse labeling system that completely describes on-line algorithms. * It explains our software's command-line options (see Figure 1). * It synopsizes the modeling concepts presented in <ref> [Bun96, Bun97b, Bun97a, Bun97c] </ref>. Usage: runDMC [-B (atch)-U (decode)-b-e-s-v-or-P-y-z-s-w-im-K-x-G-D-M-c] file :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: Alphabet_Bits: (the log of the input alphabet size) -b -1, ..., 16- (Default = 8) neg_log_EPSILON: (EPSILON = minimum frequency. <p> from its subtree. -x 2 Maximum-Order Update Exclusion :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: 3 3.1 Model Structure and Growth Suffix-tree model 2 structure is determined by the size of the input alphabet, the initial model, any order bounds, and whether the model is implemented with symbol-labelled transitions or string-labelled transitions (which are described in <ref> [Bun96, Bun97c] </ref> and denoted with a `*'). Thus, the following features specify model structure: Alphabet Bits, b: DMC uses a binary input alphabet, with b = 1; the other tech niques we test use a 256-ary alphabet, with b = 8. <p> No -m0 uniform_prior) (when combined with -m1, all nodes will have |alphabet| out_events). Max_Event_Copy_Depth: -c 0 copy predicted event all the way to new leaf (Default) -c -1,2,..,255- copy event to c min-order extns of predicting_node :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: 3.3 Frequency Updates Chapter 5 of <ref> [Bun96] </ref> fully describes three update options, how each of them affects the semantics of the model, and how to emulate them simultaneously in a single model that combines non-trivial mixtures and state selection. Update Exclusion, X: The default for PPM and PPM* variants is no update exclusion for any states.
Reference: [Bun97a] <author> S. Bunton. </author> <title> A generalization and improvement to PPM's blending. </title> <type> UW-CSE Technical Report UW-CSE-97-01-10, </type> <institution> The University of Washington, </institution> <month> January </month> <year> 1997. </year>
Reference-contexts: Full descriptions of the algorithms, and definitions of the technical terms used in this overview, are given in Chapters 2-6 of [Bun96], excerpts of which appear as the companion papers <ref> [Bun97b, Bun97a] </ref>. 1 Design Philosophy The executable cross product and nomenclature described here are used to produce and describe the experimental results of [Bun97b, Bun97a], which evaluate different state selection and probability estimation techniques, respectively. <p> the algorithms, and definitions of the technical terms used in this overview, are given in Chapters 2-6 of [Bun96], excerpts of which appear as the companion papers <ref> [Bun97b, Bun97a] </ref>. 1 Design Philosophy The executable cross product and nomenclature described here are used to produce and describe the experimental results of [Bun97b, Bun97a], which evaluate different state selection and probability estimation techniques, respectively. The cross product is also used in [Bun97c], which significantly improves the performance and memory requirements of PPMC [Mof90] and PPM* [CTW95]. The design of our software involved the following steps: 1. <p> Update the MDLs at all currently excited states. 8. Compute before-update inheritances at excited actual states, if applicable. 9. Update frequencies at excited states. 10. Compute after-update inheritances at excited states, if applicable. 3 The Cross Product of Distinguishing Features The algorithmic variants presented and tested in <ref> [Bun97b, Bun97a] </ref> are specified in the column headings of the tables using the conventions described below for describing the major features of modeling algorithms: model structure, probability estimation, frequency updates, and state selection. <p> names to each feature option, to form a language for precisely describing on-line statistical algorithms. * The symbols accompanying the English names give a terse labeling system that completely describes on-line algorithms. * It explains our software's command-line options (see Figure 1). * It synopsizes the modeling concepts presented in <ref> [Bun96, Bun97b, Bun97a, Bun97c] </ref>. Usage: runDMC [-B (atch)-U (decode)-b-e-s-v-or-P-y-z-s-w-im-K-x-G-D-M-c] file :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: Alphabet_Bits: (the log of the input alphabet size) -b -1, ..., 16- (Default = 8) neg_log_EPSILON: (EPSILON = minimum frequency. <p> eligible, redirect it and all eligible suffix-linked edges above it to distinct new states that are suffix-linked descendants of the original destination state. (Emulates PPM and WLZ.) Redirection Thresholds, y and z: for tuning redirection criteria R 1 and R 2 . 3.2 Probability Estimation with Mixtures The companion paper <ref> [Bun97a] </ref> explains how all probability estimators in the baseline algorithms can be described as recursive mixtures of the frequency distributions at different excited states. <p> are left unspecified do not have any effect on the particular combination of other options. 3 The seven algorithms are DMC [CH87], GDMC [TR93], PPM [Mof90], PPM* [CTW95], WLZ [WLZ92], plus BestFSMX [Bun97c] and BestDMC, which are the best-performing FSMX and DMC variants tested with this taxonomy so far (see <ref> [Bun97a] </ref>). PPM is listed twice because there are two very different ways to implement it as a suffix tree, both of which produce identical predictions. 4 Now, that's data compression! 7 Table 1: The State of the Art in On-Line Statistical Compressors.
Reference: [Bun97b] <author> S. Bunton. </author> <title> A percolating state selector for suffix-tree context models. </title> <booktitle> In Proceedings Data Compression Conference. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> March </month> <year> 1997. </year>
Reference-contexts: Full descriptions of the algorithms, and definitions of the technical terms used in this overview, are given in Chapters 2-6 of [Bun96], excerpts of which appear as the companion papers <ref> [Bun97b, Bun97a] </ref>. 1 Design Philosophy The executable cross product and nomenclature described here are used to produce and describe the experimental results of [Bun97b, Bun97a], which evaluate different state selection and probability estimation techniques, respectively. <p> the algorithms, and definitions of the technical terms used in this overview, are given in Chapters 2-6 of [Bun96], excerpts of which appear as the companion papers <ref> [Bun97b, Bun97a] </ref>. 1 Design Philosophy The executable cross product and nomenclature described here are used to produce and describe the experimental results of [Bun97b, Bun97a], which evaluate different state selection and probability estimation techniques, respectively. The cross product is also used in [Bun97c], which significantly improves the performance and memory requirements of PPMC [Mof90] and PPM* [CTW95]. The design of our software involved the following steps: 1. <p> Update the MDLs at all currently excited states. 8. Compute before-update inheritances at excited actual states, if applicable. 9. Update frequencies at excited states. 10. Compute after-update inheritances at excited states, if applicable. 3 The Cross Product of Distinguishing Features The algorithmic variants presented and tested in <ref> [Bun97b, Bun97a] </ref> are specified in the column headings of the tables using the conventions described below for describing the major features of modeling algorithms: model structure, probability estimation, frequency updates, and state selection. <p> names to each feature option, to form a language for precisely describing on-line statistical algorithms. * The symbols accompanying the English names give a terse labeling system that completely describes on-line algorithms. * It explains our software's command-line options (see Figure 1). * It synopsizes the modeling concepts presented in <ref> [Bun96, Bun97b, Bun97a, Bun97c] </ref>. Usage: runDMC [-B (atch)-U (decode)-b-e-s-v-or-P-y-z-s-w-im-K-x-G-D-M-c] file :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: Alphabet_Bits: (the log of the input alphabet size) -b -1, ..., 16- (Default = 8) neg_log_EPSILON: (EPSILON = minimum frequency. <p> To emulate GDMC, let C = 1. 3.4 The Selection of the Coding Model The set of state selectors is presented and empirically evaluated in <ref> [Bun97b] </ref>, using a range of state selection thresholds. The percolating state selector below is presented in [Bun97b]. All of our implementations are MDL-based for efficiency, but they could have been implemented using actual entropies of the frequency distributions. <p> To emulate GDMC, let C = 1. 3.4 The Selection of the Coding Model The set of state selectors is presented and empirically evaluated in <ref> [Bun97b] </ref>, using a range of state selection thresholds. The percolating state selector below is presented in [Bun97b]. All of our implementations are MDL-based for efficiency, but they could have been implemented using actual entropies of the frequency distributions. <p> :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: State Selectors: These options determine how the coding model is dynamically selected for each input symbol. none, S 0 : The default action is to select no state. heuristic, S 2 : Select the min-order excited state with only one out-transition ([CTW95]). percolating, S 3 : See companion paper <ref> [Bun97b] </ref>. top-down, S 5 : An MDL-based implementation of the hill-climbing method used in Rissanen's Context algorithm. bottom-up, S 6 : A bottom-up complement to S 5 : State-Selection Threshold, v: The actual threshold is v=1024, and is used to test the difference in expected bits (log base e bits, that
Reference: [Bun97c] <author> S. Bunton. </author> <title> Semantically motivated improvements for PPM variants. </title> <journal> The British Computer Journal, Special Data Compression Issue, </journal> <note> 1997. (invited paper, to appear June 1997). </note>
Reference-contexts: The cross product is also used in <ref> [Bun97c] </ref>, which significantly improves the performance and memory requirements of PPMC [Mof90] and PPM* [CTW95]. The design of our software involved the following steps: 1. Identify a set of basic, independent features that most sequential models share. 2. <p> names to each feature option, to form a language for precisely describing on-line statistical algorithms. * The symbols accompanying the English names give a terse labeling system that completely describes on-line algorithms. * It explains our software's command-line options (see Figure 1). * It synopsizes the modeling concepts presented in <ref> [Bun96, Bun97b, Bun97a, Bun97c] </ref>. Usage: runDMC [-B (atch)-U (decode)-b-e-s-v-or-P-y-z-s-w-im-K-x-G-D-M-c] file :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: Alphabet_Bits: (the log of the input alphabet size) -b -1, ..., 16- (Default = 8) neg_log_EPSILON: (EPSILON = minimum frequency. <p> from its subtree. -x 2 Maximum-Order Update Exclusion :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: 3 3.1 Model Structure and Growth Suffix-tree model 2 structure is determined by the size of the input alphabet, the initial model, any order bounds, and whether the model is implemented with symbol-labelled transitions or string-labelled transitions (which are described in <ref> [Bun96, Bun97c] </ref> and denoted with a `*'). Thus, the following features specify model structure: Alphabet Bits, b: DMC uses a binary input alphabet, with b = 1; the other tech niques we test use a 256-ary alphabet, with b = 8. <p> and explicitly point out all of the abstract differences among them. 4 For each algorithm, the options that are left unspecified do not have any effect on the particular combination of other options. 3 The seven algorithms are DMC [CH87], GDMC [TR93], PPM [Mof90], PPM* [CTW95], WLZ [WLZ92], plus BestFSMX <ref> [Bun97c] </ref> and BestDMC, which are the best-performing FSMX and DMC variants tested with this taxonomy so far (see [Bun97a]).
Reference: [CH87] <author> G. V. Cormack and R. N. S. Horspool. </author> <title> Data compression using dynamic Markov mod-elling. </title> <journal> The Computer Journal, </journal> <volume> 30(6) </volume> <pages> 541-550, </pages> <year> 1987. </year>
Reference-contexts: two lines of text completely describe seven different state-of-the-art algorithms 3 and explicitly point out all of the abstract differences among them. 4 For each algorithm, the options that are left unspecified do not have any effect on the particular combination of other options. 3 The seven algorithms are DMC <ref> [CH87] </ref>, GDMC [TR93], PPM [Mof90], PPM* [CTW95], WLZ [WLZ92], plus BestFSMX [Bun97c] and BestDMC, which are the best-performing FSMX and DMC variants tested with this taxonomy so far (see [Bun97a]).
Reference: [CTW95] <author> J. G. Cleary, W. J. Teahan, and I. H. Witten. </author> <title> Unbounded length contexts for PPM. </title> <booktitle> In Proceedings Data Compression Conference, </booktitle> <month> March </month> <year> 1995. </year>
Reference-contexts: The cross product is also used in [Bun97c], which significantly improves the performance and memory requirements of PPMC [Mof90] and PPM* <ref> [CTW95] </ref>. The design of our software involved the following steps: 1. Identify a set of basic, independent features that most sequential models share. 2. Transform influential techniques from the literature to a unifying control and data structure that decomposes into the basic features. 3. <p> seven different state-of-the-art algorithms 3 and explicitly point out all of the abstract differences among them. 4 For each algorithm, the options that are left unspecified do not have any effect on the particular combination of other options. 3 The seven algorithms are DMC [CH87], GDMC [TR93], PPM [Mof90], PPM* <ref> [CTW95] </ref>, WLZ [WLZ92], plus BestFSMX [Bun97c] and BestDMC, which are the best-performing FSMX and DMC variants tested with this taxonomy so far (see [Bun97a]). <p> Moffat's 1990 implementation, PPMC [Mof90], of Cleary and Witten's 1984 PPM algorithm [CW84], remained unchallenged until Cleary, et al. did away with PPM's order bound to produce PPM* in 1995 <ref> [CTW95] </ref>. The authors claimed that PPM* outperformed PPMC in their paper. However, PPMC was known to achieve superior compression performance as the order bound increased up to 5 [Mof90], after which its performance starts to decline.
Reference: [CW84] <author> J. G. Cleary and I. H. Witten. </author> <title> Data compression using adaptive coding and partial string matching. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 32(4) </volume> <pages> 396-402, </pages> <year> 1984. </year>
Reference-contexts: The utilities are `compress,' which is based upon Welch's popular implementation [Wel84] of the Ziv and Lempel's second major string-matching algorithm [ZL78], and `gzip,' which is based upon Ziv and Lempel's first major string-matching construction [ZL77]. Moffat's 1990 implementation, PPMC [Mof90], of Cleary and Witten's 1984 PPM algorithm <ref> [CW84] </ref>, remained unchallenged until Cleary, et al. did away with PPM's order bound to produce PPM* in 1995 [CTW95]. The authors claimed that PPM* outperformed PPMC in their paper.
Reference: [How93] <author> P. G. Howard. </author> <title> The Design and Analysis of Efficient Lossless Data Compression Systems. </title> <type> PhD thesis, </type> <institution> Brown University, </institution> <year> 1993. </year>
Reference-contexts: The authors claimed that PPM* outperformed PPMC in their paper. However, PPMC was known to achieve superior compression performance as the order bound increased up to 5 [Mof90], after which its performance starts to decline. In 1993, Howard published a simple change to PPMC's escape mechanism, called PPMD <ref> [How93] </ref>: add .5 instead of 1.0 to the escape count and scanned event count, whenever a novel event is seen. PPMD gets even better performance than PPMC.
Reference: [Mof90] <author> A. Moffat. </author> <title> Implementing the PPM data compression scheme. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 38(11) </volume> <pages> 1917-1921, </pages> <year> 1990. </year>
Reference-contexts: The cross product is also used in [Bun97c], which significantly improves the performance and memory requirements of PPMC <ref> [Mof90] </ref> and PPM* [CTW95]. The design of our software involved the following steps: 1. Identify a set of basic, independent features that most sequential models share. 2. Transform influential techniques from the literature to a unifying control and data structure that decomposes into the basic features. 3. <p> completely describe seven different state-of-the-art algorithms 3 and explicitly point out all of the abstract differences among them. 4 For each algorithm, the options that are left unspecified do not have any effect on the particular combination of other options. 3 The seven algorithms are DMC [CH87], GDMC [TR93], PPM <ref> [Mof90] </ref>, PPM* [CTW95], WLZ [WLZ92], plus BestFSMX [Bun97c] and BestDMC, which are the best-performing FSMX and DMC variants tested with this taxonomy so far (see [Bun97a]). <p> The utilities are `compress,' which is based upon Welch's popular implementation [Wel84] of the Ziv and Lempel's second major string-matching algorithm [ZL78], and `gzip,' which is based upon Ziv and Lempel's first major string-matching construction [ZL77]. Moffat's 1990 implementation, PPMC <ref> [Mof90] </ref>, of Cleary and Witten's 1984 PPM algorithm [CW84], remained unchallenged until Cleary, et al. did away with PPM's order bound to produce PPM* in 1995 [CTW95]. The authors claimed that PPM* outperformed PPMC in their paper. <p> The authors claimed that PPM* outperformed PPMC in their paper. However, PPMC was known to achieve superior compression performance as the order bound increased up to 5 <ref> [Mof90] </ref>, after which its performance starts to decline. In 1993, Howard published a simple change to PPMC's escape mechanism, called PPMD [How93]: add .5 instead of 1.0 to the escape count and scanned event count, whenever a novel event is seen. PPMD gets even better performance than PPMC.
Reference: [Ris83] <author> J. J. Rissanen. </author> <title> A universal data compression system. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 29(5) </volume> <pages> 656-664, </pages> <year> 1983. </year>
Reference-contexts: 39,611 3.87 2.68 2.98 2.67 2.49 2.40 progl 71,646 3.03 1.82 2.17 1.83 1.90 1.67 progp 49,379 3.11 1.82 2.22 1.90 1.84 1.62 trans 93,695 3.27 1.62 2.11 1.73 1.77 1.45 Average 224,402 3.64 2.71 2.74 2.52 2.48 2.34 The remaining two command lines approximate the original binary-alphabet Context algorithm <ref> [Ris83] </ref> and a 256-ary variant [Ris86]. The emulations are only approximate because the true Context model is a non-Markovian FSMX finite state machine, which cannot be represented state-for-state with an FSM that has explicit transitions.
Reference: [Ris86] <author> J. J. Rissanen. </author> <title> An image compression system. </title> <booktitle> In Proceedings HILCOM 86, </booktitle> <year> 1986. </year>
Reference-contexts: 2.49 2.40 progl 71,646 3.03 1.82 2.17 1.83 1.90 1.67 progp 49,379 3.11 1.82 2.22 1.90 1.84 1.62 trans 93,695 3.27 1.62 2.11 1.73 1.77 1.45 Average 224,402 3.64 2.71 2.74 2.52 2.48 2.34 The remaining two command lines approximate the original binary-alphabet Context algorithm [Ris83] and a 256-ary variant <ref> [Ris86] </ref>. The emulations are only approximate because the true Context model is a non-Markovian FSMX finite state machine, which cannot be represented state-for-state with an FSM that has explicit transitions.
Reference: [TR93] <author> J. Teuhola and T. Raita. </author> <title> Application of a finite-state model to text compression. </title> <journal> The Computer Journal, </journal> <volume> 36(7) </volume> <pages> 607-614, </pages> <year> 1993. </year>
Reference-contexts: of text completely describe seven different state-of-the-art algorithms 3 and explicitly point out all of the abstract differences among them. 4 For each algorithm, the options that are left unspecified do not have any effect on the particular combination of other options. 3 The seven algorithms are DMC [CH87], GDMC <ref> [TR93] </ref>, PPM [Mof90], PPM* [CTW95], WLZ [WLZ92], plus BestFSMX [Bun97c] and BestDMC, which are the best-performing FSMX and DMC variants tested with this taxonomy so far (see [Bun97a]).
Reference: [Wel84] <author> T. Welch. </author> <title> A technique for high-performance data compression. </title> <journal> IEEE Computer, </journal> <volume> 17(6) </volume> <pages> 8-19, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: The published performance of on-line stochastic algorithms from the data compression literature that have been implemented are shown in Table 1, along with the performance of two popular Unix compression utilities. The utilities are `compress,' which is based upon Welch's popular implementation <ref> [Wel84] </ref> of the Ziv and Lempel's second major string-matching algorithm [ZL78], and `gzip,' which is based upon Ziv and Lempel's first major string-matching construction [ZL77].
Reference: [WLZ92] <author> M. J. Weinberger, A. Lempel, and J. Ziv. </author> <title> A sequential algorithm for the universal coding of finite memory sources. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 38(3) </volume> <pages> 1002-1014, </pages> <year> 1992. </year>
Reference-contexts: state-of-the-art algorithms 3 and explicitly point out all of the abstract differences among them. 4 For each algorithm, the options that are left unspecified do not have any effect on the particular combination of other options. 3 The seven algorithms are DMC [CH87], GDMC [TR93], PPM [Mof90], PPM* [CTW95], WLZ <ref> [WLZ92] </ref>, plus BestFSMX [Bun97c] and BestDMC, which are the best-performing FSMX and DMC variants tested with this taxonomy so far (see [Bun97a]).
Reference: [ZL77] <author> J. Ziv and A. Lempel. </author> <title> A universal algorithm for sequential data compression. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 23(3) </volume> <pages> 337-343, </pages> <year> 1977. </year>
Reference-contexts: The utilities are `compress,' which is based upon Welch's popular implementation [Wel84] of the Ziv and Lempel's second major string-matching algorithm [ZL78], and `gzip,' which is based upon Ziv and Lempel's first major string-matching construction <ref> [ZL77] </ref>. Moffat's 1990 implementation, PPMC [Mof90], of Cleary and Witten's 1984 PPM algorithm [CW84], remained unchallenged until Cleary, et al. did away with PPM's order bound to produce PPM* in 1995 [CTW95]. The authors claimed that PPM* outperformed PPMC in their paper.
Reference: [ZL78] <author> J. Ziv and A. Lempel. </author> <title> Compression of individual sequences via variable-rate coding. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 24(5) </volume> <pages> 530-536, </pages> <month> September </month> <year> 1978. </year> <month> 10 </month>
Reference-contexts: The utilities are `compress,' which is based upon Welch's popular implementation [Wel84] of the Ziv and Lempel's second major string-matching algorithm <ref> [ZL78] </ref>, and `gzip,' which is based upon Ziv and Lempel's first major string-matching construction [ZL77]. Moffat's 1990 implementation, PPMC [Mof90], of Cleary and Witten's 1984 PPM algorithm [CW84], remained unchallenged until Cleary, et al. did away with PPM's order bound to produce PPM* in 1995 [CTW95].
References-found: 16

