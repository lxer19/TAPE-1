URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-372.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: fcwren,flavia,ali,trevor,thad,akira,cchao,hlavac, kbrussel,sandyg@media.mit.edu  
Title: Spaces for Performance and Entertainment: Untethered Interaction using Computer Vision and Audition Figure 1: User
Author: Christopher R. Wren Flavia Sparacino Ali J. Azarbayejani Trevor J. Darrell Thad E. Starner Akira Kotani Chloe M. Chao Michal Hlavac Kenneth B. Russell Alex P. Pentland 
Web: http://vismod.www.media.mit.edu/groups/vismod/  
Address: 20 Ames St., Cambridge, MA 02139 USA  
Affiliation: The MIT Media Laboratory  
Note: Perceptive  
Pubnum: Perceptual Computing Section,  
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 372 Appears in Applied Artificial Intelligence, Vol. 11, No. 4, June 1997 Abstract Bulky head-mounted displays, data gloves, and severely limited movement have become synonymous with virtual environments. This is unfortunate since virtual environments have such great potential in applications such as entertainment, animation by example, design interface, information browsing, and even expressive performance. In this paper we describe an approach to unencumbered, natural interfaces called Perceptive Spaces. The spaces are unencumbered because they utilize passive sensors that don't require special clothing and large format displays that don't isolate the user from their environment. The spaces are natural because the open environment facilitates active participation. Several applications illustrate the expressive power of this approach, as well as the challenges associated with designing these interfaces. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> ACM. Mandala: </editor> <title> Virtual Village, </title> <booktitle> ACM SIGGraph, Computer Graphics Visual Proceedings, </booktitle> <year> 1993. </year>
Reference-contexts: Some systems avoid this problem by passively or actively "watching" the user. These systems often modify the environment with specially colored or illuminated backdrops, require the user to wear special clothes, or involve special equipment like range finders or active floor tiles <ref> [11, 1, 19] </ref>. The ability to enter the virtual environment just by stepping into the sensing area is very important. The users do not have to spend time "suiting up," cleaning the apparatus, or untangling wires. <p> More recently the Man-dala group <ref> [1] </ref>, has commercialized and improved this technology by using analog chromakey video processing to isolate colored garments worn by users. In both cases, most of the focus is on improving the graphics interaction, with the visual input processing being at most a secondary concern.
Reference: [2] <author> S. Aukstakalnis and D. Blatner. </author> <title> Silicon Mirage. </title> <publisher> Peachpit Press, </publisher> <year> 1992. </year>
Reference-contexts: In order to allow a user to navigate a three dimensional space, most commercial systems encum 1 ber the user with head-mounted displays, electro-magnetic or sonic position sensors, gloves, and/or body suits <ref> [2] </ref>. While such systems can be extremely accurate, they limit the freedom of the user due to the tethers associated with the sensors and displays. Furthermore, the user must don or remove the equipment each time they want to enter or exit the environment.
Reference: [3] <author> A. Azarbayejani and A.P. Pentland. </author> <title> Recursive estimation of motion, structure, and focal length. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> 17(6) </volume> <pages> 562-575, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Initial exploration into this space 1 Use of existing image-to-image registration techniques <ref> [3, 14] </ref> allow Pfinder to function in the presence of camera rotation and zoom, but real-time performance cannot be achieved without special purpose hardware. 2 here in greyscale for printing purposes), frame (center) shows the segmentation of the user into blobs, and frame (right) shows a 3-D model reconstructed from blob
Reference: [4] <author> Ali Azarbayejani and Alex Pentland. </author> <title> Real-time self-calibrating stereo person tracking using 3-D shape estimation from blob features. </title> <booktitle> In Proceedings of 13th ICPR, </booktitle> <address> Vienna, Austria, August 1996. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference: [5] <author> A. Baumberg and D. Hogg. </author> <title> An efficient method for contour tracking using active shape models. </title> <booktitle> In Proceeding of the Workshop on Motion of Nonrigid and Articulated Objects. IEEE Computer Society, </booktitle> <year> 1994. </year>
Reference-contexts: Such approaches require relatively massive computational resources and are therefore not appropriate for human interface applications. Pfinder is perhaps most closely related to the work of Bichsel [6] and Baumberg and Hogg <ref> [5] </ref>. The limitation of these systems is that they do not analyze the person's shape or internal features, but only the silhouette of the person. Pfinder goes beyond these systems by also building a blob-based model of the person's clothing, head, hands, and feet.
Reference: [6] <author> Martin Bichsel. </author> <title> Segmenting simply connected moving objects in a static scene. </title> <journal> Pattern Analysis and Machine Intelligence, </journal> <volume> 16(11) </volume> <pages> 1138-1142, </pages> <month> Nov </month> <year> 1994. </year>
Reference-contexts: Such approaches require relatively massive computational resources and are therefore not appropriate for human interface applications. Pfinder is perhaps most closely related to the work of Bichsel <ref> [6] </ref> and Baumberg and Hogg [5]. The limitation of these systems is that they do not analyze the person's shape or internal features, but only the silhouette of the person. Pfinder goes beyond these systems by also building a blob-based model of the person's clothing, head, hands, and feet.
Reference: [7] <author> Michael A. Casey, William G. Gardner, and Sumit Basu. </author> <title> Vision steered beam-forming and transaural rendering for the artificial life interactive video environment (alive). </title> <booktitle> In Proceedings of the 99th Convention of the Aud. </booktitle> <institution> Eng. Soc. AES, </institution> <year> 1995. </year>
Reference-contexts: This provides the signal-to-noise gain necessary for remote microphones to be useful for speech recognition techniques <ref> [7] </ref>. Active cameras can also take advantage of up-to-date information about body part position to make fine distinctions about facial expression, identity, or hand posture.[8] 3 Perceptive Spaces Unencumbered interface technologies do not, by themselves, constitute an interface.
Reference: [8] <author> T. Darrell, B. Moghaddam, and A. Pentland. </author> <title> Active face tracking and pose estimation in an interactive room. In CVPR96. </title> <publisher> IEEE Computer Society, </publisher> <year> 1996. </year>
Reference: [9] <author> D. M. Gavrila and L. S. Davis. </author> <title> Towards 3-d model-based tracking and recognition of human movement: a multi-view approach. </title> <booktitle> In International Workshop on Automatic Face- and Gesture-Recognition. IEEE Computer Society, 1995. </booktitle> <address> Zurich. </address>
Reference-contexts: Pfinder goes well beyond these systems by providing a detailed level of analysis impossible with primitive binary vision.[21] Pfinder is also related to body-tracking projects like Rehg and Kanade [17], Rohr [18], and Gavrila and Davis <ref> [9] </ref> that use kinematic models, or Pentland and Horowitz [16] and Metaxas and Terzopolous [15] who use dynamic models. Such approaches require relatively massive computational resources and are therefore not appropriate for human interface applications.
Reference: [10] <author> James Klosty. Merce Cunningham: </author> <title> dancing in space and time. </title> <publisher> Saturday Review Press, </publisher> <year> 1975. </year>
Reference-contexts: The philosophy underlying DanceSpace is inspired by Merce Cunningham's approach to dance and choreography <ref> [10] </ref>. The idea is that dance and movement should be designed independently of music and that music should be subordinate to movement and may be composed later for a piece as a musical score is done for film.
Reference: [11] <author> M. W. Krueger. </author> <title> Artificial Reality II. </title> <publisher> Addison Wesley, </publisher> <year> 1990. </year>
Reference-contexts: Some systems avoid this problem by passively or actively "watching" the user. These systems often modify the environment with specially colored or illuminated backdrops, require the user to wear special clothes, or involve special equipment like range finders or active floor tiles <ref> [11, 1, 19] </ref>. The ability to enter the virtual environment just by stepping into the sensing area is very important. The users do not have to spend time "suiting up," cleaning the apparatus, or untangling wires. <p> and zoom, but real-time performance cannot be achieved without special purpose hardware. 2 here in greyscale for printing purposes), frame (center) shows the segmentation of the user into blobs, and frame (right) shows a 3-D model reconstructed from blob statistics alone (with contour shape ignored). of applications was by Krueger <ref> [11] </ref>, who showed that even 2-D binary vision processing of the human form can be used as an interesting interface. More recently the Man-dala group [1], has commercialized and improved this technology by using analog chromakey video processing to isolate colored garments worn by users.
Reference: [12] <author> Tod Machover. HyperInstruments: </author> <title> A Composer's Approach to the Evolution of Intelligent Musical Instruments, </title> <address> pages 67-76. </address> <publisher> Miller Freeman, </publisher> <year> 1992. </year>
Reference: [13] <author> Pattie Maes, Bruce Blumberg, Trevor Darrell, and Alex Pentland. </author> <title> The alive system: Full-body interaction with animated autonomous agents. </title> <journal> ACM Multimedia Systems, </journal> <volume> 5 </volume> <pages> 105-112, </pages> <year> 1997. </year> <month> 9 </month>
Reference-contexts: In this manner, the gestures employed by the user can have rich meaning which varies on the previous history, the agents internal needs and the current situation. <ref> [13] </ref> 4 Conclusion The preceding examples illustrate successful interfaces built for a wide range of application domains from animation to artistic expression to information browsing. They all differ in the mappings they employ between sensed features, and application control.
Reference: [14] <author> S. Mann and R. W. </author> <title> Picard. Video orbits: charac-terizing the coordinate transformation between two images using the projective group. </title> <editor> IEEE T. </editor> <booktitle> Image Proc., </booktitle> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: Initial exploration into this space 1 Use of existing image-to-image registration techniques <ref> [3, 14] </ref> allow Pfinder to function in the presence of camera rotation and zoom, but real-time performance cannot be achieved without special purpose hardware. 2 here in greyscale for printing purposes), frame (center) shows the segmentation of the user into blobs, and frame (right) shows a 3-D model reconstructed from blob
Reference: [15] <author> D. Metaxas and D. Terzopoulos. </author> <title> Shape and non-rigid motion estimation through physics-based synthesis. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> 15 </volume> <pages> 580-591, </pages> <year> 1993. </year>
Reference-contexts: Pfinder goes well beyond these systems by providing a detailed level of analysis impossible with primitive binary vision.[21] Pfinder is also related to body-tracking projects like Rehg and Kanade [17], Rohr [18], and Gavrila and Davis [9] that use kinematic models, or Pentland and Horowitz [16] and Metaxas and Terzopolous <ref> [15] </ref> who use dynamic models. Such approaches require relatively massive computational resources and are therefore not appropriate for human interface applications. Pfinder is perhaps most closely related to the work of Bichsel [6] and Baumberg and Hogg [5].
Reference: [16] <author> A. Pentland and B. Horowitz. </author> <title> Recovery of nonrigid motion and structure. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> 13(7) </volume> <pages> 730-742, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Pfinder goes well beyond these systems by providing a detailed level of analysis impossible with primitive binary vision.[21] Pfinder is also related to body-tracking projects like Rehg and Kanade [17], Rohr [18], and Gavrila and Davis [9] that use kinematic models, or Pentland and Horowitz <ref> [16] </ref> and Metaxas and Terzopolous [15] who use dynamic models. Such approaches require relatively massive computational resources and are therefore not appropriate for human interface applications. Pfinder is perhaps most closely related to the work of Bichsel [6] and Baumberg and Hogg [5].
Reference: [17] <author> J.M. Rehg and T. Kanade. </author> <title> Visual tracking of high dof articulated structures: An application to human hand tracking. </title> <booktitle> In European Conference on Computer Vision, </booktitle> <pages> pages B:35-46, </pages> <year> 1994. </year>
Reference-contexts: Pfinder goes well beyond these systems by providing a detailed level of analysis impossible with primitive binary vision.[21] Pfinder is also related to body-tracking projects like Rehg and Kanade <ref> [17] </ref>, Rohr [18], and Gavrila and Davis [9] that use kinematic models, or Pentland and Horowitz [16] and Metaxas and Terzopolous [15] who use dynamic models. Such approaches require relatively massive computational resources and are therefore not appropriate for human interface applications.
Reference: [18] <author> K. Rohr. </author> <title> Towards model-based recognition of human movements in image sequences. CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> 59(1) </volume> <pages> 94-115, </pages> <month> Jan </month> <year> 1994. </year>
Reference-contexts: Pfinder goes well beyond these systems by providing a detailed level of analysis impossible with primitive binary vision.[21] Pfinder is also related to body-tracking projects like Rehg and Kanade [17], Rohr <ref> [18] </ref>, and Gavrila and Davis [9] that use kinematic models, or Pentland and Horowitz [16] and Metaxas and Terzopolous [15] who use dynamic models. Such approaches require relatively massive computational resources and are therefore not appropriate for human interface applications.
Reference: [19] <author> Kenneth Russell, Thad Starner, and Alex Pentland. </author> <title> Unencumbered virtual environments. </title> <booktitle> In IJCAI-95 Workshop on Entertainment and AI/Alife, </booktitle> <year> 1995. </year>
Reference-contexts: Some systems avoid this problem by passively or actively "watching" the user. These systems often modify the environment with specially colored or illuminated backdrops, require the user to wear special clothes, or involve special equipment like range finders or active floor tiles <ref> [11, 1, 19] </ref>. The ability to enter the virtual environment just by stepping into the sensing area is very important. The users do not have to spend time "suiting up," cleaning the apparatus, or untangling wires.
Reference: [20] <author> Flavia Sparacino, Christopher Wren, Alex Pentland, and Glorianna Davenport. Hyperplex: </author> <title> a world of 3d interactive digital movies. </title> <booktitle> In IJCAI-95 Workshop on Entertainment and AI/Alife, </booktitle> <year> 1995. </year>
Reference-contexts: NetSpace capitalizes on this ability by mapping the contents of URLs into a 3D graphical world projected on the large IVE screen. This gives the user a sense the URLs existing in a surrounding 3D environment. NetSpace was conceived as a natural extension to Hy-perplex <ref> [20] </ref>, our first experiment using IVE as an im-mersive browser for movies. To navigate this virtual 3D environment, users stand in front of the screen and use voice and hand gestures to explore (Figure 9).
Reference: [21] <author> Christopher Wren, Ali Azarbayejani, Trevor Darrell, and Alex Pentland. Pfinder: </author> <title> Real-time tracking of the human body. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> 19(7) </volume> <pages> 780-785, </pages> <month> July </month> <year> 1997. </year> <month> 10 </month>
Reference-contexts: We have chosen computer vision as one tool capable of solving this problem across many situations and application domains. We have developed a real-time system called Pfinder <ref> [21] </ref> ("person finder") that substantially solves the problem for arbitrarily complex but single-person, fixed-camera situations 1 (see Figure 4a). The system has been tested on thousands of people in several installations around the world, and has been found to perform quite reliably.[21] Pfinder is descended from a variety of interesting experiments
References-found: 21

