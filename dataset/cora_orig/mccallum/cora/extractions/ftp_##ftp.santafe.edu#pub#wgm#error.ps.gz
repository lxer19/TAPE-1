URL: ftp://ftp.santafe.edu/pub/wgm/error.ps.gz
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00079.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: An Efficient Method To Estimate Bagging's Generalization Error  
Author: David H. Wolpert William G. Macready 
Address: N5Na/E2 650 Harry Road San Jose, CA, 95120  1399 Hyde Park Road Santa Fe, NM, 87501  
Affiliation: IBM Almaden Research Center Dept.  Santa Fe Institute  
Abstract: In bagging [Bre94a] one uses bootstrap replicates of the training set [Efr79, ET93] to try to improve a learning algorithm's performance. The computational requirements for estimating the resultant generalization error on a test set by means of cross-validation are often prohibitive; for leave-one-out cross-validation one needs to train the underlying algorithm on the order of m- times, where m is the size of the training set and is the number of replicates. This paper presents several techniques for exploiting the bias-variance decomposition [GBD92, Wol96] to estimate the generalization error of a bagged learning algorithm without invoking yet more training of the underlying learning algorithm. The best of our estimators exploits stacking [Wol92]. In a set of experiments reported here, it was found to be more accurate than both the alternative cross-validation-based estimator of the bagged algorithm's error and the cross-validation-based estimator of the underlying algorithm's error. This improvement was particularly pronounced for small test sets. This suggests a novel justification for using bagging| im proved estimation of generalization error.
Abstract-found: 1
Intro-found: 1
Reference: [Bre94a] <author> L. </author> <type> Breiman. </type> <institution> Bagging predictors. University of California, Dept. of Statistics, </institution> <type> TR 421, </type> <year> 1994. </year>
Reference-contexts: When the training set must be indicated but G can be assumed, we will write h d . In Breiman's "bagging" (Bootstrap Aggregating) procedure <ref> [Bre94a] </ref>, rather than use the hypothesis G creates after training on d, one uses the average of the hypotheses G creates by training on the sets d 0 i . The d 0 i are created in some manner from d. <p> If h d is a better estimate of f at the points fx i g than is d | which one would hope is a common case | then such sampling of h d to form bagging's guess may result in lower generalization error than does conventional bagging. (See <ref> [Bre94a] </ref> for related discussion.) An even more speculative issue is how best to estimate generalization accuracy for such a modification of bagging. Acknowledgments DHW was supported by TXN Inc. and the Santa Fe Institute, and WGM was supported by the Santa Fe Institute. DHW thanks Ronny Kohavi for interesting discussion.
Reference: [Bre94b] <author> L. Breiman. </author> <title> Heuristics of instability and stabilization in model selection. </title> <institution> University of California, Dept. of Statistics, </institution> <type> TR 416, </type> <year> 1994. </year>
Reference-contexts: In general, one would expect that bagging improves generalization for "unstable" learning algorithms but not for "stable" learning algorithms <ref> [Bre94b] </ref>. Since the bagged algorithm itself employs many retrainings of the underlying algorithm, the computational requirements for using cross-validation to estimate the generalization error when one is using bagging are often prohibitive. For example, for leave-one-out cross-validation one needs to train the underlying algorithm m times. <p> It measures how well the average h matches f . The second term is the variance. It measures how much h d "bounces around" as d's sampled from f change. A learning algorithm is defined to be unstable if h d is a sensitive function of d <ref> [Bre94b] </ref>. For such a learning algorithm, the variance contributes substantially to the expected error. As an example, Breiman argues that due to the existence of many local minima in the energy surface, the learning algorithm of backprop-trained neural nets is unstable. Now fix G and therefore h fl (q).
Reference: [Efr79] <author> B. </author> <title> Efron. </title> <journal> Computers and the theory of statistics: thinking the unthinkable. SIAM Review, </journal> <volume> 21:460, </volume> <year> 1979. </year>
Reference-contexts: In Breiman's work the d 0 i are "bootstrap replicates" of d, i.e., each d 0 i is created by sampling the pairs in d uniformly with replacement m times <ref> [Efr79, ET93] </ref>. In general, one would expect that bagging improves generalization for "unstable" learning algorithms but not for "stable" learning algorithms [Bre94b]. <p> Breiman argues that one should similarly expect that bagging improves average misclassifi-cation generalization error (as opposed to the quadratic error discussed above). The density estimation technique Breiman used also directly provided his samples of that density estimate: it was the bootstrap procedure. <ref> [Efr79, ET93] </ref>. This is both the maximum-likelihood density estimator and an unbiased density estimator. The samples of the bootstrap-estimated f are called "replicates".
Reference: [ET93] <author> B. Efron and R. Tibshirani. </author> <title> An introduction to the bootstrap. </title> <publisher> Chapman and Hall, </publisher> <year> 1993. </year>
Reference-contexts: In Breiman's work the d 0 i are "bootstrap replicates" of d, i.e., each d 0 i is created by sampling the pairs in d uniformly with replacement m times <ref> [Efr79, ET93] </ref>. In general, one would expect that bagging improves generalization for "unstable" learning algorithms but not for "stable" learning algorithms [Bre94b]. <p> Breiman argues that one should similarly expect that bagging improves average misclassifi-cation generalization error (as opposed to the quadratic error discussed above). The density estimation technique Breiman used also directly provided his samples of that density estimate: it was the bootstrap procedure. <ref> [Efr79, ET93] </ref>. This is both the maximum-likelihood density estimator and an unbiased density estimator. The samples of the bootstrap-estimated f are called "replicates".
Reference: [GBD92] <author> S. Geman, Elie Bienenstock, and Rene Doursat. </author> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-58, </pages> <year> 1992. </year>
Reference-contexts: Such onerous computational requirements for estimating the associated generalization error are a major impediment to the real-world use of bagging. This paper presents several techniques for circumventing this problem, and thereby facilitating the real-world use of bagging. In particular, it is shown how the bias variance formula <ref> [GBD92, Wol96] </ref> can be exploited to estimate the generalization error of a bagged learning algorithm without additional training of the underlying learning algorithm; only the original training runs used to create the bagged algorithm's hypothesis are needed. <p> is independent of the learning algorithm, the expected value of C given f , the training set size m, and q, is given by E (Cjf; m; q) = d 2 2 X P (djf; m) h d (q) h fl (q) : (1) This is the famous bias-variance formula <ref> [GBD92, Wol96] </ref>. The first term on the right-hand side is the (square of the) bias. It measures how well the average h matches f . The second term is the variance. It measures how much h d "bounces around" as d's sampled from f change.
Reference: [Tib96] <author> R. Tibshirani. </author> <title> Bias, variance and prediction error for classification rules. </title> <institution> University of Toronto Statistics Department Technical Report, </institution> <year> 1996. </year>
Reference-contexts: than the estimate of the generalization error of the underlying algorithm given by conventional cross-validation. (The details of this comparison where one tries different kinds of cross-validation besides leave-one-out including in particular bootstrap-based variants | are the subject of future work.) Simultaneously with our work, Tibshirani conducted a similar study <ref> [Tib96] </ref>. In his study, he investigated the (V 2 ; E 2 ) estimator, for classification problems (as opposed to the regression problems studied in this paper). The results in [Tib96] are quite encouraging; they suggest that the basic idea of the (V 2 ; E 2 ) estimator also works <p> in particular bootstrap-based variants | are the subject of future work.) Simultaneously with our work, Tibshirani conducted a similar study <ref> [Tib96] </ref>. In his study, he investigated the (V 2 ; E 2 ) estimator, for classification problems (as opposed to the regression problems studied in this paper). The results in [Tib96] are quite encouraging; they suggest that the basic idea of the (V 2 ; E 2 ) estimator also works well on classification problems. Combined with our results this suggests that this estimation method may be broadly applicable. Our work complements Tibshirani's study in a number of ways.
Reference: [WM96] <author> D. Wolpert and W. Macready. </author> <title> Combining stacking with bagging to improve a learning algorithm. </title> <note> Submitted, </note> <year> 1996. </year>
Reference-contexts: The density estimation technique Breiman used also directly provided his samples of that density estimate: it was the bootstrap procedure. [Efr79, ET93]. This is both the maximum-likelihood density estimator and an unbiased density estimator. The samples of the bootstrap-estimated f are called "replicates". See <ref> [WM96] </ref> for a discussion of some caveats behind the utility of bagging. 3 Estimating Bagging's Error 3.1 Overview Under the hypothesis of bagging, the mimic of h fl (q) is a close approximation to h fl (q), i.e., h fl (q) closely approximates the bagged algorithm's guess. <p> This in essence amounts to single-generalizer stacking with a bootstrap partition set [Wol92], where rather than try to improve the generalization, one is trying to improve one's estimate of its accuracy. (See <ref> [WM96] </ref> for discussion of how instead one can successfully use this same basic idea of combining bagging and stacking to get lower generalization error than that given by conventional bagging.) To implement this idea we used a simple linear model, Err (q) = aV c (q) + b, to approximate the <p> There was noise and (unlike in Breiman's experiments) model-misspecification, but the input space was only one-dimensional. Future work involves extending these experiments to more elaborate domains. As in the first set of experiments in <ref> [WM96] </ref>, our input and output space were R 1 . The target function was a third-order polynomial with two Gaussians superimposed. The coefficients of the polynomial were randomly chosen from [-1, 1]. <p> All four of these learning algorithms are stable. Accordingly, for them, the bagged learning algorithm does not outperform the underlying learning algorithm. (It performs slightly worse see <ref> [WM96] </ref>.) Accordingly, a single unstable learning algorithm is created from these four stable ones. This unstable learning algorithm again works by forming a least-squared-error fit to the training set, using a set of three cosines and associated sines. <p> Similarly, the schemes explored in <ref> [WM96] </ref> use stacking to shrink the generalization error of bagging; the results of this paper suggest that having V c (q) be one of the variables involved in the stacking would result in even greater improvement in generalization error.
Reference: [Wol92] <author> D.H. Wolpert. </author> <title> Stacked generalization. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 241-249, </pages> <year> 1992. </year>
Reference-contexts: The best of our estimators exploits the technique of stacking <ref> [Wol92] </ref>. It estimates the bagged algorithm's generalization error both more accurately than the alternative cross-validation-based estimator of the bagged algorithm's error, and more accurately than the cross-validation-based estimator of the underlying algorithm's error. <p> More directly, one can use the bootstrap information to directly estimate Err (q) from V c (q), and then combine that estimate "conservatively" with the E 2 V 2 es timate (say by averaging). This in essence amounts to single-generalizer stacking with a bootstrap partition set <ref> [Wol92] </ref>, where rather than try to improve the generalization, one is trying to improve one's estimate of its accuracy. (See [WM96] for discussion of how instead one can successfully use this same basic idea of combining bagging and stacking to get lower generalization error than that given by conventional bagging.) To <p> However again following the advice of stacking (see <ref> [Wol92] </ref>), we chose to be "conservative". The accuracy of the linear fit can be estimated by looking at its 2 residual error.
Reference: [Wol96] <author> D.H. Wolpert. </author> <title> On bias plus variance. Neural Computation, </title> <publisher> in press, </publisher> <year> 1996. </year>
Reference-contexts: Such onerous computational requirements for estimating the associated generalization error are a major impediment to the real-world use of bagging. This paper presents several techniques for circumventing this problem, and thereby facilitating the real-world use of bagging. In particular, it is shown how the bias variance formula <ref> [GBD92, Wol96] </ref> can be exploited to estimate the generalization error of a bagged learning algorithm without additional training of the underlying learning algorithm; only the original training runs used to create the bagged algorithm's hypothesis are needed. <p> is independent of the learning algorithm, the expected value of C given f , the training set size m, and q, is given by E (Cjf; m; q) = d 2 2 X P (djf; m) h d (q) h fl (q) : (1) This is the famous bias-variance formula <ref> [GBD92, Wol96] </ref>. The first term on the right-hand side is the (square of the) bias. It measures how well the average h matches f . The second term is the variance. It measures how much h d "bounces around" as d's sampled from f change.
References-found: 9

