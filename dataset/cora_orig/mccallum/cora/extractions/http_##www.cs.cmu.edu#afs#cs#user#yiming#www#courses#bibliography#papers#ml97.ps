URL: http://www.cs.cmu.edu/afs/cs/user/yiming/www/courses/bibliography/papers/ml97.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs/user/yiming/www/courses/bibliography/papers/
Root-URL: 
Email: yiming@cs.cmu.edu  jpederse@verity.com  
Title: A Comparative Study on Feature Selection in Text Categorization  
Author: Yiming Yang Jan O. Pedersen Verity, Inc. Ross Dr. 
Address: Pittsburgh, PA 15213-3702, USA  CA 94089, USA  
Affiliation: School of Computer Science Carnegie Mellon University  Sunnyvale,  
Abstract: This paper is a comparative study of feature selection methods in statistical learning of text categorization. The focus is on aggressive dimensionality reduction. Five methods were evaluated, including term selection based on document frequency (DF), information gain (IG), mutual information (MI), a 2 -test (CHI), and term strength (TS). We found IG and CHI most effective in our experiments. Using IG thresholding with a k-nearest neighbor classifier on the Reuters corpus, removal of up to 98% removal of unique terms actually yielded an improved classification accuracy (measured by average precision). DF thresholding performed similarly. Indeed we found strong correlations between the DF, IG and CHI values of a term. This suggests that DF thresholding, the simplest method with the lowest cost in computation, can be reliably used instead of IG or CHI when the computation of these measures are too expensive. TS compares favorably with the other methods with up to 50% vocabulary reduction but is not competitive at higher vocabulary reduction levels. In contrast, MI had relatively poor performance due to its bias towards favoring rare terms, and its sen sitivity to probability estimation errors.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Apte, F. Damerau, and S. Weiss. </author> <title> Towards language independent automated learning of text categorization models. </title> <booktitle> In Proceedings of the 17th Annual ACM/SIGIR conference, </booktitle> <year> 1994. </year>
Reference-contexts: A growing number of statistical classification methods and machine learning techniques have been applied to text categorization in recent years, including multivariate regression models [8, 27], nearest neighbor classification [4, 23], Bayes probabilistic approaches [20, 13], decision trees [13], neural networks [21], symbolic rule learning <ref> [1, 16, 3] </ref> and inductive learning algorithms [3, 12]. A major characteristic, or difficulty, of text categorization problems is the high dimensionality of the feature space. <p> Hence a evaluation using both classifiers should reduce the possibility of classifier bias in the results. 3.2 Data collections We use two corpora for this study: the Reuters-22173 collection and the OHSUMED collection. The Reuters news story collection is commonly used corpora in text categorization research <ref> [13, 1, 21, 16, 3] </ref> 1 . There are 21,450 documents in the full collection; less than half of the documents have human assigned topic labels. <p> We used only those documents that had at least one topic, divided randomly into a training set of 9,610 and a test set of 3,662 documents. This partition is similar to that employed in <ref> [1] </ref>, but differs from [13] who use the full collection including unla 1 A newly revised version, namely Reuters-21578, is available through http://www.research.att.com/ ~ lewis. belled documents 2 . The stories have a mean length of 90.6 words with standard deviation 91.6.
Reference: [2] <author> Kenneth Ward Church and Patrick Hanks. </author> <title> Word association norms, mutual information and lexicography. </title> <booktitle> In Proceedings of ACL 27, </booktitle> <pages> pages 76-83, </pages> <address> Vancouver, Canada, </address> <year> 1989. </year>
Reference-contexts: The entropy computations has a time complexity of O (V m). 2.3 Mutual information (MI) Mutual information is a criterion commonly used in statistical language modelling of word associations and related applications <ref> [7, 2, 21] </ref>.
Reference: [3] <author> William W. Cohen and Yoram Singer. </author> <title> Context-sensitive learning metods for text categorization. </title> <booktitle> In SIGIR '96: Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <year> 1996. </year> <pages> 307-315. </pages>
Reference-contexts: A growing number of statistical classification methods and machine learning techniques have been applied to text categorization in recent years, including multivariate regression models [8, 27], nearest neighbor classification [4, 23], Bayes probabilistic approaches [20, 13], decision trees [13], neural networks [21], symbolic rule learning <ref> [1, 16, 3] </ref> and inductive learning algorithms [3, 12]. A major characteristic, or difficulty, of text categorization problems is the high dimensionality of the feature space. <p> number of statistical classification methods and machine learning techniques have been applied to text categorization in recent years, including multivariate regression models [8, 27], nearest neighbor classification [4, 23], Bayes probabilistic approaches [20, 13], decision trees [13], neural networks [21], symbolic rule learning [1, 16, 3] and inductive learning algorithms <ref> [3, 12] </ref>. A major characteristic, or difficulty, of text categorization problems is the high dimensionality of the feature space. <p> Hence a evaluation using both classifiers should reduce the possibility of classifier bias in the results. 3.2 Data collections We use two corpora for this study: the Reuters-22173 collection and the OHSUMED collection. The Reuters news story collection is commonly used corpora in text categorization research <ref> [13, 1, 21, 16, 3] </ref> 1 . There are 21,450 documents in the full collection; less than half of the documents have human assigned topic labels.
Reference: [4] <author> R.H. Creecy, B.M. Masand, S.J. Smith, and D.L. Waltz. </author> <title> Trading mips and memory for knowledge engineering: classifying census returns on the connection machine. </title> <journal> Comm. ACM, </journal> <volume> 35 </volume> <pages> 48-63, </pages> <year> 1992. </year>
Reference-contexts: Documents categorization is one solution to this problem. A growing number of statistical classification methods and machine learning techniques have been applied to text categorization in recent years, including multivariate regression models [8, 27], nearest neighbor classification <ref> [4, 23] </ref>, Bayes probabilistic approaches [20, 13], decision trees [13], neural networks [21], symbolic rule learning [1, 16, 3] and inductive learning algorithms [3, 12]. A major characteristic, or difficulty, of text categorization problems is the high dimensionality of the feature space.
Reference: [5] <author> S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer, and R. Harshman. </author> <title> Indexing by latent semantic analysis. </title> <journal> In J Amer Soc Inf Sci 1, </journal> <volume> 6, </volume> <pages> pages 391-407, </pages> <year> 1990. </year>
Reference-contexts: Due to space limitations, we will not include phrase selection (e.g.[3]) and approaches based on principal component analysis <ref> [5, 24, 21, 19] </ref>. Section 3 describes the classifiers and the document corpus chosen for empirical validation. Section 4 presents the experiments and the results. Section 5 discusses the major findings.
Reference: [6] <author> T.E. Dunning. </author> <title> Accurate methods for the statistics of surprise and coincidence. </title> <booktitle> In Computational Linguistics, volume 19:1, </booktitle> <pages> pages 61-74, </pages> <year> 1993. </year>
Reference-contexts: However, this normalization breaks down (can no longer be ac curately compared to the 2 distribution) if any cell in the contingency table is lightly populated, which is the case for low frequency terms. Hence, the 2 statistic is known not to be reliable for low-frequency terms <ref> [6] </ref>. 2.5 Term strength (TS) Term strength is originally proposed and evaluated by Wilbur and Sirotkin [22] for vocabulary reduction in text retrieval, and later applied by Yang and Wilbur to text categorization [24, 28].
Reference: [7] <author> R. Fano. </author> <title> Transmission of Information. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1961. </year>
Reference-contexts: The entropy computations has a time complexity of O (V m). 2.3 Mutual information (MI) Mutual information is a criterion commonly used in statistical language modelling of word associations and related applications <ref> [7, 2, 21] </ref>. <p> So information gain is also called average mutual information <ref> [7] </ref>.
Reference: [8] <author> N. Fuhr, S. Hartmanna, G. Lustig, M. Schwant-ner, and K. Tzeras. </author> <title> Air/x a rule-based multistage indexing systems for large subject fields. In 606-623, editor, </title> <booktitle> Proceedings of RIAO'91, </booktitle> <year> 1991. </year>
Reference-contexts: Documents categorization is one solution to this problem. A growing number of statistical classification methods and machine learning techniques have been applied to text categorization in recent years, including multivariate regression models <ref> [8, 27] </ref>, nearest neighbor classification [4, 23], Bayes probabilistic approaches [20, 13], decision trees [13], neural networks [21], symbolic rule learning [1, 16, 3] and inductive learning algorithms [3, 12]. A major characteristic, or difficulty, of text categorization problems is the high dimensionality of the feature space. <p> Similarly, kNN treats a document as an single point in a vector space. The context sensitivity is in distinction to context-free methods based on explicit independence assumptions such as naive Bayes classifiers [13] and some other regression methods <ref> [8] </ref>). A context-sensitive classifier makes better use of the information provided by features than a context-free classifier do, thus enabling a better observation on feature selection. 5) The two classifiers differ statistically.
Reference: [9] <author> W. Hersh, C. Buckley, T.J. Leone, and D. Hick--man. Ohsumed: </author> <title> an interactive retrieval evaluation and new large text collection for research. </title> <booktitle> In 17th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'94), </booktitle> <pages> pages 192-201, </pages> <year> 1994. </year>
Reference-contexts: There are 16,039 unique terms in the collection (after performing inflectional stemming, stop word removal, and conversion to lower case). OHSUMED is a bibliographical document collection 3 , developed by William Hersh and colleagues at the Oregon Health Sciences University. It is a subset of the MEDLINE database <ref> [9] </ref>, consisting of 348,566 references from 270 medical journals from the years 1987 to 1991. All of the references have titles, but only 233,445 of them have abstracts. We refer to the title plus abstract as a document.
Reference: [10] <author> D. Koller and M. Sahami. </author> <title> Toward optimal feature selection. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <year> 1996. </year>
Reference-contexts: A recent theoretical comparison, for example, was based on the performance of decision tree algorithms in solving problems with 6 to 180 features in the native space <ref> [10] </ref>. An analysis on this scale is distant from the realities of text categorization. The focus in this paper is the evaluation and comparison of feature selection methods in the reduction of a high dimensional feature space in text categorization problems.
Reference: [11] <author> K. Lang. Newsweeder: </author> <title> Learning to filter netnews. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <year> 1995. </year>
Reference-contexts: Yang & Wilbur [28] used document clustering techniques to estimate probabilistic "term strength", and used it to reduce the variables in linear regression and nearest neighbor classification. Moulinier et al. [16] used an inductive learning algorithm to obtain features in disjunc tive normal form for news story categorization. Lang <ref> [11] </ref> used a minimum description length principle to select terms for Netnews categorization. While many feature selection techniques have been tried, thorough evaluations are rarely carried out for large text categorization problems.
Reference: [12] <author> David D. Lewis, Robert E. Schapire, James P. Callan, and Ron Papka. </author> <title> Training algorithms for linear text classifiers. </title> <booktitle> In SIGIR '96: Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <year> 1996. </year> <pages> 298-306. </pages>
Reference-contexts: number of statistical classification methods and machine learning techniques have been applied to text categorization in recent years, including multivariate regression models [8, 27], nearest neighbor classification [4, 23], Bayes probabilistic approaches [20, 13], decision trees [13], neural networks [21], symbolic rule learning [1, 16, 3] and inductive learning algorithms <ref> [3, 12] </ref>. A major characteristic, or difficulty, of text categorization problems is the high dimensionality of the feature space.
Reference: [13] <author> D.D. Lewis and M. Ringuette. </author> <title> Comparison of two learning algorithms for text categorization. </title> <booktitle> In Proceedings of the Third Annual Symposium on Document Analysis and Information Retrieval (SDAIR'94), </booktitle> <year> 1994. </year>
Reference-contexts: Documents categorization is one solution to this problem. A growing number of statistical classification methods and machine learning techniques have been applied to text categorization in recent years, including multivariate regression models [8, 27], nearest neighbor classification [4, 23], Bayes probabilistic approaches <ref> [20, 13] </ref>, decision trees [13], neural networks [21], symbolic rule learning [1, 16, 3] and inductive learning algorithms [3, 12]. A major characteristic, or difficulty, of text categorization problems is the high dimensionality of the feature space. <p> Documents categorization is one solution to this problem. A growing number of statistical classification methods and machine learning techniques have been applied to text categorization in recent years, including multivariate regression models [8, 27], nearest neighbor classification [4, 23], Bayes probabilistic approaches [20, 13], decision trees <ref> [13] </ref>, neural networks [21], symbolic rule learning [1, 16, 3] and inductive learning algorithms [3, 12]. A major characteristic, or difficulty, of text categorization problems is the high dimensionality of the feature space. <p> Automatic feature selection methods include the removal of non-informative terms according to corpus statistics, and the construction of new features which combine lower level features (i.e., terms) into higher-level orthogonal dimensions. Lewis &Ringuette <ref> [13] </ref> used an information gain measure to aggressively reduce the document vocabulary in a naive Bayes model and a decision-tree approach to binary classification. Wiener et al.[21, 19] used mutual information and a 2 statistic to select features for input to neural networks. <p> to be: G (t) = i=1 P r (c i ) log P r (c i ) P m +P r ( t) i=1 P r (c i j t) log P r (c i j t) This definition is more general than the one employed in binary classification models <ref> [13, 16] </ref>. We use the more general form because text categorization problems usually have a m-ary category space (where m may be up to tens of thousands), and we need to mea sure the goodness of a term globally with respect to all categories on average. <p> Similarly, kNN treats a document as an single point in a vector space. The context sensitivity is in distinction to context-free methods based on explicit independence assumptions such as naive Bayes classifiers <ref> [13] </ref> and some other regression methods [8]). A context-sensitive classifier makes better use of the information provided by features than a context-free classifier do, thus enabling a better observation on feature selection. 5) The two classifiers differ statistically. <p> Hence a evaluation using both classifiers should reduce the possibility of classifier bias in the results. 3.2 Data collections We use two corpora for this study: the Reuters-22173 collection and the OHSUMED collection. The Reuters news story collection is commonly used corpora in text categorization research <ref> [13, 1, 21, 16, 3] </ref> 1 . There are 21,450 documents in the full collection; less than half of the documents have human assigned topic labels. <p> We used only those documents that had at least one topic, divided randomly into a training set of 9,610 and a test set of 3,662 documents. This partition is similar to that employed in [1], but differs from <ref> [13] </ref> who use the full collection including unla 1 A newly revised version, namely Reuters-21578, is available through http://www.research.att.com/ ~ lewis. belled documents 2 . The stories have a mean length of 90.6 words with standard deviation 91.6.
Reference: [14] <author> Tom Mitchell. </author> <title> Machine Learning. </title> <address> McCraw Hill, </address> <year> 1996. </year>
Reference-contexts: That is, low-DF terms are assumed to be relatively informative and therefore should not be removed aggressively. We will re-examine this assumption with respect to text categorization tasks. 2.2 Information gain (IG) Information gain is frequently employed as a term-goodness criterion in the field of machine learning <ref> [17, 14] </ref>. It measures the number of bits of information obtained for category prediction by knowing the presence or absence of a term in a document. Let fc i g m i=1 denote the set of categories in the target space.
Reference: [15] <author> I. Moulinier. </author> <title> Is learning bias an issue on the text categorization problem? In Technical report, </title> <institution> LAFORIA-LIP6, Universite Paris VI, </institution> <note> page (to appear), </note> <year> 1997. </year>
Reference: [16] <author> I. Moulinier, G. Raskinis, and J. Ganascia. </author> <title> Text categorization: a symbolic approach. </title> <booktitle> In Proceedings of the Fifth Annual Symposium on Document Analysis and Information Retrieval, </booktitle> <year> 1996. </year>
Reference-contexts: A growing number of statistical classification methods and machine learning techniques have been applied to text categorization in recent years, including multivariate regression models [8, 27], nearest neighbor classification [4, 23], Bayes probabilistic approaches [20, 13], decision trees [13], neural networks [21], symbolic rule learning <ref> [1, 16, 3] </ref> and inductive learning algorithms [3, 12]. A major characteristic, or difficulty, of text categorization problems is the high dimensionality of the feature space. <p> Yang & Wilbur [28] used document clustering techniques to estimate probabilistic "term strength", and used it to reduce the variables in linear regression and nearest neighbor classification. Moulinier et al. <ref> [16] </ref> used an inductive learning algorithm to obtain features in disjunc tive normal form for news story categorization. Lang [11] used a minimum description length principle to select terms for Netnews categorization. <p> to be: G (t) = i=1 P r (c i ) log P r (c i ) P m +P r ( t) i=1 P r (c i j t) log P r (c i j t) This definition is more general than the one employed in binary classification models <ref> [13, 16] </ref>. We use the more general form because text categorization problems usually have a m-ary category space (where m may be up to tens of thousands), and we need to mea sure the goodness of a term globally with respect to all categories on average. <p> Hence a evaluation using both classifiers should reduce the possibility of classifier bias in the results. 3.2 Data collections We use two corpora for this study: the Reuters-22173 collection and the OHSUMED collection. The Reuters news story collection is commonly used corpora in text categorization research <ref> [13, 1, 21, 16, 3] </ref> 1 . There are 21,450 documents in the full collection; less than half of the documents have human assigned topic labels.
Reference: [17] <author> J.R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: That is, low-DF terms are assumed to be relatively informative and therefore should not be removed aggressively. We will re-examine this assumption with respect to text categorization tasks. 2.2 Information gain (IG) Information gain is frequently employed as a term-goodness criterion in the field of machine learning <ref> [17, 14] </ref>. It measures the number of bits of information obtained for category prediction by knowing the presence or absence of a term in a document. Let fc i g m i=1 denote the set of categories in the target space.
Reference: [18] <author> G. Salton. </author> <title> Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Pennsylvania, </address> <year> 1989. </year>
Reference-contexts: Given a document, for recall thresholds of 0%, 10%, 20%, ... 100%, the system assigns in decreasing score order as many categories as needed until a given recall is achieved, and computes the precision value at that point <ref> [18] </ref>. The resulting 11 point precision values are then averaged to obtain a single-number measure of system performance on that document. For a test set of documents, the average precision values of individual documents are further averaged to obtain a global measure of system performance over the entire set. <p> In the following, unless otherwise specified, we will use "precision" or "AVGP" to refer to the 11-point average precision over a set of test documents. 4.2 Experimental settings Before applying feature selection to documents, we removed the words in a standard stop word list <ref> [18] </ref>. Then each of the five feature selection methods was evaluated with a number of different term-removal thresholds. At a high threshold, it is possible that all the terms in a document are below the threshold. <p> That is, apply a threshold to a document only if it results in a non-empty document; otherwise, apply the closest threshold which results in a non-empty document. We also used the SMART system <ref> [18] </ref> for unified preprocessing followed feature selection, which includes word stemming and weighting. We tried several term weighting options ("ltc", "atc", "lnc" , "bnn" etc. in SMART's notation) which combine the term frequency (TF) measure and the Inverted Document Frequency (IDF) measure in a variety of ways.
Reference: [19] <author> H. Schutze, D.A. Hull, and J.O. Pedersen. </author> <title> A comparison of classifiers and document representations for the routing problem. </title> <booktitle> In 18th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'95), </booktitle> <pages> pages 229-237, </pages> <year> 1995. </year>
Reference-contexts: Wiener et al.[21, 19] used mutual information and a 2 statistic to select features for input to neural networks. Yang [24] and Schutze et al. <ref> [19, 21, 19] </ref> used principal component analysis to find orthogonal dimensions in the vector space of documents. Yang & Wilbur [28] used document clustering techniques to estimate probabilistic "term strength", and used it to reduce the variables in linear regression and nearest neighbor classification. <p> Due to space limitations, we will not include phrase selection (e.g.[3]) and approaches based on principal component analysis <ref> [5, 24, 21, 19] </ref>. Section 3 describes the classifiers and the document corpus chosen for empirical validation. Section 4 presents the experiments and the results. Section 5 discusses the major findings.
Reference: [20] <author> K. Tzeras and S. Hartman. </author> <title> Automatic indexing based on bayesian inference networks. </title> <booktitle> In Proc 16th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'93), </booktitle> <pages> pages 22-34, </pages> <year> 1993. </year>
Reference-contexts: Documents categorization is one solution to this problem. A growing number of statistical classification methods and machine learning techniques have been applied to text categorization in recent years, including multivariate regression models [8, 27], nearest neighbor classification [4, 23], Bayes probabilistic approaches <ref> [20, 13] </ref>, decision trees [13], neural networks [21], symbolic rule learning [1, 16, 3] and inductive learning algorithms [3, 12]. A major characteristic, or difficulty, of text categorization problems is the high dimensionality of the feature space.
Reference: [21] <author> E. Wiener, J.O. Pedersen, </author> <title> and A.S. Weigend. A neural network approach to topic spotting. </title> <booktitle> In Proceedings of the Fourth Annual Symposium on Document Analysis and Information Retrieval (SDAIR'95), </booktitle> <year> 1995. </year>
Reference-contexts: Documents categorization is one solution to this problem. A growing number of statistical classification methods and machine learning techniques have been applied to text categorization in recent years, including multivariate regression models [8, 27], nearest neighbor classification [4, 23], Bayes probabilistic approaches [20, 13], decision trees [13], neural networks <ref> [21] </ref>, symbolic rule learning [1, 16, 3] and inductive learning algorithms [3, 12]. A major characteristic, or difficulty, of text categorization problems is the high dimensionality of the feature space. <p> Wiener et al.[21, 19] used mutual information and a 2 statistic to select features for input to neural networks. Yang [24] and Schutze et al. <ref> [19, 21, 19] </ref> used principal component analysis to find orthogonal dimensions in the vector space of documents. Yang & Wilbur [28] used document clustering techniques to estimate probabilistic "term strength", and used it to reduce the variables in linear regression and nearest neighbor classification. <p> Due to space limitations, we will not include phrase selection (e.g.[3]) and approaches based on principal component analysis <ref> [5, 24, 21, 19] </ref>. Section 3 describes the classifiers and the document corpus chosen for empirical validation. Section 4 presents the experiments and the results. Section 5 discusses the major findings. <p> The entropy computations has a time complexity of O (V m). 2.3 Mutual information (MI) Mutual information is a criterion commonly used in statistical language modelling of word associations and related applications <ref> [7, 2, 21] </ref>. <p> On another variation of the Reuters collection where the training set and the test set are partitioned differently, kNN has a break-even point of 82% which is the same as the result of neural networks <ref> [21] </ref>, and LLSF has a break-even point of 81%. 2) Both systems scale to large classification problems. By "large" we mean that both the input and the output of a classifier can have thousands of dimensions or higher [25, 24]. <p> Hence a evaluation using both classifiers should reduce the possibility of classifier bias in the results. 3.2 Data collections We use two corpora for this study: the Reuters-22173 collection and the OHSUMED collection. The Reuters news story collection is commonly used corpora in text categorization research <ref> [13, 1, 21, 16, 3] </ref> 1 . There are 21,450 documents in the full collection; less than half of the documents have human assigned topic labels.
Reference: [22] <author> J.W. Wilbur and K. Sirotkin. </author> <title> The automatic identification of stop words. </title> <journal> J. Inf. Sci., </journal> <volume> 18 </volume> <pages> 45-55, </pages> <year> 1992. </year>
Reference-contexts: Hence, the 2 statistic is known not to be reliable for low-frequency terms [6]. 2.5 Term strength (TS) Term strength is originally proposed and evaluated by Wilbur and Sirotkin <ref> [22] </ref> for vocabulary reduction in text retrieval, and later applied by Yang and Wilbur to text categorization [24, 28]. This method estimates term importance based on how commonly a term is likely to appear in "closely-related" documents. <p> The value of AREL is chosen experimentally, according to how well it optimized the performance in the task. According to previous evaluations of retrieval and categorization on several document collections <ref> [22, 28] </ref>, the AREL values between 10 to 20 yield satisfactory performance.
Reference: [23] <author> Y. Yang. </author> <title> Expert network: Effective and efficient learning from human decisions in text categorization and retrieval. </title> <booktitle> In 17th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'94), </booktitle> <pages> pages 13-22, </pages> <year> 1994. </year>
Reference-contexts: Documents categorization is one solution to this problem. A growing number of statistical classification methods and machine learning techniques have been applied to text categorization in recent years, including multivariate regression models [8, 27], nearest neighbor classification <ref> [4, 23] </ref>, Bayes probabilistic approaches [20, 13], decision trees [13], neural networks [21], symbolic rule learning [1, 16, 3] and inductive learning algorithms [3, 12]. A major characteristic, or difficulty, of text categorization problems is the high dimensionality of the feature space.
Reference: [24] <author> Y. Yang. </author> <title> Noise reduction in a statistical approach to text categorization. </title> <booktitle> In Proceedings of the 18th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'95), </booktitle> <pages> pages 256-263, </pages> <year> 1995. </year>
Reference-contexts: Lewis &Ringuette [13] used an information gain measure to aggressively reduce the document vocabulary in a naive Bayes model and a decision-tree approach to binary classification. Wiener et al.[21, 19] used mutual information and a 2 statistic to select features for input to neural networks. Yang <ref> [24] </ref> and Schutze et al. [19, 21, 19] used principal component analysis to find orthogonal dimensions in the vector space of documents. Yang & Wilbur [28] used document clustering techniques to estimate probabilistic "term strength", and used it to reduce the variables in linear regression and nearest neighbor classification. <p> Due to space limitations, we will not include phrase selection (e.g.[3]) and approaches based on principal component analysis <ref> [5, 24, 21, 19] </ref>. Section 3 describes the classifiers and the document corpus chosen for empirical validation. Section 4 presents the experiments and the results. Section 5 discusses the major findings. <p> Hence, the 2 statistic is known not to be reliable for low-frequency terms [6]. 2.5 Term strength (TS) Term strength is originally proposed and evaluated by Wilbur and Sirotkin [22] for vocabulary reduction in text retrieval, and later applied by Yang and Wilbur to text categorization <ref> [24, 28] </ref>. This method estimates term importance based on how commonly a term is likely to appear in "closely-related" documents. <p> By "large" we mean that both the input and the output of a classifier can have thousands of dimensions or higher <ref> [25, 24] </ref>. We want to examine all the degrees of feature selection, from no reduction (except removing standard stop words) to extremely aggressive reduction, and observe the effects on the accuracy of a classifier over the entire target space. <p> That is, we computed only 200 largest singular values in solving LLSF, although the best results (which is similar to the performance of kNN) appeared with using 1000 singular values <ref> [24] </ref>. Nevertheless, this simplification of LLSF should not invalidate the examination of feature selection which is the focus of the experiments. An observation merges from the categorization results of kNN and LLSF on Reuters. That is, IG, DF and CHI thresholding have similar effects on the performance of the classifiers.
Reference: [25] <author> Y. Yang. </author> <title> Sampling strategies and learning efficiency in text categorization. </title> <booktitle> In AAAI Spring Symposium on Machine Learning in Information Access, </booktitle> <pages> pages 88-95, </pages> <year> 1996. </year>
Reference-contexts: By "large" we mean that both the input and the output of a classifier can have thousands of dimensions or higher <ref> [25, 24] </ref>. We want to examine all the degrees of feature selection, from no reduction (except removing standard stop words) to extremely aggressive reduction, and observe the effects on the accuracy of a classifier over the entire target space.
Reference: [26] <author> Y. Yang. </author> <title> An evaluation of statistical approach to text categorization. </title> <note> In Technical Report CMU-CS-97-127, </note> <institution> Computer Science Department, Carnegie Mellon University, </institution> <year> 1997. </year>
Reference-contexts: The regression coefficients are determined by solving a least squares fit of the mapping from training documents to training categories. Several properties of kNN and LLSF make them suitable for our experiments: 1) Both systems are top-performing, state-of-the-art classifiers. In a recent evaluation of classification methods <ref> [26] </ref> on the Reuters newswire collection (next section), the break-even point values were 85% for both kNN and LLSF, outperforming all the other systems evaluated on the same collection, including symbolic rule learning by RIPPER (80%)[3], SWAP-1 (79%)[1] and CHARADE (78%)[16], a decision approach using C4.5 (79%)[15], inductive learning by Sleeping <p> That is, a large proportion of the documents in the test set are incorrectly unlabelled. This makes the evaluation results highly questionable or non-interpretable unless these unlabelled documents are discarded, as analyzed in <ref> [26] </ref>. 3 OHSUMED is available via anonymous ftp from medir.ohsu.edu in the directory /pub/ohsumed. of recall and precision as performance measures: recall = categories found and correct total categories correct precision = categories found and correct total categories found where "categories found" means that the categories are above a given score
Reference: [27] <author> Y. Yang and C.G. Chute. </author> <title> An example-based mapping method for text categorization and retrieval. </title> <journal> ACM Transaction on Information Systems (TOIS), </journal> <pages> pages 253-277, </pages> <year> 1994. </year>
Reference-contexts: Documents categorization is one solution to this problem. A growing number of statistical classification methods and machine learning techniques have been applied to text categorization in recent years, including multivariate regression models <ref> [8, 27] </ref>, nearest neighbor classification [4, 23], Bayes probabilistic approaches [20, 13], decision trees [13], neural networks [21], symbolic rule learning [1, 16, 3] and inductive learning algorithms [3, 12]. A major characteristic, or difficulty, of text categorization problems is the high dimensionality of the feature space.
Reference: [28] <author> Y. Yang and W.J. Wilbur. </author> <title> Using corpus statistics to remove redundant words in text categorization. </title> <journal> In J Amer Soc Inf Sci, </journal> <year> 1996. </year>
Reference-contexts: Wiener et al.[21, 19] used mutual information and a 2 statistic to select features for input to neural networks. Yang [24] and Schutze et al. [19, 21, 19] used principal component analysis to find orthogonal dimensions in the vector space of documents. Yang & Wilbur <ref> [28] </ref> used document clustering techniques to estimate probabilistic "term strength", and used it to reduce the variables in linear regression and nearest neighbor classification. Moulinier et al. [16] used an inductive learning algorithm to obtain features in disjunc tive normal form for news story categorization. <p> Hence, the 2 statistic is known not to be reliable for low-frequency terms [6]. 2.5 Term strength (TS) Term strength is originally proposed and evaluated by Wilbur and Sirotkin [22] for vocabulary reduction in text retrieval, and later applied by Yang and Wilbur to text categorization <ref> [24, 28] </ref>. This method estimates term importance based on how commonly a term is likely to appear in "closely-related" documents. <p> The value of AREL is chosen experimentally, according to how well it optimized the performance in the task. According to previous evaluations of retrieval and categorization on several document collections <ref> [22, 28] </ref>, the AREL values between 10 to 20 yield satisfactory performance.
References-found: 28

