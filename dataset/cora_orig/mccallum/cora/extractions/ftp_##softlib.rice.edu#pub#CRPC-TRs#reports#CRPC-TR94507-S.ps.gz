URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR94507-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: A Model and Compilation Strategy for Out-of-Core Data Parallel Programs  
Author: Rajesh Bordawekar Alok Choudhary Ken Kennedy Charles Koelbel Mike Paleczny 
Abstract: It is widely acknowledged in high-performance computing circles that parallel input/output needs substantial improvement in order to make scalable computers truly usable. We present a data storage model that allows processors independent access to their own data and a corresponding compilation strategy that integrates data-parallel computation with data distribution for out-of-core problems. Our results compare several communication methods and I/O optimizations using two out-of-core problems, Jacobi iteration and LU factorization. 
Abstract-found: 1
Intro-found: 1
Reference: [AK87] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: Previous work on compiler improvements at the memory to disk interface starts with Abu-Sufah and Trivedi at the end of the 1970's. Highlights of managing other aspects of the memory hierarchy include: Allen and Kennedy on vector register allocation <ref> [AK87] </ref>, Carr and Kennedy on compiler blocking of scientific codes [CK92], and Mowry on software prefetching for cache [Mow94]. 8 Conclusions High Performance Fortran has already generated a great deal of interest in the user and vendor community as a language for portable parallel programming on high-performance computers.
Reference: [CBH + 94] <author> A. Choudhary, R. Bordawekar, M. Harry, R. Krishnaiyer, R. Ponnusamy, T. Singh, and R. Thakur. </author> <title> PASSION: Parallel and Scalable Software for Input-Output. </title> <type> Technical Report SCCS-636, </type> <institution> NPAC, Syracuse University, </institution> <month> Sep </month> <year> 1994. </year>
Reference-contexts: For nonlocal out-of-core, there are several possibilities. If the owning processor previously had the data in memory, the compiler can schedule it to communicate the data then and schedule the requesting processor to store the data in its local array file. Otherwise, a two-phase method <ref> [CBH + 94] </ref> where the owner reads the data and sends it to the requesting processor when needed can be used. A final possibility applies if there is some processing capability at the I/O node itself, in which case disk-directed I/O can be used to send the data [Kot94]. <p> Thus, an implementation of a collective communication routine requires reading data from local files, communicating data to the appropriate destination processors, and finally, writing the data into the files of receiving processors. A detailed description of these routines is given in <ref> [CBH + 94] </ref>. 6 Experimental Results This section presents experimental results for two out-of-core applications: Laplace equation solver by Jacobi iteration method, and LU factorization with pivoting. These applications were compiled by hand using the local placement model, storing the OCLAs for each processor into a separate file.
Reference: [CK92] <author> S. Carr and K. Kennedy. </author> <title> Compiler Blockabilty of Numerical Algorithms. </title> <booktitle> Proc. of Supercomputing'92, </booktitle> <month> November </month> <year> 1992. </year>
Reference-contexts: Previous work on compiler improvements at the memory to disk interface starts with Abu-Sufah and Trivedi at the end of the 1970's. Highlights of managing other aspects of the memory hierarchy include: Allen and Kennedy on vector register allocation [AK87], Carr and Kennedy on compiler blocking of scientific codes <ref> [CK92] </ref>, and Mowry on software prefetching for cache [Mow94]. 8 Conclusions High Performance Fortran has already generated a great deal of interest in the user and vendor community as a language for portable parallel programming on high-performance computers.
Reference: [dRBC93] <author> J. del Rosario, R. Bordawekar, and A. Choudhary. </author> <title> Improved parallel i/o via a two-phase runtime access strategy. </title> <booktitle> In Proceedings of the Workshop on I/O in Parallel Computer Systems at IPPS '93, </booktitle> <month> April </month> <year> 1993. </year>
Reference-contexts: Each file will have to be redistributed into local files based on the out-of-core distribution. These routines use the two-phase access strategy <ref> [dRBC93] </ref> which will read the data from the input data file using the most optimal access pattern (which depends on how the data is stored on disks) and redistribute the data over the processors using the high speed processor interconnection network.
Reference: [FHK + 90] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, and C. Tseng. </author> <title> Fortran D language specifications. </title> <type> Technical Report COMP TR90-141, </type> <institution> Rice University, </institution> <year> 1990. </year>
Reference-contexts: Compilers can use programmer-supplied decomposition patterns such as block and cyclic to partition computation, generate communication and synchronization, and guide optimization of the program. Languages based on this principle are called data-parallel languages and include High Performance Fortran (HPF) [Hig93], Vienna Fortran [ZBC + 92], and Fortran D <ref> [FHK + 90] </ref>. Our out-of-core approach builds on HPF. The DISTRIBUTE directive in HPF partitions an array among processors by specifying which elements of the array are mapped to each processor. <p> Performance of LU factorization with pivoting Array Size: 6400 fi 6400 Number of Processors Synchronous I/O Overlapped I/O % Overlap % of Optimal Overlap 1 12339 10991 11 67 4 3464 3206 7.4 71 16 1110 999 10 61 Fortran (HPF) [Hig93], Vienna Fortran [ZBC + 92], and Fortran D <ref> [FHK + 90] </ref>. Previous work on compiler improvements at the memory to disk interface starts with Abu-Sufah and Trivedi at the end of the 1970's.
Reference: [Fox91] <author> Geoffrey Fox. </author> <title> The architecture of problems and portable parallel software systems. </title> <type> Technical Report SCCS-78b, </type> <institution> Northeast Parallel Architectures Center, Syracuse University, Syracuse, </institution> <address> NY 13244, </address> <year> 1991. </year>
Reference-contexts: In essence, data-parallel programs apply the same conceptual operations to all elements of large data structures. This form of parallelism occurs naturally in many scientific and engineering applications such as partial differential equation solvers and linear algebra routines <ref> [Fox91] </ref>. In these programs, a decomposition of the data domain exploits the 2 inherent parallelism and adapts it to a particular machine. Compilers can use programmer-supplied decomposition patterns such as block and cyclic to partition computation, generate communication and synchronization, and guide optimization of the program.
Reference: [Hig93] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification. </title> <booktitle> Scientific Programming, </booktitle> <address> 2(1-2):1-170, </address> <year> 1993. </year>
Reference-contexts: Compilers can use programmer-supplied decomposition patterns such as block and cyclic to partition computation, generate communication and synchronization, and guide optimization of the program. Languages based on this principle are called data-parallel languages and include High Performance Fortran (HPF) <ref> [Hig93] </ref>, Vienna Fortran [ZBC + 92], and Fortran D [FHK + 90]. Our out-of-core approach builds on HPF. The DISTRIBUTE directive in HPF partitions an array among processors by specifying which elements of the array are mapped to each processor. <p> that improved performance by 200 times. 8 Table 3: Performance of LU factorization with pivoting Array Size: 6400 fi 6400 Number of Processors Synchronous I/O Overlapped I/O % Overlap % of Optimal Overlap 1 12339 10991 11 67 4 3464 3206 7.4 71 16 1110 999 10 61 Fortran (HPF) <ref> [Hig93] </ref>, Vienna Fortran [ZBC + 92], and Fortran D [FHK + 90]. Previous work on compiler improvements at the memory to disk interface starts with Abu-Sufah and Trivedi at the end of the 1970's.
Reference: [Kot94] <author> D. Kotz. </author> <title> Disk-directed I/O for MIMD Multiprocessors. </title> <type> Technical Report PCS-TR94-226, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> July </month> <year> 1994. </year> <month> 9 </month>
Reference-contexts: A final possibility applies if there is some processing capability at the I/O node itself, in which case disk-directed I/O can be used to send the data <ref> [Kot94] </ref>. The most important point to note here is that data needed by other processors is communicated while the slab is in memory when possible. Thus, this method reduces the number of disk accesses.
Reference: [Mow94] <author> T. Mowry. </author> <title> Tolerating Latency Through Software Controlled Data Prefetching. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Stanford University, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Highlights of managing other aspects of the memory hierarchy include: Allen and Kennedy on vector register allocation [AK87], Carr and Kennedy on compiler blocking of scientific codes [CK92], and Mowry on software prefetching for cache <ref> [Mow94] </ref>. 8 Conclusions High Performance Fortran has already generated a great deal of interest in the user and vendor community as a language for portable parallel programming on high-performance computers. In order to permit implementation of large-scale problems, e.g., many grand challenge problems, support for out-of-core compilation is important.

References-found: 9

