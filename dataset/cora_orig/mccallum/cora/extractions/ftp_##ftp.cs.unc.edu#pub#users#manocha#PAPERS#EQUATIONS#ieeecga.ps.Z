URL: ftp://ftp.cs.unc.edu/pub/users/manocha/PAPERS/EQUATIONS/ieeecga.ps.Z
Refering-URL: http://www.cs.unc.edu/Research/graphics/pubs.html
Root-URL: http://www.cs.unc.edu
Title: Solving Systems of Polynomial Equations  
Author: Dinesh Manocha 
Address: Chapel Hill, NC 27599-3175  
Affiliation: Department of Computer Science University of North Carolina,  
Date: March 1994  
Note: Appeared in IEEE Computer Graphics and Applications,  
Abstract: Current geometric and solid modeling systems use semi-algebraic sets for defining the boundaries of solid objects, curves and surfaces, geometric constraints with mating relationship in a mechanical assembly, physical contacts between objects, collision detection. It turns out that performing many of the geometric operations on the solid boundaries or interacting with geometric constraints is reduced to finding common solutions of the polynomial equations. Current algorithms in the literature based on symbolic, numeric and geometric methods suffer from robustness, accuracy or efficiency problems or are limited to a class of problems only. In this paper we present algorithms based on multipolynomial resultants and matrix computations for solving polynomial systems. These algorithms are based on the linear algebra formulation of resultants of equations and in many cases there is an elegant relationship between the matrix structures and the geometric formulation. The resulting algorithm involves singular value decompositions, eigendecompositions, Gauss elimination etc. In the context of floating point computation their numerical accuracy is well understood. We also present techniques to make use of the structure of the matrices to improve the performance of the resulting algorithm and highlight the performance of the algorithms on different examples. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Lavender, A. Bowyer, J. Davenport, A. Wallis, and J. Woodwark. </author> <title> Voronoi diagrams of set-theoretic solid models. </title> <journal> IEEE Computer Graphics and Applications, </journal> <pages> pages 69-77, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: For example, the geometric constraints associated with kinematic relationships in a mechanical assembly (composed of prismatic and revolute joints) define semi algebraic sets. Other applications include the representations of voronoi diagrams of set-theoretic models <ref> [1] </ref>, formulation of configuration space of a robot in motion planning applications [2] etc. A fundamental problem in these geometric computations is that solving systems of polynomial equations. <p> and surfaces, inverse kinematics of serial or parallel mechanisms, collision detection, computing the distance from a point to a curve, finding a point on the bisector between two curves or a point equidistant from three curves and solving geometric constraint systems are reduced to finding roots of nonlinear polynomial equations <ref> [3, 4, 1] </ref>. The need for solving algebraic equations also arises in surface intersection algorithms, for finding starting points on each component and locating singularities [5, 6], manipulating offset curves and surfaces [7] and geometric theorem proving [5]. Most of these problems have been extensively studied in the literature. <p> Secondly, we consider the problem of finding a distance from a point to a curve or a surface. It is well known that these problems can be reduced to solving non-linear algebraic equations and they are frequently encountered in the computation of offset curves and surfaces and voronoi surfaces <ref> [7, 1] </ref>. The matrix relationship expressed in (1) is also used for computing the birational maps. These birational maps are expressed in terms of ratio of determinants as opposed to algebraic expression. <p> Some examples include computation of the medial axis transform, voronoi surfaces, offset curve and surfaces etc. <ref> [7, 1] </ref>. It is well known in literature that this problem can either be posed as an optimization problem (minimizing the distance function) or reduce it to solving algebraic equations. Let (X; Y ) be a point and F (x; y) = 0 be the curve. <p> Then there are two equations: F (x p ; y p ) = 0 14 It turns out that these equations have more than one real solution and therefore, any algorithm based on local methods may converge to a wrong solution <ref> [1] </ref>. Algorithms based on constrained optimization may converge to a local minima of the function. Thus, none of the previous technique is able to solve this problem in a reasonable manner. Using the algebraic formulation the problem reduces to solving for x p ; y p in these two equations. <p> For example, many algorithms for boundary computations, intersection, ray tracing on Bezier curves and surfaces need the eigenvalues in the <ref> [0; 1] </ref> domain only. The matrices C; C 1 and C 2 are relatively sparse. It turns out that the Macaulay formulation results in sparse matrices as well. In other words, the matrices M i arising from Macaulay's formulation are sparse.
Reference: [2] <author> J.F. Canny. </author> <title> The Complexity of Robot Motion Planning. ACM Doctoral Dissertation Award. </title> <publisher> MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: For example, the geometric constraints associated with kinematic relationships in a mechanical assembly (composed of prismatic and revolute joints) define semi algebraic sets. Other applications include the representations of voronoi diagrams of set-theoretic models [1], formulation of configuration space of a robot in motion planning applications <ref> [2] </ref> etc. A fundamental problem in these geometric computations is that solving systems of polynomial equations. <p> Resultants have gained a lot of importance in geometric and solid modeling literature. Following Sederberg's thesis [3] on implicitization of curves and surfaces, resultants have been applied to motion planning <ref> [2] </ref> and many other geometric problems, as surveyed in [18]. Most of the earlier practical applications of resultants were limited to low degree curve and surface intersections [7]. The idea of combining resultant formulations with matrix computations has been proposed with Macaulay's formulation in [19].
Reference: [3] <author> T.W. </author> <title> Sederberg. Implicit and Parametric Curves and Surfaces. </title> <type> PhD thesis, </type> <institution> Purdue University, </institution> <year> 1983. </year>
Reference-contexts: and surfaces, inverse kinematics of serial or parallel mechanisms, collision detection, computing the distance from a point to a curve, finding a point on the bisector between two curves or a point equidistant from three curves and solving geometric constraint systems are reduced to finding roots of nonlinear polynomial equations <ref> [3, 4, 1] </ref>. The need for solving algebraic equations also arises in surface intersection algorithms, for finding starting points on each component and locating singularities [5, 6], manipulating offset curves and surfaces [7] and geometric theorem proving [5]. Most of these problems have been extensively studied in the literature. <p> The ongoing activity in sparse elimination methods is important as many polynomial systems resulting from applications in geometric constraint systems, intersection, offsets and blends are sparse [7, 17]. Resultants have gained a lot of importance in geometric and solid modeling literature. Following Sederberg's thesis <ref> [3] </ref> on implicitization of curves and surfaces, resultants have been applied to motion planning [2] and many other geometric problems, as surveyed in [18]. Most of the earlier practical applications of resultants were limited to low degree curve and surface intersections [7].
Reference: [4] <author> J. Kajiya. </author> <title> Ray tracing parametric patches. </title> <journal> Computer Graphics, </journal> <volume> 16(3) </volume> <pages> 245-254, </pages> <year> 1982. </year>
Reference-contexts: and surfaces, inverse kinematics of serial or parallel mechanisms, collision detection, computing the distance from a point to a curve, finding a point on the bisector between two curves or a point equidistant from three curves and solving geometric constraint systems are reduced to finding roots of nonlinear polynomial equations <ref> [3, 4, 1] </ref>. The need for solving algebraic equations also arises in surface intersection algorithms, for finding starting points on each component and locating singularities [5, 6], manipulating offset curves and surfaces [7] and geometric theorem proving [5]. Most of these problems have been extensively studied in the literature.
Reference: [5] <author> C.M. Hoffmann. </author> <title> Geometric and Solid Modeling. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1989. </year>
Reference-contexts: The need for solving algebraic equations also arises in surface intersection algorithms, for finding starting points on each component and locating singularities <ref> [5, 6] </ref>, manipulating offset curves and surfaces [7] and geometric theorem proving [5]. Most of these problems have been extensively studied in the literature. The currently known techniques for solving non-linear polynomial systems can be classified into symbolic, numeric and geometric methods. <p> The need for solving algebraic equations also arises in surface intersection algorithms, for finding starting points on each component and locating singularities [5, 6], manipulating offset curves and surfaces [7] and geometric theorem proving <ref> [5] </ref>. Most of these problems have been extensively studied in the literature. The currently known techniques for solving non-linear polynomial systems can be classified into symbolic, numeric and geometric methods. <p> This problem arises frequently in the boundary computations <ref> [5] </ref>. It turns out that this problem corresponds to solving 3 equations in 3 unknowns. <p> As a result, we see that F and F 1 define rational maps between the s; t; u and x; y; z; w space. Algorithms to compute the birational maps are known in the literature. They are based on Gr-obner bases <ref> [5] </ref> or multipolynomial resultants [18]. However, direct applications of these algorithms suffer from accuracy and efficiency problems as highlighted in [7]. We make use of the fact that multipolynomial resultants linearize a non-linear problem by eliminating some variables as shown in (1).
Reference: [6] <author> D. Manocha and J.F. Canny. </author> <title> A new approach for surface intersection. </title> <journal> International Journal of Computational Geometry and Applications, </journal> <volume> 1(4) </volume> <pages> 491-516, </pages> <year> 1991. </year> <note> Special issue on Solid Modeling. </note>
Reference-contexts: The need for solving algebraic equations also arises in surface intersection algorithms, for finding starting points on each component and locating singularities <ref> [5, 6] </ref>, manipulating offset curves and surfaces [7] and geometric theorem proving [5]. Most of these problems have been extensively studied in the literature. The currently known techniques for solving non-linear polynomial systems can be classified into symbolic, numeric and geometric methods. <p> In [20] the problem of curve intersection is analyzed using resultants and algorithms are presented based on Bezout resultant of two polynomials and matrix computations. Higher order intersections corresponding to tangential intersections or singular points are considered as well. For intersection of triangular or tensor product surfaces, <ref> [6] </ref> use the fact that the implicit representation of such surfaces corresponds to the determinant of a matrix and proposed a representation and evaluation of the intersection curve in terms of matrix computations. 3 Matrix Computations In this section we review techniques from linear algebra and numerical analysis used in our
Reference: [7] <author> C.M. Hoffmann. </author> <title> A dimensionality paradigm for surface interrogations. </title> <booktitle> Computer Aided Geometric Design, </booktitle> <volume> 7 </volume> <pages> 517-532, </pages> <year> 1990. </year>
Reference-contexts: The need for solving algebraic equations also arises in surface intersection algorithms, for finding starting points on each component and locating singularities [5, 6], manipulating offset curves and surfaces <ref> [7] </ref> and geometric theorem proving [5]. Most of these problems have been extensively studied in the literature. The currently known techniques for solving non-linear polynomial systems can be classified into symbolic, numeric and geometric methods. <p> As far as the use of algebraic methods in geometric and solid modeling is concerned, the current viewpoint is that they have led to better theoretical understanding of the problems, but their practical impact is unclear <ref> [7, 9] </ref>. The numeric methods for solving polynomial equations can be classified into iterative methods and homotopy methods. Iterative techniques, like the Newton's method, are good for local analysis only and work well if we are given good initial guess to each so 1 lution. <p> The ongoing activity in sparse elimination methods is important as many polynomial systems resulting from applications in geometric constraint systems, intersection, offsets and blends are sparse <ref> [7, 17] </ref>. Resultants have gained a lot of importance in geometric and solid modeling literature. Following Sederberg's thesis [3] on implicitization of curves and surfaces, resultants have been applied to motion planning [2] and many other geometric problems, as surveyed in [18]. <p> Following Sederberg's thesis [3] on implicitization of curves and surfaces, resultants have been applied to motion planning [2] and many other geometric problems, as surveyed in [18]. Most of the earlier practical applications of resultants were limited to low degree curve and surface intersections <ref> [7] </ref>. The idea of combining resultant formulations with matrix computations has been proposed with Macaulay's formulation in [19]. <p> Secondly, we consider the problem of finding a distance from a point to a curve or a surface. It is well known that these problems can be reduced to solving non-linear algebraic equations and they are frequently encountered in the computation of offset curves and surfaces and voronoi surfaces <ref> [7, 1] </ref>. The matrix relationship expressed in (1) is also used for computing the birational maps. These birational maps are expressed in terms of ratio of determinants as opposed to algebraic expression. <p> Some examples include computation of the medial axis transform, voronoi surfaces, offset curve and surfaces etc. <ref> [7, 1] </ref>. It is well known in literature that this problem can either be posed as an optimization problem (minimizing the distance function) or reduce it to solving algebraic equations. Let (X; Y ) be a point and F (x; y) = 0 be the curve. <p> Algorithms to compute the birational maps are known in the literature. They are based on Gr-obner bases [5] or multipolynomial resultants [18]. However, direct applications of these algorithms suffer from accuracy and efficiency problems as highlighted in <ref> [7] </ref>. We make use of the fact that multipolynomial resultants linearize a non-linear problem by eliminating some variables as shown in (1). In particular, we express the birational maps in terms of ratio of determinants and use matrix computations for their computation.
Reference: [8] <author> J.H. Wilkinson. </author> <title> The evaluation of the zeros of ill-conditioned polynomials. parts i and ii. </title> <journal> Numer. Math., </journal> <volume> 1 </volume> <pages> 150-166 and 167-180, </pages> <year> 1959. </year>
Reference-contexts: The major problem arises from the fact that computing roots of a univariate polynomial can be ill-conditioned for polynomials of degree greater than 14 or 15, as shown by Wilkinson <ref> [8] </ref>. As a result, it is difficult to implement these algebraic methods using finite precision arithmetic and that slows down the resulting algorithm.
Reference: [9] <author> A.A.G. Requicha and J.R. Rossignac. </author> <title> Solid modeling and beyond. </title> <journal> IEEE Computer Graphics and Applications, </journal> <pages> pages 31-44, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: As far as the use of algebraic methods in geometric and solid modeling is concerned, the current viewpoint is that they have led to better theoretical understanding of the problems, but their practical impact is unclear <ref> [7, 9] </ref>. The numeric methods for solving polynomial equations can be classified into iterative methods and homotopy methods. Iterative techniques, like the Newton's method, are good for local analysis only and work well if we are given good initial guess to each so 1 lution.
Reference: [10] <author> A. P. Morgan. </author> <title> Polynomial continuation and its relationship to the symbolic reduction of polynomial systems. </title> <booktitle> In Symbolic and Numerical Computation for Artificial Intelligence, </booktitle> <pages> pages 23-45, </pages> <year> 1992. </year>
Reference-contexts: Homotopy methods based on continuation techniques have a good theoretical background and proceed by following paths in the complex space. In theory, each path converges to a geometrically isolated solution. They have been implemented and applied to a variety of applications <ref> [10] </ref>. In practice the current implementations have many problems. The different paths being followed may not be geometrically isolated and thereby causing problems with the robustness of the approach. Moreover, continuation methods are considered to be computationally very demanding and at the moment restricted to solving dense polynomial systems only. <p> Initially, we consider an example of intersection of three surfaces from <ref> [10] </ref>. In particular, Morgan uses continuation methods to solve this problems and argues that approaches based on symbolic reduction lead to severe numeric problems. Secondly, we consider the problem of finding a distance from a point to a curve or a surface. <p> Resultants and Gr-obner bases have been used to eliminate two variables from these three equation. The resulting problem correspond to finding roots of a univari-ate polynomial, which can be ill-conditioned on low degree polynomials. We illustrate it on an example from <ref> [10] </ref> and also analyze the accuracy of the algorithm presented in this paper. <p> a cylinder and a plane described by the following equations: 1:6e3 x 2 1 + 1:6e3 x 2 2 1 = 0 1 + 5:3e4 x 2 3 + 2:7e2 x 1 1 = 0 1:4e4 x 1 + 1:0e4 x 2 + x 3 3:4e3 = 0: According to <ref> [10] </ref>, these equations have two real solutions of norm about 25 and a complex conjugate pair of order 10 9 . The two real solutions have a physical meaning. After eliminating x 2 and x 3 from these equations, the eliminant is [10]: 6:38281970398352 x 4 1 + 1 + 9:36558635415069e20 <p> + x 3 3:4e3 = 0: According to <ref> [10] </ref>, these equations have two real solutions of norm about 25 and a complex conjugate pair of order 10 9 . The two real solutions have a physical meaning. After eliminating x 2 and x 3 from these equations, the eliminant is [10]: 6:38281970398352 x 4 1 + 1 + 9:36558635415069e20 x 1 where the coefficients have been rounded to 15 digits. In the original polynomial system there is a range of 4 orders of magnitude in the coefficients and in the eliminant the range is 22.
Reference: [11] <author> T.W. Sederberg and T. Nishita. </author> <title> Curve intersection using bezier clipping. </title> <booktitle> Computer-Aided Design, </booktitle> <volume> 22 </volume> <pages> 538-549, </pages> <year> 1990. </year> <month> 21 </month>
Reference-contexts: For some particular applications, algorithms have been developed using the geometric formulation of the problem. This includes subdivision based algorithms for curves and surface intersection, ray tracing. In the general case subdivision algorithms have limited applications and their convergence is slow. Their convergence has been improved by Bezier clipping <ref> [11] </ref>. However, for low degree curve intersections algebraic methods have been found to be the fastest in practice. Similarly algorithms based on the geometric properties of mechanisms have been developed for a class of kinematics problems, constraint systems and motion planning problems.
Reference: [12] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Green--baum, S. Hammarling, and D. Sorensen. </author> <title> LAPACK User's Guide, Release 1.0. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1992. </year>
Reference-contexts: For most of the linear algebra problems, backward stable algorithms are known. This is in contrast with finding roots of high degree univariate polynomials. Furthermore, efficient implementations of backward stable routines are available as part of LAPACK <ref> [12] </ref> and we used them in our applications. The rest of the paper is organized in the following manner. In Section 2 we review the 2 results from elimination theory. We consider sparse as well as dense polynomial systems. <p> As a result, the eigenvalues of a diagonal matrix, upper triangular matrix or a lower triangular matrix correspond to the elements on its diagonal. Efficient algorithms for computing eigenvalues and eigenvectors are well known, [22], and their implementations are available as part LAPACK <ref> [12] </ref>. 3.4 Power Iterations The largest or the smallest eigenvalue of a matrix (and the corresponding eigenvector) can be computed using the Power method [22]. Power method involves multiplication of a 5 matrix by a vector and after a few steps it converges to the largest eigenvalue of a method.
Reference: [13] <author> F.S. </author> <title> Macaulay. On some formula in elimination. </title> <booktitle> Proceedings of London Mathematical Society, </booktitle> <pages> pages 3-27, </pages> <month> May </month> <year> 1902. </year>
Reference-contexts: Many formulations for computing the resultant of a system of polynomial equations are known in the literature. The classical literature in algebraic geometry deals with resultant of dense polynomial systems <ref> [13, 14] </ref>. The resultant of sparse polynomial systems has received a considerable amount of attention in the recent literature and many formulations have appeared [15, 16] based on the BKK bound relating the number of solutions of a sparse system to mixed volumes of Newton polytopes. <p> The most general formulation of resultant for dense polynomial systems expresses it as a ratio of two determinants <ref> [13] </ref>. For sparse polynomials a single determinant formulation is known for multigraded systems [15] and a formulation equivalent to Macaulay's formulation has been highlighted in [16]. <p> In particular, [19] describe a general formulation of the resultant of dense polynomial system in terms of matrices and show that the solutions can be computed by reducing it to a linear eigenvalue problem. It turns out their resultant algorithm corresponds to the Macaulay's formulation <ref> [13] </ref> and [19] do not take into account that the resultant is expressed as a ratio of two determinants. As a result, their approach may not work whenever the lower determinant vanishes. <p> computation of the form z i = Aq i1 ; i = q T After a few iterations, k corresponds to the eigenvalue of maximum magnitude and q k is the corresponding eigenvector. 3.5 Sparse Matrix Computations The general formulation of resultants corresponding to Macaulay formulation results in sparse matrices <ref> [13] </ref>. In such cases we want to make use of the sparsity of the matrix in computing its eigendecomposition. The order of Macaulay matrix is a function of the number of polynomials and the degrees of the polynomial.
Reference: [14] <author> G. Salmon. </author> <title> Lessons Introductory to the Modern Higher Algebra. G.E. </title> <publisher> Stechert & Co., </publisher> <address> New York, </address> <month> 1885. </month>
Reference-contexts: Many formulations for computing the resultant of a system of polynomial equations are known in the literature. The classical literature in algebraic geometry deals with resultant of dense polynomial systems <ref> [13, 14] </ref>. The resultant of sparse polynomial systems has received a considerable amount of attention in the recent literature and many formulations have appeared [15, 16] based on the BKK bound relating the number of solutions of a sparse system to mixed volumes of Newton polytopes. <p> All these formulations of resultants express them in terms 3 of matrices and determinants. For many special cases, corresponding to n = 2; 3; 4; 5; 6, where n is the number of equations, efficient formulations of resultants expressed as the determinant of a matrix, are given in <ref> [14] </ref>. The most general formulation of resultant for dense polynomial systems expresses it as a ratio of two determinants [13]. For sparse polynomials a single determinant formulation is known for multigraded systems [15] and a formulation equivalent to Macaulay's formulation has been highlighted in [16].
Reference: [15] <author> B. Sturmfels and A. Zelevinsky. </author> <title> Multigraded resultants of sylvester type. </title> <journal> Journal of Algebra, </journal> <note> 1993. To appear. </note>
Reference-contexts: The classical literature in algebraic geometry deals with resultant of dense polynomial systems [13, 14]. The resultant of sparse polynomial systems has received a considerable amount of attention in the recent literature and many formulations have appeared <ref> [15, 16] </ref> based on the BKK bound relating the number of solutions of a sparse system to mixed volumes of Newton polytopes. All these formulations of resultants express them in terms 3 of matrices and determinants. <p> The most general formulation of resultant for dense polynomial systems expresses it as a ratio of two determinants [13]. For sparse polynomials a single determinant formulation is known for multigraded systems <ref> [15] </ref> and a formulation equivalent to Macaulay's formulation has been highlighted in [16]. The ongoing activity in sparse elimination methods is important as many polynomial systems resulting from applications in geometric constraint systems, intersection, offsets and blends are sparse [7, 17].
Reference: [16] <author> J. Canny and I. Emiris. </author> <title> An efficient algorithm for the sparse mixed resultant. </title> <booktitle> In Proceedings of AAECC, </booktitle> <year> 1993. </year>
Reference-contexts: The classical literature in algebraic geometry deals with resultant of dense polynomial systems [13, 14]. The resultant of sparse polynomial systems has received a considerable amount of attention in the recent literature and many formulations have appeared <ref> [15, 16] </ref> based on the BKK bound relating the number of solutions of a sparse system to mixed volumes of Newton polytopes. All these formulations of resultants express them in terms 3 of matrices and determinants. <p> The most general formulation of resultant for dense polynomial systems expresses it as a ratio of two determinants [13]. For sparse polynomials a single determinant formulation is known for multigraded systems [15] and a formulation equivalent to Macaulay's formulation has been highlighted in <ref> [16] </ref>. The ongoing activity in sparse elimination methods is important as many polynomial systems resulting from applications in geometric constraint systems, intersection, offsets and blends are sparse [7, 17]. Resultants have gained a lot of importance in geometric and solid modeling literature.
Reference: [17] <author> D. Manocha. </author> <title> Algebraic and Numeric Techniques for Modeling and Robotics. </title> <type> PhD thesis, </type> <institution> Computer Science Division, Department of Electrical Engineering and Computer Science, University of California, Berkeley, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: The ongoing activity in sparse elimination methods is important as many polynomial systems resulting from applications in geometric constraint systems, intersection, offsets and blends are sparse <ref> [7, 17] </ref>. Resultants have gained a lot of importance in geometric and solid modeling literature. Following Sederberg's thesis [3] on implicitization of curves and surfaces, resultants have been applied to motion planning [2] and many other geometric problems, as surveyed in [18]. <p> As a result, if the matrix D is non-singular, the resultant of the F 1 ; F 2 ; . . . ; F n+1 corresponds exactly to the determinant of M. In case, D is singular, we replace M by its largest non-singular minor as shown in <ref> [17] </ref>. <p> Let us denote that m fi 1 vector as v. That is M (ff 1 )v = 0; (3) where 0 is a m fi 1 null vector. The roots of the determinant of M (x 1 ) correspond to the eigenvalues of C highlighted in the following theorem <ref> [17] </ref>: Theorem 4.1 Given the matrix polynomial, M (x 1 ) the roots of the polynomial corre sponding to its determinant are the eigenvalues of the matrix C = B B B B 0 I m 0 . . . 0 . . . . . . 0 0 0 . <p> Many a times the leading matrix M l is singular or close to being singular (due to high condition number). Some techniques based on linear transformations are highlighted in <ref> [17] </ref>, such that the problem of finding roots of determinant of matrix polynomial can be reduced to an eigenvalue problem. However, there are cases where they do not work. For example, when the matrices have singular pencils. <p> However, there are cases where they do not work. For example, when the matrices have singular pencils. In such cases, we reduce the intersection problem to a generalized eigenvalue problem using the following theorem <ref> [17] </ref>: Theorem 4.2 Given the matrix polynomial, M (x 1 ) the roots of the polynomial corresponding to its determinant are the eigenvalues of the generalized system C 1 x 1 C 2 , where C 1 = B B B B I m 0 0 . . . 0 . <p> However, in many cases it is possible to identify higher multiplicity eigenvalues of a matrix by identifying clusters of eigenvalues and using the knowledge of the condition number of the clusters. More details of its application to finding solutions of polynomial equations are given in <ref> [17] </ref>. 10 Given a higher multiplicity eigenvalue, ff 1 , we compute its geometric multiplicity by computing the SVD of C ff 1 I. The geometric multiplicity corresponds to the number of singular values equal to zero. <p> Many other problems like ray-tracing parametric curves and surfaces, finding singular points on algebraic curves and surface can be reduced to solving two polynomial equations in two or three unknowns. This algorithm has been successfully applied to these problems <ref> [17] </ref>. 5.3 Birational Maps Birational maps play a fundamental role in algebraic geometry. They have also gained importance in solid modeling for their use in many applications. Many problems related to boundary computation are easily solved using birational maps. <p> We applied our algorithm based on resultant, matrix polynomials and eigendecompositions to this problem. More details are given in <ref> [17] </ref>. The average running time for the given pose of the end effector is 10 milliseconds (on an IBM RS/6000 workstation) and giving up to 8 or more digits of accuracy.
Reference: [18] <author> C. Bajaj, T. Garrity, and J. Warren. </author> <title> On the applications of multi-equational resultants. </title> <type> Technical Report CSD-TR-826, </type> <institution> Department of Computer Science, Purdue University, </institution> <year> 1988. </year>
Reference-contexts: Resultants have gained a lot of importance in geometric and solid modeling literature. Following Sederberg's thesis [3] on implicitization of curves and surfaces, resultants have been applied to motion planning [2] and many other geometric problems, as surveyed in <ref> [18] </ref>. Most of the earlier practical applications of resultants were limited to low degree curve and surface intersections [7]. The idea of combining resultant formulations with matrix computations has been proposed with Macaulay's formulation in [19]. <p> As a result, we see that F and F 1 define rational maps between the s; t; u and x; y; z; w space. Algorithms to compute the birational maps are known in the literature. They are based on Gr-obner bases [5] or multipolynomial resultants <ref> [18] </ref>. However, direct applications of these algorithms suffer from accuracy and efficiency problems as highlighted in [7]. We make use of the fact that multipolynomial resultants linearize a non-linear problem by eliminating some variables as shown in (1).
Reference: [19] <author> W. Auzinger and H.J. Stetter. </author> <title> An elimination algorithm for the computation of all zeros of a system of multivariate polynomial equations. </title> <booktitle> In International Series of Numerical Mathematics, </booktitle> <volume> volume 86, </volume> <pages> pages 11-30, </pages> <year> 1986. </year>
Reference-contexts: Most of the earlier practical applications of resultants were limited to low degree curve and surface intersections [7]. The idea of combining resultant formulations with matrix computations has been proposed with Macaulay's formulation in <ref> [19] </ref>. In particular, [19] describe a general formulation of the resultant of dense polynomial system in terms of matrices and show that the solutions can be computed by reducing it to a linear eigenvalue problem. It turns out their resultant algorithm corresponds to the Macaulay's formulation [13] and [19] do not <p> Most of the earlier practical applications of resultants were limited to low degree curve and surface intersections [7]. The idea of combining resultant formulations with matrix computations has been proposed with Macaulay's formulation in <ref> [19] </ref>. In particular, [19] describe a general formulation of the resultant of dense polynomial system in terms of matrices and show that the solutions can be computed by reducing it to a linear eigenvalue problem. It turns out their resultant algorithm corresponds to the Macaulay's formulation [13] and [19] do not take into account <p> formulation in <ref> [19] </ref>. In particular, [19] describe a general formulation of the resultant of dense polynomial system in terms of matrices and show that the solutions can be computed by reducing it to a linear eigenvalue problem. It turns out their resultant algorithm corresponds to the Macaulay's formulation [13] and [19] do not take into account that the resultant is expressed as a ratio of two determinants. As a result, their approach may not work whenever the lower determinant vanishes.
Reference: [20] <author> D. Manocha and J. Demmel. </author> <title> Algorithms for intersecting parametric and algebraic curves. </title> <booktitle> In Graphics Interface '92, </booktitle> <pages> pages 232-241, </pages> <year> 1992. </year> <note> Revised version to appear in ACM Transactions on Graphics. </note>
Reference-contexts: The algorithm presented in this paper is not based on any particular formulation of the resultant. Rather it uses the fact that resultants can be expressed in terms of determinants of non-linear matrix polynomials. This approach has been specialized to particular problems of curve and surface intersections. In <ref> [20] </ref> the problem of curve intersection is analyzed using resultants and algorithms are presented based on Bezout resultant of two polynomials and matrix computations. Higher order intersections corresponding to tangential intersections or singular points are considered as well.
Reference: [21] <author> D. Manocha and J.F. Canny. </author> <title> Multipolynomial resultant algorithms. </title> <journal> Journal of Symbolic Computation, </journal> <volume> 15(2) </volume> <pages> 99-122, </pages> <year> 1993. </year>
Reference: [22] <author> G.H. Golub and C.F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> John Hopkins Press, </publisher> <address> Balti-more, </address> <year> 1989. </year>
Reference-contexts: More details on its computations are given in <ref> [22] </ref>. 3.2 Singular Value Decomposition The singular value decomposition (SVD) is a powerful tool which gives us accurate information about matrix rank in the presence of round off errors. <p> As a result, the eigenvalues of a diagonal matrix, upper triangular matrix or a lower triangular matrix correspond to the elements on its diagonal. Efficient algorithms for computing eigenvalues and eigenvectors are well known, <ref> [22] </ref>, and their implementations are available as part LAPACK [12]. 3.4 Power Iterations The largest or the smallest eigenvalue of a matrix (and the corresponding eigenvector) can be computed using the Power method [22]. <p> Efficient algorithms for computing eigenvalues and eigenvectors are well known, <ref> [22] </ref>, and their implementations are available as part LAPACK [12]. 3.4 Power Iterations The largest or the smallest eigenvalue of a matrix (and the corresponding eigenvector) can be computed using the Power method [22]. Power method involves multiplication of a 5 matrix by a vector and after a few steps it converges to the largest eigenvalue of a method. <p> In many applications we are only interested in the real solutions or solutions lying in a particular domain. The QR or QZ algorithm for eigenvalue computation returns all the eigenvalues of a given matrix and it is difficult to restrict them to finding eigenvalues in a particular domain <ref> [22] </ref>. Let us assume that ff 1 is a simple eigenvalue of C. In the rest of the paper, we carry out the analysis on the eigenvalues of C and the resulting algorithm is similar for the eigenvalues of the pencil C 1 x 1 C 2 . <p> We use this information in choosing the appropriate shifts in the double shift QR algorithm for eigendecomposition, as described in <ref> [22] </ref>. This knowledge of some of the eigenvalues speeds up its convergence. The 4 nonzero eigenvalues are 0:04037383; 0:04037383; 1:35035361e 10 + 5:773741e 10 i; 1:35035361e 10 5:773741e 10 i; where i = p 1.
Reference: [23] <author> G.W. Stewart. </author> <title> Simultaneous iteration for computing invariant subspaces of non-hermitian matrices. </title> <journal> Numerische Mathematik, </journal> <volume> 25 </volume> <pages> 123-136, </pages> <year> 1976. </year>
Reference-contexts: The sparsity of the matrix increases with the degrees of the polynomials or the number of equations. Algorithms for sparse matrix computations are based on matrix vector multiplications as highlighted in the Power iterations. For our applications, we use the algorithm highlighted in <ref> [23] </ref> for computing the invariant subspaces and thereby the eigendecomposition of a sparse matrix. 3.6 Generalized Eigenvalue Problem Given n fi n matrices, A and B, the generalized eigenvalue problem corresponds to solving Ax = sBx: We represent this problem as eigenvalues of A sB. <p> In other words, the matrices M i arising from Macaulay's formulation are sparse. As a result, it is worthwhile to use eigendecomposition algorithms for sparse matrices as opposed to the QR or QZ algorithm. In particular, we have tried the algorithm presented in <ref> [23] </ref> to compute invariant subspace of a real matrix by simultaneous iterations up to a user specified tolerance. The eigenvalues of the matrix are approximated from the invariant subspace. Although this algorithm is relatively fast as compared to the QR algorithm for eigendecomposition, its accuracy is not as good.
Reference: [24] <author> B.L. Van Der Waerden. </author> <title> Modern Algebra (third edition). </title> <editor> F. </editor> <publisher> Ungar Publishing Co., </publisher> <address> New York, </address> <year> 1950. </year> <month> 22 </month>
Reference-contexts: In either case, the entries of the resulting matrices are polynomial functions of x 1 . In case, a single matrix formulation is not possible for the given system, we use the u-resultant formulation to solve the given system of equations <ref> [24] </ref>. In particular, we add a polynomial F n+1 (x 1 ; x 2 ; . . . ; x n ) = u 0 + u 1 x 1 + . . . + u n x n to the given system of equations. <p> It is known as the u-resultant of original system of polynomial equations <ref> [24] </ref>. Moreover, the u-resultant is expressed as a ratio of two determinants, Det (M)=Det (D). However the entries of D are independent of the u i 's. This is a property of Macaulay's formulation and the u-resultants [24]. <p> It is known as the u-resultant of original system of polynomial equations <ref> [24] </ref>. Moreover, the u-resultant is expressed as a ratio of two determinants, Det (M)=Det (D). However the entries of D are independent of the u i 's. This is a property of Macaulay's formulation and the u-resultants [24]. As a result, if the matrix D is non-singular, the resultant of the F 1 ; F 2 ; . . . ; F n+1 corresponds exactly to the determinant of M. In case, D is singular, we replace M by its largest non-singular minor as shown in [17]. <p> In case, D is singular, we replace M by its largest non-singular minor as shown in [17]. Given M, whose entries are polynomials in the u i 's, the u-resultant corresponding to its determinant can be factored into linear factors of the form <ref> [24] </ref>: Det (M) = i=1 7 where k is the total number of non-trivial solution and (ff i0 ; ff i1 ; ff i2 ; . . . ; ff in ) are the projective coordinates of a solution of the given system of equations.
Reference: [25] <author> J.C. Owen. </author> <title> Algebraic solution for geometry from dimensional constraints. </title> <booktitle> In Pro--ceedings of Symposium on Solid Modeling Foundations and CAD/CAM Applications, </booktitle> <pages> pages 397-407, </pages> <year> 1991. </year>
Reference-contexts: The latter problems reduce to solving algebraic equations in the most general case. Some other approaches to geometric constraint systems using algebraic formulation have been proposed in <ref> [25] </ref> and they reduce the problem to solving non linear algebraic equations as well. We have applied our equation solving algorithm to the problem of inverse kinematics of general 6R manipulators. This had been a long standing problem in robotics literature.
Reference: [26] <author> C. Wampler and A.P. Morgan. </author> <title> Solving the 6r inverse position problem using a generic-case solution methodology. </title> <journal> Mechanisms and Machine Theory, </journal> <volume> 26(1) </volume> <pages> 91-106, </pages> <year> 1991. </year> <month> 23 </month>
Reference-contexts: Known implementations of inverse kinematics in the general case are based on continuation methods and they take about 10 sec. on an average (on an IBM 370-3090 mainframe) for a given pose of the end effector <ref> [26] </ref>. We applied our algorithm based on resultant, matrix polynomials and eigendecompositions to this problem. More details are given in [17].
References-found: 26

