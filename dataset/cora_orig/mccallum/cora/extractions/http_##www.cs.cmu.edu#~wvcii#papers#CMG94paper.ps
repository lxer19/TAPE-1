URL: http://www.cs.cmu.edu/~wvcii/papers/CMG94paper.ps
Refering-URL: http://www.cs.cmu.edu/~wvcii/
Root-URL: 
Note: To appear in Proceedings of the 1994 Computer Measurement Group Conference (CMG) Page 1 of 12  
Abstract-found: 0
Intro-found: 1
Reference: [Anderson79] <author> T. Anderson and B. Randell, </author> <title> Computing Systems Reliability, </title> <publisher> Cambridge University Press, </publisher> <year> 1979. </year>
Reference: [ANSI91] <institution> Small Computer System Interface - 2 (SCSI-2), American National Standard for Information systems, X3T9.2/86-109, Revision 10h, X3T9/89-042, Global Engineering Documents, </institution> <address> X3.131-199x, Irvine CA, </address> <month> October 17, </month> <year> 1991. </year>
Reference-contexts: It is important to understand that the status of these operations during error recovery remains execution in progress. The initiation of new operations is also resumed at this time. Finally, it is important to note that some disk systems allow clients to specify the relative ordering of operations <ref> [ANSI91] </ref>. For example, some filesystems rely upon the ordering of writes to prevent filesystem corruption [Lefer90]. This ordering must be preserved throughout error recovery process. Mechanism The recovery mechanism we present here allows operations to be executed to increase performance during normal operation.
Reference: [Anderson81] <author> T. Anderson and P. A. Lee, </author> <title> Fault Tolerance, </title> <booktitle> Principles and Practice, </booktitle> <publisher> Prentice-Hall, </publisher> <year> 1981. </year>
Reference-contexts: Group Conference (CMG) Page 5 of 12 Forward Error Recovery is Inadequate The traditional approach to error recovery in disk systems, forward recovery, attempts to remove an error by applying selective corrections to the erroneous state, simultaneously moving operations forward to completion and bringing the system to a consistent state <ref> [Anderson81] </ref>. Construction of these corrective actions requires detailed foreknowledge of the errors which may occur and the damage that they cause. This requires enumeration of all erroneous states the system may reach.
Reference: [Anderson82] <author> T. Anderson and P. A. </author> <title> Lee Fault tolerance terminology proposals. </title> <booktitle> In Proceedings of the 12th Annual International Symposium on Fault Tolerant Computing (FTCS), </booktitle> <address> Santa Monica CA, </address> <month> June </month> <year> 1982, </year> <pages> pp. 29-33. </pages>
Reference-contexts: Because reliability and availability are critical characteristics of storage systems, disk arrays are usually designed to be single fault tolerant. This is accom 1. The definitions presented here are consistent with those of the IEEE Technical Committee on Fault Tolerant Computing <ref> [Melliar-Smith77, Anderson82, Lee82] </ref>. plished by storing redundant data in the disk array [Gibson89, Gibson92].
Reference: [Bhide92] <author> A. Bhide and D. Dias, </author> <title> RAID architectures for OLTP. </title> <institution> IBM Computer Science Research Report RC 17879, </institution> <year> 1992. </year>
Reference-contexts: This property limits the scope of modifications to an existing code base, thereby restricting a designers ability to explore the design space, confining experimentation to limited departures from the current code structure. Finally, researchers are investigating more aggressive redundant disk array architectures to boost performance <ref> [Bhide92, Blaum94, Cao93, Menon93, Stodol-sky93, Holland94] </ref>. The acceptance of these proposals is put at risk due to their further increases in the complexity of error handling and the difficulty of modifying existing code structure.
Reference: [Bjork75] <author> L. A. Bjork, Jr., </author> <title> Generalized audit trail requirements and To appear in Proceedings of the 1994 Computer Measurement Group Conference (CMG) Page 12 of 12 concepts for data base applications. </title> <journal> IBM Systems Journal, </journal> <volume> Vol. 14, No. 3, </volume> <year> 1975, </year> <pages> pp. 229-245. </pages>
Reference-contexts: Audit Trails Finally, audit trail, also known as logging or journaling, techniques provide the ability to record a subset of the system state but, unlike recovery cache techniques, do not require foreknowledge of the state which will be changed by an operation <ref> [Bjork75, Verhofstad78, Gray81] </ref>. Instead, all changes to the system state are recorded in stable storage. Recovery is performed by applying the inversion of these records in LIFO fashion, thereby removing state changes. As inverted records are applied, work is undone.
Reference: [Blaum94] <author> Mario Blaum, Jim Brady, Jehoshua Bruk, Jai Menon, EVENODD: </author> <title> An optimal scheme for tolerating double disk failures in RAID architectures. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture (ISCA), </booktitle> <address> Chicago IL, </address> <month> April 18-21, </month> <year> 1994, </year> <pages> pp. 245-254. </pages>
Reference-contexts: This property limits the scope of modifications to an existing code base, thereby restricting a designers ability to explore the design space, confining experimentation to limited departures from the current code structure. Finally, researchers are investigating more aggressive redundant disk array architectures to boost performance <ref> [Bhide92, Blaum94, Cao93, Menon93, Stodol-sky93, Holland94] </ref>. The acceptance of these proposals is put at risk due to their further increases in the complexity of error handling and the difficulty of modifying existing code structure.
Reference: [Cao93] <author> Pei Cao, Swee Boon Lim, Shivakumar Venkataraman, and John Wilkes, </author> <title> The TickerTAIP parallel RAID architecture. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <address> San Diego CA, </address> <month> May </month> <year> 1993, </year> <pages> pp. 52-63. </pages>
Reference-contexts: This property limits the scope of modifications to an existing code base, thereby restricting a designers ability to explore the design space, confining experimentation to limited departures from the current code structure. Finally, researchers are investigating more aggressive redundant disk array architectures to boost performance <ref> [Bhide92, Blaum94, Cao93, Menon93, Stodol-sky93, Holland94] </ref>. The acceptance of these proposals is put at risk due to their further increases in the complexity of error handling and the difficulty of modifying existing code structure.
Reference: [Chandy72] <author> K. M. Chandy and C. V. Ramamoorthy, </author> <title> Rollback and recovery strategies for computer programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. C-21, No. 6, </volume> <month> June </month> <year> 1972, </year> <pages> pp. 546-556. </pages>
Reference-contexts: We now examine the applicability of techniques from each of these classes to the domain of redundant disk arrays. Checkpointing Systems employing checkpointing establish a recovery point, known as a checkpoint, by saving a subset of the system state, known as checkpoint data <ref> [Chandy72, Siewiorek93] </ref>. Erroneous state information is removed by returning the system to a checkpoint which is assumed to be free from error. The process of returning to a checkpoint, referred to as rollback, requires the checkpoint data associated with the checkpoint to be reinstated.
Reference: [Chandy85] <author> K. Mani Chandy and Leslie Lamport, </author> <title> Distributed snapshots: determining global states of distributed systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 3, No. 1, </volume> <month> Feb. </month> <year> 1985, </year> <pages> pp. 63-75. </pages>
Reference-contexts: A more efficient alternative is to save only a subset of the system state. For instance, a technique commonly known as consistent checkpointing creates process checkpoints, which are checkpoints of the state of a process <ref> [Chandy85] </ref>. Collectively, these process checkpoints compose a checkpoint of the system. Recursive Cache One solution to the problem of large amounts of recovery data is the recursive cache, also known as a recovery cache [Horning74].
Reference: [Gibson89] <author> Garth A. Gibson, </author> <title> Performance and reliability in redundant arrays of inexpensive disks (RAID). </title> <booktitle> In Proceedings of the 1989 Computer Measurement Group conference (CMG), </booktitle> <address> Reno NV, </address> <month> December </month> <year> 1989, </year> <pages> pp. 381-391. </pages>
Reference-contexts: This is accom 1. The definitions presented here are consistent with those of the IEEE Technical Committee on Fault Tolerant Computing [Melliar-Smith77, Anderson82, Lee82]. plished by storing redundant data in the disk array <ref> [Gibson89, Gibson92] </ref>. Instead of propagating errors resulting from a disk failure to a client to handle, the redundant disk array now performs recovery from these errors, hiding their effects from clients and providing continuous service throughout the life of the fault.
Reference: [Gibson92] <author> Garth A. Gibson, </author> <title> Redundant Disk Arrays: Reliable, Parallel Secondary Storage, </title> <publisher> The MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: This is accom 1. The definitions presented here are consistent with those of the IEEE Technical Committee on Fault Tolerant Computing [Melliar-Smith77, Anderson82, Lee82]. plished by storing redundant data in the disk array <ref> [Gibson89, Gibson92] </ref>. Instead of propagating errors resulting from a disk failure to a client to handle, the redundant disk array now performs recovery from these errors, hiding their effects from clients and providing continuous service throughout the life of the fault.
Reference: [Gibson93] <author> Garth A. Gibson, David A. Patterson, </author> <title> Designing disk arrays for high data reliability. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 17, No. </volume> <pages> 1-2, </pages> <address> Jan.-Feb. </address> <year> 1993, </year> <pages> pp. 4-27. </pages>
Reference-contexts: As the number of disks in the array increases, reliability, the probability that the disk array will function correctly, and availability, the probability that the disk array is able to service requests, may decrease to unacceptable levels since data loss occurs on the first failure <ref> [Gibson93] </ref>. This problem may be further aggravated because, as Patterson, Gibson, and Katz suggest, commodity disks may be employed in order to reduce the cost of storage in the array [Patterson88]. <p> When two or more faults exist, the array may be in a failed operating state in which data is lost and service discontinued. 2. Other faults, such as loss of power, mechanical failure of cabling, can be converted into independent single faults in orthogonal redundant disk arrays <ref> [Gibson93] </ref>. four layers of abstraction found in a typical storage hierarchy and their relationship. Disk drives provide durable storage and implement read and write actions. Redundant disk array control systems increase the reliability and availability of disk storage and implement read and write operations as a composition of disk actions.
Reference: [Gray81] <author> Jim Gray, </author> <booktitle> Notes on data base operating systems. lecture notes from The Advanced Course in Operating Systems, </booktitle> <address> July 28-August 5, </address> <year> 1977, </year> <institution> Technical University, </institution> <address> Munich, Federal Republic of Germany, </address> <booktitle> published in Operating Systems: An Advanced Course, Vol. 60 of the series Lecture Notes in Computer Science, </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1981, </year> <pages> pp. 393-481. </pages>
Reference-contexts: Audit Trails Finally, audit trail, also known as logging or journaling, techniques provide the ability to record a subset of the system state but, unlike recovery cache techniques, do not require foreknowledge of the state which will be changed by an operation <ref> [Bjork75, Verhofstad78, Gray81] </ref>. Instead, all changes to the system state are recorded in stable storage. Recovery is performed by applying the inversion of these records in LIFO fashion, thereby removing state changes. As inverted records are applied, work is undone.
Reference: [Gray87] <author> Jim Gray, Paul McJones, Mike Blasgen, Bruce Lindsay, Raymond Lorie, Tom Price, Franco Putzolu, and Irving Traiger, </author> <title> The recovery manager of the System R database manager. </title> <journal> ACM Computing Surveys, </journal> <volume> Vol. 13, No. 2, </volume> <month> June </month> <year> 1981, </year> <pages> pp. 223-242. </pages>
Reference-contexts: As inverted records are applied, work is undone. Once the system is in a consistent state, some records may be applied in FIFO fashion to restore previously completed work. The System R database recovery manager implements such an approach <ref> [Gray87] </ref>. Summary Backward error recovery is well suited for systems in which error recovery is complex. Atomicity is more easily achieved and error recovery is context free. Code modification and enhancement are also simplified. Unfortunately, backward error recovery introduces overhead which degrades normal (error-free) performance.
Reference: [Holland94] <author> Mark Holland, </author> <title> On-line data reconstruction in redundant disk arrays. </title> <type> Ph.D. dissertation, </type> <institution> Carnegie Mellon University School of Computer Science technical report CMU-CS-94-164, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: This property limits the scope of modifications to an existing code base, thereby restricting a designers ability to explore the design space, confining experimentation to limited departures from the current code structure. Finally, researchers are investigating more aggressive redundant disk array architectures to boost performance <ref> [Bhide92, Blaum94, Cao93, Menon93, Stodol-sky93, Holland94] </ref>. The acceptance of these proposals is put at risk due to their further increases in the complexity of error handling and the difficulty of modifying existing code structure.
Reference: [Horning74] <author> J. J. Horning, H. C. Lauer, P. M. Melliar-Smith, B. Ran-dell, </author> <title> A program structure for error detection and recovery. </title> <booktitle> Proceedings of an International Symposium held at Rocquencourt, </booktitle> <month> April 23-25 </month> <year> 1974, </year> <booktitle> published in Lecture Notes in Computer Science, </booktitle> <volume> Vol. 16, </volume> <publisher> Springer-Verlag, </publisher> <year> 1974, </year> <pages> pp. 171-187. </pages>
Reference-contexts: Collectively, these process checkpoints compose a checkpoint of the system. Recursive Cache One solution to the problem of large amounts of recovery data is the recursive cache, also known as a recovery cache <ref> [Horning74] </ref>. By monitoring actions which modify the system state, specific state information is saved in a recursive cache, prior to modification.
Reference: [Katz89] <author> Randy H. Katz, Garth A. Gibson, David A. Patterson, </author> <title> Disk system architectures for high performance computing. </title> <booktitle> In Proceedings of the IEEE, </booktitle> <volume> Vol. 77, No. 12, </volume> <month> December </month> <year> 1989, </year> <pages> pp. 1842-1858. </pages> <note> Also published in CMG Transactions, issue 74, </note> <month> fall </month> <year> 1991, </year> <pages> pp. 27-46. </pages>
Reference-contexts: Motivation Motivations for Redundant Disk Arrays Disk arrays are a well established method of using parallelism to reduce response time in disk storage systems <ref> [Kim86, Salem86, Katz89, Reddy89] </ref>.
Reference: [Kim86] <author> Michelle Y. Kim, </author> <title> Synchronized disk interleaving. </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. 35, No. 11, </volume> <month> November </month> <year> 1986, </year> <pages> pp. 978-988. </pages>
Reference-contexts: Motivation Motivations for Redundant Disk Arrays Disk arrays are a well established method of using parallelism to reduce response time in disk storage systems <ref> [Kim86, Salem86, Katz89, Reddy89] </ref>.
Reference: [Lampson79] <author> Butler W. Lampson and Howard E. Sturgis, </author> <title> Crash recovery in a distributed data storage system. </title> <institution> Xerox Palo Alto Research Center, </institution> <address> 3333 Coyote Hill Road, Palo Alto, California 94304, </address> <month> April 27, </month> <year> 1979. </year>
Reference-contexts: Simple disk systems are not fault tolerant; a single fault can lead to data loss. The accepted failure model of such nonredundant disk systems requires only error detection, the recognition of the presence of an error <ref> [Lampson79] </ref>. This is acceptable since applications which require fault tolerance implement schemes to survive data loss at the application level of the system in the following way.
Reference: [Lee82] <author> P. A. Lee and T. Anderson, </author> <title> Fundamental concepts of fault tolerant computing: progress report. </title> <booktitle> In Proceedings of the 12th Annual International Symposium on Fault Tolerant Computing (FTCS), </booktitle> <address> Santa Monica CA, </address> <month> June </month> <year> 1982, </year> <pages> pp. 34-38. </pages>
Reference-contexts: Because reliability and availability are critical characteristics of storage systems, disk arrays are usually designed to be single fault tolerant. This is accom 1. The definitions presented here are consistent with those of the IEEE Technical Committee on Fault Tolerant Computing <ref> [Melliar-Smith77, Anderson82, Lee82] </ref>. plished by storing redundant data in the disk array [Gibson89, Gibson92].
Reference: [Lee90] <author> Edward K. Lee and Randy H. Katz, </author> <title> Performance considerations of parity placement in disk arrays. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <address> Palo Alto CA, </address> <month> April </month> <year> 1991, </year> <pages> pp. 190-199. </pages>
Reference-contexts: RAID level 5 is block interleaved to optimize throughput and uses parity-based redundancy. Both data and parity are evenly distributed throughout the array. A variety of strategies exist to evenly distribute data units and parity units; this illustration uses the left-symmetric layout <ref> [Lee90] </ref>. Parity is the bitwise exclusive-or of all data units in the parity group. The remaining RAID levels, 2 and 4, are not shown. RAID level 2 employs Hamming codes and does not rely on a disks ECC logic to report errors.
Reference: [Lefer90] <author> Samuel J. Lefer, Marshall Kirk McKusick, Michael J. Karels, John S. Quarterman, </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System, </title> <publisher> Addison-Wesley, </publisher> <address> Reading MA, </address> <year> 1990. </year>
Reference-contexts: Errors due to the existence of multiple disk failures may result in loss of data in the array. Filesystems generally do not provide additional fault tolerance, but do attempt to minimize the effects of data loss presented to an application <ref> [Lefer90] </ref>. Databases, however, usually do provide higher levels of fault tolerance; in particular, operations are generally atomic, meaning that transactions which fail leave the applicationss view of the system unchanged. <p> Operations in a filesystem are complex and are executed concurrently; however, since filesystems are not fault-tolerant, errors which result in a data loss are acceptable. For instance, when the BSD 4.3 UNIX operating system unexpectedly loses access to a disk, data may be lost <ref> [Lefer90] </ref>. A Problem Worth Solving The demand for redundant disk arrays is growing steadily. The value of RAID systems shipped to customers is expected to be $5.0 billion in 1994, reaching $13.0 billion annually by 1997. <p> The initiation of new operations is also resumed at this time. Finally, it is important to note that some disk systems allow clients to specify the relative ordering of operations [ANSI91]. For example, some filesystems rely upon the ordering of writes to prevent filesystem corruption <ref> [Lefer90] </ref>. This ordering must be preserved throughout error recovery process. Mechanism The recovery mechanism we present here allows operations to be executed to increase performance during normal operation. Performance is increased by allowing maximal concurrency of actions within an operation and not introducing overhead by saving recovery data.
Reference: [Melliar-Smith77] <author> P. M. Melliar-Smith and B. Randell, </author> <title> Software reliability: the role of programmed exception handling. </title> <booktitle> In Proceedings of an ACM Conference on Language Design for Reliable Software, </booktitle> <address> Raleigh NC, </address> <month> March </month> <year> 1977, </year> <pages> pp. 95-100. </pages>
Reference-contexts: Because reliability and availability are critical characteristics of storage systems, disk arrays are usually designed to be single fault tolerant. This is accom 1. The definitions presented here are consistent with those of the IEEE Technical Committee on Fault Tolerant Computing <ref> [Melliar-Smith77, Anderson82, Lee82] </ref>. plished by storing redundant data in the disk array [Gibson89, Gibson92].
Reference: [Menon93] <author> J. Menon, J. Roche, and J. Kasson, </author> <title> Floating parity and data disk arrays. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 17, No. </volume> <pages> 1-2, </pages> <address> Jan.-Feb. </address> <year> 1993, </year> <pages> pp. 129-139. </pages>
Reference-contexts: This property limits the scope of modifications to an existing code base, thereby restricting a designers ability to explore the design space, confining experimentation to limited departures from the current code structure. Finally, researchers are investigating more aggressive redundant disk array architectures to boost performance <ref> [Bhide92, Blaum94, Cao93, Menon93, Stodol-sky93, Holland94] </ref>. The acceptance of these proposals is put at risk due to their further increases in the complexity of error handling and the difficulty of modifying existing code structure.
Reference: [Patterson88] <author> David A. Patterson, Garth A. Gibson, and Randy H. Katz, </author> <title> A case for redundant arrays of inexpensive disks (RAID). </title> <booktitle> In Proceedings of the 1988 ACM Conference on Management of Data (SIGMOD), </booktitle> <address> Chicago IL, </address> <month> June </month> <year> 1988, </year> <pages> pp. 109-116. </pages> <note> Also published in CMG Transactions, issue 74, </note> <month> fall </month> <year> 1991, </year> <pages> pp. 13-25. </pages>
Reference-contexts: Since commodity drives all have sufficient ECC logic, the space and complexity overhead of Hamming codes is generally considered to be unwarranted. RAID level 4 is similar to RAID level 5 but does not evenly distribute the parity, creating hot spots in the array <ref> [Patterson88] </ref>. <p> This problem may be further aggravated because, as Patterson, Gibson, and Katz suggest, commodity disks may be employed in order to reduce the cost of storage in the array <ref> [Patterson88] </ref>. Because reliability and availability are critical characteristics of storage systems, disk arrays are usually designed to be single fault tolerant. This is accom 1.
Reference: [Randell75] <author> Brian Randell, </author> <title> System structure for software fault tolerance. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. SE-1, No. 2, </volume> <month> June </month> <year> 1975, </year> <pages> pp. 220-232. </pages>
Reference-contexts: To appear in Proceedings of the 1994 Computer Measurement Group Conference (CMG) Page 7 of 12 Finally, as Randell points out, backward error recovery in systems characterized by communicating processes can lead to disastrous results <ref> [Randell75] </ref>. The problem, known as the domino effect, occurs when communication has taken place between the recovery point and the point in which an error is detected. When recovery is performed, the effects of the communication are undone, requiring recovery of the other processes involved in the communication. <p> When recovery is performed, the effects of the communication are undone, requiring recovery of the other processes involved in the communication. An illustration of this problem, taken from [Stone89], is presented as synchronize communicating processes, are known methods of avoiding the domino effect <ref> [Randell75] </ref>. A variety of backward error recovery techniques exist, all of which introduce varying degrees of overhead. These techniques fall into three broad classes: check-pointing, recursive caches, and audit trails [Ander-son81]. We now examine the applicability of techniques from each of these classes to the domain of redundant disk arrays.
Reference: [Randell78] <author> B. Randell, P. A. Lee, and P. C. Treleaven, </author> <title> Reliability issues in computing system design. </title> <journal> ACM Computing Surveys, </journal> <volume> Vol. 10, No. 2, </volume> <month> June </month> <year> 1978, </year> <pages> pp. 123-165. </pages>
Reference-contexts: Forward error recovery must be designed specifically for each system. This is a result of the dependence upon knowledge of the context in which an error occurs <ref> [Randell78] </ref>. Because of this, once a design is created, it can be very difficult to make changes to the design, RAID level 5 small-write operation. In this illustration, the erroneous state characterized by the inability of a small-write operation to read old data has been reached. <p> A recovery point is established by storing recovery data, information which describes the state of the system, as a part of normal processing. When an error is detected, the system is returned to the recovery point by reinstating the recovery data <ref> [Randell78, Stone89] </ref>. Previously completed work which is undone as a result of moving backward to a recovery point must be redone. Backward error recovery does not rely upon the type of error or the errors context in removing the error from the system. Thus, context-free error recovery is possible.
Reference: [Reddy89] <author> A. L. Narasimha Reddy and Prithviraj Banerjee, </author> <title> An evaluation of multiple-disk I/O systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. 38, No. 12, </volume> <month> December </month> <year> 1989, </year> <pages> pp. 1680-1690. </pages>
Reference-contexts: Motivation Motivations for Redundant Disk Arrays Disk arrays are a well established method of using parallelism to reduce response time in disk storage systems <ref> [Kim86, Salem86, Katz89, Reddy89] </ref>.
Reference: [Salem86] <author> K. Salem and H. Garcia-Molina, </author> <title> Disk Striping. </title> <booktitle> In Proceedings of the 2nd International Conference on Data Engineering, </booktitle> <publisher> IEEE CS Press, </publisher> <address> Los Alamitos, </address> <note> CA Order No. 827 (microfiche only), </note> <year> 1986, </year> <pages> pp. 336-342. </pages>
Reference-contexts: Motivation Motivations for Redundant Disk Arrays Disk arrays are a well established method of using parallelism to reduce response time in disk storage systems <ref> [Kim86, Salem86, Katz89, Reddy89] </ref>.
Reference: [Siewiorek92] <author> Daniel P. Siewiorek and Robert S. Swarz, </author> <title> Reliable Computer Systems: Design and Evaluation, Second Edition, </title> <publisher> Digital Press, </publisher> <year> 1992 </year>
Reference: [Stone89] <author> R. F. Stone, </author> <title> Reliable computing systems - a review. </title> <institution> University of York, Department of Computer Science Technical Report YCS 110(1989), </institution> <year> 1989 </year>
Reference-contexts: A recovery point is established by storing recovery data, information which describes the state of the system, as a part of normal processing. When an error is detected, the system is returned to the recovery point by reinstating the recovery data <ref> [Randell78, Stone89] </ref>. Previously completed work which is undone as a result of moving backward to a recovery point must be redone. Backward error recovery does not rely upon the type of error or the errors context in removing the error from the system. Thus, context-free error recovery is possible. <p> When recovery is performed, the effects of the communication are undone, requiring recovery of the other processes involved in the communication. An illustration of this problem, taken from <ref> [Stone89] </ref>, is presented as synchronize communicating processes, are known methods of avoiding the domino effect [Randell75]. A variety of backward error recovery techniques exist, all of which introduce varying degrees of overhead. These techniques fall into three broad classes: check-pointing, recursive caches, and audit trails [Ander-son81].
Reference: [Stodolsky93] <author> Daniel Stodolsky, Garth Gibson, Mark Holland, </author> <title> Parity logging: overcoming the small write problem in redundant disk arrays. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <address> San Diego CA, </address> <month> May </month> <year> 1993, </year> <pages> pp 64-75. </pages>
Reference: [Verhofstad78] <author> Joost S. M. Verhofstad, </author> <title> Recovery techniques for database systems. </title> <journal> ACM Computing Surveys, </journal> <volume> Vol. 10, No. 2, </volume> <month> June </month> <year> 1978, </year> <pages> pp. 167-195. </pages>
Reference-contexts: Audit Trails Finally, audit trail, also known as logging or journaling, techniques provide the ability to record a subset of the system state but, unlike recovery cache techniques, do not require foreknowledge of the state which will be changed by an operation <ref> [Bjork75, Verhofstad78, Gray81] </ref>. Instead, all changes to the system state are recorded in stable storage. Recovery is performed by applying the inversion of these records in LIFO fashion, thereby removing state changes. As inverted records are applied, work is undone.
References-found: 34

