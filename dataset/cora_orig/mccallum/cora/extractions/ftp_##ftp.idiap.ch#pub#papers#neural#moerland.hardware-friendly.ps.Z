URL: ftp://ftp.idiap.ch/pub/papers/neural/moerland.hardware-friendly.ps.Z
Refering-URL: http://www.idiap.ch/~perry/allpubs.html
Root-URL: http://www.idiap.ch/~perry/allpubs.html
Email: E-mail: Perry.Moerland@idiap.ch  
Title: Hardware-Friendly Learning Algorithms for Neural Networks: an Overview  
Author: P. D. Moerland and E. Fiesler 
Date: February 12-14, 1996.  
Address: Switzerland,  CP 592, CH-1920 Martigny, Switzerland  
Affiliation: Lausanne,  IDIAP  
Note: Published in the Proceedings of the Fifth International Conference on Microelectronics for Neural Networks and Fuzzy Systems: MicroNeuro'96  1: Introduction  
Abstract: The hardware implementation of artificial neural networks and their learning algorithms is a fascinating area of research with far-reaching applications. However, the mapping from an ideal mathematical model to compact and reliable hardware is far from evident. This paper presents an overview of various methods that simplify the hardware implementation of neural network models. Adaptations that are proper to specific learning rules or network architectures are discussed. These range from the use of perturbation in multilayer feedforward networks and local learning algorithms to quantization effects in self-organizing feature maps. Moreover, in more general terms, the problems of inaccuracy, limited precision, and robustness are treated. During the last decade it has been demonstrated that neural networks are capable of providing solutions to many problems in the areas of pattern recognition, signal processing, time series analysis, and many others. While software simulations are very useful for investigating the capabilities of neural network models, they cannot fulfill the need for real-time processing that is necessary for a successful application of neural networks to most real-world problems. To fully profit from the inherent parallelism of neural networks, hardware implementations are essential. However, the mapping of existing neural network algorithms or their resultant networks onto fast, compact, and reliable hardware is a difficult task. Therefore, learning rules which are better suited for hardware implementation have been proposed. These hardware-friendly learning algorithms can be divided into two subclasses, namely: 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Alspector, A. Jayakumar, and S. Luma. </author> <title> Experimental Evaluation of Learning in a Neural Mi-crosystem. </title> <booktitle> Advances in Neural Information Processing Systems (NIPS92), </booktitle> <volume> vol. 5, </volume> <pages> pp. 871-878, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1993. </year>
Reference-contexts: In Bellcore's implementation of a Boltzmann machine this annealing schedule is replaced by a gradual decrease of additive noise, which can be efficiently implemented in analog hardware <ref> [1] </ref>. Mean field theory This method is based on the idea that the simulated annealing process in the stochastic Boltzmann machine is too time-consuming and can be replaced by a deterministic mean field approximation. <p> However, most of the on-chip learning rules described in this paper have not been realized in hardware and their efficacy is difficult to judge. Some notable exceptions are Bellcore's implementation of a Boltzmann machine and the mean field theory algorithm <ref> [1] </ref>, and Bat-titi's TOTEM-chip based on the reactive tabu search [4]. Attention has also been given to learning algorithms that are not suited for hardware implementation themselves, but the resulting network of which can be efficiently implemented.
Reference: [2] <author> J. Alspector, R. Meir, B. Yuhas, and A. </author> <month> Jayakumar. </month>
Reference-contexts: This scheme has been actually implemented in hardware and shows good behaviour on some small benchmarks. The loss of parallelism in the weight perturbation scheme can also be overcome by perturbing all weights simultaneously and using the resulting error to update the weights <ref> [2] </ref>. For a reliable estimate of a gradient, multiple perturbations should be performed, but this number is normally quite small compared to the total number of weights in the network. A similar approach has in fact been pursued by Cauwenberghs [8] studying in some more detail its convergence properties.
References-found: 2

