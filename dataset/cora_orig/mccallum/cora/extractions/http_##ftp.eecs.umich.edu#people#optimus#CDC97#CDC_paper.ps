URL: http://ftp.eecs.umich.edu/people/optimus/CDC97/CDC_paper.ps
Refering-URL: http://ftp.eecs.umich.edu/people/optimus/CDC97/
Root-URL: http://www.eecs.umich.edu
Title: Computational Experiments in Robust Stability Analysis  
Author: Albert Yoon and Pramod Khargonekar 
Date: March 8, 1997  
Address: Ann Arbor, MI 48109-2122, USA  
Affiliation: Dept. of Electrical Engineering and Computer Science The University of Michigan  
Abstract: In this paper, we take a "computational experiments" approach to robust stability analysis problems. Many robust control problems have been shown to be NP hard but in spite of this, it is important to develop effective techniques for solving them. A typical robust stability analysis problem is taken and formulated as an optimization problem to which several optimization algorithms are applied. Preliminary results have shown the Shu*ed Complex Evolution method of [5] to be quite promising for robust stability analysis problems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. R. Barmish and C. M. Lagoa. </author> <title> The uniform distribution: a rigorous justification for its use in robustness analysis. </title> <booktitle> In Proceedings of the 35th IEEE International Conference on Decision and Control, </booktitle> <pages> pages 3418-3423, </pages> <address> Kobe, Japan, </address> <year> 1996. </year>
Reference-contexts: Army Research Office under Grant Nos. DAAH 04-93-G-0012 and DAAH 04-95-1-0263 y Tel. (313) 763-6724, Email: optimus@ eecs.umich.edu z Tel. (313) 764-4328, Email: pramod@ eecs.umich.edu 1 There are some results in the literature <ref> [14, 7, 1, 17] </ref>, which indicate that randomized algorithms may provide tractable approaches to the problems of robust stability analysis and performance. Genetic algorithms for robust stability analysis have been investigated in [8]. In [7], it was shown that even simple random search has potentially attractive computational complexity properties.
Reference: [2] <author> M. Bazaraa, H. Sherali, and C. M. Shetty. </author> <title> Nonlinear Programming Theory and Algorithms. </title> <publisher> Wiley & Sons, Inc., </publisher> <address> New York, </address> <note> second edition, </note> <year> 1993. </year>
Reference-contexts: Since robust control problems often lead to nondifferentiable optimization problems, direct methods may have broader applicability. The following methods are investigated here: 1. Crude and adaptive random search [7] 2. Shu*ed complex evolution method [5] 3. Rosenbrock's method <ref> [2] </ref> 4. Zangwill's method [20] These methods are described in some detail in Section 3 where we also discuss the reasons for choosing to focus on these algorithms . <p> The second approach has been termed the Shu*ed Complex Evolution (SCE) approach [5] by its creators and is a mixture of deterministic and random samplings of . Purely deterministic line search methods are also examined. Two algorithms in this class are presented, one by Rosenbrock <ref> [2] </ref> and another by Zangwill [20]. <p> With the lack of theoretical analysis for the Nelder-Mead algorithm, it is even more doubtful that any has been done for the SCE algorithm which incorporates the Nelder-Mead algorithm as a subroutine. 3.4 Line Searches There are many line search methods for solving optimization problems. The algorithms of Rosen-brock <ref> [2] </ref> and Zangwill [20] are presented here as examples. They are purely deterministic but differ from a deterministic form of SCE as they move only a single point while the simplex algorithm attempts to move a simplex of points. <p> Let S j = S j for each j, let y 1 = x k+1 , let k = k + 1, let j = 1, and go to step 1. Rosenbrock's algorithm is attractive because, unlike say Hooke and Jeeve's algorithm <ref> [2] </ref>, it changes its set of line search directions to suit the cost function. Numerical experiments have shown this algorithm to be more efficient than those that do not adapt its line search directions.
Reference: [3] <author> R. Braatz, P. Young, J. Doyle, and M. Morari. </author> <title> Computational complexity of calculation. </title> <booktitle> In Proceedings of the American Control Conference, </booktitle> <pages> pages 1682-1683, </pages> <address> San Francisco, California, </address> <year> 1993. </year>
Reference-contexts: This appears to be even more so for the case of real parameter uncertainty. Results on computational complexity <ref> [3, 10] </ref> of robust stability analysis provide strong support for such conclusions. The fundamental underlying reason appears to be the fact that these problems typically involve nonconvex optimization having lots of local minima. It is only in very special cases that one has neat analytical solutions. <p> Experience has shown that these bounds can be arbitrarily conservative and recently, it has been proven that solving these problems is NP hard <ref> [3] </ref>. Therefore a reasonable course of action is to move our focus towards efficient means of coming to an approximate solution and reduce the conservatism of the bounds.
Reference: [4] <author> R. R. E. de Gaston and M. G. Safonov. </author> <title> Exact calculation of the multiloop stability margin. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 33 </volume> <pages> 156-171, </pages> <year> 1988. </year>
Reference-contexts: If we define (M; ) := max (Re ((A + BC))) (2.2) then fl opt = inf (kk 1 : (M; ) is unstable) = inf (kk 1 : (M; ) 0): Solving this problem would typically be handled by the real structured singular value (real ) theory <ref> [4, 19] </ref>. However, real analysis and synthesis problems are very difficult and one usually resorts to computing upper and lower bounds on the quantities of interest such as fl opt .
Reference: [5] <author> Q. Duan, S. Sorooshian, and V. Gupta. </author> <title> Effective and efficient global optimization for conceptual rainfall-runoff models. </title> <journal> Water Resources Research, </journal> <volume> 28(4) </volume> <pages> 1015-1031, </pages> <year> 1992. </year>
Reference-contexts: Since robust control problems often lead to nondifferentiable optimization problems, direct methods may have broader applicability. The following methods are investigated here: 1. Crude and adaptive random search [7] 2. Shu*ed complex evolution method <ref> [5] </ref> 3. Rosenbrock's method [2] 4. Zangwill's method [20] These methods are described in some detail in Section 3 where we also discuss the reasons for choosing to focus on these algorithms . <p> The first algorithm is the pure random search which samples the space of all possible perturbations and records the best current estimated value of fl opt . The second approach has been termed the Shu*ed Complex Evolution (SCE) approach <ref> [5] </ref> by its creators and is a mixture of deterministic and random samplings of . Purely deterministic line search methods are also examined. Two algorithms in this class are presented, one by Rosenbrock [2] and another by Zangwill [20]. <p> Step 3: If i &lt; iterations then let i = i + 1 and go to step 1. Step 4: End and return fl. 3.3 Shu*ed Complex Evolution The so-called Shu*ed Complex Evolution (SCE) approach of <ref> [5] </ref> combines both random and deterministic rules for sampling the input space. This approach was proposed as a robust, intelligent, and efficient means for finding global optima. <p> A simplex step is interpreted as an evolutionary step of a population while shu*ing is interpreted as communication and co-operation between populations. The procedure can be found in <ref> [5] </ref> and is outlined below. * Shu*ing Inputs: s (such that s n + 1 where n is the dimension of the input space), p, * Initialization: Let i=1 and evaluate the cost function at sp points where s is the number of samples and p is the number of complexes <p> In this way, all points in the complex have a chance to participate in evolution, but it is the better points that will participate more often. Yet another place randomness can enter is when an infeasible point is picked due to reflection or contraction. In <ref> [5] </ref> it is suggested that a randomly chosen point replace the infeasible one. There are two termination criteria for this algorithm. Firstly, if all of the points lie in a hypercube with sides of length * then the algorithm terminates.
Reference: [6] <author> Q. Duan, S. Sorooshian, and V. Gupta. </author> <title> Optimal use of the SCE-UA global optimization method for calibrating watershed models. </title> <journal> Journal of Hydrology, </journal> <volume> 158 </volume> <pages> 265-284, </pages> <year> 1994. </year>
Reference-contexts: A higher fi may also increase robustness at the expense of decreased efficiency while a higher ff may decrease robustness since more worst-point replacements occur before a new simplex is chosen from the complex. The relationships between these parameters and robustness and efficiency are examined in <ref> [6] </ref> but are based on numerical experiments. The theoretical properties of this algorithm are not well known. Indeed, according to Subrah-manyam in [16], convergence theorems for the Nelder-Mead algorithm are practically nonexistent.
Reference: [7] <author> P. Khargonekar and A. Tikku. </author> <title> Randomized algorithms for robust control analysis and synthesis have polynomial complexity. </title> <booktitle> In Proceedings of the 35th IEEE International Conference on Decision and Control, </booktitle> <pages> pages 3470-3475, </pages> <address> Kobe, Japan, </address> <year> 1996. </year>
Reference-contexts: Army Research Office under Grant Nos. DAAH 04-93-G-0012 and DAAH 04-95-1-0263 y Tel. (313) 763-6724, Email: optimus@ eecs.umich.edu z Tel. (313) 764-4328, Email: pramod@ eecs.umich.edu 1 There are some results in the literature <ref> [14, 7, 1, 17] </ref>, which indicate that randomized algorithms may provide tractable approaches to the problems of robust stability analysis and performance. Genetic algorithms for robust stability analysis have been investigated in [8]. In [7], it was shown that even simple random search has potentially attractive computational complexity properties. <p> Genetic algorithms for robust stability analysis have been investigated in [8]. In <ref> [7] </ref>, it was shown that even simple random search has potentially attractive computational complexity properties. <p> Since robust control problems often lead to nondifferentiable optimization problems, direct methods may have broader applicability. The following methods are investigated here: 1. Crude and adaptive random search <ref> [7] </ref> 2. Shu*ed complex evolution method [5] 3. Rosenbrock's method [2] 4. Zangwill's method [20] These methods are described in some detail in Section 3 where we also discuss the reasons for choosing to focus on these algorithms . <p> Rosenbrock's method [2] 4. Zangwill's method [20] These methods are described in some detail in Section 3 where we also discuss the reasons for choosing to focus on these algorithms . We take the 55 state, 20 uncertain real parameters mul-tivariable robust stability analysis problem investigated in <ref> [7] </ref> as the benchmark example for this empirical investigation. (We expect to include results on even larger problems in the final version of this conference paper. ) To measure the computational cost, we use the number of function evaluations as a metric. <p> The price that is paid for this robustness is poor efficiency. Many more function evaluations are likely necessary to arrive at a solution of the same quality as one that is found by a more directed technique. Two random search methods, taken from <ref> [7] </ref> are used in this study. The first algorithm simply samples from the region of all perturbations with size less than or equal to unity a designated number of times and records the sample that provides the lowest cost. <p> The example problem chosen in this section is to find r max , the largest real number r such that all polynomials in a family of interval polynomials P r are stable. It is taken directly from <ref> [7] </ref> which should be referred to for further details. Using the standard analytic results, we get r max = 0:145042. <p> However this is not an issue since convexity properties are lost in the general case anyway. 4.2 Large State Space Example The LTI system, M , obtained from <ref> [7] </ref> is stable and contains 55 states and 20 inputs and outputs, i.e., the number of real parameters is 20. For the state space matrices (A; B; C) of M , the problem is to find the smallest 2 which destabilizes A + BC.
Reference: [8] <author> C. Marrison and R. Stengel. </author> <title> The use of random search and genetic algorithms to optimize stochastic robustness functions. </title> <booktitle> In Proceedings of American Control Conference, </booktitle> <pages> pages 1484-1489, </pages> <address> Baltimore, Maryland, </address> <year> 1994. </year>
Reference-contexts: Genetic algorithms for robust stability analysis have been investigated in <ref> [8] </ref>. In [7], it was shown that even simple random search has potentially attractive computational complexity properties.
Reference: [9] <author> J. A. Nelder and R. Mead. </author> <title> A simplex method for function minimization. </title> <journal> Computer Journal, </journal> <volume> 7 </volume> <pages> 308-313, </pages> <year> 1965. </year>
Reference-contexts: This approach was proposed as a robust, intelligent, and efficient means for finding global optima. The minimum-seeking part of the routine is a variation of the Nelder-Mead <ref> [9] </ref> simplex algorithm which, due to its deterministic rules, more efficiently samples the input space than a purely random search. However, the SCE algorithm incorporates some randomness in order to maintain a measure of the robustness that the random searches have.
Reference: [10] <author> A. Nemirovskii. </author> <title> Several NP-hard problems arising in robust stability analysis. </title> <journal> Math. of Control, Signals, and Systems, </journal> <volume> 6 </volume> <pages> 99-105, </pages> <year> 1993. </year>
Reference-contexts: This appears to be even more so for the case of real parameter uncertainty. Results on computational complexity <ref> [3, 10] </ref> of robust stability analysis provide strong support for such conclusions. The fundamental underlying reason appears to be the fact that these problems typically involve nonconvex optimization having lots of local minima. It is only in very special cases that one has neat analytical solutions.
Reference: [11] <author> J. M. Parkinson and D. Hutchinson. </author> <title> A consideration of nongradient algorithms for the unconstrained optimization of function of high dimensionality. </title> <editor> In F. A. Lootsma, editor, </editor> <booktitle> Numerical Methods for Nonlinear Optimization, </booktitle> <pages> pages 99-113. </pages> <publisher> Academic Press, </publisher> <year> 1972. </year>
Reference-contexts: The theoretical properties of this algorithm are not well known. Indeed, according to Subrah-manyam in [16], convergence theorems for the Nelder-Mead algorithm are practically nonexistent. Studies that examine the properties of the Nelder-Mead algorithm are composed of numerical experiments, comparisons with other direct methods <ref> [11] </ref>, and proposed modifications to the algorithm to improve its efficiency [12]. Interestingly enough, several studies have provided different conclusions. <p> Define k+1 r = k to step 1. Zangwill's algorithm is a simple modification to Powell's method in [13]. Powell's algorithm is cited frequently in literature concerning direct methods and used for comparison with other proposed 8 methods. For example, <ref> [11] </ref> provides numerical results comparing Powell's method with the Nelder--Mead simplex method. However, Powell's algorithm contains an error which Zangwill documents and removes in [20]. It corrects the problem that Powell's method had of possibly producing a set of n line search directions that are linearly dependent.
Reference: [12] <author> J. M. Parkinson and D. Hutchinson. </author> <title> An investigation into the efficiency of variants of the simplex method. </title> <editor> In F. A. Lootsma, editor, </editor> <booktitle> Numerical Methods for Nonlinear Optimization, </booktitle> <pages> pages 115-136. </pages> <publisher> Academic Press, </publisher> <year> 1972. </year>
Reference-contexts: Indeed, according to Subrah-manyam in [16], convergence theorems for the Nelder-Mead algorithm are practically nonexistent. Studies that examine the properties of the Nelder-Mead algorithm are composed of numerical experiments, comparisons with other direct methods [11], and proposed modifications to the algorithm to improve its efficiency <ref> [12] </ref>. Interestingly enough, several studies have provided different conclusions. Some claim that the simplex method is inefficient and suffer when the dimension of the input space is high while others claim that it is competitive with other methods such as that of Powell [13] especially in higher dimensions.
Reference: [13] <author> M. J. D. Powell. </author> <title> An efficient method for finding the minimum of a function of several variables without calculating derivatives. </title> <journal> Computer Journal, </journal> <volume> 7 </volume> <pages> 155-162, </pages> <year> 1964. </year>
Reference-contexts: Interestingly enough, several studies have provided different conclusions. Some claim that the simplex method is inefficient and suffer when the dimension of the input space is high while others claim that it is competitive with other methods such as that of Powell <ref> [13] </ref> especially in higher dimensions. Schwefel in [15] has performed a comprehensive numerical study involving many algorithms on a large number of different problems. His results tend to show that the simplex algorithm is very inefficient, especially as it comes close to a solution. <p> Let k n x k1 n x k1 n+1 k. Determine k n+1 to minimize J (x k n + k n+1 ) n+1 = x k n+1 k n+1 . Define k+1 r = k to step 1. Zangwill's algorithm is a simple modification to Powell's method in <ref> [13] </ref>. Powell's algorithm is cited frequently in literature concerning direct methods and used for comparison with other proposed 8 methods. For example, [11] provides numerical results comparing Powell's method with the Nelder--Mead simplex method. However, Powell's algorithm contains an error which Zangwill documents and removes in [20].
Reference: [14] <author> L. Ray and R. Stengel. </author> <title> Stochastic robustness of linear time-invariant control systems. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 36 </volume> <pages> 82-87, </pages> <year> 1991. </year> <month> 17 </month>
Reference-contexts: Army Research Office under Grant Nos. DAAH 04-93-G-0012 and DAAH 04-95-1-0263 y Tel. (313) 763-6724, Email: optimus@ eecs.umich.edu z Tel. (313) 764-4328, Email: pramod@ eecs.umich.edu 1 There are some results in the literature <ref> [14, 7, 1, 17] </ref>, which indicate that randomized algorithms may provide tractable approaches to the problems of robust stability analysis and performance. Genetic algorithms for robust stability analysis have been investigated in [8]. In [7], it was shown that even simple random search has potentially attractive computational complexity properties.
Reference: [15] <author> H. Schwefel. </author> <title> Evolution and Optimum Seeking. </title> <publisher> Wiley & Sons, Inc., </publisher> <address> New York, </address> <year> 1995. </year>
Reference-contexts: The shu*ed complex evolution seems to be clearly the best among these algorithms. Interestingly, simple random search methods perform better than the Rosenbrock and Zangwill algorithms. Quite surprisingly, the shu*ed complex evolution approach does not involve many more function calls, contrary to what 2 one might expect based on <ref> [15] </ref>. Also, the randomization aspect of the shu*ed complex evolution seems to play a significant role in improving its efficiency. Moreover, the typical computation time compares well with that required for computing upper and lower bounds for the real . <p> Interestingly enough, several studies have provided different conclusions. Some claim that the simplex method is inefficient and suffer when the dimension of the input space is high while others claim that it is competitive with other methods such as that of Powell [13] especially in higher dimensions. Schwefel in <ref> [15] </ref> has performed a comprehensive numerical study involving many algorithms on a large number of different problems. His results tend to show that the simplex algorithm is very inefficient, especially as it comes close to a solution.
Reference: [16] <author> M. B. Subrahmanyam. </author> <title> An extension of the simplex method to constrained nonlinear optimization. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 62(2) </volume> <pages> 311-319, </pages> <year> 1989. </year>
Reference-contexts: The relationships between these parameters and robustness and efficiency are examined in [6] but are based on numerical experiments. The theoretical properties of this algorithm are not well known. Indeed, according to Subrah-manyam in <ref> [16] </ref>, convergence theorems for the Nelder-Mead algorithm are practically nonexistent. Studies that examine the properties of the Nelder-Mead algorithm are composed of numerical experiments, comparisons with other direct methods [11], and proposed modifications to the algorithm to improve its efficiency [12]. Interestingly enough, several studies have provided different conclusions.
Reference: [17] <author> R. Tempo, E. W. Bai, and F. Dabbene. </author> <title> Probabilistic robustness analysis: explicit bounds for the minimum number of samples. </title> <booktitle> In Proceedings of the 35th IEEE International Conference on Decision and Control, </booktitle> <pages> pages 3424-3428, </pages> <address> Kobe, Japan, </address> <year> 1996. </year>
Reference-contexts: Army Research Office under Grant Nos. DAAH 04-93-G-0012 and DAAH 04-95-1-0263 y Tel. (313) 763-6724, Email: optimus@ eecs.umich.edu z Tel. (313) 764-4328, Email: pramod@ eecs.umich.edu 1 There are some results in the literature <ref> [14, 7, 1, 17] </ref>, which indicate that randomized algorithms may provide tractable approaches to the problems of robust stability analysis and performance. Genetic algorithms for robust stability analysis have been investigated in [8]. In [7], it was shown that even simple random search has potentially attractive computational complexity properties.
Reference: [18] <author> A. Yoon, P. Khargonekar, and K. Hebbale. </author> <title> Design of computer experiments for open-loop control and robustness analysis of clutch-to-clutch shifts in automatic transmissions. </title> <booktitle> In Proceedings of the American Control Conference, </booktitle> <address> Albuquerque, New Mexico, </address> <year> 1997. </year> <note> To Appear. </note>
Reference-contexts: Direct methods are chosen since gradient information is computationally expensive to approximate. Moreover, we wish to try to solve several problems in the future which are too complex to hope to have gradient information available. For example, in <ref> [18] </ref> the cost function is undifferentiable and each evaluation involves the solution of a system of nonlinear differential equations with table look-ups, which is done using simulation tools.
Reference: [19] <author> P. M. Young, M. P. Newlin, and J. C. Doyle. </author> <title> Let's get real. </title> <journal> ASME, Dynamic Systems and Control Division (Publication) DSC, </journal> <volume> 43 </volume> <pages> 5-12, </pages> <year> 1992. </year>
Reference-contexts: If we define (M; ) := max (Re ((A + BC))) (2.2) then fl opt = inf (kk 1 : (M; ) is unstable) = inf (kk 1 : (M; ) 0): Solving this problem would typically be handled by the real structured singular value (real ) theory <ref> [4, 19] </ref>. However, real analysis and synthesis problems are very difficult and one usually resorts to computing upper and lower bounds on the quantities of interest such as fl opt .
Reference: [20] <author> W. Zangwill. </author> <title> Minimizing a function without calculating derivatives. </title> <journal> Computer Journal, </journal> <volume> 10 </volume> <pages> 293-296, </pages> <year> 1967. </year>
Reference-contexts: Since robust control problems often lead to nondifferentiable optimization problems, direct methods may have broader applicability. The following methods are investigated here: 1. Crude and adaptive random search [7] 2. Shu*ed complex evolution method [5] 3. Rosenbrock's method [2] 4. Zangwill's method <ref> [20] </ref> These methods are described in some detail in Section 3 where we also discuss the reasons for choosing to focus on these algorithms . <p> Purely deterministic line search methods are also examined. Two algorithms in this class are presented, one by Rosenbrock [2] and another by Zangwill <ref> [20] </ref>. <p> The algorithms of Rosen-brock [2] and Zangwill <ref> [20] </ref> are presented here as examples. They are purely deterministic but differ from a deterministic form of SCE as they move only a single point while the simplex algorithm attempts to move a simplex of points. <p> Powell's algorithm is cited frequently in literature concerning direct methods and used for comparison with other proposed 8 methods. For example, [11] provides numerical results comparing Powell's method with the Nelder--Mead simplex method. However, Powell's algorithm contains an error which Zangwill documents and removes in <ref> [20] </ref>. It corrects the problem that Powell's method had of possibly producing a set of n line search directions that are linearly dependent. Zangwill's algorithm will converge to the optimum of a quadratic function in a finite number of function evaluations.
References-found: 20

