URL: http://www.research.microsoft.com/~ymwang/papers/PRFTS-1995-W.ps
Refering-URL: http://www.research.microsoft.com/~ymwang/papers/PRFTS95CR.htm
Root-URL: http://www.research.microsoft.com
Title: When Piecewise Determinism Is Almost True  
Author: Edith Cohen Yi-Min Wang Gaurav Suri 
Abstract: Most existing log-based recovery techniques assume perfect piecewise determinism. In practice, however, the behavior of certain events is determined by the execution environment, and is not replayble. In this paper, we introduce a probabilistic piecewise deterministic model, and develop a probability tracking mechanism to achieve high-confidence output commits. The paper also addresses the issue of scalability by proposing an efficient on-line algorithm that accurately estimates the probability of determinism by piggybacking only constant-size vectors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. P. Sistla and J. L. Welch, </author> <title> "Efficient distributed recovery using message logging," </title> <booktitle> in Proc. 8th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 223-238, </pages> <month> Aug. </month> <year> 1989. </year>
Reference-contexts: When a server process fails, it is desirable that the failed process can recover locally without affecting the clients. One common way to achieve that is to use log-based recovery techniques <ref> [1, 2] </ref> which employ event logging in addition to checkpointing so that a failed process can recover more pre-failure states by replaying event logs beyond the latest checkpoint. Most existing log-based recovery techniques assume perfect piecewise determinism; i.e., every nondeterministic event can be replayed. <p> Every nondeterministic event is temporarily logged in a volatile buffer. The volatile logs of a number of events are periodically flushed to stable storage. Each process P i maintains a size-N transitive dependency vector D i <ref> [1] </ref>, where D i [j] records the highest index of any state interval of P j on which P i 's current state interval (with index D i [i]) transitively depends. <p> To address the scalability issue for dependency tracking, Lowry et al. [7] introduced the concept of recovery unit gateways to compress the vector at the cost of introducing false dependencies. Direct depen-dency tracking <ref> [1, 3] </ref> can be used to piggyback only the sender's current state interval index. When transitive dependencies are needed for an output commit, the direct dependencies must be "assembled" either at a central server or through several rounds of message exchange. <p> If the ending event is not a checkpoint, then a size-n vector A is drawn based on the weight w as follows: each entry of A is sampled independently from the probability distribution with a density function w exp (wt) (an alternative is to sample y uniformly from <ref> [0; 1] </ref> and use the value ln (y)=w); the V i vector is updated to the coordinate-wise minimum of V i and A.
Reference: [2] <author> E. N. Elnozahy and W. Zwaenepoel, "Manetho: </author> <title> Transparent rollback-recovery with low overhead, limited rollback and fast output commit," </title> <journal> IEEE Trans. Comput., </journal> <volume> Vol. 41, No. 5, </volume> <pages> pp. 526-531, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: When a server process fails, it is desirable that the failed process can recover locally without affecting the clients. One common way to achieve that is to use log-based recovery techniques <ref> [1, 2] </ref> which employ event logging in addition to checkpointing so that a failed process can recover more pre-failure states by replaying event logs beyond the latest checkpoint. Most existing log-based recovery techniques assume perfect piecewise determinism; i.e., every nondeterministic event can be replayed.
Reference: [3] <author> D. B. Johnson, </author> <title> "Efficient transparent optimistic rollback recovery for distributed application programs," </title> <booktitle> in Proc. IEEE Symp. Reliable Distributed Syst., </booktitle> <pages> pp. 86-95, </pages> <month> Oct. </month> <year> 1993. </year>
Reference-contexts: In the protocol, each output commit point is assigned an undo cost based on the impact of its rollback on service quality. A cumulative expected undo cost is calculated based on all the output commit points since the previous coordinated checkpoint. Output-driven logging <ref> [3] </ref>, which usually has a lower overhead, is performed if the cost is below a global threshold; otherwise, a higher overhead coordinated checkpointing session is initiated. Finally, we address the scalability issue for the transitive probability vectors. <p> One way to allow a mix is to ask a process to turn off event logging/replay whenever a nonreplayable nondeterministic event is encountered until the next checkpoint is taken <ref> [3] </ref>. However, it may be too drastic an approach if the number of nondeterministic events is large but most of them are likely to behave the same way before and after a failure. <p> To address the scalability issue for dependency tracking, Lowry et al. [7] introduced the concept of recovery unit gateways to compress the vector at the cost of introducing false dependencies. Direct depen-dency tracking <ref> [1, 3] </ref> can be used to piggyback only the sender's current state interval index. When transitive dependencies are needed for an output commit, the direct dependencies must be "assembled" either at a central server or through several rounds of message exchange.
Reference: [4] <author> Y. Huang and C. Kintala, </author> <title> "Software implemented fault tolerance: Technologies and experience," </title> <booktitle> in Proc. IEEE Fault-Tolerant Computing Symp., </booktitle> <pages> pp. 2-9, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Upon a failure, some of the volatile event logs may be lost. Server CS first migrates the failed processes, then collects the information about the available mes 1 The central server has a backup and is supported by a fail-over mechanism based on heartbeats and checkpointing <ref> [4] </ref> to avoid single point of failure. sage logs and their dependency vectors. The maximum recoverable state which minimizes the amount of total rollback is calculated and broadcast.
Reference: [5] <author> K. M. Chandy and L. Lamport, </author> <title> "Distributed snapshots: Determining global states of distributed systems," </title> <journal> ACM Trans. Comput. Syst., </journal> <volume> Vol. 3, No. 1, </volume> <pages> pp. 63-75, </pages> <month> Feb. </month> <year> 1985. </year>
Reference-contexts: During failure-free execution, server CS periodically broadcasts a marker to initiate checkpoint coordination <ref> [5] </ref>. Every nondeterministic event is temporarily logged in a volatile buffer. The volatile logs of a number of events are periodically flushed to stable storage.
Reference: [6] <author> J. Gray and A. Reuter, </author> <title> Transaction Processing: Concepts and Techniques. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1993. </year>
Reference-contexts: In this paper, we distinguish between two types of nonreplayable nondeterministic events: Heisenevents (whose behavior is likely to change across retries) <ref> [6] </ref> and Bohrevents. The classification is based on the desire to have every nondeterministic event behave the same way before and after the failure in order to make more state intervals recoverable.
Reference: [7] <author> A. Lowry, J. R. Russell, and A. P. Goldberg, </author> <title> "Optimistic failure recovery for very large networks," </title> <booktitle> in Proc. IEEE Symp. Reliable Distributed Syst., </booktitle> <pages> pp. 66-75, </pages> <year> 1991. </year>
Reference-contexts: To address the scalability issue for dependency tracking, Lowry et al. <ref> [7] </ref> introduced the concept of recovery unit gateways to compress the vector at the cost of introducing false dependencies. Direct depen-dency tracking [1, 3] can be used to piggyback only the sender's current state interval index.
Reference: [8] <author> E. Cohen, </author> <title> "Estimating the size of the transitive closure in linear time," </title> <booktitle> in Proc. 35th IEEE Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 190-200, </pages> <year> 1994. </year>
Reference-contexts: develop an algorithm that requires piggybacking only a constant-size vector on each message to allow high-confidence, accurate on-line estimates of the probability of determinism. 5.1 The on-line estimation framework We first describe an algorithm for estimating the sum of risk factors, based on the size-estimation framework previously developed by Cohen <ref> [8] </ref>, and then extend it to handle the on-line estimation of the probability of determinism. Given a checkpoint and communication pattern, we construct a weighted directed graph as follows. Divide a process execution at checkpoint events, output commit events, message sending and receiving events. <p> The accuracy increases for larger vector sizes. The asymptotic convergence follows using Chernoff bounds and an analysis similar to the one provided in <ref> [8] </ref>. Prob fj1 yj *g = exp ((* 2 n)). By integrating we obtain the following: COROLLARY 1 The estimate is unbiased. The expected relative error and the variance are as follows.
References-found: 8

