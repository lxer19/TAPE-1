URL: ftp://ftp.eecs.umich.edu/people/dundas/publications/ThesisProposal.ps
Refering-URL: http://www.eecs.umich.edu/~dundas/research/publications.html
Root-URL: http://www.eecs.umich.edu
Title: Improving Processor Performance by Dynamically Pre-Processing the Instruction Stream  
Author: by James D. Dundas 
Degree: A thesis proposal submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy (Electrical Engineering) in The  Doctoral Committee: Professor Trevor Mudge, Chair Professor Richard Brown Professor Ronald Lomax Professor Karem Sakallah  
Date: 1997  
Affiliation: University of Michigan  
Abstract-found: 0
Intro-found: 0
Reference: [1] <author> Michael Upton, </author> <title> Architectural Trade-offs in a Latency Tolerant Gallium Arsenide Microprocessor, </title> <type> Ph.D. Thesis, </type> <institution> The University of Michigan, </institution> <year> 1996. </year>
Reference-contexts: Memory latency: implication and solutions Processor clock rates ha ve been increasing at about 40% per year , accounting for the lions share of the 59% per year increase in performance <ref> [1] </ref>. During this time, the access time of commodity DRAM has been decreasing at only 7% per year [2], resulting in an e xponentially increasing cycle time gap between processors and main-memory. <p> These simulations are intended to g auge the effectiveness of runahead for future processors, which are predicted to ha ve an increasing cycle time disparity between processors and off-chip memory <ref> [1] </ref>. A plot of the relative execution time for the first 100M instructions of the go benchmark is shown in Figure 44.
Reference: [2] <author> John Hennessy and David Patterson, </author> <title> Computer Architecture a Quantitative Approach, </title> <publisher> Morgan Kaufmann Publishers Inc., </publisher> <address> 2nd Edition, </address> <year> 1996. </year>
Reference-contexts: Memory latency: implication and solutions Processor clock rates ha ve been increasing at about 40% per year , accounting for the lions share of the 59% per year increase in performance [1]. During this time, the access time of commodity DRAM has been decreasing at only 7% per year <ref> [2] </ref>, resulting in an e xponentially increasing cycle time gap between processors and main-memory. Unless clever architectural tricks are employed, Amdahls Law [2] tells us that memory latency effects will eventually dominate the execution time of applications. <p> During this time, the access time of commodity DRAM has been decreasing at only 7% per year <ref> [2] </ref>, resulting in an e xponentially increasing cycle time gap between processors and main-memory. Unless clever architectural tricks are employed, Amdahls Law [2] tells us that memory latency effects will eventually dominate the execution time of applications. <p> This can be done in a v ariety of ways. Non-blocking L1 data caches [4] allo w a processor to continue to access the cache while a miss is serviced. Processors that allow the out-of-order completion of instructions <ref> [2] </ref> can continue to e xecute instructions while a data cache miss is serviced. This allo ws a processor to tolerate cache miss latency by attempting to keep the execution units busy while a cache miss is serviced. <p> If the priority of store traffic can be made lower than that of miss fill requests, then the a verage latency of miss fill requests can be lo wered by allowing them to proceed ahead of earlier stores. This requires both a relax ed memory consistency model <ref> [2] </ref>, as well as a means of checking to ensure that miss fill request and store addresses do not conict. <p> The easiest way to do this is to place stores into a FIFO queue called a write buffer <ref> [2] </ref>. Loads that pass stores in the buffer must compare their target address to the target addresses of the stores in the b uffer. The performance of write b uffers can be increased by coalescing stores in the b uffer that map to the same block of memory . <p> Unfortunately, this comes at the price of a load hazard. A hardware load-use interlock is control hardw are that is added to a pipeline that detects data dependencies, or load hazards, between load destination re gisters and subsequent dependent instructions <ref> [2] </ref>. The interlock hardw are can stall the pipeline until an y data dependencies can be resolved. This results in increased design comple xity, however this allows the compiler to ignore the pipeline implementation, if correct operation is all that is needed.
Reference: [3] <author> Alan J. Smith, </author> <title> Cache Memories, </title> <journal> ACM Computing Surveys, </journal> <volume> vol. 18, no. 3, </volume> <month> Sep </month> <year> 1982. </year>
Reference-contexts: There are many different established methods of attacking the problem of memory latency. 1.1 Caches The classic method of reducing the impact of memory latenc y is to employ one or more le vels of highspeed cache memory <ref> [3] </ref>. Caches work by exploiting locality of reference, meaning that if a data item is referenced once, then that item, or one near it in the address space, is lik ely to be referenced in the future. <p> The easiest way to reduce store traffic is to use a write-back, as opposed to a write-through data cache <ref> [3] </ref>. While write-back caches make sense for the lower levels of the memory hierarchy where accesses are less frequent, using a write-back L1 cache can complicate processor design [6]. <p> If data is prefetched into the L1 cache, then useless prefetches will replace other lines. As these other lines were probably fetched as a result of actual misses as opposed to prefetches, replacing these lines with useless prefetches can degrade performance. This is referred to as cache pollution <ref> [3] </ref>. If data is prefetched into a buffer where it waits for a demand miss before it is mo ved into the cache, pollution ef fects can be reduced, although useless prefetches can still reduce performance by increasing memory traffic. <p> This is more versatile, as it allows prefetching to be employed on legacy code, and across platforms. The earliest form of hardw are prefetching, sequential prefetching <ref> [3] </ref>, generates prefetches for one or more lines located immediately after a referenced line in the address space. When this is done on every access, it is referred to as al ways-prefetch. Generating sequential prefetches only when a reference to the current line misses is referred to as prefetch-on-miss. <p> In [17] a more advanced sequential prefetching scheme for shared memory multiprocessors was proposed and evaluated. They compared the simple sequential prefetching technique <ref> [3] </ref>, with an adaptive technique of their own design. Their adaptive sequential prefetching technique can dynamically adjust the number of sequential lines that are prefetched after a miss during program e xecu-tion. <p> y loads or stores in the predicted future basic block of instructions drop out of the SRPT automatically in parallel, and are then used to generate hardware prefetches in the same manner as that described by [19]. 9 1.7.2 Software data prefetching Software prefetching was first proposed in passing in <ref> [3] </ref>, and e xpanded upon without evaluation in [24]. The general idea is to add a instruction or instructions to the ISA that can compute a target address using the same addressing modes as the load and store instructions already present in the ISA. <p> As a result there are minor data cache-CPI discrepancies between some common data points for these simulations and those discussed above. Simulations of the prefetch-on-miss and al ways-prefetch sequential prefetching techniques described in <ref> [3] </ref> were performed. Prefetch-on-miss generates a prefetch for line i+1 whene ver an access of line i misses in the L1 data cache. Always-prefetch generates a prefetch for line i+1 when 20 ever line i is accessed. <p> A number of dif ferent replacement algorithms have been discussed in the past <ref> [3] </ref>. The problem with these algorithms is that the y assume that past data stream behavior is a reliable predictor of the best possible candidate line for replacement. <p> This will reduce the likelihood that a line that may be needed in the near future will be replaced. This was first proposed and investigated by the authors of <ref> [3] </ref> for use with a simple sequential data prefetching scheme. They concluded that this idea did not result in a significant impro vement in performance. This idea was reevaluated by the authors of [23], who claim that this operation signif icantly improved the performance of their more advanced prefetching method.
Reference: [4] <author> David Kroft, </author> <title> Lockup-Free Instruction Fetch/Prefetch Cache Organization, </title> <booktitle> In the Proceedings of the 8th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1981. </year>
Reference-contexts: This can be done in a v ariety of ways. Non-blocking L1 data caches <ref> [4] </ref> allo w a processor to continue to access the cache while a miss is serviced. Processors that allow the out-of-order completion of instructions [2] can continue to e xecute instructions while a data cache miss is serviced. <p> The DMAQ thus provides the functionality of a store queue, outstanding request list [19], and miss status holding re gisters <ref> [4] </ref>. It is further assumed that the DMA Q cannot coalesce store-throughs, or allow demand fetches or store-throughs to either pass or squash outstanding prefetches.
Reference: [5] <author> Richard Eickemeyer, Ross Johnson, and Steven Kunkel, </author> <title> Evaluation of Multithreaded Uniprocessors for Commercial Application Environments, </title> <booktitle> In the Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1996. </year>
Reference-contexts: This allo ws a processor to tolerate cache miss latency by attempting to keep the execution units busy while a cache miss is serviced. A conceptually similar approach is tak en by coarse-grained multi-threaded processors, which can switch between independent threads of execution when a cache miss is detected <ref> [5] </ref>. 2 1.3 Use the available memory bandwidth more effectively Another way to reduce latency is to control the store traf fic to the lower levels of the memory hierarchy.
Reference: [6] <author> Norman Jouppi, </author> <title> Cache Write Policies and Performance, </title> <institution> Digital Equipment Corporation Western Research Laboratory Research Report 91/12, </institution> <month> Dec </month> <year> 1991. </year>
Reference-contexts: The easiest way to reduce store traffic is to use a write-back, as opposed to a write-through data cache [3]. While write-back caches make sense for the lower levels of the memory hierarchy where accesses are less frequent, using a write-back L1 cache can complicate processor design <ref> [6] </ref>. If the priority of store traffic can be made lower than that of miss fill requests, then the a verage latency of miss fill requests can be lo wered by allowing them to proceed ahead of earlier stores.
Reference: [7] <author> Brian Bray and Michael Flynn, </author> <title> Writes Caches As An Alternative To Write Buffers, </title> <institution> Stanford University Computer Systems Laboratory Technical Report Number CSL-TR-91-470, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: This further reduces the amount of store traffic by combining requests that map to the same line into a single access to the next level of the memory hierarch y. An even more advanced way of controlling store traf fic is to employ a write cache <ref> [7] </ref>. A write cache is essentially a coalescing write b uffer with an LRU line replacement policy, as opposed to a FIFO replacement polic y, turning the buffer into a cache.
Reference: [8] <author> Sharon Perl and Dick Sites, </author> <title> Studies of Windows NT Performance using Dynamic Execution Traces, </title> <booktitle> In the Proceedings of the USENIX 2nd Symposium on Operating Systems Design and Implementation, </booktitle> <month> Oct </month> <year> 1996. </year>
Reference-contexts: This allows the write cache to coalesce e ven more stores, resulting in an e ven greater reduction in store traffic. 1.4 Increase the bandwidth of the memory hierarchy A low latency memory hierarchy is not enough to ensure adequate performance on many applications <ref> [8] </ref>. Memory bandwidth is also very important. Bandwidth can be increased in many different ways. Cache and main memory bandwidth can be increased by emplo ying wider cache lines or memory buses, pipelining accesses, and interlea ving cache and memory banks.
Reference: [9] <author> Markus Levy, </author> <title> The dynamics of DRAM Technology, Electronic Design News, </title> <month> Jan 5, </month> <year> 1995. </year>
Reference-contexts: Main memory 3 bandwidth can be increased e ven more by employing exotic DRAM types, such as RAMB US, as opposed to traditional commodity-type DRAMs <ref> [9] </ref>. 1.5 Statically schedule loads before stores One way to reduce the ef fects of memory latency is to statically schedule code such that loads are moved as far as possible before an y subsequent dependent instructions.
Reference: [10] <author> David Gallagher, William Chen, Scott Mahlke, John Gyllenhaal, and Wen-mei Hwu, </author> <title> Dynamic Memory Disambiguation Using the Memory Conflict Buffer, </title> <booktitle> In the Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> Oct </month> <year> 1994. </year>
Reference-contexts: These potential hazards can only be detected after address calculations have been performed, which themselves, require hazard detection. The solution to this problem is referred to as memory disambiguation <ref> [10] </ref>. Although pure hardware memory disambiguation techniques, such as the store-buffer, can allow loads to pass stores dynamically, additional performance can be obtained by impro ving the static code schedule beyond that obtainable with standard compiler optimizations. <p> One way of doing this is to employ hybrid hardwaresoftware memory disambiguation, a recent example of which is <ref> [10] </ref>, which introduced the memory conict buffer (MCB). The MCB is a hardware device used to detect dependencies between rescheduled load instructions and an y subsequent store instructions.
Reference: [11] <author> Michael Golden and Trevor Mudge, </author> <title> Comparison of two common pipeline structures, </title> <booktitle> IEE Proceedings on Computer and Digital Technology, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: This is done by re 4 executing the load instruction, as well as an y load-dependent instructions that were rescheduled by the compiler. 1.6 Executing loads early The conventional fivestage pipeline <ref> [11] </ref> uses an adder in the ALU to compute loadstore target addresses. This improves performance by reducing the branch misprediction penalty , since the ALU execute stage is the first stage that instructions enter once the y are issued. Unfortunately, this comes at the price of a load hazard. <p> An alternati ve is to require that the compiler to either insert independent instructions, or NOPs, into the slots after a load that can result in a data hazard <ref> [11] </ref>. This approach mak es the compilers job harder, as well as that of any future processor designers that may wish to radically change the microarchitecture of a subsequent implementation. <p> F or this reason this pipeline or ganization is often referred to as the address generation interlock (AGI) organization. Unfortunately it delays branch resolution by an additional cycle, resulting in an increased branch misprediction penalty . In <ref> [11] </ref> this tradeoff was evaluated, and concluded that a f ive stage AGI pipeline had to ha ve a branch misprediction rate no greater than 20% in order to beat the performance of the traditional LUI pipeline. <p> This pipeline organization has been employed in Intel i486, Pentium, Cyrix M1, and R8000 processors <ref> [11] </ref>.
Reference: [12] <author> Norman Jouppi, </author> <title> Architectural and Organizational Trade-offs in the Design of the MultiTitan CPU, </title> <type> Digital Equipment Corporation Technical Report 89/9, </type> <month> July </month> <year> 1989. </year>
Reference-contexts: This approach mak es the compilers job harder, as well as that of any future processor designers that may wish to radically change the microarchitecture of a subsequent implementation. The work in <ref> [12] </ref> first proposed eliminating the load-use interlock (LUI) in the traditional f ive-stage pipeline by performing load/store address calculation in a stage before the ALU e xecute stage. <p> This pipeline organization has been employed in Intel i486, Pentium, Cyrix M1, and R8000 processors [11]. Note that while the AGI organization was initially proposed in <ref> [12] </ref>, the y concluded that the e xtra adder that it required was expensive enough to make the LUI organization more attractive at the time. 5 Another way to perform loads early in a pipeline is to detect them early , and attempt to access the data cache ahead of time
Reference: [13] <author> Michael Golden, </author> <title> Hardware Support for Hiding Cache Latency, </title> <institution> The University of Michigan Department of Computer Science and Engineering Technical Report CSE-TR-152-93, </institution> <year> 1993. </year>
Reference-contexts: This idea w as first explained in <ref> [13] </ref> where the concept of the load tar get buffer (LTB) was introduced. The LTB is a table inde xed by the processor PC. <p> If the speculati ve load hits in the data cache, and the predicted address is correct, then the load latenc y is hidden. In <ref> [13] </ref> it w as concluded that the latency to the L1 data cache must be at least f ive cycles to justify the use of a moderate sized LTB. <p> If the carry-free addition was incorrect, or if the data cache port is busy, the load is performed in the customary MEM stage of the pipeline. The authors in [15] went on to combine the idea of the LTB <ref> [13] </ref> with their earlier work on Fast Address Calculation [14] to reduce load latenc y even further. The proposed the Base Re gister and Index Cache (BRIC), a cache of general purpose register values that is accessed in the fetch stage in parallel with the instruction cache.
Reference: [14] <author> Todd Austin, Dionisios Pnevmatikatos, and Gurindar Sohi, </author> <title> Streamlining Data Cache Access with Fast Address Calculation, </title> <booktitle> In the Proceedings of the 28th International Symposium on Microarchitecture, </booktitle> <year> 1995. </year>
Reference-contexts: In [13] it w as concluded that the latency to the L1 data cache must be at least f ive cycles to justify the use of a moderate sized LTB. In <ref> [14] </ref> a method is presented to reduce load latency by performing effective address calculation in parallel with data cache access within the cache access stage. <p> If the carry-free addition was incorrect, or if the data cache port is busy, the load is performed in the customary MEM stage of the pipeline. The authors in [15] went on to combine the idea of the LTB [13] with their earlier work on Fast Address Calculation <ref> [14] </ref> to reduce load latenc y even further. The proposed the Base Re gister and Index Cache (BRIC), a cache of general purpose register values that is accessed in the fetch stage in parallel with the instruction cache. <p> The pipeline can then use fast address calculation (FAC) <ref> [14] </ref> to potentially access the data cache during the decode stage of the pipeline. If this succeeds, then two cycles of load latency are hidden.
Reference: [15] <author> Todd Austin and Gurindar Sohi, </author> <title> Zero-Cycle Loads: Microarchitecture Support for Reducing Load Latency, </title> <booktitle> In the Proceedings of the 28th International Symposium on Microarchitecture, </booktitle> <year> 1995. </year>
Reference-contexts: This allows them to perform many loads one cycle earlier than would otherwise be the case. If the carry-free addition was incorrect, or if the data cache port is busy, the load is performed in the customary MEM stage of the pipeline. The authors in <ref> [15] </ref> went on to combine the idea of the LTB [13] with their earlier work on Fast Address Calculation [14] to reduce load latenc y even further.
Reference: [16] <author> Norman P. Jouppi, </author> <title> Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers, </title> <booktitle> In the Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: Sequential prefetch can also generate a great deal of additional traffic, which can swamp the connection to memory. A conceptually similar approach, the data stream buffer, was proposed in <ref> [16] </ref>. The data stream buffer generates fetch requests for sequential lines after a missing line. These lines are fetched into the FIFO stream buffer, where they can be subsequently placed into the cache if the processor requests them. <p> This idea was reevaluated by the authors of [23], who claim that this operation signif icantly improved the performance of their more advanced prefetching method. A similar approach would be to use runahead in conjunction with a victim cache <ref> [16] </ref>. Preprocessed load and store instructions could be used to probe the victim cache. On a victim cache hit 31 during runahead, the victim cache line and the likely future victim in the L1 data cache could swap places.
Reference: [17] <author> Fredrik Dahlgren, Michel Dubois, and Per Stenstrom, </author> <title> Fixed and Adaptive Sequential Prefetching in Shared Memory Microprocessors, </title> <booktitle> In the Proceedings of the International Conference on Parallel Processing, </booktitle> <year> 1993. </year>
Reference-contexts: Another problem with using the stream b uffer for data stream prefetching is that multiple stream buffers are usually required for adequate performance, multiplying the coherency problem. In <ref> [17] </ref> a more advanced sequential prefetching scheme for shared memory multiprocessors was proposed and evaluated. They compared the simple sequential prefetching technique [3], with an adaptive technique of their own design.
Reference: [18] <author> John Fu and Janak Patel, </author> <title> Data Prefetching in Multiprocessor Vector Cache Memories, </title> <booktitle> In the Proceedings of the 18th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: More advanced hardware data prefetching methods attempt to generate prefetches for nonsequential lines. These approaches are particularly suited for processors e xecuting scientific code, which typically access very large sparse matrices. The stride prefetch cache <ref> [18] </ref> can generate prefetches by using stride information obtained from v ector memory operations to generate the prefetch addresses. Unfortunately this only w orks for vector machines, although one could use the PowerPC load/store with update instructions in a similar fashion under certain circumstances.
Reference: [19] <author> Jean-Loup Baer and Tien-Fu Chen, </author> <title> An Effective On-Chip Preloading Scheme To Reduce Data Access Penalty, </title> <booktitle> In the Proceedings of Supercomputing, </booktitle> <year> 1991. </year>
Reference-contexts: Unfortunately this only w orks for vector machines, although one could use the PowerPC load/store with update instructions in a similar fashion under certain circumstances. Conventional processors can generate strided prefetches by caching information about the history of memory operations. This w as first done by <ref> [19] </ref> with their Reference Prediction T able (RPT). The RPT is a table that contains information about loads and stores e xecuted in the recent 8 past. <p> In [20] the Stride Prediction Table (SPT) was proposed. It is conceptually v ery similar to those developed by <ref> [19] </ref> and [21]. The primary dif ference between the SPT and the RPT [19] is that the SPT is indexed by the PC, while the RPT is inde xed by the lookahead PC (LA-PC), which can move ahead of the PC. <p> In [20] the Stride Prediction Table (SPT) was proposed. It is conceptually v ery similar to those developed by <ref> [19] </ref> and [21]. The primary dif ference between the SPT and the RPT [19] is that the SPT is indexed by the PC, while the RPT is inde xed by the lookahead PC (LA-PC), which can move ahead of the PC. This means that the RPT can generate prefetches earlier than the SPT . <p> This means that the RPT can generate prefetches earlier than the SPT . The approach taken by [21] is similar to the SPT. The authors of <ref> [19] </ref> continued their work with the RPT, and proposed three variants in [22]. The basic scheme uses the PC to inde x into the RPT, similar to that described in [20]. The lookahead scheme is basically the same as that described earlier in [19], e xcept that an additional iteration count <p> The authors of <ref> [19] </ref> continued their work with the RPT, and proposed three variants in [22]. The basic scheme uses the PC to inde x into the RPT, similar to that described in [20]. The lookahead scheme is basically the same as that described earlier in [19], e xcept that an additional iteration count field is added to the RPT that allo ws the LAPC to continue to generate prefetches when it has moved multiple loop iterations ahead of the PC. <p> of the PPG is applied to the SRPT, any cached stride information about an y loads or stores in the predicted future basic block of instructions drop out of the SRPT automatically in parallel, and are then used to generate hardware prefetches in the same manner as that described by <ref> [19] </ref>. 9 1.7.2 Software data prefetching Software prefetching was first proposed in passing in [3], and e xpanded upon without evaluation in [24]. <p> Combining aspects of both methods can result in improved performance. The authors of [32] took the idea of the reference prediction table <ref> [19] </ref> one step further by placing their instruction stride table (IST) under compiler control. <p> The DMAQ thus provides the functionality of a store queue, outstanding request list <ref> [19] </ref>, and miss status holding re gisters [4]. It is further assumed that the DMA Q cannot coalesce store-throughs, or allow demand fetches or store-throughs to either pass or squash outstanding prefetches.
Reference: [20] <author> J.W.C. Fu and J.H.Patel, </author> <title> Stride directed prefetching in scalar processors, </title> <booktitle> In the Proceedings of the 25th International Symposium on Microarchitecture, </booktitle> <month> Dec </month> <year> 1992. </year> <month> 42 </month>
Reference-contexts: If the state information for the entry indicates that the reference information can be trusted, then a potential prefetch with the target address previous_address + stride is generated if the target address misses in the cache, and there is no outstanding fetch to that address in progress. In <ref> [20] </ref> the Stride Prediction Table (SPT) was proposed. It is conceptually v ery similar to those developed by [19] and [21]. <p> The approach taken by [21] is similar to the SPT. The authors of [19] continued their work with the RPT, and proposed three variants in [22]. The basic scheme uses the PC to inde x into the RPT, similar to that described in <ref> [20] </ref>. The lookahead scheme is basically the same as that described earlier in [19], e xcept that an additional iteration count field is added to the RPT that allo ws the LAPC to continue to generate prefetches when it has moved multiple loop iterations ahead of the PC.
Reference: [21] <author> Ivan Sklenar, </author> <title> Prefetch unit for vector operations on scalar computers, </title> <journal> Computer Architecture News vol. </journal> <volume> 20, no. 4, </volume> <year> 1992. </year>
Reference-contexts: In [20] the Stride Prediction Table (SPT) was proposed. It is conceptually v ery similar to those developed by [19] and <ref> [21] </ref>. The primary dif ference between the SPT and the RPT [19] is that the SPT is indexed by the PC, while the RPT is inde xed by the lookahead PC (LA-PC), which can move ahead of the PC. <p> This means that the RPT can generate prefetches earlier than the SPT . The approach taken by <ref> [21] </ref> is similar to the SPT. The authors of [19] continued their work with the RPT, and proposed three variants in [22]. The basic scheme uses the PC to inde x into the RPT, similar to that described in [20].
Reference: [22] <author> Tien-Fu Chen and Jean-Loup Baer, </author> <title> Effective Hardware-Based Data Prefetching for High-Performance Processors, </title> <journal> In IEEE Transactions on Computers, </journal> <volume> vol. 44, no. 5, </volume> <month> May </month> <year> 1995. </year>
Reference-contexts: This means that the RPT can generate prefetches earlier than the SPT . The approach taken by [21] is similar to the SPT. The authors of [19] continued their work with the RPT, and proposed three variants in <ref> [22] </ref>. The basic scheme uses the PC to inde x into the RPT, similar to that described in [20]. <p> The correlated scheme attempts to correlate prefetches with changes in loop level, unlike the previous table approaches. The authors of [23] recognized that the reference prediction table (RPT) described in <ref> [22] </ref> could not keep up with the issue rate of superscalar processors, as it could only scan one instruction at a time. Their basic idea is the same, except that they use a modified branch target buffer (BTB) called a program progress graph (PPG).
Reference: [23] <author> Shlomit Pinter and Adi Yoaz, </author> <title> Tango: a Hardware-based Data Prefetching Technique for Superscalar Processors, </title> <booktitle> In the Proceedings of the 29th Annual International Symposium on Microarchitecture, </booktitle> <month> Dec </month> <year> 1996. </year>
Reference-contexts: The correlated scheme attempts to correlate prefetches with changes in loop level, unlike the previous table approaches. The authors of <ref> [23] </ref> recognized that the reference prediction table (RPT) described in [22] could not keep up with the issue rate of superscalar processors, as it could only scan one instruction at a time. <p> This was first proposed and investigated by the authors of [3] for use with a simple sequential data prefetching scheme. They concluded that this idea did not result in a significant impro vement in performance. This idea was reevaluated by the authors of <ref> [23] </ref>, who claim that this operation signif icantly improved the performance of their more advanced prefetching method. A similar approach would be to use runahead in conjunction with a victim cache [16]. Preprocessed load and store instructions could be used to probe the victim cache.
Reference: [24] <author> C. Scheurich and M. Dubois, </author> <title> Concurrent Miss Resolution in Multiprocessor Caches, </title> <booktitle> In the International Conference on Parallel Processing, </booktitle> <year> 1988. </year>
Reference-contexts: basic block of instructions drop out of the SRPT automatically in parallel, and are then used to generate hardware prefetches in the same manner as that described by [19]. 9 1.7.2 Software data prefetching Software prefetching was first proposed in passing in [3], and e xpanded upon without evaluation in <ref> [24] </ref>. The general idea is to add a instruction or instructions to the ISA that can compute a target address using the same addressing modes as the load and store instructions already present in the ISA.
Reference: [25] <author> D. Callahan, K. Kennedy, and A. Porterfield, </author> <title> Software Prefetching, </title> <booktitle> In the Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: In other words, nonbinding prefetches cannot cause incorrect program e xecution. Software prefetching is generally only ef fective for scientific applications that stride linearly through large matrices in a predictable manner. In <ref> [25] </ref> a simple heuristic was proposed to decide where to insert the prefetch instructions.
Reference: [26] <author> Alexander C. Klaiber and Henry M. Levy, </author> <title> An Architecture for Software-Controlled Data Prefetching, </title> <booktitle> In the Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <year> 1991. </year>
Reference-contexts: Prefetches are only generated for these accesses. This simple heuristic made software prefetching practical, at least for scientific applications that stride through lar ge matrices in a regular fashion. Additional software prefetching work was performed by <ref> [26] </ref>. The y developed the concept of the prefetch distance, or the number of loop iterations a data item should be prefetched before its first use.
Reference: [27] <author> Edward Gornish, Elana Granston, and Alexander Veidenbaum, </author> <title> Compiler-directed Data Prefetching in Multiprocessors with Memory Hierarchies, </title> <booktitle> In the Proceedings of the International Conference on Supercomputing, </booktitle> <year> 1990. </year>
Reference-contexts: This a voids displacing useful data from the cache, ho wever this comes at the cost of keeping data in the buffer and cache coherent. 10 The authors of <ref> [27] </ref> considered using softw are prefetching with a shared-memory multiprocessor. Their scheme was somewhat different as their prefetch instructions could fetch multiple cache lines.
Reference: [28] <author> William Chen, Scott Mahlke, Pohua Chang, and Wen-mei Hwu, </author> <title> Data Access Microarchitectures for Superscalar Processors with Compiler-Assisted Data Prefetching, </title> <booktitle> In the Proceedings of the 24th International Symposium on Microarchitecture, </booktitle> <year> 1991. </year>
Reference-contexts: Furthermore, they assumed that cache coherence w as maintained via software, making all prefetches binding, which forces the compiler to carefully consider control and data dependences before inserting prefetch instructions. This requires the compiler to be v ery conservative, greatly reducing the performance benefits of prefetching. In <ref> [28] </ref> an attempt w as made to use softw are prefetching on integer benchmarks. Very small caches were used with a relati vely small miss penalty in conjunction with a prefetch b uffer.
Reference: [29] <author> Todd C. Mowry, Monica S. Lam, and A.Gupta, </author> <title> Design and evaluation of a compiler algorithm for prefetch-ing, </title> <booktitle> In the Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> Oct </month> <year> 1992. </year>
Reference-contexts: The prefetch buffer helped to reduce the amount of pollution, which is critical when softw are prefetch-ing is used for inte ger codes whose access patterns are typically dif ficult to predict. Their results were mixed. The authors of <ref> [29] </ref> introduced an algorithm for inserting software prefetches that performs two very important tasks. Their algorithm emplo ys software pipelining in order to ensure that the accesses in each iteration of a loop are covered by software prefetches. <p> Unlike the other tabledriven approaches, the IST only generates prefetches when the current access misses in the cache. This allo ws the IST to be placed off-chip. The authors of [33] combined the basic ideas of the IST [32] and the prefetch predicates of <ref> [29] </ref> to form the runahead table. The major difference between the IST and the runahead table is that the runahead table can initiate prefetches whenever the processor PC touches an entry in the table. <p> The major difference between the IST and the runahead table is that the runahead table can initiate prefetches whenever the processor PC touches an entry in the table. They also eliminate redundant prefetches by adding predicate information to each entry, which is conceptually similar to that employed in <ref> [29] </ref>. 12 1.8 Our Contributions The constraints placed upon our design space for the PUMA processor [34] led us to consider a number of unorthodox architectural ideas. One of these was runahead processing. The basic idea is to allow a processor pipeline to preprocess instructions during cache miss c ycles.
Reference: [30] <author> Mikko Lipasti, William Schmidt, Steven Kunkel, and Robert Roediger, SPAID: </author> <title> Software Prefetching in Pointer and Call-Intensive Environments, </title> <booktitle> In the Proceedings of the 28th International Symposium on Mi-croarchitecture, </booktitle> <year> 1995. </year>
Reference-contexts: The general consensus up to this point is that softw are prefetching is generally only useful for scientific codes that access matrices in a predictable f ashion. A number of recently developed software prefetching schemes have been developed to address this shortcoming. The authors of <ref> [30] </ref> developed a way to generate useful software prefetches for pointer-intensive integer programs. Their scheme inserted softw are prefetch instructions at procedure call sites that passed pointers as arguments. The pointers are used to specify the prefetch tar get addresses.
Reference: [31] <author> Chi-Keung Luk and Todd C. Mowry, </author> <title> Compiler-Based Prefetching for Recursive Data Structures, </title> <booktitle> In the Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> Oct </month> <year> 1996. </year>
Reference-contexts: Their scheme inserted softw are prefetch instructions at procedure call sites that passed pointers as arguments. The pointers are used to specify the prefetch tar get addresses. This takes advantage of the fact that pointer arguments are likely to be dereferenced during a procedure call. The authors of <ref> [31] </ref> examined ways to generate software prefetches for pointer-intensive integer programs that employ recursive data structures. A recursi ve data structure (RDS) is a heap-allocated object, such as a link ed-list, tree, or graph, whose indi vidual nodes are link ed together via pointers.
Reference: [32] <author> Ricardo Bianchini and Thomas J. LeBlanc, </author> <title> A Preliminary Evaluation of Cache-Miss-Initiated Prefetching Techniques in Scalable Multiprocessors, </title> <institution> University of Rochester Computer Science Department Technical Report 515, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: A similar fault can be found with table-based hardw are prefetching methods: relying on the hardware to allocate table entries can w aste a great deal of hardw are. Combining aspects of both methods can result in improved performance. The authors of <ref> [32] </ref> took the idea of the reference prediction table [19] one step further by placing their instruction stride table (IST) under compiler control. <p> Unlike the other tabledriven approaches, the IST only generates prefetches when the current access misses in the cache. This allo ws the IST to be placed off-chip. The authors of [33] combined the basic ideas of the IST <ref> [32] </ref> and the prefetch predicates of [29] to form the runahead table. The major difference between the IST and the runahead table is that the runahead table can initiate prefetches whenever the processor PC touches an entry in the table.
Reference: [33] <author> Tien-Fu Chen, </author> <title> An Effective Programmable Prefetch Engine for On-Chip Caches, </title> <booktitle> In the Proceedings of the 28th International Symposium on Microarchitecture, </booktitle> <year> 1995. </year>
Reference-contexts: Unlike the other tabledriven approaches, the IST only generates prefetches when the current access misses in the cache. This allo ws the IST to be placed off-chip. The authors of <ref> [33] </ref> combined the basic ideas of the IST [32] and the prefetch predicates of [29] to form the runahead table. The major difference between the IST and the runahead table is that the runahead table can initiate prefetches whenever the processor PC touches an entry in the table.
Reference: [34] <author> James Dundas, </author> <title> A Description of the FXU, </title> <institution> University of Michigan PUMA Project Internal Document, </institution> <month> Oct </month> <year> 1996. </year>
Reference-contexts: They also eliminate redundant prefetches by adding predicate information to each entry, which is conceptually similar to that employed in [29]. 12 1.8 Our Contributions The constraints placed upon our design space for the PUMA processor <ref> [34] </ref> led us to consider a number of unorthodox architectural ideas. One of these was runahead processing. The basic idea is to allow a processor pipeline to preprocess instructions during cache miss c ycles. <p> All of the benchmarks were compiled using version 2.7.2 of gcc with the -O optimization level. The simulated data memory hierarchy corresponds closely to a baseline version of that which is being considered for the processor described in <ref> [34] </ref>. In the simulations an 8 entry Data Memory Access Queue (DMAQ) is used to send miss and prefetch requests, as well as store-throughs, from an on-chip L1 data cache to an of f-chip L2 data cache strictly in the order in which the y were generated.
Reference: [35] <author> Alan Eustace and Amitabh Srivastava, </author> <title> ATOM: A Flexible Interface for Building High Performance Program Analysis Tools, </title> <institution> Digital Equipment Corporation Western Research Laboratory Technical Note TN-44, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: A runahead processor implementing the aggressive runahead policy would continue to execute instructions past the branch dependency, using its branch prediction scheme to predict the proper path of execution. 3. Preliminary Runahead Data Prefetching Experiments 3.1 Simulation The simulator we used was created using ATOM <ref> [35] </ref>. It did not model pipeline stalls or penalties due to instruction cache misses, just the effects of data cache misses and any resulting prefetch-ing. Thus the CPI figures obtained in the e xperiments is simply the contrib ution due to data cache effects, data cache-CPI (MCPI), plus one.

References-found: 35

