URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR96675-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Compiler Techniques for Software Prefetching on Cache-Coherent Shared-Memory Multiprocessors  
Author: Nathaniel McIntosh Katherine Fletcher Keith Cooper, Ken Kennedy 
Affiliation: Department of Computer Science Department of Electrical and Computer Engineering Rice University  
Abstract: In this paper we present a comprehensive compiler framework for improving the efficiency of compiler-directed software prefetching on cache-coherent distributed shared-memory multiprocessors. The key component of our work is a form of global data-flow analysis that predicts at compile-time the sets of array references that are likely to cause coherence activity at run-time. The data-flow framework accurately analyzes the cache behavior in a parallel program by combining array section analysis with knowledge about the cache configuration and an encoding of the target machine's cache coherence protocol. Existing prefetching algorithms have problems issuing prefetches for coherence misses, resulting in late prefetches and latency penalties. Our compiler identifies the particular variable references and loop iterations that cause coherence misses, and schedules prefetches for these references farther in advance, effectively hiding the latency that they incur. In other situations where existing prefetching techniques encounter difficulties, such as false sharing and many-procesor read sharing, we use data-flow information to apply optimizations that decrease interconnect traffic and reduce the memory latency penalties incurred by the program.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, R. Bianchini, D. Chiaken, K. Johnson, D. Kratz, J. Kubiatowicz, B.-H. Lim, K. Mackenzie, and D. Yeung. </author> <title> The MIT Alewife machine: Architecture and performance. </title> <booktitle> In Proceedings of the 22th International Symposium on Computer Architecture, </booktitle> <address> Santa Margherita Ligure, Italy, </address> <month> June </month> <year> 1995. </year> <month> 20 </month>
Reference-contexts: Modern multiprocessors often use a distributed shared-memory (DSM) architecture, in which main memory is divided up into modules and distributed across all of the processors; the processors are then linked together with a high-speed interconnection network <ref> [1, 6, 17] </ref>. <p> In Section 4, we discuss our plans for experimentally validating our techniques. Section 5 describes related research. In Section 6, we offer concluding remarks. 2 Description 2.1 Target architecture, programs This work targets shared-memory, cache-coherent DSM multiprocessors that support invalidation-based coherence protocols <ref> [1, 6, 17] </ref>. Examples of machines that support such protocols include the Alewife machine [1], the FLASH multiprocessor [17], and the Convex Exemplar [6]. <p> Section 5 describes related research. In Section 6, we offer concluding remarks. 2 Description 2.1 Target architecture, programs This work targets shared-memory, cache-coherent DSM multiprocessors that support invalidation-based coherence protocols [1, 6, 17]. Examples of machines that support such protocols include the Alewife machine <ref> [1] </ref>, the FLASH multiprocessor [17], and the Convex Exemplar [6]. We have chosen to target scientific Fortran programs, since there is a large existing base of such applications, and since these programs often place heavy demands on the cache and memory subsystems of the machines they run on.
Reference: [2] <author> A. V. Aho, R. Sethi, and J. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <note> second edition, </note> <year> 1986. </year>
Reference-contexts: We refer the reader to previous works for a complete description of the terminology and mechanics of this form of data-flow analysis <ref> [2, 8, 9, 12, 15] </ref>. Interval analysis proceeds in two steps: an "interval contraction" phase, followed by an "interval expansion" phase. In the contraction phase, intervals are processed from innermost to outermost; an interval is only processed after all the intervals it contains are completed.
Reference: [3] <author> V. Balasundaram. </author> <title> A mechanism for keeping useful internal information in parallel programming tools: The data access descriptor. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 9(2) </volume> <pages> 154-170, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: In our framework, we capture information on array access patterns using array section analysis <ref> [3, 4, 13, 18] </ref>. When applied to a portion of the program (typically a basic block, loop, or loop nest), array section analysis produces a summary representation of the region accessed within each array. <p> The ] operator is defined as "loop translation"; when applied to a section within a given loop, it substitutes in the bounds of the loop for the loop induction variable <ref> [3] </ref>. For example, in Figure 2, the section accessed on a given iteration of the "doall j 1 " loop is a (j 1 ,1:100). Applying the operation ] j 1 to this section will produce the section a (1:100,1:100). <p> Our compiler uses Data Access Descriptors to represent array sections <ref> [3] </ref>. In addition to the multiprocessor enhancements, the compiler fully supports uniprocessor software prefetching, including reuse analysis, loop peeling, and loop unrolling [22].
Reference: [4] <author> M. Burke and R. Cytron. </author> <title> Interprocedural dependence analysis and parallelization. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <address> Palo Alto, CA, </address> <month> June </month> <year> 1986. </year>
Reference-contexts: In our framework, we capture information on array access patterns using array section analysis <ref> [3, 4, 13, 18] </ref>. When applied to a portion of the program (typically a basic block, loop, or loop nest), array section analysis produces a summary representation of the region accessed within each array.
Reference: [5] <author> K. Cooper, K. Kennedy, and N. McIntosh. </author> <title> Cross-loop reuse analysis and its application to cache optimizations. </title> <booktitle> In Proceedings of the Ninth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> San Jose, CA, </address> <month> August </month> <year> 1996. </year> <note> Springer-Verlag (Available on-line prior to publication at ftp://cs.rice.edu/public/mcintosh/lcpc96paper.ps.gz). </note>
Reference-contexts: In this section we summarize the modifications to our framework that incorporate cache size and organization. This work builds on the techniques described in a previous paper <ref> [5] </ref>. 2.8.1 Array section age First, we introduce the concept of the "age" of an array section with respect to a particular point in the program. <p> We currently estimate the "effective" size of the cache (used in the analysis) by multiplying the actual cache size by 1 1 2 S , where S is the set associativity of the cache. More details can be found in <ref> [5] </ref>. 2.8.2 Data-flow framework modifications The equations in 13 compute the same information as those in Figure 9, except that they incorporate several important changes that take cache size into account.
Reference: [6] <institution> CONVEX Computer Corporation. Exemplar Architecture. CONVEX Press, Richardson, Texas, </institution> <note> first edition, </note> <year> 1993. </year>
Reference-contexts: Modern multiprocessors often use a distributed shared-memory (DSM) architecture, in which main memory is divided up into modules and distributed across all of the processors; the processors are then linked together with a high-speed interconnection network <ref> [1, 6, 17] </ref>. <p> In Section 4, we discuss our plans for experimentally validating our techniques. Section 5 describes related research. In Section 6, we offer concluding remarks. 2 Description 2.1 Target architecture, programs This work targets shared-memory, cache-coherent DSM multiprocessors that support invalidation-based coherence protocols <ref> [1, 6, 17] </ref>. Examples of machines that support such protocols include the Alewife machine [1], the FLASH multiprocessor [17], and the Convex Exemplar [6]. <p> In Section 6, we offer concluding remarks. 2 Description 2.1 Target architecture, programs This work targets shared-memory, cache-coherent DSM multiprocessors that support invalidation-based coherence protocols [1, 6, 17]. Examples of machines that support such protocols include the Alewife machine [1], the FLASH multiprocessor [17], and the Convex Exemplar <ref> [6] </ref>. We have chosen to target scientific Fortran programs, since there is a large existing base of such applications, and since these programs often place heavy demands on the cache and memory subsystems of the machines they run on.
Reference: [7] <author> Sandhya Dwarkadas, John R. Jump, and James B. Sinclair. </author> <title> Execution-driven simulation of multiprocessors: Address and timing analysis. </title> <journal> In Journal of Transactions on Modeling and Computer Simulation, </journal> <month> October </month> <year> 1994. </year>
Reference-contexts: We compare the performance of these programs when run without prefetching, with prefetching, and with the enhanced prefetching (i.e. adding the optimizations enabled by our analysis framework). We use an execution driven parallel architecture simulator derived from RPPT <ref> [7] </ref>. Our simulated parallel machine consists of a series of clusters of RISC processors connected by a high speed interconnection network; the cache, memory, and network subsystems are modeled in detail.
Reference: [8] <author> E. Granston and A. Veidenbaum. </author> <title> Detecting redundant accesses to array data. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: We refer the reader to previous works for a complete description of the terminology and mechanics of this form of data-flow analysis <ref> [2, 8, 9, 12, 15] </ref>. Interval analysis proceeds in two steps: an "interval contraction" phase, followed by an "interval expansion" phase. In the contraction phase, intervals are processed from innermost to outermost; an interval is only processed after all the intervals it contains are completed. <p> In the expansion phase, the process is reversed: summary nodes are expanded into their original intervals and then re-analyzed. As with the work of Gross and Steenkiste [9] and of Granston and Veidenbaum <ref> [8] </ref>, we present two sets of equations, the first for computing information within an interval, and the second for collapsing the nodes in an interval into a single summary node. 2.7 Data-flow equations n, initial values of "UREF (n)" and "CREF (n)" are computed by simply inspecting n; these initial sets <p> Our interval analysis framework was inspired by that of Granston and Veidenbaum <ref> [8] </ref>, which in turn was based on the framework of Gross and Steenkiste [9]. It differs from the work of Granston and Veidenbaum in several important respects.
Reference: [9] <author> T. Gross and P. Steenkiste. </author> <title> Structured dataflow analysis for arrays and its use in an optimizing compiler. </title> <journal> Software|Practice and Experience, </journal> <volume> 20(2) </volume> <pages> 133-155, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: We refer the reader to previous works for a complete description of the terminology and mechanics of this form of data-flow analysis <ref> [2, 8, 9, 12, 15] </ref>. Interval analysis proceeds in two steps: an "interval contraction" phase, followed by an "interval expansion" phase. In the contraction phase, intervals are processed from innermost to outermost; an interval is only processed after all the intervals it contains are completed. <p> In the expansion phase, the process is reversed: summary nodes are expanded into their original intervals and then re-analyzed. As with the work of Gross and Steenkiste <ref> [9] </ref> and of Granston and Veidenbaum [8], we present two sets of equations, the first for computing information within an interval, and the second for collapsing the nodes in an interval into a single summary node. 2.7 Data-flow equations n, initial values of "UREF (n)" and "CREF (n)" are computed by <p> Our interval analysis framework was inspired by that of Granston and Veidenbaum [8], which in turn was based on the framework of Gross and Steenkiste <ref> [9] </ref>. It differs from the work of Granston and Veidenbaum in several important respects. Their work was geared towards a multiprocessor without hardware support for cache coherence, whereas we specifically incorporate support to hardware cache coherence in our data-flow framework.
Reference: [10] <author> M. Gupta and E. Schonberg. </author> <title> A framework for exploiting data availability to optimize communication. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: The pmap is a restricted version of the "mapping function descriptor" used in the Available Section Descriptor abstraction <ref> [10] </ref>. 4 do k 1 = 1, n doall k 2 = 1, 100 do k 3 = 1, 100 enddo enddoall doall k 4 = 1, 50 do k 5 = 1, 50 enddo enddoall enddo do i 1 = 1, 100 enddo doall j 1 = 1, 100 do
Reference: [11] <author> M. Gupta, E. Schonberg, and H. Srinivasan. </author> <title> A unified data-flow framework for optimizing communication. </title> <booktitle> In Proceedings of the Seventh Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, NY, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Their techniques were designed for software-controlled local memories and not caches; they have no mechanism for taking into account cache replacement effects. The data-flow analysis we use is also related to the techniques developed by Gupta, Schonberg, and Srinivasan for optimizing communication placement for programs running on distributed-memory multiprocessors <ref> [11] </ref>. Communications placement is a fairly different problem, however and analyzes the flow of values within the program, whereas our analysis focuses on the use of locations.
Reference: [12] <author> R. v. Hanxleden. </author> <title> Compiler Support for Machine-Independent Parallelization of Irregular Problems. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: We refer the reader to previous works for a complete description of the terminology and mechanics of this form of data-flow analysis <ref> [2, 8, 9, 12, 15] </ref>. Interval analysis proceeds in two steps: an "interval contraction" phase, followed by an "interval expansion" phase. In the contraction phase, intervals are processed from innermost to outermost; an interval is only processed after all the intervals it contains are completed.
Reference: [13] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: In our framework, we capture information on array access patterns using array section analysis <ref> [3, 4, 13, 18] </ref>. When applied to a portion of the program (typically a basic block, loop, or loop nest), array section analysis produces a summary representation of the region accessed within each array.
Reference: [14] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification. </title> <booktitle> Scientific Programming, </booktitle> <address> 2(1-2):1-170, </address> <year> 1993. </year>
Reference-contexts: Although our compiler does not currently incorporate data distribution directives (of the sort provided in High Performance Fortran <ref> [14] </ref>), our techniques could easily be extended to exploit information of this nature.
Reference: [15] <author> K. Kennedy. </author> <title> A survey of data flow analysis techniques. </title> <editor> In S. Muchnick and N. Jones, editors, </editor> <booktitle> Program Flow Analysis, </booktitle> <pages> pages 5-54. </pages> <publisher> Prentice-Hall, </publisher> <year> 1981. </year> <month> 21 </month>
Reference-contexts: We refer the reader to previous works for a complete description of the terminology and mechanics of this form of data-flow analysis <ref> [2, 8, 9, 12, 15] </ref>. Interval analysis proceeds in two steps: an "interval contraction" phase, followed by an "interval expansion" phase. In the contraction phase, intervals are processed from innermost to outermost; an interval is only processed after all the intervals it contains are completed.
Reference: [16] <author> Kuck & Associates, Inc. </author> <title> KAP User's Guide. </title> <address> Champaign, IL 61820, </address> <year> 1988. </year>
Reference-contexts: These restrictions and assumptions are intended to coincide with the parallel programs currently generated by commercially available automatic parallelizers such as KAP <ref> [16] </ref> or by an advanced workstation compiler. 2.2 Invalidation-based coherence protocols When a processor writes to a location that other processors may have cached, or when it reads a location that another processor may have recently written, the underlying hardware takes action to ensure that all processors see a "coherent" view
Reference: [17] <author> J. Kuskin, D. Ofelt, M. Heinrich, J. Heinlein, R. Simon, K. Gharachorloo, J. Chapin, D. Nakahira, J. Bax-ter, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The Stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of the 21th International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: Modern multiprocessors often use a distributed shared-memory (DSM) architecture, in which main memory is divided up into modules and distributed across all of the processors; the processors are then linked together with a high-speed interconnection network <ref> [1, 6, 17] </ref>. <p> In Section 4, we discuss our plans for experimentally validating our techniques. Section 5 describes related research. In Section 6, we offer concluding remarks. 2 Description 2.1 Target architecture, programs This work targets shared-memory, cache-coherent DSM multiprocessors that support invalidation-based coherence protocols <ref> [1, 6, 17] </ref>. Examples of machines that support such protocols include the Alewife machine [1], the FLASH multiprocessor [17], and the Convex Exemplar [6]. <p> Section 5 describes related research. In Section 6, we offer concluding remarks. 2 Description 2.1 Target architecture, programs This work targets shared-memory, cache-coherent DSM multiprocessors that support invalidation-based coherence protocols [1, 6, 17]. Examples of machines that support such protocols include the Alewife machine [1], the FLASH multiprocessor <ref> [17] </ref>, and the Convex Exemplar [6]. We have chosen to target scientific Fortran programs, since there is a large existing base of such applications, and since these programs often place heavy demands on the cache and memory subsystems of the machines they run on.
Reference: [18] <author> Z. Li and P. Yew. </author> <title> Efficient interprocedural analysis for program restructuring for parallel programs. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on Parallel Programming: Experience with Applications, Languages, and Systems (PPEALS), </booktitle> <address> New Haven, CT, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: In our framework, we capture information on array access patterns using array section analysis <ref> [3, 4, 13, 18] </ref>. When applied to a portion of the program (typically a basic block, loop, or loop nest), array section analysis produces a summary representation of the region accessed within each array.
Reference: [19] <author> E. Markatos and T. LeBlanc. </author> <title> Using processor affinity in loop scheduling on shared-memory multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(4) </volume> <pages> 379-400, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: We currently require that loops use barrier synchronization only, and we assume that the iterations of a given parallel loop are scheduled (assigned to processors) either statically by blocks, or through the use of an affinity scheduler <ref> [19] </ref>. Although our compiler does not currently incorporate data distribution directives (of the sort provided in High Performance Fortran [14]), our techniques could easily be extended to exploit information of this nature.
Reference: [20] <author> T. Mowry. </author> <title> Tolerating Latency Through Software Controlled Data Prefetching. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Stanford University, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Email: mcintosh@cs.rice.edu, phone 713-527-8101 x2731, fax 713-285-5930. 1 hide the effects of memory latency are an effective way to improve program performance. 1.1 Compiler-Directed Software Prefetching Compiler-directed software prefetching is a hybrid hardware/software technique for hiding the latency of cache misses <ref> [20, 22] </ref>. There are two primary components for software prefetching: hardware support for a prefetch instruction, and a compiler that inserts prefetches into the programs it compiles. The most successful prefetching schemes make use of advisory prefetches. <p> In previous studies of software prefetching on multiprocessors, researchers assumed a uniform "average" miss penalty <ref> [20, 21, 24] </ref>. This assumption was necessary because existing compilers had no accurate way to predict coherence misses, and could not take steps to avoid coherence miss latency. In this paper, we describe a new form of compiler analysis that predicts coherence activity at important points within a program. <p> Previous researchers have studied this problem; Mowry, Lam, and Gupta developed a scheme for selecting the prefetch mode using an algorithm in which references in a loop nest are divided into equivalence classes based on a form of vector-space reuse analysis <ref> [20] </ref>. The information provided by our analysis framework can be used to improve on previous methods. By comparing the array sections for a given pair of read/write references, for example, we can issue exclusive-mode prefetches only when we know that they will be profitable. <p> The data collected by the simulator include overall execution time, cache miss rates, average miss latency, total prefetches issued, useless prefetches, and late prefetches, among others. 5 Related work Mowry, Lam, and Gupta's work provides a study of compiler-directed software prefetching for uniprocessors and for DSM-style shared-memory multiprocessors <ref> [20, 21, 22] </ref>. The double pipelining we use for prefetching coherence misses is to their strategy for prefetching references that use indirection arrays.
Reference: [21] <author> T. Mowry and A. Gupta. </author> <title> Tolerating latency through software-controlled prefetching in shared-memory multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12(2) </volume> <pages> 87-106, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: In previous studies of software prefetching on multiprocessors, researchers assumed a uniform "average" miss penalty <ref> [20, 21, 24] </ref>. This assumption was necessary because existing compilers had no accurate way to predict coherence misses, and could not take steps to avoid coherence miss latency. In this paper, we describe a new form of compiler analysis that predicts coherence activity at important points within a program. <p> The data collected by the simulator include overall execution time, cache miss rates, average miss latency, total prefetches issued, useless prefetches, and late prefetches, among others. 5 Related work Mowry, Lam, and Gupta's work provides a study of compiler-directed software prefetching for uniprocessors and for DSM-style shared-memory multiprocessors <ref> [20, 21, 22] </ref>. The double pipelining we use for prefetching coherence misses is to their strategy for prefetching references that use indirection arrays.
Reference: [22] <author> T. Mowry, M. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), </booktitle> <pages> pages 62-73, </pages> <address> Boston, MA, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: Email: mcintosh@cs.rice.edu, phone 713-527-8101 x2731, fax 713-285-5930. 1 hide the effects of memory latency are an effective way to improve program performance. 1.1 Compiler-Directed Software Prefetching Compiler-directed software prefetching is a hybrid hardware/software technique for hiding the latency of cache misses <ref> [20, 22] </ref>. There are two primary components for software prefetching: hardware support for a prefetch instruction, and a compiler that inserts prefetches into the programs it compiles. The most successful prefetching schemes make use of advisory prefetches. <p> It then transforms the program, inserting a prefetch instruction prior to any load instruction that is predicted to cause a cache miss. The compiler uses a specialized type of software pipelining to insure that prefetch instructions are issued sufficiently far in advance of their corresponding loads <ref> [22] </ref>. 1.2 Prefetching for parallel programs The critical difference between uniprocessors and DSM multiprocessors with regard to prefetching is that the time required to satisfy a cache miss varies tremendously depending on data sharing patterns. <p> Our compiler uses Data Access Descriptors to represent array sections [3]. In addition to the multiprocessor enhancements, the compiler fully supports uniprocessor software prefetching, including reuse analysis, loop peeling, and loop unrolling <ref> [22] </ref>. In order to avoid having to implement an instruction-level optimizing "back end" that supports prefetch instructions, we instead create simulated prefetches at the Fortran level: for each array "A" within the application program, we introduce a shadow array "SA" of the same type, length, etc. <p> The data collected by the simulator include overall execution time, cache miss rates, average miss latency, total prefetches issued, useless prefetches, and late prefetches, among others. 5 Related work Mowry, Lam, and Gupta's work provides a study of compiler-directed software prefetching for uniprocessors and for DSM-style shared-memory multiprocessors <ref> [20, 21, 22] </ref>. The double pipelining we use for prefetching coherence misses is to their strategy for prefetching references that use indirection arrays.
Reference: [23] <author> M. Papamarcos and J. Patel. </author> <title> A Low Overhead Coherence Solution for Multiprocessors with Private Cache Memories. </title> <booktitle> In Proceedings of the 11th International Symposium on Computer Architecture, </booktitle> <pages> pages 340-347. </pages> <publisher> IEEE, </publisher> <year> 1984. </year>
Reference: [24] <author> D. Tullsen and S. Eggers. </author> <title> Limitations of cache prefetching on a bus-based multiprocessor. </title> <booktitle> In Proceedings of the 20th International Symposium on Computer Architecture, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: In previous studies of software prefetching on multiprocessors, researchers assumed a uniform "average" miss penalty <ref> [20, 21, 24] </ref>. This assumption was necessary because existing compilers had no accurate way to predict coherence misses, and could not take steps to avoid coherence miss latency. In this paper, we describe a new form of compiler analysis that predicts coherence activity at important points within a program.
Reference: [25] <author> J. Uniejewski. </author> <title> SPEC Benchmark Suite: Designed for today's advanced systems. </title> <journal> SPEC Newsletter Volume 1, </journal> <note> Issue 1, SPEC, Fall 1989. 22 </note>
Reference-contexts: as a prefetch of location N in the actual array. 4.2 Experimental strategy For the full version of this paper, we intend to provide an experimental study that will measure how well our techniques work in practice, using a set of parallelized Fortran programs taken from the SPEC benchmark suite <ref> [25] </ref>. We compare the performance of these programs when run without prefetching, with prefetching, and with the enhanced prefetching (i.e. adding the optimizations enabled by our analysis framework). We use an execution driven parallel architecture simulator derived from RPPT [7].
References-found: 25

