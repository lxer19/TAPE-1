URL: http://www.cs.rice.edu/~mpal/papers/PPOPP95.ps
Refering-URL: http://www.cs.rice.edu/~dsystem/techPapers.html
Root-URL: 
Title: A Model and Compilation Strategy for Out-of-Core Data Parallel Programs  
Author: Rajesh Bordawekar Alok Choudhary Ken Kennedy Charles Koelbel Michael Paleczny 
Abstract: It is widely acknowledged in high-performance computing circles that parallel input/output needs substantial improvement in order to make scalable computers truly usable. We present a data storage model that allows processors independent access to their own data and a corresponding compilation strategy that integrates data-parallel computation with data distribution for out-of-core problems. Our results compare several communication methods and I/O optimizations using two out-of-core problems, Jacobi iteration and LU factorization. 
Abstract-found: 1
Intro-found: 1
Reference: [AK87] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: Trivedi [Tri77a, Tri77b] shows that profitable opportunities for demand prefetching can be identified from a program's syntax. Highlights of managing other aspects of the memory hierarchy include: Allen and Kennedy on vector register allocation <ref> [AK87] </ref>, Carr and Kennedy on compiler blocking of scientific codes [CK92], and Mowry on software prefetching for cache [Mow94]. Related projects include disk-directed I/O, by David Kotz at Dartmouth College, in which I/O processors direct the transfer of data from disk to processors.
Reference: [AS79] <author> W. Abu-Sufah. </author> <title> Improving the Performance of Virtual Memory Computers. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1979. </year>
Reference-contexts: Previous work on compiler improvements at the memory to disk interface starts with Abu-Sufah and Trivedi at the end of the 1970's. Abu-Sufah <ref> [AS79] </ref> demonstrates that applying loop distribution and loop fusion can reduce the space-time costs for numerical algorithms. Trivedi [Tri77a, Tri77b] shows that profitable opportunities for demand prefetching can be identified from a program's syntax.
Reference: [CBH + 94] <author> A. Choudhary, R. Bordawekar, M. Harry, R. Krishnaiyer, R. Ponnusamy, T. Singh, and R. Thakur. </author> <title> PASSION: Parallel and Scalable Software for Input-Output. </title> <type> Technical Report SCCS-636, </type> <institution> NPAC, Syracuse University, </institution> <month> Sep </month> <year> 1994. </year>
Reference-contexts: If Page 6 the owning processor previously had the data in mem-ory, the compiler can schedule it to communicate the data at that time and schedule the requesting processor to store the data in its local array file. Otherwise, a two-phase method <ref> [CBH + 94] </ref> where the owner reads the data and sends it to the requesting processor when needed can be used. A final possibility applies if there is some processing capability at the I/O node itself, in which case disk-directed I/O can be used to send the data [Kot94]. <p> Thus, an implementation of a collective communication routine requires reading data from local files, communicating data to the appropriate destination processors, and finally, writing the data into the files of receiving processors. A detailed description of these routines is given in <ref> [CBH + 94] </ref>. 6 Experimental Results This section presents experimental results for three out-of-core applications: Laplace equation solver by the Ja-cobi iteration method, LU factorization with pivoting, and three dimensional red-black relaxation.
Reference: [CK92] <author> S. Carr and K. Kennedy. </author> <title> Compiler blockabilty of numerical algorithms. </title> <booktitle> Proc. of Supercomputing'92, </booktitle> <month> November </month> <year> 1992. </year>
Reference-contexts: Trivedi [Tri77a, Tri77b] shows that profitable opportunities for demand prefetching can be identified from a program's syntax. Highlights of managing other aspects of the memory hierarchy include: Allen and Kennedy on vector register allocation [AK87], Carr and Kennedy on compiler blocking of scientific codes <ref> [CK92] </ref>, and Mowry on software prefetching for cache [Mow94]. Related projects include disk-directed I/O, by David Kotz at Dartmouth College, in which I/O processors direct the transfer of data from disk to processors.
Reference: [dRBC93] <author> J. del Rosario, R. Bordawekar, and A. Choud-hary. </author> <title> Improved parallel I/O via a two-phase runtime access strategy. </title> <booktitle> In Proceedings of the Workshop on I/O in Parallel Computer Systems at IPPS '93, </booktitle> <month> April </month> <year> 1993. </year>
Reference-contexts: Even if the monitoring is going to be done off-line, the volume of data required for playback graphics can also affect running time adversely. Table 1 shows details of I/O requirements for some Grand Challenge applications <ref> [dRBC93] </ref>. In terms of the above discussion, temporary working storage generally comes from an out-of-core problem, archival and secondary storage usually come from checkpointing, and bandwidth requirements may be related to real-time I/O or checkpointing. <p> Each file will have to be redistributed into local files based on the out-of-core distribution. These routines use the two-phase access strategy <ref> [dRBC93] </ref> which will read the data from the input data file using the most optimal access pattern (which depends on how the data is stored on disks) and redistribute the data over the processors using the high speed processor interconnection network.
Reference: [FHK + 90] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, and C. Tseng. </author> <title> Fortran D language specifications. </title> <type> Technical Report COMP TR90-141, </type> <institution> Rice University, </institution> <year> 1990. </year>
Reference-contexts: Languages based on this principle are called data-parallel languages and include High Performance Fortran (HPF) [Hig93], Page 2 Vienna Fortran [ZBC + 92], and Fortran D <ref> [FHK + 90] </ref>. Our out-of-core approach builds on HPF. The DISTRIBUTE directive in HPF partitions an array among processors by specifying which elements of the array are mapped to each processor. <p> As is often the case, programmers suffer in advance of compiler writers. Our approach integrates compiler management of out-of-core data sets with the data-parallel approach of languages such as Fortran (HPF) [Hig93], Vienna Fortran [ZBC + 92], and Fortran D <ref> [FHK + 90] </ref>. Previous work on compiler improvements at the memory to disk interface starts with Abu-Sufah and Trivedi at the end of the 1970's. Abu-Sufah [AS79] demonstrates that applying loop distribution and loop fusion can reduce the space-time costs for numerical algorithms.
Reference: [Fox91] <author> G. Fox. </author> <title> The architecture of problems and portable parallel software systems. </title> <type> Technical Report SCCS-78b, </type> <institution> Northeast Parallel Architectures Center, Syracuse University, Syracuse, </institution> <address> NY 13244, </address> <year> 1991. </year>
Reference-contexts: In essence, data-parallel programs apply the same conceptual operations to all elements of large data structures. This form of parallelism occurs naturally in many scientific and engineering applications such as partial differential equation solvers and linear algebra routines <ref> [Fox91] </ref>. In these programs, a decomposition of the data domain exploits the inherent parallelism and adapts it to a particular machine. Compilers can use programmer-supplied decomposition patterns such as block and cyclic to partition computation, generate communication and synchronization, and guide optimization of the program.
Reference: [Hig93] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification. </title> <booktitle> Scientific Programming, </booktitle> <address> 2(1-2):1-170, </address> <year> 1993. </year>
Reference-contexts: Compilers can use programmer-supplied decomposition patterns such as block and cyclic to partition computation, generate communication and synchronization, and guide optimization of the program. Languages based on this principle are called data-parallel languages and include High Performance Fortran (HPF) <ref> [Hig93] </ref>, Page 2 Vienna Fortran [ZBC + 92], and Fortran D [FHK + 90]. Our out-of-core approach builds on HPF. The DISTRIBUTE directive in HPF partitions an array among processors by specifying which elements of the array are mapped to each processor. <p> As is often the case, programmers suffer in advance of compiler writers. Our approach integrates compiler management of out-of-core data sets with the data-parallel approach of languages such as Fortran (HPF) <ref> [Hig93] </ref>, Vienna Fortran [ZBC + 92], and Fortran D [FHK + 90]. Previous work on compiler improvements at the memory to disk interface starts with Abu-Sufah and Trivedi at the end of the 1970's.
Reference: [Kot94] <author> D. Kotz. </author> <title> Disk-Directed I/O for MIMD multiprocessors. </title> <type> Technical Report PCS-TR94-226, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: A final possibility applies if there is some processing capability at the I/O node itself, in which case disk-directed I/O can be used to send the data <ref> [Kot94] </ref>. The most important point to note here is that data needed by other processors is communicated while the slab is in memory when possible.
Reference: [Mow94] <author> T. Mowry. </author> <title> Tolerating Latency Through Software Controlled Data Prefetching. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Stanford University, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Highlights of managing other aspects of the memory hierarchy include: Allen and Kennedy on vector register allocation [AK87], Carr and Kennedy on compiler blocking of scientific codes [CK92], and Mowry on software prefetching for cache <ref> [Mow94] </ref>. Related projects include disk-directed I/O, by David Kotz at Dartmouth College, in which I/O processors direct the transfer of data from disk to processors. The Jovian framework for optimizing parallel I/O, being developed at the University of Mary-land, optimizes independent and collective I/O requests at run-time.
Reference: [Tri77a] <author> K. S. Trivedi. </author> <title> Prepaging and applications to the STAR-100 computer. </title> <booktitle> In Proceedings of the Symposium on High Speed Computer and Algorithm Organization, </booktitle> <pages> pages 435-446, </pages> <month> April </month> <year> 1977. </year>
Reference-contexts: Previous work on compiler improvements at the memory to disk interface starts with Abu-Sufah and Trivedi at the end of the 1970's. Abu-Sufah [AS79] demonstrates that applying loop distribution and loop fusion can reduce the space-time costs for numerical algorithms. Trivedi <ref> [Tri77a, Tri77b] </ref> shows that profitable opportunities for demand prefetching can be identified from a program's syntax.
Reference: [Tri77b] <author> K. S. Trivedi. </author> <title> On the paging performance of array algorithms. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-26(10):938-947, </volume> <month> October </month> <year> 1977. </year>
Reference-contexts: Previous work on compiler improvements at the memory to disk interface starts with Abu-Sufah and Trivedi at the end of the 1970's. Abu-Sufah [AS79] demonstrates that applying loop distribution and loop fusion can reduce the space-time costs for numerical algorithms. Trivedi <ref> [Tri77a, Tri77b] </ref> shows that profitable opportunities for demand prefetching can be identified from a program's syntax.
Reference: [ZBC + 92] <author> H. Zima, P. Brezany, B. Chapman, P. Mehrotra, and A. Schwald. </author> <title> Vienna Fortran a Language Specification. </title> <type> Technical Report ICASE Interim Report 21, </type> <institution> MS 132c, ICASE, NASA, </institution> <address> Hampton VA 23681, </address> <year> 1992. </year> <pages> Page 10 </pages>
Reference-contexts: Compilers can use programmer-supplied decomposition patterns such as block and cyclic to partition computation, generate communication and synchronization, and guide optimization of the program. Languages based on this principle are called data-parallel languages and include High Performance Fortran (HPF) [Hig93], Page 2 Vienna Fortran <ref> [ZBC + 92] </ref>, and Fortran D [FHK + 90]. Our out-of-core approach builds on HPF. The DISTRIBUTE directive in HPF partitions an array among processors by specifying which elements of the array are mapped to each processor. <p> As is often the case, programmers suffer in advance of compiler writers. Our approach integrates compiler management of out-of-core data sets with the data-parallel approach of languages such as Fortran (HPF) [Hig93], Vienna Fortran <ref> [ZBC + 92] </ref>, and Fortran D [FHK + 90]. Previous work on compiler improvements at the memory to disk interface starts with Abu-Sufah and Trivedi at the end of the 1970's. Abu-Sufah [AS79] demonstrates that applying loop distribution and loop fusion can reduce the space-time costs for numerical algorithms.
References-found: 13

