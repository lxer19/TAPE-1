URL: http://www.cse.psu.edu/~zha/papers/cubic.ps
Refering-URL: http://www.cse.psu.edu/~zha/papers.html
Root-URL: http://www.cse.psu.edu
Title: A CUBICALLY CONVERGENT PARALLELIZABLE METHOD FOR THE HERMITIAN EIGENVALUE PROBLEM  
Author: HONGYUAN ZHA AND ZHENYUE ZHANG 
Abstract: We propose a cubically convergent algorithm for computing the invariant subspaces of an Hermitian matrix. The building blocks of the algorithm are matrix-matrix multiplication and QR decomposition which are highly parallelizable. We present a detailed convergence analysis and explore the so-called mixed convergence phenomenon the understanding of which will be very helpful in devising convergence improvement. We also discuss a number of implementation details and demonstrate convergence properties of the algorithm using several numerical examples. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Auslander and A. Tsao. </author> <title> On parallelizable eigensolvers. </title> <booktitle> Advances in Applied Mathematics, </booktitle> <volume> 13 </volume> <pages> 253-261, </pages> <year> 1992. </year>
Reference-contexts: Recently, there have been much interest in developing algorithms for the Hermi-tian and/or general Eigenproblems that can be efficiently implemented on a variety of parallel computers <ref> [1, 2, 3, 6, 7, 8] </ref>. Those algorithms are iterative in nature and rely on a few operations such as matrix-matrix multiplication, QR decomposition, and matrix inversion as their building blocks at each iteration. <p> In LAPACK implementation, a block version is used and the Householder vectors are grouped into blocks columns of certain size. The above technique can also be modified to handle the block case. A Brief Comparison with ISDA. In <ref> [1, 3] </ref>, a simple and elegant method, invariant subspace decomposition algorithm (ISDA) was proposed to compute invariant subspaces of a symmetric matrix A. (some extension were also considered in [7].) The method first applies a linear transformation to A so that the spectrum of the transformed matrix ^ A is within <p> 3], a simple and elegant method, invariant subspace decomposition algorithm (ISDA) was proposed to compute invariant subspaces of a symmetric matrix A. (some extension were also considered in [7].) The method first applies a linear transformation to A so that the spectrum of the transformed matrix ^ A is within <ref> [0; 1] </ref>. <p> function such as B 1 (x) = 3x 2 2x 3 is repeatedly applied to ^ A A k+1 = B 1 (A k ); k = 0; 1; 2; : : : ; A 0 = ^ A; the idea is to map the eigenvalues of ^ A inside <ref> [0; 1=2) to 0 and those inside (1=2; 1] </ref> to 1. In the following we will just point out some differences of the two algorithm. Compared with Algorithm Cubic, ISDA is much simpler and only requires matrix-matrix multiplication in each iteration. <p> Certain preprocessing step will alleviate to some extent the slow convergence problem. But it will not complete go away. The linear transformation step of ISDA might creat a tighter cluster since it needs to squeeze the spectrum of A into <ref> [0; 1] </ref>. It seems that Algorithm Cubic can be readily extended to handle the SVD case, operating only on matrices of dimension n. It certainly worthwhile to develop methods that will combine the strength of both algorithms. Extension to Generalized Eigenvalue Problem.
Reference: [2] <author> Z. Bai, J. Demmel and M. Gu. </author> <title> Inverse free parallel spectral divide and conquer algorithms for nonsymmetric eigenproblems. </title> <type> Technical report CSD-94-793, </type> <institution> Computer Science Division, University of California at Berkeley, </institution> <year> 1994. </year>
Reference-contexts: Recently, there have been much interest in developing algorithms for the Hermi-tian and/or general Eigenproblems that can be efficiently implemented on a variety of parallel computers <ref> [1, 2, 3, 6, 7, 8] </ref>. Those algorithms are iterative in nature and rely on a few operations such as matrix-matrix multiplication, QR decomposition, and matrix inversion as their building blocks at each iteration. <p> An elegant method for computing those invariant subspaces is the matrix sign function method, a simple scheme of which is the following iteration 2 A j+1 = (A j + A 1 However, as pointed out in <ref> [2] </ref>, numerical difficulty will occur when A is ill-conditioned with respect to inversion. Another problem is that without explicitly forming the product A H A, the matrix sign function method can not readily be extended to the case when singular subspaces of a general matrix A is desirable. <p> For example, the above iteration scheme converges to the unitary factor in the polar decomposition of A [5]. The major inspiration for the work reported in this paper came from <ref> [8, 2] </ref>, where an quadratically convergent inverse-free method was proposed for computing the invariant subspaces of a general matrix. Actually the work got started when we tried to simplify fl Department of Computer Science and Engineering, The Pennsylvania State University, University Park, PA 16802, zha@cse.psu.edu. <p> A Parallelizable Method for Hermitian Eigenproblems 2 the algorithms in <ref> [8, 2] </ref> for the Hermitian case. Another inspiration is the Project PRISM reported in [3, 7] which forced us to think more carefully about a variety of implementation issues. A number of issues will be addressed in this paper. <p> Example 3. In this example, we demonstrate the possibility of employing a preprocessing technique to accelerate the convergence of Algorithm Cubic. Similar idea has also been used in <ref> [2, 3] </ref>.
Reference: [3] <author> C. Bischof, S. Huss-Lederman, X. Sun, A. Tsao, and T. Turnbull. </author> <title> Parallel studies of the Invariant Subspace Decomposition Approach for banded symmetric matrices. </title> <booktitle> Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> San Francisco, </address> <year> 1995. </year>
Reference-contexts: Recently, there have been much interest in developing algorithms for the Hermi-tian and/or general Eigenproblems that can be efficiently implemented on a variety of parallel computers <ref> [1, 2, 3, 6, 7, 8] </ref>. Those algorithms are iterative in nature and rely on a few operations such as matrix-matrix multiplication, QR decomposition, and matrix inversion as their building blocks at each iteration. <p> A Parallelizable Method for Hermitian Eigenproblems 2 the algorithms in [8, 2] for the Hermitian case. Another inspiration is the Project PRISM reported in <ref> [3, 7] </ref> which forced us to think more carefully about a variety of implementation issues. A number of issues will be addressed in this paper. We propose a cubically convergent algorithm for computing the orthogonal projections P jj&lt;1 and P jj&gt;1 . <p> In LAPACK implementation, a block version is used and the Householder vectors are grouped into blocks columns of certain size. The above technique can also be modified to handle the block case. A Brief Comparison with ISDA. In <ref> [1, 3] </ref>, a simple and elegant method, invariant subspace decomposition algorithm (ISDA) was proposed to compute invariant subspaces of a symmetric matrix A. (some extension were also considered in [7].) The method first applies a linear transformation to A so that the spectrum of the transformed matrix ^ A is within <p> Example 3. In this example, we demonstrate the possibility of employing a preprocessing technique to accelerate the convergence of Algorithm Cubic. Similar idea has also been used in <ref> [2, 3] </ref>.
Reference: [4] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, Maryland, 2nd edition, </address> <year> 1989. </year>
Reference-contexts: 1. Introduction. The Hermitian eigenproblem is one of the fundamental problems in matrix computations. A number of methods have been proposed in the past for computing the eigenvalues and eigenvectors of a dense Hermitian matrix, notably, the QR method, Cuppen's divide-and-conquer method, and Jacobi method <ref> [4, Chapter 8] </ref>. Recently, there have been much interest in developing algorithms for the Hermi-tian and/or general Eigenproblems that can be efficiently implemented on a variety of parallel computers [1, 2, 3, 6, 7, 8]. <p> Using the symmetry of A; H H k AH k ; and G H k G k . The storage space can be reduced to 3:5n 2 . The computation of the QR decomposition in Step. 2.1 can be done either using modified Gram-Schmidt method <ref> [4, Section 5.2.8] </ref> or Householder transformations. It is easy to see that Gram-Schmidt method allows the generation of the orthonormal matrix in place, one column at a time. Now we demonstrate that this is also the case with Householder transformations. Let F 2 C mfin with m n.
Reference: [5] <author> N.J. Higham. </author> <title> The matrix sign decomposition and its relation to the polar decomposition. </title> <journal> Linear Algebra and Its Applications, </journal> 212/213:3-20, 1994. 
Reference-contexts: For example, the above iteration scheme converges to the unitary factor in the polar decomposition of A <ref> [5] </ref>. The major inspiration for the work reported in this paper came from [8, 2], where an quadratically convergent inverse-free method was proposed for computing the invariant subspaces of a general matrix.
Reference: [6] <author> J. Howland. </author> <title> The sign matrix and the separation of matrix eigenvalues. </title> <journal> Linear Algebra and Its Applications, </journal> <volume> 49 </volume> <pages> 221-232, </pages> <year> 1983. </year>
Reference-contexts: Recently, there have been much interest in developing algorithms for the Hermi-tian and/or general Eigenproblems that can be efficiently implemented on a variety of parallel computers <ref> [1, 2, 3, 6, 7, 8] </ref>. Those algorithms are iterative in nature and rely on a few operations such as matrix-matrix multiplication, QR decomposition, and matrix inversion as their building blocks at each iteration.
Reference: [7] <author> S. Huss-Lederman, A. Tsao and T. Turnbull. </author> <title> A parallelizable eigensolver for real diagonalizable matrices with real eigenvalues. </title> <note> To appear in SIAM Journal of Scientific Computing. </note>
Reference-contexts: Recently, there have been much interest in developing algorithms for the Hermi-tian and/or general Eigenproblems that can be efficiently implemented on a variety of parallel computers <ref> [1, 2, 3, 6, 7, 8] </ref>. Those algorithms are iterative in nature and rely on a few operations such as matrix-matrix multiplication, QR decomposition, and matrix inversion as their building blocks at each iteration. <p> A Parallelizable Method for Hermitian Eigenproblems 2 the algorithms in [8, 2] for the Hermitian case. Another inspiration is the Project PRISM reported in <ref> [3, 7] </ref> which forced us to think more carefully about a variety of implementation issues. A number of issues will be addressed in this paper. We propose a cubically convergent algorithm for computing the orthogonal projections P jj&lt;1 and P jj&gt;1 . <p> The above technique can also be modified to handle the block case. A Brief Comparison with ISDA. In [1, 3], a simple and elegant method, invariant subspace decomposition algorithm (ISDA) was proposed to compute invariant subspaces of a symmetric matrix A. (some extension were also considered in <ref> [7] </ref>.) The method first applies a linear transformation to A so that the spectrum of the transformed matrix ^ A is within [0; 1].
Reference: [8] <author> A. Malyshev. </author> <title> Parallel algorithm for solving some spectral problems of linear algebra. </title> <journal> Linear Algebra and Its Applications, </journal> 188/189:489-520, 1993. 
Reference-contexts: Recently, there have been much interest in developing algorithms for the Hermi-tian and/or general Eigenproblems that can be efficiently implemented on a variety of parallel computers <ref> [1, 2, 3, 6, 7, 8] </ref>. Those algorithms are iterative in nature and rely on a few operations such as matrix-matrix multiplication, QR decomposition, and matrix inversion as their building blocks at each iteration. <p> For example, the above iteration scheme converges to the unitary factor in the polar decomposition of A [5]. The major inspiration for the work reported in this paper came from <ref> [8, 2] </ref>, where an quadratically convergent inverse-free method was proposed for computing the invariant subspaces of a general matrix. Actually the work got started when we tried to simplify fl Department of Computer Science and Engineering, The Pennsylvania State University, University Park, PA 16802, zha@cse.psu.edu. <p> A Parallelizable Method for Hermitian Eigenproblems 2 the algorithms in <ref> [8, 2] </ref> for the Hermitian case. Another inspiration is the Project PRISM reported in [3, 7] which forced us to think more carefully about a variety of implementation issues. A number of issues will be addressed in this paper.
Reference: [9] <author> ScaLAPACK, </author> <note> http://www.netlib.org/scalapack/index.html. </note>
Reference-contexts: Those operations have already been successfully implemented on many parallel architectures, and portable parallel library is now available for their efficient computation <ref> [9] </ref>. In this paper, we are concerned with the problem of computing the invariant subspace of an Hermitian matrix A corresponding to eigenvalues inside or outside the interval (1; 1). 1 We use P jj&lt;1 and P jj&gt;1 to denote the orthogonal projections onto these two invariant subspaces.
References-found: 9

