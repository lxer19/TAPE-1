URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/learn-2/schneide/psfiles/disstr.ps
Refering-URL: http://www.cs.cmu.edu/~schneide/papers.html
Root-URL: 
Title: Robot Skill Learning Through Intelligent Experimentation  
Author: by Jeff G. Schneider Professor Christopher M. Brown 
Degree: Submitted in Partial Fulfillment of the Requirements for the Degree Doctor of Philosophy Supervised by  
Date: 1995  
Address: Rochester, New York  
Affiliation: Department of Computer Science The College Arts and Sciences University of Rochester  
Abstract-found: 0
Intro-found: 1
Reference: [Aboaf et al., 1988] <author> E. Aboaf, C. Atkeson, and D. Reinkensmeyer, </author> <title> "Task-Level Robot Learning," </title> <booktitle> In Proceedings of the 88 IEEE Int. Conference on Robotics and Automation, </booktitle> <year> 1988. </year>
Reference-contexts: The result space may have any dimension, but we assume it to be lower dimension than the action space. Note that throwing a ball at a single target (as in <ref> [Aboaf et al., 1988; Branicky, 1991] </ref>) represents a zero dimension result space and corresponds to our concept of a task, but not a skill. We assume that a memory-based function approximator (in fact, table lookup with interpolation) is used to store the learned skill.
Reference: [Aboaf et al., 1989] <author> E. Aboaf, S. Drucker, and C. Atkeson, </author> <title> "Juggling a Tennis Ball More Accurately," </title> <booktitle> In Proceedings of the 89 IEEE Int. Conference on Robotics and Automation, </booktitle> <year> 1989. </year>
Reference: [Acheson and Mullin, 1993] <author> D. Acheson and T. Mullin, "Upside-down Pendulums," Nature, </author> <month> November </month> <year> 1993. </year>
Reference-contexts: Similarly, McGeer [McGeer, 1990] designed a walking robot that is stable without any sensing or actuation. It walks stably downhill powered only by gravity. Also, it is possible to stabilize an inverted pendulum open loop by driving it sinusoidally up and down <ref> [Acheson and Mullin, 1993] </ref>. These open-loop systems are interesting because they are robust without sensing. In each case the authors spent significant effort designing and analyzing their systems.
Reference: [Albus, 1981] <author> J. Albus, </author> <title> Brains, Behavior and Robotics, </title> <publisher> BYTE Press, </publisher> <year> 1981. </year>
Reference-contexts: Usually, G is one at s 0 and zero elsewhere. The addition of an output value to competitive learning would allow it to become a supervised method and another possible FA for robot skill learning. CMACs <ref> [Albus, 1981] </ref> are another function approximator similar to neural networks. Values are stored in a table whose dimensionality matches that of the input space. Outputs are determined by computing the sum of several table entries near the input vector.
Reference: [Arai et al., 1993] <author> F. Arai, L. Rong, and T. Fukuda, </author> <title> "Trajectory Control of a Flexible Plate Using Neural Network," </title> <booktitle> In Proceedings of the 93 IEEE Int. Conference on Robotics and Automation, </booktitle> <year> 1993. </year>
Reference: [Arimoto et al., 1984] <author> S. Arimoto, S. Kawamura, and F. Miyazaki, </author> <title> "Bettering Operation of Robots by Learning," </title> <journal> Journal of Robotic Systems, </journal> <volume> 1(2), </volume> <year> 1984. </year>
Reference-contexts: Generally, a nominal controller initially operates the system and a gradient descent rule is applied to improve future executions. The learning of robot dynamics for single trajectories can be done with this method <ref> [Craig, 1984; Arimoto et al., 1984; Kawamura et al., 1988] </ref>.
Reference: [Atkeson, 1991] <author> C. Atkeson, </author> <title> "Using Locally Weighted Regression for Robot Learning," </title> <booktitle> In Proceedings of the 91 IEEE Int. Conference on Robotics and Automation, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: The shape of the weighting function also affects the form of the resulting approximation. Examples of robot learning with local linear models can be found in <ref> [Atkeson, 1991; Schaal and Atkeson, 1993a; Schaal and Atkeson, 1994] </ref>.
Reference: [Atkeson, 1993] <author> C. Atkeson, </author> <title> "Using Local Trajectory Optimizers to Speed Up Global Optimization in Dynamic Programming," </title> <booktitle> In Advances in Neural Information Processing Systems (NIPS-6), </booktitle> <year> 1993. </year>
Reference-contexts: Moore has also proposed using variable resolution in the state space to speed up RL [Moore, 1991; Moore, 1993]. McCallum uses a memory-based method to glean more information from the robot's executions [McCallum, 1994]. Atkeson <ref> [Atkeson, 1993] </ref> looks at ways of using local models to do local optimizations in dynamic programming which could be incorporated into a more efficient form of reinforcement learning. Despite the work on improving efficiency, reinforcement learning does not seem appropriate for the problems we discuss later.
Reference: [Bar-Shalom and Fortmann, 1988] <author> Y. Bar-Shalom and T. Fortmann, </author> <title> Tracking and Data Association, </title> <publisher> Academic Press, </publisher> <year> 1988. </year>
Reference-contexts: This is often not the case. There are numerous methods of estimating states from imperfect sensor information, but we choose to present Kalman filters <ref> [Bar-Shalom and Fortmann, 1988; Gelb, 1974] </ref> because of the interesting parallel that can be drawn between them and the algorithm presented in Chapter 5. Fig. 2.7 shows how the Kalman filter is used for state estimation in a linear system with zero-mean, white, Gaussian noise.
Reference: [Barto et al., 1983] <author> A. Barto, R. Sutton, and C. Anderson, </author> <title> "Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <year> 1983. </year>
Reference: [Bellman, 1957] <author> R. Bellman, </author> <title> Dynamic Programming, </title> <publisher> Princeton University Press, </publisher> <year> 1957. </year>
Reference-contexts: The error is propagated around the loop back through time until the beginning of the execution is reached. The equations for doing the back-propagation are nearly identical to the equations used for some gradient descent based numerical methods. Another important method for solving optimal control problems is dynamic programming <ref> [Bellman, 1957; Bellman and Dreyfus, 1962] </ref>. To use it, the state and control spaces must be discretized. Then dynamic programming finds the sequence of (discrete) controls that minimizes the cost function.
Reference: [Bellman and Dreyfus, 1962] <author> R. Bellman and S. Dreyfus, </author> <title> Applied Dynamic Programming, </title> <publisher> Princeton University Press, </publisher> <year> 1962. </year> <month> 100 </month>
Reference-contexts: The error is propagated around the loop back through time until the beginning of the execution is reached. The equations for doing the back-propagation are nearly identical to the equations used for some gradient descent based numerical methods. Another important method for solving optimal control problems is dynamic programming <ref> [Bellman, 1957; Bellman and Dreyfus, 1962] </ref>. To use it, the state and control spaces must be discretized. Then dynamic programming finds the sequence of (discrete) controls that minimizes the cost function.
Reference: [Bianchini and Brown, 1992] <author> R. Bianchini and C. M. Brown, </author> <title> "Parallel Genetic Algorithms on Distributed-Memory Architectures," </title> <type> Technical Report 436, </type> <institution> University of Rochester, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: Each learning controller could "get its shot" at reference curves from the training set and the first one to succeed would try to pass "the answer" on to the others, as in other forms of distributed search (e.g. <ref> [Bianchini and Brown, 1992] </ref>). An important open problem is to find a suitable selection algorithm that chooses the active controller during learning. We use simple algorithms in our experiments, but it is likely that the selection algorithm is crucial for good performance.
Reference: [Bondi et al., 1988] <author> P. Bondi, G. Casalino, and L. Dambardella, </author> <title> "On the Iterative Learning Control Theory for Robotic Manipulators," </title> <journal> IEEE Journal of Robotics and Automation, </journal> <month> February </month> <year> 1988. </year>
Reference-contexts: The main drawback is that each desired trajectory has to be learned individually. In some cases it is possible to decompose learned trajectories into parts that can be reused for new trajectories <ref> [Bondi et al., 1988] </ref>. In general, eq. 2.19 can be replaced with: w k+1;i = w k;i ff @w; i where w represents the parameters of a controller.
Reference: [Branicky, 1991] <author> M. Branicky, </author> <title> "Task-Level Learning: Experiments and Extensions," </title> <booktitle> In Proceedings of the 91 IEEE Int. Conference on Robotics and Automation, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: The result space may have any dimension, but we assume it to be lower dimension than the action space. Note that throwing a ball at a single target (as in <ref> [Aboaf et al., 1988; Branicky, 1991] </ref>) represents a zero dimension result space and corresponds to our concept of a task, but not a skill. We assume that a memory-based function approximator (in fact, table lookup with interpolation) is used to store the learned skill.
Reference: [Cohn et al., 1994a] <author> D. Cohn, L. Atlas, and R. Ladner, </author> <title> "Improving Generalization with Active Learning," </title> <journal> Machine Learing, </journal> <volume> 15(2) </volume> <pages> 201-221, </pages> <year> 1994. </year>
Reference-contexts: An example of this work on classification problems is [Cohn et al., 1994b] and an example on continuous function approximation problems is <ref> [Cohn et al., 1994a] </ref>. This work is an important mathematical contribution to learning, but can not be directly applied to robot skill learning because of a subtle difference in problem formulations. Assume there is a function mapping x ! y.
Reference: [Cohn et al., 1994b] <author> D. Cohn, Z. Ghahramani, and M. Jordan, </author> <title> "Active Learning with Statistical Models," </title> <booktitle> In Advances in Neural Information Processing Systems 7, </booktitle> <year> 1994. </year>
Reference-contexts: An example of this work on classification problems is <ref> [Cohn et al., 1994b] </ref> and an example on continuous function approximation problems is [Cohn et al., 1994a]. This work is an important mathematical contribution to learning, but can not be directly applied to robot skill learning because of a subtle difference in problem formulations.
Reference: [Craig, 1984] <author> J. Craig, </author> <title> "Adaptive Control of Manipulators through Repeated Trials," </title> <booktitle> In Proceedings of the American Control Conference, </booktitle> <pages> pages 1566-1573, </pages> <year> 1984. </year>
Reference-contexts: Generally, a nominal controller initially operates the system and a gradient descent rule is applied to improve future executions. The learning of robot dynamics for single trajectories can be done with this method <ref> [Craig, 1984; Arimoto et al., 1984; Kawamura et al., 1988] </ref>.
Reference: [Craig, 1986] <author> J. Craig, </author> <title> Introduction to Robotics: Mechanics and Control, </title> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: Many researchers in artificial intelligence get inspiration from biological systems and ours comes from human athletic performance. No formal background in this area is required, however, to understand this dissertation. We assume the reader has a basic understanding of robotics concepts. A good introductory book is <ref> [Craig, 1986] </ref>. Relevant background information from the two remaining fields is the subject of this chapter. Most authors use the terms in skill learning loosely so we start this chapter with some definitions. The learning algorithms presented in this dissertation depend on the use of continuous function approximators.
Reference: [Dorf, 1986] <author> R. Dorf, </author> <title> Modern Control Systems, </title> <publisher> Addison-Wesley Publishing Co., </publisher> <address> 4th edition, </address> <year> 1986. </year>
Reference-contexts: If the functions f and g are linear in ~x and ~u, the plant is linear. If f and g are not functions of t, the plant is time-invariant. Control of linear, time-invariant systems is the best understood and can be read about in numerous text books <ref> [Dorf, 1986; Ogata, 1987] </ref> . The systems discussed in this dissertation are generally considered to be nonlinear time-invariant. PID control There are many types of linear control, but we will only describe a common method called PID (Proportional Integral Derivative) control. <p> Similarly, if the error is increasing the control will be further increased by the derivative term. The setting of the three PID constants for linear time-invariant plants can be read about in any linear control text <ref> [Dorf, 1986; Ogata, 1987] </ref>. Drawbacks of feedback control Feedback control uses its sensors to reduce tracking error, but its sensors can also be a liability. Sensors may produce significant measurement errors or even fail outright. <p> The greater the delay is, the slower the controller must move the arm in order for it to remain stable. A mathematical analysis of the stability of feedback systems can be found in any linear control text <ref> [Dorf, 1986; Ogata, 1987] </ref>. Negative feedback systems are those like the PID controller that subtract the actual from the desired state to get error and compute a control based only on the error.
Reference: [Gans, 1992] <author> R. Gans, </author> <title> "On the dynamics of a conservative elastica pendulum," </title> <journal> ASME Journal of Applied Mechanics, </journal> <pages> pages 425-430, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: The meter stick clearly deflects beyond the range of linear beam theory, as can be seen in Fig. 3.18 taken from an actual experiment. We suppose the deformation to be that of a dynamic elastica (large deformation, small strain) and have adopted the segmented model of Gans <ref> [Gans, 1992] </ref>, according to which the meter stick can be modeled by a set of rigid elements connected by linear torsional springs with spring constant N EI =L.
Reference: [Gelb, 1974] <author> A. Gelb, </author> <title> Applied Optimal Estimation, </title> <publisher> The MIT Press, </publisher> <year> 1974. </year>
Reference-contexts: This is often not the case. There are numerous methods of estimating states from imperfect sensor information, but we choose to present Kalman filters <ref> [Bar-Shalom and Fortmann, 1988; Gelb, 1974] </ref> because of the interesting parallel that can be drawn between them and the algorithm presented in Chapter 5. Fig. 2.7 shows how the Kalman filter is used for state estimation in a linear system with zero-mean, white, Gaussian noise.
Reference: [Gelfand et al., 1992] <author> J. Gelfand, M. Flax, R. Endres, S. Lane, and D. Handelman, </author> <title> "Acquisition of Automatic Activity Through Practice: Changes in Sensory Input," </title> <booktitle> In AAAI-92, </booktitle> <year> 1992. </year>
Reference-contexts: Handelman proposes a system that converts rule-based fuzzy controllers into appropriate gains for PID controllers [Handelman et al., 1991]. Gelfand consider several tasks that are easy to perform with the use of visual feedback, but assume it is preferable to do the tasks using only proprioceptive feedback <ref> [Gelfand et al., 1992] </ref>. His system is run under visual feedback while a second controller learns the correct mapping using proprioception. Eventually, the visual feedback controller can be turned off.
Reference: [Goldberg, 1989] <author> D. Goldberg, </author> <title> Genetic Algorithms in Search, Optimization, and Machine Learning, </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1989. </year>
Reference-contexts: Candidate actions are created from the most successful actions that have already been tried. Intuitively, it is similar to the search technique used by genetic algorithms <ref> [Goldberg, 1989] </ref>. The difference between GFI and genetic algorithms is that GFI candidate evaluations are considered much more expensive because they require execution on a real robot.
Reference: [Grossberg, 1976] <author> S. Grossberg, </author> <title> "Adaptive Pattern Classification and Universal Recod-ing: I. Parallel Development and Coding of Neural Feature Detectors," </title> <journal> Biological Cybernetics, </journal> <volume> 23 </volume> <pages> 121-134, </pages> <year> 1976. </year>
Reference-contexts: This results in a shift of nodes away from that area and is a type of interference as seen with back-propagation networks. The learning rule for the output weights is similar to the RBFs described above. Other neural networks Competitive learning <ref> [Rumelhart and Zipser, 1986; Grossberg, 1976] </ref> is an unsupervised learning technique that is strongly related to Kohonen networks. Unsupervised learning means there are only input data points, but no associated desired outputs. The learning algorithm just groups all the input data into clusters.
Reference: [Hager, 1992] <author> G. Hager, </author> <title> "Constraint Solving Methods and Sensor-based Decision-making," </title> <booktitle> In Proceedings of the 92 IEEE Int. Conference on Robotics and Automation, </booktitle> <year> 1992. </year>
Reference-contexts: This idea implies a cost of using sensors and several authors have looked at it. Hager discusses the cost of sensing from the viewpoint of sensor fusion <ref> [Hager and Mintz, 1991; Hager, 1992] </ref>. He attempts to recognize or model unknown objects and chooses sensing actions that maximize the amount of information obtained for the amount of cost incurred.
Reference: [Hager and Mintz, 1991] <author> G. Hager and M. Mintz, </author> <title> "Computational Methods for Task-directed Sensor Data Fusion and Sensor Planning," </title> <journal> International Journal of Robotics Research, </journal> <year> 1991. </year> <month> 101 </month>
Reference-contexts: This idea implies a cost of using sensors and several authors have looked at it. Hager discusses the cost of sensing from the viewpoint of sensor fusion <ref> [Hager and Mintz, 1991; Hager, 1992] </ref>. He attempts to recognize or model unknown objects and chooses sensing actions that maximize the amount of information obtained for the amount of cost incurred.
Reference: [Handelman et al., 1991] <author> D. Handelman, S. Lane, and J. Gelfand, </author> <title> "Goal-Directed Encoding of Task Knowledge for Robotic Skill Acquisition," </title> <booktitle> In IEEE International Symposium on Intelligent Control, </booktitle> <year> 1991. </year>
Reference-contexts: Handelman proposes a system that converts rule-based fuzzy controllers into appropriate gains for PID controllers <ref> [Handelman et al., 1991] </ref>. Gelfand consider several tasks that are easy to perform with the use of visual feedback, but assume it is preferable to do the tasks using only proprioceptive feedback [Gelfand et al., 1992].
Reference: [Hertz et al., 1991] <author> J. Hertz, A. Krogh, and R. Palmer, </author> <title> Introduction to the Theory of Neural Computation, </title> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: The resulting FA bears as much resemblance to the model-based methods described earlier and the Kohonen maps described below as it does to plain RBF function approximation. 2.2.3 Neural Networks The most common function approximators are probably those defined broadly as "neural nets." A good introductory text is <ref> [Hertz et al., 1991] </ref>. Here we review two types of these: back-propagation networks [Rumelhart et al., 1986; Werbos, 1974] and Kohonen maps [Kohonen, 1988; Ritter et al., 1992]. They are referred to as neural nets because aspects 17 of their architecture are inspired by biological neural models.
Reference: [Jacobs et al., 1991] <author> R. Jacobs, M. Jordan, S. Nowlan, and G. Hinton, </author> <title> "Adaptive Mixtures of Local Experts," </title> <booktitle> Neural Compuation, </booktitle> <year> 1991. </year>
Reference-contexts: One possibility is to select the one with the best overall performance. Another is to select different controllers dynamically during execution for different types of reference curves. This is similar to the "mixture of experts" idea in classification <ref> [Jacobs et al., 1991] </ref> or the combination of control laws in fuzzy control. 4.4.3 Experimental Results with Cooperation In the experiments described above, feedback error learning does better against some performance metrics (lower control effort) while the smoothing coach controller does better against others (faster tracking speed).
Reference: [Jeannerod, 1988] <author> M. Jeannerod, </author> <title> The Neural and Behavioural Organization of Goal-Directed Movements, </title> <publisher> Oxford University Press, </publisher> <year> 1988. </year>
Reference-contexts: In order to approximate athletic skills reasonably, the robot motor skills we consider have several important characteristics. The first is that they are nonlinear. Humans must deal with nonlinearities in their limb kinematics and dynamics and in their neural control system <ref> [Jeannerod, 1988] </ref>. Furthermore, the control of linear systems is already well understood and can often be done without learning. Second, the skills are parameterized. Possible parameters include speed of execution and distance traveled by the robot or a projectile.
Reference: [Jordan and Jacobs, 1990] <author> M. Jordan and R. Jacobs, </author> <title> "Learning to Control an Unstable System with Forward Modeling," </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <year> 1990. </year>
Reference-contexts: These are used to train a controller with a learning rule like eq. 2.20. Jordan and Jacobs show how a forward model with only partial information can still be used to train a good controller <ref> [Jordan and Jacobs, 1990] </ref>. They look at the problem of stabilizing an inverted pendulum (a restricted form of the problem in Chapter 4).
Reference: [Jordan and Rumelhart, 1992] <author> M. Jordan and D. Rumelhart, </author> <title> "Forward Models: Supervised Learning with a Distal Teacher," </title> <journal> Cognitive Science, </journal> <volume> 16, </volume> <year> 1992. </year>
Reference-contexts: It is also possible to obtain gradient information through a learned model of the plant. Jordan and Rumelhart use a back-propagation network to learn a forward model of the plant <ref> [Jordan and Rumelhart, 1992] </ref>. Then the derivatives relating controller parameters to performance error can be found by propagating the plant output back through the plant model with the usual neural network back-propagation equations. These are used to train a controller with a learning rule like eq. 2.20.
Reference: [Kawamura et al., 1988] <author> S. Kawamura, F. Miyazaki, and S. Arimoto, </author> <title> "Realization of Robot Motion Based on a Learning Method," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 18(1), </volume> <year> 1988. </year>
Reference-contexts: Generally, a nominal controller initially operates the system and a gradient descent rule is applied to improve future executions. The learning of robot dynamics for single trajectories can be done with this method <ref> [Craig, 1984; Arimoto et al., 1984; Kawamura et al., 1988] </ref>.
Reference: [Kawato et al., 1987] <author> M. Kawato, K. Furukawa, and R. Suzuki, </author> <title> "A Hierarchical Neural-Network Model for Control and Learning of Voluntary Movement," </title> <journal> Biological Cybernetics, </journal> <volume> 57, </volume> <year> 1987. </year>
Reference-contexts: PID control There are many types of linear control, but we will only describe a common method called PID (Proportional Integral Derivative) control. It is often used as the starting point for learning control systems including feedback error learning <ref> [Kawato et al., 1987] </ref> which is discussed in detail in Chapter 4. It is defined as follows. Compute the error as the difference between the desired and actual output of the plant, e = y d y. <p> It was proposed by Kawato and 65 his colleagues <ref> [Kawato et al., 1987; Kawato et al., 1988] </ref>. A block diagram is shown in Fig. 4.5. The learning control input is the reference signal.
Reference: [Kawato et al., 1988] <author> M. Kawato, Y. Uno, M. Isobe, and R. Suzuki, </author> <title> "A Hierarchical Neural Network Model for Voluntary Movement with Application to Robotics," </title> <journal> IEEE Control Systems Magazine, </journal> <volume> 8, </volume> <year> 1988. </year>
Reference-contexts: It was proposed by Kawato and 65 his colleagues <ref> [Kawato et al., 1987; Kawato et al., 1988] </ref>. A block diagram is shown in Fig. 4.5. The learning control input is the reference signal. <p> Doing so gives information about the current state of the plant by considering where it should have been previously. Refusing to look at the plant state is motivated by biological considerations <ref> [Kawato et al., 1988] </ref>, but it is probably not the best way to produce a learning controller when that state information is readily available. Therefore, we use plant state in the input space of the function approximator.
Reference: [Kirk, 1970] <author> D. Kirk, </author> <title> Optimal Control Theory An Introduction, </title> <publisher> Prentice-Hall, </publisher> <year> 1970. </year>
Reference: [Kohonen, 1988] <author> T. Kohonen, </author> <title> Self-organization and Associative Memory, </title> <publisher> Springer-Verlag, </publisher> <address> 2nd edition, </address> <year> 1988. </year>
Reference-contexts: Here we review two types of these: back-propagation networks [Rumelhart et al., 1986; Werbos, 1974] and Kohonen maps <ref> [Kohonen, 1988; Ritter et al., 1992] </ref>. They are referred to as neural nets because aspects 17 of their architecture are inspired by biological neural models. The models are formed by connecting numerous elementary computing units.
Reference: [Koza and Keane, 1990] <author> J. Koza and M. Keane, </author> <title> "Cart Centering and Broom Balancing by Genetically Breeding Populations of Control Strategy Programs," </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <year> 1990. </year>
Reference: [Kuperstein, 1988] <author> M. Kuperstein, </author> <title> "Implementation of an Adaptive Visually-guided Neural Controller for Single Postures," </title> <booktitle> In Proceedings of the American Control Conference, </booktitle> <volume> volume 3, </volume> <pages> pages 2282-2287, </pages> <year> 1988. </year>
Reference-contexts: Modifying RL to handle curve tracking is an interesting problem, but we do not address it in this dissertation. 28 2.4.2 Model learning Many learning control problems can be handled by the acquisition of a model for the robot. Kuperstein's INFANT <ref> [Kuperstein, 1988; Kuperstein and Rubenstein, 1989] </ref> is a simulated robot learning system that learns to reach for objects in its visual field. Since it is a kinematics learning problem, no intermediate sensing is required and this work fits into the Open Loop box in Fig. 1.1.
Reference: [Kuperstein and Rubenstein, 1989] <author> M. Kuperstein and J. Rubenstein, </author> <title> "Implementation of an Adaptive Neural Controller for Sensory-motor Coordination," </title> <journal> IEEE Control Systems Magazine, </journal> <volume> 9(3), </volume> <year> 1989. </year> <month> 102 </month>
Reference-contexts: Modifying RL to handle curve tracking is an interesting problem, but we do not address it in this dissertation. 28 2.4.2 Model learning Many learning control problems can be handled by the acquisition of a model for the robot. Kuperstein's INFANT <ref> [Kuperstein, 1988; Kuperstein and Rubenstein, 1989] </ref> is a simulated robot learning system that learns to reach for objects in its visual field. Since it is a kinematics learning problem, no intermediate sensing is required and this work fits into the Open Loop box in Fig. 1.1.
Reference: [Lane et al., 1992] <author> S. Lane, D. Handelman, and J. Gelfand, </author> <title> "Theory and Development of Higher-Order CMAC Neural Networks," </title> <journal> IEEE Control Systems Magazine, </journal> <month> April </month> <year> 1992. </year>
Reference-contexts: This is because the output is computed from values in the same size neighborhood regardless of how much experience has been accumulated in the table. There have been modifications to standard CMACs that use more sophisticated methods to combine table entries <ref> [Lane et al., 1992] </ref>. 20 2.3 Concepts from control theory It is impossible to do justice to the vast literature on control theory.
Reference: [Langley et al., 1994] <author> P. Langley, W. Iba, and J. Shrager, </author> <title> "Reactive and Automatic Behavior in Plan Execution," </title> <booktitle> In Proceedings of the Second International Conference on AI Planning Systems, </booktitle> <year> 1994. </year>
Reference-contexts: At the simplest level, the choice is between obtaining additional information through sensing, or trying to pick up the object with the current best estimate of which grasp will work. Langley looks at the cost of sensing in a more traditional control scenario <ref> [Langley et al., 1994] </ref>. He has a control system with a cost assigned to sensing and another assigned to tracking error. As the controller operates, the optimal sensing interval can be computed as a function of these costs, and the characteristics of the plant.
Reference: [Leitmann, 1981] <author> G. Leitmann, </author> <title> The Calculus of Variations and Optimal Control, </title> <publisher> Plenum Press, </publisher> <year> 1981. </year>
Reference-contexts: This tradeoff will be important when considering the inverted pendulum problem in Chapter 4. There are several ways to approach the solution to optimal control problems. A classical method is to use the calculus of variations <ref> [Leitmann, 1981] </ref>. This method is not discussed further as we assume our problems are too difficult or do not come with the necessary prior information to be solved by it. It is also possible to employ numerical techniques.
Reference: [Lewis, 1986] <author> F. Lewis, </author> <title> Optimal Control, </title> <publisher> John Wiley and Sons, </publisher> <year> 1986. </year>
Reference-contexts: Finding a controller to produce the true optimal control signal with respect to minimization of a cost equation such as eq 4.1 is the subject of optimal control theory (see <ref> [Lewis, 1986] </ref> for an overview). For our inverted pendulum problem that means choosing a control at each time step that results in a minimum value of the cost function within the constraints provided by eqs. 4.2 through 4.5.
Reference: [McCallum, 1994] <author> A. McCallum, </author> <title> "Instance-based State Identification for Reinforcement Learning," </title> <booktitle> In Advances in Neural Information Processings Systems 7 (NIPS-94), </booktitle> <year> 1994. </year>
Reference-contexts: Moore has also proposed using variable resolution in the state space to speed up RL [Moore, 1991; Moore, 1993]. McCallum uses a memory-based method to glean more information from the robot's executions <ref> [McCallum, 1994] </ref>. Atkeson [Atkeson, 1993] looks at ways of using local models to do local optimizations in dynamic programming which could be incorporated into a more efficient form of reinforcement learning. Despite the work on improving efficiency, reinforcement learning does not seem appropriate for the problems we discuss later.
Reference: [McGeer, 1990] <author> T. McGeer, </author> <title> "Passive Dynamic Walking," </title> <journal> International Journal of Robotics Research, </journal> <year> 1990. </year>
Reference-contexts: They devise systems that juggle completely open-loop by constructing paddles that automatically redirect the ball back toward a stable periodic trajectory with each hit they make. Similarly, McGeer <ref> [McGeer, 1990] </ref> designed a walking robot that is stable without any sensing or actuation. It walks stably downhill powered only by gravity. Also, it is possible to stabilize an inverted pendulum open loop by driving it sinusoidally up and down [Acheson and Mullin, 1993].
Reference: [Mel, 1990] <author> B. </author> <title> Mel, Connectionist Robot Motion Planning: A Neurally Inspired Approach to Visually Guided Reaching, </title> <publisher> Academic Press, </publisher> <year> 1990. </year>
Reference-contexts: If it is redundant, its inversion is not unique and the method employed by INFANT can not be used. Efficient learning of highly redundant mappings is discussed in Chapter 3. Examples of forward model learning are [Miller et al., 1987] and <ref> [Mel, 1990] </ref>. Miller is concerned with a robot that reaches for objects appearing in its visual field. Unlike Kuperstein, he learns a forward model first and then uses it to train a controller. Mel studies grasping for objects in the presence of obstacles.
Reference: [Miller et al., 1987] <author> W. Miller, F. Glanz, and L. Kraft, </author> <title> "Application of a General Learning Algorithm to the Control of Robotic Manipulators," </title> <journal> International Journal of Robotics Research, </journal> <volume> 6(2), </volume> <year> 1987. </year>
Reference-contexts: Unfortunately, inverse modeling only works when the forward mapping is not redundant. If it is redundant, its inversion is not unique and the method employed by INFANT can not be used. Efficient learning of highly redundant mappings is discussed in Chapter 3. Examples of forward model learning are <ref> [Miller et al., 1987] </ref> and [Mel, 1990]. Miller is concerned with a robot that reaches for objects appearing in its visual field. Unlike Kuperstein, he learns a forward model first and then uses it to train a controller. Mel studies grasping for objects in the presence of obstacles.
Reference: [Miyamoto et al., 1988] <author> H. Miyamoto, M. Kawato, T. Setoyama, and R. Suzuki, </author> <title> "Feedback-Error-Learning Neural Network for Trajectory Control of a Robotic Manipulator," Neural Networks, </title> <type> 1, </type> <year> 1988. </year>
Reference-contexts: Although other authors have reported success generalizing across different reference signals with feedback error learning <ref> [Miyamoto et al., 1988] </ref>, this may not always work. 66 10000 30000 50000 1.8 2 2.2 2.4 2.6 Average Speed (m/s) Control Effort Simple Controller 3 3 3 3 3 3 3 3 3 3 Feedback Error Learning + + + + + + + The input space to the function
Reference: [Moody and Darken, 1989] <author> J. Moody and C. Darken, </author> <title> "Fast Learning in Networks of Locally-Tuned Processing Units," </title> <booktitle> Neural Computation, </booktitle> <pages> pages 281-294, </pages> <year> 1989. </year>
Reference-contexts: The radial function G is hyperbolic: G (~x; ~x i ) = c 1 + (k ~x ~x i k =c 2 ) 2 (2.5) We modify the computation of eq. 2.4 as in <ref> [Moody and Darken, 1989] </ref>: ^ f (~x) = i=1 n X G (~x; ~x i ) Eq. 2.6 computes an average of the w i weighted by G. This is convenient because the w i can be interpreted as the actual value of the function near ~x i .
Reference: [Moore, 1990] <author> A. Moore, </author> <title> Efficient Memory-Based Learning for Robot Control, </title> <type> PhD thesis, </type> <institution> University of Cambridge, </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: Usually, the dimensions are rotated through cyclically down the tree. Kd-trees are important for efficient manipulation of the multi-dimensional data usually found in skill learning. Moore gives an excellent analysis of the use of kd-trees for learning in his thesis <ref> [Moore, 1990] </ref>. Even with kd-trees, the amount of memory required to implement a memory-based method may be prohibitive. The rest of the function approximators listed here do not increase their memory usage with each new presentation training data. <p> The following section describes an algorithm for open-loop robot skill learning. 3.2 The guided table fill in algorithm The Guided Fill In (GFI) algorithm (reported in [Schneider, 1994]) is a modification of the SAB control algorithm given in Moore's thesis <ref> [Moore, 1990] </ref>. He was also concerned with the efficient search of action spaces, but did not specifically address the issues of inverting a redundant action to task result mapping. <p> These actions may be in unexplored regions of the action space, but are likely to be more useful than completely random actions because of the way they were generated. Step 5 evaluates the probability of success as Moore suggests <ref> [Moore, 1990] </ref>. For each candidate action, p act , find the table entry whose action, p act near , is nearest the candidate action.
Reference: [Moore, 1991] <author> A. Moore, </author> <title> "Variable Resolution Dynamic Programming: Efficiently Learning Action Maps in Multivariate Real-Valued State-Spaces," </title> <booktitle> In Proceedings of the 8th International Workshop on Machine Learning, </booktitle> <year> 1991. </year>
Reference-contexts: Moore and Atkeson further investigate methods of determining what parts of the dynamic programming computation are most important to compute between real trials [Moore and Atkeson, 1993]. Moore has also proposed using variable resolution in the state space to speed up RL <ref> [Moore, 1991; Moore, 1993] </ref>. McCallum uses a memory-based method to glean more information from the robot's executions [McCallum, 1994]. Atkeson [Atkeson, 1993] looks at ways of using local models to do local optimizations in dynamic programming which could be incorporated into a more efficient form of reinforcement learning.
Reference: [Moore, 1993] <author> A. Moore, </author> <title> "The Parti-game Algorithm for Variable Resolution Reinforcement Learning in Multidimensional State Spaces," </title> <booktitle> In Advances in Neural Information Processings Systems 6 (NIPS-93), </booktitle> <year> 1993. </year>
Reference-contexts: Moore and Atkeson further investigate methods of determining what parts of the dynamic programming computation are most important to compute between real trials [Moore and Atkeson, 1993]. Moore has also proposed using variable resolution in the state space to speed up RL <ref> [Moore, 1991; Moore, 1993] </ref>. McCallum uses a memory-based method to glean more information from the robot's executions [McCallum, 1994]. Atkeson [Atkeson, 1993] looks at ways of using local models to do local optimizations in dynamic programming which could be incorporated into a more efficient form of reinforcement learning.
Reference: [Moore and Atkeson, 1993] <author> A. Moore and C. Atkeson, </author> <title> "Prioritized Sweeping: Reinforcement Learning with Less Data and Less Real Time," </title> <journal> Machine Learning, </journal> <volume> 13(1) </volume> <pages> 103-130, </pages> <year> 1993. </year> <month> 103 </month>
Reference-contexts: The real executions are most useful for obtaining the state transition information while the simulated ones cause more of the dynamic computation to be done. Moore and Atkeson further investigate methods of determining what parts of the dynamic programming computation are most important to compute between real trials <ref> [Moore and Atkeson, 1993] </ref>. Moore has also proposed using variable resolution in the state space to speed up RL [Moore, 1991; Moore, 1993]. McCallum uses a memory-based method to glean more information from the robot's executions [McCallum, 1994].
Reference: [Newton and Xu, 1993] <author> R. Newton and Y. Xu, </author> <title> "Real-Time Implementation of Neural Network Learning Control of a Flexible Space Manipulator," </title> <booktitle> In Proceedings of the 93 IEEE Int. Conference on Robotics and Automation, </booktitle> <year> 1993. </year>
Reference-contexts: One approach to the problem is to include information about the reference signal history with the function approximator input <ref> [Newton and Xu, 1993] </ref>. For example, one might use the reference signal from the previous step and the current reference signal to form the function approximator input. <p> Others have shown the benefits of adding information about earlier portions of the reference signal along with the current values <ref> [Newton and Xu, 1993] </ref>. Doing so gives information about the current state of the plant by considering where it should have been previously.
Reference: [Ogata, 1987] <author> K. Ogata, </author> <title> Discrete-Time Control Systems, </title> <publisher> Prentice-Hall, </publisher> <year> 1987. </year>
Reference-contexts: If the functions f and g are linear in ~x and ~u, the plant is linear. If f and g are not functions of t, the plant is time-invariant. Control of linear, time-invariant systems is the best understood and can be read about in numerous text books <ref> [Dorf, 1986; Ogata, 1987] </ref> . The systems discussed in this dissertation are generally considered to be nonlinear time-invariant. PID control There are many types of linear control, but we will only describe a common method called PID (Proportional Integral Derivative) control. <p> Similarly, if the error is increasing the control will be further increased by the derivative term. The setting of the three PID constants for linear time-invariant plants can be read about in any linear control text <ref> [Dorf, 1986; Ogata, 1987] </ref>. Drawbacks of feedback control Feedback control uses its sensors to reduce tracking error, but its sensors can also be a liability. Sensors may produce significant measurement errors or even fail outright. <p> The greater the delay is, the slower the controller must move the arm in order for it to remain stable. A mathematical analysis of the stability of feedback systems can be found in any linear control text <ref> [Dorf, 1986; Ogata, 1987] </ref>. Negative feedback systems are those like the PID controller that subtract the actual from the desired state to get error and compute a control based only on the error.
Reference: [Pearlmutter, 1989] <author> B. Pearlmutter, </author> <title> "Learning State Space Trajectories in Recurrent Neural Networks," </title> <booktitle> Neural Computation, </booktitle> <pages> pages 263-269, </pages> <year> 1989. </year>
Reference-contexts: This method is not discussed further as we assume our problems are too difficult or do not come with the necessary prior information to be solved by it. It is also possible to employ numerical techniques. An interesting method known as back-propagation-in-time <ref> [Pearlmutter, 1989; Pineda, 1989; Simard, 1991] </ref> uses the methods of back-propagation to implement a kind of numerical solution. Fig. 2.6 shows one implementation for this type of solution. The neurons are the same as the ones used in standard back-propagation.
Reference: [Pineda, 1989] <author> F. Pineda, </author> <title> "Recurrent Backpropagation and the Dyanamical Approach to Adaptive Neural Computation," </title> <booktitle> Neural Computation, </booktitle> <pages> pages 161-172, </pages> <year> 1989. </year>
Reference-contexts: This method is not discussed further as we assume our problems are too difficult or do not come with the necessary prior information to be solved by it. It is also possible to employ numerical techniques. An interesting method known as back-propagation-in-time <ref> [Pearlmutter, 1989; Pineda, 1989; Simard, 1991] </ref> uses the methods of back-propagation to implement a kind of numerical solution. Fig. 2.6 shows one implementation for this type of solution. The neurons are the same as the ones used in standard back-propagation.
Reference: [Poggio and Girosi, 1989] <author> T. Poggio and F. Girosi, </author> <title> "A Theory of Networks for Approximation and Learning," </title> <type> Technical Report 1140, </type> <institution> MIT AI Lab, </institution> <year> 1989. </year>
Reference: [Poggio and Girosi, 1990] <author> T. Poggio and F. Girosi, </author> <title> "Regularization Algorithms for Learning that are Equivalent to Multilayer Networks," </title> <booktitle> Science, </booktitle> <pages> pages 978-982, </pages> <month> Febru-ary </month> <year> 1990. </year>
Reference: [Pomerleau, 1989] <author> D. Pomerleau, "ALVINN: </author> <title> An Autonomous Land Vehicle in a Neural Network," </title> <booktitle> In Advances in Neural Information Processing Systems 1, </booktitle> <year> 1989. </year>
Reference-contexts: Whether these systems suffer the same problems as local closed loop learners usually depends on the complexity of the rules given by the system designer. Ridding a control system of an incorrect behavior is an interesting problem that has been encountered by other learning systems (ALVINN <ref> [Pomerleau, 1989] </ref> for example). The use of a high level coach with off-line learning addresses this issue also. At the end of an execution, the performance may be evaluated. If it is unsatisfactory the trace can be discarded completely without doing any learning.
Reference: [Press et al., 1992] <author> W. Press, S. Teukolsky, W. Vetterling, and B. Flannery, </author> <title> Numerical Recipes in C, </title> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: We integrate this system in time using the adaptive fourth order Runge-Kutta scheme given in <ref> [Press et al., 1992] </ref>. We typically integrate adaptively over 100 identical subintervals so that we can accumulate simulated data at fixed, repeatable intervals of time. Our simulations use a 2 second overall interval with subintervals of 20 ms. <p> We started with the 100 trial learning run on our PUMA described previously. Five throws from that set were chosen to calibrate the simulation model. The original model performed those throws with an average error of 33 cm. We use Powell's algorithm from <ref> [Press et al., 1992] </ref> to minimize the average squared error as a function of the force of gravity, the mass of the ball, and the spring constant in the flexible beam.
Reference: [Raibert, 1977] <author> M. Raibert, </author> <title> "Analytical equations vs table look-up for manipulation: a unifying concept," </title> <booktitle> In Proceedings of the IEEE Conference on Decision and Control, </booktitle> <year> 1977. </year>
Reference-contexts: Raibert shows how dimensions in a function approximator can be traded off for extra parameters in a local model for the problem of learning robot dynamics <ref> [Raibert, 1977; Raibert, 1978] </ref>. For example, assume a robot arm needs to learn torque as a function of , _ , and . One option is to use a function approximator with three dimensions.
Reference: [Raibert, 1978] <author> M. Raibert, </author> <title> "A model for sensorimotor control and learning," </title> <journal> Biological Cybernetics, </journal> <year> 1978. </year>
Reference-contexts: Raibert shows how dimensions in a function approximator can be traded off for extra parameters in a local model for the problem of learning robot dynamics <ref> [Raibert, 1977; Raibert, 1978] </ref>. For example, assume a robot arm needs to learn torque as a function of , _ , and . One option is to use a function approximator with three dimensions.
Reference: [Rimey and Brown, 1994] <author> R. Rimey and C. Brown, </author> <title> "Control of Selective Perception Using Bayes Nets and Decision Theory," </title> <journal> International Journal of Computer Vision, </journal> <year> 1994. </year>
Reference-contexts: He attempts to recognize or model unknown objects and chooses sensing actions that maximize the amount of information obtained for the amount of cost incurred. Rimey and Brown use decision theory to choose the sensing actions that will answer a question about a visual scene with minimal cost <ref> [Rimey and Brown, 1994] </ref>. Wixson and Ballard use cost considerations to guide visual search for objects [Wixson and Ballard, 1994]. Tan [Tan, 1992; Tan, 1993] includes sensing with other actions in the framework of reinforcement learning.
Reference: [Ritter et al., 1992] <author> H. Ritter, T. Martinetz, and K. Schulten, </author> <title> Neural Computation and Self-Organizing Maps, </title> <publisher> Addison-Wesley, </publisher> <year> 1992. </year>
Reference-contexts: Here we review two types of these: back-propagation networks [Rumelhart et al., 1986; Werbos, 1974] and Kohonen maps <ref> [Kohonen, 1988; Ritter et al., 1992] </ref>. They are referred to as neural nets because aspects 17 of their architecture are inspired by biological neural models. The models are formed by connecting numerous elementary computing units. <p> Kohonen maps with outputs Here we describe a Kohonen map that has been augmented with trainable outputs as proposed in <ref> [Ritter et al., 1992] </ref>. In the description to follow, the equations concerning ~w out are from Ritter, while the rest are from the original work by Kohonen. Fig. 2.4 shows the architecture. The individual neuron does not perform a computation the way back-propagation neurons do.
Reference: [Rizzi and Koditschek, 1992] <author> A. Rizzi and D. Koditschek, </author> <title> "Progress in Spatial Robot Juggling," </title> <booktitle> In Proceedings of the 92 IEEE Int. Conference on Robotics and Automation, </booktitle> <year> 1992. </year>
Reference: [Rizzi and Koditschek, 1993] <author> A. Rizzi and D. Koditschek, </author> <title> "Further Progress in Robot Juggling: The Spatial Two-Juggle," </title> <booktitle> In Proceedings of the 93 IEEE Int. Conference on Robotics and Automation, </booktitle> <year> 1993. </year>
Reference: [Rumelhart et al., 1986] <author> D. Rumelhart, G. Hinton, and R. Williams, </author> <title> Learning Internal Representations by Error Propagation, volume 1, chapter 8, </title> <publisher> MIT Press, </publisher> <year> 1986. </year> <month> 104 </month>
Reference-contexts: Here we review two types of these: back-propagation networks <ref> [Rumelhart et al., 1986; Werbos, 1974] </ref> and Kohonen maps [Kohonen, 1988; Ritter et al., 1992]. They are referred to as neural nets because aspects 17 of their architecture are inspired by biological neural models. The models are formed by connecting numerous elementary computing units.
Reference: [Rumelhart and Zipser, 1986] <author> D. Rumelhart and D. Zipser, </author> <title> Feature Discovery by Competitive Learning, </title> <booktitle> volume 1, </booktitle> <pages> pages 151-193, </pages> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: This results in a shift of nodes away from that area and is a type of interference as seen with back-propagation networks. The learning rule for the output weights is similar to the RBFs described above. Other neural networks Competitive learning <ref> [Rumelhart and Zipser, 1986; Grossberg, 1976] </ref> is an unsupervised learning technique that is strongly related to Kohonen networks. Unsupervised learning means there are only input data points, but no associated desired outputs. The learning algorithm just groups all the input data into clusters.
Reference: [Sakaguchi et al., 1993] <author> T. Sakaguchi, M. Fujita, H. Watanabe, and F. Miyazaki, </author> <title> "Motion Planning and Control for a Robot Performer," </title> <booktitle> In Proceedings of the 93 IEEE Int. Conference on Robotics and Automation, </booktitle> <year> 1993. </year>
Reference: [Salganicoff, 1992] <author> M. Salganicoff, </author> <title> Learning and Forgetting for Perception-Action: A Projection-Pursuit and Density Adaptive Approach, </title> <type> PhD thesis, </type> <institution> University of Penn-sylvania, </institution> <year> 1992. </year>
Reference-contexts: As the system learns and changes itself to approximate a time-varying function, old data becomes irrelevant and should be discarded. We say the FA must "forget" the old data as it becomes irrelevant. A good study of these "forgetting" algorithms can be found in <ref> [Salganicoff, 1992; Salganicoff, 1993] </ref>. The rest of the function approximators discussed below solve the forgetting problem implicitly because they use a finite amount of memory.
Reference: [Salganicoff, 1993] <author> M. Salganicoff, </author> <title> "Density-Adaptive Learning and Forgetting," </title> <booktitle> In Tenth International Conference on Machine Learning, </booktitle> <year> 1993. </year>
Reference-contexts: As the system learns and changes itself to approximate a time-varying function, old data becomes irrelevant and should be discarded. We say the FA must "forget" the old data as it becomes irrelevant. A good study of these "forgetting" algorithms can be found in <ref> [Salganicoff, 1992; Salganicoff, 1993] </ref>. The rest of the function approximators discussed below solve the forgetting problem implicitly because they use a finite amount of memory.
Reference: [Sanger, 1994] <author> T. Sanger, </author> <title> "Neural Network Learning Control of Robot Manipulators using Gradually Increasing Task Difficulty," </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 10 </volume> <pages> 323-333, </pages> <year> 1994. </year>
Reference-contexts: Gradient descent is dangerous if the surface has many local minima as they will prevent the system from converging to the best controller. All of the experiments described earlier in Section 2.4 were designed so there were no sub-optimal local minima. Sanger <ref> [Sanger, 1994] </ref> considers parameterizing tasks with a difficulty parameter as a method of removing undesirable local minima from learning control problems. One example is curve tracing problems where the desired speed can be adjusted. At slow 30 speeds the dynamics are simple and a linear controller works well enough.
Reference: [Schaal and Atkeson, 1993a] <author> S. Schaal and C. Atkeson, </author> <title> "Assessing the Quality of Learned Local Models," </title> <booktitle> In Advances in Neural Information Processing Systems (NIPS-6), </booktitle> <year> 1993. </year>
Reference-contexts: The shape of the weighting function also affects the form of the resulting approximation. Examples of robot learning with local linear models can be found in <ref> [Atkeson, 1991; Schaal and Atkeson, 1993a; Schaal and Atkeson, 1994] </ref>.
Reference: [Schaal and Atkeson, 1993b] <author> S. Schaal and C. Atkeson, </author> <title> "Open Loop Stable Control Strategies for Robot Juggling," </title> <booktitle> In Proceedings of the 93 IEEE Int. Conf. on Robotics and Automation, </booktitle> <year> 1993. </year>
Reference: [Schaal and Atkeson, 1994] <author> S. Schaal and C. Atkeson, </author> <title> "Robot Juggling: An Implementation of Memory-Based Learning," </title> <journal> IEEE Control Systems Magazine, </journal> <volume> 14(1), </volume> <year> 1994. </year>
Reference-contexts: The shape of the weighting function also affects the form of the resulting approximation. Examples of robot learning with local linear models can be found in <ref> [Atkeson, 1991; Schaal and Atkeson, 1993a; Schaal and Atkeson, 1994] </ref>.
Reference: [Schneider, 1994] <author> J. Schneider, </author> <title> "High Dimension Action Spaces in Robot Skill Learning," </title> <booktitle> In Twelfth National Conference on Artificial Intelligence (AAAI-94), </booktitle> <year> 1994. </year>
Reference-contexts: Each trial on the real robot is expensive so it is important to choose actions intelligently. The following section describes an algorithm for open-loop robot skill learning. 3.2 The guided table fill in algorithm The Guided Fill In (GFI) algorithm (reported in <ref> [Schneider, 1994] </ref>) is a modification of the SAB control algorithm given in Moore's thesis [Moore, 1990]. He was also concerned with the efficient search of action spaces, but did not specifically address the issues of inverting a redundant action to task result mapping.
Reference: [Schneider and Brown, 1992] <author> J. Schneider and C. Brown, </author> <title> "Robot Skill Learning and the Effects of Basis Function Choice," </title> <type> Technical Report 437, </type> <institution> University of Rochester, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: Later, the average value of the cost function over the task parameter space will be referred to as the performance and the two terms will be referred to separately as error and effort. For some skills, the dimension of the plant command space can be reduced <ref> [Schneider and Brown, 1992] </ref>. In this example, the positions and velocities of the joints as the ball is released completely determine where it will land.
Reference: [Schneider and Brown, 1993] <author> J. Schneider and C. Brown, </author> <title> "Robot Skill Learning, Basis Functions, and Control Regimes," </title> <booktitle> In Proceedings of the 93 IEEE Int. Conf. on Robotics and Automation, </booktitle> <pages> pages 403-410, </pages> <year> 1993. </year>
Reference-contexts: In skill learning the goal space is not the same as the query space. This is particularly devastating if the mapping is redundant (the query space has much higher dimensionality than the goal space). The way a problem is represented can have a large effect on skill learning <ref> [Schneider and Brown, 1993] </ref>. By altering the representation of the set of actions it is possible to expand (or shrink) the volume of "useful" actions and thus make learning easier (or more difficult). Even choice of control signal primitives is important. <p> Velocity (deg/time step) A 3 3 3 B + + + -15 -5 5 15 Time Step (200 ms each) Joint Velocity (deg/time step) Joint Velocity (deg/time step) Joint 3 3 3 3 3 Joint 5 + + + + + 50 3.4.1 Basis functions for the PUMA Other work <ref> [Schneider and Brown, 1993] </ref> has shown the benefits of having a good basis function for a skill learning robot. Specifically, basis functions can restrict the action space search to only those areas that are most relevant to the task.
Reference: [Schneider and Brown, 1994] <author> J. Schneider and C. Brown, </author> <title> "Task Level Training Signals for Learning Controllers," </title> <booktitle> In 9th IEEE International Symposium on Intelligent Control, </booktitle> <pages> pages 45-50, </pages> <year> 1994. </year>
Reference-contexts: perform on the inverted pendulum problem. 4.3.1 Global Coaching 1 This is completely separate from the problem of control in the presence of delay which can also force a controller to predict and consider the future effects of its actions. 70 Fig. 4.10 shows a modified learning controller (proposed in <ref> [Schneider and Brown, 1994] </ref>) that addresses the potential problems with basic feedback error learning. Others have shown the benefits of adding information about earlier portions of the reference signal along with the current values [Newton and Xu, 1993].
Reference: [Schneider and Gans, 1994] <author> J. Schneider and R. Gans, </author> <title> "Efficient Search for Robot Skill Learning: Simulation and Reality," </title> <booktitle> In Proceedings of the IEEE International Conference on Intelligent Robots and Systems, </booktitle> <year> 1994. </year>
Reference-contexts: In this section we investigate what improvements might be made if a model is available. We have constructed a dynamic model to simulate the ball-throwing robot, with a view to replacing most of the actual robot cycles with simulated robot cycles <ref> [Schneider and Gans, 1994] </ref>. This makes the learning process faster and cheaper. There are two aspects of the simulation that make it nontrivial. The first is the large displacement deformation exhibited by the meter stick. The second is the lack of a formal release mechanism for the ball. <p> We will discuss these two questions in turn. Following the description of our ball throwing model, we propose an algorithm that uses real executions to calibrate the model and simulated executions to replace some of the real ones. 51 3.5.1 Implementation The following description is from <ref> [Schneider and Gans, 1994] </ref>. The meter stick clearly deflects beyond the range of linear beam theory, as can be seen in Fig. 3.18 taken from an actual experiment.
Reference: [Simard, 1991] <author> P. Simard, </author> <title> Learning State Space Dynamics in Recurrent Networks, </title> <type> PhD thesis, </type> <institution> University of Rochester, </institution> <month> June </month> <year> 1991. </year> <month> 105 </month>
Reference-contexts: This method is not discussed further as we assume our problems are too difficult or do not come with the necessary prior information to be solved by it. It is also possible to employ numerical techniques. An interesting method known as back-propagation-in-time <ref> [Pearlmutter, 1989; Pineda, 1989; Simard, 1991] </ref> uses the methods of back-propagation to implement a kind of numerical solution. Fig. 2.6 shows one implementation for this type of solution. The neurons are the same as the ones used in standard back-propagation.
Reference: [Sutton, 1984] <author> R. Sutton, </author> <title> Temporal Credit Assignment in Reinforcement Learning, </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <year> 1984. </year>
Reference: [Sutton, 1990] <author> R. Sutton, </author> <title> "First Results with DYNA, an Intergrated Architecture for Learning, Planning, and Reacting," </title> <booktitle> In AAAI Spring Symposium on Planning in Uncertain, Unpredictable, or Changing Environments, </booktitle> <year> 1990. </year>
Reference-contexts: Sutton proposed DYNA which records the state transitions of individual steps and then uses them to perform simulated executions along with real ones <ref> [Sutton, 1990] </ref>. The real executions are most useful for obtaining the state transition information while the simulated ones cause more of the dynamic computation to be done. <p> We propose the following algorithm (it is similar to the idea of combining real and "mental" practice as seen in <ref> [Sutton, 1990] </ref>, for example): 1. Perform some learning trials on the real robot using the Guided Fill In algorithm of sec. 3.2. 2. Use the results of these trials to improve the model of the robot. 3. Perform some learning trials in simulation. 4.
Reference: [Tan, 1992] <author> M. Tan, </author> <title> "Learning a Cost Sensitive Internal Representation for Reinforcement Learning," </title> <booktitle> In Proceedings of the Eigth International Workshop on Machine Learning, </booktitle> <pages> pages 358-362, </pages> <year> 1992. </year>
Reference-contexts: Rimey and Brown use decision theory to choose the sensing actions that will answer a question about a visual scene with minimal cost [Rimey and Brown, 1994]. Wixson and Ballard use cost considerations to guide visual search for objects [Wixson and Ballard, 1994]. Tan <ref> [Tan, 1992; Tan, 1993] </ref> includes sensing with other actions in the framework of reinforcement learning. One task is to grasp an unknown object and his choice of actions includes various sensing and grasping actions.
Reference: [Tan, 1993] <author> M. Tan, </author> <title> "Cost-Sensitive Learning of Classification Knowledge and Its Applications in Robotics," </title> <journal> Machine Learning, </journal> <volume> 13(1) </volume> <pages> 7-32, </pages> <year> 1993. </year>
Reference-contexts: Rimey and Brown use decision theory to choose the sensing actions that will answer a question about a visual scene with minimal cost [Rimey and Brown, 1994]. Wixson and Ballard use cost considerations to guide visual search for objects [Wixson and Ballard, 1994]. Tan <ref> [Tan, 1992; Tan, 1993] </ref> includes sensing with other actions in the framework of reinforcement learning. One task is to grasp an unknown object and his choice of actions includes various sensing and grasping actions.
Reference: [Watkins, 1989] <author> C. Watkins, </author> <title> Learning from Delayed Rewards, </title> <type> PhD thesis, </type> <institution> Cambridge University, </institution> <year> 1989. </year>
Reference-contexts: They also demonstrate how GFI may be used to learn the easy part of a task first and the performance benefits of doing so. Many sequential task learners <ref> [Watkins, 1989] </ref> must operate closed loop because of the exponential explosion of action possibilities that occurs when a sequence of actions is considered. The results presented here demonstrate one way to deal with the large number of potential actions and thus offers an open-loop alternative for approaching these problems.
Reference: [Watkins and Dayan, 1992] <author> C. Watkins and P. Dayan, </author> <title> "Q Learning," </title> <booktitle> Machine Learning, </booktitle> <pages> pages 279-292, </pages> <year> 1992. </year>
Reference-contexts: Therefore, the usual dynamic programming computation can only be done for single steps as the system chooses an action and moves to a new state. In the limit that the agent tries every action from every state numerous times, the computation converges just as dynamic programming would <ref> [Watkins and Dayan, 1992] </ref>. <p> One problem is the way it approximates dynamic programming. It has no model of the state space and only updates a single step in its computation for each single step it makes. Since it has to take every step numerous times to ensure convergence <ref> [Watkins and Dayan, 1992] </ref>, the number of executions can be prohibitive in real robotic systems. To date RL has been applied to task learning, but not to skill learning. The problem is that RL normally assumes a fixed goal (i.e. a single desired task).
Reference: [Werbos, 1974] <author> P. Werbos, </author> <title> "Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences," </title> <type> Master's thesis, </type> <institution> Harvard University, </institution> <year> 1974. </year>
Reference-contexts: Here we review two types of these: back-propagation networks <ref> [Rumelhart et al., 1986; Werbos, 1974] </ref> and Kohonen maps [Kohonen, 1988; Ritter et al., 1992]. They are referred to as neural nets because aspects 17 of their architecture are inspired by biological neural models. The models are formed by connecting numerous elementary computing units.
Reference: [White and Jordan, 1992] <author> D. White and M. Jordan, </author> <title> "Optimal Control: A Foundation for Intelligent Control," In Handbook of Intelligent Control, chapter 6. </title> <publisher> Van Nostrand Reinhold, </publisher> <year> 1992. </year>
Reference-contexts: There are numerous good text books on the topic ([Kirk, 1970; Lewis, 1986], for example). A good entry point into the literature on the use of optimal control in intelligent control is <ref> [White and Jordan, 1992] </ref>.
Reference: [White and Sofge, 1992] <author> D. White and D. Sofge, </author> <title> Handbook of Intelligent Control, </title> <publisher> Van Nostrand Reinhold, </publisher> <year> 1992. </year>
Reference-contexts: For example, one of the benefits of fuzzy controllers (see <ref> [White and Sofge, 1992] </ref> for an introduction) is that it may be simpler to construct control laws using common sense rules than with other methods. Handelman proposes a system that converts rule-based fuzzy controllers into appropriate gains for PID controllers [Handelman et al., 1991].
Reference: [Wixson and Ballard, 1994] <author> L. Wixson and D. Ballard, </author> <title> "Using Intermediate Objects to Improve the Efficiency of Visual Search," </title> <journal> International Journal of Computer Vision, 1994. </journal> <volume> 106 107 </volume>
Reference-contexts: Rimey and Brown use decision theory to choose the sensing actions that will answer a question about a visual scene with minimal cost [Rimey and Brown, 1994]. Wixson and Ballard use cost considerations to guide visual search for objects <ref> [Wixson and Ballard, 1994] </ref>. Tan [Tan, 1992; Tan, 1993] includes sensing with other actions in the framework of reinforcement learning. One task is to grasp an unknown object and his choice of actions includes various sensing and grasping actions.
References-found: 94

