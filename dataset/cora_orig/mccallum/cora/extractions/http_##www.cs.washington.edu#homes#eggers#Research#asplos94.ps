URL: http://www.cs.washington.edu/homes/eggers/Research/asplos94.ps
Refering-URL: http://www.cs.washington.edu/homes/eggers/Research/multithread.html
Root-URL: http://www.cs.washington.edu
Email: fradhika,eggersg@cs.washington.edu  
Title: The Effectiveness of Multiple Hardware Contexts  
Author: Radhika Thekkath and Susan J. Eggers 
Address: Seattle, WA 98195  
Affiliation: Department of Computer Science and Engineering, FR-35 University of Washington  
Abstract: Multithreaded processors are used to tolerate long memory latencies. By executing threads loaded in multiple hardware contexts, an otherwise idle processor can keep busy, thus increasing its utilization. However, the larger size of a multi-thread working set can have a negative effect on cache conflict misses. In this paper we evaluate the two phenomena together, examining their combined effect on execution time. The usefulness of multiple hardware contexts depends on: program data locality, cache organization and degree of multiprocessing. Multiple hardware contexts are most effective on programs that have been optimized for data locality. For these programs, execution time dropped with increasing contexts, over widely varying architectures. With unoptimized applications, multiple contexts had limited value.The best performance was seen with only two contexts, and only on uniprocessors and small multiprocessors. The behavior of the unoptimized applications changed more noticeably with variations in cache associativity and cache hierarchy, unlike the optimized programs. As a mechanism for exploiting program parallelism, an additional processor is clearly better than another context. However, there were many configurations for which the addition of a few hardware contexts brought as much or greater performance than a larger multiprocessor with fewer than the optimal number of contexts. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal. </author> <title> Limits on interconnection network performance. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 398-412, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: This can be reduced with extra hardware support. We model network contention using the analytical model described and validated by Agarwal <ref> [1] </ref>. It is a reasonable compromise between a simplistic fixed latency model and a detailed, time-consuming interconnect simulation.
Reference: [2] <author> A. Agarwal. </author> <title> Performance tradeoffs in multithreaded processors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(5) </volume> <pages> 525-539, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: Both aspects of multithreading performance, processor utilization and cache/network traffic, have been investigated in the past, using studies based on simulation and analytical modeling of multithreaded processors <ref> [2, 6, 19, 26] </ref>. In this paper we address the performance bottom line, evaluating whether the benefits outweigh the drawbacks for coarse-grain multithread-ing, and whether the extra hardware needed for multiple contexts has adequate payoffs. We approach the questions from several different angles. <p> Multiple contexts hardly change the total cache miss picture: invalidations do not change significantly; and intra-thread conflicts translate to inter-thread conflicts, since the entire data space is still being accessed, but by different threads. Other studies <ref> [2, 26] </ref> report an increase in the cache miss rate with increasing contexts, e.g., Weber and Gupta [26] reported increased cache misses for LocusRoute (0.55% to 1.22%) and Mp3d (3.5% to 4.2%) when increasing contexts from one to four. <p> Agarwal <ref> [2] </ref> presents an analytical performance model that incorporates network traffic, cache interference, context switching overhead and the number of hardware contexts. The results showed that high cache miss rates and increased network contention can hurt processor utilization.
Reference: [3] <author> A. Agarwal, B-H. Lim, D. Kranz, and J. Kubiatowicz. </author> <month> APRIL: </month> <title> A processor architecture for multiprocessing. </title> <booktitle> 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Multiple hardware contexts are a technique for tolerating long instruction latencies <ref> [3, 4, 12, 21] </ref>. When one thread interlocks for a multi-cycle operation, rather than waiting until the operation completes, a multithreaded processor switches to another context and continues execution. For coarse-grain multithreaded processors, a context switch is triggered by long latency memory references (e.g., cache misses), and synchronization instructions.
Reference: [4] <author> R. Alverson, D. Callahan, D. Cummings, B. Koblenz, A. Porterfield, and B. Smith. </author> <title> The Tera computer system. </title> <booktitle> International Conference on Supercomputing, </booktitle> <pages> pages 1-6, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Multiple hardware contexts are a technique for tolerating long instruction latencies <ref> [3, 4, 12, 21] </ref>. When one thread interlocks for a multi-cycle operation, rather than waiting until the operation completes, a multithreaded processor switches to another context and continues execution. For coarse-grain multithreaded processors, a context switch is triggered by long latency memory references (e.g., cache misses), and synchronization instructions.
Reference: [5] <author> B. N. Bershad, E. D. Lazowska, and H. M. Levy. </author> <title> PRESTO: A system for object-oriented parallel programming. </title> <journal> Software: Practice and Experience, </journal> <volume> 18(8) </volume> <pages> 713-732, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: These two sets of programs were written using different parallel programming paradigms. The optimized programs, which include some of the SPLASH benchmarks [20], use fork () instructions to explicitly create processes. The unoptimized programs use the PRESTO parallel programming environment <ref> [5] </ref>, and create threads using calls to a user-level threads library. In both types of programs, thread synchronization is performed using global barriers, and mutual exclusion is implemented with explicit lock instructions.
Reference: [6] <author> B. Boothe and A. Ranade. </author> <title> Improved multithreading techniques for hiding communication latency in multiprocessors. </title> <booktitle> 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 214-223, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Both aspects of multithreading performance, processor utilization and cache/network traffic, have been investigated in the past, using studies based on simulation and analytical modeling of multithreaded processors <ref> [2, 6, 19, 26] </ref>. In this paper we address the performance bottom line, evaluating whether the benefits outweigh the drawbacks for coarse-grain multithread-ing, and whether the extra hardware needed for multiple contexts has adequate payoffs. We approach the questions from several different angles. <p> The study shows that few contexts cannot effectively hide very long memory latencies. The work by Boothe and Ranade <ref> [6] </ref> proposes an explicit context switch instruction to group together shared accesses, and hence avoid excessive context switching. As they show, this can improve multithreading's latency-hiding property, but the processor incurs the extra cost of managing multiple memory requests per thread.
Reference: [7] <author> D. Chaiken, J. Kubiatowicz, and A. Agarwal. </author> <title> LimitLESS directories: A scalable cache coherence scheme. </title> <booktitle> Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 224-234, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: A memory access is local when the address maps to the current processor, and remote for all other processors. Each processor has its own data cache, and coherency is maintained with a distributed, directory-based cache coherency protocol <ref> [7] </ref>. The simulator requires three inputs: a set of memory reference traces, a mapping that associates each program thread to a specific processor, and an architectural description file. Given these inputs the execution of the program is simulated on the given architecture and statistics are generated.
Reference: [8] <author> S. J. Eggers, D. R. Keppel, E. J. Koldinger, and H. M. Levy. </author> <title> Techniques for efficient inline tracing on a shared-memory multiprocessor. </title> <booktitle> ACM SIGMETRICS Conferenceon Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 37-46, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: In both types of programs, thread synchronization is performed using global barriers, and mutual exclusion is implemented with explicit lock instructions. Memory reference traces from these programs were generated from a parallel execution on a Sequent Symmetry [22] using MPtrace <ref> [8] </ref>, a parallel program tracing tool. Each thread is traced separately, enabling its separate simulation in the target architecture. The trace files capture synchronization instructions, but exclude spin-wait instructions, since locks may be acquired in a different order on the target architecture for a given simulated execution of the program.
Reference: [9] <author> K. I. Farkas and N. P. Jouppi. </author> <title> Complexity/performance tradeoffs with non-blocking loads. </title> <booktitle> 21th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 211-222, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The lockup-free mechanism must allow at least as many outstanding memory requests as there are hardware contexts. A lockup-free cache that can support 16-32 outstanding memory requests is complex, although its chip area requirement is not significant <ref> [9] </ref>. Our simulator assumes that such a lockup-free cache can be implemented. The cache unit maintains statistics on the individual cache miss components of first-reference, intra- and inter-thread conflict, and invalidation misses.
Reference: [10] <author> M. K. Farrens and A. R. Pleszkum. </author> <title> Strategies for achieving processor throughput. </title> <booktitle> 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 362-369, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: With every thread grouping its memory operations, this limit could be reached fairly quickly. Such grouping might also be prone to creating hot spots in the interconnect. Two studies addressed execution time, as does ours, but for different ends. Farrens and Pleszkum <ref> [10] </ref> studied the improvement in instruction issue with deeply pipelined processors, using two interleaved instruction streams. Gupta et. al. [11] evaluated multiple contexts in the presence of other latency-tolerating techniques, such as prefetching and sequential consistency.
Reference: [11] <author> A. Gupta, J. Hennesey, K. Gharachorloo, T. Mowry, and W-D. Weber. </author> <title> Comparative evaluation of latency reducing and tolerating techniques. </title> <booktitle> 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 254-263, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Such grouping might also be prone to creating hot spots in the interconnect. Two studies addressed execution time, as does ours, but for different ends. Farrens and Pleszkum [10] studied the improvement in instruction issue with deeply pipelined processors, using two interleaved instruction streams. Gupta et. al. <ref> [11] </ref> evaluated multiple contexts in the presence of other latency-tolerating techniques, such as prefetching and sequential consistency.
Reference: [12] <author> R. H. Halstead and T. Fujita. MASA: </author> <title> A multithreaded processor architecture for parallel symbolic computing. </title> <booktitle> 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 443-451, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: 1 Introduction Multiple hardware contexts are a technique for tolerating long instruction latencies <ref> [3, 4, 12, 21] </ref>. When one thread interlocks for a multi-cycle operation, rather than waiting until the operation completes, a multithreaded processor switches to another context and continues execution. For coarse-grain multithreaded processors, a context switch is triggered by long latency memory references (e.g., cache misses), and synchronization instructions.
Reference: [13] <author> T.E. Jeremiassen and S.J. Eggers. </author> <title> Computing per-process summary side-effect information. </title> <booktitle> 5th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1992. </year> <note> Also appeared as LNCS #757, pages 175-191. </note>
Reference-contexts: Given these inputs the execution of the program is simulated on the given architecture and statistics are generated. The workload spans a wide range of problem domains and comprises two types of explicitly parallel programs: data locality-optimized (either by the programmer or the compiler <ref> [13, 14] </ref>), and locality-unoptimized (Table 1). These two sets of programs were written using different parallel programming paradigms. The optimized programs, which include some of the SPLASH benchmarks [20], use fork () instructions to explicitly create processes.
Reference: [14] <author> T.E. Jeremiassen and S.J. Eggers. </author> <title> Static analysis of barrier synchronization in explicitly parallel programs. </title> <booktitle> International Conference on Parallel Architectures and Compilation Techniques, </booktitle> <address> Montreal, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Given these inputs the execution of the program is simulated on the given architecture and statistics are generated. The workload spans a wide range of problem domains and comprises two types of explicitly parallel programs: data locality-optimized (either by the programmer or the compiler <ref> [13, 14] </ref>), and locality-unoptimized (Table 1). These two sets of programs were written using different parallel programming paradigms. The optimized programs, which include some of the SPLASH benchmarks [20], use fork () instructions to explicitly create processes.
Reference: [15] <author> D. Kroft. </author> <title> Lockup-free instruction fetch/prefetch cache organization. </title> <booktitle> 8th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 81-87, </pages> <month> May </month> <year> 1981. </year>
Reference-contexts: Assuming a 32 KByte, direct-mapped base cache, the simulator adds 10% to the processor cycle time with a doubling of the cache size or with two-way set-associativity [25], again, assuming a 4-8ns clock. To support context switching on cache misses, a multithreaded processor must implement a lockup-free cache <ref> [15] </ref>. The lockup-free mechanism must allow at least as many outstanding memory requests as there are hardware contexts. A lockup-free cache that can support 16-32 outstanding memory requests is complex, although its chip area requirement is not significant [9]. Our simulator assumes that such a lockup-free cache can be implemented.
Reference: [16] <author> E. P. Markatos and T. J. LeBlanc. </author> <title> Using processor affinity in loop scheduling on shared-memory multiprocessors. </title> <booktitle> Supercomputing `92, </booktitle> <pages> pages 104-113, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Therefore we use it here. Load-balanced placements are practical to implement, and are frequently used in real systems by both compiler-and runtime-based scheduling algorithms <ref> [16, 18, 24] </ref>. The architectural description file specifies hardware parameters (Table 2). The simulator comprises three separate modules: processor, cache and interconnect. We discuss each in greater detail, including chip area and timing considerations. The Processor Each processor can model one or more hardware contexts.
Reference: [17] <author> J. H. Mulder, N. T. Quach, and M. J Flynn. </author> <title> An area model for on-chip memories and its applications. </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> 26(2) </volume> <pages> 98-106, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: Multiple hardware contexts incur a slight cost in processor chip area. Assuming a 32 integer and 32 floating point register set (64-bit machine) with two read and two write ports, the equivalent cache chip area is about 2 KBytes <ref> [17] </ref>. Thus, the area needed for multiple contexts is significant only with more than about eight register sets (16 KByte cache equivalent). The effect on cycle time is a more serious consideration.
Reference: [18] <author> C. D. Polychronopoulos and D. J. Kuck. </author> <title> Guided self-scheduling: A practical scheduling scheme for parallel supercomputers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(12):1425-1439, </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: Therefore we use it here. Load-balanced placements are practical to implement, and are frequently used in real systems by both compiler-and runtime-based scheduling algorithms <ref> [16, 18, 24] </ref>. The architectural description file specifies hardware parameters (Table 2). The simulator comprises three separate modules: processor, cache and interconnect. We discuss each in greater detail, including chip area and timing considerations. The Processor Each processor can model one or more hardware contexts.
Reference: [19] <author> R. H. Saavedra-Barrera, D. E. Culler, and T. von Eicken. </author> <title> Analysis of multithreaded architectures for parallel computing. </title> <booktitle> 2nd Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 169-178, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: Both aspects of multithreading performance, processor utilization and cache/network traffic, have been investigated in the past, using studies based on simulation and analytical modeling of multithreaded processors <ref> [2, 6, 19, 26] </ref>. In this paper we address the performance bottom line, evaluating whether the benefits outweigh the drawbacks for coarse-grain multithread-ing, and whether the extra hardware needed for multiple contexts has adequate payoffs. We approach the questions from several different angles. <p> Agarwal [2] presents an analytical performance model that incorporates network traffic, cache interference, context switching overhead and the number of hardware contexts. The results showed that high cache miss rates and increased network contention can hurt processor utilization. Saavedra-Barrera et al. <ref> [19] </ref> developed a Markov chain model for multithreaded processor efficiency that varies the number of contexts, the network latency, context switch times, and remote reference rate. (They also incorporate cache performance degradation in their model). The study shows that few contexts cannot effectively hide very long memory latencies.
Reference: [20] <author> J. P. Singh, W-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared-memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: These two sets of programs were written using different parallel programming paradigms. The optimized programs, which include some of the SPLASH benchmarks <ref> [20] </ref>, use fork () instructions to explicitly create processes. The unoptimized programs use the PRESTO parallel programming environment [5], and create threads using calls to a user-level threads library. In both types of programs, thread synchronization is performed using global barriers, and mutual exclusion is implemented with explicit lock instructions.
Reference: [21] <author> B. J. Smith. </author> <title> Architecture and applications of the HEP multiprocessor computer system. </title> <booktitle> SPIE, Real-Time Signal Processing IV, </booktitle> <volume> 298 </volume> <pages> 241-248, </pages> <year> 1981. </year>
Reference-contexts: 1 Introduction Multiple hardware contexts are a technique for tolerating long instruction latencies <ref> [3, 4, 12, 21] </ref>. When one thread interlocks for a multi-cycle operation, rather than waiting until the operation completes, a multithreaded processor switches to another context and continues execution. For coarse-grain multithreaded processors, a context switch is triggered by long latency memory references (e.g., cache misses), and synchronization instructions.
Reference: [22] <institution> Symmetry Technical Summary. Sequent Computer Systems, Inc. </institution>
Reference-contexts: In both types of programs, thread synchronization is performed using global barriers, and mutual exclusion is implemented with explicit lock instructions. Memory reference traces from these programs were generated from a parallel execution on a Sequent Symmetry <ref> [22] </ref> using MPtrace [8], a parallel program tracing tool. Each thread is traced separately, enabling its separate simulation in the target architecture.
Reference: [23] <author> R. Thekkath and S. J. Eggers. </author> <title> Impact of sharing-based thread placement on multithreaded architectures. </title> <booktitle> 21th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 176-186, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: We use a load balanced thread placement. Since thread lengths are known from the trace file lengths, an oracle load-balancer distributes threads across processors, attempting to equalize the number of instructions executed per processor. In a previous study <ref> [23] </ref>, we showed that load balancing was the superior placement algorithm on a wide range of processor/hardware context configurations for a similar suite of applications. Therefore we use it here.
Reference: [24] <author> T. H. Tzen and L. M. Ni. </author> <title> Dynamic loop scheduling for shared-memory multiprocessors. </title> <booktitle> 1991 International Conference on Parallel Processing, </booktitle> <pages> pages II:246-250, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Therefore we use it here. Load-balanced placements are practical to implement, and are frequently used in real systems by both compiler-and runtime-based scheduling algorithms <ref> [16, 18, 24] </ref>. The architectural description file specifies hardware parameters (Table 2). The simulator comprises three separate modules: processor, cache and interconnect. We discuss each in greater detail, including chip area and timing considerations. The Processor Each processor can model one or more hardware contexts.
Reference: [25] <author> T. Wada, S. Rajan, and S. A. Przybylski. </author> <title> An analytical access time model for on-chip cache memories. </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> 27(8) </volume> <pages> 1147-1156, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Assuming a 32 KByte, direct-mapped base cache, the simulator adds 10% to the processor cycle time with a doubling of the cache size or with two-way set-associativity <ref> [25] </ref>, again, assuming a 4-8ns clock. To support context switching on cache misses, a multithreaded processor must implement a lockup-free cache [15]. The lockup-free mechanism must allow at least as many outstanding memory requests as there are hardware contexts.
Reference: [26] <author> W-D. Weber and A. Gupta. </author> <title> Exploring the benefits of multiple hardware contexts in a multiprocessor architecture: Preliminary results. </title> <booktitle> 16th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 273-280, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Both aspects of multithreading performance, processor utilization and cache/network traffic, have been investigated in the past, using studies based on simulation and analytical modeling of multithreaded processors <ref> [2, 6, 19, 26] </ref>. In this paper we address the performance bottom line, evaluating whether the benefits outweigh the drawbacks for coarse-grain multithread-ing, and whether the extra hardware needed for multiple contexts has adequate payoffs. We approach the questions from several different angles. <p> Multiple contexts hardly change the total cache miss picture: invalidations do not change significantly; and intra-thread conflicts translate to inter-thread conflicts, since the entire data space is still being accessed, but by different threads. Other studies <ref> [2, 26] </ref> report an increase in the cache miss rate with increasing contexts, e.g., Weber and Gupta [26] reported increased cache misses for LocusRoute (0.55% to 1.22%) and Mp3d (3.5% to 4.2%) when increasing contexts from one to four. <p> Other studies [2, 26] report an increase in the cache miss rate with increasing contexts, e.g., Weber and Gupta <ref> [26] </ref> reported increased cache misses for LocusRoute (0.55% to 1.22%) and Mp3d (3.5% to 4.2%) when increasing contexts from one to four. These studies equated hardware contexts with program threads, and varied the two together. More contexts meant more executing threads, and consequently increased cache contention. <p> Weber and Gupta <ref> [26] </ref> estimated, via simulation, the extent to which a multithreaded architecture can overcome the effects of long access latencies.
References-found: 26

