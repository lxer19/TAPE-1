URL: http://www.is.cs.cmu.edu/papers/speech/EUROSPEECH97/EUROSPEECH97-petra.ps.gz
Refering-URL: http://www.is.cs.cmu.edu/ISL.speech.publications.html
Root-URL: 
Email: pgeutner@ira.uka.de  
Title: FUZZY CLASS RESCORING: A PART-OF-SPEECH LANGUAGE MODEL  
Author: P. Geutner 
Address: 76128 Karlsruhe, Germany  
Affiliation: Interactive Systems Laboratories Department of Computer Science, University of Karlsruhe,  
Abstract: Current speech recognition systems usually use word-based trigram language models. More elaborate models are applied to word lattices or N best lists in a rescoring pass following the acoustic decoding process. In this paper we consider techniques for dealing with class-based language models in the lattice rescor-ing framework of our JANUS large vocabulary speech recognizer. We demonstrate how to interpolate with a Part-of-Speech (POS) tag-based language model as example of a class-based model, where a word can be member of many different classes. Here the actual class membership of a word in the lattice becomes a hidden event of the A fl algorithm used for rescoring. A forward type of algorithm is defined as extension of the lattice rescorer to handle these hidden events in a mathematically sound fashion. Applying the mixture of viterbi and forward kind of rescoring procedure to the German Spontaneous Scheduling Task (GSST) yields some improvement in word accuracy. Above all, the rescoring procedure enables usage of any fuzzy/stochastic class definition for recognition units that might be determined through automatic clustering algorithms in the future. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Farhat, J.-F. Isabelle and D. O'Shaughnessy. </author> <title> Clustering Words for Statistical Language Models Based on Contextual Word Similarity. </title> <booktitle> Proceedings of the IEEE 1996 International Conference on Acoustics, Speech and Signal Processing (ICASSP), Atlanta, Georgia, </booktitle> <pages> pp. 180-183, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: More elaborate statistical language models are then applied later on using word lattice rescoring techniques to extract the first best hypothesis from lattices produced during recognition runs. Application of class-based language models is one way of rescoring the generated word lattice <ref> [1] </ref>. Using predefined classes (e.g function and content words [2]) is one option explored earlier. Also, automatically created (using optimization criteria like the amount of perplexity reduction) classes were proposed and evaluated [3].
Reference: [2] <author> P. Geutner. </author> <title> Introducing Linguistic Constraints into Statistical Language Modeling. </title> <address> ICSLP'96, Philadel-phia, Pennsylvania, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: Application of class-based language models is one way of rescoring the generated word lattice [1]. Using predefined classes (e.g function and content words <ref> [2] </ref>) is one option explored earlier. Also, automatically created (using optimization criteria like the amount of perplexity reduction) classes were proposed and evaluated [3]. One thing these clustering algorithms and hand-made word classifications have in common is, that each word is assigned to exactly one class.
Reference: [3] <author> R. Kneser and H. Ney. </author> <title> Improved Clustering Techniques for Class-Based Statistical Language Mod-elling. </title> <booktitle> 3rd European Conference on Speech Communication and Technology (EUROSPEECH), </booktitle> <address> Berlin, Germany, </address> <pages> pp. 973-976, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: Application of class-based language models is one way of rescoring the generated word lattice [1]. Using predefined classes (e.g function and content words [2]) is one option explored earlier. Also, automatically created (using optimization criteria like the amount of perplexity reduction) classes were proposed and evaluated <ref> [3] </ref>. One thing these clustering algorithms and hand-made word classifications have in common is, that each word is assigned to exactly one class.
Reference: [4] <author> T.R. </author> <title> Niesler and P.C. Woodland. Variable-length category-based n-grams for language modelling. </title> <type> Technical Report, </type> <institution> Cambridge University, UK, </institution> <month> April </month> <year> 1995. </year>
Reference-contexts: Besides, there is also the possibility of stochastic/fuzzy classes where each word is not uniquely assigned to one single class, but can be member of different classes with varying probabilities. One example shown in this paper is the use of Part-of-Speech (POS) classes <ref> [4] </ref>. Here a number of words has no unique class (= part-of-speech tag), but different tags can be potentially assigned to a word depending on the grammatical role of the word in a certain context.
Reference: [5] <author> H. Schmid. </author> <title> Improvements in Part-of-Speech Tagging with an Application to German. </title> <booktitle> EACL SIGDAT Workshop, </booktitle> <address> Dublin, Ireland, </address> <year> 1995. </year>
Reference-contexts: This database consists of human-to-human dialogues where two individuals are given different calendars with various appointments. Goal of the conversations is to schedule a meeting. The training corpus was tagged using a Part-of-Speech Tagger provided by the University of Stuttgart <ref> [5] </ref> which is based on Hidden Markov Models and could be easily applied to our German database. The tagger is reported to work with an accuracy of 96%.
Reference: [6] <author> M. Finke, P. Geutner, H. Hild, T. Kemp K. Ries and M. Westphal. </author> <title> The Karlsruhe-Verbmobil Speech Recognition Engine. </title> <booktitle> Proceedings of the IEEE 1997 International Conference on Acoustics, Speech and Signal Processing (ICASSP), </booktitle> <address> Munich, Germany, </address> <month> May </month> <year> 1997. </year>
Reference-contexts: LANGUAGE MODELS IN JRTK The Karlsruhe-Verbmobil Speech Recognition Engine is based on the Janus Speech Recognition Toolkit (JRTk) developed at the Interactive Systems Laboratories in Karlsruhe and at Carnegie Mellon University in Pittsburgh <ref> [6, 7] </ref>. This toolkit implements a new object-oriented approach. A flexible Tcl/Tk script based environment allows building state-of-the-art multimodal rec-ognizers this includes speech, handwriting and gesture recognition. Unlike other toolkits Janus is not a set of libraries and precompiled modules but a programmable shell with transparent, yet very efficient objects.
Reference: [7] <author> M. Finke, J. Fritsch, P. Geutner, K. Ries, T. Zeppenfeld and A. </author> <title> Waibel The JanusRTk Switchboard/Callhome 1997 Evaluation System. </title> <booktitle> Proceedings of the LVCSR Hub-5 Workshop, </booktitle> <address> Baltimore, Maryland, </address> <month> May </month> <year> 1997. </year>
Reference-contexts: LANGUAGE MODELS IN JRTK The Karlsruhe-Verbmobil Speech Recognition Engine is based on the Janus Speech Recognition Toolkit (JRTk) developed at the Interactive Systems Laboratories in Karlsruhe and at Carnegie Mellon University in Pittsburgh <ref> [6, 7] </ref>. This toolkit implements a new object-oriented approach. A flexible Tcl/Tk script based environment allows building state-of-the-art multimodal rec-ognizers this includes speech, handwriting and gesture recognition. Unlike other toolkits Janus is not a set of libraries and precompiled modules but a programmable shell with transparent, yet very efficient objects. <p> The overall implementation of this hierarchical approach allows for easy and convenient testing of many different language models that might be combined the same way as the two models in our example. Other examples are language models implementing hidden utterance segment boundaries <ref> [7] </ref>. 4. RESCORING ALGORITHM Since class membership of words is no longer unique, the class variable c i becomes a hidden variable.
Reference: [8] <author> P. </author> <title> Geutner et al._Integrating Different Learning Approaches into a Multilingual Spoken Language Translation System. In: Connectionist, Statistical and Symbolic Approaches to Learning for Natural Language Processing. </title> <booktitle> Lecture Notes in Artificial Intelligence, </booktitle> <editor> S. Wermter, E. Riloff and G. </editor> <booktitle> Scheler (Eds.), </booktitle> <pages> pp. 117-131, </pages> <publisher> Springer, </publisher> <year> 1996. </year>
Reference-contexts: If there is no other arc e j left in the queue add e jk to the queue. 5. RESULTS All recognition results reported have been performed with the JANUS system <ref> [8] </ref>. The training set for training the word-based trigram model consisted of 330.000 words, the same text was used to build a POS language model. An independent test set consisting of 35 GSST dialogues containing 6.300 words has been defined and word accuracy results are reported on this set.
References-found: 8

