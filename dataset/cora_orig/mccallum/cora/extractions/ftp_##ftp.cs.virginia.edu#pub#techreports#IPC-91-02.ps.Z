URL: ftp://ftp.cs.virginia.edu/pub/techreports/IPC-91-02.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/techreports/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Performance Measurement of a Parallel Input/Output System for the Intel iPSC/2 Hypercube  
Author: James C. French Terrence W. Pratt Mriganka Das 
Note: This research was supported in part by Jet Propulsion Laboratory Contract #957721 and by the Department of Energy under grant DE-FG05-88ER25063.  
Address: Charlottesville, Virginia 22903  
Affiliation: Institute for Parallel Computation School of Engineering and Applied Science University of Virginia  
Date: January 30, 1991  
Pubnum: IPC-TR-91-002  
Abstract: This paper to appear in the Proceedings of the 1991 ACM SIGMETRICS Conference on Measurement & Modeling of Computer Systems, May, 1991. 
Abstract-found: 1
Intro-found: 1
Reference: [ASBU89] <author> R. K. Asbury and D. S. Scott, </author> <title> ``Fortran I/O on the iPSC/2: Is There Read after Write?'', </title> <booktitle> Proc. 4th Conf. on Hypercube Concurrent Computers and Applications, </booktitle> <address> Monterey, </address> <month> Mar. </month> <year> 1989, </year> <pages> 129-132. </pages>
Reference-contexts: A few preliminary studies that involve measurement of early versions of the Intel CFS are presented in <ref> [ASBU89, BRAD89, PIER89, PRAT89] </ref>. Our study here is based on the production software provided by Intel for the system. To our knowledge, this study represents the first published performance measurements on the production system. The Intel CFS represents one slice from the range of design choices mentioned above.
Reference: [BOMA89] <author> L. Bomans and D. Roose, </author> <title> ``Benchmarking the iPSC/2 Hypercube Multiprocessor'', </title> <journal> Concurrency: Practice and Experience 1, </journal> <month> 1 (Sep. </month> <year> 1989), </year> <pages> 3-18. </pages>
Reference-contexts: The data transfer rate for the 1 MB file is approximately 1240 KB/sec. The maximum point-to-point communication transfer rate of the iPSC/2 is specified by Intel [PIER88] to be 2800 KB/sec and has been measured by others <ref> [BOMA89] </ref> at 2660 KB/sec. Thus this output rate is slightly less than 50% of the basic message-passing com munication rate. 2. The data transfer rate for the 8 MB file is 750-800 KB/sec or 610 KB/sec depending on where the file header is relative to the file body.
Reference: [BRAD89] <author> D. K. Bradley and D. A. Reed, </author> <title> ``Performance of the Intel iPSC/2 Input/Output System'', </title> <booktitle> Proc. 4th Conf. on Hypercube Concurrent Computers and Applications, </booktitle> <address> Monterey, </address> <month> Mar. </month> <year> 1989, </year> <pages> 141-144. </pages>
Reference-contexts: A few preliminary studies that involve measurement of early versions of the Intel CFS are presented in <ref> [ASBU89, BRAD89, PIER89, PRAT89] </ref>. Our study here is based on the production software provided by Intel for the system. To our knowledge, this study represents the first published performance measurements on the production system. The Intel CFS represents one slice from the range of design choices mentioned above.
Reference: [CROC89] <author> T. W. Crockett, </author> <title> ``File Concepts for Parallel I/O'', </title> <booktitle> Proc. Supercomputing '89, </booktitle> <address> Reno, Nevada, </address> <month> Nov. </month> <year> 1989, </year> <pages> 574-579. </pages>
Reference-contexts: Can such ``disk caching'' be used effectively for pre-fetching input blocks or for staging output blocks in a concurrent I/O system? [KOTZ90] 3. File storage structure. How should the file system be organized in a concurrent I/O system? Should individual files be ``striped'' or ``declustered''? <ref> [CROC89, LIVN87, SALE86] </ref> What is the appropriate unit of decompositionbits, bytes, or larger blocks? How should the overall file system be organized? [KOTZ90, PIER89] Almost all the research on these design alternatives has been limited to simulation studies.
Reference: [FREN89] <author> J. C. French and T. W. Pratt, </author> <title> ``Performance Measurement of Two Parallel File Systems'', </title> <type> Tech. Rep. </type> <note> IPC-TR-89-13 (Supercomputing '89 poster presentation), </note> <institution> Institute for Parallel Computation, Thornton Hall, University of Virginia, </institution> <address> Charlottesville, VA, </address> <month> Nov. </month> <year> 1989. </year>
Reference-contexts: Table 2 shows that 16 KB buffers achieve about 1.5 times the data rate of 4 KB buffers when a single reader (writer) is reading (writing) a file. Earlier measurements of the effects of buffer size on transfer rate <ref> [FREN89] </ref> showed that most improvement occurred when the buffer size was increased from 4 KB to 16 KB. After that essentially no further improvement is observed.
Reference: [KIM86] <author> M. Y. Kim, </author> <title> ``Synchronized Disk Interleaving'', </title> <journal> IEEE Trans. on Computers C-35, </journal> <volume> 11 (Nov. </volume> <year> 1986), </year> <pages> 978-988. </pages>
Reference-contexts: Some of the central issues in the design of such I/O subsystems include: 1. Architecture of the I/O subsystem. How should the hardware of the I/O subsystem be organized? Should the disks be accessed synchronously or asynchronously? <ref> [KIM86, REDD90, SALE86] </ref> Should all disks be on a single channel, or on multiple channels? How should processing of I/O requests be distributed? [PIER89, REDD89] How should reliability be achieved? [PATT88] 2. Disk caching.
Reference: [KOTZ90] <author> D. Kotz and C. S. Ellis, </author> <title> ``Prefetching in File Systems for MIMD Multiprocessors'', </title> <journal> IEEE Trans. on Parallel and Distributed Systems 1, </journal> <month> 2 (Apr. </month> <year> 1990), </year> <pages> 218-230. </pages>
Reference-contexts: Can such ``disk caching'' be used effectively for pre-fetching input blocks or for staging output blocks in a concurrent I/O system? <ref> [KOTZ90] </ref> 3. File storage structure. <p> File storage structure. How should the file system be organized in a concurrent I/O system? Should individual files be ``striped'' or ``declustered''? [CROC89, LIVN87, SALE86] What is the appropriate unit of decompositionbits, bytes, or larger blocks? How should the overall file system be organized? <ref> [KOTZ90, PIER89] </ref> Almost all the research on these design alternatives has been limited to simulation studies. A few preliminary studies that involve measurement of early versions of the Intel CFS are presented in [ASBU89, BRAD89, PIER89, PRAT89].
Reference: [LIVN87] <author> M. Livny, S. Khoshafian and H. Boral, </author> <title> ``Multi-Disk Management Algorithms'', </title> <booktitle> Proc. of the 1987 ACM Sigmetrics Conf. on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1987, </year> <pages> 69-77. </pages>
Reference-contexts: Can such ``disk caching'' be used effectively for pre-fetching input blocks or for staging output blocks in a concurrent I/O system? [KOTZ90] 3. File storage structure. How should the file system be organized in a concurrent I/O system? Should individual files be ``striped'' or ``declustered''? <ref> [CROC89, LIVN87, SALE86] </ref> What is the appropriate unit of decompositionbits, bytes, or larger blocks? How should the overall file system be organized? [KOTZ90, PIER89] Almost all the research on these design alternatives has been limited to simulation studies. <p> I/O nodes and their disks work entirely asynchronously from other I/O nodes. Memory on the I/O nodes is used for disk caching. Prefetching and ``preallocation'' are used to speed I/O operations. Individual files are split into 4KB 1 blocks which are ``declustered'' <ref> [LIVN87] </ref> hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 1 In this paper, K = 2 10 (1024), not 10 3 (1000). and spread across all the available I/O nodes and disks. Process--ing of I/O requests is distributed between the ``compute node'' that initiates the request and the I/O node that services the request.
Reference: [PATT88] <author> D. Patterson, G. Gibson and R. Katz, </author> <title> ``A Case for Redundant Arrays of Inexpensive Disks (RAID)'', </title> <booktitle> Proc. of the Inter. Conf. on Management of Data, </booktitle> <address> Chicago, IL, </address> <month> June </month> <year> 1988, </year> <pages> 109-116. </pages>
Reference-contexts: Background Input/output subsystems that utilize many inexpensive magnetic disks accessed in parallel (in contrast to the traditional reliance on a single large expensive disk, the so-called SLED design <ref> [PATT88] </ref>) have been a topic of a number of recent papers. Some of the central issues in the design of such I/O subsystems include: 1. Architecture of the I/O subsystem. <p> How should the hardware of the I/O subsystem be organized? Should the disks be accessed synchronously or asynchronously? [KIM86, REDD90, SALE86] Should all disks be on a single channel, or on multiple channels? How should processing of I/O requests be distributed? [PIER89, REDD89] How should reliability be achieved? <ref> [PATT88] </ref> 2. Disk caching. Use of memory in the I/O processor as a cache for disk blocks is a common method for smoothing the large speed difference between physical disk devices and main memory.
Reference: [PIER88] <author> P. Pierce, </author> <title> ``The NX/2 Operating System'', </title> <booktitle> Proc. 3rd Conf. on Hypercube Concurrent Computers and Applications, </booktitle> <address> Pasadena, </address> <month> Jan. </month> <year> 1988, </year> <pages> 384-390. </pages>
Reference-contexts: Consider first the write behavior as shown in Figures 2 (c) and 2 (d): 1. The data transfer rate for the 1 MB file is approximately 1240 KB/sec. The maximum point-to-point communication transfer rate of the iPSC/2 is specified by Intel <ref> [PIER88] </ref> to be 2800 KB/sec and has been measured by others [BOMA89] at 2660 KB/sec. Thus this output rate is slightly less than 50% of the basic message-passing com munication rate. 2.
Reference: [PIER89] <author> P. Pierce, </author> <title> ``A Concurrent File System for a Highly Parallel Mass Storage Subsystem'', </title> <booktitle> Proc. 4th Conf. on Hypercube Concurrent Computers and Applications, </booktitle> <address> Monterey, </address> <month> Mar. </month> <year> 1989, </year> <pages> 155-160. </pages>
Reference-contexts: Architecture of the I/O subsystem. How should the hardware of the I/O subsystem be organized? Should the disks be accessed synchronously or asynchronously? [KIM86, REDD90, SALE86] Should all disks be on a single channel, or on multiple channels? How should processing of I/O requests be distributed? <ref> [PIER89, REDD89] </ref> How should reliability be achieved? [PATT88] 2. Disk caching. Use of memory in the I/O processor as a cache for disk blocks is a common method for smoothing the large speed difference between physical disk devices and main memory. <p> File storage structure. How should the file system be organized in a concurrent I/O system? Should individual files be ``striped'' or ``declustered''? [CROC89, LIVN87, SALE86] What is the appropriate unit of decompositionbits, bytes, or larger blocks? How should the overall file system be organized? <ref> [KOTZ90, PIER89] </ref> Almost all the research on these design alternatives has been limited to simulation studies. A few preliminary studies that involve measurement of early versions of the Intel CFS are presented in [ASBU89, BRAD89, PIER89, PRAT89]. <p> A few preliminary studies that involve measurement of early versions of the Intel CFS are presented in <ref> [ASBU89, BRAD89, PIER89, PRAT89] </ref>. Our study here is based on the production software provided by Intel for the system. To our knowledge, this study represents the first published performance measurements on the production system. The Intel CFS represents one slice from the range of design choices mentioned above. <p> Paul Pierce, the designer of CFS, describes this block size as the smallest block size where the request/response performance over the communication network matches the sustained disk transfer rate of about 1MB/sec. <ref> [PIER89] </ref> He also notes that a larger block size would increase contention on communication links and increase internal fragmentation when blocks are not filled completely. Each file is split into 4KB blocks when it is initially created. A file can be made to reside on one or more volumes.
Reference: [PRAT89] <author> T. W. Pratt, J. C. French, P. M. Dickens and S. A. Janet, </author> <title> ``A Comparison of the Architecture and Performance of Two Parallel File Systems'', </title> <booktitle> Proc. 4th Conf. on Hypercube Concurrent Computers and Applications, </booktitle> <address> Monterey, CA, </address> <month> Mar. </month> <year> 1989, </year> <pages> 161-166. </pages>
Reference-contexts: A few preliminary studies that involve measurement of early versions of the Intel CFS are presented in <ref> [ASBU89, BRAD89, PIER89, PRAT89] </ref>. Our study here is based on the production software provided by Intel for the system. To our knowledge, this study represents the first published performance measurements on the production system. The Intel CFS represents one slice from the range of design choices mentioned above.
Reference: [REDD89] <author> A. L. N. Reddy and P. Banerjee, </author> <title> ``An Evaluation of Multiple-Disk I/O Systems'', </title> <journal> IEEE Trans. on Computers 38, </journal> <month> 12 (Dec. </month> <year> 1989), </year> <pages> 1680-1690. </pages>
Reference-contexts: Architecture of the I/O subsystem. How should the hardware of the I/O subsystem be organized? Should the disks be accessed synchronously or asynchronously? [KIM86, REDD90, SALE86] Should all disks be on a single channel, or on multiple channels? How should processing of I/O requests be distributed? <ref> [PIER89, REDD89] </ref> How should reliability be achieved? [PATT88] 2. Disk caching. Use of memory in the I/O processor as a cache for disk blocks is a common method for smoothing the large speed difference between physical disk devices and main memory.
Reference: [REDD90] <author> A. L. N. Reddy and P. Banerjee, </author> <title> ``Design, Analysis, and Simulation of I/O Architectures for Hypercube Multiprocessors'', </title> <journal> IEEE Trans. on Parallel and Distributed Systems 1, </journal> <month> 2 (Apr. </month> <year> 1990), </year> <pages> 140-151. </pages>
Reference-contexts: Some of the central issues in the design of such I/O subsystems include: 1. Architecture of the I/O subsystem. How should the hardware of the I/O subsystem be organized? Should the disks be accessed synchronously or asynchronously? <ref> [KIM86, REDD90, SALE86] </ref> Should all disks be on a single channel, or on multiple channels? How should processing of I/O requests be distributed? [PIER89, REDD89] How should reliability be achieved? [PATT88] 2. Disk caching.
Reference: [SALE86] <author> K. Salem and H. Garcia-Molina, </author> <title> ``Disk Striping'', </title> <booktitle> Proc. of the Second Inter. Conf. on Data Engineering, </booktitle> <year> 1986, </year> <pages> 336-342. </pages>
Reference-contexts: Some of the central issues in the design of such I/O subsystems include: 1. Architecture of the I/O subsystem. How should the hardware of the I/O subsystem be organized? Should the disks be accessed synchronously or asynchronously? <ref> [KIM86, REDD90, SALE86] </ref> Should all disks be on a single channel, or on multiple channels? How should processing of I/O requests be distributed? [PIER89, REDD89] How should reliability be achieved? [PATT88] 2. Disk caching. <p> Can such ``disk caching'' be used effectively for pre-fetching input blocks or for staging output blocks in a concurrent I/O system? [KOTZ90] 3. File storage structure. How should the file system be organized in a concurrent I/O system? Should individual files be ``striped'' or ``declustered''? <ref> [CROC89, LIVN87, SALE86] </ref> What is the appropriate unit of decompositionbits, bytes, or larger blocks? How should the overall file system be organized? [KOTZ90, PIER89] Almost all the research on these design alternatives has been limited to simulation studies.
References-found: 15

