URL: http://www.research.microsoft.com/~mhwang/dynamic.ps
Refering-URL: http://www.research.microsoft.com/~mhwang/
Root-URL: http://www.research.microsoft.com
Email: mhwang,xueh@microsoft.com  
Title: DYNAMICALLY CONFIGURABLE ACOUSTIC MODELS FOR SPEECH RECOGNITION  
Author: Mei-Yuh Hwang and Xuedong Huang 
Address: 1 Microsoft Way Redmond, WA 98052, USA  
Affiliation: Microsoft Research  
Abstract: Senones were introduced to share Hidden Markov model (HMM) parameters at a sub-phonetic level in [3] and decision trees were incorporated to predict unseen phonetic contexts in [4]. In this paper, we will describe two applications of the senonic decision tree in (1) dynamically downsizing a speech recognition system for small platforms and in (2) sharing the Gaussian covariances of continuous density HMMs (CHMMs). We experimented how to balance different parameters that can offer the best trade off between recognition accuracy and system size. The dynamically downsized system, without retraining, performed even better than the regular Baum-Welch [1] trained system. The shared covariance model provided as good a performance as the unshared full model and thus gave us the freedom to increase the number of Gaussian means to increase the accuracy of the model. Combining the downsizing and covariance sharing algorithms, a total of 8% error reduction was achieved over the Baum-Welch trained system with approximately the same parameter size. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Baum, L. E. </author> <title> An Inequality and Associated Maximization Technique in Statistical Estimation of Probabilistic Functions of Markov Processes. </title> <journal> Inequalities, </journal> <volume> vol. 3 (1972), </volume> <pages> pp. 18. </pages>
Reference-contexts: A (s), associated with data Y, L y (YjG 2 ) = b=2fd log 2 + log jS 2 j + dg When both sets of data X and Y are modeled by the same Gaussian G = N (; S) , setting the derivative of the Q function in <ref> [1] </ref> with respect to G to zero gives ML estimate of the following, assuming the data alignment is fixed: Q (G) = x2X X fl (y) log N (y; ; S) X fl (x)fd log 2 + logjSj + (x ) t 1 + y fl (y)fd log 2 + logjSj
Reference: [2] <author> Huang, X., Hwang, M., Jiang, L., and Mahajan, M. </author> <title> Deleted Interpolation and Density Sharing for Continuous Hidden Markov Models. </title> <booktitle> in: IEEE International Conference on Acoustics, Speech, and Signal Processing. </booktitle> <year> 1996. </year>
Reference-contexts: Therefore it is particularly useful in real applications to provide the user with the flexibility of dynamically configuring the size of the speech recognizer and trading the accuracy with resource utilization. While using CHMMs with Gaussian density functions, one should be careful about smoothing the covariances. The shared-Gaussian model <ref> [2] </ref> was attempting to smooth the covariance through sharing. When the covariance is too small, the model becomes tailored too much to the training data and thus is not robust with new test data.
Reference: [3] <author> Hwang, M. and Huang, X. </author> <title> Subphonetic Modeling with Markov States Senone. </title> <booktitle> in: IEEE International Conference on Acoustics, Speech, and Signal Processing. </booktitle> <volume> vol. I, </volume> <year> 1992, </year> <pages> pp. 3336. </pages>
Reference: [4] <author> Hwang, M., Huang, X., and Alleva, F. </author> <title> Predicting Unseen Triphones with Senones. </title> <booktitle> in: IEEE International Conference on Acoustics, Speech, and Signal Processing. </booktitle> <volume> vol. II, </volume> <year> 1993, </year> <pages> pp. 311314. </pages>
Reference-contexts: Due to the wide spectrum of users' platforms, one good compromise is to provide the best model and allow the user to dynamically decide how much resource he/she would like to allocate for the speech recognizer. The hierarchy in the senonic decision tree <ref> [4] </ref> provides a good downsizing architecture. Leaves of the same ancestor node represent Markov states of similar acoustic realization. Based on this principle, we first build the best acoustic model with as many parameters as possible.
Reference: [5] <author> Linde, Y., Buzo, A., and Gray, R. </author> <title> An Algorithm for Vector Quantizer Design. </title> <journal> IEEE Transactions on Communication, </journal> <volume> vol. COM-28 (1980), </volume> <pages> pp. 8495. </pages>
Reference-contexts: The Downsizing Algorithm The baseline system we built consisted of 6400 context-dependent (cd) senones, with 12 Gaussians per senone. The regular procedure to train an n 1 fl m 1 context-dependent model 1 is to * Use the LBG clustering algorithm <ref> [5] </ref> on the Markov state segmentation created by an existing model to generate the initial seed for the context-independent (ci) model. * Use Baum-Welch or Viterbi algorithm to train the ML ci model. * Train the cd model from the trained ci model.
References-found: 5

