URL: file://ftp.cs.utexas.edu/pub/neural-nets/papers/miikkulainen.natural-language.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/nn/pages/publications/abstracts.html
Root-URL: 
Title: Natural Language Processing With Modular PDP Networks and Distributed Lexicon flyz  
Author: Risto Miikkulainen and Michael G. Dyer 
Address: Los Angeles  
Affiliation: University of California,  
Abstract: An approach to connectionist natural language processing is proposed, which is based on hierarchically organized modular Parallel Distributed Processing (PDP) networks and a central lexicon of distributed input/output representations. The modules communicate using these representations, which are global and publicly available in the system. The representations are developed automatically by all networks while they are learning their processing tasks. The resulting representations reflect the regularities in the subtasks, which facilitates robust processing in the face of noise and damage, supports improved generalization, and provides expectations about possible contexts. The lexicon can be extended by cloning new instances of the items, that is, by generating a number of items with known processing properties and distinct identities. This technique combinatorially increases the processing power of the system. The recurrent FGREP module, together with a central lexicon, is used as a basic building block in modeling higher level natural language tasks. A single module is used to form case-role representations of sentences from word-by-word sequential natural language input. A hierarchical organization of four recurrent FGREP modules (the DISPAR system) is trained to produce fully expanded paraphrases of script-based stories, where unmentioned events and role fillers are inferred. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Alvarado, S., Dyer, M. G., and Flowers, M. </author> <year> (1990). </year> <title> Argument comprehension and retrieval for editorial text. </title> <journal> Knowledge-Based Systems, </journal> <volume> 3(2) </volume> <pages> 87-107. </pages> <note> 38 Ballard, </note> <author> D. H. </author> <year> (1987). </year> <title> Modular learning in neural networks. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Bower, G. H., Black, J. B., and Turner, T. J. </author> <year> (1979). </year> <title> Scripts in memory for text. </title> <journal> Cognitive Psychology, </journal> <volume> 11 </volume> <pages> 177-220. </pages>
Reference-contexts: What details are produced in the paraphrase depends on the training of the output networks. This result is consistent with psychological data on how people remember stories of familiar event sequences <ref> (Bower et al., 1979) </ref>. The distinction of what was actually mentioned and what was inferred becomes blurred.
Reference: <author> Brousse, O. and Smolensky, P. </author> <year> (1989). </year> <title> Virtual memories and massive generalization in connectionist combinatorial learning. </title> <booktitle> In Proceedings of the 11th Annual Conference of the Cognitive Science Society. </booktitle> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference-contexts: In addition, the instances automatically have intrinsic meaning coded in them. The processing knowledge is separate from the symbols that can be processed. With linear cost, the system can process a combinatorial number of inputs <ref> (Brousse and Smolensky, 1989) </ref> in a nontrivial task. 20 7 Processing sequential input and output: The recurrent FGREP module The sentence-processing architecture presented in the preceding sections relies on highly preprocessed input.
Reference: <author> Buchanan, B. G. and Shortliffe, E. H., </author> <title> editors (1985). Rule-Based Expert Systems: The MYCIN Experiments of the Stanford Heuristic Programming Project. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Cullingford, R. E. </author> <year> (1978). </year> <title> Script Application: Computer Understanding of Newspaper Stories. </title> <type> PhD thesis, </type> <institution> New Haven, CT: Department of Computer Science, Yale University. </institution> <type> Technical Report 116. </type>
Reference-contexts: In other words, script-based inferencing is grounded in the statistical regularities in the input examples, an issue which has not been addressed by symbolic script-processing models such as SAM <ref> (Cullingford, 1978) </ref>. 2 Methods for forming distributed representations Sentence case-role assignment is an example of a cognitive task that is well suited for modeling with connectionist systems.
Reference: <author> DeJong, G. F. </author> <year> (1979). </year> <title> Skimming Stories in Real Time: An Experiment in Integrated Understanding. </title> <type> PhD thesis, </type> <institution> New Haven, CT: Department of Computer Science, Yale University. </institution> <type> Technical Report 158. </type>
Reference: <author> Dolan, C. P. </author> <year> (1989). </year> <title> Tensor Manipulation Networks: Connectionist and Symbolic Approaches to Comprehension, Learning and Planning. </title> <type> PhD thesis, </type> <institution> Los Angeles: Computer Science Department, University of California, </institution> <address> Los Angeles. </address>
Reference: <author> Dolan, C. P. and Smolensky, P. </author> <year> (1989). </year> <title> Implementing a connectionist production system using tensor products. </title> <editor> In Touretzky, D. S., Hinton, G. E., and Sejnowski, T. J., editors, </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Duda, R. O., Hart, P. E., Nilsson, N. J., Reboh, R., Slocum, J., and Sutherland, G. L. </author> <year> (1977). </year> <title> Development of a computer-based consultant for mineral exploration. </title> <booktitle> Annual report, projects 5821 and 6415, </booktitle> <address> Menlo Park, CA: </address> <publisher> SRI International. </publisher>
Reference: <author> Dyer, M. G. </author> <year> (1983). </year> <title> In-Depth Understanding: A Computer Model of Integrated Processing for Narrative Comprehension. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Being able to create expectations automatically and cumulatively from the input representations turns out to be useful in building larger language-understanding systems. Such distributed expectations could replace the symbolic expectations traditionally used in natural language conceptual analyzers <ref> (e.g., Dyer, 1983) </ref>. 5.6 Generalization The term "generalization" commonly means processing inputs which the system has not seen before. In most cases this means extending the processing knowledge into new input patterns, which are different from all training patterns. Generalization in FGREP has a different character.
Reference: <author> Dyer, M. G. </author> <year> (1991). </year> <title> Symbolic NeuroEngineering for natural language processing: A multilevel research approach. </title> <editor> In Barnden, J. and Pollack, J., editors, </editor> <booktitle> Advances in Connectionist and Neural Computation Theory, </booktitle> <volume> Vol. </volume> <month> 1: </month> <title> High Level Connectionist Models. </title> <address> Norwood, NJ: </address> <publisher> Ablex. </publisher>
Reference-contexts: The choice is consistent throughout the story, because all sentences are generated from the same pattern in the food slot. Thus, DISPAR performs plausible role bindings: an essential task in high-level inferencing and postulated as very difficult for PDP systems to achieve <ref> (Dyer, 1991) </ref>. In general, it seems that a network which builds a stationary representation of a sequence might be quite sensitive to omissions and changes. Each input item is interpreted against the current position in the sequence by combining it with the previous hidden layer.
Reference: <author> Dyer, M. G., Cullingford, R. E., and Alvarado, S. </author> <year> (1987). </year> <title> Scripts. </title> <editor> In Shapiro, S. C., editor, </editor> <booktitle> Encyclopedia of Artificial Intelligence. </booktitle> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: For example, the penultimate layers of the family-tree network develop one representation for input and a different one for output. The hidden layer patterns are local, internal processing aids more than I/O representations which can be used in a larger environment. In the FGREP approach <ref> (Miikkulainen and Dyer, 1987, 1988, 1989a) </ref>, the representations are also developed automatically while the network is learning the processing task, by making use of the backpropagation error signal.
Reference: <author> Elman, J. L. </author> <year> (1989). </year> <title> Structured representations and connectionist models. </title> <booktitle> In Proceedings of the 11th Annual Conference of the Cognitive Science Society. </booktitle> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Elman, J. L. </author> <year> (1990). </year> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14(2) </volume> <pages> 179-211. </pages>
Reference: <author> Feldman, J. A. </author> <year> (1989). </year> <title> Neural representation of conceptual knowledge. </title> <editor> In Nadel, L., Cooper, L. A., Culicover, P., and Harnish, R. M., editors, </editor> <title> Neural Connections, Mental Computation. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Fellenz, C. B. </author> <year> (1989). </year> <title> A connectionist model of linguistic analysis. </title> <type> Master's thesis, </type> <institution> Chico, CA: California State University, Chico. </institution>
Reference-contexts: However, the distinction between the lexical word bat and the two concepts associated with it, baseball-bat and live-bat becomes blurred in this case. It would make more sense to develop separate representations for the distinct meanings of homonymous words. A modified version of FGREP was developed to do this <ref> (Fellenz, 1989) </ref>. The lexicon contains separate representations for the different meanings of ambiguous words, and the correct meaning is specified in the training data for each sentence.
Reference: <author> Fillmore, C. J. </author> <year> (1968). </year> <title> The case for case. In Bach, </title> <editor> E. and Harms, R. T., editors, </editor> <booktitle> Universals in Linguistic Theory. </booktitle> <address> New York: </address> <publisher> Holt, Rinehart and Winston. </publisher> <address> 39 Harnad, S. </address> <year> (1990). </year> <title> The symbol grounding problem. </title> <journal> Physica D, </journal> <volume> 42 </volume> <pages> 335-346. </pages>
Reference-contexts: Moreover, if some of the representations are fixed (e.g., the "blank" representation at all-0, and the "don't care" representation at all-0.5), all-identical representation sets are excluded altogether. 4 An example task: Assigning case roles to sentence constituents Case-role representation of sentences is based on the theory of thematic case roles <ref> (Fillmore, 1968) </ref>, adapted for computer modeling in conceptual dependency theory (Schank and Abelson, 1977; Schank and Riesbeck, 1981). In the basic version of the case-role assignment task, the syntactic structure of the sentence is given and consists of, for example, the subject, verb, object, and a with-clause.
Reference: <author> Harris, C. L. and Elman, J. L. </author> <year> (1989). </year> <title> Representing variable information with simple recurrent networks. </title> <booktitle> In Proceedings of the 11th Annual Conference of the Cognitive Science Society. </booktitle> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Hartigan, J. A. </author> <year> (1975). </year> <title> Clustering Algorithms. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: The system itself is not attempting categorization; it is forming the most efficient representation of each word for a particular task. Interestingly, hierarchical merge clustering algorithm <ref> (Hartigan, 1975) </ref> finds optimal clusters quite similar to the noun categories (Figure 4). Inspection of the representations in Figure 3 suggests that a single unit does not play a crucial role in the classification of items.
Reference: <author> Hinton, G. E. </author> <year> (1981). </year> <title> Implementing semantic networks in parallel hardware. </title> <editor> In Hinton, G. E. and Anderson, J. A., editors, </editor> <booktitle> Parallel Models of Associative Memory. </booktitle> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference-contexts: In the distributed approach, different I/O items are represented as different patterns of activity over the same set of units. One approach for forming these patterns is semantic feature encoding, used, for example, by McClelland and Kawamoto (1986) in the case-role assignment task <ref> (see, also, Hinton, 1981) </ref>. Each concept is classified along a predetermined set of dimensions such as human-nonhuman, soft-hard, and male-female. Each feature is assigned a processing unit (or a group of units, e.g. one for each value), and the classification becomes a pattern of activity over an assembly of units.
Reference: <author> Hinton, G. E. </author> <year> (1986). </year> <title> Learning distributed representations of concepts. </title> <booktitle> In Proceedings of the Eighth Annual Conference of the Cognitive Science Society. </booktitle> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference-contexts: Developing these patterns occurs as an essential part of learning the processing task, and they end up reflecting the regularities of the task <ref> (Hinton, 1986) </ref>. Another variant of the same approach was proposed by Elman (1989, 1990). A simple recurrent network is trained to predict the next word in the input word sequence.
Reference: <author> Inhelder, B. and Piaget, J. </author> <year> (1958). </year> <title> The Growth of Logical Thinking from Childhood to Adolescence. </title> <address> New York: </address> <publisher> Basic Books. </publisher>
Reference: <author> Jacobs, R. A. </author> <year> (1990). </year> <title> Task Decomposition Through Competition in a Modular Connectionist Architecture. </title> <type> PhD thesis, </type> <institution> Amherst, MA: Department of Computer and Information Science, University of Massachusetts, Amherst. </institution>
Reference: <author> Jain, A. N. </author> <year> (1989). </year> <title> A connectionist architecture for sequential symbolic domains. </title> <type> Technical Report CMU-CS-89-187, </type> <institution> Pittsburgh, PA: Computer Science Department, Carnegie Mellon University. </institution>
Reference: <author> Jordan, M. I. </author> <year> (1986). </year> <title> Attractor dynamics and parallelism in a connectionist sequential machine. </title> <booktitle> In Proceedings of the Eighth Annual Conference of the Cognitive Science Society. </booktitle> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Kohonen, T. </author> <year> (1984). </year> <title> Self-Organization and Associative Memory. </title> <address> Berlin; Heidelberg; New York: </address> <publisher> Springer. </publisher>
Reference-contexts: To obtain insight into this combined categorization, we first have to map the 12-dimensional representation vectors into two dimensions. One way to do this is Kohonen's self-organizing feature mapping <ref> (Kohonen, 1984) </ref>. This method is known to map clusters in the input space to clusters in the output space. The map is topological, that is, the distances in the map are not comparable (more dense regions are magnified), but the topological relations of the input space are preserved.
Reference: <author> Korf, R. E. </author> <year> (1987). </year> <title> Planning as search: A quantitative approach. </title> <journal> Artificial Intelligence, </journal> <volume> 33(1) </volume> <pages> 65-88. </pages>
Reference: <author> Lee, G. </author> <year> (1991). </year> <title> Distributed Semantic Representations for Goal/Plan Analysis of Narratives in a Connectionist Architecture. </title> <type> PhD thesis, </type> <institution> Los Angeles: Computer Science Department, University of California, </institution> <address> Los Angeles. </address>
Reference: <author> McClelland, J. L. and Kawamoto, A. H. </author> <year> (1986). </year> <title> Mechanisms of sentence processing: Assigning roles to constituents. </title> <editor> In McClelland, J. L. and Rumelhart, D. E., editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume 2: Psychological and Biological Models. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Their major appeal is that the processing knowledge can be extracted automatically from examples. The same architecture can learn to process a wide variety of inputs and take advantage of the implicit statistical regularities in the data, without having to be specifically programmed with particular data in mind <ref> (McClelland et al., 1986) </ref>. The gradual evolution of the system performance as it is learning often resembles human learning in the same task (Rumelhart and McClelland, 1987; Sejnowski and Rosenberg, 1987). Such PDP models typically have very little internal structure or architectural complexity.
Reference: <author> McClelland, J. L., Rumelhart, D. E., and Hinton, G. E. </author> <year> (1986). </year> <title> The appeal of parallel distributed processing. </title> <editor> In Rumelhart, D. E. and McClelland, J. L., editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume 1: Foundations. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Their major appeal is that the processing knowledge can be extracted automatically from examples. The same architecture can learn to process a wide variety of inputs and take advantage of the implicit statistical regularities in the data, without having to be specifically programmed with particular data in mind <ref> (McClelland et al., 1986) </ref>. The gradual evolution of the system performance as it is learning often resembles human learning in the same task (Rumelhart and McClelland, 1987; Sejnowski and Rosenberg, 1987). Such PDP models typically have very little internal structure or architectural complexity.
Reference: <author> Miikkulainen, R. </author> <year> (1990a). </year> <title> DISCERN: A Distributed Artificial Neural Network Model of Script Processing and Memory. </title> <type> PhD thesis, </type> <institution> Los Angeles: Computer Science Department, University of California, </institution> <address> Los Angeles. </address>
Reference-contexts: It is assumed that there exists a one-to-one mapping between lexical items and concepts and the lexicon automatically performs this mapping, so that the described system only needs to deal with concepts <ref> (see Miikkulainen, 1990a, 1990b) </ref>. Ways to deal with ambiguity are discussed in Section 11.2. 7 divided into assemblies, each holding one word representation at a time. Each unit in an input assembly is set to the activity value of the corresponding component in the lexicon entry. <p> if they were part of the original story. 10.8 Further extensions to script-processing architecture Question answering can be implemented as a separate module that receives as its input the slot-filler representation of the story, together with the representation of the question that has been parsed sequentially by the sentence-parser network <ref> (Miikkulainen, 1990a) </ref>. The module generates a case-role representation of the answer sentence, which is then output word by word by the sentence-generator network. A more complete script-processing system also has an episodic memory for the stories. <p> Later, it can be retrieved with a partial story representation, such as a question referring to that story, as a cue. The organization for the memory can be formed in a self-organizing process, and the result reflects the taxonomy of script-based stories <ref> (Miikkulainen, 1990a) </ref>. Pronoun reference is not a particularly hard problem in understanding script-based stories. People do not get confused when reading, for example, The waiter seated John. He asked him for lobster. He ate it. <p> The story networks of DISPAR contain the general semantic and script knowledge needed for inferencing, whereas the sentence networks form the specific language interface. Question-answering modules can be very easily added to the system, connecting them with the existing language-interface modules <ref> (Miikkulainen, 1990a) </ref>. After a story is read into the internal representation, the only information that is actually stored are the role bindings. The knowledge about the events of the script is in the weights of the sentence- and story-generator networks. <p> The modified FGREP network develops representations for all different meanings of ambiguous words and learns to disambiguate among them whenever possible. It is possible to go even further and completely separate the lexical and conceptual representations in the lexicon <ref> (Miikkulainen, 1990a) </ref>. The actual input to the system consists of representations for the lexical items. The lexicon translates each lexical representation to the appropriate concept representation, and the system internally processes only concept representations. 11.3 Extending the lexicon The system could extend its lexicon dynamically when needed.
Reference: <author> Miikkulainen, R. </author> <year> (1990b). </year> <title> A distributed feature map model of the lexicon. </title> <booktitle> In Proceedings of the 12th Annual Conference of the Cognitive Science Society. </booktitle> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Miikkulainen, R. </author> <year> (1990c). </year> <title> A PDP architecture for processing sentences with relative clauses. </title> <editor> In Karlgren, H., editor, </editor> <booktitle> Proceedings of the 13th International Conference on Computational Linguistics. </booktitle> <address> Helsinki, Finland: Yliopistopaino. </address> <note> 40 Miikkulainen, </note> <author> R. </author> <year> (1990d). </year> <title> Script recognition with hierarchical feature maps. </title> <journal> Connection Science, </journal> 2(1&2):83-101. 
Reference-contexts: More complex parsing with a single module is also possible, but it is better to build a parser from several modules. For example, two hierarchically organized modules can read sentences with multiple hierarchical relative clauses into a canonical internal representation <ref> (Miikkulainen, 1990c) </ref>. Building from FGREP modules is a powerful technique, and it is discussed in detail in the following sections. 9 A composite task: Paraphrasing script-based stories The recurrent FGREP modules were designed to be used as building blocks in more complex cognitive systems.
Reference: <author> Miikkulainen, R. and Dyer, M. G. </author> <year> (1987). </year> <title> Building distributed representations without micro-features. </title> <type> Technical Report UCLA-AI-87-17, </type> <institution> Los Angeles: Computer Science Department, University of California, </institution> <address> Los Angeles. </address>
Reference-contexts: For example, the penultimate layers of the family-tree network develop one representation for input and a different one for output. The hidden layer patterns are local, internal processing aids more than I/O representations which can be used in a larger environment. In the FGREP approach <ref> (Miikkulainen and Dyer, 1987, 1988, 1989a) </ref>, the representations are also developed automatically while the network is learning the processing task, by making use of the backpropagation error signal.
Reference: <author> Miikkulainen, R. and Dyer, M. G. </author> <year> (1988). </year> <title> Forming global representations with extended backpropagation. </title> <booktitle> In Proceedings of the IEEE International Conference on Neural Networks. </booktitle> <address> Piscataway, NJ: </address> <publisher> IEEE. </publisher>
Reference: <author> Miikkulainen, R. and Dyer, M. G. </author> <year> (1989a). </year> <title> Encoding input/output representations in connectionist cognitive systems. </title> <editor> In Touretzky, D. S., Hinton, G. E., and Sejnowski, T. J., editors, </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Miikkulainen, R. and Dyer, M. G. </author> <year> (1989b). </year> <title> A modular neural network architecture for sequential paraphrasing of script-based stories. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks. </booktitle> <address> Piscataway, NJ: </address> <publisher> IEEE. </publisher>
Reference-contexts: The hidden layer pattern is saved after each step in the sequence, and used as input to the hidden layer during the next step, together with the actual input. 7.2 Recurrent FGREP module Recurrent FGREP, the extension of FGREP to sequential input and output <ref> (Miikkulainen and Dyer, 1989b) </ref>, is based on Elman's Simple Recurrent Network (Elman, 1989, 1990; see, also, Jordan, 1986; Servan-Schreiber et al., 1989; St. John and McClelland, 1990).
Reference: <author> Minsky, M. </author> <year> (1963). </year> <title> Steps toward artificial intelligence. </title> <editor> In Feigenbaum, E. A. and Feldman, J. A., editors, </editor> <booktitle> Computers and Thought. </booktitle> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference: <author> Minsky, M. </author> <year> (1985). </year> <title> Society of Mind. </title> <address> New York: </address> <publisher> Simon & Schuster. </publisher>
Reference-contexts: A plausible approach for higher level cognitive modeling, therefore, is to construct the architecture from several interacting modules, which work together to produce the higher level behavior <ref> (Minsky, 1985) </ref>. Central issues to be addressed in this approach are: 1. How the overall task should be broken into modules and how the modules should be organized; 2. How PDP subnetworks should be designed so that they can serve as modular building blocks; 3.
Reference: <author> Osherson, D. N. </author> <year> (1974). </year> <title> Logical Abilities in Children. </title> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Pollack, J. B. </author> <year> (1987). </year> <title> Cascaded back-propagation on dynamic connectionist networks. </title> <booktitle> In Proceedings of the Ninth Annual Conference of the Cognitive Science Society. </booktitle> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Pollack, J. B. </author> <year> (1988). </year> <title> Recursive auto-associative memory: Devising compositional distributed representations. </title> <booktitle> In Proceedings of the 10th Annual Conference of the Cognitive Science Society. </booktitle> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference-contexts: The lexicon could play the role of a more general symbol table, containing representations for hierarchically more complex structures as well. This is essential in modeling formation of new concepts. A reduced description <ref> (see, e.g, Pollack, 1988) </ref> could be formed for a complex structure, and placed in the lexicon. A reference to the structure could be made using this lexicon entry, and communicated between modules like a word. A first step in this direction has already been taken in the DISPAR system.
Reference: <author> Quillian, M. R. </author> <year> (1967). </year> <title> Word concepts: A theory and simulation of some basic semantic capabilities. </title> <booktitle> Behavioral Science, </booktitle> <volume> 12 </volume> <pages> 410-430. </pages>
Reference-contexts: The types form semantic hierarchies, the instances inherit the properties of parent types and also accumulate specific properties of their own during processing. Whether implemented in a symbolic semantic network <ref> (Quillian, 1967) </ref> or in a semantic network/PDP hybrid (Sumida and Dyer, 1989), in effect, there are two separate systems with a very complex interaction. In the ID+content approach the identity and semantic content are kept together in a single representation and processed through the same pathways and structures.
Reference: <author> Ritter, H. </author> <year> (1989). </year> <title> Combining self-organizing maps. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks. </booktitle> <address> Piscataway, NJ: </address> <publisher> IEEE. </publisher>
Reference-contexts: Complexity of the mapping can sometimes be reduced by splitting the input space into regions, and assigning different subnetworks to different regions (Jacobs, 1990; Waibel, 1989). Alternatively, modules can be developed that process different subsections of the same input representation <ref> (Ritter, 1989) </ref>. Modularity in high-level cognitive processing has a somewhat different character. A complex cognitive task can often be broken down into simpler subtasks, which are performed sequentially, one task depending on the output of another (Ballard, 1987; Minsky, 1985).
Reference: <author> Rumelhart, D. E., Hinton, G. E., and McClelland, J. L. </author> <year> (1986a). </year> <title> A general framework for parallel distributed processing. </title> <editor> In Rumelhart, D. E. and McClelland, J. L., editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume 1: Foundations. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <year> (1986b). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E. and McClelland, J. L., editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume 1: Foundations. </booktitle> <address> Cam-bridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: The network learns the processing task by adapting the connection weights according to the standard backpropagation equations <ref> (Rumelhart et al., 1986b, pp. 327-329) </ref>. At the same time, representations for the input data are developed at the input layer according to the error signal extended to the input layer. Input and output layers are divided into assemblies and several items are represented and modified simultaneously. <p> In this analogy, the activation function is the identity function and its derivative is one. The error signal can be computed for each input unit as a simple case of the general error signal equation <ref> (Rumelhart et al., 1986b, Eq.14, pp. 326) </ref>: ffi 1i = j where ffi xy stands for the error signal for unit y in layer x, and w 1ij is the weight between unit i in the input layer and unit j in the first hidden layer.
Reference: <author> Rumelhart, D. E. and McClelland, J. L. </author> <year> (1987). </year> <title> Learning the past tenses of English verbs: Implicit rules or parallel distributed processing. </title> <editor> In MacWhinney, B., editor, </editor> <title> Mechanisms of Language Acquisition. </title> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Schank, R. and Abelson, R. </author> <year> (1977). </year> <title> Scripts, Plans, Goals, and Understanding An Inquiry into Human Knowledge Structures. </title> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. 41 Schank, </publisher> <editor> R. and Riesbeck, C. K., editors (1981). </editor> <booktitle> Inside Computer Understanding. </booktitle> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference-contexts: This approach allows the system to perform its task (e.g., answer questions) robustly, even if it does not capture all the subtleties of the words. A similar approach is, in fact, taken by NLP systems based on conceptual dependency theory <ref> (Schank and Abelson, 1977) </ref>. For example, both run and walk map to a hand-coded structure 19 the percentage of correct output words.
Reference: <author> Sejnowski, T. J. and Rosenberg, C. R. </author> <year> (1987). </year> <title> Parallel networks that learn to pronounce English text. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 145-168. </pages>
Reference: <author> Servan-Schreiber, D., Cleeremans, A., and McClelland, J. L. </author> <year> (1989). </year> <title> Learning sequential structure in simple recurrent networks. </title> <editor> In Touretzky, D. S., editor, </editor> <booktitle> Advances in Neural Information Processing Systems, volume 1. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sharkey, N. E., Sutcliffe, R. F. E., and Wobcke, W. R. </author> <year> (1986). </year> <title> Mixing binary and continuous connection schemes for knowledge access. </title> <booktitle> In Proceedings of the Sixth National Conference on Artificial Intelligence. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: As long as the required inferences are based on statistical regularities, they can be well modeled with PDP. It is quite possible to build connectionist systems that process stories with multiple scripts <ref> (see, e.g., Sharkey et al., 1986) </ref>, or even several simultaneously active scripts. Deviations from the ordinary events are harder to implement. This would require a higher level monitoring process that recognizes deviations and builds separate representations for them, possibly using techniques like plan analysis (Dolan, 1989; Lee, 1991).
Reference: <author> Simon, H. </author> <year> (1981). </year> <booktitle> The Sciences of the Artificial. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> St. John, M. F. </author> <year> (1990). </year> <title> The Story Gestalt Text Comprehension by Cue-Based Constraint Satisfaction. </title> <type> PhD thesis, </type> <institution> Pittsburgh, PA: Department of Psychology, Carnegie Mellon University. </institution>
Reference-contexts: John (1990) studied the limitations of the PDP approach in processing unusual and novel situations in his story-gestalt model. Although his architecture is quite different the story-gestalt model is an application of the sentence-gestalt architecture <ref> (St. John and McClelland, 1990) </ref> to sequences of propositions his findings are very similar to what we have come across in DISPAR. Processing knowledge in these models is based on statistical regularities, and the models cannot handle deviations from the regularities very well. <p> However, if the correlation between fancy restaurant and good food is broken with counterexamples in the training, the system has to make the taste of food a variable. This way, it is possible to train the network to handle deviations to a limited extent <ref> (St. John, 1990) </ref>. For example, if in 90% of the fancy-restaurant stories the food was good, and in 10% it was bad, the network would develop a strong expectation for good food. After reading a sentence specifying that the food was actually bad, it would change the binding to bad. <p> The network has no reason to abstract the idea of a symbolic all-or-none role binding. The bindings only emerge from the statistical correlations of the constituents <ref> (St. John, 1990) </ref>. As a result, PDP systems can naturally model semantic illusions, where the actual content of the text is overridden by semantically more likely content. But they cannot process truly novel role bindings according to a symbolic higher level rule.
Reference: <author> St. John, M. F. and McClelland, J. L. </author> <year> (1990). </year> <title> Learning and applying contextual constraints in sentence comprehension. </title> <journal> Artificial Intelligence, </journal> <volume> 46 </volume> <pages> 217-258. </pages>
Reference-contexts: John (1990) studied the limitations of the PDP approach in processing unusual and novel situations in his story-gestalt model. Although his architecture is quite different the story-gestalt model is an application of the sentence-gestalt architecture <ref> (St. John and McClelland, 1990) </ref> to sequences of propositions his findings are very similar to what we have come across in DISPAR. Processing knowledge in these models is based on statistical regularities, and the models cannot handle deviations from the regularities very well. <p> However, if the correlation between fancy restaurant and good food is broken with counterexamples in the training, the system has to make the taste of food a variable. This way, it is possible to train the network to handle deviations to a limited extent <ref> (St. John, 1990) </ref>. For example, if in 90% of the fancy-restaurant stories the food was good, and in 10% it was bad, the network would develop a strong expectation for good food. After reading a sentence specifying that the food was actually bad, it would change the binding to bad. <p> The network has no reason to abstract the idea of a symbolic all-or-none role binding. The bindings only emerge from the statistical correlations of the constituents <ref> (St. John, 1990) </ref>. As a result, PDP systems can naturally model semantic illusions, where the actual content of the text is overridden by semantically more likely content. But they cannot process truly novel role bindings according to a symbolic higher level rule.
Reference: <author> Sumida, R. A. and Dyer, M. G. </author> <year> (1989). </year> <title> Storing and generalizing multiple instances while maintaining knowledge-level parallelism. </title> <booktitle> In Proceedings of the 11th International Joint Conference on Artificial Intelligence. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The types form semantic hierarchies, the instances inherit the properties of parent types and also accumulate specific properties of their own during processing. Whether implemented in a symbolic semantic network (Quillian, 1967) or in a semantic network/PDP hybrid <ref> (Sumida and Dyer, 1989) </ref>, in effect, there are two separate systems with a very complex interaction. In the ID+content approach the identity and semantic content are kept together in a single representation and processed through the same pathways and structures.
Reference: <author> Touretzky, D. S. </author> <year> (1991). </year> <title> Connectionism and compositional semantics. </title> <editor> In Barnden, J. and Pollack, J., editors, </editor> <booktitle> Advances in Connectionist and Neural Computation Theory, </booktitle> <volume> Vol. </volume> <month> 1: </month> <title> High Level Connectionist Models. </title> <address> Norwood, NJ: </address> <note> Ablex. </note> <author> van Gelder, T. </author> <year> (1989). </year> <title> Distributed Representation. </title> <type> PhD thesis, </type> <institution> Pittsburgh, PA: Department of Philosophy, University of Pittsburgh. </institution>
Reference-contexts: Deviations from the ordinary events are harder to implement. This would require a higher level monitoring process that recognizes deviations and builds separate representations for them, possibly using techniques like plan analysis (Dolan, 1989; Lee, 1991). However, planning requires "dynamic inferencing" <ref> (Touretzky, 1991) </ref>, that is, putting several pieces of information together to form genuinely novel information, not just pattern transformation. Dynamic inferencing is currently an open problem in PDP research. 12.2 Dealing with exceptions and novel situations St.

References-found: 57

