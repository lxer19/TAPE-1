URL: ftp://ftp.cs.orst.edu/pub/tgd/papers/ml97-hebrl.ps.gz
Refering-URL: http://www.cs.orst.edu/~margindr/ML_RG/winter97-mlrg.html
Root-URL: 
Email: ftadepalli,tgdg@research.cs.orst.edu  
Title: Hierarchical Explanation-Based Reinforcement Learning  
Author: Prasad Tadepalli and Thomas G. Dietterich 
Address: Corvallis,Oregon 97331-3202  
Affiliation: Computer Science Department Oregon State University  
Abstract: Explanation-Based Reinforcement Learning (EBRL) was introduced by Dietterich and Flann as a way of combining the ability of Reinforcement Learning (RL) to learn optimal plans with the generalization ability of Explanation-Based Learning (EBL) (Di-etterich & Flann, 1995). We extend this work to domains where the agent must order and achieve a sequence of subgoals in an optimal fashion. Hierarchical EBRL can effectively learn optimal policies in some of these sequential task domains even when the subgoals weakly interact with each other. We also show that when a planner that can achieve the individual subgoals is available, our method converges even faster. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bertsekas, D. P. </author> <year> (1995). </year> <title> Dynamic Programming and Optimal Control. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, MA. </address>
Reference-contexts: This is justified because (a) only relative values are important for action selection, and (b) all these different states are "equivalent" in the sense that the probability of the next state for each action is the same for all of them. (See <ref> (Bertsekas, 1995) </ref> for a more rigorous explanation.) To encourage exploration, is initialized to a high value and is slowly decreased with a learning rate ff. This is the equivalent of optimism under uncertainty for Average-reward RL (Ok & Tadepalli, 1996).
Reference: <author> Dayan, P., & Hinton, G. E. </author> <year> (1993). </year> <title> Feudal reinforcement learning. </title> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <pages> 5. </pages>
Reference: <author> Dietterich, T. G., & Flann, N. </author> <year> (1995). </year> <title> Explanation-based learning and reinforcement learning: A unifed view. </title> <booktitle> In Proceedings of Machine Learning Conference. </booktitle>
Reference-contexts: 1 Introduction Reinforcement Learning (RL) has emerged as the method of choice for building autonomous agents that improve their performance with experience. One obstacle to scaling this approach to large problems is the lack of a robust and justifiable method to generalize from one experience to another. Dietterich and Flann <ref> (Dietterich & Flann, 1995) </ref> showed that Explanation-Based Learning (EBL) can be used to generalize the experience of a reinforcement learner across different states, provided the learner has access to a complete and correct domain theory. <p> EBRL corrects this problem by associating with each pattern the utilities of the states that match that pattern. If a state matches two patterns, then its utility is the higher of the utilities of the two patterns <ref> (Dietterich & Flann, 1995) </ref>. The EBRL method directly learns U Q -values for patterns of states rather than Q-values.
Reference: <author> Kaelbling, L. P. </author> <year> (1993). </year> <title> Hierarchical learning in stochastic domains: Preliminary results. </title> <booktitle> In Proceedings of Machine Learning Conference, </booktitle> <pages> pp. 167-173. </pages>
Reference: <author> Kaelbling, L. P., Littman, M. L., & Moore, A. W. </author> <year> (1996). </year> <title> Reinforcement learning: A survey. </title> <journal> Journal of Aritificial Intelligence Research, </journal> <volume> 4, </volume> <pages> 237-285. </pages>
Reference: <author> Koenig, S., & Simmons, R. G. </author> <year> (1996). </year> <title> The effect of representation and knowledge on goal-directed exploration with reinforcement-learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 22, </volume> <pages> 227-250. </pages>
Reference: <author> Ok, D., & Tadepalli, P. </author> <year> (1996). </year> <title> Auto-exploratory average reward reinforcement learning. </title> <booktitle> In Proceedings of AAAI-96. </booktitle>
Reference-contexts: This is the equivalent of optimism under uncertainty for Average-reward RL <ref> (Ok & Tadepalli, 1996) </ref>. The initial values of and ff were adjusted by trial and error. Unlike in the total reward case, we did not use any penalty for actions, because the initial value of plays the role of a penalty and encourages exploration.
Reference: <author> Schwartz, A. </author> <year> (1993). </year> <title> A reinforcement learning method for maximizing undiscounted rewards. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We also implemented a version of the Average-reward Hierarchical-EBRL method. This method uses a version of R-learning as the base method <ref> (Schwartz, 1993) </ref>. It estimates the value of the average reward of the current greedy policy and subtracts it from the immediate reward of each step before using it to update the D-function and the G-table.
Reference: <author> Shavlik, J. </author> <year> (1990). </year> <title> Acquiring recursive and iterative concepts with explanation-based learning. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 39-70. </pages>
Reference-contexts: Hierarchical EBRL inherits some problems of EBL such as the generalization-to-N problem, which is due to equating "an explanation" with an operator sequence <ref> (Shavlik, 1990) </ref>. In our treasure hunt domain, this problem prohibits learning a value function that can generalize across different locations of the robot. It is possible to address this problem either by generalizing over an abstract explanation or by using some inductive generalization over the results of EBRL.
Reference: <author> Singh, S. </author> <year> (1992). </year> <title> Transfer of learning by composing solutions of elemental sequential tasks. </title> <journal> Machine Learning, </journal> <volume> 8. </volume>
Reference-contexts: Unlike some other research in the RL literature where optimality is sacrificed for efficiency by using a hierarchy (Dayan & Hinton, 1993; Kaelbling, 1993), in our approach, optimality is retained. In this respect, it is similar to <ref> (Singh, 1992) </ref>, although, unlike in Singh's work, here we do not require the system to achieve the subgoals in a fixed order to get the reward. The subgoal order can be decided by the system based on the considerations of subgoal interactions and plan optimality.
Reference: <author> Watkins, C. J. C. H., & Dayan, P. </author> <year> (1992). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 279-292. </pages>
Reference-contexts: A trial consists of starting from some randomly chosen start state and reaching an absorbing goal state. For now, assume that the goal is to maximize the expected total reward received before reaching an absorbing goal state. One reinforcement learning approach for solving this problem is undiscounted Q-learning <ref> (Watkins & Dayan, 1992) </ref>. This method learns by updating the value of Q (i; u) for each i 2 S and u 2 A, which represents the expected total reward of executing action u in state i and from then on following the optimal policy.
References-found: 11

