URL: http://www.cis.hut.fi/~aapo/ps/SP98.ps
Refering-URL: http://www.cis.hut.fi/~oja/papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Independent Component Analysis by General Non-linear Hebbian-like Learning Rules  
Author: Aapo Hyvrinen and Erkki Oja 
Keyword: Key words: Independent Component Analysis, blind source separation, higher-order statistics, Hebbian learning, neural networks, robustness.  
Note: To appear in Signal Processing, vol. 64, no.  
Address: Rakentajanaukio 2 C, FIN-02150 Espoo, Finland,  
Affiliation: Helsinki University of Technology, Lab. of Computer and Information Science,  
Email: aapo.hyvarinen@hut.fi, erkki.oja@hut.fi  
Date: 3, 1998  
Abstract: A number of neural learning rules have been recently proposed for Independent Component Analysis (ICA). The rules are usually derived from information-theoretic criteria such as maximum entropy or minimum mutual information. In this paper, we show that in fact, ICA can be performed by very simple Hebbian or anti-Hebbian learning rules, which may have only weak relations to such information-theoretical quantities. Rather suprisingly, practically any non-linear function can be used in the learning rule, provided only that the sign of the Hebbian/anti-Hebbian term is chosen correctly. In addition to the Hebbian-like mechanism, the weight vector is here constrained to have unit norm, and the data is preprocessed by prewhitening, or sphering. These results imply that one can choose the non-linearity so as to optimize desired statistical or numerical criteria.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Amari, A. Cichocki, and H.H. Yang. </author> <title> A new learning algorithm for blind source separation. </title> <editor> In D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo, editors, </editor> <booktitle> Advances in Neural Information Processing 8 (Proc. NIPS'95), </booktitle> <pages> pages 757763. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference: [2] <author> A. Bell and T. J. Sejnowski. </author> <title> Edges are the independent components of natural scenes. </title> <booktitle> In Advances in Neural Information Processing 9 (NIPS*96), </booktitle> <pages> pages 831 837. </pages> <publisher> MIT Press, </publisher> <year> 1997. </year>
Reference: [3] <author> A.J. Bell and T.J. Sejnowski. </author> <title> An information-maximization approach to blind separation and blind deconvolution. </title> <booktitle> Neural Computation, </booktitle> <address> 7:11291159, </address> <year> 1995. </year>
Reference: [4] <author> J.-F. Cardoso. </author> <title> Eigen-structure of the fourth-order cumulant tensor with application to the blind source separation problem. </title> <booktitle> In Proc. ICASSP'90, </booktitle> <pages> pages 26552658, </pages> <address> Albuquerque, NM, USA, </address> <year> 1990. </year>
Reference: [5] <author> J.-F. Cardoso. </author> <title> Iterative techniques for blind source separation using only fourth-order cumulants. </title> <booktitle> In Proc. EUSIPCO, </booktitle> <pages> pages 739742, </pages> <address> Brussels, Belgium, </address> <year> 1992. </year>
Reference: [6] <author> A. Cichocki, S. I. Amari, and R. Thawonmas. </author> <title> Blind signal extraction using self-adaptive non-linear hebbian learning rule. </title> <booktitle> In Proc. NOLTA'96, </booktitle> <pages> pages 377380, </pages> <year> 1996. </year>
Reference: [7] <author> P. Comon. </author> <title> Independent component analysis a new concept? Signal Processing, </title> <address> 36:287314, </address> <year> 1994. </year>
Reference-contexts: The main applications of ICA are in blind source separation [17], feature extraction [2,18], and, in a slightly modited form, in blind deconvolution [9]. In the basic form of ICA <ref> [7] </ref>, we observe m scalar random variables x 1 ; x 2 ; :::; x m which are assumed to be linear combinations of n unknown independent components, or ICs, denoted by s 1 ; s 2 ; :::; s n . <p> For mathematical convenience, one usually detnes that the ICs s i have unit variance. This makes the (non-Gaussian) ICs unique, up to a multiplicative sign <ref> [7] </ref>. Note that this detnition of ICA implies no ordering of the ICs. The classical application of the ICA model is blind source separation [17], in which the observed values of x correspond to a realization of an m-dimensional discrete-time signal x (t), t = 1; 2; :::. <p> In other words, we assume m = n. We also assume that the data is prewhitened (or sphered), i.e., the x i are decorrelated and their variances are equalized by a linear transformation <ref> [7] </ref>. After this preprocessing, model (1) still holds, and the matrix A becomes orthogonal. 1.2 Hebbian and Anti-Hebbian Learning Rules Several neural algorithms for estimating the ICA model have been proposed recently, e.g., in [1,3,6,15,16,20,23]. Usually these algorithms use Hebbian or anti-Hebbian learning. <p> Indeed, one might distinguish between two approaches to ICA which we call the "top-down" approach and the "bottom-up" approach. In the top-down, or cumulant approach, one typically starts from the independence requirement. Mutual information is usually chosen as the measure for the degree of independence <ref> [7] </ref>. Because direct estimation of mutual information is very dicult, one then derives an approximative contrast function, often based on cumulant expansions of the densities, that can be computed more easily in practice. Finally, the problem is solved with an appropriate numerical method. <p> A drawback of our approach may be that it only works under a restricted model. In the basic case studied here, this is the linear mixing model, although generalizations are possible. 3 A General Hebbian-like Learning Rule 3.1 General One-unit Contrast Functions Contrast functions <ref> [7] </ref> provide a useful framework to describe ICA estimation. Usually they are based on a measure of the independence of the solutions. Denoting by w and x the weight vector and the input of a neuron, and slightly modifying the terminology in [7], one might also describe a contrast function as <p> Rule 3.1 General One-unit Contrast Functions Contrast functions <ref> [7] </ref> provide a useful framework to describe ICA estimation. Usually they are based on a measure of the independence of the solutions. Denoting by w and x the weight vector and the input of a neuron, and slightly modifying the terminology in [7], one might also describe a contrast function as a measure of how far the distribution of the output w T x of a neuron is from a gaussian distribution. The basic idea is then to tnd weight vectors (under a suitable constraint) that maximize the 'non-gaussianity' of the output.
Reference: [8] <author> N. Delfosse and P. Loubaton. </author> <title> Adaptive blind separation of independent sources: a deation approach. </title> <booktitle> Signal Processing, </booktitle> <address> 45:5983, </address> <year> 1995. </year>
Reference: [9] <author> D. Donoho. </author> <title> On minimum entropy deconvolution. </title> <booktitle> In Applied Time Series Analysis II, </booktitle> <pages> pages 565608. </pages> <publisher> Academic Press, </publisher> <year> 1981. </year> <month> 14 </month>
Reference-contexts: The main applications of ICA are in blind source separation [17], feature extraction [2,18], and, in a slightly modited form, in blind deconvolution <ref> [9] </ref>. In the basic form of ICA [7], we observe m scalar random variables x 1 ; x 2 ; :::; x m which are assumed to be linear combinations of n unknown independent components, or ICs, denoted by s 1 ; s 2 ; :::; s n . <p> The basic idea is then to tnd weight vectors (under a suitable constraint) that maximize the 'non-gaussianity' of the output. With such weight vectors, the output is equal to one of the independent components <ref> [9] </ref>. We mention in passing that 'non-gaussianity' is also a widely used criterion in projection pursuit [10,11], and thus the criteria and learning rules in this paper 5 also apply to the projection pursuit problem; however, in projection pursuit there is no underlying mixing model and thus no independent components.
Reference: [10] <author> J.H. Friedman. </author> <title> Exploratory projection pursuit. </title> <journal> J. of the American Statistical Association, </journal> <volume> 82(397):249266, </volume> <year> 1987. </year>
Reference: [11] <author> P.J. Huber. </author> <title> Robust Statistics. </title> <publisher> Wiley, </publisher> <year> 1981. </year>
Reference: [12] <author> A. Hyvrinen. </author> <title> A family of txed-point algorithms for independent component analysis. </title> <booktitle> In Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP'97), </booktitle> <pages> pages 39173920, </pages> <address> Munich, Germany, </address> <year> 1997. </year>
Reference-contexts: Many dierent measures of non-gaussianity can then be used for ICA estimation. A large family of such contrast functions was proposed by one of the authors in <ref> [12] </ref>. To construct a general contrast function, let us begin by choosing a suciently smooth even function, denoted by F . <p> Numerical simulations contrming these conjectures were reported in <ref> [12] </ref>. 7 3.3 Universal One-Unit Learning Rule The problem of choosing the right in (4) can be solved in two ways. First, we often have some a priori information on the distributions of the ICs. For example, speech signals are usually highly super-Gaussian.
Reference: [13] <author> A. Hyvrinen. </author> <title> Independent component analysis by minimization of mutual information. </title> <type> Technical Report A46, </type> <institution> Helsinki University of Technology, Laboratory of Computer and Information Science, </institution> <year> 1997. </year>
Reference: [14] <author> A. Hyvrinen. </author> <title> One-unit contrast functions for independent component analysis: A statistical analysis. </title> <booktitle> In Neural Networks for Signal Processing VII (Proc. IEEE Workshop on Neural Networks for Signal Processing), </booktitle> <pages> pages 388397, </pages> <address> Amelia Island, Florida, </address> <year> 1997. </year>
Reference-contexts: These learning functions were chosen according to the recommendations in <ref> [14] </ref>. The simulations consisted of blind source separation of four time signals that were linearly mixed to give raise to four mixture signals. First we applied the learning rules introduced above on signals that have visually simple forms.
Reference: [15] <author> A. Hyvrinen and E. Oja. </author> <title> Simple neuron models for independent component analysis. </title> <journal> Int. Journal of Neural Systems, </journal> <volume> 7(6):671687, </volume> <year> 1996. </year>
Reference-contexts: This is also true for a i . Thus, by choosing appropriately, one can estimate practically any independent component, using learning rule (4). The practical choice of is treated below. We assume here that the learning rate used in the implementation of (4) is annealed to zero <ref> [15] </ref> so that the stochastic gradient method really converges to one of the extrema of the objective function [21]. The theorem stated above considers local extrema, hence local convergence of the gradient algorithms only.
Reference: [16] <author> A. Hyvrinen and E. Oja. </author> <title> A fast txed-point algorithm for independent component analysis. </title> <booktitle> Neural Computation, </booktitle> <address> 9(7):14831492, </address> <year> 1997. </year>
Reference: [17] <author> C. Jutten and J. Herault. </author> <title> Blind separation of sources, part I: An adaptive algorithm based on neuromimetic architecture. </title> <booktitle> Signal Processing, </booktitle> <address> 24:110, </address> <year> 1991. </year>
Reference-contexts: 1 Introduction 1.1 Independent Component Analysis Independent Component Analysis (ICA) [7,17] is a recently developed signal processing technique whose goal is to express a set of random variables as linear combinations of statistically independent component variables. The main applications of ICA are in blind source separation <ref> [17] </ref>, feature extraction [2,18], and, in a slightly modited form, in blind deconvolution [9]. <p> For mathematical convenience, one usually detnes that the ICs s i have unit variance. This makes the (non-Gaussian) ICs unique, up to a multiplicative sign [7]. Note that this detnition of ICA implies no ordering of the ICs. The classical application of the ICA model is blind source separation <ref> [17] </ref>, in which the observed values of x correspond to a realization of an m-dimensional discrete-time signal x (t), t = 1; 2; :::. Then the components s i (t) are called source signals, which are usually original, uncorrupted signals or noise sources.
Reference: [18] <author> J. Karhunen, A. Hyvrinen, R. Vigario, J. Hurri, and E. Oja. </author> <title> Applications of neural blind separation to signal and image processing. </title> <booktitle> In Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP'97), </booktitle> <pages> pages 131134, </pages> <address> Munich, Germany, </address> <year> 1997. </year>
Reference: [19] <author> J. Karhunen and J. Joutsensalo. </author> <title> Representation and separation of signals using nonlinear PCA type learning. Neural Networks, </title> <address> 7(1):113127, </address> <year> 1994. </year>
Reference: [20] <author> J. Karhunen, E. Oja, L. Wang, R. Vigario, and J. Joutsensalo. </author> <title> A class of neural networks for independent component analysis. </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> 8(3):486504, </volume> <year> 1997. </year>
Reference-contexts: Of course, some kind of feedback is then necessary to prevent the weight vectors from converging to the same points. Because the columns of A are orthogonal, classical orthogonalizing feedbacks as in SGA [24], Sanger's algorithm [26], or the bigradient rule <ref> [20] </ref> can be used. A more detailed discussion of such feedbacks can be found in, e.g., [15,20].
Reference: [21] <author> H.J. Kushner and D.S. Clark. </author> <title> Stochastic approximation methods for constrained and unconstrained systems. </title> <publisher> Springer-Verlag, </publisher> <year> 1978. </year>
Reference-contexts: The practical choice of is treated below. We assume here that the learning rate used in the implementation of (4) is annealed to zero [15] so that the stochastic gradient method really converges to one of the extrema of the objective function <ref> [21] </ref>. The theorem stated above considers local extrema, hence local convergence of the gradient algorithms only.
Reference: [22] <author> E. Oja. </author> <title> Principal components, minor components, and linear neural networks. Neural Networks, </title> <address> 5:927935, </address> <year> 1992. </year>
Reference-contexts: Usually these algorithms use Hebbian or anti-Hebbian learning. Hebbian learning has proved to be a powerful paradigm for neural learning <ref> [22] </ref>. In the following, we call both Hebbian and anti-Hebbian learning rules 'Hebbian-like'. We use this general expression because the dierence between Hebbian and anti-Hebbian learning is sometimes quite vague. Typically, one uses the expression 'Hebbian' when the learning function is increasing and 'anti-Hebbian' when the learning function is decreasing.
Reference: [23] <author> E. Oja. </author> <title> The nonlinear PCA learning rule in independent component analysis. Neurocomputing, </title> <address> 17(1):2546, </address> <year> 1997. </year>
Reference-contexts: We then go on to prove that the extrema of the contrast function coincide with independent components. It is also possible to start from the algorithm directly like in <ref> [23] </ref> and show that independent components are asymptotically stable points of convergence for the algorithm. The bottom-up approach has some important advantages. Firstly, computational simplicity is an inherent advantage of this approach, because in this way the estimation of cumulant tensors etc. is avoided.
Reference: [24] <author> E. Oja and J. Karhunen. </author> <title> On stochastic approximation of the eigenvectors and eigenvalues of the expectation of a random matrix. </title> <journal> Journal of Math. Analysis and Applications, </journal> <volume> 106:6984, </volume> <year> 1985. </year>
Reference-contexts: Of course, some kind of feedback is then necessary to prevent the weight vectors from converging to the same points. Because the columns of A are orthogonal, classical orthogonalizing feedbacks as in SGA <ref> [24] </ref>, Sanger's algorithm [26], or the bigradient rule [20] can be used. A more detailed discussion of such feedbacks can be found in, e.g., [15,20].
Reference: [25] <author> E. Oja and L.-Y. Wang. </author> <title> Robust ttting by nonlinear neural units. Neural Networks, </title> <address> 9:435444, </address> <year> 1996. </year> <month> 15 </month>
Reference-contexts: The exact conditions for convergence are stated in the following theorem (see also Theorem 1 in <ref> [25] </ref>): Theorem 1 Assume that the input data follows the model (1), where x is prewhitened (sphered), and that F is a suciently smooth even function.
Reference: [26] <author> T.D. Sanger. </author> <title> Optimal unsupervised learning in a single-layered linear feedforward network. Neural Networks, </title> <address> 2:459473, </address> <year> 1989. </year>
Reference-contexts: Of course, some kind of feedback is then necessary to prevent the weight vectors from converging to the same points. Because the columns of A are orthogonal, classical orthogonalizing feedbacks as in SGA [24], Sanger's algorithm <ref> [26] </ref>, or the bigradient rule [20] can be used. A more detailed discussion of such feedbacks can be found in, e.g., [15,20].
Reference: [27] <author> O. Shalvi and E. Weinstein. </author> <title> New criteria for blind deconvolution of nonminimum phase systems (channels). </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> 36(2):312321, </volume> <year> 1990. </year>
Reference: [28] <author> H. H. Yang. </author> <title> Blind equalization of switching channels based on ICA and learning of learning rate. </title> <booktitle> In Proc. ICASSP'97, </booktitle> <pages> pages 18491852, </pages> <address> Munich, Germany, </address> <year> 1997. </year>
Reference: [29] <author> H. H. Yang and E.-S. Chng. </author> <title> An on-line learning algorithm for blind equalization. </title> <booktitle> In Proc. ICONIP'96, </booktitle> <pages> pages 317321, </pages> <address> Hong Kong, </address> <year> 1996. </year>
References-found: 29

