URL: http://www.aic.nrl.navy.mil/~gordon/papers/ml91.ps
Refering-URL: http://www.aic.nrl.navy.mil/~gordon/pubs.html
Root-URL: 
Title: AN ENHANCER FOR REACTIVE PLANS  
Author: Diana F. Gordon 
Address: Code 5514 Washington, D.C. 20375-5000  
Affiliation: Navy Center for Applied Research in Artificial Intelligence Naval Research Laboratory,  
Abstract: This paper describes our method for improving the comprehensibility, accuracy, and generality of reactive plans. A reactive plan is a set of reactive rules. Our method involves two phases: (1) formulate explanations of execution traces, and then (2) generate new reactive rules from the explanations. Since the explanation phase has been previously described, the primary focus of this paper is the rule generation phase. This latter phase consists of taking a subset of the explanations and using these explanations to generate a set of new reactive rules to add to the original set. The particular subset of the explanations that is chosen yields rules that provide new domain knowledge for handling knowledge gaps in the original rule set. The original rule set, in a complimentary manner, provides expertise to fill the gaps where the domain knowledge provided by the new rules is incomplete.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Brooks, R. </author> <year> (1990). </year> <title> Elephants don't play chess. </title> <booktitle> Robotics and Autonomous Systems 6: </booktitle> <pages> 3-15. </pages>
Reference-contexts: We have developed an enhancer for reactive plans that satisfies two goals. The first goal is to facilitate human understanding of plans generated by a particular class of reactive planners. This class consists of planners that gain effectiveness at the expense of human comprehensibility (e.g., <ref> [1] </ref> and [3]). The second goal is to improve the reliability and generality of reactive plans. Our approach is divided into two phases: an explanation phase and a rule generation phase. The explanation phase begins with the application of a reactive planning system to a problem.
Reference: [2] <author> Gervasio, M. and DeJong, G. </author> <year> (1989). </year> <title> Explanation-based learning of reactive operators. </title> <booktitle> Proceedings of the Sixth International Workshop on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Ithaca, NY. </address>
Reference-contexts: When EXGEN is added to the reactive planner, the newly generated set of rules helps the planner to achieve 100% success for a problem described in Section 2. Previous research is related to this work. EXGEN is similar to Learning Apprentice Systems (e.g., <ref> [2] </ref>, [10]) because it uses EBL to explain the behavior of an expert and then uses this explanation to generate new planning rules. <p> The difference is that Learning Apprentice Systems learn from a human to improve a system, whereas EXGEN learns from a system for the purposes of human understanding and system improvement. The type of EBL invoked by EXGEN is Plausible EBL, which has been developed by Gervasio and DeJong <ref> [2] </ref>. Our approach involves completing gaps in the domain theory for EBL. In terms of completing the gaps in a domain theory, several researchers provide solutions ([5], [10], [11], and [12]). However, we do not employ any of the previous approaches because their assumptions are not met in our situation.
Reference: [3] <author> Gordon, D. and Grefenstette, J. </author> <year> (1990). </year> <title> Explanations of empirically derived reactive plans. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Austin, TX. </address>
Reference-contexts: We have developed an enhancer for reactive plans that satisfies two goals. The first goal is to facilitate human understanding of plans generated by a particular class of reactive planners. This class consists of planners that gain effectiveness at the expense of human comprehensibility (e.g., [1] and <ref> [3] </ref>). The second goal is to improve the reliability and generality of reactive plans. Our approach is divided into two phases: an explanation phase and a rule generation phase. The explanation phase begins with the application of a reactive planning system to a problem. <p> The domain theory for EBL is empirically derived by a program that calculates average frequencies of sensor and action values from previous success traces. A more detailed description of the explanation phase may be found in <ref> [3] </ref>. 4 RULE GENERATION PHASE The rule generation phase is the newest part of our method. This phase involves two activities: generating rules and selecting useful rules. The first activity is done by EXGEN, but the latter is a combined human-system effort. EXGEN generates new reactive rules from explanations.
Reference: [4] <author> Grefenstette, J., Ramsey, C. and Schultz, A. </author> <year> (1990). </year> <title> Learning sequential decision rules using simulation models and competition. </title> <note> To appear in Machine Learning. </note>
Reference-contexts: A plan enhancer can increase the acceptability of reactive plans to humans. 2 THE REACTIVE PLANNER AND DOMAIN So far, we have tested our method with the SAMUEL system <ref> [4] </ref>. This system uses a genetic algorithm and other competition-based heuristics to learn high performance reactive plans in the absence of a strong domain theory. SAMUEL consists of three major components: a problem specific module, a performance module, and a learning module.
Reference: [5] <author> Hall, R. </author> <year> (1986). </year> <title> Learning by failing to explain. </title> <booktitle> Proceedings of the Fifth National Conference on Artifi cial Intelligence. </booktitle> <address> Philadelphia, PA. </address>
Reference: [6] <author> Harnad, S. </author> <year> (1990). </year> <title> The symbol grounding problem. </title> <journal> Physica D, </journal> <volume> 42(1/3): </volume> <pages> 335-346. </pages>
Reference-contexts: Instead, we use the original reactive plan to fill the knowledge gaps in the domain theory. Finally, the translation from high-level explanations to reactive rules is related to two techniques: specialization [7] and symbol grounding <ref> [6] </ref>. Despite these similarities to previous research, the approach we present here is novel. Reactive planners are becoming increasingly complex in terms of their behavior. In response to this growing complexity, we have developed a plan enhancer.
Reference: [7] <author> Langley, P. </author> <year> (1987). </year> <title> A general theory of discrimination learning. Production System Models of Learning and Development. </title> <editor> D. Klahr, P. Langley, and R. Neches (eds), </editor> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Instead, we use the original reactive plan to fill the knowledge gaps in the domain theory. Finally, the translation from high-level explanations to reactive rules is related to two techniques: specialization <ref> [7] </ref> and symbol grounding [6]. Despite these similarities to previous research, the approach we present here is novel. Reactive planners are becoming increasingly complex in terms of their behavior. In response to this growing complexity, we have developed a plan enhancer.
Reference: [8] <author> Schoppers, M. </author> <year> (1987). </year> <title> Universal plans for reactive robots in unpredictable environments. </title> <booktitle> Proceedings of the Tenth International Joint Conference on Artificial Intelligence. </booktitle> <address> Milan, Italy. </address>
Reference-contexts: 1 INTRODUCTION Reactive planning has proven to be a highly effective approach to planning (e.g., <ref> [8] </ref>). We have developed an enhancer for reactive plans that satisfies two goals. The first goal is to facilitate human understanding of plans generated by a particular class of reactive planners. This class consists of planners that gain effectiveness at the expense of human comprehensibility (e.g., [1] and [3]).
Reference: [9] <author> Schultz, A. and Grefenstette, J. </author> <year> (1990). </year> <title> Improving tactical plans with genetic algorithms. </title> <booktitle> Proceedings of the Tools for Artificial Intelligence Conference. </booktitle> <address> Herndon, VA. </address>
Reference-contexts: SAMUEL has been applied to a variety of domains. We would like to see if EXGEN can improve this system's performance in each of these domains. Schultz and Grefenstette <ref> [9] </ref> have demonstrated that the addition of manually developed rules can improve SAMUEL's performance for the Evasive Maneuvers (EM) problem. Therefore, we have decided to begin testing our approach to automatic rule generation on this problem.
Reference: [10] <editor> Tecuci, G. and Kodratoff, Y. </editor> <year> (1990). </year> <title> Apprenticeship learning in imperfect domain theories. Machine Learning: An Artificial Intelligence Approach, Volume III. </title> <editor> Y. Kodratoff and R. Michalski (eds), </editor> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: When EXGEN is added to the reactive planner, the newly generated set of rules helps the planner to achieve 100% success for a problem described in Section 2. Previous research is related to this work. EXGEN is similar to Learning Apprentice Systems (e.g., [2], <ref> [10] </ref>) because it uses EBL to explain the behavior of an expert and then uses this explanation to generate new planning rules. <p> The type of EBL invoked by EXGEN is Plausible EBL, which has been developed by Gervasio and DeJong [2]. Our approach involves completing gaps in the domain theory for EBL. In terms of completing the gaps in a domain theory, several researchers provide solutions ([5], <ref> [10] </ref>, [11], and [12]). However, we do not employ any of the previous approaches because their assumptions are not met in our situation. Instead, we use the original reactive plan to fill the knowledge gaps in the domain theory.
Reference: [11] <author> Van Lehn, K. </author> <year> (1990). </year> <title> Learning one subprocedure per lesson. </title> <journal> Artificial Intelligence, </journal> <volume> 31(1): </volume> <pages> 1-40. </pages>
Reference-contexts: The type of EBL invoked by EXGEN is Plausible EBL, which has been developed by Gervasio and DeJong [2]. Our approach involves completing gaps in the domain theory for EBL. In terms of completing the gaps in a domain theory, several researchers provide solutions ([5], [10], <ref> [11] </ref>, and [12]). However, we do not employ any of the previous approaches because their assumptions are not met in our situation. Instead, we use the original reactive plan to fill the knowledge gaps in the domain theory.
Reference: [12] <author> Wilkins, D. </author> <year> (1990). </year> <title> Knowledge base refinement as improving an incorrect and incomplete domain theory. Machine Learning: An Artificial Intelligence Approach, Volume III. </title> <editor> Y. Kodratoff and R. Michalski (eds), </editor> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The type of EBL invoked by EXGEN is Plausible EBL, which has been developed by Gervasio and DeJong [2]. Our approach involves completing gaps in the domain theory for EBL. In terms of completing the gaps in a domain theory, several researchers provide solutions ([5], [10], [11], and <ref> [12] </ref>). However, we do not employ any of the previous approaches because their assumptions are not met in our situation. Instead, we use the original reactive plan to fill the knowledge gaps in the domain theory.
References-found: 12

