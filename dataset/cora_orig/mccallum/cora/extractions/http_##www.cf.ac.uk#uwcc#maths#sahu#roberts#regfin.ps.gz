URL: http://www.cf.ac.uk/uwcc/maths/sahu/roberts/regfin.ps.gz
Refering-URL: http://www.stats.bris.ac.uk/MCMC/pages/list.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Adaptive Markov Chain Monte Carlo through Regeneration  Summary  
Author: Walter R. Gilks Gareth O. Roberts Sujit K. Sahu 
Date: January 26, 1998  
Address: Cambridge, CB2 2SR, UK.  Cambridge, CB2 1SB, UK.  Cardiff, CF2 4YH, UK.  
Affiliation: Medical Research Council Biostatistics Unit  Statistical Laboratory University of Cambridge  School of Mathematics University of Wales, Cardiff  
Abstract: Markov chain Monte Carlo (MCMC) is used for evaluating expectations of functions of interest under a target distribution . This is done by calculating averages over the sample path of a Markov chain having as its stationary distribution. For computational efficiency, the Markov chain should be rapidly mixing. This can sometimes be achieved only by careful design of the transition kernel of the chain, on the basis of a detailed preliminary exploratory analysis of . An alternative approach might be to allow the transition kernel to adapt whenever new features of are encountered during the MCMC run. However, if such adaptation occurs infinitely often, the stationary distribution of the chain may be disturbed. We describe a framework, based on the concept of Markov chain regeneration, which allows adaptation to occur infinitely often, but which does not disturb the stationary distribution of the chain or the consistency of sample-path averages. Key Words: Adaptive method; Bayesian inference; Gibbs sampling; Markov chain Monte Carlo; 
Abstract-found: 1
Intro-found: 1
Reference: <author> Basawa, I. V. and Prakasa Rao, B. L. S. </author> <title> (1980) Statistical Inference for Stochastic Processes. </title> <publisher> London: Academic press. </publisher>
Reference: <author> Bates, D. M. and Watts, D. G. </author> <title> (1988) Nonlinear Regression Analysis & its Applications. </title> <address> New York: </address> <publisher> John Wiley and Sons. </publisher>
Reference: <author> Besag, J., Green, E., Higdon, D. and Mengersen, K. </author> <title> (1995) Bayesian Computation and Stochastic Systems, (with discussion). </title> <journal> Statistical Science, </journal> <volume> 10, </volume> <pages> 3-66. </pages>
Reference: <author> Carlin, B. P and Gelfand, A. E. </author> <title> (1991) An iterative Monte Carlo method for nonconjugate Bayesian analysis. </title> <journal> Statistics and Computing, </journal> <volume> 1, </volume> <pages> 119-128. </pages>
Reference: <author> Cowles, M. K. and Rosenthal, J. S. </author> <year> (1996). </year> <title> A simulation approach to convergence rates for Markov chain Monte Carlo. </title> <type> Technical report, </type> <institution> University of Toronto, Department of Statistics. </institution>
Reference-contexts: Ideally we would like to perform splitting based on the n-step transition kernel of the Markov chain, where we allow n to vary (and probably increase) with dimension. Unfortunately, it is generally analytically intractable to perform n-step splitting for n &gt; 1 <ref> (though see Cowles and Rosenthal, 1996, for a numerical alternative) </ref>. 3.3 Hybrid samplers Several proposal distributions can be used in a hybrid sampler. At each iteration of the Markov chain, one of these proposal distributions can be chosen according to some random or systematic scheme (Tierney, 1994).
Reference: <author> Gelfand, A. E. and Sahu, S. K. </author> <title> (1994) On Markov Chain Monte Carlo Acceleration. </title> <journal> J. Comp. Graph. Statist., </journal> <volume> 3, </volume> <pages> 261-276. </pages>
Reference-contexts: There are several remedies to this problem. The most obvious would be to stop adapting after a pre-chosen iteration, or after a fixed number of adaptations, and commence the burn-in after the last adaptation <ref> (Gelfand and Sahu, 1994) </ref>. However, such strategies cannot guarantee that all the important features of will be discovered during the adaptation phase. We call this the pilot adaptation scheme, PAS, for future reference.
Reference: <author> Gelfand, A. E., Sahu, S. K. and Carlin, B. P. </author> <title> (1995) Efficient parametrizations for normal linear mixed models. </title> <journal> Biometrika 82, </journal> <pages> 479-488. </pages>
Reference-contexts: This is demonstrated by the huge range of problems routinely handled by the Gibbs sampling software BUGS (Spiegelhalter et al., 1996). However, in some applications of Gibbs sampling, model re-parameterization may be necessary to achieve rapid mixing <ref> (see for example, Gelfand et al., 1995) </ref>. Usually, analytic intractability of the target distribution prevents determination of the best parameterization in advance. Consequently it may be necessary to conduct preliminary experiments to determine an acceptable parameterization.
Reference: <author> Gelfand, A. E. and Smith, A. F. M. </author> <title> (1990) Sampling-based approaches to calculating marginal densities. </title> <journal> J. Amer. Statist. Assoc., </journal> <volume> 85, </volume> <pages> 398-409. </pages>
Reference: <author> Gelman, A., Roberts, G. O. and Gilks, W. R. </author> <title> (1996) Efficient Metropolis jumping rules. In Bayesian Statistics 5 (eds Bernardo, </title> <editor> J. M., Berger, J. O., Dawid, A. P. and Smith A. F. M.) </editor> <publisher> Oxford: Oxford University Press, </publisher> <pages> 599-608. </pages>
Reference-contexts: Then, for the Metropolis algorithm with proposal distribution q (x; y) = N m (x; kI m ) and splitting as defined by (11): max r (s; -) ! 0 exponentially as m ! 1; where k is set at its asymptotic optimal value 2:38 2 =m <ref> (Gelman, et al., 1996) </ref> and r (s; -) is the regeneration rate as given in (5). Proof see Appendix. Hence, although the method will work for low dimensional problems (see Section 4), for high-dimensional problems this method will fail to identify regeneration times. <p> The optimal acceptance rate is 0.275 with proposal densities of the form q (x; y) = N 5 (x; 2 I 5 ) with the the optimal value of being 1.10 <ref> (Gelman et al., 1996) </ref>. To implement adaptation, we use the regenerative scheme described in Section 3.2, with ~ x = 0 and D = fx : x 0 x &lt; dg, setting d = 16. Note that, in theory any positive value of d will work.
Reference: <author> Geman, S. and Geman, D. </author> <title> (1984) Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images. </title> <journal> IEEE Trans. Pattn. Anal. Mach. Intel., </journal> <volume> 6, </volume> <pages> 721-741. </pages> <note> 21 Gilks, </note> <author> W. R. and Roberts, G. O. </author> <title> (1996) Strategies for improving MCMC. In Markov Chain Monte Carlo in Practice (eds W. </title> <editor> R. Gilks, S. Richardson and D. J. Spiegelhalter). </editor> <publisher> London: Chapman and Hall, </publisher> <pages> 89-114. </pages>
Reference: <author> Gilks, W. R., Roberts, G. O. and George, E. I. </author> <title> (1994) Adaptive direction sampling. </title> <journal> The Statistician, </journal> <volume> 43, </volume> <pages> 179-189. </pages>
Reference: <author> Gilks, W. R. and Roberts, G. O. </author> <title> (1996) Strategies for improving MCMC. </title> <note> In MCMC in Practice (eds. W.R. </note>
Reference: <author> Gilks, D.J. Spiegelhalter and S. Richardson), Chapman and Hall, </author> <month> 89-114. </month>
Reference: <author> Hastings, </author> <title> W.K. (1970) Monte Carlo sampling methods using Markov chains and their applications. </title> <journal> Biometrika, </journal> <volume> 57, </volume> <pages> 97-109. </pages>
Reference: <author> Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H. and Teller, E. </author> <title> (1953) Equations of state calculations by fast computing machine. </title> <journal> J. Chem. Phys., </journal> <volume> 21, </volume> <pages> 1087-1091. </pages>
Reference: <author> Mykland, P. Tierney, L. and Yu, B. </author> <title> (1995) Regeneration in Markov chain samplers. </title> <journal> J. Amer. Statist. Assoc., </journal> <volume> 90, </volume> <pages> 233-241. </pages>
Reference: <author> Newton, M. A. and Raftery, A. E. </author> <title> (1994) Approximate Bayesian inference with the weighted likelihood bootstrap (with discussion). </title> <journal> J. R. Statist. Soc., B, </journal> <volume> 56, </volume> <pages> 3-48. </pages> <month> Nummelin, </month> <title> E (1984) General Irreducible Markov Chains and Non-Negative Operators. </title> <publisher> Cambridge: Cam-bridge University Press. </publisher>
Reference: <author> Ratkowsky, D. </author> <title> (1983) Nonlinear regression modelling. </title> <publisher> Mercel Dekker: </publisher> <address> New York. </address>
Reference: <author> Robert, C. P. </author> <title> (1995) Convergence Control Methods for Markov Chain Monte Carlo Algorithms. </title> <journal> Statistical Science, </journal> <volume> 10, </volume> <pages> 231-253. </pages>
Reference: <author> Roberts, G. O. </author> <title> (1996) Markov chain concepts related to sampling algorithms. </title> <note> In MCMC in Practice (eds. </note>
Reference-contexts: simplicity of exposition, we present only the simplest schemes; see Mykland et al. for a more general treatment. 3.1 The Independence Sampler To be effective, the independence proposal distribution, f , should be similar to but with heavier tails; a poor choice for f can produce a non-geometrically ergodic chain <ref> (Roberts and Tweedie, 1996) </ref>. In practice, a suitable f is not usually known. This suggests using the adaptive strategy outlined in Section 2.3 to adapt on f . The independence sampler is the easiest MCMC sampler to split.
Reference: <author> W.R. Gilks, D.J. Spiegelhalter and S. Richardson), Chapman and Hall, </author> <month> 45-57. </month>
Reference: <author> Roberts, G. O. and Tweedie, R. L. </author> <title> (1996) Geometric convergence and central limit theorems for multidimensional Hastings Metropolis algorithms. </title> <journal> Biometrika, </journal> <volume> 83, 1, </volume> <year> 1996, </year> <pages> 96-110. </pages>
Reference-contexts: simplicity of exposition, we present only the simplest schemes; see Mykland et al. for a more general treatment. 3.1 The Independence Sampler To be effective, the independence proposal distribution, f , should be similar to but with heavier tails; a poor choice for f can produce a non-geometrically ergodic chain <ref> (Roberts and Tweedie, 1996) </ref>. In practice, a suitable f is not usually known. This suggests using the adaptive strategy outlined in Section 2.3 to adapt on f . The independence sampler is the easiest MCMC sampler to split.
Reference: <author> Spiegelhalter, D. J., Thomas, A. and Best, N. G. </author> <title> (1996) Computation on Bayesian graphical models. In Bayesian Statistics 5, </title> <editor> (eds J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith). </editor> <publisher> Oxford: Oxford University Press, </publisher> <pages> 407-426. </pages>
Reference-contexts: In many applications, mixing is rapid for untuned MCMC methods such as the Gibbs sampler (Geman and Geman, 1984; Gelfand and Smith, 1990). This is demonstrated by the huge range of problems routinely handled by the Gibbs sampling software BUGS <ref> (Spiegelhalter et al., 1996) </ref>. However, in some applications of Gibbs sampling, model re-parameterization may be necessary to achieve rapid mixing (see for example, Gelfand et al., 1995). Usually, analytic intractability of the target distribution prevents determination of the best parameterization in advance.
Reference: <author> Tierney, L. </author> <title> (1994) Markov chains for exploring posterior distributions (with discussion). </title> <journal> Ann. Statist., </journal> <volume> 22, </volume> <pages> 1701-1762. </pages>
Reference-contexts: If the chain is irreducible, g N is a consistent estimator of IE [g] <ref> (see for example Tierney, 1994) </ref>. One practical difficulty in using g N as an estimator of IE [g] comes from the dependence within the sequence X n . <p> At each iteration of the Markov chain, one of these proposal distributions can be chosen according to some random or systematic scheme <ref> (Tierney, 1994) </ref>.
References-found: 24

