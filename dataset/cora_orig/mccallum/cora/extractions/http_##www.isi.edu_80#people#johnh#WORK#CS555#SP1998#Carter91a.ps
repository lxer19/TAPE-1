URL: http://www.isi.edu:80/people/johnh/WORK/CS555/SP1998/Carter91a.ps
Refering-URL: http://www.isi.edu:80/people/johnh/WORK/CS555/SP1998/old.html
Root-URL: http://www.isi.edu
Title: Implementation and Performance of Munin  
Author: John B. Carter, John K. Bennett, and Willy Zwaenepoel 
Address: Houston, Texas  
Affiliation: Computer Systems Laboratory Rice University  
Abstract: Munin is a distributed shared memory (DSM) system that allows shared memory parallel programs to be executed efficiently on distributed memory multiprocessors. Munin is unique among existing DSM systems in its use of multiple consistency protocols and in its use of release consistency. In Munin, shared program variables are annotated with their expected access pattern, and these annotations are then used by the runtime system to choose a consistency protocol best suited to that access pattern. Release consistency allows Munin to mask network latency and reduce the number of messages required to keep memory consistent. Munin's multi-protocol release consistency is implemented in software using a delayed update queue that buffers and merges pending outgoing writes. A sixteen-processor prototype of Munin is currently operational. We evaluate its implementation and describe the execution of two Munin programs that achieve performance within ten percent of message passing implementations of the same programs. Munin achieves this level of performance with only minor annotations to the shared memory programs. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, R. Simoni, J. Hennessy, and M. Horowitz. </author> <title> An evaluation of directory schemes for cache coherence. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 280-289, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Finally, PreAcquire () advises Munin to acquire a local copy of a particular object in anticipation of future use, thus avoiding the latency caused by subsequent read misses. 3 Implementation 3.1 Overview Munin executes a distributed directory-based cache consistency protocol <ref> [1] </ref> in software, in which each directory entry corresponds to a single object. Munin also implements locks and barriers, using a distributed queue-based synchronization protocol [20, 26]. During compilation, the sharing annotations are read by the Munin preprocessor, and an auxiliary file is created for each input file.
Reference: [2] <author> Anant Agarwal, Beng-Hong Lim, David Kranz, and John Kubiatowicz. </author> <month> APRIL: </month> <title> A processor architecture for multiprocessing. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The goal of Midway is to minimize communications costs by aggressively exploiting the relationship between shared variables and the synchronization objects that protect them. Recently, designs for hardware distributed shared memory machines have been published <ref> [2, 23] </ref>. Our work is most related to the DASH project [23], from which we adapt the concept of release consistency. Unlike Munin, though, DASH uses a write-invalidate protocol for all consistency maintenance. <p> The APRIL machine takes a different approach in combatting the latency problem on distributed shared memory machines <ref> [2] </ref>. APRIL provides sequential consistency, but relies on extremely fast processor switching to overlap memory latency with computation. A technique similar to the delayed update queue was used by the Myrias SPS multiprocessor [13].
Reference: [3] <author> Henri E. Bal and Andrew S. Tanenbaum. </author> <title> Distributed programming with shared data. </title> <booktitle> In Proceedings of the IEEE CS 1988 International Conference on Computer Languages, </booktitle> <pages> pages 82-91, </pages> <month> Oc-tober </month> <year> 1988. </year>
Reference-contexts: A load or a store is said to have "performed" when it has performed with respect to all other processors. Previous DSM systems <ref> [3, 10, 17, 25, 27] </ref> are based on sequential consistency [22]. Sequential consistency requires, roughly speaking, that each modification to a shared object become visible immediately to the other processors in the system. Release consistency postpones until the next release the time at which updates must become visible. <p> This anecdote emphasizes the difficulty of writing efficient message passing programs, and serves to emphasize the value of a DSM system like Munin. 5 Related Work A number of software DSM systems have been developed <ref> [3, 8, 10, 17, 25, 27] </ref>. All, except Midway [8], use sequential consistency [22]. Munin's use of release consistency only requires consistency to be enforced at specific synchronization points, with the resulting reduction in latency and number of messages exchanged. <p> Munin's use of multiple-writer protocols avoids the adverse effects of false sharing, without introducing the delays caused by locking a segment to a processor. Orca is also an object-oriented DSM system, but its consistency management is based on an efficient reliable ordered broadcast protocol <ref> [3] </ref>. For reasons of scalability, Munin does not rely on broadcast. In Orca, both invalidate and update protocols can be used. Munin also supports a wider variety of protocols. Unlike the designs discussed above, in Amber the programmer is responsible for the distribution of data among processors [10].
Reference: [4] <author> Vasanth Balasundaram, Geoffrey Fox, Ken Kennedy, and Uli Kremer. </author> <title> An interactive environment for data partitioning and distribution. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, South Carolina, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Munin's implementation of locks is similar to existing implementations on shared memory multiprocessors [20, 26]. An alternative approach for parallel processing on distributed memory machines is to have the compiler produce a message passing program starting from a sequential program, annotated by the programmer with data partitions <ref> [4, 30] </ref>.
Reference: [5] <author> John K. Bennett, John B. Carter, and Willy Zwaenepoel. Munin: </author> <title> Distributed shared memory based on type-specific memory coherence. </title> <booktitle> In Proceedings of the 1990 Conference on the Principles and Practice of Parallel Programming, </booktitle> <month> March </month> <year> 1990. </year>
Reference-contexts: It makes use of a version of the V kernel [11] that allows user threads to handle page faults and to modify page tables. A preliminary Munin design paper has been published previously <ref> [5] </ref>, as well as some measurements on shared memory programs that corroborate the basic design [6].
Reference: [6] <author> John K. Bennett, John B. Carter, and Willy Zwaenepoel. </author> <title> Adaptive software cache management for distributed shared memory architectures. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 125-134, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: It makes use of a version of the V kernel [11] that allows user threads to handle page faults and to modify page tables. A preliminary Munin design paper has been published previously [5], as well as some measurements on shared memory programs that corroborate the basic design <ref> [6] </ref>. This paper presents a refinement of the design, and then concentrates on the implementation of Munin and its performance. 2 Munin Overview 2.1 Munin Programming Munin programmers write parallel programs using threads, as they would on a shared memory multiprocessor [7]. <p> No modifications are necessary to these programs, other than making all synchronization operations utilize the Munin synchronization facilities. 2.3 Multiple Consistency Protocols Several studies of shared memory parallel programs have indicated that no single consistency protocol is best suited for all parallel programs <ref> [6, 14, 15] </ref>. Furthermore, within a single program, different shared variables are accessed in different ways and a particular variable's access pattern can change during execution [6]. Munin allows a separate consistency protocol for each shared variable, tuned to the access pattern of that particular variable. <p> Furthermore, within a single program, different shared variables are accessed in different ways and a particular variable's access pattern can change during execution <ref> [6] </ref>. Munin allows a separate consistency protocol for each shared variable, tuned to the access pattern of that particular variable. Moreover, the protocol for a variable can be changed over the course of the execution of the program. <p> These annotations correspond to the expected sharing pattern for the variable. The current prototype supports a small collection of these annotations that closely correspond to the sharing patterns observed in our earlier study of shared memory access patterns <ref> [6] </ref>. The low-level protocol parameters control specific aspects of the individual protocols, such as whether an object may be replicated or whether to use invalidation or update to maintain consistency. <p> Since it is quite common for an object to be updated multiple times between DUQ flushes <ref> [6] </ref>, the added overhead of handling multiple page faults makes this approach generally unacceptable. The second approach was used successfully by the Emerald system [9]. <p> This approach is an attractive alternative for systems that do not support fast page fault handling or modification of virtual memory mappings, such as the iPSC-i860 hypercube [12]. However, if the number of writes to a particular object between DUQ flushes is high, as is often the case <ref> [6] </ref>, this approach will perform relatively poorly because each write to a shared object will be slowed. We intend to study this approach more closely in future system implementations. 3.4 Synchronization Support Synchronization objects are accessed in a fundamentally different way than data objects [6], so Munin does not provide synchronization <p> high, as is often the case <ref> [6] </ref>, this approach will perform relatively poorly because each write to a shared object will be slowed. We intend to study this approach more closely in future system implementations. 3.4 Synchronization Support Synchronization objects are accessed in a fundamentally different way than data objects [6], so Munin does not provide synchronization through shared memory. Rather, each Munin node interacts with the other nodes to provide a high-level synchronization service. Munin provides support for distributed locks and barriers. More elaborate synchronization objects, such as monitors and atomic integers, can be built using these basic mechanisms.
Reference: [7] <author> Brian N. Bershad, Edward D. Lazowska, and Henry M. Levy. </author> <title> PRESTO: A system for object-oriented parallel programming. </title> <journal> Software| Practice and Experience, </journal> <volume> 18(8) </volume> <pages> 713-732, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Other than that, Munin provides thread, synchronization, and data sharing facilities like those found in shared memory parallel programming systems <ref> [7] </ref>. We report on the performance of two Munin programs: Matrix Multiply and Successive Over-Relaxation (SOR). We have hand-coded the same programs on the same hardware using message passing, taking special care to ensure that the two versions of each program perform identical computations. <p> This paper presents a refinement of the design, and then concentrates on the implementation of Munin and its performance. 2 Munin Overview 2.1 Munin Programming Munin programmers write parallel programs using threads, as they would on a shared memory multiprocessor <ref> [7] </ref>. Munin provides library routines, CreateThread () and DestroyThread (), for this purpose. Any required user initialization is performed by a sequential user init () routine, in which the programmer may also specify the number of threads and processors to be used.
Reference: [8] <author> Brian N. Bershad and Matthew J. Zekauskas. </author> <title> Shared memory parallel programming with entry consistency for distributed memory multiprocessors. </title> <type> Technical Report CMU-CS-91-170, </type> <institution> Carnegie-Mellon University, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: This anecdote emphasizes the difficulty of writing efficient message passing programs, and serves to emphasize the value of a DSM system like Munin. 5 Related Work A number of software DSM systems have been developed <ref> [3, 8, 10, 17, 25, 27] </ref>. All, except Midway [8], use sequential consistency [22]. Munin's use of release consistency only requires consistency to be enforced at specific synchronization points, with the resulting reduction in latency and number of messages exchanged. <p> This anecdote emphasizes the difficulty of writing efficient message passing programs, and serves to emphasize the value of a DSM system like Munin. 5 Related Work A number of software DSM systems have been developed [3, 8, 10, 17, 25, 27]. All, except Midway <ref> [8] </ref>, use sequential consistency [22]. Munin's use of release consistency only requires consistency to be enforced at specific synchronization points, with the resulting reduction in latency and number of messages exchanged. Ivy uses a single-writer, write-invalidate protocol, with virtual memory pages as the units of consistency [25]. <p> Munin stays closer to the more familiar shared memory programming model, hopefully improving its acceptance with parallel programmers. Midway <ref> [8] </ref> proposes a DSM system with entry consistency, a memory consistency model weaker than release consistency. The goal of Midway is to minimize communications costs by aggressively exploiting the relationship between shared variables and the synchronization objects that protect them.
Reference: [9] <author> Andrew Black, Norman Hutchinson, Eric Jul, and Henry Levy. </author> <title> Object structure in the Emerald system. </title> <booktitle> In Proceedings of the ACM Conference on Object-Oriented Programming Systems, Languages and Applications, </booktitle> <pages> pages 78-86, </pages> <month> October </month> <year> 1986. </year> <journal> Special Issue of SIGPLAN Notices, </journal> <volume> Volume 21, Number 11, </volume> <month> November, </month> <year> 1986. </year>
Reference-contexts: Munin currently does not perform any thread migration or global scheduling. User threads are run in a round robin fashion on the node on which they were created. A Munin shared object corresponds to a single shared variable, although like Emerald <ref> [9] </ref>, the programmer can specify that a collection of variables be treated as a single object or that a large variable be treated as a number of independent objects by the runtime system. By default, variables larger than a virtual memory page are broken into multiple page-sized objects. <p> Since it is quite common for an object to be updated multiple times between DUQ flushes [6], the added overhead of handling multiple page faults makes this approach generally unacceptable. The second approach was used successfully by the Emerald system <ref> [9] </ref>. We chose not to explore this approach in the prototype because we have a relatively fast page fault handler, and we did not want to modify the compiler.
Reference: [10] <author> Jeffrey S. Chase, Franz G. Amador, Edward D. Lazowska, Henry M. Levy, and Richard J. Little-field. </author> <title> The Amber system: Parallel programming on a network of multiprocessors. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 147-158, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: A load or a store is said to have "performed" when it has performed with respect to all other processors. Previous DSM systems <ref> [3, 10, 17, 25, 27] </ref> are based on sequential consistency [22]. Sequential consistency requires, roughly speaking, that each modification to a shared object become visible immediately to the other processors in the system. Release consistency postpones until the next release the time at which updates must become visible. <p> This anecdote emphasizes the difficulty of writing efficient message passing programs, and serves to emphasize the value of a DSM system like Munin. 5 Related Work A number of software DSM systems have been developed <ref> [3, 8, 10, 17, 25, 27] </ref>. All, except Midway [8], use sequential consistency [22]. Munin's use of release consistency only requires consistency to be enforced at specific synchronization points, with the resulting reduction in latency and number of messages exchanged. <p> For reasons of scalability, Munin does not rely on broadcast. In Orca, both invalidate and update protocols can be used. Munin also supports a wider variety of protocols. Unlike the designs discussed above, in Amber the programmer is responsible for the distribution of data among processors <ref> [10] </ref>. The system does not attempt to automatically move or replicate data. Good speedups are reported for SOR running on Amber. Munin automates many aspects of data distribution, and still remains efficient by asking the programmer to specify the expected access patterns for shared data variables.
Reference: [11] <author> David R. Cheriton and Willy Zwaenepoel. </author> <title> The distributed V kernel and its performance for disk-less workstations. </title> <booktitle> In Proceedings of the 9th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 129-140, </pages> <month> October </month> <year> 1983. </year>
Reference-contexts: The current prototype is implemented on a workstation-based distributed memory multiprocessor consisting of 16 SUN-3/60s connected by a dedicated 10 Mbps Ethernet. It makes use of a version of the V kernel <ref> [11] </ref> that allows user threads to handle page faults and to modify page tables. A preliminary Munin design paper has been published previously [5], as well as some measurements on shared memory programs that corroborate the basic design [6].
Reference: [12] <author> Intel Corporation. </author> <title> i860 64-bit Microprocessor Programmer's Manual. </title> <address> Santa Clara, California, </address> <year> 1990. </year>
Reference-contexts: This approach is an attractive alternative for systems that do not support fast page fault handling or modification of virtual memory mappings, such as the iPSC-i860 hypercube <ref> [12] </ref>. However, if the number of writes to a particular object between DUQ flushes is high, as is often the case [6], this approach will perform relatively poorly because each write to a shared object will be slowed.
Reference: [13] <author> Myrias Corporation. </author> <title> System Overview. </title> <address> Edmon-ton, Alberta, </address> <year> 1990. </year>
Reference-contexts: APRIL provides sequential consistency, but relies on extremely fast processor switching to overlap memory latency with computation. A technique similar to the delayed update queue was used by the Myrias SPS multiprocessor <ref> [13] </ref>. It performed the copy-on-write and diff in hardware, but required a restricted form of parallelism to ensure correctness. Munin's implementation of locks is similar to existing implementations on shared memory multiprocessors [20, 26].
Reference: [14] <author> Susan J. Eggers and Randy H. Katz. </author> <title> A characterization of sharing in parallel programs and its application to coherency protocol evaluation. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 373-383, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: No modifications are necessary to these programs, other than making all synchronization operations utilize the Munin synchronization facilities. 2.3 Multiple Consistency Protocols Several studies of shared memory parallel programs have indicated that no single consistency protocol is best suited for all parallel programs <ref> [6, 14, 15] </ref>. Furthermore, within a single program, different shared variables are accessed in different ways and a particular variable's access pattern can change during execution [6]. Munin allows a separate consistency protocol for each shared variable, tuned to the access pattern of that particular variable.
Reference: [15] <author> Susan J. Eggers and Randy H. Katz. </author> <title> The effect of sharing on the cache and bus performance of parallel programs. </title> <booktitle> In Proceedings of the 3rd International Conference on Architectural Support for Programming Languages and Systems, </booktitle> <pages> pages 257-270, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: No modifications are necessary to these programs, other than making all synchronization operations utilize the Munin synchronization facilities. 2.3 Multiple Consistency Protocols Several studies of shared memory parallel programs have indicated that no single consistency protocol is best suited for all parallel programs <ref> [6, 14, 15] </ref>. Furthermore, within a single program, different shared variables are accessed in different ways and a particular variable's access pattern can change during execution [6]. Munin allows a separate consistency protocol for each shared variable, tuned to the access pattern of that particular variable.
Reference: [16] <author> Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> Performance evaluations of memory consistency models for shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Systems, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: We believe this is not a major constraint, as many shared memory parallel programming environments already provide efficient synchronization packages. There is therefore little incentive for programmers to implement separate mechanisms. Unlike DASH, Munin does not require that each individual shared memory access be marked. Gharachorloo et al. <ref> [16, 19] </ref> have shown that a large class of programs, essentially programs with "enough" synchronization, produce the same results on a release-consistent memory as on a sequentially-consistent memory.
Reference: [17] <author> Brett D. Fleisch and Gerald J. Popek. </author> <title> Mirage: A coherent distributed shared memory design. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 211-23, </pages> <month> Decem-ber </month> <year> 1989. </year>
Reference-contexts: A load or a store is said to have "performed" when it has performed with respect to all other processors. Previous DSM systems <ref> [3, 10, 17, 25, 27] </ref> are based on sequential consistency [22]. Sequential consistency requires, roughly speaking, that each modification to a shared object become visible immediately to the other processors in the system. Release consistency postpones until the next release the time at which updates must become visible. <p> This anecdote emphasizes the difficulty of writing efficient message passing programs, and serves to emphasize the value of a DSM system like Munin. 5 Related Work A number of software DSM systems have been developed <ref> [3, 8, 10, 17, 25, 27] </ref>. All, except Midway [8], use sequential consistency [22]. Munin's use of release consistency only requires consistency to be enforced at specific synchronization points, with the resulting reduction in latency and number of messages exchanged. <p> Clouds allows a segment to be locked by a processor, to avoid the "ping-pong" effects that may result from false sharing. Mirage also attempts to avoid these effects by locking a page with a certain processor for a certain time window <ref> [17] </ref>. Munin's use of multiple-writer protocols avoids the adverse effects of false sharing, without introducing the delays caused by locking a segment to a processor. Orca is also an object-oriented DSM system, but its consistency management is based on an efficient reliable ordered broadcast protocol [3].
Reference: [18] <author> David Gelernter. </author> <title> Generative communication in Linda. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(1) </volume> <pages> 80-112, </pages> <month> January </month> <year> 1985. </year>
Reference-contexts: Linda provides a different abstraction for distributed memory programming: all shared variables reside in a tuple space, and the only operations allowed are atomic insertion, removal, and reading of objects from the tuple space <ref> [18] </ref>. Munin stays closer to the more familiar shared memory programming model, hopefully improving its acceptance with parallel programmers. Midway [8] proposes a DSM system with entry consistency, a memory consistency model weaker than release consistency.
Reference: [19] <author> Kourosh Gharachorloo, Daniel Lenoski, James Laudon, Phillip Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: First, Munin employs multiple consistency protocols. Each shared variable declaration is annotated by its expected access pattern. Munin then chooses a consistency protocol suited to that pattern. Second, Munin is the first software DSM system that provides a release-consistent memory interface <ref> [19] </ref>. Roughly speaking, release consistency requires memory to be consistent only at specific synchronization points, resulting in a reduction of overhead and number of messages. <p> This restriction is necessary for release consistency to operate correctly (see Section 2.2). 2.2 Software Release Consistency Release consistency was introduced by the DASH system. A detailed discussion and a formal definition can be found in the papers describing DASH <ref> [19, 23] </ref>. We summarize the essential aspects of that discussion. Release consistency requires that each shared memory access be classified either as a synchronization access or an ordinary access. 1 Furthermore, each synchronization accesses must be classified as either a release or an acquire. <p> The term "all previous accesses" refers to all accesses by the same thread that precede the current access in program order. A load is said to have "performed with 1 We ignore chaotic data <ref> [19] </ref> in this presentation. 2 For example, only one thread can acquire a lock at a time, and a thread attempting to acquire a lock must block until the acquire is successful. respect to another processor" when a subsequent store on that processor cannot affect the value returned by the load. <p> We believe this is not a major constraint, as many shared memory parallel programming environments already provide efficient synchronization packages. There is therefore little incentive for programmers to implement separate mechanisms. Unlike DASH, Munin does not require that each individual shared memory access be marked. Gharachorloo et al. <ref> [16, 19] </ref> have shown that a large class of programs, essentially programs with "enough" synchronization, produce the same results on a release-consistent memory as on a sequentially-consistent memory.
Reference: [20] <author> James R. Goodman, Mary K. Vernon, and Philip J. Woest. </author> <title> Efficient synchronization primitives for large-scale cache-coherent multiprocessor. </title> <booktitle> In Proceedings of the 3rd International Conference on Architectural Support for Programming Languages and Systems, </booktitle> <pages> pages 64-75, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Munin also implements locks and barriers, using a distributed queue-based synchronization protocol <ref> [20, 26] </ref>. During compilation, the sharing annotations are read by the Munin preprocessor, and an auxiliary file is created for each input file. <p> A technique similar to the delayed update queue was used by the Myrias SPS multiprocessor [13]. It performed the copy-on-write and diff in hardware, but required a restricted form of parallelism to ensure correctness. Munin's implementation of locks is similar to existing implementations on shared memory multiprocessors <ref> [20, 26] </ref>. An alternative approach for parallel processing on distributed memory machines is to have the compiler produce a message passing program starting from a sequential program, annotated by the programmer with data partitions [4, 30].
Reference: [21] <author> Debra Hensgen, Raphael Finkel, and Udi Man-ber. </author> <title> Two algorithms for barrier synchronization. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 17(1) </volume> <pages> 1-17, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: When the Munin root thread on the owner node has received messages from the specified number of threads, it replies to the blocked threads, causing them to be resumed. For future implementations on larger systems, we envision the use of barrier trees and other more scalable schemes <ref> [21] </ref>. 4 Performance We have measured the performance of two Munin programs, Matrix Multiply and Successive Over-Relaxation (SOR). We have also hand-coded the same programs on the same hardware using the underlying message passing primitives.
Reference: [22] <author> Leslie Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: A load or a store is said to have "performed" when it has performed with respect to all other processors. Previous DSM systems [3, 10, 17, 25, 27] are based on sequential consistency <ref> [22] </ref>. Sequential consistency requires, roughly speaking, that each modification to a shared object become visible immediately to the other processors in the system. Release consistency postpones until the next release the time at which updates must become visible. <p> This anecdote emphasizes the difficulty of writing efficient message passing programs, and serves to emphasize the value of a DSM system like Munin. 5 Related Work A number of software DSM systems have been developed [3, 8, 10, 17, 25, 27]. All, except Midway [8], use sequential consistency <ref> [22] </ref>. Munin's use of release consistency only requires consistency to be enforced at specific synchronization points, with the resulting reduction in latency and number of messages exchanged. Ivy uses a single-writer, write-invalidate protocol, with virtual memory pages as the units of consistency [25].
Reference: [23] <author> Dan Lenoski, James Laudon, Kourosh Gharachor-loo, Anoop Gupta, and John Hennessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: This restriction is necessary for release consistency to operate correctly (see Section 2.2). 2.2 Software Release Consistency Release consistency was introduced by the DASH system. A detailed discussion and a formal definition can be found in the papers describing DASH <ref> [19, 23] </ref>. We summarize the essential aspects of that discussion. Release consistency requires that each shared memory access be classified either as a synchronization access or an ordinary access. 1 Furthermore, each synchronization accesses must be classified as either a release or an acquire. <p> The goal of Midway is to minimize communications costs by aggressively exploiting the relationship between shared variables and the synchronization objects that protect them. Recently, designs for hardware distributed shared memory machines have been published <ref> [2, 23] </ref>. Our work is most related to the DASH project [23], from which we adapt the concept of release consistency. Unlike Munin, though, DASH uses a write-invalidate protocol for all consistency maintenance. <p> The goal of Midway is to minimize communications costs by aggressively exploiting the relationship between shared variables and the synchronization objects that protect them. Recently, designs for hardware distributed shared memory machines have been published [2, 23]. Our work is most related to the DASH project <ref> [23] </ref>, from which we adapt the concept of release consistency. Unlike Munin, though, DASH uses a write-invalidate protocol for all consistency maintenance.
Reference: [24] <author> Kai Li. </author> <title> Shared Virtual Memory on Loosely Coupled Multiprocessors. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <month> September </month> <year> 1986. </year>
Reference-contexts: Upon a write miss, an invalidation message is transmitted to all other replicas. The thread that generated the miss blocks until it has the only copy in the system <ref> [24] </ref>. A shared object is considered conventional if no annotation is provided by the programmer. The combination of protocol parameter settings for each annotation is summarized in Table 1. New sharing annotations can be added easily by modifying the preprocessor that parses the Munin program annotations. <p> the remote processors is sufficient. 3 * Synchq (optional): a pointer to the synchronization object that controls access to the object (see Section 2.4). * Probable owner (optional): used as a "best guess" to reduce the overhead of determining the identity of the Munin node that currently owns the object <ref> [24] </ref>. <p> Conventional objects result in an ownership-based write-invalidate protocol being used, similar to the one implemented in Ivy <ref> [24] </ref>. We also chose write-shared because it supports multiple writers and fine-grained sharing. The execution times for the unoptimized version of Matrix Multiply (see Table 4) and SOR, for the previous problem sizes and for 16 processors, are presented in Table 6.
Reference: [25] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: A load or a store is said to have "performed" when it has performed with respect to all other processors. Previous DSM systems <ref> [3, 10, 17, 25, 27] </ref> are based on sequential consistency [22]. Sequential consistency requires, roughly speaking, that each modification to a shared object become visible immediately to the other processors in the system. Release consistency postpones until the next release the time at which updates must become visible. <p> This anecdote emphasizes the difficulty of writing efficient message passing programs, and serves to emphasize the value of a DSM system like Munin. 5 Related Work A number of software DSM systems have been developed <ref> [3, 8, 10, 17, 25, 27] </ref>. All, except Midway [8], use sequential consistency [22]. Munin's use of release consistency only requires consistency to be enforced at specific synchronization points, with the resulting reduction in latency and number of messages exchanged. <p> All, except Midway [8], use sequential consistency [22]. Munin's use of release consistency only requires consistency to be enforced at specific synchronization points, with the resulting reduction in latency and number of messages exchanged. Ivy uses a single-writer, write-invalidate protocol, with virtual memory pages as the units of consistency <ref> [25] </ref>. The large size of the consistency unit makes the system prone to false sharing. In addition, the single-writer nature of the protocol can cause a "ping-pong" behavior between multiple writers of a shared page.
Reference: [26] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Synchronization without contention. </title> <booktitle> In Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Systems, </booktitle> <pages> pages 269-278, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Munin also implements locks and barriers, using a distributed queue-based synchronization protocol <ref> [20, 26] </ref>. During compilation, the sharing annotations are read by the Munin preprocessor, and an auxiliary file is created for each input file. <p> A technique similar to the delayed update queue was used by the Myrias SPS multiprocessor [13]. It performed the copy-on-write and diff in hardware, but required a restricted form of parallelism to ensure correctness. Munin's implementation of locks is similar to existing implementations on shared memory multiprocessors <ref> [20, 26] </ref>. An alternative approach for parallel processing on distributed memory machines is to have the compiler produce a message passing program starting from a sequential program, annotated by the programmer with data partitions [4, 30].
Reference: [27] <author> Umakishore Ramachandran, Mustaque Ahamad, and M. Yousef A. Khalidi. </author> <title> Coherence of distributed shared memory: Unifying synchronization and data transfer. </title> <booktitle> In Proceedings of the 1989 Conference on Parallel Processing, </booktitle> <address> pages II-160| II-169, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: A load or a store is said to have "performed" when it has performed with respect to all other processors. Previous DSM systems <ref> [3, 10, 17, 25, 27] </ref> are based on sequential consistency [22]. Sequential consistency requires, roughly speaking, that each modification to a shared object become visible immediately to the other processors in the system. Release consistency postpones until the next release the time at which updates must become visible. <p> Munin also provides a small collection of library routines that allow the programmer to fine-tune various aspects of Munin's operation. These "hints" are optional performance optimizations. In Munin, the programmer can specify the logical connections between shared variables and the synchronization objects that protect them <ref> [27] </ref>. Currently, this information is provided by the user using an AssociateDataAndSynch () call. If Munin knows which objects are protected by a particular lock, the required consistency information is included in the message that passes lock ownership. <p> This anecdote emphasizes the difficulty of writing efficient message passing programs, and serves to emphasize the value of a DSM system like Munin. 5 Related Work A number of software DSM systems have been developed <ref> [3, 8, 10, 17, 25, 27] </ref>. All, except Midway [8], use sequential consistency [22]. Munin's use of release consistency only requires consistency to be enforced at specific synchronization points, with the resulting reduction in latency and number of messages exchanged. <p> It is then up to the programmer or the compiler to lay out the program data structures in the shared address space such that false sharing is reduced. Clouds performs consistency management on a per-object basis, or in Clouds terminology, on a per-segment basis <ref> [27] </ref>. Clouds allows a segment to be locked by a processor, to avoid the "ping-pong" effects that may result from false sharing. Mirage also attempts to avoid these effects by locking a page with a certain processor for a certain time window [17].
Reference: [28] <author> Richard Rashid, Avadis Tevanian, Jr. Michael Young, David Golub, Robert Baron, David Black, William Bolosky, and Jonathan Chew. </author> <title> Machine-independent virtual memory management for paged uniprocessor and multiprocessor architectures. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(8) </volume> <pages> 896-908, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: When the application program is invoked, the Munin root thread starts running. It initializes the shared data segment, creates Munin worker threads to handle consistency and synchronization functions, and registers itself with the kernel as the address space's page fault handler (as is done by Mach's external pagers <ref> [28] </ref>). It then executes the user initialization routine user init (), spawns the number of remote copies of the program specified by user init (), and initializes the remote shared data segments. Finally, the Munin root thread creates and runs the user root thread.
Reference: [29] <author> Wolf-Dietrich Weber and Anoop Gupta. </author> <title> Analysis of cache invalidation patterns in multiprocessors. </title> <booktitle> In Proceedings of the 3rd International Conference on Architectural Support for Programming Languages and Systems, </booktitle> <pages> pages 243-256, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Thus, the consistency protocol simply consists of replication on demand. A runtime error is generated if a thread attempts to write to a read-only object. For migratory objects, a single thread performs multiple accesses to the object, including one or more writes, before another thread accesses the object <ref> [29] </ref>. Such an access pattern is typical of shared objects that are accessed only inside a critical section. <p> few record keeping functions and as the node of last resort if the system ever attempts to invalidate all remote copies of an object. 3 This approach does not scale well to larger systems, but an earlier study of parallel programs suggests that a processor list is often quite short <ref> [29] </ref>.
Reference: [30] <author> Hans P. Zima, Heinz J. Bast, and Michael Gerndt. </author> <title> Superb: A tool for semi-automatic SIMD/MIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year>
Reference-contexts: Munin's implementation of locks is similar to existing implementations on shared memory multiprocessors [20, 26]. An alternative approach for parallel processing on distributed memory machines is to have the compiler produce a message passing program starting from a sequential program, annotated by the programmer with data partitions <ref> [4, 30] </ref>.
References-found: 30

