URL: http://www.research.att.com/~schapire/papers/FreundSc96b.ps.Z
Refering-URL: http://www.research.att.com/~schapire/publist.html
Root-URL: 
Email: fyoav, schapireg@research.att.com  
Title: Game Theory, On-line Prediction and Boosting  
Author: Yoav Freund Robert E. Schapire 
Address: 600 Mountain Avenue Murray Hill, NJ 07974-0636  
Affiliation: AT&T Laboratories  
Note: Proceedings of the Ninth Annual Conference on Computational Learning Theory, 1996.  
Abstract: We study the close connections between game theory, on-line prediction and boosting. After a brief review of game theory, we describe an algorithm for learning to play repeated games based on the on-line prediction methods of Littlestone and War-muth. The analysis of this algorithm yields a simple proof of von Neumann's famous minmax theorem, as well as a provable method of approximately solving a game. We then show that the on-line prediction model is obtained by applying this game-playing algorithm to an appropriate choice of game and that boosting is obtained by applying the same algorithm to the dual of this game.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> David Blackwell. </author> <title> An analog of the minimax theorem for vector payoffs. </title> <journal> Pacific Journal of Mathematics, </journal> <volume> 6(1) </volume> <pages> 1-8, </pages> <month> Spring </month> <year> 1956. </year>
Reference-contexts: However, our results also apply when no assumptions are made about the goal or strategy of the column player. We return to this point below. For the sake of simplicity, we assume that all the losses are in the range <ref> [0; 1] </ref>. Simple scaling can be used to get more general results. Also, we restrict ourselves to the case where the number of choices available to each player is finite. <p> The main theorem concerning this algorithm is the following: Theorem 1 For any matrix M with n rows and entries in <ref> [0; 1] </ref>, and for any sequence of mixed strategies Q 1 ; : : : ; Q T played by the environment, the sequence of mixed strategies P 1 ; : : : ; P T produced by algorithm LW with parameter fi 2 [0; 1) satisfy: T X M (P <p> The details of the algorithm about which this corollary applies are largely unimportant and could, in principle, be applied to any algorithm with similar properties. Indeed, algorithms for this problem with similar properties were derived by Han-nan [13], 1 Blackwell <ref> [1] </ref> and Foster and Vohra [6, 5, 4]. Also, Fudenberg and Levine [10] independently proposed an algorithm equivalent to LW and proved a slightly weaker version of Corollary 2. <p> Therefore, we also need to reverse the meaning of minimum and maximum which is easily done by negating the matrix yielding M T . Finally, to adhere to our convention of losses being in the range <ref> [0; 1] </ref>, we add the constant 1 to every outcome, which has no effect on the game. Thus, the dual M 0 of M is simply M 0 = 1 M T where 1 is an all 1's matrix of the appropriate dimensions.
Reference: [2] <author> Nicolo Cesa-Bianchi, Yoav Freund, David P. Helmbold, David Haussler, Robert E. Schapire, and Manfred K. Warmuth. </author> <title> How to use expert advice. </title> <booktitle> In Proceedings of the Twenty-Fifth Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 382-391, </pages> <year> 1993. </year>
Reference-contexts: Still better bounds using more sophisticated methods were obtained by Cesa-Bianchi et al. <ref> [2] </ref> and Vovk [18]. <p> This result can be straightforwardly generalized to any bounded loss function (such as square loss rather than zero one mistake loss), and also to a setting in which the learner competes against a set of experts rather than a fixed set of hypotheses. (See, for instance, Cesa-Bianchi et al. <ref> [2] </ref> and Freund and Schapire [9].) 4 BOOSTING The third topic of this paper is boosting. Boosting is the problem of converting a weak learning algorithm that performs just slightly better than random guessing into one that performs with arbitrarily good accuracy.
Reference: [3] <author> Thomas S. Ferguson. </author> <title> Mathematical Statistics: A Decision Theoretic Approach. </title> <publisher> Academic Press, </publisher> <year> 1967. </year>
Reference-contexts: However, most of the results translate with very mild additional assumptions to cases in which the number of choices is infinite. For a discussion of infinite matrix games see, for instance, Chapter 2 in Ferguson <ref> [3] </ref>. 2.1 RANDOMIZED PLAY As described above, the players choose a single row or column. Usually, this choice of play is allowed to be randomized. That is, the row player chooses a distribution P over the rows of M, and (simultaneously) the column player chooses a distribution Q over columns.
Reference: [4] <author> Dean Foster and Rakesh Vohra. </author> <title> Regret in on-line decision making. </title> <type> unpublished manuscript, </type> <year> 1996. </year>
Reference-contexts: The details of the algorithm about which this corollary applies are largely unimportant and could, in principle, be applied to any algorithm with similar properties. Indeed, algorithms for this problem with similar properties were derived by Han-nan [13], 1 Blackwell [1] and Foster and Vohra <ref> [6, 5, 4] </ref>. Also, Fudenberg and Levine [10] independently proposed an algorithm equivalent to LW and proved a slightly weaker version of Corollary 2.
Reference: [5] <author> Dean Foster and Rakesh V. Vohra. </author> <title> Asymptotic calibration. </title> <type> unpublished manuscript, </type> <year> 1995. </year>
Reference-contexts: The details of the algorithm about which this corollary applies are largely unimportant and could, in principle, be applied to any algorithm with similar properties. Indeed, algorithms for this problem with similar properties were derived by Han-nan [13], 1 Blackwell [1] and Foster and Vohra <ref> [6, 5, 4] </ref>. Also, Fudenberg and Levine [10] independently proposed an algorithm equivalent to LW and proved a slightly weaker version of Corollary 2.
Reference: [6] <author> Dean P. Foster and Rakesh V. Vohra. </author> <title> A randomization rule for selecting forecasts. </title> <journal> Operations Research, </journal> <volume> 41(4) </volume> <pages> 704-709, </pages> <month> July-August </month> <year> 1993. </year>
Reference-contexts: The details of the algorithm about which this corollary applies are largely unimportant and could, in principle, be applied to any algorithm with similar properties. Indeed, algorithms for this problem with similar properties were derived by Han-nan [13], 1 Blackwell [1] and Foster and Vohra <ref> [6, 5, 4] </ref>. Also, Fudenberg and Levine [10] independently proposed an algorithm equivalent to LW and proved a slightly weaker version of Corollary 2.
Reference: [7] <author> Yoav Freund. </author> <title> Boosting a weak learning algorithm by majority. </title> <journal> Information and Computation, </journal> <volume> 121(2) </volume> <pages> 256-285, </pages> <year> 1995. </year>
Reference-contexts: Boosting is the problem of converting a weak learning algorithm that performs just slightly better than random guessing into one that performs with arbitrarily good accuracy. The first provably effective boosting algorithm was discovered by Schapire [17]. Freund <ref> [7] </ref> subsequently presented a much improved boosting algorithm which is optimal in particular circumstances. The boosting algorithm derived in this section is closely related to Freund and Schapire's more recent AdaBoost boosting algorithm [9]. 3 The reduction is not specific to the use of LW.
Reference: [8] <author> Yoav Freund. </author> <title> Predicting a binary sequence almost as well as the optimal biased coin. </title> <booktitle> In Proceedings of the Ninth Annual Conference on Computational Learning Theory, </booktitle> <year> 1996. </year>
Reference-contexts: The cardinality of the set of examples is actually of no real consequence. Littlestone and Warmuth [15] generalize their results to countably infinite sets of hypotheses, and Freund and Schapire [9] and Freund <ref> [8] </ref> give generalizations to uncountably infinite sets of hypotheses. 4 Thus, M (h; x) is 1 if and only if h disagrees with the target c on instance x. We call this a mistake matrix.
Reference: [9] <author> Yoav Freund and Robert E. Schapire. </author> <title> A decision-theoretic generalization of online learning and an application to boosting. </title> <booktitle> In Computational Learning Theory: 7 Second European Conference, </booktitle> <volume> EuroCOLT '95, </volume> <pages> pages 23-37. </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> A draft of the journal version is available electronically (on our web pages, or by email request). </note>
Reference-contexts: not much worse than the loss of the best strategy in hindsight min T X M (P; Q t ): An algorithm for solving this problem can be derived by a direct generalization of Littlestone and Warmuth's weighted majority algorithm [15], and is essentially equivalent to our earlier Hedge algorithm <ref> [9] </ref>. The algorithm, called LW, is quite simple. The learner maintains nonnegative weights on the rows of M; let w t (i) denote the weight at time t on row i. Initially, all the weights are set to unity: w t (i) = 1. <p> parameter fi 2 [0; 1) satisfy: T X M (P t ; Q t ) a fi min T X M (P; Q t ) + c fi ln n where a fi = 1 fi 1 : Proof: The proof follows directly from Theorem 2 of Freund and Schapire <ref> [9] </ref>, which in turn is a simple and direct generalization of Littlestone and Warmuth [15]. For completeness, we provide a short proof in the appendix. As fi approaches 1, a fi also approaches 1. <p> average per-trial loss suffered by the learner is 1 T X M (P t ; Q t ) min 1 T X M (P; Q t ) + D T where D T = 2 ln n + T r T : Proof: See Section 2.2 in Freund and Schapire <ref> [9] </ref>. Since D T ! 0 as T ! 1, we see that the amount by which the average per-trial loss of the learner exceeds that of the best mixed strategy can be made arbitrarily small for large T . <p> The cardinality of the set of examples is actually of no real consequence. Littlestone and Warmuth [15] generalize their results to countably infinite sets of hypotheses, and Freund and Schapire <ref> [9] </ref> and Freund [8] give generalizations to uncountably infinite sets of hypotheses. 4 Thus, M (h; x) is 1 if and only if h disagrees with the target c on instance x. We call this a mistake matrix. <p> straightforwardly generalized to any bounded loss function (such as square loss rather than zero one mistake loss), and also to a setting in which the learner competes against a set of experts rather than a fixed set of hypotheses. (See, for instance, Cesa-Bianchi et al. [2] and Freund and Schapire <ref> [9] </ref>.) 4 BOOSTING The third topic of this paper is boosting. Boosting is the problem of converting a weak learning algorithm that performs just slightly better than random guessing into one that performs with arbitrarily good accuracy. The first provably effective boosting algorithm was discovered by Schapire [17]. <p> The first provably effective boosting algorithm was discovered by Schapire [17]. Freund [7] subsequently presented a much improved boosting algorithm which is optimal in particular circumstances. The boosting algorithm derived in this section is closely related to Freund and Schapire's more recent AdaBoost boosting algorithm <ref> [9] </ref>. 3 The reduction is not specific to the use of LW. Other algorithms for playing repeated games can be combined with this reduction to give on-line learning algorithms. However, these algorithms need to be capable of working without complete knowledge of the matrix. <p> The algorithm presented can certainly be modified to fit the more standard (and prac tical) model in which the final error must be less than some positive parameter * (see Freund and Schapire <ref> [9] </ref> for more details). 4 Thus, boosting proceeds in rounds. <p> Rather, the booster would be given a labeled training set and all distributions would be computed over the training set. The generalization error of the final hypothesis can then be bounded using, for instance, standard VC theory (see Freund and Schapire <ref> [9] </ref> for more details). A more sophisticated version of this algorithm, called AdaBoost, is given by Freund and Schapire [9]. <p> The generalization error of the final hypothesis can then be bounded using, for instance, standard VC theory (see Freund and Schapire <ref> [9] </ref> for more details). A more sophisticated version of this algorithm, called AdaBoost, is given by Freund and Schapire [9].
Reference: [10] <author> Drew Fudenberg and David K. Levine. </author> <title> Consistency and cautious fictitious play. </title> <journal> Journal of Economic Dynamics and Control, </journal> <volume> 19 </volume> <pages> 1065-1089, </pages> <year> 1995. </year>
Reference-contexts: Indeed, algorithms for this problem with similar properties were derived by Han-nan [13], 1 Blackwell [1] and Foster and Vohra [6, 5, 4]. Also, Fudenberg and Levine <ref> [10] </ref> independently proposed an algorithm equivalent to LW and proved a slightly weaker version of Corollary 2. As a simple first corollary, we see that the loss of LW can never exceed the value of the game M by more than D T .
Reference: [11] <author> Drew Fudenberg and Jean Tirole. </author> <title> Game Theory. </title> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: Further background can be found in any introductory text on game theory; see for instance Fudenberg and Tirole <ref> [11] </ref>. We study two-person games in normal form. That is, each game is defined by a matrix M. There are two players called the row player and column player. To play the game, the row player chooses a row i, and, simultaneously, the column player chooses a column j.
Reference: [12] <author> Mikael Goldmann, Johan Hastad, and Alexander Razborov. </author> <title> Majority gates vs. general weighted threshold gates. </title> <journal> Computational Complexity, </journal> <volume> 2 </volume> <pages> 277-300, </pages> <year> 1992. </year>
Reference-contexts: Moreover, the weights used in this function (defined by distribution P fl above) are not just any old weights, but rather are a minmax strategy for the game M. A similar proof technique was previously used by Gold-mann, Hastad and Razborov <ref> [12] </ref> to prove a result about the representation power of circuits of weighted threshold gates. 4.2 IDEA FOR BOOSTING The idea of our boosting algorithm then is to approximate c by approximating the weights of this function.
Reference: [13] <author> James Hannan. </author> <title> Approximation to Bayes risk in repeated play. </title> <editor> In M. Dresher, A. W. Tucker, and P. Wolfe, editors, </editor> <title> Contributions to the Theory of Games, </title> <booktitle> volume III, </booktitle> <pages> pages 97-139. </pages> <publisher> Princeton University Press, </publisher> <year> 1957. </year>
Reference-contexts: The details of the algorithm about which this corollary applies are largely unimportant and could, in principle, be applied to any algorithm with similar properties. Indeed, algorithms for this problem with similar properties were derived by Han-nan <ref> [13] </ref>, 1 Blackwell [1] and Foster and Vohra [6, 5, 4]. Also, Fudenberg and Levine [10] independently proposed an algorithm equivalent to LW and proved a slightly weaker version of Corollary 2.
Reference: [14] <author> Nick Littlestone. </author> <title> Learning when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference-contexts: In this section, we make this connection explicit. In the on-line prediction model, first introduced by Lit tlestone <ref> [14] </ref>, the learner observes a sequence of examples and predicts their labels one at a time. The learner's goal is to minimize its prediction errors. Formally, let X be a finite set of instances, and let H be a finite set of hypotheses h : X ! f0; 1g.
Reference: [15] <author> Nick Littlestone and Manfred K. Warmuth. </author> <title> The weighted majority algorithm. </title> <journal> Information and Computation, </journal> <volume> 108 </volume> <pages> 212-261, </pages> <year> 1994. </year>
Reference-contexts: To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. for learning to play repeated games based on the on-line prediction methods of Littlestone and Warmuth <ref> [15] </ref>. The analysis of this algorithm yields a new (as far as we know) and simple proof of von Neumann's famous minmax theorem, as well as a provable method of approximately solving a game. <p> X M (P t ; Q t ) which is not much worse than the loss of the best strategy in hindsight min T X M (P; Q t ): An algorithm for solving this problem can be derived by a direct generalization of Littlestone and Warmuth's weighted majority algorithm <ref> [15] </ref>, and is essentially equivalent to our earlier Hedge algorithm [9]. The algorithm, called LW, is quite simple. The learner maintains nonnegative weights on the rows of M; let w t (i) denote the weight at time t on row i. <p> ) a fi min T X M (P; Q t ) + c fi ln n where a fi = 1 fi 1 : Proof: The proof follows directly from Theorem 2 of Freund and Schapire [9], which in turn is a simple and direct generalization of Littlestone and Warmuth <ref> [15] </ref>. For completeness, we provide a short proof in the appendix. As fi approaches 1, a fi also approaches 1. <p> Similar and closely related methods of approximately solving linear programming problems have previously appeared, for instance, in the work of Plotkin, Shmoys and Tardos [16]. 3 ON-LINE PREDICTION Since the game-playing algorithm LW presented in Section 2.4 is a direct generalization of the on-line prediction algorithm of Littlestone and Warmuth <ref> [15] </ref>, it is not surprising that an on-line prediction algorithm can be derived from the more general game-playing algorithm by an appropriate choice of game M. In this section, we make this connection explicit. <p> The cardinality of the set of examples is actually of no real consequence. Littlestone and Warmuth <ref> [15] </ref> generalize their results to countably infinite sets of hypotheses, and Freund and Schapire [9] and Freund [8] give generalizations to uncountably infinite sets of hypotheses. 4 Thus, M (h; x) is 1 if and only if h disagrees with the target c on instance x. <p> the expected number of mistakes made by the learner cannot exceed the number of mistakes made by the best hypothesis in H by more than O p A more careful analysis (using Theorem 1 rather than Corollary 2) gives a better bound identical to that obtained by Littlestone and Warmuth <ref> [15] </ref> (not surprisingly). Still better bounds using more sophisticated methods were obtained by Cesa-Bianchi et al. [2] and Vovk [18].
Reference: [16] <author> Serge A. Plotkin, David B. Shmoys, and Eva Tardos. </author> <title> Fast approximation algorithms for fractional packing and covering problems. </title> <journal> Mathematics of Operations Research, </journal> <volume> 20(2) </volume> <pages> 257-301, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Viewing LW as a method of approximately solving a game will be central to our derivation of a boosting algorithm (Section 4). Similar and closely related methods of approximately solving linear programming problems have previously appeared, for instance, in the work of Plotkin, Shmoys and Tardos <ref> [16] </ref>. 3 ON-LINE PREDICTION Since the game-playing algorithm LW presented in Section 2.4 is a direct generalization of the on-line prediction algorithm of Littlestone and Warmuth [15], it is not surprising that an on-line prediction algorithm can be derived from the more general game-playing algorithm by an appropriate choice of game
Reference: [17] <author> Robert E. Schapire. </author> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5(2) </volume> <pages> 197-227, </pages> <year> 1990. </year>
Reference-contexts: Boosting is the problem of converting a weak learning algorithm that performs just slightly better than random guessing into one that performs with arbitrarily good accuracy. The first provably effective boosting algorithm was discovered by Schapire <ref> [17] </ref>. Freund [7] subsequently presented a much improved boosting algorithm which is optimal in particular circumstances. The boosting algorithm derived in this section is closely related to Freund and Schapire's more recent AdaBoost boosting algorithm [9]. 3 The reduction is not specific to the use of LW.
Reference: [18] <author> Volodimir G. Vovk. </author> <title> Aggregating strategies. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 371-383, </pages> <year> 1990. </year>
Reference-contexts: Still better bounds using more sophisticated methods were obtained by Cesa-Bianchi et al. [2] and Vovk <ref> [18] </ref>.
References-found: 18

