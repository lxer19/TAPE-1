URL: http://www.cs.umn.edu/Users/dept/users/boley/reports/taxonomy.ps.gz
Refering-URL: http://www.cs.umn.edu/Users/dept/users/boley/reports/
Root-URL: http://www.cs.umn.edu
Title: Hierarchical Taxonomies using Divisive Partitioning  
Author: Daniel Boley 
Keyword: unsupervised clustering, hierarchical taxonomy, scalability, text document data mining.  
Note: This research was partially supported by NSF grants CCR-9405380 and  
Address: 200 Union Street S.E., Rm 4-192 Minneapolis, MN 55455, USA  
Affiliation: Department of Computer Science and Engineering University of Minnesota  
Email: e-mail: boley@cs.umn.edu  
Phone: tel: 1-612-625-3887, fax: 1-612-625-0572,  
Web: CCR-9628786.  
Abstract: We propose an unsupervised divisive partitioning algorithm for document data sets which enjoys many favorable properties. In particular, the algorithm shows excellent scalability to large data collections and produces high quality clusters which are competitive with other clustering methods. The algorithm yields information on the significant and distinctive words within each cluster, and these words can be inserted into the naturally occuring hierarchical structure produced by the algorithm. The result is an automatically generated hierarchical topical taxonomy of a document set. In this paper, we show how the algorithm's cost scales up linearly with the size of the data, illustrate experimentally the quality of the clusters produced, and show how the algorithm can produce a hierarchical topical taxonomy. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Ackerman, D. Billsus, S. Gaffney, S. Hettich, G. Khoo, D. Kim, R. Klefstad, C. Lowe, A. Ludeman, J. Muramatsu, K. Omori, M. Pazzani, D. Semler, B. Starr, and P. Yap. </author> <title> Learning probabilistic user profiles. </title> <journal> AI Magazine, </journal> <volume> 18(2) </volume> <pages> 47-56, </pages> <year> 1997. </year>
Reference-contexts: Many engines have been proposed that attempt to record documents of interest to the user. The number of times a user visits a document and the total amount of time a user spends viewing a document are just a few methods for determining user interest <ref> [1, 4, 5] </ref>. Syskill & Webert [38, 1] represents an HTML page with a Boolean feature vector, and then uses simple Bayesian classification to find Web pages that are similar, but for only a given single user profile. <p> The number of times a user visits a document and the total amount of time a user spends viewing a document are just a few methods for determining user interest [1, 4, 5]. Syskill & Webert <ref> [38, 1] </ref> represents an HTML page with a Boolean feature vector, and then uses simple Bayesian classification to find Web pages that are similar, but for only a given single user profile.
Reference: [2] <author> A. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and A. Verkamo. </author> <title> Fast discovery of association rules. </title> <editor> In U. Fayyad, G. Piatetsky-Shapiro, P. Smith, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge 20 Discovery and Data Mining, </booktitle> <pages> pages 307-328. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: We have also had a few comparisons with a recently developed association rule hypergraph clustering method (ARHP) [24, 25, 31], based on association rule hypergraphs <ref> [2, 6] </ref>, but this last method is harder to compare since it eliminates some documents so that the result is not strictly speaking a partitioning of the entire document set.
Reference: [3] <author> T. Anderson. </author> <title> On estimation of parameters in latent structure analysis. </title> <journal> Psychometrika, </journal> <volume> 19 </volume> <pages> 1-10, </pages> <year> 1954. </year>
Reference-contexts: The key to its speed is a fast method to obtain the principal direction. This is not the same as Latent Semantic Indexing <ref> [3, 7, 18] </ref>, as discussed below in x7. <p> Hence the distances between the cluster means may not provide a good measure of separation between clusters. This issue must be addressed in any agglomerative algorithm, but is much less critical in a divisive algorithm. The PDDP method differs from that of Latent Semantic Indexing (LSI) <ref> [3, 7, 18] </ref> in many ways. Latent Semantic Indexing (LSI) was originally proposed for a different purpose, namely as a method to aid query-based document retrieval. LSI is useful when noise is present in data sets of very high dimensionality. 17 LSI first reduces the dimensionality by orthogonal projection.
Reference: [4] <author> R. Armstrong, D. Freitag, T. Joachims, and T. Mitchell. WebWatcher: </author> <title> A learning apprentice for the World Wide Web. </title> <booktitle> In Proc. AAAI Spring Symposium on Information Gathering from Heterogeneous, Distributed Environments. </booktitle> <publisher> AAAI Press, </publisher> <year> 1995. </year>
Reference-contexts: Many engines have been proposed that attempt to record documents of interest to the user. The number of times a user visits a document and the total amount of time a user spends viewing a document are just a few methods for determining user interest <ref> [1, 4, 5] </ref>. Syskill & Webert [38, 1] represents an HTML page with a Boolean feature vector, and then uses simple Bayesian classification to find Web pages that are similar, but for only a given single user profile.
Reference: [5] <author> M. Balabanovic, Y. Shoham, and Y. Yun. </author> <title> An adaptive agent for automated Web browsing. Journal of Visual Communication and Image Representation, </title> <type> 6(4), </type> <year> 1995. </year>
Reference-contexts: Many engines have been proposed that attempt to record documents of interest to the user. The number of times a user visits a document and the total amount of time a user spends viewing a document are just a few methods for determining user interest <ref> [1, 4, 5] </ref>. Syskill & Webert [38, 1] represents an HTML page with a Boolean feature vector, and then uses simple Bayesian classification to find Web pages that are similar, but for only a given single user profile. <p> Syskill & Webert [38, 1] represents an HTML page with a Boolean feature vector, and then uses simple Bayesian classification to find Web pages that are similar, but for only a given single user profile. Also, Balabanovic <ref> [5] </ref> presents a system that uses a single well-defined profile to find similar Web documents for a user. Candidate Web pages are located using best-first search, comparing their word vectors against a user profile vector, and returning the highest -scoring pages.
Reference: [6] <author> C. Berge. </author> <title> Graphs and Hypergraphs. </title> <publisher> American Elsevier, </publisher> <year> 1976. </year>
Reference-contexts: We have also had a few comparisons with a recently developed association rule hypergraph clustering method (ARHP) [24, 25, 31], based on association rule hypergraphs <ref> [2, 6] </ref>, but this last method is harder to compare since it eliminates some documents so that the result is not strictly speaking a partitioning of the entire document set.
Reference: [7] <author> M. W. Berry, S. T. Dumais, and G. W. O'Brien. </author> <title> Using linear algebra for intelligent information retrieval. </title> <journal> SIAM Review, </journal> <volume> 37 </volume> <pages> 573-595, </pages> <year> 1995. </year>
Reference-contexts: The key to its speed is a fast method to obtain the principal direction. This is not the same as Latent Semantic Indexing <ref> [3, 7, 18] </ref>, as discussed below in x7. <p> Hence the distances between the cluster means may not provide a good measure of separation between clusters. This issue must be addressed in any agglomerative algorithm, but is much less critical in a divisive algorithm. The PDDP method differs from that of Latent Semantic Indexing (LSI) <ref> [3, 7, 18] </ref> in many ways. Latent Semantic Indexing (LSI) was originally proposed for a different purpose, namely as a method to aid query-based document retrieval. LSI is useful when noise is present in data sets of very high dimensionality. 17 LSI first reduces the dimensionality by orthogonal projection.
Reference: [8] <author> G. Biswas, L. Weinberg, and C. Li. Iterate: </author> <title> A conceptual clustering method for knowledge discovery in databases. </title> <editor> In B. Braunschweig and R. Day, editors, </editor> <booktitle> Innovative Applications of Artificial Intelligence in the Oil and Gas Industry, </booktitle> <year> 1994. </year>
Reference-contexts: Many researchers have studied this approach either as a stand-alone method or as a way to refine an initial hierarchy constructed by another method <ref> [19, 35, 9, 8] </ref>.
Reference: [9] <author> G. Biswas, L. Weinberg, Q. Yang, and G. Koller. </author> <title> Conceptual clustering and exploratory data analysis. </title> <booktitle> In Proc. 8th Int'l. Machine Learning Workshop, </booktitle> <pages> pages 591-595, </pages> <year> 1991. </year>
Reference-contexts: Many researchers have studied this approach either as a stand-alone method or as a way to refine an initial hierarchy constructed by another method <ref> [19, 35, 9, 8] </ref>.
Reference: [10] <author> D. Boley. </author> <title> Principal Direction Divisive Partitioning. </title> <type> Technical Report TR-97-056, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, </institution> <year> 1997. </year>
Reference-contexts: This behavior is illustrated in Fig. 1 and is analysed at greater length in <ref> [10] </ref>. In data sets where the number of distinct words appearing in each document remains relatively invariant across the entire corpus, the number of nonzeroes, and hence the total cost of the algorithm, will scale linearly with the number of documents. <p> Some of these aspects were addressed in <ref> [10] </ref>, but they deserve further study. Also the effect of different scalings and the algorithm behavior on more homogeneous datasets are still open questions.
Reference: [11] <author> A. Z. Broder, S. C. Glassman, M. S. Manasse, and G. Zweig. </author> <title> Syntactic clustering of the Web. </title> <booktitle> In Proc. of 6th International World Wide Web Conference, </booktitle> <year> 1997. </year>
Reference-contexts: The WWW also is a very dynamic environment with documents begin added and replaced very rapidly. This has lead to interest in autonomous agents to explore the WWW, such as that proposed in <ref> [12, 11, 22, 26, 34, 50, 48] </ref>. Such agents must be capable of automatically classifying and/or categorizing large datasets of documents without user intervention. <p> A motivation of this work is its application in a Web agent [26]. A number of Web agents use various information retrieval techniques and characteristics of open hypertext Web documents to automatically retrieve, filter, and categorize these documents <ref> [12, 11, 22, 34, 50, 48] </ref>. For example, HyPursuit [48] uses semantic information embedded in link structures as well as document content to create cluster hierarchies of hypertext documents and structure an information space.
Reference: [12] <author> C. Chang and C. Hsu. </author> <title> Customizable multi-engine search tool with clustering. </title> <booktitle> In Proc. of 6th International World Wide Web Conference, </booktitle> <year> 1997. </year>
Reference-contexts: The WWW also is a very dynamic environment with documents begin added and replaced very rapidly. This has lead to interest in autonomous agents to explore the WWW, such as that proposed in <ref> [12, 11, 22, 26, 34, 50, 48] </ref>. Such agents must be capable of automatically classifying and/or categorizing large datasets of documents without user intervention. <p> A motivation of this work is its application in a Web agent [26]. A number of Web agents use various information retrieval techniques and characteristics of open hypertext Web documents to automatically retrieve, filter, and categorize these documents <ref> [12, 11, 22, 34, 50, 48] </ref>. For example, HyPursuit [48] uses semantic information embedded in link structures as well as document content to create cluster hierarchies of hypertext documents and structure an information space.
Reference: [13] <author> P. Cheeseman and J. Stutz. </author> <title> Bayesian classification (AutoClass): Theory and results. </title> <editor> In U. Fayyad, G. Piatetsky-Shapiro, P. Smith, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 153-180. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: In Table 2 we compare the times for the divisive PDDP algorithm with the hierarchical agglomerative clustering algorithm (HAC) [19], which tends to be much more expensive [17]. We also compared this algorithm with AutoClass <ref> [13] </ref>, but this turned out to take even more time than the HAC on every example in the `J' series of experiments. <p> The left column labels were created using the principal direction data, and the middle using centroid vector data. The right column shows the document labels assigned by hand for each individual document based on the categories in Table 3 with one letter per document. 16 AutoClass <ref> [13] </ref> is a method using Bayesian analysis based on the probabilistic mixture modeling [47]. Given a data set it finds maximum parameter values for a specific probability distribution functions of the clusters. <p> The cost was on the same order of magnitude as an agglomerative method, and the quality of the resulting clusters was not competitive with the other methods (see x3). Probabilistic methods such as Bayesian classification used in AutoClass <ref> [13] </ref> do not perform well when the size of the feature space is much larger than the size of the sample set and the features are not statistically independent, as is typical of document categorization applications on the Web.
Reference: [14] <author> H. Chen, K. Lynch, K. Basu, and T. D. Ng. </author> <title> Generating, integrating, and activating thesauri for concept-based document retrieval. </title> <journal> IEEE Expert, </journal> <volume> 8(2) </volume> <pages> 25-34, </pages> <month> April </month> <year> 1993. </year> <month> 21 </month>
Reference-contexts: Some other Web agents designed to aid the user navigate through the Web have been proposed in [42] using a graph distance matrix, in <ref> [14, 29] </ref> using a separate layer of thesaurus terms or predefined categories. In [43], they extract the context of the words to refine the matches and use a nearest neighbor-like clustering approach. The broad category of statistical approaches are compared to knowledge-based or semantic approaches in [16].
Reference: [15] <author> C. Chute and Y. Yang. </author> <title> An overview of statistical methods for the classification and retrieval of patient events. </title> <journal> Meth. Inform. Med., </journal> <volume> 34 </volume> <pages> 104-110, </pages> <year> 1995. </year>
Reference-contexts: Thereafter, the singular value computation is applied only to subclusters, resulting in a much lower cost. Another method related to LSI is that of Linear Least Squares Fit (LLSF) <ref> [15] </ref>. This method is also intended as an aid in query-based document retrieval and is also based on the use of the singular value decomposition, but it adds an extra layer with a list of concepts between the search words and the documents, enhancing the quality of query search results.
Reference: [16] <author> W. Croft. </author> <title> Knowledge-based and statistical approaches to text retrieval. </title> <journal> IEEE Expert, </journal> <volume> 8(2) </volume> <pages> 8-12, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: In [43], they extract the context of the words to refine the matches and use a nearest neighbor-like clustering approach. The broad category of statistical approaches are compared to knowledge-based or semantic approaches in <ref> [16] </ref>. Many engines have been proposed that attempt to record documents of interest to the user. The number of times a user visits a document and the total amount of time a user spends viewing a document are just a few methods for determining user interest [1, 4, 5].
Reference: [17] <author> D. Cutting, D. Karger, J. Pedersen, and J. Tukey. Scatter/gather: </author> <title> a cluster-based approach to browsing large document collections. </title> <booktitle> In 15th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'92), </booktitle> <pages> pages 318-329, </pages> <year> 1992. </year>
Reference-contexts: In Table 2 we compare the times for the divisive PDDP algorithm with the hierarchical agglomerative clustering algorithm (HAC) [19], which tends to be much more expensive <ref> [17] </ref>. We also compared this algorithm with AutoClass [13], but this turned out to take even more time than the HAC on every example in the `J' series of experiments. <p> Agglomerative methods can suffer from several difficulties. The cost of standard agglomerative nearest-neighbor methods can easily reach O (m 2 n), where m; n are the number of documents and words, respectively. A modified version of an agglomerative approach was used in <ref> [17] </ref> as part of a document classification and exploration process. In their approach the entire set was partitioned into smaller subsets on each of which the agglomerative algorithm was applied.
Reference: [18] <author> S. Deerwester, S. Dumais, G. Furnas, L. T.K., and R. Harshman. </author> <title> Indexing by latent semantic analysis. </title> <journal> J. Amer. Soc. Inform. Sci., </journal> <volume> 41:41, </volume> <year> 1990. </year>
Reference-contexts: The key to its speed is a fast method to obtain the principal direction. This is not the same as Latent Semantic Indexing <ref> [3, 7, 18] </ref>, as discussed below in x7. <p> Hence the distances between the cluster means may not provide a good measure of separation between clusters. This issue must be addressed in any agglomerative algorithm, but is much less critical in a divisive algorithm. The PDDP method differs from that of Latent Semantic Indexing (LSI) <ref> [3, 7, 18] </ref> in many ways. Latent Semantic Indexing (LSI) was originally proposed for a different purpose, namely as a method to aid query-based document retrieval. LSI is useful when noise is present in data sets of very high dimensionality. 17 LSI first reduces the dimensionality by orthogonal projection.
Reference: [19] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley & Sons, </publisher> <year> 1973. </year>
Reference-contexts: The method used to select the next node to split can be completely independent of the rest of the algorithm, depending on the specific domain. In our examples we have used a modified scatter value <ref> [19] </ref>, defined to be the sum of squares of the distances from each document in the cluster to the overall mean (centroid) of the cluster. The computation of the scatter value is very straightforward. <p> In Table 2 we compare the times for the divisive PDDP algorithm with the hierarchical agglomerative clustering algorithm (HAC) <ref> [19] </ref>, which tends to be much more expensive [17]. We also compared this algorithm with AutoClass [13], but this turned out to take even more time than the HAC on every example in the `J' series of experiments. <p> Many researchers have studied this approach either as a stand-alone method or as a way to refine an initial hierarchy constructed by another method <ref> [19, 35, 9, 8] </ref>.
Reference: [20] <author> D. Fisher. </author> <title> Iterative optimization and simplification of hierarchical clusterings. </title> <journal> J. Art. Intell. Res., </journal> <volume> 4 </volume> <pages> 147-179, </pages> <year> 1996. </year>
Reference-contexts: The iterative approach is a variation of a hierarchical clustering method in which a seed hierarchy (tree) is updated incrementally by adjusting the clusters based on a "quality" measure. In <ref> [20] </ref>, they describe a novel method for updating a whole subtree of the hierarchy which would be applicable to our top-down tree as well as to their bottom-up tree.
Reference: [21] <author> W. B. Frakes. </author> <title> Stemming algorithms. </title> <editor> In W. B. Frakes and R. Baeza-Yates, editors, </editor> <booktitle> Information Retrieval Data Structures and Algorithms, </booktitle> <pages> pages 131-160. </pages> <publisher> Prentice Hall, </publisher> <year> 1992. </year>
Reference-contexts: The documents were processed by eliminating stop words and HTML tags, stemming the remaining words using Porter's suffix-stripping algorithm [39] as implemented by <ref> [21] </ref>. etc., and then counting up the occurences of the remaining words in every document. A vector of word counts was constructed for each document, scaled to have unit length (the norm scaling), and assembled into the term frequency matrix M.
Reference: [22] <author> W. B. Frakes and R. Baeza-Yates. </author> <title> Information Retrieval Data Structures and Algorithms. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1992. </year>
Reference-contexts: The WWW also is a very dynamic environment with documents begin added and replaced very rapidly. This has lead to interest in autonomous agents to explore the WWW, such as that proposed in <ref> [12, 11, 22, 26, 34, 50, 48] </ref>. Such agents must be capable of automatically classifying and/or categorizing large datasets of documents without user intervention. <p> It is easier to assign a unique topic label to each cluster at each level with less topic overlap. 7 Related Work Existing approaches to document clustering are generally based on either probabilistic methods, or distance and similarity measures (see <ref> [22] </ref>). Distance-based methods such as k-means analysis, hierarchical clustering [30] and nearest-neighbor clustering [33] use a selected set of words (features) appearing in different documents as the dimensions. Each such feature vector, representing a document, can be viewed as a point in this multi-dimensional space. <p> A motivation of this work is its application in a Web agent [26]. A number of Web agents use various information retrieval techniques and characteristics of open hypertext Web documents to automatically retrieve, filter, and categorize these documents <ref> [12, 11, 22, 34, 50, 48] </ref>. For example, HyPursuit [48] uses semantic information embedded in link structures as well as document content to create cluster hierarchies of hypertext documents and structure an information space.
Reference: [23] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins Univ. Press, 3rd edition, </publisher> <year> 1996. </year>
Reference-contexts: This direction corresponds to the largest eigenvalue of the sample covariance matrix for the cluster. The computation of u C is the most expensive step in this whole process. We have used a fast Lanczos-based solver for the singular values of the matrix of documents in the cluster <ref> [23] </ref>. This algorithm is able to take full advantage of the fact the matrices are extremely sparse, often with less than 4% fill, where the fill is defined as the fraction of all matrix entries that are nonzero. <p> We will use e = (1; : : : ; 1) T to denote a vector of all ones of appropriate dimension, and kAk F def q P ij to denote the Frobenius norm of a given matrix A <ref> [23] </ref>. <p> The vectors u; v are actually computed as the left and right singular vectors, respectively of the matrix A = M C w C e T <ref> [23] </ref>. <p> For i 2 C, if v i 0, assign document i to L, else assign it to R. 6. Result: A binary tree with c max leaf nodes forming a partitioning of the entire data set. Table 1: PDDP Algorithm Summary solver is used <ref> [23] </ref>. Therefore, we take advantage of the fact that the matrix M in these applications tends to be extremely sparse. In our examples, the fill (fraction of all entries which are nonzero) ranged from a high of 4% to as low as 0.68%. <p> We also take advantage of the fact that we seek only the leading singular value and associated left and right singular vectors. Given the above considerations, we use a singular value solver in which the matrix enters only in the form of matrix vector products, namely the Lanczos method <ref> [23] </ref>. For the purposes of this problem, it suffices to adapt a Lanczos method for the symmetric eigenvalue problem (such as for the covariance matrix A), since high accuracy is not required. <p> These methods have become well established for symmetric eigenvalue problems, especially when the leading eigenvalue and associated eigenvector are sought <ref> [23] </ref>. Therefore, we will not derive this algorithm in detail. However we will make some remarks about this algorithm in this application in order to be able to say something about its cost. <p> It is well known that the cost of this step is proportional to the number of nonzeros present in A <ref> [23] </ref>. The only other critical factor affecting costs is the number of iterations of the Lanczos algorithm (i.e. the number of matrix-vector products). <p> In addition, computing 100 leading singular values and vectors is considerably more complicated and expensive than computing just one as in the PDDP Algorithm. It is well known from the Kaniel-Paige theory that the largest eigenvalue or singular value is among the first to converge in the Lanczos method <ref> [23] </ref>, and also fewer temporary Lanczos vectors must be generated saving much memory as well. In addition, though the PDDP method repeats this singular value computation at every step, it is applied to the entire dataset only once.
Reference: [24] <author> E. Han, G. Karypis, V. Kumar, and B. Mobasher. </author> <title> Clustering based on association rule hypergraphs (position paper). </title> <booktitle> In Workshop on Research Issues on Data Mining and Knowledge Discovery, </booktitle> <pages> pages 9-13, </pages> <address> Tucson, Arizona, </address> <year> 1997. </year>
Reference-contexts: We have also had a few comparisons with a recently developed association rule hypergraph clustering method (ARHP) <ref> [24, 25, 31] </ref>, based on association rule hypergraphs [2, 6], but this last method is harder to compare since it eliminates some documents so that the result is not strictly speaking a partitioning of the entire document set.
Reference: [25] <author> E. Han, G. Karypis, V. Kumar, and B. Mobasher. </author> <title> Clustering in a high-dimensional space using hyper-graph models. </title> <type> Technical Report TR-97-063, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, </institution> <year> 1997. </year>
Reference-contexts: We have also had a few comparisons with a recently developed association rule hypergraph clustering method (ARHP) <ref> [24, 25, 31] </ref>, based on association rule hypergraphs [2, 6], but this last method is harder to compare since it eliminates some documents so that the result is not strictly speaking a partitioning of the entire document set.
Reference: [26] <author> S. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G. Karypis, V. Kumar, B. Mobasher, and J. Moore. WebACE: </author> <title> A web agent for document categorization and exploration. </title> <booktitle> In Autonomous Agents'98 Conf., </booktitle> <year> 1998. </year>
Reference-contexts: The WWW also is a very dynamic environment with documents begin added and replaced very rapidly. This has lead to interest in autonomous agents to explore the WWW, such as that proposed in <ref> [12, 11, 22, 26, 34, 50, 48] </ref>. Such agents must be capable of automatically classifying and/or categorizing large datasets of documents without user intervention. <p> The agent must be capable of categorizing large numbers of documents in order to present the user with a short "summary" or "taxonomy" of the documents found. Many autonomous agents, such as WebACE <ref> [26] </ref>, also extract the "significant" words in order to construct new WWW search queries to find related documents. In this paper, we present an unsupervised clustering algorithm, called Principal Direction Divisive Partitioning (PDDP), that enjoys many desireable properties. <p> The purpose of this paper is to explore some of these properties: specifically its performance and its use in the construction of hierarchical taxonomies. This work grew out of a larger project involving the design of a Web agent, in which several competing algorithms have been explored <ref> [26] </ref>. The PDDP algorithm has shown itself to be competitive in both cost and quality of results as compared to the other algorithms tried in this project. <p> A motivation of this work is its application in a Web agent <ref> [26] </ref>. A number of Web agents use various information retrieval techniques and characteristics of open hypertext Web documents to automatically retrieve, filter, and categorize these documents [12, 11, 22, 34, 50, 48].
Reference: [27] <author> T. Honkela, S. Kaski, K. Lagus, and T. Kohonen. </author> <title> Newsgroup exploration with WEBSOM method and browsing interface. </title> <type> Technical Report A32, </type> <institution> Helsinki University of Technology, Laboratory of Computer and Information Science, Espoo, Finland, </institution> <year> 1996. </year>
Reference-contexts: A "trigger word model" is used to form 19 word groups with which search queries may be refined. The Kohonen Self-Organizing Feature Map <ref> [27, 32] </ref> is a neural-network-based Web agent that projects high dimensional input data into a feature map of a smaller dimension such that the proximity relationships among input data are preserved.
Reference: [28] <author> J. E. Jackson. </author> <title> A User's Guide To Principal Components. </title> <publisher> John Wiley & Sons, </publisher> <year> 1991. </year>
Reference-contexts: R, and the parent of C, denoted P; * the set of documents within the cluster, collected in an n fi k matrix M C ; * the cluster centroid vector: w C def k M C e; * The principal direction vector (also known as the leading principal component <ref> [28] </ref>), which is the eigenvector corresponding to the largest eigenvalue of the sample covariance matrix C = k 1 ; * The scatter value: scatter C = P 2 j kM C w C e T k 2 F (alternatively, set scatter C = k or even scatter C = the
Reference: [29] <author> P. Jacobs. </author> <title> Using statistical methods to improve knowledge-based news categorization. </title> <journal> IEEE Expert, </journal> <volume> 8(2) </volume> <pages> 13-24, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: Some other Web agents designed to aid the user navigate through the Web have been proposed in [42] using a graph distance matrix, in <ref> [14, 29] </ref> using a separate layer of thesaurus terms or predefined categories. In [43], they extract the context of the words to refine the matches and use a nearest neighbor-like clustering approach. The broad category of statistical approaches are compared to knowledge-based or semantic approaches in [16].
Reference: [30] <author> A. Jain and R. C. Dubes. </author> <title> Algorithms for Clustering Data. </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: It is easier to assign a unique topic label to each cluster at each level with less topic overlap. 7 Related Work Existing approaches to document clustering are generally based on either probabilistic methods, or distance and similarity measures (see [22]). Distance-based methods such as k-means analysis, hierarchical clustering <ref> [30] </ref> and nearest-neighbor clustering [33] use a selected set of words (features) appearing in different documents as the dimensions. Each such feature vector, representing a document, can be viewed as a point in this multi-dimensional space.
Reference: [31] <author> G. Karypis, R. Aggarwal, V. Kumar, and S. Shekhar. </author> <title> Multilevel hypergraph partitioning: Application in VLSI domain. </title> <booktitle> In Proceedings ACM/IEEE Design Automation Conference, </booktitle> <year> 1997. </year>
Reference-contexts: We have also had a few comparisons with a recently developed association rule hypergraph clustering method (ARHP) <ref> [24, 25, 31] </ref>, based on association rule hypergraphs [2, 6], but this last method is harder to compare since it eliminates some documents so that the result is not strictly speaking a partitioning of the entire document set.
Reference: [32] <author> T. Kohonen. </author> <title> Self-Organizing Maps. </title> <publisher> Springer-Verlag, </publisher> <address> 2nd edition, </address> <year> 1997. </year>
Reference-contexts: A "trigger word model" is used to form 19 word groups with which search queries may be refined. The Kohonen Self-Organizing Feature Map <ref> [27, 32] </ref> is a neural-network-based Web agent that projects high dimensional input data into a feature map of a smaller dimension such that the proximity relationships among input data are preserved.
Reference: [33] <author> S. Lu and K. Fu. </author> <title> A sentence-to-sentence clustering procedure for pattern analysis. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 8 </volume> <pages> 381-389, </pages> <year> 1978. </year>
Reference-contexts: Distance-based methods such as k-means analysis, hierarchical clustering [30] and nearest-neighbor clustering <ref> [33] </ref> use a selected set of words (features) appearing in different documents as the dimensions. Each such feature vector, representing a document, can be viewed as a point in this multi-dimensional space. Most of these methods tend to be agglomerative (bottom-up) in contrast to our divisive (top-down) method.
Reference: [34] <author> Y. S. Maarek and I. B. Shaul. </author> <title> Automatically organizing bookmarks per content. </title> <booktitle> In Proc. of 5th International World Wide Web Conference, </booktitle> <year> 1996. </year>
Reference-contexts: The WWW also is a very dynamic environment with documents begin added and replaced very rapidly. This has lead to interest in autonomous agents to explore the WWW, such as that proposed in <ref> [12, 11, 22, 26, 34, 50, 48] </ref>. Such agents must be capable of automatically classifying and/or categorizing large datasets of documents without user intervention. <p> A motivation of this work is its application in a Web agent [26]. A number of Web agents use various information retrieval techniques and characteristics of open hypertext Web documents to automatically retrieve, filter, and categorize these documents <ref> [12, 11, 22, 34, 50, 48] </ref>. For example, HyPursuit [48] uses semantic information embedded in link structures as well as document content to create cluster hierarchies of hypertext documents and structure an information space.
Reference: [35] <author> R. Michalski and R. Stepp. </author> <title> Automated construction of classifications: Conceptual clustering versus numerical taxonomy. </title> <journal> IEEE Trans. Patt. Anal. Mach. Intell., </journal> <volume> 5 </volume> <pages> 219-243, </pages> <year> 1993. </year>
Reference-contexts: Many researchers have studied this approach either as a stand-alone method or as a way to refine an initial hierarchy constructed by another method <ref> [19, 35, 9, 8] </ref>.
Reference: [36] <author> J. Mingers. </author> <title> Am empirical comparison of selection measures for decision-tree induction. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 319-342, </pages> <year> 1989. </year>
Reference-contexts: In [20], they describe a novel method for updating a whole subtree of the hierarchy which would be applicable to our top-down tree as well as to their bottom-up tree. They also propose a category index (similar in form to the Gini Index <ref> [36, 49] </ref>) as a measure of cluster "quality," which could be adapted in the PDDP algorithm as a substitute for our scatter value.
Reference: [37] <author> M. Nadler and E. P. Smith. </author> <title> Pattern Recognition Engineering. </title> <publisher> Wiley, </publisher> <year> 1993. </year>
Reference-contexts: The formula (2) is an example of a linear discriminant function. Linear discriminant functions have been used extensively to partition samples in a test set into two classes. An example is the Fisher linear discriminant function (see e.g. <ref> [37] </ref>), typically used with a training set of samples with known "correct" classifications. As such it is usually used as a tool in "supervised learning," in which a training set with previously known class designations are used. <p> Previous algorithms for unsupervised clustering based on the use of one-dimensional projection Bayesian analysis or linear discriminants is very limited, at least to the knowledge of this author. A hint in this direction appears on <ref> [37, p500] </ref>, where the repeated use of a Fisher-style linear discriminant is suggested, 18 resulting in a hierarchical classification. It is even suggested that the Karhunen Loeve transformation might lead to good directions.
Reference: [38] <author> M. Pazzani, J. Muramatsu, and D. Billsus. Syskill & Webert: </author> <title> Identifying interesting Web sites. </title> <booktitle> In National Conference on Artificial Intelligence, </booktitle> <pages> pages 54-61, </pages> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: The number of times a user visits a document and the total amount of time a user spends viewing a document are just a few methods for determining user interest [1, 4, 5]. Syskill & Webert <ref> [38, 1] </ref> represents an HTML page with a Boolean feature vector, and then uses simple Bayesian classification to find Web pages that are similar, but for only a given single user profile.
Reference: [39] <author> M. F. Porter. </author> <title> An algorithm for suffix stripping. </title> <booktitle> Program, </booktitle> <volume> 14(3) </volume> <pages> 130-137, </pages> <year> 1980. </year>
Reference-contexts: The documents were processed by eliminating stop words and HTML tags, stemming the remaining words using Porter's suffix-stripping algorithm <ref> [39] </ref> as implemented by [21]. etc., and then counting up the occurences of the remaining words in every document. A vector of word counts was constructed for each document, scaled to have unit length (the norm scaling), and assembled into the term frequency matrix M.
Reference: [40] <author> J. R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: However, in order to compare the performance of different algorithms in a quantitative manner, we used these labels to to compute an entropy <ref> [40] </ref> measure on the resulting partitions. The entropy of a cluster j is defined by e j = i c (i; j) i c (i; j) log c (i; j) i c (i; j) ; where c (i; j) is the number of times label i occurs in cluster j.
Reference: [41] <author> H. Ritter and T. Kohonen. </author> <title> Self-organizing semantic maps. </title> <journal> Biological Cyvernetics, </journal> <volume> 61 </volume> <pages> 241-254, </pages> <year> 1989. </year> <month> 23 </month>
Reference-contexts: On data sets of very large dimensionality such as those discussed here, the word set was preprocessed by removing stop words and infrequent words and then by using a self-organizing semantic map <ref> [41] </ref> to coalesce the remaining words down to a few hundred word clusters before the self-organizing algorithm was applied. 8 Conclusions In this paper we have described an unsupervised divisive partitioning algorithm which exhibits the property of scalability while yielding good quality clusters.
Reference: [42] <author> E. Rivlin, R. Botafogo, and B. Shneiderman. </author> <title> Navigating in hyperspace: Designing a structure-based toolbox. </title> <journal> Comm. ACM, </journal> <volume> 37(2) </volume> <pages> 87-96, </pages> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: Some other Web agents designed to aid the user navigate through the Web have been proposed in <ref> [42] </ref> using a graph distance matrix, in [14, 29] using a separate layer of thesaurus terms or predefined categories. In [43], they extract the context of the words to refine the matches and use a nearest neighbor-like clustering approach.
Reference: [43] <author> G. Salton, J. Allan, and C. Buckley. </author> <title> Automatic structuring and retrieval of large text files. </title> <journal> Comm. A.C.M, </journal> <volume> 37(2) </volume> <pages> 97-108, </pages> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: Some other Web agents designed to aid the user navigate through the Web have been proposed in [42] using a graph distance matrix, in [14, 29] using a separate layer of thesaurus terms or predefined categories. In <ref> [43] </ref>, they extract the context of the words to refine the matches and use a nearest neighbor-like clustering approach. The broad category of statistical approaches are compared to knowledge-based or semantic approaches in [16]. Many engines have been proposed that attempt to record documents of interest to the user.
Reference: [44] <author> G. Salton and M. J. McGill. </author> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill, </publisher> <year> 1983. </year>
Reference-contexts: This scaling is called the norm scaling. An alternate scaling is the tfidf scaling <ref> [44] </ref>. <p> In fact, the use of a distance measure is limited to the selection of the next cluster to split, and this choice could be made by any user-supplied method. Techniques such as tfidf <ref> [44] </ref> have been proposed precisely to deal with several issues, including the presence of words appearing in all the documents. <p> Also, Balabanovic [5] presents a system that uses a single well-defined profile to find similar Web documents for a user. Candidate Web pages are located using best-first search, comparing their word vectors against a user profile vector, and returning the highest -scoring pages. A tfidf scheme <ref> [44] </ref> is used to calculate the word weights, normalized for document length. The system needs to keep a large dictionary and is limited to one user.
Reference: [45] <author> K. Sycara and L. Chen. WebMate: </author> <title> A personal agent for World-Wide Web browsing and searching. </title> <booktitle> In Autonomous Agents'98 Conf., </booktitle> <year> 1998. </year>
Reference-contexts: A tfidf scheme [44] is used to calculate the word weights, normalized for document length. The system needs to keep a large dictionary and is limited to one user. The Web agent "WebMate" proposed in <ref> [45] </ref> also uses tfidf scaling, and is based on an incremental nearest neighbor approach [46] in which each new document vector is appended to a list of vectors representing topic centers, and then the two closest vectors are agglomerated.
Reference: [46] <author> K. Sycara and A. Pannu. </author> <title> A learning personal agent for text filtering and notification. </title> <booktitle> In Proc. Int'l. Conf. Knowledge Based Systems (KBCS 96), </booktitle> <year> 1996. </year>
Reference-contexts: A tfidf scheme [44] is used to calculate the word weights, normalized for document length. The system needs to keep a large dictionary and is limited to one user. The Web agent "WebMate" proposed in [45] also uses tfidf scaling, and is based on an incremental nearest neighbor approach <ref> [46] </ref> in which each new document vector is appended to a list of vectors representing topic centers, and then the two closest vectors are agglomerated. A "trigger word model" is used to form 19 word groups with which search queries may be refined.
Reference: [47] <author> D. Titterington, A. Smith, and U. Makov. </author> <title> Statistical Analysis of Finite Mixture Distributions. </title> <publisher> John Wiley & Sons, </publisher> <year> 1985. </year>
Reference-contexts: The right column shows the document labels assigned by hand for each individual document based on the categories in Table 3 with one letter per document. 16 AutoClass [13] is a method using Bayesian analysis based on the probabilistic mixture modeling <ref> [47] </ref>. Given a data set it finds maximum parameter values for a specific probability distribution functions of the clusters. In our experiments, we have found that the performance of this algorithm suffers when applied to very large datasets as in our application.
Reference: [48] <author> R. Weiss, B. Velez, M. A. Sheldon, C. Nemprempre, P. Szilagyi, A. Duda, and D. K. Gifford. HyPursuit: </author> <title> A hierarchical network search engine that exploits content-link hypertext clustering. </title> <booktitle> In Seventh ACM Conference on Hypertext, </booktitle> <month> Mar. </month> <year> 1996. </year>
Reference-contexts: The WWW also is a very dynamic environment with documents begin added and replaced very rapidly. This has lead to interest in autonomous agents to explore the WWW, such as that proposed in <ref> [12, 11, 22, 26, 34, 50, 48] </ref>. Such agents must be capable of automatically classifying and/or categorizing large datasets of documents without user intervention. <p> A motivation of this work is its application in a Web agent [26]. A number of Web agents use various information retrieval techniques and characteristics of open hypertext Web documents to automatically retrieve, filter, and categorize these documents <ref> [12, 11, 22, 34, 50, 48] </ref>. For example, HyPursuit [48] uses semantic information embedded in link structures as well as document content to create cluster hierarchies of hypertext documents and structure an information space. <p> A motivation of this work is its application in a Web agent [26]. A number of Web agents use various information retrieval techniques and characteristics of open hypertext Web documents to automatically retrieve, filter, and categorize these documents [12, 11, 22, 34, 50, 48]. For example, HyPursuit <ref> [48] </ref> uses semantic information embedded in link structures as well as document content to create cluster hierarchies of hypertext documents and structure an information space.
Reference: [49] <author> S. Weiss and C. </author> <title> Kulikowski. Computer Systems that Learn. </title> <publisher> Morgan Kaufman, </publisher> <year> 1991. </year>
Reference-contexts: In [20], they describe a novel method for updating a whole subtree of the hierarchy which would be applicable to our top-down tree as well as to their bottom-up tree. They also propose a category index (similar in form to the Gini Index <ref> [36, 49] </ref>) as a measure of cluster "quality," which could be adapted in the PDDP algorithm as a substitute for our scatter value.
Reference: [50] <author> M. R. Wulfekuhler and W. F. Punch. </author> <title> Finding salient features for personal Web page categories. </title> <booktitle> In Proc. of 6th International World Wide Web Conference, </booktitle> <month> Apr. </month> <year> 1997. </year> <month> 24 </month>
Reference-contexts: The WWW also is a very dynamic environment with documents begin added and replaced very rapidly. This has lead to interest in autonomous agents to explore the WWW, such as that proposed in <ref> [12, 11, 22, 26, 34, 50, 48] </ref>. Such agents must be capable of automatically classifying and/or categorizing large datasets of documents without user intervention. <p> A motivation of this work is its application in a Web agent [26]. A number of Web agents use various information retrieval techniques and characteristics of open hypertext Web documents to automatically retrieve, filter, and categorize these documents <ref> [12, 11, 22, 34, 50, 48] </ref>. For example, HyPursuit [48] uses semantic information embedded in link structures as well as document content to create cluster hierarchies of hypertext documents and structure an information space. <p> For example, HyPursuit [48] uses semantic information embedded in link structures as well as document content to create cluster hierarchies of hypertext documents and structure an information space. Pattern recognition methods and word clustering using the Hartigan's K-means partitional clustering algorithm are used in <ref> [50] </ref> to discover salient HTML document features (words) that can be used in finding similar HTML documents on the Web.
References-found: 50

