URL: http://www.iscs.nus.sg/~plong/papers/nonmal.ps
Refering-URL: 
Root-URL: 
Title: Halfspace Learning, Linear Programming, and Nonmalicious Distributions log 2 n n 3:38 log time. For
Author: Philip M. Long 
Keyword: Computational learning theory, analysis of algorithms.  
Note: O n 2  which is O(n 2 5  Supported by Air Force Office of Scientific Research grant F49620-92-J-0515 and a Lise Meitner Fellowship from the Fonds zur Forderung der wissenschaftlichen Forschung (Austria).  
Date: September 7, 1994  
Address: P.O. Box 90129 Durham, North Carolina 27708  
Affiliation: Computer Science Department Duke University  
Abstract: We study the application of a linear programming algorithm due to Vaidya to the problem of learning halfspaces in Baum's nonmalicious distribution model. We prove that, in n dimensions, this algorithm learns up to * accuracy with probability 1 * in 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 319-342, </pages> <year> 1988. </year>
Reference-contexts: model, we have found that a different approach doesn't require any assumptions on the precision of the ~x j 's, and yields significantly improved time bounds. 3 The iterations in Vaidya's linear programming algorithm can be viewed as what are called queries in another model of learning (due to Angluin <ref> [1] </ref>) often called the equivalence query model [12]. Implicit in Vaidya's analysis of his algorithm are bounds the number of iterations made by this algorithm in terms of the volume of the solution set of the system of linear inequalities [18]. <p> These bounds can therefore also be viewed as equivalence query bounds. Using conversions from this model to the PAC model <ref> [1, 10, 11] </ref>, one can see that, for arbitrary * &gt; 0, if the volume of the set of normal vectors defining halfspaces that "agree" with the target halfspace H on ~x 1 ; :::; ~x m is not unreasonably small, then, with high probability, a randomized variant of Vaidya's algorithm <p> We will also discuss concept classes not indexed by n. It should be obvious how to adjust the above definitions for this case. Finally, we define the equivalence-query model <ref> [1] </ref>. In the equivalence-query model, when the algorithm is learning a class C, it sequentially produces queries which are (representations of) elements of C. When the algorithm correctly guesses the function C to be learned, the learning process is over.
Reference: [2] <author> E. Baum. </author> <title> The perceptron algorithm is fast for nonmalicious distributions. </title> <journal> Neural Computation, </journal> <volume> 2 </volume> <pages> 248-260, </pages> <year> 1990. </year>
Reference-contexts: In this paper, we analyze the use of one of Vaidya's linear programming algorithms [18] to learn halfspaces in a variant of the PAC model [19] due to Baum <ref> [2] </ref>, called the non-malicious distribution (NMD) model. In the NMD model, a halfspace H IR n is chosen randomly. There is a straightforward reduction from the problem of learning arbitrary halfspaces to learning homogeneous halfspaces, i.e., those that go through the origin (see, e.g. [5]). <p> However, combining the two simplifies our bounds greatly, and it seems that in practice the two should at least be close. Both being extremely sure of having a mediocre hypothesis, and being fairly sure of having an outstanding hypothesis seem unusual goals. 1 Baum <ref> [2] </ref> showed that the perceptron algorithm [15] accomplishes the above in O (n 2 =* 5 ) time. In this paper, we show that an algorithm based on linear programming [18], 2 requires O n 2 log 2 n + n 3:38 log * time. <p> Now we turn to the NMD model <ref> [2] </ref>. Suppose, for each n 2 IN , P n is a probability distribution on C n , and P = fP n : n 1g.
Reference: [3] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> JACM, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <year> 1989. </year>
Reference-contexts: probability of failure depends on the random choice of the target as well as the random sample. 2 The idea of using linear programming for recovering a "separating" halfspace is quite old (see [5]), and its use to generate polynomial time learning algorithms originates with Blumer, Ehrenfeucht, Haussler and Warmuth <ref> [3] </ref>. 1 O (n 3 ). However, replacing the n 2:38 in our learning bounds with n 3 only changes the n 3:38 in our bound to an n 4 . An O (n 4 log n) bound would still be a significant improvement over O (n 7 ). <p> , and then find a halfspace ^ H for which each of ~x 1 ; :::; ~x m is in ^ H if and only if it is in the halfspace H to be learned, since this requirement amounts to m linear constraints on the normal vector of ^ H <ref> [3] </ref>. <p> Such an algorithm would imply unit-cost polynomial time algorithms for learning in the NMD model, as well as Valiant's PAC model [19] (see <ref> [3] </ref>).) In the NMD model, we have found that a different approach doesn't require any assumptions on the precision of the ~x j 's, and yields significantly improved time bounds. 3 The iterations in Vaidya's linear programming algorithm can be viewed as what are called queries in another model of learning <p> The following bound will also be useful. Lemma 4 (see <ref> [16, 6, 3] </ref>) Choose m; n 2 IN , and S = f~x 1 ; :::; ~x m g 2 IR n .
Reference: [4] <author> D. Coppersmith and S. Winograd. </author> <title> Matrix multiplication via arithmetic progressions. </title> <booktitle> Proceedings of the 19th ACM Symposium on the Theory of Computation, </booktitle> <year> 1987. </year>
Reference-contexts: We make use of O (n 2:38 ) time matrix multiplication <ref> [4] </ref> as a subroutine.
Reference: [5] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference-contexts: In the NMD model, a halfspace H IR n is chosen randomly. There is a straightforward reduction from the problem of learning arbitrary halfspaces to learning homogeneous halfspaces, i.e., those that go through the origin (see, e.g. <ref> [5] </ref>). We will therefore restrict our attention to homogeneous halfspaces. <p> brought down to being logarithmic cannot be easily modified for this model, since, loosely speaking, here the probability of failure depends on the random choice of the target as well as the random sample. 2 The idea of using linear programming for recovering a "separating" halfspace is quite old (see <ref> [5] </ref>), and its use to generate polynomial time learning algorithms originates with Blumer, Ehrenfeucht, Haussler and Warmuth [3]. 1 O (n 3 ). However, replacing the n 2:38 in our learning bounds with n 3 only changes the n 3:38 in our bound to an n 4 .
Reference: [6] <author> H. Edelsbrunner. </author> <title> Algorithms in Combinatorial Geometry. </title> <publisher> Springer Verlag, </publisher> <year> 1984. </year>
Reference-contexts: The following bound will also be useful. Lemma 4 (see <ref> [16, 6, 3] </ref>) Choose m; n 2 IN , and S = f~x 1 ; :::; ~x m g 2 IR n .
Reference: [7] <author> W. Feller. </author> <title> An Introduction to Probability and its Applications, volume 1. </title> <publisher> John Wiley and Sons, </publisher> <address> third edition, </address> <year> 1968. </year>
Reference-contexts: By Fubini's Theorem (see <ref> [7, volume 2, page 120] </ref>), since this holds for arbitrary S, it holds for randomly chosen S as well.
Reference: [8] <author> D. Haussler, M. Kearns, N. Littlestone, and M. Warmuth. </author> <title> Equivalence of models for polynomial learnability. </title> <journal> Information and Computation, </journal> <volume> 95 </volume> <pages> 129-161, </pages> <year> 1991. </year>
Reference-contexts: Since the constants in the analysis of this matrix multiplication algorithm, as well as those for other o (n 3 ) algorithms, are quite large, some claim that in practice the time required for multiplying n fi n matrices grows like 1 The proof of results for the PAC model <ref> [8] </ref> that show that the dependence of the requirements on the inverse of the "confidence" can always be brought down to being logarithmic cannot be easily modified for this model, since, loosely speaking, here the probability of failure depends on the random choice of the target as well as the random
Reference: [9] <author> D. Haussler, M. Kearns, and R. Schapire. </author> <title> Bounds on the sample complexity of bayesian learning using information theory and the VC-dimension. </title> <booktitle> The 1991 Workshop on Computational Learning Theory, </booktitle> <pages> pages 61-74, </pages> <year> 1991. </year> <month> 6 </month>
Reference-contexts: This model takes a Bayesian viewpoint, as has been done in <ref> [9, 14] </ref> and elsewhere. In Baum's original formulation, there were two separate parameters measuring the accuracy and the probability that the given accuracy was achieved. However, combining the two simplifies our bounds greatly, and it seems that in practice the two should at least be close.
Reference: [10] <author> M. Kearns, M. Li, L. Pitt, and L. Valiant. </author> <title> Recent results on boolean concept learning. </title> <editor> In P. Langley, editor, </editor> <booktitle> Proceedings of the Fourth International Workshop on Machine Learning, </booktitle> <pages> pages 337-352, </pages> <address> Irvine, California, June 1987. </address> <publisher> (published by Morgan Kaufmann, </publisher> <address> Los Altos California). </address>
Reference-contexts: These bounds can therefore also be viewed as equivalence query bounds. Using conversions from this model to the PAC model <ref> [1, 10, 11] </ref>, one can see that, for arbitrary * &gt; 0, if the volume of the set of normal vectors defining halfspaces that "agree" with the target halfspace H on ~x 1 ; :::; ~x m is not unreasonably small, then, with high probability, a randomized variant of Vaidya's algorithm
Reference: [11] <author> N. Littlestone. </author> <title> From on-line to batch learning. </title> <booktitle> The 1989 Workshop on Computational Learning Theory, </booktitle> <pages> pages 269-284, </pages> <year> 1989. </year>
Reference-contexts: These bounds can therefore also be viewed as equivalence query bounds. Using conversions from this model to the PAC model <ref> [1, 10, 11] </ref>, one can see that, for arbitrary * &gt; 0, if the volume of the set of normal vectors defining halfspaces that "agree" with the target halfspace H on ~x 1 ; :::; ~x m is not unreasonably small, then, with high probability, a randomized variant of Vaidya's algorithm
Reference: [12] <author> W. Maass and G. Turan. </author> <title> How fast can a threshold gate learn? Technical Report 321, </title> <institution> Graz Technical University, </institution> <year> 1991. </year> <note> Previous versions appeared in FOCS89 and FOCS90. </note>
Reference-contexts: approach doesn't require any assumptions on the precision of the ~x j 's, and yields significantly improved time bounds. 3 The iterations in Vaidya's linear programming algorithm can be viewed as what are called queries in another model of learning (due to Angluin [1]) often called the equivalence query model <ref> [12] </ref>. Implicit in Vaidya's analysis of his algorithm are bounds the number of iterations made by this algorithm in terms of the volume of the solution set of the system of linear inequalities [18]. These bounds can therefore also be viewed as equivalence query bounds. <p> HALF S;v = fH ~w;S : ~w 2 IR n ; volume (E S;~w ) vg: Now we are ready for another lemma, which is implicit in the analysis of Vaidya [18], if his linear programming algorithm is viewed as a query algorithm in the manner of Maass and Turan <ref> [12] </ref>.
Reference: [13] <author> W. S. McCulloch and W. Pitts. </author> <title> A logical calculus of ideas imminent in neural nets. </title> <journal> Bulletin of Mathematical Biophysics, </journal> <volume> 5 </volume> <pages> 115-137, </pages> <year> 1943. </year>
Reference-contexts: 1 Introduction In part due to their interpretation as artificial neurons <ref> [13, 15] </ref>, a great deal has been written about the analysis of halfspaces in different computational models of learning.
Reference: [14] <author> M. Opper and D. Haussler. </author> <title> Calculation of the learning curve of Bayes optimal classification algorithm for learning a perceptron with noise. </title> <booktitle> In Computational Learning Theory: Proceedings of the Fourth Annual Workshop, </booktitle> <pages> pages 75-87. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: This model takes a Bayesian viewpoint, as has been done in <ref> [9, 14] </ref> and elsewhere. In Baum's original formulation, there were two separate parameters measuring the accuracy and the probability that the given accuracy was achieved. However, combining the two simplifies our bounds greatly, and it seems that in practice the two should at least be close.
Reference: [15] <author> F. Rosenblatt. </author> <title> Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms. </title> <publisher> Spartan Books, </publisher> <address> Washington, D. C., </address> <year> 1962. </year>
Reference-contexts: 1 Introduction In part due to their interpretation as artificial neurons <ref> [13, 15] </ref>, a great deal has been written about the analysis of halfspaces in different computational models of learning. <p> Both being extremely sure of having a mediocre hypothesis, and being fairly sure of having an outstanding hypothesis seem unusual goals. 1 Baum [2] showed that the perceptron algorithm <ref> [15] </ref> accomplishes the above in O (n 2 =* 5 ) time. In this paper, we show that an algorithm based on linear programming [18], 2 requires O n 2 log 2 n + n 3:38 log * time.
Reference: [16] <author> N. Sauer. </author> <title> On the density of families of sets. </title> <journal> J. Combinatorial Theory (A), </journal> <volume> 13 </volume> <pages> 145-147, </pages> <year> 1972. </year>
Reference-contexts: The following bound will also be useful. Lemma 4 (see <ref> [16, 6, 3] </ref>) Choose m; n 2 IN , and S = f~x 1 ; :::; ~x m g 2 IR n .
Reference: [17] <author> R. Tarjan. </author> <title> Data Structures and Network Algorithms. </title> <publisher> SIAM, </publisher> <year> 1983. </year>
Reference-contexts: For sets S 1 and S 2 , let S 1 S 2 denote the symmetric difference of S 1 and S 2 . We use the unit cost RAM model of computation (see, e.g. <ref> [17] </ref>), where it is assumed that the standard operations on real numbers can be done in unit time. Choose X = [ n X n , and for all n, let C n be a class of subsets of X n , and C = [ n C n .
Reference: [18] <author> P. Vaidya. </author> <title> A new algorithm for minimizing convex functions over convex sets. </title> <booktitle> Proceedings of the 30th Annual Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 338-343, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction In part due to their interpretation as artificial neurons [13, 15], a great deal has been written about the analysis of halfspaces in different computational models of learning. In this paper, we analyze the use of one of Vaidya's linear programming algorithms <ref> [18] </ref> to learn halfspaces in a variant of the PAC model [19] due to Baum [2], called the non-malicious distribution (NMD) model. In the NMD model, a halfspace H IR n is chosen randomly. <p> In this paper, we show that an algorithm based on linear programming <ref> [18] </ref>, 2 requires O n 2 log 2 n + n 3:38 log * time. For many values of n and *, this represents a substantial improvement. <p> Implicit in Vaidya's analysis of his algorithm are bounds the number of iterations made by this algorithm in terms of the volume of the solution set of the system of linear inequalities <ref> [18] </ref>. These bounds can therefore also be viewed as equivalence query bounds. <p> Finally, for a finite set S IR n , and v 0, let HALF S;v = fH ~w;S : ~w 2 IR n ; volume (E S;~w ) vg: Now we are ready for another lemma, which is implicit in the analysis of Vaidya <ref> [18] </ref>, if his linear programming algorithm is viewed as a query algorithm in the manner of Maass and Turan [12].
Reference: [19] <author> L. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <year> 1984. </year> <month> 7 </month>
Reference-contexts: In this paper, we analyze the use of one of Vaidya's linear programming algorithms [18] to learn halfspaces in a variant of the PAC model <ref> [19] </ref> due to Baum [2], called the non-malicious distribution (NMD) model. In the NMD model, a halfspace H IR n is chosen randomly. There is a straightforward reduction from the problem of learning arbitrary halfspaces to learning homogeneous halfspaces, i.e., those that go through the origin (see, e.g. [5]). <p> Such an algorithm would imply unit-cost polynomial time algorithms for learning in the NMD model, as well as Valiant's PAC model <ref> [19] </ref> (see [3]).) In the NMD model, we have found that a different approach doesn't require any assumptions on the precision of the ~x j 's, and yields significantly improved time bounds. 3 The iterations in Vaidya's linear programming algorithm can be viewed as what are called queries in another model <p> A then outputs an algorithm for computing a function h : X n ! f0; 1g. Usually, the algorithm can be understood from the description of h, and we will refer to the two interchangeably in the rest of this paper. We begin with the PAC model <ref> [19] </ref>.
References-found: 19

