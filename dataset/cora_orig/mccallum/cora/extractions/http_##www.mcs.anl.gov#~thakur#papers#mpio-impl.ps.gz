URL: http://www.mcs.anl.gov/~thakur/papers/mpio-impl.ps.gz
Refering-URL: http://www.mcs.anl.gov/~thakur/papers.html
Root-URL: http://www.mcs.anl.gov
Email: @mcs.anl.gov  
Title: On Implementing MPI-IO Portably and with High Performance  
Author: Rajeev Thakur William Gropp Ewing Lusk fthakur, gropp, luskg 
Address: Argonne, IL 60439, USA  
Affiliation: Mathematics and Computer Science Division Argonne National Laboratory  
Date: May 1999.  
Note: To appear in Proc. of the Sixth Workshop on I/O in Parallel and Distributed Systems,  c 1999 ACM.  
Abstract: We discuss the issues involved in implementing MPI-IO portably on multiple machines and file systems and also achieving high performance. One way to implement MPI-IO portably is to implement it on top of the basic Unix I/O functions (open, lseek, read, write, and close), which are themselves portable. We argue that this approach has limitations in both functionality and performance. We instead advocate an implementation approach that combines a large portion of portable code and a small portion of code that is optimized separately for different machines and file systems. We have used such an approach to develop a high-performance, portable MPI-IO implementation, called ROMIO. In addition to basic I/O functionality, we consider the issues of supporting other MPI-IO features, such as 64-bit file sizes, noncontiguous accesses, collective I/O, asynchronous I/O, consistency and atomicity semantics, user-supplied hints, shared file pointers, portable data representation, and file preallocation. We describe how we implemented each of these features on various machines and file systems. The machines we consider are the HP Exemplar, IBM SP, Intel Paragon, NEC SX-4, SGI Origin2000, and networks of workstations; and the file systems we consider are HP HFS, IBM PIOFS, Intel PFS, NEC SFS, SGI XFS, NFS, and any general Unix file system (UFS). We also present our thoughts on how a file system can be designed to better support MPI-IO. We provide a list of features desired from a file system that would help in implementing MPI-IO correctly and with high performance.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Baylor and C. Wu. </author> <title> Parallel I/O Workload Characteristics Using Vesta. </title> <editor> In R. Jain, J. Werth, and J. Browne, editors, </editor> <booktitle> Input/Output in Parallel and Distributed Computer Systems, chapter 7, </booktitle> <pages> pages 167185. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1996. </year>
Reference-contexts: Noncontiguous locations in memory can be specified by using a derived datatype in the read/write call. The ability of users to specify noncontiguous accesses in a single function call is very important, because noncontiguous accesses are very common in parallel applications <ref> [1, 4, 21, 26, 27, 32] </ref>. Most file systems, however, do not provide functions for noncontiguous I/O. The Unix functions readv/writev are widely supported, but they allow noncontiguity only in memory and not in the file.
Reference: [2] <author> P. Cao, E. Felten, A. Karlin, and K. Li. </author> <title> Implementation and Performance of Integrated Application-Controlled File Caching, Prefetching, and Disk Scheduling. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 14(4):311343, </volume> <month> November </month> <year> 1996. </year>
Reference-contexts: In such cases, the MPI-IO implementation cannot use the nonatomic mode on PFS. 3.8 Hints MPI-IO provides a mechanism for the user to pass hints to the implementation. Hints, such as access-pattern information, can help the implementation optimize file access <ref> [2, 22] </ref>. Hints do not change the semantics of the MPI-IO interface; an implementation may choose to ignore all hints, and the program would still be functionally correct. MPI-IO has some predefined hints for specifying file-striping parameters, access patterns, and so on. An implementation is free to define additional hints. <p> The file system must therefore either detect and automatically adapt to changing access patterns [16, 17] or provide an interface for the user to specify the access pattern or caching/prefetching policy <ref> [2, 22] </ref>. 10. File Preallocation. It is easy and inexpensive for a file system to provide a function to preallocate disk space for a file.
Reference: [3] <author> P. Corbett, J. Prost, C. Demetriou, G. Gibson, E. Rei-del, J. Zelenka, Y. Chen, E. Felten, K. Li, J. Hartman, L. Peterson, B. Bershad, A. Wolman, and R. Aydt. </author> <title> Proposal for a Common Parallel File System Programming Interface, </title> <note> Version 1.0. On the World-Wide Web at http://www.cs.arizona.edu/sio/api1.0.ps.gz, September 1996. </note>
Reference-contexts: Client-side caching must be disabled by locking the portion of the file being accessed, by using fcntl. A lock and unlock are therefore needed across the read/write call. 4. Many research file systems provide their own APIs <ref> [9, 3, 11, 15, 20] </ref>. Implementing MPI-IO on top of Unix I/O functions will not be portable to these file systems. An alternative is to implement MPI-IO on top of the POSIX I/O interface [12] instead of the basic Unix I/O functions.
Reference: [4] <author> P. Crandall, R. Aydt, A. Chien, and D. Reed. </author> <title> Input-Output Characteristics of Scalable Parallel Applications. </title> <booktitle> In Proceedings of Supercomputing '95. </booktitle> <publisher> ACM Press, </publisher> <month> December </month> <year> 1995. </year>
Reference-contexts: Noncontiguous locations in memory can be specified by using a derived datatype in the read/write call. The ability of users to specify noncontiguous accesses in a single function call is very important, because noncontiguous accesses are very common in parallel applications <ref> [1, 4, 21, 26, 27, 32] </ref>. Most file systems, however, do not provide functions for noncontiguous I/O. The Unix functions readv/writev are widely supported, but they allow noncontiguity only in memory and not in the file.
Reference: [5] <author> J. del Rosario, R. Bordawekar, and A. Choudhary. </author> <title> Improved Parallel I/O via a Two-Phase Run-time Access Strategy. </title> <booktitle> In Proceedings of the Workshop on I/O in Parallel Computer Systems at IPPS '93, </booktitle> <pages> pages 5670, </pages> <month> April </month> <year> 1993. </year> <note> Also published in Computer Architecture News, 21(5):3138, Decem-ber 1993. </note>
Reference-contexts: The merged request can therefore be serviced efficiently. Such optimization is broadly referred to as collective I/O. Collective I/O has been shown to be a very important optimization in parallel I/O and can improve performance significantly <ref> [5, 14, 25, 30, 33] </ref>. Since none of the file systems on which ROMIO is implemented perform collective I/O, ROMIO performs two-phase collective I/O on top of the file system. In the communication phase, interpro-cess communication is used to rearrange data into large chunks. <p> Leave Collective I/O to the MPI-IO Implementation. It is not entirely clear whether collective I/O is better if performed in the file system or as a library above the file system. Both techniques have been proposed in the literature <ref> [5, 14, 25] </ref>.
Reference: [6] <author> P. Dickens and R. Thakur. </author> <title> Improving Collective I/O Performance Using Threads. </title> <booktitle> In Proceedings of the 13th International Parallel Processing Symposium and 10th Symposium on Parallel and Distributed Processing, </booktitle> <month> April </month> <year> 1999. </year>
Reference-contexts: The most natural way to implement split collective I/O in a nonblocking fashion is to spawn a thread that performs the entire collective-I/O operation in the background. The results in <ref> [6] </ref>, however, indicate that, on most machines, this approach performs much worse than if collective I/O were done entirely in the main thread during the begin function. <p> The split-collective-I/O functions in ROMIO, at present, perform the entire collective-I/O operation in the main thread during the begin function. We plan to implement true nonblocking collective I/O in ROMIO by incorporating the results of <ref> [6] </ref>. 3.5 Nonblocking (Asynchronous) I/O Many file systems support nonblocking I/O. One way to implement MPI-IO's nonblocking I/O functions is to use the nonblock-ing functions of the file system. Intel PFS supports nonstandard functions called iread and iwrite.
Reference: [7] <author> S. Fineberg, P. Wong, B. Nitzberg, and C. Kuszmaul. </author> <title> PMPIOA Portable Implementation of MPI-IO. </title> <booktitle> In Proceedings of the Sixth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 188195. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1996. </year>
Reference-contexts: MPI-IO is a comprehensive API with many features intended specifically for I/O parallelism, portability, and high performance. Implementations of MPI-IO, both portable and machine-specific, are already available <ref> [7, 13, 23, 24, 34] </ref>. In this paper, we discuss the issues involved in implementing MPI-IO portably on multiple machines and file systems and also achieving high performance.
Reference: [8] <author> I. Foster, D. Kohr, R. Krishnaiyer, and J. Mogill. </author> <title> Remote I/O: Fast Access to Distant Storage. </title> <booktitle> In Proceedings of the Fifth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 1425. </pages> <publisher> ACM Press, </publisher> <month> November </month> <year> 1997. </year>
Reference-contexts: Another application of ADIO is for implementing remote I/O. An MPI-IO implementation can enable a program running on one machine to access files from remote machines by providing an ADIO implementation that accesses data from an ADIO server running at a remote site. Such an implementation is described in <ref> [8] </ref> and also illustrated in Figure 1. A similar abstract-device interface is used in MPICH [10] for implementing MPI portably. 3 Implementing MPI-IO We describe how we implemented each feature of MPI-IO on various machines and file systems. <p> In such a case, the MPI-IO implementation would need to have servers that implement a virtual shared file system on top of the individual file systems on these machines. Another example is when MPI-IO is used to access files from remote machines, as described in <ref> [8] </ref>. 3.12.2 Operating with Multiple MPI-1 Implementations MPI-IO can be implemented in a way that it can operate with any MPI-1 implementation that also has a few functions from the MPI-2 external-interfaces chapter.
Reference: [9] <author> G. Gibson, D. Stodolsky, P. Chang, W. Courtwright II, C. Demetriou, E. Ginting, M. Holland, Q. Ma, L. Neal, R. Pat-terson, J. Su, R. Youssef, and J. Zelenka. </author> <title> The Scotch Parallel Storage Systems. </title> <booktitle> In Proceedings of 40th IEEE Computer Society International Conference (COMPCON 95), </booktitle> <pages> pages 403 410. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> Spring </month> <year> 1995. </year>
Reference-contexts: Client-side caching must be disabled by locking the portion of the file being accessed, by using fcntl. A lock and unlock are therefore needed across the read/write call. 4. Many research file systems provide their own APIs <ref> [9, 3, 11, 15, 20] </ref>. Implementing MPI-IO on top of Unix I/O functions will not be portable to these file systems. An alternative is to implement MPI-IO on top of the POSIX I/O interface [12] instead of the basic Unix I/O functions.
Reference: [10] <author> W. Gropp, E. Lusk, N. Doss, and A. Skjellum. </author> <title> A High-Performance, Portable Implementation of the MPI Message-Passing Interface Standard. </title> <journal> Parallel Computing, </journal> <volume> 22(6):789 828, </volume> <month> September </month> <year> 1996. </year>
Reference-contexts: Such an implementation is described in [8] and also illustrated in Figure 1. A similar abstract-device interface is used in MPICH <ref> [10] </ref> for implementing MPI portably. 3 Implementing MPI-IO We describe how we implemented each feature of MPI-IO on various machines and file systems.
Reference: [11] <author> J. Huber, C. Elford, D. Reed, A. Chien, and D. Blumenthal. </author> <title> PPFS: A High Performance Portable Parallel File System. </title> <booktitle> In Proceedings of the 9th ACM International Conference on Supercomputing, </booktitle> <pages> pages 385394. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1995. </year>
Reference-contexts: Client-side caching must be disabled by locking the portion of the file being accessed, by using fcntl. A lock and unlock are therefore needed across the read/write call. 4. Many research file systems provide their own APIs <ref> [9, 3, 11, 15, 20] </ref>. Implementing MPI-IO on top of Unix I/O functions will not be portable to these file systems. An alternative is to implement MPI-IO on top of the POSIX I/O interface [12] instead of the basic Unix I/O functions.
Reference: [12] <author> IEEE/ANSI Std. 1003.1. </author> <title> Portable Operating System Interface (POSIX)Part 1: System Application Program Interface (API) [C Language], </title> <note> 1996 edition. </note>
Reference-contexts: Many research file systems provide their own APIs [9, 3, 11, 15, 20]. Implementing MPI-IO on top of Unix I/O functions will not be portable to these file systems. An alternative is to implement MPI-IO on top of the POSIX I/O interface <ref> [12] </ref> instead of the basic Unix I/O functions. The POSIX interface is an international standard with greater functionality than basic Unix I/O. For example, POSIX supports asynchronous I/O and list-directed I/O. This approach, however, also has limitations. Although POSIX is a standard, it is not yet widely implemented. <p> Some file systems support the POSIX list-directed I/O function lio listio, which allows users to submit multiple I/O requests at a time. This function also has limitations because of the way it is defined. The POSIX standard <ref> [12] </ref> allows a mixture of read and write requests in the list and says that each of the requests will be submitted as a separate nonblocking (asynchronous) I/O request. Therefore, POSIX implementations cannot optimize I/O for the entire list of requests. <p> MPI-IO's consistency semantics are actually weaker than the consistency semantics in Unix [29] or POSIX <ref> [12] </ref>. In Unix and POSIX, after a write function returns, the data is guaranteed to be visible to every other process in the system.
Reference: [13] <author> T. Jones, R. Mark, J. Martin, J. May, E. Pierce, and L. Stan-berry. </author> <title> An MPI-IO Interface to HPSS. </title> <booktitle> In Proceedings of the Fifth NASA Goddard Conference on Mass Storage Systems, </booktitle> <pages> pages I:3750, </pages> <month> September </month> <year> 1996. </year>
Reference-contexts: MPI-IO is a comprehensive API with many features intended specifically for I/O parallelism, portability, and high performance. Implementations of MPI-IO, both portable and machine-specific, are already available <ref> [7, 13, 23, 24, 34] </ref>. In this paper, we discuss the issues involved in implementing MPI-IO portably on multiple machines and file systems and also achieving high performance. <p> HPSS is different from other file systems in its goals and design features; for example, it supports third-party transfer. A group at Lawrence Liver-more National Laboratory has implemented MPI-IO on HPSS, and we refer interested readers to <ref> [13] </ref> for a discussion of issues related to implementing MPI-IO on HPSS. By making MPI-IO available everywhere and also delivering high performance, we expect that it will be widely used and popular among application programmers.
Reference: [14] <author> D. Kotz. </author> <title> Disk-directed I/O for MIMD Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 15(1):4174, </volume> <month> February </month> <year> 1997. </year>
Reference-contexts: The merged request can therefore be serviced efficiently. Such optimization is broadly referred to as collective I/O. Collective I/O has been shown to be a very important optimization in parallel I/O and can improve performance significantly <ref> [5, 14, 25, 30, 33] </ref>. Since none of the file systems on which ROMIO is implemented perform collective I/O, ROMIO performs two-phase collective I/O on top of the file system. In the communication phase, interpro-cess communication is used to rearrange data into large chunks. <p> Leave Collective I/O to the MPI-IO Implementation. It is not entirely clear whether collective I/O is better if performed in the file system or as a library above the file system. Both techniques have been proposed in the literature <ref> [5, 14, 25] </ref>.
Reference: [15] <author> O. Krieger and M. Stumm. </author> <title> HFS: A Performance-Oriented Flexible File System Based on Building-Block Compositions. </title> <booktitle> In Proceedings of Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 95108. </pages> <publisher> ACM Press, </publisher> <month> May </month> <year> 1996. </year>
Reference-contexts: Client-side caching must be disabled by locking the portion of the file being accessed, by using fcntl. A lock and unlock are therefore needed across the read/write call. 4. Many research file systems provide their own APIs <ref> [9, 3, 11, 15, 20] </ref>. Implementing MPI-IO on top of Unix I/O functions will not be portable to these file systems. An alternative is to implement MPI-IO on top of the POSIX I/O interface [12] instead of the basic Unix I/O functions.
Reference: [16] <author> T. Madhyastha and D. Reed. </author> <title> Intelligent, Adaptive File System Policy Selection. </title> <booktitle> In Proceedings of the Sixth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 172 179. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1996. </year>
Reference-contexts: Variable Caching/Prefetching Policies. Parallel applications exhibit such a wide variation in access patterns that any one caching/prefetching policy is unlikely to perform well for all applications [27]. The file system must therefore either detect and automatically adapt to changing access patterns <ref> [16, 17] </ref> or provide an interface for the user to specify the access pattern or caching/prefetching policy [2, 22]. 10. File Preallocation. It is easy and inexpensive for a file system to provide a function to preallocate disk space for a file.
Reference: [17] <author> T. Madhyastha and D. Reed. </author> <title> Exploiting Global Input/Output Access Pattern Classification. </title> <booktitle> In Proceedings of SC97: High Performance Networking and Computing. </booktitle> <publisher> ACM Press, </publisher> <month> November </month> <year> 1997. </year>
Reference-contexts: Variable Caching/Prefetching Policies. Parallel applications exhibit such a wide variation in access patterns that any one caching/prefetching policy is unlikely to perform well for all applications [27]. The file system must therefore either detect and automatically adapt to changing access patterns <ref> [16, 17] </ref> or provide an interface for the user to specify the access pattern or caching/prefetching policy [2, 22]. 10. File Preallocation. It is easy and inexpensive for a file system to provide a function to preallocate disk space for a file.
Reference: [18] <author> Message Passing Interface Forum. </author> <title> MPI: A Message-Passing Interface Standard. </title> <note> Version 1.1, June 1995. On the World-Wide Web at http://www.mpi-forum.org/docs/docs.html. </note>
Reference-contexts: ADIOADIO MPI-IO Unix NFS SFS SGI XFSPIOFS IBM Intel PFS HFS Portable Implementation network remote site Implementations File-system-specific separately for different file systems. 3.1.1 Open MPI File open is a collective function. One of its arguments is an MPI communicator <ref> [18] </ref> that specifies the group of processes that will call this open function and any other collective MPI-IO function that the user may choose to use thereafter on the open file.
Reference: [19] <author> Message Passing Interface Forum. </author> <title> MPI-2: Extensions to the Message-Passing Interface. </title> <month> July </month> <year> 1997. </year> <note> On the World-Wide Web at http://www.mpi-forum.org/docs/docs.html. </note>
Reference-contexts: To overcome these limitations, the MPI Forum defined a new API for parallel I/O (commonly referred to as MPI-IO) as part of the MPI-2 standard <ref> [19] </ref>. MPI-IO is a comprehensive API with many features intended specifically for I/O parallelism, portability, and high performance. Implementations of MPI-IO, both portable and machine-specific, are already available [7, 13, 23, 24, 34]. <p> The user can specify noncontiguous locations in the file by creating a file view with MPI's derived datatypes <ref> [19] </ref>. Noncontiguous locations in memory can be specified by using a derived datatype in the read/write call. The ability of users to specify noncontiguous accesses in a single function call is very important, because noncontiguous accesses are very common in parallel applications [1, 4, 21, 26, 27, 32]. <p> On machines and file systems that do not support nonblocking I/O, ROMIO just calls the corresponding blocking I/O functions. bandwidth, and the figure on the right shows write bandwidth. 3.6 Consistency Semantics MPI-IO's consistency semantics (Section 9.6 of <ref> [19] </ref>) define the results users can expect with concurrent file accesses from multiple processes. MPI-IO's consistency semantics are actually weaker than the consistency semantics in Unix [29] or POSIX [12].
Reference: [20] <author> N. Nieuwejaar and D. Kotz. </author> <title> The Galley Parallel File System. </title> <booktitle> Parallel Computing, </booktitle> <address> 23(4):447476, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: Client-side caching must be disabled by locking the portion of the file being accessed, by using fcntl. A lock and unlock are therefore needed across the read/write call. 4. Many research file systems provide their own APIs <ref> [9, 3, 11, 15, 20] </ref>. Implementing MPI-IO on top of Unix I/O functions will not be portable to these file systems. An alternative is to implement MPI-IO on top of the POSIX I/O interface [12] instead of the basic Unix I/O functions.
Reference: [21] <author> N. Nieuwejaar, D. Kotz, A. Purakayastha, C. Ellis, and M. </author> <title> Best. File-Access Characteristics of Parallel Scientific Workloads. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 7(10):10751089, </volume> <month> October </month> <year> 1996. </year>
Reference-contexts: Noncontiguous locations in memory can be specified by using a derived datatype in the read/write call. The ability of users to specify noncontiguous accesses in a single function call is very important, because noncontiguous accesses are very common in parallel applications <ref> [1, 4, 21, 26, 27, 32] </ref>. Most file systems, however, do not provide functions for noncontiguous I/O. The Unix functions readv/writev are widely supported, but they allow noncontiguity only in memory and not in the file.
Reference: [22] <author> R. Patterson, G. Gibson, E. Ginting, D. Stodolsky, and J. Ze-lenka. </author> <title> Informed Prefetching and Caching. </title> <booktitle> In Proceedings of the 15th Symposium on Operating System Principles, </booktitle> <pages> pages 7995. </pages> <publisher> ACM Press, </publisher> <month> December </month> <year> 1995. </year>
Reference-contexts: In such cases, the MPI-IO implementation cannot use the nonatomic mode on PFS. 3.8 Hints MPI-IO provides a mechanism for the user to pass hints to the implementation. Hints, such as access-pattern information, can help the implementation optimize file access <ref> [2, 22] </ref>. Hints do not change the semantics of the MPI-IO interface; an implementation may choose to ignore all hints, and the program would still be functionally correct. MPI-IO has some predefined hints for specifying file-striping parameters, access patterns, and so on. An implementation is free to define additional hints. <p> The file system must therefore either detect and automatically adapt to changing access patterns [16, 17] or provide an interface for the user to specify the access pattern or caching/prefetching policy <ref> [2, 22] </ref>. 10. File Preallocation. It is easy and inexpensive for a file system to provide a function to preallocate disk space for a file.
Reference: [23] <author> J. Prost. MPI-IO/PIOFS. </author> <note> World-Wide Web page at http://www.research.ibm.com/people/p/ prost/sections/mpiio.html, </note> <year> 1996. </year>
Reference-contexts: MPI-IO is a comprehensive API with many features intended specifically for I/O parallelism, portability, and high performance. Implementations of MPI-IO, both portable and machine-specific, are already available <ref> [7, 13, 23, 24, 34] </ref>. In this paper, we discuss the issues involved in implementing MPI-IO portably on multiple machines and file systems and also achieving high performance.
Reference: [24] <author> D. Sanders, Y. Park, and M. Brodowicz. </author> <title> Implementation and Performance of MPI-IO File Access Using MPI Datatypes. </title> <type> Technical Report UH-CS-96-12, </type> <institution> University of Houston, </institution> <month> November </month> <year> 1996. </year>
Reference-contexts: MPI-IO is a comprehensive API with many features intended specifically for I/O parallelism, portability, and high performance. Implementations of MPI-IO, both portable and machine-specific, are already available <ref> [7, 13, 23, 24, 34] </ref>. In this paper, we discuss the issues involved in implementing MPI-IO portably on multiple machines and file systems and also achieving high performance.
Reference: [25] <author> K. Seamons, Y. Chen, P. Jones, J. Jozwiak, and M. Winslett. </author> <title> Server-Directed Collective I/O in Panda. </title> <booktitle> In Proceedings of Supercomputing '95. </booktitle> <publisher> ACM Press, </publisher> <month> December </month> <year> 1995. </year>
Reference-contexts: The merged request can therefore be serviced efficiently. Such optimization is broadly referred to as collective I/O. Collective I/O has been shown to be a very important optimization in parallel I/O and can improve performance significantly <ref> [5, 14, 25, 30, 33] </ref>. Since none of the file systems on which ROMIO is implemented perform collective I/O, ROMIO performs two-phase collective I/O on top of the file system. In the communication phase, interpro-cess communication is used to rearrange data into large chunks. <p> Leave Collective I/O to the MPI-IO Implementation. It is not entirely clear whether collective I/O is better if performed in the file system or as a library above the file system. Both techniques have been proposed in the literature <ref> [5, 14, 25] </ref>.
Reference: [26] <author> E. Smirni, R. Aydt, A. Chien, and D. Reed. </author> <title> I/O Requirements of Scientific Applications: An Evolutionary View. </title> <booktitle> In Proceedings of the Fifth IEEE International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 4959. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1996. </year>
Reference-contexts: Noncontiguous locations in memory can be specified by using a derived datatype in the read/write call. The ability of users to specify noncontiguous accesses in a single function call is very important, because noncontiguous accesses are very common in parallel applications <ref> [1, 4, 21, 26, 27, 32] </ref>. Most file systems, however, do not provide functions for noncontiguous I/O. The Unix functions readv/writev are widely supported, but they allow noncontiguity only in memory and not in the file.
Reference: [27] <author> E. Smirni and D. Reed. </author> <title> Lessons from Characterizing the Input/Output Behavior of Parallel Scientific Applications. Performance Evaluation: </title> <note> An International Journal, 33(1):2744, </note> <month> June </month> <year> 1998. </year>
Reference-contexts: Noncontiguous locations in memory can be specified by using a derived datatype in the read/write call. The ability of users to specify noncontiguous accesses in a single function call is very important, because noncontiguous accesses are very common in parallel applications <ref> [1, 4, 21, 26, 27, 32] </ref>. Most file systems, however, do not provide functions for noncontiguous I/O. The Unix functions readv/writev are widely supported, but they allow noncontiguity only in memory and not in the file. <p> Variable Caching/Prefetching Policies. Parallel applications exhibit such a wide variation in access patterns that any one caching/prefetching policy is unlikely to perform well for all applications <ref> [27] </ref>. The file system must therefore either detect and automatically adapt to changing access patterns [16, 17] or provide an interface for the user to specify the access pattern or caching/prefetching policy [2, 22]. 10. File Preallocation.
Reference: [28] <author> H. Stern. </author> <title> Managing NFS and NIS. </title> <publisher> O'Reilly & Associates, Inc., </publisher> <year> 1991. </year>
Reference-contexts: When using the Network File System (NFS), it is not sufficient to call just the Unix read/write functions. Since NFS performs noncoherent client-side caching by default, file consistency is not guaranteed if multiple processes write to a common file <ref> [28] </ref>. Client-side caching must be disabled by locking the portion of the file being accessed, by using fcntl. A lock and unlock are therefore needed across the read/write call. 4. Many research file systems provide their own APIs [9, 3, 11, 15, 20]. <p> For any other case, the data is visible to another process only after both the writer and reader call MPI File sync. MPI-IO's consistency semantics are therefore automatically guaranteed on file systems that support Unix consistency semantics. 4 NFS, by default, does not <ref> [28] </ref>. To obtain Unix consistency semantics on NFS, ROMIO uses byte-range locking (fcntl) across the reads and writes in order to turn off the noncoherent client-side caching that NFS otherwise performs. Turning off client-side caching reduces performance considerably but is, nonetheless, necessary for correctness.
Reference: [29] <author> W. Stevens. </author> <title> Advanced Programming in the UNIX Environment. </title> <publisher> Addison-Wesley Publishing Company, Inc., </publisher> <year> 1992. </year>
Reference-contexts: We provide a list of features desired from a file system that would help in implementing MPI-IO correctly and with high performance. 2 Achieving Portability and Performance The basic Unix I/O functions (open, lseek, read, write, and close) <ref> [29] </ref> are supported without variation on all machines with a Unix-like operating system. One way to implement MPI-IO portably, therefore, is to implement MPI-IO functions on top of these basic Unix I/O functions. <p> MPI-IO's consistency semantics are actually weaker than the consistency semantics in Unix <ref> [29] </ref> or POSIX [12]. In Unix and POSIX, after a write function returns, the data is guaranteed to be visible to every other process in the system.
Reference: [30] <author> R. Thakur and A. Choudhary. </author> <title> An Extended Two-Phase Method for Accessing Sections of Out-of-Core Arrays. </title> <booktitle> Scientific Programming, </booktitle> <address> 5(4):301317, </address> <month> Winter </month> <year> 1996. </year>
Reference-contexts: The merged request can therefore be serviced efficiently. Such optimization is broadly referred to as collective I/O. Collective I/O has been shown to be a very important optimization in parallel I/O and can improve performance significantly <ref> [5, 14, 25, 30, 33] </ref>. Since none of the file systems on which ROMIO is implemented perform collective I/O, ROMIO performs two-phase collective I/O on top of the file system. In the communication phase, interpro-cess communication is used to rearrange data into large chunks.
Reference: [31] <author> R. Thakur, W. Gropp, and E. Lusk. </author> <title> An Abstract-Device Interface for Implementing Portable Parallel-I/O Interfaces. </title> <booktitle> In Proceedings of the 6th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 180187. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1996. </year>
Reference-contexts: We describe such an architecture, called ADIO, which we use in our MPI-IO implementation, ROMIO [34]. 2.1 Abstract-Device Interface for I/O ADIO <ref> [31] </ref>, an abstract-device interface for I/O, is a mechanism specifically designed for implementing parallel-I/O APIs portably on multiple file systems. We developed ADIO before MPI-IO became a standard, as a means to implement and experiment with various parallel-I/O APIs that existed at the time. <p> We used ADIO to implement Intel's PFS API and subsets of IBM's PIOFS API and the original MPI-IO proposal [36] on multiple file systems. By following such an approach, we achieved portability with very low overhead <ref> [31] </ref>. Now that MPI-IO has emerged as the standard, we use ADIO as a mechanism for implementing MPI-IO portably (see Figure 1). This MPI-IO implementation is called ROMIO [34].
Reference: [32] <author> R. Thakur, W. Gropp, and E. Lusk. </author> <title> An Experimental Evaluation of the Parallel I/O Systems of the IBM SP and Intel Paragon Using a Production Application. </title> <booktitle> In Proceedings of the 3rd International Conference of the Austrian Center for Parallel Computation (ACPC) with Special Emphasis on Parallel Databases and Parallel I/O, pages 2435. Lecture Notes in Computer Science 1127. </booktitle> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1996. </year>
Reference-contexts: Noncontiguous locations in memory can be specified by using a derived datatype in the read/write call. The ability of users to specify noncontiguous accesses in a single function call is very important, because noncontiguous accesses are very common in parallel applications <ref> [1, 4, 21, 26, 27, 32] </ref>. Most file systems, however, do not provide functions for noncontiguous I/O. The Unix functions readv/writev are widely supported, but they allow noncontiguity only in memory and not in the file.
Reference: [33] <author> R. Thakur, W. Gropp, and E. Lusk. </author> <title> Data Sieving and Collec--tive I/O in ROMIO. </title> <booktitle> In Proceedings of the 7th Symposium on the Frontiers of Massively Parallel Computation. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> February </month> <year> 1999. </year>
Reference-contexts: Such an implementation, however, results in a large number of small requests to the file system, and performance degrades drastically <ref> [33] </ref>. ROMIO instead performs an optimization, called data sieving, to access noncontiguous data with high performance. The basic idea in data sieving is to make large I/O requests to the file system and extract, in memory, the data that is really needed. Details of this optimization can be found in [33]. <p> <ref> [33] </ref>. ROMIO instead performs an optimization, called data sieving, to access noncontiguous data with high performance. The basic idea in data sieving is to make large I/O requests to the file system and extract, in memory, the data that is really needed. Details of this optimization can be found in [33]. 3.3 Collective I/O MPI-IO provides collective-I/O functions, which must be called by all processes that together opened the file. 3 This property enables the MPI-IO implementation (or file system) to analyze and merge the requests of different processes. <p> The merged request can therefore be serviced efficiently. Such optimization is broadly referred to as collective I/O. Collective I/O has been shown to be a very important optimization in parallel I/O and can improve performance significantly <ref> [5, 14, 25, 30, 33] </ref>. Since none of the file systems on which ROMIO is implemented perform collective I/O, ROMIO performs two-phase collective I/O on top of the file system. In the communication phase, interpro-cess communication is used to rearrange data into large chunks. <p> The communicator could represent any subset (or all) of the processes of the application. should actually perform I/O in the I/O phase of the two-phase operation. Details of ROMIO's collective-I/O implementation can be found in <ref> [33] </ref>. template, DIST3D, when I/O is performed in three ways: using Unix-style independent I/O, data sieving, and collective I/O. This application accesses a three-dimensional distributed array of size 512 fi 512 fi 512 from a file. <p> On some machines data sieving performed only slightly better than Unix-style independent I/O; on others it performed considerably better. Collective I/O always performed the best and resulted in I/O bandwidths ranging from 51 Mbytes/sec to 563 Mbytes/sec, depending on the machine. For detailed performance results, see <ref> [33] </ref>. 3.4 Split Collective I/O MPI-IO provides a restricted form of nonblocking collective I/O called split collective I/O. The user can call a begin function to start the collective-I/O operation and an end function to complete the operation. <p> This approach keeps the file-system code simpler and, as ROMIO demon strates <ref> [33] </ref>, can also deliver high performance. 12. No shared file pointers.
Reference: [34] <author> R. Thakur, E. Lusk, and W. Gropp. </author> <title> Users Guide for ROMIO: A High-Performance, Portable MPI-IO Implementation. </title> <type> Technical Report ANL/MCS-TM-234, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <note> Revised July 1998. </note>
Reference-contexts: MPI-IO is a comprehensive API with many features intended specifically for I/O parallelism, portability, and high performance. Implementations of MPI-IO, both portable and machine-specific, are already available <ref> [7, 13, 23, 24, 34] </ref>. In this paper, we discuss the issues involved in implementing MPI-IO portably on multiple machines and file systems and also achieving high performance. <p> We believe that the only way to implement MPI-IO portably with complete functionality and high performance is to have a mechanism that can utilize the special features and functions of each file system. We describe such an architecture, called ADIO, which we use in our MPI-IO implementation, ROMIO <ref> [34] </ref>. 2.1 Abstract-Device Interface for I/O ADIO [31], an abstract-device interface for I/O, is a mechanism specifically designed for implementing parallel-I/O APIs portably on multiple file systems. <p> By following such an approach, we achieved portability with very low overhead [31]. Now that MPI-IO has emerged as the standard, we use ADIO as a mechanism for implementing MPI-IO portably (see Figure 1). This MPI-IO implementation is called ROMIO <ref> [34] </ref>. ROMIO runs on the following machines: IBM SP; Intel Paragon; Cray T3E; HP Exemplar; SGI Origin2000; NEC SX-4; other symmetric multiprocessors from HP, SGI, Sun, DEC, and IBM; and networks of workstations (Sun, SGI, HP, IBM, DEC, Linux, and FreeBSD).
Reference: [35] <author> R. Thakur, E. Lusk, and W. Gropp. </author> <title> I/O in Parallel Applications: The Weakest Link. </title> <journal> International Journal of High Performance Computing Applications, </journal> <volume> 12(4):389395, </volume> <month> Winter </month> <year> 1998. </year>
Reference-contexts: Furthermore, the Unix API is not an appropriate API for parallel I/O: it lacks some of the features necessary to express access patterns common in parallel programs, such as noncontiguous accesses and collective I/O, resulting in poor performance <ref> [35] </ref>. To overcome these limitations, the MPI Forum defined a new API for parallel I/O (commonly referred to as MPI-IO) as part of the MPI-2 standard [19]. MPI-IO is a comprehensive API with many features intended specifically for I/O parallelism, portability, and high performance.
Reference: [36] <author> The MPI-IO Committee. </author> <title> MPI-IO: A Parallel File I/O Interface for MPI, </title> <note> Version 0.5. On the World-Wide Web at http://parallel.nas.nasa.gov/MPI-IO, April 1996. </note>
Reference-contexts: ADIO thus separates the machine-dependent and machine-independent aspects involved in implementing an API. The ADIO implementation on a particular file system is optimized for that file system. We used ADIO to implement Intel's PFS API and subsets of IBM's PIOFS API and the original MPI-IO proposal <ref> [36] </ref> on multiple file systems. By following such an approach, we achieved portability with very low overhead [31]. Now that MPI-IO has emerged as the standard, we use ADIO as a mechanism for implementing MPI-IO portably (see Figure 1). This MPI-IO implementation is called ROMIO [34].
Reference: [37] <author> R. Watson and R. Coyne. </author> <title> The Parallel I/O Architecture of the High-Performance Storage System (HPSS). </title> <booktitle> In Proceedings of the Fourteenth IEEE Symposium on Mass Storage Systems, </booktitle> <pages> pages 2744. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> September </month> <year> 1995. </year>
Reference-contexts: The discussion in this paper covers numerous file systems almost all the file systems on commercially available machines. An important storage system that we did not discuss (mainly because ROMIO is not implemented on it) is HPSS <ref> [37] </ref>. HPSS is different from other file systems in its goals and design features; for example, it supports third-party transfer. A group at Lawrence Liver-more National Laboratory has implemented MPI-IO on HPSS, and we refer interested readers to [13] for a discussion of issues related to implementing MPI-IO on HPSS.
References-found: 37

