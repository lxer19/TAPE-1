URL: http://www.cs.colorado.edu/~suvas/papers/IPPS-94-long-barrier.ps
Refering-URL: http://www.cs.colorado.edu/~suvas/Papers.html
Root-URL: http://www.cs.colorado.edu
Email: (Email:fgrunwald,suvasg@cs.colorado.edu)  
Phone: 430,  
Title: Efficient Barriers for Distributed Shared Memory Computers  
Author: Dirk Grunwald Suvas Vajracharya 
Date: Sept 1993  
Address: Campus Box  Boulder, CO 80309-0430  Boulder  
Affiliation: Department of Computer Science,  University of Colorado,  ffi University of Colorado at  
Pubnum: CU-CS-703-94-93  
Abstract: Technical Report CU-CS-703-94-93 Department of Computer Science Campus Box 430 University of Colorado Boulder, Colorado 80309 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T.E. Anderson. </author> <title> The performance of spinlock alternatives for shared-memory multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: The hotspot problem can be somewhat alleviated in cache coherent machines because a copy of the counter is copied to local memory and updated via broadcasts; this reduces the amount of network communication. Various centralized 2 barrier and lock designs make effective use of local caches <ref> [1] </ref>. However, processors writeing to the barrier contend between themselves, and arrival still requires O (N) time. 2.2 Software Combining Trees Yew et al. [11] proposed a combining tree barrier to reduce the occurence of hot spots. A software combining tree spreads the congestion over a tree of variables.
Reference: [2] <author> Seif Haridi Erik Hagersten and David H.D. Warren. </author> <title> Cache and interconnect architectures in multiprocessors. The cache-coherence protocol of the data diffusion machine, </title> <year> 1990. </year>
Reference: [3] <author> Allan Gottlieb, Ralph Grishman, Clyde P. Kruskal, Kevin P. McAuliffe, Larray Rudolph, and Marc Snir. </author> <title> The nyu ultracomputer: Designing a mimd, shared-memory parallel machine. </title> <booktitle> Proceedings of 9th Annual International Symposium on Computer, </booktitle> <volume> 10(3) </volume> <pages> 27-42, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: Once the rendezvous has been achieved, the counter is reset to zero for the next rendezvous. There are two disadvantages to this approach. First, the counter must be updated atomically, either via explicit locking or hardware operations such as fetch and <ref> [3] </ref>. Second, all processes must contend with each other to read and write a single memory location. As mentioned, this causes hot-spots, or points of high traffic congestion. Consequently, this barrier is not scalable since each read and a write involves serialized actions.
Reference: [4] <author> D. Hensgen, R. Finkel, and U. Manber. </author> <title> Two algorithms for barrier synchronization. </title> <journal> Intl. Journal of Parallel Programming, </journal> <volume> 17(1), </volume> <year> 1988. </year>
Reference-contexts: To alleviate these problems, Brooks described the butterfly barrier [6]; this communication pattern is similar to the exchange-swap operation used in hyper-cube interconnection networks. Hensgen et al <ref> [4] </ref> improved the butterfly-network for situations where the number of processors accessing the barrier are not a power of two. Their dissemination barrier uses the communication structure shown in Figure 1.
Reference: [5] <author> W. Daniel Hillis and G.L. Steele. </author> <title> Data parallel algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 29, No.12:1170-1183, </volume> <month> December </month> <year> 1986. </year>
Reference-contexts: 1 Introduction Barriers are a synchronization tool for parallel computers, including shared and distributed (message-passing) address-space architectures. No processor may pass the barrier until all processes have arrived at the barrier; this synchronization tool is used in many algorithms, and is central to the data-parallel programming model <ref> [5] </ref>. There are numerous barrier algorithms for message-passing and shared-address space computers. Some architectures, such as the Thinking Machines CM-5, provide special hardware support for barrier synchronization. On architectures lacking such hardware support, scalable barriers must be implemented in software, using the underlying communication network.
Reference: [6] <author> Edward D. Brooks III. </author> <title> The butterfly barrier. </title> <journal> Intl. Journal of Parallel Programming, </journal> <volume> 15(4) </volume> <pages> 295-307, </pages> <year> 1986. </year>
Reference-contexts: Second, both methods spin at a remote memory location, leading to unnecessary contention for the interconnection bandwidth on machines that are not broadcast-based and lack cache-coherency. To alleviate these problems, Brooks described the butterfly barrier <ref> [6] </ref>; this communication pattern is similar to the exchange-swap operation used in hyper-cube interconnection networks. Hensgen et al [4] improved the butterfly-network for situations where the number of processors accessing the barrier are not a power of two. Their dissemination barrier uses the communication structure shown in Figure 1.
Reference: [7] <author> Boris D. Lubachecsky. </author> <title> Synchronization barrier and relation tools for shared memory parallel programs. </title> <booktitle> In Proc. of the 1989 Int. Conf. on Parallel Processing, pages II-175-II-179. </booktitle> <institution> Penn State, </institution> <year> 1989. </year>
Reference-contexts: We believe the communication structure of a barrier algorithm must match the physical interconnection network for best performance. 4 2.4 Tournament Algorithm To counter the additional communication in the dissemination algorithm, Hensgen et al also developed the tournament algorithm, apparently at the suggestion of Lubachevsky <ref> [7] </ref>. As with the dissemination algorithm, the tournament algorithm avoids special hardware by using a predetermined communication structure; however, the tournament algorithm incurs only O (log 2 P ) total network transactions, as illustrated by the communication structure shown in Figure 1.
Reference: [8] <author> John Mellor-Crummey and Michael Scott. </author> <title> Algorithms for scalable synchronization on shared memory multiprocessors. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: In this paper, we describe several existing barrier implementations, propose two new barrier al-gorithm and describe the performance of each algorithm on a Cache Only Memory Access (COMA) architecture, the KSR-1, and a shared-bus cache-consistent architecture (the Sequent Symmetry). We show that conclusions drawn in previous studies of barriers <ref> [8] </ref> are not necessarily true on COMA architectures, highlighting the need for a formal model to predict the performance of barrier algorithms. 2 Previous Barrier Algorithms As computer architectures have evolved, numerous barrier algorithms have been proposed. A central barrier suffices for small-scale cache-coherent multiprocessors with 2-10 processors. <p> Processors that are not group leaders busy-wait, awaiting wakeup notification of the barrier completion. A number of methods can be used for wakeup notification. In broadcast cache-coherent architectures (such as the KSR or Sequent), a central busy-flag can be used <ref> [8] </ref>. On architectures where non-local memory references must use the network, a tree-based wake-up algorithm is used. The counters at each node are reset when the barrier is reinitialized. <p> During each synchronization `round', each processor either waits on a locally available flag or signals the partner via writing the flag. The flags are reinitialized by sense-reversing the flag after each round. In a previous study <ref> [8] </ref>, the dissemination algorithm achieved the best performance of a variety of barrier algorithms on non-uniform memory shared-address machines without cache-coherency and broadcasting, such as BBN Butterfly. On such architectures, the `local spinning' provides an advantage for this algorithm. <p> Wakeup notification is done using either a global wakeup flag for broadcast-based machines or a tree-wakeup algorithm. Reinitialization is done by sense reversing the flags. 2.5 MCS-Tree Algorithm Mellor-Crummey et al <ref> [8] </ref> proposed a variation on the tournament algorithm called the MCS-Tree barrier. Figure 2 depicts the communication structure for the MCS-Tree barrier algorithm, and (parents) and writers (children) are statically pre-determined during barrier creation. <p> For all the tree-based algorithms, we used a central global flag that is broadcasted to the waiting processes during the wakeup phase of the algorithms. This method was shown to be more appropriate in a previous study <ref> [8] </ref> for cache-coherent machines that provide fast broadcast operations. On the KSR, we implemented the broadcast using the _pstsp (post-store subpage) instruction. In theory, we can use this instruction to update the contents of the modified word in a single tour of the network. <p> Note that the dissemination barrier suffers a sharp increase in the time to rendezvous between 32 and 33 processors; this is when inter-cluster communication occurs in the KSR-1 interconnection network. Previous studies <ref> [8] </ref> have shown that the MCS algorithm was faster than other algorithms such as the tournament algorithm. However, there is a greater degree of parallelism in the tournament algorithm, and that parallelism can be supported on the KSR-1. This was also recently noted by Ramachandran et al [10].
Reference: [9] <author> G. Pfister and V. Norton. </author> <title> Hot spot contention and combining in multistage interconnection networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-34(10):943-948, </volume> <month> October </month> <year> 1985. </year>
Reference-contexts: A central barrier suffices for small-scale cache-coherent multiprocessors with 2-10 processors. As the promise of larger-scale systems including tens and hundreds of processors was realized, it was noted that centralized barriers limited system performance. Concentrated communication to a single memory location can induce a hot spot <ref> [9] </ref> in the network. Both hardware solutions, including combining networks, and software solutions [11] have been proposed to alleviate hotspots. Several barrier algorithms have been described that distribute the communication, either over different cache locations, network connections or processor clusters.
Reference: [10] <author> Umakishore Ramachandran, Gautam Shah, S. Ravikumar, and Jeyakumar Muthuku-marasamy. </author> <title> Scalability study of the ksr-1. </title> <type> GIT-CC 93/03, </type> <institution> Georgia Inst. of Technology, </institution> <year> 1993. </year>
Reference-contexts: Previous studies [8] have shown that the MCS algorithm was faster than other algorithms such as the tournament algorithm. However, there is a greater degree of parallelism in the tournament algorithm, and that parallelism can be supported on the KSR-1. This was also recently noted by Ramachandran et al <ref> [10] </ref>. Furthermore, the tournament algorithm involves less inter-cluster communication, which is very important on the KSR-1 intra-cluster memory references take 150 machine cycles, while inter-cluster references take 600 cycles.

References-found: 10

