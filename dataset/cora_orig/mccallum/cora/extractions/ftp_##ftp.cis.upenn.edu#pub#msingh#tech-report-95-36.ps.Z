URL: ftp://ftp.cis.upenn.edu/pub/msingh/tech-report-95-36.ps.Z
Refering-URL: http://www.cis.upenn.edu/~msingh/frames/papers_list.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> S.K. Andersen, K.G. Olesen, F.V. Jensen, and F. Jensen. </author> <title> HUGIN|a Shell for Building Belief Universes for Expert Systems. </title> <booktitle> In Proc.IJCAI, </booktitle> <pages> pages 1080-1085, </pages> <year> 1989. </year>
Reference-contexts: We performed inference on the networks using the Lauritzen-Spiegelhalter inference algorithm as implemented in the HUGIN <ref> [1] </ref> system. 5.2 Results The average predictive accuracies of the networks generated by the various methods are shown in Tables 2 and 3. Each result describes an average predictive accuracy on the test set for the 30 trials, followed by the sample standard deviation.
Reference: [2] <author> W.L. Buntine and T. Niblett. </author> <title> A further comparison of splitting rules for decision-tree induction. </title> <journal> Machine Learning, </journal> <volume> 7, </volume> <year> 1992. </year>
Reference-contexts: In this paper we use information to guide the search process in order to learn better, or at least as good, selective Bayesian network classifiers in a more efficient way. Information based metrics have been widely used within the Machine Learning community to learn highly accurate decision trees <ref> [17, 2, 11] </ref>. Induction of a decision tree from a training set of cases is normally handled by a recursive partitioning algorithm which, at each non-terminal node in the tree, branches on that attribute which discriminates best between the cases filtered down to that node. <p> The voting1 database was derived from the voting database by deleting the most significant attribute physician-fee-freeze <ref> [2] </ref>. 5.1 Experimental Design The experimental method, applied to each data set, is described below: 1.
Reference: [3] <author> G.F. Cooper and E. Herskovits. </author> <title> A Bayesian Method for the Induction of of Probabilistic Networks from Data. </title> <booktitle> In Machine Learning 9, </booktitle> <pages> pages 54-62, </pages> <publisher> Kluwer, </publisher> <year> 1992. </year>
Reference-contexts: Attributes are added until the addition of any other attribute results in reduced accuracy. 3 Bayesian Network Classifiers Cooper and Herskovits <ref> [3] </ref> proposed a Bayesian method for the induction of Bayesian networks from data. <p> In the worst case, it selects all the attributes and hence stops when jj = n 2. Thus, the worst case time complexity of the node-selection phase is given by 4 Cooper and Herskovits <ref> [3] </ref> used a similar technique to calculate P (B S ; D) efficiently. 9 n2 X (mkr + mr 2 ) (n k 1) = O (mrn 2 (r + n)): 5 Experimental Comparison of Info-AS with other induction methods In our experiments we used a variety of databases acquired from
Reference: [4] <author> Cowell, R.G., Dawid, P. and Spiegelhalter, D.J. </author> <year> (1993). </year> <title> Sequential model criticism in probabilistic expert systems. </title> <journal> IEEE Transactions of Pattern Analysis and Machine Intelligence, </journal> <volume> 15(3), </volume> <pages> 209-219. </pages>
Reference-contexts: Note that even though a given model may be "correct" in the sense of generating the data, it need not be the best model when it comes to making predictions <ref> [4] </ref>. However, as we shall see in section 5, it does suffice in showing that feature selection is a useful tool in learning BNs that display good classification accuracy while also improving the inference efficiency of the resulting networks. <p> Firstly, as pointed out earlier, the CB algorithm may not be the best algorithm since we are interested in maximizing predictive accuracy and not just fitting the data on hand. Cowell et. al. <ref> [4] </ref> describe "global monitors" to measure the predictive quality of a Bayesian network. We intend to extend that approach to learn Bayesian networks so as to maximize their predictive accuracy. Another interesting issue stems from what has been known as the `curse of dimensionality'.
Reference: [5] <author> Heckerman, D., Geiger, D. and Chickering, M. </author> <year> (1994). </year> <title> Learning Bayesian networks: the combination of knowledge and statistical data. </title> <type> Technical report MSR-TR-94-09, </type> <institution> Microsoft research, </institution> <address> Redmond, WA. </address>
Reference-contexts: We must emphasize here that our choice of CB as the learning algorithm was totally arbitrary. One could have chosen any other methods for learning BN's just as well, e.g. Heckerman et. al. <ref> [5] </ref>. The main contribution of this paper lies in showing the feasibility, advantages and effectiveness of learning Bayesian networks using feature selection, and presenting an efficient means of doing so.
Reference: [6] <author> E. Herskovits. </author> <title> Computer-based probabilistic-network construction. </title> <type> Doctoral dissertation, </type> <institution> Medical Information Sciences, Stanford University, Stanford, </institution> <address> CA., </address> <year> 1991 </year>
Reference: [7] <author> E. Herskovits and G.F. Cooper. KUTATO: </author> <title> An Entropy-Driven System for Construction of Probabilistic Expert Systems from Databases. </title> <booktitle> In Proc. Conf. Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 54-62, </pages> <year> 1990. </year>
Reference: [8] <author> I. Kononenko. </author> <title> Comparison of Inductive and noise Bayesian Learning Approaches to automatic knowledge acquisition. </title> <editor> In B. Wielinga et al. </editor> <booktitle> Current Trends in Knowledge Acquisition, </booktitle> <address> Amsterdam, </address> <publisher> IOS Press, </publisher> <year> 1990. </year>
Reference-contexts: 1 Introduction One of the simplest Bayesian induction methods is the naive Bayesian classifier <ref> [8, 9] </ref>. Despite its simplicity and the strong assumption that the attributes are conditionally independent given the class variable, the naive Bayesian classifier has been shown to perform remarkably well in some domains. The simplest naive classifier is one which consists of all attributes (non-selective naive Bayesian classifier).
Reference: [9] <author> P. Langley. </author> <title> Induction of Recursive Bayesian Classifiers In Proc. </title> <booktitle> European Conf. on Machine Learning, </booktitle> <pages> pages 153-164. </pages> <publisher> Springer Verlag, </publisher> <year> 1993. </year> <month> 22 </month>
Reference-contexts: 1 Introduction One of the simplest Bayesian induction methods is the naive Bayesian classifier <ref> [8, 9] </ref>. Despite its simplicity and the strong assumption that the attributes are conditionally independent given the class variable, the naive Bayesian classifier has been shown to perform remarkably well in some domains. The simplest naive classifier is one which consists of all attributes (non-selective naive Bayesian classifier). <p> We refer to this naive Bayesian classifier (that models all attributes) as naive-ALL. Although, its performance is remarkably good given its simplicity, it is typically limited to learning classes that can be separated by a single decision boundary <ref> [9] </ref>, and in domains in which the attributes are correlated given the class variable, its performance can be worse than other approaches which can account for such correlations. Bayesian networks can account for correlations among attributes, so they are a natural extension of the naive approach.
Reference: [10] <author> P. Langley and S. Sage. </author> <title> Induction of selective Bayesian classifiers. </title> <booktitle> In Proc. Conf. on Uncertainty in AI, </booktitle> <pages> pages 399-406. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: The simplest naive classifier is one which consists of all attributes (non-selective naive Bayesian classifier). However, due to its strong independence assumptions, the classification performance of such classifiers can degrade immensely in domains which contains attributes that are correlated. Langley and Sage <ref> [10] </ref> proposed a method to eliminate this drawback by using only a subset of the attributes to construct the classifier (selective naive Bayesian classifier) in an attempt to exclude highly correlated attributes, thereby improving performance in domains with many dependencies between attributes. <p> Bayesian networks can account for correlations among attributes, so they are a natural extension of the naive approach. The selective naive Bayesian classifier <ref> [10] </ref> (referred to as naive-AS) is an extension to the naive Bayesian classifier, and is designed to perform better in domains with redundant attributes. The intuition is that if highly correlated attributes are not selected, the classifier should perform better given its attribute independence assumptions.
Reference: [11] <author> R. Lopez de Mantaras. </author> <title> A distance-based attribute selection measure for decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 81-92, </pages> <year> 1991. </year>
Reference-contexts: In this paper we use information to guide the search process in order to learn better, or at least as good, selective Bayesian network classifiers in a more efficient way. Information based metrics have been widely used within the Machine Learning community to learn highly accurate decision trees <ref> [17, 2, 11] </ref>. Induction of a decision tree from a training set of cases is normally handled by a recursive partitioning algorithm which, at each non-terminal node in the tree, branches on that attribute which discriminates best between the cases filtered down to that node. <p> A number of information-based splitting rules (that decide which of the available attributes is the "best" attribute to branch on) have been studied by various researchers. Amongst these are Quinlan's information gain and gain ratio [17] and Lopez de Mantaras's distance measure (d N ) <ref> [11] </ref>. In the case of information gain and gain ratio that attribute is selected (from the available attributes) which maximizes this measure. The distance measure on the other hand needs to be minimized over the attributes; hence, one often uses the complement of this measure, 1 d N .
Reference: [12] <author> P.M. Murphy and D.W. Aha. </author> <title> UCI Repository of Machine Learning Databases. Machine-readable data repository, </title> <institution> Dept. of Information and Computer Science, Univ. of California, Irvine. </institution>
Reference-contexts: D) efficiently. 9 n2 X (mkr + mr 2 ) (n k 1) = O (mrn 2 (r + n)): 5 Experimental Comparison of Info-AS with other induction methods In our experiments we used a variety of databases acquired from the University of Cali-fornia, Irvine Repository of Machine Learning databases <ref> [12] </ref>. The databases we used were Michalski's Soybean database, Schlimmer's Mushroom and Voting databases, the Gene-Splicing database due to Towell, Noordewier and Shavlik and Shapiro's Chess Endgame database. Further information on these databases can be obtained from the UCI repository by anonymous ftp to ics.uci.edu.
Reference: [13] <author> M. Pazzani. </author> <title> Don't be so naive: Detecting dependencies when learning Bayesian Classifiers. </title> <note> Submitted for publication. </note>
Reference-contexts: Langley and Sage [10] proposed a method to eliminate this drawback by using only a subset of the attributes to construct the classifier (selective naive Bayesian classifier) in an attempt to exclude highly correlated attributes, thereby improving performance in domains with many dependencies between attributes. Pazzani <ref> [13] </ref> attempted to deal with this problem in a different way. His `backward sequential elimination and joining algorithm' attempts to take into account the dependencies between various attributes by merging them into a single attribute. <p> One way to avoid this problem, and to improve the quality of the generated classifier would be to use a statistical test (e.g. 2 ) as a stopping criterion. Another very interesting issue is that of `joining' of nodes. As pointed out earlier, Pazzani <ref> [13] </ref> has achieved a lot of success in improving the accuracy of naive Bayesian classifiers by joining attributes to take into account the dependencies between them.
Reference: [14] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: This approach does not deal with the question of independence directly; rather, it tries to determine whether joining attributes will improve the predictive accuracy. We directly model dependencies between the various attributes using Bayesian networks <ref> [14] </ref>, which are a natural extension of naive Bayesian classifiers. However, inference using Bayesian networks is an N P-hard problem. Hence, learning Bayesian network classifiers from a subset of the attributes may be a good compromise between naive Bayesian classifiers and Bayesian networks that model all attributes.
Reference: [15] <author> G. Provan and M. Singh. </author> <title> Learning Bayesian Networks using Feature Selection. </title> <note> To appear in Lecture Notes in Statistics, </note> <editor> (ed. Fisher, D. and Lenz, H.), </editor> <publisher> Springer Verlag, </publisher> <address> New York. </address>
Reference-contexts: Moreover, K2-AS, by selecting a subset of attributes prior to learning the networks, not only significantly improves the inference efficiency of the resulting networks, but also achieves a predictive accuracy comparable to Bayesian networks learned using the full set of attributes <ref> [15, 16] </ref>. However, one of the main drawbacks of this approach is that the subset learning phase is computationally intensive. <p> It keeps constructing networks for increasing orders of CI tests as long as the predictive accuracy of the resultant network keeps increasing. Singh and Provan <ref> [18, 15] </ref> proposed an attribute selection based enhancement of this approach called K2-AS. <p> Similar results were obtained with the K2-AS algorithm <ref> [15, 16] </ref>. However, the Info-AS algorithm has a polynomial subset-selection phase as compared to the exponential time complexity of the corresponding phase in K2-AS.
Reference: [16] <author> G. Provan and M. Singh. </author> <title> Learning Bayesian Networks using Feature Selection. </title> <booktitle> In Working Notes Fifth International Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> pages 450-456, </pages> <year> 1995. </year>
Reference-contexts: Moreover, K2-AS, by selecting a subset of attributes prior to learning the networks, not only significantly improves the inference efficiency of the resulting networks, but also achieves a predictive accuracy comparable to Bayesian networks learned using the full set of attributes <ref> [15, 16] </ref>. However, one of the main drawbacks of this approach is that the subset learning phase is computationally intensive. <p> Similar results were obtained with the K2-AS algorithm <ref> [15, 16] </ref>. However, the Info-AS algorithm has a polynomial subset-selection phase as compared to the exponential time complexity of the corresponding phase in K2-AS.
Reference: [17] <author> J. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: In this paper we use information to guide the search process in order to learn better, or at least as good, selective Bayesian network classifiers in a more efficient way. Information based metrics have been widely used within the Machine Learning community to learn highly accurate decision trees <ref> [17, 2, 11] </ref>. Induction of a decision tree from a training set of cases is normally handled by a recursive partitioning algorithm which, at each non-terminal node in the tree, branches on that attribute which discriminates best between the cases filtered down to that node. <p> A number of information-based splitting rules (that decide which of the available attributes is the "best" attribute to branch on) have been studied by various researchers. Amongst these are Quinlan's information gain and gain ratio <ref> [17] </ref> and Lopez de Mantaras's distance measure (d N ) [11]. In the case of information gain and gain ratio that attribute is selected (from the available attributes) which maximizes this measure.
Reference: [18] <author> M. Singh and G. Provan. </author> <title> A Comparison of Induction Algorithms for Selective and non-Selective Bayesian Classifiers. </title> <booktitle> In Proceedings of the 12th International Conference on Machine Learning, </booktitle> <pages> 497-505. </pages>
Reference-contexts: However, inference using Bayesian networks is an N P-hard problem. Hence, learning Bayesian network classifiers from a subset of the attributes may be a good compromise between naive Bayesian classifiers and Bayesian networks that model all attributes. The K2-AS algorithm <ref> [18] </ref> was an attempt in this direction and has been shown to outperform both selective as well as non-selective naive Bayesian classifiers. <p> It keeps constructing networks for increasing orders of CI tests as long as the predictive accuracy of the resultant network keeps increasing. Singh and Provan <ref> [18, 15] </ref> proposed an attribute selection based enhancement of this approach called K2-AS.
Reference: [19] <author> M. Singh and M. Valtorta. </author> <title> Construction of Bayesian Network Structures from Data: a Brief Survey and an Efficient Algorithm. </title> <journal> International Journal of Approximate Reasoning, </journal> <volume> 12, </volume> <pages> 111-131, </pages> <year> 1995. </year> <month> 23 </month>
Reference-contexts: It stops adding parents to the node when adding any additional single parent cannot increase the probability. Whereas K2 assumes a node ordering, the CB algorithm <ref> [19, 20] </ref> uses conditional independence (CI) tests to generate a "good" node ordering from the data, and then uses the K2 algorithm to generate the Bayesian network from the database using this node ordering.
Reference: [20] <author> M. Singh and M. Valtorta. </author> <title> An Algorithm for the Construction of Bayesian Net--work Structures from Data. </title> <booktitle> In Proc. Conf. Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 259-265, </pages> <publisher> Morgan-Kaufmann Publishers, </publisher> <year> 1993. </year>
Reference-contexts: It stops adding parents to the node when adding any additional single parent cannot increase the probability. Whereas K2 assumes a node ordering, the CB algorithm <ref> [19, 20] </ref> uses conditional independence (CI) tests to generate a "good" node ordering from the data, and then uses the K2 algorithm to generate the Bayesian network from the database using this node ordering.
Reference: [21] <author> A. White and W. Liu. </author> <title> Bias in information-based measures in decision tree induction. </title> <booktitle> Machine Learning, </booktitle> <pages> 321-329, </pages> <booktitle> 1994. </booktitle> <volume> 24 25 26 27 28 </volume>
Reference-contexts: These probabilities can be calculated by constructing a 3-d contingency table from the given cases. Then, following the terminology of White and Liu <ref> [21] </ref>, we can define H Cell l = k X m X p ij=l log p ij=l (3) H A l = j=1 H C l = i=1 We can then define the three metrics as follows: Definition 4.1 CIG (A; ) = Gain (A=) = l=1 p l (H C
References-found: 21

