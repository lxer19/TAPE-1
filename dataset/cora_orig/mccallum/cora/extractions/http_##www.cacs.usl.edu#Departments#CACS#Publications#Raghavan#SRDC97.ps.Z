URL: http://www.cacs.usl.edu/Departments/CACS/Publications/Raghavan/SRDC97.ps.Z
Refering-URL: http://www.cacs.usl.edu/~raghavan/raghavan-1.html
Root-URL: http://www.cacs.usl.edu/~raghavan/raghavan-1.html
Title: Comparison of Classification Methods  
Author: Suresh K. Choubey Jitender S. Deogun Vijay V. Raghavan Hayri Sever 
Abstract: In this paper, we experimentally compare classification of concepts based on rough sets using upper, lower, and elementary set methods in the context of feature selection algorithms. We study the performance of lower, upper, and elementary set classifiers on several machine learning data sets and a real-world data set on duodenal ulcer. The experimental set-up is such that the approximation space (i.e., the set of features retained) can be different depending on the classification method used. In data mining applications, we are more interested in describing the data at hand. Hence, we have used upperbound experiments, where same set of data is used for training and testing. Our initial result suggests that upper classifier performs better than lower classifier for the data set on duodenal ulcer that we have used. Keywords- Rough sets, feature selection, upper classifier, lower classifier, elementary set, database mining, knowledge discovery. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> V. V. Raghavan and H. </author> <title> Sever, "The state of rough sets for database mining applications," </title> <booktitle> in Proceedings of 23rd Computer Science Conference Workshop on Rough Sets and Database Mining (T. </booktitle> <editor> Y. Lin, ed.), </editor> <address> (San Jose State University, San Jose, CA), </address> <pages> pp. 1-11, </pages> <month> mar </month> <year> 1995. </year>
Reference-contexts: Moreover, a database may contain null values as well as redundant data. A knowledge discovery system for database mining applications, must be able to overcome these problems <ref> [1] </ref>. The rough set theory provides a formal framework for investigating problems relating to ultra large and dynamic data in real-world databases. Feature selection is the problem of selecting a small subset of features that is necessary and sufficient to describe a target set of concepts.
Reference: [2] <author> K. Kira and L. Rendell, </author> <title> "The feature selection problem: Tradational methods and a new algorithm," </title> <booktitle> in Proceedings of AAAI-92, </booktitle> <pages> pp. 129-134, </pages> <publisher> AAAI Press, </publisher> <year> 1992. </year>
Reference-contexts: It is an important problem because of its potential for enhancing both the processes of concept learning as well as classification of objects, and reducing the cost of classification (e.g., eliminating redundant tests in medical diagnosis), and improving the quality of classification <ref> [2] </ref>. We have proposed, in Deogun et al. [3], several ways to improve upper classifiers one of the classification methods in rough set theory. The enhancement is achieved by a sequential backward selection (SBS) algorithm to preprocess a given set of features.
Reference: [3] <author> J. S. Deogun, V. V. Raghavan, and H. </author> <title> Sever, "Exploiting upper approximations in the rough set methodology," </title> <booktitle> in The First International Conference on Knowledge Discovery and Data Mining, </booktitle> <address> (Montreal, Quebec, Canada), </address> <pages> pp. 1-10, </pages> <month> aug </month> <year> 1995. </year>
Reference-contexts: We have proposed, in Deogun et al. <ref> [3] </ref>, several ways to improve upper classifiers one of the classification methods in rough set theory. The enhancement is achieved by a sequential backward selection (SBS) algorithm to preprocess a given set of features.
Reference: [4] <author> S. K. Choubey, J. S. Deogun, V. V. Ragha-van, and H. </author> <title> Sever, "A Comparison of Feature Selection Algorithms in the Context of Rough Classifiers," </title> <booktitle> in Proceedings of Fifth IEEE International Conference on Fuzzy Systems, </booktitle> <volume> vol. </volume> <pages> 2, </pages> <address> (New Orleans, LA), </address> <pages> pp. 1122-1128, </pages> <month> September </month> <year> 1996. </year>
Reference-contexts: All these algorithms fall under the class of Sequential Backward Selection (SBS) algorithms. These methods are: (1.) Best fit SBS (BF S), (2.)Hybrid Heuristic SBS (HHS), (3.)Alternating Heuristic SBS (AHS), and (4.) K-level Best SBS (KBS). Choubey et al. <ref> [4] </ref> have provided the details of these feature selection algorithms. <p> The feasibility condition for a current node to be expanded is imposed as ' fl S=F (U= g DEC) ' fl ): 4 Experimental Results We have performed upperbound experiments <ref> [4] </ref> with the goal to study the performance of classification methods on duodenal ulcer data [5]. We first ran the experiment with UC to obtain several promising feature subsets as shown in Table 1. Each is a -reduct, where = 0:09.
Reference: [5] <author> Z. Pawlak, K. Slowinski, and R. Slowinski, </author> <title> "Rough classification of patients after highly selective vagotomy for duodenal ulcer," </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> vol. 24, </volume> <pages> pp. 413-433, </pages> <year> 1986. </year> <month> 4 </month>
Reference-contexts: The feasibility condition for a current node to be expanded is imposed as ' fl S=F (U= g DEC) ' fl ): 4 Experimental Results We have performed upperbound experiments [4] with the goal to study the performance of classification methods on duodenal ulcer data <ref> [5] </ref>. We first ran the experiment with UC to obtain several promising feature subsets as shown in Table 1. Each is a -reduct, where = 0:09. For each -reduct, % accuracy for using LC, UC, and EC are reported in Table 2.
References-found: 5

