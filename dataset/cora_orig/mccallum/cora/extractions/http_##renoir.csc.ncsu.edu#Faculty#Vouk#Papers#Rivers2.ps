URL: http://renoir.csc.ncsu.edu/Faculty/Vouk/Papers/Rivers2.ps
Refering-URL: http://zagreb.csc.ncsu.edu/CSC791V/Assignments/Reading/Notes.html
Root-URL: http://www.csc.ncsu.edu
Title: Resource-Constrained NonOperational Testing of Software 1  
Author: Anthony T. Rivers and Mladen A. Vouk 
Address: Raleigh, NC 27695-7911  
Affiliation: Box 8206, North Carolina State University,  
Abstract: In classical testing approaches, learning is said to occur if testers dynamically improve the efficiency of their testing as they progress through a testing phase. However, the pressures of modern business and software development practices seem to favor an approach to testing which is very akin to a sampling without replacement of a relatively limited number of predetermined structures and functions conducted under significant schedule and resource constraints. The primary driver is often the desire to cover ONLY previously untested functions, operations or code constructs, and to meet milestones. We develop and evaluate a model that describes the fault detection and removal process in such an environment. Results indicate that in environments where coverage-based testing is promoted, but resources and decisions are constrained, very little dynamic learning takes place, and that it may be an artifact of program structure or of the test-case sequencing policy. 
Abstract-found: 1
Intro-found: 1
Reference: [Bel95] <author> F. Belli, O. Jack, </author> <title> "A Test Coverage Notion for Logic Programming," </title> <booktitle> Proc. 6th ISSRE, </booktitle> <address> Toulouse, France, </address> <month> October </month> <year> 1995. </year>
Reference-contexts: Usually, the models combine the concepts of functional coverage or code coverage, with some practical testing strategy. We call these models coverage-based models since they imply that coverage guides the testing rather than some other criterion, such as failure intensity, and/or marketing deadlines <ref> [e.g., Gok96, Fra95, Bel95, Mal94, Vou92] </ref>.
Reference: [Cha91] <author> Chan, F., Dasiewicz, P., and Seviora, R., </author> <title> Metrics for Evaluation of Software Reliability Growth Models," </title> <booktitle> Proc. 2nd ISSRE, </booktitle> <address> Austin, Texas, </address> <month> May </month> <year> 1991, </year> <pages> pp. 163-167 </pages>
Reference: [Che95] <author> Mei-Hwa Chen, Aditya P. Mathur, Vernon J. Rego, </author> <title> "Effect of Testing Techniques on Software Reliability Estimates Obtained Using A TimeDomain Model," </title> <journal> IEEE Trans. on Reliability, </journal> <volume> Vol. 44 (1), </volume> <month> Mar </month> <year> 1995, </year> <pages> pp. 97-103. </pages>
Reference-contexts: This threshold coverage value will tend to vary even between functionally equivalent implementations. The model shows that metric saturation can occur, but that additional testing may still detect errors. This is in good agreement with the experimental observations, as well as conclusions of other researchers working in the field <ref> [e.g., Che95] </ref>. For some metrics, such as fraction of planned test cases, this effect allows introduction of the concept of hidden constructs, reevaluation of the plan and estimation of the additional time to target intensity in a manner similar to classical time-based models [e.g., Mus87].
Reference: [Coe96] <author> D.M. Cohen, S.R. Dalal, J. Parelius and G.C. Patton, </author> <title> The Combinatorial Design Approach to Automatic Test Generation,, </title> <journal> IEEE Software, </journal> <volume> Vol. 13(15), </volume> <month> Sep. </month> <year> 1996, </year>
Reference-contexts: However, learning (improvements) can take place in consecutive testing phases. Our data seem to support both of these theories. At this point, it is not clear which metrics and testing strategies actually optimize the returns. Two candidate strategies are the pairwise testing <ref> [Coe96] </ref> and predicate-based testing [Par97]. Work on that is in progress.
Reference: [Cus97] <author> Cusick J., Fine M. </author> <title> "Guiding Reengineering with the Operational Profile," </title> <booktitle> 8th ISSRE, Case Studies, </booktitle> <month> Nov. </month> <pages> 2-5, </pages> <year> 1997, </year> <pages> pp. 15-25. </pages>
Reference: [Eck91] <author> D.E. Eckhardt, A.K. Caglayan, J.P.J. Kelly, J.C. Knight, L.D. Lee, D.F. McAllister, and M.A. Vouk, </author> <title> "An Experimental Evaluation of Software Redundancy as a Strategy for Improving Reliability," </title> <journal> IEEE Trans. Soft. Eng., </journal> <volume> Vol. 17(7), </volume> <pages> pp. 692-702, </pages> <year> 1991. </year>
Reference-contexts: Empirical Results 3.1 Data The empirical data used in this investigation come from three sources. One set is from the paper by Ohba [Ohb84], the second set comes from the NASA Langley Research Center (LaRC) sponsored multi-version multi-university software experiment <ref> [Kel88, Vou90, Eck91, Par97] </ref>, and the third set comes from the functional test debug stage of a large telecommunications system. The first (Ohba) dataset represents testand-debug data of a PL/I database application program described in [Ohb84], Appendix F, Table 4. <p> After the functional and random certification tests, the programs were subjected to an independent operational evaluation test which found additional defects in the code. Detailed descriptions of the detected faults and the employed testing strategies are given in <ref> [Vou90, Eck91, Par97] </ref>. The third (Commercial) dataset comes from real software testing and evaluation efforts conducted during late unit and early integration testing of four consecutive releases of a very large (millions of lines of code) commercial telecommunications product. <p> In this illustration we use puse coverage data for three of the 20 functionally equivalent programs, and we have set N to the number of faults eventually detected in these programs through extensive tests that followed the certification phase <ref> [Vou90, Eck91] </ref>. In the figures, we plot the instantaneous or per teststep i, or test-case efficiency, and its arithmetic average over the covered metric space.
Reference: [Fra95] <author> F. Del Frate, P. Garg, A.P. Mathur, A. Pasquini, </author> <title> "On the Correlation Between Code Coverage and Software Reliability," </title> <booktitle> Proc. 6th ISSRE, </booktitle> <address> Toulouse, France, Oct.1995. </address>
Reference-contexts: Usually, the models combine the concepts of functional coverage or code coverage, with some practical testing strategy. We call these models coverage-based models since they imply that coverage guides the testing rather than some other criterion, such as failure intensity, and/or marketing deadlines <ref> [e.g., Gok96, Fra95, Bel95, Mal94, Vou92] </ref>.
Reference: [Fri95] <author> Michael A. Friedman and Jeffrey M. Voas, </author> <title> Software Assessment Reliability, Safety, Testability, </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1995. </year>
Reference-contexts: 1. Introduction Software reliability models attempt to characterize software defect discovery and removal processes during various stages of the software development cycle, and from that deduce software behavior later in its life. <ref> [Mus87, Fri95] </ref>. Most classical reliability models assume (explicitly or implicitly) that software testing profile corresponds closely to the operational software usage profile. Consequently, these models tend to be based on assumptions and distributions which belong to a process that can be best described as sampling with replacement.
Reference: [Gok96] <author> Swapna S. Gokhale, Teebu Philip, Peter N. Marinos, Kishor S. Trivedi, </author> <title> Unification of Finite Failure NonHomogeneous Poisson Process Models through Test Coverage, </title> <booktitle> 7th International Symposium on Software Reliability Engineering, </booktitle> <month> Oct, </month> <year> 1996, </year> <pages> pp. 299-307. </pages>
Reference-contexts: Usually, the models combine the concepts of functional coverage or code coverage, with some practical testing strategy. We call these models coverage-based models since they imply that coverage guides the testing rather than some other criterion, such as failure intensity, and/or marketing deadlines <ref> [e.g., Gok96, Fra95, Bel95, Mal94, Vou92] </ref>.
Reference: [Hou94] <author> Rong-Huei Hou, Sy-Yen Kuo, YiPing Chang, </author> <title> "Applying Various Learning Curves to the Hyper-Geometric Distribution Software Reliability Growth Model," </title> <booktitle> 5th ISSRE, </booktitle> <month> Nov. </month> <pages> 6-9, </pages> <year> 1994, </year> <pages> pp. 196-205. </pages>
Reference-contexts: Another family of related models is based on the hyper-geometric distribution which is used to describe the process of sampling without replacement stochastically <ref> [Hou94, Jac92, Tho89, Sho83, Piw93] </ref>. 1.1 Market-driven Process The pressures of modern market-driven software development practices, such as the use of various types of incentives to influence workers to reduce time to market and overall development cost, seem to favor a resource constrained approach which is different from the classical software <p> Published models, and empirical data, suggest that efficiency growth due to learning may follow any number of growth curves from linear to that described by the logistic function <ref> [e.g., Hou94, Hua97] </ref>. On the other hand, our work seems to indicate that in a resource-constrained environment very little actual "learning" takes place during nonoperational testing, although efficiency may change when an explicit change in testing strategy takes place. <p> Similar correction factors appear in different forms in other hyper-geometric and coverage-based models. For instance, Tohma et al. [Toh89] used the term sensitivity factor, ease of test, and skill in developing their Hyper-Geometric Distribution Model. Hou et al. <ref> [Hou94] </ref> used the term learning factor, while Piwowarski et al. [Piw93] used the term error detection rate constant to account for error detection variability in their coverage-based model. <p> It is interesting to note that equation (3.1), in fact, subsumes the learning factor proposed by Hou et al. <ref> [Hou94] </ref>, and derived from the sensitivity factor defined by Tohma et al. [Tho89], i.e., w i = N - E i-1 where w i is the sensitivity factor. It also subsumes the metric-based failure intensity, DE i . <p> It also subsumes the metric-based failure intensity, DE i . We define Average Testing Efficiency as AverageTestingEfficiency g q n i ( ) 1 3.2.1 Learning. Figures 3.1 and 3.2 illustrate the difference between learning factor and testing efficiency functions using the same data as Hou et al. <ref> [Hou94, Ohb84] </ref>. We assume that the testing schedule of 19 weeks was planned, and that N=358. Therefore, on the horizontal axis we plot the fraction of the schedule that is completed.
Reference: [Hua97] <author> Chin-Yu Huang, Sy-Yen Kuo, I.Y. </author> <title> Chen "Analysis of a aoftware reliability growth model with logistic testing-effort function," </title> <booktitle> 8th ISSRE, </booktitle> <month> Nov. </month> <year> 1997, </year> <pages> pp. 378-388. </pages>
Reference-contexts: Published models, and empirical data, suggest that efficiency growth due to learning may follow any number of growth curves from linear to that described by the logistic function <ref> [e.g., Hou94, Hua97] </ref>. On the other hand, our work seems to indicate that in a resource-constrained environment very little actual "learning" takes place during nonoperational testing, although efficiency may change when an explicit change in testing strategy takes place.
Reference: [Jac92] <author> Raymond Jacoby, Kaori Masuzawa, </author> <title> "Test Coverage Dependent Software Reliability Estimation by the HGD Model," </title> <booktitle> Proc. 3rd ISSRE, </booktitle> <institution> Research Triangle Park, North Carolina, </institution> <month> October </month> <year> 1992, </year> <pages> pp. 193-204. </pages>
Reference-contexts: Another family of related models is based on the hyper-geometric distribution which is used to describe the process of sampling without replacement stochastically <ref> [Hou94, Jac92, Tho89, Sho83, Piw93] </ref>. 1.1 Market-driven Process The pressures of modern market-driven software development practices, such as the use of various types of incentives to influence workers to reduce time to market and overall development cost, seem to favor a resource constrained approach which is different from the classical software <p> For example, one test case may, on the average, cover 50-60% of statements <ref> [Jac92] </ref>. Function g i has strong influence on the shape of the failure intensity of the model. This function encapsulates the information about the change in the ability of the test-debug process to detect faults as coverage increases.
Reference: [Jon96] <author> Jones W. and Vouk M.A., </author> <title> "Software Reliability Field Data Analysis," Chapter 11 in Handbook of Software Reliability Engineering, </title> <publisher> McGraw Hill, </publisher> <editor> ed. M. Lyu., </editor> <year> 1996. </year>
Reference-contexts: When calculating inservice time, 1000 months of inservice time may be associated with 500 months (clock time) of two systems, or 1 month (clock time) of 1000 systems, or 1000 months (clock time) of 1 system <ref> [Jon96] </ref>.
Reference: [Kel88] <author> J. Kelly, D. Eckhardt, A. Caglayan, J. Knight, D. McAllister, M. Vouk, </author> <title> "A Large Scale Second Generation Experiment in Multi-Version Software: Description and Early Results", </title> <booktitle> Proc. </booktitle> <volume> FTCS 18, </volume> <pages> pp. 9-14, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Empirical Results 3.1 Data The empirical data used in this investigation come from three sources. One set is from the paper by Ohba [Ohb84], the second set comes from the NASA Langley Research Center (LaRC) sponsored multi-version multi-university software experiment <ref> [Kel88, Vou90, Eck91, Par97] </ref>, and the third set comes from the functional test debug stage of a large telecommunications system. The first (Ohba) dataset represents testand-debug data of a PL/I database application program described in [Ohb84], Appendix F, Table 4. <p> The second (NASA) dataset comes from the NASA LaRC experiment in which twenty implementations of a sensor management inertial navigation system were developed to the same specification <ref> [Kel88] </ref>. The data discussed in this paper are from the "certification" phases. Independent certification testing consisted of a functional certification test suite and a random certification test suite.
Reference: [Mal94] <author> Yashwant K. Malaiya, Rick Karcich, </author> <title> "The Relationship Between Test Coverage and Reliability," </title> <booktitle> 5 th ISSRE, </booktitle> <month> Nov. </month> <pages> 6-9, </pages> <year> 1994, </year> <pages> pp. 186-195. </pages>
Reference-contexts: Usually, the models combine the concepts of functional coverage or code coverage, with some practical testing strategy. We call these models coverage-based models since they imply that coverage guides the testing rather than some other criterion, such as failure intensity, and/or marketing deadlines <ref> [e.g., Gok96, Fra95, Bel95, Mal94, Vou92] </ref>.
Reference: [Mus87] <author> J.D. Musa, A. Iannino, and K. Okumoto, </author> <title> Software Reliability: Measurement Predictions, Application, </title> <publisher> McGraw-Hill Book Co., </publisher> <year> 1987. </year>
Reference-contexts: 1. Introduction Software reliability models attempt to characterize software defect discovery and removal processes during various stages of the software development cycle, and from that deduce software behavior later in its life. <ref> [Mus87, Fri95] </ref>. Most classical reliability models assume (explicitly or implicitly) that software testing profile corresponds closely to the operational software usage profile. Consequently, these models tend to be based on assumptions and distributions which belong to a process that can be best described as sampling with replacement. <p> Consequently, these models tend to be based on assumptions and distributions which belong to a process that can be best described as sampling with replacement. It is also commonly assumed that the exposure to the testing is measured using some unbounded central processing unit time or calendar time <ref> [e.g., Mus87] </ref>. Unfortunately, in practice modern schedule , market and cost-driven constraints often invalidate these assumptions, especially during the early testing phases (e.g., unit testing, feature and integration testing phases). In these situations, interpretation of classical software reliability models becomes difficult, and may be deceptive. <p> In these situations, interpretation of classical software reliability models becomes difficult, and may be deceptive. A time-based solution to this problem was offered by Musa in the form of compression factors <ref> [Mus87] </ref>. 1 This research was supported in part by a Nortel grant. Many attempts have been made also to relate practical nonoperational testing and defect removal processes to coverage of software control and data-flow and other constructs, such as branches and puses, program functions, or other logical entities. <p> Hou et al. [Hou94] used the term learning factor, while Piwowarski et al. [Piw93] used the term error detection rate constant to account for error detection variability in their coverage-based model. Malaya et al. [Mal92] analyzed the Fault Exposure Ratio <ref> [Mus87] </ref> to account for the fluctuation of per-fault detectability while developing their exponential and logarithmic coverage based models. The hyper-geometric probability mass function describes the process of sampling without replacement [e.g., Wal78]. <p> For some metrics, such as fraction of planned test cases, this effect allows introduction of the concept of hidden constructs, reevaluation of the plan and estimation of the additional time to target intensity in a manner similar to classical time-based models <ref> [e.g., Mus87] </ref>.
Reference: [Ohb84] <author> M. Ohba, </author> <title> "Software Reliability Analysis Models," </title> <journal> IBM J. of Res. and Development, </journal> <volume> Vol. 28 (4), </volume> <pages> pp. 428-443, </pages> <year> 1984. </year>
Reference-contexts: Empirical Results 3.1 Data The empirical data used in this investigation come from three sources. One set is from the paper by Ohba <ref> [Ohb84] </ref>, the second set comes from the NASA Langley Research Center (LaRC) sponsored multi-version multi-university software experiment [Kel88, Vou90, Eck91, Par97], and the third set comes from the functional test debug stage of a large telecommunications system. <p> The first (Ohba) dataset represents testand-debug data of a PL/I database application program described in <ref> [Ohb84] </ref>, Appendix F, Table 4. The size of the software is about 1.4 million lines of code, and the failure data were collected using calendar and CPU execution time. <p> It also subsumes the metric-based failure intensity, DE i . We define Average Testing Efficiency as AverageTestingEfficiency g q n i ( ) 1 3.2.1 Learning. Figures 3.1 and 3.2 illustrate the difference between learning factor and testing efficiency functions using the same data as Hou et al. <ref> [Hou94, Ohb84] </ref>. We assume that the testing schedule of 19 weeks was planned, and that N=358. Therefore, on the horizontal axis we plot the fraction of the schedule that is completed. <p> Comparison of the failure intensity and cumulative failure fits obtained using our model with Ohbas models, shows that our coverage model fits the data as well as the exponential or the S-shaped models [Riv98]. 1.00.80.60.40.20.0 0.1 0.3 Ohba DataSet <ref> [Ohb84] </ref> Coverage of Planned Schedule L r n g a t o r per week average computed using data from [Ohb84], Table 4. 1.00.80.60.40.20.0 1 3 Ohba DataSet [Ohb84] Coverage of Planned Schedule T s g f f i c n y per week average computed using data from [Ohb84], Table <p> obtained using our model with Ohbas models, shows that our coverage model fits the data as well as the exponential or the S-shaped models [Riv98]. 1.00.80.60.40.20.0 0.1 0.3 Ohba DataSet <ref> [Ohb84] </ref> Coverage of Planned Schedule L r n g a t o r per week average computed using data from [Ohb84], Table 4. 1.00.80.60.40.20.0 1 3 Ohba DataSet [Ohb84] Coverage of Planned Schedule T s g f f i c n y per week average computed using data from [Ohb84], Table 4. 3.2.2 Impact of Program Structure. <p> that our coverage model fits the data as well as the exponential or the S-shaped models [Riv98]. 1.00.80.60.40.20.0 0.1 0.3 Ohba DataSet <ref> [Ohb84] </ref> Coverage of Planned Schedule L r n g a t o r per week average computed using data from [Ohb84], Table 4. 1.00.80.60.40.20.0 1 3 Ohba DataSet [Ohb84] Coverage of Planned Schedule T s g f f i c n y per week average computed using data from [Ohb84], Table 4. 3.2.2 Impact of Program Structure. Figures 3.3 to 3.5 illustrate the testing efficiency that was measured for the certification testsuite used in the NASALaRC experiment. <p> DataSet <ref> [Ohb84] </ref> Coverage of Planned Schedule L r n g a t o r per week average computed using data from [Ohb84], Table 4. 1.00.80.60.40.20.0 1 3 Ohba DataSet [Ohb84] Coverage of Planned Schedule T s g f f i c n y per week average computed using data from [Ohb84], Table 4. 3.2.2 Impact of Program Structure. Figures 3.3 to 3.5 illustrate the testing efficiency that was measured for the certification testsuite used in the NASALaRC experiment.
Reference: [Par97] <author> A. Paradkar, K.C. Tai, and M. Vouk, </author> <title> "Specification-based Testing Using Cause-Effect Graphs," </title> <journal> Annals of Software Engineering, </journal> <volume> Vol 4, </volume> <pages> pp. 133-158, </pages> <year> 1997. </year>
Reference-contexts: Empirical Results 3.1 Data The empirical data used in this investigation come from three sources. One set is from the paper by Ohba [Ohb84], the second set comes from the NASA Langley Research Center (LaRC) sponsored multi-version multi-university software experiment <ref> [Kel88, Vou90, Eck91, Par97] </ref>, and the third set comes from the functional test debug stage of a large telecommunications system. The first (Ohba) dataset represents testand-debug data of a PL/I database application program described in [Ohb84], Appendix F, Table 4. <p> After the functional and random certification tests, the programs were subjected to an independent operational evaluation test which found additional defects in the code. Detailed descriptions of the detected faults and the employed testing strategies are given in <ref> [Vou90, Eck91, Par97] </ref>. The third (Commercial) dataset comes from real software testing and evaluation efforts conducted during late unit and early integration testing of four consecutive releases of a very large (millions of lines of code) commercial telecommunications product. <p> This is a strong indicator that a constrained testing strategy may perform quite differently in different situations, and that a specification-based test generation needs to be supplemented with a more rigorous test-case selection approach such as predicate-based testing <ref> [Par97] </ref>. 0.850.750.650.550.450.350.35 1 3 Program P13 (Certification Testsuite detected 6 out of 9 faults) Coverage (p-uses) T s g f f i c i e n Per Testcase Average for program P13 (NASA DataSet). 3.2.3 Impact of Consecutive Releases. <p> However, learning (improvements) can take place in consecutive testing phases. Our data seem to support both of these theories. At this point, it is not clear which metrics and testing strategies actually optimize the returns. Two candidate strategies are the pairwise testing [Coe96] and predicate-based testing <ref> [Par97] </ref>. Work on that is in progress.
Reference: [Piw93] <author> P. Piwowarski, M. Ohba, J. Caruso, </author> <title> "Coverage measurement experience during function test," </title> <booktitle> ICSE 9 3 , 1993, </booktitle> <pages> pp. 287-300. </pages>
Reference-contexts: Another family of related models is based on the hyper-geometric distribution which is used to describe the process of sampling without replacement stochastically <ref> [Hou94, Jac92, Tho89, Sho83, Piw93] </ref>. 1.1 Market-driven Process The pressures of modern market-driven software development practices, such as the use of various types of incentives to influence workers to reduce time to market and overall development cost, seem to favor a resource constrained approach which is different from the classical software <p> Similar correction factors appear in different forms in other hyper-geometric and coverage-based models. For instance, Tohma et al. [Toh89] used the term sensitivity factor, ease of test, and skill in developing their Hyper-Geometric Distribution Model. Hou et al. [Hou94] used the term learning factor, while Piwowarski et al. <ref> [Piw93] </ref> used the term error detection rate constant to account for error detection variability in their coverage-based model. Malaya et al. [Mal92] analyzed the Fault Exposure Ratio [Mus87] to account for the fluctuation of per-fault detectability while developing their exponential and logarithmic coverage based models. <p> The probability of finding a new defect is a function only of different defect metrics and does not explicitly contain terms that address the number of remaining constructs that are being covered in the testing process On the other hand, the Piwowarski, Ohba and Caruso Block Coverage Model <ref> [Piw93] </ref> focuses on coverage of the constructs. It incorporates the defect detection only indirectly.
Reference: [Pot97] <author> T. Potok, and M. Vouk, </author> <title> "The Effects of the Business Model on the ObjectOriented Software Development Productivity," </title> <journal> IBM Systems Journal, </journal> <volume> Vol 36 (1), </volume> <pages> pp. 140-161, </pages> <year> 1997. </year>
Reference-contexts: Sho83, Piw93]. 1.1 Market-driven Process The pressures of modern market-driven software development practices, such as the use of various types of incentives to influence workers to reduce time to market and overall development cost, seem to favor a resource constrained approach which is different from the classical software engineering approaches <ref> [Pot97] </ref>. Meeting the schedule is a major concern. In this case, the testing process is often analogous to a sampling without replacement of a finite (and sometimes very limited) number of predetermined structures, functions, and environments.
Reference: [Riv95] <author> A. Rivers and M.A. Vouk, </author> <title> "An Experimental Evaluation of Hyper-Geometric Code Testing Coverage Modelt", </title> <booktitle> in Proc. 1995 Software Engineering Research Forum, </booktitle> <address> Boca Raton, FL, </address> <pages> pp. 41-50, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: In the next section we present a model that describes constrained testing. An early version of the model with some initial results, was presented at SERF95 <ref> [Riv95] </ref>. In section 3 we present and discuss some empirical results related to the nature of nonoperational testing efficiency. In section 4 we discuss the relationship between testing efficiency of nonoperational testing and field quality of the software. Conclusions are in section 5. 2. <p> For instance, faults that require at least two passes through a loop may not be detectable by a strategy S that requires the test cases to cover all branches only once. Consider the metric space (M-space) offered by a program P and schematically depicted in Figure 2.1. <ref> [Vou92, Riv95] </ref>. offered by a program P. Let each square represent a construct of the metric M (e.g. du-pair, a branch, a sub-path, a functionality, a test case). Let the dark areas represent faults detectable through execution of constructs of M .
Reference: [Riv98] <author> A. </author> <title> Rivers "Modeling Software Reliability During NonOperational Testing", </title> <type> Ph.D. </type> <institution> disstertation North Carolina State University, </institution> <year> 1998. </year>
Reference-contexts: The first case is a discrete solution of (2.4) to (2.6) for the number of defects that would be shipped after teststep i based on an ideal nonoperational testing strategy is <ref> [Riv98] </ref>: ShippedDefectsWithOut N E N N g h i i j = - = - 1 1 (2.17) In the second case, to emulate an ideal strategy based on sampling with replacement, we start with a variant of equation (2.6) with K in place of u i . <p> It can be shown then that the discrete solution for test case i is <ref> [Riv98] </ref>: ShippedDefectsWith N E N N g h i j = - = - 1 1 (2.18) In an ideal situation the testsuite has in it test-cases that are capable of detecting all faults (all faults are M - detectable), and defect removal is instantaneous and perfect. <p> The impact of different (but constant) g i values on sampling without replacement (non-operational testing) was explored using a variant of (2.12 and 2.17) <ref> [Riv98] </ref>: ShippedDefects N E i i = - = -( ) 1 (2.19) When g i is less than 1, some faults remain undetected. In general, the (natural) desire to constrain the number of test-cases and sample without replacement is driven by business decisions and resource constraints. <p> Comparison of the failure intensity and cumulative failure fits obtained using our model with Ohbas models, shows that our coverage model fits the data as well as the exponential or the S-shaped models <ref> [Riv98] </ref>. 1.00.80.60.40.20.0 0.1 0.3 Ohba DataSet [Ohb84] Coverage of Planned Schedule L r n g a t o r per week average computed using data from [Ohb84], Table 4. 1.00.80.60.40.20.0 1 3 Ohba DataSet [Ohb84] Coverage of Planned Schedule T s g f f i c n y per week average <p> Fig. 3.6 results are for Rel. 1 and Fig. 3.7 results are for Release 4. Similar results were obtained for Releases 2 and 3 <ref> [Riv98] </ref>. All releases of the software had their testing cycle constrained in both the time and test size domains. We obtain estimates for the parameter N by fitting the constant testing efficiency failure intensity model equation (2.13) to grouped failure intensity data of each release. <p> W also attempted to describe the data using a number of other software reliability models. The constant testing efficiency model was the only one that gave reasonable parameter estimates for all four releases <ref> [Riv98] </ref>. Relative error tests show that the model (equation 2.12) fits the data quite well for all releases. Most of the time, the relative error is of the order of 10% or less. 4.
Reference: [Sho83] <author> M.L. </author> <title> Shooman , Software Engineering, </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: Another family of related models is based on the hyper-geometric distribution which is used to describe the process of sampling without replacement stochastically <ref> [Hou94, Jac92, Tho89, Sho83, Piw93] </ref>. 1.1 Market-driven Process The pressures of modern market-driven software development practices, such as the use of various types of incentives to influence workers to reduce time to market and overall development cost, seem to favor a resource constrained approach which is different from the classical software <p> All the variables are related to coding constructs, in this case the coding linear blocks. Similar models can be constructed for other code or specification constructs. Finally, the Shooman model <ref> [Sho83] </ref> is the one that comes closest to fusing defect and coverage in a way appropriate for processes that strive to use sampling without replacement.
Reference: [Toh89] <author> Yoshihiro Tohma, Raymond Jacoby, Yukihisa Murata, Moriki Yamamoto, </author> <title> "Hyper-Geometric Distribution Model to Estimate the Number of Residual Software Faults," </title> <booktitle> Proc. COMPSAC 89, </booktitle> <address> Orlando, Florida, </address> <month> September </month> <year> 1989, </year> <pages> pp. 610-617. </pages>
Reference-contexts: This function represents the "visibility" or "cross-section" of the remaining defects to testing step i . Similar correction factors appear in different forms in other hyper-geometric and coverage-based models. For instance, Tohma et al. <ref> [Toh89] </ref> used the term sensitivity factor, ease of test, and skill in developing their Hyper-Geometric Distribution Model. Hou et al. [Hou94] used the term learning factor, while Piwowarski et al. [Piw93] used the term error detection rate constant to account for error detection variability in their coverage-based model.
Reference: [Vou89] <author> Vouk, M.A., and Coyle, R.E., "BGG: </author> <title> A Testing Coverage Tool," </title> <booktitle> Proc. Seventh Annual Pacific Northwest Software Quality Conference, </booktitle> <publisher> Lawrence and Craig, Inc., </publisher> <address> Portland, OR, pp.212-233, </address> <month> September </month> <year> 1989. </year>
Reference-contexts: The data discussed in this paper are from the "certification" phases. Independent certification testing consisted of a functional certification test suite and a random certification test suite. The cumulative execution coverage of the code by the test cases was determined through post-experimental code instrumentation and execution tracing using BGG <ref> [Vou89] </ref>. After the functional and random certification tests, the programs were subjected to an independent operational evaluation test which found additional defects in the code. Detailed descriptions of the detected faults and the employed testing strategies are given in [Vou90, Eck91, Par97].
Reference: [Vou90] <author> M. Vouk, A. Caglayan, D.E. Eckhardt , J.P.J. Kelly, J. Knight, D.F. McAllister, L. Walker, </author> <title> "Analysis of faults detected in a large-scale multiversion software development experiment," </title> <booktitle> Proc. </booktitle> <volume> DASC '90, </volume> <pages> pp. 378-385, </pages> <year> 1990. </year>
Reference-contexts: Empirical Results 3.1 Data The empirical data used in this investigation come from three sources. One set is from the paper by Ohba [Ohb84], the second set comes from the NASA Langley Research Center (LaRC) sponsored multi-version multi-university software experiment <ref> [Kel88, Vou90, Eck91, Par97] </ref>, and the third set comes from the functional test debug stage of a large telecommunications system. The first (Ohba) dataset represents testand-debug data of a PL/I database application program described in [Ohb84], Appendix F, Table 4. <p> After the functional and random certification tests, the programs were subjected to an independent operational evaluation test which found additional defects in the code. Detailed descriptions of the detected faults and the employed testing strategies are given in <ref> [Vou90, Eck91, Par97] </ref>. The third (Commercial) dataset comes from real software testing and evaluation efforts conducted during late unit and early integration testing of four consecutive releases of a very large (millions of lines of code) commercial telecommunications product. <p> In this illustration we use puse coverage data for three of the 20 functionally equivalent programs, and we have set N to the number of faults eventually detected in these programs through extensive tests that followed the certification phase <ref> [Vou90, Eck91] </ref>. In the figures, we plot the instantaneous or per teststep i, or test-case efficiency, and its arithmetic average over the covered metric space.
Reference: [Vou92] <author> M. A. Vouk, </author> <title> "Using Reliability Models During Testing with NonOperational Profiles," </title> <booktitle> Proc. 2nd Bellcore/Purdue workshop issues in Software Reliability Estimation, </booktitle> <month> Oct. </month> <year> 1992, </year> <pages> pp. 103-111. </pages>
Reference-contexts: Usually, the models combine the concepts of functional coverage or code coverage, with some practical testing strategy. We call these models coverage-based models since they imply that coverage guides the testing rather than some other criterion, such as failure intensity, and/or marketing deadlines <ref> [e.g., Gok96, Fra95, Bel95, Mal94, Vou92] </ref>. <p> For instance, faults that require at least two passes through a loop may not be detectable by a strategy S that requires the test cases to cover all branches only once. Consider the metric space (M-space) offered by a program P and schematically depicted in Figure 2.1. <ref> [Vou92, Riv95] </ref>. offered by a program P. Let each square represent a construct of the metric M (e.g. du-pair, a branch, a sub-path, a functionality, a test case). Let the dark areas represent faults detectable through execution of constructs of M .
Reference: [Wal78] <author> R. E. Walpole, R. H. </author> <title> Myers "Probability and Statistics for Engineers and Scientists," </title> <publisher> Macmillan Publishing Co., Inc., </publisher> <year> 1978. </year>
Reference-contexts: Malaya et al. [Mal92] analyzed the Fault Exposure Ratio [Mus87] to account for the fluctuation of per-fault detectability while developing their exponential and logarithmic coverage based models. The hyper-geometric probability mass function describes the process of sampling without replacement <ref> [e.g., Wal78] </ref>. Under our assumptions, the probability that test case T i detects x i defects is at least: p x i |u i , b i , h i ( ) = x i h i xi h i Where b i = g i d i . <p> The expected value x i of equation 2.4 from <ref> [Wal78] </ref> is x i = b i u i (2.5), which gives x i = g i d i ( ) h i since b i = g i d i .Then, b i takes into account the visibility/cross-section of defectives.
References-found: 28

