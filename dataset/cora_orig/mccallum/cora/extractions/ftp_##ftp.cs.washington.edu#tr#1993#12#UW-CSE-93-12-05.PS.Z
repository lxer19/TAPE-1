URL: ftp://ftp.cs.washington.edu/tr/1993/12/UW-CSE-93-12-05.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-title.html
Root-URL: 
Title: Concord: Re-Thinking the Division of Labor in a Distributed Shared Memory System  
Abstract: J. William Lee Department of Computer Science and Engineering University of Washington Technical Report 93-12-05 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. E. Bal and A. S. Tanenbaum. </author> <title> Distributed Programming with Shared Data. </title> <booktitle> In Proceedings of the IEEE CS 1988 International Conference on Computer Languages, </booktitle> <pages> pages 82-91, </pages> <month> Oct. </month> <year> 1988. </year>
Reference-contexts: These update messages are not acknowledged. This update-based protocol is correct. Because write acquire primitives invalidate other valid copies of a folder, the synchronization of folder primitives is ensured. Furthermore, this protocol avoids the pitfall described by Bal and Tanenbaum <ref> [1] </ref> because an update of a folder can only cause an event in the computation of a processor when a thread on the processor acquires the folder. When a folder has a stable sharing pattern, this update-based protocol can be more efficient than the invalidation-based protocol for two reasons.
Reference: [2] <author> B. N. Bershad, M. J. Zekauskas, and W. A. Sawdon. </author> <title> The Midway Distributed Shared Memory System. </title> <booktitle> In Proceedings of the 38th IEEE Computer Society International Conference (COMPCON '93), </booktitle> <pages> pages 528-537, </pages> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: The key in our approach is a careful division of responsibilities among the programmer, the compiler, and the runtime system. Previous distributed shared memory systems have explored the benefits of various divisions of responsibilities. Amber [4], Orca [17], and Midway <ref> [2] </ref> can avoid introducing false sharing by maintaining coherence at a granularity specified by the programmer: the programmer specifies the granularity of coherence either by defining objects [4, 17] or by associating locks with program data [2]. DASH [14], Munin [3], and Midway [2] reduce the overhead of maintaining coherence via <p> Amber [4], Orca [17], and Midway <ref> [2] </ref> can avoid introducing false sharing by maintaining coherence at a granularity specified by the programmer: the programmer specifies the granularity of coherence either by defining objects [4, 17] or by associating locks with program data [2]. DASH [14], Munin [3], and Midway [2] reduce the overhead of maintaining coherence via relaxed memory consistency models; these relaxed consistency models guarantee the shared memory to be consistent as long as the programmer writes correct programs, either by correctly synchronizing threads [7] or by correctly associating locks with program <p> Amber [4], Orca [17], and Midway <ref> [2] </ref> can avoid introducing false sharing by maintaining coherence at a granularity specified by the programmer: the programmer specifies the granularity of coherence either by defining objects [4, 17] or by associating locks with program data [2]. DASH [14], Munin [3], and Midway [2] reduce the overhead of maintaining coherence via relaxed memory consistency models; these relaxed consistency models guarantee the shared memory to be consistent as long as the programmer writes correct programs, either by correctly synchronizing threads [7] or by correctly associating locks with program data [2]. <p> [14], Munin [3], and Midway <ref> [2] </ref> reduce the overhead of maintaining coherence via relaxed memory consistency models; these relaxed consistency models guarantee the shared memory to be consistent as long as the programmer writes correct programs, either by correctly synchronizing threads [7] or by correctly associating locks with program data [2]. Following the direction of Amber, Orca, and Midway, our approach also maintains memory coherence at a granularity specified by the programmer and guarantees the shared memory to be consistent only for correct programs. <p> Defining objects in a parallel program to match the right size for high performance can be difficult, because objects are designed to represent logical entities in a program. Associating locks with program data can also complicate programming, because an error in the process may lead to inconsistent memory <ref> [2] </ref>. Our approach meets this challenge by carefully splitting the responsibilities: * To simplify programming, the runtime system and the compiler cooperate to make it easier for the programmer to partition shared data at fine granularity. <p> because when the DSM system is processing an acquire request, the system does not know when the next acquire will occur and if the system can process the next acquire in parallel. 2.2 The Consistency Model The consistency model supported by Concord, Concord consistency , is similar to entry consistency <ref> [2] </ref>: a thread is guaranteed to access the most up-to-date data in a folder after it has obtained the folder. <p> Below we compare our approach with Munin [3], Tread-Marks [12], and Midway <ref> [2] </ref>. We also discuss briefly the tradeoff between our approach and the approach of compiling data-parallel languages such as High Performance Fortran [9]. The latter has received considerable support from both academia and industry. Munin implements release consistency and employs multiple consistency protocols. <p> Nevertheless, it is difficult to assess the cost of managing logical time-stamps from the limited performance results published about Midway <ref> [2] </ref>. Unlike Midway, Concord attempts to simplify programming through the language, the folder abstraction, and the runtime error checking. The folder primitives include not only lock primitives, but also primitives for multiple producer-consumer type of synchronization. Concord always transfers a complete folder.
Reference: [3] <author> J. B. Carter, J. K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: Amber [4], Orca [17], and Midway [2] can avoid introducing false sharing by maintaining coherence at a granularity specified by the programmer: the programmer specifies the granularity of coherence either by defining objects [4, 17] or by associating locks with program data [2]. DASH [14], Munin <ref> [3] </ref>, and Midway [2] reduce the overhead of maintaining coherence via relaxed memory consistency models; these relaxed consistency models guarantee the shared memory to be consistent as long as the programmer writes correct programs, either by correctly synchronizing threads [7] or by correctly associating locks with program data [2]. <p> Below we compare our approach with Munin <ref> [3] </ref>, Tread-Marks [12], and Midway [2]. We also discuss briefly the tradeoff between our approach and the approach of compiling data-parallel languages such as High Performance Fortran [9]. The latter has received considerable support from both academia and industry. Munin implements release consistency and employs multiple consistency protocols.
Reference: [4] <author> J. S. Chase, F. Amador, E. D. Lazowska, H. M. Levy, and R. J. Littlefield. </author> <title> The Amber System: Parallel Programming on a Network of Multiprocessors. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 147-158, </pages> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: The key in our approach is a careful division of responsibilities among the programmer, the compiler, and the runtime system. Previous distributed shared memory systems have explored the benefits of various divisions of responsibilities. Amber <ref> [4] </ref>, Orca [17], and Midway [2] can avoid introducing false sharing by maintaining coherence at a granularity specified by the programmer: the programmer specifies the granularity of coherence either by defining objects [4, 17] or by associating locks with program data [2]. <p> Previous distributed shared memory systems have explored the benefits of various divisions of responsibilities. Amber [4], Orca [17], and Midway [2] can avoid introducing false sharing by maintaining coherence at a granularity specified by the programmer: the programmer specifies the granularity of coherence either by defining objects <ref> [4, 17] </ref> or by associating locks with program data [2]. <p> The synchronization of folder primitives is combined with the management of memory coherence, as described below. 2.5.2 Coherence Management Concord maintains memory coherence in two levels. First, Concord ensures the consistency of folder data structures. Each processor detects uninitialized folder data structures using an approach similar to Amber <ref> [4] </ref>. Second, through the consistent folder data structures, Concord maintains the replication and consistency of program data using both an invalidation-based and an update-based consistency protocol. Concord's invalidation-based consistency protocol is similar to existing invalidation-based consistency protocols. Each local folder has two states: valid and invalid. <p> But the update-based consistency protocol is not very useful for the three SPLASH benchmark applications because these applications are very dynamic. 4 Discussion and Related Work There is a large body of literatures about distributed shared memory systems <ref> [15, 14, 4, 17, 6] </ref>. Below we compare our approach with Munin [3], Tread-Marks [12], and Midway [2]. We also discuss briefly the tradeoff between our approach and the approach of compiling data-parallel languages such as High Performance Fortran [9].
Reference: [5] <author> J.-D. Choi and S. L. Min. </author> <title> Race Frontier: Reproducing Data Races in Parallel-Program Debugging. </title> <booktitle> In Proceedings of the third ACM SIGPLAN Symposium on the Principles and Practice of Parallel Programming, </booktitle> <pages> pages 145-154, </pages> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: Although Concord requires the programmer to do more work, Concord also helps the programmer in debugging programs. The runtime error checking can identify in some cases race conditions, a common type of program error that is often considered as one of the most important barriers in programming shared-memory programs <ref> [11, 5] </ref>. Thus it is difficult to determine if it is more difficult and how much more difficult it is to program in Concord than either in Munin or TreadMarks. Midway is probably the existing DSM system that is closest to our approach.
Reference: [6] <author> M. J. Feeley and H. M. Levy. </author> <title> Distributed Shared Memory with Versioned Objects. </title> <booktitle> In Proceedings of the 1992 Conference on Object-Oriented Programming: Systems, Languages, and Applications, </booktitle> <pages> pages 247-262, </pages> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: But the update-based consistency protocol is not very useful for the three SPLASH benchmark applications because these applications are very dynamic. 4 Discussion and Related Work There is a large body of literatures about distributed shared memory systems <ref> [15, 14, 4, 17, 6] </ref>. Below we compare our approach with Munin [3], Tread-Marks [12], and Midway [2]. We also discuss briefly the tradeoff between our approach and the approach of compiling data-parallel languages such as High Performance Fortran [9].
Reference: [7] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: DASH [14], Munin [3], and Midway [2] reduce the overhead of maintaining coherence via relaxed memory consistency models; these relaxed consistency models guarantee the shared memory to be consistent as long as the programmer writes correct programs, either by correctly synchronizing threads <ref> [7] </ref> or by correctly associating locks with program data [2]. Following the direction of Amber, Orca, and Midway, our approach also maintains memory coherence at a granularity specified by the programmer and guarantees the shared memory to be consistent only for correct programs.
Reference: [8] <author> D. Hensgen, R. Finkel, and U. Manber. </author> <title> Two Algorithms for Barrier Synchronization. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 17(1) </volume> <pages> 1-17, </pages> <year> 1988. </year>
Reference-contexts: In our implementation of barriers, threads on the same processor rendezvous with each other via a count variable. User-level thread systems on different processors rendezvous with each other in a way similar to the tournament barrier <ref> [8] </ref>. The synchronization of folder primitives is combined with the management of memory coherence, as described below. 2.5.2 Coherence Management Concord maintains memory coherence in two levels. First, Concord ensures the consistency of folder data structures. Each processor detects uninitialized folder data structures using an approach similar to Amber [4].
Reference: [9] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification. </title> <note> Version 1.0 DRAFT, </note> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: This approach is in contrast to the approach of High Performance Fortran <ref> [9] </ref>. Our approach attempts to provide the programmer with a few simple mechanisms for partitioning shared data. 2.4 The Compiler The High C compiler supports the extended language constructs and runtime error checking. Our prototype High C compiler is modified from gcc. <p> Below we compare our approach with Munin [3], Tread-Marks [12], and Midway [2]. We also discuss briefly the tradeoff between our approach and the approach of compiling data-parallel languages such as High Performance Fortran <ref> [9] </ref>. The latter has received considerable support from both academia and industry. Munin implements release consistency and employs multiple consistency protocols. It incorporates an update-based multiple-writer protocol that uses the diff operation to find out the updates within a page.
Reference: [10] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Preliminary Experiences with the Fortran D Compiler. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 338-350, </pages> <address> Portland, Oregon, </address> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: High Performance Fortran (HPF) is a data-parallel language. It augments Fortran with parallel loops and data distribution annotations. Programming in HPF can be easy because the program annotations are treated as hints. Reasonable performance has been achieved by compilers for relatively small and static data-parallel programs <ref> [10] </ref>. Achieving good performance for larger and more dynamic programs may be harder, because runtime analysis causes overhead and static analysis can be difficult for large programs due to interprocedure analysis and symbolic computation. Compared to HPF, our programming language, High C, may be harder to program for data-parallel applications.
Reference: [11] <author> A. H. Karp and R. G. B. </author> <title> II. A Comparison of 12 Parallel Fortran Dialects. </title> <journal> IEEE Software, </journal> <volume> 5(5) </volume> <pages> 52-66, </pages> <month> Sept. </month> <year> 1988. </year>
Reference-contexts: Although Concord requires the programmer to do more work, Concord also helps the programmer in debugging programs. The runtime error checking can identify in some cases race conditions, a common type of program error that is often considered as one of the most important barriers in programming shared-memory programs <ref> [11, 5] </ref>. Thus it is difficult to determine if it is more difficult and how much more difficult it is to program in Concord than either in Munin or TreadMarks. Midway is probably the existing DSM system that is closest to our approach.
Reference: [12] <author> P. Keleher, A. Cox, S. Dwarkadas, and W. Zwaenepoel. TreadMarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <booktitle> In Proceedings of the 1994 Winter USENIX Conference, </booktitle> <pages> pages 115-131, </pages> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: Below we compare our approach with Munin [3], Tread-Marks <ref> [12] </ref>, and Midway [2]. We also discuss briefly the tradeoff between our approach and the approach of compiling data-parallel languages such as High Performance Fortran [9]. The latter has received considerable support from both academia and industry. Munin implements release consistency and employs multiple consistency protocols. <p> This approach reduces the impact of false sharing, but eager update-based protocols may send more messages than necessary for dynamic programs. TreadMarks alleviates this problem by implementing release consistency lazily, reducing communication considerably for dynamic programs <ref> [12] </ref>. Compared with Concord, both Munin and Tread-Marks provide a programming model that is much closer to the conventional shared-memory programming model. But Concord may potentially achieve better performance because the programmer associates data with folders.
Reference: [13] <author> M. S. Lam and M. C. Rinard. </author> <title> Coarse-Grain Parallel Programming in Jade. </title> <booktitle> In Proceedings of the Third ACM SIGPLAN Symposium on the Principles and Practice of Parallel Programming, </booktitle> <pages> pages 94-105, </pages> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: The runtime system provides primitives to check if accesses to a region of memory are allowed, and the compiler provides an option to insert a check before every access to shared memory. Runtime error checking has been proposed before <ref> [13] </ref>, but we believe that we are the first to implement it in a general DSM system and report experience of using it for real applications. * To further improve performance, the runtime system incorporates a new update-based consistency protocol and coalesces coherence messages using information from the programmer.
Reference: [14] <author> D. Lenoski, J. Laudon, T. Joe, D. Nakahira, L. Stevens, A. Gupta, and J. Hennessy. </author> <title> The DASH Prototype: Implementation and Performance. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 92-103, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Amber [4], Orca [17], and Midway [2] can avoid introducing false sharing by maintaining coherence at a granularity specified by the programmer: the programmer specifies the granularity of coherence either by defining objects [4, 17] or by associating locks with program data [2]. DASH <ref> [14] </ref>, Munin [3], and Midway [2] reduce the overhead of maintaining coherence via relaxed memory consistency models; these relaxed consistency models guarantee the shared memory to be consistent as long as the programmer writes correct programs, either by correctly synchronizing threads [7] or by correctly associating locks with program data [2]. <p> The performance is measured on an Intel iPSC/2. The iPSC/2 we used is a relatively slow machine. Its 386-based processor ranks about 0.3 MFLOPS and 2-3 VAX MIPS. A round trip short message on the iPSC/2 takes about 700 microseconds. Compared to DASH <ref> [14] </ref>, the iPSC/2 processor is about 10-30 times slower. Sending a round trip short message on the iPSC/2 is about 170 times slower than fetching a remote cache line on DASH in the worst case. As shown in the figure, all applications achieve good speedup except LocusRoute. <p> Compared with the measured runs of these three applications on the DASH application reduction in Reduction in # of messages runtime Red/Black SOR 71% 10.1% Barnes-Hut 34% 8.4% LocusRoute 1.4% 1.4% Table 3: The advantage of using asynchronous acquire primitives prototype <ref> [14] </ref>, our measured runs used the same problem sizes for Water and LocusRoute, and a smaller problem size for Barnes-Hut. <p> But the update-based consistency protocol is not very useful for the three SPLASH benchmark applications because these applications are very dynamic. 4 Discussion and Related Work There is a large body of literatures about distributed shared memory systems <ref> [15, 14, 4, 17, 6] </ref>. Below we compare our approach with Munin [3], Tread-Marks [12], and Midway [2]. We also discuss briefly the tradeoff between our approach and the approach of compiling data-parallel languages such as High Performance Fortran [9].
Reference: [15] <author> K. Li and P. Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> Nov. </month> <year> 1989. </year>
Reference-contexts: But the update-based consistency protocol is not very useful for the three SPLASH benchmark applications because these applications are very dynamic. 4 Discussion and Related Work There is a large body of literatures about distributed shared memory systems <ref> [15, 14, 4, 17, 6] </ref>. Below we compare our approach with Munin [3], Tread-Marks [12], and Midway [2]. We also discuss briefly the tradeoff between our approach and the approach of compiling data-parallel languages such as High Performance Fortran [9].
Reference: [16] <author> J. P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: accesses to a range of memory against this list of folders, in the same way as marshaling and unmarshaling folders. 3 Performance This section evaluates the performance of Concord and reports our experience of using Concord to program applications, including three applications from the SPLASH benchmark: Water, Barnes-Hut, and LocusRoute <ref> [16] </ref>. These three applications are real scientific and engineering applications, as claimed by their creators. Two of them, Barnes-Hut and Locus-Route, have irregular data structures. They represent the kind of applications for which it is very difficult to write message passing programs.
Reference: [17] <author> A. S. Tanenbaum, M. F. Kaashoek, and H. E. Bal. </author> <title> Parallel Programming Using Shared Objects and Broadcasting. </title> <booktitle> Computer, </booktitle> <pages> pages 10-19, </pages> <month> Aug. </month> <year> 1992. </year> <month> 8 </month>
Reference-contexts: The key in our approach is a careful division of responsibilities among the programmer, the compiler, and the runtime system. Previous distributed shared memory systems have explored the benefits of various divisions of responsibilities. Amber [4], Orca <ref> [17] </ref>, and Midway [2] can avoid introducing false sharing by maintaining coherence at a granularity specified by the programmer: the programmer specifies the granularity of coherence either by defining objects [4, 17] or by associating locks with program data [2]. <p> Previous distributed shared memory systems have explored the benefits of various divisions of responsibilities. Amber [4], Orca [17], and Midway [2] can avoid introducing false sharing by maintaining coherence at a granularity specified by the programmer: the programmer specifies the granularity of coherence either by defining objects <ref> [4, 17] </ref> or by associating locks with program data [2]. <p> But the update-based consistency protocol is not very useful for the three SPLASH benchmark applications because these applications are very dynamic. 4 Discussion and Related Work There is a large body of literatures about distributed shared memory systems <ref> [15, 14, 4, 17, 6] </ref>. Below we compare our approach with Munin [3], Tread-Marks [12], and Midway [2]. We also discuss briefly the tradeoff between our approach and the approach of compiling data-parallel languages such as High Performance Fortran [9].
References-found: 17

