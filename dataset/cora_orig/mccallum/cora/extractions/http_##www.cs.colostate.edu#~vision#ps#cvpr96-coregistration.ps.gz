URL: http://www.cs.colostate.edu/~vision/ps/cvpr96-coregistration.ps.gz
Refering-URL: http://www.cs.colostate.edu/~vision/html/publications.html
Root-URL: 
Email: schwicke/ross@cs.colostate.edu  
Title: Coregistration of Range and Optical Images Using Coplanarity and Orientation Constraints  
Author: Anthony N. A. Schwickerath J. Ross Beveridge 
Address: Fort Collins, Colorado 80523  
Affiliation: Computer Science Department Colorado State University  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. K. Aggarwal. </author> <title> Multisensor Fusion for Automatic Scene Interpretation. </title> <editor> In Ramesh C. Jain and Anil K. Jain, editors, </editor> <title> Analysis and Interpretation of Range Images, chapter 8. </title> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: However, For reasons of efficiency we have intentionally used what Hel-Or and Werman call the 'parametric' approach. Future work will test the relative merits of the 'parametric' versus Kalman filter approaches. Coregistration is a form of model-based sensor fu sion. In summarizing past sensor fusion work, Ag--garwal <ref> [1] </ref> makes two points particularly relevant to this paper. Aggarwal notes that past work on sensor fusion emphasized single modality sensors, with comparatively little work on different sensor modalities [21, 27]. He further states that to properly perform mixed-modality sensor fusion, coordinate transformations between images need to be adaptively determined. <p> The second, E fit;r , is simply the sum-of-squared Euclidean distances between corresponding model and range points. The weighting term 0 ff fit 1 controls the relative importance of the optical and the range data. The terms E fit;o and E fit;r are normalized between <ref> [0; 1] </ref> based upon the expected amount of noise present in the features 2 , and consequently E fit also falls in this range. This normalization allows comparison of data from two separate sources. <p> Given the Gaussian noise assumption, this threshold can be thought of in terms of the standard deviation, since only 4% of the data will fall more than 2 standard deviations from the true value. While this assumption does not absolutely place E fit;o in the range <ref> [0; 1] </ref>, this will generally be the case. For range data, 3D Cartesian points are formed by back-projecting range pixels into the scene. The model-to-range fit error, E fit;r , is defined to be the squared Euclidean distance between each back-projected range point and its corresponding model point. <p> This is desirable, since increasing the nonlinearity of a system of equations tends to increase the instability of the solution. Again, we use a weighted form based on a threshold t mr , which keeps us generally in the range <ref> [0; 1] </ref>. If we treat R mo as a 3 fi 3 rotation matrix, solving for R mo by minimizing Equation 1 and allowing all 9 terms to vary independently violates the constraint that R mo be a rotation matrix.
Reference: [2] <author> Anthony N. A. Schwickerath and J. Ross Beveridge. </author> <title> Coreg-istering 3D Models, Range, and Optical Imagery Using Least-Median Squares Fitting. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 719-722, </pages> <address> Los Altos, CA, </address> <month> February </month> <year> 1996. </year> <title> ARPA, </title> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: The range pixels in the LADAR images serve as range point features. For the CCD data, a steerable filter and a global edge fitting procedure is used to find locally optimal straight edge segments [22]. How these features are derived is further explained in <ref> [28, 22, 2] </ref>. A coplanarity constraint limits the freedom of movement of the range sensor relative to the optical sensor. Thus, the range reference coordinate system may translate in the common x-y image plane of the two sensors, but otherwise the two sensors move together.
Reference: [3] <author> J. Ross Beveridge. </author> <title> Local Search Algorithms for Geometric Obejct Recognition: Optimal Correspondence and Pose. </title> <type> PhD thesis, </type> <institution> University of Massachuesetts at Amherst, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: S. Army Research Office. Least-squares techniques are sensitive to outliers. Median-filtering more robustly estimates the true coregistration and is compared with a local search matching capability <ref> [3, 4] </ref>. Results are presented for both controlled synthetic and real data. 2 Background & Motivation A long tradition of work on object recognition has emphasized finding matches between object and image features such that there is a single globally consistent alignment of features. <p> Lowe [20], Huttenlocher [15], Grimson [10], and we <ref> [5, 6, 3, 4] </ref> have all proposed system within this paradigm. Our work here provides an example of extending these methods to multiple constrained sensors. Others have worked on problems similar to the coregistration problem discussed in this paper. <p> Unsuccessful termination occurs if the total number of iterations exceeds a maximum number of iterations. The Levenberg-Marquardt [24] method has been found to be robust in our past single sensor pose work <ref> [3] </ref>, and it is used here to find the optimal pose+registration parameters. 4 Building Matches: Median Filtering and Local Search The coregistration method above requires a matching between model and image features. As a least-squares method, it requires that this matching be free of outliers. <p> Using only this measure, we would soon find that the optimal correspondence consisted of a small number of correspondences with zero total fit error. As a result, we also need to consider another error term which offers a penalty for not explaining model features. As in our previous work <ref> [3] </ref>, we combine the omission error (E om ) with the fit error to form a total match error per sensor of E match;sensor = E fit;sensor + E om;sensor (23) and the total match error for the suite of E match = ff match E match;o +(1ff match )E match;r <p> While other, more complete neighborhoods, such as 1- and 2- Hamming distance, have been proposed, this satisfies the tractability requirements of a first pass. The steepest descent transition is made after every possible move from the current state has been evaluated, as in <ref> [3] </ref>. When no neighbors of the current state offer improvement, a local optima has been reached and the search terminates. 5 Empirical Results Two empirical studies are presented. The first is a controlled sensitivity study on synthetic data designed to test the robustness and accuracy of the algorithm. <p> We set the omission attenuation for the CCD data at a mo = 0:5 based upon our previous work <ref> [3] </ref>. Since we lack specific knowledge for the loss of LADAR data, we use a mr = 1:0 for a linear omission response.
Reference: [4] <author> J. Ross Beveridge and Edward M. Riseman. </author> <title> Optimal Geo--metric Model Matching Under Full 3D Perspective. Computer Vision and Image Understanding, </title> <note> 61(3):351 - 364, 1995. (short version in IEEE Second CAD-Based Vision Workshop). </note>
Reference-contexts: S. Army Research Office. Least-squares techniques are sensitive to outliers. Median-filtering more robustly estimates the true coregistration and is compared with a local search matching capability <ref> [3, 4] </ref>. Results are presented for both controlled synthetic and real data. 2 Background & Motivation A long tradition of work on object recognition has emphasized finding matches between object and image features such that there is a single globally consistent alignment of features. <p> Lowe [20], Huttenlocher [15], Grimson [10], and we <ref> [5, 6, 3, 4] </ref> have all proposed system within this paradigm. Our work here provides an example of extending these methods to multiple constrained sensors. Others have worked on problems similar to the coregistration problem discussed in this paper.
Reference: [5] <author> J. Ross Beveridge, Rich Weiss, and Edward M. Riseman. </author> <title> Combinatorial Optimization Applied to Variable Scale 2D Model Matching. </title> <booktitle> In Proceedings of the IEEE International Conference on Pattern Recognition 1990, Atlantic City, </booktitle> <pages> pages 18 - 23. </pages> <publisher> IEEE, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: Lowe [20], Huttenlocher [15], Grimson [10], and we <ref> [5, 6, 3, 4] </ref> have all proposed system within this paradigm. Our work here provides an example of extending these methods to multiple constrained sensors. Others have worked on problems similar to the coregistration problem discussed in this paper.
Reference: [6] <author> J. Ross Beveridge, Rich Weiss, and Edward M. Rise-man. </author> <title> Optimization of 2-Dimensional Model Matching. </title> <editor> In Hatem Nasr, editor, </editor> <booktitle> Selected Papers on Automatic Object Recognition (originally appeared in DARPA Image Understanding Workshop, 1989), SPIE Milestone Series. SPIE, </booktitle> <address> Bellingham, WA, </address> <year> 1991. </year>
Reference-contexts: Lowe [20], Huttenlocher [15], Grimson [10], and we <ref> [5, 6, 3, 4] </ref> have all proposed system within this paradigm. Our work here provides an example of extending these methods to multiple constrained sensors. Others have worked on problems similar to the coregistration problem discussed in this paper.
Reference: [7] <author> James E. Bevington. </author> <title> Laser Radar ATR Algorithms: Phase III Final Report. </title> <type> Technical report, </type> <institution> Alliant Techsystems, Inc., </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: Coregistration will support multi-sensor object verification but is not itself an object recognition tool. It presupposes a prior system has hypothesized the presence of specific objects and provided approximate positions and orientations relative to the sensor suite. Others have already undertaken this hypothesis generation work <ref> [7, 29, 8, 16] </ref>. As in previous pose determination work [9, 19], the best pose+registration parameters jointly minimize a quadratic, non-linear function.
Reference: [8] <author> Richard L. Delanoy, Jacques G. Verly, and Dan E. Dudgeon. </author> <title> Machine Intelligent Automatic Recognition of Critical Mobile Targets in Laser Radar Imagery. </title> <journal> The Lincoln Laboratory Journal, </journal> <volume> 6(1) </volume> <pages> 161-186, </pages> <month> Spring </month> <year> 1993. </year>
Reference-contexts: Coregistration will support multi-sensor object verification but is not itself an object recognition tool. It presupposes a prior system has hypothesized the presence of specific objects and provided approximate positions and orientations relative to the sensor suite. Others have already undertaken this hypothesis generation work <ref> [7, 29, 8, 16] </ref>. As in previous pose determination work [9, 19], the best pose+registration parameters jointly minimize a quadratic, non-linear function.
Reference: [9] <author> R. O. Eason and R. C. Gonzalez. </author> <title> Least-Squares Fusion of Multisensory Data. </title> <editor> In Mongi A. Abidi and Rafael C. Gonzalez, editors, </editor> <booktitle> Data Fusion in Robotics and Machine Intelligence, chapter 9. </booktitle> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: 1 Introduction Coregistration describes a process which simultaneously refines both the estimated 3D pose of an object relative to a sensor suite as well as the registration parameters relating the coordinate systems of a range sensor and an optical sensor. It extends single sensor pose work <ref> [14, 17, 9, 19] </ref> by imposing contraints on both sensor and object geometry. Coregistration will support multi-sensor object verification but is not itself an object recognition tool. It presupposes a prior system has hypothesized the presence of specific objects and provided approximate positions and orientations relative to the sensor suite. <p> It presupposes a prior system has hypothesized the presence of specific objects and provided approximate positions and orientations relative to the sensor suite. Others have already undertaken this hypothesis generation work [7, 29, 8, 16]. As in previous pose determination work <ref> [9, 19] </ref>, the best pose+registration parameters jointly minimize a quadratic, non-linear function. <p> Others have worked on problems similar to the coregistration problem discussed in this paper. Her-bert [11] presents a least-squares mechanism for computing the 2D registration between a range and color CCD sensor based upon corresponding image points in the two sensor images. Both Eason <ref> [9] </ref> and Hel-Or [13] develop least-squares multisensor pose algorithms. However, they do not support simultaneous sensor registration. In terms of constraints, all of these methods assume a known sensor-to-sensor registration.
Reference: [10] <author> W. Eric L. </author> <title> Grimson. Object Recognition by Computer: The Role of Geometric Constraints. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: Results are presented for both controlled synthetic and real data. 2 Background & Motivation A long tradition of work on object recognition has emphasized finding matches between object and image features such that there is a single globally consistent alignment of features. Lowe [20], Huttenlocher [15], Grimson <ref> [10] </ref>, and we [5, 6, 3, 4] have all proposed system within this paradigm. Our work here provides an example of extending these methods to multiple constrained sensors. Others have worked on problems similar to the coregistration problem discussed in this paper.
Reference: [11] <author> Martial Hebert, Takeo Kanade, and InSo Kweon. </author> <title> 3-D Vision Techniques for Autonomous Vehicles. </title> <editor> In Ramesh C. Jain and Anil K. Jain, editors, </editor> <title> Analysis and Interpretation of Range Images, chapter 7. </title> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: Lowe [20], Huttenlocher [15], Grimson [10], and we [5, 6, 3, 4] have all proposed system within this paradigm. Our work here provides an example of extending these methods to multiple constrained sensors. Others have worked on problems similar to the coregistration problem discussed in this paper. Her-bert <ref> [11] </ref> presents a least-squares mechanism for computing the 2D registration between a range and color CCD sensor based upon corresponding image points in the two sensor images. Both Eason [9] and Hel-Or [13] develop least-squares multisensor pose algorithms. However, they do not support simultaneous sensor registration.
Reference: [12] <author> Y. Hel-or. </author> <title> Model Based Pose Estimation from Uncertain Data. </title> <type> PhD thesis, </type> <institution> Hebrew Univeristy in Jerusalem, </institution> <year> 1993. </year>
Reference-contexts: Both Eason [9] and Hel-Or [13] develop least-squares multisensor pose algorithms. However, they do not support simultaneous sensor registration. In terms of constraints, all of these methods assume a known sensor-to-sensor registration. More recent work by Hel-Or and Werman <ref> [30, 12] </ref> adds degrees of freedom to account for articulated objects and handles variable constraints in a single extended Kalman filter formulation. Their general Kalman filter approach could be applied to the coreg-istration problem formulated here.
Reference: [13] <author> Y. Hel-Or and M. Werman. </author> <title> Absolute Orientation from Uncertain Data: A Unified Approach. </title> <booktitle> In Proceedings: Computer Vision and Pattern Recognition, </booktitle> <pages> pages 77 - 82. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> June </month> <year> 1993. </year>
Reference-contexts: Others have worked on problems similar to the coregistration problem discussed in this paper. Her-bert [11] presents a least-squares mechanism for computing the 2D registration between a range and color CCD sensor based upon corresponding image points in the two sensor images. Both Eason [9] and Hel-Or <ref> [13] </ref> develop least-squares multisensor pose algorithms. However, they do not support simultaneous sensor registration. In terms of constraints, all of these methods assume a known sensor-to-sensor registration.
Reference: [14] <author> B. K. P. Horn. </author> <title> Robot Vision. </title> <publisher> The MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: 1 Introduction Coregistration describes a process which simultaneously refines both the estimated 3D pose of an object relative to a sensor suite as well as the registration parameters relating the coordinate systems of a range sensor and an optical sensor. It extends single sensor pose work <ref> [14, 17, 9, 19] </ref> by imposing contraints on both sensor and object geometry. Coregistration will support multi-sensor object verification but is not itself an object recognition tool. It presupposes a prior system has hypothesized the presence of specific objects and provided approximate positions and orientations relative to the sensor suite. <p> The lack of a rotation parameter between mc mapped points and the range sensor coordinate system constrains the sensor-to-sensor orientation. Notice that these constraints retain the same degree of nonlinearity (degree 2) found in the original Ku-mar [17, 19] and Horn <ref> [14] </ref> equations. This is desirable, since increasing the nonlinearity of a system of equations tends to increase the instability of the solution. Again, we use a weighted form based on a threshold t mr , which keeps us generally in the range [0; 1].
Reference: [15] <author> Daniel P. Huttenlocher and Shimon Ullman. </author> <title> Recognizing Solid Objects by Alignment with an Image. </title> <journal> International Journal of Computer Vision, </journal> <volume> 5(2):195 - 212, </volume> <month> November </month> <year> 1990. </year>
Reference-contexts: Results are presented for both controlled synthetic and real data. 2 Background & Motivation A long tradition of work on object recognition has emphasized finding matches between object and image features such that there is a single globally consistent alignment of features. Lowe [20], Huttenlocher <ref> [15] </ref>, Grimson [10], and we [5, 6, 3, 4] have all proposed system within this paradigm. Our work here provides an example of extending these methods to multiple constrained sensors. Others have worked on problems similar to the coregistration problem discussed in this paper.
Reference: [16] <author> Alexander Akerman III, Ronald Patton, Walter H. Delash-mit, and Robert Hummel. </author> <title> Multisensor fusion using FLIR and LADAR identification. </title> <type> Technical Report NRC-TR-94-052, </type> <institution> Nichols Research Corporation, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: Coregistration will support multi-sensor object verification but is not itself an object recognition tool. It presupposes a prior system has hypothesized the presence of specific objects and provided approximate positions and orientations relative to the sensor suite. Others have already undertaken this hypothesis generation work <ref> [7, 29, 8, 16] </ref>. As in previous pose determination work [9, 19], the best pose+registration parameters jointly minimize a quadratic, non-linear function.
Reference: [17] <author> Rakesh Kumar. </author> <title> Determination of Camera Location and Orientation. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 870 - 881, </pages> <address> Los Altos, CA, </address> <month> June </month> <year> 1989. </year> <title> DARPA, </title> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: 1 Introduction Coregistration describes a process which simultaneously refines both the estimated 3D pose of an object relative to a sensor suite as well as the registration parameters relating the coordinate systems of a range sensor and an optical sensor. It extends single sensor pose work <ref> [14, 17, 9, 19] </ref> by imposing contraints on both sensor and object geometry. Coregistration will support multi-sensor object verification but is not itself an object recognition tool. It presupposes a prior system has hypothesized the presence of specific objects and provided approximate positions and orientations relative to the sensor suite. <p> The first term, E fit;o , measures distance be tween corresponding optical and model features. This term is precisely the point-to-plane error criterion defined by Kumar <ref> [17, 19] </ref> for computing camera-to-model pose 1 . The second, E fit;r , is simply the sum-of-squared Euclidean distances between corresponding model and range points. The weighting term 0 ff fit 1 controls the relative importance of the optical and the range data. <p> The weighting term oi is typically 1, but can be used to bias some features over others. For example, oi can be used to weight lines based on inverse distance to perform a normalization similar to that considered in Kumar's later measures <ref> [17, 19] </ref>. The t mo in the normalizing weight for optical fit, w fit;o , represents the maximum anticipated error. <p> The lack of a rotation parameter between mc mapped points and the range sensor coordinate system constrains the sensor-to-sensor orientation. Notice that these constraints retain the same degree of nonlinearity (degree 2) found in the original Ku-mar <ref> [17, 19] </ref> and Horn [14] equations. This is desirable, since increasing the nonlinearity of a system of equations tends to increase the instability of the solution. Again, we use a weighted form based on a threshold t mr , which keeps us generally in the range [0; 1]. <p> While the matrix terms (r mc 1;1 . . . r mc 3;3 ) could be constructed in such a way as to allow only rigid rotations, this would increase the degree of nonlinearity in the equation. Ku-mar <ref> [17, 19] </ref> suggests a better approach: Rodriguez's formula, which is an approximation appropriate for small rotations.
Reference: [18] <author> Rakesh Kumar. </author> <title> Model Dependent Inference of 3D Information From a Sequence of 2D Images. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, Amherst, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: The subsets need to be at least large enough to cover the degrees of freedom, so at least 3 optical lines and 1 range point are needed. However, Kumar <ref> [18] </ref> found that selecting a minimal number of features caused the solution to be sensitive to the Gaussian noise that we assume is overlaid onto the true data. As a consequence, it is better to select a larger subset to stabilize the optimal pose against noise.
Reference: [19] <author> Rakesh Kumar and Allen R. Hanson. </author> <title> Robust methods for estimating pose and a sensitivity analysis. </title> <booktitle> CVGIP:Image Understanding, </booktitle> <volume> 11, </volume> <year> 1994. </year>
Reference-contexts: 1 Introduction Coregistration describes a process which simultaneously refines both the estimated 3D pose of an object relative to a sensor suite as well as the registration parameters relating the coordinate systems of a range sensor and an optical sensor. It extends single sensor pose work <ref> [14, 17, 9, 19] </ref> by imposing contraints on both sensor and object geometry. Coregistration will support multi-sensor object verification but is not itself an object recognition tool. It presupposes a prior system has hypothesized the presence of specific objects and provided approximate positions and orientations relative to the sensor suite. <p> It presupposes a prior system has hypothesized the presence of specific objects and provided approximate positions and orientations relative to the sensor suite. Others have already undertaken this hypothesis generation work [7, 29, 8, 16]. As in previous pose determination work <ref> [9, 19] </ref>, the best pose+registration parameters jointly minimize a quadratic, non-linear function. <p> As in previous pose determination work [9, 19], the best pose+registration parameters jointly minimize a quadratic, non-linear function. An iterative technique solves for the 6 pose and 2 registration parameters which jointly minimize a weighted sum of Kumar's <ref> [19] </ref> measure for optical features and squared 3D Euclidean distance for range data. fl This work was sponsored by the Advanced Research Projects Agency (ARPA) under grant DAAH04-93-G-422, monitored by the U. S. Army Research Office. Least-squares techniques are sensitive to outliers. <p> The first term, E fit;o , measures distance be tween corresponding optical and model features. This term is precisely the point-to-plane error criterion defined by Kumar <ref> [17, 19] </ref> for computing camera-to-model pose 1 . The second, E fit;r , is simply the sum-of-squared Euclidean distances between corresponding model and range points. The weighting term 0 ff fit 1 controls the relative importance of the optical and the range data. <p> The weighting term oi is typically 1, but can be used to bias some features over others. For example, oi can be used to weight lines based on inverse distance to perform a normalization similar to that considered in Kumar's later measures <ref> [17, 19] </ref>. The t mo in the normalizing weight for optical fit, w fit;o , represents the maximum anticipated error. <p> The lack of a rotation parameter between mc mapped points and the range sensor coordinate system constrains the sensor-to-sensor orientation. Notice that these constraints retain the same degree of nonlinearity (degree 2) found in the original Ku-mar <ref> [17, 19] </ref> and Horn [14] equations. This is desirable, since increasing the nonlinearity of a system of equations tends to increase the instability of the solution. Again, we use a weighted form based on a threshold t mr , which keeps us generally in the range [0; 1]. <p> While the matrix terms (r mc 1;1 . . . r mc 3;3 ) could be constructed in such a way as to allow only rigid rotations, this would increase the degree of nonlinearity in the equation. Ku-mar <ref> [17, 19] </ref> suggests a better approach: Rodriguez's formula, which is an approximation appropriate for small rotations.
Reference: [20] <author> David G. Lowe. </author> <title> Fitting Parameterized Three-Dimensional Models to Images. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(5):441 - 450, </volume> <month> May </month> <year> 1991. </year>
Reference-contexts: Results are presented for both controlled synthetic and real data. 2 Background & Motivation A long tradition of work on object recognition has emphasized finding matches between object and image features such that there is a single globally consistent alignment of features. Lowe <ref> [20] </ref>, Huttenlocher [15], Grimson [10], and we [5, 6, 3, 4] have all proposed system within this paradigm. Our work here provides an example of extending these methods to multiple constrained sensors. Others have worked on problems similar to the coregistration problem discussed in this paper.
Reference: [21] <author> M. J. Magee, B. A. Boyter, C. H. Chien, and J. K. Ag-garwal. </author> <title> Experiments in Intensity Guided Range Sensing Recognition of Three-Dimensional Objects. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 7(6):629 - 637, </volume> <month> November </month> <year> 1985. </year>
Reference-contexts: Coregistration is a form of model-based sensor fu sion. In summarizing past sensor fusion work, Ag--garwal [1] makes two points particularly relevant to this paper. Aggarwal notes that past work on sensor fusion emphasized single modality sensors, with comparatively little work on different sensor modalities <ref> [21, 27] </ref>. He further states that to properly perform mixed-modality sensor fusion, coordinate transformations between images need to be adaptively determined.
Reference: [22] <author> Mark R. Stevens and J. Ross Beveridge. </author> <title> Optical Linear Feature Detection Based on Model Pose. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 695-697, </pages> <address> Los Altos, CA, </address> <month> February </month> <year> 1996. </year> <title> ARPA, </title> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: The range pixels in the LADAR images serve as range point features. For the CCD data, a steerable filter and a global edge fitting procedure is used to find locally optimal straight edge segments <ref> [22] </ref>. How these features are derived is further explained in [28, 22, 2]. A coplanarity constraint limits the freedom of movement of the range sensor relative to the optical sensor. <p> The range pixels in the LADAR images serve as range point features. For the CCD data, a steerable filter and a global edge fitting procedure is used to find locally optimal straight edge segments [22]. How these features are derived is further explained in <ref> [28, 22, 2] </ref>. A coplanarity constraint limits the freedom of movement of the range sensor relative to the optical sensor. Thus, the range reference coordinate system may translate in the common x-y image plane of the two sensors, but otherwise the two sensors move together.
Reference: [23] <author> Christos H. Papadimitriou and Kenneth Steiglitz. </author> <title> Combinatorial Optimization: Algorithms and Complexity, chapter Local Search, </title> <booktitle> pages 454 - 480. </booktitle> <publisher> Prentice-Hall, </publisher> <address> Engle-wood Cliffs, NJ, </address> <year> 1982. </year>
Reference-contexts: The brute force expansion of all possible combinations of correspondences, while robust, is also combinatorially explosive. Applying an alternative such as local search <ref> [23] </ref> will find a locally optimal solution. While this makes no guarantees of globality, we can gain confidence in the globality of the optima through multiple search restarts.
Reference: [24] <author> William H. Press, Brian P. Flannery, Saul A. Teukolsky, and William T. Vetterling. </author> <title> Numerical Recipes in C. </title> <publisher> Cam-bridge University Press, </publisher> <address> Cambridge, </address> <year> 1988. </year>
Reference-contexts: The constants in Table 1 are recomputed each time through the loop. The algorithm converges when the amount by which E fit drops between successive iterations falls below a preset threshold. Unsuccessful termination occurs if the total number of iterations exceeds a maximum number of iterations. The Levenberg-Marquardt <ref> [24] </ref> method has been found to be robust in our past single sensor pose work [3], and it is used here to find the optimal pose+registration parameters. 4 Building Matches: Median Filtering and Local Search The coregistration method above requires a matching between model and image features.
Reference: [25] <author> Peter J. Rousseeuw and Annick M. Leroy. </author> <title> Robust Regression and Outlier Detection. </title> <publisher> Wiley, </publisher> <year> 1987. </year>
Reference-contexts: As a least-squares method, it requires that this matching be free of outliers. Below, two methods for constructing outlier-free matches are presented. 4.1 Median Filtering Median filtering <ref> [25] </ref> handles outliers by fitting to the subset of the data which minimizes the ensemble median error value. It is a robust statistic when there are less than 50% outliers.
Reference: [26] <author> Anthony N. A. Schwickerath and J. Ross Beveridge. </author> <title> Model to Multisensor Coregistration with Eight Degrees of Freedom. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 481 - 490, </pages> <address> Los Altos, CA, </address> <month> November </month> <year> 1994. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: These partial derivatives are shown in <ref> [26] </ref>. In these equations, we simplify these by rewriting the weighting terms as w mo = ff fit w fit;o and w mr = (1 ff fit )w fit;r . The following matrix M moi is introduced to simplify the expressions. <p> Both tests were run on four synthetic models. The models exhibit different geometric characteristics including planarity or lack of planarity, symmetry or lack of symmetry, and few versus many features. Complete results of these tests are reported in <ref> [26] </ref>. In Test I, we found that, given perfect image data, the coregistration algorithm could reliably recover correct pose+registration given up to a 30 ffi error in orientation. The correct solution was often found even orientation errors as large as 50 ffi and initial translation errors up to 100 meters. <p> With significantly higher errors, though, ( = 5 for both sensors), coregistration yielded a final rotation error around 5 ffi . 3 The weights are the combined threshold and ff fit term described in <ref> [26] </ref> 5.2 Real Data In our previous work [26], instabilities and pathological behavior were found when running coregistra-tion on hand-picked features. This behavior has been traced to outliers present in the hand-picked data. To address this issue, median filtering and local search are used to construct outlier-free correspondences. <p> With significantly higher errors, though, ( = 5 for both sensors), coregistration yielded a final rotation error around 5 ffi . 3 The weights are the combined threshold and ff fit term described in <ref> [26] </ref> 5.2 Real Data In our previous work [26], instabilities and pathological behavior were found when running coregistra-tion on hand-picked features. This behavior has been traced to outliers present in the hand-picked data. To address this issue, median filtering and local search are used to construct outlier-free correspondences. In the following tests, the pose+registration parameters are held constant.
Reference: [27] <author> A. Stentz and Y. </author> <title> Goto. </title> <booktitle> The CMU Navigational Architecture. In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 440-446, </pages> <address> Los Angeles, CA, </address> <month> February </month> <year> 1987. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Coregistration is a form of model-based sensor fu sion. In summarizing past sensor fusion work, Ag--garwal [1] makes two points particularly relevant to this paper. Aggarwal notes that past work on sensor fusion emphasized single modality sensors, with comparatively little work on different sensor modalities <ref> [21, 27] </ref>. He further states that to properly perform mixed-modality sensor fusion, coordinate transformations between images need to be adaptively determined.
Reference: [28] <author> Mark R. Stevens. </author> <title> Obtaining 3D Shilhouettes and Sampled Surfaces from Solid Models for use in Computer Vision. </title> <type> Master's thesis, </type> <institution> Colorado State Univeristy, Fort Collins, Colorado, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: The range pixels in the LADAR images serve as range point features. For the CCD data, a steerable filter and a global edge fitting procedure is used to find locally optimal straight edge segments [22]. How these features are derived is further explained in <ref> [28, 22, 2] </ref>. A coplanarity constraint limits the freedom of movement of the range sensor relative to the optical sensor. Thus, the range reference coordinate system may translate in the common x-y image plane of the two sensors, but otherwise the two sensors move together.
Reference: [29] <author> J. G. Verly, D. E. Dudgeon, and R. T. Lacoss. </author> <title> Progress Report on the Development of the Automatic Target Recognition System for the UGV/RSTA LADAR. </title> <type> Technical Report 1006, </type> <institution> Massachusetts Institute of Technology, Lincoln Laboratory, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: Coregistration will support multi-sensor object verification but is not itself an object recognition tool. It presupposes a prior system has hypothesized the presence of specific objects and provided approximate positions and orientations relative to the sensor suite. Others have already undertaken this hypothesis generation work <ref> [7, 29, 8, 16] </ref>. As in previous pose determination work [9, 19], the best pose+registration parameters jointly minimize a quadratic, non-linear function.
Reference: [30] <author> Y. Hel-Or and M. Werman. </author> <title> Constraint-Fusion for Interpretation of Articulated Objects. </title> <booktitle> In Proceedings: Computer Vision and Pattern Recognition, </booktitle> <pages> pages 39 - 45. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> June </month> <year> 1994. </year>
Reference-contexts: Both Eason [9] and Hel-Or [13] develop least-squares multisensor pose algorithms. However, they do not support simultaneous sensor registration. In terms of constraints, all of these methods assume a known sensor-to-sensor registration. More recent work by Hel-Or and Werman <ref> [30, 12] </ref> adds degrees of freedom to account for articulated objects and handles variable constraints in a single extended Kalman filter formulation. Their general Kalman filter approach could be applied to the coreg-istration problem formulated here.
References-found: 30

