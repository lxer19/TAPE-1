URL: http://www.neci.nj.nec.com/homepages/lawrence/papers/speech-nips95/speech-nips95-a4.ps.gz
Refering-URL: http://www.neci.nj.nec.com/homepages/lawrence/papers/speech-nips95/
Root-URL: http://www.neci.nj.nec.com
Email: flawrence,act,backg@elec.uq.edu.au  
Title: The Gamma MLP for Speech Phoneme Recognition  
Author: Steve Lawrence Ah Chung Tsoi, Andrew D. Back 
Address: 4072 Australia  
Affiliation: Department of Electrical and Computer Engineering University of Queensland St. Lucia Qld  
Note: Appears in Advances in Neural Information Processing Systems 8, edited by D. Touretzky, M. Mozer and M. Hasselmo, pp. 785-791, MIT Press, 1996.  
Abstract: We define a Gamma multi-layer perceptron (MLP) as an MLP with the usual synaptic weights replaced by gamma filters (as proposed by de Vries and Principe (de Vries & Principe 1992)) and associated gain terms throughout all layers. We derive gradient descent update equations and apply the model to the recognition of speech phonemes. We find that both the inclusion of gamma filters in all layers, and the inclusion of synaptic gains, improves the performance of the Gamma MLP. We compare the Gamma MLP with TDNN, Back-Tsoi FIR MLP, and Back-Tsoi IIR MLP architectures, and a local approximation scheme. We find that the Gamma MLP results in a substantial reduction in error rates. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Back, A. D. & Tsoi, A. C. </author> <year> (1991a), </year> <title> Analysis of hidden layer weights in a dynamic locally recurrent network, </title> <editor> in O. Simula, ed., </editor> <booktitle> `Proceedings International Conference on Artificial Neural Networks, ICANN-91', </booktitle> <volume> Vol. </volume> <pages> 1, </pages> <address> Espoo, Finland, </address> <pages> pp. 967-976. </pages>
Reference-contexts: The error surface will be different in each case, and the results indicate that the surface for the synaptic gains case is more amenable to gradient descent. One view of the situation is seen by Back & Tsoi with their FIR and IIR MLP networks <ref> (Back & Tsoi 1991a) </ref>: From a signal processing perspective the response of each synapse is determined by pole-zero positions. With no synaptic gains, the weights determine both the static gain and the pole-zero positions of the synapses.
Reference: <author> Back, A. & Tsoi, A. </author> <year> (1991b), </year> <title> `FIR and IIR synapses, a new neural network architecture for time series modelling', </title> <booktitle> Neural Computation 3(3), </booktitle> <pages> 337-350. </pages>
Reference-contexts: l (neuron index), l = 0; 1; :::; L (layer), and z l kij j i=0;j6=0 = 0; c l kij j i=0 = 1 (bias). 2 For comparison purposes, we have used the TDNN (Time Delay Neural Network) architecture 2 , the Back-Tsoi FIR 3 and IIR MLP architectures <ref> (Back & Tsoi 1991b) </ref> where every synapse contains an FIR or IIR filter and a gain term, and the local ap proximation algorithm used by Casdagli (k-NN LA) (Casdagli 1991) 4 .
Reference: <author> Casdagli, M. </author> <year> (1991), </year> <title> `Chaos and deterministic versus stochastic non-linear modelling', J.R. </title> <journal> Statistical Society B 54(2), </journal> <pages> 302-328. </pages>
Reference-contexts: comparison purposes, we have used the TDNN (Time Delay Neural Network) architecture 2 , the Back-Tsoi FIR 3 and IIR MLP architectures (Back & Tsoi 1991b) where every synapse contains an FIR or IIR filter and a gain term, and the local ap proximation algorithm used by Casdagli (k-NN LA) <ref> (Casdagli 1991) </ref> 4 . The Gamma MLP is a special case of the IIR MLP. 3 TASK 3.1 MOTIVATION Accurate speech recognition requires models which can account for a high degree of variability in the data.
Reference: <author> Darken, C. & Moody, J. </author> <year> (1991), </year> <title> Note on learning rate schedules for stochastic optimization, </title> <booktitle> in `Neural Information Processing Systems 3', </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 832-838. </pages> <editor> de Vries, B. & Principe, J. </editor> <year> (1992), </year> <title> `The gamma model anew neural network for temporal processing', </title> <booktitle> Neural Networks 5(4), </booktitle> <pages> 565-576. </pages>
Reference-contexts: This is similar to the schedule proposed in <ref> (Darken & Moody 1991) </ref> with an additional term to decrease the learning rate towards zero over the final epochs 8 .
Reference: <author> Lang, K. J., Waibel, A. H. & Hinton, G. </author> <year> (1990), </year> <title> `A time-delay neural network architecture for isolated word recognition', </title> <booktitle> Neural Networks 3, </booktitle> <pages> 23-43. </pages>
Reference-contexts: problem suffers from the so called "curse of dimensionality" and the difficulty in optimizing a function with limited control over the nature of the error surface. 2 We use TDNN to refer to an MLP with a time window of inputs, not the replicated architecture introduced by Lang et al. <ref> (Lang, Waibel & Hinton 1990) </ref>. 3 We distinguish the Back-Tsoi FIR network from the Wan FIR network in that the Wan architecture has no synaptic gains, and the update algorithms are different.
Reference: <author> Shynk, J. </author> <year> (1989), </year> <title> `Adaptive IIR filtering', </title> <journal> IEEE ASSP Magazine pp. </journal> <pages> 4-21. </pages>
Reference-contexts: This may be attributed to the fact that a) there could be instability during training and b) the gradient fl http://www.neci.nj.nec.com/homepages/lawrence descent training procedures are not guaranteed to locate the global optimum in the possibly non-convex error surface <ref> (Shynk 1989) </ref>. De Vries and Principe proposed using gamma filters (de Vries & Principe 1992), a special case of IIR filters, at the input to an otherwise standard MLP.
References-found: 6

