URL: http://ftp.cs.yale.edu/pub/ghosh/imacs92.ps.gz
Refering-URL: http://ftp.cs.yale.edu/pub/ghosh/
Root-URL: http://www.cs.yale.edu
Title: Algorithms and Use of a Portable Parallel Scientific Library  
Author: Bhaskar Ghosh Martin H. Schultz 
Date: October 1992  
Affiliation: Yale University  Yale University  
Abstract: This paper describes a new approach towards providing an efficient Level-3 BLAS library over a variety of parallel architectures. The implementations of our parallel subroutines are in C-Linda. A blocked linear algebra program calling the sequential Level-3 BLAS can now run on both shared and distributed memory environments (which support Linda) by simply replacing each call by a call to the corressponding parallel Level-3 BLAS . We give some flavour of the implementation and algorithmic issues related to the triangular solve subroutine. All the matrix algorithms being block-structured, we are particularly interested in parallel computers with hierarchical memory systems. Experimental data for our implementations show substantial speedups on shared memory, disjoint memory and networked configurations of processors. We also present the use of our parallel subroutines in blocked dense LU decomposition and present some preliminary experimental data. 
Abstract-found: 1
Intro-found: 1
Reference: [BF89] <author> L. Brochard and A. Freau, </author> <title> "Designing Algorithms on Hierarchical Memory Multiprocessors". </title> <type> RC 15271 (68213), </type> <institution> IBM Yorktown Heights. </institution> <year> 1989. </year>
Reference-contexts: They are intended to provide efficient and portable building blocks for linear algebra algorithms on high performance computers. The problem of implementing and tuning the Level-3 BLAS on specific machines to achieve good performance has been studied in detail for a wide variety of machines in <ref> [BF89, LRW91, Chen90] </ref>. Dayde and Duff [DD89, DD90] have reported on parallel Level-3 BLAS and their uses in various blocked forms of LU factorisation on shared memory machines with vector facilities, like the CRAY-2, IBM 3090 VF and Alliant FX/80.
Reference: [CG90] <author> N. Carriero and D. </author> <title> Gelernter , "How to write Parallel Programs". </title> <publisher> M.I.T. Press, </publisher> <address> Boston, </address> <year> 1990. </year> <month> 11 </month>
Reference-contexts: Thus, for example, C with the addition of the Linda operations becomes the parallel programming language C-Linda. For an introduction to the tuple-space approach of parallel computation and to some general discussions of Linda implementations we refer the reader to <ref> [CG90, CarThesis] </ref>. We will assume that the reader has minimal familiarity with the primitive operations in the Linda language. The algorithmic paradigm that we use in designing our algorithms is the oft-used Master-Worker Model [CG90, CarThesis]. <p> of parallel computation and to some general discussions of Linda implementations we refer the reader to <ref> [CG90, CarThesis] </ref>. We will assume that the reader has minimal familiarity with the primitive operations in the Linda language. The algorithmic paradigm that we use in designing our algorithms is the oft-used Master-Worker Model [CG90, CarThesis]. In this model, a single process acts as the master process: spawning workers, maintaining overall control of the entire computation, distributing data and collecting results. The actual computation is done by a number of worker processes running independently and in parallel on different CPUs.
Reference: [CarThesis] <author> N. J. Carriero, </author> <title> "Implementation of Tuple Space Machines". </title> <address> YALEU/DCS/RR-567, </address> <month> Dec. </month> <year> 1987. </year>
Reference-contexts: Thus, for example, C with the addition of the Linda operations becomes the parallel programming language C-Linda. For an introduction to the tuple-space approach of parallel computation and to some general discussions of Linda implementations we refer the reader to <ref> [CG90, CarThesis] </ref>. We will assume that the reader has minimal familiarity with the primitive operations in the Linda language. The algorithmic paradigm that we use in designing our algorithms is the oft-used Master-Worker Model [CG90, CarThesis]. <p> of parallel computation and to some general discussions of Linda implementations we refer the reader to <ref> [CG90, CarThesis] </ref>. We will assume that the reader has minimal familiarity with the primitive operations in the Linda language. The algorithmic paradigm that we use in designing our algorithms is the oft-used Master-Worker Model [CG90, CarThesis]. In this model, a single process acts as the master process: spawning workers, maintaining overall control of the entire computation, distributing data and collecting results. The actual computation is done by a number of worker processes running independently and in parallel on different CPUs.
Reference: [Chen90] <author> Dingju Chen, </author> <title> "Hierarchical Blocking and Data Flow Analysis for Numerical Linear Algebra.". </title> <booktitle> Conference Proceedings of Supercomputing, 1990. </booktitle> <address> New York City. </address>
Reference-contexts: They are intended to provide efficient and portable building blocks for linear algebra algorithms on high performance computers. The problem of implementing and tuning the Level-3 BLAS on specific machines to achieve good performance has been studied in detail for a wide variety of machines in <ref> [BF89, LRW91, Chen90] </ref>. Dayde and Duff [DD89, DD90] have reported on parallel Level-3 BLAS and their uses in various blocked forms of LU factorisation on shared memory machines with vector facilities, like the CRAY-2, IBM 3090 VF and Alliant FX/80.
Reference: [DD89] <author> M. Dayde and Iain Duff, </author> <title> "Level 3 BLAS in LU factorisation in LU factorisation on the CRAY-2 etc.". </title> <booktitle> The Intl. J. of Supercomputing Applications, </booktitle> <volume> 3 - 2, </volume> <month> Summer </month> <year> 1989. </year>
Reference-contexts: The problem of implementing and tuning the Level-3 BLAS on specific machines to achieve good performance has been studied in detail for a wide variety of machines in [BF89, LRW91, Chen90]. Dayde and Duff <ref> [DD89, DD90] </ref> have reported on parallel Level-3 BLAS and their uses in various blocked forms of LU factorisation on shared memory machines with vector facilities, like the CRAY-2, IBM 3090 VF and Alliant FX/80. <p> The LU factorisation algorithm can be expressed in blocked form and involves calls to Level-3 BLAS subroutines. Blocked LU factorisation is an extensively researched topic and we refer the readers to papers mentioned in <ref> [DD89, DD90] </ref>. There can be six block forms of LU factorisation corresponding to the six ways of 7 reordering the loops that constitute the algorithm. <p> We implemented three block column forms (following the nomenclature of [DDDH88]) because we store the matrix in column order to allow calls to the Fortran Level-3 BLAS routines which are hidden inside our C-Linda block level primitives. These forms have been well described and analysed in <ref> [DD89] </ref>. In this chapter we will discuss only one blocked form called kji-saxpy without pivoting and use of our parallel subroutines. We quote the following algorithm from Dayde and Duff [DD89]. 3.1 KJI-SAXPY Algorithm At the k th step of the elimination a block column of L and a block row <p> These forms have been well described and analysed in <ref> [DD89] </ref>. In this chapter we will discuss only one blocked form called kji-saxpy without pivoting and use of our parallel subroutines. We quote the following algorithm from Dayde and Duff [DD89]. 3.1 KJI-SAXPY Algorithm At the k th step of the elimination a block column of L and a block row of U are computed and the trailing submatrix M k is updated, as shown in Figure 6.
Reference: [DD90] <author> M. J. Dayde and Iain Duff, </author> " <title> Use of level 3 BLAS in LU factorization in a multiprocessor environment on three vector multiprocessors, the Alliant FX/80, the Cray-2 and the IBM 3090 VF". </title> <booktitle> ICS 1990 Proceedings. </booktitle>
Reference-contexts: The problem of implementing and tuning the Level-3 BLAS on specific machines to achieve good performance has been studied in detail for a wide variety of machines in [BF89, LRW91, Chen90]. Dayde and Duff <ref> [DD89, DD90] </ref> have reported on parallel Level-3 BLAS and their uses in various blocked forms of LU factorisation on shared memory machines with vector facilities, like the CRAY-2, IBM 3090 VF and Alliant FX/80. <p> The LU factorisation algorithm can be expressed in blocked form and involves calls to Level-3 BLAS subroutines. Blocked LU factorisation is an extensively researched topic and we refer the readers to papers mentioned in <ref> [DD89, DD90] </ref>. There can be six block forms of LU factorisation corresponding to the six ways of 7 reordering the loops that constitute the algorithm.
Reference: [DDDH88] <author> J. J. Dongarra, J. Du Croz, I. Duff and S. Hammarling, </author> <title> "A Set of Level 3 Basic Linear Algebra Subprograms". </title> <institution> Argonne National Laboratory, </institution> <month> August </month> <year> 1988. </year>
Reference-contexts: There can be six block forms of LU factorisation corresponding to the six ways of 7 reordering the loops that constitute the algorithm. We implemented three block column forms (following the nomenclature of <ref> [DDDH88] </ref>) because we store the matrix in column order to allow calls to the Fortran Level-3 BLAS routines which are hidden inside our C-Linda block level primitives. These forms have been well described and analysed in [DD89].
Reference: [Gal87] <author> Kyle Gallivan et. al. </author> , <title> "The impact of hierarchical memory systems on linear algebra algorithm design". </title> <type> CSRD Report 625, </type> <institution> UIUC, </institution> <year> 1987. </year>
Reference: [GPS90] <author> Kyle Gallivan, R. J. Plemmons and A. H. </author> <title> Sameh , "Parallel Dense Linear Algebra algorithms". </title> <journal> Siam Review, </journal> <month> March </month> <year> 1990. </year>
Reference: [Gel85] <author> D. H. Gelernter, </author> <title> "Generative Communication in Linda." </title> <journal> ACM TOPLAS, </journal> <volume> 7(1). </volume> <month> January </month> <year> 1985. </year>
Reference: [GS1] <author> Bhaskar Ghosh and Martin H. Schultz, </author> <title> "Portable Parallel Level-3 BLAS in Linda", </title> <booktitle> Proceedings of SHPCC, </booktitle> <address> Williamsburg, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: The optimal blocking factors and processors to be used are chosen based on the problem size and the particular machine in use. We used square or rectangular blocking for Linda sgemm (refer to <ref> [GS1] </ref> and [GS2] for details) and the parallel-solve version of Linda strsm since the left hand side was small enough for optimal block sizes of matrix A. <p> Second, the copying overhead for such a Level-3 BLAS based LU decomposition grows with the number of blocks and is shown in <ref> [GS1] </ref> to be O (N 3 ) for N by N matrices.
Reference: [GS2] <author> Bhaskar Ghosh and Martin H. Schultz, </author> <title> "Portable Parallel Level-3 BLAS: Algorithms, Performance and Use", </title> <institution> Technical Report (forthcoming) Yale University, Computer Science Department, </institution> <year> 1992. </year>
Reference-contexts: Current experimental data is presented. With the efficient parallel subroutines thus available we can speed up a blocked-LU factorisation scheme expressible in Level-3 BLAS primitives. For further details please refer to our detailed report <ref> [GS2] </ref>. 1.3 Background Linda consists of a small number of powerful operations that may be integrated into a conventional base language, yielding a dialect that supports parallel programming. Thus, for example, C with the addition of the Linda operations becomes the parallel programming language C-Linda. <p> The optimal blocking factors and processors to be used are chosen based on the problem size and the particular machine in use. We used square or rectangular blocking for Linda sgemm (refer to [GS1] and <ref> [GS2] </ref> for details) and the parallel-solve version of Linda strsm since the left hand side was small enough for optimal block sizes of matrix A. The existence of tuple space and parallel processes working on data shared through tuple space is invisible to the higher level block LU program. <p> To understand why the speedups cannot be any higher and do not scale well with p, we have tried to predict (in <ref> [GS2] </ref>) the maximal speedup possible given p, the problem size, blocking parameters and the I/O fraction 2 k. The observed speedup was always less than and very close to the maximal speedup predicted by this analysis. There are two main factors limiting the speedup that can be achieved.
Reference: [LRW91] <author> Monica Lam, E. Rothberg and M. Wolf, </author> <title> "The Cache Performance and Optimizations of Blocked Algorithms". </title> <booktitle> ASPLOS IV, </booktitle> <address> Palo Alto, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: They are intended to provide efficient and portable building blocks for linear algebra algorithms on high performance computers. The problem of implementing and tuning the Level-3 BLAS on specific machines to achieve good performance has been studied in detail for a wide variety of machines in <ref> [BF89, LRW91, Chen90] </ref>. Dayde and Duff [DD89, DD90] have reported on parallel Level-3 BLAS and their uses in various blocked forms of LU factorisation on shared memory machines with vector facilities, like the CRAY-2, IBM 3090 VF and Alliant FX/80.
Reference: [Mol72] <author> Cleve Moler, </author> <title> "Matrix Computations with Fortran and Paging". </title> <journal> CACM 15, </journal> <volume> 4. </volume> <month> April </month> <year> 1972. </year>
Reference: [MC69] <author> McKellar, A.C. and Coffman, E.G.Jr. </author> <title> "Organising Matrices and matrix operations for paged memory systems. </title> " <journal> CACM 12, </journal> <volume> 3. </volume> <month> March </month> <year> 1969. </year> <month> 12 </month>
References-found: 15

