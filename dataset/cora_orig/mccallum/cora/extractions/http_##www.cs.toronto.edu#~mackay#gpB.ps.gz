URL: http://www.cs.toronto.edu/~mackay/gpB.ps.gz
Refering-URL: http://www.cs.toronto.edu/~mackay/README.html
Root-URL: http://www.cs.toronto.edu
Email: mackay@mrao.cam.ac.uk  
Title: INTRODUCTION TO GAUSSIAN PROCESSES MacKay (1997), and will assess whether, for supervised regression and classification
Author: DAVID J.C. MACKAY 
Note: In this chapter I will review work on this idea by Williams and Ras-mussen (1996), Neal (1997), Barber and Williams (1997) and Gibbs and  has been superceded.  
Address: Madingley Road, Cambridge, CB3 0HE. United Kingdom.  
Affiliation: Department of Physics, Cambridge University. Cavendish Laboratory,  
Abstract: Feedforward neural networks such as multilayer perceptrons are popular tools for nonlinear regression and classification problems. From a Bayesian perspective, a choice of a neural network model can be viewed as defining a prior probability distribution over non-linear functions, and the neural network's learning process can be interpreted in terms of the posterior probability distribution over the unknown function. (Some learning algorithms search for the function with maximum posterior probability and other Monte Carlo methods draw samples from this posterior probability). In the limit of large but otherwise standard networks, Neal (1996) has shown that the prior distribution over non-linear functions implied by the Bayesian neural network falls in a class of probability distributions known as Gaussian processes. The hyperparameters of the neural network model determine the characteristic lengthscales of the Gaussian process. Neal's observation motivates the idea of discarding parameterized networks and working directly with Gaussian processes. Computations in which the parameters of the network are optimized are then replaced by simple matrix operations using the covariance matrix of the Gaussian process. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Abrahamsen, P.: </author> <year> 1997, </year> <title> A review of Gaussian random fields and correlation functions, </title> <type> Technical Report 917, </type> <institution> Norwegian Computing Center, </institution> <address> Box 114, Blindern, N-0314 Oslo, Norway. 2nd edition. </address>
Reference: <author> Bar-Shalom, Y. and Fortmann, T.: </author> <year> 1988, </year> <title> Tracking and Data Association, </title> <publisher> Academic Press. </publisher>
Reference: <author> Barber, D. and Williams, C. K. I.: </author> <year> 1997, </year> <title> Gaussian processes for Bayesian classification via hybrid Monte Carlo, </title> <editor> in M. C. Mozer, M. I. Jordan and T. Petsche (eds), </editor> <booktitle> Neural Information Processing Systems 9, </booktitle> <publisher> MIT Press, </publisher> <editor> p. ? Barnett, S.: </editor> <year> 1979, </year> <title> Matrix Methods for Engineers and Scientists, </title> <publisher> McGraw-Hill. </publisher>
Reference: <author> Cressie, N.: </author> <year> 1993, </year> <title> Statistics for Spatial Data, </title> <publisher> Wiley. </publisher>
Reference-contexts: MULTIPLE OUTPUTS The subject of multiple outputs (or co-kriging <ref> (Cressie 1993) </ref>) is problematic. It is possible to define Gaussian processes with multiple outputs but it is not clear in general how the covariance function should be defined.
Reference: <author> Gibbs, M. N.: </author> <year> 1997, </year> <title> Bayesian Gaussian Processes for Regression and Classification, </title> <type> PhD thesis, </type> <institution> Cambridge University. </institution>
Reference-contexts: CONCLUSION Gaussian processes can be used to produce effective binary classifiers. The results using the variational Gaussian process classifier are comparable to the best of current classification models. Multi-class classification problems can also be solved with Monte Carlo methods (Neal 1997) and variational methods <ref> (Gibbs 1997) </ref>. 11. Discussion Gaussian processes are moderately simple to implement and use. Because very few parameters of the model need to be determined by hand (generally only the priors on the hyperparameters), Gaussian processes are useful tools for automated tasks where fine tuning for each problem is not possible.
Reference: <author> Gibbs, M. N. and MacKay, D. J. C.: </author> <year> 1996, </year> <title> Efficient implementation of Gaussian processes for interpolation, </title> <note> in preparation. </note>
Reference-contexts: These methods are useful when the number of data points exceeds a few hundred <ref> (Gibbs and MacKay 1996) </ref>. 8. Regression Examples We present two simple examples. The first is a one dimensional regression problem. This consists of a set of 37 noisy training data points shown in figure 5 (a). We used the basic covariance function given in equation (45).
Reference: <author> Gibbs, M. N. and MacKay, D. J. C.: </author> <year> 1997, </year> <title> Variational Gaussian process classifiers, </title> <note> in preparation. </note>
Reference-contexts: CONCLUSION Gaussian processes can be used to produce effective binary classifiers. The results using the variational Gaussian process classifier are comparable to the best of current classification models. Multi-class classification problems can also be solved with Monte Carlo methods (Neal 1997) and variational methods <ref> (Gibbs 1997) </ref>. 11. Discussion Gaussian processes are moderately simple to implement and use. Because very few parameters of the model need to be determined by hand (generally only the priors on the hyperparameters), Gaussian processes are useful tools for automated tasks where fine tuning for each problem is not possible.
Reference: <author> Ichikawa, K., Bhadeshia, H. K. D. H. and MacKay, D. J. C.: </author> <year> 1996, </year> <title> Model for hot cracking in low-alloy steel weld metals, </title> <booktitle> Science and Technology of Welding and Joining 1, </booktitle> <pages> 43-50. </pages>
Reference-contexts: We wish to predict whether a given weld will crack by examining the dependence of cracking on 13 input variables describing a weld. This problem has previously been tackled using Bayesian neural networks <ref> (Ichikawa, Bhadeshia and MacKay 1996) </ref>. An initial test was performed using a training set of 77 examples and a test set of 77 examples. <p> An initial test was performed using a training set of 77 examples and a test set of 77 examples. The test error rates and test log likelihoods for the VGC and the Bayesian neural network approach <ref> (Ichikawa et al. 1996) </ref> can be seen in table 2 where the test log likelihood is defined as test log likelihood = N test X n=1 28 DAVID J.C. <p> GAUSSIAN PROCESSES 29 0.00 0.05 0.10 Carbon content in Weld metal / wt.% 0.0 0.4 0.8 Predicted Probability of Cracking ichikawa L approx. the predictions as a function of one the 13 input variables given by a committee of neural networks <ref> (Ichikawa et al. 1996) </ref> and those found using the lower bound approximation of a VGC. Both have the same large scale features. However we do not appear to sacrifice any performance for this simplicity.
Reference: <author> Jaakkola, T. S. and Jordan, M. I.: </author> <year> 1996, </year> <title> Computing upper and lower bounds on likelihoods in intractable networks, </title> <booktitle> Proceedings of the Twelfth Conference on Uncertainty in AI, </booktitle> <publisher> Morgan Kaufman. </publisher>
Reference: <author> Kimeldorf, G. S. and Wahba, G.: </author> <year> 1970, </year> <title> A correspondence between Bayesian estimation of stochastic processes and smoothing by splines, </title> <journal> Annals of Mathematical Statistics 41(2), </journal> <pages> 495-502. </pages>
Reference-contexts: MACKAY 2.3. NONPARAMETRIC APPROACHES. In nonparametric methods, predictions are obtained without giving the unknown function y (x) an explicit parameterization. One well known nonpara-metric approach to the regression problem is the spline smoothing method <ref> (Kimeldorf and Wahba 1970) </ref>.
Reference: <author> Kitanidis, P. K.: </author> <year> 1986, </year> <title> Parameter uncertainty in estimation of spatial functions: Bayesian analysis, </title> <booktitle> Water Resources Research 22, </booktitle> <pages> 499-507. </pages>
Reference: <author> Lauritzen, S. L.: </author> <year> 1981, </year> <title> Time series analysis in 1880, a discussion of contributions made by T.N. Thiele, </title> <booktitle> ISI Review 49, </booktitle> <pages> 319-333. </pages>
Reference-contexts: LITERATURE The study of Gaussian processes for regression is far from new. Time series analysis was being performed by the astronomer T.N. Thiele using Gaussian processes in 1880 <ref> (Lauritzen 1981) </ref>. In the 1940s, Wiener-Kolmogorov prediction theory was introduced for prediction of trajectories of military targets (Wiener 1948). Within the geostatistics field, Matheron (1963) proposed a framework for regression using optimal linear estimators which he called `kriging' after D.G. Krige, a South African mining engineer.
Reference: <author> Lowe, D. G.: </author> <year> 1995, </year> <title> Similarity metric learning for a variable kernel classifier, </title> <booktitle> Neural Computation 7, </booktitle> <pages> 72-85. </pages>
Reference-contexts: Kalman filters are widely used to implement inferences for stationary one-dimensional Gaussian processes (Bar-Shalom and Fortmann 1988). O'Hagan (1978) introduced an approach similar to Gaussian processes. Generalized radial basis functions (Poggio and Girosi 1989), ARMA models (Wahba 1990) and variable metric kernel methods <ref> (Lowe 1995) </ref> are all closely related to Gaussian processes.
Reference: <author> MacKay, D. J. C.: </author> <year> 1992, </year> <title> Bayesian interpolation, </title> <booktitle> Neural Computation 4(3), </booktitle> <pages> 415-447. </pages>
Reference: <author> MacKay, D. J. C.: </author> <year> 1994, </year> <title> Bayesian methods for backpropagation networks, </title> <editor> in E. Domany, J. L. van Hemmen and K. Schulten (eds), </editor> <booktitle> Models of Neural Networks III, </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, chapter 6, </address> <pages> pp. 211-254. </pages>
Reference-contexts: A very large length scale means that y is expected to be essentially a constant function of that input. Such an input could be said to be irrelevant, as in the automatic relevance determination (ARD) method for neural networks <ref> (MacKay 1994, Neal 1996) </ref>. The 1 hyperparameter defines the vertical scale of variations of a typical function.
Reference: <author> Matheron, G.: </author> <year> 1963, </year> <booktitle> Principles of geostatistics, Economic Geology 58, </booktitle> <pages> 1246-1266. </pages>
Reference: <author> Neal, R. M.: </author> <year> 1996, </year> <title> Bayesian Learning for Neural Networks, </title> <booktitle> number 118 in Lecture Notes in Statistics, </booktitle> <publisher> Springer, </publisher> <address> New York. </address>
Reference-contexts: P (t N+1 jx N+1 ; t N ; X N ) ' P (t N+1 jx N+1 ; t N ; X N ; fi MP ) (60) 2. Or we can perform the integration over fi numerically using Monte Carlo methods <ref> (Williams and Rasmussen 1996, Neal 1997) </ref>. Either of these approaches is implemented most efficiently if the gradient of the posterior probability of fi can be evaluated. 6.1.
Reference: <author> Neal, R. M.: </author> <year> 1997, </year> <title> Monte Carlo implementation of Gaussian process models for Bayesian regression and classification, </title> <type> Technical Report CRG-TR-97-2, </type> <institution> Dept. of Computer Science, University of Toronto. </institution> <note> 32 DAVID J.C. </note> <author> MACKAY O'Hagan, A.: </author> <year> 1978, </year> <title> On curve fitting and optimal design for regression, </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> B 40, </volume> <pages> 1-42. </pages>
Reference-contexts: CONCLUSION Gaussian processes can be used to produce effective binary classifiers. The results using the variational Gaussian process classifier are comparable to the best of current classification models. Multi-class classification problems can also be solved with Monte Carlo methods <ref> (Neal 1997) </ref> and variational methods (Gibbs 1997). 11. Discussion Gaussian processes are moderately simple to implement and use.
Reference: <author> Omre, H.: </author> <year> 1987, </year> <title> Bayesian kriging merging observations and qualified guesses in kriging, </title> <booktitle> Mathematical Geology 19, </booktitle> <pages> 25-39. </pages>
Reference-contexts: Krige, a South African mining engineer. This framework is identical to the Gaussian process approach to regression. Kriging has been developed considerably in the last thirty years (see Cressie (1993) for an excellent review) including several Bayesian treatments <ref> (Omre 1987, Kitanidis 1986) </ref>. However the geostatistics approach to GAUSSIAN PROCESSES 31 the Gaussian process model has concentrated mainly on low-dimensional problems and has largely ignored any probabilistic interpretation of the model and any interpretation of the individual parameters of the covari-ance function.
Reference: <author> Poggio, T. and Girosi, F.: </author> <year> 1989, </year> <title> A theory of networks for approximation and learning, </title> <type> Technical Report A.I. 1140, </type> <institution> M.I.T. </institution>
Reference-contexts: Kalman filters are widely used to implement inferences for stationary one-dimensional Gaussian processes (Bar-Shalom and Fortmann 1988). O'Hagan (1978) introduced an approach similar to Gaussian processes. Generalized radial basis functions <ref> (Poggio and Girosi 1989) </ref>, ARMA models (Wahba 1990) and variable metric kernel methods (Lowe 1995) are all closely related to Gaussian processes.
Reference: <author> Rasmussen, C. E.: </author> <year> 1996, </year> <title> Evaluation of Gaussian Processes and Other Methods for NonLinear Regression, </title> <type> PhD thesis, </type> <institution> University of Toronto. </institution>
Reference-contexts: P (t N+1 jx N+1 ; t N ; X N ) ' P (t N+1 jx N+1 ; t N ; X N ; fi MP ) (60) 2. Or we can perform the integration over fi numerically using Monte Carlo methods <ref> (Williams and Rasmussen 1996, Neal 1997) </ref>. Either of these approaches is implemented most efficiently if the gradient of the posterior probability of fi can be evaluated. 6.1. <p> This means that each output is modelled independently | a method known as multi-kriging <ref> (Williams and Rasmussen 1996) </ref>. At this point we may feel that Gaussian processes are missing out on something, since we might intuitively expect two output variables to have some features in common.
Reference: <author> Ripley, B. D.: </author> <year> 1991, </year> <title> Statistical Inference for Spatial Processes, </title> <publisher> Cambridge. </publisher>
Reference-contexts: The actual function y (x) in any one data modelling problem is assumed to be a single sample from this Gaussian distribution. Gaussian processes are already well established models for various spatial and temporal problems <ref> (Ripley 1991) </ref> | for example, Brownian motion, Langevin processes and Wiener processes are all examples of Gaussian processes; Kalman filters, widely used to model speech waveforms, also correspond to Gaussian process models; the method of `kriging' in geostatistics is a Gaussian process regression method. 1.1.
Reference: <author> Ripley, B. D.: </author> <year> 1994, </year> <title> Flexible non-linear approaches to classification, </title> <editor> in V. Cherkassky, J. H. Friedman and H. Wechsler (eds), </editor> <title> From Statistics to Neural Networks. Theory and Pattern Recognition Applications, </title> <booktitle> number subseries F in ASI Proceedings, </booktitle> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Pima and Crabs Results : The table shows the performance of a range of different classification models on the Pima and Crabs problems <ref> (Ripley 1994, Ripley 1996) </ref>. The number of classification errors and the percentage of errors both refer to the test set. The error bars given are calculated using binomial statistics. The results quoted for the VGC are those obtained using the approximations from the lower bound.
Reference: <author> Ripley, B. D.: </author> <year> 1996, </year> <title> Pattern Recognition and Neural Networks, </title> <publisher> Cambridge. </publisher>
Reference: <author> Rumelhart, D. E., Hinton, G. E. and Williams, R. J.: </author> <year> 1986, </year> <title> Learning representations by back-propagating errors, </title> <booktitle> Nature 323, </booktitle> <pages> 533-536. </pages>
Reference: <author> Scholkopf, B., Burges, C. and Vapnik, V.: </author> <year> 1995, </year> <title> Extracting support data for a given task, </title> <editor> in U. M. Fayyad and R. Uthurusamy (eds), </editor> <booktitle> Proceedings First International Conference on Knowledge Discovery and Data Mining, </booktitle> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA. </address>
Reference-contexts: But further research is going to be needed before Gaussian processes can be applied to more than about 10,000 data points. I speculate that there may be a useful connection to be made between Gaussian processes and `support vector' learning machines <ref> (Scholkopf, Burges and Vapnik 1995, Vapnik 1995) </ref>, which are in some ways quite similar to Gaussian processes. A problem with the variational approach for classification is the profil-eration of variational parameters when dealing with large amounts of the data.
Reference: <author> Skilling, J.: </author> <year> 1993, </year> <title> Bayesian numerical analysis, </title> <editor> in W. T. Grandy, Jr. and P. Milonni (eds), </editor> <title> Physics and Probability, C.U.P., </title> <publisher> Cambridge. </publisher>
Reference-contexts: The cost of direct methods of inversion may become prohibitive when the number of data points N is greater than ' 1000. In Gibbs and MacKay (1996) efficient methods for matrix inversion <ref> (Skilling 1993) </ref> are developed that allow large data sets to be tackled. But further research is going to be needed before Gaussian processes can be applied to more than about 10,000 data points.
Reference: <author> Vapnik, V.: </author> <year> 1995, </year> <title> The Nature of Statistical Learning Theory, </title> <publisher> Springer Verlag, </publisher> <address> New York. </address>
Reference-contexts: But further research is going to be needed before Gaussian processes can be applied to more than about 10,000 data points. I speculate that there may be a useful connection to be made between Gaussian processes and `support vector' learning machines <ref> (Scholkopf, Burges and Vapnik 1995, Vapnik 1995) </ref>, which are in some ways quite similar to Gaussian processes. A problem with the variational approach for classification is the profil-eration of variational parameters when dealing with large amounts of the data.
Reference: <author> Wahba, G.: </author> <year> 1990, </year> <title> Spline Models for Observational Data, </title> <booktitle> Society for Industrial and Applied Mathematics. CBMS-NSF Regional Conference series in applied mathematics. </booktitle>
Reference-contexts: Kalman filters are widely used to implement inferences for stationary one-dimensional Gaussian processes (Bar-Shalom and Fortmann 1988). O'Hagan (1978) introduced an approach similar to Gaussian processes. Generalized radial basis functions (Poggio and Girosi 1989), ARMA models <ref> (Wahba 1990) </ref> and variable metric kernel methods (Lowe 1995) are all closely related to Gaussian processes.
Reference: <author> Wiener, N.: </author> <year> 1948, </year> <title> Cybernetics, </title> <publisher> Wiley. </publisher>
Reference-contexts: LITERATURE The study of Gaussian processes for regression is far from new. Time series analysis was being performed by the astronomer T.N. Thiele using Gaussian processes in 1880 (Lauritzen 1981). In the 1940s, Wiener-Kolmogorov prediction theory was introduced for prediction of trajectories of military targets <ref> (Wiener 1948) </ref>. Within the geostatistics field, Matheron (1963) proposed a framework for regression using optimal linear estimators which he called `kriging' after D.G. Krige, a South African mining engineer. This framework is identical to the Gaussian process approach to regression.
Reference: <author> Williams, C. K. I. and Rasmussen, C. E.: </author> <year> 1996, </year> <title> Gaussian processes for regression, </title> <editor> in D. </editor> <publisher> S. </publisher>
Reference-contexts: P (t N+1 jx N+1 ; t N ; X N ) ' P (t N+1 jx N+1 ; t N ; X N ; fi MP ) (60) 2. Or we can perform the integration over fi numerically using Monte Carlo methods <ref> (Williams and Rasmussen 1996, Neal 1997) </ref>. Either of these approaches is implemented most efficiently if the gradient of the posterior probability of fi can be evaluated. 6.1. <p> This means that each output is modelled independently | a method known as multi-kriging <ref> (Williams and Rasmussen 1996) </ref>. At this point we may feel that Gaussian processes are missing out on something, since we might intuitively expect two output variables to have some features in common.
Reference: <editor> Touretzky, M. C. Mozer and M. E. Hasselmo. (eds), </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <publisher> MIT Press. Version 2.2. </publisher> <month> May 18, </month> <year> 1998 </year>
References-found: 32

