URL: ftp://ftp.cs.brown.edu/pub/techreports/94/cs94-32.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-94-32.html
Root-URL: http://www.cs.brown.edu/
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Ozalp Babaoglu and Keith Marzullo, </author> <title> ``Consistent Global States of Distributed Systems: Fundamental Concepts and Mechanisms,'' </title> <type> Technical Report UBLCS-93-1, </type> <institution> University of Bologna, </institution> <month> (January </month> <year> 1993). </year>
Reference-contexts: If such a frontier exists, then r could have accepted either of the messages in condition (2). Condition (2) states that the senders must send on a channel over which the receive is accepting messages. Condition (3) is required for the frontier to be a consistent cut <ref> [1] </ref>, which means that it represents a state at which all processes could have simultaneously arrived.
Reference: [2] <author> R. Curtis and L. Wittie, ``BugNet: </author> <title> A Debugging System for Parallel Programming Environments,'' </title> <booktitle> Proc. of the 3rd Intl. Conf. on Dist. Computing Systems, </booktitle> <pages> pp. </pages> <month> 394-399 </month> <year> (1982). </year>
Reference-contexts: This strategy is effective because the racing messages are exactly those that introduce nondeterminacy into the execution. Our work is novel in that only the racing messages are traced. In contrast, earlier trace and replay schemes for message-passing programs require tracing every message <ref> [6, 2, 12, 10, 7, 4] </ref>. Replay was first introduced by Curtis and Wittie in the BugNet system for debugging distributed C programs [2]. LeBlanc and Mellor-Crummey [6] also addressed replay but considered both shared-memory and message-passing parallel programs. <p> Our work is novel in that only the racing messages are traced. In contrast, earlier trace and replay schemes for message-passing programs require tracing every message [6, 2, 12, 10, 7, 4]. Replay was first introduced by Curtis and Wittie in the BugNet system for debugging distributed C programs <ref> [2] </ref>. LeBlanc and Mellor-Crummey [6] also addressed replay but considered both shared-memory and message-passing parallel programs. They trace only the order in which messages are delivered (and not their contents). By reproducing only the order of message deliveries, their contents (and hence the original computation) will also be reproduced. <p> To replay the execution for debugging, we must first trace the order in which the messages are delivered, and then use this trace to force a re-execution to exhibit the same message deliveries. Earlier trace and replay schemes propose tracing the order in which all messages are delivered <ref> [6, 2, 12, 10, 7, 4] </ref>. For example, they would record that Msg1 was delivered to the first Recv in P 2 and that Msg2 was delivered to the second Recv. During replay, the receive operations are modified to accept only the appropriate messages.
Reference: [3] <author> C. J. Fidge, </author> <title> ``Partial Orders for Parallel Debugging,'' </title> <booktitle> SIGPLAN/SIGOPS Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pp. </pages> <address> 183-194 Madison, WI, </address> <month> (May </month> <year> 1988). </year> <note> Also appears in SIGPLAN Notices 24(1) (January 1989). </note>
Reference-contexts: Our implementation of this algorithm uses a standard method of maintaining the iiiiiHB relation during execution by keeping a vector timestamp in each process. A vector timestamp is a vector of length p (the number of processes) containing event serial numbers <ref> [3] </ref>. These timestamps are maintained by appending them onto user messages and updating them after each receive operation.
Reference: [4] <author> Arthur P. Goldberg, Ajei Gopal, Andy Lowry, </author> <title> a nd Rob Strom, ``Restoring Consistent Global States of Distributed Computations,'' </title> <booktitle> ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pp. </pages> <address> 144-154 Santa Cruz, CA, </address> <month> (May </month> <year> 1991). </year>
Reference-contexts: This strategy is effective because the racing messages are exactly those that introduce nondeterminacy into the execution. Our work is novel in that only the racing messages are traced. In contrast, earlier trace and replay schemes for message-passing programs require tracing every message <ref> [6, 2, 12, 10, 7, 4] </ref>. Replay was first introduced by Curtis and Wittie in the BugNet system for debugging distributed C programs [2]. LeBlanc and Mellor-Crummey [6] also addressed replay but considered both shared-memory and message-passing parallel programs. <p> To replay the execution for debugging, we must first trace the order in which the messages are delivered, and then use this trace to force a re-execution to exhibit the same message deliveries. Earlier trace and replay schemes propose tracing the order in which all messages are delivered <ref> [6, 2, 12, 10, 7, 4] </ref>. For example, they would record that Msg1 was delivered to the first Recv in P 2 and that Msg2 was delivered to the second Recv. During replay, the receive operations are modified to accept only the appropriate messages.
Reference: [5] <author> Leslie Lamport, </author> <title> ``Time, Clocks, and the Ordering of Events in a Distributed System,'' </title> <journal> CACM 21(7) pp. </journal> <month> 558-565 (July </month> <year> 1978). </year> <month> 16 </month>
Reference-contexts: Representing the Actual Behavior An actual program execution represents one execution of a message-passing program, and is a pair, P = E, iiiiiHB , where E is a finite set of events and iiiiiHB is the happened-before relation defined over E <ref> [5] </ref>. We assume that an execution consists of a fixed number of processes, each of which performs a sequence of events. We distinguish between two types of events, computation and synchronization. A computation event simply represents all computation performed in a process between synchronization operations. <p> Modeling message passing with logical channels is very general; any message-passing scheme (such as ports, mailboxes, or links) can be represented. The happened-before relation, iiiiiHB , shows the relative order in which events execute and how they potentially affect one another <ref> [5] </ref>. This relation is defined as the irreexive transitive closure of the union of two other relations: iiiiiHB = ( iiiiiXO iiiiiM ) + . The iiiiiXO relation shows the order in which events in the same process execute.
Reference: [6] <author> Thomas J. LeBlanc and John M. Mellor-Crummey, </author> <title> ``Debugging Parallel Programs with Instant Replay,'' </title> <journal> IEEE Trans. on Computers C-36(4) pp. </journal> <month> 471-482 (April </month> <year> 1987). </year>
Reference-contexts: This strategy is effective because the racing messages are exactly those that introduce nondeterminacy into the execution. Our work is novel in that only the racing messages are traced. In contrast, earlier trace and replay schemes for message-passing programs require tracing every message <ref> [6, 2, 12, 10, 7, 4] </ref>. Replay was first introduced by Curtis and Wittie in the BugNet system for debugging distributed C programs [2]. LeBlanc and Mellor-Crummey [6] also addressed replay but considered both shared-memory and message-passing parallel programs. <p> In contrast, earlier trace and replay schemes for message-passing programs require tracing every message [6, 2, 12, 10, 7, 4]. Replay was first introduced by Curtis and Wittie in the BugNet system for debugging distributed C programs [2]. LeBlanc and Mellor-Crummey <ref> [6] </ref> also addressed replay but considered both shared-memory and message-passing parallel programs. They trace only the order in which messages are delivered (and not their contents). By reproducing only the order of message deliveries, their contents (and hence the original computation) will also be reproduced. <p> To replay the execution for debugging, we must first trace the order in which the messages are delivered, and then use this trace to force a re-execution to exhibit the same message deliveries. Earlier trace and replay schemes propose tracing the order in which all messages are delivered <ref> [6, 2, 12, 10, 7, 4] </ref>. For example, they would record that Msg1 was delivered to the first Recv in P 2 and that Msg2 was delivered to the second Recv. During replay, the receive operations are modified to accept only the appropriate messages. <p> These events can be identified by maintaining in each process a local counter (incremented after every synchronization operation) that is used to assign serial numbers to events <ref> [6] </ref>. It suffices to trace the event serial numbers of the sender and receiver and the process number of the sender. If one trace file is maintained for each process in the program execution, the process number of the receiver is implicit and need not be recorded.
Reference: [7] <author> Eric Leu, Andre Schiper, and Abdelwahab Zramdini, </author> <title> ``Efficient Execution Replay Technique for Distributed Memory Architectures,'' </title> <booktitle> 2nd European Distributed Memory Computing Conference, </booktitle> <publisher> LNCS 487, Springer-Verlag, </publisher> <address> Munich, </address> <year> (1991). </year>
Reference-contexts: This strategy is effective because the racing messages are exactly those that introduce nondeterminacy into the execution. Our work is novel in that only the racing messages are traced. In contrast, earlier trace and replay schemes for message-passing programs require tracing every message <ref> [6, 2, 12, 10, 7, 4] </ref>. Replay was first introduced by Curtis and Wittie in the BugNet system for debugging distributed C programs [2]. LeBlanc and Mellor-Crummey [6] also addressed replay but considered both shared-memory and message-passing parallel programs. <p> To replay the execution for debugging, we must first trace the order in which the messages are delivered, and then use this trace to force a re-execution to exhibit the same message deliveries. Earlier trace and replay schemes propose tracing the order in which all messages are delivered <ref> [6, 2, 12, 10, 7, 4] </ref>. For example, they would record that Msg1 was delivered to the first Recv in P 2 and that Msg2 was delivered to the second Recv. During replay, the receive operations are modified to accept only the appropriate messages.
Reference: [8] <author> James E. Lumpp, Jr., Julie A. Gannon, Mark S. Andersland, and Thomas L. Casavant, </author> <title> ``A Technique for Recovering from Software Instrumentation Intrusion in Message-Passing Systems,'' </title> <type> Technical Report TR-ECE-920817, </type> <institution> University of Iowa Department of Electrical and Computer Engineering, </institution> <month> (August </month> <year> 1992). </year>
Reference-contexts: Our implementation performed trace I/O by sending messages over the diagnostic network. The unusually high cost of this I/O makes the overhead of tracing all messages several orders of magnitude higher than race-based tracing, making comparisons unrealistic. 15 fect in nondeterministic systems remains an issue of current research <ref> [9, 8] </ref>. 6. Conclusions In this paper we presented a trace and replay strategy for message-passing parallel programs that substantially reduces tracing overhead over past schemes. The small traces and low overhead produced allow even long-running programs to be replayed that previously could not have been replayed in practice.
Reference: [9] <author> Allen D. Malony and Daniel A. Reed, </author> <title> ``Models for Performance Perturbation Analysis,'' </title> <booktitle> ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pp. </pages> <address> 15-25 Santa Cruz, CA, </address> <month> (May </month> <year> 1991). </year>
Reference-contexts: Our implementation performed trace I/O by sending messages over the diagnostic network. The unusually high cost of this I/O makes the overhead of tracing all messages several orders of magnitude higher than race-based tracing, making comparisons unrealistic. 15 fect in nondeterministic systems remains an issue of current research <ref> [9, 8] </ref>. 6. Conclusions In this paper we presented a trace and replay strategy for message-passing parallel programs that substantially reduces tracing overhead over past schemes. The small traces and low overhead produced allow even long-running programs to be replayed that previously could not have been replayed in practice.
Reference: [10] <author> Barton P. Miller and Jong-Deok Choi, </author> <title> ``A Mechanism for Efficient Debugging of Parallel Programs,'' </title> <booktitle> SIG-PLAN Conf. on Programming Language Design and Implementation, </booktitle> <pages> pp. </pages> <address> 135-144 Atlanta, GA, </address> <month> (June </month> <year> 1988). </year>
Reference-contexts: This strategy is effective because the racing messages are exactly those that introduce nondeterminacy into the execution. Our work is novel in that only the racing messages are traced. In contrast, earlier trace and replay schemes for message-passing programs require tracing every message <ref> [6, 2, 12, 10, 7, 4] </ref>. Replay was first introduced by Curtis and Wittie in the BugNet system for debugging distributed C programs [2]. LeBlanc and Mellor-Crummey [6] also addressed replay but considered both shared-memory and message-passing parallel programs. <p> To replay the execution for debugging, we must first trace the order in which the messages are delivered, and then use this trace to force a re-execution to exhibit the same message deliveries. Earlier trace and replay schemes propose tracing the order in which all messages are delivered <ref> [6, 2, 12, 10, 7, 4] </ref>. For example, they would record that Msg1 was delivered to the first Recv in P 2 and that Msg2 was delivered to the second Recv. During replay, the receive operations are modified to accept only the appropriate messages.
Reference: [11] <author> Reinhard Schwarz and Friedemann Mattern, </author> <title> ``Detecting Causal Relationships in Distributed Computations: In Search of the Holy Grail,'' </title> <type> Technical Report SFB124-15/92, </type> <institution> Dept. of Computer Science, Univ. of Kaiser-slautern, Kaiserslautern, Germany, </institution> <month> (December </month> <year> 1992). </year>
Reference-contexts: Using vector timestamps to track the iiiiiHB relation has the advantage that they work even if channels are not FIFO, and they do not require processes to synchronize their clocks <ref> [11] </ref>. The race check in line 4 of our tracing algorithm (Figure 4) is performed easily using the timestamps. The sender 's timestamp (which is appended to the incoming message) is compared to the serial number of the previous receive to determine if the receive happened before the sender.
Reference: [12] <author> Kuo-Chung Tai and Sanjiv Ahuja, </author> <title> ``Reproducible Testing of Communication Software,'' </title> <booktitle> IEEE COMPSAC '87, </booktitle> <pages> pp. </pages> <month> 331-337 </month> <year> (1987). </year>
Reference-contexts: This strategy is effective because the racing messages are exactly those that introduce nondeterminacy into the execution. Our work is novel in that only the racing messages are traced. In contrast, earlier trace and replay schemes for message-passing programs require tracing every message <ref> [6, 2, 12, 10, 7, 4] </ref>. Replay was first introduced by Curtis and Wittie in the BugNet system for debugging distributed C programs [2]. LeBlanc and Mellor-Crummey [6] also addressed replay but considered both shared-memory and message-passing parallel programs. <p> To replay the execution for debugging, we must first trace the order in which the messages are delivered, and then use this trace to force a re-execution to exhibit the same message deliveries. Earlier trace and replay schemes propose tracing the order in which all messages are delivered <ref> [6, 2, 12, 10, 7, 4] </ref>. For example, they would record that Msg1 was delivered to the first Recv in P 2 and that Msg2 was delivered to the second Recv. During replay, the receive operations are modified to accept only the appropriate messages.
References-found: 12

