URL: http://www.ai.mit.edu/people/viola/research/publications/IJCV-97.ps.gz
Refering-URL: http://www.ai.mit.edu/projects/lv/publications/
Root-URL: 
Email: EMAIL: viola@ai.mit.edu sw@ai.mit.edu  
Title: Alignment by Maximization of Mutual Information approach registering magnetic resonance (MR) images, aligning a complex
Author: Paul Viola and William M. Wells III 
Address: 545 Technology Square  Cambridge, MA 02139  
Affiliation: Artificial Intelligence Laboratory  Massachusetts Institute of Technology  
Date: 137154, 1997  
Note: International Journal of Computer Vision, 24(2) pg  Experiments are presented that demonstrate the  
Abstract: A new information-theoretic approach is presented for finding the pose of an object in an image. The technique does not require information about the surface properties of the object, besides its shape, and is robust with respect to variations of illumination. In our derivation few assumptions are made about the nature of the imaging process. As a result the algorithms are quite general and may foreseeably be used in a wide variety of imaging situations. The method is based on a formulation of the mutual information between the model and the image. As applied here the technique is intensity-based, rather than feature-based. It works well in domains where edge or gradient-magnitude based methods have difficulty, yet it is more robust than traditional correlation. Additionally, it has an efficient implementation that is based on stochastic approximation. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Becker, S. and Hinton, G. E. </author> <year> (1992). </year> <title> Learning to make coherent predictions in domains with discontinuities. </title> <editor> In Moody, J. E., Hanson, S. J., and Lippmann, R. P., editors, </editor> <booktitle> Advances in Neural Information Processing, volume 4, </booktitle> <address> Denver 1991. </address> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address>
Reference-contexts: None of these technique uses a non-parametric scheme for density/entropy estimation as we do. In most cases the distributions are assumed to be either binomial or Gaussian. Entropy and mutual information plays a role in the work of (Linsker, 1986), <ref> (Becker and Hinton, 1992) </ref> and (Bell and Sejnowski, 1995). 23 24 25 Though the model is made up of multiple images, only one is shown here.
Reference: <author> Bell, A. J. and Sejnowski, T. J. </author> <year> (1995). </year> <title> An information-maximisation approach to blind separation. </title> <booktitle> In Advances in Neural Information Processing, volume 7, </booktitle> <address> Denver 1994. </address> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco. </address>
Reference-contexts: None of these technique uses a non-parametric scheme for density/entropy estimation as we do. In most cases the distributions are assumed to be either binomial or Gaussian. Entropy and mutual information plays a role in the work of (Linsker, 1986), (Becker and Hinton, 1992) and <ref> (Bell and Sejnowski, 1995) </ref>. 23 24 25 Though the model is made up of multiple images, only one is shown here.
Reference: <author> Besl, P. and Jain, R. </author> <year> (1985). </year> <title> Three-Dimensional Object Recognition. </title> <journal> Computing Surveys, 17:75145. </journal>
Reference: <author> Bridle, J. S. </author> <year> (1989). </year> <title> Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters. </title> <editor> In Touretzky, D. S., editor, </editor> <booktitle> Advances in Neural Information Processing 2, </booktitle> <pages> pages 211217. </pages> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: It is equivalent to using the softmax function of neural networks <ref> (Bridle, 1989) </ref> on the negative of the Mahalanobis distance to indicate correspondence between z i and elements of A.
Reference: <author> Chin, R. and Dyer, C. </author> <year> (1986). </year> <title> Model-Based Recognition in Robot Vision. </title> <journal> Computing Surveys, 18:67108. </journal>
Reference: <author> Collignon, A., Vandermuelen, D., Suetens, P., and Marchal, G. </author> <year> (1995). </year> <title> 3D Multi-Modality Medical Image Registration Using Feature Space Clustering. </title> <editor> In Ayache, N., editor, </editor> <booktitle> Computer Vision, Virtual Reality and Robotics in Medicine, </booktitle> <pages> pages 195 204. </pages> <publisher> Springer Verlag. </publisher>
Reference-contexts: We believe that mutual information is perhaps a more direct measure of the salient property of the joint data at alignment, and demonstrate an efficient means of estimating and extrem-izing it. Recently, Collignon et al. <ref> (Collignon et al., 1995) </ref> described the use of joint entropy as a criterion for registration of CT and MRI data. They demonstrated a good minimum by probing the criterion, but no search techniques were described. Image-based approaches to modeling have been previously explored by several authors.
Reference: <author> Cover, T. M. and Thomas, J. A. </author> <year> (1991). </year> <title> Elements of Information Theory. </title> <publisher> John Wiley and Sons. </publisher>
Reference-contexts: 10 The entropy of a random variable is defined as h (y) p (y) ln p (y)dy : (9) The joint entropy of two random variables x and y is h (z; y) p (z; y) ln p (z; y)dzdy : (10) Log likelihood and entropy are closely related (see <ref> (Cover and Thomas, 1991) </ref> for an excellent review of entropy and its relation to statistics). <p> These parameters may be chosen so that they are optimal in the maximum likelihood sense with respect to samples drawn from the random variables. This approach is equivalent to minimizing the cross entropy of the estimated distribution with the true distribution <ref> (Cover and Thomas, 1991) </ref>. For simplicity, we assume that the covariance matrices are diagonal.
Reference: <author> Duda, R. and Hart, P. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley and Sons. </publisher>
Reference-contexts: It is described in the textbook by Duda and Hart <ref> (Duda and Hart, 1973) </ref>. Use of the Gaussian density in the Parzen density estimate will simplify some of our subsequent analysis, but it is not necessary. Any differentiable function could be used. Another good choice is the Cauchy density. <p> It will be near zero if some other element of A is significantly closer to z i . Distance is interpreted with respect to the squared Mahalanobis distance (see <ref> (Duda and Hart, 1973) </ref>) D (z) z T 1 z : Thus, W z (z i ; z j ) is an indicator of the degree of match between its arguments, in a soft sense.
Reference: <author> Haykin, S. </author> <year> (1994). </year> <title> Neural Networks: A comprehensive foundation. </title> <publisher> Macmillan College Publishing. </publisher>
Reference-contexts: For smaller sample sizes, less effort is expended, but additional noise is introduced into the derivative estimates. Stochastic approximation is a scheme that uses noisy derivative estimate instead of the true derivative for optimizing a function (see (Widrow and Hoff, 1960), (Ljung and Soderstrom, 1983), and <ref> (Haykin, 1994) </ref>). Convergence can be proven for particular linear systems, provided that the derivative estimates are unbiased, and the learning rate is annealed (decreased over time).
Reference: <author> Hill, D. L., Studholme, C., and Hawkes, D. J. </author> <year> (1994). </year> <title> Voxel Similarity Measures for Automated Image Registration. </title> <booktitle> In Proceedings of the Third Conference on Visualization in Biomedical Computing, </booktitle> <pages> pages 205 216. SPIE. </pages>
Reference-contexts: This metric can then be used both for image/model comparison and for pose refinement. Additional technical details on the relationship between mutual information and other measures of alignment may be found in (Viola, 1995). Alignment by extremizing properties of the joint signal has been used by Hill and Hawkes <ref> (Hill, Studholme and Hawkes, 1994) </ref> to align MRI, CT, and other medical image modalities. They use third order moments of the joint histogram to characterize the clustering of the joint data.
Reference: <author> Horn, B. </author> <year> (1986). </year> <title> Robot Vision. </title> <publisher> McGraw-Hill, </publisher> <address> New York. </address>
Reference-contexts: For any particular F and q, another function F q (u (x)) = F (u (x); q) can be defined. The combined function is best thought of as a reflectance map <ref> (Horn, 1986) </ref>. It maps the normals of an object directly into intensities. The three dimensional alignment procedure we will describe manipulates a similar combined function. How might Equation 6 be approximated efficiently? It seems reasonable to assume that for most real imaging functions similar inputs should yield similar outputs. <p> In this experiment, the camera has a viewing angle of 18 degrees. We represent T , the transformation from model to image coordinates, as a double quaternion followed by a perspective projection <ref> (Horn, 1986) </ref>. We used a vector difference metric for the normals. Assuming diagonal covariance matrices four different variances are necessary, three for the joint entropy estimate and one for the image entropy estimate. <p> As we've seen the 3D shape is sufficient to constrain the match between image and model. When multiple images of an object are available a technique called photometric stereo can 20 be used to estimate 3D shape <ref> (Horn, 1986) </ref>. Photometric stereo works with images which are taken from the same location but under different illumination conditions. It is assumed that detailed information both about illumination and surface properties are available for each image. As a result a reflectance map can be computed for each image.
Reference: <author> Huttenlocher, D., Kedem, K., Sharir, K., and Sharir, M. </author> <year> (1991). </year> <title> The Upper Envelope of Voronoi Surfaces and its Applications. </title> <booktitle> In Proceedings of the Seventh ACM Symposium on Computational Geometry, </booktitle> <pages> pages 194293. </pages>
Reference-contexts: There are many schemes that represent models and images by collections of edges and define a distance metric between them, Huttenlocher's use of the Hausdorff distance <ref> (Huttenlocher et al., 1991) </ref> is an example. Some methods use a metric that is proportional to the number of edges that coincide (see the excellent survey articles: (Besl and Jain, 1985; Chin and Dyer, 1986)).
Reference: <author> Linsker, R. </author> <year> (1986). </year> <title> From basic network principles to neural architecture. </title> <booktitle> Proceedings of the National Academy of Sciences, USA, 83:75087512, </booktitle> <volume> 83908394, </volume> <pages> 87798783. </pages>
Reference-contexts: None of these technique uses a non-parametric scheme for density/entropy estimation as we do. In most cases the distributions are assumed to be either binomial or Gaussian. Entropy and mutual information plays a role in the work of <ref> (Linsker, 1986) </ref>, (Becker and Hinton, 1992) and (Bell and Sejnowski, 1995). 23 24 25 Though the model is made up of multiple images, only one is shown here.
Reference: <author> Ljung, L. and Soderstrom, T. </author> <year> (1983). </year> <title> Theory and Practice of Recursive Identification. </title> <publisher> MIT Press. </publisher> <address> 28 Lowe, D. </address> <year> (1985). </year> <title> Perceptual Organization and Visual Recognition. </title> <publisher> Kluwer Academic Publish--ers. </publisher>
Reference-contexts: For smaller sample sizes, less effort is expended, but additional noise is introduced into the derivative estimates. Stochastic approximation is a scheme that uses noisy derivative estimate instead of the true derivative for optimizing a function (see (Widrow and Hoff, 1960), <ref> (Ljung and Soderstrom, 1983) </ref>, and (Haykin, 1994)). Convergence can be proven for particular linear systems, provided that the derivative estimates are unbiased, and the learning rate is annealed (decreased over time).
Reference: <author> Shashua, A. </author> <year> (1992). </year> <title> Geometry and Photometry in 3D Visual Recognition. </title> <type> PhD thesis, </type> <institution> M.I.T Artificial Intelligence Laboratory, AI-TR-1401. </institution>
Reference-contexts: We do not see a straightforward extension of this or similar eigenspace work to the problem of pose refinement. In other related work, Shashua has shown that all of the images, under different lighting, of a Lambertian surface are a linear combination of any 22 three of the images <ref> (Shashua, 1992) </ref>. A procedure for image alignment could be derived from this theory. In contrast, our image alignment method does not assume that the object has a Lambertian surface. Entropy is playing an ever increasing role within the field of neural networks.
Reference: <author> Turk, M. and Pentland, A. </author> <year> (1991). </year> <title> Face Recognition using Eigenfaces. </title> <booktitle> In Proceedings of the Computer Society Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 586 591, </pages> <address> Lahaina, Maui, Hawaii. </address> <publisher> IEEE. </publisher>
Reference-contexts: Turk and Pentland have used a large collection of face images to train a system to construct representations that are invariant to some changes in lighting and pose <ref> (Turk and Pentland, 1991) </ref>. These representations are a projection onto the largest eigenvectors of the distribution of images within the collection. Their system addresses the problem of recognition rather than alignment, and as a result much of the emphasis and many of the results are different.
Reference: <author> Viola, P. A. </author> <year> (1995). </year> <title> Alignment by Maximization of Mutual Information. </title> <type> PhD thesis, </type> <institution> Mas-sachusetts Institute of Technology. </institution>
Reference-contexts: Though weighted neighbor likelihood is a powerful technique, it has three significant drawbacks (see <ref> (Viola, 1995) </ref> for a more detailed discussion). Firstly, it will only work when the image is a function of the model. Though this was assumed at the outset, in several important applications the image data may not be a function of the model. <p> In practice, we have found that successful alignment may be obtained using relatively small sample sizes, for example N A = N B = 50. We have proven that the technique will always converge to a pose estimate that is close to locally optimal <ref> (Viola, 1995) </ref>. It has been observed that the noise introduced by the sampling can effectively penetrate small local minima. Such local minima are often characteristic of continuous alignment schemes, and we have found that local minima can be overcome in this manner in these applications as well. <p> Y Z j 4 j 10 10 5.94 5.56 6.11 5.11 .61 .53 5.49 3.22 100 20 20 10.12 12.04 10.77 11.56 1.11 .41 9.18 3.31 96 Table 1: Skull Alignments Results Table presence of occlusion because the measure of mutual information used is robust to outliers and noise (see <ref> (Viola, 1995) </ref> for further discussion). These experiments demonstrate that the alignment procedure is reliable when the initial pose is close to the correct pose. Outside of this range gradient descent, by itself, is not capable of converging to the correct solution. The capture range is not unreasonably small however. <p> This metric can then be used both for image/model comparison and for pose refinement. Additional technical details on the relationship between mutual information and other measures of alignment may be found in <ref> (Viola, 1995) </ref>. Alignment by extremizing properties of the joint signal has been used by Hill and Hawkes (Hill, Studholme and Hawkes, 1994) to align MRI, CT, and other medical image modalities. They use third order moments of the joint histogram to characterize the clustering of the joint data.
Reference: <author> Wells III, W. </author> <year> (1992). </year> <title> Statistical Object Recognition. </title> <type> PhD thesis, </type> <institution> MIT Department Electrical Engineering and Computer Science, </institution> <address> Cambridge, Mass. </address> <publisher> MIT AI Laboratory TR 1398. </publisher>
Reference: <author> Widrow, B. and Hoff, M. </author> <year> (1960). </year> <title> Adaptive switching circuits. </title> <booktitle> In 1960 IRE WESCON Convention Record, </booktitle> <volume> volume 4, </volume> <pages> pages 96104. </pages> <address> IRE, New York. </address> <month> 29 </month>
Reference-contexts: For smaller sample sizes, less effort is expended, but additional noise is introduced into the derivative estimates. Stochastic approximation is a scheme that uses noisy derivative estimate instead of the true derivative for optimizing a function (see <ref> (Widrow and Hoff, 1960) </ref>, (Ljung and Soderstrom, 1983), and (Haykin, 1994)). Convergence can be proven for particular linear systems, provided that the derivative estimates are unbiased, and the learning rate is annealed (decreased over time).
References-found: 19

