URL: http://www.cs.ucsd.edu/users/mihir/papers/epcp.ps.gz
Refering-URL: http://www.cs.ucsd.edu/users/mihir/papers/complexity-papers.html
Root-URL: http://www.cs.ucsd.edu
Title: Efficient Probabilistically Checkable Proofs and Applications to Approximation  
Author: M. Bellare S. Goldwasser C. Lund A. Russell 
Address: Mail Code  San Diego, 9500 Gilman Drive, La Jolla, CA 92093.  Square, Cambridge, MA 02139, USA.  Room 2C324, 600 Mountain Avenue, P. O. Box 636, Murray Hill, NJ 07974-0636, USA.  Square, Cambridge, MA 02139, USA.  
Affiliation: Department of Computer Science Engineering,  University of California at  MIT Laboratory for Computer Science, 545 Technology  AT&T Bell Laboratories,  MIT Laboratory for Computer Science, 545 Technology  
Note: Appears in Proceedings of the 25th Annual ACM Symposium on the Theory of Computing, ACM (1993).  lcs.mit.edu. Partially supported by NSF FAW grant No. 9023312-CCR, DARPA grant No. N00014-92-J-1799, and grant No. 89-00312 from the United States Israel Binational Science Foundation (BSF), Jerusalem, Israel.  mit.edu. Supported by a NSF Graduate Fellowship and by NSF grant 92-12184, AFOSR 89-0271, and DARPA N00014-92-J-1799.  
Email: E-mail: mihir@cs.ucsd.edu.  e-mail: shafi@theory.  email: lund@research.att.com.  e-mail: acr@theory.lcs.  
Phone: 0114,  
Date: May 1993  
Abstract: We construct multi-prover proof systems for NP which use only a constant number of provers to simultaneously achieve low error, low randomness and low answer size. As a consequence, we obtain asymptotic improvements to approximation hardness results for a wide range of optimization problems including minimum set cover, dominating set, maximum clique, chromatic number, and quartic programming; and constant factor improvements on the hardness results for MAXSNP problems. In particular, we show that approximating minimum set cover within any constant is NP-complete; approximating minimum set cover within fi(log n) implies NP DTIME(n log log n ); approximating the maximum of a quartic program within any constant is NP-hard; approximating maximum clique within n 1=30 implies NP BPP; approximating chromatic number within n 1=146 implies NP BPP; and approximating MAX-3SAT within 113=112 is NP-complete. 
Abstract-found: 1
Intro-found: 1
Reference: [AS] <author> S. Arora and S. Safra. </author> <title> Approximating clique is NP-complete. </title> <booktitle> Proceedings of the 33rd Annual IEEE Symposium on the Foundations of Computer Science, IEEE (1992). </booktitle>
Reference-contexts: 1 Introduction The last two years have witnessed major advances in classifying the complexity of approximating classical optimization problems <ref> [Co, FGLSS, AS, ALMSS, BR, FL, Be, Zu, LY1, LY2] </ref>. <p> important problems P , with values of Q and T which differ from problem to problem; for example, it was shown by [LY1] that approximating the size of the minimum set cover to within fi (log N ) implies NP DTIME (n polylog n ), and it was shown by <ref> [FGLSS, AS, ALMSS] </ref> that for some constant c &gt; 0 approximating the size of a maximum clique in a graph within factor n c implies that P = NP. <p> We focus on two of them. The first is the (single round version of the) multi-prover model of Ben-Or, Gold-wasser, Kilian and Wigderson [BGKW]. The second is the "oracle" model of Fortnow, Rompel and Sipser [FRS], renamed "probabilistically checkable proofs" by Arora and Safra <ref> [AS] </ref>. In each case, we may distinguish five parameters which we denote by r; p; a; q and * (all are in general functions of the input length n). <p> We let MIP 1 = MIP 1 [poly (n); poly (n); poly (n); poly (n); 1=2]. It is known that MIP 1 = NEXP [BFL]. Probabilistically checkable proofs, as defined in <ref> [AS, ALMSS] </ref> are the same as what [FRS] had earlier called the oracle model. In this model, a verifier V has access to an input x 2 f0; 1g n and an oracle which is regarded as a function from f0; 1g q to f0; 1g a . <p> We also let PCP [r; p] = PCP [r; p; 1; O (r); 1=2] as defined in <ref> [AS] </ref>. It is known that PCP = MIP 1 [FRS]. Furthermore, it is easy to see that MIP 1 [r; p; a; q; *] PCP [r; p; a; q; *]. But whether or not the converse containment is true remains an open question. <p> So the total error is at most 2* = 2 k . We omit the details. 3.2 Reducing Answer Sizes Answer sizes will be reduced by recursion (cf. <ref> [AS] </ref>). First need to define carefully how we look at MIP 1 verifiers. Let V be a MIP 1 [r; p; a; q; 2 k (n) ] verifier for some language L. <p> The proof will use Theorem 1.1 and ideas in <ref> [AS, ALMSS] </ref>. The main improvement over the construction of [ALMSS] is our improved recursion step (Lemma 3.2 and Lemmas 3.5 and 3.10) which allows us to combine proof systems with almost no increase in the error probability.
Reference: [ALMSS] <author> S. Arora, C. Lund, R. Motwani, M. Su-dan and M. Szegedy. </author> <title> Proof verification and intractability of approximation problems. </title> <booktitle> Proceedings of the 33rd Annual IEEE Symposium on the Foundations of Computer Science, IEEE (1992). </booktitle>
Reference-contexts: 1 Introduction The last two years have witnessed major advances in classifying the complexity of approximating classical optimization problems <ref> [Co, FGLSS, AS, ALMSS, BR, FL, Be, Zu, LY1, LY2] </ref>. <p> important problems P , with values of Q and T which differ from problem to problem; for example, it was shown by [LY1] that approximating the size of the minimum set cover to within fi (log N ) implies NP DTIME (n polylog n ), and it was shown by <ref> [FGLSS, AS, ALMSS] </ref> that for some constant c &gt; 0 approximating the size of a maximum clique in a graph within factor n c implies that P = NP. <p> We omit the question size q from the table because it is in all cases O (r) and doesn't matter in reductions anyway. The main result of <ref> [ALMSS] </ref>, which states that there is a constant t such that NP = PCP [O (log n); t; 1; O (log n); 1=2], is incorporated as the 1 It is easy to see that MIP 1 [r; p; a; q; *] PCP [r; p; a; q; *], but the converse containment <p> For explanations and more information we refer the reader to x2. special case k (n) = 1 of the result shown in (2). The re-sult shown in (1) is obtained as follows. First apply the transformation of [FRS] to the <ref> [ALMSS] </ref> result to get NP MIP 1 [O (log n); 2; t; O (logn); 1 1=(2t)] where t is the constant from [ALMSS] as above. Then apply [Fe] to bring the error to any constant strictly greater than 1=2, at constant factor cost in the other parameters. <p> The re-sult shown in (1) is obtained as follows. First apply the transformation of [FRS] to the <ref> [ALMSS] </ref> result to get NP MIP 1 [O (log n); 2; t; O (logn); 1 1=(2t)] where t is the constant from [ALMSS] as above. Then apply [Fe] to bring the error to any constant strictly greater than 1=2, at constant factor cost in the other parameters. Comparing these results with ours, we note the following features, all of which are important to our applications. <p> On the other hand, the number of provers we use is four rather than the two achieved in the results (1) and (3), but this will suffice for our applications to approximation. The (constant) number of bits t that one needs to check in the <ref> [ALMSS] </ref> result NP PCP [O (log n); t; 1; O (log n); 1=2] is of the order of 10 4 . Some reductions in this value were obtained by [PS]. <p> Some reductions in this value were obtained by [PS]. Our improved complexity of four prover proofs for NP in terms of randomness and answer size for a given error, together with a careful analysis of the <ref> [ALMSS] </ref> construction and proofs enable us to obtain substantially smaller values than previously known. Specifically, focusing on the expected value, we show the following. Theorem 1.2 NP = PCP 0 [O (log n); 29; 1; O (logn); 1=2]. <p> Then NP DTIME (n log log n ). Similar improvements follow for all of the following problems: dominating set, hitting set, hypergraph transversal, minimum exact cover (cf. [KT, LY1]). Max Clique, Chromatic number. Known results for Max-Clique are based on the result of <ref> [ALMSS] </ref> which states that there are constants t; d such that NP = PCP [d log n; t; 1; O (log n); 1=2]. <p> Then NP BPP. The factor can be increased to n 1=121 if the conclusion is weakened to NEXP BPEXP. Max-3SAT. Using the same characterization of NP as for Max-Clique, it was also shown by <ref> [ALMSS] </ref> that there exists a constant c &gt; 1 such that approximating the maximum number of simultaneously satisfiable clauses in a 3SAT instance to within factor c is NP-complete. We can prove the same with a higher value of c than previously achieved. <p> we said that the best known value of the constant error * achievable for the result NP MIP 1 [O (log n); 2; O (1); O (log n); *] was a value close to 1, namely * = 1 1=(2t) where t is the number of bits queried in the <ref> [ALMSS] </ref> proof. However the result of [Fe] can be used to bring * to any constant greater than 1=2, as now indicated in Figure 1 (1). <p> We let MIP 1 = MIP 1 [poly (n); poly (n); poly (n); poly (n); 1=2]. It is known that MIP 1 = NEXP [BFL]. Probabilistically checkable proofs, as defined in <ref> [AS, ALMSS] </ref> are the same as what [FRS] had earlier called the oracle model. In this model, a verifier V has access to an input x 2 f0; 1g n and an oracle which is regarded as a function from f0; 1g q to f0; 1g a . <p> It is obtained by applying standard transformations (cf. [BGKW, FRS]) to the NP PCP [O (log n); O (1); 1; O (log n); 1=2] result of <ref> [ALMSS] </ref>. Lemma 2.1 Suppose k (n) O (log n). Then NP MIP 1 [O (k (n) log n); O (k (n)); O (1); O (log n); 2 k (n) ]. <p> The proof will use Theorem 1.1 and ideas in <ref> [AS, ALMSS] </ref>. The main improvement over the construction of [ALMSS] is our improved recursion step (Lemma 3.2 and Lemmas 3.5 and 3.10) which allows us to combine proof systems with almost no increase in the error probability. <p> The proof will use Theorem 1.1 and ideas in [AS, ALMSS]. The main improvement over the construction of <ref> [ALMSS] </ref> is our improved recursion step (Lemma 3.2 and Lemmas 3.5 and 3.10) which allows us to combine proof systems with almost no increase in the error probability. <p> Furthermore we have an improved analysis of the linearity test in [BLR] and of the matrix multiplication test for the special case needed in <ref> [ALMSS] </ref>. We will now sketch our construction. Some knowledge of the proof in [ALMSS] is assumed of the reader. First we need some different encoding schemes. Let X be an n-bit string. <p> Furthermore we have an improved analysis of the linearity test in [BLR] and of the matrix multiplication test for the special case needed in <ref> [ALMSS] </ref>. We will now sketch our construction. Some knowledge of the proof in [ALMSS] is assumed of the reader. First we need some different encoding schemes. Let X be an n-bit string. <p> Let W 0 = (W j W k ) j&lt;k . If the proof is valid then Z R the assignments to z R equals E R (A 1 ; : : : ; B 4 ; W; W 0 ). 8 In <ref> [ALMSS] </ref> it was shown how to probabilistically verify such proofs. Assume that (Y 0 1 ; : : :; Y 0 such a proof and that l = jA 1 j + jA 2 j + + jW 0 j. Hence jZ R j = 2 l for every R. <p> : Thus the test is the following: Pick v; u; w 2 R f0; 1g l and s; t 2 R f0; 1g jW j and test that ( su + u )( tv + v ) = ( s@tw + w ): Consistency Test (T R 3 ): In <ref> [ALMSS] </ref> they showed that from D and s 2 f0; 1g jW j it is possible to com pute an index v s of E R (A 1 ; : : : ; B 4 ; W; W 0 ) and a bit b s such that for all W if <p> The first case follows from a Lemma in <ref> [BLR, ALMSS] </ref>. Hence assume that we are in case two. Let S be the set of u 2 f0; 1g l such that u 6= E R (Y ) u . Let q be the probability that u + v 6= u+v and let s = jSj=2 l . <p> Their proofs are omitted in this abstract. Lemma 3.8 If P R 1 and :P R 2 then the probability that T R 2 rejects is at least 3ffi 02 8 (1 2ffi 0 ). Lemma 3.9 <ref> [ALMSS] </ref> If P R 1 ; P R 3 then the probability that T R 3 rejects is at least (1=2)(1 2ffi 0 ).
Reference: [ADP] <author> G. Ausiello, A. D'Atri and M. Protasi. </author> <title> Structure preserving reductions among convex optimization problems. </title> <journal> Journal of Computer and System Sciences 21, </journal> <month> 136-153 </month> <year> (1980). </year>
Reference-contexts: Note that we are maximizing over a subset of R n ; solutions are not restricted to integers. We denote by f fl the maximum of f over the feasible region, and by f fl the minimum. Following <ref> [ADP, Va] </ref> we say that ~ f is a -approximation, where 2 [0; 1], if jf fl ~ f j jf fl f fl j.
Reference: [BFL] <author> L. Babai, L. Fortnow and C. Lund. </author> <title> NonDeterministic Exponential Time has Two-Prover Interactive Protocols. </title> <booktitle> Proceedings of the 31st Annual IEEE Symposium on the Foundations of Computer Science, IEEE (1990). </booktitle>
Reference-contexts: When complexity is ignored the models are of course the same <ref> [FRS, BFL] </ref>. For explanations and more information we refer the reader to x2. special case k (n) = 1 of the result shown in (2). The re-sult shown in (1) is obtained as follows. <p> MIP 1 [r; p; a; q; *] denotes the class of languages possessing MIP 1 [r; p; a; q; *] verifiers. We let MIP 1 = MIP 1 [poly (n); poly (n); poly (n); poly (n); 1=2]. It is known that MIP 1 = NEXP <ref> [BFL] </ref>. Probabilistically checkable proofs, as defined in [AS, ALMSS] are the same as what [FRS] had earlier called the oracle model.
Reference: [BFLS] <author> L. Babai, L. Fortnow, L. Levin, and M. Szegedy. </author> <title> Checking Computations in Polyloga-rithmic Time. </title> <booktitle> Proceedings of the 23rd Annual ACM Symposium on the Theory of Computing, ACM (1991). </booktitle>
Reference-contexts: Let us now proceed to our first step. 3.1 Reducing Randomness As discussed above, the transformation of [FL, LS] reduces the error without increasing the number of provers, but costs in randomness. We combine this transformation with the idea of <ref> [BFLS] </ref> of using as "base field" not f0; 1g but some larger subset H of the underlying finite field F . <p> = 1 there exists an 1 i p such that (X 0 i ; E P (X i )) 1 2 k 0 (n) , then Pr R 0 [C 0 R 0 (X 0 p ; Z 1 ; Z 2 ) = Proof: The proof combines ideas in <ref> [BFLS, BLR, FRS, LS, FL] </ref>. Step 1: First we obtain a circuit family fC 00 1 ]; [x 0 p ]; z)g; where the circuit size and number of additional random bits are poly (log jCj; k 0 (n)). <p> We use the construction of <ref> [BFLS] </ref> to construct these proofs. From [BFLS] it follows that we verify with probability 1/2 that W 0 has the correct properties by using poly (log jCj) random bits, reading poly (log jCj) bits and perform ing a computation that corresponds to circuits of size poly (log jCj). <p> We use the construction of <ref> [BFLS] </ref> to construct these proofs. From [BFLS] it follows that we verify with probability 1/2 that W 0 has the correct properties by using poly (log jCj) random bits, reading poly (log jCj) bits and perform ing a computation that corresponds to circuits of size poly (log jCj).
Reference: [Be] <author> M. Bellare. </author> <title> Interactive Proofs and Approximation: Reductions from Two Provers in One Round. </title> <booktitle> Proceedings of the 2nd Israel Symposium on Theory and Computing Systems, </booktitle> <address> IEEE (June 1993). </address> <note> Preliminary version: IBM Research Report RC 17969 (May 1992). 11 </note>
Reference-contexts: 1 Introduction The last two years have witnessed major advances in classifying the complexity of approximating classical optimization problems <ref> [Co, FGLSS, AS, ALMSS, BR, FL, Be, Zu, LY1, LY2] </ref>. <p> However, the reduction, like those of <ref> [BR, FL, Be] </ref>, additionally requires that the number of provers be p = 2, although (in this case) it can be extended easily to any constant.
Reference: [BGG] <author> M. Bellare, O. Goldreich and S. Gold--wasser. </author> <title> Randomness in Interactive Proofs. </title> <booktitle> Proceedings of the 31st Annual IEEE Symposium on the Foundations of Computer Science, IEEE (1990). </booktitle>
Reference-contexts: See Lemma 4.1 for the precise tradeoff. 3 c &gt; 0 such that approximating the size of the maximum clique in a graph to within n c is NP-complete; this result uses randomness efficient error reduction techniques such as <ref> [CW, IZ, BGG] </ref>, and c depends on t; d as well as other constants arising from the error-reduction. Zuck-erman [Zu] uses a random construction which achieves c = 1=(1 + t) at the cost of weakening the conclusion to NP BPP.
Reference: [BR] <author> M. Bellare and P. Rogaway. </author> <title> The Complexity of Approximating a Nonlinear program. Complexity of Numerical Optimization, Ed. P.M. Parda-los, World Scientific (1993). </title> <note> Preliminary version: IBM Research Report RC 17831 (March 1992). </note>
Reference-contexts: 1 Introduction The last two years have witnessed major advances in classifying the complexity of approximating classical optimization problems <ref> [Co, FGLSS, AS, ALMSS, BR, FL, Be, Zu, LY1, LY2] </ref>. <p> However, the reduction, like those of <ref> [BR, FL, Be] </ref>, additionally requires that the number of provers be p = 2, although (in this case) it can be extended easily to any constant. <p> We know that there exists a constant c 2 (0; 1) such that the following is true: the existence of a polynomial time algorithm for c-approximating 3 the maximum of a quartic program implies P = NP; this follows from the corresponding result of <ref> [BR, FL] </ref> for quadratic programming. Based on Theorem 1.1 we can improve this to show the same for any constant. 3 The definition of approximation used in this context of "continuous" optimization problems is not the usual one of approximating to within a multiplicative factor; see x4.3 for details. <p> Quartic programming is important in applications such as the "molecule embedding problem." The reduction of two prover proofs to quadratic programming in <ref> [BR, FL] </ref> is easily extended to a reduction of four prover proofs to quartic programming. Theorem 1.1 guarantees for us four prover proof sys tems for NP with logarithmic randomness and answer sizes, and error an arbitrary constant. Put together these facts enable us to prove Theorem 1.8.
Reference: [BGKW] <author> M. Ben-Or, S. Goldwasser, J. Kilian and A. Wigderson. </author> <title> Multi-Prover Interactive Proofs: How to Remove Intractability Assumptions. </title> <booktitle> Proceedings of the 20th Annual ACM Symposium on the Theory of Computing, ACM (1988). </booktitle>
Reference-contexts: We focus on two of them. The first is the (single round version of the) multi-prover model of Ben-Or, Gold-wasser, Kilian and Wigderson <ref> [BGKW] </ref>. The second is the "oracle" model of Fortnow, Rompel and Sipser [FRS], renamed "probabilistically checkable proofs" by Arora and Safra [AS]. In each case, we may distinguish five parameters which we denote by r; p; a; q and * (all are in general functions of the input length n). <p> It is obtained by applying standard transformations (cf. <ref> [BGKW, FRS] </ref>) to the NP PCP [O (log n); O (1); 1; O (log n); 1=2] result of [ALMSS]. Lemma 2.1 Suppose k (n) O (log n). Then NP MIP 1 [O (k (n) log n); O (k (n)); O (1); O (log n); 2 k (n) ].
Reference: [BLR] <author> M. Blum, M. Luby, and R. Rubinfeld. </author> <title> Self-testing and self-correcting programs, with applications to numerical programs. </title> <booktitle> Proceedings of the 22nd Annual ACM Symposium on the Theory of Computing, ACM (1990). </booktitle>
Reference-contexts: = 1 there exists an 1 i p such that (X 0 i ; E P (X i )) 1 2 k 0 (n) , then Pr R 0 [C 0 R 0 (X 0 p ; Z 1 ; Z 2 ) = Proof: The proof combines ideas in <ref> [BFLS, BLR, FRS, LS, FL] </ref>. Step 1: First we obtain a circuit family fC 00 1 ]; [x 0 p ]; z)g; where the circuit size and number of additional random bits are poly (log jCj; k 0 (n)). <p> Lastly, assuming that there exists some W such that (W 0 ; E P (W )) &lt; 1=3, we use the idea of self-correction <ref> [BLR, GS] </ref>, which allows us to probabilistically access the segments of X 00 i . <p> The main improvement over the construction of [ALMSS] is our improved recursion step (Lemma 3.2 and Lemmas 3.5 and 3.10) which allows us to combine proof systems with almost no increase in the error probability. Furthermore we have an improved analysis of the linearity test in <ref> [BLR] </ref> and of the matrix multiplication test for the special case needed in [ALMSS]. We will now sketch our construction. Some knowledge of the proof in [ALMSS] is assumed of the reader. First we need some different encoding schemes. Let X be an n-bit string. <p> Since the values of W and W 0 is not directly accessible in the technique of self-correction is used <ref> [BLR] </ref>. This technique give us a probabilistic method to access the bits encoded in . <p> The first case follows from a Lemma in <ref> [BLR, ALMSS] </ref>. Hence assume that we are in case two. Let S be the set of u 2 f0; 1g l such that u 6= E R (Y ) u . Let q be the probability that u + v 6= u+v and let s = jSj=2 l .
Reference: [CW] <author> A. Cohen and A. Wigderson. Dispersers, </author> <title> Deterministic Amplification, and Weak Random Sources. </title> <booktitle> Proceedings of the 30th Annual IEEE Symposium on the Foundations of Computer Science, IEEE (1989). </booktitle>
Reference-contexts: reduction used. 1 r = r (n) p = p (n) a = a (n) * = *(n) How (in a word) (1) O (log n) 2 O (1) 1 2 + * fl [ALMSS]+[FRS]+[Fe] (2) O (log n) O (k (n)) O (1) 2 k (n) O (k (n)) <ref> [CW, IZ] </ref>-style repetitions of (1). (3) O (k (n) log 2 n) 2 O (k (n) log 2 n) 2 k (n) [FL] (4) O (k (n) log n) + poly (k (n); log log n) 4 poly (k (n); log log n) 2 k (n) This paper. of the form <p> See Lemma 4.1 for the precise tradeoff. 3 c &gt; 0 such that approximating the size of the maximum clique in a graph to within n c is NP-complete; this result uses randomness efficient error reduction techniques such as <ref> [CW, IZ, BGG] </ref>, and c depends on t; d as well as other constants arising from the error-reduction. Zuck-erman [Zu] uses a random construction which achieves c = 1=(1 + t) at the cost of weakening the conclusion to NP BPP. <p> Lemma 2.1 Suppose k (n) O (log n). Then NP MIP 1 [O (k (n) log n); O (k (n)); O (1); O (log n); 2 k (n) ]. Note we could further reduce the randomness to O (log n) by using the techniques of <ref> [CW, IZ] </ref>; this is the result we stated in Figure 1 (2). But the advantage will be lost in the transformations we will apply later, so we don't bother. 3 Efficient Proof Systems We prove our main theorem in two steps.
Reference: [Co] <author> A. Condon. </author> <title> The complexity of the max word problem, or the power of one-way interactive proof systems. </title> <note> STACS 91. </note>
Reference-contexts: 1 Introduction The last two years have witnessed major advances in classifying the complexity of approximating classical optimization problems <ref> [Co, FGLSS, AS, ALMSS, BR, FL, Be, Zu, LY1, LY2] </ref>.
Reference: [FRS] <author> L. Fortnow, J. Rompel and M. Sipser. </author> <title> On the power of multiprover interactive protocols. </title> <note> Structures 1988. </note>
Reference-contexts: We focus on two of them. The first is the (single round version of the) multi-prover model of Ben-Or, Gold-wasser, Kilian and Wigderson [BGKW]. The second is the "oracle" model of Fortnow, Rompel and Sipser <ref> [FRS] </ref>, renamed "probabilistically checkable proofs" by Arora and Safra [AS]. In each case, we may distinguish five parameters which we denote by r; p; a; q and * (all are in general functions of the input length n). <p> When complexity is ignored the models are of course the same <ref> [FRS, BFL] </ref>. For explanations and more information we refer the reader to x2. special case k (n) = 1 of the result shown in (2). The re-sult shown in (1) is obtained as follows. <p> For explanations and more information we refer the reader to x2. special case k (n) = 1 of the result shown in (2). The re-sult shown in (1) is obtained as follows. First apply the transformation of <ref> [FRS] </ref> to the [ALMSS] result to get NP MIP 1 [O (log n); 2; t; O (logn); 1 1=(2t)] where t is the constant from [ALMSS] as above. Then apply [Fe] to bring the error to any constant strictly greater than 1=2, at constant factor cost in the other parameters. <p> We let MIP 1 = MIP 1 [poly (n); poly (n); poly (n); poly (n); 1=2]. It is known that MIP 1 = NEXP [BFL]. Probabilistically checkable proofs, as defined in [AS, ALMSS] are the same as what <ref> [FRS] </ref> had earlier called the oracle model. In this model, a verifier V has access to an input x 2 f0; 1g n and an oracle which is regarded as a function from f0; 1g q to f0; 1g a . <p> We also let PCP [r; p] = PCP [r; p; 1; O (r); 1=2] as defined in [AS]. It is known that PCP = MIP 1 <ref> [FRS] </ref>. Furthermore, it is easy to see that MIP 1 [r; p; a; q; *] PCP [r; p; a; q; *]. But whether or not the converse containment is true remains an open question. In particular, known simulations of probabilistically checkable proofs by multi-prover ones (such as those used by [FRS] <p> <ref> [FRS] </ref>. Furthermore, it is easy to see that MIP 1 [r; p; a; q; *] PCP [r; p; a; q; *]. But whether or not the converse containment is true remains an open question. In particular, known simulations of probabilistically checkable proofs by multi-prover ones (such as those used by [FRS] to show PCP = MIP 1 ) don't preserve complexity. 4 Let us now state the (well known) lemma which will be our starting point. <p> It is obtained by applying standard transformations (cf. <ref> [BGKW, FRS] </ref>) to the NP PCP [O (log n); O (1); 1; O (log n); 1=2] result of [ALMSS]. Lemma 2.1 Suppose k (n) O (log n). Then NP MIP 1 [O (k (n) log n); O (k (n)); O (1); O (log n); 2 k (n) ]. <p> Choosing h = log jHj appropriately we need use only O (k (n) log n) random bits to get error 2 k (n) at the cost of answer sizes which grow exponentially in h. 4 The basic result of <ref> [FRS] </ref> is PCP [r; p; a; q; *] MIP 1 [r + lg p; 2; a; q; 1 (1 *)=p]. Theorem 3.1 Let k = k (n) O (log n) be any func-tion. Let h (n) = max (k (n); log log n). <p> = 1 there exists an 1 i p such that (X 0 i ; E P (X i )) 1 2 k 0 (n) , then Pr R 0 [C 0 R 0 (X 0 p ; Z 1 ; Z 2 ) = Proof: The proof combines ideas in <ref> [BFLS, BLR, FRS, LS, FL] </ref>. Step 1: First we obtain a circuit family fC 00 1 ]; [x 0 p ]; z)g; where the circuit size and number of additional random bits are poly (log jCj; k 0 (n)). <p> Step 2: Next we transform the C 00 circuits into circuits fC 000 R 00 ;R 000 ([x 0 p ]; [z (1) ]; : : : ; [z (p 0 ) ])g (i.e., a one-round p + p 0 -prover proof system) as follows using the standard method in <ref> [FRS] </ref>. (For every j = 1; 2; : : :; p 0 , Z (j) should equal Z.) Assume that C 00 R 00 depends on the variables z i 1 ; : : : ; z i q . Note that q jC 00 R 00 j.
Reference: [Fe] <author> U. Feige. </author> <title> On the Success Probability of the Two Provers in One Round Proof Systems. </title> <note> Structures 1991. </note>
Reference-contexts: The re-sult shown in (1) is obtained as follows. First apply the transformation of [FRS] to the [ALMSS] result to get NP MIP 1 [O (log n); 2; t; O (logn); 1 1=(2t)] where t is the constant from [ALMSS] as above. Then apply <ref> [Fe] </ref> to bring the error to any constant strictly greater than 1=2, at constant factor cost in the other parameters. Comparing these results with ours, we note the following features, all of which are important to our applications. <p> However the result of <ref> [Fe] </ref> can be used to bring * to any constant greater than 1=2, as now indicated in Figure 1 (1).
Reference: [FGLSS] <author> U. Feige, S. Goldwasser, L. Lov asz, S. Safra, and M. Szegedy. </author> <title> Approximating clique is almost NP-complete. </title> <booktitle> Proceedings of the 32nd Annual IEEE Symposium on the Foundations of Computer Science, IEEE (1991). </booktitle>
Reference-contexts: 1 Introduction The last two years have witnessed major advances in classifying the complexity of approximating classical optimization problems <ref> [Co, FGLSS, AS, ALMSS, BR, FL, Be, Zu, LY1, LY2] </ref>. <p> important problems P , with values of Q and T which differ from problem to problem; for example, it was shown by [LY1] that approximating the size of the minimum set cover to within fi (log N ) implies NP DTIME (n polylog n ), and it was shown by <ref> [FGLSS, AS, ALMSS] </ref> that for some constant c &gt; 0 approximating the size of a maximum clique in a graph within factor n c implies that P = NP. <p> Max Clique, Chromatic number. Known results for Max-Clique are based on the result of [ALMSS] which states that there are constants t; d such that NP = PCP [d log n; t; 1; O (log n); 1=2]. Using the reduction of <ref> [FGLSS] </ref>, it follows that there exists a constant 2 This is a simplification of the actual situation, but will suffice for the purpose of this discussion. <p> The changes are summarized below. In the STOC version we said that the variant of the <ref> [FGLSS] </ref> clique reduction given in [Zu] shows that approximating clique to within n 1=t implies NP BPP, and thus had Theorem 1.5 with a factor of n 1=29 .
Reference: [FL] <author> U. Feige and L. Lov asz. </author> <title> Two-Prover One Round Proof Systems: Their Power and their Problems. </title> <booktitle> Proceedings of the 24th Annual ACM Symposium on the Theory of Computing, ACM (1992). </booktitle>
Reference-contexts: 1 Introduction The last two years have witnessed major advances in classifying the complexity of approximating classical optimization problems <ref> [Co, FGLSS, AS, ALMSS, BR, FL, Be, Zu, LY1, LY2] </ref>. <p> (1) O (log n) 2 O (1) 1 2 + * fl [ALMSS]+[FRS]+[Fe] (2) O (log n) O (k (n)) O (1) 2 k (n) O (k (n)) [CW, IZ]-style repetitions of (1). (3) O (k (n) log 2 n) 2 O (k (n) log 2 n) 2 k (n) <ref> [FL] </ref> (4) O (k (n) log n) + poly (k (n); log log n) 4 poly (k (n); log log n) 2 k (n) This paper. of the form NP MIP 1 [r; p; a; q; *]. <p> Second, the number of provers we use is a constant independent of the error, which is not true of the result shown in (2). Finally, for small k (n) and given error 2 k (n) our randomness and answer sizes are smaller than those of the result of <ref> [FL] </ref> shown in (3); in particular, for k (n) = log o (1) n we have O (k (n) log n) randomness and answer sizes, and for k (n) = O (log log n) we have polyloglog (n) answer sizes. <p> However, the reduction, like those of <ref> [BR, FL, Be] </ref>, additionally requires that the number of provers be p = 2, although (in this case) it can be extended easily to any constant. <p> We know that there exists a constant c 2 (0; 1) such that the following is true: the existence of a polynomial time algorithm for c-approximating 3 the maximum of a quartic program implies P = NP; this follows from the corresponding result of <ref> [BR, FL] </ref> for quadratic programming. Based on Theorem 1.1 we can improve this to show the same for any constant. 3 The definition of approximation used in this context of "continuous" optimization problems is not the usual one of approximating to within a multiplicative factor; see x4.3 for details. <p> The starting point is the proof system of Lemma 2.1. This proof system has O (k (n)) provers, and we need to reduce this to a constant. Applying the transformation of <ref> [LS, FL] </ref> will not suffice, because in reducing the number of provers this transformation increases the amount of randomness as well as the answer sizes (cf. Figure 1 (3)). We tackle this problem by concentrating first on keeping the randomness down. <p> However, in a second step we apply recursion to reduce the answer sizes while keeping the other quantities under control. Let us now proceed to our first step. 3.1 Reducing Randomness As discussed above, the transformation of <ref> [FL, LS] </ref> reduces the error without increasing the number of provers, but costs in randomness. We combine this transformation with the idea of [BFLS] of using as "base field" not f0; 1g but some larger subset H of the underlying finite field F . <p> = 1 there exists an 1 i p such that (X 0 i ; E P (X i )) 1 2 k 0 (n) , then Pr R 0 [C 0 R 0 (X 0 p ; Z 1 ; Z 2 ) = Proof: The proof combines ideas in <ref> [BFLS, BLR, FRS, LS, FL] </ref>. Step 1: First we obtain a circuit family fC 00 1 ]; [x 0 p ]; z)g; where the circuit size and number of additional random bits are poly (log jCj; k 0 (n)). <p> Step 3: Lastly we transform the C 000 circuits into the C 0 circuits using the same construction as in Theorem 3.1. I.e., we use the transformation from p 0 provers into 2 provers in <ref> [LS, FL] </ref> on the inputs z (j) to get segmented circuits with p + 2 inputs. This will use an additional O (p 0 log (jzj)k 0 (n)) random bits. <p> Quartic programming is important in applications such as the "molecule embedding problem." The reduction of two prover proofs to quadratic programming in <ref> [BR, FL] </ref> is easily extended to a reduction of four prover proofs to quartic programming. Theorem 1.1 guarantees for us four prover proof sys tems for NP with logarithmic randomness and answer sizes, and error an arbitrary constant. Put together these facts enable us to prove Theorem 1.8.
Reference: [GS] <author> P. Gemmell and M. Sudan. </author> <title> Highly Resilient Correctors For Polynomials. </title> <journal> IPL, </journal> <year> 1992. </year>
Reference-contexts: Lastly, assuming that there exists some W such that (W 0 ; E P (W )) &lt; 1=3, we use the idea of self-correction <ref> [BLR, GS] </ref>, which allows us to probabilistically access the segments of X 00 i .
Reference: [IZ] <author> R. Impagliazzo and D. Zuckerman. </author> <title> How to Recycle Random Bits. </title> <booktitle> Proceedings of the 30th Annual IEEE Symposium on the Foundations of Computer Science, IEEE (1989). </booktitle>
Reference-contexts: reduction used. 1 r = r (n) p = p (n) a = a (n) * = *(n) How (in a word) (1) O (log n) 2 O (1) 1 2 + * fl [ALMSS]+[FRS]+[Fe] (2) O (log n) O (k (n)) O (1) 2 k (n) O (k (n)) <ref> [CW, IZ] </ref>-style repetitions of (1). (3) O (k (n) log 2 n) 2 O (k (n) log 2 n) 2 k (n) [FL] (4) O (k (n) log n) + poly (k (n); log log n) 4 poly (k (n); log log n) 2 k (n) This paper. of the form <p> See Lemma 4.1 for the precise tradeoff. 3 c &gt; 0 such that approximating the size of the maximum clique in a graph to within n c is NP-complete; this result uses randomness efficient error reduction techniques such as <ref> [CW, IZ, BGG] </ref>, and c depends on t; d as well as other constants arising from the error-reduction. Zuck-erman [Zu] uses a random construction which achieves c = 1=(1 + t) at the cost of weakening the conclusion to NP BPP. <p> Lemma 2.1 Suppose k (n) O (log n). Then NP MIP 1 [O (k (n) log n); O (k (n)); O (1); O (log n); 2 k (n) ]. Note we could further reduce the randomness to O (log n) by using the techniques of <ref> [CW, IZ] </ref>; this is the result we stated in Figure 1 (2). But the advantage will be lost in the transformations we will apply later, so we don't bother. 3 Efficient Proof Systems We prove our main theorem in two steps.
Reference: [Jo] <author> D. Johnson. </author> <title> Approximation algorithms for combinatorial problems. </title> <journal> J. of Computer and System Sciences 9, </journal> <month> 256-278 </month> <year> (1974). </year>
Reference-contexts: For a definition of the problem we refer the reader to x4.1. Recall that there exists a polynomial time algorithm for approximating the size of the minimum set cover to within a factor of fi (log N ), where N is the number of elements in the base set <ref> [Jo, Lo] </ref>. Hardness of approximation was recently shown by Lund and Yannakakis [LY1].
Reference: [KT] <author> P. Kolaitis and M. Thakur. </author> <title> Approximation properties of NP minimization classes. Structure in Complexity Theory, </title> <year> 1991. </year>
Reference-contexts: Suppose there is a polynomial time algorithm which approximates the size of the minimum set cover to within c log N . Then NP DTIME (n log log n ). Similar improvements follow for all of the following problems: dominating set, hitting set, hypergraph transversal, minimum exact cover (cf. <ref> [KT, LY1] </ref>). Max Clique, Chromatic number. Known results for Max-Clique are based on the result of [ALMSS] which states that there are constants t; d such that NP = PCP [d log n; t; 1; O (log n); 1=2].
Reference: [LS] <author> D. Lapidot and A. Shamir. </author> <title> Fully Parallelized Multi-Prover Protocols for NEXP-time. </title> <booktitle> Proceedings of the 32nd Annual IEEE Symposium on the Foundations of Computer Science, IEEE (1991). </booktitle>
Reference-contexts: The starting point is the proof system of Lemma 2.1. This proof system has O (k (n)) provers, and we need to reduce this to a constant. Applying the transformation of <ref> [LS, FL] </ref> will not suffice, because in reducing the number of provers this transformation increases the amount of randomness as well as the answer sizes (cf. Figure 1 (3)). We tackle this problem by concentrating first on keeping the randomness down. <p> However, in a second step we apply recursion to reduce the answer sizes while keeping the other quantities under control. Let us now proceed to our first step. 3.1 Reducing Randomness As discussed above, the transformation of <ref> [FL, LS] </ref> reduces the error without increasing the number of provers, but costs in randomness. We combine this transformation with the idea of [BFLS] of using as "base field" not f0; 1g but some larger subset H of the underlying finite field F . <p> Regard each Q i as divided into s pieces, each of size h (that is, Q i = Q 1 i : : : Q s i with each Q j that Q i is an element of H s F s . We now apply the technique of <ref> [LS] </ref>. V 0 chooses, randomly and uniformly, y 1 ; : : : ; y p from F s . For each i we let l i : F ! F s be (a canonical representation of) the unique line through Q i and y i . <p> Since h (n) log log n and q = O (log n) this is O (k (n) log n 2 h (n) ) as desired. To prove the soundness, we need to trace through the proof of <ref> [LS] </ref>, making the appropriate modifica tions. The error stemming from the construction of [LS] (i.e. the probability that the provers' replies are not "functional") can be bounded by p sd which is at most 2 h1 2 k1 = * for large enough n. <p> To prove the soundness, we need to trace through the proof of <ref> [LS] </ref>, making the appropriate modifica tions. The error stemming from the construction of [LS] (i.e. the probability that the provers' replies are not "functional") can be bounded by p sd which is at most 2 h1 2 k1 = * for large enough n. So the total error is at most 2* = 2 k . <p> = 1 there exists an 1 i p such that (X 0 i ; E P (X i )) 1 2 k 0 (n) , then Pr R 0 [C 0 R 0 (X 0 p ; Z 1 ; Z 2 ) = Proof: The proof combines ideas in <ref> [BFLS, BLR, FRS, LS, FL] </ref>. Step 1: First we obtain a circuit family fC 00 1 ]; [x 0 p ]; z)g; where the circuit size and number of additional random bits are poly (log jCj; k 0 (n)). <p> Step 3: Lastly we transform the C 000 circuits into the C 0 circuits using the same construction as in Theorem 3.1. I.e., we use the transformation from p 0 provers into 2 provers in <ref> [LS, FL] </ref> on the inputs z (j) to get segmented circuits with p + 2 inputs. This will use an additional O (p 0 log (jzj)k 0 (n)) random bits. <p> Put together these facts enable us to prove Theorem 1.8. We omit the details. Acknowledgments We are grateful to Dror Lapidot, for explaining to us the construction and proof of <ref> [LS] </ref>: these were crucial to the proof of Theorem 3.1. We thank Rajeev Motwani, Nick Reingold, Muli Safra, Madhu Sudan, Mario Szegedy, Steve Vavasis and Mihalis Yannakakis for their helpful insights. Work done while the first author was at the IBM T.J. Watson Research Center, New York.
Reference: [Lo] <author> L. Lov asz. </author> <title> On the ratio of optimal integral and fractional covers. </title> <booktitle> Discrete Mathematics 13, </booktitle> <month> 383-390 </month> <year> (1975). </year>
Reference-contexts: For a definition of the problem we refer the reader to x4.1. Recall that there exists a polynomial time algorithm for approximating the size of the minimum set cover to within a factor of fi (log N ), where N is the number of elements in the base set <ref> [Jo, Lo] </ref>. Hardness of approximation was recently shown by Lund and Yannakakis [LY1].
Reference: [LY1] <author> C. Lund and M. Yannakakis. </author> <title> On the Hardness of Approximating Minimization Problems. </title> <booktitle> Proceedings of the 25th Annual ACM Symposium on the Theory of Computing, ACM (1993). </booktitle>
Reference-contexts: 1 Introduction The last two years have witnessed major advances in classifying the complexity of approximating classical optimization problems <ref> [Co, FGLSS, AS, ALMSS, BR, FL, Be, Zu, LY1, LY2] </ref>. <p> Today such results are known for many important problems P , with values of Q and T which differ from problem to problem; for example, it was shown by <ref> [LY1] </ref> that approximating the size of the minimum set cover to within fi (log N ) implies NP DTIME (n polylog n ), and it was shown by [FGLSS, AS, ALMSS] that for some constant c &gt; 0 approximating the size of a maximum clique in a graph within factor n <p> Recall that there exists a polynomial time algorithm for approximating the size of the minimum set cover to within a factor of fi (log N ), where N is the number of elements in the base set [Jo, Lo]. Hardness of approximation was recently shown by Lund and Yannakakis <ref> [LY1] </ref>. <p> Suppose there is a polynomial time algorithm which approximates the size of the minimum set cover to within c log N . Then NP DTIME (n log log n ). Similar improvements follow for all of the following problems: dominating set, hitting set, hypergraph transversal, minimum exact cover (cf. <ref> [KT, LY1] </ref>). Max Clique, Chromatic number. Known results for Max-Clique are based on the result of [ALMSS] which states that there are constants t; d such that NP = PCP [d log n; t; 1; O (log n); 1=2]. <p> Then NP BPP. The factor can be increased to n 1=25 if the conclusion is weakened to NEXP BPEXP. For the problem of approximating the chromatic number of a graph, the reduction of <ref> [LY1] </ref> implies the following. Theorem 1.6 Suppose there exists a polynomial time algorithm which can approximate the chromatic number of a graph to within n 1=146 . Then NP BPP. The factor can be increased to n 1=121 if the conclusion is weakened to NEXP BPEXP. Max-3SAT. <p> Thanks to David Zuckerman for pointing this out. In the STOC version we implied that our result for clique implied a result for chromatic number with the same approximation factor. We had forgotten to factor in the cost of the reduction of <ref> [LY1] </ref> which increases the size of the graph. So the factor for chromatic number is n 1=146 as now stated in Theorem 1.6. Thanks to Madhu Sudan for pointing this out. <p> In this lemma, l is a parameter to be chosen at will, and Q i refers to the question spaces of the uniformity condition. The statement and proof of this lemma that appear in <ref> [LY1] </ref> are only for the case p = 2, but the authors say later that it extends to any constant, and this extension is indeed easy. 5 For completeness we provide the construction for this extension. <p> Let m def P p be a "set system" as per <ref> [LY1, Lemma 3.3] </ref>. The base set S of the set cover instance S ' associated to ' is defined by S = f0; 1g r fi B. <p> i we have the set S (i; Q; A) defined as f hR; bi 2 S : Q = Q V (R; i) and b 2 B A g : The proof that this construction works is a simple extension of the proof for the case p = 2 in <ref> [LY1] </ref>. We claim (proof omitted) that the verifier of Theorem 1.1 satisfies functionality and uniformity. Equality of question space sizes can then be achieved by a simple transformation as shown in [LY1], and dis-jointness of answer spaces by a simple padding; the cost is only a constant factor in randomness, question <p> proof that this construction works is a simple extension of the proof for the case p = 2 in <ref> [LY1] </ref>. We claim (proof omitted) that the verifier of Theorem 1.1 satisfies functionality and uniformity. Equality of question space sizes can then be achieved by a simple transformation as shown in [LY1], and dis-jointness of answer spaces by a simple padding; the cost is only a constant factor in randomness, question sizes, and answer sizes. Thus we have canonical MIP 1 [r; 4; a; q; 2 k (n) ] verifiers for SAT with r; q; a being as in Theorem 1.1.
Reference: [LY2] <author> C. Lund and M. Yannakakis. </author> <title> The Approximation of Maximum Subgraph Problems. </title> <booktitle> ICALP 93. </booktitle>
Reference-contexts: 1 Introduction The last two years have witnessed major advances in classifying the complexity of approximating classical optimization problems <ref> [Co, FGLSS, AS, ALMSS, BR, FL, Be, Zu, LY1, LY2] </ref>.
Reference: [MS] <author> F. MacWilliams and N. Sloane. </author> <title> The Theory of Error-Correcting Codes. </title> <publisher> North-Holland, </publisher> <year> 1981. </year>
Reference-contexts: Proof: The following lemma implies that each segment of Y 0 i can only be decoded to a constant number of dif ferent segments of Y i . Lemma 3.6 <ref> [MS] </ref> Let A = A (n; d; w) be the maximum number of binary vectors of length n, each with weight (i.e., number of non-zeroes) at most w, and any two of which are distance at least d apart.
Reference: [PY] <author> C. Papadimitriou and M. Yannakakis. </author> <title> Optimization, approximation, and complexity classes. </title> <booktitle> Proceedings of the 20th Annual ACM Symposium on the Theory of Computing, ACM (1988). </booktitle>
Reference: [PS] <author> S. Phillips and S. Safra. </author> <title> PCP and tighter bounds for approximating MAXSNP. </title> <type> Manuscript, </type> <year> 1992. </year>
Reference-contexts: The (constant) number of bits t that one needs to check in the [ALMSS] result NP PCP [O (log n); t; 1; O (log n); 1=2] is of the order of 10 4 . Some reductions in this value were obtained by <ref> [PS] </ref>. Our improved complexity of four prover proofs for NP in terms of randomness and answer size for a given error, together with a careful analysis of the [ALMSS] construction and proofs enable us to obtain substantially smaller values than previously known.
Reference: [Va] <author> S. Vavasis. </author> <title> On approximation algorithms for concave programming. Recent Advances in Global Optimization, </title> <editor> C. A. Floudas and P.M. </editor> <booktitle> Pardalos, </booktitle> <pages> pp. 3-18, </pages> <publisher> Princeton University Press, </publisher> <year> 1992. </year>
Reference-contexts: Note that we are maximizing over a subset of R n ; solutions are not restricted to integers. We denote by f fl the maximum of f over the feasible region, and by f fl the minimum. Following <ref> [ADP, Va] </ref> we say that ~ f is a -approximation, where 2 [0; 1], if jf fl ~ f j jf fl f fl j. <p> Following [ADP, Va] we say that ~ f is a -approximation, where 2 [0; 1], if jf fl ~ f j jf fl f fl j. We refer the reader to <ref> [Va] </ref> for discussion of the appropriateness of this definition, and the inappropriateness of the one we have been using above, in the context of continuous optimization problems.
Reference: [Zu] <author> D. Zuckerman. </author> <title> NP-Complete Problems have a version that is hard to Approximate. Structure in Complexity Theory, </title> <booktitle> 1993. </booktitle> <pages> 12 </pages>
Reference-contexts: 1 Introduction The last two years have witnessed major advances in classifying the complexity of approximating classical optimization problems <ref> [Co, FGLSS, AS, ALMSS, BR, FL, Be, Zu, LY1, LY2] </ref>. <p> Zuck-erman <ref> [Zu] </ref> uses a random construction which achieves c = 1=(1 + t) at the cost of weakening the conclusion to NP BPP. Based on Theorem 1.2, we can improve the value of c in this result. <p> The changes are summarized below. In the STOC version we said that the variant of the [FGLSS] clique reduction given in <ref> [Zu] </ref> shows that approximating clique to within n 1=t implies NP BPP, and thus had Theorem 1.5 with a factor of n 1=29 . In truth [Zu] achieves n 1=(1+t) so that the factor in Theorem 1.5 should be the n 1=30 which now appears there. <p> The changes are summarized below. In the STOC version we said that the variant of the [FGLSS] clique reduction given in <ref> [Zu] </ref> shows that approximating clique to within n 1=t implies NP BPP, and thus had Theorem 1.5 with a factor of n 1=29 . In truth [Zu] achieves n 1=(1+t) so that the factor in Theorem 1.5 should be the n 1=30 which now appears there. Thanks to David Zuckerman for pointing this out. In the STOC version we implied that our result for clique implied a result for chromatic number with the same approximation factor.
References-found: 29

