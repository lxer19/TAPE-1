URL: http://www.cs.toronto.edu/~mackay/ica.ps.gz
Refering-URL: http://www.cs.toronto.edu/~mackay/README.html
Root-URL: http://www.cs.toronto.edu
Email: mackay@mrao.cam.ac.uk  
Title: Maximum Likelihood and Covariant Algorithms for Independent Component Analysis somewhat more biologically plausible, involving no
Author: David J.C. MacKay 
Note: Second, a covariant version of the algorithm is derived. This algorithm is simpler and  Fourth, a collection of formulae are given that may be useful for the adaptation of the non-linearity in the ICA algorithm.  
Date: December 19, 1996| Draft 3.7  
Address: Madingley Road Cambridge CB3 0HE  
Affiliation: University of Cambridge Cavendish Laboratory  
Abstract: Bell and Sejnowski (1995) have derived a blind signal processing algorithm for a non-linear feedforward network from an information maximization viewpoint. This paper first shows that the same algorithm can be viewed as a maximum likelihood algorithm for the optimization of a linear generative model. Third, this paper gives a partial proof of the `folk-theorem' that any mixture of sources with high-kurtosis histograms is separable by the classic ICA algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Amari, S., Cichocki, A., and Yang, H., </author> <title> (1996) A new learning algorithm for blind signal separation. </title> <note> To appear in NIPS96. </note>
Reference: <author> Bell, A. J., and Sejnowski, T. J. </author> <title> (1995) An information maximization approach to blind separation and blind deconvolution. </title> <booktitle> Neural Computation 7 (6): </booktitle> <pages> 1129-1159. </pages>
Reference-contexts: Perhaps a more general theorem could be proved which relies on some moment properties of p rather than a heavy-tailedness defined in terms of smoothness properties. 5 Learning of the nonlinearity Let us conclude by discussing how one might learn the density on the latent variables. <ref> (Bell and Sejnowski 1995) </ref> discuss the concept of learning the nonlinearity, but don't give an explicit algorithm for doing this.
Reference: <author> Bretthorst, G. </author> <title> (1988) Bayesian spectrum analysis and parameter estimation. </title> <publisher> Springer. </publisher>
Reference: <author> Comon, P., Jutten, C., and Herault, J. </author> <title> (1991) Blind separation of sources .2. problems statement. </title> <booktitle> Signal Processing 24 (1): </booktitle> <pages> 11-20. </pages>
Reference: <author> Dayan, P., Hinton, G. E., Neal, R. M., and Zemel, R. S. </author> <title> (1995) The Helmholtz machine. </title> <booktitle> Neural Computation 7 (5): </booktitle> <pages> 889-904. </pages>
Reference: <author> Everitt, B. S. </author> <title> (1984) An Introduction to Latent Variable Models. </title> <publisher> London: Chapman and Hall. </publisher>
Reference-contexts: Fourth, a collection of formulae are given that may be useful for the adaptation of the nonlinearity in the ICA algorithm. 2 Maximum likelihood derivation of ICA 2.1 Latent variable models Many statistical models are generative models that make use of latent variables to describe a probability distribution over observables <ref> (Everitt 1984) </ref>.
Reference: <author> Green, A. G., and MacKay, D. J. C. </author> <title> (1996) Bayesian analysis of linear phased-array radar. In Maximum Entropy and Bayesian Methods, </title> <editor> Santa Barbara 1993 , ed. by G. </editor> <booktitle> Heidbreder, </booktitle> <pages> pp. 309-318, </pages> <address> Dordrecht. </address> <publisher> Kluwer. </publisher>
Reference: <author> Gull, S. F. </author> <title> (1989) Developments in maximum entropy data analysis. In Maximum Entropy and Bayesian Methods, Cambridge 1988 , ed. by J. </title> <booktitle> Skilling, </booktitle> <pages> pp. 53-71, </pages> <address> Dordrecht. </address> <publisher> Kluwer. </publisher>
Reference-contexts: The Newton algorithm converges to the minimum in a single step if L is quadratic. In some problems it may be that the curvature A consists of both data-dependent terms and data-independent terms; in this case, one might choose to define the metric using the data-independent terms only <ref> (Gull 1989) </ref>. The resulting algorithm will still be covariant but it will not implement an exact Newton step. Obviously there are many covariant algorithms; there is no unique choice.
Reference: <author> Hanson, R., Stutz, J., and Cheeseman, P. </author> <title> (1991) Bayesian classification with correlation and inheritance. </title> <booktitle> In Proceedings of the 12th International Joint Conference on Artificial Intelligence, Sydney, Australia, </booktitle> <volume> volume 2, </volume> <pages> pp. 692-698. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Examples of latent variable models include mixture models, which model the observables as coming from a superposed mixture of simple probability distributions <ref> (Hanson et al. 1991) </ref> (the latent variables are the unknown class labels of the examples); hidden Markov models (Rabiner and Juang 1986); factor analysis; Helmholtz machines (Hinton et al. 1995; Dayan et al. 1995); and density networks (MacKay 1995; MacKay 1996).
Reference: <author> Hendin, O., Horn, D., and Hopfield, J. J. </author> <title> (1994) Decomposition of a mixture of signals in a model of the olfactory- bulb. </title> <booktitle> Proceedings of the National Academy of Sciences of the United States of America 91 (13): </booktitle> <pages> 5942-5946. </pages> <note> 14 Hinton, </note> <author> G. E., Dayan, P., Frey, B. J., and Neal, R. M. </author> <title> (1995) The wake-sleep algorithm for unsupervised neural networks. </title> <booktitle> Science 268 (5214): </booktitle> <pages> 1158-1161. </pages>
Reference: <author> Jutten, C., and Herault, J. </author> <title> (1991) Blind separation of sources .1. an adaptive algorithm based on neuromimetic architecture. </title> <booktitle> Signal Processing 24 (1): </booktitle> <pages> 1-10. </pages>
Reference: <author> Knuth, D. E. </author> <booktitle> (1968) The art of computer programming. Volume 1: fundamental algorithms. </booktitle> <address> Reading, Mass.: </address> <publisher> Addison Wesley. </publisher>
Reference-contexts: Some designers of learning algorithms advocate the principle of covariance, which says, colloquially, that a consistent algorithm should give the same results independent of the units in which quantities are measured <ref> (Knuth 1968) </ref>. A prime example of a non-covariant algorithm is the popular steepest descents rule.
Reference: <author> MacKay, D. J. C. </author> <title> (1992) Bayesian interpolation. </title> <booktitle> Neural Computation 4 (3): </booktitle> <pages> 415-447. </pages>
Reference: <author> MacKay, D. J. C. </author> <title> (1995) Bayesian neural networks and density networks. Nuclear Instruments and Methods in Physics Research, </title> <booktitle> Section A 354 (1): </booktitle> <pages> 73-80. </pages>
Reference-contexts: The following approximation is accurate to within 8% for all t <ref> (MacKay and Peto 1995) </ref>: (t + 1=2) (t) ' 1=t + log (t=(t + 1=2)): (47) 5.2 Learning of a tanh non-linearity Let the distribution be p (xjfi) = 1=Z (fi) (cosh (fix)) 1=fi : (48) As fi increases this distribution tends to the biexponential distribution.
Reference: <author> MacKay, D. J. C. </author> <title> (1996) Density networks and their application to protein modelling. In Maximum Entropy and Bayesian Methods, </title> <address> Cambridge 1994 , ed. </address> <note> by J. </note> <author> Skilling and S. </author> <month> Sibisi, </month> <pages> pp. 259-268, </pages> <address> Dordrecht. </address> <publisher> Kluwer. </publisher>
Reference: <author> MacKay, D. J. C., and Peto, L. </author> <title> (1995) A hierarchical Dirichlet language model. </title> <booktitle> Natural Language Engineering 1 (3): </booktitle> <pages> 1-19. </pages>
Reference-contexts: The following approximation is accurate to within 8% for all t <ref> (MacKay and Peto 1995) </ref>: (t + 1=2) (t) ' 1=t + log (t=(t + 1=2)): (47) 5.2 Learning of a tanh non-linearity Let the distribution be p (xjfi) = 1=Z (fi) (cosh (fix)) 1=fi : (48) As fi increases this distribution tends to the biexponential distribution.
Reference: <author> Pearlmutter, B. A., and Parra, L. C., </author> <title> (1996) A context-sensitive generalization of ica. </title> <note> To appear in ICONIP. Also available at http://www.cnl.salk.edu/ ~bap/papers/iconip-96-cica.ps.gz. </note>
Reference: <author> Rabiner, L. R., and Juang, B. H. </author> <title> (1986) An introduction to hidden Markov models. </title> <journal> IEEE ASSP Magazine pp. </journal> <pages> 4-16. 15 </pages>
Reference-contexts: Examples of latent variable models include mixture models, which model the observables as coming from a superposed mixture of simple probability distributions (Hanson et al. 1991) (the latent variables are the unknown class labels of the examples); hidden Markov models <ref> (Rabiner and Juang 1986) </ref>; factor analysis; Helmholtz machines (Hinton et al. 1995; Dayan et al. 1995); and density networks (MacKay 1995; MacKay 1996). Note that it is usual for the latent variables to have a simple distribution, often a separable distribution.
References-found: 18

