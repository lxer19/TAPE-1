URL: http://elysium.cs.ucdavis.edu/~nico/seminar/papers/rtperf.ps.gz
Refering-URL: http://elysium.cs.ucdavis.edu/~nico/seminar/seminar.html
Root-URL: http://www.cs.ucdavis.edu
Email: E-mail: fvijayk,jplevyak,achieng@cs.uiuc.edu  
Title: Runtime Mechanisms for Efficient Dynamic Multithreading  
Author: Vijay Karamcheti, John Plevyak and Andrew A. Chien 
Affiliation: Concurrent Systems Architecture Group Department of Computer Science University of Illinois at Urbana-Champaign  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Agarwal, A., Kubiatowicz, J., Kranz, D., Lim, B.-H., Yeung, D., D'Souza, G., and Parkin, M. Sparcle: </author> <title> An evolutionary processor design for large-scale multiprocessors. </title> <booktitle> IEEE Micro 13, </booktitle> <month> 3 (June </month> <year> 1993), </year> <pages> 48-61. </pages>
Reference-contexts: Our work is related to previous research which has looked at efficient support for thread management and communication; however, the primary distinction is its emphasis on delivering high performance for dynamic, fine-grained applications without specialized hardware support. Efficient Thread Management Specialized hardware approaches have suggested providing multiple hardware contexts <ref> [1, 13] </ref> and integrating thread creation with message reception [42, 33]. In contrast, hybrid stack-heap execution can be supported entirely on stock hardware benefiting from advances in commodity microprocessor architectures.
Reference: [2] <author> Barnes, J., and Hut, P. </author> <title> A hierarchical O(N log N) force calculation algorithm. </title> <type> Tech. rep., </type> <institution> The Institute for Advanced Study, Princeton, </institution> <address> New Jersey, </address> <year> 1986. </year>
Reference-contexts: In addition, modern algorithms often make use of complex data structures to achieve high efficiency <ref> [2, 19] </ref>. Thus, such computation structures are not easily amenable to expression either in a regular data-parallel model, or in a message passing model that requires the programmer to map the computation into a fixed number of threads synchronizing using matching communication primitives.
Reference: [3] <author> Blumofe, R. D., Joerg, C. F., Kuszmaul, B. C., Leiserson, C. E., Randall, K. H., Shaw, A., and Zhou, Y. Cilk: </author> <title> An efficient multithreaded runtime system. </title> <booktitle> In Proceedings of Principles and Practice of Parallel Programming (1995). </booktitle>
Reference-contexts: Their computation structure often does not fit into data parallel models, and message passing requires the programmer to deal explicitly with the complexities of data placement, addressability, and concurrency control. Consequently, programming models based on dynamic thread creation and multi-threading <ref> [21, 3, 12, 41, 8, 17, 28, 16] </ref> are increasingly popular for expressing such problems. Such models involve user-defined computation units (hereafter referred to as logical threads) which are dynamically created to reflect the natural concurrency structure of the program; multithreading maps these onto the physical machine improving processor utilization. <p> Portable runtime systems [21, 16, 40] build these mechanisms on top of vendor-supported, standardized lightweight thread management [27] and communication [15, 44] interfaces; however, these incur relatively large overheads, requiring coarse-granularity threads for efficiency. While systems with specialized runtimes <ref> [3, 8, 28] </ref> can provide efficient primitives supporting finer grained threads, they still incur large thread management and communication overheads for irregular, dynamic computations: * Such computations exhibit wide variations in thread granularity and are typically not amenable to compile-time analyses which can coalesce logical threads into sufficiently coarse-grained physical threads. <p> Finer grained threads can be supported by programming systems such as Mentat [20], Cilk <ref> [3] </ref>, COOL [8], and Charm++ [28]. However, such systems typically assume minimal compiler support, relying on the programmer to control thread granularity and mapping for performance. Automating these decisions requires 25 compile-time knowledge of how the execution unfolds; consequently, such systems achieve low efficiency for irregular, dynamic computations.
Reference: [4] <author> Borkar, S., Cohn, R., Cox, G., Gross, T., Kung, H. T., Lam, M., Levine, M., Moore, B., Moore, W., Peterson, C., Susman, J., Sutton, J., Urbanski, J., and Webb, J. </author> <title> Supporting systolic and memory communication in iWarp. </title> <booktitle> In Proceedings of the 17th International Symposium on Computer Architecture (1990), IEEE Computer Society, </booktitle> <pages> pp. 70-81. </pages>
Reference-contexts: Thus, unlike hybrid stack-heap execution, they use the same mechanism to support both work distribution and latency-hiding and cannot optimize their code for either situation. Efficient Communication Previous research has largely focused on reducing point-to-point messaging costs and paid less attention to multi-party communication characteristics. Hardware approaches <ref> [13, 4] </ref> have argued for a closer integration of the network interface with the processor. Software approaches have investigated the design of messaging layers with minimal kernel interaction [14, 6], and the active messages [46] approach of o*oading all but the essential operations from the messaging layer.
Reference: [5] <author> Brewer, E. A., and Kuszmaul, B. C. </author> <title> How to get good performance from the CM-5 data network. </title> <booktitle> In Proceedings of the International Parallel Processing Symposium (1994), </booktitle> <pages> pp. 858-867. </pages>
Reference-contexts: Callahan [7] proposes a network interface which limits the number of outstanding messages between pairs of processors to improve network performance even for unbalanced traffic. Pull messaging achieves the same effect with general-purpose hardware: remote memory access and synchronization hardware is available in several current parallel machines. Brewer <ref> [5] </ref> has looked at software solutions for improving performance for all-pairs communication traffic on the CM-5. However, their solutions consider only permutation traffic patterns, requiring all processors to cooperate for achieving high performance.
Reference: [6] <author> C. A. Thekkath, H. L., and Lazowska, E. D. </author> <title> Separating data and control transfer in distributed operating systems. </title> <booktitle> In Proceedings of the Sixth Symposium on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI) (1994). </booktitle>
Reference-contexts: Hardware approaches [13, 4] have argued for a closer integration of the network interface with the processor. Software approaches have investigated the design of messaging layers with minimal kernel interaction <ref> [14, 6] </ref>, and the active messages [46] approach of o*oading all but the essential operations from the messaging layer. These approaches alone are inadequate to prevent performance degradation arising from sender-receiver decoupling in the presence of unbalanced and unsynchronized communication traffic. Two recent approaches address the above shortcoming.
Reference: [7] <author> Callahan, T., and Goldstein, S. NIFDY: </author> <title> A low overhead, high throughput network interface. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture (1995). </booktitle>
Reference-contexts: These approaches alone are inadequate to prevent performance degradation arising from sender-receiver decoupling in the presence of unbalanced and unsynchronized communication traffic. Two recent approaches address the above shortcoming. Callahan <ref> [7] </ref> proposes a network interface which limits the number of outstanding messages between pairs of processors to improve network performance even for unbalanced traffic. Pull messaging achieves the same effect with general-purpose hardware: remote memory access and synchronization hardware is available in several current parallel machines.
Reference: [8] <author> Chandra, R., Gupta, A., and Hennessy, J. L. </author> <title> Data locality and load balancing in COOL. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (1993). </booktitle>
Reference-contexts: Their computation structure often does not fit into data parallel models, and message passing requires the programmer to deal explicitly with the complexities of data placement, addressability, and concurrency control. Consequently, programming models based on dynamic thread creation and multi-threading <ref> [21, 3, 12, 41, 8, 17, 28, 16] </ref> are increasingly popular for expressing such problems. Such models involve user-defined computation units (hereafter referred to as logical threads) which are dynamically created to reflect the natural concurrency structure of the program; multithreading maps these onto the physical machine improving processor utilization. <p> Portable runtime systems [21, 16, 40] build these mechanisms on top of vendor-supported, standardized lightweight thread management [27] and communication [15, 44] interfaces; however, these incur relatively large overheads, requiring coarse-granularity threads for efficiency. While systems with specialized runtimes <ref> [3, 8, 28] </ref> can provide efficient primitives supporting finer grained threads, they still incur large thread management and communication overheads for irregular, dynamic computations: * Such computations exhibit wide variations in thread granularity and are typically not amenable to compile-time analyses which can coalesce logical threads into sufficiently coarse-grained physical threads. <p> Finer grained threads can be supported by programming systems such as Mentat [20], Cilk [3], COOL <ref> [8] </ref>, and Charm++ [28]. However, such systems typically assume minimal compiler support, relying on the programmer to control thread granularity and mapping for performance. Automating these decisions requires 25 compile-time knowledge of how the execution unfolds; consequently, such systems achieve low efficiency for irregular, dynamic computations.
Reference: [9] <author> Chien, A., Karamcheti, V., and Plevyak, J. </author> <title> The Concert system|compiler and runtime support for efficient fine-grained concurrent object-oriented programs. </title> <type> Tech. Rep. </type> <institution> UIUCDCS-R-93-1815, Department of Computer Science, University of Illinois, Urbana, Illinois, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: A blocked thread becomes ready when the future it is blocked on is filled by another thread holding the continuation. The above programming and computation models are supported by the Illinois Concert system <ref> [9] </ref> which is an implementation platform for the Concurrent Aggregates [11] and ICC++ [10] concurrent object-oriented languages. The Concert system consists of an optimizing compiler [39] and the runtime system described in the rest of the paper.
Reference: [10] <author> Chien, A., and Reddy, U. </author> <title> ICC++ language definition. Concurrent Systems Architecture Group Memo, </title> <note> Also available from http://www-csag.cs.uiuc.edu/, February 1995. 27 </note>
Reference-contexts: A blocked thread becomes ready when the future it is blocked on is filled by another thread holding the continuation. The above programming and computation models are supported by the Illinois Concert system [9] which is an implementation platform for the Concurrent Aggregates [11] and ICC++ <ref> [10] </ref> concurrent object-oriented languages. The Concert system consists of an optimizing compiler [39] and the runtime system described in the rest of the paper.
Reference: [11] <author> Chien, A. A. </author> <title> Concurrent Aggregates: Supporting Modularity in Massively-Parallel Programs. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: Such models involve user-defined computation units (hereafter referred to as logical threads) which are dynamically created to reflect the natural concurrency structure of the program; multithreading maps these onto the physical machine improving processor utilization. These models form the basis for several concurrent object-oriented languages <ref> [20, 11, 49] </ref> and message-driven systems [28], and simplify program expression by supporting flexible computation and synchronization structures. Several researchers have investigated multithreading runtime systems which provide mechanisms for communication and thread creation, synchronization and scheduling. <p> A blocked thread becomes ready when the future it is blocked on is filled by another thread holding the continuation. The above programming and computation models are supported by the Illinois Concert system [9] which is an implementation platform for the Concurrent Aggregates <ref> [11] </ref> and ICC++ [10] concurrent object-oriented languages. The Concert system consists of an optimizing compiler [39] and the runtime system described in the rest of the paper. <p> Continuation-passing: Lazy Continuation (and Context) Creation Explicit continuation passing can improve the composability of concurrent programs <ref> [49, 11] </ref>. However, when continuation passing occurs, invocations on the stack are complicated because the callee may want its continuation.
Reference: [12] <author> Culler, D., Sah, A., Schauser, K. E., von Eicken, T., and Wawrzynek, J. </author> <title> Fine-grain parallelism with minimal hardware support: A compiler-controlled threaded abstract machine. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages an Operating Systems (1991), </booktitle> <pages> pp. 164-75. </pages>
Reference-contexts: Their computation structure often does not fit into data parallel models, and message passing requires the programmer to deal explicitly with the complexities of data placement, addressability, and concurrency control. Consequently, programming models based on dynamic thread creation and multi-threading <ref> [21, 3, 12, 41, 8, 17, 28, 16] </ref> are increasingly popular for expressing such problems. Such models involve user-defined computation units (hereafter referred to as logical threads) which are dynamically created to reflect the natural concurrency structure of the program; multithreading maps these onto the physical machine improving processor utilization. <p> Automating these decisions requires 25 compile-time knowledge of how the execution unfolds; consequently, such systems achieve low efficiency for irregular, dynamic computations. The close compiler-runtime coupling which characterizes hybrid stack-heap execution is also found in other dynamic, fine-grained programming systems. The Threaded Abstract Machine (TAM) <ref> [12] </ref> model provides a cost hierarchy, enabling the compiler to manage synchronization, scheduling and storage at the activation-frame level. Our work complements TAM's by providing mechanisms which enable robust performance in spite of statically unpredictable situations.
Reference: [13] <author> Dally, W. J., Chien, A., Fiske, S., Horwat, W., Keen, J., Larivee, M., Lethin, R., Nuth, P., Wills, S., Carrick, P., and Fyler, G. </author> <title> The J-Machine: A fine-grain concurrent computer. </title> <booktitle> In Information Processing 89, Proceedings of the IFIP Congress (August 1989), </booktitle> <pages> pp. 1147-1153. </pages>
Reference-contexts: Our work is related to previous research which has looked at efficient support for thread management and communication; however, the primary distinction is its emphasis on delivering high performance for dynamic, fine-grained applications without specialized hardware support. Efficient Thread Management Specialized hardware approaches have suggested providing multiple hardware contexts <ref> [1, 13] </ref> and integrating thread creation with message reception [42, 33]. In contrast, hybrid stack-heap execution can be supported entirely on stock hardware benefiting from advances in commodity microprocessor architectures. <p> Thus, unlike hybrid stack-heap execution, they use the same mechanism to support both work distribution and latency-hiding and cannot optimize their code for either situation. Efficient Communication Previous research has largely focused on reducing point-to-point messaging costs and paid less attention to multi-party communication characteristics. Hardware approaches <ref> [13, 4] </ref> have argued for a closer integration of the network interface with the processor. Software approaches have investigated the design of messaging layers with minimal kernel interaction [14, 6], and the active messages [46] approach of o*oading all but the essential operations from the messaging layer.
Reference: [14] <author> Druschel, P., and Peterson, L. L. Fbufs: </author> <title> A high-bandwidth cross-domain transfer facility. </title> <booktitle> In Proceedings of Fourteenth ACM Symposium on Operating Systems Principles (December 1993), ACM SIGOPS, </booktitle> <publisher> ACM Press, </publisher> <pages> pp. 189-202. </pages>
Reference-contexts: Hardware approaches [13, 4] have argued for a closer integration of the network interface with the processor. Software approaches have investigated the design of messaging layers with minimal kernel interaction <ref> [14, 6] </ref>, and the active messages [46] approach of o*oading all but the essential operations from the messaging layer. These approaches alone are inadequate to prevent performance degradation arising from sender-receiver decoupling in the presence of unbalanced and unsynchronized communication traffic. Two recent approaches address the above shortcoming.
Reference: [15] <author> Forum, M. P. I. </author> <title> The MPI message passing interface standard. </title> <type> Tech. rep., </type> <institution> University of Tennessee, Knoxville, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: For the most part, these systems consist of a few general-purpose, bundled mechanisms which assume minimal compoler and hardware support. Portable runtime systems [21, 16, 40] build these mechanisms on top of vendor-supported, standardized lightweight thread management [27] and communication <ref> [15, 44] </ref> interfaces; however, these incur relatively large overheads, requiring coarse-granularity threads for efficiency. <p> The base Concert primitives represent a high-performance implementation of traditional multithreading runtime systems providing compiler-oblivious bundled primitives. For example, on the IBM SP/2, the Nexus runtime system [16] which is layered on top of a standard thread package (pthreads [27]) and IBM's implementation of MPI <ref> [15] </ref>, incurs thread creation and communication overheads (0-byte latency) of 32.0s and 44.0s respectively. <p> In contrast, hybrid stack-heap execution can be supported entirely on stock hardware benefiting from advances in commodity microprocessor architectures. Portable multithreading runtime systems such as Chant [21], Nexus [16] and PORTS [40], built on top of vendor-supported, standardized light-weight thread [27] and communication <ref> [15, 44] </ref> interfaces, incur large thread and communication overheads (30-50s) requiring large granularity threads for efficiency. Finer grained threads can be supported by programming systems such as Mentat [20], Cilk [3], COOL [8], and Charm++ [28].
Reference: [16] <author> Foster, I., Kesselman, C., Olson, R., and Tuecke, S. </author> <title> Nexus: An interoperability layer for parallel and distributed computer systems. </title> <type> Tech. Rep. Version 1.3, </type> <institution> Argonne National Laboratory, </institution> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: Their computation structure often does not fit into data parallel models, and message passing requires the programmer to deal explicitly with the complexities of data placement, addressability, and concurrency control. Consequently, programming models based on dynamic thread creation and multi-threading <ref> [21, 3, 12, 41, 8, 17, 28, 16] </ref> are increasingly popular for expressing such problems. Such models involve user-defined computation units (hereafter referred to as logical threads) which are dynamically created to reflect the natural concurrency structure of the program; multithreading maps these onto the physical machine improving processor utilization. <p> Several researchers have investigated multithreading runtime systems which provide mechanisms for communication and thread creation, synchronization and scheduling. For the most part, these systems consist of a few general-purpose, bundled mechanisms which assume minimal compoler and hardware support. Portable runtime systems <ref> [21, 16, 40] </ref> build these mechanisms on top of vendor-supported, standardized lightweight thread management [27] and communication [15, 44] interfaces; however, these incur relatively large overheads, requiring coarse-granularity threads for efficiency. <p> The communication mechanisms provide primitives to send and receive data. The base Concert primitives represent a high-performance implementation of traditional multithreading runtime systems providing compiler-oblivious bundled primitives. For example, on the IBM SP/2, the Nexus runtime system <ref> [16] </ref> which is layered on top of a standard thread package (pthreads [27]) and IBM's implementation of MPI [15], incurs thread creation and communication overheads (0-byte latency) of 32.0s and 44.0s respectively. <p> The compiler can also optimize thread switching cost by managing live state at suspension points. More importantly, heap-allocated contexts enable creation and deletion costs of less than 1s, an order of magnitude cheaper than that for portable multithreading systems such as Chant [21], Nexus <ref> [16] </ref>, etc. These latter systems require a dedicated stack per thread because they rely on a sequential compiler infrastructure which does not provide any support for saving and restoring thread state across suspensions. <p> In contrast, hybrid stack-heap execution can be supported entirely on stock hardware benefiting from advances in commodity microprocessor architectures. Portable multithreading runtime systems such as Chant [21], Nexus <ref> [16] </ref> and PORTS [40], built on top of vendor-supported, standardized light-weight thread [27] and communication [15, 44] interfaces, incur large thread and communication overheads (30-50s) requiring large granularity threads for efficiency.
Reference: [17] <author> Freeh, V. W., Lowenthal, D. K., and Andrews, G. R. </author> <title> Distributed filaments: Efficient fine-grain parallelism on a cluster of workstations. </title> <booktitle> In First Symposium on Operating Systems Design and Implementation (Monterey, </booktitle> <address> CA, </address> <month> Nov. </month> <year> 1994), </year> <pages> pp. 201-212. </pages>
Reference-contexts: Their computation structure often does not fit into data parallel models, and message passing requires the programmer to deal explicitly with the complexities of data placement, addressability, and concurrency control. Consequently, programming models based on dynamic thread creation and multi-threading <ref> [21, 3, 12, 41, 8, 17, 28, 16] </ref> are increasingly popular for expressing such problems. Such models involve user-defined computation units (hereafter referred to as logical threads) which are dynamically created to reflect the natural concurrency structure of the program; multithreading maps these onto the physical machine improving processor utilization.
Reference: [18] <author> Goldstein, S. C., Schauser, K. E., and Culler, D. </author> <title> Lazy threads, stacklets, and synchronizers: Enabling primitives for parallel languages. </title> <booktitle> In Proceedings of POOMA'94 (1994). </booktitle>
Reference-contexts: The idea of lazily creating threads as required by runtime situations can also be found in the work on Lazy Task Creation [32] and Leapfrogging [47] in the context of shared-memory machines, and Olden [41], Stacklets <ref> [18] </ref>, and StackThreads [45] in the context of distributed-memory machines. The former two schemes were developed in the context of parallel languages with explicit futures [22], and allow stealing previously deferred stack frames to adaptively control execution granularity and work distribution.
Reference: [19] <author> Greengard, L., and Rokhlin, V. </author> <title> A fast algorithm for particle simulations. </title> <journal> Journal of Computational Physics 73 (1987), </journal> <pages> 325-48. </pages>
Reference-contexts: In addition, modern algorithms often make use of complex data structures to achieve high efficiency <ref> [2, 19] </ref>. Thus, such computation structures are not easily amenable to expression either in a regular data-parallel model, or in a message passing model that requires the programmer to map the computation into a fixed number of threads synchronizing using matching communication primitives.
Reference: [20] <author> Grimshaw, A. </author> <title> Easy-to-use object-oriented parallel processing with Mentat. </title> <booktitle> IEEE Computer 5, </booktitle> <month> 26 (May </month> <year> 1993), </year> <pages> 39-51. </pages>
Reference-contexts: Such models involve user-defined computation units (hereafter referred to as logical threads) which are dynamically created to reflect the natural concurrency structure of the program; multithreading maps these onto the physical machine improving processor utilization. These models form the basis for several concurrent object-oriented languages <ref> [20, 11, 49] </ref> and message-driven systems [28], and simplify program expression by supporting flexible computation and synchronization structures. Several researchers have investigated multithreading runtime systems which provide mechanisms for communication and thread creation, synchronization and scheduling. <p> Finer grained threads can be supported by programming systems such as Mentat <ref> [20] </ref>, Cilk [3], COOL [8], and Charm++ [28]. However, such systems typically assume minimal compiler support, relying on the programmer to control thread granularity and mapping for performance. Automating these decisions requires 25 compile-time knowledge of how the execution unfolds; consequently, such systems achieve low efficiency for irregular, dynamic computations.
Reference: [21] <author> Haines, M., Cronk, D., and Mehrotra, P. </author> <title> On the design of Chant: A talking threads package. </title> <booktitle> In Proceedings of Supercomputing'94 (November 1994), </booktitle> <pages> pp. 350-359. </pages>
Reference-contexts: Their computation structure often does not fit into data parallel models, and message passing requires the programmer to deal explicitly with the complexities of data placement, addressability, and concurrency control. Consequently, programming models based on dynamic thread creation and multi-threading <ref> [21, 3, 12, 41, 8, 17, 28, 16] </ref> are increasingly popular for expressing such problems. Such models involve user-defined computation units (hereafter referred to as logical threads) which are dynamically created to reflect the natural concurrency structure of the program; multithreading maps these onto the physical machine improving processor utilization. <p> Several researchers have investigated multithreading runtime systems which provide mechanisms for communication and thread creation, synchronization and scheduling. For the most part, these systems consist of a few general-purpose, bundled mechanisms which assume minimal compoler and hardware support. Portable runtime systems <ref> [21, 16, 40] </ref> build these mechanisms on top of vendor-supported, standardized lightweight thread management [27] and communication [15, 44] interfaces; however, these incur relatively large overheads, requiring coarse-granularity threads for efficiency. <p> The compiler can also optimize thread switching cost by managing live state at suspension points. More importantly, heap-allocated contexts enable creation and deletion costs of less than 1s, an order of magnitude cheaper than that for portable multithreading systems such as Chant <ref> [21] </ref>, Nexus [16], etc. These latter systems require a dedicated stack per thread because they rely on a sequential compiler infrastructure which does not provide any support for saving and restoring thread state across suspensions. <p> Efficient Thread Management Specialized hardware approaches have suggested providing multiple hardware contexts [1, 13] and integrating thread creation with message reception [42, 33]. In contrast, hybrid stack-heap execution can be supported entirely on stock hardware benefiting from advances in commodity microprocessor architectures. Portable multithreading runtime systems such as Chant <ref> [21] </ref>, Nexus [16] and PORTS [40], built on top of vendor-supported, standardized light-weight thread [27] and communication [15, 44] interfaces, incur large thread and communication overheads (30-50s) requiring large granularity threads for efficiency.
Reference: [22] <author> Halstead Jr., R. H. </author> <title> Multilisp: A language for concurrent symbolic computation. </title> <journal> ACM Transactions on Programming Languages and Systems 7, </journal> <month> 4 (October </month> <year> 1985), </year> <pages> 501-538. </pages>
Reference-contexts: In this work, we assume a fine-grained concurrent object-oriented model where objects reside in a global namespace and each method invocation corresponds to a logical thread. Synchronization between threads is achieved via futures <ref> [22] </ref>: if the caller thread touches the future, i.e. it attempts to read its value, before the thread responsible for writing the value is finished, then the caller thread blocks. <p> The former two schemes were developed in the context of parallel languages with explicit futures <ref> [22] </ref>, and allow stealing previously deferred stack frames to adaptively control execution granularity and work distribution. Hybrid stack-heap execution differs in that it allows eager work distribution. The latter three schemes all allocate new threads whenever the current one blocks due to a remote operation.
Reference: [23] <author> Hanrahan, P., Salzman, D., and Aupperle, L. </author> <title> A rapid hierarchical radiosity algorithm. </title> <booktitle> Computer Graphics (Proc Siggraph) 25, </booktitle> <month> 4 (July </month> <year> 1991), </year> <pages> 197-206. </pages>
Reference-contexts: We use an algorithm due to Hanra-han <ref> [23] </ref>, modeled after hierarchical N-body methods. The method starts with the initial patches comprising the scene and computes light transport between pairs of patches, hierarchically subdividing each patch as needed to ensure accuracy. Each patch maintains interaction lists of potentially visible neighbors.
Reference: [24] <author> Hermans, J., and Carson, M. </author> <title> Cedar documentation. Unpublished manual for CEDAR, </title> <year> 1985. </year>
Reference-contexts: The application is challenging to parallelize on distributed memory machines because of its irregular computation and communication structure. IC-CEDAR is based on CEDAR <ref> [24] </ref>, a sequential MD program written in C and FORTRAN. Simulations are carried out in discrete time steps with four computation phases: calculation of neighbor lists, computation of bonded and non-bonded forces, integration of motion, and SHAKE (an iterative coordinate correction phase).
Reference: [25] <author> Horwat, W., Chien, A., and Dally, W. </author> <title> Experience with CST: </title> <booktitle> Programming and implementation. In Proceedings of the SIGPLAN Conference on Programming Language Design and Implementation (1989), ACM SIGPLAN, </booktitle> <publisher> ACM Press, </publisher> <pages> pp. 101-9. </pages>
Reference-contexts: Since one of our goals is to execute forwarded invocations <ref> [25] </ref> on the stack, lazy allocation of the continuation is essential. As we shall see, allocation of a continuation also implies creation of the context in which the returned value will be stored.
Reference: [26] <author> Hwang, Y.-S., Das, R., Saltz, J., Brooks, B., and Hodo s cek, M. </author> <title> Parallelizing molecular dynamics programs for distributed memory machines: An application of the CHAOS runtime support library. </title> <type> Tech. Rep. </type> <institution> CS-TR-3374 and UMIACS-TR-94-125, University of Maryland, </institution> <month> Nov. </month> <year> 1994. </year> <note> to appear in IEEE Computational Science and Engineering. </note>
Reference-contexts: On a SparcStation 20, the sequential performance of IC-CEDAR is within 75% of the performance achieved by the C/FORTRAN version of CEDAR. To compare our parallel performance, we looked at the performance achieved by CHARMM <ref> [26] </ref>, a highly tuned SPMD implementation built on top of sequential FORTRAN code with manually inserted calls to the CHAOS runtime library responsible for communication and load-balancing. Despite differences in simulation models, both IC-CEDAR and CHARMM exhibit similar computation and communication behavior, and perform similar amounts of work.
Reference: [27] <author> IEEE. </author> <title> Thread extensions for portable operating systems. </title> <type> (Draft 7), </type> <month> Feb. </month> <year> 1992. </year>
Reference-contexts: For the most part, these systems consist of a few general-purpose, bundled mechanisms which assume minimal compoler and hardware support. Portable runtime systems [21, 16, 40] build these mechanisms on top of vendor-supported, standardized lightweight thread management <ref> [27] </ref> and communication [15, 44] interfaces; however, these incur relatively large overheads, requiring coarse-granularity threads for efficiency. <p> The communication mechanisms provide primitives to send and receive data. The base Concert primitives represent a high-performance implementation of traditional multithreading runtime systems providing compiler-oblivious bundled primitives. For example, on the IBM SP/2, the Nexus runtime system [16] which is layered on top of a standard thread package (pthreads <ref> [27] </ref>) and IBM's implementation of MPI [15], incurs thread creation and communication overheads (0-byte latency) of 32.0s and 44.0s respectively. <p> In contrast, hybrid stack-heap execution can be supported entirely on stock hardware benefiting from advances in commodity microprocessor architectures. Portable multithreading runtime systems such as Chant [21], Nexus [16] and PORTS [40], built on top of vendor-supported, standardized light-weight thread <ref> [27] </ref> and communication [15, 44] interfaces, incur large thread and communication overheads (30-50s) requiring large granularity threads for efficiency. Finer grained threads can be supported by programming systems such as Mentat [20], Cilk [3], COOL [8], and Charm++ [28].
Reference: [28] <author> Kale, L. V., and Krishnan, S. CHARM++: </author> <title> A portable concurrent object oriented system based on C++. </title> <booktitle> In Proceedings of OOPSLA'93 (1993), </booktitle> <pages> pp. 91-108. 28 </pages>
Reference-contexts: Their computation structure often does not fit into data parallel models, and message passing requires the programmer to deal explicitly with the complexities of data placement, addressability, and concurrency control. Consequently, programming models based on dynamic thread creation and multi-threading <ref> [21, 3, 12, 41, 8, 17, 28, 16] </ref> are increasingly popular for expressing such problems. Such models involve user-defined computation units (hereafter referred to as logical threads) which are dynamically created to reflect the natural concurrency structure of the program; multithreading maps these onto the physical machine improving processor utilization. <p> These models form the basis for several concurrent object-oriented languages [20, 11, 49] and message-driven systems <ref> [28] </ref>, and simplify program expression by supporting flexible computation and synchronization structures. Several researchers have investigated multithreading runtime systems which provide mechanisms for communication and thread creation, synchronization and scheduling. For the most part, these systems consist of a few general-purpose, bundled mechanisms which assume minimal compoler and hardware support. <p> Portable runtime systems [21, 16, 40] build these mechanisms on top of vendor-supported, standardized lightweight thread management [27] and communication [15, 44] interfaces; however, these incur relatively large overheads, requiring coarse-granularity threads for efficiency. While systems with specialized runtimes <ref> [3, 8, 28] </ref> can provide efficient primitives supporting finer grained threads, they still incur large thread management and communication overheads for irregular, dynamic computations: * Such computations exhibit wide variations in thread granularity and are typically not amenable to compile-time analyses which can coalesce logical threads into sufficiently coarse-grained physical threads. <p> Finer grained threads can be supported by programming systems such as Mentat [20], Cilk [3], COOL [8], and Charm++ <ref> [28] </ref>. However, such systems typically assume minimal compiler support, relying on the programmer to control thread granularity and mapping for performance. Automating these decisions requires 25 compile-time knowledge of how the execution unfolds; consequently, such systems achieve low efficiency for irregular, dynamic computations.
Reference: [29] <author> Karamcheti, V., and Chien, A. </author> <title> Concert efficient runtime support for concurrent object-oriented programming languages on stock hardware. </title> <booktitle> In Proceedings of Supercomputing'93 (1993). </booktitle>
Reference-contexts: In addition the compiler can customize the bundling of the runtime mechanisms affecting procedure call boundary crossings (e.g., the compiler can inline some of the runtime primitive calls), further reducing cost in situations where it has static information about thread interactions <ref> [29] </ref>. Corresponding to these basic primitives, we define a version of the runtime, Base, which serves as a competitive baseline for comparing the performance advantages of the other mechanisms. The Base runtime provides bundled versions of thread creation and scheduling primitives.
Reference: [30] <author> Karamcheti, V., and Chien, A. A. </author> <title> A comparison of architectural support for messaging on the TMC CM-5 and the Cray T3D. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture (1995). </booktitle>
Reference-contexts: Specifically, one must ensure that protocols involving sends from within message handlers must be deadlock free. This is another place where the availability of a compiler helps reduce the cost of primitive runtime mechanisms: a compiler can enforce the required discipline. The T3D implementation of the Fast Messages interface <ref> [30] </ref> makes use of hardware support for fetch-and-increment and remote memory access [34] to perform buffer management and data transfer without involving the destination processor. This decouples the sending processor from destination processor activity, improving communication performance. <p> The performance degradation due to unresponsive receivers and output contention even with modest fan-in can be severe, increasing send overheads by up to an order of magnitude <ref> [30] </ref>. Our solution exploits hardware support on the T3D (also present in several current and likely future machines) to build a distributed message queue with lazy receiver-initiated data transfer which decouples senders from receivers and eliminates output contention. <p> This overhead difference is largely due to the high cost of interaction with the T3D prefetch queue: 0.1s (15 cycles) to issue a single word fetch, and 0.15s (20 cycles) to extract a word from the prefetch queue. Note however, that simple architectural improvements <ref> [30] </ref> can make the costs of pull messaging competitive with push messaging for all message sizes. 17 Implementation Message size (in bytes) 16 32 64 128 256 512 1024 Push Messaging Src 1.84 1.85 2.03 2.84 3.53 8.26 14.65 (Fetch-and-increment based) Dest 0.73 0.73 0.73 0.73 0.73 0.73 0.73 Pull Messaging
Reference: [31] <author> Lenoski, D., and et al. </author> <title> The Stanford DASH Multiprocessor. </title> <booktitle> IEEE Computer (Mar 1992), </booktitle> <pages> 63-79. </pages>
Reference-contexts: Our parallel performance can be compared with previously published application speedup numbers (obtained from [43]) based on a hand-tuned implementation running on the DASH <ref> [31] </ref>, a cache-coherent shared-memory machine.
Reference: [32] <author> Mohr, E., Kranz, D., and Halstead Jr., R. </author> <title> Lazy task creation: A technique for increasing the granularity of parallel programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems 2, </journal> <month> 3 (July </month> <year> 1991), </year> <pages> 264-280. </pages>
Reference-contexts: Our work complements TAM's by providing mechanisms which enable robust performance in spite of statically unpredictable situations. The idea of lazily creating threads as required by runtime situations can also be found in the work on Lazy Task Creation <ref> [32] </ref> and Leapfrogging [47] in the context of shared-memory machines, and Olden [41], Stacklets [18], and StackThreads [45] in the context of distributed-memory machines.
Reference: [33] <author> Nikhil, R. S., Papadopoulos, G. M., and Arvind. </author> <title> *T: A multithreaded massively parallel architecture. </title> <booktitle> In The 19th Annual International Symposium on Computer Architecture (1992), Association for Computing Machinery, </booktitle> <pages> pp. 156-167. </pages>
Reference-contexts: Efficient Thread Management Specialized hardware approaches have suggested providing multiple hardware contexts [1, 13] and integrating thread creation with message reception <ref> [42, 33] </ref>. In contrast, hybrid stack-heap execution can be supported entirely on stock hardware benefiting from advances in commodity microprocessor architectures.
Reference: [34] <author> Numrich, R. W. </author> <title> The Cray T3D address space and how to use it. </title> <type> Tech. rep., </type> <institution> Cray Research, Inc., </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: This is another place where the availability of a compiler helps reduce the cost of primitive runtime mechanisms: a compiler can enforce the required discipline. The T3D implementation of the Fast Messages interface [30] makes use of hardware support for fetch-and-increment and remote memory access <ref> [34] </ref> to perform buffer management and data transfer without involving the destination processor. This decouples the sending processor from destination processor activity, improving communication performance.
Reference: [35] <author> Pakin, S., Lauria, M., and Chien, A. </author> <title> High performance messaging on workstations: Illinois Fast Messages (FM) for Myrinet. </title> <booktitle> In Supercomputing (December 1995). </booktitle>
Reference-contexts: The communication primitives in the Concert runtime system exploit a lean messaging interface to incur 5-10 times lower latency as compared to vendor communication libraries. The Concert communication primitives use the low-level Fast Messages <ref> [35] </ref> interface.
Reference: [36] <author> Pfister, G. F., and Norton, V. A. </author> <title> Hot spot contention and combining in multistage interconnection networks. </title> <journal> IEEE Transactions on Computers C-34, </journal> <month> 10 (October </month> <year> 1985), </year> <pages> 943-948. </pages>
Reference-contexts: For the message size considered above, the source overheads are 95s for the hotspot,x16 pattern (3:1 degradation), and 455s for the all-to-one pattern (15:1 degradation). This performance loss is due to classic hot spot saturation <ref> [36] </ref> which backs up the network, slowing the injection of all messages into the network. Such hot spot contention will occur in any messaging layer which eagerly pushes messages to their destination.
Reference: [37] <author> Plevyak, J., and Chien, A. A. </author> <title> Precise concrete type inference of object-oriented programs. </title> <booktitle> In Proceedings of OOPSLA'94, Object-Oriented Programming Systems, Languages and Architectures (1994), </booktitle> <pages> pp. 324-340. </pages>
Reference-contexts: The Concert system consists of an optimizing compiler [39] and the runtime system described in the rest of the paper. The compiler is capable of resolving interprocedural control and data flow information <ref> [37] </ref>, enabling it to specialize the generated code based on synchronization and communication features required by the computation. <p> In this case, we create both the callee's context as well as the continuation lazily. Our compiler selects the appropriate schema for each thread interaction based on a 11 global flow analysis <ref> [37] </ref> which conservatively determines the blocking and continuation requirements of each thread body [38]. A novel aspect of our hybrid stack-heap execution model is that it is implemented entirely in C, and consequently, is portable across a variety of parallel platforms.
Reference: [38] <author> Plevyak, J., Karamcheti, V., Zhang, X., and Chien, A. </author> <title> A hybrid execution model for fine-grained languages on distributed memory multicomputers. </title> <booktitle> In Proceedings of Supercomputing'95 (1995). </booktitle>
Reference-contexts: In this case, we create both the callee's context as well as the continuation lazily. Our compiler selects the appropriate schema for each thread interaction based on a 11 global flow analysis [37] which conservatively determines the blocking and continuation requirements of each thread body <ref> [38] </ref>. A novel aspect of our hybrid stack-heap execution model is that it is implemented entirely in C, and consequently, is portable across a variety of parallel platforms. <p> Figure 8 shows the stack unwinding and linkages which need to be set up as part of the fallback. The reader is referred to <ref> [38] </ref> for implementation details. A A B B (i) (ii) (iii) (i) (ii) . . . .
Reference: [39] <author> Plevyak, J., Zhang, X., and Chien, A. A. </author> <title> Obtaining sequential efficiency in concurrent object-oriented programs. </title> <booktitle> In Proceedings of the ACM Symposium on the Principles of Programming Languages (January 1995), </booktitle> <pages> pp. 311-321. </pages>
Reference-contexts: The above programming and computation models are supported by the Illinois Concert system [9] which is an implementation platform for the Concurrent Aggregates [11] and ICC++ [10] concurrent object-oriented languages. The Concert system consists of an optimizing compiler <ref> [39] </ref> and the runtime system described in the rest of the paper. The compiler is capable of resolving interprocedural control and data flow information [37], enabling it to specialize the generated code based on synchronization and communication features required by the computation.
Reference: [40] <author> PORTS Consortium. </author> <title> The PORTS0 interface. </title> <type> Tech. Rep. Version 0.3, </type> <institution> Argonne National Laboratory, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: Several researchers have investigated multithreading runtime systems which provide mechanisms for communication and thread creation, synchronization and scheduling. For the most part, these systems consist of a few general-purpose, bundled mechanisms which assume minimal compoler and hardware support. Portable runtime systems <ref> [21, 16, 40] </ref> build these mechanisms on top of vendor-supported, standardized lightweight thread management [27] and communication [15, 44] interfaces; however, these incur relatively large overheads, requiring coarse-granularity threads for efficiency. <p> In contrast, hybrid stack-heap execution can be supported entirely on stock hardware benefiting from advances in commodity microprocessor architectures. Portable multithreading runtime systems such as Chant [21], Nexus [16] and PORTS <ref> [40] </ref>, built on top of vendor-supported, standardized light-weight thread [27] and communication [15, 44] interfaces, incur large thread and communication overheads (30-50s) requiring large granularity threads for efficiency. Finer grained threads can be supported by programming systems such as Mentat [20], Cilk [3], COOL [8], and Charm++ [28].
Reference: [41] <author> Rogers, A., Carlisle, M., Reppy, J., and Hendren, L. </author> <title> Supporting dynamic data structures on distributed memory machines. </title> <journal> ACM Transactions on Programming Languages and Systems (1995). </journal>
Reference-contexts: Their computation structure often does not fit into data parallel models, and message passing requires the programmer to deal explicitly with the complexities of data placement, addressability, and concurrency control. Consequently, programming models based on dynamic thread creation and multi-threading <ref> [21, 3, 12, 41, 8, 17, 28, 16] </ref> are increasingly popular for expressing such problems. Such models involve user-defined computation units (hereafter referred to as logical threads) which are dynamically created to reflect the natural concurrency structure of the program; multithreading maps these onto the physical machine improving processor utilization. <p> The idea of lazily creating threads as required by runtime situations can also be found in the work on Lazy Task Creation [32] and Leapfrogging [47] in the context of shared-memory machines, and Olden <ref> [41] </ref>, Stacklets [18], and StackThreads [45] in the context of distributed-memory machines. The former two schemes were developed in the context of parallel languages with explicit futures [22], and allow stealing previously deferred stack frames to adaptively control execution granularity and work distribution.
Reference: [42] <author> Sakai, S., Yamaguchi, Y., Hiraki, K., Kodama, Y., and Yuba, T. </author> <title> An architecture of a dataflow single chip processor. </title> <booktitle> In International Symposium on Computer Architecture (1989). </booktitle>
Reference-contexts: Efficient Thread Management Specialized hardware approaches have suggested providing multiple hardware contexts [1, 13] and integrating thread creation with message reception <ref> [42, 33] </ref>. In contrast, hybrid stack-heap execution can be supported entirely on stock hardware benefiting from advances in commodity microprocessor architectures.
Reference: [43] <author> Singh, J. P., Gupta, A., and Levoy, M. </author> <title> Parallel visualization algorithms: Performance and architectural implications. </title> <booktitle> IEEE Computer 27, </booktitle> <month> 7 (July </month> <year> 1994), </year> <pages> 45-56. </pages>
Reference-contexts: The performance degradation on the T3D arises from the inability of the generated code to effectively exploit the small L1 cache (along with the lack of an L2 cache) on the DEC Alpha 21064. Our parallel performance can be compared with previously published application speedup numbers (obtained from <ref> [43] </ref>) based on a hand-tuned implementation running on the DASH [31], a cache-coherent shared-memory machine.
Reference: [44] <author> Sunderam, V. S. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concurrency, Practice and Experience 2, </journal> <volume> 4 ([12] 1990), </volume> <pages> 315-340. </pages>
Reference-contexts: For the most part, these systems consist of a few general-purpose, bundled mechanisms which assume minimal compoler and hardware support. Portable runtime systems [21, 16, 40] build these mechanisms on top of vendor-supported, standardized lightweight thread management [27] and communication <ref> [15, 44] </ref> interfaces; however, these incur relatively large overheads, requiring coarse-granularity threads for efficiency. <p> In contrast, hybrid stack-heap execution can be supported entirely on stock hardware benefiting from advances in commodity microprocessor architectures. Portable multithreading runtime systems such as Chant [21], Nexus [16] and PORTS [40], built on top of vendor-supported, standardized light-weight thread [27] and communication <ref> [15, 44] </ref> interfaces, incur large thread and communication overheads (30-50s) requiring large granularity threads for efficiency. Finer grained threads can be supported by programming systems such as Mentat [20], Cilk [3], COOL [8], and Charm++ [28].
Reference: [45] <author> Taura, K., Matsuoka, S., and Yonezawa, A. StackThreads: </author> <title> An abstract machine for scheduling fine-grain threads on stock CPUs. </title> <booktitle> In Joint Symposium on Parallel Processing (1994). [46] von Eicken, </booktitle> <editor> T., Culler, D., Goldstein, S., and Schauser, K. </editor> <title> Active Messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture (1992). </booktitle> <pages> 29 </pages>
Reference-contexts: The idea of lazily creating threads as required by runtime situations can also be found in the work on Lazy Task Creation [32] and Leapfrogging [47] in the context of shared-memory machines, and Olden [41], Stacklets [18], and StackThreads <ref> [45] </ref> in the context of distributed-memory machines. The former two schemes were developed in the context of parallel languages with explicit futures [22], and allow stealing previously deferred stack frames to adaptively control execution granularity and work distribution. Hybrid stack-heap execution differs in that it allows eager work distribution.
Reference: [47] <author> Wagner, D. B., and Calder, B. G. </author> <title> Leapfrogging: A portable technique for implementing efficient futures. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on the Principles and Practice of Parallel Programming (1993), </booktitle> <pages> pp. 208-217. </pages>
Reference-contexts: Our work complements TAM's by providing mechanisms which enable robust performance in spite of statically unpredictable situations. The idea of lazily creating threads as required by runtime situations can also be found in the work on Lazy Task Creation [32] and Leapfrogging <ref> [47] </ref> in the context of shared-memory machines, and Olden [41], Stacklets [18], and StackThreads [45] in the context of distributed-memory machines.
Reference: [48] <author> Woo, S. C., Ohara, M., Torrie, E., Singh, J. P., and Gupta, A. </author> <title> The SPLASH-2 programs: Characterization and methodological considerations. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture (1995), </booktitle> <pages> pp. 24-36. </pages>
Reference-contexts: Subdivided patches acquire their own interaction lists and contribute a weighted term to their parent's radiosity. The visibility calculation dominates the computation time. Our parallel algorithm is derived from the radiosity application in the SPLASH-2 benchmark suite <ref> [48] </ref>. There are three levels of parallelism in each iteration: across all input patches, across child patches of a subdivided patch, and across neighbor patches stored in the interaction list.
Reference: [49] <editor> Yonezawa, A., Ed. </editor> <title> ABCL: An Object-Oriented Concurrent System. </title> <publisher> MIT Press, </publisher> <year> 1990. </year> <note> ISBN 0-262-24029-7. </note>
Reference-contexts: Such models involve user-defined computation units (hereafter referred to as logical threads) which are dynamically created to reflect the natural concurrency structure of the program; multithreading maps these onto the physical machine improving processor utilization. These models form the basis for several concurrent object-oriented languages <ref> [20, 11, 49] </ref> and message-driven systems [28], and simplify program expression by supporting flexible computation and synchronization structures. Several researchers have investigated multithreading runtime systems which provide mechanisms for communication and thread creation, synchronization and scheduling. <p> Continuation-passing: Lazy Continuation (and Context) Creation Explicit continuation passing can improve the composability of concurrent programs <ref> [49, 11] </ref>. However, when continuation passing occurs, invocations on the stack are complicated because the callee may want its continuation.
References-found: 48

