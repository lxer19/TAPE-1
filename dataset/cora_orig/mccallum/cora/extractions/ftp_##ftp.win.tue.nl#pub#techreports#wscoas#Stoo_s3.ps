URL: ftp://ftp.win.tue.nl/pub/techreports/wscoas/Stoo_s3.ps
Refering-URL: http://as.win.tue.nl/publieng.html
Root-URL: http://www.win.tue.nl
Title: Linear Quadratic Regulator Problem with Positive Controls  
Author: W.P.M.H. Heemels S.J.L. van Eijndhoven and A.A. Stoorvogel 
Date: February 14, 1998  
Abstract: In this paper, the Linear Quadratic Regulator Problem with a positivity constraint on the admissible control set is addressed. Necessary and sufficient conditions for optimality are presented in terms of inner products, projections on closed convex sets, Pontryagin's maximum principle and dynamic programming. The main results are concerned with smoothness of the optimal control and the value function. The maximum principle will be extended to the infinite horizon case. Based on these analytical methods, we propose a numerical algorithm for the computation of the optimal controls for the finite and infinite horizon problem. The numerical methods will be justified by convergence properties between the finite and infinite horizon case on one side and discretised optimal controls and the true optimal controls on the other. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anderson, B.D.O., and Moore, J.B., </author> <year> 1990, </year> <title> Optimal Control Linear Quadratic Methods. </title> <publisher> Prentice-Hall. </publisher>
Reference-contexts: 1 Introduction In the literature, the Linear Quadratic Regulator Problem has been solved by the use of Riccati equations <ref> [1] </ref>. In this paper the same problem will be treated with the additional constraint that the control function can only take values in a closed convex polyhedral cone, like the nonnegative orthant in a Euclidean space. <p> This resulting optimal controllers are nonsmooth "bang-bang" controllers attaining only the saturation borders of the restraint set. Example 4 Looking at the optimal control problem above with restraint set := <ref> [0; 1] </ref> it is obvious that u (s) = 1, s 2 [0; 1) and u (s) = 0, s 1 is the optimal control. Note that the optimal control is discontinuous. <p> It is well-known that the optimal feedback in the unconstrained case arises from the Algebraic Riccati Equation <ref> [1] </ref>. To show that such a simple connection is not easily established, we consider the pendulum as in figure 1. On a pendulum the gravitation f and the control force u act as a vertical force and horizontal force, respectively. <p> Consider the function h with domain <ref> [0; 1] </ref> defined by h : ff 7! f (x 0 + ff [x 1 x 0 ]): This function is continuous on [0; 1] and differentiable on (0; 1) with derivative dh (ff) = (f x (x 0 + ff [x 1 x 0 ]) j x 1 x 0 <p> Consider the function h with domain <ref> [0; 1] </ref> defined by h : ff 7! f (x 0 + ff [x 1 x 0 ]): This function is continuous on [0; 1] and differentiable on (0; 1) with derivative dh (ff) = (f x (x 0 + ff [x 1 x 0 ]) j x 1 x 0 ) Using the mean value theorem for functions of one real variable [19] we arrive at the existence of a fi 2 (0;
Reference: [2] <author> Astrom, K.J. and Wittenmark, B., </author> <year> 1984, </year> <title> Computer-controlled Systems Theory and Design. </title> <publisher> Prentice-Hall. </publisher>
Reference-contexts: So, we can now recursively determine the value function V h and store the optimal control values for every point (i; x). The integral in (39) can be expressed explicitly in terms of x and the chosen value of v <ref> [2] </ref>, thereby facilitating the calculations. The above approximation avoids the problem of solving the Hamilton-Jacobi-Bellman partial differential equation. However, we obviously cannot store the value function into a computer without discretisation (also called "gridding") of the state space.
Reference: [3] <author> Bellman, R., </author> <year> 1967, </year> <title> Introduction to the Mathematical Theory of Control Processes. </title> <publisher> Academic Press. </publisher>
Reference-contexts: The convergence results between the finite and infinite horizon problem that will be established, can be exploited to derive, under rather mild conditions, a maximum principle on infinite horizon for the "positive Linear Quadratic Regulator problem." The second approach, dynamic programming, was originally conceived by Bellman <ref> [3] </ref> as a fruitful numerical method to compute optimal controls in discrete time processes. Later, people realized that the same ideas can be used for optimal control problems in continuous time. <p> In fact, by the discretisations the problem is transformed to an optimal control problem of a discrete time system in a way that the original techniques of Belmann's dynamics programming for discrete time systems can be exploited, see e.g. <ref> [3] </ref> and [11]. Every control u h 2 P h [t; T ] can be parametrised as u h = i=1 i 1 [t i1 ;t i ) (35) with u h i 2 .
Reference: [4] <author> Feichtinger, G., and Hartl, R.F., </author> <year> 1986, </year> <title> Okonomischer Prozesse: Anwendung des Maxi-mumprinzips in den Wirtschaftswissenschaften. </title> <publisher> De Gruyter, </publisher> <address> Berlin. </address>
Reference-contexts: All these issues will be considered in the current paper. Mainly, there are two classical approaches in optimal control theory. The first approach is the maximum principle, initiated by Pontryagin et al. [17]. The original maximum principle has been used and extended by many others <ref> [13, 4, 15, 11] </ref>. To get a complete treatise of dealing with constrained finite horizon optimal control problems the application of the maximum principle is incorporated. However, for the infinite horizon case a similar result is nontrivial.
Reference: [5] <author> Fleming, W.H., and Soner, H.M., </author> <year> 1993, </year> <title> Controlled Markov Processes and Viscosity Solutions. </title> <publisher> Springer-Verlag. </publisher>
Reference-contexts: However, in many problems, the value function does not behave smoothly, which causes both analytical and numerical problems. Recently, the notion of viscosity solutions has been introduced, which generalizes the concept of a solution of the HJB equation <ref> [5] </ref>. In this paper, it will be shown that the value function in our problem is continuously differentiable and satisfies the HJB equation in the classical sense. <p> The difficulty in using this partial differential equation is often that the value function is not smooth and classical results do not apply. Recently, an extended solution concept is used called "viscosity solution" of the HJB-equation <ref> [5] </ref>. We show in this subsection that this complicated solution concept is not required for the LQ-problem with positive controls: the value function is continuously differentiable and a classical solution to the HJB equation. <p> Before continuing we introduce the function space L 1 [t; T ] m as the normed space of all bounded Lebesgue measurable functions on [t; T ]. The so-called "verification theorem" <ref> [5] </ref> states: if W is a continuously differentiable solution of the HJB equation, satisfying the boundary conditions W (T; x) = 0; x 2 IR n , then W (t; x) V (t; x), where V denotes the value function as introduced in section 2.
Reference: [6] <author> Hautus, M.L.J., </author> <title> Stabilization, controllability and observability of linear autonomous systems, 1969. </title> <journal> Nederl. Akad. Wetensch., Proc., Ser. A73, </journal> <volume> p. </volume> <pages> 448-455. </pages>
Reference-contexts: A well-known result in systems theory is that an equivalent characterisation of detectability of the pair (C; A) is the existence of a matrix G such that A + GC is stable <ref> [6] </ref>. Since D has full column rank, (A; B; C; D) is minimum phase iff (C + DF 2 ; A + BF 2 ) is detectable, where F 2 is uniquely determined by (C + DF 2 ) &gt; D = 0.
Reference: [7] <author> Heemels, W.P.M.H., </author> <year> 1995, </year> <title> Optimal Positive Control & Optimal Control with Positive State Entries. </title> <institution> Master's thesis Eindhoven University of Technology. </institution>
Reference-contexts: This fact will be verified in a lemma in the appendix. Combining (25) and (29) then completes the proof. Note that continuity of the partial derivative follows by some simple calculations. For a formal proof, see <ref> [7] </ref>. 2 Comparing (24) with the adjoint equation of the Maximum Principle yields a connection between the adjoint variable and the gradient of V : OE t;T;x 0 (s) = V x (s; x t;T;x 0 (s)); (31) where OE t;T;x 0 is the solution to the adjoint equation corresponding to <p> Note that finishing the complete recursive scheme gives you the optimal controls for all stored initial states. Since the optimal control values depend continuously on the initial conditions (as is proven in <ref> [7] </ref>), interpolation between stored values give good approximations of the optimal control values for the non-stored states. For similar techniques and more details, we refer to monographs like [12]. A problem is of course how small to choose the time-step h.
Reference: [8] <author> Heemels, W.P.M.H., </author> <year> 1998, </year> <title> Necessary and sufficient conditons for positive stabilizability of a system (A; B). </title> <institution> Internal Report of Eindhoven University of Technology, Dept. of Elect. Eng, </institution> <note> Measurement and Control Systems, Nr. 98 I/01. 24 </note>
Reference-contexts: For the infinite horizon case the existence of an admissible control is even not clear at all. By admissible control we mean a nonnegative control function that keeps the cost functional finite and the state square integrable. This problem is treated in <ref> [8] </ref>, were necessary and sufficient conditions are stated that are easily verified. Under the assumption of positive stabilizability of the system, existence and uniqueness of the optimal controls for the infinite horizon case are established. <p> The positive stabilizability of (A; B) guarantees the existence of positive controls, that keep the cost function finite. In <ref> [8] </ref> necessary and sufficient conditions are stated for positive stabilizability of a system (A; B). In fact, these conditions are easily verified. 7.2 Existence and Uniqueness In the remainder of this paper it will be assumed that 1. (A; B) is positively stabilizable; 2.
Reference: [9] <editor> Hiriart-Urruty, J.-B., and Lemarechal, C., </editor> <year> 1993, </year> <title> Convex Analysis and Minimization Algorithms I. </title> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Formally, P K x = k 0 2 K () kx k 0 k kx kk 8k 2 K (7) This concept can be found for instance in <ref> [9] </ref>. <p> k = 2k 0 in the above equation to observe that (x k 0 j k 0 ) = 0 and hence, (x k 0 j k) 0 8k 2 K: (9) To make the paper self-contained, the following two lemmas describing further properties that can be found also in <ref> [9] </ref> are included. Lemma 7 (Continuity of P K ) P K is globally Lipschitz continuous. In particular, for all x; y 2 H it holds that kP K x P K yk kx yk (10) Proof.
Reference: [10] <author> Kailath, T., </author> <year> 1980, </year> <title> Linear Systems. </title> <publisher> Prentice-Hall, Inc.. </publisher>
Reference: [11] <author> Kirk, D.E., </author> <year> 1970, </year> <title> Optimal Control Theory, An Introduction. </title> <publisher> Prentice-Hall Inc.. </publisher>
Reference-contexts: All these issues will be considered in the current paper. Mainly, there are two classical approaches in optimal control theory. The first approach is the maximum principle, initiated by Pontryagin et al. [17]. The original maximum principle has been used and extended by many others <ref> [13, 4, 15, 11] </ref>. To get a complete treatise of dealing with constrained finite horizon optimal control problems the application of the maximum principle is incorporated. However, for the infinite horizon case a similar result is nontrivial. <p> For more details on implementation aspects, we refer to more specialised books like [12]. We discretise our optimization problem in both time and state. Similar techniques can be found, for instance in <ref> [11, 12] </ref> with some illustrative examples. In later sections, we will see how this method can be used to approximate also the infinite horizon optimal 2 feedback. This method is justified by the convergence results between the finite and infinite horizon problems. The organization of the paper is as follows. <p> In fact, by the discretisations the problem is transformed to an optimal control problem of a discrete time system in a way that the original techniques of Belmann's dynamics programming for discrete time systems can be exploited, see e.g. [3] and <ref> [11] </ref>. Every control u h 2 P h [t; T ] can be parametrised as u h = i=1 i 1 [t i1 ;t i ) (35) with u h i 2 .
Reference: [12] <author> Kushner, H.J., and Dupuis, P.G., </author> <year> 1992, </year> <title> Numerical Methods for Stochastic Control Problems in Continuous Time. </title> <publisher> Springer-Verlag. </publisher>
Reference-contexts: However, the analysis of convergence of the approximations to the exact optimal control is crucial and justifies the proposed algorithm. For more details on implementation aspects, we refer to more specialised books like <ref> [12] </ref>. We discretise our optimization problem in both time and state. Similar techniques can be found, for instance in [11, 12] with some illustrative examples. In later sections, we will see how this method can be used to approximate also the infinite horizon optimal 2 feedback. <p> For more details on implementation aspects, we refer to more specialised books like [12]. We discretise our optimization problem in both time and state. Similar techniques can be found, for instance in <ref> [11, 12] </ref> with some illustrative examples. In later sections, we will see how this method can be used to approximate also the infinite horizon optimal 2 feedback. This method is justified by the convergence results between the finite and infinite horizon problems. The organization of the paper is as follows. <p> Since the optimal control values depend continuously on the initial conditions (as is proven in [7]), interpolation between stored values give good approximations of the optimal control values for the non-stored states. For similar techniques and more details, we refer to monographs like <ref> [12] </ref>. A problem is of course how small to choose the time-step h. The time constants of the system (A; B) provides us with some information about the size to make efficient control possible. Explicit formulas specifying upper bounds on the time-step corresponding to a certain performance are not available. <p> In the example below T = 15 is sufficiently large to approximate the feedback, because the system is stabilized within this time span. In our investigation of connections between finite and infinite horizon problems, the approximation of the stationary feedback as proposed is justified by the convergence results. In <ref> [12] </ref>, much more approximation techniques are considered in particular for value functions. Numerical robustness of the method is guaranteed by choosing sufficiently large T and sufficiently small h.
Reference: [13] <author> Lee, E.B., and Markus, L., </author> <year> 1967, </year> <title> Foundations of Optimal Control Theory. </title> <publisher> John Wiley & Sons, Inc.. </publisher>
Reference-contexts: All these issues will be considered in the current paper. Mainly, there are two classical approaches in optimal control theory. The first approach is the maximum principle, initiated by Pontryagin et al. [17]. The original maximum principle has been used and extended by many others <ref> [13, 4, 15, 11] </ref>. To get a complete treatise of dealing with constrained finite horizon optimal control problems the application of the maximum principle is incorporated. However, for the infinite horizon case a similar result is nontrivial.
Reference: [14] <author> Luenberger, D.G., </author> <year> 1969, </year> <title> Optimization by Vector Space Methods. </title> <publisher> John Wiley & Sons, Inc.. </publisher>
Reference-contexts: This motivates the introduction of a generalization of orthogonal projections on closed subspaces in Hilbert spaces. Consider the following fundamental theorem in Hilbert space theory concerning the minimum distance to a closed convex set. For a proof, see chapter 3 in <ref> [14] </ref>. Theorem 5 (Minimum Distance to a Convex Set) Let x be a vector in a Hilbert space H with inner product ( j ) and let K be a closed convex subset of H. <p> This proof resembles the proof of Theorem 1 in <ref> [14] </ref>. Let fk n g n2IN be a sequence that converges to P K x 2 K with the property that k n 2 K n for all n 2 IN .
Reference: [15] <author> Macki, J., and Strauss, A., </author> <year> 1982, </year> <title> Introduction to Optimal Control Theory. </title> <publisher> Springer-Verlag Inc.. </publisher>
Reference-contexts: All these issues will be considered in the current paper. Mainly, there are two classical approaches in optimal control theory. The first approach is the maximum principle, initiated by Pontryagin et al. [17]. The original maximum principle has been used and extended by many others <ref> [13, 4, 15, 11] </ref>. To get a complete treatise of dealing with constrained finite horizon optimal control problems the application of the maximum principle is incorporated. However, for the infinite horizon case a similar result is nontrivial.
Reference: [16] <author> Pachter, M., </author> <year> 1980, </year> <title> The Linear-Quadratic Optimal Control Problem with Positive Controllers, </title> <journal> Int. Journal of Control, </journal> <volume> Vol. 32. Iss. 4, </volume> <pages> 589-608. </pages>
Reference-contexts: The existence part leads to a verification whether the set of admissible controls has been chosen properly. In section 2 we shortly discuss this problem. Existence of solutions to the optimal control problem is studied in <ref> [16] </ref> based on "regular synthesis," a method based on strengthening Pontryagin's necessary conditions for optimality into sufficient conditions. The form of the conditions in [16] is only explicit in the case where the integrand in the cost functional is independent of the trajectory x (i.e. <p> In section 2 we shortly discuss this problem. Existence of solutions to the optimal control problem is studied in <ref> [16] </ref> based on "regular synthesis," a method based on strengthening Pontryagin's necessary conditions for optimality into sufficient conditions. The form of the conditions in [16] is only explicit in the case where the integrand in the cost functional is independent of the trajectory x (i.e. Q = 0 in terms of [16] and C &gt; D = 0, C &gt; C = 0 in terms of section 2) below). <p> The form of the conditions in <ref> [16] </ref> is only explicit in the case where the integrand in the cost functional is independent of the trajectory x (i.e. Q = 0 in terms of [16] and C &gt; D = 0, C &gt; C = 0 in terms of section 2) below). This special case is hardly studied in the literature and also in our paper these results do not apply. <p> This problem is treated in [8], were necessary and sufficient conditions are stated that are easily verified. Under the assumption of positive stabilizability of the system, existence and uniqueness of the optimal controls for the infinite horizon case are established. In addition to papers like <ref> [16] </ref> where only existence of optimal controls is studied, we charac-terise the optimal controls in various equivalent forms. Moreover, smoothness properties of the optimal control and value function and convergence results between finite and infinite horizon are stated. In [16] no attention is paid to the infinite horizon case and the <p> In addition to papers like <ref> [16] </ref> where only existence of optimal controls is studied, we charac-terise the optimal controls in various equivalent forms. Moreover, smoothness properties of the optimal control and value function and convergence results between finite and infinite horizon are stated. In [16] no attention is paid to the infinite horizon case and the problem of how to compute or approximate the optimal control in both the finite and infinite horizon problem. All these issues will be considered in the current paper. Mainly, there are two classical approaches in optimal control theory. <p> Parametrising ~ as ~ = F , where F is an ~m fi m matrix and taking ( ~ A; ~ B; ~ C; ~ D) = (A; BF; C; DF ) translates the problem into an optimization problem with positive controls (as in <ref> [16] </ref>). 3 Along with this problem formulation, there are several questions to be answered. <p> The answering of these questions is the main goal of this paper. Note that the only question considered in <ref> [16] </ref> is the first one. Of course, this question is treated there for a more general class of optimal control problems. In fact, this is a minimum-norm problem over a closed convex cone. <p> This follows immediately from (8). 2 5 Optimal Controls In this section, we answer the first two questions raised in Section 2, just after the problem formulation. 5.1 Existence and Uniqueness As mentioned before, to establish existence of optimal controls the results from <ref> [16] </ref> cannot be used, because the condition formulated there are only explicit in case of C &gt; C = 0, C &gt; D = 0. As discussed in Section 2, a standing assumption in the remainder of the paper will be the full column rankness (or injectivity) of D.
Reference: [17] <author> Pontryagin, L.S., Boltyanskii, V.G., Gamkrelidze, R.V., and Mishchenko, E.F., </author> <year> 1962, </year> <title> The Mathematical Theory of Optimal Processes. </title> <publisher> Interscience Publishers, John Wiley & Sons. </publisher>
Reference-contexts: All these issues will be considered in the current paper. Mainly, there are two classical approaches in optimal control theory. The first approach is the maximum principle, initiated by Pontryagin et al. <ref> [17] </ref>. The original maximum principle has been used and extended by many others [13, 4, 15, 11]. To get a complete treatise of dealing with constrained finite horizon optimal control problems the application of the maximum principle is incorporated. However, for the infinite horizon case a similar result is nontrivial. <p> positive-homogeneity of P, it is clear that for ff 0 u t;T;ffx 0 = ffu t;T;x 0 (13) V (t; ffx 0 ) = ff 2 V (t; x 0 ): (14) 5.2 Maximum Principle In spite of the fact that the results in this subsection are classical (see e.g. <ref> [17] </ref>) they are stated here, because in section 7 this theory is exploited to derive a maximum principle for the infinite horizon case and convergence results between finite and infinite horizon optimal controls.
Reference: [18] <author> Fleming, W.H., and Rishel, R.W., </author> <year> 1975, </year> <title> Deterministic and Stochastic Optimal Control. </title> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Later, people realized that the same ideas can be used for optimal control problems in continuous time. For continuous time problems, dynamic programming leads to a partial differential equation, the so-called Hamilton-Jacobi-Bellman equation, which has the value function among its solutions <ref> [18] </ref>. Traditionally, one needed assumptions on the smoothness of the value function to apply this theory. However, in many problems, the value function does not behave smoothly, which causes both analytical and numerical problems.
Reference: [19] <author> Thomas, G.B., and Finney, R.L.,1996, </author> <title> Calculus and Analytic Geometry. </title> <publisher> Addison-Wesley Publishing Company. </publisher>
Reference-contexts: For proving the differentiability of V with respect to t we need an auxiliary result, which we formulate in the next lemma. It is an extension of the mean value theorem for functions of a real variable (see <ref> [19] </ref>). 10 Lemma 12 Let f be a function from IR n to IR, which is differentiable on IR n with gradient f x . Let x 0 ; x 1 2 IR n . <p> [x 1 x 0 ]): This function is continuous on [0; 1] and differentiable on (0; 1) with derivative dh (ff) = (f x (x 0 + ff [x 1 x 0 ]) j x 1 x 0 ) Using the mean value theorem for functions of one real variable <ref> [19] </ref> we arrive at the existence of a fi 2 (0; 1) such that h (1) h (0) = dg (ff) dff (fi).
Reference: [20] <author> Yosida, K., </author> <year> 1980, </year> <title> Functional Analysis. </title> <publisher> Springer. </publisher> <pages> 25 </pages>
Reference-contexts: n g n2IN in a Hilbert space H is said to converge weakly to x 2 H, if for every h 2 H we have (x n j h) ! (x j h) (n ! 1): Notation: x n w The following properties of weak limits are classical (see e.g. <ref> [20] </ref>). Lemma 22 1. A weakly convergent sequence is bounded. 2. Each bounded sequence in a Hilbert space, has a weakly convergent subsequence. 3. In a Hilbert space x n ! x if and only if x n w 4.
References-found: 20

