URL: http://polaris.cs.uiuc.edu/reports/1436.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: A STUDY ON INSTRUCTION-LEVEL PARALLELISM ARCHITECTURES AND OVERHEAD ANALYSIS OF PARALLEL EXECUTION  
Author: BY ADAM STUART BLOCK 
Degree: 1994 THESIS Submitted in partial fulfillment of the requirements for the degree of Master of Science in Electrical Engineering in the Graduate College of the  
Address: 1995 Urbana, Illinois  
Affiliation: B.S., University of Illinois at Urbana-Champaign,  University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> D. Alpert and D. Avnon, </author> <title> "Architecture of the Pentium microprocessor," </title> <booktitle> IEEE Micro, </booktitle> <pages> pp. 11-21, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: This chapter of my thesis will provide a critical overview of six superscalar architectures, giving the reader an appreciation of the diversity of approaches available to this type of processor. 2.2 Intel Pentium The first superscalar architecture that will be discussed is the Intel Pentium <ref> [1] </ref>. The Pentium is the latest addition to the X86 architecture, which dates back to 1978. The 4 processor uses 3.1 million transistors in 0.6-m BiCMOS with the most recent chip release operating at 100 MHz.
Reference: [2] <author> R. R. Oehler and M. W. Blasgen, </author> <title> "IBM RISC System/6000: Architecture and performance," </title> <booktitle> IEEE Micro, </booktitle> <pages> pp. 55-62, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Although many different versions of the RISC System/6000 have been developed over the years, this section will discuss the properties of the original design <ref> [2] </ref>. At the time of the original design, the Model 550 was the fastest version operating at a speed of 42 MHz. The first implementations of this architecture incorporated three discrete CMOS chips for the processor components. <p> The branch unit's register set can contain condition codes, target addresses, and loop counts, in addition to separate link and count registers. The integer unit contains 32, 32-bit general-purpose registers, and the floating point unit contains 32, 64-bit floating point registers (actually 38 in hardware). The paper <ref> [2] </ref> indicates that the branch chip contains an 8-kb instruction cache, while a 16-kb data cache chip is replicated two or four times in the processor. The newer version of the 550L instruction and data caches are each 32 kb [3].
Reference: [3] <author> IBM Corp., </author> <title> "RISC System/6000 500 Series deskside servers," </title> <type> Tech. Rep., </type> <institution> IBM Corporate Web Site, </institution> <year> 1994. </year>
Reference-contexts: The paper [2] indicates that the branch chip contains an 8-kb instruction cache, while a 16-kb data cache chip is replicated two or four times in the processor. The newer version of the 550L instruction and data caches are each 32 kb <ref> [3] </ref>. The caches utilize a write-back method and are software dependent for synchronization with the main memory and other caches. Also, there is no level-two cache available for this version of the processor.
Reference: [4] <author> K. Diefendorff and M. Allen, </author> <title> "The Motorola 88110 superscalar RISC microprocesor," </title> <booktitle> in COMPCON Spring 1992, </booktitle> <pages> pp. 157-162, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: The space needed for this ability perhaps could be utilized for other purposes, while having the branch pipeline perform both branch operations and conditional register modification. 2.4 Motorola 88110 The Motorola 88110 is the single chip descendant of the 88100/200 architecture <ref> [4] </ref> and [5]. The processor uses under 1.3 million transistors using 1-m CMOS design rules clocked during simulations at a speed of 50 MHz. <p> Because the logical address of the branch is used to access the TIC, the branch target instructions will be ready for the next fetch with no bubbles. From papers <ref> [4] </ref> and [5], it is not clear how the sequencer decides which path of execution to follow. A FIFO replacement policy is utilized to fill misses in the TIC.
Reference: [5] <author> K. Diefendorff and M. Allen, </author> <title> "Organization of the Motorola 88110 superscalar RISC processor," </title> <booktitle> IEEE Micro, </booktitle> <pages> pp. 40-63, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: The space needed for this ability perhaps could be utilized for other purposes, while having the branch pipeline perform both branch operations and conditional register modification. 2.4 Motorola 88110 The Motorola 88110 is the single chip descendant of the 88100/200 architecture [4] and <ref> [5] </ref>. The processor uses under 1.3 million transistors using 1-m CMOS design rules clocked during simulations at a speed of 50 MHz. <p> Because the logical address of the branch is used to access the TIC, the branch target instructions will be ready for the next fetch with no bubbles. From papers [4] and <ref> [5] </ref>, it is not clear how the sequencer decides which path of execution to follow. A FIFO replacement policy is utilized to fill misses in the TIC.
Reference: [6] <author> S. P. Song and M. Denman, </author> <title> "The PowerPC 604 RISC microprocessor," </title> <institution> Motorola, Inc. and IBM Corp., </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Perhaps by connecting units with similar execution latencies, these output conflicts can be avoided. 2.5 PowerPC 604 The PowerPC 604 is the most recent incarnation of the PowerPC architecture <ref> [6] </ref> and [7]. It is fully compatible with its predecessors, the 601 and 603. The 604 incorporates 3.6 million transistors using 0.5-m CMOS clocked at a speed of 100 MHz.
Reference: [7] <institution> IBM Microelectronics and Motorola Literature Distribution, </institution> <type> PowerPC 604 RISC microprocessor technical summary, </type> <month> May </month> <year> 1994. </year>
Reference-contexts: Perhaps by connecting units with similar execution latencies, these output conflicts can be avoided. 2.5 PowerPC 604 The PowerPC 604 is the most recent incarnation of the PowerPC architecture [6] and <ref> [7] </ref>. It is fully compatible with its predecessors, the 601 and 603. The 604 incorporates 3.6 million transistors using 0.5-m CMOS clocked at a speed of 100 MHz.
Reference: [8] <author> G. Blanck and S. Krueger, </author> <title> "The SuperSPARC microprocessor," </title> <booktitle> in COMPCON Spring 1992, </booktitle> <pages> pp. 136-141, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Cache touch instructions are available to improve hit probability. Also there are separate MMUs (Memory Management Units) for instruction and data. 2.6 SuperSPARC The SuperSPARC is a machine fully compatible with the SPARC version 8 architecture <ref> [8] </ref> and [9]. The processor is composed of 3.1 million transistors in 0.8-m BiCMOS, 23 operating at a speed of 60 MHz. The integer unit uses a four-cycle pipeline, in which each cycle has two phases.
Reference: [9] <institution> Sun Microsystems Computer Corporation, Mountain View, CA, The SuperSPARC microprocessor: </institution> <type> Technical white paper, </type> <year> 1992. </year>
Reference-contexts: Cache touch instructions are available to improve hit probability. Also there are separate MMUs (Memory Management Units) for instruction and data. 2.6 SuperSPARC The SuperSPARC is a machine fully compatible with the SPARC version 8 architecture [8] and <ref> [9] </ref>. The processor is composed of 3.1 million transistors in 0.8-m BiCMOS, 23 operating at a speed of 60 MHz. The integer unit uses a four-cycle pipeline, in which each cycle has two phases.
Reference: [10] <institution> MIPS Technologies, Inc., Mountain View, CA, R10000 microprocessor chip set product overview, </institution> <year> 1994. </year>
Reference-contexts: The quantity of transistors in the processor is not available, but the transistors will utilize sub 0.5-m CMOS at a clock speed of 200 MHz <ref> [10] </ref>. There are five execution units that can operate independently in this processor: two arithmetic units, a floating point adder, a floating point multiplier, and two secondary floating point units to handle long latency instructions. This is in addition to the branch unit and load/store unit.
Reference: [11] <author> P. Y.-T. Hsu, </author> <title> "Designing the TFP microprocessor," </title> <booktitle> IEEE Micro, </booktitle> <pages> pp. 23-33, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The R10000 supports out-of- order execution. 27 The most novel feature of the MIPS R10000 is its method of providing alignment-free instruction dispatching <ref> [11] </ref> and [12]. This processor allows for the dispatching of anywhere from zero to four instructions per cycle, not limited to an even number of instructions as in some processors.
Reference: [12] <author> MIPS Technologies, Inc., </author> <title> Mountain View, CA, R8000 microprocessor chip set product overview, </title> <booktitle> 1994. </booktitle> <pages> 133 </pages>
Reference-contexts: The R10000 supports out-of- order execution. 27 The most novel feature of the MIPS R10000 is its method of providing alignment-free instruction dispatching [11] and <ref> [12] </ref>. This processor allows for the dispatching of anywhere from zero to four instructions per cycle, not limited to an even number of instructions as in some processors.
Reference: [13] <author> B. R. Rau and C. D. Glaeser, </author> <title> "Some scheduling techniques and an easily schedulable horizontal architecture for high performance scientific computing," </title> <booktitle> in Proceedings of the Fourteenth Annual Workshop on Microprogramming, </booktitle> <pages> pp. 183-198, </pages> <month> October </month> <year> 1981. </year>
Reference-contexts: A more effective method of producing highly optimal loop code is software pipelining <ref> [13] </ref>, [14], [15]. Software pipelining functions in much the same way that functional units in processors utilize pipelining to improve performance. In Figure 3.2, the original loop is now pipelined to improve performance. This schedule for software pipelining assumes that there are only interloop dependencies of distance one. <p> Regrettably, a good schedule is difficult to create when resource constraints need to be considered. By utilizing specialized hardware, certain types of resource constraints can be eliminated, thereby making the creation of a schedule simpler. Rau and Glaeser <ref> [13] </ref> present a technique to schedule loops, then they illustrate 53 a hardware solution for the execution of these schedules. All references to "the paper" in this section will refer to Rau and Glaeser [13]. 3.3.1 Resource allocation The researchers separate the resources into explicitly and implicitly scheduled resources. <p> Rau and Glaeser <ref> [13] </ref> present a technique to schedule loops, then they illustrate 53 a hardware solution for the execution of these schedules. All references to "the paper" in this section will refer to Rau and Glaeser [13]. 3.3.1 Resource allocation The researchers separate the resources into explicitly and implicitly scheduled resources. An explicitly scheduled resource is defined as a resource that can receive simultaneous, conflicting requests by a faulty schedule. Functional units, register files, memories, and shared buses all fall into this resource category. <p> A careful choice must be made because different schedule lengths can be derived depending on which operations are chosen to execute. To create an optimal schedule, all of the different combinations of operations must be attempted. Since this would be too time consuming, the paper <ref> [13] </ref> utilized a heuristic to assign priorities to the ready operations 54 to assist in operation selection. The heuristic assigns a priority of 0 to all operations with no successors. <p> The heuristic assigns a priority of 0 to all operations with no successors. The other operations are assigned a priority equal to the maximum of all of their successors plus the execution and interconnect time for the particular operation. 3.3.2 Loop scheduling issues The paper <ref> [13] </ref> first implements a software pipelining algorithm for loops known as generalized vector computations (GVC). This model assumes that: 1) the number of iterations is known before run-time, 2) there are no conditional branches in the loop, and 3) there are no cross-iteration dependencies. <p> The polycyclic architecture simplifies the creation of schedules by eliminating the need for registers to contain delay values. Instead, delay elements are placed on the interconnect between every explicitly scheduled resource. In Figure 3.12 <ref> [13] </ref>, we see the delay elements on all of the possible paths between the resources. These delay elements are considered implicitly scheduled resources, because no possibility of conflicting demands exists. <p> Similarly, the read from the delay element is implicitly scheduled 58 on the same cycle that the data is consumed. By utilizing these implicitly scheduled resources, the scheduler is relieved of the burden of allocating registers. The paper <ref> [13] </ref> indicates that this scheme of utilizing delay elements can fail if the capacity of any one delay element is exceeded. This can result from the overlapping of successive iterations of a loop in which an instance of a variable is generated before the previous value has been utilized. <p> This can result from the overlapping of successive iterations of a loop in which an instance of a variable is generated before the previous value has been utilized. For the programs analyzed in the paper <ref> [13] </ref>, there are very few instances of more than four values stored at a delay node. Unfortunately, this is of little consolation to cases in which more than four values need to be stored at a 59 delay node. <p> To keep track of the proper locations of the values with respect to the pending operations, a bookkeeping function must be incorporated into the code generator. The bookkeeping function will allow the same code to be utilized for each iteration without the spurious, programmed movement of data <ref> [13] </ref>. Unfortunately, a bookkeeping function cannot account for asynchronous interrupts, and can therefore create an extra problem for delaying values when performing interrupt code. <p> If a system contained as much register capacity as delay elements capacity, then the standard scheduling algorithm would not have to backtrack as often, and the likelihood of finding a schedule matching MII would be increased. The paper <ref> [13] </ref> indicates that the complexity of the register scheduling procedure remains much the same, but also that delay elements substantially increased the size of the interconnect. Also, contributing to the overhead is the need to utilize a full crossbar network.
Reference: [14] <author> R. F. Touzeau, </author> <title> "A FORTRAN compiler for the FPS-164 scientific computer," </title> <booktitle> in Proceedings of ACM SIGPLAN '84 Symposium on Compiler Construction, </booktitle> <pages> pp. 48-57, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: A more effective method of producing highly optimal loop code is software pipelining [13], <ref> [14] </ref>, [15]. Software pipelining functions in much the same way that functional units in processors utilize pipelining to improve performance. In Figure 3.2, the original loop is now pipelined to improve performance. This schedule for software pipelining assumes that there are only interloop dependencies of distance one. <p> As in the polycyclic architecture described in Section 3.3, the FPS-164 offers a hardware solution to the problem of creating a software pipelining schedule <ref> [14] </ref>. The problem of functional unit allocation and limitation is addressed by the instruction word style of the FPS-164. The FPS-164 utilizes a fixed VLIW format along with the compiler techniques implemented for this processor. <p> This prevents the compiler from exploiting loops 64 with larger bodies. The recognition and optimization of recursive references is another feature of the optimizer. Memory accesses bound by interloop dependencies that refer to the previous iteration are replaced by variables. Figure 3.14 <ref> [14] </ref> illustrates this optimization. Because the variable TEMP would be replaced by a global register, a memory access would be avoided. While avoiding memory accesses is useful, register allocation must now be carefully examined. <p> If hardware support, such as speculative execution, was implemented, this problem could be avoided by allowing the kernel to be issued repetitively until the exit condition is met. For example, Figure 3.15 (a modification of Figure 4 from <ref> [14] </ref>) shows a pipelined loop that executed for three iterations. The SETMA operation will set the memory address register. Normally, the FPS-164 compiler would depend on hand-scheduling to eliminate the extra (crossed out) instructions for iterations that will not complete. <p> Through the use of fixed VLIW slots, the difficulties of functional unit constraints are reduced. The problem with this approach is that there is no explicit discussion in the paper <ref> [14] </ref> about handling functional units that are not fully pipelined. While the hardware eases the difficulty of functional unit allocation, it increases the difficulty of register allocation.
Reference: [15] <author> M. Lam, </author> <title> "Software pipelining: An effective scheduling technique for VLIW machines," </title> <booktitle> in Proceedings of ACM SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 318-327, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: A more effective method of producing highly optimal loop code is software pipelining [13], [14], <ref> [15] </ref>. Software pipelining functions in much the same way that functional units in processors utilize pipelining to improve performance. In Figure 3.2, the original loop is now pipelined to improve performance. This schedule for software pipelining assumes that there are only interloop dependencies of distance one. <p> Also, different registers have different overwriting policies. With an improved register system, the FPS-164 would be a better machine for executing pipelined code. 3.5 Software Pipelining for VLIW Machines In the paper by Lam <ref> [15] </ref>, a technique for creating software pipelined code without modifying hardware is illustrated. The procedure involves many techniques taken from other papers, and enhances a few of them. <p> The smallest units of scheduling are minimal indivisible sequences of micro-instructions. In Figure 3.16 <ref> [15] </ref>, the value produced by the add must be consumed by the write operation exactly two cycles after the add is begun. These two operations are combined into one indivisible sequence that can be modeled as a node in the DDG. <p> The 72 only difference between the two techniques is that this paper <ref> [15] </ref> utilizes a linear search to find the II, while the method for the FPS-164 implements a binary search. This linear search was shown to perform better than the binary search because in empirical tests a schedule for the lower bound can be found. <p> Normally, the length of the kernel for small loops is equivalent to the initiation interval. However, for a case with n repeating code sequences, the kernel would have to be unrolled n times. An example of this unrolling can be seen in Figure 3.17 <ref> [15] </ref>. tion Fragment. Right: Pipelined Code with Kernel Unrolled Once To implement modulo variable expansion, the variables redefined at the beginning of every loop iteration are found. At first, every iteration has a dedicated register for every variable, and all interloop dependencies are removed from these variables. <p> This should minimize code growth while conserving registers and avoiding register spill code. 3.5.3 Hierarchical reduction The hierarchical reduction method discussed in the paper <ref> [15] </ref> is much like the current hierarchical task graph representations utilized in some compilers. The method allows for the pipelining of loops that contain conditional statements. <p> As in trace scheduling, after the loop is reduced to simple, acyclic sequences, global code motion can take place. With hierarchical reduction, the penalty of short software pipelined loops is minimized because the prologue and epilogue can be overlapped with instructions outside of the loop. The paper <ref> [15] </ref> also claims that software pipelining can be applied to outer loops. When evaluating the performance of the project's compiler, some large, and perhaps unrealistic, assumptions are made. First, all loop bounds are assumed to be known at compile time. <p> If this was not the case, and one data-dependent branch was taken much more often, then the cost associated with the duplication of code may hinder performance. 78 3.5.4 Conclusions The technique illustrated in this paper <ref> [15] </ref> combines many earlier methods. Modulo scheduling, using modifications of FPS-164 approaches, is used to software pipeline the loop. The algorithm of modulo variable expansion is introduced in this paper, but the algorithm was too complicated to be utilized.
Reference: [16] <author> J. A. Fisher, </author> <title> "Trace scheduling: A technique for global microcode compaction," </title> <journal> IEEE Transactions on Computers, </journal> <pages> pp. 478-490, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: In this chapter, different techniques for parallelizing loops will be discussed, each containing one or both of these problem-solving approaches. 38 3.2 Trace Scheduling A crucial component of the capacity to software pipeline code is the ability to find parallel instructions. Trace scheduling <ref> [16] </ref> utilizes the principle of compaction to create parallel code. Earlier algorithms compacted each basic block of code separately, without considering the properties of surrounding blocks, which resulted in many of the compactions having to be undone in order to gain better parallelism. <p> Microinstruction list scheduling (or just list scheduling), assigns priority values to the MOPs before scheduling. To create these priority values for each MOP, another heuristic, such as highest levels first <ref> [16] </ref>, is employed. Highest levels first determines the priority of a MOP by length of the longest chain from the MOP to a leaf. Simulations have shown 40 that this heuristic for assigning priorities performs within a few percentage points of optimal. <p> Trace scheduling optimizes code by operating on traces instead of basic blocks. A trace is defined as a loop-free sequence of instruction that might be executed sequentially for some choice of data <ref> [16] </ref>. A trace with no conditional branches can be seen as analogous to a basic block (or portion of a basic block), but usually consists of many basic blocks. A trace is incorporated into larger pieces of code by creating dummy MIs for the entries and exits of the trace. <p> MOPs can also be placed into the loop representative as long as the MOP is loop invariant with respect to the loop, and the addition of the MOP does not lengthen the schedule of the loop <ref> [16] </ref>. The second requirement of incorporating a MOP into a loop implies that as long as free resources are available during any cycle of the loop, a potential exists for other MOPs to be placed in the loop. <p> To avoid this problem, a suggested enhancement to trace scheduling is to add edges to the DAG to prevent the movement of the loop representative when it will cause duplication <ref> [16] </ref>. While bringing extra code into the loops would be more efficient for very short loops, this technique would provide little gain for larger loops. This occurs because the instruction incorporated into the loop was only executed once originally. <p> The algorithm of modulo variable expansion is introduced in this paper, but the algorithm was too complicated to be utilized. The principle of hierarchical reduction was also introduced, although it is almost identical to the technique used in trace scheduling <ref> [16] </ref>. Also, hierarchical reduction can be seen as an analogue to HTG (Hierarchical Task Graph) operations.
Reference: [17] <author> A. Aiken and A. Nicolau, </author> <title> "Perfect pipelining: A new loop parallelization technique," </title> <booktitle> in Proceedings of the 1988 European Symposium on Programming, </booktitle> <pages> pp. 221-235, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: Trace scheduling relies on profiling techniques to determine the flow of the program, while other techniques may ignore branches entirely. Perfect pipelining <ref> [17] </ref> produces a pattern for any type of control flow and utilizes multi-way branches to handle control problems. In other loop unrolling and compaction algorithms, problems can occur when interloop dependencies exist. <p> This knowledge is required in order to utilize dependence analysis. Without this knowledge, it will be unknown whether it is safe to perform two instructions in parallel. The paper <ref> [17] </ref> does not indicate which method it 80 used to determine conflicts between instructions. Unless an effective technique can be implemented, the ability to perform the transformations that make up the foundation of perfect pipelining may be severely hampered. <p> This transform is restricted by the fact that no 82 dependence can exist between x and the instructions contained in m. After the test x is moved up, the node following x must be modified. In Figure 3.21 <ref> [17] </ref>, the node n has been split into node n t and n f . These two nodes represent the original true and false path following x in the original node n. <p> By modifying the move algorithm, resource constraints can be satisfied by allowing the movement of an instruction into a node only if the constraints are not violated. 85 After the simple rule finishes with the unrolled iterations, the perfect pipelining algorithm attempts to find a repeating pattern. The paper <ref> [17] </ref> proves that the simple rule will produce a pattern on every path through the loop. An additional assertion is that the first node on any path that does not contain an instruction from the first iteration will be repeated. <p> Unfortunately, this approach requires special support from the hardware, because many architectures can evaluate only one branch condition at a time. 3.6.3 Resource allocation Other than the modification that can be made to the move algorithm, the paper <ref> [17] </ref> contains little information about how to deal with resource conflicts. For instance, there is no way to handle a limited number of registers with the unmodified algorithm. The 86 simple fix to the move algorithm may prevent all possible parallelism from being extracted from the loop code.
Reference: [18] <author> J. Moreira, </author> <title> On the implementation and effectiveness of autoscheduling for shared-memory multiprocessors, </title> <type> Ph.D. dissertation, </type> <institution> Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, </institution> <year> 1995. </year>
Reference-contexts: Superscalar processors create instruction-level parallelism at run-time, while software pipelining creates instruction-level parallelism at compile-time. Unfortunately, neither of these concepts deals with the issue of providing parallelism at a higher level, so that a multiprocessor machine can execute parallel code. Autoscheduling <ref> [18] </ref> is a technique that produces parallelism that can be exploited across many processors. Au-toscheduling embeds drive code into the program to allow task scheduling to be performed at run-time. <p> In this chapter, the relationship between the overhead for executing parallel tasks and the speedup of the code as compared to the serial case will be analyzed. An execution-driven simulator, as described in <ref> [18] </ref>, will be used to show this relation on different benchmark codes. 4.2 Overview of Simulator Starting from the HTG representation, the autoscheduling compiler creates auto-scheduling C++ code for a given program. This C++ code is linked to instrumented versions of run-time libraries and data definitions. <p> The simulated processor that 90 called CountTime (t) would have its timer T incremented by the cycle cost of the elemen tary operation, t. The values utilized by CountTime (t) for the elementary operations in the execution-driven simulator are shown in Table 4.1 <ref> [18] </ref>. For example, if the operation A fl B is executed on processor n, where A and B are floating-point values, then the cost in cycles of the multiply operation would be 5. <p> In this section, the amounts, and types of overhead involved for loop scheduling will be enumerated. Also, the relation between overhead and speedup will be examined. To help illustrate these points, the DOTEST benchmark will be utilized. As shown in Figure 4.1 <ref> [18] </ref>, the DOTEST benchmark utilizes a doall loop to perform a number of pseudo-multiply operations. The delay (m) instruction will add 8 fl m to the timer of the 94 calling processor. This will simulate as if m floating-point multiplies had taken place. <p> The general organization of a DOALL after compilation is shown in Figure 4.2 <ref> [18] </ref> (the line numbers are added for clarity of future examples, and are not produced by the compiler). When multiple processors execute the loop, each processor will receive a chunk of loop iterations to execute by the loop scheduler at Line 9 in Figure 4.2. <p> In this section, the types and amounts of overhead incurred while exploiting functional parallelism will be examined. Then, the simulator will be employed to determine the connection between scheduling overhead and speedup. The TASK64 benchmark, shown in Figure 4.7 <ref> [18] </ref>, will be utilized to assist in the examination of task-level parallelism. The TASK64 program is a 64-way fully parallel task graph. Each task consists of a delay (n) instruction to act as pseudo-multiply instructions. <p> The DAG reveals which tasks are dependent on other tasks for execution. In the case of TASK64, there are no dependencies between tasks, thereby allowing all tasks to execute in parallel. After compilation, the general organization of a TASK is shown in Figure 4.8 <ref> [18] </ref>. The line numbers have been included for future explanation and are not included by the compiler. In the entry (START) block of the DAG, the determination must be made as to whether the TASKs of the DAG will execute serially or in parallel. <p> flag is set by the following: mode = 8 &gt; &gt; &gt; : parallel if Q n =P ff serial otherwise where Q n is the number of tasks in the process queue, P is the number of processors currently assigned to the process, and ff is a given threshold <ref> [18] </ref>. Similar to the technique of 109 GSS, dynamic granularity control will create more parallelism if the number of processors is increased, and will prevent parallelism if the number of processors is decreased. The ff value is utilized to control the effect of the above ratio. <p> The size of the matrices (n) can be varied. SMM utilizes only functional parallelism. The task graph for this algorithm is shown in Figure 4.13 <ref> [18] </ref>. <p> In the given benchmark, the matrix, or submatrix for recursive calls, must be of size greater than 64; otherwise, the normal matrix multiply technique is utilized. The amount of potential speedup for this algorithm increases with the size of n <ref> [18] </ref>. The derivation of this fact is illustrated in the referenced paper. In Figure 4.14, the results of simulating the SMM with sizes 256 and 128 utilizing standard scheduling overhead are shown. As proven in [18], the SMM of size 256 120 provided greater speedup than the SMM of size 128. <p> The amount of potential speedup for this algorithm increases with the size of n <ref> [18] </ref>. The derivation of this fact is illustrated in the referenced paper. In Figure 4.14, the results of simulating the SMM with sizes 256 and 128 utilizing standard scheduling overhead are shown. As proven in [18], the SMM of size 256 120 provided greater speedup than the SMM of size 128. Due to overhead and too few processors, neither problem size reaches its full potential speedup (37.57 for n = 128 and 133.16 for n = 256). <p> This section of the code, shown in loop contains 14 small (50 operations each) parallel tasks. This version of the benchmark will be referred to as utilizing internal functional parallelism. In Figure 4.18 <ref> [18] </ref>, the 124 benchmark MDJ doall i = 1; 1000 enddoall end loop and task graphs are interchanged, thereby increasing the size of each task. This version of the benchmark utilizes external functional parallelism. In Figure 4.19, the simulation results for four different styles of the MDJ benchmark are shown.
Reference: [19] <author> C. D. Polychronopoulos and D. J. Kuck, </author> <title> "Guided self-scheduling: A practical scheduling scheme for parallel supercomputers," </title> <journal> IEEE Transactions on Computers, </journal> <pages> pp. 1425-1439, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: When multiple processors execute the loop, each processor will receive a chunk of loop iterations to execute by the loop scheduler at Line 9 in Figure 4.2. The chunk size is determined by the Guided self-scheduling (GSS) formula <ref> [19] </ref>: C = R where R is the number of iterations still to be scheduled in the loop and P is the number of processors executing the loop. <p> total amount of scheduling overhead incurred to execute each chunk is given by the summation of Equations (4.6) and (4.8): Scheduling Overhead = 2 fl (Acquire Lock + Release Lock) +Schedule a Chunk of Iterations: (4.9) In the worst case, the GSS algorithm will create O (p ln (N=p)) chunks <ref> [19] </ref>, where p corresponds to the number of processors executing the loop, and N corresponds to the number of iteration of the loop. <p> For further comparison, an overhead trace will also be generated using previous techniques (referenced in <ref> [19] </ref>), instead of GSS, to calculate the number of chunks issued. In the previous algorithms, the number of chunks issued increases linearly with the number of iterations, as opposed to logarithmically in GSS. The results of applying the theoretical formulas are illustrated in Figure 4.3.
References-found: 19

