URL: http://www.cs.ucsb.edu/~ambuj/wdag97.ps
Refering-URL: http://www.cs.ucsb.edu/~ambuj/research.html
Root-URL: http://www.cs.ucsb.edu
Title: Fault Tolerance Bounds for Memory Consistency  
Author: Jerry James and Ambuj K. Singh 
Address: Santa Barbara  
Affiliation: Department of Computer Science University of California at  
Abstract: We analyze the achievable fault tolerances of shared memory consistency conditions in the form of t-resilience, the ability to withstand up to t node failures. We derive tight bounds for linearizability, sequential consistency, processor consistency, and some weaker memories in totally asynchronous systems, in which failed and slow nodes cannot be distinguished. For linearizability, we show that neither the read nor the write operation can tolerate more failures than a minority of the nodes. For sequential consistency, processor consistency, and related conditions, we show that one operation can be wait-free and the other cannot tolerate more failures than a minority of the nodes. Several weaker conditions can have both operations wait-free.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Agrawal, D., Choy, M., Leong, H. V., and Singh, A. K. </author> <title> Mixed consistency: A model for parallel programming. </title> <address> In PODC '94 (Los Angeles, CA, USA, 14 17 Aug. </address> <year> 1994), </year> <pages> pp. 10110. </pages>
Reference-contexts: Weaker conditions have been detned in an attempt to ameliorate such eects, such as release consistency [12], causal memory [2], pipelined RAM [23], and mixed consistency <ref> [1] </ref>. The capabilities and characteristics of the various memory consistency conditions has been a fruitful topic of study. Both qualitative and quantitative comparisons have been carried out.
Reference: 2. <author> Ahamad, M., Neiger, G., Burns, J. E., Kohli, P., and Hutto, P. W. </author> <title> Causal memory: </title> <booktitle> Detnitions, implementation and programming. Distributed Computing 9, </booktitle> <month> 1 (Aug. </month> <year> 1995), </year> <month> 3749. </month>
Reference-contexts: However, such strong shared memories can have a signitcant negative impact on the performance of applications, and can limit the scalability of a system [5]. Weaker conditions have been detned in an attempt to ameliorate such eects, such as release consistency [12], causal memory <ref> [2] </ref>, pipelined RAM [23], and mixed consistency [1]. The capabilities and characteristics of the various memory consistency conditions has been a fruitful topic of study. Both qualitative and quantitative comparisons have been carried out.
Reference: 3. <author> Attiya, H. </author> <title> Ecient and robust sharing of memory in message-passing systems. </title> <booktitle> In WDAG '96 (Bologna, </booktitle> <address> Italy, 911 Oct. </address> <year> 1996), </year> . <editor> Babaoglu and K. Marzullo, Eds., </editor> <volume> vol. </volume> <booktitle> 1151 of Lecture Notes in Computer Science, </booktitle> <publisher> Springer-Verlag, </publisher> <pages> pp. 5670. </pages>
Reference-contexts: The message bound can be improved at the expense of greater memory usage by substituting Attiya's new construction of a 1-writer multi-reader atomic reg ister <ref> [3] </ref>. 3 Sequential Consistency A DSM implementation guarantees sequential consistency if a serialization exists for every history of the system. In [20] it is shown that for any implementation of sequential consistency, the read and write operations cannot both be wait-free, or (N 1)-resilient.
Reference: 4. <author> Attiya, H., Bar-Noy, A., and Dolev, D. </author> <title> Sharing memory robustly in message-passing systems. </title> <journal> J. ACM 42, </journal> <month> 1 (Jan. </month> <year> 1995), </year> <month> 12442. </month>
Reference-contexts: Next we show that the bounds of theorems 2 and 3 are tight, by exhibiting an implementation that achieves those bounds. Theorem 4. There exists an implementation of linearizability with ( d N 2 e 1)- resilient read and write operations. Proof. Attiya et al. <ref> [4] </ref> give a construction for a 1-writer multi-reader atomic register on a totally asynchronous system. This register has the property that both the read and the write operation are ( d N 2 e 1)-resilient. <p> Since linearizability is a local property, a collection of such registers constitutes a linearizable shared memory. ut The constructions in both <ref> [4] </ref> and [22] are bounded. A straightforward composition requires O (N 4 ) space on each node; each read or write operation results in the sending of O (N 3 ) messages, each of size O (N 3 ), in O (N ) messaging rounds.
Reference: 5. <author> Attiya, H., and Welch, J. L. </author> <title> Sequential consistency versus linearizability. </title> <journal> ACM Trans. Comput. Syst. </journal> <volume> 12, </volume> <month> 2 (May </month> <year> 1994), </year> <month> 91122. </month>
Reference-contexts: However, such strong shared memories can have a signitcant negative impact on the performance of applications, and can limit the scalability of a system <ref> [5] </ref>. Weaker conditions have been detned in an attempt to ameliorate such eects, such as release consistency [12], causal memory [2], pipelined RAM [23], and mixed consistency [1]. The capabilities and characteristics of the various memory consistency conditions has been a fruitful topic of study. <p> They derive lower bounds on the response time of sequentially consistent and lineariz-able DSMs in terms of d and u. Our results are for a totally asynchronous system, in which u and d are unknown. Attiya and Welch <ref> [5] </ref> compared linearizability and sequential consistency, and found that sequential consistency can be implemented more eciently in a partially synchronous setting. <p> We now show that the upper bound for sequential consistency is tight, by demonstrating two implementations. Each has one wait-free operation and one ( d N 2 e 1)-resilient operation. They are similar to the Atomic Broadcast-based fast read and fast write implementations of Attiya and Welch <ref> [5] </ref>. However, since any solution to Atomic Broadcast must be 0-resilient, we cannot use it. Our solutions employ an O (N )-round algorithm for the resilient operation. Thus, although we have shown that resilient solutions exist, our solution is very ine-cient.
Reference: 6. <author> Borowsky, E., and Gafni, E. </author> <title> Generalized FLP impossibility result for t-resilient asynchronous computations (extended abstract). </title> <address> In STOC '93 (San Diego, CA, USA, </address> <month> 1618 May </month> <year> 1993), </year> <pages> pp. 91100. </pages>
Reference-contexts: Note that any linearization of H is also a serialization of H. In this paper, we investigate fault-tolerance in the form of t-resilience, the ability to withstand t node failures. The notion of t-resilience has been detned in terms of consensus protocols [10], asynchronous tasks <ref> [6, 15] </ref>, and shared objects [7, 24]. We say that an operation is t-resilient if it can complete (i.e., not block) in spite of t node failures.
Reference: 7. <author> Chandra, T., Hadzilacos, V., Jayanti, P., and Toueg, S. </author> <title> Wait-freedom vs. t-resiliency and the robustness of wait-free hierarchies. </title> <address> In PODC '94 (Los Angeles, CA, USA, 1417 Aug. </address> <year> 1994), </year> <pages> pp. 33443. </pages>
Reference-contexts: In this paper, we investigate fault-tolerance in the form of t-resilience, the ability to withstand t node failures. The notion of t-resilience has been detned in terms of consensus protocols [10], asynchronous tasks [6, 15], and shared objects <ref> [7, 24] </ref>. We say that an operation is t-resilient if it can complete (i.e., not block) in spite of t node failures.
Reference: 8. <author> Chandra, T. D., and Toueg, S. </author> <title> Unreliable failure detectors for reliable distributed systems. </title> <journal> J. ACM 43, </journal> <month> 2 (Mar. </month> <year> 1996), </year> <month> 22567. </month>
Reference-contexts: It does so in the context of a totally asynchronous message passing system, where message delays are not known a priori. In such a system, failed processors are indistinguishable from slow processors; in particular, we assume the absence of a failure detector <ref> [8] </ref> or membership service [9]. Algorithms designed for such a model are able to continue processing while fault detection and recovery take place, rather than waiting until such actions have completed. They are also more tolerant of temporary uctuations in network speeds than algorithms designed to assume synchrony.
Reference: 9. <author> Cristian, F. </author> <title> Reaching agreement on processor group membership in synchronous distributed systems. </title> <booktitle> Distributed Computing 4, </booktitle> <month> 4 (Apr. </month> <year> 1991), </year> <month> 17587. </month>
Reference-contexts: It does so in the context of a totally asynchronous message passing system, where message delays are not known a priori. In such a system, failed processors are indistinguishable from slow processors; in particular, we assume the absence of a failure detector [8] or membership service <ref> [9] </ref>. Algorithms designed for such a model are able to continue processing while fault detection and recovery take place, rather than waiting until such actions have completed. They are also more tolerant of temporary uctuations in network speeds than algorithms designed to assume synchrony.
Reference: 10. <author> Dolev, D., Dwork, C., and Stockmeyer, L. </author> <title> On the minimal synchronism needed for distributed consensus. </title> <journal> J. ACM 34, </journal> <month> 1 (Jan. </month> <year> 1987), </year> <month> 7797. </month>
Reference-contexts: Note that any linearization of H is also a serialization of H. In this paper, we investigate fault-tolerance in the form of t-resilience, the ability to withstand t node failures. The notion of t-resilience has been detned in terms of consensus protocols <ref> [10] </ref>, asynchronous tasks [6, 15], and shared objects [7, 24]. We say that an operation is t-resilient if it can complete (i.e., not block) in spite of t node failures.
Reference: 11. <author> Friedman, R. </author> <title> Implementing hybrid consistency with high-level synchronization operations. </title> <booktitle> Distributed Computing 9, </booktitle> <month> 3 (Dec. </month> <year> 1995), 11929. </year>
Reference-contexts: As in our analysis, Kosa found that a tradeo occurs; improving one operation may require making other operations worse. The author also showed that hybrid consistency does not necessarily perform better than sequential consistency for arbitrary types. Friedman <ref> [11] </ref> gave implementations of hybrid consistency with synchronization operations, in addition to the usual read and write operations. He derived lower bounds on the time required to implement several synchronization operations in a partially asynchronous setting when d = u (i.e., message delays are in the range [0; d]).
Reference: 12. <author> Gharachorloo, K., Lenoski, D., Laudon, J., Gibbons, P., Gupta, A., and Hennessy, J. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In ISCA '90 (Seattle, </booktitle> <address> WA, USA, </address> <month> 2831 May </month> <year> 1990), </year> <pages> pp. </pages> <note> 1526. Revised in Stanford CSL Technical Report 93568. </note>
Reference-contexts: However, such strong shared memories can have a signitcant negative impact on the performance of applications, and can limit the scalability of a system [5]. Weaker conditions have been detned in an attempt to ameliorate such eects, such as release consistency <ref> [12] </ref>, causal memory [2], pipelined RAM [23], and mixed consistency [1]. The capabilities and characteristics of the various memory consistency conditions has been a fruitful topic of study. Both qualitative and quantitative comparisons have been carried out.
Reference: 13. <author> Goodman, J. R. </author> <title> Cache consistency and sequential consistency. </title> <type> Tech. Rep. 1006, </type> <institution> Computer Sciences Department, University of Wisconsin-Madison, </institution> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: The theorem follows by Lemma 5. ut Theorem 6 does not hold for implementations of sequential consistency with one shared variable. In that case, sequential consistency collapses to the weaker condition of cache consistency or coherence <ref> [13] </ref>. As we show in the full paper, cache consistency has an implementation in which both operations are wait-free. We now show that the upper bound for sequential consistency is tight, by demonstrating two implementations. Each has one wait-free operation and one ( d N 2 e 1)-resilient operation.
Reference: 14. <author> Herlihy, M. P. </author> <title> Wait-free synchronization. </title> <journal> ACM Trans. Program. Lang. Syst. </journal> <volume> 13, </volume> <month> 1 (Jan. </month> <year> 1991), </year> <month> 12449. </month>
Reference-contexts: The notion of t-resilience has been detned in terms of consensus protocols [10], asynchronous tasks [6, 15], and shared objects [7, 24]. We say that an operation is t-resilient if it can complete (i.e., not block) in spite of t node failures. An (N 1)-resilient operation is wait-free <ref> [14] </ref>; that is, it can complete without the assistance of any other node. 2 Linearizability A DSM implementation guarantees linearizability if a linearization exists for every history of the system.
Reference: 15. <author> Herlihy, M. P., and Shavit, N. </author> <title> The asynchronous computability theorem for t-resilient tasks. </title> <address> In STOC '93 (San Diego, CA, USA, </address> <month> 1618 May </month> <year> 1993), </year> <pages> pp. 11120. </pages>
Reference-contexts: Note that any linearization of H is also a serialization of H. In this paper, we investigate fault-tolerance in the form of t-resilience, the ability to withstand t node failures. The notion of t-resilience has been detned in terms of consensus protocols [10], asynchronous tasks <ref> [6, 15] </ref>, and shared objects [7, 24]. We say that an operation is t-resilient if it can complete (i.e., not block) in spite of t node failures.
Reference: 16. <author> Herlihy, M. P., and Wing, J. M. </author> <title> Linearizability: A correctness condition for concurrent objects. </title> <journal> ACM Trans. Program. Lang. Syst. </journal> <volume> 12, </volume> <month> 3 (July </month> <year> 1990), </year> <month> 46392. </month>
Reference-contexts: When a shared memory abstraction is built on top of a message passing system, it is called Distributed Shared Memory [21] (DSM). A DSM system provides a contract to the programmer in the form of a consistency condition. Two well-known consistency conditions are linearizability <ref> [16, 19] </ref> and sequential consistency [18], which guarantee that the DSM behaves like a single physical memory module with no cache. However, such strong shared memories can have a signitcant negative impact on the performance of applications, and can limit the scalability of a system [5]. <p> An (N 1)-resilient operation is wait-free [14]; that is, it can complete without the assistance of any other node. 2 Linearizability A DSM implementation guarantees linearizability if a linearization exists for every history of the system. Linearizability is a local property <ref> [16] </ref>; if each individual object in a system is linearizable, then the entire system is linearizable. Due to this property, a collection of individually linearizable variables forms a linearizable shared memory. Neither the read nor the write operation can be wait-free, or (N 1)-resilient, as shown in [20].
Reference: 17. <author> Kosa, M. J. </author> <title> Making operations of concurrent data types fast. </title> <address> In PODC '94 (Los Angeles, CA, USA, 1417 Aug. </address> <year> 1994), </year> <pages> pp. 3241. </pages>
Reference-contexts: In the synchronous case, both conditions can have one instantaneous operation and one that suers a network latency. Our proofs use a similar technique, but we work in a totally asynchronous setting. Kosa <ref> [17] </ref> compares the costs of linearizability, sequential consistency, and hybrid consistency of ADTs in terms of worst-case time complexity. In addition to the consistency condition, two factors were considered: the synchrony of the system (synchronous or partially synchronous); and the algebraic properties of a given ADT.
Reference: 18. <author> Lamport, L. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Trans. Comput. </journal> <volume> 28, </volume> <month> 9 (Sept. </month> <year> 1979), </year> <month> 6901. </month>
Reference-contexts: When a shared memory abstraction is built on top of a message passing system, it is called Distributed Shared Memory [21] (DSM). A DSM system provides a contract to the programmer in the form of a consistency condition. Two well-known consistency conditions are linearizability [16, 19] and sequential consistency <ref> [18] </ref>, which guarantee that the DSM behaves like a single physical memory module with no cache. However, such strong shared memories can have a signitcant negative impact on the performance of applications, and can limit the scalability of a system [5].
Reference: 19. <author> Lamport, L. </author> <title> On interprocess communication, parts I and II. </title> <booktitle> Distributed Computing 1, </booktitle> <month> 2 (Apr. </month> <year> 1986), </year> <month> 77101. </month>
Reference-contexts: When a shared memory abstraction is built on top of a message passing system, it is called Distributed Shared Memory [21] (DSM). A DSM system provides a contract to the programmer in the form of a consistency condition. Two well-known consistency conditions are linearizability <ref> [16, 19] </ref> and sequential consistency [18], which guarantee that the DSM behaves like a single physical memory module with no cache. However, such strong shared memories can have a signitcant negative impact on the performance of applications, and can limit the scalability of a system [5].
Reference: 20. <author> Lencevicius, R., and Singh, A. K. </author> <title> Latency bounds for memory consistency. </title> <note> Submitted for publication, </note> <year> 1997. </year>
Reference-contexts: In Sect. 1.2 we detne the basic concepts. We analyze linearizability in Sect. 2. In Sect. 3, we analyze sequential consistency using a general theorem that also applies to other conditions. We close with some concluding remarks. 1.1 Related Work We build on the results of <ref> [20] </ref>, which shows that neither operation can be wait-free in any implementation of linearizability over a totally asynchronous system. It also shows that a sequentially consistent implementation cannot have both the read and the write operation be wait-free. We strengthen these bounds to those shown in Table 1. <p> Due to this property, a collection of individually linearizable variables forms a linearizable shared memory. Neither the read nor the write operation can be wait-free, or (N 1)-resilient, as shown in <ref> [20] </ref>. This is proved by showing that two concurrent wait-free operations can execute with no messages received at any node, making it impossible to determine the global time ordering of the operations. We strengthen this result by showing that neither operation can be d N 2 e -resilient. <p> We strengthen this result by showing that neither operation can be d N 2 e -resilient. We do so by extending the proof of <ref> [20] </ref> to sets of b N 2 c nodes. We will refer to a set of nodes G as a group of nodes. <p> Our remaining proof obligation is to show that P 1 can complete a read operation by communicating only with the nodes in G 1 , even when W 2 G 1 . Suppose not. We know from <ref> [20] </ref> that the read operation cannot be wait-free, or (N 1)- resilient. Then it is t-resilient, with ceil (N=2) t &lt; N 1. When P 1 invokes a read operation R, there is some set of t + 1 nodes whose failure causes R to block. <p> The message bound can be improved at the expense of greater memory usage by substituting Attiya's new construction of a 1-writer multi-reader atomic reg ister [3]. 3 Sequential Consistency A DSM implementation guarantees sequential consistency if a serialization exists for every history of the system. In <ref> [20] </ref> it is shown that for any implementation of sequential consistency, the read and write operations cannot both be wait-free, or (N 1)-resilient. If both operations are wait-free, then two nodes can each complete a read and a write without receiving any messages from the other. <p> If both operations are wait-free, then two nodes can each complete a read and a write without receiving any messages from the other. This fact is used to show that such an implementation cannot avoid non-sequential behavior. We strengthen that result by extending the proof of <ref> [20] </ref> to sets of b N 2 c nodes. We show that one operation can be no more than ( d N 2 e 1)-resilient, but that the other operation can be wait-free. In the full paper, we show that processor consistency has the same fault tolerance bounds.
Reference: 21. <author> Li, K., and Hudak, P. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Trans. Comput. Syst. </journal> <volume> 7, </volume> <month> 4 (Nov. </month> <year> 1989), </year> <month> 32159. </month>
Reference-contexts: The shared memory paradigm is attractive since it hides the details of such management, making both local and remote data available through read and write operations. When a shared memory abstraction is built on top of a message passing system, it is called Distributed Shared Memory <ref> [21] </ref> (DSM). A DSM system provides a contract to the programmer in the form of a consistency condition. Two well-known consistency conditions are linearizability [16, 19] and sequential consistency [18], which guarantee that the DSM behaves like a single physical memory module with no cache.
Reference: 22. <author> Li, M., Tromp, J., and Vitnyi, P. M. B. </author> <title> How to share concurrent wait-free variables. </title> <journal> J. ACM 43, </journal> <month> 4 (July </month> <year> 1996), </year> <month> 72346. </month>
Reference-contexts: Proof. Attiya et al. [4] give a construction for a 1-writer multi-reader atomic register on a totally asynchronous system. This register has the property that both the read and the write operation are ( d N 2 e 1)-resilient. Li et al. <ref> [22] </ref> give a construction of a multi-writer multi-reader atomic register from 1-writer multi-reader atomic registers 1 . <p> Since linearizability is a local property, a collection of such registers constitutes a linearizable shared memory. ut The constructions in both [4] and <ref> [22] </ref> are bounded. A straightforward composition requires O (N 4 ) space on each node; each read or write operation results in the sending of O (N 3 ) messages, each of size O (N 3 ), in O (N ) messaging rounds.
Reference: 23. <author> Lipton, R. J., and Sandberg, J. S. </author> <title> PRAM: A scalable shared memory. </title> <type> Tech. Rep. </type> <institution> CS-TR18088, Department of Computer Science, Princeton University, </institution> <month> Sept. </month> <year> 1988. </year>
Reference-contexts: However, such strong shared memories can have a signitcant negative impact on the performance of applications, and can limit the scalability of a system [5]. Weaker conditions have been detned in an attempt to ameliorate such eects, such as release consistency [12], causal memory [2], pipelined RAM <ref> [23] </ref>, and mixed consistency [1]. The capabilities and characteristics of the various memory consistency conditions has been a fruitful topic of study. Both qualitative and quantitative comparisons have been carried out.
Reference: 24. <author> Lo, W.-K. </author> <title> More on t-resilience vs. </title> <booktitle> wait-freedom. In PODC '95 (Ottawa, </booktitle> <address> Ontario, Canada, </address> <year> 2023 </year> <month> Aug. </month> <year> 1995), </year> <pages> pp. 11019. </pages>
Reference-contexts: In this paper, we investigate fault-tolerance in the form of t-resilience, the ability to withstand t node failures. The notion of t-resilience has been detned in terms of consensus protocols [10], asynchronous tasks [6, 15], and shared objects <ref> [7, 24] </ref>. We say that an operation is t-resilient if it can complete (i.e., not block) in spite of t node failures.
Reference: 25. <author> Malki, D., Birman, K., Ricciardi, A., and Schiper, A. </author> <title> Uniform actions in asynchronous distributed systems. </title> <address> In PODC '94 (Los Angeles, CA, USA, 14 17 Aug. </address> <year> 1994), </year> <pages> pp. 27483. </pages>
Reference-contexts: It also shows that a sequentially consistent implementation cannot have both the read and the write operation be wait-free. We strengthen these bounds to those shown in Table 1. Malki et al. <ref> [25] </ref> study the Dynamic Uniformity problem. Whenever any node in a particular group takes an action from some set A, all other nodes in the group must eventually take that same action, even if the trst node subsequently fails. Nodes can leave and join the group during the computation.
Reference: 26. <author> Mavronicolas, M., and Roth, D. Ecient, </author> <title> strongly consistent implementations of shared memory (extended abstract). </title> <booktitle> In WDAG '92 (Haifa, </booktitle> <address> Israel, 24 Nov. </address> <year> 1992), </year> <editor> A. Segall and S. Zaks, Eds., </editor> <volume> vol. </volume> <booktitle> 647 of Lecture Notes in Computer Science, </booktitle> <publisher> Springer-Verlag, </publisher> <pages> pp. 34661. </pages>
Reference-contexts: Some of the problems we consider are similar, but there is no dynamic group membership. We show that the sequential consistency and linearizability problems, which impose a serial ordering constraint, have ( d N 2 e 1)-resilient solutions. Mavronicolas and Roth <ref> [26] </ref> give two implementations of linearizable shared memory for a partially synchronous system. In such a system, all message latencies are in the range [d u; d] for known constants d (delay) and u (uncertainty), 0 u &lt; d.
References-found: 26

