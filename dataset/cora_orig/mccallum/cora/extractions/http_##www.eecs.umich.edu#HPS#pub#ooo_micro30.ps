URL: http://www.eecs.umich.edu/HPS/pub/ooo_micro30.ps
Refering-URL: http://www.eecs.umich.edu/HPS/hps_micro.html
Root-URL: http://www.cs.umich.edu
Title: Reducing the Performance Impact of Instruction Cache Misses by Writing Instructions into the Reservation Stations Out-of-Order  
Abstract: Copyright 1997 IEEE. Published in the Proceedings of Micro-30, December 1-3, 1997 in Research Triangle Park, North Carolina. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal and S. D. Pudar, </author> <title> "A technique for reducing the miss rate of direct-mapped caches," </title> <booktitle> in Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 179-190, </pages> <year> 1993. </year>
Reference-contexts: Stream buffers [10] are used to prefetch a large number of consecutive cache lines without polluting the cache. They are used to eliminate some capacity and compulsory misses. With code re-ordering [13, 8], the compiler re-arranges the code to minimize misses. Set prediction <ref> [20, 1, 12] </ref> is used to build caches with direct-mapped access times and set-associative hit rates. The second area contains research on multi-threading, multiprocessors, and Multiscalar processors. With multithreading [21], the processor switches to another thread upon encountering an instruction cache miss.
Reference: [2] <author> T. M. Austin and G. S. Sohi, </author> <title> "Dynamic dependency analysis of ordinary programs," </title> <booktitle> in Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 342-351, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction Significant parallelism exists within a single instruction stream <ref> [11, 4, 2] </ref>. To achieve high performance, today's processors are being built to take advantage of some of this instruction level parallelism.
Reference: [3] <author> M. Butler and Y. Patt, </author> <title> "An area-efficient register alias table for implementing HPS," </title> <booktitle> in Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <pages> pp. 611-612, </pages> <year> 1990. </year>
Reference-contexts: The hole is created as the fetch blocks are being skipped over. The hole is filled in as the cache misses are serviced and the missing instructions become available for issue. To create the hole, a checkpoint of the current architectural register file is established <ref> [3] </ref>. When the hardware fills in the hole, it will use this checkpoint to source the proper architectural register values required for the instructions being inserted into the hole.
Reference: [4] <author> M. Butler, T.-Y. Yeh, Y. Patt, M. Alsup, H. Scales, and M. Shebanow, </author> <title> "Single instruction stream parallelism is greater than two," </title> <booktitle> in Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 276-286, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction Significant parallelism exists within a single instruction stream <ref> [11, 4, 2] </ref>. To achieve high performance, today's processors are being built to take advantage of some of this instruction level parallelism. <p> These initial experiments were performed using a fairly abstract machine model. Although the model is abstract, it accounts for many practical constraints on CPU design. Our model is an extension of the RDF model of execution <ref> [4] </ref>. The RDF model of execution is characterized by three parameters: window size, issue rate, and instruction class latencies.
Reference: [5] <author> J. A. Fisher, </author> <title> "Trace scheduling: A technique for global microcode compaction," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-30, no. 7, </volume> <pages> pp. 478-490, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: For all experiments, we use a window size of 256 instructions, an issue rate of 16 instructions per cycle, and the instruction class latencies specified in Table 2. (Trace scheduling <ref> [5] </ref>, superblocks [9], block structured ISAs [15], VLIW tree instructions [17], and trace caches [16, 6, 18] are some of the methods which can be used in achieving an issue rate of 16 instructions per cycle.) The RDF model of execution, with its perfect instruction cache and omniscient branch prediction, is
Reference: [6] <author> M. Franklin and M. Smotherman, </author> <title> "A fill-unit approach to multiple instruction issue," </title> <booktitle> in Proceedings of the 27th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 162-171, </pages> <year> 1994. </year>
Reference-contexts: For all experiments, we use a window size of 256 instructions, an issue rate of 16 instructions per cycle, and the instruction class latencies specified in Table 2. (Trace scheduling [5], superblocks [9], block structured ISAs [15], VLIW tree instructions [17], and trace caches <ref> [16, 6, 18] </ref> are some of the methods which can be used in achieving an issue rate of 16 instructions per cycle.) The RDF model of execution, with its perfect instruction cache and omniscient branch prediction, is not realizable.
Reference: [7] <author> M. Franklin and G. S. Sohi, </author> <title> "The expandable split window paradigm for exploiting fine-grain parallelism," </title> <booktitle> in Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 58-67, </pages> <year> 1992. </year>
Reference-contexts: With multithreading [21], the processor switches to another thread upon encountering an instruction cache miss. The processor does not switch back to the thread that caused the miss until the instruction cache miss is serviced. In multiprocessors and Multi-scalar processors <ref> [7] </ref>, programs are broken up into a series of tasks. The tasks are distributed to processing elements. Each processing element has its own instruction cache. When one of the processing elements suffers an instruction cache miss, execution of the task on that processing element stalls.
Reference: [8] <author> W. W. Hwu and P. P. Chang, </author> <title> "Achieving high instruction cache performance with an optimizing compiler," </title> <booktitle> in Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <year> 1989. </year>
Reference-contexts: Stream buffers [10] are used to prefetch a large number of consecutive cache lines without polluting the cache. They are used to eliminate some capacity and compulsory misses. With code re-ordering <ref> [13, 8] </ref>, the compiler re-arranges the code to minimize misses. Set prediction [20, 1, 12] is used to build caches with direct-mapped access times and set-associative hit rates. The second area contains research on multi-threading, multiprocessors, and Multiscalar processors.
Reference: [9] <author> W. W. Hwu, S. A. Mahlke, W. Y. Chen, P. P. Chang, N. J. Warter, R. A. Bringmann, R. G. Ouellette, R. E. Hank, T. Kiyohara, G. E. Haab, J. G. Holm, and D. M. Lavery, </author> <title> "The superblock: An effective technique for VLIW and superscalar compilation," </title> <journal> Journal of Supercomputing, </journal> <volume> vol. 7, no. </volume> <pages> 9-50, </pages> , <year> 1993. </year>
Reference-contexts: For all experiments, we use a window size of 256 instructions, an issue rate of 16 instructions per cycle, and the instruction class latencies specified in Table 2. (Trace scheduling [5], superblocks <ref> [9] </ref>, block structured ISAs [15], VLIW tree instructions [17], and trace caches [16, 6, 18] are some of the methods which can be used in achieving an issue rate of 16 instructions per cycle.) The RDF model of execution, with its perfect instruction cache and omniscient branch prediction, is not realizable.
Reference: [10] <author> N. P. Jouppi, </author> <title> "Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers," </title> <booktitle> in Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 364-373, </pages> <year> 1990. </year>
Reference-contexts: With prefetching [19], the hardware or the compiler anticipates that data is about to be accessed which is not in the cache. The hardware attempts to fetch this data before the access occurs. As presented in <ref> [10] </ref>, victim caches are small fully associative caches that are used in conjunction with a direct mapped cache to reduce the number of conflict misses. Stream buffers [10] are used to prefetch a large number of consecutive cache lines without polluting the cache. <p> The hardware attempts to fetch this data before the access occurs. As presented in <ref> [10] </ref>, victim caches are small fully associative caches that are used in conjunction with a direct mapped cache to reduce the number of conflict misses. Stream buffers [10] are used to prefetch a large number of consecutive cache lines without polluting the cache. They are used to eliminate some capacity and compulsory misses. With code re-ordering [13, 8], the compiler re-arranges the code to minimize misses.
Reference: [11] <author> N. P. Jouppi and D. W. Wall, </author> <title> "Available instruction-level parallelism for superscalar and superpipelined machines," </title> <booktitle> in Proceedings of the 2th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 272-282, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction Significant parallelism exists within a single instruction stream <ref> [11, 4, 2] </ref>. To achieve high performance, today's processors are being built to take advantage of some of this instruction level parallelism.
Reference: [12] <author> T. Juan, T. Lang, and J. J. Navarro, </author> <title> "The difference-bit cache," </title> <booktitle> in Proceedings of the 23st Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 114-120, </pages> <year> 1996. </year>
Reference-contexts: Stream buffers [10] are used to prefetch a large number of consecutive cache lines without polluting the cache. They are used to eliminate some capacity and compulsory misses. With code re-ordering [13, 8], the compiler re-arranges the code to minimize misses. Set prediction <ref> [20, 1, 12] </ref> is used to build caches with direct-mapped access times and set-associative hit rates. The second area contains research on multi-threading, multiprocessors, and Multiscalar processors. With multithreading [21], the processor switches to another thread upon encountering an instruction cache miss.
Reference: [13] <author> S. McFarling, </author> <title> "Program optimization for instruction caches," </title> <booktitle> in Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 183-191, </pages> <year> 1989. </year>
Reference-contexts: Stream buffers [10] are used to prefetch a large number of consecutive cache lines without polluting the cache. They are used to eliminate some capacity and compulsory misses. With code re-ordering <ref> [13, 8] </ref>, the compiler re-arranges the code to minimize misses. Set prediction [20, 1, 12] is used to build caches with direct-mapped access times and set-associative hit rates. The second area contains research on multi-threading, multiprocessors, and Multiscalar processors.
Reference: [14] <author> S. McFarling, </author> <title> "Combining branch predictors," </title> <type> Technical Report TN-36, </type> <institution> Digital Western Research Laboratory, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: For all experiments, the branch predictor is a modified Two-Level Global Adaptive Branch Predictor (GAg [22]) scheme (gshare <ref> [14] </ref>) which exclusive-ORs a global history with the fetch address to select the appropriate pattern history table entry. We use a 16-bit global history. Subroutine returns are predicted using a 64 entry Return Address Stack. The targets of indirect jumps are predicted using a simple last-time scheme.
Reference: [15] <author> S. Melvin and Y. Patt, </author> <title> "Enhancing instruction scheduling with a block-structured ISA," </title> <booktitle> International Journal on Parallel Processing, </booktitle> <year> 1994. </year>
Reference-contexts: For all experiments, we use a window size of 256 instructions, an issue rate of 16 instructions per cycle, and the instruction class latencies specified in Table 2. (Trace scheduling [5], superblocks [9], block structured ISAs <ref> [15] </ref>, VLIW tree instructions [17], and trace caches [16, 6, 18] are some of the methods which can be used in achieving an issue rate of 16 instructions per cycle.) The RDF model of execution, with its perfect instruction cache and omniscient branch prediction, is not realizable.
Reference: [16] <author> S. W. Melvin and Y. N. Patt, </author> <title> "Performance benefits of large execution atomic units in dynamically scheduled machines," </title> <booktitle> in Proceedings of Supercomputing '89, </booktitle> <pages> pp. 427-432, </pages> <year> 1989. </year>
Reference-contexts: For all experiments, we use a window size of 256 instructions, an issue rate of 16 instructions per cycle, and the instruction class latencies specified in Table 2. (Trace scheduling [5], superblocks [9], block structured ISAs [15], VLIW tree instructions [17], and trace caches <ref> [16, 6, 18] </ref> are some of the methods which can be used in achieving an issue rate of 16 instructions per cycle.) The RDF model of execution, with its perfect instruction cache and omniscient branch prediction, is not realizable.
Reference: [17] <author> S.-M. Moon and K. Ebcioglu, </author> <title> "An efficient resource-constrained global scheduling technique for superscalar and VLIW processors," </title> <booktitle> in Proceedings of the 25th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 55-71, </pages> <year> 1992. </year>
Reference-contexts: For all experiments, we use a window size of 256 instructions, an issue rate of 16 instructions per cycle, and the instruction class latencies specified in Table 2. (Trace scheduling [5], superblocks [9], block structured ISAs [15], VLIW tree instructions <ref> [17] </ref>, and trace caches [16, 6, 18] are some of the methods which can be used in achieving an issue rate of 16 instructions per cycle.) The RDF model of execution, with its perfect instruction cache and omniscient branch prediction, is not realizable.
Reference: [18] <author> E. Rotenberg, S. Bennett, and J. E. Smith, </author> <title> "Trace cache: a low latency approach to high bandwidth instruction fetching," </title> <booktitle> in Proceedings of the 29th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <year> 1996. </year>
Reference-contexts: For all experiments, we use a window size of 256 instructions, an issue rate of 16 instructions per cycle, and the instruction class latencies specified in Table 2. (Trace scheduling [5], superblocks [9], block structured ISAs [15], VLIW tree instructions [17], and trace caches <ref> [16, 6, 18] </ref> are some of the methods which can be used in achieving an issue rate of 16 instructions per cycle.) The RDF model of execution, with its perfect instruction cache and omniscient branch prediction, is not realizable.
Reference: [19] <author> A. J. Smith, </author> <title> "Cache memories," </title> <journal> ACM Computing Surveys, </journal> <volume> vol. 14, no. 3, </volume> , <month> September </month> <year> 1982. </year>
Reference-contexts: The second area contains all the research that looks at reducing the impact of instruction cache misses. Out-of-order issue falls into the second area. The first area contains research on prefetching, victim caches, stream buffers, code re-ordering, and set prediction. With prefetching <ref> [19] </ref>, the hardware or the compiler anticipates that data is about to be accessed which is not in the cache. The hardware attempts to fetch this data before the access occurs.
Reference: [20] <author> K. So and R. N. Rechtschaffen, </author> <title> "Cache operations by MRU change," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 37, no. 6, </volume> <pages> pp. 700-709, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Stream buffers [10] are used to prefetch a large number of consecutive cache lines without polluting the cache. They are used to eliminate some capacity and compulsory misses. With code re-ordering [13, 8], the compiler re-arranges the code to minimize misses. Set prediction <ref> [20, 1, 12] </ref> is used to build caches with direct-mapped access times and set-associative hit rates. The second area contains research on multi-threading, multiprocessors, and Multiscalar processors. With multithreading [21], the processor switches to another thread upon encountering an instruction cache miss.
Reference: [21] <author> D. M. Tullsen, S. J. Eggers, J. S. Emer, and H. M. Levy, </author> <title> "Exploiting choice: Instruction fetch and issue on an implementable simultaneous multi-threading processor," </title> <booktitle> in Proceedings of the 23st Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 191-202, </pages> <year> 1996. </year>
Reference-contexts: With code re-ordering [13, 8], the compiler re-arranges the code to minimize misses. Set prediction [20, 1, 12] is used to build caches with direct-mapped access times and set-associative hit rates. The second area contains research on multi-threading, multiprocessors, and Multiscalar processors. With multithreading <ref> [21] </ref>, the processor switches to another thread upon encountering an instruction cache miss. The processor does not switch back to the thread that caused the miss until the instruction cache miss is serviced. In multiprocessors and Multi-scalar processors [7], programs are broken up into a series of tasks.
Reference: [22] <author> T.-Y. Yeh and Y. N. Patt, </author> <title> "Alternative implementations of two-level adaptive branch prediction," </title> <booktitle> in Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 124-134, </pages> <year> 1992. </year>
Reference-contexts: For all experiments, the branch predictor is a modified Two-Level Global Adaptive Branch Predictor (GAg <ref> [22] </ref>) scheme (gshare [14]) which exclusive-ORs a global history with the fetch address to select the appropriate pattern history table entry. We use a 16-bit global history. Subroutine returns are predicted using a 64 entry Return Address Stack. The targets of indirect jumps are predicted using a simple last-time scheme.
References-found: 22

