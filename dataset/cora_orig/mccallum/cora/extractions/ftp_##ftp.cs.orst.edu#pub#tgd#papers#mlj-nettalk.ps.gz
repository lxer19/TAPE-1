URL: ftp://ftp.cs.orst.edu/pub/tgd/papers/mlj-nettalk.ps.gz
Refering-URL: http://www.cs.orst.edu/~tgd/cv/pubs.html
Root-URL: 
Email: tgd@cs.orst.edu  hhild@i13d1.ira.uka.de  
Title: A Comparison of ID3 and Backpropagation for English Text-to-Speech Mapping  
Author: THOMAS G. DIETTERICH HERMANN HILD GHULUM BAKIRI Editor: Dennis Kibler 
Keyword: ID3, backpropagation, experimental comparisons, text-to-speech  
Address: 303 Dearborn Hall,  Corvallis, OR 97331-3202  
Affiliation: Department of Computer Science,  Oregon State University,  
Note: Small Journal Name, 1-40 c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Abstract: The performance of the error backpropagation (BP) and ID3 learning algorithms was compared on the task of mapping English text to phonemes and stresses. Under the distributed output code developed by Sejnowski and Rosenberg, it is shown that BP consistently out-performs ID3 on this task by several percentage points. Three hypotheses explaining this difference were explored: (a) ID3 is overfitting the training data, (b) BP is able to share hidden units across several output units and hence can learn the output units better, and (c) BP captures statistical information that ID3 does not. We conclude that only hypothesis (c) is correct. By augmenting ID3 with a simple statistical learning procedure, the performance of BP can be closely matched. More complex statistical procedures can improve the performance of both BP and ID3 substantially in this domain. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bakiri, G. </author> <year> (1991). </year> <title> Converting English text to speech: A machine learning approach. </title> <type> Doctoral Dissertation (Technical Report 91-30-1). </type> <institution> Corvallis, OR: Oregon State University, Department of Computer Science. </institution>
Reference-contexts: Discussion 6.1. Improving These Algorithms There are many directions that can be explored for improving these algorithms. We have pursued several of these directions in order to develop a high-performance text-to-speech system. Our efforts are reported in detail elsewhere <ref> (Bakiri, 1991) </ref>. One approach is to design better output codes for phoneme/stress pairs. Our experiments have shown that BCH error correcting codes provide better output codes than the output code used in this paper. Randomly-generated bit-strings produce similar performance improvements (see Dietterich & Bakiri, 1991). <p> Our efforts are reported in detail elsewhere (Bakiri, 1991). One approach is to design better output codes for phoneme/stress pairs. Our experiments have shown that BCH error correcting codes provide better output codes than the output code used in this paper. Randomly-generated bit-strings produce similar performance improvements <ref> (see Dietterich & Bakiri, 1991) </ref>. Another approach is to widen the seven-letter window and introduce context. Lucassen and Mercer (1984) employ a 9-letter window. They also include as inputs the phonemes and stresses of the four letters to the left of the letter at the center of the window.
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. </author> <year> (1984). </year> <title> Classification and regression trees. </title> <address> Monterey, CA: </address> <publisher> Wadsworth and Brooks. </publisher>
Reference: <author> Buntine, W. </author> <year> (1990). </year> <title> A theory of learning classification rules. </title> <type> Doctoral dissertation. </type> <institution> School of Computing Science, University of Technology, </institution> <address> Sydney, Australia. </address>
Reference: <author> Dietterich, T. G. </author> <year> (1989). </year> <title> Limits of inductive learning. </title> <booktitle> In Proceedings of the Sixth International Conference on Machine Learning (pp. </booktitle> <pages> 124-128). </pages> <address> Ithaca, NY. San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: 1. Introduction There is no universal learning algorithm that can take a sample S = fhx i ; f (x i )ig of training examples for an arbitrary unknown function f and produce a good approximation to f <ref> (see Dietterich, 1989) </ref>. Instead, every learning algorithm embodies some assumptions (or "bias") about the nature of the learning problems to which it will be applied. Some algorithms, for example, assume that only a small number of the features describing the data are relevant.
Reference: <author> Dietterich, T. G., & Bakiri, G. </author> <year> (1991). </year> <title> Error-correcting output codes: A general method for improving multiclass inductive learning programs. </title> <booktitle> Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <address> Anaheim, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Our efforts are reported in detail elsewhere (Bakiri, 1991). One approach is to design better output codes for phoneme/stress pairs. Our experiments have shown that BCH error correcting codes provide better output codes than the output code used in this paper. Randomly-generated bit-strings produce similar performance improvements <ref> (see Dietterich & Bakiri, 1991) </ref>. Another approach is to widen the seven-letter window and introduce context. Lucassen and Mercer (1984) employ a 9-letter window. They also include as inputs the phonemes and stresses of the four letters to the left of the letter at the center of the window.
Reference: <author> Dietterich, T. G., Hild, H., & Bakiri, G. </author> <year> (1990). </year> <title> A comparative study of ID3 and backpropagation for English text-to-speech mapping. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning (pp. </booktitle> <pages> 24-31). </pages> <address> Austin, TX: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Klatt, D. </author> <year> (1987). </year> <title> Review of text-to-speech conversion for English. </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 82, </volume> <pages> 737-793. </pages>
Reference-contexts: We have been unable to test a similar configuration with BPCV because of the huge computational resources that would be required. Bakiri (1991) describes a study in which human judges compared the output of this system to the output of the DECtalk <ref> (Klatt, 1987) </ref> letter-to-sound rule base. The results show that this system (and two other machine learning approaches) significantly out-perform DECtalk. 6.2. Applying ID3 to Aid BPCV An interesting observation from this and other studies is that the performance of ID3 and BPCV is highly correlated.
Reference: <author> Lucassen, J. M., & Mercer, R. L. </author> <year> (1984). </year> <title> An information theoretic approach to the automatic determination of phonemic base forms. </title> <booktitle> Proc. Int. Conf. Acoust. Speech Signal Process. </booktitle> <address> ICASSP-84, 42.5.1-42.5.4. </address>
Reference: <author> Lang, K. J., Waibel, A. H., & Hinton, G. E. </author> <year> (1990). </year> <title> A time-delay neural network architecture for isolated word recognition. </title> <booktitle> Neural Networks, </booktitle> <volume> 3, </volume> <pages> 33-43. </pages> <note> ID3 AND BACKPROPAGATION 29 Martin, </note> <author> G. L., & Pittman, J. A. </author> <year> (1990). </year> <title> Recognizing hand-printed letters and digits. </title>
Reference: <editor> In D. Touretzky (Ed.), </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> 405-414. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> McClelland, J. L., & Rumelhart, D. E. </author> <year> (1988). </year> <title> Explorations in parallel distributed processing, </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Mingers, J. </author> <year> (1989). </year> <title> An empirical comparison of pruning methods for decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 4, </volume> <pages> 227-243. </pages>
Reference: <author> Mooney, R., Shavlik, J., Towell, G., & Gove, A. </author> <year> (1989). </year> <title> An experimental com-parison of symbolic and connectionist learning algorithms. </title> <booktitle> IJCAI-89: Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> (pp. 775-80). </pages>
Reference-contexts: The first row repeats the basic ID3 results given above, for comparison purposes. The second row shows the effect of applying a 2 test (at the .90 confidence level) to decide whether further growth of the decision tree is statistically justified (Quin- lan, 1986a). As other authors have reported <ref> (Mooney et al., 1989) </ref>, this hurts performance in the NETtalk domain. The third row shows the effect of applying Quinlan's technique of reduce-error pruning (Quinlan, 1987). Mingers (1989) ID3 AND BACKPROPAGATION 13 provides evidence that this is one of the best pruning techniques.
Reference: <author> Quinlan, J. R. </author> <year> (1983). </year> <title> Learning efficient classification procedures and their appli-cation to chess endgames. </title> <editor> In R. S. Michalski, J. Carbonell, & T. M. Mitchell, (eds.), </editor> <booktitle> Machine learning: An artificial intelligence approach, 1, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The algorithm has been extended to handle features with more than 2 values and features with continuous values as well. In our implementation of ID3, we did not employ windowing <ref> (Quinlan, 1983) </ref>, CHI-square forward pruning (Quinlan, 1986a), or any kind of reverse pruning (Quin- lan, 1987).
Reference: <author> Quinlan, J. R. </author> <year> (1986a). </year> <title> The effect of noise on concept learning. </title> <editor> In R. S. Michalski, J. Carbonell, & T. M. Mitchell, (eds.), </editor> <booktitle> Machine learning: An artificial intelligence approach, 1, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The algorithm has been extended to handle features with more than 2 values and features with continuous values as well. In our implementation of ID3, we did not employ windowing (Quinlan, 1983), CHI-square forward pruning <ref> (Quinlan, 1986a) </ref>, or any kind of reverse pruning (Quin- lan, 1987).
Reference: <author> Quinlan, J. R. </author> <year> (1986b). </year> <title> Induction of decision trees, </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1987). </year> <title> Simplifying decision trees. </title> <journal> International Journal of ManMachine Studies, </journal> <volume> 27, </volume> <pages> 221-234. </pages>
Reference-contexts: As other authors have reported (Mooney et al., 1989), this hurts performance in the NETtalk domain. The third row shows the effect of applying Quinlan's technique of reduce-error pruning <ref> (Quinlan, 1987) </ref>. Mingers (1989) ID3 AND BACKPROPAGATION 13 provides evidence that this is one of the best pruning techniques. For this row, a decision tree was built using the 800-word S tr set and then pruned using the S cv cross-validation set.
Reference: <author> Rumelhart, D. E., Hinton, G. E., & Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart, & J. L. McClelland (Eds.), </editor> <booktitle> Parallel distributed processing, </booktitle> <volume> (Vol 1). </volume> <publisher> Cambridge, </publisher> <address> MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: The result is that we do not have a good understanding of the range of problems for which ID3 is appropriate. Similarly, the backpropagation algorithm <ref> (Rumelhart, Hinton, & Williams, 1986) </ref> assumes, at a minimum, that the unknown function f can be represented as a multilayer feed-forward network of sigmoid units. Although there have been many successful applications of backpropagation (Touretzky, 1989, 1990), we still lack an understanding of the situations for which it is appropriate. <p> To apply ID3 to this task, the algorithm must be executed 26 times|once for each mapping h i . Each of these executions produces a separate decision tree. 3.2. Backpropagation The error backpropagation method <ref> (Rumelhart, Hinton, & Williams, 1986) </ref> is widely applied to train artificial neural networks. However, in its standard form, the algorithm requires substantial assistance from the user.
Reference: <author> Rosenberg, C. R. </author> <year> (1988). </year> <title> Learning the connection between spelling and sound: A network model of oral reading. </title> <type> Doctoral Dissertation. (CSL Report 18). </type> <institution> Princeton, NJ: Princeton University, Cognitive Science Laboratory. </institution>
Reference: <author> Sejnowski, T. J., & Rosenberg, C. R. </author> <year> (1987). </year> <title> Parallel networks that learn to pro-nounce English text. </title> <journal> Complex Systems, </journal> <volume> 1, </volume> <pages> 145-168. </pages>
Reference: <editor> Touretzky, D. S. (Ed.) </editor> <booktitle> (1989). Advances in neural information processing systems 1. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Similarly, the backpropagation algorithm (Rumelhart, Hinton, & Williams, 1986) assumes, at a minimum, that the unknown function f can be represented as a multilayer feed-forward network of sigmoid units. Although there have been many successful applications of backpropagation <ref> (Touretzky, 1989, 1990) </ref>, we still lack an understanding of the situations for which it is appropriate. Furthermore, because clear statements of the assumptions made by ID3 and backpropagation are unavailable, we do not understand the relationship between these two algorithms.
Reference: <editor> Touretzky, D. S. (Ed.) </editor> <booktitle> (1990). Advances in neural information processing systems 2. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. 30 DIETTERICH, HILD & BAKIRI </publisher>
References-found: 22

