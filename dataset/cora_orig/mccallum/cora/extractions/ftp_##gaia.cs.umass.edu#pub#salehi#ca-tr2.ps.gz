URL: ftp://gaia.cs.umass.edu/pub/salehi/ca-tr2.ps.gz
Refering-URL: http://www.cs.umass.edu/~salehi/home.html
Root-URL: 
Author: James D. Salehi James F. Kurose Don Towsley 
Note: This work was supported by NSF under grant NCR-9206908 and by ARPA under ESD/AVS contract F-19628-92-C-0089. The authors can be contacted at [salehi,kurose,towsley]@cs.umass.edu.  
Date: June 24, 1995  May 1995  
Address: Amherst, MA 01003  
Affiliation: Department of Computer Science University of Massachusetts  University of Massachusetts  
Pubnum: Technical Report UM-CS-1995-046,  
Abstract: Further Results in Affinity-Based Scheduling of Parallel Networking fl Abstract In this paper, we present further results in processor-cache affinity scheduling of parallel network protocol processing, in a setting in which protocol processing executes on the multiprocessor host concurrently with a general workload of non-protocol activity. In earlier work [31, 32] we evaluated affinity-based scheduling of receive-side protocol processing under two parallelization approaches: Locking and Independent Protocol Stacks (IPS). In this work, we i) evaluate affinity-based scheduling of send-side UDP/IP/FDDI processing, ii) examine the performance of affinity-based scheduling as a function of stream burstiness and source locality, iii) explore under IPS the impact of varying the number of independent protocol stacks, and iv) incorporate into our results the overhead of copying uncached packet data. We obtain our results following the research methodology developed in our earlier work, extending the developed infrastructure as necessary. Our results show that affinity-based scheduling performs well for send-side processing, and is resilient to stream burstiness and source localitytwo well-known properties of network traffic. We find that the performance of IPS improves dramatically when the number of stacks increases with the number of admitted streams, due to lower packet processing times and improved packet queueing behavior. Finally, we show that the benefit of affinity-based scheduling remains significant when processing includes the overhead of copying uncached packet data, in spite of the increased packet delay. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Mark B. Abbott and Larry L. Peterson. </author> <title> Increasing network throughput by integrating protocol layers. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 1(5) </volume> <pages> 600-610, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: Although UDP supports checksumming we have not incorporated this overhead into our results. In fact, there are several techniques for reducing or avoiding the overhead of checksumming in UDP (and TCP) [18]: * Checksumming can be combined with data copying in integrated layer processing (ILP) <ref> [5, 1] </ref>.
Reference: [2] <author> Thomas E. Anderson, Edward D. Lazowska, and Henry M. Levy. </author> <title> The performance implications of thread management alternatives for shared-memory multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12) </volume> <pages> 1631-1644, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Table 1 summarizes the affinity-based scheduling objectives, the resources managed, and the scheduling policies we consider. 4 Note that while the load-balancing and synchronization benefits of per-processor thread pools has been explored <ref> [2] </ref>, the cache affinity benefits of such organization have not previously been evaluated. 5 For simplicity, we assume that each free-memory pool is implemented as an array of pointers to segments of available memory.
Reference: [3] <author> Mats Bjorkman and Per Gunningberg. </author> <title> Locking effects in multiprocessor implementations of protocols. </title> <booktitle> In Proceedings of the ACM SIGCOMM Conference on Communications, Architectures, Protocols and Applications, </booktitle> <pages> pages 74-83, </pages> <address> San Francisco, CA, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: We study affinity-based scheduling of parallel network protocol processing, which has recently become an area of active research <ref> [3, 8, 12, 14, 17, 20, 24, 34, 35, 36, 41] </ref> and significant applied/commercial interest [8, 12, 29, 34]. The use of parallelism in protocol processing is motivated by the development of high-speed networks (such as ATM) capable of delivering gigabit-range bandwidth to individual machines. <p> In layer parallelism, packets visit multiple processors in a pipelined fashion [9, 30, 35]. Packet-level <ref> [3, 8, 12, 14, 15, 24, 29, 34, 35, 36] </ref> and connection-level [8, 12, 29, 34, 36] parallelisms enable concurrency at higher levels of granularity. 1 We present the following results. * We study affinity-based scheduling of send-side UDP/IP/FDDI protocol processing (Section 6).
Reference: [4] <author> Torsten Braun and Claudia Schmidt. </author> <title> Implementation of a parallel transport subsystem on a multiprocessor architecture. </title> <booktitle> In Second International Symposium on High-Performance Distributed Computing, </booktitle> <address> Spokane, Washington, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: problem in parallel networking involves not only the concurrent management of protocol threads and available processors, but also of packets and the memory resources accessed during packet processing. 1 Depending on the coherence state of the referenced cache line. 2 In functional parallelism, an individual packet concurrently visits multiple processors <ref> [4, 17, 20] </ref>. In layer parallelism, packets visit multiple processors in a pipelined fashion [9, 30, 35].
Reference: [5] <author> D. D. Clark and D. L. Tennenhouse. </author> <title> Architectural considerations for a new generation of protocols. </title> <booktitle> In Proceedings of the ACM SIGCOMM Conference on Communications, Architectures, Protocols and Applications, </booktitle> <pages> pages 200-208, </pages> <address> Philadelphia, PA, </address> <month> September </month> <year> 1990. </year> <note> ACM. </note>
Reference-contexts: Although UDP supports checksumming we have not incorporated this overhead into our results. In fact, there are several techniques for reducing or avoiding the overhead of checksumming in UDP (and TCP) [18]: * Checksumming can be combined with data copying in integrated layer processing (ILP) <ref> [5, 1] </ref>.
Reference: [6] <author> Peter B. Danzig, Sugih Jamin, Ramon Caceres, Danny J. Mitzel, and Deborah Estrin. </author> <title> An empirical workload model for driving wide-area TCP/IP network simulations. </title> <journal> Journal of Internetworking: Research and Experience, </journal> <volume> 3(1) </volume> <pages> 1-26, </pages> <year> 1992. </year>
Reference-contexts: Although copying and checksumming (which scale with packet size) are expensive, the reason the non-data touching overheads predominate is that typically in real environments most packets are small <ref> [6, 11, 18, 23] </ref> 10 . 10 Some common applications do not touch the data at all. For example, the SGI implementation of the NFS server takes advantage of the fact that most SGI network interfaces support checksumming in firmware [25]. <p> Kay and Pasquale establish that the trace is representative by comparing the empirical distribution with that of LAN traffic in another departmental environment, finding very similar results. In addition, they note that observed packet size distributions match those of Ethernet-based packet traces reported in <ref> [6, 11, 23] </ref>. The packet size distribution, taken from Figure 5b of [18], is shown in Table 2. In formulating this distribution, we have assumed that each 8KB UDP packet is sent as two 4KB FDDI frames. <p> While the Poisson model in [32] was a reasonable starting point, it is well-known that network traffic is generally not Poisson <ref> [6, 11, 16, 21, 27, 28] </ref>. The Poisson process is not very bursty (its coefficient of variation, an informal measure of burstiness, is just 1), nor does the independent Poisson model capture the related property of source locality, a well-known aspect of network traffic [16, 23].
Reference: [7] <author> Murthy Devarakonda and Arup Mukherjee. </author> <title> Issues in implementation of cache-affinity scheduling. </title> <booktitle> In Proceedings of the Winter 1992 USENIX Conference, </booktitle> <pages> pages 345-357, </pages> <address> San Francicso, CA, </address> <month> January </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Processor-cache affinity scheduling is of growing interest as processor speeds continue to increase faster than memory speeds <ref> [7, 10, 22, 32, 38, 40] </ref>. On modern shared-memory machines, the time to access an uncached memory location is typically much larger than when accessing one cached locally. <p> Distributed applications, on the other hand, require very low latency network communication. Network parallelism in the host operating systemboth within and among connectionscan both increase the bandwidth and decrease the latency of multiprocessor communication. While previous studies have explored the benefits of affinity-based scheduling of non-network-related application processing <ref> [7, 10, 22, 40] </ref>, our work [31, 32, 33] is the first to apply the technique to operating system network processing.
Reference: [8] <author> Arun Garg. </author> <title> Parallel STREAMS: A multi-processor implementation. </title> <booktitle> In Proceedings of the Winter 1990 USENIX Conference, </booktitle> <pages> pages 163-176, </pages> <address> Washington, D.C., </address> <month> January </month> <year> 1990. </year> <month> 26 </month>
Reference-contexts: We study affinity-based scheduling of parallel network protocol processing, which has recently become an area of active research <ref> [3, 8, 12, 14, 17, 20, 24, 34, 35, 36, 41] </ref> and significant applied/commercial interest [8, 12, 29, 34]. The use of parallelism in protocol processing is motivated by the development of high-speed networks (such as ATM) capable of delivering gigabit-range bandwidth to individual machines. <p> We study affinity-based scheduling of parallel network protocol processing, which has recently become an area of active research [3, 8, 12, 14, 17, 20, 24, 34, 35, 36, 41] and significant applied/commercial interest <ref> [8, 12, 29, 34] </ref>. The use of parallelism in protocol processing is motivated by the development of high-speed networks (such as ATM) capable of delivering gigabit-range bandwidth to individual machines. Emerging large-scale server applications, such as digital multimedia information repositories, require application-level access to such high bandwidth. <p> In layer parallelism, packets visit multiple processors in a pipelined fashion [9, 30, 35]. Packet-level <ref> [3, 8, 12, 14, 15, 24, 29, 34, 35, 36] </ref> and connection-level [8, 12, 29, 34, 36] parallelisms enable concurrency at higher levels of granularity. 1 We present the following results. * We study affinity-based scheduling of send-side UDP/IP/FDDI protocol processing (Section 6). <p> In layer parallelism, packets visit multiple processors in a pipelined fashion [9, 30, 35]. Packet-level [3, 8, 12, 14, 15, 24, 29, 34, 35, 36] and connection-level <ref> [8, 12, 29, 34, 36] </ref> parallelisms enable concurrency at higher levels of granularity. 1 We present the following results. * We study affinity-based scheduling of send-side UDP/IP/FDDI protocol processing (Section 6).
Reference: [9] <author> Dario Giarrizzo, Matthias Kaiserswerth, Thomas Wicki, and Robin C. Williamson. </author> <title> High-speed parallel protocol imple mentation. </title> <booktitle> First IFIP WG6.1/WG6.4 International Workshop on Protocols for High-Speed Networks, </booktitle> <pages> pages 165-180, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: In layer parallelism, packets visit multiple processors in a pipelined fashion <ref> [9, 30, 35] </ref>. Packet-level [3, 8, 12, 14, 15, 24, 29, 34, 35, 36] and connection-level [8, 12, 29, 34, 36] parallelisms enable concurrency at higher levels of granularity. 1 We present the following results. * We study affinity-based scheduling of send-side UDP/IP/FDDI protocol processing (Section 6).
Reference: [10] <author> Anoop Gupta, Andrew Tucker, and Shigeru Urushibara. </author> <title> The impact of operating system scheduling policies and synchro nization methods on the performance of parallel applications. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 120-132, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Processor-cache affinity scheduling is of growing interest as processor speeds continue to increase faster than memory speeds <ref> [7, 10, 22, 32, 38, 40] </ref>. On modern shared-memory machines, the time to access an uncached memory location is typically much larger than when accessing one cached locally. <p> Distributed applications, on the other hand, require very low latency network communication. Network parallelism in the host operating systemboth within and among connectionscan both increase the bandwidth and decrease the latency of multiprocessor communication. While previous studies have explored the benefits of affinity-based scheduling of non-network-related application processing <ref> [7, 10, 22, 40] </ref>, our work [31, 32, 33] is the first to apply the technique to operating system network processing.
Reference: [11] <author> Riccardo Gusella. </author> <title> A measurement study of diskless workstation traffic on an Ethernet. </title> <journal> IEEE Transactions on Communi cations, </journal> <volume> 38(9) </volume> <pages> 1557-1568, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Although copying and checksumming (which scale with packet size) are expensive, the reason the non-data touching overheads predominate is that typically in real environments most packets are small <ref> [6, 11, 18, 23] </ref> 10 . 10 Some common applications do not touch the data at all. For example, the SGI implementation of the NFS server takes advantage of the fact that most SGI network interfaces support checksumming in firmware [25]. <p> Kay and Pasquale establish that the trace is representative by comparing the empirical distribution with that of LAN traffic in another departmental environment, finding very similar results. In addition, they note that observed packet size distributions match those of Ethernet-based packet traces reported in <ref> [6, 11, 23] </ref>. The packet size distribution, taken from Figure 5b of [18], is shown in Table 2. In formulating this distribution, we have assumed that each 8KB UDP packet is sent as two 4KB FDDI frames. <p> While the Poisson model in [32] was a reasonable starting point, it is well-known that network traffic is generally not Poisson <ref> [6, 11, 16, 21, 27, 28] </ref>. The Poisson process is not very bursty (its coefficient of variation, an informal measure of burstiness, is just 1), nor does the independent Poisson model capture the related property of source locality, a well-known aspect of network traffic [16, 23]. <p> Note however that back-to-back arrivals (which the Compound Poisson essentially models) occur rarely in real networks <ref> [11, 16] </ref>. 7.2 Send-side processing increases in stream burstiness. Note that although the delay under stream affinity scheduling rises dramaticallyin fact, the curve is off the scale in Figure 12cit is of interest merely for informal comparison with the receive-side results.
Reference: [12] <author> Ian Heavens. </author> <title> Experiences in parallelisation of streams-based communications drivers. </title> <booktitle> OpenForum Conference on Distributed Systems, </booktitle> <month> November </month> <year> 1992. </year>
Reference-contexts: We study affinity-based scheduling of parallel network protocol processing, which has recently become an area of active research <ref> [3, 8, 12, 14, 17, 20, 24, 34, 35, 36, 41] </ref> and significant applied/commercial interest [8, 12, 29, 34]. The use of parallelism in protocol processing is motivated by the development of high-speed networks (such as ATM) capable of delivering gigabit-range bandwidth to individual machines. <p> We study affinity-based scheduling of parallel network protocol processing, which has recently become an area of active research [3, 8, 12, 14, 17, 20, 24, 34, 35, 36, 41] and significant applied/commercial interest <ref> [8, 12, 29, 34] </ref>. The use of parallelism in protocol processing is motivated by the development of high-speed networks (such as ATM) capable of delivering gigabit-range bandwidth to individual machines. Emerging large-scale server applications, such as digital multimedia information repositories, require application-level access to such high bandwidth. <p> In layer parallelism, packets visit multiple processors in a pipelined fashion [9, 30, 35]. Packet-level <ref> [3, 8, 12, 14, 15, 24, 29, 34, 35, 36] </ref> and connection-level [8, 12, 29, 34, 36] parallelisms enable concurrency at higher levels of granularity. 1 We present the following results. * We study affinity-based scheduling of send-side UDP/IP/FDDI protocol processing (Section 6). <p> In layer parallelism, packets visit multiple processors in a pipelined fashion [9, 30, 35]. Packet-level [3, 8, 12, 14, 15, 24, 29, 34, 35, 36] and connection-level <ref> [8, 12, 29, 34, 36] </ref> parallelisms enable concurrency at higher levels of granularity. 1 We present the following results. * We study affinity-based scheduling of send-side UDP/IP/FDDI protocol processing (Section 6).
Reference: [13] <author> Norman C. Hutchinson and Larry L. Peterson. </author> <title> The x-Kernel: An architecture for implementing network protocols. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(1) </volume> <pages> 64-76, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: While previous studies have explored the benefits of affinity-based scheduling of non-network-related application processing [7, 10, 22, 40], our work [31, 32, 33] is the first to apply the technique to operating system network processing. In our experimental environment (consisting of a parallelized x-kernel <ref> [13, 26] </ref> running in user-space on an 8-processor MIPS R4400-based SGI Challenge XL), packet execution times can vary by as much as a factor of four, depending on the state of the processor cache. This suggests that affinity-based scheduling presents a promising research opportunity in parallel networking.
Reference: [14] <author> Mabo Ito, Len Takeuchi, and Gerald Neufeld. </author> <title> A multiprocessing approach for meeting the processing requirements for OSI. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 11(2) </volume> <pages> 220-227, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: We study affinity-based scheduling of parallel network protocol processing, which has recently become an area of active research <ref> [3, 8, 12, 14, 17, 20, 24, 34, 35, 36, 41] </ref> and significant applied/commercial interest [8, 12, 29, 34]. The use of parallelism in protocol processing is motivated by the development of high-speed networks (such as ATM) capable of delivering gigabit-range bandwidth to individual machines. <p> In layer parallelism, packets visit multiple processors in a pipelined fashion [9, 30, 35]. Packet-level <ref> [3, 8, 12, 14, 15, 24, 29, 34, 35, 36] </ref> and connection-level [8, 12, 29, 34, 36] parallelisms enable concurrency at higher levels of granularity. 1 We present the following results. * We study affinity-based scheduling of send-side UDP/IP/FDDI protocol processing (Section 6).
Reference: [15] <author> Niraj Jain, Mischa Schwartz, and Theordore R. Bashkow. </author> <title> Transport protocol processing at Gbps rates. </title> <booktitle> In Proceedings of the ACM SIGCOMM Conference on Communications, Architectures, Protocols and Applications, </booktitle> <pages> pages 188-199, </pages> <address> Philadelphia, PA, </address> <month> September </month> <year> 1990. </year> <note> ACM. </note>
Reference-contexts: In layer parallelism, packets visit multiple processors in a pipelined fashion [9, 30, 35]. Packet-level <ref> [3, 8, 12, 14, 15, 24, 29, 34, 35, 36] </ref> and connection-level [8, 12, 29, 34, 36] parallelisms enable concurrency at higher levels of granularity. 1 We present the following results. * We study affinity-based scheduling of send-side UDP/IP/FDDI protocol processing (Section 6).
Reference: [16] <author> Raj Jain and Shawn Routhier. </author> <title> Packet trains: Measurements and a new model for computer network traffic. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 4(6) </volume> <pages> 986-995, </pages> <month> September </month> <year> 1986. </year>
Reference-contexts: In the earlier work, we assumed a workload of a number of Poisson streams. In this work we improve the source model, varying the number of streams (Section 4) and modeling individual streams with the Packet-Train model developed in <ref> [16] </ref> (Section 7). <p> The task execution time is modeled as D + RC. 9 (a) Packet scaling (b) Stream scaling subsequent packets are from the same connection <ref> [16, 23] </ref>, the packet is demultiplexed based on the table cache value, and the active map lookup is avoided. <p> While the Poisson model in [32] was a reasonable starting point, it is well-known that network traffic is generally not Poisson <ref> [6, 11, 16, 21, 27, 28] </ref>. The Poisson process is not very bursty (its coefficient of variation, an informal measure of burstiness, is just 1), nor does the independent Poisson model capture the related property of source locality, a well-known aspect of network traffic [16, 23]. <p> The Poisson process is not very bursty (its coefficient of variation, an informal measure of burstiness, is just 1), nor does the independent Poisson model capture the related property of source locality, a well-known aspect of network traffic <ref> [16, 23] </ref>. <p> To capture stream burstiness and source locality, we refine the per-stream arrival process to the Packet-Train model developed by Jain and Routhier <ref> [16] </ref>. Each stream is modeled as sending bursts of packets, where the burst size B (in packets) has distribution B with mean B. The time I between packets in a burst has distribution I with mean I ; the time T between bursts has distribution T with mean T . <p> Note however that back-to-back arrivals (which the Compound Poisson essentially models) occur rarely in real networks <ref> [11, 16] </ref>. 7.2 Send-side processing increases in stream burstiness. Note that although the delay under stream affinity scheduling rises dramaticallyin fact, the curve is off the scale in Figure 12cit is of interest merely for informal comparison with the receive-side results. <p> We find the technique performs well, in agreement with our earlier receive-side results. * We have examined the impact of stream burstiness and source locality as captured by the Packet-Train source model <ref> [16] </ref>.
Reference: [17] <author> Matthias Kaiserswerth. </author> <title> The parallel protocol engine. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 1(6) </volume> <pages> 650-663, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: We study affinity-based scheduling of parallel network protocol processing, which has recently become an area of active research <ref> [3, 8, 12, 14, 17, 20, 24, 34, 35, 36, 41] </ref> and significant applied/commercial interest [8, 12, 29, 34]. The use of parallelism in protocol processing is motivated by the development of high-speed networks (such as ATM) capable of delivering gigabit-range bandwidth to individual machines. <p> problem in parallel networking involves not only the concurrent management of protocol threads and available processors, but also of packets and the memory resources accessed during packet processing. 1 Depending on the coherence state of the referenced cache line. 2 In functional parallelism, an individual packet concurrently visits multiple processors <ref> [4, 17, 20] </ref>. In layer parallelism, packets visit multiple processors in a pipelined fashion [9, 30, 35].
Reference: [18] <author> Jonathan Kay and Joseph Pasquale. </author> <title> The importance of non-data touching processing overheads in TCP/IP. </title> <booktitle> In Proceedings of the ACM SIGCOMM Conference on Communications, Architectures, Protocols and Applications, </booktitle> <pages> pages 259-268, </pages> <address> San Francisco, CA, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: For example, Kay and Pasquale <ref> [18] </ref> show that 84% of the processing time for TCP packets and 60% for UDP packets in their FDDI LAN environment is attributed to the non-data touching operations of protocol stack operations, buffer management, and OS overheads. <p> Although copying and checksumming (which scale with packet size) are expensive, the reason the non-data touching overheads predominate is that typically in real environments most packets are small <ref> [6, 11, 18, 23] </ref> 10 . 10 Some common applications do not touch the data at all. For example, the SGI implementation of the NFS server takes advantage of the fact that most SGI network interfaces support checksumming in firmware [25]. <p> Incorporating this per-byte overhead into our simulation model requires a packet-size distribution. We chose the empirical UDP/IP/FDDI distribution published by Kay and Pasquale in <ref> [18] </ref>. The authors gathered this distribution from a traffic trace taken from their departmental FDDI LAN, which supports mostly workstations and file servers. <p> In addition, they note that observed packet size distributions match those of Ethernet-based packet traces reported in [6, 11, 23]. The packet size distribution, taken from Figure 5b of <ref> [18] </ref>, is shown in Table 2. In formulating this distribution, we have assumed that each 8KB UDP packet is sent as two 4KB FDDI frames. <p> Although UDP supports checksumming we have not incorporated this overhead into our results. In fact, there are several techniques for reducing or avoiding the overhead of checksumming in UDP (and TCP) <ref> [18] </ref>: * Checksumming can be combined with data copying in integrated layer processing (ILP) [5, 1]. <p> Although TCP is a far more complex protocol than UDP, our results are likely to hold directly for TCP, for two reasons. First, the breakdowns of overall processing time overheads for TCP and UDP packets are very similar (compare for example graphs 3a and 3b in <ref> [18] </ref>). Second, as pointed out in [18], at its most influential (i.e., for 1-byte packets), TCP-specific processing only accounts for around 15% of overall packet execution time (for the DEC Ultrix 4.2a TCP/IP/FDDI in-kernel protocol stack studied in that work). <p> First, the breakdowns of overall processing time overheads for TCP and UDP packets are very similar (compare for example graphs 3a and 3b in <ref> [18] </ref>). Second, as pointed out in [18], at its most influential (i.e., for 1-byte packets), TCP-specific processing only accounts for around 15% of overall packet execution time (for the DEC Ultrix 4.2a TCP/IP/FDDI in-kernel protocol stack studied in that work).
Reference: [19] <author> Jonathan Kay and Joseph Pasquale. </author> <title> Measurement, analysis, and improvement of UDP/IP throughput for the DECstation 5000. </title> <booktitle> In Proceedings of the Winter 1993 USENIX Conference, </booktitle> <pages> pages 249-258, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: On our platform (given the relative overheads), it is likely that ILP would completely hide the checksumming overhead in the memory access latency of the uncached copy 12 . * The hardware interface can support TCP or UDP checksumming, as is generally done in Silicon Graphics interfaces <ref> [19, 25] </ref>. * Checksumming can be disabled when redundant with the network interface's cyclic redundancy check (CRC). This last point is developed in detail by Kay and Pasquale in [19]. <p> This last point is developed in detail by Kay and Pasquale in <ref> [19] </ref>. <p> We examined the send-side execution path from the top of the stack down through the UDP, IP, VNET 14 and FDDI protocols and into the SIMFDDI simulated device driver. The 12 Thanks to David Yates for pointing this out. 13 Substantiated in <ref> [19] </ref> by LAN traffic measurements. 14 VNET is a virtual protocol for managing multiple link-layer protocols, such as FDDI. 15 goal was to identify the protocol and stream-specific data structures written during a send operation, as a first step toward developing send-side IPS and Locking implementations.
Reference: [20] <author> Thomas F. LaPorta and Mischa Schwartz. </author> <title> Performance analysis of MSP: Feature-rich high-speed transport protocol. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 1(6) </volume> <pages> 740-753, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: We study affinity-based scheduling of parallel network protocol processing, which has recently become an area of active research <ref> [3, 8, 12, 14, 17, 20, 24, 34, 35, 36, 41] </ref> and significant applied/commercial interest [8, 12, 29, 34]. The use of parallelism in protocol processing is motivated by the development of high-speed networks (such as ATM) capable of delivering gigabit-range bandwidth to individual machines. <p> problem in parallel networking involves not only the concurrent management of protocol threads and available processors, but also of packets and the memory resources accessed during packet processing. 1 Depending on the coherence state of the referenced cache line. 2 In functional parallelism, an individual packet concurrently visits multiple processors <ref> [4, 17, 20] </ref>. In layer parallelism, packets visit multiple processors in a pipelined fashion [9, 30, 35].
Reference: [21] <author> Will E. Leland, Murad S. Taqqu, Walter Willinger, and Daniel V. Wilson. </author> <title> On the self-similar nature of Ethernet traffic (extended version). </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 2(1) </volume> <pages> 1-15, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: While the Poisson model in [32] was a reasonable starting point, it is well-known that network traffic is generally not Poisson <ref> [6, 11, 16, 21, 27, 28] </ref>. The Poisson process is not very bursty (its coefficient of variation, an informal measure of burstiness, is just 1), nor does the independent Poisson model capture the related property of source locality, a well-known aspect of network traffic [16, 23].
Reference: [22] <author> Evangelos P. Markatos and Thomas J. LeBlanc. </author> <title> Using processor affinity in loop scheduling on shared-memory multipro cessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(4) </volume> <pages> 379-400, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Processor-cache affinity scheduling is of growing interest as processor speeds continue to increase faster than memory speeds <ref> [7, 10, 22, 32, 38, 40] </ref>. On modern shared-memory machines, the time to access an uncached memory location is typically much larger than when accessing one cached locally. <p> Distributed applications, on the other hand, require very low latency network communication. Network parallelism in the host operating systemboth within and among connectionscan both increase the bandwidth and decrease the latency of multiprocessor communication. While previous studies have explored the benefits of affinity-based scheduling of non-network-related application processing <ref> [7, 10, 22, 40] </ref>, our work [31, 32, 33] is the first to apply the technique to operating system network processing.
Reference: [23] <author> Jeffrey C. Mogul. </author> <title> Network locality at the scale of processes. </title> <booktitle> In Proceedings of the ACM SIGCOMM Conference on Communications, Architectures, Protocols and Applications, </booktitle> <pages> pages 273-284, </pages> <address> Zurich, Switzerland, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: The task execution time is modeled as D + RC. 9 (a) Packet scaling (b) Stream scaling subsequent packets are from the same connection <ref> [16, 23] </ref>, the packet is demultiplexed based on the table cache value, and the active map lookup is avoided. <p> Although copying and checksumming (which scale with packet size) are expensive, the reason the non-data touching overheads predominate is that typically in real environments most packets are small <ref> [6, 11, 18, 23] </ref> 10 . 10 Some common applications do not touch the data at all. For example, the SGI implementation of the NFS server takes advantage of the fact that most SGI network interfaces support checksumming in firmware [25]. <p> Kay and Pasquale establish that the trace is representative by comparing the empirical distribution with that of LAN traffic in another departmental environment, finding very similar results. In addition, they note that observed packet size distributions match those of Ethernet-based packet traces reported in <ref> [6, 11, 23] </ref>. The packet size distribution, taken from Figure 5b of [18], is shown in Table 2. In formulating this distribution, we have assumed that each 8KB UDP packet is sent as two 4KB FDDI frames. <p> The Poisson process is not very bursty (its coefficient of variation, an informal measure of burstiness, is just 1), nor does the independent Poisson model capture the related property of source locality, a well-known aspect of network traffic <ref> [16, 23] </ref>.
Reference: [24] <author> Erich M. Nahum, David J. Yates, James F. Kurose, and Don Towsley. </author> <title> Performance issues in parallelized network protocols. </title> <booktitle> In Proceedings of the First USENIX Symposium on Operating Systems Design and Implementation (OSDI), </booktitle> <pages> pages 125-137, </pages> <address> Monterey, CA, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: We study affinity-based scheduling of parallel network protocol processing, which has recently become an area of active research <ref> [3, 8, 12, 14, 17, 20, 24, 34, 35, 36, 41] </ref> and significant applied/commercial interest [8, 12, 29, 34]. The use of parallelism in protocol processing is motivated by the development of high-speed networks (such as ATM) capable of delivering gigabit-range bandwidth to individual machines. <p> In layer parallelism, packets visit multiple processors in a pipelined fashion [9, 30, 35]. Packet-level <ref> [3, 8, 12, 14, 15, 24, 29, 34, 35, 36] </ref> and connection-level [8, 12, 29, 34, 36] parallelisms enable concurrency at higher levels of granularity. 1 We present the following results. * We study affinity-based scheduling of send-side UDP/IP/FDDI protocol processing (Section 6). <p> We began with an unparallelized x-kernel (version 3.2) running in user-space above the native IRIX 5.2 operating system; a simulated device driver emulates the protocol functionality associated with managing an FDDI network interface attachment. We developed in-memory drivers (a technique also used in <ref> [24, 36] </ref>), since the Challenge's eight 100MHz R4400 processors are together much faster than the single FDDI network attachment on our machine. Data is not actually received from (or sent on) the actual FDDI network. <p> The overhead of copying uncached data is incorporated into all results in the remainder of this paper. 5.2 Checksumming The cost of checksumming cached data on our platform is 0:031s/byte <ref> [24] </ref>. Although UDP supports checksumming we have not incorporated this overhead into our results. In fact, there are several techniques for reducing or avoiding the overhead of checksumming in UDP (and TCP) [18]: * Checksumming can be combined with data copying in integrated layer processing (ILP) [5, 1].
Reference: [25] <author> William Nowicki. </author> <type> Personal communication, </type> <month> April </month> <year> 1995. </year> <title> Silicon Graphics, </title> <publisher> Inc. </publisher>
Reference-contexts: For example, the SGI implementation of the NFS server takes advantage of the fact that most SGI network interfaces support checksumming in firmware <ref> [25] </ref>. By DMA'ing data directly to the network interface, the server 12 There are good reasons, however, for explicitly extending our results by incorporating data copying overheads. Most implementations copy packet data between user and kernel space. <p> On our platform (given the relative overheads), it is likely that ILP would completely hide the checksumming overhead in the memory access latency of the uncached copy 12 . * The hardware interface can support TCP or UDP checksumming, as is generally done in Silicon Graphics interfaces <ref> [19, 25] </ref>. * Checksumming can be disabled when redundant with the network interface's cyclic redundancy check (CRC). This last point is developed in detail by Kay and Pasquale in [19].
Reference: [26] <author> Sean W. O'Malley and Larry L. Peterson. </author> <title> A dynamic network architecture. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(2) </volume> <pages> 110-143, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: While previous studies have explored the benefits of affinity-based scheduling of non-network-related application processing [7, 10, 22, 40], our work [31, 32, 33] is the first to apply the technique to operating system network processing. In our experimental environment (consisting of a parallelized x-kernel <ref> [13, 26] </ref> running in user-space on an 8-processor MIPS R4400-based SGI Challenge XL), packet execution times can vary by as much as a factor of four, depending on the state of the processor cache. This suggests that affinity-based scheduling presents a promising research opportunity in parallel networking.
Reference: [27] <author> Vern Paxson. </author> <title> Empirically-derived analytic models of wide-area TCP connections. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 2(4) </volume> <pages> 316-336, </pages> <month> August </month> <year> 1994. </year> <month> 27 </month>
Reference-contexts: While the Poisson model in [32] was a reasonable starting point, it is well-known that network traffic is generally not Poisson <ref> [6, 11, 16, 21, 27, 28] </ref>. The Poisson process is not very bursty (its coefficient of variation, an informal measure of burstiness, is just 1), nor does the independent Poisson model capture the related property of source locality, a well-known aspect of network traffic [16, 23].
Reference: [28] <author> Vern Paxson and Sally Floyd. </author> <title> Wide area traffic: The failure of Poisson modeling. </title> <booktitle> In Proceedings of the ACM SIGCOMM Conference on Communications, Architectures, Protocols and Applications, </booktitle> <pages> pages 257-268, </pages> <address> London, England UK, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: While the Poisson model in [32] was a reasonable starting point, it is well-known that network traffic is generally not Poisson <ref> [6, 11, 16, 21, 27, 28] </ref>. The Poisson process is not very bursty (its coefficient of variation, an informal measure of burstiness, is just 1), nor does the independent Poisson model capture the related property of source locality, a well-known aspect of network traffic [16, 23].
Reference: [29] <author> David Presotto. </author> <title> Multiprocessor STREAMS for Plan 9. In United Kingdom UNIX User's Group, </title> <month> January </month> <year> 1993. </year>
Reference-contexts: We study affinity-based scheduling of parallel network protocol processing, which has recently become an area of active research [3, 8, 12, 14, 17, 20, 24, 34, 35, 36, 41] and significant applied/commercial interest <ref> [8, 12, 29, 34] </ref>. The use of parallelism in protocol processing is motivated by the development of high-speed networks (such as ATM) capable of delivering gigabit-range bandwidth to individual machines. Emerging large-scale server applications, such as digital multimedia information repositories, require application-level access to such high bandwidth. <p> In layer parallelism, packets visit multiple processors in a pipelined fashion [9, 30, 35]. Packet-level <ref> [3, 8, 12, 14, 15, 24, 29, 34, 35, 36] </ref> and connection-level [8, 12, 29, 34, 36] parallelisms enable concurrency at higher levels of granularity. 1 We present the following results. * We study affinity-based scheduling of send-side UDP/IP/FDDI protocol processing (Section 6). <p> In layer parallelism, packets visit multiple processors in a pipelined fashion [9, 30, 35]. Packet-level [3, 8, 12, 14, 15, 24, 29, 34, 35, 36] and connection-level <ref> [8, 12, 29, 34, 36] </ref> parallelisms enable concurrency at higher levels of granularity. 1 We present the following results. * We study affinity-based scheduling of send-side UDP/IP/FDDI protocol processing (Section 6).
Reference: [30] <author> Erich Rutsche and Mattias Kaiserswerth. </author> <title> TCP/IP on the parallel protocol engine. </title> <booktitle> Fourth IFIP TC6.1/WG6.4 International Conference on High Performance Networking, </booktitle> <pages> pages 119-134, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: In layer parallelism, packets visit multiple processors in a pipelined fashion <ref> [9, 30, 35] </ref>. Packet-level [3, 8, 12, 14, 15, 24, 29, 34, 35, 36] and connection-level [8, 12, 29, 34, 36] parallelisms enable concurrency at higher levels of granularity. 1 We present the following results. * We study affinity-based scheduling of send-side UDP/IP/FDDI protocol processing (Section 6).
Reference: [31] <author> James Salehi, James Kurose, and Don Towsley. </author> <title> Scheduling for cache affinity in parallelized communication protocols. </title> <type> Technical Report UM-CS-1994-075, </type> <institution> University of Massachusetts, Amherst, </institution> <month> October </month> <year> 1994. </year> <note> Available via FTP from gaia.cs.umass.edu in pub/Sale94:Scheduling.Z. </note>
Reference-contexts: Network parallelism in the host operating systemboth within and among connectionscan both increase the bandwidth and decrease the latency of multiprocessor communication. While previous studies have explored the benefits of affinity-based scheduling of non-network-related application processing [7, 10, 22, 40], our work <ref> [31, 32, 33] </ref> is the first to apply the technique to operating system network processing. <p> In Section 3 we discuss the research methodology, summarizing the relevant aspects of the simulation and analytic models. Performance results are presented in Sections 4-8. We summarize the results in Section 9. For a discussion of related work in affinity-based scheduling, see <ref> [31, 32] </ref>. 2 Problem formulation 2.1 Affinity-based scheduling of protocol processing Consider Figure 1, which depicts a simplified view of in-kernel, receive-side protocol processing on a multiprocessor with FDDI network attachment. <p> In this section, we summarize the salient aspects of our approach; extensive details are provided in <ref> [31, 32] </ref>. 6 A single per-stack lock ensures serialization of its processing. 6 We begin by developing a multiprocessor simulation model that closely follows the behavior of Figure 1. <p> Details are provided in <ref> [31, 32] </ref>. F 1 (x i ) and F 2 (x i ) are formulated to reflect the cache architecture and organization of our SGI Challenge. 7 The migration overhead incurred by P i when accessing a global free-memory pool is incorporated in a slightly different manner. <p> To acquire the timing bounds, we measured the time to process a packet in a set of experiments on our multiprocessor in which we varied the scheduling of protocol threads and explicitly manipulated the processor caches. The receive-side experimental design and resulting measurements are presented in extensive detail in <ref> [31, 32] </ref>. The send-side experimental analysis, which is similar but simpler, appears in Section 6 of this paper.
Reference: [32] <author> James Salehi, James Kurose, and Don Towsley. </author> <title> The performance impact of scheduling for cache affinity in parallel network processing. </title> <booktitle> In Fourth IEEE International Symposium on High-Performance Distributed Computing, </booktitle> <address> Pentagon City, VA, </address> <month> August </month> <year> 1995. </year> <note> Available via FTP from gaia.cs.umass.edu in pub/Sale95:Performance.Z. </note>
Reference-contexts: 1 Introduction Processor-cache affinity scheduling is of growing interest as processor speeds continue to increase faster than memory speeds <ref> [7, 10, 22, 32, 38, 40] </ref>. On modern shared-memory machines, the time to access an uncached memory location is typically much larger than when accessing one cached locally. <p> Network parallelism in the host operating systemboth within and among connectionscan both increase the bandwidth and decrease the latency of multiprocessor communication. While previous studies have explored the benefits of affinity-based scheduling of non-network-related application processing [7, 10, 22, 40], our work <ref> [31, 32, 33] </ref> is the first to apply the technique to operating system network processing. <p> In Section 3 we discuss the research methodology, summarizing the relevant aspects of the simulation and analytic models. Performance results are presented in Sections 4-8. We summarize the results in Section 9. For a discussion of related work in affinity-based scheduling, see <ref> [31, 32] </ref>. 2 Problem formulation 2.1 Affinity-based scheduling of protocol processing Consider Figure 1, which depicts a simplified view of in-kernel, receive-side protocol processing on a multiprocessor with FDDI network attachment. <p> In this section, we summarize the salient aspects of our approach; extensive details are provided in <ref> [31, 32] </ref>. 6 A single per-stack lock ensures serialization of its processing. 6 We begin by developing a multiprocessor simulation model that closely follows the behavior of Figure 1. <p> Details are provided in <ref> [31, 32] </ref>. F 1 (x i ) and F 2 (x i ) are formulated to reflect the cache architecture and organization of our SGI Challenge. 7 The migration overhead incurred by P i when accessing a global free-memory pool is incorporated in a slightly different manner. <p> To acquire the timing bounds, we measured the time to process a packet in a set of experiments on our multiprocessor in which we varied the scheduling of protocol threads and explicitly manipulated the processor caches. The receive-side experimental design and resulting measurements are presented in extensive detail in <ref> [31, 32] </ref>. The send-side experimental analysis, which is similar but simpler, appears in Section 6 of this paper. <p> We now turn to the first set of set of performance results. 4 Stream scaling We begin by examining how affinity-based scheduling performs as the number of streams in the protocol stack varies. In <ref> [32] </ref>, we fixed the number of streams at eight, and examined affinity-based scheduling under packet scalingby varying the mean packet rate of the eight individual streams. <p> Stream scaling is a more realistic paradigm in the sense that most multiprocessor server applications (e.g., file servers, web servers) must support a varying and potentially large number of concurrent streams. In the following two subsections, we evaluate stream scaling, compare it to the packet-scaled results of <ref> [32] </ref>, and discuss the reasons behind the differences. Under stream scaling, we set the individual stream packet rate to 125 packets/s. The inter-packet delay distribution is taken to be exponential (as in the packet-scaled results of [32]) to enable a meaningful comparison. <p> following two subsections, we evaluate stream scaling, compare it to the packet-scaled results of <ref> [32] </ref>, and discuss the reasons behind the differences. Under stream scaling, we set the individual stream packet rate to 125 packets/s. The inter-packet delay distribution is taken to be exponential (as in the packet-scaled results of [32]) to enable a meaningful comparison. As background for understanding the impact of stream scaling, consider the following. Each protocol stack contains (at each protocol layer) fast-path demultiplexing support for the anticipated common case of network source locality. <p> Thus, we can expect that under Locking the impact of stream scaling should be relatively small, since N = 8 in the earlier studyand 1/8 of the overhead avoided by the fast-path optimization (about 25s <ref> [32] </ref>) is only 3s. show code and stream affinity scheduling; the lower graphs show thread and free memory affinity scheduling 9 . The axes have been aligned to allow direct comparison of the graphs. <p> The axes have been aligned to allow direct comparison of the graphs. In the upper right of each graph, the maximum supportable throughput for each policy is indicated (e.g., 64,000 packets/s under code affinity scheduling in Figure 9 The graphs in Figures 4a and 5a are borrowed from <ref> [32] </ref>. 10 4a). It is evident that, in general, the impact of the varying number of streams is small under Locking. Code affinity scheduling is impacted the most (rising above stream affinity scheduling in range of 70-140 streams) since the fast-path optimization is experienced most frequently under that scheduling policy. <p> Thus, the corresponding curves converge at high arrival rate. Since free-memory affinity scheduling reduces packet latency at high arrival rate, it increases the maximum number of supportable streams (in Figure 4b by 5%, from 480 to 505). 4.2 IPS In <ref> [32] </ref>, we matched the number of independent stacks with the number of processors (eight in that study) to enable all processors to work concurrently under IPS. For simplicity, we also set the number of streams to be eight. <p> To ensure the data is uncached, the referenced memory locations are written by another processor prior to protocol processing for the packet. We repeated the timing experiments, varying B from 1 to 4KB, and found that the receive-side timings presented in <ref> [32] </ref> (as well as the send-side timings presented below in Table 3) scale up nearly linearly at a rate of 0.04s per byte. Incorporating this per-byte overhead into our simulation model requires a packet-size distribution. We chose the empirical UDP/IP/FDDI distribution published by Kay and Pasquale in [18]. <p> In the UDP implementation, a flag is added per network interface indicating whether hardware CRC is supported. During UDP processing, the route structure is consulted; the checksum is skipped when the next hop is the destination host and the interface supports a hardware CRC. 6 Send-side processing In <ref> [32] </ref>, we explored affinity-based scheduling of receive-side UDP/IP/FDDI protocol processing. How does the scheduling technique apply to send-side processing, and what is the comparative performance? To explore this, we revisited the unparallelized x-kernel implementation. <p> A sufficiently large number of independent runs (100 in our experiments) are performed to ensure that the 95% confidence interval half-widths do not exceed 1% of the overall mean, for all values. The obtained timing measurements are shown in Table 3. (The corresponding receive-side numbers from <ref> [32] </ref> are presented for comparison purposes). The timings suggest the importance of code affinity scheduling of send-side processing. <p> On the receive side, stream affinity scheduling increases maximum throughput by about 19% (Figure 6). * Free-memory scheduling increases maximum throughput by a larger fraction on the send-side. 7 Stream burstiness and source locality We now turn our attention to refining the stream model. While the Poisson model in <ref> [32] </ref> was a reasonable starting point, it is well-known that network traffic is generally not Poisson [6, 11, 16, 21, 27, 28]. <p> However, with B sufficiently large, code affinity scheduling under Locking offers lower per-packet latency than under IPS. If the mean inter-packet delay I were smaller, the latency under IPS would rise above Locking even sooner (i.e., with respect to increasing B). In <ref> [32] </ref>, we considered Compound Poisson arrivals, which represent the I = 0 case: Figure 13 21 (a) B = 1 (b) B = 2 (c) B = 8 in that paper shows the latency to be lower under Locking for all values of B greater than 1.
Reference: [33] <author> James Salehi, James Kurose, and Don Towsley. </author> <title> Scheduling for cache affinity in parallelized communication protocols (extended abstract). </title> <booktitle> In Proceedings of 1995 SIGMETRICS/Performance International Conference on Measurement and Modeling of Computer Systems, </booktitle> <address> Ottawa, Canada, </address> <month> May </month> <year> 1995. </year> <note> Available via FTP from gaia.cs.umass.edu in pub/Sale95:Scheduling.Z. </note>
Reference-contexts: Network parallelism in the host operating systemboth within and among connectionscan both increase the bandwidth and decrease the latency of multiprocessor communication. While previous studies have explored the benefits of affinity-based scheduling of non-network-related application processing [7, 10, 22, 40], our work <ref> [31, 32, 33] </ref> is the first to apply the technique to operating system network processing.
Reference: [34] <author> Sunil Saxena, J. Kent Peacock, Fred Yang, Vijaya Verma, and Mohan Krishnan. </author> <title> Pitfalls in multithreading SVR4 STREAMS and other weightless processes. </title> <booktitle> In Proceedings of the Winter 1993 USENIX Conference, </booktitle> <pages> pages 85-96, </pages> <address> San Diego, CA, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: We study affinity-based scheduling of parallel network protocol processing, which has recently become an area of active research <ref> [3, 8, 12, 14, 17, 20, 24, 34, 35, 36, 41] </ref> and significant applied/commercial interest [8, 12, 29, 34]. The use of parallelism in protocol processing is motivated by the development of high-speed networks (such as ATM) capable of delivering gigabit-range bandwidth to individual machines. <p> We study affinity-based scheduling of parallel network protocol processing, which has recently become an area of active research [3, 8, 12, 14, 17, 20, 24, 34, 35, 36, 41] and significant applied/commercial interest <ref> [8, 12, 29, 34] </ref>. The use of parallelism in protocol processing is motivated by the development of high-speed networks (such as ATM) capable of delivering gigabit-range bandwidth to individual machines. Emerging large-scale server applications, such as digital multimedia information repositories, require application-level access to such high bandwidth. <p> In layer parallelism, packets visit multiple processors in a pipelined fashion [9, 30, 35]. Packet-level <ref> [3, 8, 12, 14, 15, 24, 29, 34, 35, 36] </ref> and connection-level [8, 12, 29, 34, 36] parallelisms enable concurrency at higher levels of granularity. 1 We present the following results. * We study affinity-based scheduling of send-side UDP/IP/FDDI protocol processing (Section 6). <p> In layer parallelism, packets visit multiple processors in a pipelined fashion [9, 30, 35]. Packet-level [3, 8, 12, 14, 15, 24, 29, 34, 35, 36] and connection-level <ref> [8, 12, 29, 34, 36] </ref> parallelisms enable concurrency at higher levels of granularity. 1 We present the following results. * We study affinity-based scheduling of send-side UDP/IP/FDDI protocol processing (Section 6).
Reference: [35] <author> Douglas C. Schmidt and Tatsuya Suda. </author> <title> Measuring the impact of alternative parallel process architectures on communication subsystem performance. </title> <booktitle> In Proceedings of the 4 th International Workshop on Protocols for High-Speed Networks, </booktitle> <address> Vancouver, British Columbia, </address> <month> August </month> <year> 1994. </year> <pages> IFIP. </pages>
Reference-contexts: We study affinity-based scheduling of parallel network protocol processing, which has recently become an area of active research <ref> [3, 8, 12, 14, 17, 20, 24, 34, 35, 36, 41] </ref> and significant applied/commercial interest [8, 12, 29, 34]. The use of parallelism in protocol processing is motivated by the development of high-speed networks (such as ATM) capable of delivering gigabit-range bandwidth to individual machines. <p> In layer parallelism, packets visit multiple processors in a pipelined fashion <ref> [9, 30, 35] </ref>. Packet-level [3, 8, 12, 14, 15, 24, 29, 34, 35, 36] and connection-level [8, 12, 29, 34, 36] parallelisms enable concurrency at higher levels of granularity. 1 We present the following results. * We study affinity-based scheduling of send-side UDP/IP/FDDI protocol processing (Section 6). <p> In layer parallelism, packets visit multiple processors in a pipelined fashion [9, 30, 35]. Packet-level <ref> [3, 8, 12, 14, 15, 24, 29, 34, 35, 36] </ref> and connection-level [8, 12, 29, 34, 36] parallelisms enable concurrency at higher levels of granularity. 1 We present the following results. * We study affinity-based scheduling of send-side UDP/IP/FDDI protocol processing (Section 6).
Reference: [36] <author> Douglas C. Schmidt and Tatsuya Suda. </author> <title> Measuring the performance of parallel message-based process architectures. </title> <booktitle> In Proceedings of the Conference on Computer Communications (IEEE Infocom), </booktitle> <pages> pages 624-633, </pages> <address> Boston, MA, </address> <month> April </month> <year> 1995. </year> <note> IEEE. </note>
Reference-contexts: We study affinity-based scheduling of parallel network protocol processing, which has recently become an area of active research <ref> [3, 8, 12, 14, 17, 20, 24, 34, 35, 36, 41] </ref> and significant applied/commercial interest [8, 12, 29, 34]. The use of parallelism in protocol processing is motivated by the development of high-speed networks (such as ATM) capable of delivering gigabit-range bandwidth to individual machines. <p> In layer parallelism, packets visit multiple processors in a pipelined fashion [9, 30, 35]. Packet-level <ref> [3, 8, 12, 14, 15, 24, 29, 34, 35, 36] </ref> and connection-level [8, 12, 29, 34, 36] parallelisms enable concurrency at higher levels of granularity. 1 We present the following results. * We study affinity-based scheduling of send-side UDP/IP/FDDI protocol processing (Section 6). <p> In layer parallelism, packets visit multiple processors in a pipelined fashion [9, 30, 35]. Packet-level [3, 8, 12, 14, 15, 24, 29, 34, 35, 36] and connection-level <ref> [8, 12, 29, 34, 36] </ref> parallelisms enable concurrency at higher levels of granularity. 1 We present the following results. * We study affinity-based scheduling of send-side UDP/IP/FDDI protocol processing (Section 6). <p> We began with an unparallelized x-kernel (version 3.2) running in user-space above the native IRIX 5.2 operating system; a simulated device driver emulates the protocol functionality associated with managing an FDDI network interface attachment. We developed in-memory drivers (a technique also used in <ref> [24, 36] </ref>), since the Challenge's eight 100MHz R4400 processors are together much faster than the single FDDI network attachment on our machine. Data is not actually received from (or sent on) the actual FDDI network.
Reference: [37] <author> Jaswinder Pal Singh, Harold S. Stone, and Dominique F. Thiebaut. </author> <title> A model of workloads and its use in miss-rate prediction for fully associative caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(7) </volume> <pages> 811-825, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: To capture the displacement of the cached protocol footprint by the non-protocol workload, we develop an analytic model of packet execution time (combining established analytic results from other researchers <ref> [37, 39] </ref>) that reflects the specific cache architecture and organization of our SGI Challenge. We then conduct a set of multiprocessor experiments, designed to measure packet execution times under specific conditions of cache state, and parameterize the analytic model with the experimentally-measured values. <p> The appropriate set of timing measurements is selected before the packet execution time is computed 7 . The non-protocol workload displaces the cached protocol footprint at each layer of the cache hierarchy. To capture this displacement, the model first computes (using analytic results from <ref> [37, 39] </ref>) the fractions F 1 (x i ) and F 2 (x i ) of the footprint that have been flushed from L1 and L2, respectively, given that non-protocol processing has executed for time x i since P i last executed protocol code. Details are provided in [31, 32].
Reference: [38] <author> Mark S. Squillante and Edward D. Lazowska. </author> <title> Using processor cache affinity information in shared-memory multiprocessor scheduling. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(2) </volume> <pages> 131-143, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Processor-cache affinity scheduling is of growing interest as processor speeds continue to increase faster than memory speeds <ref> [7, 10, 22, 32, 38, 40] </ref>. On modern shared-memory machines, the time to access an uncached memory location is typically much larger than when accessing one cached locally. <p> In addition, the approachwhich is based on timing measurementsdoes not require identifying the footprint of the task being affinity scheduled (as is assumed, e.g., in <ref> [38] </ref>), and thus avoids the difficulties inherent in capturing memory traces from large parallel applications. 3.1 Simulation model of multiprocessor networking Consider first the simulation of Locking, under which there are N processors and N protocol threads. <p> In the common case that 8 Task execution time as the linear interpolation of the maximum reload transient is also the approach taken in <ref> [38] </ref>. In the authors' formulation, the inherent computing demand of a task is denoted D, the average time to reload the entire footprint is C, and the fraction of the footprint displaced is R.
Reference: [39] <author> Dominique F. Thiebaut and Harold S. Stone. </author> <title> Footprints in the cache. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 5(4):305 329, </volume> <month> November </month> <year> 1987. </year>
Reference-contexts: To capture the displacement of the cached protocol footprint by the non-protocol workload, we develop an analytic model of packet execution time (combining established analytic results from other researchers <ref> [37, 39] </ref>) that reflects the specific cache architecture and organization of our SGI Challenge. We then conduct a set of multiprocessor experiments, designed to measure packet execution times under specific conditions of cache state, and parameterize the analytic model with the experimentally-measured values. <p> The appropriate set of timing measurements is selected before the packet execution time is computed 7 . The non-protocol workload displaces the cached protocol footprint at each layer of the cache hierarchy. To capture this displacement, the model first computes (using analytic results from <ref> [37, 39] </ref>) the fractions F 1 (x i ) and F 2 (x i ) of the footprint that have been flushed from L1 and L2, respectively, given that non-protocol processing has executed for time x i since P i last executed protocol code. Details are provided in [31, 32].
Reference: [40] <author> Raj Vaswani and John Zahorjan. </author> <title> The implications of cache affinity on processor scheduling for multiprogrammed, shared memory multiprocessors. </title> <booktitle> In Proceedings of the Thirteenth Symposium on Operating Systems Principles, </booktitle> <pages> pages 26-40, </pages> <address> Pacific Grove, CA, </address> <month> October </month> <year> 1991. </year> <note> ACM. </note>
Reference-contexts: 1 Introduction Processor-cache affinity scheduling is of growing interest as processor speeds continue to increase faster than memory speeds <ref> [7, 10, 22, 32, 38, 40] </ref>. On modern shared-memory machines, the time to access an uncached memory location is typically much larger than when accessing one cached locally. <p> Distributed applications, on the other hand, require very low latency network communication. Network parallelism in the host operating systemboth within and among connectionscan both increase the bandwidth and decrease the latency of multiprocessor communication. While previous studies have explored the benefits of affinity-based scheduling of non-network-related application processing <ref> [7, 10, 22, 40] </ref>, our work [31, 32, 33] is the first to apply the technique to operating system network processing.
Reference: [41] <author> C. Murray Woodside and R. Greg Franks. </author> <title> Alternative software architectures for parallel protocol execution with syn chronous IPC. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 1(2) </volume> <pages> 178-186, </pages> <month> April </month> <year> 1993. </year> <month> 28 </month>
Reference-contexts: We study affinity-based scheduling of parallel network protocol processing, which has recently become an area of active research <ref> [3, 8, 12, 14, 17, 20, 24, 34, 35, 36, 41] </ref> and significant applied/commercial interest [8, 12, 29, 34]. The use of parallelism in protocol processing is motivated by the development of high-speed networks (such as ATM) capable of delivering gigabit-range bandwidth to individual machines.
References-found: 41

