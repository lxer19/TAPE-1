URL: ftp://ftp.cs.yale.edu/pub/hager/papers/IFA_IJCV.ps.gz
Refering-URL: http://www.cs.yale.edu/users/hager/papers.html
Root-URL: http://www.cs.yale.edu
Title: Incremental Focus of Attention for Robust Vision-Based Tracking  
Author: Kentaro Toyama and Gregory Hager P. O. 
Keyword: visual tracking, real-time vision, face tracking, robust tracking.  
Note: Submitted to IJCV special edition on vision at Yale.  
Address: Box 208285 New Haven, CT 06520-8285  
Affiliation: Department of Computer Science Yale University  
Email: E-mail: toyama@cs.yale.edu, hager@cs.yale.edu  
Phone: Phone: (203) 432-6432  
Abstract: We present the Incremental Focus of Attention (IFA) architecture for robust, adaptive, real-time motion tracking. IFA systems combine several visual search and vision-based tracking algorithms into a layered hierarchy. The architecture controls the transitions between layers and executes algorithms appropriate to the visual environment at hand: When conditions are good, tracking is accurate and precise; as conditions deteriorate, more robust, yet less accurate algorithms take over; when tracking is lost altogether, layers cooperate to perform a rapid search for the target in order to recover it and continue tracking. Implemented IFA systems are extremely robust to most common types of temporary visual disturbances. They resist minor visual perturbances and recover quickly after full occlusions, illumination changes, major distractions, and target disappearances. Analysis of the algorithm's recovery times are supported by simulation results and experiments on real data. In particular, examples show that recovery times after lost tracking depend primarily on the number of objects visually similar to the target in the field of view. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Blake, R. Curwen, and A. Zisserman. </author> <title> Affine-invariant contour tracking with automatic control of spatiotemporal scale. </title> <booktitle> In Proc. Int'l Conference on Computer Vision, </booktitle> <pages> pages 421-430, </pages> <address> Berlin, Germany, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: This requires some predictability in the target trajectory but eliminates the need to examine the entire image. Another way to handle distraction is through foveation, effectively blurring the image region around the target [6, 37]. Finally, a tracking system may filter output estimates based on expected motion and noise <ref> [1] </ref>. Each technique avoids distraction by ignoring or filtering out 3 competing object states which are unlikely to be the target. Other visual disturbances have other solutions. <p> Fast or unpredictable motion requires combinations of faster hardware, full-frame processing [5, 30], or probabilistic dynamics [22]. Occlusions, where opaque objects intercept the camera's line of sight to the target, can be handled by robust matching [13, 14, 26, 25] or by dynamic state prediction <ref> [1, 36, 38] </ref>. Ante-failure work generally seeks to handle problems one at a time, but some researchers have sought to design systems that are ante-failure robust to many types of visual perturbations simultaneously.
Reference: [2] <author> K.J. Bradshaw, P.F. McLauchlan, I.D. Reid, and D.W. Murray. </author> <title> Saccade and pursuit on an active head/eye platform. </title> <journal> Image and Vision Computing, </journal> <volume> 12(3) </volume> <pages> 155-163, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Second, IFA is not restricted to computing positional or directional information about a target, as are many active tracking systems which emphasize the control aspects of keeping an object in view <ref> [2, 8, 31, 33, 46] </ref>. Rather, IFA search may recover rotation, scaling, shear, and even full 3D pose information. Third, we pose tracking itself as a flexible multi-stage focus of attention rather than remaining within the standard two-stage paradigm [27, 31] or even a fixed k-stage architecture [7, 9, 32].
Reference: [3] <author> R. Burridge, A. Rizzi, and D. Koditschek. </author> <title> Towards a dynamical pick and place. </title> <booktitle> In Proc. Int'l Conf. Intel. Rob. and Sys., </booktitle> <volume> volume 2, </volume> <pages> pages 292-297, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: Finally, we note that recent work in control of robotic systems lays a possible theoretical foundation for IFA. "Dynamical pick and place" tasks, in contrast to static pick and place tasks, require a robot to control the dynamics of an object in addition to controlling its position <ref> [3] </ref>. An example is the problem of using a flat, gripper-less, 3-DOF arm to catch a ball thrown into the robot's workspace, where "catching" means bringing the ball to eventual standstill on the bat. <p> By sequential composition of locally convergent controllers, one for each subregion in the state space of the ball, the ball can be caught from almost any initial state <ref> [3, 4] </ref>: When the ball is falling at high speeds, the controller switches to bouncing control to slow the ball down. At lower speeds, the controller may switch to "palming" behavior, where the bat is able to maintain contact with the ball continuously without bouncing.
Reference: [4] <author> R. Burridge, A. Rizzi, and D. Koditschek. </author> <title> Sequential composition of dynamically dexterous robot behaviors. </title> <journal> Int'l J. Robot. Res., </journal> <note> 1997. To appear. </note>
Reference-contexts: By sequential composition of locally convergent controllers, one for each subregion in the state space of the ball, the ball can be caught from almost any initial state <ref> [3, 4] </ref>: When the ball is falling at high speeds, the controller switches to bouncing control to slow the ball down. At lower speeds, the controller may switch to "palming" behavior, where the bat is able to maintain contact with the ball continuously without bouncing.
Reference: [5] <author> P.J. Burt. </author> <title> Attention mechanisms for vision in a dynamic world. </title> <booktitle> In Int'l Conference on Patt. Recog., </booktitle> <pages> pages 977-987, </pages> <year> 1988. </year>
Reference-contexts: Other visual disturbances have other solutions. Changes in ambient lighting have been handled by concentrating on color cues [35, 50], by tracking based on edges [13, 24, 26], or by explicitly modeling illumination parameters [20]. Fast or unpredictable motion requires combinations of faster hardware, full-frame processing <ref> [5, 30] </ref>, or probabilistic dynamics [22]. Occlusions, where opaque objects intercept the camera's line of sight to the target, can be handled by robust matching [13, 14, 26, 25] or by dynamic state prediction [1, 36, 38].
Reference: [6] <author> P.J. Burt and G.S. van der Wal. </author> <title> An architecture for multiresolution, focal, image analysis. </title> <booktitle> In Int'l Conference on Patt. Recog., </booktitle> <pages> pages 305-311, </pages> <year> 1990. </year>
Reference-contexts: One method is to consider only a small set of states surrounding the target [15, 47]. This requires some predictability in the target trajectory but eliminates the need to examine the entire image. Another way to handle distraction is through foveation, effectively blurring the image region around the target <ref> [6, 37] </ref>. Finally, a tracking system may filter output estimates based on expected motion and noise [1]. Each technique avoids distraction by ignoring or filtering out 3 competing object states which are unlikely to be the target. Other visual disturbances have other solutions.
Reference: [7] <author> V. Concepcion and H. Wechsler. </author> <title> Detection and localization of objects in time-varying imagery using attention, representation and memory pyramids. Patt. </title> <journal> Recog., </journal> <volume> 29(9) </volume> <pages> 1543-1557, </pages> <month> September </month> <year> 1996. </year>
Reference-contexts: In face tracking, for example, some systems are able to robustly determine such things as the facial expression of the target in real-time [9, 32]. Another system is able to detect and track a limited set of vehicles using a three-stage focus of attention scheme <ref> [7] </ref>. Others, focusing on the information contained in multiple cues focus on fusing different types of image information to identify a unique target [27, 46]. IFA differs from prior work in several ways. <p> Rather, IFA search may recover rotation, scaling, shear, and even full 3D pose information. Third, we pose tracking itself as a flexible multi-stage focus of attention rather than remaining within the standard two-stage paradigm [27, 31] or even a fixed k-stage architecture <ref> [7, 9, 32] </ref>. Lastly, our framework is not restricted to particular targets or specific cue types. Techniques such as eye-blink detection [9] or mouth localization [32] are specifically designed for frontal views of faces only and do not generalize for other target types.
Reference: [8] <author> D. Coombs and C.M. Brown. </author> <title> Real-time binocular smooth-pursuit. </title> <journal> Int'l Journal of Computer Vision, </journal> <volume> 11(2) </volume> <pages> 147-165, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: Second, IFA is not restricted to computing positional or directional information about a target, as are many active tracking systems which emphasize the control aspects of keeping an object in view <ref> [2, 8, 31, 33, 46] </ref>. Rather, IFA search may recover rotation, scaling, shear, and even full 3D pose information. Third, we pose tracking itself as a flexible multi-stage focus of attention rather than remaining within the standard two-stage paradigm [27, 31] or even a fixed k-stage architecture [7, 9, 32].
Reference: [9] <author> J. Crowley and F. Berard. </author> <title> Multi-modal tracking of faces for video communication. </title> <booktitle> In Computer Vision and Patt. </booktitle> <address> Recog., </address> <year> 1997. </year>
Reference-contexts: Notable among these are probabilistic methods, which sample and interpolate likelihoods over the state space [22], and sensor fusion techniques, which track based on multiple cues <ref> [9, 23, 32, 34, 41] </ref>. None of these efforts are error-free. As in other domains, ante-failure methods cannot eliminate failure entirely. Most often, the limitations are a reflection not of poor algorithmic design, but rather of the impossibility of perfect vision-based tracking in complex circumstances. <p> Some recent work extends this notion to using more than two stages or multiple cues for tracking. In face tracking, for example, some systems are able to robustly determine such things as the facial expression of the target in real-time <ref> [9, 32] </ref>. Another system is able to detect and track a limited set of vehicles using a three-stage focus of attention scheme [7]. Others, focusing on the information contained in multiple cues focus on fusing different types of image information to identify a unique target [27, 46]. <p> Rather, IFA search may recover rotation, scaling, shear, and even full 3D pose information. Third, we pose tracking itself as a flexible multi-stage focus of attention rather than remaining within the standard two-stage paradigm [27, 31] or even a fixed k-stage architecture <ref> [7, 9, 32] </ref>. Lastly, our framework is not restricted to particular targets or specific cue types. Techniques such as eye-blink detection [9] or mouth localization [32] are specifically designed for frontal views of faces only and do not generalize for other target types. <p> Third, we pose tracking itself as a flexible multi-stage focus of attention rather than remaining within the standard two-stage paradigm [27, 31] or even a fixed k-stage architecture [7, 9, 32]. Lastly, our framework is not restricted to particular targets or specific cue types. Techniques such as eye-blink detection <ref> [9] </ref> or mouth localization [32] are specifically designed for frontal views of faces only and do not generalize for other target types. IFA, in contrast, allows utilization of a variety 4 of algorithms and thus confers robustness to many different target types.
Reference: [10] <author> J. L. Crowley, P. Stelmaszyk, T. Skordas, and P. Puget. </author> <title> Measurement and integration of 3-D structures by tracking edge lines. </title> <journal> Int'l Journal of Computer Vision, </journal> <volume> 8(1) </volume> <pages> 29-52, </pages> <year> 1992. </year>
Reference-contexts: this tracker may perform the same function with a subset of the affine transformations. * An SSD-based multi-pattern tracker performs as above, but it tracks successfully if the current image contains a match to any of several stored images. * A 3-D model tracker (such as the ones described in <ref> [10, 13, 26] </ref>) tracks the 3-D pose of an object with respect to the camera. The input configuration set would be the approximate pose of the object with an error margin, and the output set could be a smaller set of poses.
Reference: [11] <author> S. M. Culhane and J. K. Tsotsos. </author> <title> An attentional prototype for early vision. </title> <booktitle> In Computer Vision - ECCV '92, </booktitle> <pages> pages 551-560, </pages> <address> Italy, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Others, focusing on the information contained in multiple cues focus on fusing different types of image information to identify a unique target [27, 46]. IFA differs from prior work in several ways. First, rather than trying to build computational models of biological visual systems <ref> [11, 45] </ref>, we are primarily concerned with synthesizing methods for vision-based tracking which are computationally efficient and robust.
Reference: [12] <author> W. Feiten, G. D. Hager, J. Bauer, B. Magnussen, and K. Toyama. </author> <title> Modeling and control for mobile manipulation in everyday environments. </title> <booktitle> In Int'l Symp. on Robot. </booktitle> <institution> Res., </institution> <year> 1997. </year>
Reference-contexts: But, as with face tracking, the system was able to rapidly recover from all temporary perturbations. In another illustration of IFA applied to robotics, we developed a robust doorknob tracking module for use with mobile robots <ref> [12] </ref>. This particular implementation used an intermediate object [48], where a search for the door takes place to facilitate finding the doorknob itself (characterized as an "edge-rich" object on doors approximately at waist height).
Reference: [13] <author> D. B. Gennery. </author> <title> Visual tracking of known three-dimensional objects. </title> <journal> Int'l Journal of Computer Vision, </journal> <volume> 7(3) </volume> <pages> 243-270, </pages> <year> 1992. </year>
Reference-contexts: Each technique avoids distraction by ignoring or filtering out 3 competing object states which are unlikely to be the target. Other visual disturbances have other solutions. Changes in ambient lighting have been handled by concentrating on color cues [35, 50], by tracking based on edges <ref> [13, 24, 26] </ref>, or by explicitly modeling illumination parameters [20]. Fast or unpredictable motion requires combinations of faster hardware, full-frame processing [5, 30], or probabilistic dynamics [22]. <p> Fast or unpredictable motion requires combinations of faster hardware, full-frame processing [5, 30], or probabilistic dynamics [22]. Occlusions, where opaque objects intercept the camera's line of sight to the target, can be handled by robust matching <ref> [13, 14, 26, 25] </ref> or by dynamic state prediction [1, 36, 38]. Ante-failure work generally seeks to handle problems one at a time, but some researchers have sought to design systems that are ante-failure robust to many types of visual perturbations simultaneously. <p> this tracker may perform the same function with a subset of the affine transformations. * An SSD-based multi-pattern tracker performs as above, but it tracks successfully if the current image contains a match to any of several stored images. * A 3-D model tracker (such as the ones described in <ref> [10, 13, 26] </ref>) tracks the 3-D pose of an object with respect to the camera. The input configuration set would be the approximate pose of the object with an error margin, and the output set could be a smaller set of poses.
Reference: [14] <author> G. Hager and P.N. Belhumeur. </author> <title> Occlusion insensitive tracking of image regions with changes in geometry and illumination. </title> <type> Technical Report DCS-TR-1122, </type> <institution> Yale University, </institution> <year> 1996. </year>
Reference-contexts: Fast or unpredictable motion requires combinations of faster hardware, full-frame processing [5, 30], or probabilistic dynamics [22]. Occlusions, where opaque objects intercept the camera's line of sight to the target, can be handled by robust matching <ref> [13, 14, 26, 25] </ref> or by dynamic state prediction [1, 36, 38]. Ante-failure work generally seeks to handle problems one at a time, but some researchers have sought to design systems that are ante-failure robust to many types of visual perturbations simultaneously.
Reference: [15] <author> G. Hager and K. Toyama. XVision: </author> <title> A portable substrate for real-time vision applications. </title> <booktitle> In Proc. European Conf. on Computer Vision, </booktitle> <volume> volume 2, </volume> <pages> pages 507-512, </pages> <address> Cambridge, UK, </address> <year> 1996. </year>
Reference-contexts: For instance, distractions objects which are close to the target both in appearance and state can be handled in several ways. One method is to consider only a small set of states surrounding the target <ref> [15, 47] </ref>. This requires some predictability in the target trajectory but eliminates the need to examine the entire image. Another way to handle distraction is through foveation, effectively blurring the image region around the target [6, 37].
Reference: [16] <author> G. Hager and K. Toyama. XVision: </author> <title> Interaction through real-time visual tracking. </title> <booktitle> In CVPR Demo Program, </booktitle> <address> San Francisco, CA, </address> <year> 1996. </year> <month> 28 </month>
Reference-contexts: Recovery never occurs for permanent perturbations where the disturbance prevents target confirmation in any tracking layer. Overall, the system is highly robust to any type of temporary disturbance. The same conclusions were informally drawn from repeated demonstrations to skeptical, well-informed audiences <ref> [16] </ref>. 6.1.2 False Positives The worst error a tracking system can make is to track the wrong object, believing it to be the correct target. As mentioned in Section 5.2.2 such false positives should only occur in an IFA framework when there are twins or when sleight of hand occurs.
Reference: [17] <author> G. Hager and K. Toyama. XVision: </author> <title> A portable substrate for real-time vision applications. </title> <booktitle> Computer Vision and Image Understanding, </booktitle> <year> 1998. </year>
Reference-contexts: It returns an output set whose elements are configurations that correspond to affine transformations of the template which match with the current camera 23 image. It fails if the computed sum-of-squared-difference (SSD) residue is above a given threshold (see <ref> [17] </ref>). The residue of an SSD computation is the actual pixel-by-pixel sum of squared differences.
Reference: [18] <author> G. D. Hager. </author> <title> Calibration-free visual control using projective invariance. </title> <booktitle> In Proc. 5th Int. Conf. Comp. Vision, </booktitle> <pages> pages 1009-1015, </pages> <year> 1995. </year>
Reference-contexts: between the extent of other visual perturbations (extent of occlusion, deviation from object description, etc.) and precision are also handled by IFA in the same manner. 6.2 Other IFA Systems In the past, we have often used 3.5 inch diskettes as a convenient target for various experiments with visual servoing <ref> [18, 40] </ref>. These experiments, while successful in demonstrating hand-eye coordination, were nevertheless extremely sensitive to disturbances.
Reference: [19] <author> G. D. Hager. </author> <title> The "X-Vision" system: A general purpose substrate for real-time vision-based robotics. </title> <booktitle> In Proceedings of the Workshop on Vision for Robots, </booktitle> <pages> pages 56-63, </pages> <year> 1995. </year> <note> Also available as Yale CS-RR-1078. </note>
Reference-contexts: set includes only those configurations of the polygon which would be consistent with the object projecting the tracked edge onto the camera image. * A feature-based polygon tracker tracks polygonal shapes as multiple corners, which are in turn tracked as two intersecting line segments (a rectangle tracker is described in <ref> [19] </ref>). The polygonal tracker will narrow a compact set of input configurations, into an smaller set of configurations which corresponds to the image projection of the polygon together with some allowance for noise.
Reference: [20] <author> G. D. Hager and P.N. Belhumeur. </author> <title> Real-time tracking of image regions with changes in geometry and illumination. </title> <booktitle> In Computer Vision and Patt. Recog., </booktitle> <pages> pages 403-410. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1996. </year>
Reference-contexts: Other visual disturbances have other solutions. Changes in ambient lighting have been handled by concentrating on color cues [35, 50], by tracking based on edges [13, 24, 26], or by explicitly modeling illumination parameters <ref> [20] </ref>. Fast or unpredictable motion requires combinations of faster hardware, full-frame processing [5, 30], or probabilistic dynamics [22]. Occlusions, where opaque objects intercept the camera's line of sight to the target, can be handled by robust matching [13, 14, 26, 25] or by dynamic state prediction [1, 36, 38]. <p> This image, and its spatial derivatives, are used as templates for the SSD trackers at the top two layers. For a more detailed account of how SSD tracking operates, see <ref> [20] </ref>. The system operates exactly as expected it resists failure for minor disturbances and recovers when mistracking occurs. Figure 7 shows face tracking at different layers. (Layer 1) (Layer 3) (Layer 6) selector. Small squares mark areas of the image in which flesh-color is detected.
Reference: [21] <author> E. Huber and D. Kortenkamp. </author> <title> Using stereo vision to pursue moving agents with a mobile robot. </title> <booktitle> In Proc. Int'l Conf. on Robot. and Autom., </booktitle> <pages> pages 2340-2346, </pages> <address> Nagoya, Japan, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: Most of this work focuses on a single means to find and track a target, using various cues: intensity [33], color [35], or motion <ref> [21, 28] </ref>. These methods are efficient, but do not take advantage of the multiple cues which identify real-world objects. Some recent work extends this notion to using more than two stages or multiple cues for tracking.
Reference: [22] <author> M. Isard and A. Blake. </author> <title> Contour tracking by stochastic propagation of conditional density. </title> <booktitle> In Proc. European Conf. on Computer Vision, </booktitle> <pages> pages I:343-356, </pages> <year> 1996. </year>
Reference-contexts: Changes in ambient lighting have been handled by concentrating on color cues [35, 50], by tracking based on edges [13, 24, 26], or by explicitly modeling illumination parameters [20]. Fast or unpredictable motion requires combinations of faster hardware, full-frame processing [5, 30], or probabilistic dynamics <ref> [22] </ref>. Occlusions, where opaque objects intercept the camera's line of sight to the target, can be handled by robust matching [13, 14, 26, 25] or by dynamic state prediction [1, 36, 38]. <p> Ante-failure work generally seeks to handle problems one at a time, but some researchers have sought to design systems that are ante-failure robust to many types of visual perturbations simultaneously. Notable among these are probabilistic methods, which sample and interpolate likelihoods over the state space <ref> [22] </ref>, and sensor fusion techniques, which track based on multiple cues [9, 23, 32, 34, 41]. None of these efforts are error-free. As in other domains, ante-failure methods cannot eliminate failure entirely.
Reference: [23] <author> R.E. Kahn, M.J. Swain, </author> <title> P.N. Prokopowicz, and R.J. Firby. Gesture recognition using Perseus architecture. </title> <booktitle> In Computer Vision and Patt. Recog., </booktitle> <pages> pages 734-741, </pages> <year> 1996. </year>
Reference-contexts: Notable among these are probabilistic methods, which sample and interpolate likelihoods over the state space [22], and sensor fusion techniques, which track based on multiple cues <ref> [9, 23, 32, 34, 41] </ref>. None of these efforts are error-free. As in other domains, ante-failure methods cannot eliminate failure entirely. Most often, the limitations are a reflection not of poor algorithmic design, but rather of the impossibility of perfect vision-based tracking in complex circumstances.
Reference: [24] <author> H. Kass, A. Witkin, and D. Terzopoulos. Snakes: </author> <title> Active contour models. </title> <journal> Int'l Journal of Computer Vision, </journal> <volume> 1 </volume> <pages> 321-331, </pages> <year> 1987. </year>
Reference-contexts: Each technique avoids distraction by ignoring or filtering out 3 competing object states which are unlikely to be the target. Other visual disturbances have other solutions. Changes in ambient lighting have been handled by concentrating on color cues [35, 50], by tracking based on edges <ref> [13, 24, 26] </ref>, or by explicitly modeling illumination parameters [20]. Fast or unpredictable motion requires combinations of faster hardware, full-frame processing [5, 30], or probabilistic dynamics [22].
Reference: [25] <author> A. Kosaka and G. Nakazawa. </author> <title> Vision-based motion tracking of rigid objects using prediction of uncertainties. </title> <booktitle> In Proc. Int'l Conf. on Robot. and Autom., </booktitle> <pages> pages 2637-2644, </pages> <address> Nagoya, Japan, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: Fast or unpredictable motion requires combinations of faster hardware, full-frame processing [5, 30], or probabilistic dynamics [22]. Occlusions, where opaque objects intercept the camera's line of sight to the target, can be handled by robust matching <ref> [13, 14, 26, 25] </ref> or by dynamic state prediction [1, 36, 38]. Ante-failure work generally seeks to handle problems one at a time, but some researchers have sought to design systems that are ante-failure robust to many types of visual perturbations simultaneously.
Reference: [26] <author> D. G. Lowe. </author> <title> Robust model-based motion tracking through the integration of search and estimation. </title> <journal> Int'l Journal of Computer Vision, </journal> <volume> 8(2) </volume> <pages> 113-122, </pages> <year> 1992. </year>
Reference-contexts: Each technique avoids distraction by ignoring or filtering out 3 competing object states which are unlikely to be the target. Other visual disturbances have other solutions. Changes in ambient lighting have been handled by concentrating on color cues [35, 50], by tracking based on edges <ref> [13, 24, 26] </ref>, or by explicitly modeling illumination parameters [20]. Fast or unpredictable motion requires combinations of faster hardware, full-frame processing [5, 30], or probabilistic dynamics [22]. <p> Fast or unpredictable motion requires combinations of faster hardware, full-frame processing [5, 30], or probabilistic dynamics [22]. Occlusions, where opaque objects intercept the camera's line of sight to the target, can be handled by robust matching <ref> [13, 14, 26, 25] </ref> or by dynamic state prediction [1, 36, 38]. Ante-failure work generally seeks to handle problems one at a time, but some researchers have sought to design systems that are ante-failure robust to many types of visual perturbations simultaneously. <p> this tracker may perform the same function with a subset of the affine transformations. * An SSD-based multi-pattern tracker performs as above, but it tracks successfully if the current image contains a match to any of several stored images. * A 3-D model tracker (such as the ones described in <ref> [10, 13, 26] </ref>) tracks the 3-D pose of an object with respect to the camera. The input configuration set would be the approximate pose of the object with an error margin, and the output set could be a smaller set of poses.
Reference: [27] <author> A. Maki, P. Nordlund, and J. Eklundh. </author> <title> A computational model of depth-based attention. </title> <booktitle> In Proc. Int'l Conf. on Patt. </booktitle> <address> Recog., page D9E.1, </address> <year> 1996. </year>
Reference-contexts: Another system is able to detect and track a limited set of vehicles using a three-stage focus of attention scheme [7]. Others, focusing on the information contained in multiple cues focus on fusing different types of image information to identify a unique target <ref> [27, 46] </ref>. IFA differs from prior work in several ways. First, rather than trying to build computational models of biological visual systems [11, 45], we are primarily concerned with synthesizing methods for vision-based tracking which are computationally efficient and robust. <p> Rather, IFA search may recover rotation, scaling, shear, and even full 3D pose information. Third, we pose tracking itself as a flexible multi-stage focus of attention rather than remaining within the standard two-stage paradigm <ref> [27, 31] </ref> or even a fixed k-stage architecture [7, 9, 32]. Lastly, our framework is not restricted to particular targets or specific cue types.
Reference: [28] <author> D. Murray and A. Basu. </author> <title> Motion tracking with an active camera. </title> <journal> IEEE Trans. Patt. Anal. and Mach. Intel., </journal> <volume> 16(5) </volume> <pages> 449-459, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Most of this work focuses on a single means to find and track a target, using various cues: intensity [33], color [35], or motion <ref> [21, 28] </ref>. These methods are efficient, but do not take advantage of the multiple cues which identify real-world objects. Some recent work extends this notion to using more than two stages or multiple cues for tracking.
Reference: [29] <author> U. Neisser. </author> <title> Cognitive Psychology. </title> <address> Appleton-Century-Crofts, New York, </address> <year> 1967. </year>
Reference-contexts: Cognitive science research in focus of attention suggests that biological vision systems are broadly organized into pre-attentive and post-attentive stages: The pre-attentive stage rapidly finds image subregions of interest on which to focus the attention of a post-attentive stage, which examines the attended region more closely <ref> [29, 43, 44, 49] </ref>. Among those interested in vision-based tracking by computer, many have incorporated knowledge gained from cognitive science and built tracking systems that manage two separate algorithms, where the first algorithm rapidly finds relevant candidate regions in an image and the second performs tracking.
Reference: [30] <author> H. K. Nishihara. Teleos AVP. </author> <booktitle> In CVPR Demo Program, </booktitle> <year> 1996. </year>
Reference-contexts: Other visual disturbances have other solutions. Changes in ambient lighting have been handled by concentrating on color cues [35, 50], by tracking based on edges [13, 24, 26], or by explicitly modeling illumination parameters [20]. Fast or unpredictable motion requires combinations of faster hardware, full-frame processing <ref> [5, 30] </ref>, or probabilistic dynamics [22]. Occlusions, where opaque objects intercept the camera's line of sight to the target, can be handled by robust matching [13, 14, 26, 25] or by dynamic state prediction [1, 36, 38].
Reference: [31] <author> P. Nordlund and T. Uhlin. </author> <title> Closing the loop: Detection and pursuit of a moving object by a moving observer. </title> <journal> Image and Vision Computing, </journal> <volume> 14 </volume> <pages> 265-275, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Second, IFA is not restricted to computing positional or directional information about a target, as are many active tracking systems which emphasize the control aspects of keeping an object in view <ref> [2, 8, 31, 33, 46] </ref>. Rather, IFA search may recover rotation, scaling, shear, and even full 3D pose information. Third, we pose tracking itself as a flexible multi-stage focus of attention rather than remaining within the standard two-stage paradigm [27, 31] or even a fixed k-stage architecture [7, 9, 32]. <p> Rather, IFA search may recover rotation, scaling, shear, and even full 3D pose information. Third, we pose tracking itself as a flexible multi-stage focus of attention rather than remaining within the standard two-stage paradigm <ref> [27, 31] </ref> or even a fixed k-stage architecture [7, 9, 32]. Lastly, our framework is not restricted to particular targets or specific cue types.
Reference: [32] <author> N. Oliver, A. Pentland, and F. Berard. LAFTER: </author> <title> Lips and face real time tracker. </title> <booktitle> In Proc. Computer Vision and Patt. </booktitle> <address> Recog., </address> <year> 1997. </year>
Reference-contexts: Notable among these are probabilistic methods, which sample and interpolate likelihoods over the state space [22], and sensor fusion techniques, which track based on multiple cues <ref> [9, 23, 32, 34, 41] </ref>. None of these efforts are error-free. As in other domains, ante-failure methods cannot eliminate failure entirely. Most often, the limitations are a reflection not of poor algorithmic design, but rather of the impossibility of perfect vision-based tracking in complex circumstances. <p> Some recent work extends this notion to using more than two stages or multiple cues for tracking. In face tracking, for example, some systems are able to robustly determine such things as the facial expression of the target in real-time <ref> [9, 32] </ref>. Another system is able to detect and track a limited set of vehicles using a three-stage focus of attention scheme [7]. Others, focusing on the information contained in multiple cues focus on fusing different types of image information to identify a unique target [27, 46]. <p> Rather, IFA search may recover rotation, scaling, shear, and even full 3D pose information. Third, we pose tracking itself as a flexible multi-stage focus of attention rather than remaining within the standard two-stage paradigm [27, 31] or even a fixed k-stage architecture <ref> [7, 9, 32] </ref>. Lastly, our framework is not restricted to particular targets or specific cue types. Techniques such as eye-blink detection [9] or mouth localization [32] are specifically designed for frontal views of faces only and do not generalize for other target types. <p> Lastly, our framework is not restricted to particular targets or specific cue types. Techniques such as eye-blink detection [9] or mouth localization <ref> [32] </ref> are specifically designed for frontal views of faces only and do not generalize for other target types. IFA, in contrast, allows utilization of a variety 4 of algorithms and thus confers robustness to many different target types.
Reference: [33] <author> K. Pahlavan and J.-O. Eklundh. </author> <title> A head-eye system analysis and design. CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> 56 </volume> <pages> 41-56, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Most of this work focuses on a single means to find and track a target, using various cues: intensity <ref> [33] </ref>, color [35], or motion [21, 28]. These methods are efficient, but do not take advantage of the multiple cues which identify real-world objects. Some recent work extends this notion to using more than two stages or multiple cues for tracking. <p> Second, IFA is not restricted to computing positional or directional information about a target, as are many active tracking systems which emphasize the control aspects of keeping an object in view <ref> [2, 8, 31, 33, 46] </ref>. Rather, IFA search may recover rotation, scaling, shear, and even full 3D pose information. Third, we pose tracking itself as a flexible multi-stage focus of attention rather than remaining within the standard two-stage paradigm [27, 31] or even a fixed k-stage architecture [7, 9, 32].
Reference: [34] <author> P. Prokopowicz, M. Swain, and R. Kahn. </author> <title> Task and environment-sensitive tracking. </title> <type> Technical Report 94-05, </type> <institution> University of Chicago, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Notable among these are probabilistic methods, which sample and interpolate likelihoods over the state space [22], and sensor fusion techniques, which track based on multiple cues <ref> [9, 23, 32, 34, 41] </ref>. None of these efforts are error-free. As in other domains, ante-failure methods cannot eliminate failure entirely. Most often, the limitations are a reflection not of poor algorithmic design, but rather of the impossibility of perfect vision-based tracking in complex circumstances.
Reference: [35] <author> C. Rasmussen, K. Toyama, and G. D. Hager. </author> <title> Tracking objects by color alone. </title> <type> Technical Report DCS-TR-1114, </type> <institution> Yale University, </institution> <month> June </month> <year> 1996. </year> <month> 29 </month>
Reference-contexts: Each technique avoids distraction by ignoring or filtering out 3 competing object states which are unlikely to be the target. Other visual disturbances have other solutions. Changes in ambient lighting have been handled by concentrating on color cues <ref> [35, 50] </ref>, by tracking based on edges [13, 24, 26], or by explicitly modeling illumination parameters [20]. Fast or unpredictable motion requires combinations of faster hardware, full-frame processing [5, 30], or probabilistic dynamics [22]. <p> Most of this work focuses on a single means to find and track a target, using various cues: intensity [33], color <ref> [35] </ref>, or motion [21, 28]. These methods are efficient, but do not take advantage of the multiple cues which identify real-world objects. Some recent work extends this notion to using more than two stages or multiple cues for tracking.
Reference: [36] <author> A.A. Rizzi and D. E. Koditschek. </author> <title> Further progress in robot juggling: The spatial two-juggle. </title> <booktitle> In Proc. Int'l Conf. on Robot. and Autom., </booktitle> <pages> pages 919-924, </pages> <year> 1993. </year>
Reference-contexts: Fast or unpredictable motion requires combinations of faster hardware, full-frame processing [5, 30], or probabilistic dynamics [22]. Occlusions, where opaque objects intercept the camera's line of sight to the target, can be handled by robust matching [13, 14, 26, 25] or by dynamic state prediction <ref> [1, 36, 38] </ref>. Ante-failure work generally seeks to handle problems one at a time, but some researchers have sought to design systems that are ante-failure robust to many types of visual perturbations simultaneously.
Reference: [37] <author> D. Terzopoulos and T. F. Rabie. </author> <title> Animat vision: </title> <booktitle> Active vision in artificial animals. In Int'l Conf. on Comp. Vision, </booktitle> <pages> pages 801-808, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: One method is to consider only a small set of states surrounding the target [15, 47]. This requires some predictability in the target trajectory but eliminates the need to examine the entire image. Another way to handle distraction is through foveation, effectively blurring the image region around the target <ref> [6, 37] </ref>. Finally, a tracking system may filter output estimates based on expected motion and noise [1]. Each technique avoids distraction by ignoring or filtering out 3 competing object states which are unlikely to be the target. Other visual disturbances have other solutions.
Reference: [38] <author> D. Terzopoulos and R. Szeliski. </author> <title> Tracking with Kalman snakes. </title> <editor> In A. Blake and A. Yuille, editors, </editor> <title> Active Vision. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1992. </year>
Reference-contexts: Fast or unpredictable motion requires combinations of faster hardware, full-frame processing [5, 30], or probabilistic dynamics [22]. Occlusions, where opaque objects intercept the camera's line of sight to the target, can be handled by robust matching [13, 14, 26, 25] or by dynamic state prediction <ref> [1, 36, 38] </ref>. Ante-failure work generally seeks to handle problems one at a time, but some researchers have sought to design systems that are ante-failure robust to many types of visual perturbations simultaneously.
Reference: [39] <author> K. Toyama and G. Hager. </author> <title> Incremental focus of attention for robust visual tracking. </title> <booktitle> In Computer Vision and Patt. Recog., </booktitle> <pages> pages 189-195, </pages> <year> 1996. </year>
Reference-contexts: The results for disk tracking are qualitatively similar to the results for face tracking <ref> [39] </ref>. The major difference in performance was that more disturbances caused tracking to fail completely. The poorer performance can be explained by the lack of robustness in the high-layer tracking for rectangles. But, as with face tracking, the system was able to rapidly recover from all temporary perturbations.
Reference: [40] <author> K. Toyama, G. Hager, and J. Wang. SERVOMATIC: </author> <title> a modular system for robust positioning using stereo visual servoing. </title> <booktitle> In Proc. Int'l Conf. on Robot. and Autom., </booktitle> <pages> pages 2636-2643, </pages> <year> 1996. </year>
Reference-contexts: between the extent of other visual perturbations (extent of occlusion, deviation from object description, etc.) and precision are also handled by IFA in the same manner. 6.2 Other IFA Systems In the past, we have often used 3.5 inch diskettes as a convenient target for various experiments with visual servoing <ref> [18, 40] </ref>. These experiments, while successful in demonstrating hand-eye coordination, were nevertheless extremely sensitive to disturbances.
Reference: [41] <author> K. Toyama and G. D. Hager. </author> <title> Tracker fusion for robustness in visual feature tracking. </title> <booktitle> In SPIE Int'l Sym. Intel. Sys. and Adv. Manufacturing, volume 2589, </booktitle> <address> Philadelphia, PA, </address> <month> October </month> <year> 1995. </year>
Reference-contexts: Notable among these are probabilistic methods, which sample and interpolate likelihoods over the state space [22], and sensor fusion techniques, which track based on multiple cues <ref> [9, 23, 32, 34, 41] </ref>. None of these efforts are error-free. As in other domains, ante-failure methods cannot eliminate failure entirely. Most often, the limitations are a reflection not of poor algorithmic design, but rather of the impossibility of perfect vision-based tracking in complex circumstances.
Reference: [42] <author> K. Toyama and G. D. Hager. </author> <title> If at first you don't succeed... </title> <booktitle> In Proc. Nat'l Conf. on AI, </booktitle> <address> Providence, RI, </address> <year> 1997. </year>
Reference-contexts: The robust vision-based tracking problem is, therefore, a vision-based tracking subproblem the problem of coping with a complex environment. 3 Previous Work in Robust Tracking The existing literature on robust tracking can be broadly categorized into research that contributes to either ante-failure or post-failure robustness <ref> [42] </ref>. Ante-failure robust systems seek to avoid tracking failure altogether through specialized algorithms that anticipate visual disturbances and attempt to track despite them. Post-failure systems, on the other hand, accept the inevitability of mistracking and are designed to recover from failure once it happens.
Reference: [43] <author> A. Treisman. </author> <booktitle> Preattentive processing in vision. CVGIP: Image Understanding, </booktitle> <volume> 31 </volume> <pages> 156-177, </pages> <year> 1985. </year>
Reference-contexts: Cognitive science research in focus of attention suggests that biological vision systems are broadly organized into pre-attentive and post-attentive stages: The pre-attentive stage rapidly finds image subregions of interest on which to focus the attention of a post-attentive stage, which examines the attended region more closely <ref> [29, 43, 44, 49] </ref>. Among those interested in vision-based tracking by computer, many have incorporated knowledge gained from cognitive science and built tracking systems that manage two separate algorithms, where the first algorithm rapidly finds relevant candidate regions in an image and the second performs tracking.
Reference: [44] <author> J. K. Tsotsos. </author> <title> An inhibitory beam for attentional selection. In Spatial Vision for Humans and Robots. </title> <publisher> Cambridge University Press, </publisher> <year> 1993. </year>
Reference-contexts: Cognitive science research in focus of attention suggests that biological vision systems are broadly organized into pre-attentive and post-attentive stages: The pre-attentive stage rapidly finds image subregions of interest on which to focus the attention of a post-attentive stage, which examines the attended region more closely <ref> [29, 43, 44, 49] </ref>. Among those interested in vision-based tracking by computer, many have incorporated knowledge gained from cognitive science and built tracking systems that manage two separate algorithms, where the first algorithm rapidly finds relevant candidate regions in an image and the second performs tracking.
Reference: [45] <author> J. K. Tsotsos. </author> <title> Towards a computational model of visual attention. </title> <editor> In T. Papathomas, C. Chubb, A. Gorea, and E. Kowler, editors, </editor> <booktitle> Early Vision and Beyond, </booktitle> <pages> pages 207-218. </pages> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: Others, focusing on the information contained in multiple cues focus on fusing different types of image information to identify a unique target [27, 46]. IFA differs from prior work in several ways. First, rather than trying to build computational models of biological visual systems <ref> [11, 45] </ref>, we are primarily concerned with synthesizing methods for vision-based tracking which are computationally efficient and robust.
Reference: [46] <author> T. Uhlin, P. Nordlund, A. Maki, and J.-O. Eklundh. </author> <title> Towards an active visual observer. </title> <booktitle> In Proc. Int'l Conf. on Computer Vision, </booktitle> <pages> pages 679-686, </pages> <address> Cambridge, MA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Another system is able to detect and track a limited set of vehicles using a three-stage focus of attention scheme [7]. Others, focusing on the information contained in multiple cues focus on fusing different types of image information to identify a unique target <ref> [27, 46] </ref>. IFA differs from prior work in several ways. First, rather than trying to build computational models of biological visual systems [11, 45], we are primarily concerned with synthesizing methods for vision-based tracking which are computationally efficient and robust. <p> Second, IFA is not restricted to computing positional or directional information about a target, as are many active tracking systems which emphasize the control aspects of keeping an object in view <ref> [2, 8, 31, 33, 46] </ref>. Rather, IFA search may recover rotation, scaling, shear, and even full 3D pose information. Third, we pose tracking itself as a flexible multi-stage focus of attention rather than remaining within the standard two-stage paradigm [27, 31] or even a fixed k-stage architecture [7, 9, 32].
Reference: [47] <author> M. Vincze. </author> <title> Optimal window size for visual tracking. </title> <booktitle> In Int'l Conference on Patt. </booktitle> <address> Recog., page A91.1, </address> <year> 1996. </year>
Reference-contexts: For instance, distractions objects which are close to the target both in appearance and state can be handled in several ways. One method is to consider only a small set of states surrounding the target <ref> [15, 47] </ref>. This requires some predictability in the target trajectory but eliminates the need to examine the entire image. Another way to handle distraction is through foveation, effectively blurring the image region around the target [6, 37].
Reference: [48] <author> L.E. Wixson and D.H. Ballard. </author> <title> Using intermediate objects to improve the efficiency of visual-search. </title> <booktitle> Int'l J. of Computer Vision, </booktitle> <address> 12(2-3):209-230, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: But, as with face tracking, the system was able to rapidly recover from all temporary perturbations. In another illustration of IFA applied to robotics, we developed a robust doorknob tracking module for use with mobile robots [12]. This particular implementation used an intermediate object <ref> [48] </ref>, where a search for the door takes place to facilitate finding the doorknob itself (characterized as an "edge-rich" object on doors approximately at waist height).
Reference: [49] <author> J. Wolfe. </author> <title> Guided search 2.0: A revised model of visual search. </title> <journal> Psychonomic Bulletin and Review, </journal> <volume> 1(2) </volume> <pages> 202-238, </pages> <year> 1995. </year>
Reference-contexts: Cognitive science research in focus of attention suggests that biological vision systems are broadly organized into pre-attentive and post-attentive stages: The pre-attentive stage rapidly finds image subregions of interest on which to focus the attention of a post-attentive stage, which examines the attended region more closely <ref> [29, 43, 44, 49] </ref>. Among those interested in vision-based tracking by computer, many have incorporated knowledge gained from cognitive science and built tracking systems that manage two separate algorithms, where the first algorithm rapidly finds relevant candidate regions in an image and the second performs tracking.
Reference: [50] <author> C.R. Wren, A. Azarbayejani, T. Darrell, and A. Pentland. Pfinder: </author> <title> Real-time tracking of the human body. </title> <booktitle> In Vismod, </booktitle> <year> 1995. </year> <month> 30 </month>
Reference-contexts: Each technique avoids distraction by ignoring or filtering out 3 competing object states which are unlikely to be the target. Other visual disturbances have other solutions. Changes in ambient lighting have been handled by concentrating on color cues <ref> [35, 50] </ref>, by tracking based on edges [13, 24, 26], or by explicitly modeling illumination parameters [20]. Fast or unpredictable motion requires combinations of faster hardware, full-frame processing [5, 30], or probabilistic dynamics [22].
References-found: 50

