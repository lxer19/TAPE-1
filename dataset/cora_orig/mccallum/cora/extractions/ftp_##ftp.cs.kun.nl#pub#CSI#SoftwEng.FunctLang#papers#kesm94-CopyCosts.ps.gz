URL: ftp://ftp.cs.kun.nl/pub/CSI/SoftwEng.FunctLang/papers/kesm94-CopyCosts.ps.gz
Refering-URL: http://www.cs.kun.nl/~clean/Clean.Papers.html
Root-URL: 
Title: Reducing Graph Copying Costs Time to Wrap it Up  
Author: Marco Kesseler 
Keyword: parallel functional programming, distributed memory, graph copying, graph conversions, delays  
Address: Toernooiveld 1, 6525 ED Nijmegen, The Netherlands  
Affiliation: Faculty of Mathematics and Computer Science, University of Nijmegen,  
Abstract: Graph copying is used in parallel implementations of functional languages on architectures with distributed memory. This paper will explore the costs of graph copying in detail. It will make clear that these costs can form a bottleneck for a class of serious parallel programs. This not only comprises practical divide and conquer style programs - of which parallel matrix multiplication is an example -, but also programs that use pipelines, such as the sieve of Erathostenes. One can observe that copying costs vary widely for different data structures. We will show how arrays can be used to reduce copying costs considerably. This resulted in significant speedups for the examples above. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Plasmeijer M.J., Eekelen M.C.J.D. </author> <title> van (1993). Functional Programming and Parallel Graph Rewriting. </title> <publisher> Addison Wesley, </publisher> <year> 1993. </year>
Reference-contexts: 1. Introduction Graph rewriting systems have been studied as a computational model for functional languages and their implementation. The lazy functional language Concurrent Clean, for instance, is based on such a graph rewriting system and the Clean compiler is known for the efficient code it produces <ref> [1, 2, 3, 4] </ref>. Graph rewriting systems represent a program as a set of rewrite rules. Starting with an initial graph, rewriting (reduction) takes place according to these rules. At any moment in time there will be multiple redexes, and therefore a reduction strategy is needed.
Reference: [2] <author> Ncker E.G.J.M.H., Smetsers J.E.W., Eekelen M.C.J.D. van, Plasmeijer M.J., </author> <year> (1991). </year> <title> 'Concurrent Clean', </title> <booktitle> In proceedings of Parallel Architectures and Languages Europe (PARLE'91). Springer LNCS 505, </booktitle> <volume> Vol. II, </volume> <pages> page 202-219. </pages>
Reference-contexts: 1. Introduction Graph rewriting systems have been studied as a computational model for functional languages and their implementation. The lazy functional language Concurrent Clean, for instance, is based on such a graph rewriting system and the Clean compiler is known for the efficient code it produces <ref> [1, 2, 3, 4] </ref>. Graph rewriting systems represent a program as a set of rewrite rules. Starting with an initial graph, rewriting (reduction) takes place according to these rules. At any moment in time there will be multiple redexes, and therefore a reduction strategy is needed. <p> Each assumes the first number in the stream to be a prime number. It will first deliver this prime and next remove all numbers from the stream that are divisible by it. The resulting stream is passed to the next filter. a generator filter 2 filter 3 filter 5 <ref> [2, 3, 5, ...] </ref> Erathostenes The definitions for the most important functions are given below. We will start with a short explanation of the Concurrent Clean syntax. For clarity, notations are in bold face. The annotations I-, -P- and -P AT proc- are all process annotations.
Reference: [3] <author> Eekelen M.C.J.D. van, Plasmeijer M.J., Smetsers J.E.W., </author> <year> (1990). </year> <booktitle> 'Parallel Graph Rewriting on Loosely Coupled Machine Architectures' In proceedings of the workshop on CTRS'90. </booktitle> <address> Montreal Canada. </address>
Reference-contexts: 1. Introduction Graph rewriting systems have been studied as a computational model for functional languages and their implementation. The lazy functional language Concurrent Clean, for instance, is based on such a graph rewriting system and the Clean compiler is known for the efficient code it produces <ref> [1, 2, 3, 4] </ref>. Graph rewriting systems represent a program as a set of rewrite rules. Starting with an initial graph, rewriting (reduction) takes place according to these rules. At any moment in time there will be multiple redexes, and therefore a reduction strategy is needed. <p> Each assumes the first number in the stream to be a prime number. It will first deliver this prime and next remove all numbers from the stream that are divisible by it. The resulting stream is passed to the next filter. a generator filter 2 filter 3 filter 5 <ref> [2, 3, 5, ...] </ref> Erathostenes The definitions for the most important functions are given below. We will start with a short explanation of the Concurrent Clean syntax. For clarity, notations are in bold face. The annotations I-, -P- and -P AT proc- are all process annotations.
Reference: [4] <author> Smetsers J.E.W., Ncker E.G.J.M.H., Van Groningen J.C., Plasmeijer M.J. </author> <year> (1991). </year> <title> Generating Efficient Code for Lazy Functional Languages. </title> <booktitle> In proceedings of the Fifth International Conference on Functional Programming Languages and Computer Architecture (FPCA91), </booktitle> <address> Cambridge, U.S.A., </address> <publisher> Springer LNCS 523, </publisher> <pages> page 592-617. </pages>
Reference-contexts: 1. Introduction Graph rewriting systems have been studied as a computational model for functional languages and their implementation. The lazy functional language Concurrent Clean, for instance, is based on such a graph rewriting system and the Clean compiler is known for the efficient code it produces <ref> [1, 2, 3, 4] </ref>. Graph rewriting systems represent a program as a set of rewrite rules. Starting with an initial graph, rewriting (reduction) takes place according to these rules. At any moment in time there will be multiple redexes, and therefore a reduction strategy is needed.
Reference: [5] <author> L. Augustsson, T. </author> <title> Johnsson (1989). Parallel Graph Reduction with the &lt;u,G&gt; - machine. </title> <booktitle> In proceedings of the Fourth International Conference on Functional Programming Languages and Computer Architecture (FPCA 89), </booktitle> <pages> page 202-213, </pages> <address> London, U.K. </address> <note> ACM 1989. </note>
Reference-contexts: In the first place, of all implementations, there are not many aimed at distributed memory architectures. Most recent research in parallel functional programming focuses at implementations for (virtual) shared memory architectures (&lt; u,G&gt;, AMPGR, GAML, GRIP, Flagship, and HyperM) <ref> [5, 6, 7, 8, 9, 10, 11, 12, 13] </ref>. And secondly, the exact costs of copying have not been made explicit for distributed memory implementations. Sometimes the use of an interpreter causes uncertainty about realistic copying costs (PAM, pRED + ) [14, 15]. <p> Each assumes the first number in the stream to be a prime number. It will first deliver this prime and next remove all numbers from the stream that are divisible by it. The resulting stream is passed to the next filter. a generator filter 2 filter 3 filter 5 <ref> [2, 3, 5, ...] </ref> Erathostenes The definitions for the most important functions are given below. We will start with a short explanation of the Concurrent Clean syntax. For clarity, notations are in bold face. The annotations I-, -P- and -P AT proc- are all process annotations.
Reference: [6] <author> L. </author> <title> George (1989). An Abstract Machine for Parallel Graph Reduction. </title> <booktitle> In proceedings of the Fourth International Conference on Functional Programming Languages and Computer Architecture (FPCA 89), </booktitle> <pages> page 214-229, </pages> <address> London, U.K. </address> <note> ACM 1989. </note>
Reference-contexts: In the first place, of all implementations, there are not many aimed at distributed memory architectures. Most recent research in parallel functional programming focuses at implementations for (virtual) shared memory architectures (&lt; u,G&gt;, AMPGR, GAML, GRIP, Flagship, and HyperM) <ref> [5, 6, 7, 8, 9, 10, 11, 12, 13] </ref>. And secondly, the exact costs of copying have not been made explicit for distributed memory implementations. Sometimes the use of an interpreter causes uncertainty about realistic copying costs (PAM, pRED + ) [14, 15].
Reference: [7] <author> L. </author> <title> Maranget (1991). GAML: a Parallel Implementation of Lazy ML. In proceedings of the Fifth International Conference on Functional Programming Languages and C om p u t e r A r c h i te c tu r e ( F P C A 91 ) , Cambridge, </title> <address> U.S.A., </address> <publisher> Springer LNCS 523, </publisher> <pages> page 102-123. </pages>
Reference-contexts: In the first place, of all implementations, there are not many aimed at distributed memory architectures. Most recent research in parallel functional programming focuses at implementations for (virtual) shared memory architectures (&lt; u,G&gt;, AMPGR, GAML, GRIP, Flagship, and HyperM) <ref> [5, 6, 7, 8, 9, 10, 11, 12, 13] </ref>. And secondly, the exact costs of copying have not been made explicit for distributed memory implementations. Sometimes the use of an interpreter causes uncertainty about realistic copying costs (PAM, pRED + ) [14, 15].
Reference: [8] <author> S. L. Peyton Jones, C. Clack, J. Salkild, M. </author> <month> Hardie </month> <year> (1987). </year> <title> GRIP - a High Performance Architecture for Parallel Graph Reduction. </title> <booktitle> In Proceedings of the Third International Conference on Functional Programming Languages and Computer Architecture (FPCA 87). </booktitle> <address> Portland, U.S.A., </address> <publisher> Springer LNCS 274, </publisher> <pages> page 98-112. </pages>
Reference-contexts: In the first place, of all implementations, there are not many aimed at distributed memory architectures. Most recent research in parallel functional programming focuses at implementations for (virtual) shared memory architectures (&lt; u,G&gt;, AMPGR, GAML, GRIP, Flagship, and HyperM) <ref> [5, 6, 7, 8, 9, 10, 11, 12, 13] </ref>. And secondly, the exact costs of copying have not been made explicit for distributed memory implementations. Sometimes the use of an interpreter causes uncertainty about realistic copying costs (PAM, pRED + ) [14, 15].
Reference: [9] <author> S. L. Peyton Jones, C. Clack, J. </author> <title> Salkild (1989). High Performance Parallel Graph Reduction. </title> <booktitle> In Proceedings of Parallel Architecture and Languages Europe (PARLE 89), </booktitle> <address> Eindhoven, the Netherlands, </address> <publisher> Springer LNCS 365/366, </publisher> <pages> page 193-206. </pages>
Reference-contexts: In the first place, of all implementations, there are not many aimed at distributed memory architectures. Most recent research in parallel functional programming focuses at implementations for (virtual) shared memory architectures (&lt; u,G&gt;, AMPGR, GAML, GRIP, Flagship, and HyperM) <ref> [5, 6, 7, 8, 9, 10, 11, 12, 13] </ref>. And secondly, the exact costs of copying have not been made explicit for distributed memory implementations. Sometimes the use of an interpreter causes uncertainty about realistic copying costs (PAM, pRED + ) [14, 15].
Reference: [10] <author> P. Watson and I. </author> <title> Watson (1986). Graph Reduction in a Parallel Virtual Memory Environment. In Graph Reduction, </title> <address> Santa F, New Mexico. </address> <publisher> Springer LNCS 279, </publisher> <pages> page 265-214. </pages>
Reference-contexts: In the first place, of all implementations, there are not many aimed at distributed memory architectures. Most recent research in parallel functional programming focuses at implementations for (virtual) shared memory architectures (&lt; u,G&gt;, AMPGR, GAML, GRIP, Flagship, and HyperM) <ref> [5, 6, 7, 8, 9, 10, 11, 12, 13] </ref>. And secondly, the exact costs of copying have not been made explicit for distributed memory implementations. Sometimes the use of an interpreter causes uncertainty about realistic copying costs (PAM, pRED + ) [14, 15].
Reference: [11] <author> P. Watson and I. </author> <title> Watson (1987). </title> <booktitle> Evaluation of Functional Programs on the Flagship Machine. In proceedings of the third International Conference on Functional Programming Languages and Computer Architecture (FPCA 87). </booktitle> <address> Portland, U.S.A., </address> <publisher> Springer LNCS 274, </publisher> <pages> page 80-97. </pages>
Reference-contexts: In the first place, of all implementations, there are not many aimed at distributed memory architectures. Most recent research in parallel functional programming focuses at implementations for (virtual) shared memory architectures (&lt; u,G&gt;, AMPGR, GAML, GRIP, Flagship, and HyperM) <ref> [5, 6, 7, 8, 9, 10, 11, 12, 13] </ref>. And secondly, the exact costs of copying have not been made explicit for distributed memory implementations. Sometimes the use of an interpreter causes uncertainty about realistic copying costs (PAM, pRED + ) [14, 15].
Reference: [12] <author> I. Watson, V. Woods, P. Watson, R. Banach, M. Greenberg, J. </author> <month> Sargeant </month> <year> (1988). </year> <title> Flagship: A Parallel Architecture for Declarative Programming, In 15th IEEE/ACM synp. Computer Architecture. Honolulu, </title> <journal> Hawaii. SIGARCH newsletter, </journal> <volume> 16(2), </volume> <year> 1988, </year> <pages> pages 124-130. </pages>
Reference-contexts: In the first place, of all implementations, there are not many aimed at distributed memory architectures. Most recent research in parallel functional programming focuses at implementations for (virtual) shared memory architectures (&lt; u,G&gt;, AMPGR, GAML, GRIP, Flagship, and HyperM) <ref> [5, 6, 7, 8, 9, 10, 11, 12, 13] </ref>. And secondly, the exact costs of copying have not been made explicit for distributed memory implementations. Sometimes the use of an interpreter causes uncertainty about realistic copying costs (PAM, pRED + ) [14, 15].
Reference: [13] <author> H.P. Barendregt, M. Beemster, P. H. Hartel, L. O. Hertzberger, R. F. H. Hofman, K.G. Langendoen, L. L. Li, R. Milikowski, J. C. </author> <title> Mulder, W.G. Vree (1992). Programming Clustered Reduction Machines. </title> <type> Technical report CS-92-05, </type> <institution> Dept. of. Comp. Sys, Univ. of Amsterdam, </institution> <year> 1992. </year>
Reference-contexts: In the first place, of all implementations, there are not many aimed at distributed memory architectures. Most recent research in parallel functional programming focuses at implementations for (virtual) shared memory architectures (&lt; u,G&gt;, AMPGR, GAML, GRIP, Flagship, and HyperM) <ref> [5, 6, 7, 8, 9, 10, 11, 12, 13] </ref>. And secondly, the exact costs of copying have not been made explicit for distributed memory implementations. Sometimes the use of an interpreter causes uncertainty about realistic copying costs (PAM, pRED + ) [14, 15].
Reference: [14] <author> R. Loogen, H. Kuchen, K. Indermark, W. </author> <title> Damm (1989). Distributed Implementation of Programmed Graph Reduction. </title> <booktitle> In proceedings Parallel Architectures and Languages Europe (PARLE 89), </booktitle> <address> Eindhoven, the Netherlands, </address> <publisher> Springer LNCS 365/366, </publisher> <pages> pages 136-157. </pages>
Reference-contexts: And secondly, the exact costs of copying have not been made explicit for distributed memory implementations. Sometimes the use of an interpreter causes uncertainty about realistic copying costs (PAM, pRED + ) <ref> [14, 15] </ref>. In other cases only some simple divide-and-conquer programs have been tested that are not very conclusive with respect to overall communication overheads (HDG, SkelML) [16,17]. In particular pipelines of processes are not considered at all. This paper will explore the costs of graph copying in detail.
Reference: [15] <author> T. Blck, A. Held, W. Kluge, S. Pantke, C. Rathsack, S. Scholz, </author> <title> R Schrder (1993). Preliminary Experience with a pRED + Implementation on an nCUBE/2 System. </title> <booktitle> In proceedings of the fifth International Workshop on Implementation of Functional Languages, </booktitle> <institution> Nijmegen, the Netherlands. </institution> <type> Technical report 93-21, </type> <institution> Faculty of Mathematics and Computer Science, Univ. of Nijmegen, </institution> <year> 1993. </year>
Reference-contexts: And secondly, the exact costs of copying have not been made explicit for distributed memory implementations. Sometimes the use of an interpreter causes uncertainty about realistic copying costs (PAM, pRED + ) <ref> [14, 15] </ref>. In other cases only some simple divide-and-conquer programs have been tested that are not very conclusive with respect to overall communication overheads (HDG, SkelML) [16,17]. In particular pipelines of processes are not considered at all. This paper will explore the costs of graph copying in detail.
Reference: [16] <author> H. Kingdon, D. R. Lester, G.L. </author> <title> Burn (1991). The HDG-machine: a Highly Distributed Graph-Reducer for a Transputer Network. </title> <journal> The Computer Journal, </journal> <volume> 34(4): </volume> <pages> 290-301, </pages> <year> 1991. </year>
Reference: [17] <author> T. A. </author> <month> Bratvold </month> <year> (1993). </year> <title> A Skeleton-Based Parallelising Compiler for ML. </title> <booktitle> In proceedings of the fifth International Workshop on Implementation of Functional Languages, </booktitle> <institution> Nijmegen, the Netherlands. </institution> <type> Technical report 93-21, </type> <institution> Faculty of Mathematics and Computer Science, Univ. of Nijmegen, </institution> <year> 1993. </year>
Reference: [18] <author> H. Kuchen, R. Plasmeijer, H. </author> <month> Stoltze </month> <year> (1993). </year> <title> Efficient Distributed Memory Implementation of Functional Languages. </title> <booktitle> In proceedings of the fifth International Workshop on Implementation of Functional Languages, </booktitle> <institution> Nijmegen, the Netherlands. </institution> <type> Technical report 93-21, </type> <institution> Faculty of Mathematics and Computer Science, Univ. of Nijmegen, </institution> <year> 1993. </year>
Reference-contexts: We will show how one can avoid problems by using appropriate data structures. These can either be used implicitly, via skeletons, or explicitly. Considerable speedups can be obtained this way. Our work has some relation with that presented in <ref> [18] </ref>, where a set of skeletons on special data structures has been proposed to exploit data-parallelism. This is based on the assumption that some data structures are cheaper than others for certain problems. The effect of different data structures on efficiency of communication is not examined in detail. <p> In contrast, we will not limit ourselves to data parallelism in any way, nor will we present a set of skeletons. We will not examine algorithmic properties of data structures, such as the ability to efficiently access elements in a random way (as in <ref> [18] </ref>). Instead, we will focus on efficiency of communication alone and explain why certain data structures are cheaper than others in this respect. We have structured this paper as follows. The first section will point out two directions of interest for reducing copying costs.
Reference: [19] <author> S. Smetsers, E. Barendsen, M. van Eekelen, R. </author> <title> Plasmeijer (1993). Guaranteeing Safe Destructive Updates through a Type System with Uniqueness Information for Graphs. </title> <booktitle> In Graph Transformations in Computer Science. </booktitle> <address> Dagstuhl Castle, Germany, </address> <publisher> Springer LNCS 776, </publisher> <pages> pages 358-379. </pages>
References-found: 19

