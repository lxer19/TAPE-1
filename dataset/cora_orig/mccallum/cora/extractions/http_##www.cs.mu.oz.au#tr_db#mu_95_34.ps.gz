URL: http://www.cs.mu.oz.au/tr_db/mu_95_34.ps.gz
Refering-URL: http://www.cs.mu.oz.au/tr_db/TR.html
Root-URL: 
Title: An Agent-Based Approach for Robot Vision System  
Author: Tak Keung CHENG Leslie KITCHEN Zhi-Qiang LIU James COOPER 
Pubnum: Technical Report 95/34  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> J.(Y.) Aloimonos, I. Weiss, and A. Bandyopad-hyay. </author> <title> Active vision. </title> <journal> International Journal of Computer Vision, </journal> <volume> 1 </volume> <pages> 333-356, </pages> <year> 1988. </year>
Reference-contexts: This approach has similarities to the "active vision" paradigm (see, for example, <ref> [1, 2, 3, 11] </ref>), but is wider in its scope. By "real-time", we mean that the outputs of visual processing must be obtained in a timely fashion to be useful. This is quite different from merely fast processing, or "frame-rate" processing.
Reference: [2] <author> R. </author> <title> Bajcsy. Active perception. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 76 </volume> <pages> 996-1005, </pages> <year> 1988. </year>
Reference-contexts: This approach has similarities to the "active vision" paradigm (see, for example, <ref> [1, 2, 3, 11] </ref>), but is wider in its scope. By "real-time", we mean that the outputs of visual processing must be obtained in a timely fashion to be useful. This is quite different from merely fast processing, or "frame-rate" processing.
Reference: [3] <author> D.H. Ballard. </author> <title> Animate vision. </title> <journal> Artificial Intelligence, </journal> <volume> 48 </volume> <pages> 57-86, </pages> <year> 1991. </year>
Reference-contexts: This approach has similarities to the "active vision" paradigm (see, for example, <ref> [1, 2, 3, 11] </ref>), but is wider in its scope. By "real-time", we mean that the outputs of visual processing must be obtained in a timely fashion to be useful. This is quite different from merely fast processing, or "frame-rate" processing.
Reference: [4] <author> Tak-Keung Cheng and Leslie Kitchen. </author> <title> Preliminary results on real-time 3D feature-based tracker. In Digital Image Computing: Techniques and Applications, </title> <address> DICTA-93, Sydney, Australia, </address> <month> December </month> <year> 1993. </year> <journal> Australian Pattern Recognition Society. </journal> <note> Longer version available as TR 93/30, </note> <institution> Computer Science Department, University of Melbourne. </institution>
Reference-contexts: An earlier version of it was described in <ref> [4] </ref>.
Reference: [5] <author> James Cooper. </author> <title> Real-Time Task-Directed Robot Vision. </title> <type> Ph.D. thesis, </type> <institution> The University of Western Australia, </institution> <year> 1992. </year>
Reference-contexts: Because of space limitations, many important technical details must be omitted. 2 Real-Time Task-Directed Vision Our approach, "real-time task-directed robot vision" <ref> [6, 9, 5] </ref>, is based on the principle that a robot must interact with its environment in performing its assigned tasks. This approach has similarities to the "active vision" paradigm (see, for example, [1, 2, 3, 11]), but is wider in its scope.
Reference: [6] <author> James Cooper and Leslie Kitchen. </author> <title> A proposal for real-time, task-directed robot vision. </title> <type> Technical Report 89/10, </type> <institution> Computer Science Department, The University of Western Australia, Nedlands, </institution> <address> W.A. 6009, Australia, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Because of space limitations, many important technical details must be omitted. 2 Real-Time Task-Directed Vision Our approach, "real-time task-directed robot vision" <ref> [6, 9, 5] </ref>, is based on the principle that a robot must interact with its environment in performing its assigned tasks. This approach has similarities to the "active vision" paradigm (see, for example, [1, 2, 3, 11]), but is wider in its scope.
Reference: [7] <author> James Cooper and Leslie Kitchen. </author> <title> Multi-agent segmentation for real-time task-directed vision. </title> <booktitle> In Fourth Australian Joint Artificial Intelligence Conference, </booktitle> <pages> pages 729-739, </pages> <address> Perth, </address> <institution> Western Aus-tralia, </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: But this is not a crucial issue for our current research, except that our architecture is consciously designed to permit such fast implementations. 4 3D Feature-Based Tracker (3DFBT) The 3DFBT builds on previous work in real-time edge detection, region tracking [8], and 2D feature tracking <ref> [7] </ref>. It is the most complex agent-based system we have built to date. Its goal is to track and segment moving 3D objects in real time, using input from a stereo pair of video cameras. The system is made up of six agents, as shown in Figure 1.
Reference: [8] <author> James Cooper and Leslie Kitchen. </author> <title> A region-based object tracker. </title> <booktitle> In Australian Robot Association Third National Conference on Robotics, </booktitle> <pages> pages 154-164, </pages> <address> Melbourne, Australia, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: But this is not a crucial issue for our current research, except that our architecture is consciously designed to permit such fast implementations. 4 3D Feature-Based Tracker (3DFBT) The 3DFBT builds on previous work in real-time edge detection, region tracking <ref> [8] </ref>, and 2D feature tracking [7]. It is the most complex agent-based system we have built to date. Its goal is to track and segment moving 3D objects in real time, using input from a stereo pair of video cameras.
Reference: [9] <author> James Cooper and Leslie Kitchen. </author> <title> Issues, architectures and techniques in real-time vision. </title> <booktitle> In Proc. Conf. on AI, Simulation and Planning in High Autonomy Systems, </booktitle> <pages> pages 218-224, </pages> <address> Perth, Western Australia, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Because of space limitations, many important technical details must be omitted. 2 Real-Time Task-Directed Vision Our approach, "real-time task-directed robot vision" <ref> [6, 9, 5] </ref>, is based on the principle that a robot must interact with its environment in performing its assigned tasks. This approach has similarities to the "active vision" paradigm (see, for example, [1, 2, 3, 11]), but is wider in its scope.
Reference: [10] <author> James Cooper, Svetha Venkatesh, and Leslie Kitchen. </author> <title> Early jump-out corner detectors. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-15(8):823-828, </volume> <month> August </month> <year> 1993. </year>
Reference-contexts: The feature points are obtained using a special fast corner-detector <ref> [10] </ref>. These features are passed to the Feature Matching Agent for determination of the feature correspondences. Each corresponding pair of 2D features determines a 3D feature, whose 3D location will be calculated (by the Converting Agent) and stored in the database. After this, the system is ready for tracking. <p> It then projects the 3D location into the two 2D image positions in the stereo views. Image matches are computed, using the Early Jump-Out method <ref> [10] </ref>, in a small region near each predicted position. 4.4 Feature Matching Agent This agent finds the point correspondences between left and right views. The correspondence matching method we use has similarities to that in [12]. Each matched pair of 2D feature points gives one 3D feature.
Reference: [11] <author> E.D. Dickmanns and V. Graefe. </author> <title> Dynamic monocular machine vision. </title> <journal> Machine Vision Applications, </journal> <volume> 1 </volume> <pages> 223-240, </pages> <year> 1988. </year>
Reference-contexts: This approach has similarities to the "active vision" paradigm (see, for example, <ref> [1, 2, 3, 11] </ref>), but is wider in its scope. By "real-time", we mean that the outputs of visual processing must be obtained in a timely fashion to be useful. This is quite different from merely fast processing, or "frame-rate" processing.
Reference: [12] <author> C.-Y. Tang, L. Kitchen, Y.-P. Hung, and Z. Chen. </author> <title> Visual tracking of 3D motion using stereo. </title> <booktitle> In Computer Vision, Graphics and Image Processing Workshop, </booktitle> <address> Taiwan, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Image matches are computed, using the Early Jump-Out method [10], in a small region near each predicted position. 4.4 Feature Matching Agent This agent finds the point correspondences between left and right views. The correspondence matching method we use has similarities to that in <ref> [12] </ref>. Each matched pair of 2D feature points gives one 3D feature. From the two sets of camera parameters, depth and epipolar constraints are used to delimit the range of possible matching positions for a feature point. Within these limits, we use template matching to find matching gfeatures.
Reference: [13] <author> Roger Y. Tsai. </author> <title> A versatile camera calibration technique for high-accuracy 3D machine vision metrology using off-the-shelf TV cameras and lens. </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> RA-3(4):323-344, </volume> <month> August </month> <year> 1987. </year>
Reference-contexts: We use a simple pinhole camera model, which is adequate for our purposes. From the known 3D positions of control points on a special calibration object, and their measured 2D projections, this agent uses either Tsai's method <ref> [13] </ref> or non-linear optimization to search for a set of camera parameters that gives minimum sum of squared errors in the 2D image projections. 4.2 Converting Agent This agent obtains the 2D image positions of each feature in both views from the Stereo Feature Agent. <p> Images 256 fi 256 pixels in size with 256 grey levels were used. Tsai's two stage method <ref> [13] </ref> was used to calibrate a stereo pair of cameras pointing downward to the tracked objects. The error for calibration is quite large, because our measurement error for control points is around 1cm. However this does not seriously affect the tracking performance. In this experiment, there were two objects.
References-found: 13

