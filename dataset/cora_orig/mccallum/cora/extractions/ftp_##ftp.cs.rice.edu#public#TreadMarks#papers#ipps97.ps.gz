URL: ftp://ftp.cs.rice.edu/public/TreadMarks/papers/ipps97.ps.gz
Refering-URL: http://www.cs.rice.edu:80/~hhl/
Root-URL: 
Email: falc, hhl, willyg@cs.rice.edu  sandhya@cs.rochester.edu  
Title: Evaluating the Performance of Software Distributed Shared Memory as a Target for Parallelizing Compilers  
Author: Alan L. Cox Sandhya Dwarkadas Honghui Lu and Willy Zwaenepoel 
Address: Houston, TX 77005-1892  Rochester, NY14627-0226  
Affiliation: Rice University  University of Rochester  
Abstract: In this paper, we evaluate the use of software distributed shared memory (DSM) on a message passing machine as the target for a parallelizing compiler. We compare this approach to compiler-generated message passing, hand-coded software DSM, and hand-coded message passing. For this comparison, we use six applications: four that are regular and two that are irregular. Our results are gathered on an 8-node IBM SP/2 using the TreadMarks software DSM system. We use the APR shared-memory (SPF) compiler to generate the shared memory programs, and the APR XHPF compiler to generate message passing programs. The hand-coded message passing programs run with the IBM PVMe optimized message passing library. On the regular programs, both the compiler-generated and the hand-coded message passing outperform the SPF/TreadMarks combination: the compiler-generated message passing by 5.5% to 40%, and the hand-coded message passing by 7.5% to 49%. On the irregular programs, the SPF/TreadMarks combination outperforms the compiler-generated message passing by 38% and 89%, and only slightly underperforms the hand-coded message passing, differing by 4.4% and 16%. We also identify the factors that account for the performance differences, estimate their relative importance, and describe methods to improve the performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. P. Amarasinghe, J. M. Anderson, M. S. Lam, and C. W. Tseng. </author> <title> The SUIF compiler for scalable parallel machines. </title> <booktitle> In Proceedings of the 7th SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: Keleher and Tseng [10] perform a similar study which also compares the performance of compiler-generated DSM programs with compiler-generated message passing programs. Instead of using commercial Fortran compilers to compile all the programs, they use the Stanford SUIF <ref> [1] </ref> parallelizing compiler version 1.0 to generate parallel C programs for the DSM system, and the commercial IBM HPF or DEC HPF compilers to generate the parallel Fortran programs in message passing. Using a different set of applications, they arrive at results similar to ours. 8.
Reference: [2] <author> C. Amza, A. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Ra-jamony, W. Yu, and W. Zwaenepoel. TreadMarks: </author> <title> Shared memory computing on networks of workstations. </title> <journal> IEEE Computer, </journal> <volume> 29(2) </volume> <pages> 18-28, </pages> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: 1. Introduction This paper evaluates the potential for using software distributed shared memory (DSM) <ref> [2, 6, 11] </ref> as a target for a par-allelizing compiler on a message passing machine. We compare this approach with the more common method whereby the compiler directly targets the underlying message passing system (e.g., [9]). <p> We include regular applications in our application suite, because we want to investigate the general applicability of our approach. Our experimental environment is an 8-node IBM SP/2, on which we use the TreadMarks DSM system <ref> [2] </ref> to provide shared memory. We use 6 applications: Jacobi, Shallow, Modified Gramm-Schmidt (MGS), 3-D FFT, IGrid, and Non-Bonded Force (NBF). The first four have regular access patterns, while the latter two are irregular. <p> The compiler allocates in shared memory all the scalars or arrays that are accessed in parallel loops, regardless of whether two processors will access the same storage location or not. Shared arrays are padded to page boundaries in order to reduce false sharing. 2.2. TreadMarks TreadMarks <ref> [2] </ref> is a user-level software DSM system that runs on most Unix platforms. It provides a global shared address space on top of physically distributed memory. The parallel processors synchronize via primitives similar to those used in hardware shared memory machines: barriers and mutex locks.
Reference: [3] <author> Applied Parallel Research, Inc. </author> <title> FORGE High Performance Fortran User's Guide, </title> <note> version 2.0 edition. </note>
Reference-contexts: We use 6 applications: Jacobi, Shallow, Modified Gramm-Schmidt (MGS), 3-D FFT, IGrid, and Non-Bonded Force (NBF). The first four have regular access patterns, while the latter two are irregular. We use the APR Forge XHPF compiler <ref> [3] </ref> to generate message passing code, and the APR Forge SPF compiler [4] to generate shared memory code. We present the performance of the compiler-generated message passing and shared memory programs. In addition, we present the performance of hand-coded message passing and shared memory programs for the same applications. <p> This optimization reduces the number of messages from 8 fi (n 1) to 2 fi (n 1), and has a significant effect on execution time. All results in this paper are obtained with this improved interface. 2.4. Forge XHPF Forge XHPF is a parallelizing compiler for High Performance Fortran <ref> [3] </ref> on distributed memory multiprocessor systems. It transforms a sequential Fortran program annotated with subset HPF data decomposition directives and Fortran 90 array syntax into a SPMD (Single Program Multiple Data) parallelized Fortran 77 program.
Reference: [4] <institution> Applied Parallel Research, Inc. </institution> <note> FORGE Shared Memory Parallelizer User's Guide, version 2.0 edition. </note>
Reference-contexts: We use 6 applications: Jacobi, Shallow, Modified Gramm-Schmidt (MGS), 3-D FFT, IGrid, and Non-Bonded Force (NBF). The first four have regular access patterns, while the latter two are irregular. We use the APR Forge XHPF compiler [3] to generate message passing code, and the APR Forge SPF compiler <ref> [4] </ref> to generate shared memory code. We present the performance of the compiler-generated message passing and shared memory programs. In addition, we present the performance of hand-coded message passing and shared memory programs for the same applications.
Reference: [5] <author> D. Bailey, J. Barton, T. Lasinski, and H. Simon. </author> <title> The NAS parallel benchmarks. </title> <type> Technical Report 103863, </type> <institution> NASA, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: Jacobi is an iterative method for solving partial differential equations. Shallow is the shallow water benchmark from the National Center for Atmospheric Research. MGS implements a Modified Gramm-Schmidt algorithm for computing an orthonormal basis. 3-D FFT, from the NAS <ref> [5] </ref> benchmark suite, numerically solves a partial differential equation using three dimensional forward and inverse FFT's. IGrid is a 9-point stencil computation in which the neighbor elements are accessed indirectly through a mapping established at run-time. The NBF (Non-Bonded Force) program is the kernel of a molecular dynamics simulation. <p> Results of Hand Optimizations We hand-modified the program to merge the data and the synchronization, and modified TreadMarks to use a broadcast. The speedup improved to 5.09 from 4.19. 5.4. 3-D FFT 3-D FFT, from the NAS <ref> [5] </ref> benchmark suite, numerically solves a partial differential equation using three dimensional forward and inverse FFT's. Assume the input array A is of size n 1 fi n 2 fi n 3 , organized in column-major order.
Reference: [6] <author> J. Carter, J. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: 1. Introduction This paper evaluates the potential for using software distributed shared memory (DSM) <ref> [2, 6, 11] </ref> as a target for a par-allelizing compiler on a message passing machine. We compare this approach with the more common method whereby the compiler directly targets the underlying message passing system (e.g., [9]).
Reference: [7] <author> S. Dwarkadas, A. Cox, and W. Zwaenepoel. </author> <title> An integrated compile-time/run-time software distributed shared memory system. </title> <booktitle> In Proceedings of the 7th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: In addition, to quantify the contribution of some of the sources of overhead, we have hand-modified the SPF-generated code to eliminate those sources. We use the enhanced TreadMarks interface proposed by Dwarkadas et al. <ref> [7] </ref> to achieve communication aggregation, consistency elimination, and pushing (instead of pulling) data to the processors that will use it. We also eliminate redundant barriers in the program. 4. Applications We use 6 applications (Jacobi, Shallow, MGS, 3-D FFT, IGrid, and NBF). <p> Further Optimizations In Section 5, we have shown the considerable benefits of hand-applied optimizations for the SPF-generated DSM programs. The optimizations include aggregating data communication, merging synchronization and data, and pushing data instead of the default request-response data communication in the DSM system. Dwarkadas et al. <ref> [7] </ref> have shown that these optimizations can be implemented automatically by a compiler and DSM runtime system. Those techniques could be integrated with the APR compiler. Elimination of redundant barriers was proposed by Tseng [17] in the context of automatic parallelization for hardware distributed shared memory machine.
Reference: [8] <author> G. Geist and V. Sunderam. </author> <title> Network-based concurrent computing on the PVM system. </title> <journal> Concurrency: Practice and Experience, </journal> <pages> pages 293-311, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: We use version 2.0 of the SPF compiler and the the XHPF compiler. Both TreadMarks and the XHPF compiler use the user-level MPL communication library as the underlying message passing system. The hand-coded versions of the message passing programs use PVMe, an implementation of PVM <ref> [8] </ref> optimized for the IBM SP/2. Our primary goal is to assess the performance of the compiler-generated shared memory programs. Performance is quantified primarily by speedup, but we also provide statistics on the number of messages and the amount of data exchanged during execution.
Reference: [9] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: 1. Introduction This paper evaluates the potential for using software distributed shared memory (DSM) [2, 6, 11] as a target for a par-allelizing compiler on a message passing machine. We compare this approach with the more common method whereby the compiler directly targets the underlying message passing system (e.g., <ref> [9] </ref>). Shared memory is an attractive target, especially for ir regular applications, because the DSM system greatly eases the burden on the parallelizing compiler. Compilers generating message passing code for irregular accesses are either inefficient or quite complex (e.g., the inspector-executor model [15]).
Reference: [10] <author> P. Keleher and C. Tseng. </author> <title> Enhancing software DSM for compiler-parallelized applications. </title> <booktitle> In Proceedings of the 11th International Parallel Processing Symposium, </booktitle> <year> 1997. </year>
Reference-contexts: The difference varies from 2% to 20%. In general, two factors account for the difference. The compiler-generated shared-memory programs have excess synchro nization and additional data communication. The latter is because there is less processor locality in the programs' data access patterns. Keleher and Tseng <ref> [10] </ref> perform a similar study which also compares the performance of compiler-generated DSM programs with compiler-generated message passing programs.
Reference: [11] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> Nov. </month> <year> 1989. </year>
Reference-contexts: 1. Introduction This paper evaluates the potential for using software distributed shared memory (DSM) <ref> [2, 6, 11] </ref> as a target for a par-allelizing compiler on a message passing machine. We compare this approach with the more common method whereby the compiler directly targets the underlying message passing system (e.g., [9]).
Reference: [12] <author> H. Lu, A. Cox, S. Dwarkadas, R. Rajamony, and W. Zwaenepoel. </author> <title> Software distributed shared memory support for irregular applications. </title> <note> 1996. Submitted for publication. </note>
Reference-contexts: Mukherjee et al. [14] compared the CHAOS inspector-executor system to the TSM (transparent shared memory) and the XSM (ex-tendible shared memory) systems. They concluded that TSM is not competitive with CHAOS, while XSM achieves performance comparable to CHAOS after introducing several hand-coded special-purpose protocols. In a more recent paper <ref> [12] </ref>, we compared CHAOS to TreadMarks with simple compiler support for describing accesses to the indirection array. With the compiler support, the TreadMarks DSM system achieves similar performance to the inspector-executor method supported by the Chaos run-time library.
Reference: [13] <author> H. Lu, S. Dwarkadas, A. Cox, and W. Zwaenepoel. </author> <title> Message passing versus distributed shared memory on networks of workstations. </title> <booktitle> In Proceedings SuperComputing '95, </booktitle> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: We identify two groups of causes contributing to these differences. First, there are the known factors contributing to the performance differences between message passing and software DSM, regardless of whether the programs are compiler-generated or hand-coded <ref> [13] </ref>. For the applications in this study, the relevant factors are the overhead of the shared memory implementation (detecting modifications), the separation of data and synchronization, the absence of data communication aggregation in shared memory, and false sharing.
Reference: [14] <author> S. Mukherjee, S. Sharma, M. Hill, J. Larus, A. Rogers, and J. Saltz. </author> <title> Efficient support for irregular applications on distributed memory machines. </title> <booktitle> In Proceedings of the 5th Symposium on the Principles and Practice of Parallel Programming, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Our results indicate that with minimal compiler support, our software DSM system has performance comparable to hand-coded message passing for the irregular applications we have considered. The inspector-executor model [15] has been proposed to efficiently execute irregular computations in the message passing paradigm. Mukherjee et al. <ref> [14] </ref> compared the CHAOS inspector-executor system to the TSM (transparent shared memory) and the XSM (ex-tendible shared memory) systems. They concluded that TSM is not competitive with CHAOS, while XSM achieves performance comparable to CHAOS after introducing several hand-coded special-purpose protocols.
Reference: [15] <author> J. Saltz, H. Berryman, and J. Wu. </author> <title> Multiprocessors and run-time compilation. </title> <journal> Concurrency:Practice and Experience, </journal> <volume> 3(6) </volume> <pages> 573-592, </pages> <month> Dec. </month> <year> 1991. </year>
Reference-contexts: Shared memory is an attractive target, especially for ir regular applications, because the DSM system greatly eases the burden on the parallelizing compiler. Compilers generating message passing code for irregular accesses are either inefficient or quite complex (e.g., the inspector-executor model <ref> [15] </ref>). Without the inspector-executor model, imprecise compiler analysis leads to large amounts of communication in the compiler-generated message passing programs. Because the compiler does not know what data will be accessed, it broadcasts all data in each processor's partition to all other processors. <p> Our results indicate that with minimal compiler support, our software DSM system has performance comparable to hand-coded message passing for the irregular applications we have considered. The inspector-executor model <ref> [15] </ref> has been proposed to efficiently execute irregular computations in the message passing paradigm. Mukherjee et al. [14] compared the CHAOS inspector-executor system to the TSM (transparent shared memory) and the XSM (ex-tendible shared memory) systems.
Reference: [16] <author> C.-W. Tseng. </author> <title> An Optimizing Fortran D Compiler for MIMD Distributed-Memory Machines. </title> <type> PhD thesis, </type> <institution> Rice University, Houston, </institution> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: The translated code relies on a small run-time system to handle process creation, to assign loops to processors, and to perform the underlying communication. To implement loop distribution, the run-time system maintains descriptors for all distributed arrays, and tries to generate loop distri butions that satisfy the owner-computes rule <ref> [16] </ref>. In case the communication pattern is unknown at compile time, the compiler inserts instructions to broadcast all the data in a processor's partition (specified by the user) at the end of the parallel loop, regardless of whether the data will actually be used. 3.
Reference: [17] <author> C.-W. Tseng. </author> <title> Compiler optimizations for eliminating barrier synchronization. </title> <booktitle> In Proceedings of the 5th Symposium on the Principles and Practice of Parallel Programming, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Dwarkadas et al. [7] have shown that these optimizations can be implemented automatically by a compiler and DSM runtime system. Those techniques could be integrated with the APR compiler. Elimination of redundant barriers was proposed by Tseng <ref> [17] </ref> in the context of automatic parallelization for hardware distributed shared memory machine. His results show a significant reduction in the number of barriers, although only a limited reduction in the execution time.
References-found: 17

