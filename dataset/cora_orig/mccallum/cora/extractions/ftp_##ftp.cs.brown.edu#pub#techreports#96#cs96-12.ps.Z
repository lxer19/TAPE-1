URL: ftp://ftp.cs.brown.edu/pub/techreports/96/cs96-12.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-96-12.html
Root-URL: http://www.cs.brown.edu
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Bobrow, Robert J. </author> <year> (1990). </year> <title> Statistical Agenda Parsing. </title> <booktitle> DARPA Speech and Language Workshop, </booktitle> <pages> 222-224. </pages>
Reference-contexts: Figure 1 shows a graph of %non-0 E, that is, the percent of nonzero-length edges needed to get 95% of the probability mass, for each sentence length. 5 Previous work Bobrow <ref> [1] </ref> and Chitrao and Grishman [5] introduced statistical agenda-based parsing techniques. Chitrao and Grishman implemented a best-first probabilistic parser and noted the parser's tendency to prefer shorter constituents. They proposed a heuristic solution of penalizing shorter constituents by a fixed amount per word.
Reference: [2] <author> Carroll, Glenn and Eugene Charniak (1992). </author> <title> Learning probabilistic dependency grammars from labeled text. </title> <booktitle> Working Notes, Fall Symposium Series, AAAI, </booktitle> <pages> 25-32. </pages>
Reference-contexts: The p (N i j;k ; t 0;j ) term is computed as in the normalized ff L fi model. We will refer to this figure as the prefix estimate. 3 The Experiment We used as our grammar a probabilistic context-free grammar learned from the Brown corpus (see [6], <ref> [2] </ref>, [3], [4]).
Reference: [3] <author> Carroll, Glenn and Eugene Charniak (1992). </author> <title> Two Experiments on Learning Probabilistic Dependency Grammars from Corpora. </title> <booktitle> Workshop Notes, Statistically-Based NLP Techniques, AAAI, </booktitle> <pages> 1-13. </pages>
Reference-contexts: We will refer to this figure as the prefix estimate. 3 The Experiment We used as our grammar a probabilistic context-free grammar learned from the Brown corpus (see [6], [2], <ref> [3] </ref>, [4]).
Reference: [4] <author> Charniak, Eugene and Glenn Carroll (1994). </author> <title> Context-sensitive statistics for improved grammatical language models. </title> <booktitle> Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> 728-733. </pages>
Reference-contexts: We will refer to this figure as the prefix estimate. 3 The Experiment We used as our grammar a probabilistic context-free grammar learned from the Brown corpus (see [6], [2], [3], <ref> [4] </ref>).
Reference: [5] <author> Chitrao, Mahesh V. and Ralph Grishman (1990). </author> <title> Statistical Parsing of Messages. </title> <booktitle> DARPA Speech and Language Workshop, </booktitle> <pages> 263-266. </pages>
Reference-contexts: Figure 1 shows a graph of %non-0 E, that is, the percent of nonzero-length edges needed to get 95% of the probability mass, for each sentence length. 5 Previous work Bobrow [1] and Chitrao and Grishman <ref> [5] </ref> introduced statistical agenda-based parsing techniques. Chitrao and Grishman implemented a best-first probabilistic parser and noted the parser's tendency to prefer shorter constituents. They proposed a heuristic solution of penalizing shorter constituents by a fixed amount per word.
Reference: [6] <author> Francis, W. Nelson, and Henry Kucera (1982). </author> <title> Frequency Analysis of English Usage: Lexicon and Grammar. </title> <address> Boston: </address> <publisher> Houghton Mi*in. </publisher>
Reference-contexts: The p (N i j;k ; t 0;j ) term is computed as in the normalized ff L fi model. We will refer to this figure as the prefix estimate. 3 The Experiment We used as our grammar a probabilistic context-free grammar learned from the Brown corpus (see <ref> [6] </ref>, [2], [3], [4]).
Reference: [7] <author> Jelinek, Frederick and John D. </author> <title> Lafferty (1991). Computation of the Probability of Initial Substring Generation by Stochastic Context-Free Grammars. </title> <journal> Computational Linguistics, </journal> <volume> 17, </volume> <pages> 315-323. </pages>
Reference-contexts: A method for calculating ff L can be derived from the calculations given in <ref> [7] </ref>. A simple extension to the normalized fi model allows us to estimate the per-word probability of all tags in the sentence through the end of the con stituent under consideration. This allows us to take advantage of information already obtained in a left-right parse.
Reference: [8] <author> Kochman, Fred and Joseph Kupin (1991). </author> <title> Calculating the Probability of a Partial Parse of a Sentence. </title> <booktitle> DARPA Speech and Language Workshop, </booktitle> <pages> 237-240. </pages>
Reference-contexts: Miller and Fox [11] compare the performance of parsers using three different types of grammars, and show that a probabilistic context-free grammar using inside probability (unnormalized) as a figure of merit outperforms both a context-free grammar and a context-dependent grammar. Kochman and Kupin <ref> [8] </ref> propose a figure of merit closely related to our prefix estimate. They do not actually incorporate this figure into a best-first parser. Magerman and Marcus [9] use the geometric mean to compute a figure of merit that is independent of constituent length.
Reference: [9] <author> Magerman, David M. and Mitchell P. </author> <title> Marcus (1991). Parsing the Voyager Domain Using Pearl. </title> <booktitle> DARPA Speech and Language Workshop, </booktitle> <pages> 231-236. </pages>
Reference-contexts: Kochman and Kupin [8] propose a figure of merit closely related to our prefix estimate. They do not actually incorporate this figure into a best-first parser. Magerman and Marcus <ref> [9] </ref> use the geometric mean to compute a figure of merit that is independent of constituent length.
Reference: [10] <author> Magerman, David M. and Carl Weir (1992). </author> <title> Efficiency, Robustness and Accuracy in Picky Chart Parsing. </title> <booktitle> Proceedings of the 30th ACL Conference, </booktitle> <pages> 40-47. </pages>
Reference-contexts: Kochman and Kupin [8] propose a figure of merit closely related to our prefix estimate. They do not actually incorporate this figure into a best-first parser. Magerman and Marcus [9] use the geometric mean to compute a figure of merit that is independent of constituent length. Magerman and Weir <ref> [10] </ref> use a similar model with a different parsing algorithm. 7 8 6 Conclusions The best performance in terms of edge counts of the figures we tested was the model which used the most information available from the sentence, the prefix model.
Reference: [11] <author> Miller, Scott and Heidi Fox (1994). </author> <title> Automatic Grammar Acquisition. </title> <booktitle> Proceedings of the Human Language Technology Workshop, </booktitle> <pages> 268-271. 10 </pages>
Reference-contexts: Chitrao and Grishman implemented a best-first probabilistic parser and noted the parser's tendency to prefer shorter constituents. They proposed a heuristic solution of penalizing shorter constituents by a fixed amount per word. Miller and Fox <ref> [11] </ref> compare the performance of parsers using three different types of grammars, and show that a probabilistic context-free grammar using inside probability (unnormalized) as a figure of merit outperforms both a context-free grammar and a context-dependent grammar.
References-found: 11

