URL: http://www-cad.eecs.berkeley.edu:80/HomePages/aml/publications/ml95_def.ps.gz
Refering-URL: http://www-cad.eecs.berkeley.edu:80/HomePages/aml/publications/index.html
Root-URL: 
Email: aml@inesc.pt  alberto@ic.berkeley.edu  
Title: Inferring Reduced Ordered Decision Graphs of Minimum Description Length  
Author: Arlindo L. Oliveira Alberto Sangiovanni-Vincentelli 
Address: Lisbon, Portugal  UC Berkeley  
Affiliation: IST/INESC  Dept. of EECS  
Abstract: We propose an heuristic algorithm that induces decision graphs from training sets using Rissanen's minimum description length principle to control the tradeoff between accuracy in the training set and complexity of the hypothesis description.
Abstract-found: 1
Intro-found: 1
Reference: [Brace et al., 1989] <author> Brace, K., Rudell, R., and Bryant, R. </author> <year> (1989). </year> <title> Efficient implementation of a BDD package. </title> <booktitle> In Design Automation Conference. </booktitle>
Reference-contexts: A combination of this approach with some of the techniques introduced here may be of interest. The approach described in this paper is radically different from any of these solutions. It uses well known induction algorithms [Quinlan, 1986, Pagallo and Haussler, 1990] and techniques for the manipulation of RODGs <ref> [Brace et al., 1989] </ref> in the generation of the initial decision graph. The algorithm then derives a compact decision graph by performing incremental changes until a graph that corresponds to a solution with minimal description length [Rissanen, 1978] is obtained. <p> a 1 in position j iff the function defined by node n i has the value 1 for the jth instance in the training set: y i = f i (d j ) (2) 3 MANIPULATING DISCRETE FUNCTIONS USING RODGS Packages that manipulate reduced ordered decision graphs are widely available <ref> [Brace et al., 1989] </ref> and have become the most commonly used tool for discrete function manipulation in the logic synthesis community. A detailed exposition of the algorithms used to manipulate RODG structures is outside the scope of this paper and this section intends to introduce only the basic ideas.
Reference: [Breiman et al., 1984] <author> Breiman, L., Friedman, J. H., Ol-shen, R. A., and Stone, C. J. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: 1 INTRODUCTION Decision graphs can be viewed as a generalization of decision trees, a very successful approach for the inference of classification rules <ref> [Breiman et al., 1984, Quinlan, 1986] </ref>. The selection of decision graphs instead of decision trees as the representation scheme is important because many concepts of interest require very large decision trees.
Reference: [Bryant, 1986] <author> Bryant, R. E. </author> <year> (1986). </year> <title> Graph-based algorithms for Boolean function manipulation. </title> <journal> IEEE Transactions on Computers. </journal>
Reference-contexts: A decision graph is called reduced if no two nodes exist that branch exactly in the same way, and it is never the case that all outgoing edges of a given node terminate in the same node <ref> [Bryant, 1986] </ref>. A graph that is both reduced and ordered is called a reduced ordered decision graph (RODG). In this work, only problems defined over Boolean spaces will be considered.
Reference: [Coudert et al., 1989] <author> Coudert, O., Berthet, C., and Madre, J. C. </author> <year> (1989). </year> <title> Verification of synchronous sequential machines based on symbolic execution. </title> <editor> In Sifakis, J., editor, </editor> <booktitle> Proceedings of the Workshop on Automatic Verification Methods for Finite State Systems, volume 407 of Lecture Notes in Computer Science, </booktitle> <pages> pages 365-373. </pages> <publisher> Springer-Verlag. </publisher>
Reference-contexts: In this case, the final RODG is the same as the one obtained using the initial decision tree although this is not always the case. 5.1.3 Initialization Using the Restrict Operator The third way to initialize the algorithm is to use the restrict operator <ref> [Coudert et al., 1989] </ref>. This RODG operator can be used to obtain a more compact RODG representation for an incompletely specified function. The restrict operator belongs to a family of heuristics that generate a small RODG by merging, in a bottom up fashion, nodes in an RODG.
Reference: [Goldman, 1994] <author> Goldman, J. A. </author> <year> (1994). </year> <title> Machine learning: A comparative study of pattern theory and C4.5. </title> <type> Technical Report WL-TR-94-1102, </type> <institution> Wright Laboratory, USAF, </institution> <address> WL/AART, WPAFB, OH 45433-6543. </address>
Reference-contexts: They kindly agreed to run smog in a benchmark his group assembled <ref> [Goldman, 1994] </ref> for the purpose of evaluating the effectiveness of a set of learning algorithms. Each one of the problems is defined over a space of 8 Boolean attributes.
Reference: [Kohavi, 1994] <author> Kohavi, R. </author> <year> (1994). </year> <title> Bottom-up induction of oblivious read-once decision graphs. </title> <booktitle> In European Conference in Machine Learning. </booktitle>
Reference-contexts: Very recently, a new approach was proposed [Kohavi, 1995] that reportedly represents an improvement over a previous algorithm <ref> [Kohavi, 1994] </ref> by the same author. This last approach is also based on the identification of common subtrees in a decision tree, but this tree is constrained to exhibit the same ordering of tests for all possible paths in the tree.
Reference: [Kohavi, 1995] <author> Kohavi, R. </author> <year> (1995). </year> <title> Oblivious decision trees, graphs and top-down pruning. </title> <note> In IJCAI-95. to be published. </note>
Reference-contexts: The author reported improvements over the use of decision trees in relatively simple problems but our experiments using a similar approach failed in more complex test cases because the algorithm tends to perform prema ture joins in complex problems. Very recently, a new approach was proposed <ref> [Kohavi, 1995] </ref> that reportedly represents an improvement over a previous algorithm [Kohavi, 1994] by the same author.
Reference: [Mahoney and Mooney, 1991] <author> Mahoney, J. J. and Mooney, R. J. </author> <year> (1991). </year> <title> Initializing ID5R with a domain theory: some negative results. </title> <type> Technical Report 91-154, </type> <institution> CS Dept., University of Texas at Austin, Austin, TX. </institution>
Reference-contexts: One natural approach is to identify related subtrees in a decision tree obtained using standard methods but other authors have reported limited success using this method <ref> [Mahoney and Mooney, 1991] </ref>. Another approach uses a greedy algorithm that performs either a join or a split operation, depending on which one reduces more the description length [Oliver, 1993].
Reference: [Murphy and Aha, 1991] <author> Murphy, P. M. and Aha, D. W. </author> <year> (1991). </year> <title> Repository of Machine Learning Databases - Machine readable data repository. </title> <institution> University of Cali-fornia, Irvine. </institution>
Reference-contexts: This procedure is applied once to all variables and can be, optionally, iterated to convergence. 6 EXPERIMENTS 6.1 RESULTS IN PROBLEMS FROM THE LITERATURE In this section, the comparison between different algorithms was made using a set of problems that have been addressed in the machine learning literature <ref> [Pagallo and Haussler, 1990, Murphy and Aha, 1991] </ref> and are described in detail elsewhere [Oliveira, 1994]. 6.1.1 Experimental Setup The preferred approach to perform the performance comparison between different algorithms is to use mul tiple runs with fixed training set sizes.
Reference: [Oliveira, 1994] <author> Oliveira, A. L. </author> <year> (1994). </year> <title> Inductive Learning by Selection of Minimal Complexity Representations. </title> <type> PhD thesis, </type> <institution> UC Berkeley. </institution> <note> Also available as UCB/ERL Technical Report M94/97. </note>
Reference-contexts: We will henceforth assume that all attributes are Boolean valued and that D = f0; 1g N = B N . <ref> [Oliveira, 1994] </ref> discusses the advantages and drawbacks of this approach. Consider now a decision tree or a decision graph defined over the domain D. Let n 0 ; n 1 ; :::; n s be the nodes in the tree or graph. <p> iterated to convergence. 6 EXPERIMENTS 6.1 RESULTS IN PROBLEMS FROM THE LITERATURE In this section, the comparison between different algorithms was made using a set of problems that have been addressed in the machine learning literature [Pagallo and Haussler, 1990, Murphy and Aha, 1991] and are described in detail elsewhere <ref> [Oliveira, 1994] </ref>. 6.1.1 Experimental Setup The preferred approach to perform the performance comparison between different algorithms is to use mul tiple runs with fixed training set sizes.
Reference: [Oliveira and Vincentelli, 1993] <author> Oliveira, A. L. and Vin-centelli, A. S. </author> <year> (1993). </year> <title> Learning complex boolean functions : Algorithms and applications. </title> <booktitle> In Advances in Neural Information Processing Systems 6, </booktitle> <address> Denver, </address> <publisher> CO. Morgan Kaufmann. </publisher>
Reference-contexts: The fulfringe constructive induction algorithm <ref> [Oliveira and Vincentelli, 1993] </ref> belongs to a family of constructive induction algorithms that identify patterns near the fringes of the decision tree and uses them to build new attributes [Pagallo and Haussler, 1990, Yang et al., 1991].
Reference: [Oliver, 1993] <author> Oliver, J. J. </author> <year> (1993). </year> <title> Decision graphs an extension of decision trees. </title> <type> Technical Report 92/173, </type> <institution> Monash University, Clayton, </institution> <address> Victoria 3168, Australia. </address>
Reference-contexts: The selection of decision graphs instead of decision trees as the representation scheme is important because many concepts of interest require very large decision trees. In particular, the quality of the generalization performed by a decision tree induced from data suffers because of two well known problems <ref> [Oliver, 1993] </ref>: the replication of subtrees required to represent some concepts and the rapid fragmentation of the training set data when attributes that can take a high number of values are tested at a node. Decision graphs have been proposed as one way to alleviate these problems. <p> Another approach uses a greedy algorithm that performs either a join or a split operation, depending on which one reduces more the description length <ref> [Oliver, 1993] </ref>. The author reported improvements over the use of decision trees in relatively simple problems but our experiments using a similar approach failed in more complex test cases because the algorithm tends to perform prema ture joins in complex problems.
Reference: [Pagallo and Haussler, 1990] <author> Pagallo, G. and Haussler, D. </author> <year> (1990). </year> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5(1) </volume> <pages> 71-100. </pages>
Reference-contexts: A combination of this approach with some of the techniques introduced here may be of interest. The approach described in this paper is radically different from any of these solutions. It uses well known induction algorithms <ref> [Quinlan, 1986, Pagallo and Haussler, 1990] </ref> and techniques for the manipulation of RODGs [Brace et al., 1989] in the generation of the initial decision graph. <p> The fulfringe constructive induction algorithm [Oliveira and Vincentelli, 1993] belongs to a family of constructive induction algorithms that identify patterns near the fringes of the decision tree and uses them to build new attributes <ref> [Pagallo and Haussler, 1990, Yang et al., 1991] </ref>. Fulfringe identifies the patterns shown in figure 4 which are a superset of the ones identified by dcfringe. <p> This procedure is applied once to all variables and can be, optionally, iterated to convergence. 6 EXPERIMENTS 6.1 RESULTS IN PROBLEMS FROM THE LITERATURE In this section, the comparison between different algorithms was made using a set of problems that have been addressed in the machine learning literature <ref> [Pagallo and Haussler, 1990, Murphy and Aha, 1991] </ref> and are described in detail elsewhere [Oliveira, 1994]. 6.1.1 Experimental Setup The preferred approach to perform the performance comparison between different algorithms is to use mul tiple runs with fixed training set sizes.
Reference: [Quinlan, 1986] <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106. </pages>
Reference-contexts: 1 INTRODUCTION Decision graphs can be viewed as a generalization of decision trees, a very successful approach for the inference of classification rules <ref> [Breiman et al., 1984, Quinlan, 1986] </ref>. The selection of decision graphs instead of decision trees as the representation scheme is important because many concepts of interest require very large decision trees. <p> A combination of this approach with some of the techniques introduced here may be of interest. The approach described in this paper is radically different from any of these solutions. It uses well known induction algorithms <ref> [Quinlan, 1986, Pagallo and Haussler, 1990] </ref> and techniques for the manipulation of RODGs [Brace et al., 1989] in the generation of the initial decision graph. <p> Since the tradeoff between hypothesis complexity and accuracy in the training set will be controlled by the local optimization algorithm, we used a simple decision tree approach <ref> [Quinlan, 1986] </ref> and performed no pruning on the tree derived using this method. Once the decision tree has been obtained, the reduced ordered decision graph is obtained applying expression (1) to the root of the decision tree and using the RODG package to manipulate the intermediate Boolean functions obtained.
Reference: [Quinlan and Rivest, 1989] <author> Quinlan, J. R. and Rivest, R. L. </author> <year> (1989). </year> <title> Inferring decision trees using the Minimum Description Length Principle. </title> <journal> Inform. Comput., </journal> <volume> 80(3) </volume> <pages> 227-248. </pages>
Reference-contexts: The reduced ordered decision graph and the exceptions are encoded using a scheme inspired in the one proposed for the induction of decision trees <ref> [Quinlan and Rivest, 1989] </ref>, but modified to take into account the fact that a node can be visited more than once and restricted to consider only decision graphs with two terminal nodes.
Reference: [Rissanen, 1978] <author> Rissanen, J. </author> <year> (1978). </year> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471. </pages>
Reference-contexts: The algorithm then derives a compact decision graph by performing incremental changes until a graph that corresponds to a solution with minimal description length <ref> [Rissanen, 1978] </ref> is obtained. The problem of selecting an appropriate ordering for the variables is solved in a novel way using one of the highly effective heuristic algorithms for variable reordering proposed in the logic synthesis literature [Rudell, 1993].
Reference: [Rudell, 1993] <author> Rudell, R. </author> <year> (1993). </year> <title> Dynamic variable ordering for ordered binary decision diagrams. </title> <booktitle> In ICCAD, </booktitle> <pages> pages 42-47. </pages> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: The problem of selecting an appropriate ordering for the variables is solved in a novel way using one of the highly effective heuristic algorithms for variable reordering proposed in the logic synthesis literature <ref> [Rudell, 1993] </ref>. <p> Selecting the optimal ordering for a given function is NP-complete [S. Tani and Yajima, 1993] but many heuristic algorithms have been proposed for this problem. Our implementation uses the sift algorithm for dynamic RODG ordering <ref> [Rudell, 1993] </ref>. This approach is based on the fact that swapping the order of two variables in the RODG ordering can be done very efficiently because only the nodes in these two levels are affected.
Reference: [S. Tani and Yajima, 1993] <author> S. Tani, K. H. and Yajima, S. </author> <year> (1993). </year> <title> The complexity of the optimal variable ordering problems of shared binary decision diagrams. </title> <booktitle> In Algorithms and Computation, 4th Internation Symposium, </booktitle> <pages> pages 389-98. </pages>
Reference-contexts: Selecting the optimal ordering for a given function is NP-complete <ref> [S. Tani and Yajima, 1993] </ref> but many heuristic algorithms have been proposed for this problem. Our implementation uses the sift algorithm for dynamic RODG ordering [Rudell, 1993].
Reference: [Schaffer, 1994] <author> Schaffer, C. </author> <year> (1994). </year> <title> A conservation law for generalization performance. </title> <booktitle> In Proceedings of the Eleventh International Conference in Machine Learning, </booktitle> <address> San Mateo. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Although it is possible to prove that, in general no bias is inherently superior to any other bias <ref> [Schaffer, 1994] </ref>, we argue, by showing the results of a series of experiments, that the selection of small decision graphs is appropriate for many interesting problems. 2 DECISION TREES AND DECISION GRAPHS We address the problem of inferring a classification rule from a labeled set of instances, the training set.
Reference: [Takenaga and Yajima, 1993] <author> Takenaga, Y. and Yajima, S. </author> <year> (1993). </year> <title> NP-completeness of minimum binary decision diagram identification. </title> <type> Technical Report COMP 92-99, </type> <institution> Institute of Electronics, Information and Communication Engineers (of Japan). </institution>
Reference-contexts: Decision graphs have been proposed as one way to alleviate these problems. The problem of selecting the minimum decision graph consistent with a training set is NP-complete <ref> [Takenaga and Yajima, 1993] </ref> and the need for heuristics is clear but the algorithms proposed to date for the construction of these graphs have met limited acceptability.
Reference: [Yang et al., 1991] <author> Yang, D. S., Rendell, L., and Blix, G. </author> <year> (1991). </year> <title> Fringe-like feature construction: A comparative study and a unifying scheme. </title> <booktitle> In Proceedings of the Eight International Conference in Machine Learning, </booktitle> <pages> pages 223-227, </pages> <address> San Mateo. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The fulfringe constructive induction algorithm [Oliveira and Vincentelli, 1993] belongs to a family of constructive induction algorithms that identify patterns near the fringes of the decision tree and uses them to build new attributes <ref> [Pagallo and Haussler, 1990, Yang et al., 1991] </ref>. Fulfringe identifies the patterns shown in figure 4 which are a superset of the ones identified by dcfringe.
References-found: 21

