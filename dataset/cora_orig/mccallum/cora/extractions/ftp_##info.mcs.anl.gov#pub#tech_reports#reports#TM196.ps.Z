URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/TM196.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts95.htm
Root-URL: http://www.mcs.anl.gov
Title: Fortran 77 Interface Specification to the SparsLinC 1.0 Library  
Author: by Christian H. Bischof, Alan Carle, and Peyvand Khademi 
Note: ANL authors' email addresses:  Address: Center for Research on Parallel  
Address: 9700 South Cass Avenue Argonne, IL 60439  St., Houston, TX 77005;  
Affiliation: ARGONNE NATIONAL LABORATORY  Mathematics and Computer Science Division  Computation, Rice University, 6100 S. Main  
Pubnum: Technical Memorandum No. 196  
Email: bischof@mcs.anl.gov khademi@mcs.anl.gov  email: carle@cs.rice.edu.  
Web: ANL/MCS-TM-196  
Date: May 1995  
Abstract: This work was supported by the Mathematical, Information, and Computational Sciences Division subprogram of the Office of Computational and Technology Research, U.S. Department of Energy, under Contract W-31-109-Eng-38; by the National Aerospace Agency under Purchase Order L25935D and Cooperative Agreement No. NCCW-0027; and by the National Science Foundation, through the Center for Research on Parallel Computation, under Cooperative Agreement No. CCR-9120008. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alfred Aho, John Hopcroft, and Jeffrey Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1974. </year>
Reference-contexts: If one cannot rely on the fact that a positive value for VPTR contains a valid pointer to a sparse data structure, one must resort to memory authentication schemes to be able to answer this question (see, for example, <ref> [1, Problem 2.12] </ref>). 2.2 Initialization of C Data Structures SparsLinC employs data structures that have to be initialized before any of the other SparsLinC routines can be called.
Reference: [2] <author> Christian Bischof, Alan Carle, George Corliss, Andreas Griewank, and Paul Hovland. ADIFOR: </author> <title> Generating derivative codes from Fortran programs. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 11-29, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction A fundamental kernel in numerical linear algebra and also in automatic differentiation (see, e.g., <ref> [2] </ref>) is the computation of a linear combination of some vectors, namely, w = i=1 where each ff i is referred to as a "multiplier," w as the "left-hand side vector," and any of the v i 's as a "right-hand side vector." Following Golub and Van Loan [4], we call <p> We adopt the convention that the Fortran INTEGER variable VPTR acts as a pointer to a sparse vector object, called sparse object (VPTR). 1 Table 1: Default Assumptions on Correspondence of Fortran and C Floating-Point Types Fortran 77 C REAL float DOUBLE PRECISION double COMPLEX float <ref> [2] </ref> DOUBLE COMPLEX double [2] 2.1 Valid Pointer Values We require that the Fortran INTEGER value "0" and the C pointer value "NULL " are identical. This assumption is critical in deciding whether VPTR contains a valid address of a sparse derivative object. <p> We adopt the convention that the Fortran INTEGER variable VPTR acts as a pointer to a sparse vector object, called sparse object (VPTR). 1 Table 1: Default Assumptions on Correspondence of Fortran and C Floating-Point Types Fortran 77 C REAL float DOUBLE PRECISION double COMPLEX float <ref> [2] </ref> DOUBLE COMPLEX double [2] 2.1 Valid Pointer Values We require that the Fortran INTEGER value "0" and the C pointer value "NULL " are identical. This assumption is critical in deciding whether VPTR contains a valid address of a sparse derivative object.
Reference: [3] <author> David Goldberg. </author> <title> What every computer scientist should know about floating-point arithmetic. </title> <journal> ACM Computing Surveys, </journal> <volume> 23(1) </volume> <pages> 5-48, </pages> <year> 1991. </year>
Reference-contexts: Accumulate these into a (possibly temporary) vector of that precision, by using the sparse arithmetic routines. 5. If necessary, truncate this vector to the precision desired for the left-hand side, by using the sparse conversion routines. As an alternative, the following scheme has been suggested by Goldberg <ref> [3, p. 31] </ref>. Assume that an expression is represented as an expression tree, with the final result at its root. Then proceed as follows: Step 1: Assign each operation a tentative precision, which is the maximum of its two operands, proceeding from the leaves to the root.
Reference: [4] <author> Gene H. Golub and Charles F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, 2nd edition, </address> <year> 1989. </year> <month> 30 </month>
Reference-contexts: (see, e.g., [2]) is the computation of a linear combination of some vectors, namely, w = i=1 where each ff i is referred to as a "multiplier," w as the "left-hand side vector," and any of the v i 's as a "right-hand side vector." Following Golub and Van Loan <ref> [4] </ref>, we call this operation a GAXPY. In the cases of interest for automatic differentiation, the number k of vectors on the right-hand side is usually moderate, with k 3 forming the bulk of computations.
References-found: 4

