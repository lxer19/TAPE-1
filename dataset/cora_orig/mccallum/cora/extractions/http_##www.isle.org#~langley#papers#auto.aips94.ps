URL: http://www.isle.org/~langley/papers/auto.aips94.ps
Refering-URL: http://www.isle.org/~langley/pubs.html
Root-URL: 
Email: langley@cs.stanford.edu iba@wind.arc.nasa.gov shrager@xerox.com  
Title: Reactive and Automatic Behavior in Plan Execution some contexts, successful agents appear to prefer nonreactive
Author: Pat Langley Wayne Iba Jeff Shrager 
Keyword: Reactive and Automatic Execution  
Address: 3333 Coyote Hill Road Stanford, CA 94305 Moffett Field, CA 94035 Palo Alto, CA 94304  
Affiliation: Robotics Laboratory Recom Technologies Palo Alto Research Center Computer Science Dept. Mail Stop 269-2 Xerox Corporation Stanford University NASA Ames Research Center  
Note: To appear in Proceedings of the Second International Conference on AI Planning Systems (1994). Chicago: AAAI Press.  of sensory feedback (Schmidt, 1982). Thus, at least in  
Abstract: Much of the work on execution assumes that the agent constantly senses the environment, which lets it respond immediately to errors or unexpected events. In this paper, we argue that this purely reactive strategy is only optimal if sensing is inexpensive, and we formulate a simple model of execution that incorporates the cost of sensing. We present an average-case analysis of this model, which shows that in domains with high sensing cost or low probability of error, a more `automatic' strategy - one with long intervals between sensing can lead to less expensive execution. The analysis also shows that the distance to the goal has no effect on the optimal sensing interval. These results run counter to the prevailing wisdom in the planning community, but they promise a more balanced approach to the interleaving of execution and sensing. Much of the recent research on plan execution and control has focused on reactive systems. One central characteristic of such approaches is that the agent senses the environment on each time step, thus ensuring that it can react promptly to any errors or other unexpected events. This holds whether the agent draws on large-scale knowledge structures, such as plans (Howe & Co-hen, 1991) or cases (Hammond, Converse, & Marks, 1988), or bases its decisions on localized structures, such as control rules (Bresina, Drummond, & Kedar, 1993; Grefenstette, Ramsey, & Schultz, 1990) or neural networks (Sutton, 1988; Kaelbling, 1993). However, human beings still provide the best examples of robust physical agents, and the psychological literature reveals that humans do not always behave in a reactive manner. People can certainly operate in reactive or `closed-loop' mode, which closely couples execution with sensing (Adams, 1971). But at least in some domains, humans instead operate in automatic or `open-loop' mode, in which execution proceeds in the absence One explanation for this phenomenon is that there exists a tradeoff between the cost of sensing, which models of reactive agents typically ignore, and the cost of errors that occur during execution. For some situations, the optimal sensing strategy is completely reactive behavior, in which the agent observes the environment after each execution step. For other situations, the best strategy is completely automatic behavior, in which execution occurs without sensing. In most cases, the optimum will presumably fall somewhere between these two extremes, with the agent sensing the world during execution, but not after every step. There exist other reasons for preferring automatic to reactive behavior in some situations. At least for humans, the former appears to require fewer attentional resources, which lets them execute multiple automatic procedures in parallel. Humans also exhibit a well-known tradeoff between speed and accuracy, and in some cases one may desire an automated, rapid response to a reactive, accurate one. However, our goal here is not to provide a detailed account of human execution strategies, but to better understand the range of such strategies and the conditions under which they are appropriate. Thus, we will focus on the first explanation above, which assigns an explicit cost to the sensing process. In the following pages, we attempt to formalize the tradeoff between the cost of sensing and the cost of errors, and to identify the optimal position for an agent to take along the continuum from closed-loop, reactive behavior to open-loop, automatic behavior. In the next section, we present an idealized model of execution that takes both factors into account, followed by an analysis 
Abstract-found: 1
Intro-found: 1
Reference: <author> Adams, J. A. </author> <year> (1971). </year> <title> A closed-loop theory of motor learning. </title> <note> Journal of Motor Behavior , 3 , 111-149. </note>
Reference-contexts: But in domains where sensing costs less and errors are unlikely, they will gradually move to an automatic execution strategy. Unfortunately, most existing studies of human motor behavior have forced subjects into either closed-loop processing <ref> (e.g., Adams, 1971) </ref> or open-loop mode (e.g., Schmidt, 1982), and they have not systematically varied the domain characteristics with an eye toward the transition from the former to the latter.
Reference: <author> Bresina, J., Drummond, M., & Kedar, S. </author> <year> (1993). </year> <title> Reactive, integrated systems pose new problems for machine learning. </title> <editor> In S. Minton (Ed.), </editor> <title> Machine learning methods for planning . San Mateo, </title> <address> CA: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference: <author> Chrisman, L., & Simmons, R. </author> <year> (1991). </year> <title> Sensible planning: Focusing perceptual attention. </title> <booktitle> Proceeding of the Ninth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 756-761). </pages> <address> Anaheim, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Grefenstette, J. J., Ramsey, C. L., & Schultz, A. C. </author> <year> (1990). </year> <title> Learning sequential decision rules using simulation models and competition. </title> <booktitle> Machine Learning, </booktitle> <pages> 5 , 355-381. </pages>
Reference: <author> Hammond, K. J., Converse, T., & Marks, M. </author> <year> (1988). </year> <title> Learning from opportunities: Storing and reusing execution-time optimization. </title> <booktitle> Proceeding of the Seventh National Conference on Artificial Intelligence (pp. </booktitle> <pages> 536-540). </pages> <address> St. Paul, MN: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Hansen, E. A., & Cohen, P. R. </author> <year> (1993). </year> <title> Learning monitoring strategies to compensate for model uncertainty. </title> <booktitle> Working Notes of the AAAI-93 Workshop on Learning Action Models (pp. </booktitle> <pages> 33-35). </pages> <address> Washington, D.C.: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Howe, A. E., & Cohen, P. R. </author> <year> (1991). </year> <title> Failure recovery: A model and experiments. </title> <booktitle> Proceeding of the Ninth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 801-808). </pages> <address> Anaheim, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Iba, W. </author> <year> (1991). </year> <title> Acquisition and improvement of human motor skills: Learning through observation and practice. </title> <type> Doctoral dissertation, </type> <institution> Department of Information & Computer Science, University of California, Irvine. </institution>
Reference: <author> Kaelbling, L. P. </author> <year> (1993). </year> <title> Hierarchical learning in stochastic domains: Preliminary results. </title> <booktitle> Proceedings of the Tenth International Conference on Machine Learning (pp. </booktitle> <pages> 167-173). </pages> <address> Amherst, MA. </address>
Reference: <author> Kresbach, K., Olawsky, D., & Gini, M. </author> <year> (1992). </year> <title> An empirical study of sensing and defaulting in planning. </title> <booktitle> Proceedings of First International Conference on Artificial Intelligence Planning Systems (pp. </booktitle> <pages> 136-144). </pages> <address> College Park, MD: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Schmidt, R. A. </author> <year> (1982). </year> <title> More on motor programs. </title> <editor> In J. A. S. Kelso (Ed.), </editor> <title> Human motor behavior: An introduction. </title> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum. </publisher>
Reference-contexts: But in domains where sensing costs less and errors are unlikely, they will gradually move to an automatic execution strategy. Unfortunately, most existing studies of human motor behavior have forced subjects into either closed-loop processing (e.g., Adams, 1971) or open-loop mode <ref> (e.g., Schmidt, 1982) </ref>, and they have not systematically varied the domain characteristics with an eye toward the transition from the former to the latter.
Reference: <author> Shrager, J., Hogg, T., & Huberman, B. A. </author> <year> (1988). </year> <title> A graph-dynamic model of the power law of practice and the problem-solving fan effect. </title> <booktitle> Science, </booktitle> <pages> 242 , 414-416. </pages>
Reference: <author> Schoppers, M. </author> <year> (1992). </year> <title> Building plans to monitor and exploit open-loop and closed-loop dynamics. </title> <booktitle> Proceedings of First International Conference on Artificial Intelligence Planning Systems (pp. </booktitle> <pages> 204-213). </pages> <address> College Park, MD: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Simmons, R. </author> <year> (1990). </year> <title> Robust behavior with limited resources. </title> <booktitle> Proceedings of the AAAI Spring Symposium on Planning in Uncertain, Unpredictable, or Changing Environments. </booktitle> <address> Stanford, CA. </address>
Reference-contexts: Two characteristics of the plan's interaction with the environment also come into play: the probability p that, on a given time step, an error will occur that takes the 2 In some work <ref> (e.g., Simmons, 1990) </ref>, the results of sensing instead determine the path followed in a conditional plan, but here we assume the aim is to follow a single desired path. agent away from the desired path, and the distance d (or the number of time steps) to the goal if no such
Reference: <author> Sutton, R. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <note> Machine Learning , 3 , 9-44. </note>
Reference: <author> Tan, M. </author> <year> (1991). </year> <title> Cost-sensitive robot learning . Doctoral dissertation, </title> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA. </address>
Reference: <author> Yang, H. </author> <year> (1992). </year> <title> Learning and using macro-operators for AI planning . Doctoral dissertation, </title> <institution> Department of Computer Science, Vanderbilt University, Nashville. </institution>
References-found: 17

