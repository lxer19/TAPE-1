URL: http://bayes.stat.washington.edu/PAPERS/dami.ps
Refering-URL: http://www.ics.uci.edu/~mlearn/MLPapers.html
Root-URL: 
Title: Data Mining and Knowledge Discovery,  Statistical Themes and Lessons for Data Mining  
Author: CLARK GLYMOUR DAVID MADIGAN DARYL PREGIBON PADHRAIC SMYTH Editor: Usama Fayyad 
Keyword: Statistics, uncertainty, modeling, bias, variance  
Address: Pittsburgh, PA 15213  Seattle, WA 98195  Murray Hill, NJ 07974  Irvine, CA 92717  
Affiliation: Department of Cognitive Psychology, Carnegie Mellon University,  Department of Statistics, Box 354322, University of Washington,  Statistics Research, AT&T Laboratories,  Information and Computer Science, University of California,  
Note: c 1996 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Pubnum: 1,  
Email: cg09@andrew.cmu.edu  madigan@stat.washington.edu  daryl@research.att.com  smyth@ics.uci.edu  
Date: 25-42 (1996)  
Abstract: Data mining is on the interface of Computer Science and Statistics, utilizing advances in both disciplines to make progress in extracting information from large databases. It is an emerging field that has attracted much attention in a very short period of time. This article highlights some statistical themes and lessons that are directly relevant to data mining and attempts to identify opportunities where close cooperation between the statistical and computational communities might reasonably provide synergy for further progress in data analysis. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Akaike, H. </author> <year> 1974. </year> <title> A new look at the statistical model identification. </title> <journal> IEEE Trans. Automat. Contr. AC-19:716-723. </journal>
Reference-contexts: For the reasons just considered, scoring rules are often an attractive alternative to tests. Typical rules assign models a value determined by the likelihood function associated with the model, the number of parameters, or dimension, of the model, and the data. Popular rules include the Akaike Information Criterion <ref> (Akaike, 1974) </ref>, Bayes Information Criterion (Raftery, 1995), and Minimum Description length (Rissanen, 1978). Given a prior probability distribution over models, the posterior probability on the data is itself a scoring function, arguably a privileged one. The Bayes Information Criterion approximates posterior probabilities in large samples.
Reference: <author> Berger, J.O. and Sellke, T. </author> <year> 1987. </year> <title> Testing a point null hypothesis: the irreconcilability of P values and evidence (with discussion). </title> <journal> Journal of the American Statistical Association 82 </journal> <pages> 112-122. </pages>
Reference: <author> Breiman, L. </author> <year> 1996. </year> <note> Bagging predictors. Machine Learning, to appear. </note>
Reference-contexts: Intuitively, ambiguity over the model should dilute information about effect parameters and predictions, since "part of the evidence is spent to specify the model" (Leamer, 1978, p.91). Promising techniques for properly accounting for this source of uncertainty include Bayesian model averaging (Draper, 1995) and resampling methods <ref> (Breiman, 1996) </ref>. The main point here is that data miners need to think carefully about model assessment and look beyond commonly used goodness-of-fit measures such as mean square error. 5.
Reference: <author> Chasnoff, I.J., Griffith, D.R., MacGregor, S., Dirkes, K., Burns, K.A. </author> <year> 1989. </year> <title> Temporal patterns of cocaine use in pregnancy: </title> <journal> Perinatal outcome. Journal of the American Medical Association 261(12) </journal> <pages> 1741-4. </pages>
Reference: <author> Chatfield, C. </author> <year> 1995. </year> <title> Model uncertainty, data mining, and statistical inference (with discussion). </title> <journal> Journal of the Royal Statistical Society (Series A) 158 </journal> <pages> 419-466. </pages>
Reference-contexts: GLYMOUR, D. MADIGAN, D. PREGIBON AND P. SMYTH In any event, recent research has lead to increased awareness that within-model uncertainty (as discussed in the previous paragraph) may often, in practice, be dominated by between-model uncertainty <ref> (Chatfield, 1995, Draper, 1995, Madigan and York, 1995) </ref>. It is common practice nowadays for statisticians and data miners to use computationally intensive model selection algorithms to seek out a single optimal model from an enormous class of potential models.
Reference: <author> Dalal, S.R., Fowlkes, E.B. and Hoadley, B. </author> <year> 1989. </year> <title> Risk analysis of the space shuttle: Pre-Challenger prediction of failure. </title> <journal> Journal of the American Statistical Association 84 </journal> <pages> 945-957. </pages>
Reference: <author> Diggle, P. and Kenward, M. G. </author> <year> 1994. </year> <title> Informative drop-out in longitudinal data analysis (with discussion). </title> <journal> Applied Statistics: </journal> <volume> 43 </volume> <pages> 49-93. </pages>
Reference-contexts: Hand (1994) provides a series of examples illustrating how easy it is to give the right answers to the wrong question. For example, he discusses the problem of analyzing clinical trial data where patients drop out due to adverse side-effects of a particular treatment <ref> (Diggle and Kenward, 1994) </ref>. In this case, the important issue is which population is one interested in modelling? The population at large versus the population who remain within the trial? This problem arises in more general settings than in clinical trials, e.g., non-respondents (refusers) in survey data.
Reference: <author> Draper, D., Gaver, D.P., Goel, P.K., Greenhouse, J.B., Hedges, L.V., Morris, C.N., Tucker, J., and Waternaux, C. </author> <year> 1993. </year> <title> Combining information: National Research Council Panel on Statistical Issues and Opportunities for Research in the Combination of Information. </title> <address> Washington: </address> <publisher> National Academy Press. </publisher>
Reference: <author> Draper, D. </author> <year> 1995. </year> <title> Assessment and propagation of model uncertainty (with discussion). </title> <journal> Journal of the Royal Statistical Society (Series B). </journal> <volume> 57 </volume> <pages> 45-97. </pages>
Reference-contexts: GLYMOUR, D. MADIGAN, D. PREGIBON AND P. SMYTH In any event, recent research has lead to increased awareness that within-model uncertainty (as discussed in the previous paragraph) may often, in practice, be dominated by between-model uncertainty <ref> (Chatfield, 1995, Draper, 1995, Madigan and York, 1995) </ref>. It is common practice nowadays for statisticians and data miners to use computationally intensive model selection algorithms to seek out a single optimal model from an enormous class of potential models. <p> Intuitively, ambiguity over the model should dilute information about effect parameters and predictions, since "part of the evidence is spent to specify the model" (Leamer, 1978, p.91). Promising techniques for properly accounting for this source of uncertainty include Bayesian model averaging <ref> (Draper, 1995) </ref> and resampling methods (Breiman, 1996). The main point here is that data miners need to think carefully about model assessment and look beyond commonly used goodness-of-fit measures such as mean square error. 5.
Reference: <author> Efron, B. and Tibshirani, R.J. </author> <year> 1993. </year> <title> An Introduction to the Boostrap. New York: Chapman and Hall. Energy Modeling Forum 1982. World Oil: </title> <type> Summary report. EMF Report 6, </type> <institution> Energy Modeling Forum, Stanford University, Stanford, </institution> <address> CA. </address>
Reference-contexts: Classical statistics investigates such distributions of estimators in order to establish basic properties such as reliability and uncertainty. A variety of resampling and simulation techniques also exist for assessing estimator uncertainty <ref> (Efron and Tibshirani, 1993) </ref>. Estimation almost always requires some set of assumptions. Such assumptions are typically false, but often useful. If a model (which we can think of as a set of assumptions) is incorrect, estimates based on it can be expected to be incorrect as well.
Reference: <author> Fisher, R. A. </author> <year> 1958. </year> <title> Statistical methods for research workers. </title> <address> New York: </address> <publisher> Hafner Pub. Co. </publisher>
Reference-contexts: From the beginning of the subject, in the work of Bernoulli and Laplace, the absence of causal connection between two variables has been taken to imply their probabilistic independence (see Stigler, 1986), and the same idea is fundamental in the theory of experimental design <ref> (Fisher, 1958) </ref>. Early in this century, Wright (1921) introduced directed graphs to represent causal hypotheses (with vertices as random variables and edges representing direct influ 30 C. GLYMOUR, D. MADIGAN, D. PREGIBON AND P.
Reference: <author> Freedman, D.A. </author> <year> 1983. </year> <title> A note on screening regression equations. </title> <booktitle> The American Statistician 37 </booktitle> <pages> 152-155. </pages>
Reference-contexts: As a consequence, indiscriminate use of P -values with "standard" fixed ff-levels can lead to undesirable outcomes such as selecting a model with parameters that are highly significantly different from zero, even when the training data are pure noise <ref> (Freedman, 1983) </ref>. This point is of fundamental importance for data miners. 3. The P -value is the probability associated with the event that the test statistic was as extreme as the value observed, or more so.
Reference: <author> Geiger, D. Heckerman, D., and Meek, C. </author> <year> 1996. </year> <title> Asymptotic model selection for directed networks with hidden variables. </title> <booktitle> Proceedings of the Twelfth Annual Conference on Uncertainty in Artificial Intelligence. </booktitle> <address> San Francisco: </address> <publisher> Morgan Kaufman. </publisher>
Reference: <author> Gilks, W.R., Richardson, S., and Spiegelhalter, D.J. </author> <year> 1996. </year> <title> Markov chain Monte Carlo in practice. </title> <publisher> London: Chapman and Hall. </publisher>
Reference: <author> Hand, D. J. </author> <year> 1994. </year> <title> Deconstructing statistical questions (with discussion). </title> <journal> Journal of the Royal Statistical Society (Series A) 157 </journal> <pages> 317-356. </pages>
Reference: <author> Hastie, T.J. and Tibshirani, R. </author> <year> 1990. </year> <title> Generalized Additive Models. </title> <publisher> London: Chapman and Hall. </publisher>
Reference-contexts: A major achievement of statistical methodological research has been the development of very general and flexible model classes. Generalized Linear Models, for instance, embrace many classical linear models, and unify estimation and testing theory for such models (McCullagh and Nelder, 1989). Generalized Additive Models show similar potential <ref> (Hastie and Tibshirani, 1990) </ref>. Graphical models (Lauritzen, 1996) represent probabilistic and statistical models with planar graphs, where the vertices represent (possibly latent) random variables and the edges represent stochastic dependences. This provides a powerful language for describing models and the graphs themselves make modeling assumptions explicit.
Reference: <author> Hoerl, R. W., Hooper, J. H., Jacobs, P. J., Lucas, J. M. </author> <year> 1993. </year> <title> Skills for industrial statisticians to survive and prosper in the emerging quality environment. </title> <booktitle> The American Statistician 47 </booktitle> <pages> 280-292. </pages>
Reference: <author> Huber, P. J. </author> <year> 1981. </year> <title> Robust Statistics. </title> <address> New York: </address> <publisher> Wiley. </publisher> <address> 42 C. </address> <note> GLYMOUR, </note> <author> D. MADIGAN, D. PREGIBON AND P. SMYTH Jeffreys, H. </author> <year> 1980. </year> <title> Some general points in probability theory. </title> <editor> In: A. Zellner (Ed.), </editor> <title> Bayesian Analysis in Econometrics and Statistics. </title> <publisher> Amsterdam: North-Holland, </publisher> <pages> 451-454. </pages>
Reference-contexts: If a model (which we can think of as a set of assumptions) is incorrect, estimates based on it can be expected to be incorrect as well. One of the aims of statistical research is to find ways to weaken the assumptions necessary for good estimation. "Robust Statistics" <ref> (Huber, 1981) </ref> looks for estimators that work satisfactorily for larger families of distributions and have small errors when assumptions are violated. Bayesian estimation emphasizes that alternative models and their competing assumptions are often plausible.
Reference: <author> Kass, R.E. and Raftery, A.E. </author> <year> 1995. </year> <title> Bayes factors. </title> <journal> Journal of the American Statistical Association 90 </journal> <pages> 773-795. </pages>
Reference: <author> Kiiveri, H. </author> <title> and Speed, T.P. 1982. Structural analysis of multivariate data: A review. Sociological Methodology 209-289. </title>
Reference: <author> Kooperberg, C., Bose, S., and Stone, C.J. </author> <year> 1996. </year> <title> Polychotomous regression. </title> <journal> Journal of the American Statistical Association, </journal> <note> to appear. </note>
Reference: <author> Lauritzen, S.L. </author> <year> 1996. </year> <title> Graphical Models. </title> <publisher> Oxford: Oxford University Press. </publisher>
Reference-contexts: Generalized Linear Models, for instance, embrace many classical linear models, and unify estimation and testing theory for such models (McCullagh and Nelder, 1989). Generalized Additive Models show similar potential (Hastie and Tibshirani, 1990). Graphical models <ref> (Lauritzen, 1996) </ref> represent probabilistic and statistical models with planar graphs, where the vertices represent (possibly latent) random variables and the edges represent stochastic dependences. This provides a powerful language for describing models and the graphs themselves make modeling assumptions explicit.
Reference: <author> Leamer, E. E. </author> <year> 1978. </year> <title> Specification Searches. Ad Hoc Inference with Nonexperimental Data. </title> <publisher> Wiley: </publisher> <address> New York. </address>
Reference-contexts: The problem is that several different models may be close to optimal, yet lead to different inferences. Intuitively, ambiguity over the model should dilute information about effect parameters and predictions, since "part of the evidence is spent to specify the model" <ref> (Leamer, 1978, p.91) </ref>. Promising techniques for properly accounting for this source of uncertainty include Bayesian model averaging (Draper, 1995) and resampling methods (Breiman, 1996).
Reference: <author> Madigan, D. and Raftery, A.E. </author> <year> 1994. </year> <title> Model selection and accounting for model uncertainty in graphical models using Occam's Window. </title> <journal> Journal of the American Statistical Association 89 </journal> <pages> 1335-1346. </pages>
Reference-contexts: Bayesian estimation emphasizes that alternative models and their competing assumptions are often plausible. Rather than making an estimate based on a single model, several models can be considered and an estimate obtained as the weighted average of the estimates given by the individual models <ref> (Madigan and Raftery, 1994) </ref>. In fact, such Bayesian model averaging is bound to improve predictive performance, on average. Since the models obtained in data mining are usually the results of some automated search procedure, accounting for the potential errors associated with the search itself is crucial.
Reference: <author> Madigan, D. and York, J. </author> <year> 1995. </year> <title> Bayesian graphical models for discrete data. </title> <journal> International Statistical Review 63 </journal> <pages> 215-232. </pages>
Reference-contexts: GLYMOUR, D. MADIGAN, D. PREGIBON AND P. SMYTH In any event, recent research has lead to increased awareness that within-model uncertainty (as discussed in the previous paragraph) may often, in practice, be dominated by between-model uncertainty <ref> (Chatfield, 1995, Draper, 1995, Madigan and York, 1995) </ref>. It is common practice nowadays for statisticians and data miners to use computationally intensive model selection algorithms to seek out a single optimal model from an enormous class of potential models.
Reference: <author> Matheson, J.E. and Winkler, R.L. </author> <year> 1976. </year> <title> Scoring rules for continuous probability distributions. </title> <booktitle> Management Science 22 </booktitle> <pages> 1087-1096. </pages>
Reference: <author> McCullagh, P. and Nelder, J.A. </author> <year> 1989. </year> <title> Generalized Linear Models. </title> <publisher> London: Chapman and Hall. </publisher>
Reference-contexts: Generalized model classes. A major achievement of statistical methodological research has been the development of very general and flexible model classes. Generalized Linear Models, for instance, embrace many classical linear models, and unify estimation and testing theory for such models <ref> (McCullagh and Nelder, 1989) </ref>. Generalized Additive Models show similar potential (Hastie and Tibshirani, 1990). Graphical models (Lauritzen, 1996) represent probabilistic and statistical models with planar graphs, where the vertices represent (possibly latent) random variables and the edges represent stochastic dependences.
Reference: <author> Michelangeli, P. A., Vautard, R., and Legras, B. </author> <year> 1995. </year> <title> Weather regimes: recurrence and quasi-stationarity. </title> <journal> Journal of the Atmospheric Sciences 52(8) </journal> <pages> 1237-56. </pages>
Reference: <author> Miller, R. G. Jr. </author> <year> 1981. </year> <title> Simultaneous statistical inference (Second Edition). </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: In special cases, rules (some 28 C. GLYMOUR, D. MADIGAN, D. PREGIBON AND P. SMYTH times called contrasts) exist for simultaneously testing several hypotheses <ref> (Miller, 1981) </ref>. An important corollary for data mining is that the ff level of a test has nothing directly to do with the probability of error in a search procedure that involves testing a series of hypothesis.
Reference: <author> Neyman, J. and Pearson, E.S. </author> <year> 1933. </year> <title> On the problem of the most efficient tests of statistical hypotheses. </title> <journal> Philosophical Transactions of the Royal Society (Series A) 231 </journal> <pages> 289-337. </pages>
Reference-contexts: The standard advice that statistics educators provide, and scientific journals rigidly adhere to, is to choose ff to be 0.05 or 0.01, regardless of sample size. These particular ff-levels arose in Sir Ronald Fisher's study of relatively small agricultural experiments (on the order of 30-200 plots). Textbook advice <ref> (e.g., Neyman and Pearson, 1933) </ref> has emphasized the need to take account of the power of the test against H A when setting ff, and somehow reduce ff when the sample size is large. This crucial but vague advice has largely fallen on deaf ears. 2.
Reference: <author> Raftery, A.E. </author> <year> 1995. </year> <title> Bayesian model selection in social research (with discussion). </title> <editor> In Sociological Methodology (ed. P. V. Marsden), </editor> <publisher> Oxford, U.K.: Blackwells, </publisher> <pages> 111-196. </pages>
Reference-contexts: Typical rules assign models a value determined by the likelihood function associated with the model, the number of parameters, or dimension, of the model, and the data. Popular rules include the Akaike Information Criterion (Akaike, 1974), Bayes Information Criterion <ref> (Raftery, 1995) </ref>, and Minimum Description length (Rissanen, 1978). Given a prior probability distribution over models, the posterior probability on the data is itself a scoring function, arguably a privileged one. The Bayes Information Criterion approximates posterior probabilities in large samples.
Reference: <author> Rissanen, J. </author> <year> 1978. </year> <title> Modeling by shortest data description. </title> <type> Automatica 14 </type> <pages> 465-471. </pages>
Reference-contexts: Typical rules assign models a value determined by the likelihood function associated with the model, the number of parameters, or dimension, of the model, and the data. Popular rules include the Akaike Information Criterion (Akaike, 1974), Bayes Information Criterion (Raftery, 1995), and Minimum Description length <ref> (Rissanen, 1978) </ref>. Given a prior probability distribution over models, the posterior probability on the data is itself a scoring function, arguably a privileged one. The Bayes Information Criterion approximates posterior probabilities in large samples.
Reference: <author> Schervish, M.J. </author> <year> 1995. </year> <institution> Theory of Statistics, </institution> <address> New York: </address> <publisher> Springer Verlag. </publisher>
Reference: <author> Schwartz, G. </author> <year> 1978. </year> <title> Estimating the dimension of a model. </title> <journal> Annals of Statistics 6 </journal> <pages> 461-464. </pages>
Reference-contexts: The Bayes Information Criterion approximates posterior probabilities in large samples. There is a notion of consistency appropriate to scoring rules; in the large sample limit, almost surely the true model should be among those receiving maximal scores. AIC scores are not, in general, consistent <ref> (Schwartz, 1978) </ref>. There are also uncertainties associated with scores, since two different samples of the same size from the same distribution may yield not only different numerical values for the same model, but even different orderings of models.
Reference: <author> Selvin, H. and Stuart, A. </author> <year> 1966. </year> <title> Data dredging procedures in survey analysis. </title> <booktitle> The American Statistician 20(3) </booktitle> <pages> 20-23. </pages>
Reference-contexts: Is Data Mining "Statistical Deja Vu" (All Over Again)? In the mid 1960's, the statistics community referred to unfettered exploration of data as "fishing" or "data dredging" <ref> (Selvin and Stuart, 1966) </ref>. The community, enamored by elegant (analytical) mathematical solutions to inferential problems, argued that since their theories were invalidated by "looking at the data", it was wrong to do so. The major proponent of the exploratory data analysis (EDA) school, J.W.
Reference: <author> Simpson, C. H. </author> <year> 1951. </year> <title> The interpretation of interaction in contingency tables. </title> <journal> Journal of the Royal Statistical Society (Series B) 13 </journal> <pages> 238-241. </pages>
Reference-contexts: Paradoxically, it could even be the case that appendectomy adversely affects outcomes for both high-risk patients and low-risk patients, but appears to positively affect outcomes when the low-risk and high-risk patients are combined. WHN do not provide enough data to check whether this so-called "Simpson's Paradox" <ref> (Simpson, 1951) </ref> occurred in this example. However, Table 3 presents data that are plausible and consistent with WHN's data. Table 3. Fictitious data consistent with the Wen et al. (1995) data.
Reference: <author> Smith, A. F. M. and Roberts, G. </author> <year> 1993. </year> <title> Bayesian computation via the Gibbs sampler and related Markov chain Monte Carlo methods (with discussion). </title> <journal> Journal of the Royal Statistical Society (Series B) 55 </journal> <pages> 3-23. </pages>
Reference: <author> Spirtes, P., Glymour C., and Scheines, R. </author> <year> 1993. </year> <title> Causation, Prediction and Search, </title> <booktitle> Springer Lecture Notes in Statistics, </booktitle> <address> New York: </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: Indeed, controlling for possibly confounding variables with multiple regression can in some cases produce inferior estimates of effect sizes. Procedures recently developed in the artificial intelligence and statistics literature <ref> (Spirtes et al., 1993) </ref> address some of the problems associated with latent variables and mixing, but so far only for two families of probability distributions, the normal and multinomial. STATISTICAL THEMES AND LESSONS FOR DATA MINING 37 5.2. <p> For graphical representations of causal hypotheses according to the Markov condition, general algorithms for predicting the outcomes of interventions from complete or incomplete causal models were developed in <ref> (Spirtes et al., 1993) </ref>. Some of these procedures have been extended and made into a more convenient calculus by Pearl (1995). A related theory without graphical models was developed earlier by Rubin (1974) and others, and by Robbins (1986). Consider the following example.
Reference: <author> Stigler, S. M. </author> <year> 1986. </year> <title> The history of statistics: The measurement of uncertainty before 1900. </title> <publisher> Harvard: Harvard university Press. </publisher>
Reference-contexts: Inference to Causes. Understanding causation is the hidden force behind the historical development of statistics. From the beginning of the subject, in the work of Bernoulli and Laplace, the absence of causal connection between two variables has been taken to imply their probabilistic independence <ref> (see Stigler, 1986) </ref>, and the same idea is fundamental in the theory of experimental design (Fisher, 1958). Early in this century, Wright (1921) introduced directed graphs to represent causal hypotheses (with vertices as random variables and edges representing direct influ 30 C. GLYMOUR, D. MADIGAN, D. PREGIBON AND P.
Reference: <author> Wen, S.W., Hernandez, R., and Naylor, C.D. </author> <year> 1995. </year> <title> Pitfalls in nonrandomized studies: The case of incidental appendectomy with open cholecystectomy. </title> <journal> Journal of the American Medical Association 274 </journal> <pages> 1687-1691. </pages>
Reference: <author> Wright, S. </author> <year> 1921. </year> <title> Correlation and causation. </title> <journal> Journal of Agricultural Research 20 </journal> <pages> 557-585. </pages> <note> Received Date Accepted Date Final Manuscript Date </note>
References-found: 41

