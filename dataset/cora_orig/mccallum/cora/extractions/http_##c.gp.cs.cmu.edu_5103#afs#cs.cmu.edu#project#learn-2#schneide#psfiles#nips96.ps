URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/project/learn-2/schneide/psfiles/nips96.ps
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs/Web/People/awm/papers.html
Root-URL: http://www.cs.cmu.edu
Email: Email: schneide@cs.cmu.edu  
Title: Category: Control, Navigation and Planning Preference: Oral presentation Exploiting Model Uncertainty Estimates for Safe Dynamic
Author: Jeff G. Schneider 
Address: Pittsburgh, PA 15213  
Affiliation: The Robotics Institute, Carnegie Mellon University  
Abstract: Model learning combined with dynamic programming has been shown to be effective for learning control of continuous state dynamic systems. The simplest method assumes the learned model is correct and applies dynamic programming to it, but many approximators provide uncertainty estimates on the fit. How can they be exploited? This paper addresses the case where the system must be prevented from having catastrophic failures during learning. We propose a new algorithm adapted from the dual control literature and use Bayesian locally weighted regression models with stochastic dynamic programming. A common reinforcement learning assumption is that aggressive exploration should be encouraged. This paper addresses the converse case in which the system has to reign in exploration. The algorithm is illustrated on a 4 dimensional simulated control problem.
Abstract-found: 1
Intro-found: 1
Reference: [ Atkeson, 1989 ] <author> C. Atkeson. </author> <title> Using local models to control movement. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <year> 1989. </year>
Reference-contexts: Operate the system using the new policy and record additional data. 5. Repeat to step 2 while there is still some improvement in performance. In the rest of this section we describe steps 2 and 3. 2.1 Bayesian locally weighted regression We use a form of locally weighted regression <ref> [ Cleveland and Delvin, 1988, Atkeson, 1989, Moore, 1992 ] </ref> called Bayesian locally weighted regression [ Moore and Schneider, 1995 ] to build a model from data.
Reference: [ Atkeson, 1993 ] <author> C. Atkeson. </author> <title> Using local trajectory optimizers to speed up global optimization in dynamic programming. </title> <booktitle> In Advances in Neural Information Processing Systems (NIPS-6), </booktitle> <year> 1993. </year>
Reference-contexts: Dynamic programming methods such as reinforcement learning have the advantage that they do not make strong assumptions about the system, or the form of the performance measure. It has been suggested <ref> [ Atkeson, 1995, Atkeson, 1993 ] </ref> that techniques used in global linear control, including caution and probing, may also be applicable in the local case.
Reference: [ Atkeson, 1995 ] <author> C. Atkeson. </author> <title> Local methods for active learning. </title> <booktitle> Invited talk at AAAI Fall Symposium on Active Learning, </booktitle> <year> 1995. </year>
Reference-contexts: Dynamic programming methods such as reinforcement learning have the advantage that they do not make strong assumptions about the system, or the form of the performance measure. It has been suggested <ref> [ Atkeson, 1995, Atkeson, 1993 ] </ref> that techniques used in global linear control, including caution and probing, may also be applicable in the local case.
Reference: [ Bar-Shalom and Tse, 1976 ] <author> Y. Bar-Shalom and E. Tse. </author> <title> Concepts and Methods in Stochastic Control. </title> <publisher> Academic Press, </publisher> <year> 1976. </year>
Reference-contexts: The literature on adaptive and optimal linear control theory has explored this problem considerably under the names stochastic control and dual control. Overviews can be found in <ref> [ Kendrick, 1981, Bar-Shalom and Tse, 1976 ] </ref> . The control decision is based on three components call the deterministic, cautionary, and probing terms. The deterministic term assumes the model is perfect and attempts to control for the best performance.
Reference: [ Barto et al., 1983 ] <author> A. Barto, R. Sutton, and C. Anderson. </author> <title> Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <year> 1983. </year>
Reference-contexts: We assume we have a controller that can balance the pole and would like to learn to move the cart quickly to new positions, but never drop the pole during the learning process. The simulation equations and parameters are from <ref> [ Barto et al., 1983 ] </ref> and the task is illustrated at the top of fig. 2. The state vector is x = [ pole angle (), pole angular velocity ( _ ), cart position (), cart velocity ( _) ].
Reference: [ Cleveland and Delvin, 1988 ] <author> W. Cleveland and S. Delvin. </author> <title> Locally weighted regression: An approach to regression analysis by local fitting. </title> <journal> Journal of the American Statistical Association, </journal> <pages> pages 596-610, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: Operate the system using the new policy and record additional data. 5. Repeat to step 2 while there is still some improvement in performance. In the rest of this section we describe steps 2 and 3. 2.1 Bayesian locally weighted regression We use a form of locally weighted regression <ref> [ Cleveland and Delvin, 1988, Atkeson, 1989, Moore, 1992 ] </ref> called Bayesian locally weighted regression [ Moore and Schneider, 1995 ] to build a model from data.
Reference: [ Davies, 1996 ] <author> S. Davies. </author> <title> Applying grid-based interpolation to reinforcement learning. </title> <booktitle> In submitted Neural Information Processing Systems 9, </booktitle> <year> 1996. </year>
Reference-contexts: Multilinear interpolation is used to compute the value function at the mean of the predicted next state <ref> [ Davies, 1996 ] </ref> . * Otherwise, a stochastic update will be done according to eq. 2. The pdf of each of the state variables is stored, discretized at the same intervals as the grid representing the value function.
Reference: [ DeGroot, 1970 ] <author> M. </author> <title> DeGroot. Optimal Statistical Decisions. </title> <publisher> McGraw-Hill, </publisher> <year> 1970. </year>
Reference-contexts: The result of a prediction is a t distribution on the output that is well defined in the absence of data (see [ Moore and Schneider, 1995 ] and <ref> [ DeGroot, 1970 ] </ref> for details). The distribution of the prediction in regions where there is little data is crucial to the performance of the stochastic DP algorithm.
Reference: [ Dyer and McReynolds, 1970 ] <author> P. Dyer and S. McReynolds. </author> <title> The Computation and Theory of Optimal Control. </title> <publisher> Academic Press, </publisher> <year> 1970. </year>
Reference-contexts: This is repeated to obtain a total of 20 data points. That data is used to fit a global linear model mapping x onto x 0 . An LQR controller is constructed from the model and the given cost function following the derivation in <ref> [ Dyer and McReynolds, 1970 ] </ref> .
Reference: [ Gordon, 1995 ] <author> G. Gordon. </author> <title> Stable function approximation in dynamic programming. </title> <booktitle> In The 12th International Conference on Machine Learning, </booktitle> <year> 1995. </year>
Reference-contexts: Recent results on the convergence of dynamic programming methods when using various interpolation methods to represent the value (or cost-to-go) function have given a sound theoretical basis for applying reinforcement learning to continuous valued state spaces <ref> [ Gordon, 1995 ] </ref> . These are important steps toward the eventual application of these methods to industrial learning and control problems.
Reference: [ Kendrick, 1981 ] <author> D. Kendrick. </author> <title> Stochastic Control for Economic Models. </title> <publisher> McGraw-Hill, </publisher> <year> 1981. </year>
Reference-contexts: The literature on adaptive and optimal linear control theory has explored this problem considerably under the names stochastic control and dual control. Overviews can be found in <ref> [ Kendrick, 1981, Bar-Shalom and Tse, 1976 ] </ref> . The control decision is based on three components call the deterministic, cautionary, and probing terms. The deterministic term assumes the model is perfect and attempts to control for the best performance.
Reference: [ Moore and Atkeson, 1993 ] <author> A. Moore and C. Atkeson. </author> <title> Prioritized sweeping: Reinforcement learning with less data and less real time. </title> <journal> Machine Learning, </journal> <volume> 13(1) </volume> <pages> 103-130, </pages> <year> 1993. </year>
Reference: [ Moore and Schneider, 1995 ] <author> A. Moore and J. Schneider. </author> <title> Memory based stochastic optimization. </title> <booktitle> In Advances in Neural Information Processing Systems (NIPS-8), </booktitle> <year> 1995. </year>
Reference-contexts: In the rest of this section we describe steps 2 and 3. 2.1 Bayesian locally weighted regression We use a form of locally weighted regression [ Cleveland and Delvin, 1988, Atkeson, 1989, Moore, 1992 ] called Bayesian locally weighted regression <ref> [ Moore and Schneider, 1995 ] </ref> to build a model from data. When a query, x q , is made, each of the stored data points receives a weight w i = exp (kx i x q k 2 =K). <p> The result of a prediction is a t distribution on the output that is well defined in the absence of data (see <ref> [ Moore and Schneider, 1995 ] </ref> and [ DeGroot, 1970 ] for details). The distribution of the prediction in regions where there is little data is crucial to the performance of the stochastic DP algorithm.
Reference: [ Moore, 1992 ] <author> A. Moore. </author> <title> Fast, robust adaptive control by learning only forward models. </title> <booktitle> In Advances in Neural Information Processing Systems 4, </booktitle> <year> 1992. </year>
Reference-contexts: Operate the system using the new policy and record additional data. 5. Repeat to step 2 while there is still some improvement in performance. In the rest of this section we describe steps 2 and 3. 2.1 Bayesian locally weighted regression We use a form of locally weighted regression <ref> [ Cleveland and Delvin, 1988, Atkeson, 1989, Moore, 1992 ] </ref> called Bayesian locally weighted regression [ Moore and Schneider, 1995 ] to build a model from data.
Reference: [ Schaal and Atkeson, 1993 ] <author> S. Schaal and C. Atkeson. </author> <title> Assessing the quality of learned local models. </title> <booktitle> In Advances in Neural Information Processing Systems (NIPS-6), </booktitle> <year> 1993. </year>
Reference: [ Sutton, 1990 ] <author> R. Sutton. </author> <title> First results with dyna, an intergrated architecture for learning, planning, and reacting. </title> <booktitle> In AAAI Spring Symposium on Planning in Uncertain, Unpredictable, or Changing Environments, </booktitle> <year> 1990. </year>
References-found: 16

