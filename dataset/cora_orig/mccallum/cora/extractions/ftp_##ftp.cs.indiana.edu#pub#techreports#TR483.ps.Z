URL: ftp://ftp.cs.indiana.edu/pub/techreports/TR483.ps.Z
Refering-URL: http://www.cs.indiana.edu/trindex.html
Root-URL: 
Email: ajcbik@cs.indiana.edu  
Phone: 215,  
Title: A Note on Native Level 1 BLAS in Java  
Author: Aart J.C. Bik and Dennis B. Gannon 
Affiliation: Computer Science Department, Indiana University  
Address: Lindley Hall  Bloomington, Indiana 47405-4101, USA  
Abstract: In this research note, we explore the potential of extending the Java Application Programming Interface with some mathematical primitives to improve the performance of certain operations in Java programs while maintaining portability. In particular, we show that providing straightforward native implementations of primitives from Level 1 BLAS can already improve the performance substantially. On multi-processors, combining this native Level 1 BLAS with the multi-threading mechanism of Java may even provide a simple and portable way to obtain a Java program that runs faster than compiled serial C code.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Aart J.C. Bik and Dennis B. Gannon. </author> <title> Automatically exploiting implicit parallelism in Java. </title> <note> To Appear in a Special Issue of Concurrency, Practice and Experience, </note> <year> 1997. </year>
Reference-contexts: In earlier work <ref> [1, 2] </ref>, we have shown how loop parallelization can be expressed in Java by means of multi-threading.
Reference: [2] <author> Aart J.C. Bik, Juan E. Villacis, and Dennis B. Gannon. </author> <title> javar manual. </title> <institution> Computer Science Department, Indiana University, </institution> <year> 1997. </year> <note> This manual and the complete source of javar is made available at http://www.extreme.indiana.edu/hpjava/. </note>
Reference-contexts: In earlier work <ref> [1, 2] </ref>, we have shown how loop parallelization can be expressed in Java by means of multi-threading.
Reference: [3] <author> Zoran Budimlic and Ken Kennedy. </author> <title> Optimizing Java theory and practice. </title> <note> To Appear in a Special Issue of Concurrency, Practice and Experience, </note> <year> 1997. </year>
Reference-contexts: Clearly, for Java applications that are computational intensive, it would be desirable to reduce this performance penalty without sacrificing the portability of the language. One approach, for example, is to optimize the Java bytecode <ref> [3, 5] </ref>, either at compile-time (where a machine independent bytecode to bytecode optimization is added as additional phase to the Java compiler), or at run-time (where optimizations that require knowledge of the target machine are applied before execution).
Reference: [4] <author> Bryan Carpenter, Yuh-Jye Chang, Geoffrey C. Fox, Donald Leskiw, and Xiaoming Li. </author> <title> Experiments with HP Java. </title> <note> To Appear in a Special Issue of Concurrency, Practice and Experience, </note> <year> 1997. </year>
Reference: [5] <author> Michal Cierniak and Wei Li. </author> <title> Optimizing Java bytecodes. </title> <note> To Appear in a Special Issue of Concurrency, Practice and Experience, </note> <year> 1997. </year>
Reference-contexts: Clearly, for Java applications that are computational intensive, it would be desirable to reduce this performance penalty without sacrificing the portability of the language. One approach, for example, is to optimize the Java bytecode <ref> [3, 5] </ref>, either at compile-time (where a machine independent bytecode to bytecode optimization is added as additional phase to the Java compiler), or at run-time (where optimizations that require knowledge of the target machine are applied before execution).
Reference: [6] <author> Jack J. Dongarra, Jeremy Du Croz, Sven Hammarling, and Iain Duff. </author> <title> A set of level 3 basic linear algebra subprograms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 16:117, </volume> <year> 1990. </year>
Reference-contexts: The original set of vector-vector operations is now commonly referred to as Level 1 BLAS [14, 15]. The set has been extended to Level 2 BLAS [8, 9] and Level 3 BLAS <ref> [6, 7] </ref> to provide more opportunities to exploit vector processing facilities for matrix-vector operations and memory hierarchies or parallelism for matrix-matrix operations, respectively. Once an efficient implementation of BLAS is available, new mathematical software can be easily build on top of the primitives.
Reference: [7] <author> Jack J. Dongarra, Jeremy Du Croz, Sven Hammarling, and Iain Duff. </author> <title> A set of level 3 basic linear algebra subprograms: Model implementation and test programs. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 16 </volume> <pages> 18-28, </pages> <year> 1990. </year>
Reference-contexts: The original set of vector-vector operations is now commonly referred to as Level 1 BLAS [14, 15]. The set has been extended to Level 2 BLAS [8, 9] and Level 3 BLAS <ref> [6, 7] </ref> to provide more opportunities to exploit vector processing facilities for matrix-vector operations and memory hierarchies or parallelism for matrix-matrix operations, respectively. Once an efficient implementation of BLAS is available, new mathematical software can be easily build on top of the primitives.
Reference: [8] <author> Jack J. Dongarra, Jeremy Du Croz, Sven Hammarling, and Richard J. Hanson. </author> <title> An extended set of FORTRAN basic linear algebra subprograms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 14 </volume> <pages> 1-17, </pages> <year> 1988. </year>
Reference-contexts: The most well-known example of such a set of routines is formed by the Basic Linear Al- gebra Subprograms [10, ch5]. The original set of vector-vector operations is now commonly referred to as Level 1 BLAS [14, 15]. The set has been extended to Level 2 BLAS <ref> [8, 9] </ref> and Level 3 BLAS [6, 7] to provide more opportunities to exploit vector processing facilities for matrix-vector operations and memory hierarchies or parallelism for matrix-matrix operations, respectively. Once an efficient implementation of BLAS is available, new mathematical software can be easily build on top of the primitives.
Reference: [9] <author> Jack J. Dongarra, Jeremy Du Croz, Sven Hammarling, and Richard J. Hanson. </author> <title> An extended set of FORTRAN basic linear algebra subprograms: Model implementation and test programs. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 14 </volume> <pages> 18-32, </pages> <year> 1988. </year>
Reference-contexts: The most well-known example of such a set of routines is formed by the Basic Linear Al- gebra Subprograms [10, ch5]. The original set of vector-vector operations is now commonly referred to as Level 1 BLAS [14, 15]. The set has been extended to Level 2 BLAS <ref> [8, 9] </ref> and Level 3 BLAS [6, 7] to provide more opportunities to exploit vector processing facilities for matrix-vector operations and memory hierarchies or parallelism for matrix-matrix operations, respectively. Once an efficient implementation of BLAS is available, new mathematical software can be easily build on top of the primitives.
Reference: [10] <author> Jack J. Dongarra, Iain S. Duff, Danny C. Sorensen, and Henk A. van der Vorst. </author> <title> Solving Linear Systems on Vector and Shared Memory Computers. </title> <institution> Society for Industrial and Applied Mathematics, </institution> <year> 1991. </year>
Reference-contexts: The most well-known example of such a set of routines is formed by the Basic Linear Al- gebra Subprograms <ref> [10, ch5] </ref>. The original set of vector-vector operations is now commonly referred to as Level 1 BLAS [14, 15].
Reference: [11] <author> Jack J. Dongarra et al. </author> <title> Java Linpack Benchmark. </title> <note> This benchmark is made available at http://www.netlib.org/benchmark/linpackjava/. </note>
Reference-contexts: for example, can be implemented in a straightforward manner in C as follows, where the macro unhand () is used to dereference an object handle: 1 Because Java does not support passing of arbitrary sub-arrays, offsets into arrays are added as additional parameters (cf. the Java implementation of BLAS in <ref> [11] </ref>). 2 #include &lt;StubPreamble.h&gt; #include "Blas.h" double Blas_ddot (struct HBlas *this, long n, HArrayOfDouble *x_, long xoff, long incx, HArrayOfDouble *y_, long yoff, long incy) - double *x = unhand (x_)-&gt;body; double *y = unhand (y_)-&gt;body; double d = 0; if (n &gt; 0) - if ((incx == 1) && (incy <p> Obviously, on this uni-processor, no speedup can be expected from loop parallelization. 3.2 Linpack Benchmark In this section, we present some performance numbers for a Java Linpack benchmark <ref> [11] </ref> that solves a system of linear equations (i.e. factorization followed by forward and back substitution). n JDK1.0.2 500 Mflops: 3.3 Time: 25.8 s. Norm Res: 5.28 1000 Mflops: 3.2 Time: 210.9 s. Norm Res: 9.61 JDK1.0.2 + native Level 1 BLAS 500 Mflops: 8.1 Time: 10.3 s.
Reference: [12] <author> Geoffrey C. Fox and Wojtek Furmanski. </author> <title> Java for parallel computing and as a general language for scientific and engineering simulation and modelling. </title> <note> To Appear in a Special Issue of Concurrency, Practice and Experience, </note> <year> 1997. </year>
Reference: [13] <author> James Gosling, Bill Joy, and Guy Steele. </author> <title> Java Programming Language. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1996. </year>
Reference-contexts: 1 Introduction Portability of the Java programming language <ref> [13] </ref> is obtained by compiling Java programs into architectural neutral instructions (bytecode) of the Java Virtual Machine (JVM) [16], rather than into native machine code. Bytecode runs on any platform that supports an implementation of the JVM.
Reference: [14] <author> C.L. Lawson, R.J. Hanson, D.R. Kincaid, and F.T. Krogh. </author> <title> Algorithm 539: Basic lin-ear algebra subprograms for FORTRAN usage. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 5 </volume> <pages> 324-325, </pages> <year> 1979. </year>
Reference-contexts: The most well-known example of such a set of routines is formed by the Basic Linear Al- gebra Subprograms [10, ch5]. The original set of vector-vector operations is now commonly referred to as Level 1 BLAS <ref> [14, 15] </ref>. The set has been extended to Level 2 BLAS [8, 9] and Level 3 BLAS [6, 7] to provide more opportunities to exploit vector processing facilities for matrix-vector operations and memory hierarchies or parallelism for matrix-matrix operations, respectively.
Reference: [15] <author> C.L. Lawson, R.J. Hanson, D.R. Kincaid, and F.T. Krogh. </author> <title> Basic linear algebra subpro-grams for FORTRAN usage. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 5 </volume> <pages> 308-323, </pages> <year> 1979. </year>
Reference-contexts: The most well-known example of such a set of routines is formed by the Basic Linear Al- gebra Subprograms [10, ch5]. The original set of vector-vector operations is now commonly referred to as Level 1 BLAS <ref> [14, 15] </ref>. The set has been extended to Level 2 BLAS [8, 9] and Level 3 BLAS [6, 7] to provide more opportunities to exploit vector processing facilities for matrix-vector operations and memory hierarchies or parallelism for matrix-matrix operations, respectively.
Reference: [16] <author> Tim Lindholm and Frank Yellin. </author> <title> The Java Virtual Machine Specification. </title> <address> AddisonWesley, Reading, Massachusetts, </address> <year> 1996. </year>
Reference-contexts: 1 Introduction Portability of the Java programming language [13] is obtained by compiling Java programs into architectural neutral instructions (bytecode) of the Java Virtual Machine (JVM) <ref> [16] </ref>, rather than into native machine code. Bytecode runs on any platform that supports an implementation of the JVM. Although the interpretation of bytecode is substantially faster than the interpretation of most high level languages, still a performance penalty must be paid for this portability.
Reference: [17] <author> Beth Stearns. </author> <title> Integrating Native Code and Java Programs. </title> <note> This documents is made available at http://java.sun.com/nav/read/Tutorial/native1.1/. 7 8 9 10 11 12 13 </note>
Reference-contexts: More details on integrating native methods in Java can be found on the Web <ref> [17] </ref>. 3 Experiments In this section, we present the results of a series of experiments that have been conducted on an IBM RS/6000 G30 with four PowerPC 604 processors using the AIX4.2 JDK1.0.2B (with JITC) and the AIX4.2 JDK1.1beta (without JITC), a Sun with two 175MHz. ultra SPARC processors using the
References-found: 17

