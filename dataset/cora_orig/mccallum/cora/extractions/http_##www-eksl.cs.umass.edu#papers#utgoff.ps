URL: http://www-eksl.cs.umass.edu/papers/utgoff.ps
Refering-URL: http://www-eksl.cs.umass.edu/iil/iil-papers.html
Root-URL: 
Email: futgoffjcoheng@cs.umass.edu  
Title: Applicability of Reinforcement Learning  
Author: Paul E. Utgoff Paul R. Cohen 
Keyword: activity, hierarchical reinforcement learning, autonomous agent, Q learning, activity-independent task specification, function approximator, hierarchical control, continuity of state trajectory, operator model, compilation vs reasoning, statically structured spaces, planning.  
Note: Contact author: utgoff@cs.umass.edu, 413-545-4843  
Address: Amherst, MA 01002  
Affiliation: Department of Computer Science University of Massachusetts  
Abstract: We describe our experiences in trying to implement a hierarchical reinforcement learning system. This includes our objectives before we started, the problems we encountered along the way, the solutions we devised for some of these problems, and our conclusions afterward about the kinds of tasks for which reinforcement learning may be suitable. Our experience has made it clearer to us when and when not to select a reinforcement learning method. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Dietterich, T. G. </author> <year> (1997). </year> <journal> Machine learning research. AI Magazine (pp. </journal> <pages> 97-136). </pages>
Reference-contexts: Researchers have begun to study how skills learned through reinforcement learning can be composed, controlled, and learned effectively. This work is still in its infancy, and represents one of the open problems in reinforcement learning <ref> (Dietterich, 1997) </ref>. Kaelbling (1993) discusses her HDG learning algorithm, which uses landmarks as subgoals in its state space. This helps to organize the state space hierarchically, so that the agent can consider travel in larger chunks by travelling from one landmark to the next.
Reference: <author> Kaelbling, L. P. </author> <year> (1993). </year> <title> Hierarchical learning in stochastic domains: Preliminary results. </title> <booktitle> Proceedings of the Tenth International Conference on Ma 15 chine Learning (pp. </booktitle> <pages> 167-173). </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Mataric, M. J. </author> <year> (1994). </year> <title> Reward functions for accelerated learning. </title> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference (pp. </booktitle> <pages> 181-189). </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Distance is measured by the sum of the distances of the current mapped state values to the goal values for the goal variables. Distance reduction is a form of progress estimator <ref> (Mataric, 1994) </ref>. Tsitsiklis and Van Roy (1996) discuss the importance of online sampling more generally. 5 An Experiment The simulated system as described above was run for 2,000 trials. For each trial, the system was initialized to a quasi-random state, and then allowed to run until the goal was achieved.
Reference: <author> Nilsson, N. J. </author> <year> (1965). </year> <title> Learning machines. </title> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference-contexts: The resulting Q function is much like a -machine <ref> (Nilsson, 1965) </ref> because it partitions the mapped state space into convex regions in which one of the operator (action) values is greater the others. The activity hierarchy is shown in Figure 1. The lowest level activities are the six primitive operators.
Reference: <author> Sutton, R. S., & Barto, A. G. </author> <title> (in press). Reinforcement learning: An introduction. </title> <publisher> MIT Press. </publisher>
Reference: <author> Tadepalli, P., & Dietterich, T. G. </author> <year> (1997). </year> <title> Hierarchical explanation-based reinforcement learning. </title> <booktitle> Machine Learning: Proceedings of the Fourteenth International Conference (pp. </booktitle> <pages> 358-366). </pages> <address> Nashville, TN: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Tesauro, G. </author> <year> (1992). </year> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 257-277. </pages>
Reference-contexts: It is not practical to assume a state space with a fixed constituency, at least for autonomous agents in dynamic spaces. It can be practical for fixed-constituency spaces such as backgammon <ref> (Tesauro, 1992) </ref>. Of course, the present theory of reinforcement learning is not tied to a vector representation of state, though most function approximators are.
Reference: <author> Thrun, S., & Schwartz, A. </author> <year> (1995). </year> <title> Finding structure in reinforcement learning. </title>
Reference: <editor> In Tesauro, Touretzky & Leen (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufman. </publisher>
Reference: <author> Tsitsiklis, J. N. , & Van Roy, B. </author> <year> (1996). </year> <title> An analysis of temporal-difference learning with function approximation, </title> <type> (Technical report LIDS-P-2322), </type> <address> Cambridge, MA: </address> <publisher> MIT. </publisher>
Reference: <author> Watkins, C.J.C.H., & Dayan, P. </author> <year> (1992). </year> <title> Q-Learning. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 279-292. </pages>
Reference-contexts: Enough problems arose in the simulated world that we have yet to make such a transfer. A related project which tried to implement hiearchical reinforcement learning on the Pioneer, ran into the same difficulties as this one. We adopted Q-learning <ref> (Watkins & Dayan, 1992) </ref> because it does not require an operator model. In a robotics application, the successor state that will result from the application of an operator is not entirely predictable.
References-found: 11

