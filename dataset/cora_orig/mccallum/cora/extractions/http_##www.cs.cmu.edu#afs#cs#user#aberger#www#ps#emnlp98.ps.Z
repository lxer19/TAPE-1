URL: http://www.cs.cmu.edu/afs/cs/user/aberger/www/ps/emnlp98.ps.Z
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/aberger/www/lm.html
Root-URL: 
Email: aberger@cs.cmu.edu  printz@watson.ibm.com  
Title: A Comparison of Criteria for Maximum Entropy Minimum Divergence Feature Selection  
Author: Adam Berger Harry Printz 
Address: Pittsburgh, PA 15232  Yorktown Heights, NY 10598  
Affiliation: Carnegie Mellon University School of Computer Science  IBM Watson Research Center  
Abstract: In this paper we study the gain, a naturally-arising statistic from the theory of memd modeling [2], as a figure of merit for selecting features for an memd language model. We compare the gain with two popular alternatives|empirical activation and mutual information|and argue that the gain is the preferred statistic, on the grounds that it directly measures a feature's contribution to improving upon the base model. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Beeferman, A. Berger, J. Lafferty, </author> <title> "A Model of Lexical Attraction and Repulsion," </title> <booktitle> Proc. of the ACL-EACL'97 Joint Conference, </booktitle> <address> Madrid, Spain. </address>
Reference-contexts: A fundamental issue in applying this technique is the criterion used to select features. The work described in [3], for instance, incorporates every feature which either appears with above-threshold count in a training corpus, or which exhibits high mutual information. In [11] and <ref> [1] </ref>, the authors select features based on a mutual information statistic. As we argue below, both these methods have drawbacks. In this paper, we examine a statistic for selecting memd model features, called the gain. The gain was introduced in [4], and studied in greater detail in [5] and [2]. <p> Use of a Base Model In the work reported here, the base model q is decidedly not a constant: it is a linearly-interpolated trigram model, trained on a corpus of 44,761,334 words. This approach, while not novel <ref> [1] </ref>, is one of the key departures of our work from [3]. This departure is significant for three reasons. First, it gives us a computationally efficient way to incorporate a large amount of valuable information into our model. <p> Some words are in fact good predictors of themselves: seeing Japanese once in a sentence raises the likelihood it will appear again later. Word pairs such as these, where the appearance of the first is strongly correlated with the subsequent appearance of the second, are called trigger pairs <ref> [1, 11] </ref>. Note that the trigger property is not necessarily symmetric: we would expect a left parenthesis ( to trigger a right parenthesis ), but not the other way around. Our model incorporates these relationships through trigger features. Let u; v be some trigger pair.
Reference: [2] <author> A. Berger, S. Della Pietra, V. Della Pietra, </author> <title> "A Maximum Entropy Approach to Natural Language Processing," </title> <journal> Computational Linguistics, </journal> <volume> 22(1): </volume> <pages> 39-71, </pages> <month> March </month> <year> 1996. </year>
Reference-contexts: Introduction Maximum entropy / minimum divergence (memd) modeling is a powerful technique for building statistical models of linguistic phenomena. It has been applied to problems as diverse as machine translation <ref> [2] </ref>, parsing [10], word morphology [5] and language modeling [6, 11, 3, 9]. The heart of the method is to choose a collection of informative features, each encoding some linguistically significant event, and then to incorporate these features into a family of conditional models. <p> As we argue below, both these methods have drawbacks. In this paper, we examine a statistic for selecting memd model features, called the gain. The gain was introduced in [4], and studied in greater detail in [5] and <ref> [2] </ref>. We present intuition, theory and experimental results for this statistic, as a criterion for selecting features for an memd language model. We believe our work marks the first time it has been used in memd language modeling, and the first side-by-side comparison with other selection criteria. <p> Fundamentals of MEMD Models The individual word probabilities p (w i j h i ) appearing in equation (1) above are determined by a minimum divergence model. Here we review of the fundamentals of such models; a thorough description appears in reference <ref> [2] </ref>. As above, let w stand for the word or future to be predicted, and let h stand for the history upon which this prediction is based. Suppose that f (w h) is a binary-valued indicator function of some linguistic event. <p> By familiar manipulations with Lagrange multipliers, as detailed in <ref> [2] </ref>, the solution to this problem can be shown to be p (w j h) = Z ( ff h) where Z ( ff h) = w2V Here f (w h) is a vector of 0s and 1s, depending upon the value of each feature function at the point w h. <p> We supplied the resulting candidate set F , containing 1,538,998 features, to the next stage of the feature selection process. Ranking In this section we will motivate and develop the central feature of this paper, which is the notion of gain. First introduced in <ref> [2] </ref>, and further developed in [5], the gain is a statistic computed for a given feature f, with respect to a base model, over some fixed corpus. We will argue that the gain is the appropriate figure of merit for ranking features. <p> We regard this approach as eminently reasonable. But there is this danger of inefficiency: we may incorporate two or more features that capture essentially the same linguistic information. As a prophyllaxis against this, some authors <ref> [2] </ref> have advocated feature induction. Feature induction is an Scatterplot of feature gain against empirical activation. Right: Scatterplot of feature gain against mutual information. iterative algorithm for choosing features; it selects one new feature on each iteration.
Reference: [3] <author> C. Chelba, et. al., </author> <title> "Structure and Performance of a Dependency Language Model," </title> <booktitle> Proc. of Eu-rospeech '97, </booktitle> <address> Rhodes, Greece, </address> <month> September </month> <year> 1997. </year>
Reference-contexts: Introduction Maximum entropy / minimum divergence (memd) modeling is a powerful technique for building statistical models of linguistic phenomena. It has been applied to problems as diverse as machine translation [2], parsing [10], word morphology [5] and language modeling <ref> [6, 11, 3, 9] </ref>. The heart of the method is to choose a collection of informative features, each encoding some linguistically significant event, and then to incorporate these features into a family of conditional models. A fundamental issue in applying this technique is the criterion used to select features. <p> The heart of the method is to choose a collection of informative features, each encoding some linguistically significant event, and then to incorporate these features into a family of conditional models. A fundamental issue in applying this technique is the criterion used to select features. The work described in <ref> [3] </ref>, for instance, incorporates every feature which either appears with above-threshold count in a training corpus, or which exhibits high mutual information. In [11] and [1], the authors select features based on a mutual information statistic. As we argue below, both these methods have drawbacks. <p> Though our experimental results concern language models exclusively, we note that the gain can be used to select features for any memd model on a discrete space. The language model we present is based on dependency grammars. It is similar to, but extends upon, the work reported in <ref> [3] </ref>. Two important differences between that work and ours are that ours is a true minimum-divergence model, and ours incorporates both link and trigger features. The paper is organized as follows. <p> Use of a Base Model In the work reported here, the base model q is decidedly not a constant: it is a linearly-interpolated trigram model, trained on a corpus of 44,761,334 words. This approach, while not novel [1], is one of the key departures of our work from <ref> [3] </ref>. This departure is significant for three reasons. First, it gives us a computationally efficient way to incorporate a large amount of valuable information into our model. <p> Tests and Results Our experiments were designed to address three issues. First, given a training corpus over 20 times larger than the Switchboard transcripts used in <ref> [3] </ref>, we were curi-ous to see how large a model we could feasibly train. Second, we wanted to conduct an experimental study of the gain as a criterion for feature selection, compared to empirical activation and mutual information. Third, we wished to investigate the addivity of the gain.
Reference: [4] <author> S. Della Pietra, V. Della Pietra, J. Gillett, J. Laf-ferty, H. Printz, L. Ures, </author> <title> "Inference and Estimation of a Long-Range Trigram Model," </title> <booktitle> Second International Colloquium on Grammatical Inference, </booktitle> <address> Alicante, Spain, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: In [11] and [1], the authors select features based on a mutual information statistic. As we argue below, both these methods have drawbacks. In this paper, we examine a statistic for selecting memd model features, called the gain. The gain was introduced in <ref> [4] </ref>, and studied in greater detail in [5] and [2]. We present intuition, theory and experimental results for this statistic, as a criterion for selecting features for an memd language model.
Reference: [5] <author> S. Della Pietra, V. Della Pietra, and J. Lafferty, </author> <title> Inducing Features of Random Fields, </title> <journal> ieee Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 19(4): </volume> <pages> 380-393, </pages> <month> April </month> <year> 1997. </year>
Reference-contexts: Introduction Maximum entropy / minimum divergence (memd) modeling is a powerful technique for building statistical models of linguistic phenomena. It has been applied to problems as diverse as machine translation [2], parsing [10], word morphology <ref> [5] </ref> and language modeling [6, 11, 3, 9]. The heart of the method is to choose a collection of informative features, each encoding some linguistically significant event, and then to incorporate these features into a family of conditional models. <p> As we argue below, both these methods have drawbacks. In this paper, we examine a statistic for selecting memd model features, called the gain. The gain was introduced in [4], and studied in greater detail in <ref> [5] </ref> and [2]. We present intuition, theory and experimental results for this statistic, as a criterion for selecting features for an memd language model. We believe our work marks the first time it has been used in memd language modeling, and the first side-by-side comparison with other selection criteria. <p> We supplied the resulting candidate set F , containing 1,538,998 features, to the next stage of the feature selection process. Ranking In this section we will motivate and develop the central feature of this paper, which is the notion of gain. First introduced in [2], and further developed in <ref> [5] </ref>, the gain is a statistic computed for a given feature f, with respect to a base model, over some fixed corpus. We will argue that the gain is the appropriate figure of merit for ranking features. Motivation At the heart of the issue lie the following two questions. <p> This fact is demonstrated in <ref> [5] </ref>, along with a proof that the maximizing ff ? is unique. Thus the probability of the complete corpus, according to the memd model p ff ? , is just P fff ? (C). <p> Models Trained We trained a total of fifteen models; in all cases we trained on the complete corpus. We performed memd training using the improved iterative scaling algorithm of <ref> [5] </ref>, using the relative change in conditional perplexity, R t , as a stopping criterion.
Reference: [6] <author> R. Lau, R. Rosenfeld, S. Roukos, </author> <title> "Trigger-Based Language Models: a Maximum Entropy Approach," </title> <booktitle> Proc. of the International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pp II: 45-48, </pages> <address> Minneapolis, MN, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: Introduction Maximum entropy / minimum divergence (memd) modeling is a powerful technique for building statistical models of linguistic phenomena. It has been applied to problems as diverse as machine translation [2], parsing [10], word morphology [5] and language modeling <ref> [6, 11, 3, 9] </ref>. The heart of the method is to choose a collection of informative features, each encoding some linguistically significant event, and then to incorporate these features into a family of conditional models. A fundamental issue in applying this technique is the criterion used to select features.
Reference: [7] <author> D. Magerman, </author> <title> Natural Language Parsing as Statistical Pattern Recognition, </title> <type> Ph.D. Thesis, </type> <institution> Department of Computer Science, Stanford University, </institution> <address> Palo Alto, CA, </address> <month> February </month> <year> 1994. </year>
Reference-contexts: By parsed we mean that for each sentence S of the corpus text T , we have its linkage K (S) at our disposal. The parser we trained and then used was a modified version of the decision-tree parser described in <ref> [7] </ref>. Our parser training corpus consisted of 990,145 words of Treebank Release II data, and our base model corpus consisted of 44,761,334 words of Wall Street Journal data, both prepared by the Linguistic Data Consortium. This parser constructs a conventional parse tree.
Reference: [8] <author> H. Printz, </author> <title> "Fast Computation of Maximum Entropy / Minimum Divergence Feature Gain," </title> <booktitle> submitted to International Conference on Speech and Language Processing, </booktitle> <address> Sydney, Australia, </address> <month> Septem-ber </month> <year> 1998. </year>
Reference-contexts: Clearly, computing a feature's gain is intimately related to training an memd model containing this single feature. But because the model p ff ? involves only one feature, substantial computational speedup is possible. A fast algorithm for computing the gain appears in <ref> [8] </ref>. The notion of gain extends naturally to a set of features M . If P M (C) is the corpus probability according to a trained memd model built with feature set M , then we define G M = (1=N ) log (P M (C)=P (C)).
Reference: [9] <author> P. S. Rao, S. Dharanipragada, S. Roukos, </author> <title> "MDI Adaptation of Language Models Across Corpora," </title> <booktitle> Proc. of Eurospeech '97, </booktitle> <pages> pages 1979-1982, </pages> <address> Rhodes, Greece, </address> <month> September </month> <year> 1997. </year>
Reference-contexts: Introduction Maximum entropy / minimum divergence (memd) modeling is a powerful technique for building statistical models of linguistic phenomena. It has been applied to problems as diverse as machine translation [2], parsing [10], word morphology [5] and language modeling <ref> [6, 11, 3, 9] </ref>. The heart of the method is to choose a collection of informative features, each encoding some linguistically significant event, and then to incorporate these features into a family of conditional models. A fundamental issue in applying this technique is the criterion used to select features.
Reference: [10] <author> A. Ratnaparkhi, J. Reynar, S. Roukos, </author> <title> "A Maximum Entropy Model for Prepositional Phrase Attachment," </title> <booktitle> Proc. of the Human Language Technology Workshop, </booktitle> <address> Plainsboro, NJ, </address> <month> March </month> <year> 1994. </year>
Reference-contexts: Introduction Maximum entropy / minimum divergence (memd) modeling is a powerful technique for building statistical models of linguistic phenomena. It has been applied to problems as diverse as machine translation [2], parsing <ref> [10] </ref>, word morphology [5] and language modeling [6, 11, 3, 9]. The heart of the method is to choose a collection of informative features, each encoding some linguistically significant event, and then to incorporate these features into a family of conditional models.
Reference: [11] <author> R. Rosenfeld, </author> <title> Adaptive Statistical Language Modeling: A Maximum Entropy Approach, </title> <type> Ph.D. Thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Introduction Maximum entropy / minimum divergence (memd) modeling is a powerful technique for building statistical models of linguistic phenomena. It has been applied to problems as diverse as machine translation [2], parsing [10], word morphology [5] and language modeling <ref> [6, 11, 3, 9] </ref>. The heart of the method is to choose a collection of informative features, each encoding some linguistically significant event, and then to incorporate these features into a family of conditional models. A fundamental issue in applying this technique is the criterion used to select features. <p> A fundamental issue in applying this technique is the criterion used to select features. The work described in [3], for instance, incorporates every feature which either appears with above-threshold count in a training corpus, or which exhibits high mutual information. In <ref> [11] </ref> and [1], the authors select features based on a mutual information statistic. As we argue below, both these methods have drawbacks. In this paper, we examine a statistic for selecting memd model features, called the gain. <p> Some words are in fact good predictors of themselves: seeing Japanese once in a sentence raises the likelihood it will appear again later. Word pairs such as these, where the appearance of the first is strongly correlated with the subsequent appearance of the second, are called trigger pairs <ref> [1, 11] </ref>. Note that the trigger property is not necessarily symmetric: we would expect a left parenthesis ( to trigger a right parenthesis ), but not the other way around. Our model incorporates these relationships through trigger features. Let u; v be some trigger pair.
References-found: 11

