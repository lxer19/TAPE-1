URL: http://suif.stanford.edu/papers/wolf92.ps
Refering-URL: http://suif.stanford.edu/papers/papers.html
Root-URL: 
Title: IMPROVING LOCALITY AND PARALLELISM IN NESTED LOOPS  
Author: Michael Edward Wolf 
Degree: a dissertation submitted to the department of computer science and the committee on graduate studies of stanford university in partial fulfillment of the requirements for the degree of doctor of philosophy By  
Date: August 1992  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> W. Abu-Sufah. </author> <title> Improving the Performance of Virtual Memory Computers. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> Nov. </month> <year> 1978. </year>
Reference-contexts: To achieve acceptable performance on scientific codes, then, we must make scientific code utilize the memory hierarchy more effectively. A well known technique for improving locality of reference in scientific code is called blocking <ref> [1] </ref>, [22], [28], [31], [43], or tiling [58]. Tiling can be applied to improve utilization of the cache, and also registers, virtual memory, the TLB (translation lookaside buffer) and so on. Tiled code can also be run effectively in parallel, which is important for multiple processor machines. <p> We use the notation ` + ' as shorthand for <ref> [1; 1] </ref>, `' as shorthand for [1; 1], and `' as shorthand for [1; 1]. They correspond to the Wolfe's directions ` &lt; ', ` &gt; ', and `fl' respectively [59]. <p> We use the notation ` + ' as shorthand for <ref> [1; 1] </ref>, `' as shorthand for [1; 1], and `' as shorthand for [1; 1]. They correspond to the Wolfe's directions ` &lt; ', ` &gt; ', and `fl' respectively [59]. <p> We use the notation ` + ' as shorthand for <ref> [1; 1] </ref>, `' as shorthand for [1; 1], and `' as shorthand for [1; 1]. They correspond to the Wolfe's directions ` &lt; ', ` &gt; ', and `fl' respectively [59]. <p> They correspond to the Wolfe's directions ` &lt; ', ` &gt; ', and `fl' respectively [59]. We also use the shorthand d + to mean <ref> [d; 1] </ref> and d to mean [1; d]. 2.3.2 Generality of Dependence Representation As we have shown, our notation allows us to represent both direction and distance vectors in a uniform notation. <p> They correspond to the Wolfe's directions ` &lt; ', ` &gt; ', and `fl' respectively [59]. We also use the shorthand d + to mean [d; 1] and d to mean <ref> [1; d] </ref>. 2.3.2 Generality of Dependence Representation As we have shown, our notation allows us to represent both direction and distance vectors in a uniform notation. <p> = . . . = d i1 = 0 */ if d max i &lt; 0 then return D; end if; i &gt; 0 then D = D [ f ~ d g; return D; end if; i := 0; i &gt; 0 then D.insert (ReplaceComponent ( ~ d ; <ref> [1; d max i ] </ref>; i); i := 0; end for; /* ~ d all zeros, if reach here */ return D; end; /* ReplaceComponent returns ~ d with the ith component set to c */ ReplaceComponent ( ~ d :DependenceVector,c:DependenceComponent,i:integer) return DependenceVector; ~ d 0 : DependenceVector := ~ d <p> Component addition is defined to be [a; b] + [c; d] = [a + c; b + d] where for all s 2 Z [ f1g, s + 1 is 1 and for all s 2 Z [ f1g, s + 1 is 1. Thus 2 + <ref> [3; 1] </ref> = [2; 2] + [3; 1] = [1; 1]. <p> Thus 2 + <ref> [3; 1] </ref> = [2; 2] + [3; 1] = [1; 1]. <p> Thus 2 + [3; 1] = [2; 2] + [3; 1] = <ref> [1; 1] </ref>. <p> Thus we use the model in Figure 2.10 (a) rather than that in Figure 2.10 (b). Using this model is equivalent to conceptually performing the non-basic-to-basic-loop transformation of Abu-Sufah <ref> [1] </ref> and Wolfe [59]: 6 6 Our approach involves a straightforward application of Abu-Sufah's non-basic-to-basic-loop transformation. However, he used it only for blocking, and was not precise about when the transformation is legal to apply. He also did not attempt to remove the extra computation from the innermost loop. <p> In recognition of this, the latest implementation of the Basic Linear Algebra Subprogram package, BLAS3, now uses tiled subroutines [22]. Abu-Sufah The first significant research into automatic improvement of paging performance by the compiler was undertaken by Abu-Sufah at the University of Illinois <ref> [1] </ref>, [2], CHAPTER 2. LOOP TRANSFORMATION THEORY BASICS 59 [3]. This work discussed a tiling-like transformation called vertical distribution, analyzed its dependence requirements, and discussed the resultant improvement of some programs with the transformation. <p> Since T~e is lexicographically positive, this completes the proof for sets of distance vectors. Before extending the proof to direction vectors, we first outline the difficulty of doing so. Suppose we have the transformation U = 2 0 1 3 5 and the dependence vector ~ d = 4 <ref> [0; 1] </ref> 5 . U ~ d = 4 [1; 1] 5 so the transformation appears to be illegal, even though in fact for each ~e 2 E ( ~ d ), U~e is lexicographically positive. <p> Before extending the proof to direction vectors, we first outline the difficulty of doing so. Suppose we have the transformation U = 2 0 1 3 5 and the dependence vector ~ d = 4 [0; 1] 5 . U ~ d = 4 <ref> [1; 1] </ref> 5 so the transformation appears to be illegal, even though in fact for each ~e 2 E ( ~ d ), U~e is lexicographically positive. <p> The fraction of reuse that is utilized is typically proportional to the inverse of these. 1=N and 1=B are both small, whereas there is a very large difference between 1=N and 1=u. 6 Others researchers have studied compiler optimization for non-cache levels of the memory hierarchy. Abu-Sufah <ref> [1] </ref>, [2] optimizes for virtual memory, but does so with the primitive method of tiling whenever legal (whether beneficial or not), as discussed in Section 2.8.
Reference: [2] <author> W. Abu-Sufah, D. J. Kuck, and D. H. Lawrie. </author> <title> Automatic program transformations for virtual memory computers. </title> <booktitle> Proc. 1979 National Computer Conference, </booktitle> <pages> pages 969-974, </pages> <month> June </month> <year> 1979. </year>
Reference-contexts: Thus 2 + [3; 1] = <ref> [2; 2] </ref> + [3; 1] = [1; 1]. <p> In recognition of this, the latest implementation of the Basic Linear Algebra Subprogram package, BLAS3, now uses tiled subroutines [22]. Abu-Sufah The first significant research into automatic improvement of paging performance by the compiler was undertaken by Abu-Sufah at the University of Illinois [1], <ref> [2] </ref>, CHAPTER 2. LOOP TRANSFORMATION THEORY BASICS 59 [3]. This work discussed a tiling-like transformation called vertical distribution, analyzed its dependence requirements, and discussed the resultant improvement of some programs with the transformation. <p> The fraction of reuse that is utilized is typically proportional to the inverse of these. 1=N and 1=B are both small, whereas there is a very large difference between 1=N and 1=u. 6 Others researchers have studied compiler optimization for non-cache levels of the memory hierarchy. Abu-Sufah [1], <ref> [2] </ref> optimizes for virtual memory, but does so with the primitive method of tiling whenever legal (whether beneficial or not), as discussed in Section 2.8.
Reference: [3] <author> W. Abu-Sufah, D. J. Kuck, and D. H. Lawrie. </author> <title> On the performance enhancement of paging systems through program analysis and transformations. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-30(5):341-356, </volume> <year> 1981. </year>
Reference-contexts: Component addition is defined to be [a; b] + [c; d] = [a + c; b + d] where for all s 2 Z [ f1g, s + 1 is 1 and for all s 2 Z [ f1g, s + 1 is 1. Thus 2 + <ref> [3; 1] </ref> = [2; 2] + [3; 1] = [1; 1]. <p> Thus 2 + <ref> [3; 1] </ref> = [2; 2] + [3; 1] = [1; 1]. <p> Abu-Sufah The first significant research into automatic improvement of paging performance by the compiler was undertaken by Abu-Sufah at the University of Illinois [1], [2], CHAPTER 2. LOOP TRANSFORMATION THEORY BASICS 59 <ref> [3] </ref>. This work discussed a tiling-like transformation called vertical distribution, analyzed its dependence requirements, and discussed the resultant improvement of some programs with the transformation. Abu-Sufah starts by laying out all matrices in submatrix form, "because of its advantages as presented in" [43].
Reference: [4] <author> A. Aiken and A. Nicolau. </author> <title> Loop quantization: an analysis and algorithm. </title> <type> Technical Report 87-821, </type> <institution> Cornell University, </institution> <year> 1987. </year>
Reference-contexts: Definition 2.5 A loop nest is tilable if its dependences after tiling by an with any positive tile size are lexicographically positive. If the tile sizes are known in advance, then loops may be able to be tiled without this condition being met. Loop quantization <ref> [4] </ref> takes advantage of this. Typically, a loop nest is not tilable by Definition 2.5 but still meets the conditions for loop quantization only for extremely small tile sizes.
Reference: [5] <author> J. R. Allen and K. Kennedy. </author> <title> PFC: A program to convert FORTRAN to parallel form. </title> <type> Technical Report MASC TR82-6, </type> <institution> Rice University, </institution> <month> March </month> <year> 1982. </year>
Reference-contexts: The SUIF language contains annotations, which phases use to communicate with one another. Annotations are used to pass information such as def-use chains and dependence information. The data locality optimizer is invoked after local and interprocedural constant propagation, induction variable substitution <ref> [5] </ref>, [56] and dead code elimination. After completion of locality optimization and parallelization, the SUIF IL is lowered; array expressions are translated into explicit address arithmetic and FOR, IF and LOOP control constructs into branches and labels. After lowering of the IL, scalar optimization and code generation are performed.
Reference: [6] <author> R. Allen and K. Kennedy. </author> <title> Automatic translation of FORTRAN programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <year> 1987. </year>
Reference-contexts: we need a floor it is a simple matter to adjust l j j i;0 and replace the ceiling with the floor, and likewise if a floor occurs where we need a ceiling.) If any loop increment is not one, then it must first be made so via loop normalization <ref> [6] </ref>. If the bounds are not of the proper form, then the given loop cannot be involved in any transformations, and the loop nest is effectively divided into two: those outside the loop and those nested in the loop. The iteration space described by these bounds is indeed a polytope. <p> Thus the loop bound finding algorithm is O (nq). 2.5 Tiling The tiling transformation is a primary transformation for improving locality (Chapter 4), and can also be employed to expose coarse-grain parallelism (Chapter 3). Tiling encompasses the well known transformations of strip-mine and interchange <ref> [6] </ref> and unroll-and-jam [6], as we shall discuss in Section 2.5.2. As with unimodular transformations, we must consider under what conditions the transformation is legal, and how applying it to a loop nest changes the loop nest. For this section, we only consider rectangular tiling. <p> Thus the loop bound finding algorithm is O (nq). 2.5 Tiling The tiling transformation is a primary transformation for improving locality (Chapter 4), and can also be employed to expose coarse-grain parallelism (Chapter 3). Tiling encompasses the well known transformations of strip-mine and interchange <ref> [6] </ref> and unroll-and-jam [6], as we shall discuss in Section 2.5.2. As with unimodular transformations, we must consider under what conditions the transformation is legal, and how applying it to a loop nest changes the loop nest. For this section, we only consider rectangular tiling. <p> Using distance information, wavefronting this loop is easy. The work done by Rice University, typified by Allen and Kennedy <ref> [6] </ref>, uses the same basic framework with, for the purposes of this discussion, the same general strengths and weaknesses. Loop Transformation as Matrix Transformation An alternate approach, based on matrix transformations, has been proposed and used for an important subset of loop nests.
Reference: [7] <author> C. Ancourt. </author> <title> Generation Automatique de Codes de Transfert pour Multiprocesseurs a Memoires Locales. </title> <type> PhD thesis, </type> <institution> Universite Pierre et Marie Curie, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: If so, then the compiler can allocate a contiguous block of memory and copy the block reused in the loop nest into the contiguous memory before execution of the corresponding tile. The techniques of Ancourt <ref> [7] </ref> and Gallivan, Jalby and Gannon [27] can be used to determine more precisely which data need to be copied in and out of the block. If copying is not applicable, remapping (Section 5.5) is also easy to apply.
Reference: [8] <author> C. Ancourt and F. Irigoin. </author> <title> Scanning polyhedra with DO loops. </title> <booktitle> In Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> Apr. </month> <year> 1991. </year> <note> BIBLIOGRAPHY 205 </note>
Reference-contexts: These equations are the ones that determine the polytope defining the loop bounds. We wish to apply the unimodular transformation T to the iteration space, thereby transforming the equations and therefore the loop bounds. A complete technique to perform this computation is discussed by Ancourt and Irigoin <ref> [8] </ref>. Their approach is to use Fourier-Motzkin elimination to both project the polytope onto the new space and eliminate redundant equations. Their approach is effective but potentially slow, since this approach leads to a large number of constraints, and Fourier-Motzkin operates in a pairwise fashion over these constraints. <p> A more general CHAPTER 2. LOOP TRANSFORMATION THEORY BASICS 43 @ @ @ @ fl fl fl fl ? - i min i max approach is discussed by Ancourt and Irigoin <ref> [8] </ref>. Their approach involves projecting a 2n-dimensional polytope into the n-dimensional polytope representing the controlling loops, and then using their unimodular bound finding techniques (see the discussion of their work in Section 2.4.6) to find the bounds for the tile loops. No empty tiles are created with this approach. <p> In this case, again reuse should be carried by the outermost loop of the tile, so that a block of data is indeed being reused. The new matrix size can be chosen as discussed in Section 5.5 to minimize self interference. In this case, techniques from Ancourt and Irigoin <ref> [8] </ref> are useful in ensuring that excessive copying is not performed, since if the entire matrix is copying and only part of it used within the loop nest, the computation could be slowed quite a bit, especially if the unused data resides on otherwise unused pages in memory.
Reference: [9] <author> U. Banerjee. </author> <title> Data dependence in ordinary programs. </title> <type> Technical Report 76-837, </type> <institution> University of Illinios at Urbana-Champaign, </institution> <month> Nov. </month> <year> 1976. </year>
Reference-contexts: parallelism, but the dependences also cannot be represented by a finite number of distance vectors: for i := 0 to N do A [i,j] := A [i + 1,B [j]]; To represent dependences of this type, previous research on vectorizing and parallelizing compilers has introduced the concept of direction vectors <ref> [9] </ref>, [59]. Using their notation, the direction vector for the first example above would have been (`fl '; ` fl '), indicating that all the iterations are using the same data B, and must be serialized.
Reference: [10] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic, </publisher> <year> 1988. </year>
Reference-contexts: In order to generate the code in Figure 2.4 (b), we must properly transform the loop bounds and loop body. This is discussed in Section 2.4.6. 2.4.2 General Unimodular Transformations A unimodular transformation is a transformation that can be represented by a unimodular matrix <ref> [10] </ref>. A unimodular matrix has three important properties. First, it is square, meaning that it maps an n-dimensional iteration space to an n-dimensional iteration space. Second, it has all integral components, so it maps integer vectors to integer vectors. Third, the absolute value of its determinant is one.
Reference: [11] <author> U. Banerjee. </author> <title> Unimodular transformations of double loops. </title> <editor> In A. Nicolau, D. Gel-ernter, T. Gross, and D. Padua, editors, </editor> <booktitle> Advances in Languages and Compilers for Parallel Processing, </booktitle> <pages> pages 192-219. </pages> <publisher> The MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: is stride-one, accessing consecutive elements of the arrays: for I 2 = 1 to N Z [I 1 ,I 2 ] := Y [I 1 ,I 2 ]; This interchange transformation is one example of a general class of transformations called unimodular transformations, which include loop interchange, skewing and reversal <ref> [11] </ref>. Unimodular transformation and tiling combined can perform a superset of the CHAPTER 1. INTRODUCTION 7 transformations that have typically been applied to loop nests to improve locality of reference. Three challenges confront a programmer who tries by hand to transform a loop nest to improve locality and parallelism. <p> By the construction of the transformation T , the I 0 1 component of all dependences are positive, so all dependences 2 Banerjee discusses a method for choosing the direction that maximizes the number of iterations that can be performed in parallel <ref> [11] </ref>. CHAPTER 3. PARALLELIZATION 103 are lexicographically positive. Thus we can skew the I 0 2 loop with respect to the I 0 1 loop by calling SRP to make the nest fully permutable.
Reference: [12] <author> M. Barnett and C. Lengauer. </author> <title> Loop parallelization and unimodularity. </title> <type> Technical Report ECS-LFCS-92-197, </type> <institution> University of Edinburgh, </institution> <month> Jan. </month> <year> 1992. </year>
Reference-contexts: On the other hand, requiring unimodularity simplifies the transformation process, because the loop bounds are easier to determine for unimodular transformations (see Section 2.4.6) than for transformations where the determinant is not 1, since there are no "holes" in the iteration space <ref> [12] </ref>. Figure 2.5 shows the unimodular and non-unimodular transformation.
Reference: [13] <author> A. J. Bernstein. </author> <title> Program analysis for parallel processing. </title> <journal> IEEE Transactions on Electronic Computers, </journal> <volume> EC-15(5):757-762, </volume> <month> Oct. </month> <year> 1966. </year>
Reference-contexts: An ordering constraint arises as follows: If the iteration ~p may access a memory location, and the iteration ~p 0 ~p may also access the same memory location, and at least one of the accesses is a write, then by Bernstein's conditions <ref> [13] </ref>, it may change the semantics to reorder execution of these nodes. Thus, a data dependence arc is included from ~p to ~p 0 .
Reference: [14] <author> M. Berry, D. Chen, P. Koss, D. Kuck, S. Lo, Y. Pang, L. Pointer, R. Roloff, A. Sameh, E. Clementi, S. Chin, D. Schneider, G. Fox, P. Messina, D. Walker, C. Hsiung, J. Schwarzmeier, K. Lue, S. Orszag, F. Seidl, O. Johnson, and J. Martin R. Goodrum. </author> <title> The perfect club benchmarks: effective performance evaluation of supercomputers. </title> <type> Technical Report (UIUCSRD) 827, </type> <institution> University of Illinois, Urbana-Champaign. Center for Supercomputing Research and Development, </institution> <year> 1989. </year>
Reference-contexts: These are numbers generated directly from our compiler without hand optimization, and show the impact a real compiler can have on a set of important loop nests. We also present performance results for the Perfect Club <ref> [14] </ref> . The NASA kernels show marked improvement or locality optimization while the Perfect Club does not. This is because the NASA kernels are large enough to benefit by cache locality optimization, whereas the Perfect Club data tend to be small.
Reference: [15] <author> M. Burke and R. Cytron. </author> <title> Interprocedural dependence analysis and parallelization. </title> <booktitle> In Proc. SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <month> June </month> <year> 1986. </year>
Reference-contexts: For example, the dependence (`'; `'), or (`fl'; `fl'), as arose in a previous example, is not lexicographically positive, because (2; 3), which is not lexicographically positive, 5 Burke and Cytron <ref> [15] </ref> incorporate the notion of plausible direction vectors, which are what we call lexicographically positive direction vectors, into their scheme for finding dependences, but not all other techniques do. CHAPTER 2.
Reference: [16] <author> D. A. Calahan. </author> <title> Block-oriented, local-memory-based linear equation solution on the Cray-2: Uniprocessor algorithms. </title> <booktitle> In Proc. International Conf. on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1986. </year>
Reference-contexts: But it was very significant that so many important matrix operations benefited by being computed with submatrix operations. Since then, many studies have confirmed the practical importance of tiled algorithms for both uniprocessors <ref> [16] </ref> and multiprocessors [28]. Tiling is advantageous to apply not only at the page level, but at all levels of the memory hierarchy, including for caches and registers [17]. In recognition of this, the latest implementation of the Basic Linear Algebra Subprogram package, BLAS3, now uses tiled subroutines [22].
Reference: [17] <author> D. Callahan, S. Carr, and K. Kennedy. </author> <title> Improving register allocation for subscripted variables. </title> <booktitle> In Proc. ACM SIGPLAN '90 Conf. on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: The SGI 4D/380 has eight MIPS/R3000 processors running at 33 Mhz. Each processor has a 64 KB direct mapped first-level cache and a 256 KB direct mapped second-level cache. We ran four different experiments: without tiling, tiling to reuse data in caches, tiling to reuse data in registers <ref> [17] </ref>, and tiling for both register and caches. For cache tiling, the data are copied into consecutive locations to avoid cache interference (see Chapter 5). Tiling improves the performance on a single processor by a factor of 2:75. <p> Since then, many studies have confirmed the practical importance of tiled algorithms for both uniprocessors [16] and multiprocessors [28]. Tiling is advantageous to apply not only at the page level, but at all levels of the memory hierarchy, including for caches and registers <ref> [17] </ref>. In recognition of this, the latest implementation of the Basic Linear Algebra Subprogram package, BLAS3, now uses tiled subroutines [22]. Abu-Sufah The first significant research into automatic improvement of paging performance by the compiler was undertaken by Abu-Sufah at the University of Illinois [1], [2], CHAPTER 2. <p> In addition, his overflow iteration only gives information on the appropriate choice of tile size when a two-deep loop is being tiled, so for example the technique could not tell us how to tile matrix multiplication. CHAPTER 2. LOOP TRANSFORMATION THEORY BASICS 63 Callahan, Carr and Kennedy This paper <ref> [17] </ref> discusses promotion of array references into registers (scalar replacement), and tiling for registers via the unroll-and-jam transformation. The outer loops to unroll-and-jam are chosen by examining the dependence graph and seeing which loops carry the most true and input dependences. <p> 1 + 2]; /* The following is in case N is not divisible by 4. */ for I 1 := I 1 to N do B [I 2 ] := B [I 2 ] + A [I 1 ]; Note that in the original loop nest, even after scalar replacement <ref> [17] </ref>, there is one load and one store per iteration, whereas in the improved loop nest there is only one load and one store per four iterations after scalar replacement. <p> Abu-Sufah [1], [2] optimizes for virtual memory, but does so with the primitive method of tiling whenever legal (whether beneficial or not), as discussed in Section 2.8. Callahan, Carr and Kennedy <ref> [17] </ref> discuss "unroll-and-jam" and other techniques specifically aimed at register locality improvement, but do not attack such techniques as loop interchange in a systematic way. CHAPTER 4.
Reference: [18] <author> D. Callahan, J. Cocke, and K. Kennedy. </author> <title> Estimating interlock and improving balance for pipelined architectures. </title> <booktitle> In Proc. International Conf. on Parallel Processing, </booktitle> <year> 1987. </year> <note> BIBLIOGRAPHY 206 </note>
Reference-contexts: In this example, the other alternative is to tile so that the I 1 loop is innermost after unrolling: for I 2 := 1 to N 3 by 4 do 4 When tiling followed by unrolling the innermost loop, the effect is equivalent to the unroll-and-jam transformation <ref> [18] </ref>. CHAPTER 4.
Reference: [19] <author> R. P. Colwell, R. P. Nix, J. J. O'Donnell, D. B. Papworth, and P. K. Rodman. </author> <title> A VLIW architecture for a trace scheduling compiler. </title> <booktitle> In Proc. 2nd Intl. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> Oct. </month> <year> 1987. </year>
Reference-contexts: High data cache miss rates and poor locality of reference are characteristic of many numerical applications. In recognition of the fact that scientific code does not typically benefit from caches, both the Cydra [51] and the Multiflow Trace <ref> [19] </ref> were built without caches at all. However, designing cacheless machines to have acceptable performance will become increasingly difficult as memory latencies grow, and current high performance computers are again being designed with more extensive memory hierarchies.
Reference: [20] <author> R. Cytron. </author> <title> Compile-time Scheduling and Optimization for Asynchronous Machines. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1984. </year>
Reference-contexts: An example of a wavefront of tiles is highlighted in Figure 3.3, where the lightly shaded tiles can run in parallel. To further reduce the synchronization cost, we can apply the concept of a DOACROSS loop <ref> [20] </ref> to the tile level [58]. After tiling, instead of wavefronting the loops statically to form DOALL loops, the computation is allowed to wavefront dynamically by explicit synchronization between data dependent tiles.
Reference: [21] <author> J.-M. Delosme and I. C. F. Ipsen. </author> <title> Efficient systolic arrays for the solution of Toeplitz Systems: an illustration of a methodology for the construction of systolic architectures in VLSI. </title> <type> Technical Report 370, </type> <institution> Yale University, </institution> <year> 1985. </year>
Reference-contexts: In developing his theory, he recognized the importance of lexicographical positiveness. He also handled the limited form of direction vectors that arose when one of the index variables of a loop did not appear in array references. Since then, this framework has been very popular among systolic array researchers <ref> [21] </ref>, [25], [49]. Of course, the problem for systolic arrays is somewhat different than that for multiple processor machines. Typically, an iteration space of dimensionality d is mapped linearly into a processing array of dimensionality d 1, each node executing its several iterations over time.
Reference: [22] <author> J. Dongarra, J. Du Croz, S. Hammarling, and I. Duff. </author> <title> A set of level 3 basic linear algebra subprograms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <pages> pages 1-17, </pages> <month> Mar. </month> <year> 1990. </year>
Reference-contexts: To achieve acceptable performance on scientific codes, then, we must make scientific code utilize the memory hierarchy more effectively. A well known technique for improving locality of reference in scientific code is called blocking [1], <ref> [22] </ref>, [28], [31], [43], or tiling [58]. Tiling can be applied to improve utilization of the cache, and also registers, virtual memory, the TLB (translation lookaside buffer) and so on. Tiled code can also be run effectively in parallel, which is important for multiple processor machines. <p> Tiling is advantageous to apply not only at the page level, but at all levels of the memory hierarchy, including for caches and registers [17]. In recognition of this, the latest implementation of the Basic Linear Algebra Subprogram package, BLAS3, now uses tiled subroutines <ref> [22] </ref>. Abu-Sufah The first significant research into automatic improvement of paging performance by the compiler was undertaken by Abu-Sufah at the University of Illinois [1], [2], CHAPTER 2. LOOP TRANSFORMATION THEORY BASICS 59 [3].
Reference: [23] <author> M. L. Dowling. </author> <title> Optimal code parallelization using unimodular transformations. </title> <journal> Parallel Computing, </journal> <volume> 16 </volume> <pages> 157-171, </pages> <year> 1990. </year>
Reference-contexts: In fact, while for a given set of dependence vectors it is easy to automatically find a ~ t that meets the legality criteria [49], finding an optimal ~ t is of higher complexity <ref> [23] </ref> although not impossible [26]. Moldovan [44] discusses a package to determine many legal ~ t , so that the user can choose the best one. The matrix theory approach has been applied to the problem of parallelizing loop nests for execution on vector processors and shared memory multiprocessors.
Reference: [24] <author> J. L. Elshoff. </author> <title> Some programming techniques for processing multi-dimensional matrices in a paging environment. </title> <booktitle> In Proc. of the NCC, </booktitle> <year> 1974. </year>
Reference-contexts: First, he does not have to decide when to vertically distribute to improve locality: he always wants to when legal|he calls this "matching the pattern of reference to the storage schemes", a concept he attributes to Elshoff <ref> [24] </ref>. Sometimes, vertical distribution helps just a little bit, as in matrix addition, but sometimes it helps a great deal, as in matrix multiplication. The second simplification is that he never needs to consider subtleties in choosing the correct tile size.
Reference: [25] <author> J. A. B. Fortes and D. I. Moldovan. </author> <title> Parallelism detection and transformation techniques useful for VLSI algorithms. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 2 </volume> <pages> 277-301, </pages> <year> 1985. </year>
Reference-contexts: He also handled the limited form of direction vectors that arose when one of the index variables of a loop did not appear in array references. Since then, this framework has been very popular among systolic array researchers [21], <ref> [25] </ref>, [49]. Of course, the problem for systolic arrays is somewhat different than that for multiple processor machines. Typically, an iteration space of dimensionality d is mapped linearly into a processing array of dimensionality d 1, each node executing its several iterations over time.
Reference: [26] <author> J.A.B. Fortes and F. Parisi-Presicce. </author> <title> Optimal linear schedules for the parallel execution of algorithms. </title> <booktitle> In Proc. 13th International Conf. on Parallel Processing, </booktitle> <pages> pages 319-329, </pages> <month> Jan. </month> <year> 1988. </year>
Reference-contexts: In fact, while for a given set of dependence vectors it is easy to automatically find a ~ t that meets the legality criteria [49], finding an optimal ~ t is of higher complexity [23] although not impossible <ref> [26] </ref>. Moldovan [44] discusses a package to determine many legal ~ t , so that the user can choose the best one. The matrix theory approach has been applied to the problem of parallelizing loop nests for execution on vector processors and shared memory multiprocessors.
Reference: [27] <author> K. Gallivan, W. Jalby, and D. Gannon. </author> <title> On the problem of optimizing data transfers for complex memory systems. </title> <booktitle> In Proc. of the 1988 International Conference on Supercomputing, </booktitle> <pages> pages 238-253, </pages> <month> July </month> <year> 1988. </year> <note> BIBLIOGRAPHY 207 </note>
Reference-contexts: The approach taken in this model|calculating footprints, mathematically deriving an equation for the best block size, and so on|is feasible but complex. The footprints can be determined in general from an approach like that in <ref> [27] </ref>. A more straightforward method for calculating footprints is to simply use the access CHAPTER 5. TILING FOR REAL CACHES 174 per iteration estimates from Section 4.3.2 to determine how much reuse there is for each uniformly generated set. <p> If so, then the compiler can allocate a contiguous block of memory and copy the block reused in the loop nest into the contiguous memory before execution of the corresponding tile. The techniques of Ancourt [7] and Gallivan, Jalby and Gannon <ref> [27] </ref> can be used to determine more precisely which data need to be copied in and out of the block. If copying is not applicable, remapping (Section 5.5) is also easy to apply.
Reference: [28] <author> K. Gallivan, W. Jalby, U. Meier, and A. Sameh. </author> <title> The impact of hierarchical memory systems on linear algebra algorithm design. </title> <type> Technical report, </type> <institution> University of Illinios, </institution> <year> 1987. </year>
Reference-contexts: To achieve acceptable performance on scientific codes, then, we must make scientific code utilize the memory hierarchy more effectively. A well known technique for improving locality of reference in scientific code is called blocking [1], [22], <ref> [28] </ref>, [31], [43], or tiling [58]. Tiling can be applied to improve utilization of the cache, and also registers, virtual memory, the TLB (translation lookaside buffer) and so on. Tiled code can also be run effectively in parallel, which is important for multiple processor machines. <p> Locality Algorithm We have also applied our representation of transformations to the problem of data locality. Even with very simple machine models (for example, unipro-cessors with data caches), complex loop transformations may be necessary <ref> [28] </ref>, [30], [48]. The consideration of data locality makes it more important to be able to combine primitive loop transformations in a systematic manner. Using the same theoretical framework, we have developed a locality optimization that applies unimodular loop transforms and tiling to use the memory hierarchy efficiently. <p> But it was very significant that so many important matrix operations benefited by being computed with submatrix operations. Since then, many studies have confirmed the practical importance of tiled algorithms for both uniprocessors [16] and multiprocessors <ref> [28] </ref>. Tiling is advantageous to apply not only at the page level, but at all levels of the memory hierarchy, including for caches and registers [17]. In recognition of this, the latest implementation of the Basic Linear Algebra Subprogram package, BLAS3, now uses tiled subroutines [22].
Reference: [29] <author> D. Gannon and W. Jalby. </author> <title> The influence of memory hierarchy on algorithm organization: Programming ffts on a vector multiprocessor. In The Characteristics of Parallel Algorithms. </title> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: We now investigate a totally different approach that eliminates self interference altogether, thus guaranteeing a high cache utilization for all problem sizes. The approach is to copy non-contiguous data to be reused into a contiguous area <ref> [29] </ref>. By doing so, each word within the block is mapped to its own cache location, thus making self interference within a block impossible. This technique, when applicable, can bound the cache misses to within a factor of two of the ideal.
Reference: [30] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 587-616, </pages> <year> 1988. </year>
Reference-contexts: Locality Algorithm We have also applied our representation of transformations to the problem of data locality. Even with very simple machine models (for example, unipro-cessors with data caches), complex loop transformations may be necessary [28], <ref> [30] </ref>, [48]. The consideration of data locality makes it more important to be able to combine primitive loop transformations in a systematic manner. Using the same theoretical framework, we have developed a locality optimization that applies unimodular loop transforms and tiling to use the memory hierarchy efficiently. <p> The strengths and weaknesses of his approach are those discussed in Section 2.8. The relationship between strip-mine and interchange, unroll-and-jam, tiling and other transformations is discussed in more detail in Section 2.5. Reference Windows In 1988, Gannon, Jalby and Gallivan <ref> [30] </ref> tackled analysis of cache and local memory behavior in loop nests. Their key insight was that in order to properly understand and improve memory hierarchy performance, it was necessary to model reuse of data within a loop nest. Their model is based upon the notion of a reference window. <p> Porterfield models the cache as fully-associative with a least recently used (LRU) replacement policy. Given this model, he, like Gannon et. al. <ref> [30] </ref> above, is able to estimate the miss ratios for a given nest. He also discusses how the application of various transformations|fission, fusion, interchange, strip-mining, unrolling, skewing|change the cache behavior. <p> Strip-mine and interchange is very similar to unroll-and-jam, and other more complex transformations are even more similar). The result is that there is a huge transformation space to search. Porterfield uses the concept of an overflow iteration to help in determining the correct tile size. Unlike Gannon et. al. <ref> [30] </ref>, he uses information on the number of references that should be in the cache at a given time to determine the best tile size to choose. Unfortunately, he does so assuming an LRU cache, which can be very misleading and result in damagingly large tile sizes. <p> Such references are known as uniformly generated references. Use of the concept of uniformly generated references in locality studies was proposed by Gannon et. al. <ref> [30] </ref> for estimating reference windows. Definition 4.1 Let n be the depth of a loop nest, and d be the dimensionality of an array A.
Reference: [31] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins University Press, </publisher> <year> 1989. </year>
Reference-contexts: To achieve acceptable performance on scientific codes, then, we must make scientific code utilize the memory hierarchy more effectively. A well known technique for improving locality of reference in scientific code is called blocking [1], [22], [28], <ref> [31] </ref>, [43], or tiling [58]. Tiling can be applied to improve utilization of the cache, and also registers, virtual memory, the TLB (translation lookaside buffer) and so on. Tiled code can also be run effectively in parallel, which is important for multiple processor machines.
Reference: [32] <author> J. L. Hennessy and D. A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1990. </year>
Reference-contexts: The trend of increasing memory latency relative to floating-point speed is expected to continue. Hennessy and Patterson <ref> [32] </ref> expect CPU cycle time to continue increasing by 50% to 100% per year while DRAM are only improving in speed by about 7% per year. A computer system built today might reasonably have a 64 KB cache and a 10 cycle miss latency.
Reference: [33] <author> F. Irigoin. </author> <title> Partitionnement Des Boucles Imbeiquees: Une Technique D'optimisation four les Programmes Scientifiques. </title> <type> PhD thesis, </type> <institution> Universite Pierre et Marie Curie, </institution> <month> June </month> <year> 1987. </year>
Reference-contexts: This problem does not occur in the body of the innermost loop, and thus the generated code is correct and the extra computation cost should be negligible. These problems can be removed via well known integer linear system algorithms <ref> [33] </ref>. The algorithm to determining loop bounds is efficient. For an n-deep loop nest, there are at least 2n inequalities, since each loop has at least one lower and upper bound. In general, each loop bound may be maximum and minimum functions, with every term contributing one inequality. <p> They are able to represent both unimodular transformations on the iteration space and tiling, including the tile sizes, via a single matrix. Given this matrix, and the dependences, he can mathematically determine legality and generate proper code, some of the detail for which is contained in Irigoin's thesis <ref> [33] </ref>. Irigoin's approach is elegant, but for our purposes lacking on two fronts. First, the dependences again must be summarizable as distance vectors (his method of determining such distance vectors, using dependence cones, is discussed in [35]), so this theory cannot be applied to all loop nests.
Reference: [34] <author> F. Irigoin. </author> <title> Code generation for the hyperplane method and for loop interchange. </title> <type> Technical Report ENSMP-CAI-88-E102/CAI/I, </type> <institution> Ecole des Mines de Paris, </institution> <month> Oct. </month> <year> 1988. </year>
Reference-contexts: Then there exists a unimodular transformation U with first row ~u such that 8 ~ d 2 D : U ~ d ~ 0 . 1 A similar fact is also proven by Irigoin <ref> [34] </ref>, whose approach is based on a theorem proven in 1920 by A. Hoffman. CHAPTER 3. PARALLELIZATION 80 Proof: We first prove the theorem under the assumption that all the dependence vectors ~ d are distance vectors ~e.
Reference: [35] <author> F. Irigoin and R. Triolet. </author> <title> Computing dependence direction vectors and dependence cones. </title> <type> Technical Report E94, </type> <institution> Centre D'Automatique et Informatique, </institution> <year> 1988. </year>
Reference-contexts: Our dependence vector representation is a generalization of both distance and direction vectors, keeping available the precision of distance vectors when such information is available, while allowing us to represent dependences in general loop nests concisely. Techniques for extracting dependence vectors from loop nests are well known <ref> [35] </ref>, [42], [61], [59]. In particular, Irigoin and Triolet [35] discuss a technique to summarize dependence vectors as a finite number distance vectors if possible. 2.3.1 Representation The representation we use for dependence vectors encompasses both distance and direction vectors. <p> Techniques for extracting dependence vectors from loop nests are well known <ref> [35] </ref>, [42], [61], [59]. In particular, Irigoin and Triolet [35] discuss a technique to summarize dependence vectors as a finite number distance vectors if possible. 2.3.1 Representation The representation we use for dependence vectors encompasses both distance and direction vectors. Each component d i of a dependence vector ~ d is a possibly infinite range of CHAPTER 2. <p> This sort of summary is exactly what is achieved by Irigoin and Triolet's dependence cone work <ref> [35] </ref>. CHAPTER 2. LOOP TRANSFORMATION THEORY BASICS 19 by the induction hypothesis (e 2 ; . . . ; e n ) ~ 0 , so ~e ~ 0 . Thus in all cases ~e ~ 0 and so ~ d ~ 0 by Definition 2.1. <p> Irigoin's approach is elegant, but for our purposes lacking on two fronts. First, the dependences again must be summarizable as distance vectors (his method of determining such distance vectors, using dependence cones, is discussed in <ref> [35] </ref>), so this theory cannot be applied to all loop nests. Second, the theory is not constructive: he does not discuss how a compiler can choose a matrix that best, or even suitably, transforms a loop nest. CHAPTER 2. <p> A related but simpler problem, referred to here as the time cone problem, is to find a linear transformation matrix for a finite set of distance vectors such that the first components of all transformed distance vectors are all positive <ref> [35, 37, 49, 52] </ref>. This means that the rest of the loops are DOALL loops. We have shown that if all the distance vectors are lexicographically positive, this can be achieved by first skewing to make the loops fully permutable, and wavefronting to generate the DOALL loops.
Reference: [36] <author> F. Irigoin and R. Triolet. </author> <title> Dependence approximation and global parallel code generation for nested loops. </title> <booktitle> In International Workshop on Parallel and Distributed Algorithms, </booktitle> <month> Oct. </month> <year> 1988. </year>
Reference-contexts: CHAPTER 2. LOOP TRANSFORMATION THEORY BASICS 58 In later work, he discusses how to choose transformations when the space spanned by the dependences is a proper subset of the iteration space <ref> [36] </ref>. That work is the direct inspiration for the presentation of Section 3.2.2 of this thesis. Related Work on Improving Locality It has been known for many years that locality of reference, or lack thereof, has a large impact on performance. <p> In this example, the iteration space is 2-dimensional, but the space spanned by the dependence vectors is only 1-dimensional. When the dependence vectors do not span the entire iteration space, it is possible to perform a transformation that makes outermost DOALL loops <ref> [36] </ref>. By choosing the transformation matrix T with first row ~ t 1 such that ~ t 1 ~ d = 0 for all dependence vectors ~ d , the transformation produces an outermost DOALL.
Reference: [37] <author> F. Irigoin and R. Triolet. </author> <title> Supernode partitioning. </title> <booktitle> In Proc. 15th Annual ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages, </booktitle> <pages> pages 319-329, </pages> <month> Jan. </month> <year> 1988. </year> <note> BIBLIOGRAPHY 208 </note>
Reference-contexts: This model makes it possible to determine the compound transformation directly by maximizing some objective function. Loop nests whose dependences can be represented as distance vectors have the property that an n-deep loop nest has at least n 1 degrees of parallelism <ref> [37] </ref>, and can exploit data locality in all possible loop dimensions (see Chapter 4). Distance vectors cannot represent the dependences of general loop nests, where two or more loops must execute sequentially. Lamport [41] did important early this work in automatic parallelization with this framework. <p> The matrix theory approach has been applied to the problem of parallelizing loop nests for execution on vector processors and shared memory multiprocessors. The seminal paper on this transformation style is by Irigoin and Triolet <ref> [37] </ref>. They are able to represent both unimodular transformations on the iteration space and tiling, including the tile sizes, via a single matrix. Given this matrix, and the dependences, he can mathematically determine legality and generate proper code, some of the detail for which is contained in Irigoin's thesis [33]. <p> Their implementation only performs unroll-and-jam and scalar replacement; no loop interchange, skewing or other more complex transformations are performed, although the paper briefly discusses why loop interchange can be useful. Other Locality Work Irigoin and Triolet <ref> [37] </ref> use their framework express transformations that can improve locality, but do not discuss in detail how to choose the best transformation to apply. Schreiber and Dongarra [52] build upon the framework of Irigoin and Triolet to develop an algorithm to improve locality. <p> First, we discuss the case where all dependences can be represented as distance vectors. For this case, we know that we can transform to get at least n 1 DOALL loops <ref> [37] </ref>; we present a very simple and fast algorithm to produce such code, and discuss alternative code generation strategies to produce the parallelism at the desired granularity. We then extend the approach to handle general dependence vectors as presented in Section 2.3. <p> Fine-Grain Parallelism A nest of n fully permutable loops can be transformed to code containing at least n 1 degrees of parallelism <ref> [37] </ref>. In the degenerate case when no dependences are carried by these n loops, the degree of parallelism is n. Otherwise, n 1 parallel loops can be obtained applying the wavefront transformation discussed in the following theorem. <p> A related but simpler problem, referred to here as the time cone problem, is to find a linear transformation matrix for a finite set of distance vectors such that the first components of all transformed distance vectors are all positive <ref> [35, 37, 49, 52] </ref>. This means that the rest of the loops are DOALL loops. We have shown that if all the distance vectors are lexicographically positive, this can be achieved by first skewing to make the loops fully permutable, and wavefronting to generate the DOALL loops.
Reference: [38] <author> N. P. Jouppi. </author> <title> Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers. </title> <booktitle> In The 18th Annual Intl. Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: Hennessy and Patterson [32] expect CPU cycle time to continue increasing by 50% to 100% per year while DRAM are only improving in speed by about 7% per year. A computer system built today might reasonably have a 64 KB cache and a 10 cycle miss latency. Jouppi <ref> [38] </ref> estimates that by the mid-90's, a 1000 MIPS processor will have roughly the following cache characteristics: Level Min. Capacity Line Size Miss Penalty 1st level 4 KB 16B 24 instruction issues 2nd level 1 MB 128B 320 instruction issues Machines with long latencies are already being designed. <p> of this and the very long miss latencies, performance of these scientific codes is poor on his model of a future 1000 MIPS machine: LINPACK runs at only half its potential peak because of memory latency, and the Livermore loops benchmark runs at approximately one third of its potential peak <ref> [38] </ref>. The reason scientific codes do so poorly with caches is that they tend to operate on large data sets.
Reference: [39] <author> M. S. Lam. </author> <title> Software pipelining: An effective scheduling technique for VLIW machines. </title> <booktitle> In Proc.' ACM SIGPLAN 88 Conf. on Programming Language Design and Implementation, </booktitle> <pages> pages 318-328, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Coalescing multiple DOALL loops prevents the pitfall of parallelizing only a loop with a small iteration count. It can reduce further the overhead of starting and finishing a parallel loop if code scheduling techniques such as software pipelining <ref> [39] </ref> are used. Coarsest Granularity of Parallelism For MIMD machines, having as many outermost DOALLs as possible reduces the synchronization overhead. The wavefront transformation produces the maximal degree of parallelism, but makes the outermost loop sequential, if any are. <p> We assume that our instruction scheduler finds significant parallelism within and across the iterations of the innermost loop only, as is typical in such techniques as software pipelining <ref> [39] </ref>. If the Innermost Nest is Tiled for Locality Suppose the locality improving algorithm tiles the innermost nest. We know that parallelism exists within the tile, so we must expose it.
Reference: [40] <author> M. S. Lam, E. E. Rothberg, and M. E. Wolf. </author> <title> The cache performance and optimizations of blocked algorithms. </title> <booktitle> In Proc. Fourth International Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: Chapter 4 focuses on compiler improvement of locality. Chapter 5 discusses the implementation of tiling on real caches, and Chapter 6 contains the experimental results. Finally, Chapter 7 contains a summary and discussion of open questions. 1 This is joint work with Ed Rothberg <ref> [40] </ref>. Chapter 2 Loop Transformation Theory Basics We begin this chapter by introducing our model of a perfectly nested loop nest, based upon iteration spaces and dependences in a loop nest. We then introduce unimodu-lar transformations, which are a unification and generalization of the loop interchange, skewing and reversal transformations.
Reference: [41] <author> Leslie Lamport. </author> <title> The parallel execution of do loops. </title> <journal> CACM, </journal> <volume> 17(2) </volume> <pages> 83-93, </pages> <month> Feb. </month> <year> 1974. </year>
Reference-contexts: For example, the transformation from Figure 2.9 (a) to Figure 2.9 (b) is a skew of the inner loop with respect to the outer loop by a factor of one, which can be represented as 2 1 0 3 The wavefront transformation <ref> [41] </ref>, [59] is an example of a unimodular transformation that can be thought of as a combination of other unimodular transformations. <p> Distance vectors cannot represent the dependences of general loop nests, where two or more loops must execute sequentially. Lamport <ref> [41] </ref> did important early this work in automatic parallelization with this framework. He parallelized loops via the wavefronts, using a technique he called the hyperplane method.
Reference: [42] <author> D. E. Maydan, J. L. Hennessy, and M. S. Lam. </author> <title> Efficient and exact data dependence analysis. </title> <booktitle> In Proc. ACM SIGPLAN 91 Conf. on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: Our dependence vector representation is a generalization of both distance and direction vectors, keeping available the precision of distance vectors when such information is available, while allowing us to represent dependences in general loop nests concisely. Techniques for extracting dependence vectors from loop nests are well known [35], <ref> [42] </ref>, [61], [59]. In particular, Irigoin and Triolet [35] discuss a technique to summarize dependence vectors as a finite number distance vectors if possible. 2.3.1 Representation The representation we use for dependence vectors encompasses both distance and direction vectors. <p> In the 2-dimensional case, the statement S1a can only execute when I 2 = L 2 (I 1 ). Likewise, S1b can only execute when I 2 = U 2 (I 1 ). By augmenting the set of constraints that the data dependence analyzer is solving <ref> [42] </ref> with these additional constraints, the data dependence analyzer has the necessary information to fully take the non-perfect nesting into account. 7 In the n-dimensional case, the non-perfectly nested instructions will simply be protected by the conjunction of one or more constraints; the dependence analyzer can take all these constraints into <p> EXPERIMENTS 178 can take short cuts by not transforming nests with certain characteristics. 6.1 The SUIF Compiler The SUIF compiler is an experimental compiler under development at Stanford University. The compiler implements high-level transformations, including those described in this thesis, based upon sophisticated dependence analysis <ref> [42] </ref>. It also contains a scalar optimizer based upon the Sharlit optimizer generator tool [55] and a prototype software pipeliner. The compiler consists of independent phases that communicate through an intermediate language called SUIF (Stanford University Intermediate Form).
Reference: [43] <author> A. C. McKeller and E. G. Coffman. </author> <title> The organization of matrices and matrix operations in a paged multiprogramming environment. </title> <journal> CACM, </journal> <volume> 12(3) </volume> <pages> 153-165, </pages> <year> 1969. </year>
Reference-contexts: To achieve acceptable performance on scientific codes, then, we must make scientific code utilize the memory hierarchy more effectively. A well known technique for improving locality of reference in scientific code is called blocking [1], [22], [28], [31], <ref> [43] </ref>, or tiling [58]. Tiling can be applied to improve utilization of the cache, and also registers, virtual memory, the TLB (translation lookaside buffer) and so on. Tiled code can also be run effectively in parallel, which is important for multiple processor machines. <p> That work is the direct inspiration for the presentation of Section 3.2.2 of this thesis. Related Work on Improving Locality It has been known for many years that locality of reference, or lack thereof, has a large impact on performance. In 1969, McKellar and Coffman <ref> [43] </ref> wrote a paper demonstrating the gargantuan differences in page fault rate that can result from running matrix algorithms navely versus running them with consideration for the paging behavior of the machine. The authors studied the paging behavior of several common matrix operations: addition, transpose, multiplication and inverse. <p> LOOP TRANSFORMATION THEORY BASICS 59 [3]. This work discussed a tiling-like transformation called vertical distribution, analyzed its dependence requirements, and discussed the resultant improvement of some programs with the transformation. Abu-Sufah starts by laying out all matrices in submatrix form, "because of its advantages as presented in" <ref> [43] </ref>. Each page of size Z contains a p p Z submatrix of an N fi N matrix.
Reference: [44] <author> D. I. Moldovan. ADVIS: </author> <title> A software package for the design of systolic arrays. </title> <journal> IEEE Transactions on Computer-Aided Design, </journal> <volume> CAD-6(1):33-40, </volume> <month> Jan. </month> <year> 1987. </year>
Reference-contexts: In fact, while for a given set of dependence vectors it is easy to automatically find a ~ t that meets the legality criteria [49], finding an optimal ~ t is of higher complexity [23] although not impossible [26]. Moldovan <ref> [44] </ref> discusses a package to determine many legal ~ t , so that the user can choose the best one. The matrix theory approach has been applied to the problem of parallelizing loop nests for execution on vector processors and shared memory multiprocessors.
Reference: [45] <author> D. I. Moldovan and J. A. B. Fortes. </author> <title> Partitioning and mapping algorithms into fixed size systolic arrays. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-35(1):1-12, </volume> <month> Jan. </month> <year> 1986. </year>
Reference-contexts: CHAPTER 2. LOOP TRANSFORMATION THEORY BASICS 64 Systolic array designers and users must consider how to map a large problem onto a finite set of processors, which is also a locality problem, since they want as little communication as possible between computations running at different times. Moldovan and Fortes <ref> [45] </ref> suggest what is essentially tiling and then having the systolic array work on a tile at a time. However, they do not discuss how to "choose the tiling basis"|again, their work is non-constructive. Summary The Abu-Sufah, Gannon and Porterfield works discussed here all contain very important ideas and insights.
Reference: [46] <author> T. C. Mowry, M. S. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Proc. 5th Intl. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: For example, such information about reuse will be crucial in partitioning loops and data for distributed memory machines. Reuse information is also exactly the kind of knowledge that is necessary and useful for "smart prefetching <ref> [46] </ref>. This section briefly discusses why reuse information is useful for prefetching as an example of the wider applicability of reuse information in compiler optimization. When prefetching, we only wish to prefetch those memory references that will miss in the cache.
Reference: [47] <author> O. A. Olukotun, T. N. Mudge, and R. B. Brown. </author> <title> Implementing a cache for a high-performance gas microprocessor. </title> <booktitle> In Proc. 18th Intl. Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1991. </year> <note> BIBLIOGRAPHY 209 </note>
Reference-contexts: Capacity Line Size Miss Penalty 1st level 4 KB 16B 24 instruction issues 2nd level 1 MB 128B 320 instruction issues Machines with long latencies are already being designed. The caches for the University of Michigan's 250 MHz GaS microprocessor <ref> [47] </ref> are projected to have the following characteristics for read misses: 1 CHAPTER 1. INTRODUCTION 2 Level Capacity Line Size Miss Penalty 1st level 16 KB 16 bytes 6 instruction issues 2st level 1 MB 128 bytes 143 instruction issues with longer latencies for write misses.
Reference: [48] <author> A. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Locality Algorithm We have also applied our representation of transformations to the problem of data locality. Even with very simple machine models (for example, unipro-cessors with data caches), complex loop transformations may be necessary [28], [30], <ref> [48] </ref>. The consideration of data locality makes it more important to be able to combine primitive loop transformations in a systematic manner. Using the same theoretical framework, we have developed a locality optimization that applies unimodular loop transforms and tiling to use the memory hierarchy efficiently. <p> If a legal loop nest can be non-rectangularly tiled, then after application of an appropriate skew unimodular transformation, rectangular tiling can legally be applied. For example, tiling Figure 2.9 (d) is equivalent to wavefront-blocking <ref> [48] </ref> on On the other hand, non-unimodular transformations combined with rectangular tiling can produce tile shapes that unimodular transformations combined with tiling cannot achieve. This may be useful when transforming to minimize communication in a distributed memory machine [50]. <p> However, the reference window concept does not help much in choosing the transformation to apply: it can only evaluate a given transformation. If a large space of transformations are to be considered, this alone will CHAPTER 2. LOOP TRANSFORMATION THEORY BASICS 62 not do. Porterfield Porterfield's dissertation <ref> [48] </ref> contains a study of compiler techniques for improving data cache performance. He uses many modern techniques, including skewing and tiling, and his is perhaps the only work (before this one) that one could use as a basis for designing a practical locality improving compiler.
Reference: [49] <author> P. </author> <title> Quinton. Automatic synthesis of systolic arrays from uniform recurrent equations. </title> <booktitle> In Proc. 11th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1984. </year>
Reference-contexts: He also handled the limited form of direction vectors that arose when one of the index variables of a loop did not appear in array references. Since then, this framework has been very popular among systolic array researchers [21], [25], <ref> [49] </ref>. Of course, the problem for systolic arrays is somewhat different than that for multiple processor machines. Typically, an iteration space of dimensionality d is mapped linearly into a processing array of dimensionality d 1, each node executing its several iterations over time. <p> The optimal ~ t is not easy to find, since different ~ t result in different communication patterns on a given array. In fact, while for a given set of dependence vectors it is easy to automatically find a ~ t that meets the legality criteria <ref> [49] </ref>, finding an optimal ~ t is of higher complexity [23] although not impossible [26]. Moldovan [44] discusses a package to determine many legal ~ t , so that the user can choose the best one. <p> A related but simpler problem, referred to here as the time cone problem, is to find a linear transformation matrix for a finite set of distance vectors such that the first components of all transformed distance vectors are all positive <ref> [35, 37, 49, 52] </ref>. This means that the rest of the loops are DOALL loops. We have shown that if all the distance vectors are lexicographically positive, this can be achieved by first skewing to make the loops fully permutable, and wavefronting to generate the DOALL loops.
Reference: [50] <author> J. Ramanujam and P. Sadayappan. </author> <title> Tiling multidimensional iteration spaces for nonshared memory machines. </title> <booktitle> In Proc. Supercomputing '91, </booktitle> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: For example, tiling Figure 2.9 (d) is equivalent to wavefront-blocking [48] on On the other hand, non-unimodular transformations combined with rectangular tiling can produce tile shapes that unimodular transformations combined with tiling cannot achieve. This may be useful when transforming to minimize communication in a distributed memory machine <ref> [50] </ref>. The optimization algorithms we present in Chapters 3 and 4 apply unimodular transformation and tiling tandem. <p> Their algorithm does not necessarily pick the best tiling basis because of their assumption of a well-behaved cache and because the orthogonalization step does not guarantee finding an optimally orthogonal legal tile. Ramanujam <ref> [50] </ref> talks about solving for the best tile shape and size for nonshared memory machines: optimally for two-deep loops and allowing only a restricted set of transformations for loops of greater nesting. CHAPTER 2.
Reference: [51] <author> M. Schlansker and M. McNamara. </author> <title> The Cydra 5 computer system architecture. </title> <booktitle> In Proc. of the 1988 IEEE Intl. Conf. on Computer Design: VLSI in Computers and Processors|ICCD '88, </booktitle> <month> Oct. </month> <year> 1988. </year>
Reference-contexts: High data cache miss rates and poor locality of reference are characteristic of many numerical applications. In recognition of the fact that scientific code does not typically benefit from caches, both the Cydra <ref> [51] </ref> and the Multiflow Trace [19] were built without caches at all. However, designing cacheless machines to have acceptable performance will become increasingly difficult as memory latencies grow, and current high performance computers are again being designed with more extensive memory hierarchies.
Reference: [52] <author> R. Schreiber and J. Dongarra. </author> <title> Automatic blocking of nested loops. </title> <type> Personal communication., </type> <year> 1990. </year>
Reference-contexts: Other Locality Work Irigoin and Triolet [37] use their framework express transformations that can improve locality, but do not discuss in detail how to choose the best transformation to apply. Schreiber and Dongarra <ref> [52] </ref> build upon the framework of Irigoin and Triolet to develop an algorithm to improve locality. <p> A related but simpler problem, referred to here as the time cone problem, is to find a linear transformation matrix for a finite set of distance vectors such that the first components of all transformed distance vectors are all positive <ref> [35, 37, 49, 52] </ref>. This means that the rest of the loops are DOALL loops. We have shown that if all the distance vectors are lexicographically positive, this can be achieved by first skewing to make the loops fully permutable, and wavefronting to generate the DOALL loops. <p> However, if the distances are not lexicographically positive, the complexity of typical methods to find such a transformation assuming one exists is at least O (d n1 ), where d is the number of dependences and n is the loop nest depth <ref> [52] </ref>. The problem of finding the largest fully permutable loop nest is even harder, since we need to find the largest nest for which such a transformation exists. CHAPTER 3. <p> The question of whether there exists a combination of unimodular transformation and tiling that is legal and creates the desired innermost subspace is a difficult one. An existing algorithm that attempts to find such a transformation, given exclusively distance vectors, is exponential in the number of loops <ref> [52] </ref>. The general question of finding a legal transformation that for general dependence vectors minimizes the number of memory accesses as determined by the intersection of the localized and reused vector spaces is a generalization of that problem, and so at least as hard.
Reference: [53] <author> SPEC. </author> <title> The SPEC Benchmark Report. </title> <publisher> Waterside Associates, </publisher> <address> Fremont, CA, </address> <month> Jan. </month> <year> 1990. </year>
Reference-contexts: These experiments show that tiling is can improve performance dramatically on single-processor and especially multiple processor machines. The third section shows how the SUIF compiler optimizes single processor performance of the dnasa7 specmark <ref> [53] </ref>, the NASA kernels. These are numbers generated directly from our compiler without hand optimization, and show the impact a real compiler can have on a set of important loop nests. We also present performance results for the Perfect Club [14] .
Reference: [54] <author> S. Tjiang, M. Wolf, M. Lam, K. Pieper, and J. Hennessy. </author> <title> Integrated scalar optimization and parallelization. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing (Fourth International Workshop), </booktitle> <pages> pages 137-151. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: the characteristics of architectural solutions, such as higher associativity caches, and software solutions, such as selecting tile sizes that avoid mapping problems. 1 Implementation and Evaluation This thesis is not just a theoretical exercise: we have implemented both the parallelization and locality improving algorithms in our SUIF compiler at Stanford <ref> [54] </ref>, [55]. We present performance data on NASA kernels and the Perfect Club. We also present concrete data on the frequency with which the compiler performs various transformations on the above programs. The organization of this dissertation is as follows. <p> Stanford's SUIF (Stanford University Intermediate Format) compiler <ref> [54] </ref>, [55] currently does not implement either copying or remapping. Instead, it uses information on reuse factors to estimate footprints and how much data is in the cache. It then chooses a block size that uses a small percentage of the cache. <p> Chapter 6 Experiments In this chapter, we explore how loop transformations such as tiling improve performance on real machines. Our presentation is in four parts. First we briefly discuss the implementation of the data locality optimizing pass of the SUIF compiler, a compiler under development at Stanford University <ref> [54] </ref>. Then we discuss uniprocessor and multiprocessor performance of hand-tiled, highly tuned loops that can benefit from tiling. These experiments show that tiling is can improve performance dramatically on single-processor and especially multiple processor machines.
Reference: [55] <author> S. W. K. Tjiang. </author> <title> Automatic Generation of Data-flow Analyzers: A Tool for Building Optimizers. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1992. </year> <note> In preparation. </note>
Reference-contexts: characteristics of architectural solutions, such as higher associativity caches, and software solutions, such as selecting tile sizes that avoid mapping problems. 1 Implementation and Evaluation This thesis is not just a theoretical exercise: we have implemented both the parallelization and locality improving algorithms in our SUIF compiler at Stanford [54], <ref> [55] </ref>. We present performance data on NASA kernels and the Perfect Club. We also present concrete data on the frequency with which the compiler performs various transformations on the above programs. The organization of this dissertation is as follows. <p> Stanford's SUIF (Stanford University Intermediate Format) compiler [54], <ref> [55] </ref> currently does not implement either copying or remapping. Instead, it uses information on reuse factors to estimate footprints and how much data is in the cache. It then chooses a block size that uses a small percentage of the cache. <p> The compiler implements high-level transformations, including those described in this thesis, based upon sophisticated dependence analysis [42]. It also contains a scalar optimizer based upon the Sharlit optimizer generator tool <ref> [55] </ref> and a prototype software pipeliner. The compiler consists of independent phases that communicate through an intermediate language called SUIF (Stanford University Intermediate Form). This intermediate language is a simple infinite register quad-style IL, but can represent high-level control structures (FOR, IF, LOOP) and arrays.
Reference: [56] <author> M. J. Wolfe. </author> <title> Techniques for improving the inherent parallelism in programs. </title> <type> Technical Report UIUCDCS-R-78-929, </type> <institution> University of Illinois, </institution> <year> 1978. </year>
Reference-contexts: The SUIF language contains annotations, which phases use to communicate with one another. Annotations are used to pass information such as def-use chains and dependence information. The data locality optimizer is invoked after local and interprocedural constant propagation, induction variable substitution [5], <ref> [56] </ref> and dead code elimination. After completion of locality optimization and parallelization, the SUIF IL is lowered; array expressions are translated into explicit address arithmetic and FOR, IF and LOOP control constructs into branches and labels. After lowering of the IL, scalar optimization and code generation are performed.
Reference: [57] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <type> PhD thesis, </type> <institution> University of Illinois, </institution> <month> Oct. </month> <year> 1982. </year>
Reference: [58] <author> M. J. Wolfe. </author> <title> More iteration space tiling. </title> <booktitle> In Supercomputing '89, </booktitle> <month> Nov. </month> <year> 1989. </year>
Reference-contexts: To achieve acceptable performance on scientific codes, then, we must make scientific code utilize the memory hierarchy more effectively. A well known technique for improving locality of reference in scientific code is called blocking [1], [22], [28], [31], [43], or tiling <ref> [58] </ref>. Tiling can be applied to improve utilization of the cache, and also registers, virtual memory, the TLB (translation lookaside buffer) and so on. Tiled code can also be run effectively in parallel, which is important for multiple processor machines. <p> We would have parallelized this loop with tiling or a wavefront transformation (see Chapter 3). Wolfe can also do this <ref> [58] </ref>. The node-splitting technique is not effective for all loop nests. Consider the SOR loop nest: DO 10 i = 2,N-1 10 A (i,j) = 0.2 * (A (i,j) + A (i+1,j) + The graph in this case is shown in Figure 2.12. <p> The technique Wolfe suggests for using on this loop, of course, is the wavefront transformation, in the guise of a loop skew followed by a loop interchange [59]; and indeed, those transformations are effective in this context. In another paper <ref> [58] </ref>, he discusses how the tiling transformation can also be used to achieve parallelism. The weakness in his approach is not the scope of the transformations, which is impressive, but lack of a decision algorithm to choose which transformation to apply to CHAPTER 2. <p> Abu-Sufah's general approach was modified by Wolfe, so that tiling was achieved CHAPTER 2. LOOP TRANSFORMATION THEORY BASICS 61 via the strip-mining and loop interchange [59]. Wolfe was also the one to introduce the term "tiling", and he discussed how loop skewing can be incorporated to make non-rectangular tiles <ref> [58] </ref>. The strengths and weaknesses of his approach are those discussed in Section 2.8. The relationship between strip-mine and interchange, unroll-and-jam, tiling and other transformations is discussed in more detail in Section 2.5. <p> The two outer (controlling) loops, represented by the two axes in the figure, execute the 3 fi 4 tiles. each iteration of the outer n loops is a tile of iterations instead of an individual iteration. Tiling can therefore increase the granularity of synchronization <ref> [58] </ref>. Without CHAPTER 3. <p> An example of a wavefront of tiles is highlighted in Figure 3.3, where the lightly shaded tiles can run in parallel. To further reduce the synchronization cost, we can apply the concept of a DOACROSS loop [20] to the tile level <ref> [58] </ref>. After tiling, instead of wavefronting the loops statically to form DOALL loops, the computation is allowed to wavefront dynamically by explicit synchronization between data dependent tiles.
Reference: [59] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> MIT Press, </publisher> <year> 1989. </year> <note> BIBLIOGRAPHY 210 </note>
Reference-contexts: but the dependences also cannot be represented by a finite number of distance vectors: for i := 0 to N do A [i,j] := A [i + 1,B [j]]; To represent dependences of this type, previous research on vectorizing and parallelizing compilers has introduced the concept of direction vectors [9], <ref> [59] </ref>. Using their notation, the direction vector for the first example above would have been (`fl '; ` fl '), indicating that all the iterations are using the same data B, and must be serialized. <p> Techniques for extracting dependence vectors from loop nests are well known [35], [42], [61], <ref> [59] </ref>. In particular, Irigoin and Triolet [35] discuss a technique to summarize dependence vectors as a finite number distance vectors if possible. 2.3.1 Representation The representation we use for dependence vectors encompasses both distance and direction vectors. <p> We use the notation ` + ' as shorthand for [1; 1], `' as shorthand for [1; 1], and `' as shorthand for [1; 1]. They correspond to the Wolfe's directions ` &lt; ', ` &gt; ', and `fl' respectively <ref> [59] </ref>. We also use the shorthand d + to mean [d; 1] and d to mean [1; d]. 2.3.2 Generality of Dependence Representation As we have shown, our notation allows us to represent both direction and distance vectors in a uniform notation. <p> Theorem 2.2 A dependence vector ~ d ~ 0 if and only if 9i : A dependence vector ~ d - ~ 0 if and only if ~ d ~ 0 or 8j : d j 0. The typical data dependence finding mechanisms <ref> [59] </ref> only recognize that two iterations ~p and ~p 0 may touch the same memory location, and therefore have an ordering constraint between them. The dependence that therefore arises between them is either ~p ~p 0 or ~p 0 ~p, depending on which is lexicographically positive. <p> For example, the matrix repre senting loop reversal of the outermost loop of a two-deep loop nest is 2 1 0 3 * Skewing: Skewing loop I j by an integer factor f with respect to loop I i <ref> [59] </ref> maps iteration (p 1 ; . . . ; p i1 ; p i ; p i+1 ; . . . ; p j1 ; p j ; p j+1 ; . . . ; p n ) (p 1 ; . . . ; p i1 ; p i <p> For example, the transformation from Figure 2.9 (a) to Figure 2.9 (b) is a skew of the inner loop with respect to the outer loop by a factor of one, which can be represented as 2 1 0 3 The wavefront transformation [41], <ref> [59] </ref> is an example of a unimodular transformation that can be thought of as a combination of other unimodular transformations. <p> LOOP TRANSFORMATION THEORY BASICS 39 permutable (see Theorem 2.5), a result we show in Theorem 2.10. This result is well known, if usually expressed somewhat differently <ref> [59] </ref>. We show how we derive these legality conditions for tiling in our framework. <p> We thus discuss only the changes to the for loops and bounds. While it has been suggested that strip-mining and interchanging be applied to determine the bounds of a tiled loop, this approach is not straightforward when the iteration space is not rectangular <ref> [59] </ref>. A more general CHAPTER 2. LOOP TRANSFORMATION THEORY BASICS 43 @ @ @ @ fl fl fl fl ? - i min i max approach is discussed by Ancourt and Irigoin [8]. <p> Thus we use the model in Figure 2.10 (a) rather than that in Figure 2.10 (b). Using this model is equivalent to conceptually performing the non-basic-to-basic-loop transformation of Abu-Sufah [1] and Wolfe <ref> [59] </ref>: 6 6 Our approach involves a straightforward application of Abu-Sufah's non-basic-to-basic-loop transformation. However, he used it only for blocking, and was not precise about when the transformation is legal to apply. He also did not attempt to remove the extra computation from the innermost loop. <p> Removal of unnecessary ifs from the innermost loop should improve performance of the resultant code. Let us consider SAa, the lexically topmost conditionally executed code. 8 In particular, our framework allows non-perfectly nested loops to be interchanged. In contrast, Wolfe's non-perfect nest interchange technique <ref> [59] </ref> requires an impractically complex dependence notation and computation. Wolfe's non-perfectly nested interchange transformation is non-unimodular (in fact, nonlinear), so the legality conditions are different. Neither Wolfe's non-perfectly nested interchange nor ours is a strict superset of the other's. CHAPTER 2. <p> Node splitting does not remove any cycles and so is not effective in parallelizing this loop. The technique Wolfe suggests for using on this loop, of course, is the wavefront transformation, in the guise of a loop skew followed by a loop interchange <ref> [59] </ref>; and indeed, those transformations are effective in this context. In another paper [58], he discusses how the tiling transformation can also be used to achieve parallelism. <p> We don't study it in this thesis. Abu-Sufah's general approach was modified by Wolfe, so that tiling was achieved CHAPTER 2. LOOP TRANSFORMATION THEORY BASICS 61 via the strip-mining and loop interchange <ref> [59] </ref>. Wolfe was also the one to introduce the term "tiling", and he discussed how loop skewing can be incorporated to make non-rectangular tiles [58]. The strengths and weaknesses of his approach are those discussed in Section 2.8.
Reference: [60] <author> M. J. Wolfe. </author> <title> Beyond induction variables. </title> <booktitle> In ACM SIGPLAN '92 Conf. on Programming Language Design and Implementation., </booktitle> <month> Jun. </month> <year> 1992. </year>
Reference-contexts: MRSPQ is an induction variable, but it is non-linear in MP, in that MRSPQ = (MP 1)MP + MQ: Recognizing such non-linear induction variables is possible <ref> [60] </ref>, but our compiler does not do so. Because the induction variable is non-linear, our compiler was unable to even move MRSPQ out of the MQ loop. Had it done so, the code would have been MRSPQ = 0 DO 40 MP=1,NP VAL=XRSPQ (MRSPQ+MQ) DO 20 MI=1,MORB CHAPTER 6.
Reference: [61] <author> M. J. Wolfe and C.-W. Tseng. </author> <title> The Power test for data dependence. </title> <type> Technical Report CS/E 90-015, </type> <institution> Dept. of Computer Science and Engineering, Oregon Graduate Institute, </institution> <month> August </month> <year> 1990. </year>
Reference-contexts: Our dependence vector representation is a generalization of both distance and direction vectors, keeping available the precision of distance vectors when such information is available, while allowing us to represent dependences in general loop nests concisely. Techniques for extracting dependence vectors from loop nests are well known [35], [42], <ref> [61] </ref>, [59]. In particular, Irigoin and Triolet [35] discuss a technique to summarize dependence vectors as a finite number distance vectors if possible. 2.3.1 Representation The representation we use for dependence vectors encompasses both distance and direction vectors.
References-found: 61

