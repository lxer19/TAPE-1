URL: http://www.cse.ucsc.edu/research/ml/papers/scale_sensitive_dimensions.ps
Refering-URL: http://www.cse.ucsc.edu/~haussler/pubs.html
Root-URL: http://www.cse.ucsc.edu
Email: Email: noga@math.tau.ac.il. Email: shai@cs.technion.ac.il.  Email: cesabian@dsi.unimi.it Email: haussler@cse.ucsc.edu  
Title: Scale-sensitive Dimensions, Uniform Convergence, and Learnability  
Author: Noga Alon R. and B. Sackler Shai Ben-David Nicolo Cesa-Bianchi David Haussler 
Keyword: Uniform laws of large numbers, Glivenko-Cantelli classes, Vapnik-Chervonenkis dimension, PAC learning.  
Note: Research supported in part by a USA-Israeli BSF grant.  This is the author to whom all correspondence should be sent. Part of this research was done while this author was visiting UC Santa Cruz partially supported by the "Progetto finalizzato  under grant 91.00884.69.115.09672. The author also acknowledges support of ESPRIT Working Group in Neural and Computational Learning, NeuroCOLT 8556.  
Address: Israel  Haifa 32000, Israel  Via Comelico 39 20135 Milano, Italy  Santa Cruz, CA 95064, USA  
Affiliation: Department of Mathematics  Faculty of Exact Sciences Tel Aviv University,  Dept. of Computer Science Technion  DSI, Universita di Milano  Dept. of Computer Science UC Santa Cruz  sistemi informatici e calcolo parallelo" of CNR  
Abstract: Learnability in Valiant's PAC learning model has been shown to be strongly related to the existence of uniform laws of large numbers. These laws define a distribution-free convergence property of means to expectations uniformly over classes of random variables. Classes of real-valued functions enjoying such a property are also known as uniform Glivenko-Cantelli classes. In this paper we prove, through a generalization of Sauer's lemma that may be interesting in its own right, a new characterization of uniform Glivenko-Cantelli classes. Our characterization yields Dudley, Gine, and Zinn's previous characterization as a corollary. Furthermore, it is the first based on a simple combinatorial quantity generalizing the Vapnik-Chervonenkis dimension. We apply this result to obtain the weakest combinatorial condition known to imply PAC learnability in the statistical regression (or "agnostic") framework. Furthermore, we show a characterization of learnability in the probabilistic concept model, solving an open problem posed by Kearns and Schapire. These results show that the accuracy parameter plays a crucial role in determining the effective complexity of the learner's hypothesis class. An extended abstract of this paper appeared in the Proceedings of the 34th Annual Symposium on the Foundations of Computer Science, IEEE Press, 1993. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Alon and V.D. Milman. </author> <title> Embedding of ` k 1 in finite dimensional Banach spaces. </title> <journal> Israel Journal of Mathematics, </journal> <volume> 45 </volume> <pages> 265-280, </pages> <year> 1983. </year>
Reference-contexts: In this model, a finite sample of pairs (x; y) is obtained through independent draws from a fixed distribution P over X fi <ref> [0; 1] </ref>. The goal of the learner is to be able to estimate the conditional expectation of y given x. This quantity is defined by a function f : X ! [0; 1], called the regression function in statistics. <p> finite sample of pairs (x; y) is obtained through independent draws from a fixed distribution P over X fi <ref> [0; 1] </ref>. The goal of the learner is to be able to estimate the conditional expectation of y given x. This quantity is defined by a function f : X ! [0; 1], called the regression function in statistics. The learner is given a class H of candidate regression functions, which may or may not include the true regression function f . <p> This new lemma considerably extends some previously known results concerning f0; 1; flg tournament codes [21, 7]. As other related variants of Sauer's Lemma were proven useful in different areas, such as geometry and Banach space theory (see, e.g., <ref> [15, 1] </ref>), we also have hope to apply this result further. 2 Uniform Glivenko-Cantelli classes The uniform, distribution-free convergence of empirical means to true expectations for classes of real-valued functions has been studied by Dudley, Gine, Pollard, Talagrand, Vapnik, Zinn, and others in the area of empirical processes. <p> These results go under the general name of uniform laws of large numbers. We give a new combinatorial characterization of this phenomenon using methods related to those pioneered by Vapnik and Chervonenkis. Let F be a class of functions from a set X into <ref> [0; 1] </ref>. (All the results presented in this section can be generalized to classes of functions taking values in any bounded real range.) Let P denote a probability distribution over X such that f is P -measurable for all f 2 F . <p> Then F is a uniform Glivenko Cantelli class if and only if V C-dim (F ) is finite. Several generalizations of the V C-dimension to classes of real-valued functions have been previously proposed: Let F be a class of <ref> [0; 1] </ref>-valued functions on some domain set X. * (Pollard [16], see also [12]): We say F P -shatters a set A X if there exists a function s : A ! R such that, for every E A, there exists some f E 2 F satisfying: For every x 2 <p> Let F be a class of <ref> [0; 1] </ref>-valued functions on some domain set X and let fl be a positive real number. We say F P fl shatters a set A X if there exists a function s : A ! [0; 1] such that for every 4 Actually Dudley et al. use outer measure here, to <p> Let F be a class of <ref> [0; 1] </ref>-valued functions on some domain set X and let fl be a positive real number. We say F P fl shatters a set A X if there exists a function s : A ! [0; 1] such that for every 4 Actually Dudley et al. use outer measure here, to avoid some measurability problems in certain cases. 4 E A there exists some f E 2 F satisfying: For every x 2 A n E, f E (x) s (x) fl and, for every x <p> The second lemma is proven through the pigeonhole principle. Lemma 2.1 For any F and any fl &gt; 0, P fl -dim (F ) P -dim (F ) and V fl -dim (F ) V -dim (F ). Lemma 2.2 For any class F of <ref> [0; 1] </ref>-valued functions and for all fl &gt; 0, V fl -dim (F ) P fl -dim (F ) 2 1 2 The P fl and the V fl dimensions have the advantage of being sensitive to the scale at which differences in function values are considered significant. <p> Our main result of this section is the following new characterization of uniform Glivenko-Cantelli classes, which exploits the scale-sensitive quality of the P fl and the V fl dimensions. Theorem 2.2 Let F be a class of functions from X into <ref> [0; 1] </ref>. 1. <p> our purposes) shows that the finiteness of neither P -dim nor V -dim yields a characterization of Glivenko Cantelli classes. (Throughout the paper we use ln to denote the natural logarithm and log to denote the logarithm in base 2.) 5 Example 2.1 Let F be the class of all <ref> [0; 1] </ref>-valued functions f defined on the positive integers and such that f (x) e x for all x 2 N and all f 2 F . Observe that, for all fl &gt; 0, P fl -dim (F ) = V fl -dim (F ) = ln 1 k . <p> Theorem 2.3 ([9, Theorem 6, page 500]) Let F be a class of functions from X into <ref> [0; 1] </ref>. Then 1. F is a uniform Glivenko-Cantelli class if and only if lim n!1 H n (*; F )=n = 0 for all * &gt; 0. 2. <p> For a class F of nonnegative real-valued functions let F def = ff : f 2 F g. We need the following lemma. Lemma 3.1 For any class F of <ref> [0; 1] </ref>-valued functions on a set X and for any &gt; 0, 1. for every fl =2, S-dim (F ) P fl -dim (F ); 2. for every * 2 and every x n 2 X n , M (*; F ; x n ) M (2; F ; x n <p> Thus A is P =2 -shattered by F , as can be seen using the function s 0 : A ! <ref> [0; 1] </ref> defined by s 0 (x) def To prove part 2 of the lemma it is enough to observe that, by the definition of F , for all f; g 2 F and all x 2 X, jf (x) g (x)j 2 implies jf (x) g (x)j 2. 2 7 <p> The first one is a straightforward adaptation of [22, Section A.6, p. 223]. Lemma 3.3 Let F be a class of functions from X into <ref> [0; 1] </ref> and let P be a distribution over X. <p> Lemma 3.4 Let F be a class of functions from X into <ref> [0; 1] </ref> and P a distribution over X. Choose 0 &lt; * &lt; 1 and let d = P *=4 -dim (F ). <p> It is unknown if log 2 n can be replaced by log ff n where 1 ff &lt; 2. From the proof of Theorem 2.2 we can obtain bounds on the sample size sufficient to guarantee that, with high probability, in a class of <ref> [0; 1] </ref>-valued random variables each mean is close to its expectation. Theorem 3.1 Let F be a class of functions from X into [0; 1]. <p> From the proof of Theorem 2.2 we can obtain bounds on the sample size sufficient to guarantee that, with high probability, in a class of <ref> [0; 1] </ref>-valued random variables each mean is close to its expectation. Theorem 3.1 Let F be a class of functions from X into [0; 1]. <p> This solves an open problem from [14]. Let us begin by briefly introducing our learning model. The model examines learning problems involving statistical regression on <ref> [0; 1] </ref>-valued data. Assume X is an arbitrary set (as above), and Y = [0; 1]. Let Z = X fi Y , and let P be an unknown distribution on Z. <p> This solves an open problem from [14]. Let us begin by briefly introducing our learning model. The model examines learning problems involving statistical regression on <ref> [0; 1] </ref>-valued data. Assume X is an arbitrary set (as above), and Y = [0; 1]. Let Z = X fi Y , and let P be an unknown distribution on Z. Let X and Y be random variables respectively distributed according to the marginal of P on X and Y . <p> In general we cannot hope to approximate the regression function f for an arbitrary distribution P . Therefore we choose a hypothesis space H, which is a family of mappings h : X ! <ref> [0; 1] </ref>, and settle for a function in H that is close to the best approximation to f in the hypothesis space H. To this end, for each hypothesis h 2 H, let the function ` h : Z ! [0; 1] be defined by: ` h (x; y) = (h <p> which is a family of mappings h : X ! <ref> [0; 1] </ref>, and settle for a function in H that is close to the best approximation to f in the hypothesis space H. To this end, for each hypothesis h 2 H, let the function ` h : Z ! [0; 1] be defined by: ` h (x; y) = (h (x) y) 2 , for all x 2 X and y 2 [0; 1]. Thus P (` h ) is the mean square loss of h. <p> To this end, for each hypothesis h 2 H, let the function ` h : Z ! <ref> [0; 1] </ref> be defined by: ` h (x; y) = (h (x) y) 2 , for all x 2 X and y 2 [0; 1]. Thus P (` h ) is the mean square loss of h. <p> This follows by noting that, for every s; t; w 2 <ref> [0; 1] </ref>, j (s w) 2 (t w) 2 j 2js tj. 2 We end the proof of Theorem 4.1 by proving part 1. By Lemma 4.1, it suffices to show that ` H is (afl)-uniform Glivenko-Cantelli for some a &gt; 0.
Reference: [2] <author> P. Assouad and R.M. Dudley. </author> <title> Minimax nonparametric estimation over classes of sets. </title> <type> Preprint, </type> <year> 1989. </year>
Reference-contexts: The following was established by Vapnik and Chervonenkis [24] for the "if" part and (in a stronger version) by Assouad and Dudley <ref> [2] </ref> (see [9, proposition 11, page 504].) Theorem 2.1 Let F be a class of functions from X into f0; 1g. Then F is a uniform Glivenko Cantelli class if and only if V C-dim (F ) is finite.
Reference: [3] <author> P.L. Bartlett and P.M. </author> <title> Long. More theorems about scale-sensitive dimensions and learning. </title> <booktitle> In Proceedings of the 8th Annual Conference on Computational Learning Theory, </booktitle> <pages> pages 392-401. </pages> <publisher> ACM Press, </publisher> <year> 1995. </year>
Reference-contexts: We omit the proof of this theorem and mention instead that an improved sample size bound has been shown by Bartlett and Long <ref> [3, Equation (5), Theorem 9] </ref>. <p> In fact, suppose that Lemma 3.4 is improved by showing that sup x n M (*; F ; x n ) for some positive constant c and for d = P *=4 -dim (F ) (note that this implies our conjecture.) Then, combining this with <ref> [3, Lemma 10-11] </ref>, we can easily show a sample complexity bound of O 1 1 + ln ffi ; for any 0 &lt; t &lt; 1=8 for which d = P (1=8t)* -dim (F ) is finite.
Reference: [4] <author> P.L. Bartlett, </author> <title> P.M. Long, and R.C. Williamson. Fat-shattering and the learnability of real-valued functions. </title> <booktitle> In Proceedings of the 7th Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 299-310. </pages> <publisher> ACM Press, </publisher> <year> 1994. </year> <note> To appear in Machine Learning. </note>
Reference-contexts: We solve this problem showing that this condition is also sufficient for learning in the harder agnostic model. This last result has been recently complemented by Bartlett, Long, and Williamson <ref> [4] </ref>, who have shown that the P fl -dimension characterizes agnostic learnability with respect to the mean absolute error. In [20], Simon has independently proven a partial characterization of (nonagnostic) learnability using a slightly different notion of dimension. <p> In this model, originally introduced in [12] and also known as "agnostic learning", the learning task is to approximate the regression function of an unknown distribution. The probabilistic concept learning of Kearns and Schapire [14] and the real-valued function learning with noise investigated by Bartlett, Long, and Williamson <ref> [4] </ref> are special cases of this framework. We show that a class of functions is *-learnable whenever its P a* -dimension is finite for some constant a &gt; 0. <p> We formulate the lemma in terms of the square loss but it may be readily generalized to other loss functions. A similar result was independently proven by Bartlett, Long, and Williamson in <ref> [4] </ref> for the absolute loss L (x; y) = jx yj (and with respect to the l 1 metric rather than the l 1 metric used here).
Reference: [5] <author> S. Ben-David, N. Cesa-Bianchi, D. Haussler, </author> <title> and P.M. Long. Characterizations of learnability for classes of f0; : : :; ng-valued functions. </title> <journal> Journal of Computer and Systems Sciences, </journal> <volume> 50(1):74 86, </volume> <year> 1995. </year>
Reference-contexts: An open problem is what other notions of dimension may characterize uniform Glivenko-Cantelli classes. In fact, for classes of functions with finite range, the same characterization is achieved by each member of a family of several notions of dimension (see <ref> [5] </ref>). A second open problem is the asymptotic behaviour of the metric entropy: we have already shown that for all * &gt; 0, H n (*; F ) = O (log 2 n) if F is a uniform Glivenko-Cantelli class and H n (*; F ) = (n) otherwise.
Reference: [6] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M.K. Warmuth. </author> <title> Learnability and the Vapnik Chervonenkis dimension. </title> <journal> Journal of the ACM, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <year> 1989. </year>
Reference-contexts: In this case it is well known that the learnability of H is completely characterized by the finiteness of a simple combinatorial quantity known as the Vapnik-Chervonenkis (VC) dimension of H <ref> [24, 6] </ref>. An analogous combinatorial quantity for the probabilistic concept case was introduced by Kearns and Schapire. We call this quantity the P fl dimension of H, where fl &gt; 0 is a parameter that measures the "scale" to which the dimension of the class H is measured.
Reference: [7] <author> K.L. Collins, P.W. Shor, and J.R. Stembridge. </author> <title> A lower bound for f0; 1; flg tournament codes. </title> <journal> Discrete Mathematics, </journal> <volume> 63 </volume> <pages> 15-19, </pages> <year> 1987. </year>
Reference-contexts: In addition, our results rely on a combinatorial result that generalizes Sauer's Lemma [18, 19]. This new lemma considerably extends some previously known results concerning f0; 1; flg tournament codes <ref> [21, 7] </ref>. <p> Our result extends some previous work concerning f0; 1; flg tournament codes, proven in a completely different way (see <ref> [21, 7] </ref>). The lemma concerns the l 1 packing numbers of classes of functions into a finite range. It shows that, if such a class has a finite strong dimension, then its 2-packing number is bounded by a subexponential function of the cardinality of its domain.
Reference: [8] <author> R.M. Dudley. </author> <title> A course on empirical processes. </title> <booktitle> In Lecture Notes in Mathematics, </booktitle> <volume> volume 1097, </volume> <pages> pages 2-142. </pages> <publisher> Springer, </publisher> <year> 1984. </year>
Reference-contexts: Part of our results builds on this discretization technique. 3 All notions of approximation are with respect to mean square error. 2 paper, we assume that H (and later F ) satisfies some mild measurability conditions. A suitable such condition is the "image admissible Suslin" property (see <ref> [8, Section 10.3.1, page 101] </ref>.) The special case where the distribution P is taken over X fi f0; 1g was studied in [14] by Kearns and Schapire, who called this setting probabilistic concept learning. <p> Proof. A well-known result (see e.g. <ref> [8, Lemma 11.1.5] </ref> or [10, Lemma 2.5]) shows that, for all n 2=* 2 , ( f2F ) ( f2F * ) where P n 0 (f ) = 1 n n i ), P n 00 (f ) = 1 i=n+1 f (x 0 We combine this with a result
Reference: [9] <author> R.M. Dudley, E. Gine, and J. Zinn. </author> <title> Uniform and universal Glivenko-Cantelli classes. </title> <journal> Journal of Theoretical Probability, </journal> <volume> 4 </volume> <pages> 485-510, </pages> <year> 1991. </year>
Reference-contexts: In our main theorem, we establish the first combinatorial characterization of those classes of random variables whose means uniformly converge to their expectations for all distributions. Such classes of random variables have been called Glivenko-Cantelli classes in the empirical processes literature <ref> [9] </ref>. Given the usefulness of related uniform convergence results in combinatorics and randomized algorithms, we feel that this result may have many applications beyond those we give here. In addition, our results rely on a combinatorial result that generalizes Sauer's Lemma [18, 19]. <p> P . By P n (f ) we denote the random variable 1 n n where x 1 ; x 2 ; : : : ; x n are drawn independently at random according to P . 3 Following Dudley, Gine and Zinn <ref> [9] </ref>, we say that F is an *-uniform Glivenko-Cantelli class if lim sup Pr sup sup jP m (f ) P (f )j &gt; * = 0: (1) Here Pr denotes the probability with respect to the points x 1 ; x 2 ; : : : ; drawn independently at <p> the probability with respect to the points x 1 ; x 2 ; : : : ; drawn independently at random according to P . 4 The supremum is understood with respect to all distributions P over X (with respect to some suitable -algebra of subsets of X ; see <ref> [9] </ref>). We say that F satisfies a distribution-free uniform strong law of large numbers, or more briefly, that F is a uniform Glivenko-Cantelli class, if F is an *-uniform Glivenko-Cantelli class for all * &gt; 0. <p> The following was established by Vapnik and Chervonenkis [24] for the "if" part and (in a stronger version) by Assouad and Dudley [2] (see <ref> [9, proposition 11, page 504] </ref>.) Theorem 2.1 Let F be a class of functions from X into f0; 1g. Then F is a uniform Glivenko Cantelli class if and only if V C-dim (F ) is finite. <p> Note however that part 1 trivially implies part 2. The following simple example (a special case of <ref> [9, Example 4, page 508] </ref>, adapted to our purposes) shows that the finiteness of neither P -dim nor V -dim yields a characterization of Glivenko Cantelli classes. (Throughout the paper we use ln to denote the natural logarithm and log to denote the logarithm in base 2.) 5 Example 2.1 Let <p> The proof of part 1.b follows immediately from Lemma 2.2. 2 The proof of Theorem 2.2, in addition to being simpler than the proof in <ref> [9] </ref> (see Theorem 2.3 in this paper), also provides new insights into the behaviour of the metric entropy used in that characterization.
Reference: [10] <author> E. Gine and J. Zinn. </author> <title> Some limit theorems for empirical processes. </title> <journal> The Annals of Probability, </journal> <volume> 12 </volume> <pages> 929-989, </pages> <year> 1984. </year>
Reference-contexts: Proof. A well-known result (see e.g. [8, Lemma 11.1.5] or <ref> [10, Lemma 2.5] </ref>) shows that, for all n 2=* 2 , ( f2F ) ( f2F * ) where P n 0 (f ) = 1 n n i ), P n 00 (f ) = 1 i=n+1 f (x 0 We combine this with a result by Vapnik [22, pp.
Reference: [11] <author> I. Guyon, V. Vapnik, B. Boser, L. Bottou, and S.A. Solla. </author> <title> Structural risk minimization for character recognition. </title> <booktitle> In Proceedings of the 1991 Conference on Advances in Neural Informa tion Processing Systems, </booktitle> <pages> pages 471-479, </pages> <year> 1991. </year>
Reference-contexts: This position can be related to Rissanen's MDL principle [17], Vapnik's structural minimization method [22], and Guyon et al.'s notion of effective dimension <ref> [11] </ref>. Intuitively, the "dimension" of a class of functions decreases as the coarseness of the scale at which it is measured increases.
Reference: [12] <author> D. Haussler. </author> <title> Decision theoretic generalizations of the PAC model for neural net and other learning applications. </title> <journal> Information and Computation, </journal> <volume> 100(1) </volume> <pages> 78-150, </pages> <year> 1992. </year>
Reference-contexts: In this paper we demonstrate quantitatively how the accuracy parameter plays a crucial role in determining the effective complexity of the learner's hypothesis class. 2 We work within the decision-theoretic extension of the PAC framework, introduced in <ref> [12] </ref> and also known as agnostic learning. In this model, a finite sample of pairs (x; y) is obtained through independent draws from a fixed distribution P over X fi [0; 1]. The goal of the learner is to be able to estimate the conditional expectation of y given x. <p> Several generalizations of the V C-dimension to classes of real-valued functions have been previously proposed: Let F be a class of [0; 1]-valued functions on some domain set X. * (Pollard [16], see also <ref> [12] </ref>): We say F P -shatters a set A X if there exists a function s : A ! R such that, for every E A, there exists some f E 2 F satisfying: For every x 2 A n E, f E (x) &lt; s (x) and, for every x <p> In this model, originally introduced in <ref> [12] </ref> and also known as "agnostic learning", the learning task is to approximate the regression function of an unknown distribution. The probabilistic concept learning of Kearns and Schapire [14] and the real-valued function learning with noise investigated by Bartlett, Long, and Williamson [4] are special cases of this framework.
Reference: [13] <author> D. Haussler and P.M. </author> <title> Long. A generalization of Sauer's lemma. </title> <journal> J. Combinatorial Theory (A), </journal> <volume> 71 </volume> <pages> 219-240, </pages> <year> 1995. </year>
Reference-contexts: f is not in H, an *-close approximation to a function in H that best approximates f . (This analysis of learnability is purely information-theoretic, and does not take into account computational complexity.) Throughout the 1 Adapted from [14]. 2 Our philosophy can be compared to the approach studied in <ref> [13] </ref>, where the range of the functions in the hypothesis class is discretized in a number of elements proportional to the accuracy. In this case, one is interested in bounding the complexity of the discretized class through the dimension of the original class.
Reference: [14] <author> M. Kearns and R.E. Schapire. </author> <title> Efficient distribution-free learning of probabilistic concepts. </title> <journal> Journal of Computer and Systems Sciences, </journal> <volume> 48(3) </volume> <pages> 464-497, </pages> <year> 1994. </year> <booktitle> An extended abstract appeared in the Proceedings of the 30th Annual Symposium on the Foundations of Computer Science. </booktitle> <pages> 15 </pages>
Reference-contexts: an *-close approximation 3 to f within the class H, or if f is not in H, an *-close approximation to a function in H that best approximates f . (This analysis of learnability is purely information-theoretic, and does not take into account computational complexity.) Throughout the 1 Adapted from <ref> [14] </ref>. 2 Our philosophy can be compared to the approach studied in [13], where the range of the functions in the hypothesis class is discretized in a number of elements proportional to the accuracy. <p> A suitable such condition is the "image admissible Suslin" property (see [8, Section 10.3.1, page 101].) The special case where the distribution P is taken over X fi f0; 1g was studied in <ref> [14] </ref> by Kearns and Schapire, who called this setting probabilistic concept learning. If we further demand that the functions in H take only values in f0; 1g, it turns out that this reduces to one of the standard PAC learning frameworks for learning deterministic concepts. <p> F V -shatters sets of unbounded finite sizes, then let V -dim (F ) = 1.) It is easily verified (see below) that the finiteness of neither of these combinatorial quantities provides a characterization of uniform Glivenko-Cantelli classes (more precisely, they both provide only a sufficient condition.) Kearns and Schapire <ref> [14] </ref> introduced the following parametrized variant of the P -dimension. Let F be a class of [0; 1]-valued functions on some domain set X and let fl be a positive real number. <p> In this model, originally introduced in [12] and also known as "agnostic learning", the learning task is to approximate the regression function of an unknown distribution. The probabilistic concept learning of Kearns and Schapire <ref> [14] </ref> and the real-valued function learning with noise investigated by Bartlett, Long, and Williamson [4] are special cases of this framework. We show that a class of functions is *-learnable whenever its P a* -dimension is finite for some constant a &gt; 0. <p> This solves an open problem from <ref> [14] </ref>. Let us begin by briefly introducing our learning model. The model examines learning problems involving statistical regression on [0; 1]-valued data. Assume X is an arbitrary set (as above), and Y = [0; 1]. <p> The proof of part 2 uses arguments similar to those used to prove part 1.d of Theorem 2.2. Finally note that part 1 follows from part 2 by Lemma 2.2 (we remark that a more restricted version of part 1 was proven in Theorem 11 of <ref> [14] </ref>.) 2 5 Conclusions and open problems In this work we have shown a characterization of uniform Glivenko-Cantelli classes based on a combinatorial notion generalizing the Vapnik-Chervonenkis dimension.
Reference: [15] <author> V.D. Milman. </author> <title> Some remarks about embedding of ` k 1 in finite dimensional spaces. </title> <journal> Israel Journal of Mathematics, </journal> <volume> 43 </volume> <pages> 129-138, </pages> <year> 1982. </year>
Reference-contexts: This new lemma considerably extends some previously known results concerning f0; 1; flg tournament codes [21, 7]. As other related variants of Sauer's Lemma were proven useful in different areas, such as geometry and Banach space theory (see, e.g., <ref> [15, 1] </ref>), we also have hope to apply this result further. 2 Uniform Glivenko-Cantelli classes The uniform, distribution-free convergence of empirical means to true expectations for classes of real-valued functions has been studied by Dudley, Gine, Pollard, Talagrand, Vapnik, Zinn, and others in the area of empirical processes.
Reference: [16] <author> D. Pollard. </author> <title> Empirical Processes : Theory and Applications, </title> <booktitle> volume 2 of NSF-CBMS Regional Conference Series in Probability and Statistics. </booktitle> <institution> Institute of Math. Stat. and Am. Stat. Assoc., </institution> <year> 1990. </year>
Reference-contexts: Then F is a uniform Glivenko Cantelli class if and only if V C-dim (F ) is finite. Several generalizations of the V C-dimension to classes of real-valued functions have been previously proposed: Let F be a class of [0; 1]-valued functions on some domain set X. * (Pollard <ref> [16] </ref>, see also [12]): We say F P -shatters a set A X if there exists a function s : A ! R such that, for every E A, there exists some f E 2 F satisfying: For every x 2 A n E, f E (x) &lt; s (x) and,
Reference: [17] <author> J. Rissanen. </author> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471, </pages> <year> 1978. </year>
Reference-contexts: Whenever the learner is allowed a low degree of accuracy, the complexity of the hypothesis class might be measured on a coarse "scale" since, in this case, we do not need the full power of the entire set of models. This position can be related to Rissanen's MDL principle <ref> [17] </ref>, Vapnik's structural minimization method [22], and Guyon et al.'s notion of effective dimension [11]. Intuitively, the "dimension" of a class of functions decreases as the coarseness of the scale at which it is measured increases.
Reference: [18] <author> N. Sauer. </author> <title> On the density of families of sets. </title> <journal> J. Combinatorial Theory (A), </journal> <volume> 13 </volume> <pages> 145-147, </pages> <year> 1972. </year>
Reference-contexts: Given the usefulness of related uniform convergence results in combinatorics and randomized algorithms, we feel that this result may have many applications beyond those we give here. In addition, our results rely on a combinatorial result that generalizes Sauer's Lemma <ref> [18, 19] </ref>. This new lemma considerably extends some previously known results concerning f0; 1; flg tournament codes [21, 7].
Reference: [19] <author> S. Shelah. </author> <title> A combinatorial problem: Stability and order for models and theories in infinitary languages. </title> <journal> Pacific Journal of Mathematics, </journal> <volume> 41 </volume> <pages> 247-261, </pages> <year> 1972. </year>
Reference-contexts: Given the usefulness of related uniform convergence results in combinatorics and randomized algorithms, we feel that this result may have many applications beyond those we give here. In addition, our results rely on a combinatorial result that generalizes Sauer's Lemma <ref> [18, 19] </ref>. This new lemma considerably extends some previously known results concerning f0; 1; flg tournament codes [21, 7].
Reference: [20] <author> H.U. Simon. </author> <title> Bounds on the number of examples needed for learning functions. </title> <booktitle> In Proceedings of the First Euro-COLT Workshop, </booktitle> <pages> pages 83-94. </pages> <booktitle> The Institute of Mathematics and its Applications, </booktitle> <year> 1994. </year>
Reference-contexts: This last result has been recently complemented by Bartlett, Long, and Williamson [4], who have shown that the P fl -dimension characterizes agnostic learnability with respect to the mean absolute error. In <ref> [20] </ref>, Simon has independently proven a partial characterization of (nonagnostic) learnability using a slightly different notion of dimension. As in the pioneering work of Vapnik and Chervonenkis [24], our analysis of learnability begins by establishing appropriate uniform laws of large numbers.
Reference: [21] <author> J.H. van Lint. </author> <title> f0; 1; flg distance problems in combinatorics. </title> <booktitle> In Lecture Notes of the London Mathematical Society, </booktitle> <volume> volume 103, </volume> <pages> pages 113-135. </pages> <publisher> Cambridge University Press, </publisher> <year> 1985. </year>
Reference-contexts: In addition, our results rely on a combinatorial result that generalizes Sauer's Lemma [18, 19]. This new lemma considerably extends some previously known results concerning f0; 1; flg tournament codes <ref> [21, 7] </ref>. <p> Our result extends some previous work concerning f0; 1; flg tournament codes, proven in a completely different way (see <ref> [21, 7] </ref>). The lemma concerns the l 1 packing numbers of classes of functions into a finite range. It shows that, if such a class has a finite strong dimension, then its 2-packing number is bounded by a subexponential function of the cardinality of its domain.
Reference: [22] <author> V.N. Vapnik. </author> <title> Estimation of Dependences Based on Empirical Data. </title> <publisher> Springer Verlag, </publisher> <year> 1982. </year>
Reference-contexts: This position can be related to Rissanen's MDL principle [17], Vapnik's structural minimization method <ref> [22] </ref>, and Guyon et al.'s notion of effective dimension [11]. Intuitively, the "dimension" of a class of functions decreases as the coarseness of the scale at which it is measured increases. <p> Thus in either case t (2 (nb 2 ) dlog 2 ye ; n) y, completing the proof. 2 Before proving Theorem 2.2, we need two more lemmas. The first one is a straightforward adaptation of <ref> [22, Section A.6, p. 223] </ref>. Lemma 3.3 Let F be a class of functions from X into [0; 1] and let P be a distribution over X. <p> or [10, Lemma 2.5]) shows that, for all n 2=* 2 , ( f2F ) ( f2F * ) where P n 0 (f ) = 1 n n i ), P n 00 (f ) = 1 i=n+1 f (x 0 We combine this with a result by Vapnik <ref> [22, pp. 225-228] </ref> showing that for all * &gt; 0 Pr sup jP n 0 (f ) P n 00 (f )j &gt; * 6n E N (*=3; F ; x 0 fl This concludes the proof. 2 The next result applies Lemma 3.2 to bound the expected covering number of <p> We do so via the next two lemmas. Let ` H = f` h : h 2 Hg. Lemma 4.1 If ` H is an *-uniform Glivenko-Cantelli class, then H is (3*)-learnable. 12 Proof. The proof uses the method of empirical risk minimization, analyzed by Vapnik <ref> [22] </ref>.
Reference: [23] <author> V.N. Vapnik. </author> <title> Inductive principles of the search for empirical dependencies. </title> <booktitle> In Proceedings of the 2nd Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 3-21, </pages> <year> 1989. </year>
Reference-contexts: Let the P -dimension (denoted by P -dim) be the maximal cardinality of a set A X that is P -shattered by F . (If F P -shatters sets of unbounded finite sizes, then let P -dim (F ) = 1.) * (Vapnik <ref> [23] </ref>): We say F V -shatters a set A X if there exists a constant ff 2 R such that, for every E A, there exists some f E 2 F satisfying: For every x 2 A n E, f E (x) &lt; ff and, for every x 2 E, f
Reference: [24] <author> V.N. Vapnik and A.Y. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theory of Probability and its Applications, </journal> <volume> 16(2) </volume> <pages> 264-280, </pages> <year> 1971. </year>
Reference-contexts: In this case it is well known that the learnability of H is completely characterized by the finiteness of a simple combinatorial quantity known as the Vapnik-Chervonenkis (VC) dimension of H <ref> [24, 6] </ref>. An analogous combinatorial quantity for the probabilistic concept case was introduced by Kearns and Schapire. We call this quantity the P fl dimension of H, where fl &gt; 0 is a parameter that measures the "scale" to which the dimension of the class H is measured. <p> In [20], Simon has independently proven a partial characterization of (nonagnostic) learnability using a slightly different notion of dimension. As in the pioneering work of Vapnik and Chervonenkis <ref> [24] </ref>, our analysis of learnability begins by establishing appropriate uniform laws of large numbers. In our main theorem, we establish the first combinatorial characterization of those classes of random variables whose means uniformly converge to their expectations for all distributions. <p> The following was established by Vapnik and Chervonenkis <ref> [24] </ref> for the "if" part and (in a stronger version) by Assouad and Dudley [2] (see [9, proposition 11, page 504].) Theorem 2.1 Let F be a class of functions from X into f0; 1g. <p> The results by Dudley et al. also give similar characterizations using l p norms in place of the l 1 norm. Related results were proved earlier by Vapnik and Chervonenkis <ref> [24, 25] </ref>. In particular, they proved an analogue of Theorem 2.3, where the convergence of means to expectations is characterized for a single distribution P .
Reference: [25] <author> V.N. Vapnik and A.Y. Chervonenkis. </author> <title> Necessary and sufficient conditions for uniform convergence of means to mathematical expectations. </title> <journal> Theory of Probability and its Applications, </journal> <volume> 26(3) </volume> <pages> 532-553, </pages> <year> 1981. </year>
Reference-contexts: The results by Dudley et al. also give similar characterizations using l p norms in place of the l 1 norm. Related results were proved earlier by Vapnik and Chervonenkis <ref> [24, 25] </ref>. In particular, they proved an analogue of Theorem 2.3, where the convergence of means to expectations is characterized for a single distribution P .
References-found: 25

