URL: ftp://ftp.cs.colorado.edu/users/mozer/papers/lightctl.ps
Refering-URL: http://www.cs.colorado.edu/~mozer/papers/lightctl.html
Root-URL: http://www.cs.colorado.edu
Title: A comparison of neural net and conventional techniques for lighting control  
Author: Robert Dodier Diane Lukianow James Ries and Michael C. Mozer 
Note: Adaptive House are also discussed.  
Date: August 25, 1994  
Abstract: We compare two techniques for lighting control in an actual room equipped with seven banks of lights and photoresistors to detect the lighting level at four sensing points. Each bank of lights can be independently set to one of sixteen intensity levels. The task is to determine the device intensity levels that achieve a particular configuration of sensor readings. One technique we explored uses a neural network to approximate the mapping between sensor readings and device intensity levels. The other technique we examined uses a conventional feedback control loop. The neural network approach appears superior both in that it does not require experimentation on the fly (and hence fluctuating light intensity levels during settling, and lengthy settling times) and in that it can deal with complex interactions that conventional control techniques do not handle well. This comparison was performed as part of the "Adaptive House" project, which is described briefly. Further directions for control in the 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Barto, A. </author> <title> "Connectionist Learning for Control," in Neural Networks for Control, </title> <editor> T. Miller, R. Sutton, P. Werbos, eds. </editor> <address> Cambridge, MA, </address> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: subsection.) The separation of the back-prop class from the forward model class means that existing back-prop code could be used, without compromising its integrity, in the experimental forward model class. 4.2 Determining controls with the model The key idea behind the neural network light control is differentiating the network model <ref> [1] </ref>, a technique also referred to as backpropagating through the model. Since the network maps device level inputs to sensor level outputs, it is necessary to adjust the inputs to achieve the desired outputs.
Reference: [2] <author> Cybenko, G. </author> <title> "Approximation by Superpositions of a Sigmoidal Function." Mathematics of Control, Signals, </title> <journal> and Systems, </journal> <volume> vol. 2 (1989), </volume> <pages> pp 303-314. </pages>
Reference-contexts: The relevant properties of the neural networks include smoothness (since the output is the result of the composition of smooth functions), and universal approximation capability. It has been shown <ref> [2, 6] </ref> that a net with one hidden layer can approximate any continuous mapping if the hidden layer has enough units, although in practice it is difficult to tell how many hidden units is "enough." In the problem at hand, we need a forward model which predicts how light device levels
Reference: [3] <author> Dayan, P., and G. Hinton. </author> <title> "Feudal Reinforcement Learning," </title> <booktitle> in Advances in Neural Information Processing Systems 5, </booktitle> <editor> S. Hanson, J. Cowan, C. Giles, eds. </editor> <address> San Mateo, CA, </address> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1993. </year> <pages> pp 271-278. </pages>
Reference-contexts: Thus to some extent dissatisfaction can be measured by the magnitude of the changes which the inhabitant makes directly. We believe that a two-level control system may be appropriate here, a so-called "feudal" control scheme, as proposed by Dayan and Hinton <ref> [3] </ref>. In the Adaptive House, the higher level would anticipate the desires of the inhabitant, and would specify appropriate setpoints to the lower level. The lower level would consist of the neural network control system described in this paper, and would try to achieve the specified setpoints.
Reference: [4] <author> Dodier, R. </author> <title> "Increase of Apparent Complexity is Due to Decrease of Training Set Error," </title> <booktitle> Proceedings of the 1993 Connectionist Models Summer School, </booktitle> <address> Hillsdale, NJ, </address> <publisher> Lawrence Erlbaum Associates, </publisher> <year> 1993. </year> <pages> pp 343-350. </pages>
Reference: [5] <author> Finnoff, W., F. Hergert, and H. Zimmerman. </author> <title> "Improving Model Selection by Nonconvergent Methods." </title> <booktitle> Neural Networks, </booktitle> <volume> vol 6 no 6 (1993), p 771. </volume>
Reference-contexts: Thus each transformed variable has mean zero and unit standard deviation. Altogether, 2925 patterns were selected at random for training the network, and the remaining 2925 patterns were used for validation, as described by Finnoff et al. <ref> [5] </ref>; it has been shown by Plutowski et al. [7] that validation error is an estimate of generalization error. The forward-model network was trained for about 3000 epochs. During this time the validation error continued to decrease, suggesting the the net is not over-trained.
Reference: [6] <author> Hornik, K., M. Stinchcombe, and H. White. </author> <title> "Multilayer Feedforward Networks are Universal Approximators." </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 2 (1989), </volume> <pages> pp 359-366. </pages>
Reference-contexts: The relevant properties of the neural networks include smoothness (since the output is the result of the composition of smooth functions), and universal approximation capability. It has been shown <ref> [2, 6] </ref> that a net with one hidden layer can approximate any continuous mapping if the hidden layer has enough units, although in practice it is difficult to tell how many hidden units is "enough." In the problem at hand, we need a forward model which predicts how light device levels
Reference: [7] <author> Plutowski, M., S. Sakata, H. White. </author> <title> "Cross-Validation Estimates IMSE," </title> <booktitle> in Advances in Neural Information Processing Systems 6, </booktitle> <editor> J. Cowan, G. Tesauro, J. Alspector, eds. </editor> <address> San Francisco, CA, </address> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1994. </year> <pages> pp 391-398. </pages>
Reference-contexts: Thus each transformed variable has mean zero and unit standard deviation. Altogether, 2925 patterns were selected at random for training the network, and the remaining 2925 patterns were used for validation, as described by Finnoff et al. [5]; it has been shown by Plutowski et al. <ref> [7] </ref> that validation error is an estimate of generalization error. The forward-model network was trained for about 3000 epochs. During this time the validation error continued to decrease, suggesting the the net is not over-trained.
Reference: [8] <author> Press, W., B. Flannery, S. Teukolsky, W. Vetterling. </author> <title> Numerical Recipes in C. </title> <publisher> Cambridge, Cambridge University Press, </publisher> <year> 1988. </year>
Reference-contexts: The ratio f = X 1 =X 2 of two independent chi-square variables X 1 and X 2 has an F distribution, and the significance of the ratio can be computed using the incomplete beta function I ff ; as described by Press et al. <ref> [8] </ref>. Assuming that f &gt; 1; Pr (F f) + Pr F f = 2I ff ( 2 -1 ); ff = 1 + -1 (7) Here -1 and -2 are the d.f. of X 1 and X 2 ; respectively.
Reference: [9] <author> Rumelhart, D., R. Durbin, R. Golden, and Y. Chauvin. </author> <title> "Backpropagation: The Basic Theory," to appear in Mathematical Perspectives on Neural Networks, </title> <editor> P. Smolensky, M. Mozer, and D. Rumelhart, eds. </editor> <volume> 17 </volume>
Reference-contexts: It is of interest that the weight decay scheme discussed by Rumelhart et al. <ref> [9] </ref> assumes that weights should be distributed in just this manner; the decay parameter is related to the variance of the weight distribution, with = 1= 2 ; where 2 is the variance of the prior distribution of weights.
Reference: [10] <author> Weigend, A., B. Huberman, D. Rumelhart. </author> <title> "Predicting Sunspots and Ex--change Rates with Connectionist Networks," in Nonlinear Modeling and Forecasting, </title> <editor> M. Casdagli and S. Eubank, eds. </editor> <booktitle> Santa Fe Institute Studies in the Sciences of Complexity Proc. Vol XII. </booktitle> <address> Reading, MA, </address> <publisher> Addison-Wesley, </publisher> <year> 1992. </year> <pages> pp 395-432. </pages>
Reference-contexts: While the number of very small weights did not change much during training, some of the medium-sized weights became larger, increasing the spread of weights around zero and increasing the nonlinearity of the network. For another view of weight distributions, see Weigend et al. <ref> [10] </ref>. 1 It is not known what performance index is most meaningful for training, but there is some justification for a squared-error performance index.
References-found: 10

