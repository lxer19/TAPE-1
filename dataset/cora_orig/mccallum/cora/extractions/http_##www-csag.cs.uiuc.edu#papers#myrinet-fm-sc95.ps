URL: http://www-csag.cs.uiuc.edu/papers/myrinet-fm-sc95.ps
Refering-URL: http://www-csag.cs.uiuc.edu/papers/index.html
Root-URL: http://www.cs.uiuc.edu
Title: High Performance Messaging on Workstations: Illinois Fast Messages (FM) for Myrinet  
Note: 2 which is two orders of magnitude smaller (54 vs. 4,409 bytes).  
Abstract: Scott Pakin y Mario Lauria z Andrew Chien y Copyright c fl1995 by the Association for Computing Machinery, Inc. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request Permissions from Publication Dept., ACM Inc., Fax +1 (212) 869-0481, or &lt;permissions@acm.org&gt;. Abstract In most computer systems, software overhead dominates the cost of messaging, reducing delivered performance, especially for short messages. Efficient software messaging layers are needed to deliver the hardware performance to the application level and to support tightly-coupled workstation clusters. Illinois Fast Messages (FM) 1.0 is a high speed messaging layer that delivers low latency and high bandwidth for short messages. For 128-byte packets, FM achieves bandwidths of 16.2 MB/s and one-way latencies 32 s on Myrinet-connected SPARCstations (user-level to user-level). For shorter packets, we have measured one-way latencies of 25 s, and for larger packets, bandwidth as high as to 19.6 MB/s | delivered bandwidth greater than OC-3. FM is also superior to the Myrinet API messaging layer, not just in terms of latency and usable bandwidth, but also in terms of the message half-power point (n 1 We describe the FM messaging primitives and the critical design issues in building a low-latency messaging layers for workstation clusters. Several issues are critical: the division of labor between host and network coprocessor, management of the input/output (I/O) bus, and buffer management. To achieve high performance, messaging layers should assign as much functionality as possible to the host. If the network interface has DMA capability, the I/O bus should be used asymmetrically, with the host processor moving data to the network and exploiting DMA to move data to the host. Finally, buffer management should be extremely simple in the network coprocessor and match queue structures between the network coprocessor and host memory. Detailed measurements show how each of these features contribute to high performance.
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> The Generic Active Message Interface Specification. </institution> <note> Available from http://now.cs.berkeley.edu/Papers/Papers/gam spec.ps, </note> <year> 1994. </year>
Reference-contexts: Based on these considerations, we chose a 128-byte frame size for FM 1.0. Larger messages will require segmentation and reassembly into frames of this size. Our approach differs from the Generic Active Messages model <ref> [1] </ref> which provides extremely short (4 word) messages and longer network DMA transfers. Serendipitously, the FM frame size is close to the best size for supporting TCP/IP and UDP/IP traffic, where the vast majority of packets would fit into a single frame [5].
Reference: [2] <author> R. Alverson, D. Callahan, D. Cummings, B. Koblenz, A. Porterfield, and B. Smith. </author> <title> The Tera computer system. </title> <booktitle> 1990 International Conf. on Supercomputing, </booktitle> <month> June 11-15 </month> <year> 1990. </year> <note> Published as Computer Architecture News 18:3. </note>
Reference-contexts: Further, the buffer requirements for a particular node do not increase with the number of hosts in the system. The idea behind return-to-sender has been used in MPP's such as the Cray T3D, and is similar to deflection and chaos routing as used in the TERA-1 machine <ref> [2] </ref>. The well-known drawback of all of these retransmission schemes is that delivery order is not preserved. Because the reject queue holds packets rejected by any node, it can be though of as "network window" and provides an efficient use of pinned memory.
Reference: [3] <author> T. Anderson, D. Culler, and D. Patterson. </author> <title> A case for NOW (networks of workstations). </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 54-64, </pages> <year> 1995. </year>
Reference-contexts: 1 Introduction As the performance of workstations reaches hundreds of megaflops (even gigaflops), networks of workstations provide an increasingly attractive vehicle for high performance computation <ref> [3] </ref>. In fact, workstation clusters have a number of advantages over their major competitors (massively parallel processors based on workstation processors). These advantages can include lower cost, a larger software base, and greater accessibility.
Reference: [4] <author> T. M. Anderson and R. S. Cornelius. </author> <title> High-performance switching with Fibre Channel. </title> <booktitle> In Digest of Papers Compcon 1992, </booktitle> <pages> pages 261-268. </pages> <publisher> IEEE Computer Society Press, </publisher> <address> 1992. Los Alamitos, Calif. </address>
Reference-contexts: Springfield Ave., Urbana, IL 61801, USA z Dipartimento di Informatica e Sistemistica, Universita di Napoli "Federico II", via Claudio 21, 80125 Napoli, Italy 1 interconnects such as ATM [7], Fibre Channel <ref> [4] </ref>, FDDI [13], and Myrinet [6] present the possibility that workstation clusters can deliver good performance on a broader range of parallel computations. Achieving efficient communication is the major challenge in synthesizing effective parallel machines from networks of workstations. <p> The communication primitives in these libraries have typically exploited operating system communication services, running atop 10 Mb/s Ethernet, or more recently some higher speed physical media such as FDDI [13], ATM [7] or Fibre Channel <ref> [4] </ref>. While such facilities are useful for coarse-grain decoupled parallelism, they suffer from high software communication overhead (operating system calls) and low achieved bandwidth (media limits or software overhead), and thus cannot support more tightly coupled or finer-grained parallelism.
Reference: [5] <author> G. Armitage and K. Adams. </author> <title> How inefficient is IP over ATM anyway? IEEE Network, </title> <month> Jan/Feb </month> <year> 1995. </year>
Reference-contexts: Serendipitously, the FM frame size is close to the best size for supporting TCP/IP and UDP/IP traffic, where the vast majority of packets would fit into a single frame <ref> [5] </ref>. This presents the possibility that a single low-level messaging layer can support both efficient parallel computation and traditional protocols. As mentioned in Section 4.5, return-to-sender is an optimistic flow control protocol.
Reference: [6] <author> Nanette J. Boden, Danny Cohen, Robert E. Felderman, Alan E. Kulawik, Charles L. Seitz, Jakov N. Seizovic, and Wen-King Su. </author> <title> Myrinet|a gigabit-per-second local-area network. </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 29-36, </pages> <month> February </month> <year> 1995. </year> <note> Available from http://www.myri.com/myricom/Hot.ps </note> . 
Reference-contexts: Springfield Ave., Urbana, IL 61801, USA z Dipartimento di Informatica e Sistemistica, Universita di Napoli "Federico II", via Claudio 21, 80125 Napoli, Italy 1 interconnects such as ATM [7], Fibre Channel [4], FDDI [13], and Myrinet <ref> [6] </ref> present the possibility that workstation clusters can deliver good performance on a broader range of parallel computations. Achieving efficient communication is the major challenge in synthesizing effective parallel machines from networks of workstations. <p> The Fast Messages project focuses on optimizing the software messaging layer that resides between lower-level communication services and the hardware. It is available on both the Cray T3D [22, 23] and Myricom's Myrinet <ref> [6] </ref>. Using the Myrinet, FM provides MPP-like communication performance on workstation clusters. FM on the Myrinet achieves low-latency, high-bandwidth messaging for short messages delivering 32 s latency and 16 MBytes/s bandwidth for 128 byte packets (user-level to user-level). <p> Consequently, many parts of the workstation architecture affect performance; in the following paragraphs we describe the performance of these salient network and workstation features. Myrinet Network Features Myrinet is a high speed LAN interconnect which uses byte-wide parallel copper links to achieve physical link bandwidth of 76.3 MB/s <ref> [6] </ref>. Myrinet uses a network coprocessor (LANai) which controls the physical link and contains three DMA engines (incoming channel, outgoing channel, and host) to move data efficiently. Host-LANai coordination is achieved by mapping the LANai's memory into the host address space. <p> While most services can be programmed on either the host or the network interface, balanced decompositions allow overlapping of the two levels and supporting a higher message rate. Of course, configurations of host and network coprocessor in which the host processor is much faster and has significantly more memory <ref> [6, 20, 26] </ref> (e.g. our configuration) favor assigning more work to the host. 4 Fast Messages 1.0 Implementation Design The FM 1.0 implementation consists of two basic parts: the host program and the LANai control program (LCP).
Reference: [7] <author> CCITT, </author> <title> SG XVIII, Report R34. Draft Recommendation I.150: B-ISDN ATM functional characteristics, </title> <month> June </month> <year> 1990. </year>
Reference-contexts: Springfield Ave., Urbana, IL 61801, USA z Dipartimento di Informatica e Sistemistica, Universita di Napoli "Federico II", via Claudio 21, 80125 Napoli, Italy 1 interconnects such as ATM <ref> [7] </ref>, Fibre Channel [4], FDDI [13], and Myrinet [6] present the possibility that workstation clusters can deliver good performance on a broader range of parallel computations. Achieving efficient communication is the major challenge in synthesizing effective parallel machines from networks of workstations. <p> The communication primitives in these libraries have typically exploited operating system communication services, running atop 10 Mb/s Ethernet, or more recently some higher speed physical media such as FDDI [13], ATM <ref> [7] </ref> or Fibre Channel [4]. While such facilities are useful for coarse-grain decoupled parallelism, they suffer from high software communication overhead (operating system calls) and low achieved bandwidth (media limits or software overhead), and thus cannot support more tightly coupled or finer-grained parallelism.
Reference: [8] <author> Andrew A. Chien, Vijay Karamcheti, John Plevyak, and Xingbin Zhang. </author> <title> Concurrent aggregates language report 2.0. </title> <note> Available via anonymous ftp from cs.uiuc.edu in /pub/csag or from http://www-csag.cs.uiuc.edu/, September 1993. </note>
Reference-contexts: FM is designed to support efficient implementation of a variety of communication libraries and run-time systems. To explore interface, buffering, and scheduling issues, we are building 16 implementations of MPI [14], TCP/IP [10], and the Illinois Concert system's runtime <ref> [8] </ref>. MPI is of growing popularity among application builders, presents interesting collective communication operations, and there are efficient implementations to compare against [15]. TCP/IP is a legacy protocol in widespread use. And the Illinois Concert system is a fine-grained programming system which depends critically on low-cost high performance communication.
Reference: [9] <author> D. Clark, V. Jacobson, J Romkey, and H. Salwen. </author> <title> An analysis of TCP processing overhead. </title> <journal> IEEE Communication Magazine, </journal> <volume> 27(6) </volume> <pages> 23-29, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Achieving efficient communication is the major challenge in synthesizing effective parallel machines from networks of workstations. Unfortunately, to date the most common messaging layers used for clusters (TCP/IP <ref> [9] </ref>, PVM [27]) generally have not delivered a large fraction of the underlying communication hardware performance to the applications. Reasons for this include protocol overhead, buffer management, link management, and operating system overhead.
Reference: [10] <author> Douglas E. Comer. </author> <title> Internetworking with TCP/IP Vol I: Principles Protocols, and Architecture, 2nd edition. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1991. </year>
Reference-contexts: FM is designed to support efficient implementation of a variety of communication libraries and run-time systems. To explore interface, buffering, and scheduling issues, we are building 16 implementations of MPI [14], TCP/IP <ref> [10] </ref>, and the Illinois Concert system's runtime [8]. MPI is of growing popularity among application builders, presents interesting collective communication operations, and there are efficient implementations to compare against [15]. TCP/IP is a legacy protocol in widespread use.
Reference: [11] <author> Cray Research, Inc. </author> <title> Cray T3D System Architecture Overview, </title> <month> March </month> <year> 1993. </year>
Reference-contexts: For shorter packets, latency drops to 25 s, and for larger packets, bandwidth rises to 19.6 MB/s. This delivered bandwidth is greater than OC-3 ATM's physical link bandwidth of 19.4 MB/s. FM's performance exceeds the messaging performance of commercial messaging layers on numerous massively-parallel machines <ref> [21, 29, 11] </ref>. A good characterization of a messaging layer's usable bandwidth (bandwidth for short messages) is n 1 2 , the packet size to achieve half of the peak bandwidth ( r 1 2 ). FM achieves an n 1 2 of 54 bytes.
Reference: [12] <author> Peter Druschel and Larry L. Peterson. Fbufs: </author> <title> A high-bandwidth cross-domain transfer facility. </title> <booktitle> In Proceedings of Fourteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 189-202. </pages> <publisher> ACM SIGOPS, ACM Press, </publisher> <month> December </month> <year> 1993. </year>
Reference-contexts: The goal of the Illinois Fast Messages (FM) project is to deliver a large fraction of the network's physical performance (latency and bandwidth) to the user at small packet sizes. 1 Building efficient software messaging layers is not a unique goal <ref> [12, 25, 30, 31] </ref>, but FM is distinguished by its hardware context (Myrinet) and high performance. The Fast Messages project focuses on optimizing the software messaging layer that resides between lower-level communication services and the hardware. It is available on both the Cray T3D [22, 23] and Myricom's Myrinet [6].
Reference: [13] <institution> Fiber-distributed data interface (FDDI)|Token ring media access control (MAC). American National Standard for Information Systems ANSI X3.139-1987, </institution> <month> July </month> <year> 1987. </year> <institution> American National Standards Institute. </institution>
Reference-contexts: Springfield Ave., Urbana, IL 61801, USA z Dipartimento di Informatica e Sistemistica, Universita di Napoli "Federico II", via Claudio 21, 80125 Napoli, Italy 1 interconnects such as ATM [7], Fibre Channel [4], FDDI <ref> [13] </ref>, and Myrinet [6] present the possibility that workstation clusters can deliver good performance on a broader range of parallel computations. Achieving efficient communication is the major challenge in synthesizing effective parallel machines from networks of workstations. <p> The communication primitives in these libraries have typically exploited operating system communication services, running atop 10 Mb/s Ethernet, or more recently some higher speed physical media such as FDDI <ref> [13] </ref>, ATM [7] or Fibre Channel [4]. While such facilities are useful for coarse-grain decoupled parallelism, they suffer from high software communication overhead (operating system calls) and low achieved bandwidth (media limits or software overhead), and thus cannot support more tightly coupled or finer-grained parallelism.
Reference: [14] <author> Message Passing Interface Forum. </author> <title> The MPI message passing interface standard. </title> <type> Technical report, </type> <institution> University of Tennessee, Knoxville, </institution> <month> April </month> <year> 1994. </year> <note> Can be found at http://www.mcs.anl.gov/mpi/mpi-report.ps. </note>
Reference-contexts: In addition, delivering effective low-latency communication requires coordinated scheduling, so we are exploring integrating messaging with the node scheduler. FM is designed to support efficient implementation of a variety of communication libraries and run-time systems. To explore interface, buffering, and scheduling issues, we are building 16 implementations of MPI <ref> [14] </ref>, TCP/IP [10], and the Illinois Concert system's runtime [8]. MPI is of growing popularity among application builders, presents interesting collective communication operations, and there are efficient implementations to compare against [15]. TCP/IP is a legacy protocol in widespread use.
Reference: [15] <author> H. Franke, C. E. Wu, M Riviere, P Pattnik, and M Snir. </author> <title> MPI programming environment for IBM SP1/SP2. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1995. </year>
Reference-contexts: To explore interface, buffering, and scheduling issues, we are building 16 implementations of MPI [14], TCP/IP [10], and the Illinois Concert system's runtime [8]. MPI is of growing popularity among application builders, presents interesting collective communication operations, and there are efficient implementations to compare against <ref> [15] </ref>. TCP/IP is a legacy protocol in widespread use. And the Illinois Concert system is a fine-grained programming system which depends critically on low-cost high performance communication.
Reference: [16] <author> F. Hady, R. Minnich, and D. Burns. </author> <title> The Memory Integrated Network Interface. </title> <booktitle> In Proceedings of the IEEE Symposium on Hot Interconnects, </booktitle> <year> 1994. </year> <month> 17 </month>
Reference-contexts: As a result, parallel computing on workstation clusters has largely been limited to coarse-grained applications. Attempts to improve performance based on specialized hardware can achieve dramatically higher performance, but generally require specialized components and interfacing deep into a computer system design <ref> [16, 18, 19] </ref>. This increases cost, and decreases the potential market (and hence sale volume) of the network hardware. <p> This is a key difference which affects the buffering protocols feasible in the two systems. A number of other researchers have explored the development of special hardware to achieve low latency/high bandwidth communication (MINI <ref> [16] </ref>, FUNet [18], VUNet [19], etc.). However, these hardware approaches have the drawback that they depend on specific memory bus interfaces, and require significant hardware investment. FM demonstrates that decent performance can be achieved without moving the interfaces closer to the host processor.
Reference: [17] <author> Mark Henderson, Bill Nickless, and Rick Stevens. </author> <title> A scalable high-performance I/O system. </title> <booktitle> In Pro--ceedings of the Scalable High-Performance Computing Conference, </booktitle> <pages> pages 79-86, </pages> <year> 1994. </year>
Reference: [18] <author> James Hoe and A. Boughton. </author> <title> Network substrate for parallel processing on a workstation cluster. </title> <booktitle> In Proceedings of the IEEE Symposium on Hot Interconnects, </booktitle> <year> 1994. </year>
Reference-contexts: As a result, parallel computing on workstation clusters has largely been limited to coarse-grained applications. Attempts to improve performance based on specialized hardware can achieve dramatically higher performance, but generally require specialized components and interfacing deep into a computer system design <ref> [16, 18, 19] </ref>. This increases cost, and decreases the potential market (and hence sale volume) of the network hardware. <p> This is a key difference which affects the buffering protocols feasible in the two systems. A number of other researchers have explored the development of special hardware to achieve low latency/high bandwidth communication (MINI [16], FUNet <ref> [18] </ref>, VUNet [19], etc.). However, these hardware approaches have the drawback that they depend on specific memory bus interfaces, and require significant hardware investment. FM demonstrates that decent performance can be achieved without moving the interfaces closer to the host processor.
Reference: [19] <author> H. Houh, J. Adam, M. Ismert, C. Lindblad, and D. Tennenhouse. </author> <title> The VuNet desk area network: Architecture, implementation and experience. </title> <journal> IEEE Journal of Selected Areas in Communications, </journal> <year> 1995. </year>
Reference-contexts: As a result, parallel computing on workstation clusters has largely been limited to coarse-grained applications. Attempts to improve performance based on specialized hardware can achieve dramatically higher performance, but generally require specialized components and interfacing deep into a computer system design <ref> [16, 18, 19] </ref>. This increases cost, and decreases the potential market (and hence sale volume) of the network hardware. <p> This is a key difference which affects the buffering protocols feasible in the two systems. A number of other researchers have explored the development of special hardware to achieve low latency/high bandwidth communication (MINI [16], FUNet [18], VUNet <ref> [19] </ref>, etc.). However, these hardware approaches have the drawback that they depend on specific memory bus interfaces, and require significant hardware investment. FM demonstrates that decent performance can be achieved without moving the interfaces closer to the host processor.
Reference: [20] <institution> IBM 9076 Scalable POWERparallel 1: General information. IBM brochure GH26-7219-00, </institution> <month> February </month> <year> 1993. </year> <note> Available from http://ibm.tc.cornell.edu/ibm/pps/sp2/index.html </note> . 
Reference-contexts: While most services can be programmed on either the host or the network interface, balanced decompositions allow overlapping of the two levels and supporting a higher message rate. Of course, configurations of host and network coprocessor in which the host processor is much faster and has significantly more memory <ref> [6, 20, 26] </ref> (e.g. our configuration) favor assigning more work to the host. 4 Fast Messages 1.0 Implementation Design The FM 1.0 implementation consists of two basic parts: the host program and the LANai control program (LCP). <p> For example, while FM's latencies are larger than Active Messages on the CM-5, the bandwidth is much higher. FM also compares favorably to recent MPPs <ref> [20, 21] </ref> in both bandwidth and latency. While there may appear to be many design tradeoffs involving performance for short or long messages (latency versus bandwidth), the design of FM is a counterexample.
Reference: [21] <author> Intel Corporation. </author> <title> Paragon XP/S Product Overview, </title> <year> 1991. </year>
Reference-contexts: For shorter packets, latency drops to 25 s, and for larger packets, bandwidth rises to 19.6 MB/s. This delivered bandwidth is greater than OC-3 ATM's physical link bandwidth of 19.4 MB/s. FM's performance exceeds the messaging performance of commercial messaging layers on numerous massively-parallel machines <ref> [21, 29, 11] </ref>. A good characterization of a messaging layer's usable bandwidth (bandwidth for short messages) is n 1 2 , the packet size to achieve half of the peak bandwidth ( r 1 2 ). FM achieves an n 1 2 of 54 bytes. <p> For example, while FM's latencies are larger than Active Messages on the CM-5, the bandwidth is much higher. FM also compares favorably to recent MPPs <ref> [20, 21] </ref> in both bandwidth and latency. While there may appear to be many design tradeoffs involving performance for short or long messages (latency versus bandwidth), the design of FM is a counterexample.
Reference: [22] <author> Vijay Karamcheti and Andrew A. Chien. </author> <title> A comparison of architectural support for messaging on the TMC CM-5 and the Cray T3D. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1995. </year> <note> Available from http://www-csag.cs.uiuc.edu/papers/cm5-t3d-messaging.ps </note> . 
Reference-contexts: The Fast Messages project focuses on optimizing the software messaging layer that resides between lower-level communication services and the hardware. It is available on both the Cray T3D <ref> [22, 23] </ref> and Myricom's Myrinet [6]. Using the Myrinet, FM provides MPP-like communication performance on workstation clusters. FM on the Myrinet achieves low-latency, high-bandwidth messaging for short messages delivering 32 s latency and 16 MBytes/s bandwidth for 128 byte packets (user-level to user-level). <p> bandwidths are greater than the processor-mediated SBus bandwidth, and hence are not a critical performance factors. 4 3 The Fast Messages Approach 3.1 Illinois Fast Messages (FM) 1.0 Illinois Fast Messages (FM) is a high performance messaging layer which is available on several parallel platforms (Cray T3D and workstation clusters) <ref> [22, 23] </ref>. The design goal of FM is to deliver network hardware performance to the application level with a simple interface. FM is appropriate for implementors of compilers, language runtimes, communications libraries, and in some cases application programmers.
Reference: [23] <author> Vijay Karamcheti and Andrew A. Chien. </author> <title> FM|fast messaging on the Cray T3D. </title> <note> Available from http://www-csag.cs.uiuc.edu/papers/t3d-fm-manual.ps, February 1995. </note>
Reference-contexts: The Fast Messages project focuses on optimizing the software messaging layer that resides between lower-level communication services and the hardware. It is available on both the Cray T3D <ref> [22, 23] </ref> and Myricom's Myrinet [6]. Using the Myrinet, FM provides MPP-like communication performance on workstation clusters. FM on the Myrinet achieves low-latency, high-bandwidth messaging for short messages delivering 32 s latency and 16 MBytes/s bandwidth for 128 byte packets (user-level to user-level). <p> bandwidths are greater than the processor-mediated SBus bandwidth, and hence are not a critical performance factors. 4 3 The Fast Messages Approach 3.1 Illinois Fast Messages (FM) 1.0 Illinois Fast Messages (FM) is a high performance messaging layer which is available on several parallel platforms (Cray T3D and workstation clusters) <ref> [22, 23] </ref>. The design goal of FM is to deliver network hardware performance to the application level with a simple interface. FM is appropriate for implementors of compilers, language runtimes, communications libraries, and in some cases application programmers.
Reference: [24] <author> M. Liu, J. Hsieh, D. Hu, J. Thomas, and J. MacDonald. </author> <title> Distributed network computing over Local ATM Networks. </title> <booktitle> In Supercomputing '94, </booktitle> <year> 1995. </year>
Reference-contexts: Some ATM systems provide memory mapped input/output bus interfaces, but achieving performance is still a challenging proposition. For example, delivered bandwidths of 1-3 MB/s are typical <ref> [24] </ref>. Achieving high performance requires careful management of the hardware resources by the software messaging layer. How deeply network interfaces will be integrated into a typical system is a debate currently raging in the workstation cluster community.
Reference: [25] <author> R. Martin. HPAM: </author> <title> An Active Message layer for a network of HP workstation. </title> <booktitle> In Proceedings of the IEEE Symposium on Hot Interconnects, </booktitle> <year> 1994. </year> <note> Available from ftp://ftp.cs.berkeley.edu/ucb/CASTLE/Active Messages/hotipaper.ps. </note>
Reference-contexts: The goal of the Illinois Fast Messages (FM) project is to deliver a large fraction of the network's physical performance (latency and bandwidth) to the user at small packet sizes. 1 Building efficient software messaging layers is not a unique goal <ref> [12, 25, 30, 31] </ref>, but FM is distinguished by its hardware context (Myrinet) and high performance. The Fast Messages project focuses on optimizing the software messaging layer that resides between lower-level communication services and the hardware. It is available on both the Cray T3D [22, 23] and Myricom's Myrinet [6]. <p> These characteristics have every indication of continuing in workstation cluster systems of the future. Other researchers have built messaging layers for workstation clusters, using other commercial hardware <ref> [30, 25] </ref>. FM on Myrinet has performance beyond von Eicken et. al.'s SPARCstation Active Messages (SSAM), which employs ATM interface cards on the SBus. SSAM achieves 26 s latency on 4-word messages, assuming a 10 s switch latency.
Reference: [26] <author> Meiko World Incorporated. </author> <title> Meiko Computing Surface Communications Processor Overview, </title> <year> 1993. </year>
Reference-contexts: While most services can be programmed on either the host or the network interface, balanced decompositions allow overlapping of the two levels and supporting a higher message rate. Of course, configurations of host and network coprocessor in which the host processor is much faster and has significantly more memory <ref> [6, 20, 26] </ref> (e.g. our configuration) favor assigning more work to the host. 4 Fast Messages 1.0 Implementation Design The FM 1.0 implementation consists of two basic parts: the host program and the LANai control program (LCP).
Reference: [27] <author> V. S. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concurrency, Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-340, </pages> <month> [12] </month> <year> 1990. </year>
Reference-contexts: Achieving efficient communication is the major challenge in synthesizing effective parallel machines from networks of workstations. Unfortunately, to date the most common messaging layers used for clusters (TCP/IP [9], PVM <ref> [27] </ref>) generally have not delivered a large fraction of the underlying communication hardware performance to the applications. Reasons for this include protocol overhead, buffer management, link management, and operating system overhead. <p> We discuss our findings in Section 5 and provide a brief summary of the paper and conclusions in Section 6. 2 Background For some time, researchers and even production sites have been using workstation clusters for parallel computation. Many libraries are available to support such distributed parallel computing (PVM <ref> [27] </ref> atop UDP or TCP [28] is perhaps the most popular). The communication primitives in these libraries have typically exploited operating system communication services, running atop 10 Mb/s Ethernet, or more recently some higher speed physical media such as FDDI [13], ATM [7] or Fibre Channel [4].
Reference: [28] <author> A. S. Tanenbaum. </author> <title> Computer networks. </title> <booktitle> Prentice-Hall 2nd ed. </booktitle> <year> 1989, 1981. </year>
Reference-contexts: Many libraries are available to support such distributed parallel computing (PVM [27] atop UDP or TCP <ref> [28] </ref> is perhaps the most popular). The communication primitives in these libraries have typically exploited operating system communication services, running atop 10 Mb/s Ethernet, or more recently some higher speed physical media such as FDDI [13], ATM [7] or Fibre Channel [4]. <p> Because all networks have finite buffering, flow control is necessary to achieve reliable delivery, ensuring a receiver has enough buffer space to store incoming messages. Traditional flow control schemes include windows <ref> [28] </ref> which combine flow control and retransmission for fault tolerance. However window protocols generally require buffer space proportional to the number of senders, incurring large memory overheads in large clusters.
Reference: [29] <institution> Thinking Machines Corporation, </institution> <address> 245 First Street, Cambridge, MA 02154-1264. </address> <booktitle> The Connection Machine CM-5 Technical Summary, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: For shorter packets, latency drops to 25 s, and for larger packets, bandwidth rises to 19.6 MB/s. This delivered bandwidth is greater than OC-3 ATM's physical link bandwidth of 19.4 MB/s. FM's performance exceeds the messaging performance of commercial messaging layers on numerous massively-parallel machines <ref> [21, 29, 11] </ref>. A good characterization of a messaging layer's usable bandwidth (bandwidth for short messages) is n 1 2 , the packet size to achieve half of the peak bandwidth ( r 1 2 ). FM achieves an n 1 2 of 54 bytes.
Reference: [30] <author> T. von Eicken, A. Basu, and V. </author> <title> Buch. Low-latency communication over ATM networks using Active Messages. </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 46-53, </pages> <year> 1995. </year>
Reference-contexts: The goal of the Illinois Fast Messages (FM) project is to deliver a large fraction of the network's physical performance (latency and bandwidth) to the user at small packet sizes. 1 Building efficient software messaging layers is not a unique goal <ref> [12, 25, 30, 31] </ref>, but FM is distinguished by its hardware context (Myrinet) and high performance. The Fast Messages project focuses on optimizing the software messaging layer that resides between lower-level communication services and the hardware. It is available on both the Cray T3D [22, 23] and Myricom's Myrinet [6]. <p> These characteristics have every indication of continuing in workstation cluster systems of the future. Other researchers have built messaging layers for workstation clusters, using other commercial hardware <ref> [30, 25] </ref>. FM on Myrinet has performance beyond von Eicken et. al.'s SPARCstation Active Messages (SSAM), which employs ATM interface cards on the SBus. SSAM achieves 26 s latency on 4-word messages, assuming a 10 s switch latency.
Reference: [31] <author> T. von Eicken, D. Culler, S. Goldstein, and K. Schauser. </author> <title> Active Messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1992. </year> <note> Available from http://www.cs.cornell.edu/Info/People/tve/ucb papers/isca92.ps </note> . 
Reference-contexts: The goal of the Illinois Fast Messages (FM) project is to deliver a large fraction of the network's physical performance (latency and bandwidth) to the user at small packet sizes. 1 Building efficient software messaging layers is not a unique goal <ref> [12, 25, 30, 31] </ref>, but FM is distinguished by its hardware context (Myrinet) and high performance. The Fast Messages project focuses on optimizing the software messaging layer that resides between lower-level communication services and the hardware. It is available on both the Cray T3D [22, 23] and Myricom's Myrinet [6]. <p> Each message carries a pointer to a sender-specified function (called a "handler") that consumes the data at the destination. The handler-carrying message concept is similar to Active Messages <ref> [31] </ref>, but in FM there is no notion of request-reply coupling. There are no restrictions on the actions that can be performed by an handler, and it is left to the programmer of preventing deadlock situations.
References-found: 31

