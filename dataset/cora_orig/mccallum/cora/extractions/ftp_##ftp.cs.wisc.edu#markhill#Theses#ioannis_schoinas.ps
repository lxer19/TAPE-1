URL: ftp://ftp.cs.wisc.edu/markhill/Theses/ioannis_schoinas.ps
Refering-URL: http://www.cs.wisc.edu/~markhill/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Fine-Grain Distributed Shared Memory on Clusters of Workstations  
Author: by Ioannis T. Schoinas 
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy (Computer Sciences) at the UNIVERSITY OF WISCONSINMADISON  
Date: 1998  
Abstract-found: 0
Intro-found: 1
Reference: [AB86] <author> J. Archibald and J.-L. Baer. </author> <title> Cache coherence protocols: Evaluation using a multiprocessor simulation model. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(4):273298, </volume> <year> 1986. </year>
Reference-contexts: Blizzard implements the Tempest interface [Rei94], which allows the implementation of shared memory coherence protocols as user-level libraries. The Tempest interface has been specifically designed to support fine-grain shared memory coherence protocols. Typically, such protocols are modeled after hardware-based invalidate directory protocols <ref> [AB86] </ref>. They transfer data in small, cache-block sized quantities. Moreover, they are blocking protocols, in which the computation is suspended on access violations until the protocol events (e.g. misses) have been processed.
Reference: [ACC + 90] <author> Robert Alverson, David Callahan, Daniel Cummings, Brian Koblenz, Allan Porter-field, and Burton Smith. </author> <title> The Tera Computer System. </title> <booktitle> In Proceedings of the 1990 International Conference on Supercomputing, </booktitle> <pages> pages 16, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Consequently, it can serve as the benchmark against which we can judge more sophisticated policies. 3.3.2 Return-to-Sender This policy briefly appeared in the first release of the Illinois Fast Messages (FM) for Myri-net [PLC95]. It was inspired from the deflection and chaos routing policies of the Tera-1 machine <ref> [ACC + 90] </ref>. This policy uses a shared buffer pool for all senders. It is based in the idea that messages which cannot be delivered to the receiver will be routed back to the sender. There, they will be placed in an overflow queue and retransmitted again.
Reference: [ACP94] <author> Tom Anderson, David Culler, and David Patterson. </author> <title> A case for networks of workstations: NOW. </title> <type> Technical report, </type> <institution> Computer Science Division (EECS), University of Cali-fornia at Berkeley, </institution> <month> July </month> <year> 1994. </year>
Reference: [ACP95] <author> Thomas E. Anderson, David E. Culler, and David A. Patterson. </author> <title> A Case for Networks of Workstations: NOW. </title> <booktitle> IEEE Micro, </booktitle> <month> February </month> <year> 1995. </year>
Reference-contexts: Recently however, the network technology has caught up with the other components of a workstation, prompting researchers to advocate using networks of workstations (NOWs) as parallel minicomputers <ref> [ACP95] </ref>. NOWs can take advantage of commodity components to a greater extent than MPPs. Not only commodity microprocessors can be used, but all the system components including backplane busses, system logic, and peripherals can now be off the self equipment.
Reference: [ADC97] <author> Andrea C. Arpaci-Dusseau and David E. Culler. </author> <title> Extending proportional-share scheduling to a network of workstations. </title> <booktitle> In International Conference on Parallel and Distributed Processing Techniques and Applications (PDPTA97), </booktitle> <month> June </month> <year> 1997. </year>
Reference: [AS83] <author> Gregory R. Andrews and Fred B. Schneider. </author> <title> Concepts and notations for concurrent programming. </title> <journal> ACM Computing Surveys, </journal> <volume> 15(1):344, </volume> <month> March </month> <year> 1983. </year>
Reference-contexts: An alternative approach relies on local memory-based locks to synchronize protocol handlers. This allows concurrent execution of protocol handlers, which increases the available processing bandwidth. However, locks should only be allowed to support mutual exclusion. Condition synchronization, in which the synchronization primitives are used to arbitrate the execution order <ref> [AS83] </ref>, should not be allowed. In practical terms the meaning of this restriction is that the handlers should release all locks held before they complete. This model is sufficient for shared memory coherence protocols that only require synchronization mechanisms to safely access internal protocol data structures. <p> This is very difficult to do transparently to the protocol code while requiring the handlers to rollback if an attempt to lock fails introduces complexity. Finally, the third one is to provide a restricted synchronization model that allows only mutual exclusion and not conditional synchronization <ref> [AS83] </ref>. This can be achieved by constraining the protocol handlers to release all the locks they acquired before they are allowed to finish. This last scheme looks quite promising for shared memory protocols since they have modest synchronization requirements. The fault queues are conceptually similar to the input message queue.
Reference: [BBD + 87] <author> James Boyle, Ralph Butler, Terrence Disz, Barnett Glickfieldand Ewing Lusk, Ross Overbeek, James Patterson, and Rick Stevens. </author> <title> Portable Programs for Parallel Processors. </title> <publisher> Holt, Rinehart and Winston Inc., </publisher> <year> 1987. </year>
Reference-contexts: A set of fine-grain tags enforce access semantics for shared remote memory blocks. Upon access violation, the address of the faulting memory block and the type of access are inserted 1. Blizzard supports the PARMACS programming model <ref> [BBD + 87] </ref>. PARMACS offers to each process of a parallel application a private address space with fork-like semantics. Shared memory support is limited to the special shared heap. 8 in the fault queue. <p> Within this segment, the user handles accesses to unmapped pages, and controls the accessibility of mapped memory at a fine granularity. Blizzard supports the PARMACS programming model <ref> [BBD + 87] </ref>. PARMACS offers to each process of a parallel application a private address space with fork-like semantics while shared memory support is limited to the special shared memory segment. Blizzard preallo-cates an address range within the application address space for the shared memory segment.
Reference: [BBLS91] <author> David Bailey, John Barton, Thomas Lasinski, and Horst Simon. </author> <title> The NAS Parallel Benchmarks. </title> <type> Technical Report RNR-91-002 Revision 2, </type> <institution> Ames Research Center, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: For each one, I mention the scientific problem that it solves. Then, I examine how it is parallelized. In particular, I focus in its data structures, the data partitioning among the processors and its sharing patterns. Appbt is a NAS Parallel Benchmark <ref> [BBLS91] </ref> produced by NASA as an exemplar of computation and communication pattern in three-dimensional computational fluid dynamics applications. At each time step, it performs three computation phases in each of three dimensions. In the first phase, it calculates a block tridiagonal matrix A.
Reference: [BBO + 83] <author> B. R. Brooks, R. E. Bruccoleri, B. D. Olafson, D . J. States, S. Swamintathan, and M. Karplus. Charmm: </author> <title> A program for macromolecular energy, minimization, and dynamics calculation. </title> <journal> Journal of Computational Chemistry, </journal> <volume> 4(187), </volume> <year> 1983. </year>
Reference-contexts: Therefore, it results in better overall performance, especially on FGDSM systems such as Blizzard [ZIS + 97]. Moldyn is a molecular dynamics application <ref> [BBO + 83] </ref>. Molecules are uniformly distributed in a cubical region and the system computes a molecules velocity and the force exerted by other molecules. An interaction list (rebuilt every 20 iterations) limits interaction with molecules within a cutoff radius. We examine both DSM and CS versions. <p> 3D electromagnetic wave propa gation [CDG + 93] 128K nodes, degree 6, 40% remote edges, distance span 1, 100 iterations 166 lu Contiguous blocked dense LU factorization [WOT + 95] 512x512 matrix, 16x16 blocks 75 ocean Simulation of eddy currents [WOT + 95] 514x514 ocean size 62 moldyn Molecular dynamics <ref> [BBO + 83] </ref> 8722 molecules, 40 iterations. 128 tomcatv Thompsons solver mesh genera tion [SPE90] 512x512 matrices, 100 iters 147 waterns Water molecule force simulation [WOT + 95] 4096 molecules 273 watersp Spatial water molecule force sim ulation [WOT + 95] 2744 molecules 65 95 col and the messaging subsystem that
Reference: [BCF + 95] <author> Nanette J. Boden, Danny Cohen, Robert E. Felderman, Alan E. Kulawik, Charles L. Seitz, Jakov N. Seizovic, and Wen-King Su. Myrinet: </author> <title> A gigabit-per-second local area network. </title> <journal> IEEE Micro, </journal> <volume> 15(1):2936, </volume> <month> February </month> <year> 1995. </year> <month> 193 </month>
Reference-contexts: Commercial designs such as Myricom Myrinet <ref> [BCF + 95] </ref>, DEC Memory Channel [GCP96], Tandem TNet [Hor95] have become widely available. Collectively, these designs have been called system-area networks [Bel96] to distinguish them from traditional local area networks that have higher messaging overheads. <p> Under Solaris 2.4, the I/O MMU directly maps kernel virtual addresses to Sbus virtual addresses, which limits DMA operations to the kernel address space. The operating system of the COW nodes is Solaris 2.4. Each COW node contains a custom-built Vortex card [Pfi95] and a Myrinet network interface <ref> [BCF + 95] </ref>. The Vortex card plugs into the MBus and performs fine-grain access control by snooping bus transactions. Each node also contains a Myrinet interface, which consists of a slow (78 MIPS) custom processor (LANai -2) and 128 KBytes of memory. <p> First, Tempest messages are not constrained to follow a request-reply protocol. Second, Tempest messages are delivered without explicit polling by application programs. The Wisconsin Cluster of Workstations (COW) includes low-latency Myricoms Myrinet hardware <ref> [BCF + 95] </ref>. The Blizzard messaging subsystem originated out of Berkeleys Active Messages (AM) messaging library 1 [CLMY96]. <p> This can slowdown message processing. For these reasons, it has not been implemented in Blizzard. 3.4 COW Communication Infrastructure COW nodes (Figure 3-5) are equipped with a Myrinet NI that connects them through Myri-net switches <ref> [BCF + 95] </ref>. The NI is located on the I/O bus (SBUS [Mic91]), which supports processor coherent I/O transfers between the memory and I/O devices. The bus bridge supports an I/O memory management unit (IOMMU) which provides translation from SBUS addresses to physical addresses. <p> Moving the actual message data, which consists the bulk of the time in sending or receiving messages, can occur concurrently. Such is the case with network interfaces that support remote memory accesses or even simply implement software message queues in onboard user-accessible 111 memory <ref> [BCF + 95] </ref>. Message data can be accessed through memory pointers with normal memory accesses even after the queue head pointer does not point to the message. Executing handlers requires a synchronization model for accesses by concurrent handlers to FGDSM resources. Two alternatives are atomic handlers or protocol locks. <p> Examples of such designs include the Arizona Application Device Channels (ADCs) [DPD94], Cornell U-Net [vEBBV95], HP Hamlyn [Wil92], Princeton SHRIMP [BDFL96]. The result of such recent research has been commercial designs like Myricom Myrinet <ref> [BCF + 95] </ref>, Fore 200 [CMSB91], Mitsubishi DART [OZH + 96]. With the OS removed from the critical path, the memory subsystem emerges as a major hurdle for delivering the network performance to the application. <p> The cache-coherent 50 Mhz MBus connects the processors and memory. I/O devices sit on the 25 Mhz SBus, which is connected through a bridge to the MBus. Each COW node contains a Vortex card [Pfi95] and a Myrinet network interface <ref> [BCF + 95] </ref>. The Vortex card plugs into the MBus and performs fine-grain access control by snooping bus transactions. If the Vortex card is not present, two additional HyperSPARC processors can take its place, raising the total number of processors per node to four.
Reference: [BCL + 95] <author> Eric A. Brewer, Frederic T. Chong, Lok T. Liu, Shamik D. Sharma, and John Kubi-atowicz. </author> <title> Remote queues: Exposing message queues or optimization and atomicity. </title> <booktitle> In Proceedings of the Sixth ACM Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <pages> pages 4253, </pages> <year> 1995. </year>
Reference: [BCM94] <author> Eric Barton, James Cownie, and Moray McLaren. </author> <title> Message passing on the Meiko CS-2. </title> <booktitle> Parallel Computing, </booktitle> <address> 20:497507, </address> <year> 1994. </year>
Reference-contexts: While the channel interface is still supported in Blizzard, it is being implemented using active messages. Nevertheless, it remains part of the Tempest specification, since it is capable of abstracting hardware capabilities that accelerate bulk data transfers such as DMA engines <ref> [BCM94] </ref>. 3.2 Buffer Allocation And Active Messages Dealing with buffer allocation is a fundamental problem in any messaging subsystem. An incoming message requires temporary buffers where it is held until it can be processed. <p> Studies have shown that even today, network protocols spend a significant amount of time simply copying data [Ste94]. Therefore, many designs have attempted to avoid redundant copying at the application interface [DP93, RAC96, Wil92], the OS [kJC96], and the network interface <ref> [OZH + 96, DWB + 93, LC95, BCM94] </ref>. <p> Furthermore, there are not any provisions for a fallback action. Thus, the design requires fast kernel interfaces to gracefully degrade once the translation limits are exceeded. Designs with a network coprocessor, like Meiko CS <ref> [BCM94] </ref> and Intel Paragon [Int93] can support minimal messaging using the microprocessors address translation hardware and a separate DMA engine. Nonetheless, address translation mechanisms implemented for CPUs (TLBs) are not always appropriate for NIs. There are two potential problems.
Reference: [BCZ90] <author> John K. Bennett, John B. Carter, and Willy Zwaenepoel. Munin: </author> <title> Distributed shared memory based on typespecific memory coherence. </title> <booktitle> In Second ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 168176, </pages> <month> February </month> <year> 1990. </year>
Reference: [BDFL96] <author> Matthias A. Blumrich, Cesary Dubnicki, Edward W. Felten, and Kai Li. </author> <title> Protected User-level DMA for the SHRIMP Network Interface. </title> <booktitle> In Proceedings of the Second IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: Thereafter, the application can initiate message operations communicating directly with the device using loads and stores to send and receive messages. Examples of such designs include the Arizona Application Device Channels (ADCs) [DPD94], Cornell U-Net [vEBBV95], HP Hamlyn [Wil92], Princeton SHRIMP <ref> [BDFL96] </ref>. The result of such recent research has been commercial designs like Myricom Myrinet [BCF + 95], Fore 200 [CMSB91], Mitsubishi DART [OZH + 96]. With the OS removed from the critical path, the memory subsystem emerges as a major hurdle for delivering the network performance to the application. <p> Memory accesses on the sender are captured by the NI and forwarded to the associated page on the receiver. Page associations are either direct (the sender knows the remote physical address) or 163 indirect (through global network addresses). Examples of this approach include Princeton SHRIMP <ref> [BDFL96] </ref>, Forth Telegraphos II [MK96], DEC Memory Channel [GCP96] and Tandem TNet [Hor95]. SHRIMP and Telegraphos II use direct page associations. The Memory Channel and TNet use indirect page associations. Common characteristic of these designs is their inability to handle misses in the translation structures. <p> In these systems, minimum messaging is achieved if the NI can directly access the main memory. However, the kernel is involved in every transfer and thus, user-level messaging is not supported. Princeton User-level DMA (UDMA) <ref> [BDFL96] </ref> avoids OS intervention and supports minimal messaging when it is used both to send and receive messages. UDMA is used in SHRIMP to initiate DMA transfers.
Reference: [Bel96] <author> Gordon Bell. </author> <title> 1995 Observations on Supercomputing Alternatives: Did the MPP Bandwagon Lead to a Cul-de-Sac? Communications of the ACM, </title> <address> 39(3):1115, </address> <month> March </month> <year> 1996. </year>
Reference-contexts: Commercial designs such as Myricom Myrinet [BCF + 95], DEC Memory Channel [GCP96], Tandem TNet [Hor95] have become widely available. Collectively, these designs have been called system-area networks <ref> [Bel96] </ref> to distinguish them from traditional local area networks that have higher messaging overheads. System-area networks also enable the construction of clustered shared memory servers targeted for high availability and scalability. <p> On one hand, data intensive applications like multimedia depend on high throughput to stream large amounts of data through the network. On the other hand, clientserver and parallel computing applications depend on low latency for fast response times. Network performance will become even more important as system area networks <ref> [Bel96] </ref> are used for clustered servers. While the network hardware has been able to achieve high throughput and low latency, it has not proven easy to deliver this performance to the application. <p> On the other hand, if it is very unlikely for most applications to encounter them, only the applications with high reliability requirements should need to deal with them. This is likely to be the case when system areas networks <ref> [Bel96] </ref> are used to connect the cluster nodes. In this case, we can define an interface through which transient message faults are reported to the application. In other words, the responsibility of the system is to provide fault detection to the applications that wish to deal with these faults.
Reference: [BGvN46] <author> A. W. Burks, H. H. Goldstine, and J. von Neumann. </author> <title> Preliminary discussion of the logical design of an electronic computing instrument, 1946. Report to the U.S. </title> <institution> Army Ordinance Department. </institution>
Reference-contexts: Unlike sequential computing where the von Neumann model <ref> [BGvN46] </ref> has been dominant right from the start, no consensus has ever been reached on how to organize and control many activities for a single purpose. Numerous parallel programming models have been proposed to 2 facilitate this goal.
Reference: [BHMW94] <author> Douglas C. Burger, Rahmat S. Hyder, Barton P. Miller, and David A. Wood. </author> <title> Paging tradeoffs in distributed-shared-memory multiprocessors. </title> <booktitle> In Proceedings of Supercomputing 94, </booktitle> <pages> pages 590599, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: In effect, for the duration of a quantum, the user has complete possession of the cluster resources. The importance of coscheduling has been noted very early in the history of such systems [OSS80]. The same result has been demonstrated in the context of shared memory parallel programs <ref> [BHMW94] </ref> executing scientific codes. Gang scheduling, however, is a very restrictive model. Moreover, it is very difficult to strictly implement it in a distributed environment without extensive modifications in the kernel philosophy. In addition, in such environments the cost of a parallel context switch can be prohibitive without hardware support.
Reference: [BJM + 96] <author> Greg Buzzard, David Jacobson, Milon Mackey, Scott Marovich, and John Wilkes. </author> <title> An Implementation of the Hamlyn Sender-Managed Interface Architecture. </title> <booktitle> In Second USENIX Symposium on Operating Systems Design and Implementation, </booktitle> <month> October </month> <year> 1996. </year> <institution> Seattle, WA. </institution>
Reference-contexts: In theory, you can define message segments that cover the entire application address space. However, in current Hamlyn [BJM + 96,Wil92] and Active Messages [Mar94,LC95,CLMY96] implementations the address translation structures are not there or are limited to their reach. For example, the latest prototype Hamlyn implementation <ref> [BJM + 96] </ref> is built on hardware identical to ours (Myrinet), yet message buffers must be pinned in main memory.
Reference: [BLA + 94] <author> Matthias A. Blumrich, Kai Li, Richard Alpert, Cezary Dubnicki, Edward W. Felten, and Jonathon Sandberg. </author> <title> Virtual memory mapped network interface for the SHRIMP multicomputer. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 142153, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: In order to support replication efficiently, we need network interfaces that can efficiently service separate output queues and demultiplex incoming messages to separate input queues. For example, network interfaces that support a remote memory access model can trivially support separate message queues <ref> [BLA + 94, Sco96] </ref> by building separate circular message queues in main memory. Incoming messages are directed to the appropriate memory location and therefore, the appropriate message queue on the remote node. Sharing introduces extra overheads to synchronize competing accesses to the message queues. <p> I will assume that this is done with two address translations: at the sender and at the receiver using users virtual addresses (Figure 6-1). Alternatively the sender could perform the receivers address translation and send messages with destination physical addresses <ref> [BLA + 94] </ref>.
Reference: [BM95] <author> Doug Burger and Sanjay Mehta. </author> <title> Parallelizing Appbt for a Shared-Memory Multiprocessor. </title> <type> Technical Report 1286, </type> <institution> Computer Sciences Department, University of Wiscon-sinMadison, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: The table lists the applications, the scientific problem they solve, the input data set and the time it takes (in secs) to run the sequential application (without FGDSM overhead) on a single COW node. Application Problem Input Data Set Sequential Time (secs) appbt 3D implementation of CFD <ref> [BM95] </ref> 40x40x40 matrices, 4 iters 110 barnes Barnes-Hut N-body simulation [WOT + 95] 16K particles, 4 iters 33 em3d 3D electromagnetic wave propa gation [CDG + 93] 128K nodes, degree 6, 40% remote edges, distance span 1, 100 iterations 166 lu Contiguous blocked dense LU factorization [WOT + 95] 512x512 matrix, <p> Table 5.3: Applications and input parameters. Application Problem Input Data Set em3d 3D electromagnetic wave propagation [CDG + 93] 128K nodes, degree 6, 40% remote edges, distance span 1 (processors share edges only with their immediate neighbors), 100 iterations appbt 3D implementation of CFD <ref> [BM95] </ref> 40x40x40 matrices, 4 iters barnes Barnes-Hut N-body simulation [WOT + 95] 16K particles, 4 iters tomcatv Thompsons solver mesh generation [SPE90] 512x512 matrices, 100 iters lu Contiguous blocked dense LU factoriza tion [WOT + 95] 512x512 matrix, 16x16 blocks water Spatial water molecule force simulation [WOT + 95] 4096 molecules
Reference: [BS96] <author> Jose Carlos Brustoloni and Peter Steenkiste. </author> <title> Effects of buffering semantics on I/O performance. </title> <booktitle> In Second USENIX Symposium on Operating Systems Design and Implementation, </booktitle> <month> October </month> <year> 1996. </year> <institution> Seattle, WA. </institution> <month> 194 </month>
Reference-contexts: In abstraction layers that offer copy semantics, in which outgoing or incoming messages contain copies of 140 the application data, it is difficult to fully support minimal messaging. In general, a change in semantics introduces extra overheads <ref> [BS96] </ref>. Therefore, it is important that the lowest abstraction layer in the NI architecture to offer appropriate semantics or else implementations of all abstraction layers will suffer from the mismatch in semantics. As a negative example of inflexible low-level interfaces, consider ADCs, which have been designed to optimize stream traffic. <p> Such approaches include page remapping in the kernel (implemented in Solaris 2.6 TCP [kJC96]), Washingtons inkernel emulation of the remote memory access model [TLL94] and other VM manipulations <ref> [BS96] </ref>. In these systems, minimum messaging is achieved if the NI can directly access the main memory. However, the kernel is involved in every transfer and thus, user-level messaging is not supported.
Reference: [BS98] <author> Manjunath Bangalore and Anand Sivasubramaniam. </author> <title> Remote subpaging across a fast network. </title> <booktitle> In Workshop on Communication, Architecture, and Applications for Network-based Parallel Computing (CANPC), </booktitle> <month> February </month> <year> 1998. </year>
Reference-contexts: In this way, they were able to enable fine-grain access control in their environment without me distributing a replacement for the Solaris kernel that was on their machines. Subsequently, these researchers used fine-grain access control to experiment with remote subpaging policies in a network environment <ref> [BS98] </ref>. An operating system can support fine-grain access control in two ways. The first approach treats access control as a direct extension of virtual memory semantics.
Reference: [BTK90] <author> Henri E. Bal, Andrew S. Tanenbaum, and M. Frans Kaashoek. Orca: </author> <title> A language for distributed programming. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 25(5):1724, </volume> <month> May </month> <year> 1990. </year>
Reference-contexts: Either a compiler or a program executable editing tool [LB94,LS95] can insert software tests. With the latter approach every compiler need not reimplement test analysis and code generation. Blizzard/S and Shasta [SGT96] also follow this approach (Section 2.3.1). Compiler-inserted lookups can exploit application-level information. Orca <ref> [BTK90] </ref>, for example, provides access control on program objects instead of blocks. Translation Lookahead Buffer (TLB). Standard address translation hardware provides access control, though at memory page granularity. Nevertheless, it forms the basis of several distributed-shared-memory systems such as IVY [LH89], Munin [CBZ91] and TreadMarks [KDCZ93].
Reference: [BWvE97] <author> Anindya Basu, Matt Welsh, and Thorsten von Eicken. </author> <title> Incorporating memory management into user-level network interfaces. In Hot Interconnects 97, </title> <year> 1997. </year>
Reference-contexts: In the original UNet paper [vEBBV95], a direct access UNet architecture is discussed that includes communication segments able to encompass all the user address space but the architecture is restricted to future NI designs. Recent work <ref> [BWvE97] </ref> attempted to incorporate address translation mechanisms for existing NIs. Nevertheless, the ADC abstraction has not changed and therefore, the designs are unable to move data to their final destination without extra copying.
Reference: [CBZ91] <author> John B. Carter, John K. Bennett, and Willy Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating System Principles (SOSP), </booktitle> <pages> pages 152164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Compiler-inserted lookups can exploit application-level information. Orca [BTK90], for example, provides access control on program objects instead of blocks. Translation Lookahead Buffer (TLB). Standard address translation hardware provides access control, though at memory page granularity. Nevertheless, it forms the basis of several distributed-shared-memory systems such as IVY [LH89], Munin <ref> [CBZ91] </ref> and TreadMarks [KDCZ93]. Blizzard/E partially relies on the TLB to emulate fine-grain access control (Section 2.3.2). Though unimplemented by current commodity processors, additional, 18 per-block access bits in a TLB entry could provide fine-grain access control. <p> Many variations thereof have since emerged such as Munin <ref> [CBZ91] </ref>, TreadMarks [KDCZ93], SoftFLASH [ENCH96], MGS [YKA96], and Cashmere [KHS + 97]. These systems all use standard virtual address translation mechanisms to implement coherence across nodes, and rely on relaxed memory consistency models and careful program annotation to support fine-grain sharing.
Reference: [CCBS95] <author> Frederic T. Chong, Shamik D. Charma, Eric A. Brewer, and Joel Saltz. </author> <title> Multiprocessor runtime support for fine-grained, irregular DAGs. </title> <journal> Parallel Processing Letters: Special Issue on Partitioning and Scheduling, </journal> <month> December </month> <year> 1995. </year>
Reference-contexts: Buffer Requirements Per Graph Concurrent Activation Graphs Few Many Small Invalidate DSM coherence protocols [LH89, RLW94, SGT96] Update DSM coherence proto cols, applicationspecific proto cols [FLR + Large Dataflow protocols <ref> [CCBS95] </ref> 63 deadlocks [WGH + 97]. However, such strategy is not appropriate in a general platform like Blizzard, which it is targeted to allow the development of application specific protocols. Finally, there are protocols that create an arbitrary number of concurrent activation graphs with large buffer requirements. <p> Depending on the parallelism available dataflow graphs with different characteristics are generated. Similar protocols appeared for the execution of irregular directed acyclic graphs in distributed memory machines that were developed for numerical computations <ref> [CCBS95] </ref>. The classification of parallel programming models reveals that the messaging subsystem must provide three kinds of services to user protocols: It must be able to accommodate and service short-term bursts. It must provide long-term flow control for those protocols that require a feedback mecha nism. <p> Moreover, as some researchers have pointed, targeting pure request/reply protocols does not always result in the most efficient use of the network <ref> [CCBS95] </ref>. 3.3.5 Sender-Overflow Revisited In Section 3.3.3, we show that sender-overflow is not a viable policy. However, it is the only one that exhibits the bandwidth matching property.
Reference: [CDG + 93] <author> D. E. Culler, A. Dusseau, S. C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel programming in Split-C. </title> <booktitle> In Proceedings of Supercomputing 93, </booktitle> <pages> pages 262273, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Accesses during this phase do not exhibit any regularity and moreover, the initial data placement is random. Therefore, the application does not exhibit any regular sharing patterns. EM3D models the propagation of electromagnetic waves through objects in three dimensions <ref> [CDG + 93] </ref>. The problem is formulated as a computation on a bipartite graph with directed edges from E nodes, which represent electric fields, to H nodes, which represent magnetic fields, and vice versa. The main loop computes the change in E and H values over time. <p> Application Problem Input Data Set Sequential Time (secs) appbt 3D implementation of CFD [BM95] 40x40x40 matrices, 4 iters 110 barnes Barnes-Hut N-body simulation [WOT + 95] 16K particles, 4 iters 33 em3d 3D electromagnetic wave propa gation <ref> [CDG + 93] </ref> 128K nodes, degree 6, 40% remote edges, distance span 1, 100 iterations 166 lu Contiguous blocked dense LU factorization [WOT + 95] 512x512 matrix, 16x16 blocks 75 ocean Simulation of eddy currents [WOT + 95] 514x514 ocean size 62 moldyn Molecular dynamics [BBO + 83] 8722 molecules, 40 <p> SMP-correctness overhead in Blizzard/S is higher for applications with frequent instrumented memory operations (such barnes, lu). Table 5.3: Applications and input parameters. Application Problem Input Data Set em3d 3D electromagnetic wave propagation <ref> [CDG + 93] </ref> 128K nodes, degree 6, 40% remote edges, distance span 1 (processors share edges only with their immediate neighbors), 100 iterations appbt 3D implementation of CFD [BM95] 40x40x40 matrices, 4 iters barnes Barnes-Hut N-body simulation [WOT + 95] 16K particles, 4 iters tomcatv Thompsons solver mesh generation [SPE90] 512x512
Reference: [CDK + 94] <author> Alan L. Cox, Sandhya Dwarkadas, Pete Keleher, Honghui Lu, Ramakrishnan Raja-mony, and Willy Zwaenepoel. </author> <title> Software versus hardware shared-memory implementation: A case study. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 106117, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: They were originally written for sequentially consistent (transparent) hardware shared memory systems. Sequentially consistent page-based systems perform poorly on most of these codes because of their fine-grain communication and write sharing <ref> [CDK + 94, SFL + 94] </ref>. Page-granularity systems with weaker consistency models however, offer competitive performance to sequentially consistent software FGDSM systems [ZIS + 97] but complicate the programming model.
Reference: [Cen93] <institution> Army High Performance Computing Research Center. Distributed Job Manager Man Pages, </institution> <year> 1993. </year>
Reference-contexts: Moreover, it can interrupt the host processor. The Myrinet switches are connected in a tree topology with the nodes attached as leaves of the tree. Jobs intended to run on the COW processing nodes are submitted through the Distributed Job Manager (DJM) <ref> [Cen93] </ref>. <p> Moreover, it can interrupt the host processor. The Myrinet switches are connected in a tree topology with the nodes attached as leaves of the tree. A.2 Distributed Job Manager (DJM) Jobs intended to run on the COW processing nodes are submitted through DJM <ref> [Cen93] </ref>. The version of DJM running on the COW is based on the CM-5 1.0 version. However, it has been heavily modified for the COW context by Mark Dionne [Dio96].
Reference: [CKA91] <author> David Chaiken, John Kubiatowicz, and Anant Agarwal. </author> <title> Limitless directories: A scalable cache coherence scheme. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 224234, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Unfortunately, these bits can only partially support fine-grain access control since they do not provide the ReadOnly state. Tamir, et al., have proposed extensions to the format of the page table in order to fully support fine-grain access control [TJ92]. Cache Controller. The MIT Alewife <ref> [CKA91] </ref> and Kendall Square Research KSR-1 [Ken92] shared-memory systems use custom cache controllers to implement access control. In addition to detecting misses in hardware caches, these controllers determine when to invoke a protocol action. <p> Dedicated hardware provides high performance for a single protocol. While custom hardware performs an action quickly, research has shown that no single protocol is optimal for all applications [KMRS88] or even for all data structures within an application [BCZ90,FLR + 94]. Hybrid hardware/software protocols such as Alewifes LimitLESS <ref> [CKA91] </ref>, Dir 1 SW [HLRW93], and Typhoon-2 [Rei96] implement the expected common cases in hardware and trap to system software to handle complex, infrequent events. High design costs and resource constraints make custom hardware unattractive.
Reference: [CL96] <author> Satish Chandra and James R. Larus. </author> <title> HPF on Fine-Grain Distributed Shared Memory: Early Experience. </title> <editor> In Utpal Banerjee, Alexandru Nicolau, David Gelernter, and David Padua, editors, </editor> <booktitle> Proceedings of the Ninth Workshop on Languages and Compilers for Parallel Computing. </booktitle> <month> August </month> <year> 1996. </year>
Reference: [CLMY96] <author> David Culler, Lok Tin Liu, Richard Martin, and Chad Yoshikawa. </author> <title> LogP performance assessment of fast network interfaces. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 3543, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: Consequently, novel hardware and software designs sought to address this problem. The result has been novel research and commercial designs that often achieve a fraction of the latencies of the traditional communication architectures <ref> [CLMY96] </ref>. On the hardware side, the overhead of traditional, kernel-arbitrated access to a network has been too large for low latency communication. However, a network devices memory can be mapped to the user address space. <p> Second, Tempest messages are delivered without explicit polling by application programs. The Wisconsin Cluster of Workstations (COW) includes low-latency Myricoms Myrinet hardware [BCF + 95]. The Blizzard messaging subsystem originated out of Berkeleys Active Messages (AM) messaging library 1 <ref> [CLMY96] </ref>. By carefully considering hardwaresoftware tradeoffs, such as interrupts or polling upon message arrival, it achieves latencies as low as 35 msecs round-trip for control messages or 72 msecs for data messages with 128 byte data blocks (Section 3.7). The contributions of this study are twofold. <p> The division of labor is flexible since the LANai processor is programmable. However, the SPARC processor is far faster, which effectively limits the LANai processor to simple tasks [PLC95]. Blizzards communication library started from the LANai Control Program (LCP) used in Berkeleys LAM library <ref> [CLMY96] </ref>. The LCP was modified to fit the Tempest active message model and to improve performance for its expected usage. The changes are small, and, in fact, the modified LCP is still compatible with Berkeleys LAM library. <p> The float-mt and fix-mt are similar to the float and fix models respectively but they are able to support more than one computation thread per processor. Blizzard currently supports one messaging implementation. Blizzards communication library started from the LANai Control Program (LCP) used in Berkeleys LAM library <ref> [CLMY96] </ref>. The LCP was modified to fit the Tempest active message model and to improve performance for its expected usage. The changes are small, and, in fact, the modified LCP is still source-level compatible with Berkeleys LAM library.
Reference: [CM88] <author> Albert Chang and Mark F. Mergen. </author> <title> 801 storage: Architecture and programming. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1):2850, </volume> <month> February </month> <year> 1988. </year> <month> 195 </month>
Reference-contexts: Blizzard/E partially relies on the TLB to emulate fine-grain access control (Section 2.3.2). Though unimplemented by current commodity processors, additional, 18 per-block access bits in a TLB entry could provide fine-grain access control. The lock bits in certain IBM RISC machines, including the 801 <ref> [CM88] </ref> and RS/6000 [OG90], provide access control on 128-byte blocks. Unfortunately, these bits can only partially support fine-grain access control since they do not provide the ReadOnly state. Tamir, et al., have proposed extensions to the format of the page table in order to fully support fine-grain access control [TJ92].
Reference: [CMSB91] <author> Eric Cooper, Onat Menziolcioglu, Robert Sansom, and Francois Bitz. </author> <title> Host interface design for ATM LANs. </title> <booktitle> In Proceedings of the 16th Conference on Local Computer Networks, </booktitle> <pages> pages 1417, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Examples of such designs include the Arizona Application Device Channels (ADCs) [DPD94], Cornell U-Net [vEBBV95], HP Hamlyn [Wil92], Princeton SHRIMP [BDFL96]. The result of such recent research has been commercial designs like Myricom Myrinet [BCF + 95], Fore 200 <ref> [CMSB91] </ref>, Mitsubishi DART [OZH + 96]. With the OS removed from the critical path, the memory subsystem emerges as a major hurdle for delivering the network performance to the application.
Reference: [Cor94] <institution> Convex Computer Corp. SPP1000 Systems Overview, </institution> <year> 1994. </year>
Reference: [CRD + 95] <author> John Chapin, Mendel Rosenblum, Scott Devine, Tirthankar Lahiri, Dan Teeodosiu, and Anoop Gupta. Hive: </author> <title> Fault containment for shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating System Principles (SOSP), </booktitle> <pages> pages 1225, </pages> <month> December </month> <year> 1995. </year>
Reference: [CRL96] <author> Satish Chandra, Brad Richards, and James R. Larus. Teapot: </author> <title> Language support for writing memory coherence protocols. </title> <booktitle> In Proceedings of the SIGPLAN 96 Conference on Programming Language Design and Implementation (PLDI), </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Therefore, the compiler (or the user protocol for hand coded protocols) must explicitly manage handler continuations. Buffer allocation depends on the global system state while continuations depend on the local state. Therefore, it is easier to solve this problem without support from the messaging subsystem. Indeed, Teapot <ref> [CRL96] </ref>, a high level pro tocol language and compiler developed for Tempest, supports blocking handlers. It hides the com plexity of managing the handler continuations in the runtime library without adverse performance effects. 69 ers and the maximum sender-overflow queue, then the deadlock detection condition remains the same.
Reference: [DAC96] <author> Andrea C. Dusseau, Remzi H. Arpaci, and David E. Culler. </author> <title> Effective distributed scheduling of parallel workloads. </title> <booktitle> In Proceedings of the 1996 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <year> 1996. </year>
Reference: [DAPP93] <author> Peter Druschel, Mark B. Abbot, Michael A. Pagels, and Larry L. Peterson. </author> <title> Network subsystem design. </title> <journal> IEEE Network, </journal> <volume> 7(4):819, </volume> <month> July </month> <year> 1993. </year>
Reference-contexts: On the receiver, the data end up in incoming buffers allocated out of a queue of free buffers in the application address space. An address translation mechanism can be used to map the application addresses to physical addresses <ref> [DAPP93] </ref> but it is not sufficient to fully support minimal messaging because the abstraction does allow the NI to move incoming data directly to user data structures.
Reference: [DCF + 89] <author> William J. Dally, Andrew Chien, Stuart Fiske, Waldemar Horwat, John Keen, Michael Larivee, Rich Nuth, Scott Wills, Paul Carrick, and Greg Flyer. </author> <title> The j-machine: A fine-grain concurrent computer. </title> <editor> In G. X. Ritter, editor, </editor> <booktitle> Proc. Information Processing 89. </booktitle> <publisher> Elsevier North-Holland, Inc., </publisher> <year> 1989. </year>
Reference-contexts: A typical handler performs the actions dictated by a coherence protocol to allow the access and then resumes the computation. The fine-grain access control mechanism is similar to full/empty bits of dataflow architectures <ref> [DCF + 89] </ref> but it is tailored to support the implementation of shared memory protocols. For this reason, it extends the twostate model of the full/empty bits to a threestate model that includes a readonly state. More specifically, Tempests fine-grain access control is based on tagged memory blocks.
Reference: [Dio96] <author> Mark Dionne. </author> <title> DJM-COW User Guide, </title> <note> 1996. http://www.cs.wisc.edu/ cow/djm.html. </note>
Reference-contexts: Jobs intended to run on the COW processing nodes are submitted through the Distributed Job Manager (DJM) [Cen93]. The version of DJM running on the COW is based on the CM-5 1.0 version, but it has been heavily modified for the COW environment by Mark Dionne <ref> [Dio96] </ref>. 1.2.2 Blizzard and FGDSM Resources In Blizzard, shared memory is supported only for regions allocated through special mal-loc ()-like calls that manage a shared heap 1 . Blizzard preallocates an address range within the application address space for the shared heap (Figure 1-1). <p> A.2 Distributed Job Manager (DJM) Jobs intended to run on the COW processing nodes are submitted through DJM [Cen93]. The version of DJM running on the COW is based on the CM-5 1.0 version. However, it has been heavily modified for the COW context by Mark Dionne <ref> [Dio96] </ref>. Readers interested in actually running DJM jobs are referred for more details to the DJM man pages from which the following information was extracted.
Reference: [DP93] <author> Peter Druschel and Larry L. Peterson. Fbufs: </author> <title> A high-bandwidth cross-domain transfer facility. </title> <booktitle> In Proceedings of the 14th ACM Symposium on Operating System Principles (SOSP), </booktitle> <pages> pages 189202, </pages> <month> December </month> <year> 1993. </year> <pages> Asheville, </pages> <address> NC. </address>
Reference-contexts: Studies have shown that even today, network protocols spend a significant amount of time simply copying data [Ste94]. Therefore, many designs have attempted to avoid redundant copying at the application interface <ref> [DP93, RAC96, Wil92] </ref>, the OS [kJC96], and the network interface [OZH + 96, DWB + 93, LC95, BCM94]. <p> Applications can use them to access the network but likely they will prefer higher level messaging models such as Fbufs <ref> [DP93] </ref>, MPI [For94] or TCP/IP. Minimal messaging, by definition, provides a path for the message data through the abstraction layers to the application data structures. At the lowest level, incoming or outgoing messages should point to application data structures.
Reference: [DPD94] <author> Peter Druschel, Larry L. Peterson, and Bruce S. Davie. </author> <title> Experiences with a high-speed network adaptor: A software perspective. </title> <booktitle> In Proceedings of the ACM SIGCOMM 94 Conference, </booktitle> <pages> pages 213, </pages> <month> September </month> <year> 1994. </year> <institution> London, UK. </institution>
Reference-contexts: Typically, the OS maps the device registers and/or memory into the user address space. Thereafter, the application can initiate message operations communicating directly with the device using loads and stores to send and receive messages. Examples of such designs include the Arizona Application Device Channels (ADCs) <ref> [DPD94] </ref>, Cornell U-Net [vEBBV95], HP Hamlyn [Wil92], Princeton SHRIMP [BDFL96]. The result of such recent research has been commercial designs like Myricom Myrinet [BCF + 95], Fore 200 [CMSB91], Mitsubishi DART [OZH + 96]. <p> In most cases, applications access the network through a layer of messaging abstractions. We can distinguish between low-level network access models and high-level user messaging models. Network access models such as ADCs <ref> [DPD94] </ref>, U-Net [vEBBV95], Active Messages [vECGS92], Fast Messages [PLC95], provide protected user access to the NI and serve as a consistent low-level model across NIs. Applications can use them to access the network but likely they will prefer higher level messaging models such as Fbufs [DP93], MPI [For94] or TCP/IP. <p> Similarly, the Active Messages implementation on Myrinet uses a single-copy approach through an intermediate shared user/kernel buffer where the NI pulls or pushes message data. Arizona ADCs <ref> [DPD94] </ref> have been designed to optimize stream traffic. In Section 6.2, we discussed why this design cannot fully support minimal messaging. The base Cornell UNet [vEBBV95] architecture supports an abstraction similar to ADCs, and therefore has the same limitations as ADCs.
Reference: [DR97] <author> Dave Dunning and Greg Regnier. </author> <title> The virtual interface architecture. In Hot Interconnects 97, </title> <year> 1997. </year>
Reference-contexts: Thereafter, the NI must enforce the access rights when it sends or receives messages. Moreover, extra care must be taken to break the remote connection when a process is destroyed. An equivalent model is described in detail in Berkeleys Active Message specification [MC95] or Intels Virtual Interface Architecture <ref> [DR97] </ref>. 6.2 Address Translation Properties For Minimal Messaging In this section, I present the properties that the address translation mechanism should satisfy and I argue why I consider them desirable for minimal messaging. First, NI address translation mechanisms must be incorporated within an abstraction that can support minimal messaging. <p> Thereafter, the network interface must enforce the access rights when it sends or receives messages. Such models have been defined in detail within the context of Berkeleys Active Message specification [MC95] and Intels Virtual Interface Architecture <ref> [DR97] </ref>. Memory Allocation. Shared memory coherence protocols such as Stache that implement a simple cache-only memory architecture (S-COMA) in software, pose an interesting dilemma on what to do when shared memory becomes overcommitted on a particular node. In particular, there are two alternatives.
Reference: [DW89] <author> William J. Dally and D. Scott Wills. </author> <title> Universal mechanism for concurrency. </title> <booktitle> In PARLE 89: Parallel Architectures and Languages Europe. </booktitle> <publisher> Springer-Verlag, </publisher> <month> June </month> <year> 1989. </year>
Reference-contexts: The challenge in this case was to incorporate it in a commodity multiprocessor operating system. Since Section 2.4 presents implementation details, I only mention here its basic characteristics. Although several systems have memory tags and fine-grain access control <ref> [DW89] </ref>, contemporary commercial machines lack this facility. Blizzard-E synthesizes the Invalid state by forcing uncorrectable errors in the memorys error correcting code (ECC) via a diagnostic mode. Running the SPARC cache in write-back mode causes all cache misses to appear as cache block fills.
Reference: [DWB + 93] <author> Chris Dalton, Greg Watson, David Banks, Costas Calamvokis, Aled Edwards, and John Lumley. </author> <title> Afterburner. </title> <journal> IEEE Network, </journal> <pages> pages 3643, </pages> <month> July </month> <year> 1993. </year> <month> 196 </month>
Reference-contexts: Studies have shown that even today, network protocols spend a significant amount of time simply copying data [Ste94]. Therefore, many designs have attempted to avoid redundant copying at the application interface [DP93, RAC96, Wil92], the OS [kJC96], and the network interface <ref> [OZH + 96, DWB + 93, LC95, BCM94] </ref>.
Reference: [EK89] <author> Susan J. Eggers and Randy H. Katz. </author> <title> The effect of sharing on the cache and bus performance of parallel programs. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS III), </booktitle> <pages> pages 257270, </pages> <year> 1989. </year>
Reference-contexts: Shared virtual memory (SVM) [LH89,CBZ91] is the most common form of software DSM and implements coherence at page granularity using standard address translation hardware found in commodity microprocessors. Such systems suffer from fragmentation and false sharing and can perform poorly in the presence of fine-grain sharing <ref> [EK89] </ref>. For acceptable performance, page-based systems often resort to weaker shared memory consistency models [KDCZ93,ENCH96,KHS + Shared virtual memory systems lack fine-grain access control, a key feature of hardware shared memory machines. Access control is the ability to selectively restrict reads and writes to memory regions.
Reference: [ENCH96] <author> Andrew Erlichson, Neal Nuckolls, Greg Chesson, and John Hennessy. Softflash: </author> <title> Analyzing the performance of clustered distributed virtual shared memory supporting fine-grain shared memory. </title> <booktitle> In Proceedings of the 7th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VII), </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: Protocol Action Blizzard/S Blizzard/E Blizzard/ES Blizzard/T Read Miss 10.0 16.4 14.1 21.1 Write Miss 10.1 63.7 58.1 19.6 Upgrade 7.4 59.1 51.8 14.8 49 cessors on page protection changes. This overhead can significantly limit the performance of page-based systems in multiprocessor nodes <ref> [ENCH96] </ref>. Chapter 5 discusses further implications of multiprocessor nodes. 2.6 Conclusions Fine-grain access control is a fundamental operation in shared memory systems. It can be implemented in many ways that involve hardware, software or combination techniques. <p> When a protocol handler wishes to downgrade the fine-grain protection, it will only need to synchronize with those processors that they have accessed the block in the past. Similar techniques have been used to reduce the cost of TLB shootdowns in SMP machines <ref> [ENCH96] </ref>. An FGDSM system that implements replicated fine-grain tags is Shasta/SMP [SGA97]. In Section 5.3, we will examine in detail how the different Blizzard tag implementations address the atomicity problem. Message & Access Fault Queues Running on an SMP node offers the opportunity to support separate message queues per processor. <p> Many variations thereof have since emerged such as Munin [CBZ91], TreadMarks [KDCZ93], SoftFLASH <ref> [ENCH96] </ref>, MGS [YKA96], and Cashmere [KHS + 97]. These systems all use standard virtual address translation mechanisms to implement coherence across nodes, and rely on relaxed memory consistency models and careful program annotation to support fine-grain sharing. SoftFLASH, MGS, and Cashmere address SMP-node implementations of VM-based software DSM.
Reference: [Fal97] <author> Babak Falsafi. </author> <title> Fine-Grain protocol execution mechanisms and scheduling policies on SMP clusters. </title> <type> PhD thesis, </type> <institution> Computer Sciences Department, University of Wisconsin Madison, </institution> <year> 1997. </year>
Reference: [FLR + 94] <author> Babak Falsafi, Alvin Lebeck, Steven Reinhardt, Ioannis Schoinas, Mark D. Hill, James Larus, Anne Rogers, and David Wood. </author> <title> Applicationspecific protocols for user-level shared memory. </title> <booktitle> In Proceedings of Supercomputing 94, </booktitle> <pages> pages 380389, </pages> <month> Novem-ber </month> <year> 1994. </year>
Reference-contexts: By default, shared memory programs are linked against a library that implements an 128-byte S-COMA-like [HSL94] software protocol [RLW94] to maintain coherence across nodes. Tempests flexibility allows the implementation of applicationspecific custom coherence protocols <ref> [FLR + 94] </ref> which integrate shared memory with messaging. Such protocols, called hybrid protocols, can reduce the overhead of fine-grain access control by being tailored to the 1. Throughout the thesis, Blizzard without qualification refers to the COW implementation. <p> Therefore, Tempest requires low-overhead messages to provide low-latency communication, which is fundamental to the performance of many parallel programs. 51 Tempest also supports application specific coherence protocols <ref> [FLR + 94] </ref>. In such protocols, the coherence protocol has been optimized to fit the application communication requirements. Often understanding the underlying communication patterns allows to transform fine grain communication to bulk data transfers. In this case, high bandwidth becomes as important as low latency. <p> Streaming protocols generate many concurrent activations graphs with small buffer requirements. Examples of such protocols include many protocols developed on top of active messages using Split-C [CDG + 93,MVCA97] as well as update-based applicationspecific coherence protocols developed for Tempest <ref> [FLR + 94] </ref>. The common characteristic of these protocols is that there is not an inherent limit in the number of injected messaged. Instead, they rely on the messaging subsystem to implicitly limit the injection rate. <p> Blizzard supports the Tempest interface [Rei94] which exposes to user level the access control and messaging mechanisms. Thereby, Blizzard allows the development of applicationspecific coherence protocols <ref> [FLR + 94] </ref> that optimize the data transfers to match the application sharing patterns. <p> We examine both TSM and CS versions of this application. In the TSM version, the processors transparently access remote data as they compute the new values of their graph nodes. Therefore, the computation is interleaved with communication. The CS version <ref> [FLR + 94] </ref> separates the computation and communication in distinct phases. Instead of the moving the data through shared memory, the node that produced the new values is responsible for pushing them to the nodes that shall need them in the next iteration. <p> Unique among FGDSM systems is Blizzards ability to harness the raw messaging performance and offer it to the application. The Tempest interface [Rei94] exposes the access control and messaging mechanisms to user level and therefore, allows the development of applicationspecific coherence protocol <ref> [FLR + 94] </ref>. Such protocols eliminate the access control overhead resulting in lower latencies for small data transfers. <p> We proceed by evaluating the impact of SMP-correctness overheads on application performance, and finally evaluate the performance of SMP nodes. Table 5.3 presents the shared-memory applications used in this study, the problems they address, and their input parameters. Em3d-cs and appbt-cs <ref> [FLR + 94] </ref> take advantage of the flexibility of software DSM and use customized protocols. In both cases, the applications bypass shared memory and use direct messaging to optimize communication. 5.6.1 Base System Performance Table 5.4 presents application speedups running on the base systems. <p> Since FGDSM systems typically transfer data in small, cache-block sized quantities, the relative cost of copying is small, compared to the total cost of the messaging operation. However, application specific protocols can often change the picture <ref> [FLR + 94] </ref> as they batch data transfers in larger messages. In general, however, this study extends beyond the domain of FGDMS systems. There are many networking applications that place a demand for high throughput and low latency on the network subsystem.
Reference: [For94] <author> Message Passing Interface Forum. </author> <title> MPI: A Message-Passing Interface Standard. </title> <type> Technical Report Draft, </type> <institution> University of Tennessee, Knoxville, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Due to the proliferation of parallel programming models and computing platforms, portability of parallel programs remains a significant concern for parallel programs. The emergence of standard message-passing libraries, such as PVM and MPI <ref> [GBD + 94, For94] </ref>, that have been implemented both for message-passing and shared memory platforms has addressed the portability issue for message-passing programs. 4 Shared memory has been limited to page-based systems [LH89,CBZ91]. <p> Applications can use them to access the network but likely they will prefer higher level messaging models such as Fbufs [DP93], MPI <ref> [For94] </ref> or TCP/IP. Minimal messaging, by definition, provides a path for the message data through the abstraction layers to the application data structures. At the lowest level, incoming or outgoing messages should point to application data structures.
Reference: [FW96] <author> Babak Falsafi and David A. Wood. </author> <title> When does dedicated protocol processing make sense? Technical Report 1302, </title> <institution> Computer Sciences Department, University of Wiscon-sinMadison, </institution> <month> February </month> <year> 1996. </year>
Reference: [FW97] <author> Babak Falsafi and David A. Wood. </author> <title> Scheduling communication on an SMP node parallel machine. </title> <booktitle> In Proceedings of the Third IEEE Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 128138, </pages> <month> February </month> <year> 1997. </year>
Reference-contexts: In the results presented in this section, message notification is done with the polling code inserted directly into the executable as discussed in Chapter 3. A dedicated network processor is not used because it does result in the best utilization for that processor <ref> [FW97] </ref>. Chapter 5 discusses alternative ways to employ more processors per node in FGDSM systems. The inserted code polls on a cacheable memory location. On message arrivals, this memory location is updated by the network interface using DMA.
Reference: [GBD + 94] <author> Al Geist, Adam Beguelin, Jack Dongarra, Weicheng Jiang, Robert Manchek, and Vaidy Sunderam. </author> <title> PVM 3 users guide and reference manual. </title> <type> Technical Report ORNL/ TM-12187, </type> <institution> Oak Ridge National Laboratory, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Due to the proliferation of parallel programming models and computing platforms, portability of parallel programs remains a significant concern for parallel programs. The emergence of standard message-passing libraries, such as PVM and MPI <ref> [GBD + 94, For94] </ref>, that have been implemented both for message-passing and shared memory platforms has addressed the portability issue for message-passing programs. 4 Shared memory has been limited to page-based systems [LH89,CBZ91].
Reference: [GCP96] <author> R. Gillett, M. Collins, and D. Pimm. </author> <title> Overview of Memory Channel Network for PCI. </title> <booktitle> In Proceedings of the 41th IEEE Computer Society International Conference (COMPCON96), </booktitle> <year> 1996. </year>
Reference-contexts: Commercial designs such as Myricom Myrinet [BCF + 95], DEC Memory Channel <ref> [GCP96] </ref>, Tandem TNet [Hor95] have become widely available. Collectively, these designs have been called system-area networks [Bel96] to distinguish them from traditional local area networks that have higher messaging overheads. System-area networks also enable the construction of clustered shared memory servers targeted for high availability and scalability. <p> Page associations are either direct (the sender knows the remote physical address) or 163 indirect (through global network addresses). Examples of this approach include Princeton SHRIMP [BDFL96], Forth Telegraphos II [MK96], DEC Memory Channel <ref> [GCP96] </ref> and Tandem TNet [Hor95]. SHRIMP and Telegraphos II use direct page associations. The Memory Channel and TNet use indirect page associations. Common characteristic of these designs is their inability to handle misses in the translation structures.
Reference: [Gil94] <author> George Gilder. </author> <title> The bandwidth tidal wave. </title> <booktitle> Forbes ASAP, </booktitle> <month> December </month> <year> 1994. </year> <note> Available from http://www.forbes.com/asap/gilder/telecosm10a.htm. </note>
Reference-contexts: This disparity has led some to argue that we are on the verge of a major paradigm shift that will transform the entire structure of information technology <ref> [Gil94] </ref>. Studies have shown that even today, network protocols spend a significant amount of time simply copying data [Ste94]. Therefore, many designs have attempted to avoid redundant copying at the application interface [DP93, RAC96, Wil92], the OS [kJC96], and the network interface [OZH + 96, DWB + 93, LC95, BCM94].
Reference: [GJ91] <author> David B. Gustavson and David V. James, </author> <title> editors. SCI: Scalable Coherent Interface: Logical, Physical and Cache Coherence Specifications, </title> <journal> volume P1596/D2.00 18Nov91. IEEE, </journal> <month> November </month> <year> 1991. </year> <note> Draft 2.00 for Recirculation to the Balloting Body. 197 </note>
Reference-contexts: In turn, applications with high reliability requirements can use cautious shared memory protocol that keep a copy of every message until safe remote receipt of the data is confirmed. For example, SCI <ref> [GJ91] </ref> is a shared memory coherence protocol that has been designed to deal in this manner with transient transmission faults. Process & Node Faults. The cluster environment comprises of many workstations, able to operate independently.
Reference: [Gro95] <author> PCI Special Interest Group. </author> <title> PCI Local Bus Specification, Revision 2.1, </title> <year> 1995. </year>
Reference: [Hal85] <author> Robert H. Halstead, Jr. </author> <title> Multilisp: A language for concurrent symbolic computation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(4):501538, </volume> <month> October </month> <year> 1985. </year>
Reference: [HGDG94] <author> John Heinlein, Kourosh Gharachorloo, Scott A. Dresser, and Anoop Gupta. </author> <title> Integration of message passing and shared memory in the Stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pages 3850, </pages> <year> 1994. </year>
Reference-contexts: In addition, Tempest provides an attractive target to build runtime systems for parallel languages [LRV94,CL96] where compilers can tailor the coherence protocol to incorporate high-level knowledge about the application access patterns. While other hardware shared memory systems integrate message passing and shared memory <ref> [HGDG94] </ref>, none offers the same flexibility to develop applicationspecific protocols as user (rather than system) libraries. High-end Tempest implementations such as the Typhoon designs [RLW94,RPW96] include extensive hardware support for fine-grain access control and protocol actions. <p> Therefore, a newer generation of low overhead messaging interfaces attacked the software overheads. Some software architectures originated in the networking community [DP93,Osb94] while others arose from the multicomputer community <ref> [vECGS92,PLC95, HGDG94] </ref>. The emergence of system area networks and networks of workstations have blurred the distinction. In general however, the latter have been more preoccupied with low latencies than the former. Among the key proposals that emerged from the multicomputer community have been the Berkeley active messages. <p> Lookup Miss Service NI CPU NI Hardware Structures Network Coprocessors (Section 6.3.1) Custom Designs (Section 6.3.2) Software Structures Network Microcontrol lers (Section 6.3.1) Software TLBs (Section 6.3.2) CPU User-controlled map pings (Section 6.3.3) 143 We can implement NI translation structures in software, similar to the software TLBs proposed for FLASH <ref> [HGDG94] </ref>. To implement software structures, we need an NI microcon-troller that it is flexible enough to synchronize with the nodes CPU to access its own page tables in main memory. Such structures have small associativity and many entries. <p> The key characteristic of this design is that it supports only the NI translation structures and not special device tables in main memory. NIs with microcontrollers can implement the lookup with software TLBs <ref> [HGDG94] </ref>. Some designs however, include hardware support for the lookup [OZH + 96] in the form of custom finite state machines for message processing. Whenever a miss occurs, the device triggers an interrupt invoking the device drivers interrupt handler, which handles the misses.
Reference: [HHS + 95] <author> Chris Holt, Mark Heinrich, Jaswinder Pal Singh, Edward Rothberg, and John Hen-nessy. </author> <title> The effects of latency, occupancy, and bandwidth in distributed shared memory multiprocessors. </title> <type> Technical Report CSL-TR-95-660, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: Furthermore, these cards typically do not provide support for simultaneous dispatch of multiple messages. Handler execution, therefore, is inherently a serial operation and parallelizing it would incur high software synchronization overhead, result in high protocol occupancies <ref> [HHS + 95] </ref> and lower overall performance. Blizzard serializes protocol handler execution on SMP nodes. Serializing handler execution obviates the need for synchronization of accesses for resources used only by the protocol, i.e., the message queues and the directory state.
Reference: [Hil87] <author> Mark Donald Hill. </author> <title> Aspects of cache memory and instruction buffer performance. </title> <type> Technical Report UCB/CSD 87/381, </type> <institution> Computer Science Division (EECS), University of California at Berkeley, </institution> <month> November </month> <year> 1987. </year> <type> Ph.D. dissertation. </type>
Reference-contexts: In general, large caches do not always translate to reduced miss rates for remote data. Caches exhibit compulsory, capacity, conflict <ref> [Hil87] </ref> and coherence misses [Jou90]. Capacity misses occur when the cache cannot contain all the blocks and conflict misses occur when two blocks map to the same cache location. Changing the cache characteristics reduces (or even eliminates for some applications) capacity and conflict misses.
Reference: [Hil97] <author> Mark D. Hill. </author> <title> Multiprocessors should support simple memory consistency models. </title> <type> Technical Report CSD TR97-1353, </type> <institution> Department of Computer Science, University of Washington, </institution> <month> October </month> <year> 1997. </year>
Reference-contexts: However, weaker consistency models complicate introduce complexity in the software design. With the exception of the Alpha and PowerPC architectures, most hardware implementations of shared support simple consistency models. Moreover, Hill argues that future hardware implementations of shared memory should support simple consistency models <ref> [Hil97] </ref> because relaxed consistency models do not offer a significant performance advantage to justify the extra software complexity. Introducing relaxed consistency models specifically for software DSM systems, complicates the porting of shared memory applica tions to these systems. <p> In the long run, this may be a relatively insignificant issue since Hill <ref> [Hil97] </ref> presents a convincing case that future multiprocessors should support simple memory consistency models.
Reference: [HLRW93] <author> Mark D. Hill, James R. Larus, Steven K. Reinhardt, and David A. Wood. </author> <title> Cooperative shared memory: Software and hardware for scalable multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(4):300318, </volume> <month> November </month> <year> 1993. </year> <booktitle> Earlier version appeared in it Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS V). </booktitle>
Reference-contexts: While custom hardware performs an action quickly, research has shown that no single protocol is optimal for all applications [KMRS88] or even for all data structures within an application [BCZ90,FLR + 94]. Hybrid hardware/software protocols such as Alewifes LimitLESS [CKA91], Dir 1 SW <ref> [HLRW93] </ref>, and Typhoon-2 [Rei96] implement the expected common cases in hardware and trap to system software to handle complex, infrequent events. High design costs and resource constraints make custom hardware unattractive.
Reference: [HLW95] <author> Mark D. Hill, James R. Larus, and David A. Wood. </author> <title> Tempest: A substrate for portable parallel programs. </title> <booktitle> In COMPCON 95, </booktitle> <pages> pages 327332, </pages> <address> San Francisco, California, March 1995. </address> <publisher> IEEE Computer Society. </publisher>
Reference-contexts: Fine-Grain Access Control. Fine-grain access control is the Tempest mechanism that detects reads or writes to invalid blocks or writes to read-only blocks and traps to user-level code, which uses the resulting exceptions to execute a coherence protocol action <ref> [HLW95] </ref>. At each memory reference, the system must ensure that the referenced datum is local and accessible. Therefore, each memory reference is logically preceded by a check that has the following semantics: if (!lookup (Address)) CallHandler (Address, AccessType) memory-reference (Address) If the access is allowed, the reference proceeds normally. <p> Tempests fine-grain access control mechanism detects reads or writes to invalid blocks or writes to read-only blocks and traps to user-level code, which uses the resulting exceptions to execute a coherence protocol action in software <ref> [HLW95] </ref>. At each memory reference, the system must ensure that the referenced datum is local and accessible. If the access is allowed, the reference proceeds normally. Otherwise, the shared-memory protocol software must be invoked, which is achieved by suspending the computation and invoking a user-level handler. <p> Fine-Grain Access Control. Fine-grain access control is the Tempest mechanism that detects reads or writes to invalid blocks or writes to read-only blocks and traps to user-level code, which uses the resulting exceptions to execute a coherence protocol action <ref> [HLW95] </ref>. At each memory reference, the system must ensure that the referenced datum is local and accessible. Therefore, each memory reference is logically preceded by a check that has the following semantics: if (!lookup (Address)) CallHandler (Address, AccessType) memory-reference (Address) If the access is allowed, the reference proceeds normally.
Reference: [Hor95] <author> Robert W. Horst. TNet: </author> <title> A reliable system area network. </title> <booktitle> IEEE Micro, </booktitle> <month> February </month> <year> 1995. </year>
Reference-contexts: Commercial designs such as Myricom Myrinet [BCF + 95], DEC Memory Channel [GCP96], Tandem TNet <ref> [Hor95] </ref> have become widely available. Collectively, these designs have been called system-area networks [Bel96] to distinguish them from traditional local area networks that have higher messaging overheads. System-area networks also enable the construction of clustered shared memory servers targeted for high availability and scalability. <p> Page associations are either direct (the sender knows the remote physical address) or 163 indirect (through global network addresses). Examples of this approach include Princeton SHRIMP [BDFL96], Forth Telegraphos II [MK96], DEC Memory Channel [GCP96] and Tandem TNet <ref> [Hor95] </ref>. SHRIMP and Telegraphos II use direct page associations. The Memory Channel and TNet use indirect page associations. Common characteristic of these designs is their inability to handle misses in the translation structures.
Reference: [HP90] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: The situation was conceptually similar to the one that computer architects had faced earlier with complex instruction sets. Such instruction sets required extensive logic to decode and execute the instructions which introduced significant overheads even for simple operations <ref> [HP90] </ref>. The semantic gap between the instruction set and what the hardware could directly support with simple instructions limited the performance of microprocessors as they had to rely in slow complex mechanisms to implement the instructions. This problem was addressed with the introduction of reduced instruction set computers (RISC). <p> Although, the manufacturing cost of computer products is typically related to cost of components, cost from the perspective of a customer is related to price which is also dictated by market forces <ref> [HP90] </ref>. In a cost-performance study related to the work presented in this chapter, we found that dual-processor nodes were more cost-effective in that specific timeframe than either uniprocessor or quad-processor nodes [SFH + 97]. The rest of this chapter is organized as follows. <p> It should be noted however, that although manufacturing cost of computer products is typically related to cost of components, cost from the perspective of a customer is related to price which is also dictated by market forces <ref> [HP90] </ref>. High-performance products, for instance, tend to target smaller markets and as such carry larger margins, charging higher premiums. In a related study [SFH + 97] we found that today quad-processor servers command high price premiums. In contrast, dual-processor machines offer the lowest overall system cost.
Reference: [HSL94] <author> Erik Hagersten, Ashley Saulsbury, and Anders Landin. </author> <title> Simple COMA node implementations. </title> <booktitle> In Proceedings of the 27th Hawaii International Conference on System Sciences, </booktitle> <month> January </month> <year> 1994. </year>
Reference-contexts: Unlike Shasta, Blizzard uses the Tempest interface [Rei94] to support software distributed shared memory. Tempest separates the mechanisms required to implement distributed shared memory from the coherence protocols that enforce the shared memory semantics. By default, shared memory programs are linked against a library that implements an 128-byte S-COMA-like <ref> [HSL94] </ref> software protocol [RLW94] to maintain coherence across nodes. Tempests flexibility allows the implementation of applicationspecific custom coherence protocols [FLR + 94] which integrate shared memory with messaging. Such protocols, called hybrid protocols, can reduce the overhead of fine-grain access control by being tailored to the 1. <p> In Tempest, a processor initiates a bulk data transfer much as it would start a conventional DMA transaction, by specifying virtual addresses on both source and destination nodes. The transfer is logically asynchronous with the computation. 1.2.4 Stache Tempests default coherence protocol is an 128-byte S-COMA-like <ref> [HSL94] </ref> software protocol [RLW94] called stache. Stache maintains internal data structures to correctly implement a sequentially consistent shared memory coherence protocol. For each shared page, one node acts as the home node. <p> Capacity misses occur when the cache cannot contain all the blocks and conflict misses occur when two blocks map to the same cache location. Changing the cache characteristics reduces (or even eliminates for some applications) capacity and conflict misses. Blizzard and other FGDSM systems [RLW94,SGT96] typically implement simple COMA-like <ref> [HSL94] </ref> software coherence protocols to maintain coherence across nodes. Since the local memory is used as a node cache, in practice, capacity 104 and conflict misses are eliminated or transformed to local misses. In contrast, compulsory and conflict misses directly correspond to the characteristics of the parallel applications. <p> A.4.2 Blizzard Protocol libraries Tempests default coherence protocol is an 128-byte S-COMA-like <ref> [HSL94] </ref> software protocol [RLW94]. Stache maintains internal data structures to correctly implement a sequentially consistent shared memory coherence protocol. For each shared page, one node acts as the home node. By default home pages are distributed among the nodes in a round-robin manner.
Reference: [Hsu94] <author> Peter Yan-Tek Hsu. </author> <title> Designing the TFP microprocessor. </title> <journal> IEEE Micro, </journal> <volume> 14(2):2333, </volume> <month> April </month> <year> 1994. </year> <month> 198 </month>
Reference-contexts: Misses to remote physical addresses always invoke an action. Due to the KSR-1s COMA architecture, any reference that misses in the remote memory cache requires protocol action. A trend toward on-chip second-level cache controllers <ref> [Hsu94] </ref> has made modified cache controllers incompatible with modern commodity processors such as the Pentium Pro [Int96] and PowerPC 750 processors. Memory Controller.
Reference: [Inc91] <author> Sun Microsystems Inc. </author> <title> SPARC MBus Interface Specification, </title> <month> April </month> <year> 1991. </year>
Reference-contexts: Table 6.2: Simulation Node Parameters CPU Characteristics 300 MHz dual issue HyperSparc 1 MB direct mapped processor cache with 32 byte transfer block size Memory Bus Characteristics MBus level-2 coherence protocol <ref> [Inc91] </ref>, 100 MHz, 64 bit wide Memory Bus Bandwidth 800 MB/sec (peak) 320 MB/sec (32 byte block transfers) 200 MB/sec (uncached stores to the NI) 120 MB/sec (uncached loads from the NI) Network Characteristics 100 CPU cycle latency per message Infinite bandwidth (limited by eight-message sliding window flow control protocol) 153 <p> Overall, the latency results show similar trends as in the simulator. Table 6.4: Myrinet Node Parameters CPU Characteristics 66 MHz dual issue HyperSparc 256 MB direct mapped processor cache with 32 byte transfer block size Memory Bus Characteristics MBus level-2 coherence protocol <ref> [Inc91] </ref>, 50 MHz, 64 bit wide I/O Bus Characteristics 25 MHz 32 bit SBus Memory Bus Bandwidth 400 MB/sec (peak) 160 MB/sec (32 byte block transfers) 20 MB/sec (through I/O bus bridge) Network Character istics 17 msecs latency (for messages without data) 40 MB/sec bandwidth (limited by eight-mes sage sliding window
Reference: [Int90] <author> Intel Corporation. </author> <title> iPSC/2 and iPSC/860 Users Guide. </title> <publisher> Intel Corporation, </publisher> <year> 1990. </year>
Reference-contexts: These parallel platforms have been called massively parallel processors (MPPs). Since relatively little extra hardware except the custom network is required, this approach has enjoyed popularity in older machines such as Intel iPSC860 <ref> [Int90] </ref> and TMC CM-5 [Thi91]. It is still present today in commercial systems such as IBM SP-2. At a high level of detail, there is not much difference between MPPs and a collection of workstations with the exception of the custom network.
Reference: [Int93] <author> Intel Corporation. </author> <title> Paragon technical summary. Intel Supercomputer Systems Division, </title> <year> 1993. </year>
Reference-contexts: Furthermore, there are not any provisions for a fallback action. Thus, the design requires fast kernel interfaces to gracefully degrade once the translation limits are exceeded. Designs with a network coprocessor, like Meiko CS [BCM94] and Intel Paragon <ref> [Int93] </ref> can support minimal messaging using the microprocessors address translation hardware and a separate DMA engine. Nonetheless, address translation mechanisms implemented for CPUs (TLBs) are not always appropriate for NIs. There are two potential problems.
Reference: [Int96] <author> Intel Corporation. </author> <title> Pentium Pro Family Developers Manual, Volume 3: Operating System Writers Manual, </title> <month> January </month> <year> 1996. </year>
Reference-contexts: Due to the KSR-1s COMA architecture, any reference that misses in the remote memory cache requires protocol action. A trend toward on-chip second-level cache controllers [Hsu94] has made modified cache controllers incompatible with modern commodity processors such as the Pentium Pro <ref> [Int96] </ref> and PowerPC 750 processors. Memory Controller. If the system can guarantee that the processors hardware caches never contain Invalid blocks and that ReadOnly blocks are cached in a read-only state, the memory controller can perform the lookup on hardware cache misses.
Reference: [JKW95] <author> Kirk L. Johnson, M. Frans Kaashoek, and Deborah A. Wallach. </author> <title> CRL: High-performance all-software distributed shared memory. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating System Principles (SOSP), </booktitle> <pages> pages 213228, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: Fine-grain distributed shared memory has traditionally only been implemented in hardware. Blizzard/CM-5 is the first implementation of FGDSM either entirely in software, or with hardware assist [SFL + 94]. Shasta [SGT96] is a successor of Blizzard and an all-software implementation of FGDSM. CRL <ref> [JKW95] </ref> is a region-based software FGDSM and requires program annotations for accessing regions. Neither Blizzard/CM-5 nor CRL have SMP-node implementations. SMP-node implementations of Shasta [SGA97] have recently become available. Overall, Shasta/SMP appears to behave similar to Blizzard implementations performance-wise. This work builds upon theirs in several ways.
Reference: [Jou90] <author> Norman P. Jouppi. </author> <title> Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers. </title> <booktitle> In The 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 364373, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: In general, large caches do not always translate to reduced miss rates for remote data. Caches exhibit compulsory, capacity, conflict [Hil87] and coherence misses <ref> [Jou90] </ref>. Capacity misses occur when the cache cannot contain all the blocks and conflict misses occur when two blocks map to the same cache location. Changing the cache characteristics reduces (or even eliminates for some applications) capacity and conflict misses.
Reference: [KA93] <author> John Kubiatowicz and Anant Agarwal. </author> <title> Anatomy of a message in the alewife multiprocessor. </title> <booktitle> In Proceedings of the 1993 ACM International Conference on Supercomputing, </booktitle> <year> 1993. </year>
Reference-contexts: prevent this type of deadlock, the system must extend the buffer space by copying messages to an overflow software queue in main memory [BCL 95,MKAK94] of the receiver. 67 Earlier systems aggressively buffer messages when the sender blocks [SFL + + after it has been blocked for a timeout interval <ref> [KA93] </ref>. However, the extra copies this entails can potentially degrade performance [MFHW96]. Blizzard, instead, uses a conservative deadlock detection scheme, which only buffers messages when a deadlock may have occurred.
Reference: [KBG97] <author> Alain Kagi, Doug Burger, and James R. Goodman. </author> <title> Efficient synchronization: Let them eat QOLB. </title> <booktitle> In Proceedings of the 24th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1997. </year>
Reference-contexts: The Blizzard message locks were designed by the author because MCS locks did not perform well under high-contention. They have been subsequently evaluated by Kagi et al <ref> [KBG97] </ref> and their results suggest that the scheme satisfies its design goal being superior to any shared memory-based locks in the Blizzard environment.
Reference: [KBM + 96] <author> Yousef A. Khalidi, Jose M. Bernabeu, Vlada Matena, Ken Shirriff, and Moti Thadani. </author> <title> Solaris MC: A multi computer OS. </title> <booktitle> In Proceedings of the 1996 USENIX Conference, </booktitle> <month> page ?, January </month> <year> 1996. </year>
Reference-contexts: The situation is likely to improve as commodity operating systems timidly start introducing clustering features (Microsoft Wolf-pack [Lab97], Sun Solaris MC <ref> [KBM + 96] </ref>). Many issues need to be addressed, however, before the integration becomes seamless at the cluster level. In this section, I discuss issues relevant to FGDSM systems. Relaxing the single user assumption, represents a conceptual change of focus in the examination of the system.
Reference: [KDCZ93] <author> Pete Keleher, Sandhya Dwarkadas, Alan Cox, and Willy Zwaenepoel. Treadmarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <type> Technical Report 93-214, </type> <institution> Department of Computer Science, Rice University, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: Previous approaches to shared memory on networks of workstations mainly targeted page-based distributed shared memory (DSM) systems [LH89]. Such systems required relaxed memory consistency models, which complicate the programming model, to overcome the effects of false sharing and fragmentation and offer acceptable performance <ref> [KDCZ93] </ref>. Zhou, et al., however, have shown that sequentially consistent fine-grain DSM systems are competitive with page-based systems that support weaker consistency models [ZIS + Blizzard explores four different fine-grain tag implementations. These designs cover a substantial range of the design space for fine-grain access control mechanisms [SFL + 94]. <p> Orca [BTK90], for example, provides access control on program objects instead of blocks. Translation Lookahead Buffer (TLB). Standard address translation hardware provides access control, though at memory page granularity. Nevertheless, it forms the basis of several distributed-shared-memory systems such as IVY [LH89], Munin [CBZ91] and TreadMarks <ref> [KDCZ93] </ref>. Blizzard/E partially relies on the TLB to emulate fine-grain access control (Section 2.3.2). Though unimplemented by current commodity processors, additional, 18 per-block access bits in a TLB entry could provide fine-grain access control. <p> Blizzards fine-grain access control cannot reach this performance level. However, the Blizzard techniques are based on mostly commodity software and hardware. Page-based software shared memory systems have fine-grain access control overheads that an order of magnitude greater that the slowest Blizzard system <ref> [LH89,CBZ91, KDCZ93] </ref>. Many of the kernel optimizations presented in this chapter are also applicable to these systems. <p> On one hand, their performance characteristics are worse than hardware shared memory [LL97], albeit with little or no hardware support and presumably at a lower cost point. On the other hand, unlike modern software page-based shared memory approaches <ref> [KDCZ93] </ref>, they can support shared memory without relaxing the consistency model from sequential consistency [KDCZ93], while still offering the same or better performance [ZIS + 97]. Unique among existing FGDSM systems is Blizzards ability to eliminate the cost of shared memory and offer to the application the raw messaging performance. <p> On the other hand, unlike modern software page-based shared memory approaches <ref> [KDCZ93] </ref>, they can support shared memory without relaxing the consistency model from sequential consistency [KDCZ93], while still offering the same or better performance [ZIS + 97]. Unique among existing FGDSM systems is Blizzards ability to eliminate the cost of shared memory and offer to the application the raw messaging performance. <p> However, they typically achieve higher remote memory bandwidth since messaging hardware is optimized for large messages. Unfortunately, the large size of memory pages typically causes excessive false sharing and fragmentation, which limits their application domain to coarse grain applications [CDK + + Later software DSM systems such as TreadMarks <ref> [KDCZ93] </ref>, mitigate the effects of false sharing and fragmentation with relaxed memory consistency models. Studies have shown that they can offer competitive performance to sequentially consistent software FGDSM systems [ZIS + 97]. However, weaker consistency models complicate introduce complexity in the software design. <p> Many variations thereof have since emerged such as Munin [CBZ91], TreadMarks <ref> [KDCZ93] </ref>, SoftFLASH [ENCH96], MGS [YKA96], and Cashmere [KHS + 97]. These systems all use standard virtual address translation mechanisms to implement coherence across nodes, and rely on relaxed memory consistency models and careful program annotation to support fine-grain sharing. SoftFLASH, MGS, and Cashmere address SMP-node implementations of VM-based software DSM.
Reference: [Ken92] <institution> Kendall Square Research. Kendall square research technical summary, </institution> <year> 1992. </year>
Reference-contexts: Tamir, et al., have proposed extensions to the format of the page table in order to fully support fine-grain access control [TJ92]. Cache Controller. The MIT Alewife [CKA91] and Kendall Square Research KSR-1 <ref> [Ken92] </ref> shared-memory systems use custom cache controllers to implement access control. In addition to detecting misses in hardware caches, these controllers determine when to invoke a protocol action. On Alewife, a local directory is consulted on misses to local physical addresses to determine if a protocol action is required.
Reference: [KHS + 97] <author> Leonidas Kontothanassis, Galen Hunt, Robert Stets, Nikolaos Hardavellas, Michal Cierniak, Srinivasan Parthasarathy, Jr. Wagner Meira, Sandhya Dwarkadas, and Michael Scott. </author> <title> VM-based shared memory on low-latency, remote-memory-access networks. </title> <booktitle> In Proceedings of the 24th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1997. </year>
Reference-contexts: Many variations thereof have since emerged such as Munin [CBZ91], TreadMarks [KDCZ93], SoftFLASH [ENCH96], MGS [YKA96], and Cashmere <ref> [KHS + 97] </ref>. These systems all use standard virtual address translation mechanisms to implement coherence across nodes, and rely on relaxed memory consistency models and careful program annotation to support fine-grain sharing. SoftFLASH, MGS, and Cashmere address SMP-node implementations of VM-based software DSM.
Reference: [kJC96] <author> Hsiao keng Jerry Chu. </author> <title> Zero-copy tcp in solaris. </title> <booktitle> In Proceedings of the 96 USENIX Conference, January 1996. </booktitle> <address> San Diego, CA. </address> <month> 199 </month>
Reference-contexts: Studies have shown that even today, network protocols spend a significant amount of time simply copying data [Ste94]. Therefore, many designs have attempted to avoid redundant copying at the application interface [DP93, RAC96, Wil92], the OS <ref> [kJC96] </ref>, and the network interface [OZH + 96, DWB + 93, LC95, BCM94]. <p> A class of designs supports minimal messaging by using the CPU in kernel mode to instruct the NI to move the data to the appropriate place. Such approaches include page remapping in the kernel (implemented in Solaris 2.6 TCP <ref> [kJC96] </ref>), Washingtons inkernel emulation of the remote memory access model [TLL94] and other VM manipulations [BS96]. In these systems, minimum messaging is achieved if the NI can directly access the main memory. However, the kernel is involved in every transfer and thus, user-level messaging is not supported.
Reference: [KMRS88] <author> Anna R. Karlin, Mark S. Manasse, Larry Rudolph, and Daniel D. Sleator. </author> <title> Competitive snoopy caching. </title> <journal> Algorithmica, </journal> <volume> (3):79119, </volume> <year> 1988. </year>
Reference-contexts: The list includes older systems like DASH, KSR-1, and S3.mp as well as newer systems like Convex Exemplar and SGI Origin. Dedicated hardware provides high performance for a single protocol. While custom hardware performs an action quickly, research has shown that no single protocol is optimal for all applications <ref> [KMRS88] </ref> or even for all data structures within an application [BCZ90,FLR + 94]. Hybrid hardware/software protocols such as Alewifes LimitLESS [CKA91], Dir 1 SW [HLRW93], and Typhoon-2 [Rei96] implement the expected common cases in hardware and trap to system software to handle complex, infrequent events.
Reference: [KS96] <author> Magnus Karlsson and Per Stenstrom. </author> <title> Performance evaluation of a cluster-based multiprocessor build from ATM switches and bus-based multiprocessor servers. </title> <booktitle> In Proceedings of the Second IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference: [K + 94] <author> Jeffrey Kuskin et al. </author> <title> The stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 302313, </pages> <month> April </month> <year> 1994. </year>
Reference: [KT87] <author> R. Koo and S. Toueg. </author> <title> Checkpointing and rollback-recovery for distributed systems. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> 13(1):2331, </volume> <month> January </month> <year> 1987. </year>
Reference: [Lab97] <institution> Ames Scalable Computing Laboratory. Microsoft Wolfpack references. </institution> <note> http:// www.scl.ameslab.gov/workshops/wolfpack.html, February 1997. </note>
Reference-contexts: The situation is likely to improve as commodity operating systems timidly start introducing clustering features (Microsoft Wolf-pack <ref> [Lab97] </ref>, Sun Solaris MC [KBM + 96]). Many issues need to be addressed, however, before the integration becomes seamless at the cluster level. In this section, I discuss issues relevant to FGDSM systems. Relaxing the single user assumption, represents a conceptual change of focus in the examination of the system.
Reference: [LB94] <author> James R. Larus and Thomas Ball. </author> <title> Rewriting executable files to measure program behavior. </title> <journal> Software Practice & Experience, </journal> <volume> 24(2):197218, </volume> <month> February </month> <year> 1994. </year>
Reference-contexts: Chapter 2 investigates fine-grain access control implementations on commodity workstations. It presents four different fine-grain tag implementations with different performance characteristics. The first, BlizzardS, adds a fast lookup before each shared-memory reference [LW94] by modifying the programs executable <ref> [LB94] </ref>. The second, Blizzard-E, uses the memorys error-correcting code (ECC) bits as valid bits and the page protection to emulate read-only bits [RFW93]. The third, Blizzard-ES, combines the two techniques using software lookups for stores and ECC for loads [SFL + 94]. <p> In that system, it required 15 instructions (18 cycles on the CM-5 SPARC processors) before every load and store instruction [SFL + 94]. The tool for inserting the checks originated from the vt tool, which in turn originated from qpt, a general purpose profiling tool <ref> [LB94] </ref>. The vt tool was used in the Wisconsin WindTunnel (WWT) simulator [RHL + 93] to insert instructions that tracked the simulated time in target executables. It was not a sophisticated tool since it could only process the executable on a basic block basis.
Reference: [LC95] <author> Lok T. Liu and David E. Culler. </author> <title> Evaluation of the Intel Paragon on active message communication. </title> <booktitle> In Proceedings of Intel Supercomputer Users Group Conference, </booktitle> <year> 1995. </year>
Reference-contexts: Studies have shown that even today, network protocols spend a significant amount of time simply copying data [Ste94]. Therefore, many designs have attempted to avoid redundant copying at the application interface [DP93, RAC96, Wil92], the OS [kJC96], and the network interface <ref> [OZH + 96, DWB + 93, LC95, BCM94] </ref>.
Reference: [LC96] <author> Tom Lovett and Russell Clapp. STiNG: </author> <title> A CC-NUMA computer for the commercial marketplace. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Most often, the system logic implements the distributed shared memory (DSM) paradigm that provides a shared-memory abstraction over the physically distributed memory in a parallel 3 machine. Many recent commercial systems such as Sequent STiNG <ref> [LC96] </ref> and SGI Origin [LL97] follow this approach. However, the associated fixed design costs lead to high premiums charged for these shared memory platforms. A less costly way to realize the performance potential of parallel processing is to connect commodity microprocessors using a custom low-latency network. <p> When a processor supports a bus-based coherence scheme, a separate bussnooping agent can perform a lookup similar to that performed by a memory controller. Stanford DASH [LLG + 92] and Typhoon [RLW94] among experimental designs, employ this approach. Many recent commercial shared memory machines, such as Sequent STiNG <ref> [LC96] </ref> and SGI Origin [LL97] also follow this approach. 19 2.1.2 Protocol Action Custom Hardware. High performance shared memory systems use dedicated hardware to execute the protocol actions. The list includes older systems like DASH, KSR-1, and S3.mp as well as newer systems like Convex Exemplar and SGI Origin.
Reference: [LH89] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4):321359, </volume> <month> November </month> <year> 1989. </year>
Reference-contexts: Blizzard relies mainly on commodity software and hardware to provide fine-grain access control. However, simple custom hardware support for the access test can be cost-effective and therefore, it is also considered. Previous approaches to shared memory on networks of workstations mainly targeted page-based distributed shared memory (DSM) systems <ref> [LH89] </ref>. Such systems required relaxed memory consistency models, which complicate the programming model, to overcome the effects of false sharing and fragmentation and offer acceptable performance [KDCZ93]. <p> Compiler-inserted lookups can exploit application-level information. Orca [BTK90], for example, provides access control on program objects instead of blocks. Translation Lookahead Buffer (TLB). Standard address translation hardware provides access control, though at memory page granularity. Nevertheless, it forms the basis of several distributed-shared-memory systems such as IVY <ref> [LH89] </ref>, Munin [CBZ91] and TreadMarks [KDCZ93]. Blizzard/E partially relies on the TLB to emulate fine-grain access control (Section 2.3.2). Though unimplemented by current commodity processors, additional, 18 per-block access bits in a TLB entry could provide fine-grain access control. <p> Instead, they rely on the messaging subsystem to implicitly limit the injection rate. Request protocols generate few concurrent requests which most of the time have small buffer requirements. Such protocols are limited by design on how many messages can inject into the network. Invalidate-based shared memory coherence protocols <ref> [LH89, RLW94, SGT96] </ref> including Stache, belong in this category. Shared memory programs using Stache can inject at most as many messages as the number of block access faults that can occur concurrently. This number is limited by the number of threads in the program. <p> Buffer Requirements Per Graph Concurrent Activation Graphs Few Many Small Invalidate DSM coherence protocols <ref> [LH89, RLW94, SGT96] </ref> Update DSM coherence proto cols, applicationspecific proto cols [FLR + Large Dataflow protocols [CCBS95] 63 deadlocks [WGH + 97]. However, such strategy is not appropriate in a general platform like Blizzard, which it is targeted to allow the development of application specific protocols. <p> Latency-limited applications benefit from the hardware support and justify the premiums charged for such platforms. Previous software distributed shared memory (DSM) systems relied on virtual memory page protection hardware to detect and control shared references. Sequentially consistent page-based software DSM systems such as Ivy <ref> [LH89] </ref> exhibit latencies as much as an order of magnitude higher than Blizzard. However, they typically achieve higher remote memory bandwidth since messaging hardware is optimized for large messages. <p> Given the performance results in this section, unless the current trends change dual-processor nodes are likely to be more cost-effective as building blocks for FGDSM systems. 5.7 Related Work Virtual-memory-based software distributed shared memory was originally proposed by Li and Hudak <ref> [LH89] </ref>. Many variations thereof have since emerged such as Munin [CBZ91], TreadMarks [KDCZ93], SoftFLASH [ENCH96], MGS [YKA96], and Cashmere [KHS + 97].
Reference: [LL97] <author> James Laudon and Daniel Lenoski. </author> <title> The SGI Origin: A ccNUMA highly scalable server. </title> <booktitle> In Proceedings of the 24th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1997. </year>
Reference-contexts: Most often, the system logic implements the distributed shared memory (DSM) paradigm that provides a shared-memory abstraction over the physically distributed memory in a parallel 3 machine. Many recent commercial systems such as Sequent STiNG [LC96] and SGI Origin <ref> [LL97] </ref> follow this approach. However, the associated fixed design costs lead to high premiums charged for these shared memory platforms. A less costly way to realize the performance potential of parallel processing is to connect commodity microprocessors using a custom low-latency network. <p> Stanford DASH [LLG + 92] and Typhoon [RLW94] among experimental designs, employ this approach. Many recent commercial shared memory machines, such as Sequent STiNG [LC96] and SGI Origin <ref> [LL97] </ref> also follow this approach. 19 2.1.2 Protocol Action Custom Hardware. High performance shared memory systems use dedicated hardware to execute the protocol actions. The list includes older systems like DASH, KSR-1, and S3.mp as well as newer systems like Convex Exemplar and SGI Origin. <p> We are now able to put all the pieces together and examine the complete picture. FGDSM systems [SFL + + 96,SGT96] on networks of workstations carve a middle path between hardware and page-based software shared memory approaches. On one hand, their performance characteristics are worse than hardware shared memory <ref> [LL97] </ref>, albeit with little or no hardware support and presumably at a lower cost point. <p> Instead, it should compared to other shared memory approaches. Hardware shared memory systems offer excellent performance but require additional hardware. In loosely-coupled hardware distributed shared memory (DSM) systems such as Sequents STiNG or Convex Exemplar X, remote memory references are five to ten times slower than local ones <ref> [Cor94,LC96, LL97] </ref>. More tightly-coupled DSM systems such as SGIs Origin [LL97], exhibit even better performance characteristics. Remote memory references are only two times slower than local ones. <p> Hardware shared memory systems offer excellent performance but require additional hardware. In loosely-coupled hardware distributed shared memory (DSM) systems such as Sequents STiNG or Convex Exemplar X, remote memory references are five to ten times slower than local ones [Cor94,LC96, LL97]. More tightly-coupled DSM systems such as SGIs Origin <ref> [LL97] </ref>, exhibit even better performance characteristics. Remote memory references are only two times slower than local ones. Symmetric multiprocessors with aggressive interconnection networks such as Suns ES 6000, support a UMA model where no distinction exists between local and remote memory references. Clearly, Blizzard does not belong in this category.
Reference: [LLG + 92] <author> Daniel Lenoski, James Laudon, Kourosh Gharachorloo, Wolf-Dietrich Weber, Anoop Gupta, John Hennessy, Mark Horowitz, and Monica Lam. </author> <title> The stanford DASH multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3):6379, </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: When a processor supports a bus-based coherence scheme, a separate bussnooping agent can perform a lookup similar to that performed by a memory controller. Stanford DASH <ref> [LLG + 92] </ref> and Typhoon [RLW94] among experimental designs, employ this approach. Many recent commercial shared memory machines, such as Sequent STiNG [LC96] and SGI Origin [LL97] also follow this approach. 19 2.1.2 Protocol Action Custom Hardware. High performance shared memory systems use dedicated hardware to execute the protocol actions.
Reference: [LRV94] <author> James R. Larus, Brad Richards, and Guhan Viswanathan. </author> <title> LCM: Memory system support for parallel language implementation. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, pages 208218, </booktitle> <address> San Jose, California, </address> <year> 1994. </year> <month> 200 </month>
Reference: [LS95] <author> James R. Larus and Eric Schnarr. Eel: </author> <title> Machine-independent executable editing. </title> <booktitle> In Proceedings of the SIGPLAN 95 Conference on Programming Language Design and Implementation (PLDI), </booktitle> <pages> pages 291300, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: These designs cover a substantial range of the design space for fine-grain access control mechanisms [SFL + 94]. Blizzard/S uses a software method based on locally available executable-editing technology <ref> [LS95] </ref>. Blizzard/E uses commodity hardware to accelerate the access check. The memory controller supports Invalid blocks while the TLB is used to emulate ReadOnly blocks. Blizzard/ES also uses the memory controller for Invalid blocks but relies on the same software method as Blizzard/S for ReadOnly blocks. <p> In this way, condition codes were not modified. A similar access check sequence is still used today when better alternatives cannot be applied (Figure 2-1 (c)). To address these problems, Eric Schnarr developed a new binary rewriting tool [SFH + based on the EEL executable rewriting library <ref> [LS95] </ref>. Besides software access control, I have been using this tool for rewriting Blizzard executables for other purposes as well (e.g., implicit network polling; see Chapter 3). The EEL library provides extensive infrastructure to analyze and modify executables. <p> First, it requires some effort to implement the kernel code. Second, very early low-cost polling-based alternatives for transparent message notification were introduced in Blizzard. 74 3.5.2 Polling Message Notification Alternatives Blizzard resorts to two forms of implicit polling. One approach relies on executable editing using the EEL tool <ref> [LS95] </ref> to insert explicit polling code on backwards loop edges and potentially recursive procedure calls. This code (listed in Figure 3-8) reads a location to test if a message arrived. If the conditions codes are not live, which is the common case, the inserted sequence takes seven instructions. <p> Blizzard/T bypasses tag lookups by writing to uncached aliased page mappings to memory. Blizzard/S: Software Tags Blizzard/S implements the tags in software by storing them in memory. Using executable editing <ref> [LS95] </ref>, Blizzard/S inserts access control tests around shared-memory loads and stores in a fully compiled and linked program. Blizzard/S uses two forms of tests to detect access violations. Invalid memory is marked with a sentinel value that has a low probability of occurring in the program [SFH + 96,SGT96]. <p> Blizzard supports four different fine-grain tag implementations. These designs cover a substantial range of the design space for fine-grain access control mechanisms [SFL + 94]. Blizzard/S uses a software method based on executable-editing technology locally available <ref> [LS95] </ref>. Blizzard/E uses commodity hardware to accelerate the access check. The memory controller supports Invalid blocks while the TLB is used to emulate ReadOnly blocks. Blizzard/ES also uses the memory controller for Invalid blocks but relies on the same software method as Blizzard/S for ReadOnly blocks. <p> Three separate directories, blzsysm, eccmemd, and vortexd, contain the source for the BlzSysM, EccMemD and VortexD modules, respectively. The binary rewriting tool [SFH + 96] used to insert the software checks for Blizzard/S is based on the EEL executable rewriting library <ref> [LS95] </ref>. The EEL library provides extensive infrastructure to analyze and modify executables. Besides software access control, I have been using this tool for rewriting Blizzard executables for other purposes as well (e.g., implicit network polling). The source files for the binary rewriting tool are located under the directory $(WWT_ROOT)/tools/blizzard_s2.
Reference: [LW94] <author> Alvin R. Lebeck and David A. Wood. Fast-cache: </author> <title> A new abstraction for memory system simulation. </title> <type> Technical Report 1211, </type> <institution> Computer Sciences Department, University of WisconsinMadison, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: A brief summary of the main points in each chapter of the thesis follows. Chapter 2 investigates fine-grain access control implementations on commodity workstations. It presents four different fine-grain tag implementations with different performance characteristics. The first, BlizzardS, adds a fast lookup before each shared-memory reference <ref> [LW94] </ref> by modifying the programs executable [LB94]. The second, Blizzard-E, uses the memorys error-correcting code (ECC) bits as valid bits and the page protection to emulate read-only bits [RFW93]. The third, Blizzard-ES, combines the two techniques using software lookups for stores and ECC for loads [SFL + 94]. <p> These tests use an instructions effective address to index a table. The table contains the access control information (a byte per cache block) that determines if the memory reference should invoke a handler. This technique descends from the Fast-Cache technique used in the simulation of memory systems <ref> [LW94] </ref>. It was first introduced in Blizzard/CM-5. In that system, it required 15 instructions (18 cycles on the CM-5 SPARC processors) before every load and store instruction [SFL + 94].
Reference: [Mar94] <author> Richard P. Martin. Hpam: </author> <title> An active message layer for a network of HP workstations. In Hot Interconnects 94, </title> <year> 1994. </year>
Reference-contexts: With pure request/reply protocols, the graph for each injected message contains exactly two messages, the request and the reply. Therefore, each message exchange requires exactly one buffer at each end of the communication. The advantage of this constraint is that it allows a very simple buffer allocation policy <ref> [Mar94] </ref> based on a simple credit-based flow-control scheme with distinct buffer pools for each sender. Half of the available buffers on each node are allocated for requests and half for replies. Then, the system implicitly preallocates a buffer for the reply when a request is being sent.
Reference: [MC95] <author> Alan Mainwaring and David Culler. </author> <title> Active Messages: </title> <booktitle> Organization and Applications Programming Interface, </booktitle> <year> 1995. </year>
Reference-contexts: However, it is possible to extend this functionality to multiple parallel programs by replicating the queue structures while using virtual memory hardware to enforce protection. An equivalent model is described in the last Active Message specification <ref> [MC95] </ref>. 3.5 Transparent Message Notification To reduce round-trip latencies, a node must quickly respond to arriving messages. Conceptually, an arriving message causes an interrupt, which invokes a message handler. <p> Thereafter, the NI must enforce the access rights when it sends or receives messages. Moreover, extra care must be taken to break the remote connection when a process is destroyed. An equivalent model is described in detail in Berkeleys Active Message specification <ref> [MC95] </ref> or Intels Virtual Interface Architecture [DR97]. 6.2 Address Translation Properties For Minimal Messaging In this section, I present the properties that the address translation mechanism should satisfy and I argue why I consider them desirable for minimal messaging. <p> Thereafter, the network interface must enforce the access rights when it sends or receives messages. Such models have been defined in detail within the context of Berkeleys Active Message specification <ref> [MC95] </ref> and Intels Virtual Interface Architecture [DR97]. Memory Allocation. Shared memory coherence protocols such as Stache that implement a simple cache-only memory architecture (S-COMA) in software, pose an interesting dilemma on what to do when shared memory becomes overcommitted on a particular node. In particular, there are two alternatives.
Reference: [MCS91] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Algorithms for scalable synchronization on shared-memory multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1):2165, </volume> <month> February </month> <year> 1991. </year>
Reference-contexts: The clustering benefit, direct and indirect, is not enough to avert an increase in the per node protocol traffic as the clustering degree increases. Unlike Chapter 4, in these runs barnes uses MCS locks <ref> [MCS91] </ref> instead of the default Blizzard message locks, which have not designed to deal with multiprocessor nodes. The message locks are distributed among the nodes. Each lock or unlock request is implemented by sending an explicit message to the node that the lock resides. <p> If however, no other nodes were involved only two messages need to be exchanged. The util protocol library implements synchronization primitives, typically used in PAR-MACS applications. The primitives supported include locks and barriers. The user can choose at compile time the lock implementation between message and MCS <ref> [MCS91] </ref> locks. In message locks, the default lock implementation, the locks are distributed in a round-robin fashion among the nodes. Then, each lock or unlock request is implemented by sending an explicit message to the node that the lock resides. MCS locks are shared memory-based locks.
Reference: [MFHW96] <author> Shubhendu S. Mukherjee, Babak Falsafi, Mark D. Hill, and David A. Wood. </author> <title> Coherent network interfaces for fine-grain communication. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 247258, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: However, the extra copies this entails can potentially degrade performance <ref> [MFHW96] </ref>. Blizzard, instead, uses a conservative deadlock detection scheme, which only buffers messages when a deadlock may have occurred.
Reference: [MGBK96] <author> Christine Morin, Alain Gefflaut, Michel Banatre, and Anne-Marie Kermarrec. </author> <title> Coma: an opportunity for building fault-tolerant scalable shared memory multiprocessors. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference: [Mic91] <author> Sun MicroSystems. </author> <title> Sun-4M System Architecture, </title> <journal> Rev 2.0, </journal> <year> 1991. </year>
Reference-contexts: This can slowdown message processing. For these reasons, it has not been implemented in Blizzard. 3.4 COW Communication Infrastructure COW nodes (Figure 3-5) are equipped with a Myrinet NI that connects them through Myri-net switches [BCF + 95]. The NI is located on the I/O bus (SBUS <ref> [Mic91] </ref>), which supports processor coherent I/O transfers between the memory and I/O devices. The bus bridge supports an I/O memory management unit (IOMMU) which provides translation from SBUS addresses to physical addresses. The NI is controlled by a 5 MIPS 16-bit processor. <p> Therefore, including the appropriate support in commodity operating systems requires significant commitment from the OS groups for a specific platform. For example, in Solaris 2.4, the only device (other than processors) for which the kernel supports page tables in the Sun-4M architecture <ref> [Mic91] </ref>, is the standard SX graphics controller [Mic94]. <p> In COW, the NI is located on the I/O bus (SBUS <ref> [Mic91] </ref>) which supports processor coherent I/O transfers between the memory and I/O devices. The bus bridge supports an I/O memory management unit (IOMMU) which provides translation from SBUS addresses to physical addresses.
Reference: [Mic94] <author> Sun MicroSystems. </author> <title> Kodiak SX Memory Controller Specification, </title> <year> 1994. </year>
Reference-contexts: Therefore, including the appropriate support in commodity operating systems requires significant commitment from the OS groups for a specific platform. For example, in Solaris 2.4, the only device (other than processors) for which the kernel supports page tables in the Sun-4M architecture [Mic91], is the standard SX graphics controller <ref> [Mic94] </ref>.
Reference: [MK96] <author> Evangelos P. Markatos and Manolis G.H. Katevenis. Telegraphos: </author> <title> High-performance networking for parallel processing on workstation clusters. </title> <booktitle> In Proceedings of the Second IEEE Symposium on High-Performance Computer Architecture, </booktitle> <year> 1996. </year>
Reference-contexts: Memory accesses on the sender are captured by the NI and forwarded to the associated page on the receiver. Page associations are either direct (the sender knows the remote physical address) or 163 indirect (through global network addresses). Examples of this approach include Princeton SHRIMP [BDFL96], Forth Telegraphos II <ref> [MK96] </ref>, DEC Memory Channel [GCP96] and Tandem TNet [Hor95]. SHRIMP and Telegraphos II use direct page associations. The Memory Channel and TNet use indirect page associations. Common characteristic of these designs is their inability to handle misses in the translation structures.
Reference: [MKAK94] <author> Kenneth Mackenzie, John Kubiatowicz, Anant Agarwal, and Frans Kaashoek. Fugu: </author> <title> Implementing translation and protection in a multiuser, multimodel multiprocessor. </title> <type> Technical Memo MIT/LCS/TM-503, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> Octo-ber </month> <year> 1994. </year>
Reference: [Mog91] <author> Jeff Mogul. </author> <title> Network locality at the scale of processes. </title> <booktitle> In Proceedings of the ACM SIGCOMM 91 Conference, </booktitle> <month> September </month> <year> 1991. </year> <institution> Zurich. </institution>
Reference-contexts: In any translation scheme we would like to take advantage of potential locality by keeping recent mappings handy for future uses. Applications exhibit temporal and spatial locality at various levels, which include their network behavior <ref> [Mog91] </ref>. This locality will be reflected in the application data addresses from which the applications send or receive messages. The address translation mechanism should take advantage of this behavior when it exists. (D) Degrade gracefully when system limits are exceeded.
Reference: [MP97] <author> Christine Morin and Isabelle Puaut. </author> <title> A survey of recoverable distributed shared virtual memory systems. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <month> September </month> <year> 1997. </year> <month> 201 </month>
Reference: [MRF + 97] <author> Shubhendu S. Mukherjee, Steven K. Reinhardt, Babak Falsafi, Mike Litzkow, Steve Huss-Lederman, Mark D. Hill, James R. Larus, and David A. Wood. </author> <title> Wisconsin Wind Tunnel II: A fast and portable parallel architecture simulator. In Workshop on Performance Analysis and Its Impact on Design (PAID), </title> <month> June </month> <year> 1997. </year>
Reference: [MSH + 95] <author> Shubhendu S. Mukherjee, Shamik D. Sharma, Mark D. Hill, James R. Larus, Anne Rogers, and Joel Saltz. </author> <title> Efficient support for irregular applications on distributed-memory machines. </title> <booktitle> In Fifth ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 6879, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: An interaction list (rebuilt every 20 iterations) limits interaction with molecules within a cutoff radius. We examine both DSM and CS versions. The TSM version makes effective use of shared memory <ref> [MSH + 95] </ref>. The programs reduction phase performs operations on large arrays with minimum locking. However, in this phase, each processor accesses a large portion of the shared memory, which limits Blizzards speedups. The CS version used explicit messages to avoid cache misses in the reduction phase.
Reference: [MVCA97] <author> Richard P. Martin, Annin M. Vadhat, David E. Culler, and Thomas E. Anderson. </author> <title> Effects of communication latency, overhead, and bandwidth in a cluster architecture. </title> <booktitle> In Proceedings of the 24th Annual International Symposium on Computer Architecture, </booktitle> <year> 1997. </year>
Reference: [Nik94] <author> Rishiyur S. Nikhil. </author> <title> A multithreaded implementation of Id using P-RISC graphs. </title> <booktitle> In Languages and Compilers for Parallel Computing (Proceedings of the Sixth Interna-tiona Workshop), </booktitle> <pages> pages 390405. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference: [NIM93] <author> NIMBUS Technology. </author> <title> NIM 6133 memory controller specification. </title> <type> Technical report, </type> <institution> NIMBUS Technology, </institution> <year> 1993. </year>
Reference: [NMP + 93] <author> A. Nowatzyk, M. Monger, M. Parkin, E. Kelly, M. Borwne, G. Aybay, and D. Lee. S3.mp: </author> <title> A multiprocessor in a matchbox. </title> <booktitle> In Proc. </booktitle> <address> PASA, </address> <year> 1993. </year>
Reference-contexts: Memory Controller. If the system can guarantee that the processors hardware caches never contain Invalid blocks and that ReadOnly blocks are cached in a read-only state, the memory controller can perform the lookup on hardware cache misses. This approach is used by Suns S3.mp <ref> [NMP + 93] </ref>, NIMBUSs NIM 6133 [NIM93,SFL + 94], and Stanfords FLASH [K 94]. S3.mp has a custom memory controller that performs a hardware lookup at every bus request. NIM 6133 integrates fine-grain access control with memorys error correction code.
Reference: [OG90] <author> R. R. Oehler and R. D. Groves. </author> <title> IBM RISC system/6000 processor architecture. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 34(1):3236, </volume> <month> January </month> <year> 1990. </year>
Reference-contexts: Blizzard/E partially relies on the TLB to emulate fine-grain access control (Section 2.3.2). Though unimplemented by current commodity processors, additional, 18 per-block access bits in a TLB entry could provide fine-grain access control. The lock bits in certain IBM RISC machines, including the 801 [CM88] and RS/6000 <ref> [OG90] </ref>, provide access control on 128-byte blocks. Unfortunately, these bits can only partially support fine-grain access control since they do not provide the ReadOnly state. Tamir, et al., have proposed extensions to the format of the page table in order to fully support fine-grain access control [TJ92]. Cache Controller.
Reference: [Ope92] <institution> Open Software Foundation and Carnegie Mellon University. </institution> <note> Mach 3 Kernel Interfaces, </note> <month> November </month> <year> 1992. </year>
Reference-contexts: For each segment, Solaris maintains a pointer to a set of functions, which are used to manage that segment. The set of functions is known as a segment driver. The segment driver interface is quite similar to Mach 3.0s well known external pager interface <ref> [Ope92] </ref>. Unlike Mach 3.0s interface however, it is a strictly internal interface and it does not support external pager processes. The segment drivers maintain their own data structures for each segment.
Reference: [Osb94] <author> Randy Osborne. </author> <title> A hybrid deposit model for low overhead communication in high speed lans. </title> <booktitle> In Fourth International Workshop on Protocols for High Speed Networks, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: DART core has been designed to support ADCs including sophisticated address translation support. Moreover, it defines an interface to a separate coprocessor that process messages. Presumably, it will be used to support the hybrid deposit model <ref> [Osb94] </ref>, an abstraction similar to Active Messages, in which the data destination is a function of the receivers and t senders state. Unfortunately, the address translation is geared towards ADCs. As a result, the translation structures are not flexible enough to efficiently support minimal messaging.
Reference: [OSS80] <author> John K. Ousterhout, Donald A. Scelza, and Pradeep S. Sindhu. </author> <title> Medusa: An experiment in distributed operating system structure. </title> <journal> Communications of the ACM, </journal> <volume> 23(2):92 105, </volume> <month> February </month> <year> 1980. </year>
Reference-contexts: All the nodes and the network are concurrently changing context. In effect, for the duration of a quantum, the user has complete possession of the cluster resources. The importance of coscheduling has been noted very early in the history of such systems <ref> [OSS80] </ref>. The same result has been demonstrated in the context of shared memory parallel programs [BHMW94] executing scientific codes. Gang scheduling, however, is a very restrictive model. Moreover, it is very difficult to strictly implement it in a distributed environment without extensive modifications in the kernel philosophy.
Reference: [OZH + 96] <author> Randy Osborne, Qin Zheng, John Howard, Ross Casley, and Doug Hahn. </author> <title> DART - a low overhead ATM network interface chip. In Hot Interconnects, </title> <year> 1996. </year>
Reference-contexts: Examples of such designs include the Arizona Application Device Channels (ADCs) [DPD94], Cornell U-Net [vEBBV95], HP Hamlyn [Wil92], Princeton SHRIMP [BDFL96]. The result of such recent research has been commercial designs like Myricom Myrinet [BCF + 95], Fore 200 [CMSB91], Mitsubishi DART <ref> [OZH + 96] </ref>. With the OS removed from the critical path, the memory subsystem emerges as a major hurdle for delivering the network performance to the application. <p> Studies have shown that even today, network protocols spend a significant amount of time simply copying data [Ste94]. Therefore, many designs have attempted to avoid redundant copying at the application interface [DP93, RAC96, Wil92], the OS [kJC96], and the network interface <ref> [OZH + 96, DWB + 93, LC95, BCM94] </ref>. <p> The key characteristic of this design is that it supports only the NI translation structures and not special device tables in main memory. NIs with microcontrollers can implement the lookup with software TLBs [HGDG94]. Some designs however, include hardware support for the lookup <ref> [OZH + 96] </ref> in the form of custom finite state machines for message processing. Whenever a miss occurs, the device triggers an interrupt invoking the device drivers interrupt handler, which handles the misses. <p> Recent work [BWvE97] attempted to incorporate address translation mechanisms for existing NIs. Nevertheless, the ADC abstraction has not changed and therefore, the designs are unable to move data to their final destination without extra copying. Mitsubishi DART <ref> [OZH + 96] </ref> is a commercially available NI that comes close to properly support minimal messaging. DART core has been designed to support ADCs including sophisticated address translation support. Moreover, it defines an interface to a separate coprocessor that process messages.
Reference: [PC90] <author> Gregory M. Papadopoulos and David E. Culler. Monsoon: </author> <title> An explicit token store architecture. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 8291, </pages> <month> May </month> <year> 1990. </year> <month> 202 </month>
Reference-contexts: Among the key proposals that emerged from the multicomputer community have been the Berkeley active messages. The design has been heavily influenced by earlier work in message-directed computation in the context of dataflow architectures and the J-machine <ref> [DCF 89, PC90] </ref>. Berkeley active messages sought to reduce latencies by eliminating the software complexity associated with traditional multicomputer messaging interfaces. Tempests 52 messaging interface is based on the Berkeley active messages [vECGS92].
Reference: [PD97] <author> Larry L. Peterson and Bruce S. Davie. </author> <title> Computer Networks: A Systems Perspective. </title> <publisher> Prentice Hall, </publisher> <year> 1997. </year>
Reference-contexts: channel. int TPPI_set_channel_src (dest, send completion function) void TPPI_set_channel_dst (src, channel, buffer address, buffer length, receive completion function) void TPPI_channel_send (dest, channel, buffer address, buffer length) void TPPI_destroy_channel_src (dest, channel) void TPPI_destroy_channel_dst (src, channel) void TPPI_reset_channel_dst (dest, channel) void TPPI_reset_channel_src (src, channel) 57 Traditional network abstractions such as UDP/IP sockets <ref> [PD97] </ref> address the buffer allocation problem by simply dropping messages when the network buffers fill up. These protocols rely on the existence of the standard mechanisms in place for error detection and retransmission to recover from such events. <p> Such protocols use credit-based flow control schemes to pipeline the injection of new messages with the receipt of acknowledg 64 ments. These schemes are very similar to flow control schemes originally used in link-level flow control protocols for virtual circuits <ref> [PD97] </ref>. The only exception to this rule is return-to-sender which uses shared buffers. 3.3.1 Pure Request/Reply Berkeley active messages [vECGS92] dictate that the application should be limited to pure request/reply message operations.
Reference: [Pfi95] <author> Robert W. Pfile. </author> <title> Typhoon-Zero implementation: The Vortex module. </title> <type> Technical report, </type> <institution> Computer Sciences Department, University of WisconsinMadison, </institution> <year> 1995. </year>
Reference-contexts: Under Solaris 2.4, the I/O MMU directly maps kernel virtual addresses to Sbus virtual addresses, which limits DMA operations to the kernel address space. The operating system of the COW nodes is Solaris 2.4. Each COW node contains a custom-built Vortex card <ref> [Pfi95] </ref> and a Myrinet network interface [BCF + 95]. The Vortex card plugs into the MBus and performs fine-grain access control by snooping bus transactions. Each node also contains a Myrinet interface, which consists of a slow (78 MIPS) custom processor (LANai -2) and 128 KBytes of memory. <p> The third, Blizzard-ES, combines the two techniques using software lookups for stores and ECC for loads [SFL + 94]. The fourth, Blizzard/T, uses Vortex, a custom fine-grain access control accelerator board. The board was originally developed by members of the Wisconsin WindTunnel project <ref> [Pfi95] </ref> to demonstrate the simplest possible hardware Tempest implementation (Typhoon-0 [RPW96]). Blizzard/T explores the Vortexs ability to support hardware fine-grain tags modified without kernel intervention but unlike Typhoon-0, it does not use any of its other abilities to accelerate Tempest operations (e.g., mechanisms to accelerate dispatch of access fault handlers). <p> The memory controller supports Invalid blocks while the TLB is used to emulate ReadOnly blocks. Blizzard/ES also uses the memory controller for Invalid blocks but relies on the same software method as Blizzard/S for ReadOnly blocks. Finally, Blizzard/T uses Vortex, a custom board <ref> [Pfi95] </ref> to accelerate many fine-grain access control operations. We shall subsequently review alternative techniques to implement the access check (Section 2.1.1) and alternative locations to execute the protocol actions (Section 2.1.2). 2.1.1 Access Check Software. <p> In this section, we will examine the various implementations of fine-grain tags in Blizzard and their support for SMP nodes. Blizzard/T: Custom Board SRAM Tags Blizzard/T uses Vortex <ref> [Pfi95] </ref>, a custom board that snoops memory bus transactions, performs fine-grain access control tests through a SRAM lookup, and coordinates intra-node communication. The board contains two FPGAs, two SRAM, and some minor logic [RPW96]. <p> The cache-coherent 50 Mhz MBus connects the processors and memory. I/O devices sit on the 25 Mhz SBus, which is connected through a bridge to the MBus. Each COW node contains a Vortex card <ref> [Pfi95] </ref> and a Myrinet network interface [BCF + 95]. The Vortex card plugs into the MBus and performs fine-grain access control by snooping bus transactions. If the Vortex card is not present, two additional HyperSPARC processors can take its place, raising the total number of processors per node to four. <p> The memory controller supports Invalid blocks while the TLB is used to emulate ReadOnly blocks. Blizzard/ES also uses the memory controller for Invalid blocks but relies on the same software method as Blizzard/S for ReadOnly blocks. Finally, Blizzard/T uses Vortex, a custom board <ref> [Pfi95] </ref> to accelerate fine-grain many access control operations. Figure A-1. Resources in a FGDSM system.
Reference: [PKC97] <author> Scott Pakin, Vijay Karamcheti, and Andrew A. Chien. </author> <title> Fast messages (FM): Efficient, portable communication for workstation clusters and massively-parallel processor. </title> <note> IEEE Concurrency (to appear), </note> <year> 1997. </year>
Reference-contexts: Therefore, it is not surprising that the FM library did not guarantee deadlock-free behavior when new requests where generated inside message handlers. Interestingly enough, this policy did not survive long in the FM library. In its next release <ref> [PKC97] </ref>, it has been replaced by a standard credit-based flow control scheme. While no extensive details for the new policy have been published, it appears to be a standard credit-based protocol. By itself such protocol, it is not sufficient to support new requests generated inside message handlers.
Reference: [PLC95] <author> Scott Pakin, Mario Laura, and Andrew Chien. </author> <title> High performance messaging on workstations: Illinois fast messages (FM) for Myrinet. </title> <booktitle> In Proceedings of Supercomputing 95, </booktitle> <year> 1995. </year>
Reference-contexts: This approach permits a program to communicate without kernel intervention and is supported in recent low-latency communication hardware [BCF 95,BLA 94, Hor95,OZH 96]. Fast access to the hardware by itself is not enough to achieve low latencies. As the experience from message-passing multicomputers has shown <ref> [vECGS92, PLC95, vEBBV95] </ref>, traditional messaging interfaces have not been able to realize the hardware performance due to fixed software overheads associated with sending and receiving messages. Therefore, a newer generation of low overhead messaging interfaces attacked the software overheads. <p> Section 3.6 describes implementation details of the Blizzard messaging subsystem. Section 3.7 presents microbenchmark results to characterize Blizzards messaging performance. Finally, Section 3.8 summarizes with the conclusions of this study. 1. During the early stages in Blizzards development, the Illinois Fast Messages (FM) library <ref> [PLC95] </ref> was used as the low-level communication infrastructure. We switched to the Berkeley library for two reasons. First, licensing restrictions made it very difficult to customize the FM library for Blizzard. Second, FMs performance was modestly worse than AMs for typical shared memory coherence protocols. <p> Consequently, it can serve as the benchmark against which we can judge more sophisticated policies. 3.3.2 Return-to-Sender This policy briefly appeared in the first release of the Illinois Fast Messages (FM) for Myri-net <ref> [PLC95] </ref>. It was inspired from the deflection and chaos routing policies of the Tera-1 machine [ACC + 90]. This policy uses a shared buffer pool for all senders. It is based in the idea that messages which cannot be delivered to the receiver will be routed back to the sender. <p> With Myrinet hardware, the host (SPARC) processor and Myrinet LANai processor cooperate to send and receive data. The division of labor is flexible since the LANai processor is programmable. However, the SPARC processor is far faster, which effectively limits the LANai processor to simple tasks <ref> [PLC95] </ref>. Blizzards communication library started from the LANai Control Program (LCP) used in Berkeleys LAM library [CLMY96]. The LCP was modified to fit the Tempest active message model and to improve performance for its expected usage. <p> In most cases, applications access the network through a layer of messaging abstractions. We can distinguish between low-level network access models and high-level user messaging models. Network access models such as ADCs [DPD94], U-Net [vEBBV95], Active Messages [vECGS92], Fast Messages <ref> [PLC95] </ref>, provide protected user access to the NI and serve as a consistent low-level model across NIs. Applications can use them to access the network but likely they will prefer higher level messaging models such as Fbufs [DP93], MPI [For94] or TCP/IP. <p> The division of labor is flexible since the LANai processor is programmable. However, the SPARC processor is far faster, which effectively limits the LANai processor to simple tasks <ref> [PLC95] </ref>. A Solaris device driver maps the Myrinet interfaces local memory into a programs address space where it can accessed with uncached memory operations. To permit DMA between the LANai and the user address space, the driver allocates a kernel buffer mapped into the user address space.
Reference: [RAC96] <author> Steven H. Rodrigues, Thomas E. Anderson, and David E. Culler. </author> <title> High performance local area communication with fast sockets. </title> <booktitle> In Proceedings of the 96 USENIX Conference, January 1996. </booktitle> <address> San Diego, CA. </address>
Reference-contexts: Studies have shown that even today, network protocols spend a significant amount of time simply copying data [Ste94]. Therefore, many designs have attempted to avoid redundant copying at the application interface <ref> [DP93, RAC96, Wil92] </ref>, the OS [kJC96], and the network interface [OZH + 96, DWB + 93, LC95, BCM94].
Reference: [Rei94] <author> Steven K. Reinhardt. </author> <title> Tempest Interface Specification, </title> <journal> Rev 1.1, </journal> <year> 1994. </year>
Reference-contexts: FGDSM systems rely on fine-grain access control to selectively restrict memory accesses to cache-block-sized memory regions. The thesis presents Blizzard, a family of FGDSM systems running on the Wisconsin Cluster Of Workstations. Blizzard supports the Tempest interface <ref> [Rei94] </ref> that implements shared memory coherence as user-level libraries. In this way, applicationspecific protocols can be developed to eliminate the overhead of fine-grain access control. This section is organized as follows. <p> Blizzard/COW 1 + 96], which is its direct descendant, was developed on the Wisconsin Cluster of Workstations (COW) and is the focus of this thesis. Digitals Shasta [SGT96] is another FGDSM system inspired by Blizzard/CM-5. Unlike Shasta, Blizzard uses the Tempest interface <ref> [Rei94] </ref> to support software distributed shared memory. Tempest separates the mechanisms required to implement distributed shared memory from the coherence protocols that enforce the shared memory semantics. <p> Stack Text Static Data Uninitialized Data Shared Data Access SIGSEGV Signal handler Enable fine-grain access control for page Change page protection to read-write Computation 9 1.2.3 Tempest Tempest is an interface that enables user-level software to control a machines memory management and communication facilities <ref> [Rei94] </ref>. Software can compose the primitive mechanisms to support either message-passing, shared-memory, or hybrid applications. <p> Blizzard provides shared memory and consequently, must implement these two operations. The design and implementation of Blizzards fine-grain access control mechanisms is the focus of this chapter. Blizzard supports the Tempest interface <ref> [Rei94] </ref>, which exposes fine-grain access control tags to user level, allowing shared memory coherence protocols to be implemented as user-level libraries. <p> Blizzard has been designed to be portable across a range of fine-grain tag implementations. Therefore, it is preferable to treat fine-grain access control as a memory attribute with a virtual name and a physical content. The approach followed in Blizzard deviates from the Tempest specification <ref> [Rei94] </ref>. Tempest, like Blizzard, associates the attribute name with the virtual address space and the attribute content with the physical address space. However, Tempest distinguishes among the memory mappings to the same physical page. One is considered the primary mapping. <p> In Chapter 2, we examined the cost of the access control mechanisms under different fine-grain tag implementations. In this chapter, we will focus on messaging. Blizzard implements the Tempest interface <ref> [Rei94] </ref>, which allows the implementation of shared memory coherence protocols as user-level libraries. The Tempest interface has been specifically designed to support fine-grain shared memory coherence protocols. Typically, such protocols are modeled after hardware-based invalidate directory protocols [AB86]. They transfer data in small, cache-block sized quantities. <p> Unique among existing FGDSM systems is Blizzards ability to eliminate the cost of shared memory and offer to the application the raw messaging performance. Blizzard supports the Tempest interface <ref> [Rei94] </ref> which exposes to user level the access control and messaging mechanisms. Thereby, Blizzard allows the development of applicationspecific coherence protocols [FLR + 94] that optimize the data transfers to match the application sharing patterns. <p> Both these systems achieve comparable speedups for shared memory applications despite hardware and software differ ences between the two systems [SGT96,SFH + 97]. Unique among FGDSM systems is Blizzards ability to harness the raw messaging performance and offer it to the application. The Tempest interface <ref> [Rei94] </ref> exposes the access control and messaging mechanisms to user level and therefore, allows the development of applicationspecific coherence protocol [FLR + 94]. Such protocols eliminate the access control overhead resulting in lower latencies for small data transfers. <p> Moreover, it serves as an example of including minimal support in other messaging interfaces. The section first discusses general issues regarding the introduction of minimal messaging in an existing messaging interface and subsequently focuses on the interaction of minimal messaging with fine-grain access control. Blizzard supports Tempests <ref> [Rei94] </ref> messaging interface. The Tempest messaging interface implements an active message model where upon a message arrival a message handler is invoked. It supports small and large messages. Small messages transfer few words. <p> Section A.1 presents general information on the COW environment and in particular, it describes the hardware platform. Section A.2 presents an overview of the tools available on COW to launch and control parallel applications. Section A.3 briefly summarizes the key characteristics of the Tempest interface <ref> [Rei94] </ref>. Finally, Section A.4 presents an overview of Blizzard, discusses the default protocol libraries offered by Blizzard, and elaborates on the implementation of the Tempest primitives in Blizzard. A.1 Wisconsin Cluster Of Workstations (COW) Wisconsin COW consists of 40 dual-processor Sun SPARCStation 20s. <p> The synchronization is done through the first node in the partition. A.3 Tempest Interface Tempest is an interface that enables user-level software to control a machines memory management and communication facilities <ref> [Rei94] </ref>. Software can compose the primitive mechanisms to support either message-passing, shared-memory, or hybrid applications.
Reference: [Rei96] <author> Steven K. Reinhardt. </author> <title> Mechanisms for Distributed Shared Memory. </title> <type> PhD thesis, </type> <institution> Computer Sciences Department, University of WisconsinMadison, </institution> <month> December </month> <year> 1996. </year>
Reference-contexts: While custom hardware performs an action quickly, research has shown that no single protocol is optimal for all applications [KMRS88] or even for all data structures within an application [BCZ90,FLR + 94]. Hybrid hardware/software protocols such as Alewifes LimitLESS [CKA91], Dir 1 SW [HLRW93], and Typhoon-2 <ref> [Rei96] </ref> implement the expected common cases in hardware and trap to system software to handle complex, infrequent events. High design costs and resource constraints make custom hardware unattractive. <p> An effective virtualization requires the name to be associated with the virtual address space. This decision precludes exposing names in the physical address space, as is being done in the Typhoon prototypes <ref> [Rei96] </ref>. However, it also precludes hardware acceleration of access faults using snooping devices if the virtual address is not readily available on the bus. As for the attribute content, an example helps illustrate the implications of alternative choices. Suppose we have two virtual memory mappings for the same physical memory. <p> Blizzard/T, however, does not require a second mapping with always read-write protection since it does not use the read-only page protection to emulate the ReadOnly state. In addition, the Vortex device registers are mapped into the user address space <ref> [Rei96] </ref>. One of the registers is The figure displays the segments that comprise the address space of Blizzard processes under Blizzard/ES. To support shared memory, three new segments are added to the traditional segments of a Unix process. The segments are the same as the corresponding ones for Blizzard/E. <p> In this study, I examine Vortexs ability to offer a complete fine-grain access control solution, in isolation from its other capabilities that have been used in the Typhoon-0 prototype. More specifically, in the Typhoon designs <ref> [Rei96] </ref>, there is a single computation processor while a second processor is dedicated for protocol actions. The Vortex board facilitates intranode communication by mapping to user space a cacheable control register.
Reference: [RFW93] <author> Steven K. Reinhardt, Babak Falsafi, and David A. Wood. </author> <title> Kernel support for the Wisconsin Wind Tunnel. </title> <booktitle> In Proceedings of the Usenix Symposium on Microkernels and Other Kernel Architectures, </booktitle> <month> September </month> <year> 1993. </year>
Reference-contexts: The first, BlizzardS, adds a fast lookup before each shared-memory reference [LW94] by modifying the programs executable [LB94]. The second, Blizzard-E, uses the memorys error-correcting code (ECC) bits as valid bits and the page protection to emulate read-only bits <ref> [RFW93] </ref>. The third, Blizzard-ES, combines the two techniques using software lookups for stores and ECC for loads [SFL + 94]. The fourth, Blizzard/T, uses Vortex, a custom fine-grain access control accelerator board. <p> Furthermore, the library provides certain amenities that simplify the task of developing instruction sequences. It can determine whether 1. In retrospect, this is not surprising given the influence of the Wisconsin Wind Tunnels supervisor interface <ref> [RFW93] </ref> on the design of Tempest. 23 register values are alive at any particular instance in the program, it can reassign registers so that the inserted code uses free registers and it can spill registers automatically when free registers are not available. <p> The reason for this complexity is that the signal interface has been designed as a general purpose notification mechanism that deals with both synchronous and asynchronous faults. Fortunately, there are well known techniques to support fast exceptions <ref> [RFW93, TL94a] </ref>. The main idea is that the user trap handler is invoked as soon as the kernel realizes that there is a hardware fault. In addition, the kernel is not involved again when we return to the faulted instruction (Figure 2-10 (b)).
Reference: [RHL + 93] <author> Steven K. Reinhardt, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, and David A. Wood. </author> <title> The Wisconsin Wind Tunnel: Virtual prototyping of parallel computers. </title> <booktitle> In Proceedings of the 1993 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 4860, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The tool for inserting the checks originated from the vt tool, which in turn originated from qpt, a general purpose profiling tool [LB94]. The vt tool was used in the Wisconsin WindTunnel (WWT) simulator <ref> [RHL + 93] </ref> to insert instructions that tracked the simulated time in target executables. It was not a sophisticated tool since it could only process the executable on a basic block basis.
Reference: [RLW94] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-level shared memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325337, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Tempest separates the mechanisms required to implement distributed shared memory from the coherence protocols that enforce the shared memory semantics. By default, shared memory programs are linked against a library that implements an 128-byte S-COMA-like [HSL94] software protocol <ref> [RLW94] </ref> to maintain coherence across nodes. Tempests flexibility allows the implementation of applicationspecific custom coherence protocols [FLR + 94] which integrate shared memory with messaging. Such protocols, called hybrid protocols, can reduce the overhead of fine-grain access control by being tailored to the 1. <p> In Tempest, a processor initiates a bulk data transfer much as it would start a conventional DMA transaction, by specifying virtual addresses on both source and destination nodes. The transfer is logically asynchronous with the computation. 1.2.4 Stache Tempests default coherence protocol is an 128-byte S-COMA-like [HSL94] software protocol <ref> [RLW94] </ref> called stache. Stache maintains internal data structures to correctly implement a sequentially consistent shared memory coherence protocol. For each shared page, one node acts as the home node. <p> When a processor supports a bus-based coherence scheme, a separate bussnooping agent can perform a lookup similar to that performed by a memory controller. Stanford DASH [LLG + 92] and Typhoon <ref> [RLW94] </ref> among experimental designs, employ this approach. Many recent commercial shared memory machines, such as Sequent STiNG [LC96] and SGI Origin [LL97] also follow this approach. 19 2.1.2 Protocol Action Custom Hardware. High performance shared memory systems use dedicated hardware to execute the protocol actions. <p> Instead, they rely on the messaging subsystem to implicitly limit the injection rate. Request protocols generate few concurrent requests which most of the time have small buffer requirements. Such protocols are limited by design on how many messages can inject into the network. Invalidate-based shared memory coherence protocols <ref> [LH89, RLW94, SGT96] </ref> including Stache, belong in this category. Shared memory programs using Stache can inject at most as many messages as the number of block access faults that can occur concurrently. This number is limited by the number of threads in the program. <p> Buffer Requirements Per Graph Concurrent Activation Graphs Few Many Small Invalidate DSM coherence protocols <ref> [LH89, RLW94, SGT96] </ref> Update DSM coherence proto cols, applicationspecific proto cols [FLR + Large Dataflow protocols [CCBS95] 63 deadlocks [WGH + 97]. However, such strategy is not appropriate in a general platform like Blizzard, which it is targeted to allow the development of application specific protocols. <p> Instead, it has to copy any such messages to an overflow queue in its local memory. Messages from the overflow queue will be injected in the network when new credits arrive. This policy has been employed in the Tempest libraries for the Typhoon parallel machines <ref> [RLW94] </ref> developed on the WWT-II simulator [MRF + Sender-overflow was designed to deal with temporary buffer shortages. It is ideal for protocols with message activation graphs that each requires a small number of buffers and few of which are traversed concurrently. <p> A.4.2 Blizzard Protocol libraries Tempests default coherence protocol is an 128-byte S-COMA-like [HSL94] software protocol <ref> [RLW94] </ref>. Stache maintains internal data structures to correctly implement a sequentially consistent shared memory coherence protocol. For each shared page, one node acts as the home node. By default home pages are distributed among the nodes in a round-robin manner.
Reference: [ROS93] <author> ROSS Technology, Inc. </author> <title> SPARC RISC Users Guide: </title> <address> hyperSPARC Edition, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: Each contains two 66 Mhz Ross HyperSPARC processors <ref> [ROS93] </ref> with a 256 KB L2 cache memory and 64 Table 1.1: Parallel programming platforms vs. parallel programming models. <p> These misses are serviced by the processor cache controller. On COW, the controller brings 32 bytes into the processor cache both for read and write misses since the HyperSparc processor is running with the cache in writeback mode <ref> [ROS93] </ref>. For parallel programs on FGDSM systems, the situation is more complicated. As for sequential programs, they can cause local misses when accessing local data. They can also cause local misses when accessing remote data for which a copy exists locally. <p> A.1 Wisconsin Cluster Of Workstations (COW) Wisconsin COW consists of 40 dual-processor Sun SPARCStation 20s. Each contains two 66 Mhz Ross HyperSPARC processors <ref> [ROS93] </ref> with a 256 KB L2 cache memory and 64 MB of memory. The cache-coherent 50 Mhz MBus connects the processors and memory. I/O devices sit on the 25 Mhz SBus, which is connected through a bridge to the MBus.
Reference: [RPW96] <author> Steven K. Reinhardt, Robert W. Pfile, and David A. Wood. </author> <title> Decoupled hardware support for distributed shared memory. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: High-end Tempest implementations such as the Typhoon designs [RLW94,RPW96] include extensive hardware support for fine-grain access control and protocol actions. Such designs achieve performance competitive to other hardware shared memory approaches <ref> [RPW96] </ref> but still offer Tempests flexibility to support coherence protocols. Blizzard, however, implements coherence in software but maintains fine-grain access semantics either in software or hardware using mostly commodity hardware and software. <p> The fourth, Blizzard/T, uses Vortex, a custom fine-grain access control accelerator board. The board was originally developed by members of the Wisconsin WindTunnel project [Pfi95] to demonstrate the simplest possible hardware Tempest implementation (Typhoon-0 <ref> [RPW96] </ref>). Blizzard/T explores the Vortexs ability to support hardware fine-grain tags modified without kernel intervention but unlike Typhoon-0, it does not use any of its other abilities to accelerate Tempest operations (e.g., mechanisms to accelerate dispatch of access fault handlers). Chapter 2 makes the following three contributions. <p> Thereby, Blizzard allows the development of applicationspecific coherence protocols [FLR + 94] that optimize the data transfers to match the application sharing patterns. Applicationspecific protocols are particularly appropriate for FGDSM systems on low-cost platforms because their relative performance benefit is significantly higher than other proposed tightly-integrated high-end Tempest implementations <ref> [RPW96] </ref>. 87 This chapter examines whether FGDSM systems on networks of workstations are a realistic platform for parallel applications using both low level measurements and application benchmarks. The question is answered with a qualified yes. Parallel applications with small communication requirements easily achieve significant speedups using transparent shared memory. <p> Without efficient support for atomicity, accesses to frequently shared resources may incur high overheads and result in lower SMP-node performance. FGDSM designs vary in the degree of support for atomic accesses, from zero-overhead custom hardware implementations providing atomic accesses directly in hardware <ref> [RPW96] </ref> to low-overhead synchronization operations in software [SFH + 96,SGA97]. Performance results presented in this chapter, indicate that grouping processors in SMP nodes benefits from custom hardware support. Custom hardware support for fine-grain sharing significantly reduces synchronization overhead in SMP nodes. <p> Such an access is simply atomic by nature and does not require special support. Fine-grain tags may support atomic accesses from the application and protocol handlers either in hardware <ref> [RPW96, SFL + 94] </ref> or software [SGT96, SFL + 94]. Hardware implementations may store tag values in SRAM on a custom bus-based snooping board, or in error-correcting code (ECC) in the memory system. <p> Blizzard/T: Custom Board SRAM Tags Blizzard/T uses Vortex [Pfi95], a custom board that snoops memory bus transactions, performs fine-grain access control tests through a SRAM lookup, and coordinates intra-node communication. The board contains two FPGAs, two SRAM, and some minor logic <ref> [RPW96] </ref>. Vortex enforces fine-grain access semantics by asserting a bus error in response to memory transactions that incur access violationse.g., read/write to an invalid memory block or write to a read-only block.
Reference: [SCB93] <author> Daniel Stodolsky, J. Brad Chen, and Brian Bershad. </author> <title> Fast interrupt priority management in operating systems. </title> <booktitle> In Second USENIX Symposium on Microkernels and Other Kernel Archtitectures, </booktitle> <pages> pages 105110, </pages> <address> September 1993. San Diego, CA. </address> <month> 203 </month>
Reference-contexts: The CM-5 did not provide user-level access to the interrupt mask. Therefore, relative expensive kernel traps were required to both disable and re-enable interrupts. Instead, Blizzard/CM-5 used a software interrupt masking scheme similar to one proposed by Stodolsky, et al. <ref> [SCB93] </ref>. The key observation is that interrupts occur much less frequently than critical sections, so you should optimize for this common case. This approach used a software flag to mark critical sections. The lowest-level interrupt handler checked this software-disable flag.
Reference: [Sco96] <author> Steve L. Scott. </author> <title> Synchronization and communication in the T3E multiprocessor. </title> <booktitle> In Proceedings of the 7th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VII), </booktitle> <pages> pages 2636, </pages> <year> 1996. </year>
Reference-contexts: In order to support replication efficiently, we need network interfaces that can efficiently service separate output queues and demultiplex incoming messages to separate input queues. For example, network interfaces that support a remote memory access model can trivially support separate message queues <ref> [BLA + 94, Sco96] </ref> by building separate circular message queues in main memory. Incoming messages are directed to the appropriate memory location and therefore, the appropriate message queue on the remote node. Sharing introduces extra overheads to synchronize competing accesses to the message queues. <p> In the common case, both avoid kernel intervention for data transfers and in both the CPU is in the critical path for message operations. Unlike our design, UDMA requires hardware support in NIs to capture transfer requests. Cray T3E <ref> [Sco96] </ref> combines remote memory accesses with an approach similar to UDMA. It supports minimal messaging through special NI registers. The CPU initiates transfers directly from remote memory to the NI registers on the local node. It subsequently initiates the transfer of the data from the NI registers to local memory.
Reference: [SFH + 96] <author> Ioannis Schoinas, Babak Falsafi, Mark D. Hill, James R. Larus, Christopher E. Lucas, Shubhendu S. Mukherjee, Steven K. Reinhardt, Eric Schnarr, and David A. Wood. </author> <title> Implementing fine-grain distributed shared memory on commodity smp workstations. </title> <type> Technical Report 1307, </type> <institution> Computer Sciences Department, University of Wiscon-sinMadison, </institution> <month> March </month> <year> 1996. </year>
Reference-contexts: Using similar techniques, we have been able to reduce the roundtrip time for synchronous traps on 66 MHz HyperSparcs, from 101 msecs with the standard Solaris 2.4 signal interface to 5 msecs with optimized kernel interfaces <ref> [SFH + 96] </ref>. Other researchers have reported similar results [RFW93,TL94b]. However, specialized fast interfaces are not viable in the long run. First, we violate kernel structuring principles. Device specific support must be implemented at the lowest kernel levels where no public interfaces exist to support this functionality. <p> The Tempest messaging interface implements an active message model where upon a message arrival a message handler is invoked. It supports small and large messages. Small messages transfer few words. Typically, these messages are implemented by including the word data in the message header <ref> [SFL + 94, SFH + 96] </ref>. Therefore, minimal messaging is not applicable to these messages. Large messages transfer either word or cache-block aligned regions. The former are used for general messaging operations while the latter are optimized for transferring cache blocks. <p> The source files for all the kernel modules and drivers are located inside the Solaris 2.4 source tree under the uts/sun4m architecturespecific directory. Three separate directories, blzsysm, eccmemd, and vortexd, contain the source for the BlzSysM, EccMemD and VortexD modules, respectively. The binary rewriting tool <ref> [SFH + 96] </ref> used to insert the software checks for Blizzard/S is based on the EEL executable rewriting library [LS95]. The EEL library provides extensive infrastructure to analyze and modify executables.
Reference: [SFH + 97] <author> Ioannis Schoinas, Babak Falsafi, Mark D. Hill, James R. Larus, and David A. Wood. Blizzard: </author> <title> Cost-effective fine-grain distributed shared memory. </title> <note> Submitted for Publication, </note> <year> 1997. </year>
Reference-contexts: In a cost-performance study related to the work presented in this chapter, we found that dual-processor nodes were more cost-effective in that specific timeframe than either uniprocessor or quad-processor nodes <ref> [SFH + 97] </ref>. The rest of this chapter is organized as follows. Section 5.1 describes the mechanisms and system resources required to implement FGDSM on a node of a parallel machine and discusses the alternative policies for each resource. Section 5.2 summarizes Blizzards choices for each resource and their rationale. <p> High-performance products, for instance, tend to target smaller markets and as such carry larger margins, charging higher premiums. In a related study <ref> [SFH + 97] </ref> we found that today quad-processor servers command high price premiums. In contrast, dual-processor machines offer the lowest overall system cost. The main reason is that most system components used in single processor machines are designed to support up to two processors.
Reference: [SFL + 94] <author> Ioannis Schoinas, Babak Falsafi, Alvin R. Lebeck, Steven K. Reinhardt, James R. Larus, and David A. Wood. </author> <title> Fine-grain access control for distributed shared memory. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pages 297307, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Like SVM systems, these systems use address translation hardware to map shared addresses to local memory pages but enforce coherence at a finer granularity. FGDSM systems achieve performance competitive to SVM systems [ZIS + 97] without having to resort to weak consistency models. Blizzard/CM-5 <ref> [SFL + 94] </ref>, developed by the author and others for the TMC CM-5, was the first FGDSM system on messaging passing hardware. Blizzard/COW 1 + 96], which is its direct descendant, was developed on the Wisconsin Cluster of Workstations (COW) and is the focus of this thesis. <p> The second, Blizzard-E, uses the memorys error-correcting code (ECC) bits as valid bits and the page protection to emulate read-only bits [RFW93]. The third, Blizzard-ES, combines the two techniques using software lookups for stores and ECC for loads <ref> [SFL + 94] </ref>. The fourth, Blizzard/T, uses Vortex, a custom fine-grain access control accelerator board. The board was originally developed by members of the Wisconsin WindTunnel project [Pfi95] to demonstrate the simplest possible hardware Tempest implementation (Typhoon-0 [RPW96]). <p> Zhou, et al., however, have shown that sequentially consistent fine-grain DSM systems are competitive with page-based systems that support weaker consistency models [ZIS + Blizzard explores four different fine-grain tag implementations. These designs cover a substantial range of the design space for fine-grain access control mechanisms <ref> [SFL + 94] </ref>. Blizzard/S uses a software method based on locally available executable-editing technology [LS95]. Blizzard/E uses commodity hardware to accelerate the access check. The memory controller supports Invalid blocks while the TLB is used to emulate ReadOnly blocks. <p> This technique descends from the Fast-Cache technique used in the simulation of memory systems [LW94]. It was first introduced in Blizzard/CM-5. In that system, it required 15 instructions (18 cycles on the CM-5 SPARC processors) before every load and store instruction <ref> [SFL + 94] </ref>. The tool for inserting the checks originated from the vt tool, which in turn originated from qpt, a general purpose profiling tool [LB94]. <p> for loads and stores (does not use condition codes) subcc %g0, sentinel, %g5 ! Compare to sentinel bne,a next ! Avoid jump to handler nop jmpl %g6 + %l0, %g6 ! Jump to handler jump table nop next: (b) Sentinel sequence for loads (uses condition codes) 25 control in Blizzard/CM-5 <ref> [SFL + 94] </ref> and in the simulation of memory systems [RHL + UNMS94]. The challenge in this case was to incorporate it in a commodity multiprocessor operating system. Since Section 2.4 presents implementation details, I only mention here its basic characteristics. <p> Host Memory LANai Memory Message Header Queues Send Queue Recv Queue Send Queue Recv Queue Message Data Queues CPU LANai DMA Loads/Stores Network DMA Cached Loads/Stores Uncached Load/Stores 72 3.5.1 Interrupt Message Notification Alternatives Blizzard/CM-5 <ref> [SFL + 94] </ref>, the first Blizzard implementation, used interrupts triggered by the NI as the message notification mechanism. This approach delivered the best performance in the CM-5 environment. A similar scheme has been also implemented for the Blizzard. However, it failed to offer good performance. <p> They were originally written for sequentially consistent (transparent) hardware shared memory systems. Sequentially consistent page-based systems perform poorly on most of these codes because of their fine-grain communication and write sharing <ref> [CDK + 94, SFL + 94] </ref>. Page-granularity systems with weaker consistency models however, offer competitive performance to sequentially consistent software FGDSM systems [ZIS + 97] but complicate the programming model. <p> Such an access is simply atomic by nature and does not require special support. Fine-grain tags may support atomic accesses from the application and protocol handlers either in hardware <ref> [RPW96, SFL + 94] </ref> or software [SGT96, SFL + 94]. Hardware implementations may store tag values in SRAM on a custom bus-based snooping board, or in error-correcting code (ECC) in the memory system. <p> Such an access is simply atomic by nature and does not require special support. Fine-grain tags may support atomic accesses from the application and protocol handlers either in hardware [RPW96, SFL + 94] or software <ref> [SGT96, SFL + 94] </ref>. Hardware implementations may store tag values in SRAM on a custom bus-based snooping board, or in error-correcting code (ECC) in the memory system. <p> Fortunately, similar to hardware implementations, sentinels do not require additional synchronization since the lookup is implicitely tied to the datra reference. Software tag table lookups however, use memory instructions and are not atomic with respect to data references. Uniprocessor-node implementations such as Shasta [SGT96] and Blizzard/S <ref> [SFL + 94] </ref> eliminate the race between protocol handlers and the application by suppressing all protocol activity during the test; both systems only use instrumented polling rather than interrupts to invoke protocol code upon message arrivals. <p> SoftFLASH, MGS, and Cashmere address SMP-node implementations of VM-based software DSM. Fine-grain distributed shared memory has traditionally only been implemented in hardware. Blizzard/CM-5 is the first implementation of FGDSM either entirely in software, or with hardware assist <ref> [SFL + 94] </ref>. Shasta [SGT96] is a successor of Blizzard and an all-software implementation of FGDSM. CRL [JKW95] is a region-based software FGDSM and requires program annotations for accessing regions. Neither Blizzard/CM-5 nor CRL have SMP-node implementations. SMP-node implementations of Shasta [SGA97] have recently become available. <p> The Tempest messaging interface implements an active message model where upon a message arrival a message handler is invoked. It supports small and large messages. Small messages transfer few words. Typically, these messages are implemented by including the word data in the message header <ref> [SFL + 94, SFH + 96] </ref>. Therefore, minimal messaging is not applicable to these messages. Large messages transfer either word or cache-block aligned regions. The former are used for general messaging operations while the latter are optimized for transferring cache blocks. <p> The characteristics of the network were chosen so that any performance degradation is strictly due to data transfers within the node (e.g., the network latency is only 100 CPU cycles). The detailed node parameters are listed in Table 6.2. User-level messaging libraries model the messaging subsystem in Blizzard/CM-5 <ref> [SFL + 94] </ref> (without the fragmentation support required due to the small packet size of the original CM-5 NI). The CPU constructs messages by writing with uncached stores to the output queue of the NI. It receives the messages with uncached loads from the input queue. <p> Blizzard supports four different fine-grain tag implementations. These designs cover a substantial range of the design space for fine-grain access control mechanisms <ref> [SFL + 94] </ref>. Blizzard/S uses a software method based on executable-editing technology locally available [LS95]. Blizzard/E uses commodity hardware to accelerate the access check. The memory controller supports Invalid blocks while the TLB is used to emulate ReadOnly blocks. <p> The processors initiate a bulk data transfer much like it would start a conventional DMA transaction, by specifying vir 188 tual addresses on both source and destination nodes. The channel interface was first developed for the Blizzard/CM-5 <ref> [SFL + 94] </ref>. On the CM-5 platform, the small network packet size (5 words) supported by the network hardware, did not allow the efficient implementation of active messages with large data blocks.
Reference: [SGA97] <author> Daniel J. Scales, Kourosh Gharachorloo, and Anshu Aggarwal. </author> <title> Fine-grain software distributed shared memory on smp clusters. </title> <type> Technical Report 97/3, </type> <institution> Digital Equipment Corporation, Western Research Laboratory, </institution> <month> February </month> <year> 1997. </year>
Reference-contexts: Multiple processors can also improve performance by overlapping application execution with protocol actions. However, simultaneous sharing of nodes resources (e.g., memory) between the application and the protocol requires mechanisms for guaranteeing atomic accesses <ref> [SGA97] </ref>. Without efficient support for atomicity, accesses to frequently shared resources may incur high overheads and result in lower performance with multiprocessor nodes. Chapter 5 makes the following two contributions. <p> Multiple SMP processors can also improve performance by overlapping applications execution with protocol processingi.e., handling FGDSM messages. Simultaneous sharing of nodes resources (e.g., memory) between the application and protocol, however, requires mechanisms for guaranteeing atomic accesses <ref> [SGA97] </ref>. Without efficient support for atomicity, accesses to frequently shared resources may incur high overheads and result in lower SMP-node performance. <p> Similar techniques have been used to reduce the cost of TLB shootdowns in SMP machines [ENCH96]. An FGDSM system that implements replicated fine-grain tags is Shasta/SMP <ref> [SGA97] </ref>. In Section 5.3, we will examine in detail how the different Blizzard tag implementations address the atomicity problem. Message & Access Fault Queues Running on an SMP node offers the opportunity to support separate message queues per processor. <p> If however, we are targetting an SMP that only supports a weaker consistency model and the the cost of such instructions is prohibitively expensive, we can consider a fourth handshake method that has been implemented in Shasta/SMP <ref> [SGA97] </ref>. In the Shasta scheme, the protocol handler explicitly initiates a handshake event with a message to the other processors. Upon receipt of the handshake request, the processors have to reply with explicit response messages. <p> Shasta [SGT96] is a successor of Blizzard and an all-software implementation of FGDSM. CRL [JKW95] is a region-based software FGDSM and requires program annotations for accessing regions. Neither Blizzard/CM-5 nor CRL have SMP-node implementations. SMP-node implementations of Shasta <ref> [SGA97] </ref> have recently become available. Overall, Shasta/SMP appears to behave similar to Blizzard implementations performance-wise. This work builds upon theirs in several ways. First, the study evaluates how hardware (vs. software) implementations of sharing can help eliminate SMP-node overheads.
Reference: [SGC93] <author> Rafael H. Saavedra, R. Stockton Gaines, and Michael J. Carlton. </author> <title> Micro benchmark analysis of the KSR1. </title> <booktitle> In Proceedings of Supercomputing 93, </booktitle> <pages> pages 202213, </pages> <month> Novem-ber </month> <year> 1993. </year>
Reference-contexts: In this section, I present microbenchmark performance results that measure the latency and bandwidth of local and remote memory accesses. The performance characteristics of the local and remote memory references are measured using microbenchmarks <ref> [SGC93] </ref>, simple small programs that measure the performance characteristics of specific platform features. The experiments compare and contrast the memory performance of remote and local memory references. Sequential programs access the local memory when they miss in the processor cache. These misses are serviced by the processor cache controller.
Reference: [SGT96] <author> Daniel J. Scales, Kourosh Gharachorloo, and Chandramohan A. Thekkath. </author> <title> Shasta: A low overhead, software-only approach for supporting fine-grain shared memory. </title> <booktitle> In Proceedings of the 7th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VII), </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: Blizzard/COW 1 + 96], which is its direct descendant, was developed on the Wisconsin Cluster of Workstations (COW) and is the focus of this thesis. Digitals Shasta <ref> [SGT96] </ref> is another FGDSM system inspired by Blizzard/CM-5. Unlike Shasta, Blizzard uses the Tempest interface [Rei94] to support software distributed shared memory. Tempest separates the mechanisms required to implement distributed shared memory from the coherence protocols that enforce the shared memory semantics. <p> For example, a dedicated processor for protocol actions can minimize invocation and action overhead while still exploiting commodity computation processors. However, this approach requires either a com 17 plex ASIC or full-custom chip design, which significantly increases design time and manufacturing cost. Blizzard and Digitals Shasta <ref> [SGT96] </ref>, on the other hand, are FGDSM systems targeted toward networks of workstations or personal computers. In such environments, the cost and complexity of additional hardware is more important because they must compete on uniprocessor cost/performance. Blizzard relies mainly on commodity software and hardware to provide fine-grain access control. <p> The code in a software lookup checks a main-memory data structure to determine the state of a block before a reference. Static analysis can detect and potentially eliminate redundant tests or even batch accesses to the consecutive cache blocks as in Shasta <ref> [SGT96] </ref>. Either a compiler or a program executable editing tool [LB94,LS95] can insert software tests. With the latter approach every compiler need not reimplement test analysis and code generation. Blizzard/S and Shasta [SGT96] also follow this approach (Section 2.3.1). Compiler-inserted lookups can exploit application-level information. <p> detect and potentially eliminate redundant tests or even batch accesses to the consecutive cache blocks as in Shasta <ref> [SGT96] </ref>. Either a compiler or a program executable editing tool [LB94,LS95] can insert software tests. With the latter approach every compiler need not reimplement test analysis and code generation. Blizzard/S and Shasta [SGT96] also follow this approach (Section 2.3.1). Compiler-inserted lookups can exploit application-level information. Orca [BTK90], for example, provides access control on program objects instead of blocks. Translation Lookahead Buffer (TLB). Standard address translation hardware provides access control, though at memory page granularity. <p> In other words, the ratio of loads vs. stores that get instrumented also depends on the application. There is evidence to suggest that the software access method can achieve better results. The evidence come from Digitals Shasta <ref> [SGT96] </ref> for which it is reported that the fine-grain access control overhead increases the application running time by 8%-40% relative to the sequential application. <p> The application times are normalized against the runs of sequential applications (no FGDSM overhead). The applications and their input sets are described in Chapter 4. The Shasta results are taken from the Shasta ASPLOS paper <ref> [SGT96] </ref>. Benchmark Blizzard/S Blizzard/ES Shasta without batching Shasta with batching Appbt 1.88 1.16 - Barnes 1.63 1.22 1.14 1.08 LU 2.07 1.34 1.65 1.29 MolDyn 1.35 1.06 - Tomcatv 2.22 2.00 - WaterSp 1.50 1.12 1.23 1.18 WaterNq 1.59 1.20 1.22 1.14 45 trol. <p> Instead, they rely on the messaging subsystem to implicitly limit the injection rate. Request protocols generate few concurrent requests which most of the time have small buffer requirements. Such protocols are limited by design on how many messages can inject into the network. Invalidate-based shared memory coherence protocols <ref> [LH89, RLW94, SGT96] </ref> including Stache, belong in this category. Shared memory programs using Stache can inject at most as many messages as the number of block access faults that can occur concurrently. This number is limited by the number of threads in the program. <p> Buffer Requirements Per Graph Concurrent Activation Graphs Few Many Small Invalidate DSM coherence protocols <ref> [LH89, RLW94, SGT96] </ref> Update DSM coherence proto cols, applicationspecific proto cols [FLR + Large Dataflow protocols [CCBS95] 63 deadlocks [WGH + 97]. However, such strategy is not appropriate in a general platform like Blizzard, which it is targeted to allow the development of application specific protocols. <p> Introducing relaxed consistency models specifically for software DSM systems, complicates the porting of shared memory applica tions to these systems. FGDSM systems like Blizzard and Digitals Shasta can also support relaxed consistency models but the performance benefits are not significant <ref> [SGT96, ZIS + 97] </ref>. Reported application performance results for Blizzard and Digitals Shasta demonstrate that FGDSM systems can achieve significant speedups for many scientific applications. Both these systems achieve comparable speedups for shared memory applications despite hardware and software differ ences between the two systems [SGT96,SFH + 97]. <p> Such an access is simply atomic by nature and does not require special support. Fine-grain tags may support atomic accesses from the application and protocol handlers either in hardware [RPW96, SFL + 94] or software <ref> [SGT96, SFL + 94] </ref>. Hardware implementations may store tag values in SRAM on a custom bus-based snooping board, or in error-correcting code (ECC) in the memory system. <p> Fortunately, similar to hardware implementations, sentinels do not require additional synchronization since the lookup is implicitely tied to the datra reference. Software tag table lookups however, use memory instructions and are not atomic with respect to data references. Uniprocessor-node implementations such as Shasta <ref> [SGT96] </ref> and Blizzard/S [SFL + 94] eliminate the race between protocol handlers and the application by suppressing all protocol activity during the test; both systems only use instrumented polling rather than interrupts to invoke protocol code upon message arrivals. <p> SoftFLASH, MGS, and Cashmere address SMP-node implementations of VM-based software DSM. Fine-grain distributed shared memory has traditionally only been implemented in hardware. Blizzard/CM-5 is the first implementation of FGDSM either entirely in software, or with hardware assist [SFL + 94]. Shasta <ref> [SGT96] </ref> is a successor of Blizzard and an all-software implementation of FGDSM. CRL [JKW95] is a region-based software FGDSM and requires program annotations for accessing regions. Neither Blizzard/CM-5 nor CRL have SMP-node implementations. SMP-node implementations of Shasta [SGA97] have recently become available.
Reference: [SPE90] <author> SPEC. </author> <title> SPEC benchmark suite release 1.0, </title> <month> Winter </month> <year> 1990. </year>
Reference-contexts: Moreover, once the fine-grain communication of the reduction phase has been eliminated, the program can use a larger stache block size (1024 Kbytes) to reduce the access control overhead. Tomcatv is a parallel version of the SPEC benchmark <ref> [SPE90] </ref>. The input matrix is partitioned among the processors in contiguous rows. Sharing occurs across the boundary rows (similar to ocean) and therefore, it exhibits nearest neighbor communication. Waterns (n squared) is one of the SPLASH-2 applications. <p> edges, distance span 1, 100 iterations 166 lu Contiguous blocked dense LU factorization [WOT + 95] 512x512 matrix, 16x16 blocks 75 ocean Simulation of eddy currents [WOT + 95] 514x514 ocean size 62 moldyn Molecular dynamics [BBO + 83] 8722 molecules, 40 iterations. 128 tomcatv Thompsons solver mesh genera tion <ref> [SPE90] </ref> 512x512 matrices, 100 iters 147 waterns Water molecule force simulation [WOT + 95] 4096 molecules 273 watersp Spatial water molecule force sim ulation [WOT + 95] 2744 molecules 65 95 col and the messaging subsystem that provide additional evidence for the interaction of the applications with the FGDSM systems. 4.3.1 <p> propagation [CDG + 93] 128K nodes, degree 6, 40% remote edges, distance span 1 (processors share edges only with their immediate neighbors), 100 iterations appbt 3D implementation of CFD [BM95] 40x40x40 matrices, 4 iters barnes Barnes-Hut N-body simulation [WOT + 95] 16K particles, 4 iters tomcatv Thompsons solver mesh generation <ref> [SPE90] </ref> 512x512 matrices, 100 iters lu Contiguous blocked dense LU factoriza tion [WOT + 95] 512x512 matrix, 16x16 blocks water Spatial water molecule force simulation [WOT + 95] 4096 molecules 129 The loop-backedge handshake on the average lowers SMP-correctness overhead in Blizzard/S (Blizzard/ES) by up to 4%.
Reference: [SPWC97] <author> Patrick G. Sobalvarro, Scott Pakin, William E. Weihl, and Andrew A. Chien. </author> <title> Dynamic coscheduling on workstation clusters. </title> <note> Submitted for publication, </note> <month> March </month> <year> 1997. </year>
Reference-contexts: Recently, researchers have focused their attention and attempted to address exactly this scheduling problem. Emerging proposals include implicit scheduling [DAC96,ADC97] and dynamic coscheduling <ref> [SPWC97] </ref>. In implicit scheduling, processes relinguish control of the processor if they block waiting for a message more than some interval, statically or dynamically determined. In dynamic coscheduling, the system manipulates the priority of the processes to effect coscheduling based on observed traffic patterns. 172 Network Allocation.
Reference: [SS86] <author> Paul Sweazey and Alan Jay Smith. </author> <title> A class of compatible cache consistency protocols and their support by the IEEE Futurebus. </title> <booktitle> In Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 414423, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: At the low-end, shared memory is becoming ubiquitous in the form of smallscale symmetric multiprocessors (SMPs). In these platforms, the processors are attached on the same memory bus. Both commodity microprocessors and system logic are specifically designed to support shared memory using bus-based coherence protocols <ref> [SS86] </ref>. Consequently, it becomes easy to build low-end shared memory systems since the fixed design costs are amortized over the large sales volumes of commodity microprocessors. Therefore, these systems are achieving cost-performance superior to their uniprocessor counterparts.
Reference: [Ste94] <author> Peter Steenkiste. </author> <title> A systematic approach to host interface design for highspeed networks. </title> <booktitle> IEEE Computer, </booktitle> <month> March </month> <year> 1994. </year> <month> 204 </month>
Reference-contexts: This disparity has led some to argue that we are on the verge of a major paradigm shift that will transform the entire structure of information technology [Gil94]. Studies have shown that even today, network protocols spend a significant amount of time simply copying data <ref> [Ste94] </ref>. Therefore, many designs have attempted to avoid redundant copying at the application interface [DP93, RAC96, Wil92], the OS [kJC96], and the network interface [OZH + 96, DWB + 93, LC95, BCM94].
Reference: [SWG92] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford parallel applications for shared memory. Computer Architecture News, </title> <address> 20(1):544, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: The CS version addresses the synchronization problem by transferring data with messages which implicitly synchronize the processors in the correct order. 91 Barnes is one of the SPLASH-2 benchmarks <ref> [SWG92] </ref>. It simulates the evolution of bodies in a gravitational system. Its primary data structure is a shared oct-tree, which represents bodies locations in space. In each iteration, the tree is constructed from scratch. <p> Data are batched in large (4-Kbyte) messages, which takes advantage of the superior bandwidth achieved with large messages. LU is one of SPLASH-2 kernels <ref> [SWG92] </ref>. It performs the blocked LU factorization of a dense matrix. This version partitions the matrix into blocks and the data for each block are allocated sequential in memory. There are three steps to the algorithm. First, a diagonal block is factored. <p> The process is repeated until the matrix has been factored. The processors are positioned along a rectangular grid and the grid is repeated for all the blocks in the matrix. 92 Ocean is based in the noncontiguous version of the SPLASH-2 benchmark <ref> [SWG92] </ref>. It simulates eddy currents in an ocean basin. In the original SPLASH-2 application, processors shared data with their neighbors in a two-dimensional partition of the problem. In this version however, the problem grid is partitioned rowwise among the processors.
Reference: [TH90] <author> V. Tam and M. Hsu. </author> <title> Fast recovery in distributed shared virtual memory systems. </title> <booktitle> In IEEE 10th International Conference on Distributed Computing Systems, </booktitle> <year> 1990. </year>
Reference: [Thi91] <author> Thinking Machines Corporation. </author> <title> The connection machine CM-5 technical summary, </title> <year> 1991. </year>
Reference-contexts: These parallel platforms have been called massively parallel processors (MPPs). Since relatively little extra hardware except the custom network is required, this approach has enjoyed popularity in older machines such as Intel iPSC860 [Int90] and TMC CM-5 <ref> [Thi91] </ref>. It is still present today in commercial systems such as IBM SP-2. At a high level of detail, there is not much difference between MPPs and a collection of workstations with the exception of the custom network. <p> Therefore, the cost of breaking a bulk data transfer into small active messages was too high to realize bandwidth comparable to the one achieved by the default CM-5 messaging library <ref> [Thi91] </ref>. In contrast, on the COW, the use of the channel interface has been depreciated mainly because the active message layer can support messages up to four Kbytes, which are big enough to offer the raw hardware thorughput. <p> Sharing introduces extra overheads to synchronize competing accesses to the message queues. It is the only alternative with restricted interfaces that support strict queue semantics such as the CM-5 network interface <ref> [Thi91] </ref>. Since only one processor can access the message in the top of the queue, message operations need to be serialized and replication is not an option. However, when the hardware allows random queue entry addressing, the extra sharing overhead is relatively small. <p> The simulator is able to execute actual Sparc binaries by rewriting them to insert code that keeps track of the execution cycles. Each node contains a 300 MHz dual-issue SPARC processor modeled after the ROSS HyperSPARC and an NI similar to the CM-5 NI <ref> [Thi91] </ref>, augmented to support coherent DMA and address translation. As on the CM-5, the NI resides on the memory bus. The original CM-5 NI supported packets up to twenty bytes long. <p> Therefore, the cost of breaking a bulk data transfer into small active messages was too high to realize bandwidth comparable to the one achieved by the default CM-5 messaging library <ref> [Thi91] </ref>. On the COW implementation, the use of the channel interface has been depreciated mainly because the fine-grain messaging layer can support messages up to 4 Kbytes, which are big enough to offer the maximum throughput possible in this platform.
Reference: [TJ92] <author> Yuval Tamir and G. Janakiraman. </author> <title> Hierarchical coherency management for shared virtual memory multicomputers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 15(4):408419, </volume> <month> August </month> <year> 1992. </year>
Reference-contexts: Unfortunately, these bits can only partially support fine-grain access control since they do not provide the ReadOnly state. Tamir, et al., have proposed extensions to the format of the page table in order to fully support fine-grain access control <ref> [TJ92] </ref>. Cache Controller. The MIT Alewife [CKA91] and Kendall Square Research KSR-1 [Ken92] shared-memory systems use custom cache controllers to implement access control. In addition to detecting misses in hardware caches, these controllers determine when to invoke a protocol action.
Reference: [TL94a] <author> Chandramohan A. Thekkath and Henry M. Levy. </author> <title> Hardware and software support for efficient exception handling. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pages 110119, </pages> <year> 1994. </year>
Reference-contexts: The reason for this complexity is that the signal interface has been designed as a general purpose notification mechanism that deals with both synchronous and asynchronous faults. Fortunately, there are well known techniques to support fast exceptions <ref> [RFW93, TL94a] </ref>. The main idea is that the user trap handler is invoked as soon as the kernel realizes that there is a hardware fault. In addition, the kernel is not involved again when we return to the faulted instruction (Figure 2-10 (b)). <p> The kernel itself is not further involved in dealing with the fault. Both the WWT, and later, the Blizzard/CM-5 used these techniques to speedup fault delivery. However, the CM-5 nodes run a very simple kernel without minimal support for virtual memory. Thekkath, et al. <ref> [TL94a] </ref>, used similar techniques in the context of a commercial workstation operating system. Nevertheless, the operating system used, Digital Ultrix, had a 39 simplevirtual memory architecture. Moreover, neither approach was designed for multiprocessor nodes, compared to Solaris. Solariss virtual memory architecture significantly complicates fast trap dispatch.
Reference: [TL94b] <author> Chandramohan A. Thekkath and Henry M. Levy. </author> <title> Hardware and software support for efficient exception handling. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 110 119, </pages> <address> San Jose, California, </address> <year> 1994. </year>
Reference: [TLL94] <author> Chandramohan A. Thekkath, Henry M. Levy, and Edward D. Lazowska. </author> <title> Separating data and control transfer in distributed operation systems. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 211, </pages> <address> San Jose, California, </address> <year> 1994. </year>
Reference-contexts: A class of designs supports minimal messaging by using the CPU in kernel mode to instruct the NI to move the data to the appropriate place. Such approaches include page remapping in the kernel (implemented in Solaris 2.6 TCP [kJC96]), Washingtons inkernel emulation of the remote memory access model <ref> [TLL94] </ref> and other VM manipulations [BS96]. In these systems, minimum messaging is achieved if the NI can directly access the main memory. However, the kernel is involved in every transfer and thus, user-level messaging is not supported.
Reference: [UNMS94] <author> Richard Uhlig, David Nagle, Trevor Mudge, and Stuart Sechrest. Tapeworm II: </author> <title> A new method for measuring os effects on memory architecture performance. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pages 132144, </pages> <month> October </month> <year> 1994. </year>
Reference: [Vah96] <author> Uresh Vahalia. </author> <title> Unix Internals: The New Frontiers. </title> <publisher> Prentice Hall, </publisher> <year> 1996. </year>
Reference-contexts: While the kernel can optimize memory usage by keeping the same physical page associated with different virtual addresses, as soon as an operation is performed that would reveal this fact, a new copy the page is created. The SysV shared memory interface <ref> [Vah96] </ref> exposes memory pages for which the name is associated with the virtual address space but the content is associated with the physical address space. In this case, a modification through one virtual name will be visible through other virtual names. <p> For the interested reader, an excellent source for further information is Uresh Vahalias book titled Unix Internals: The New Frontiers <ref> [Vah96] </ref>. The Blizzard System Module (BLZSysM) acts as a straightjacket around the Solaris kernel to extend its functionality. The ECC Memory and Vortex device drivers (ECCMemD & VortexD) support hardware fine-grain access control using its services. <p> This allows the messaging subsystem to scale to large SMP nodes. Its scalability has been verified using an experimental testbed, Blizzard/SYSV 1 , which emulates high speed messaging using System V shared memory <ref> [Vah96] </ref>. Blizzard/SYSV runs on SMP workstations and servers and it has been used to tune the use of critical sections. For this purpose, a ES-6000 Sun Enterprise Server with twelve processors was used to emulate SMP multiprocessor nodes. <p> Second, processor page tables are maintained in main memory from which CPU (s) quickly load mappings for pages resident in main memory. Third, a complete description of a process address space, including pages that have been swapped out or not accessed yet, is maintained in internal kernel data structures <ref> [Vah96] </ref>. The OS defines public kernel interfaces to access and modify mappings in all these levels. In addition, it maintains consistency among the different translation levels. For example, when a page is swapped out to secondary storage, any mappings for that page must be flushed throughout the system.
Reference: [vE93] <author> Thorsten von Eicken. </author> <title> Active Messages: an Efficient Communication Architecture for Multiprocessors. </title> <type> PhD thesis, UCB, </type> <month> November </month> <year> 1993. </year>
Reference-contexts: Therefore, protocols designed for such environments favor solutions that guarantee buffer availability. Traditional multicomputer messaging models address the buffer allocation problem with a variety of techniques <ref> [vE93] </ref>. The data transfer either occurs after both the sender and the receiver have called the appropriate primitives (mostly used with sychronous messaging primitives) or the messaging subsystem allocates buffers on the fly as soon the data reach the destination (mostly used with asynchronous messaging primitives). <p> The RISC instruction set exposed simple instructions that the hardware could efficiently implement while the complex functionality was implemented in software using a sequence of simple instrutions. Accordingly, Berkeley active messages applied the RISC lessons in the design of the messaging subsystem and its programming abstraction <ref> [vE93] </ref>. Following this design philosophy to its extremes, user protocols were restricted to pure request/reply operations, with the rationale being to reduce the buffer management overheads associated with more permissive messaging models. However, it can be argued that the push to simplicity went further than it should.
Reference: [vEBBV95] <author> Thorsten von Eicken, Anindya Basu, Vineet Buch, and Werner Vogels. U-net: </author> <title> A user-level network interface for parallel and distributed computing. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating System Principles (SOSP), </booktitle> <pages> pages 4053, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: This approach permits a program to communicate without kernel intervention and is supported in recent low-latency communication hardware [BCF 95,BLA 94, Hor95,OZH 96]. Fast access to the hardware by itself is not enough to achieve low latencies. As the experience from message-passing multicomputers has shown <ref> [vECGS92, PLC95, vEBBV95] </ref>, traditional messaging interfaces have not been able to realize the hardware performance due to fixed software overheads associated with sending and receiving messages. Therefore, a newer generation of low overhead messaging interfaces attacked the software overheads. <p> Typically, the OS maps the device registers and/or memory into the user address space. Thereafter, the application can initiate message operations communicating directly with the device using loads and stores to send and receive messages. Examples of such designs include the Arizona Application Device Channels (ADCs) [DPD94], Cornell U-Net <ref> [vEBBV95] </ref>, HP Hamlyn [Wil92], Princeton SHRIMP [BDFL96]. The result of such recent research has been commercial designs like Myricom Myrinet [BCF + 95], Fore 200 [CMSB91], Mitsubishi DART [OZH + 96]. <p> In most cases, applications access the network through a layer of messaging abstractions. We can distinguish between low-level network access models and high-level user messaging models. Network access models such as ADCs [DPD94], U-Net <ref> [vEBBV95] </ref>, Active Messages [vECGS92], Fast Messages [PLC95], provide protected user access to the NI and serve as a consistent low-level model across NIs. Applications can use them to access the network but likely they will prefer higher level messaging models such as Fbufs [DP93], MPI [For94] or TCP/IP. <p> Arizona ADCs [DPD94] have been designed to optimize stream traffic. In Section 6.2, we discussed why this design cannot fully support minimal messaging. The base Cornell UNet <ref> [vEBBV95] </ref> architecture supports an abstraction similar to ADCs, and therefore has the same limitations as ADCs. In the original UNet paper [vEBBV95], a direct access UNet architecture is discussed that includes communication segments able to encompass all the user address space but the architecture is restricted to future NI designs. <p> Arizona ADCs [DPD94] have been designed to optimize stream traffic. In Section 6.2, we discussed why this design cannot fully support minimal messaging. The base Cornell UNet <ref> [vEBBV95] </ref> architecture supports an abstraction similar to ADCs, and therefore has the same limitations as ADCs. In the original UNet paper [vEBBV95], a direct access UNet architecture is discussed that includes communication segments able to encompass all the user address space but the architecture is restricted to future NI designs. Recent work [BWvE97] attempted to incorporate address translation mechanisms for existing NIs.
Reference: [vECGS92] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active messages: a mechanism for integrating communication and computa 205 tion. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 256266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: These faults are similar to page faults. They suspend the thread making the access and invoke a user-level handler. A typical handler performs the actions dictated by a coherence protocol to make the access possible and updates the tag. Then the access is retried. Low-Overhead Messaging. Low-overhead active messages <ref> [vECGS92] </ref> provide low-latency communication, which is fundamental to the performance of many parallel programs. In Tempest, a processor sends a message by specifying the destination node, handler address, and a string of arguments. <p> For applicationspecific protocols, where the coherence protocol has been optimized to fit the 12 application communication requirements, high bandwidth is of equal concern to low latency. Tempests messaging interface is based on the Berkeley active messages <ref> [vECGS92] </ref>, but it differs in two important aspects, necessary to use Tempest messages to implement transparent shared memory protocols. Tempest messages are not constrained to follow a request-reply protocol and are delivered without explicit polling by an application program. Chapter 3 makes the following four contributions. <p> This approach permits a program to communicate without kernel intervention and is supported in recent low-latency communication hardware [BCF 95,BLA 94, Hor95,OZH 96]. Fast access to the hardware by itself is not enough to achieve low latencies. As the experience from message-passing multicomputers has shown <ref> [vECGS92, PLC95, vEBBV95] </ref>, traditional messaging interfaces have not been able to realize the hardware performance due to fixed software overheads associated with sending and receiving messages. Therefore, a newer generation of low overhead messaging interfaces attacked the software overheads. <p> Berkeley active messages sought to reduce latencies by eliminating the software complexity associated with traditional multicomputer messaging interfaces. Tempests 52 messaging interface is based on the Berkeley active messages <ref> [vECGS92] </ref>. It differs from Ber-keley active messages in two important aspects that are necessary to use Tempest messages to implement transparent shared memory protocols. First, Tempest messages are not constrained to follow a request-reply protocol. Second, Tempest messages are delivered without explicit polling by application programs. <p> We will concentrate on its differences for other similar interfaces. Furthermore, we will discuss the complications that it introduces in the design and implementation of the low level network protocols. 3.1.1 Fine-grain communication: Active Messages The programming model supported by Tempest is based on the Berkeley active messages <ref> [vECGS92] </ref>. A processor sends a message by specifying the destination node, handler address, and a string of arguments. The messages arrival at its destination creates a thread that runs the message handler, which extracts the remainder of the message. <p> In Berkeley active messages, the justification for atomic handlers was the high costs associated with blocking handlers. Previous messagedriven architectures such the J-Machine and Moonsoon [DCF + 89,PC90] had supported blocking handlers at a high cost, either in processing overhead or hardware resources <ref> [vECGS92] </ref>. For example, in J-Machine when a handler was blocked, the message had to be copied in the nodes local memory to allow further processing of messages. Therefore, Berkeley active message sought to eliminate such costs using atomic handlers. <p> In the Berkeley active message terminology <ref> [vECGS92] </ref>, messages are distinguished into two classes. The first class contains messages injected into the network by the computation and are called request messages. The second class contains messages injected into the network by the active messages handlers. <p> The data transfer either occurs after both the sender and the receiver have called the appropriate primitives (mostly used with sychronous messaging primitives) or the messaging subsystem allocates buffers on the fly as soon the data reach the destination (mostly used with asynchronous messaging primitives). However, studies have shown <ref> [vECGS92] </ref> that buffer management overheads, regardless of the kind of primitives used, often dominate the communication cost. It is not unusual for the traditional message models to achieve a fraction of the raw hardware performance. <p> These schemes are very similar to flow control schemes originally used in link-level flow control protocols for virtual circuits [PD97]. The only exception to this rule is return-to-sender which uses shared buffers. 3.3.1 Pure Request/Reply Berkeley active messages <ref> [vECGS92] </ref> dictate that the application should be limited to pure request/reply message operations. In this way, the programming model itself forces the parallel programs to reduce the size of the message activation graphs for injected messages. <p> Instead, the compiler itself should generate code to explicitly manage buffers in the message handlers while the message subsystem should be freed from this task. In this way, message handlers could exploit and optimize special cases <ref> [vECGS92] </ref>. The alternative approach followed by Blizzard 1 still allows the compiler to optimize special cases. However, it relies on a robust fallback mechanism to handle the general case. <p> In most cases, applications access the network through a layer of messaging abstractions. We can distinguish between low-level network access models and high-level user messaging models. Network access models such as ADCs [DPD94], U-Net [vEBBV95], Active Messages <ref> [vECGS92] </ref>, Fast Messages [PLC95], provide protected user access to the NI and serve as a consistent low-level model across NIs. Applications can use them to access the network but likely they will prefer higher level messaging models such as Fbufs [DP93], MPI [For94] or TCP/IP. <p> These faults suspend the thread making the access and invoke a user-level handler. A typical handler performs the actions dictated by a coherence protocol to make the access possible. Then, it updates the tag, and the access is retried. Low-Overhead Messaging. Low-overhead active messages <ref> [vECGS92] </ref> provide low-latency communication, which is fundamental to the performance of many parallel programs. 1. Tempest defines a fourth state, Busy, which has the same semantics as the Invalid state. This state is supported in Blizzard but no protocol library has ever used it. .
Reference: [WF90] <author> K.L. Wu and W.K. Fuchs. </author> <title> Recoverable distributed shared memory. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 460469, </pages> <month> April </month> <year> 1990. </year>
Reference: [WG94] <author> David L. Weaver and Tom Germond, </author> <title> editors. SPARC Architecture Manual (Version 9). </title> <publisher> PTR Prentice Hall, </publisher> <year> 1994. </year>
Reference-contexts: Memory barriers instructions (shaded dark gray) are required for correctness with weaker SMP consistency models. I use Sparc V9s <ref> [WG94] </ref> memory barrier instructions to illustrate the ordering requirements with respect to weaker consistency models. These instructions bitencode the ordering relationships.
Reference: [WGH + 97] <author> Wolf-Dietrich Weber, Stephen Gold, Pat Helland, Takeshi Shimizu, Thomas Wicki, and Winfried Wilcke. </author> <title> The Mercury interconnect architecture: A cost-effective infrastructure for high-performance servers. </title> <booktitle> In Proceedings of the 24th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1997. </year>
Reference-contexts: Buffer Requirements Per Graph Concurrent Activation Graphs Few Many Small Invalidate DSM coherence protocols [LH89, RLW94, SGT96] Update DSM coherence proto cols, applicationspecific proto cols [FLR + Large Dataflow protocols [CCBS95] 63 deadlocks <ref> [WGH + 97] </ref>. However, such strategy is not appropriate in a general platform like Blizzard, which it is targeted to allow the development of application specific protocols. Finally, there are protocols that create an arbitrary number of concurrent activation graphs with large buffer requirements.
Reference: [WH95] <author> David A. Wood and Mark D. Hill. </author> <title> Cost-effective parallel computing. </title> <journal> IEEE Computer, </journal> <volume> 28(2):6972, </volume> <month> February </month> <year> 1995. </year>
Reference-contexts: SMP-nodes do not necessarily have to achieve the best performance to prefer using them as building blocks for FGDSM systems. Rather, it may be sufficient if SMP nodes are cost-effective, i.e., if they achieve the lowest cost-performance <ref> [WH95] </ref> of all compared configurations. Using SMP nodes, many machine components such as the number of processors and memory requirements remain fixed across the platforms.
Reference: [WHJ + 95] <author> Deborah A. Wallach, Wilson C. Hsieh, Kirk L. Johnson, M. Frans Kaashoek, and William E. Weihl. </author> <title> Optimistic active messages; a mechanisms for scheduling communication with computation. </title> <booktitle> In Fifth ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 217226, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: In active messages, where by definition a message handler executes atomically with respect to other handlers, the problem is further exaggerated. However, it is not restricted only to pure active messages; more general messagedriven models that restrict this limitation have similar problems <ref> [WHJ + 95] </ref>. Each blocked handler requires a distinct thread of control with its associated resources. We cannot expect the messaging subsystem to efficiently support an infinite number of concurrent handlers. Using these definitions, we can now develop a methodology for determining the buffer requirements of parallel programs.
Reference: [Wil92] <author> John Wilkes. </author> <title> Hamlyn an interface for sender-based communications. </title> <type> Technical Report HP-OSR-92-13, </type> <institution> HP, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: Thereafter, the application can initiate message operations communicating directly with the device using loads and stores to send and receive messages. Examples of such designs include the Arizona Application Device Channels (ADCs) [DPD94], Cornell U-Net [vEBBV95], HP Hamlyn <ref> [Wil92] </ref>, Princeton SHRIMP [BDFL96]. The result of such recent research has been commercial designs like Myricom Myrinet [BCF + 95], Fore 200 [CMSB91], Mitsubishi DART [OZH + 96]. <p> Studies have shown that even today, network protocols spend a significant amount of time simply copying data [Ste94]. Therefore, many designs have attempted to avoid redundant copying at the application interface <ref> [DP93, RAC96, Wil92] </ref>, the OS [kJC96], and the network interface [OZH + 96, DWB + 93, LC95, BCM94].
Reference: [Woo96] <author> David A. Wood. </author> <type> Personal communication, </type> <month> October </month> <year> 1996. </year>
Reference-contexts: A flow control policy must throttle the injection rate at the sender to the consumption rate at the receiver. Therefore, a proposed solution is to restrict the size of the sender-overflow queue <ref> [Woo96] </ref>. This implies that the number of messages that a handler can send to the same node is restricted likewise. Furthermore, no new handlers can be invoked if there are not enough buffers available for all potential message destinations.
Reference: [Woo97] <author> David A. Wood. </author> <type> Personal communication, </type> <month> December </month> <year> 1997. </year>
Reference-contexts: We can address this shortcoming by specifying the destination virtual address when the message is sent as the index in the receivers translation table. This scheme, suggested by David Wood <ref> [Woo97] </ref>, requires that each node to maintain a local copy of the receivers translation table for all potential destination nodes. Accordingly, the nodes must collaborate to keep these copies consistent and inform each other of changes in the translation maps.
Reference: [WOT + 95] <author> Steven Cameron Woo, Moriyoshi Ohara, Evan Torrie, Jaswinder Pal Singh, and Anoop Gupta. </author> <title> The SPLASH-2 programs: Characterization and methodological considerations. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2436, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: Application Problem Input Data Set Sequential Time (secs) appbt 3D implementation of CFD [BM95] 40x40x40 matrices, 4 iters 110 barnes Barnes-Hut N-body simulation <ref> [WOT + 95] </ref> 16K particles, 4 iters 33 em3d 3D electromagnetic wave propa gation [CDG + 93] 128K nodes, degree 6, 40% remote edges, distance span 1, 100 iterations 166 lu Contiguous blocked dense LU factorization [WOT + 95] 512x512 matrix, 16x16 blocks 75 ocean Simulation of eddy currents [WOT + <p> 3D implementation of CFD [BM95] 40x40x40 matrices, 4 iters 110 barnes Barnes-Hut N-body simulation <ref> [WOT + 95] </ref> 16K particles, 4 iters 33 em3d 3D electromagnetic wave propa gation [CDG + 93] 128K nodes, degree 6, 40% remote edges, distance span 1, 100 iterations 166 lu Contiguous blocked dense LU factorization [WOT + 95] 512x512 matrix, 16x16 blocks 75 ocean Simulation of eddy currents [WOT + 95] 514x514 ocean size 62 moldyn Molecular dynamics [BBO + 83] 8722 molecules, 40 iterations. 128 tomcatv Thompsons solver mesh genera tion [SPE90] 512x512 matrices, 100 iters 147 waterns Water molecule force simulation [WOT + 95] <p> simulation <ref> [WOT + 95] </ref> 16K particles, 4 iters 33 em3d 3D electromagnetic wave propa gation [CDG + 93] 128K nodes, degree 6, 40% remote edges, distance span 1, 100 iterations 166 lu Contiguous blocked dense LU factorization [WOT + 95] 512x512 matrix, 16x16 blocks 75 ocean Simulation of eddy currents [WOT + 95] 514x514 ocean size 62 moldyn Molecular dynamics [BBO + 83] 8722 molecules, 40 iterations. 128 tomcatv Thompsons solver mesh genera tion [SPE90] 512x512 matrices, 100 iters 147 waterns Water molecule force simulation [WOT + 95] 4096 molecules 273 watersp Spatial water molecule force sim ulation [WOT + 95] <p> LU factorization <ref> [WOT + 95] </ref> 512x512 matrix, 16x16 blocks 75 ocean Simulation of eddy currents [WOT + 95] 514x514 ocean size 62 moldyn Molecular dynamics [BBO + 83] 8722 molecules, 40 iterations. 128 tomcatv Thompsons solver mesh genera tion [SPE90] 512x512 matrices, 100 iters 147 waterns Water molecule force simulation [WOT + 95] 4096 molecules 273 watersp Spatial water molecule force sim ulation [WOT + 95] 2744 molecules 65 95 col and the messaging subsystem that provide additional evidence for the interaction of the applications with the FGDSM systems. 4.3.1 Transparent Shared Memory Performance Blizzard/T. <p> eddy currents <ref> [WOT + 95] </ref> 514x514 ocean size 62 moldyn Molecular dynamics [BBO + 83] 8722 molecules, 40 iterations. 128 tomcatv Thompsons solver mesh genera tion [SPE90] 512x512 matrices, 100 iters 147 waterns Water molecule force simulation [WOT + 95] 4096 molecules 273 watersp Spatial water molecule force sim ulation [WOT + 95] 2744 molecules 65 95 col and the messaging subsystem that provide additional evidence for the interaction of the applications with the FGDSM systems. 4.3.1 Transparent Shared Memory Performance Blizzard/T. Moreover, each execution time is broken down into three components: Computation time. <p> Application Problem Input Data Set em3d 3D electromagnetic wave propagation [CDG + 93] 128K nodes, degree 6, 40% remote edges, distance span 1 (processors share edges only with their immediate neighbors), 100 iterations appbt 3D implementation of CFD [BM95] 40x40x40 matrices, 4 iters barnes Barnes-Hut N-body simulation <ref> [WOT + 95] </ref> 16K particles, 4 iters tomcatv Thompsons solver mesh generation [SPE90] 512x512 matrices, 100 iters lu Contiguous blocked dense LU factoriza tion [WOT + 95] 512x512 matrix, 16x16 blocks water Spatial water molecule force simulation [WOT + 95] 4096 molecules 129 The loop-backedge handshake on the average lowers SMP-correctness <p> span 1 (processors share edges only with their immediate neighbors), 100 iterations appbt 3D implementation of CFD [BM95] 40x40x40 matrices, 4 iters barnes Barnes-Hut N-body simulation <ref> [WOT + 95] </ref> 16K particles, 4 iters tomcatv Thompsons solver mesh generation [SPE90] 512x512 matrices, 100 iters lu Contiguous blocked dense LU factoriza tion [WOT + 95] 512x512 matrix, 16x16 blocks water Spatial water molecule force simulation [WOT + 95] 4096 molecules 129 The loop-backedge handshake on the average lowers SMP-correctness overhead in Blizzard/S (Blizzard/ES) by up to 4%. <p> 3D implementation of CFD [BM95] 40x40x40 matrices, 4 iters barnes Barnes-Hut N-body simulation <ref> [WOT + 95] </ref> 16K particles, 4 iters tomcatv Thompsons solver mesh generation [SPE90] 512x512 matrices, 100 iters lu Contiguous blocked dense LU factoriza tion [WOT + 95] 512x512 matrix, 16x16 blocks water Spatial water molecule force simulation [WOT + 95] 4096 molecules 129 The loop-backedge handshake on the average lowers SMP-correctness overhead in Blizzard/S (Blizzard/ES) by up to 4%. Table 5.4: Base system speedups for 16 uniprocessor nodes Application speedups running on a uniprocessor-node (no SMP overhead) implementation of Blizzard with 16 nodes.
Reference: [Wul81] <author> William A Wulf. </author> <title> Compilers and computer architecture. </title> <journal> IEEE Computer, </journal> <volume> 14(7):41 47, </volume> <month> July </month> <year> 1981. </year>
Reference-contexts: Since these primitives are available to programs running at user level, a programmer or compiler can tailor memory semantics to a particular program or data structure, much as RISC processors enable compilers to tailor instruction sequences to a function call or data reference <ref> [Wul81] </ref>. Tempest provides four types of mechanisms: Virtual Memory Management. With user-level virtual-memory management, a compiler or runtime system can manage a programs address space. Tempest provides memory mapping calls that the coherence protocol uses to manage a conventional flat address space. <p> Since these primitives are available to programs running at user level, a programmer or compiler can tailor memory semantics to a particular program or data structure, much as RISC processors enable compilers to tailor instruction sequences to a function call or data reference <ref> [Wul81] </ref>. Tempest provides four types of mechanisms: 177 Virtual Memory Management. With user-level virtual-memory management, a compiler or runtime system can manage a programs address space. Tempest provides memory mapping calls that coherence protocol uses to manage a conventional flat address space.
Reference: [YKA96] <author> Donald Yeung, John Kubiatowicz, and Anant Agarwal. MGS: </author> <title> A multigrain shared memory system. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Quad-processor nodes also exhibit performance competitive to uniprocessor nodes, and are especially beneficial for Blizzard/E, converting high-overhead FGDSM operations (e.g., write to read-only pages) to fast SMP local accesses. This result corroborates previous findings for (high-overhead) DVSM implementations on SMP clusters <ref> [YKA96] </ref>. Quad-processor nodes also increase synchronization time in the loop-backedge handshake because a protocol handler must wait for three processors to reach a loop-backedge. This result indicates that the loop-backedge handshake may be suitable for smallscale SMP nodes while instrumented synchronization may be suitable for larger SMP nodes. <p> Many variations thereof have since emerged such as Munin [CBZ91], TreadMarks [KDCZ93], SoftFLASH [ENCH96], MGS <ref> [YKA96] </ref>, and Cashmere [KHS + 97]. These systems all use standard virtual address translation mechanisms to implement coherence across nodes, and rely on relaxed memory consistency models and careful program annotation to support fine-grain sharing. SoftFLASH, MGS, and Cashmere address SMP-node implementations of VM-based software DSM.
Reference: [ZIS + 97] <author> Yuanyuan Zhou, Liviu Iftode, Jaswinder Pal Singh, Kai Li, Brian R. Toonen, Ioannis Schoinas, Mark D. Hill, and David A. Wood. </author> <title> Relaxed consistency and coherence granularity in dsm systems: A performance evaluation. </title> <booktitle> In Sixth ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <month> June </month> <year> 1997. </year> <month> 206 </month>
Reference-contexts: Therefore, FGDSM systems on message-passing hardware must rely on techniques that require little or no additional hardware. Like SVM systems, these systems use address translation hardware to map shared addresses to local memory pages but enforce coherence at a finer granularity. FGDSM systems achieve performance competitive to SVM systems <ref> [ZIS + 97] </ref> without having to resort to weak consistency models. Blizzard/CM-5 [SFL + 94], developed by the author and others for the TMC CM-5, was the first FGDSM system on messaging passing hardware. <p> The interrupt is caught by the network device driver and a Unix signal is forwarded to the user process.The roundtrip time per notification is around 70 microseconds (~7600 cycles), nearly an order of magnitude higher than in the CM-5. Published performance results <ref> [ZIS + 97] </ref> from this implementation suggest that only in limited cases is this scheme competitive to other alternatives. In particular, it performs adequately when the program creates very little messaging activity or when it creates too much. <p> On the other hand, unlike modern software page-based shared memory approaches [KDCZ93], they can support shared memory without relaxing the consistency model from sequential consistency [KDCZ93], while still offering the same or better performance <ref> [ZIS + 97] </ref>. Unique among existing FGDSM systems is Blizzards ability to eliminate the cost of shared memory and offer to the application the raw messaging performance. Blizzard supports the Tempest interface [Rei94] which exposes to user level the access control and messaging mechanisms. <p> Sequentially consistent page-based systems perform poorly on most of these codes because of their fine-grain communication and write sharing [CDK + 94, SFL + 94]. Page-granularity systems with weaker consistency models however, offer competitive performance to sequentially consistent software FGDSM systems <ref> [ZIS + 97] </ref> but complicate the programming model. Besides the nine transparent shared memory (TSM) versions, for three of the applications custom protocol (CS) versions have been developed by the WWT research group [FLR + 94,MSH + 95]. <p> However, at the same time it decreases fragmentation and therefore, the amount of useless data transferred (when sharing along the column a whole block is transferred to access a single word). Therefore, it results in better overall performance, especially on FGDSM systems such as Blizzard <ref> [ZIS + 97] </ref>. Moldyn is a molecular dynamics application [BBO + 83]. Molecules are uniformly distributed in a cubical region and the system computes a molecules velocity and the force exerted by other molecules. An interaction list (rebuilt every 20 iterations) limits interaction with molecules within a cutoff radius. <p> Studies have shown that they can offer competitive performance to sequentially consistent software FGDSM systems <ref> [ZIS + 97] </ref>. However, weaker consistency models complicate introduce complexity in the software design. With the exception of the Alpha and PowerPC architectures, most hardware implementations of shared support simple consistency models. <p> Introducing relaxed consistency models specifically for software DSM systems, complicates the porting of shared memory applica tions to these systems. FGDSM systems like Blizzard and Digitals Shasta can also support relaxed consistency models but the performance benefits are not significant <ref> [SGT96, ZIS + 97] </ref>. Reported application performance results for Blizzard and Digitals Shasta demonstrate that FGDSM systems can achieve significant speedups for many scientific applications. Both these systems achieve comparable speedups for shared memory applications despite hardware and software differ ences between the two systems [SGT96,SFH + 97].
References-found: 167

