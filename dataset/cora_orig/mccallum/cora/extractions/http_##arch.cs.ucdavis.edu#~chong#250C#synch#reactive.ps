URL: http://arch.cs.ucdavis.edu/~chong/250C/synch/reactive.ps
Refering-URL: http://arch.cs.ucdavis.edu/~chong/250C.html
Root-URL: http://www.cs.ucdavis.edu
Title: Reactive Synchronization Algorithms for Multiprocessors  
Author: Beng-Hong Lim and Anant Agarwal 
Address: Cambridge, MA 02139  
Affiliation: Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: Synchronization algorithms that are efficient across a wide range of applications and operating conditions are hard to design because their performance depends on unpredictable run-time factors. The designer of a synchronization algorithm has a choice of protocols to use for implementing the synchronization operation. For example, candidate protocols for locks include test-and-set protocols and queueing protocols. Frequently, the best choice of protocols depends on the level of contention: previous research has shown that test-and-set protocols for locks outperform queueing protocols at low contention, while the opposite is true at high contention. This paper investigates reactive synchronization algorithms that dynamically choose protocols in response to the level of contention. We describe reactive algorithms for spin locks and fetch-and-op that choose among several shared-memory and message-passing protocols. Dynamically choosing protocols presents a challenge: a reactive algorithm needs to select and change protocols efficiently, and has to allow for the possibility that multiple processes may be executing different protocols at the same time. We describe the notion of consensus objects that the reactive algorithms use to preserve correctness in the face of dynamic protocol changes. Experimental measurements demonstrate that reactive algorithms perform close to the best static choice of protocols at all levels of contention. Furthermore, with mixed levels of contention, reactive algorithms outperform passive algorithms with fixed protocols, provided that contention levels do not change too frequently. Measurements of several parallel applications show that reactive algorithms result in modest performance gains for spin locks and significant gains for fetch-and-op. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal et al. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. </title> <booktitle> In Proceedings of Workshop on Scalable Shared Memory Multiprocessors. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year> <note> An extended version of this paper appears as MIT/LCS Memo TM-454, </note> <year> 1991. </year>
Reference-contexts: Without contention, the overhead is the latency of an acquire-release pair. With contention, the overhead is the time to pass ownership of the lock from a process to another. <ref> [1] </ref>, a cache-coherent distributed-memory multiprocessor. Without contention, the shaded bars on each time-line represent the total time to execute a lock acquire and release. With contention, the shaded bars represent the time to pass ownership of the lock from the releasing processor to a waiting processor. <p> can give up on being reactive and use the best protocol for the average contention level experienced thus far. 4 Experimental Results In this section, we present experimental measurements that compare the performance of the reactive spin lock and fetch-and-op algorithms with conventional passive algorithms on the MIT Alewife multiprocessor <ref> [1] </ref>. The Alewife architecture is representative of a large-scale multiprocessor with cache-coherent distributed shared memory. Nodes in the multiprocessor communicate via messages through a 2-D mesh network. Memory is physically distributed among the nodes, and cache coherence is maintained using the LimitLESS cache coherence protocol [4].
Reference: [2] <author> Thomas E. Anderson. </author> <title> The Performance Implications of Spin Lock Alternatives for Shared-Memory Multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: A remedy is a queueing protocol that constructs a software queue of waiters to reduce contention. However, queuing comes at the price of a higher latency in the absence of contention <ref> [2, 16] </ref>. The right choice between the two protocols depends on the level of contention experienced by the lock. for an acquire-release pair of a spin lock on the Alewife machine protocols spin-wait. Without contention, the overhead is the latency of an acquire-release pair. <p> In Section 5 we will also consider message-passing protocols for spin locks and fetch-and-op. 2.1 Passive Spin-Lock Algorithms Recent research has resulted in scalable spin-lock algorithms that alleviate the detrimental effects of memory contention <ref> [2, 8, 16] </ref>. The research demonstrated that test-and-set with randomized exponential backoff and queuing were the most promising spin-lock protocols. <p> A process releases a lock by setting the flag to false. To avoid excessive communication traffic under high contention, a waiting process introduces some delay in between accesses to the flag. In <ref> [2] </ref>, Anderson demonstrated that randomized exponential backoff was an effective method for selecting the amount of delay. Henceforth, we will refer to test--and-set with exponential backoff simply as test-and-set locks, and test-and-test-and-set with exponential backoff simply as test-and-test-and-set locks. <p> Memory contention is reduced because each waiter spins on a different memory location and only one waiter is signalled when the lock is released. Queue locks have the additional advantage of providing fair access to the lock. Several queue lock protocols were developed independently by Anderson <ref> [2] </ref>, Graunke and Thakkar [8], and Mellor-Crummey and Scott [16]. In this paper, we use the MCS (Mellor-Crummey and Scott) queue lock because it has the best performance among the queue locks on our system. <p> The delay between lock acquisitions forces the lock to migrate between caches when there is contention. This test program is similar to that used by Anderson <ref> [2] </ref>. Each data point represents the average lock overhead per critical section with P processors contending for the lock. The overhead is measured by subtracting the amount of time the test would have taken, given zero-overhead spin locks, from the actual running time.
Reference: [3] <author> P.J. Burns et al. </author> <title> Vectorization of Monte-Carlo Particle Transport: An Architectural Study using the LANL Benchmark "Gamteb". </title> <booktitle> In Proc. Supercomputing '89, </booktitle> <address> New York, NY, </address> <month> November </month> <year> 1989. </year> <pages> IEEE/ACM. </pages>
Reference-contexts: They demonstrate the utility of having a reactive algorithm select the protocol to use. To better understand the results, we describe the characteristics of each application. Gamteb Gamteb <ref> [3] </ref> is a photon transport simulation based on the Monte Carlo method. In this simulation, Gamteb was run with an input parameter of 2048 particles. Gamteb updates a set of nine interaction counters using fetch-and-increment.
Reference: [4] <author> David Chaiken, John Kubiatowicz, and Anant Agarwal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 224-234. </pages> <publisher> ACM, </publisher> <month> April </month> <year> 1991. </year>
Reference-contexts: The Alewife architecture is representative of a large-scale multiprocessor with cache-coherent distributed shared memory. Nodes in the multiprocessor communicate via messages through a 2-D mesh network. Memory is physically distributed among the nodes, and cache coherence is maintained using the LimitLESS cache coherence protocol <ref> [4] </ref>. Cache-coherent atomic fetch-and-store is directly supported in hardware. At the time this research was performed, the Alewife multiprocessor was still being implemented. We relied on an accurate cycle-by-cycle simulation of the machine to gather the data presented in this section. The simulations assume a processor 33MHz clock frequency.
Reference: [5] <author> James R. Goodman, Mary K. Vernon, and Philip J. Woest. </author> <title> Efficient Synchronization Primitives for Large-Scale Cache-Coherent Multiprocessors. </title> <booktitle> In Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS III), </booktitle> <pages> pages 64-75, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: In this paper, we use the software combining tree algorithm for fetch-and-op presented by Goodman, Vernon and Woest in <ref> [5] </ref>. 2.3 The Problem with Passive Algorithms The problem with passive algorithms is that they fix their choice of protocols, and are thus optimized for a certain level of contention/concurrency at the synchronization operation. <p> A variable protected by a test-and-test-and-set lock. 2. A variable protected by a queue lock. 3. A software combining tree by Goodman et al. <ref> [5] </ref>. Each of these protocols satisfies the properties listed in Section 3.3, allowing the reactive fetch-and-op algorithm to use mechanisms similar to the reactive spin lock algorithm for dynamically selecting and changing protocols. For the first two protocols, the consensus objects are the locks protecting the centralized variables. <p> Reactive algorithms also outperform passive algorithms when there are a number of synchronization objects, each with different levels of contention. Measurements of several applications show that the reactive algorithms result in modest performance gains for spin locks and significant gains for fetch-and-op. Several multiprocessor architectures, e.g., the Wisconsin Mul-ticube <ref> [5] </ref> and Stanford DASH [13], include hardware support for queue locks. Hardware queuing has the advantage of low lock latency in the absence of contention.
Reference: [6] <author> A. Gottlieb, R. Grishman, C. P. Kruskal, K. P. McAuliffe, L. Rudolph, and M. Snir. </author> <title> The NYU Ultracomputer Designing a MIMD Shared-Memory Parallel Machine. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-32(2):175-189, </volume> <month> February </month> <year> 1983. </year>
Reference-contexts: If latency is a concern, the reactive spin lock algorithm will provide the low latency of a test-and-set lock with the scalability of a queue lock. This reduces the motivation for providing hardware support for queue locks. The NYU Ultracomputer <ref> [6] </ref> includes hardware support for combining fetch-and-add operations through the interconnection network. Although software combining algorithms have been proposed as an alternative to hardware combining, they have the disadvantage of high latencies.
Reference: [7] <author> Allan Gottlieb, B. D. Lubachevsky, and Larry Rudolph. </author> <title> Basic Techniques for the Efficient Coordination of Very Large Numbers of Cooperating Sequential Processors. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(2) </volume> <pages> 164-189, </pages> <month> April </month> <year> 1983. </year>
Reference-contexts: In this simulation, TSP solves an 11-city tour. To ensure a deterministic amount of work, we seed the best path value with the optimal path. The global task queue is based on an algorithm for a concurrent queue in <ref> [7] </ref> that allows multiple processes simultaneous access to the queue. Fetch-and-increment operations synchronize access to the queue. Contention for the fetch-and-increment operation in this application depends on the number of processors.
Reference: [8] <author> Gary Graunke and Shreekant Thakkar. </author> <title> Synchronization Algorithms for Shared-Memory Multiprocessors. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 60-70, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: In Section 5 we will also consider message-passing protocols for spin locks and fetch-and-op. 2.1 Passive Spin-Lock Algorithms Recent research has resulted in scalable spin-lock algorithms that alleviate the detrimental effects of memory contention <ref> [2, 8, 16] </ref>. The research demonstrated that test-and-set with randomized exponential backoff and queuing were the most promising spin-lock protocols. <p> Queue locks have the additional advantage of providing fair access to the lock. Several queue lock protocols were developed independently by Anderson [2], Graunke and Thakkar <ref> [8] </ref>, and Mellor-Crummey and Scott [16]. In this paper, we use the MCS (Mellor-Crummey and Scott) queue lock because it has the best performance among the queue locks on our system. The MCS queue lock maintains a pointer to the tail of a software queue of lock waiters.
Reference: [9] <author> Anna Karlin, Kai Li, Mark Manasse, and Susan Owicki. </author> <title> Empirical Studies of Competitive Spinning for A Shared-Memory Multiprocessor. </title> <booktitle> In 13th ACM Symposium on Operating Systems Principles (SOSP), </booktitle> <pages> pages 41-55, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: This paper focuses on reactive synchronization algorithms that choose protocols dynamically, but that assume spin waiting as a fixed waiting mechanism. Previous research <ref> [9, 14] </ref> has already demonstrated the utility of dynamically choosing waiting mechanisms. However, to the best of our knowledge, there has not been any experimental research on the feasibility and performance benefits of dynamically selecting synchronization protocols.
Reference: [10] <author> Clyde Kruskal, Larry Rudolph, and Marc Snir. </author> <title> Efficient Synchronization on Multiprocessors with Shared Memory. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(4) </volume> <pages> 579-601, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: If so, it signals that successor, otherwise it empties the queue. See [16] for further details. 2.2 Passive Fetch-and-Op Algorithms Fetch-and-op is a powerful primitive that can be used to implement higher-level synchronization operations. When the operation is combinable <ref> [10] </ref>, e.g., fetch-and-add, combining techniques can be used to compute the operation in parallel. Several software algorithms can be used to implement fetch-and-op. We consider the following in this paper. Lock-Based Fetch-and-Op A straightforward implementation of fetch-and-op is to protect the fetch-and-op variable with a mutual exclusion lock.
Reference: [11] <author> John Kubiatowicz and Anant Agarwal. </author> <title> Anatomy of a Message in the Alewife Multiprocessor. </title> <booktitle> In International Supercomputing Conference (ICS) 1993, </booktitle> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year> <note> IEEE. </note>
Reference-contexts: As in MP3D, we see that the higher latency of the MCS lock has a negligible impact on execution times. 5 Reactive Algorithms and Message-Passing Protocols We now consider reactive algorithms that select between shared-memory and message-passing protocols. Recent architectures for scalable shared-memory multiprocessors <ref> [11, 12, 17] </ref> implement the shared-memory abstraction on top of a collection of processing nodes that communicate via messages through an interconnection network. They allow software to bypass the shared-memory abstraction and access the message layer directly. These architectures provide an opportunity to use message-passing protocols for synchronization operations.
Reference: [12] <author> Jeffrey Kuskin et al. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year> <note> IEEE. </note>
Reference-contexts: As in MP3D, we see that the higher latency of the MCS lock has a negligible impact on execution times. 5 Reactive Algorithms and Message-Passing Protocols We now consider reactive algorithms that select between shared-memory and message-passing protocols. Recent architectures for scalable shared-memory multiprocessors <ref> [11, 12, 17] </ref> implement the shared-memory abstraction on top of a collection of processing nodes that communicate via messages through an interconnection network. They allow software to bypass the shared-memory abstraction and access the message layer directly. These architectures provide an opportunity to use message-passing protocols for synchronization operations.
Reference: [13] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam. </author> <title> The Stanford Dash Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Measurements of several applications show that the reactive algorithms result in modest performance gains for spin locks and significant gains for fetch-and-op. Several multiprocessor architectures, e.g., the Wisconsin Mul-ticube [5] and Stanford DASH <ref> [13] </ref>, include hardware support for queue locks. Hardware queuing has the advantage of low lock latency in the absence of contention. However, application measurements indicate that the additional latency of the MCS queue lock is not a significant factor unless locking is performed frequently at a very fine granularity.
Reference: [14] <author> Beng-Hong Lim and Anant Agarwal. </author> <title> Waiting Algorithms for Synchronization in Large-Scale Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(3) </volume> <pages> 253-294, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: This paper focuses on reactive synchronization algorithms that choose protocols dynamically, but that assume spin waiting as a fixed waiting mechanism. Previous research <ref> [9, 14] </ref> has already demonstrated the utility of dynamically choosing waiting mechanisms. However, to the best of our knowledge, there has not been any experimental research on the feasibility and performance benefits of dynamically selecting synchronization protocols.
Reference: [15] <author> Mark S. Manasse, Lyle A. McGeoch, and Daniel D. Sleator. </author> <title> Competitive Algorithms for On-line Problems. </title> <booktitle> In Proceedings of the 20th Annual Symposium on Theory of Computing, </booktitle> <pages> pages 322-333, </pages> <address> Chicago, IL, </address> <month> May </month> <year> 1988. </year> <note> ACM. </note>
Reference-contexts: Since the decision to switch is an instance of an on-line problem where decisions have to be made without knowledge of future contention levels, a possible policy is to use competitive techniques <ref> [15] </ref> to decide when to switch protocols. With competitive techniques, the worst-case performance of the switching policy can be bounded by a constant.
Reference: [16] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: A remedy is a queueing protocol that constructs a software queue of waiters to reduce contention. However, queuing comes at the price of a higher latency in the absence of contention <ref> [2, 16] </ref>. The right choice between the two protocols depends on the level of contention experienced by the lock. for an acquire-release pair of a spin lock on the Alewife machine protocols spin-wait. Without contention, the overhead is the latency of an acquire-release pair. <p> In Section 5 we will also consider message-passing protocols for spin locks and fetch-and-op. 2.1 Passive Spin-Lock Algorithms Recent research has resulted in scalable spin-lock algorithms that alleviate the detrimental effects of memory contention <ref> [2, 8, 16] </ref>. The research demonstrated that test-and-set with randomized exponential backoff and queuing were the most promising spin-lock protocols. <p> Queue locks have the additional advantage of providing fair access to the lock. Several queue lock protocols were developed independently by Anderson [2], Graunke and Thakkar [8], and Mellor-Crummey and Scott <ref> [16] </ref>. In this paper, we use the MCS (Mellor-Crummey and Scott) queue lock because it has the best performance among the queue locks on our system. The MCS queue lock maintains a pointer to the tail of a software queue of lock waiters. <p> If the queue was empty, the process owns the lock; otherwise it waits for a signal from its predecessor. To release a lock, a process checks to see if it has a waiting successor. If so, it signals that successor, otherwise it empties the queue. See <ref> [16] </ref> for further details. 2.2 Passive Fetch-and-Op Algorithms Fetch-and-op is a powerful primitive that can be used to implement higher-level synchronization operations. When the operation is combinable [10], e.g., fetch-and-add, combining techniques can be used to compute the operation in parallel. Several software algorithms can be used to implement fetch-and-op.
Reference: [17] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year> <note> IEEE. </note>
Reference-contexts: As in MP3D, we see that the higher latency of the MCS lock has a negligible impact on execution times. 5 Reactive Algorithms and Message-Passing Protocols We now consider reactive algorithms that select between shared-memory and message-passing protocols. Recent architectures for scalable shared-memory multiprocessors <ref> [11, 12, 17] </ref> implement the shared-memory abstraction on top of a collection of processing nodes that communicate via messages through an interconnection network. They allow software to bypass the shared-memory abstraction and access the message layer directly. These architectures provide an opportunity to use message-passing protocols for synchronization operations.
Reference: [18] <author> Z. Segall and L. Rudolph. </author> <title> Dynamic Decentralized Cache Schemes for MIMD Parallel Processors. </title> <booktitle> In Proceedions of the 11th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 340-347. </pages> <publisher> IEEE, </publisher> <month> June </month> <year> 1984. </year>
Reference-contexts: Test-and-set with Exponential Backoff With a test-and-set protocol, a process requests a lock by repeatedly executing a test-and-set instruction on a boolean flag until it successfully changes the flag from false to true. In a variation, known as test-and-test-and-set <ref> [18] </ref>, waiting processes read-poll the flag and attempt a test-and-set only when the flag appears to be false. A process releases a lock by setting the flag to false. To avoid excessive communication traffic under high contention, a waiting process introduces some delay in between accesses to the flag.
Reference: [19] <author> J.P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <type> Technical Report CSL-TR-92-526, </type> <institution> Stanford University, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: Nevertheless, the reactive spin lock still achieves performance that is close to the best passive algorithm. It should be useful for applications that perform locking frequently and at a very fine grain such that lock latencies becomes significant. MP3D MP3D is part of the SPLASH parallel benchmark suite <ref> [19] </ref>. For this simulation, we use problem sizes of 3,000 and 10,000 particles with the locking option turned on. We measured the time taken for 5 iterations. Locks are used in MP3D for atomic updating for cell parameters, where a cell represents a discretization of space.
Reference: [20] <author> Thorsten von Eicken, David Culler, Seth Goldstein, and Klaus Schauser. </author> <title> Active Messages: A Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: The advantage of using message-passing to implement synchronization operations over shared-memory is that under high contention, message-passing results in more efficient communication patterns, and atomicity is easily provided by making message handlers atomic with respect to other message handlers <ref> [20] </ref>. For example, fetch-and-op can be implemented by allocating the fetch-and-op variable in a memory location. To perform a fetch-and-op, a process sends a message to the home processor associated with that memory location. The message handler computes the operation on that location and returns the result with its reply.
Reference: [21] <author> Pen-Chung Yew, Nian-Feng Tzeng, and Duncan H. Lawrie. </author> <title> Distributing Hot-Spot Addressing in Large-Scale Multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(4):388-395, </volume> <month> April </month> <year> 1987. </year>
Reference-contexts: To execute a fetch-and-op, a process acquires the lock, updates the value of the fetch-and-op variable, and releases the lock. Software Combining Tree A drawback of lock-based implementations of fetch-and-op is that they may serialize fetch-and-op operations unnecessarily. Software combining techniques <ref> [21] </ref> can be used to compute the fetch-and-op in parallel. The idea is to combine multiple operations from different processes into a single operation whenever possible. The fetch-and-op variable is stored in the root of a software combining tree, and combining takes place at the internal nodes of the tree.
References-found: 21

