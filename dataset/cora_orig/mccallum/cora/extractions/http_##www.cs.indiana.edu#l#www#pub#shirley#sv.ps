URL: http://www.cs.indiana.edu/l/www/pub/shirley/sv.ps
Refering-URL: http://www.cs.indiana.edu/l/www/pub/shirley/
Root-URL: http://www.cs.indiana.edu
Title: Global Illumination via Density-Estimation  
Author: Peter Shirley Bretton Wade Philip M. Hubbard David Zareski Bruce Walter Donald P. Greenberg 
Address: Ithaca, NY, USA.  
Affiliation: Program of Computer Graphics, Cornell University,  
Abstract-found: 0
Intro-found: 1
Reference: 1. <author> A. Appel. </author> <title> Some techniques for shading machine renderings of solids. </title> <booktitle> In AFIPS 1968 Spring Joint Computing Conference, </booktitle> <pages> pages 37-49, </pages> <year> 1968. </year>
Reference-contexts: Because specular transport through glass is important in many applications, we have chosen to pursue the Monte Carlo approach. Although Monte Carlo radiosity schemes have been applied with great success using a priori computational meshes [11], there has been little success generating adaptive computational meshes. Appel <ref> [1] </ref> traced particles from the source to estimate direct lighting. Arvo [2] extended this idea to include illumination reflecting from mirrors before striking surfaces. Heckbert [8] extended Arvo's work to include adaptive meshing, and was the first to observe that this was a form of density-estimation.
Reference: 2. <author> James Arvo. </author> <title> Backward ray tracing. </title> <booktitle> Developments in Ray Tracing, </booktitle> <pages> pages 259-263, </pages> <year> 1986. </year> <note> ACM Siggraph '86 Course Notes. </note>
Reference-contexts: Although Monte Carlo radiosity schemes have been applied with great success using a priori computational meshes [11], there has been little success generating adaptive computational meshes. Appel [1] traced particles from the source to estimate direct lighting. Arvo <ref> [2] </ref> extended this idea to include illumination reflecting from mirrors before striking surfaces. Heckbert [8] extended Arvo's work to include adaptive meshing, and was the first to observe that this was a form of density-estimation. <p> It is important to note that surfaces that receive fewer particles will get wider kernels and coarser meshes. This avoids the under-sampling problems of traditional illumination ray tracing <ref> [2] </ref> in a manner similar to Collins [5]. Unlike Collins, we do not require any coherence between adjacent particle paths, so we can choose ap propriate kernel sizes for data that includes diffuse interreflection. Fig. 3.
Reference: 3. <author> Shenchang Eric Chen, Holly Rushmeier, Gavin Miller, and Douglass Turner. </author> <title> A progressive multi-pass method for global illumination. </title> <journal> Computer Graphics, </journal> <volume> 25(4) </volume> <pages> 165-174, </pages> <month> July </month> <year> 1991. </year> <booktitle> ACM Siggraph '91 Conference Proceedings. </booktitle>
Reference-contexts: Arvo [2] extended this idea to include illumination reflecting from mirrors before striking surfaces. Heckbert [8] extended Arvo's work to include adaptive meshing, and was the first to observe that this was a form of density-estimation. Chen et al. <ref> [3] </ref> used a kernel-based density-estimation technique to deal with caustic maps, and our density-estimation work can be considered an extension of their caustic map techniques to account for all illumination effects in a scene.
Reference: 4. <author> Micheal F. Cohen, Donald P. Greenberg, David S. Immel, and Philip J. Brock. </author> <title> An efficient radiosity approach for realistic image synthesis. </title> <journal> IEEE Computer Graphics & Applications, </journal> <volume> 6(2) </volume> <pages> 26-35, </pages> <year> 1986. </year>
Reference: 5. <author> Steven Collins. </author> <title> Adaptive splatting for specular to diffuse light transport. </title> <booktitle> In Proceedings of the Fifth Eurographics Workshop on Rendering, </booktitle> <pages> pages 119-135, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Chen et al. [3] used a kernel-based density-estimation technique to deal with caustic maps, and our density-estimation work can be considered an extension of their caustic map techniques to account for all illumination effects in a scene. Collins has one of the most sophisticated density-estimation techniques for global illumination <ref> [5] </ref>, but his method does not account for multiple diffuse reflections. Our strategy is similar to Heckbert's and Collins', but differs in that the meshing is delayed until all Monte Carlo particle tracing has been completed. <p> This generates a smoother estimate for f . An example kernel estimate is shown in Figure 2. Using kernel functions on the hit points is analogous to the idea of splatting in volume rendering [20], and is similar to the illumination ray tracing of Collins <ref> [5] </ref>. Sil-verman [16] notes that whatever properties the derivatives of k 1 have will be shared by f , so we can ensure a smooth estimate for f by choosing a smooth k 1 . <p> It is important to note that surfaces that receive fewer particles will get wider kernels and coarser meshes. This avoids the under-sampling problems of traditional illumination ray tracing [2] in a manner similar to Collins <ref> [5] </ref>. Unlike Collins, we do not require any coherence between adjacent particle paths, so we can choose ap propriate kernel sizes for data that includes diffuse interreflection. Fig. 3.
Reference: 6. <author> Reid Gershbein, Peter Schroder, and Pat Hanrahan. </author> <title> Textures and radiosity: Controlling emission and reflection with texture maps. </title> <journal> Computer Graphics, </journal> <pages> pages 51-58, </pages> <month> July </month> <year> 1994. </year> <booktitle> ACM Siggraph '94 Conference Proceedings. </booktitle>
Reference-contexts: If the volume of the function we are approximating, A, is not unity (so the function is not a probability density function), we can compensate by multiplying the sum by A. Fig. 2. Kernel estimate showing individual kernels. 3 See the recent texturing work of Gershbein et al. <ref> [6] </ref> for a more detailed analysis. In two dimensions, the irradiance function can be estimated as: H i (x) = n j=1 Where x j is the position of the jth hit point.
Reference: 7. <author> E. Bruce Goldstein. </author> <title> Sensation and Perception. </title> <publisher> Wadsworth Publishing Co., </publisher> <address> Belmont, Cali-fornia, </address> <year> 1980. </year>
Reference-contexts: We scale the brightness dimension by using z = R (x; y)H (x; y)= 1=3 where R (x; y) is photometric reflectance, H (x; y) is illuminance, w 0 is the white-point, and the exponent is due to Stevens Law <ref> [7] </ref>. We clamp z to 1.0, and choose w 0 so that z = * corresponds to the largest allowable brightness error. In practice, determining the scaling parameters requires some trial and error. Fortunately, in our experience we found useful parameter values after only two or three attempts.
Reference: 8. <author> Paul S. Heckbert. </author> <title> Adaptive radiosity textures for bidirectional ray tracing. </title> <journal> Computer Graphics, </journal> <volume> 24(3) </volume> <pages> 145-154, </pages> <month> August </month> <year> 1990. </year> <booktitle> ACM Siggraph '90 Conference Proceedings. </booktitle>
Reference-contexts: Appel [1] traced particles from the source to estimate direct lighting. Arvo [2] extended this idea to include illumination reflecting from mirrors before striking surfaces. Heckbert <ref> [8] </ref> extended Arvo's work to include adaptive meshing, and was the first to observe that this was a form of density-estimation. <p> It seems logical to guess a reasonable irradiance from the local denseness or sparseness of these hit points. For example, where the density of these points is high, we expect a high irradiance. As pointed out by Heckbert <ref> [8] </ref>, this is a classic density estimation problem, where we attempt to guess a plausible density function given a set of non-uniform random samples 2 .
Reference: 9. <author> Henrik Wann Jensen and Niels Jorgen Christensen. </author> <title> Bidirectional monte carlo ray tracing of complex objects using photon maps. </title> <journal> Computers & Graphics, </journal> <volume> 19(2), </volume> <year> 1995. </year>
Reference-contexts: An alternative way to store hit points would be with surface normal information in a 3D data structure, as is done by Ward [19] and by Jensen and Christensen <ref> [9] </ref>.
Reference: 10. <author> Dani Lischinski, Filippo Tampieri, and Donald P. Greenberg. </author> <title> Combining hierarchical ra-diosity and discontinuity meshing. </title> <journal> Computer Graphics, </journal> <pages> pages 199-208, </pages> <month> August </month> <year> 1993. </year> <booktitle> ACM Siggraph '93 Conference Proceedings. </booktitle>
Reference-contexts: Teller et al. argue that the reason for these surprisingly small limits is the high memory overhead of the data structures associated with the computational mesh. To solve this problem, we draw on an observation by Lischinski et al. <ref> [10] </ref>, that the computational mesh and the display mesh have different purposes and characteristics and therefore should be decoupled.
Reference: 11. <author> Laszlo Neumann, Martin Feda, Manfred Kopp, and Werner Purgathofer. </author> <title> A new stochastic radiosity method for highly complex scenes. </title> <booktitle> In Proceedings of the Fifth Eurographics Workshop on Rendering, </booktitle> <pages> pages 195-206, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Typically, a single representation is used for both the computational and display meshes (e.g. the static mesh used by Neumann et al. <ref> [11] </ref> and the adaptive mesh used by Teller et al. [18]). Very few display mesh solutions have been produced for environments with more than a few thousand initial surfaces. <p> Because specular transport through glass is important in many applications, we have chosen to pursue the Monte Carlo approach. Although Monte Carlo radiosity schemes have been applied with great success using a priori computational meshes <ref> [11] </ref>, there has been little success generating adaptive computational meshes. Appel [1] traced particles from the source to estimate direct lighting. Arvo [2] extended this idea to include illumination reflecting from mirrors before striking surfaces.
Reference: 12. <author> S. N. Pattanaik. </author> <title> Computational Methods for Global Illumination and Visualization of Complex 3D Environments. </title> <type> PhD thesis, </type> <institution> Birla Institute of Technology & Science, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: These limitations need to be overcome in a single system if the use of view independent global illumination solutions are to become widespread. Keeping the memory overhead low favors a Monte Carlo particle-shooting approach <ref> [12] </ref>, which only requires a ray-tracing acceleration structure. Handling local complexity in sub-quadratic time suggests either a clustering approach [17] or a Monte Carlo shooting approach 1 .
Reference: 13. <author> Holly E. Rushmeier. </author> <title> Realistic Image Synthesis for Scenes with Radiatively Participating Media. </title> <type> PhD thesis, </type> <institution> Cornell University, </institution> <month> May </month> <year> 1988. </year>
Reference-contexts: Quadratic time complexity. Any algorithm that computes interactions between all pairs of N objects will require at least O (N 2 ) time. This limits their utility in dealing with large models. Ideal specular effects. Many radiosity algorithms can only use the virtual image method <ref> [13] </ref>, which is practical for solving models with only a limited number of ideal, planar, specular objects. Real models have windows, gloss paint, and metal luminaire-reflectors, and in general, non-diffuse surfaces. Lack of parallelism.
Reference: 14. <author> William J. Schroeder, Jonathan A. Zarge, and William E. Lorensen. </author> <title> Decimation of triangle meshes. </title> <editor> In Edwin E. Catmull, editor, </editor> <booktitle> Computer Graphics (SIGGRAPH '92 Proceedings), </booktitle> <volume> volume 26, </volume> <pages> pages 65-70, </pages> <month> July </month> <year> 1992. </year>
Reference: 15. <author> Peter Shirley. </author> <title> Time complexity of monte carlo radiosity. </title> <booktitle> In Eurographics '91, </booktitle> <pages> pages 459-466, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: Handling local complexity in sub-quadratic time suggests either a clustering approach [17] or a Monte Carlo shooting approach 1 . A Monte 1 No global illumination algorithm has been proven to be sub-quadratic, but there is empirical evidence that both Monte Carlo shooting algorithms <ref> [15] </ref> and clustering algorithms [17] are sub-quadratic for reasonably well-behaved environments. Carlo shooting approach also allows for more general specular transport, and possesses inherent parallelism, as each shot can be processed independently.
Reference: 16. <author> B. W. Silverman. </author> <title> Density Estimation for Statistics and Data Analysis. </title> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1985. </year>
Reference-contexts: This generates a smoother estimate for f . An example kernel estimate is shown in Figure 2. Using kernel functions on the hit points is analogous to the idea of splatting in volume rendering [20], and is similar to the illumination ray tracing of Collins [5]. Sil-verman <ref> [16] </ref> notes that whatever properties the derivatives of k 1 have will be shared by f , so we can ensure a smooth estimate for f by choosing a smooth k 1 . Good choices for k 1 are similar to the choices used for splatting or pixel filtering. <p> We use triangles as mesh elements instead of rectangles because it simplifies the mesh-decimation algorithm. We have chosen to use Silverman's K 2 kernel function <ref> [16] </ref>: K 2 (u; v) = (3) The width of the kernel function, h, is chosen to relate it to the average distance between sample points on the ith surface. <p> Since each polygon can be meshed independent of other polygons, we only need keep this grid in memory for one polygon at a time, and paging has not been a problem on any of our runs. Near the boundary we use the reflection method <ref> [16] </ref> to avoid darkening near the edge of polygons. The trade-off between noise and blurring as controlled by C 1 is shown in Figure 3. In this figure C 2 has been set to 8, which is small enough to not affect the images.
Reference: 17. <author> Brian E. Smits, James R. Arvo, and Donald P. Greenberg. </author> <title> A clustering algorithm for ra-diosity in complex environments. </title> <journal> Computer Graphics, </journal> <volume> 28(3) </volume> <pages> 435-442, </pages> <month> July </month> <year> 1994. </year> <booktitle> ACM Siggraph '94 Conference Proceedings. </booktitle>
Reference-contexts: Most traditional radiosity algorithms also use a computational mesh to represent intermediate results in the light transport calculation (e.g., the piecewise-constant global solution of Smits et al. <ref> [17] </ref>). Typically, a single representation is used for both the computational and display meshes (e.g. the static mesh used by Neumann et al. [11] and the adaptive mesh used by Teller et al. [18]). <p> Keeping the memory overhead low favors a Monte Carlo particle-shooting approach [12], which only requires a ray-tracing acceleration structure. Handling local complexity in sub-quadratic time suggests either a clustering approach <ref> [17] </ref> or a Monte Carlo shooting approach 1 . A Monte 1 No global illumination algorithm has been proven to be sub-quadratic, but there is empirical evidence that both Monte Carlo shooting algorithms [15] and clustering algorithms [17] are sub-quadratic for reasonably well-behaved environments. <p> Handling local complexity in sub-quadratic time suggests either a clustering approach <ref> [17] </ref> or a Monte Carlo shooting approach 1 . A Monte 1 No global illumination algorithm has been proven to be sub-quadratic, but there is empirical evidence that both Monte Carlo shooting algorithms [15] and clustering algorithms [17] are sub-quadratic for reasonably well-behaved environments. Carlo shooting approach also allows for more general specular transport, and possesses inherent parallelism, as each shot can be processed independently. Because specular transport through glass is important in many applications, we have chosen to pursue the Monte Carlo approach.
Reference: 18. <author> Seth Teller, Celeste Fowler, Thomas Funkhouser, and Pat Hanrahan. </author> <title> Partitioning and ordering large radiosity calculations. </title> <journal> Computer Graphics, </journal> <volume> 28(3) </volume> <pages> 443-450, </pages> <month> July </month> <year> 1994. </year> <booktitle> ACM Siggraph '94 Conference Proceedings. </booktitle>
Reference-contexts: Typically, a single representation is used for both the computational and display meshes (e.g. the static mesh used by Neumann et al. [11] and the adaptive mesh used by Teller et al. <ref> [18] </ref>). Very few display mesh solutions have been produced for environments with more than a few thousand initial surfaces. The only implementation we are aware of that has produced a display mesh for more than 10,000 initial surfaces is the system by Teller et al. [18], which was run on a <p> used by Teller et al. <ref> [18] </ref>). Very few display mesh solutions have been produced for environments with more than a few thousand initial surfaces. The only implementation we are aware of that has produced a display mesh for more than 10,000 initial surfaces is the system by Teller et al. [18], which was run on a model with approximately 40,000 initial surfaces. Teller et al. argue that the reason for these surprisingly small limits is the high memory overhead of the data structures associated with the computational mesh. <p> Current radiosity methods use large data structures to accelerate visibility computations. These data structures are usually the limiting factor in performing large radiosity simulations <ref> [18] </ref>. In practice, an algorithm that stores more than a few hundred bytes per polygon in physical memory will not be practical. Difficulty with local complexity. <p> Difficulty with local complexity. In cases where the model has a very high global complexity (large numbers of surfaces), but a limited local complexity (only small subsets of surfaces are mutually visible), partitioning can be used to decompose the model into subsets which can be solved separately <ref> [18] </ref>. But if any subset has a high local complexity, then partitioning may not reduce the subproblems to a solvable size. This is a problem in an environment such as a hotel atrium. Quadratic time complexity.
Reference: 19. <author> Gregory J. Ward. </author> <title> The radiance lighting simulation and rendering system. </title> <journal> Computer Graphics, </journal> <volume> 28(2) </volume> <pages> 459-472, </pages> <month> July </month> <year> 1994. </year> <booktitle> ACM Siggraph '94 Conference Proceedings. </booktitle>
Reference-contexts: An alternative way to store hit points would be with surface normal information in a 3D data structure, as is done by Ward <ref> [19] </ref> and by Jensen and Christensen [9].
Reference: 20. <author> Lee Westover. </author> <title> Footprint evaluation for volume randering. </title> <journal> Computer Graphics, </journal> <volume> 24(4) </volume> <pages> 367-376, </pages> <month> August </month> <year> 1990. </year> <booktitle> ACM Siggraph '90 Conference Proceedings. </booktitle>
Reference-contexts: This generates a smoother estimate for f . An example kernel estimate is shown in Figure 2. Using kernel functions on the hit points is analogous to the idea of splatting in volume rendering <ref> [20] </ref>, and is similar to the illumination ray tracing of Collins [5]. Sil-verman [16] notes that whatever properties the derivatives of k 1 have will be shared by f , so we can ensure a smooth estimate for f by choosing a smooth k 1 .
Reference: 21. <author> David Zareski, Bretton Wade, Philip Hubbard, and Peter Shirley. </author> <title> Efficient parallel global illumination using density estimation. </title> <booktitle> In Proceedings of the 1995 Parallel Rendering Symposium, </booktitle> <year> 1995. </year>
Reference-contexts: We have also implemented a parallel version of the algorithm in C++ as two separate parallel programs <ref> [21] </ref>. The first is a parallel version of the serial particle tracer, and the second is a combined, parallel version of the both the serial density-estimation and mesh-decimation programs. The second parallel program also performs the hit point sorting. <p> A total of 96 million particles were traced (32 million in each of the red, green, and blue channels). Preliminary investigations indicate time can be reduced by more than one order of magnitude by using our parallel workstation cluster <ref> [21] </ref>. The proportion of time spent in each of the three programs was: particle-tracing: 49%; density-estimation and initial-meshing: 29%; mesh-decimation: 22%. 5 Conclusion We have presented a new global illumination method based on density estimation.
References-found: 21

