URL: ftp://ftp.cag.lcs.mit.edu/pub/papers/futures.ps.Z
Refering-URL: http://www.cag.lcs.mit.edu/alewife/papers/futures.html
Root-URL: 
Email: mohr@cs.yale.edu  kranz@lcs.mit.edu  halstead@crl.dec.com  
Title: Lazy Task Creation: A Technique for Increasing the Granularity of Parallel Programs  
Author: Eric Mohr David A. Kranz Robert H. Halstead, Jr. 
Keyword: Key words and phrases: parallel programming languages, load balancing, program partitioning, process migration, parallel Lisp, task management.  
Affiliation: Yale University  M.I.T.  DEC Cambridge Research Lab  
Abstract: Many parallel algorithms are naturally expressed at a fine level of granularity, often finer than a MIMD parallel system can exploit efficiently. Most builders of parallel systems have looked to either the programmer or a parallelizing compiler to increase the granularity of such algorithms. In this paper we explore a third approach to the granularity problem by analyzing two strategies for combining parallel tasks dynamically at run-time. We reject the simpler load-based inlining method, where tasks are combined based on dynamic load level, in favor of the safer and more robust lazy task creation method, where tasks are created only retroactively as processing resources become available. These strategies grew out of work on Mul-T [15], an efficient parallel implementation of Scheme, but could be used with other languages as well. We describe our Mul-T implementations of lazy task creation for two contrasting machines, and present performance statistics which show the method's effectiveness. Lazy task creation allows efficient execution of naturally expressed algorithms of a substantially finer grain than possible with previous parallel Lisp systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Agarwal, A., Lim, B.H., Kranz, D. and Kubiatow-icz, J., </author> <month> "APRIL: </month> <title> A Processor Architecture for Multiprocessing," </title> <booktitle> 17th Annual Int'l. Symp. on Computer Architecture, </booktitle> <address> Seattle, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: Checking for stack overflow. 3. Use of a global resource (the bus) for locking op erations. The ALEWIFE machine|a cache-coherent machine being developed at MIT with distributed, globally shared memory|is designed to address these problems. Its processing elements are modified SPARC 9 chips <ref> [1] </ref>. The modifications of interest here are fast traps for strict operations on futures and support for full/empty bits in each memory word. If a strict arithmetic operation or memory reference operates on a future a trap occurs; thus, explicit checks are not needed.
Reference: [2] <author> Arvind and D. Culler, </author> <title> "Dataflow Architectures," </title> <booktitle> Annual Reviews in Computer Science, Annual Reviews, </booktitle> <publisher> Inc., </publisher> <address> Palo Alto, Ca., </address> <year> 1986, </year> <pages> pp. 225-253. </pages>
Reference-contexts: Some researchers look to hardware specially designed to handle fine-grained tasks <ref> [2, 9] </ref>, while others have looked for ways to increase task granularity by grouping a number of potentially parallel operations together into a single sequential thread. <p> However, the main purpose of throttling is to reduce the memory requirements of parallel computations, not to increase granularity (which is generally fixed at a very fine level by data-flow architectures <ref> [2, 9] </ref>). Throttling thus serves the same purpose as our preference for depth-first scheduling and is not directly related to lazy task creation. 7 Conclusions and Future Work We are encouraged that our performance statistics support the theoretical benefits of lazy task creation.
Reference: [3] <author> Chaiken, D., Kubiatowicz, J., and Agarwal, A., </author> <title> "LimitLESS Directories: A Scalable Cache Coherence Scheme," MIT VLSI Memo. </title> <note> Submitted for publication. </note>
Reference-contexts: ALEWIFE's distributed caching scheme <ref> [3] </ref> reduces the need for remote references; preliminary results of current research at MIT on a new scheduler for ALEWIFE Mul-T show good performance even with simulation of network delays. 12 These programs are described either in Section 5.3 or in [15]. 13 It is interesting to note that the presence
Reference: [4] <author> Culler, D.E., </author> <title> "Managing Parallelism and Resources in Scientific Dataflow Programs," </title> <type> Ph.D. thesis, </type> <institution> M.I.T. Dept. of Electrical Engineering and Computer Science, </institution> <address> Cambridge, Mass., </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Our philosophy of encouraging programmers to expose parallelism while relying on the implementation to curb excess parallelism resembles that of data-flow researchers who have been concerned with throttling <ref> [4, 20] </ref>. However, the main purpose of throttling is to reduce the memory requirements of parallel computations, not to increase granularity (which is generally fixed at a very fine level by data-flow architectures [2, 9]).
Reference: [5] <author> Gabriel, R.P., and J. McCarthy, </author> <booktitle> "Queue-based Multi-processing Lisp," 1984 ACM Symp. on Lisp and Functional Programming, </booktitle> <address> Austin, Tex., </address> <month> Aug. </month> <year> 1984, </year> <pages> pp. 25-44. </pages>
Reference-contexts: At the other end of the spectrum a language can leave granularity decisions up to the programmer, providing tools for building tasks of acceptable granularity such as the propositional parameters of Qlisp <ref> [5, 7, 8] </ref>. Such fine control can be necessary in some cases to maximize performance, but there are costs in programmer effort and program clarity. <p> If such a Qlisp-style mechanism were used to create a hypothetical qfuture construct, we might write psum-tree as in Figure 3 (very similar to an example in <ref> [5] </ref>). In this example, cutoff-depth specifies a depth beyond which no tasks should be created. The predicate (&gt; cutoff-depth 0) tells qfuture whether or not to inline the recursive call.
Reference: [6] <author> Goldberg, B., </author> <title> "Multiprocessor Execution of Functional Programs," </title> <booktitle> Int'l. J. of Parallel Programming 17:5, </booktitle> <month> Oct. </month> <year> 1988, </year> <pages> pp. 425-473. </pages>
Reference-contexts: In the most attractive world, the programmer leaves the job of identifying parallel tasks to a parallelizing compiler. To achieve good performance, the compiler must create tasks of sufficient size based on estimating the cost of various pieces of code <ref> [6, 13] </ref>. But when execution paths are highly data-dependent (as for example with recursive symbolic programs), the cost of a piece of code is often unknown at compile time. If only known costs are used, the tasks produced may still be too fine-grained.
Reference: [7] <author> Goldman, R., and R.P. Gabriel, </author> <title> "Preliminary Results with the Initial Implementation of Qlisp," </title> <booktitle> 1988 ACM Symp. on Lisp and Functional Programming, </booktitle> <address> Snowbird, Utah, </address> <month> July </month> <year> 1988, </year> <pages> pp. 143-152. </pages>
Reference-contexts: At the other end of the spectrum a language can leave granularity decisions up to the programmer, providing tools for building tasks of acceptable granularity such as the propositional parameters of Qlisp <ref> [5, 7, 8] </ref>. Such fine control can be necessary in some cases to maximize performance, but there are costs in programmer effort and program clarity.
Reference: [8] <author> Goldman, R., R. Gabriel, and C. Sexton, </author> <title> "Qlisp: </title> <booktitle> Parallel Processing in Lisp," Springer-Verlag LNCS 441, Proceedings of U.S./Japan Workshop on Parallel Lisp, </booktitle> <month> June 5-7, </month> <year> 1989, </year> <institution> Tohoku University, </institution> <address> Sendai, Japan. </address>
Reference-contexts: At the other end of the spectrum a language can leave granularity decisions up to the programmer, providing tools for building tasks of acceptable granularity such as the propositional parameters of Qlisp <ref> [5, 7, 8] </ref>. Such fine control can be necessary in some cases to maximize performance, but there are costs in programmer effort and program clarity. <p> What if we control task creation explicitly as in Qlisp? In many of Qlisp's parallel constructs the programmer may supply a predicate which, when evaluated at run time, will determine whether or not a separate task is created (one such predicate, (qemptyp) <ref> [8] </ref>, tests the length of the work queue, achieving the same effect as our load-based inlining). If such a Qlisp-style mechanism were used to create a hypothetical qfuture construct, we might write psum-tree as in Figure 3 (very similar to an example in [5]). <p> do not believe that the differences between Encore and ALEWIFE task counts reflect significant differences in the implementations. 6 Related Work Load-based inlining has been studied previously in the Mul-T parallel Lisp system [15], and is also available in Qlisp by using (deque-size) or (qemptyp) to sense the current load <ref> [8, 26] </ref>. An analytical model of load-based inlining for programs like psum-tree has been developed by Weening [26, 27].
Reference: [9] <author> Gurd, J., C. Kirkham, and I. Watson, </author> <title> "The Manchester Prototype Dataflow Computer," </title> <journal> Comm. ACM 28:1, </journal> <month> January </month> <year> 1985, </year> <pages> pp. 34-52. </pages>
Reference-contexts: Some researchers look to hardware specially designed to handle fine-grained tasks <ref> [2, 9] </ref>, while others have looked for ways to increase task granularity by grouping a number of potentially parallel operations together into a single sequential thread. <p> However, the main purpose of throttling is to reduce the memory requirements of parallel computations, not to increase granularity (which is generally fixed at a very fine level by data-flow architectures <ref> [2, 9] </ref>). Throttling thus serves the same purpose as our preference for depth-first scheduling and is not directly related to lazy task creation. 7 Conclusions and Future Work We are encouraged that our performance statistics support the theoretical benefits of lazy task creation.
Reference: [10] <author> Halstead, R., </author> <title> "Multilisp: A Language for Concurrent Symbolic Computation," </title> <journal> ACM Trans. on Prog. Languages and Systems 7:4, </journal> <month> October </month> <year> 1985, </year> <pages> pp. 501-538. </pages>
Reference-contexts: Similar problems arise when a parallelizing compiler is parameterized with details of a certain machine. 1 We've taken an intermediate position in our research on Mul-T [15], a parallel version of Scheme based on the future construct of Multilisp <ref> [10, 11] </ref>. The programmer takes on the burden of identifying what can be computed safely in parallel, leaving the decision of exactly how the division will take place to the run-time system.
Reference: [11] <author> Halstead, R., </author> <title> "An Assessment of Multilisp: Lessons from Experience," </title> <booktitle> Int'l. J. of Parallel Programming 15:6, </booktitle> <month> Dec. </month> <year> 1986, </year> <pages> pp. 459-501. </pages>
Reference-contexts: Similar problems arise when a parallelizing compiler is parameterized with details of a certain machine. 1 We've taken an intermediate position in our research on Mul-T [15], a parallel version of Scheme based on the future construct of Multilisp <ref> [10, 11] </ref>. The programmer takes on the burden of identifying what can be computed safely in parallel, leaving the decision of exactly how the division will take place to the run-time system. <p> The sequentiality of iteration inherently limits parallelism; even if task creation overhead were nonexistent, the parallelism in a loop like parmap-cars will never exceed t f =t l (the cost of computing f vs. the cost of one loop iteration) <ref> [11] </ref>. For a fine-grained loop this ratio represents a real limitation on the number of processors that can be kept busy. This is a case where expressing an algorithm in the most convenient way inherently limits parallel performance.
Reference: [12] <author> Hudak, P., </author> <title> "Para-Functional Programming," </title> <journal> Computer, </journal> <volume> 19(8) </volume> <pages> 60-71, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: effort involved is little more than that required for systems with parallelizing compilers, where the programmer must be sure to code in such a way that parallelism is available. (We note that these dynamics of parallel programming are shared by functional languages; the philosophy and goals of the "para-functional" approach <ref> [12, 14] </ref> are similar to ours.) In order to support this programming style we must deal with questions of efficiency.
Reference: [13] <author> Hudak, P., and B. Goldberg, </author> <title> "Serial Combina-tors: `Optimal' Grains of Parallelism," </title> <booktitle> Functional Programming Languages and Computer Architecture, </booktitle> <publisher> Springer-Verlag LNCS 201, </publisher> <month> September </month> <year> 1985, </year> <pages> pp. 382-388. </pages>
Reference-contexts: In the most attractive world, the programmer leaves the job of identifying parallel tasks to a parallelizing compiler. To achieve good performance, the compiler must create tasks of sufficient size based on estimating the cost of various pieces of code <ref> [6, 13] </ref>. But when execution paths are highly data-dependent (as for example with recursive symbolic programs), the cost of a piece of code is often unknown at compile time. If only known costs are used, the tasks produced may still be too fine-grained.
Reference: [14] <author> Hudak, P., and L. Smith, </author> <title> "Para-Functional Programming: a Paradigm for Programming Multiprocessor Systems," </title> <booktitle> 12th ACM Symposium on Principles of Programming Languages, </booktitle> <month> January </month> <year> 1986, </year> <pages> pp. 243-254. </pages>
Reference-contexts: effort involved is little more than that required for systems with parallelizing compilers, where the programmer must be sure to code in such a way that parallelism is available. (We note that these dynamics of parallel programming are shared by functional languages; the philosophy and goals of the "para-functional" approach <ref> [12, 14] </ref> are similar to ours.) In order to support this programming style we must deal with questions of efficiency.
Reference: [15] <author> Kranz, D., R. Halstead, and E. Mohr, </author> <month> "Mul-T, </month>
Reference-contexts: Similar problems arise when a parallelizing compiler is parameterized with details of a certain machine. 1 We've taken an intermediate position in our research on Mul-T <ref> [15] </ref>, a parallel version of Scheme based on the future construct of Multilisp [10, 11]. The programmer takes on the burden of identifying what can be computed safely in parallel, leaving the decision of exactly how the division will take place to the run-time system. <p> The Encore Multimax 1 implementation of Mul-T <ref> [15] </ref>, based on the T system's ORBIT compiler [16, 17], is proof that the underlying parallel Lisp system can be made efficient enough; we must now figure out how to achieve sufficient task granularity. <p> We now consider how these idealized execution patterns match up with real-life execution patterns for these methods. 3 Comparison of Dynamic Methods Load-based inlining has an appealing simplicity and does in fact produce good results for some programs <ref> [15] </ref>, but we have noted several factors which decrease its effectiveness. <p> The second step is considered in Section 5.3. For the Encore implementation, the sequential comparison shows the overhead due to compiler-inserted future? checks on strict operations. Although the Encore implementation is engineered to minimize future-checking overhead <ref> [15] </ref>, the cost can be significant for some programs. Table 1 compares running times of several sequential programs 12 in T3.1 with the same program run in Mul-T on one processor. The Mul-T programs run between 1.4 and 2.2 times as long as their T3.1 counterparts. <p> ALEWIFE's distributed caching scheme [3] reduces the need for remote references; preliminary results of current research at MIT on a new scheduler for ALEWIFE Mul-T show good performance even with simulation of network delays. 12 These programs are described either in Section 5.3 or in <ref> [15] </ref>. 13 It is interesting to note that the presence of hardware tag checking may be more significant in machines supporting parallel Lisp than in machines supporting sequential Lisp. <p> creation are rather variable from run to run due to the variable timing of steal operations; we do not believe that the differences between Encore and ALEWIFE task counts reflect significant differences in the implementations. 6 Related Work Load-based inlining has been studied previously in the Mul-T parallel Lisp system <ref> [15] </ref>, and is also available in Qlisp by using (deque-size) or (qemptyp) to sense the current load [8, 26]. An analytical model of load-based inlining for programs like psum-tree has been developed by Weening [26, 27]. <p> Lazy task creation has no such restriction, interacting well with the unlimited lifetime of futures in Mul-T. The potential for deadlock when using load-based in-lining was described in <ref> [15] </ref>, but the example of Section 3.3 is more plausible than the scenario painted in [15]. It is interesting to note that selective load-based inlining, as is possible in Qlisp, could be used by a sophisticated programmer to ensure that inlining is never performed where it might cause deadlock. <p> Lazy task creation has no such restriction, interacting well with the unlimited lifetime of futures in Mul-T. The potential for deadlock when using load-based in-lining was described in <ref> [15] </ref>, but the example of Section 3.3 is more plausible than the scenario painted in [15]. It is interesting to note that selective load-based inlining, as is possible in Qlisp, could be used by a sophisticated programmer to ensure that inlining is never performed where it might cause deadlock.
References-found: 15

