URL: ftp://ftp.cs.toronto.edu/vis/sean/papers/dolson.msc_thesis.ps.gz
Refering-URL: http://www.cs.toronto.edu/vis/publications/abstracts/dolsonMSc.html
Root-URL: 
Title: Attentive Object Recognition in the Selective Tuning Network  
Author: by David C. Dolson 
Degree: A thesis submitted in conformity with the requirements for the degree of Master of Science  
Note: c Copyright by David C. Dolson 1997  
Address: Toronto  
Affiliation: Graduate Department of Computer Science University of  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> James R. Bergen, P. Anandan, Keith J. Hanna, and Rajesh Hingorani. </author> <title> Hierarchical model-based motion estimation. </title> <editor> In G. Sandini, editor, </editor> <booktitle> Computer Vision|ECCV'92: Second European Conference on Computer Vision, volume 588 of Lecture Notes in Computer Science, </booktitle> <pages> pages 237-252. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: Image orientation may also be analyzed at various scales via steerable pyramids [31]. A large amount of data can be analyzed at various scales for efficient analysis of texture segmentation, motion, or stereo correspondence over the entire image <ref> [1] </ref>. at the top of this pyramid. Figure 2.2 shows a processing pyramid with several top-level features. The high resolution input image is presented at the bottom of the pyramid. Successively higher levels compute sets of features at decreasing resolutions. <p> Bias units take a real value b l;p (x) 2 <ref> [0; 1] </ref>. The activity of a bias unit depends upon activities in higher level bias units, and modulates the weighting of the corresponding interpretive unit.
Reference: [2] <author> D. E. Broadbent. </author> <title> Perception and Communication. </title> <publisher> Pergamon Press, </publisher> <year> 1958. </year>
Reference-contexts: The phenomenon of attention has been likened to a filtering mechanism which selects particular stimuli to be passed along a limited-capacity channel to the main processing mechanism of the brain and to awareness <ref> [2] </ref>. Computationally, the general vision problem is intractable, but the use of attention allows tractable vision tasks to be performed [36]. So whether the limitation is communication channel capacity or algorithm complexity, selective filtering of stimulus is required.
Reference: [3] <author> P. J. Burt. </author> <title> The pyramid as a structure for efficient computation. </title> <editor> In A. Rosen-feld, editor, </editor> <title> Multiresolution Image Processing and Analysis, </title> <booktitle> chapter 2, </booktitle> <pages> pages 6-35. </pages> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference-contexts: This arrangement bears resemblance to the processing pyramid, which is advocated by the computer vision community as an efficient data structure for image processing. For example, Gaussian and Laplacian pyramids allow low-pass and band-pass analysis, respectively, of images at various scales <ref> [3] </ref>. Image orientation may also be analyzed at various scales via steerable pyramids [31]. A large amount of data can be analyzed at various scales for efficient analysis of texture segmentation, motion, or stereo correspondence over the entire image [1]. at the top of this pyramid.
Reference: [4] <author> Leonardo Chelazzi, Earl K. Miller, John Duncan, and Robert Desimone. </author> <title> A neural basis for visual search in inferior temporal cortex. </title> <journal> Nature, </journal> <volume> 363 </volume> <pages> 345-347, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: For the same stimulus in the same location, the cell responses varied depending on the location of the target of attention relative to the stimulus. Similar attentional modulations of cells have been recorded in IT by Chelazzi et al. <ref> [4] </ref>. In analogy to the experiments with V4 cells, effective and ineffective visual stimuli (pictures of objects in this case) were discovered for a neuron.
Reference: [5] <author> Charles E. Connor, Jack L. Gallant, Dean C. Preddie, and David C. Van Essen. </author> <title> Responses in area V4 depend on the spatial relationship between stimulus and attention. </title> <journal> Journal of Neurophysiology, </journal> <volume> 75(3) </volume> <pages> 1306-1308, </pages> <month> March </month> <year> 1996. </year>
Reference-contexts: The term "classical receptive field" denotes the size of a receptive field without the effects of attention. Chapter 2. Background 11 suppression only occurs when the attended, ineffective stimulus falls within the receptive field of the probed neuron. Neurobiological studies by Connor et al. <ref> [5] </ref> demonstrate attention-modulated V4 cell responses which apparently encode a conjunction of feature and location in an object-centered reference frame. For the same stimulus in the same location, the cell responses varied depending on the location of the target of attention relative to the stimulus.
Reference: [6] <author> Jon Driver. </author> <title> Object segmentation and visual neglect. </title> <journal> Behavioural Brain Research, </journal> <volume> 71 </volume> <pages> 135-146, </pages> <year> 1995. </year>
Reference-contexts: objects. 2.1.3 Is Attention Object-Based? The two stage model is perhaps a useful way of thinking about how early parallel processes may direct attention, but if the spotlight is directed solely according to basic features, how do we account for the Gestalt claim of immediate perception of entire figures? Driver <ref> [6] </ref> continues to argue Neisser's claim [24] that preattentive processes produce segmented objects. Recent experiments with neglect patients seem to indicate that attention is deployed to image regions that have already undergone figure-ground segmentation.
Reference: [7] <author> John Duncan. </author> <title> Cooperating brain systems in selective perception and action. </title> <editor> In T. Inui and J. L. McClelland, editors, </editor> <booktitle> Attention and Performance XVI, </booktitle> <pages> pages 549-578. </pages> <publisher> The MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: A review of early experiments can be found by Treisman [33], and recent perspectives are provided by Duncan <ref> [7] </ref> and Nakayama and Joseph [23]. Search seems easy when target items can be discriminated from non-targets by measuring a single dimension of colour contrast, curvature, orientation, direction of motion, or depth.
Reference: [8] <author> Kunihiko Fukushima. </author> <title> A neural network model for selective attention. </title> <editor> In Maureen Caudill and Charles Butler, editors, </editor> <booktitle> Proceedings of IEEE First International Con 104 BIBLIOGRAPHY 105 ference on Neural Networks, </booktitle> <volume> volume II, </volume> <pages> pages 11-18, </pages> <address> San Diego, CA, </address> <month> June </month> <year> 1987. </year> <note> IEEE. </note>
Reference-contexts: This is done recursively at each level of the network, forming not only a spotlight on the image but also a beam passing through all levels. 2.4.3 Attentive Neocognitron Fukushima describes a neural network capable of selective attention <ref> [8] </ref>. If there is more than one object in the field of view which might be recognized by the network, the network settles on one of them. The network can them be perturbed externally to cause it to settle on another object.
Reference: [9] <author> Kunihiko Fukushima and Taro Imagawa. </author> <title> Recognition and segmentation of connected characters with selective attention. </title> <booktitle> Neural Networks, </booktitle> <volume> 6 </volume> <pages> 33-41, </pages> <year> 1993. </year>
Reference-contexts: When a backward path is selected, the gain is increased in the forward path, providing another means by which forward signals are enhanced. So just as the forward paths gate the backward paths, the backward paths also gate the forward paths, Fukushima and Imagawa <ref> [9] </ref> modify the attentive neocognitron by adding a "search controller" that applies an attentional bias for location (a spotlight) to each of the layers. As expected, this spotlight reduces the number of objects that compete, thereby improving recognition performance.
Reference: [10] <author> Kunihiko Fukushima and Sei Miyarke. </author> <title> Neocognitron: A new algrorithm for pattern recognition tolerant of deformations and shifts in position. </title> <journal> Pattern Recognition, </journal> <volume> 15(6) </volume> <pages> 455-469, </pages> <year> 1982. </year>
Reference-contexts: The network can them be perturbed externally to cause it to settle on another object. Unsupervised learning allows the network to classify objects from the training set. The model is multi-layered and consists of both feed-forward and feedback connections. The forward connections resemble those of the neocognitron <ref> [10] </ref>. The backward connections serve to modify the gains and thresholds of forward units to achieve attention to one item. The feed-forward path is composed of two different types of cell layers: U S cells Chapter 2.
Reference: [11] <author> Peter Gouras. </author> <title> Color vision. </title> <editor> In Eric R. Kandel and James H. Schwartz, editors, </editor> <booktitle> Principles of Neural Science, chapter 30, </booktitle> <pages> pages 384-395. </pages> <publisher> Elsevier Science Publishing Co., Inc., </publisher> <address> second edition, </address> <year> 1985. </year>
Reference-contexts: Cell recordings have shown the visual cortex to be made up of many retinotopic mappings corresponding to different visual features with different sizes of receptive fields. In V1, for example, a hypercolumn contains columns for various orientations, plus pegs (also called blobs) concerned only with colour, not orientation <ref> [16, 11] </ref>. The inferotemporal cortex (IT) of the macaque monkey is thought to play an important role in the "what" of object discrimination and recognition.
Reference: [12] <author> John Hertz, Anders Krogh, and Richard G. Palmer. </author> <title> Introduction to the Theory of Neural Computation, </title> <booktitle> volume 1 of Lecture Notes: Santa Fe Institute Studies in the Sciences of Complexity. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: There are other alternatives that might be considered for the equations governing the relaxation of gating units. The most general case would allow both excitatory and inhibitory connections. If the arrangement of gating units were considered to be a Hopfield network, then various patterns could be learned and recalled <ref> [12] </ref>. 3.4.4 Interpretive Units When the iterative WTA has converged, interpretive units in this layer are computed from the converged values of the gating units: I l;p (x) = S 6 0 @ ^ G2M l;p (x) l;p C 3 5 : (3.17) A fixed bias w fl l;p , sometimes <p> This function is of the "sigmoid" form commonly used in the neural network literature to cause units to smoothly saturate <ref> [12] </ref>. <p> Should these kernels be constrained to particular forms, such as Gaussian shape? The lateral relaxation within assemblies of gating units might be made to learn the organization of inputs in the manner of Hopfield networks (see <ref> [12] </ref>), in which case position-independent constrains should also be added. It is also probably useful to divide features into shape and region features and hard-code the top-down bias weights to reflect this. When a global objective function is not immediately apparent, researchers often resort to using the Hebb rule.
Reference: [13] <author> G. E. Hinton, J. L. McClelland, and D. E. Rumelhart. </author> <title> Distributed representations. </title> <editor> In David E. Rumelhart, James L. McClelland, and the PDP Research Group, editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <volume> volume 1, chapter 3, </volume> <pages> pages 77-109. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: In the Tsotsos et al. formulation, all features compete with one another to find one winner, at odds with producing a cooperative encoding. Cooperative encodings are necessary for efficient and complete object representation <ref> [13] </ref>. It is worth explaining why simpler extensions of the selective tuning model are inap Chapter 3. The Selective Object Recognition Network 45 propriate.
Reference: [14] <author> Geoffrey E. Hinton. </author> <title> Shape representation in parallel systems. </title> <booktitle> In Proceedings of Seventh International Joint Conference on Artificial Intellignence, </booktitle> <pages> pages 1088-1096, </pages> <year> 1981. </year>
Reference-contexts: The relationship between the reference frame of the object and the retina of the observer must be represented within the observer. Segmentation and recognition of a particular object requires a consistent transformation to be applied to all features of the object <ref> [14] </ref>. A bottom-up approach to estimating the reference frame occurs in the hierarchical TRAFFIC scheme of Zemel et al. [40]. At each level in a bottom-up process, transformation parameters and object confidence are estimated together.
Reference: [15] <author> Geoffrey E. Hinton and Richard S. Zemel. Autoencoders, </author> <title> minimum description length and Helmholtz free energy. </title> <editor> In J. D. Cowan, G. Tesauro, and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6. </booktitle> <publisher> Morgan Kauffmann: </publisher> <address> San Mateo, CA, </address> <year> 1994. </year> <note> BIBLIOGRAPHY 106 </note>
Reference-contexts: In the minimum description length (MDL) framework, the objective is to minimize the total coding cost of communicating a set of data by transmitting both the model parameters and the data not explained by the model <ref> [15] </ref>. But it is not obvious how standard methods of learning should occur within the attentional network.
Reference: [16] <author> Eric R. Kandel. </author> <title> Processing of form and movement in the visual system. </title> <editor> In Eric R. Kandel and James H. Schwartz, editors, </editor> <booktitle> Principles of Neural Science, chapter 29, </booktitle> <pages> pages 366-383. </pages> <publisher> Elsevier Science Publishing Co., Inc., </publisher> <address> second edition, </address> <year> 1985. </year>
Reference-contexts: Cell recordings have shown the visual cortex to be made up of many retinotopic mappings corresponding to different visual features with different sizes of receptive fields. In V1, for example, a hypercolumn contains columns for various orientations, plus pegs (also called blobs) concerned only with colour, not orientation <ref> [16, 11] </ref>. The inferotemporal cortex (IT) of the macaque monkey is thought to play an important role in the "what" of object discrimination and recognition.
Reference: [17] <author> C. Koch and S. Ullman. </author> <title> Shifts in selective visual attention: Towards the underlying neural circuitry. </title> <journal> Human Neurobiology, </journal> <volume> 4 </volume> <pages> 219-227, </pages> <year> 1985. </year>
Reference-contexts: The thrust of this thesis is to improve understanding of human attention by analysis of a particular model of selective visual object recognition. It is often claimed that the brain contains position-independent object representations [32] facilitated by a mechanism of attention <ref> [17, 22, 28] </ref>, so this thesis attempts to provide a concrete implementation of the type of processing that could happen between the retina and the hypothesized position-independent representation by extending the work of Tsotsos et al.[37, 38]. <p> Koch and Ullman model the preattentive process as a parallel bank of filters that computes a set of simple, local features <ref> [17] </ref>. A saliency map is computed from a weighted sum of the simple feature maps. A hierarchical winner-take-all process finds the global maximum in the saliency map; features from the location of the maximum are inspected in a central representation where judgements are made about complex conjunctions of features. <p> The final measurement of the selected object is now done at the top of the pyramid. 3.5.7 Inhibition of Return No model of attention is complete without explaining why attention does not get stuck at the most salient image location. Koch and Ullman <ref> [17] </ref> inhibit return to the same location by including a fading process into the saliency units of their model. Regions under attention gradually become less interesting until they are no longer the winners.
Reference: [18] <author> Yann le Cun. </author> <title> Generalization and network design strategies. </title> <type> Technical Report CRG-TR-89-4, </type> <institution> University of Toronto, </institution> <year> 1989. </year>
Reference-contexts: But how can a network learn to attend to an object before it has learned it? Perhaps the network should be trained during the dynamical interactions that occur during movement of attention. To simplify learning and to improve generalization, it is often useful to enforce constraints between network parameters <ref> [18] </ref>. In particular, position invariance is already encouraged by single kernels which are applied uniformly to feature planes.
Reference: [19] <author> Jitendra Malik and Pietro Perona. </author> <title> Preattentive texture discrimination with early vision mechanisms. </title> <journal> Journal of the Optical Society of America, </journal> <volume> 7(5) </volume> <pages> 923-932, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Gabor difference of Gaussians (DOG) kernel of amplitude a and scale , centered at the origin is defined by K DOG (x; y; a; ) = (1:14) 2 aG (x; y; 0:71; 0:71) (0:71) 2 aG (x; y; 1:14; 1:14); (4.3) 2 Gaussian filters are very versatile: Malik and Perona <ref> [19] </ref> use filters of this type for measuring texture over various scales and orientations. Chapter 4. Design of Weights 53 which is the "Mexican hat" shape. An odd-symmetric difference of offset Gaussians (DOOG1) kernel is useful for step-edge detection. <p> Just as object segmentation can be done by refining selection to a particular chosen colour, it ought to be possible also to segment on the basis of region textures. In the preattentive texture segmentation model described by Malik and Perona <ref> [19] </ref>, a wide variety of low-level features are computed. Each of these feature maps are rectified and significantly blurred by a local maximum competition and passed through edge detectors. The largest of these edge responses represent the pop-out texture boundaries.
Reference: [20] <author> Stephane G. Mallat. </author> <title> A theory for multiresolution signal decomposition: The wavelet representation. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 11(7) </volume> <pages> 674-693, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: Chapter 4. Design of Weights 52 Since each level effectively subsamples from a convolution with the level below, the convolution kernel must not contain high frequency components or else aliasing will occur. Kernels might contain high-frequency components if information is not lost; e.g., wavelet pyramids under-sample while preserving information <ref> [20] </ref>. See [31] for a discussion of shiftable transforms.
Reference: [21] <author> Jeffrey Moran and Robert Desimone. </author> <title> Selective attention gates visual processing in the extrastriate cortex. </title> <journal> Science, </journal> <volume> 229 </volume> <pages> 782-784, </pages> <year> 1985. </year>
Reference-contexts: Moran and Desimone recorded single cells in area V4 of an awake, performing monkey <ref> [21] </ref>. They discovered an interesting set of attentional properties that have been demonstrated in variations since. The monkey performs a task involving two types of visual stimuli; an effective stimulus to which the probed neuron responds, and an ineffective stimulus to which the neuron does not respond.
Reference: [22] <author> M.C. Mozer. </author> <title> The Perception of Multiple Objects: A Connectionist Approach. </title> <publisher> MIT Press., </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: The thrust of this thesis is to improve understanding of human attention by analysis of a particular model of selective visual object recognition. It is often claimed that the brain contains position-independent object representations [32] facilitated by a mechanism of attention <ref> [17, 22, 28] </ref>, so this thesis attempts to provide a concrete implementation of the type of processing that could happen between the retina and the hypothesized position-independent representation by extending the work of Tsotsos et al.[37, 38]. <p> Only stimulus with the correct temporal tag would be used for object recognition [26]. Not all models require special routing or tagging to identify stimulus from one location. Mozer demonstrates a position-invariant network for word recognition <ref> [22] </ref> in which Chapter 2. Background 18 units at the top of a pyramid respond to particular letter clusters occurring anywhere in the image. A word is represented by a particular combination of simultaneously activated letter clusters.
Reference: [23] <author> Ken Nakayama and Julian S. Joseph. </author> <title> Attention, pattern recognition, and popout in visual search. </title> <editor> In R. Parasuraman, editor, </editor> <publisher> The Attentive Brain. MIT Press, </publisher> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: A review of early experiments can be found by Treisman [33], and recent perspectives are provided by Duncan [7] and Nakayama and Joseph <ref> [23] </ref>. Search seems easy when target items can be discriminated from non-targets by measuring a single dimension of colour contrast, curvature, orientation, direction of motion, or depth. Such targets can be found in roughly constant time, regardless of the number of distractors in the visual field. <p> An area of the brain exhibiting central processing properties is anterior inferotemporal cortex (IT) in which cells are maximally activated by specific complex shapes or specific conjunctions of texture or colour with shape [32]. Although the preattentive/attentive model is thought to be overly simplistic <ref> [23] </ref>, it is useful to retain the basic idea that low levels of vision are largely parallel processes whereas higher levels of vision are increasing selective for a particular stimulus, resulting in sequential inspection of visual objects. 2.1.3 Is Attention Object-Based? The two stage model is perhaps a useful way of <p> Recent experiments with neglect patients seem to indicate that attention is deployed to image regions that have already undergone figure-ground segmentation. For example, neglect patients can lack awareness of the left side of every object in a visual scene, suggesting that object segmentation precedes visual attention. Nakayama and Joseph <ref> [23] </ref> suggest that the term "preattentive" should be dropped from our vocabulary for describing attention because of its connotations to a model in which attention depends solely upon the strengths of precomputed features. They emphasize that attention is drawn to entire perceptual surfaces.
Reference: [24] <author> Ulric Neisser. </author> <title> Cognitive Psychology. </title> <address> Appleton-Century-Crofts, </address> <year> 1967. </year>
Reference-contexts: Neisser originally postulated "preattentive processes" to segregate all figural units of the scene in parallel, to be subsequently examined by attentive mechanisms <ref> [24] </ref>. In Gestalt manner, the whole object was thought to be perceived immediately, with the parts becoming available for inspection subsequent to attentive analysis. <p> two stage model is perhaps a useful way of thinking about how early parallel processes may direct attention, but if the spotlight is directed solely according to basic features, how do we account for the Gestalt claim of immediate perception of entire figures? Driver [6] continues to argue Neisser's claim <ref> [24] </ref> that preattentive processes produce segmented objects. Recent experiments with neglect patients seem to indicate that attention is deployed to image regions that have already undergone figure-ground segmentation.
Reference: [25] <author> E. Niebur and C. Koch. </author> <title> Computational architectures for attention. </title> <editor> In R. Parasura-man, editor, </editor> <publisher> The Attentive Brain. MIT Press, </publisher> <address> Cambridge, MA, </address> <publisher> in press. </publisher>
Reference-contexts: The idea is that regions of high salience are most likely to contain something interesting and worthy of attention; these regions of high salience are visited sequentially. In a survey paper, Niebur and Koch <ref> [25] </ref> discuss several models of attention in the context of experimental psychophysical data. Niebur and Koch emphasize that attention is often unavoidably drawn to interesting features.
Reference: [26] <author> Ernst Niebur and Christof Koch. </author> <title> A model for the neuronal implementation of selective visual attention based on temporal correlation among neurons. </title> <journal> Journal of Computational Neuroscience, </journal> <volume> 1 </volume> <pages> 141-158, </pages> <year> 1994. </year> <note> BIBLIOGRAPHY 107 </note>
Reference-contexts: The temporal tagging hypothesis is that inputs are always connected but a particular phase or firing pattern is introduced into units at the level of V1 that can be detected at higher levels of processing. Only stimulus with the correct temporal tag would be used for object recognition <ref> [26] </ref>. Not all models require special routing or tagging to identify stimulus from one location. Mozer demonstrates a position-invariant network for word recognition [22] in which Chapter 2. Background 18 units at the top of a pyramid respond to particular letter clusters occurring anywhere in the image.
Reference: [27] <author> B. A. Olshausen. </author> <title> Neural Routing Circuits for Forming Invariant Representations of Visual Objects. </title> <type> PhD thesis, </type> <institution> California Institute of Technology, Pasadena, California, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: Background 15 features are suppressed. However, since the selection happens bottom-up, level by level, all units choose reference frames independently. A global choice of reference frame was apparently not imposed in the TRAFFIC model. Other authors have deemed coordinate transformation important. The neural routing circuits of Olshausen <ref> [27] </ref> were designed to this end. <p> Olshausen models such a selection with routing circuits that, somewhat like a telephone switching system, connect a particular region of features to the central representation with topographic mapping preserved <ref> [27] </ref>. Input may come from different locations and different scales; it is scaled to fit the top-level map. Control units achieve scaling and translation transformations by manipulating bottom-up network weights.
Reference: [28] <author> Rajesh P. N. Rao and Dana H. Ballard. </author> <title> Localized receptive fields may mediate transformation-invariant recognition in the visual cortex. </title> <type> Technical Report 97.2, </type> <institution> National Resource Laboratory for the Study of Brain and Behavior, Department of Computer Science, University of Rochester, Rochester, </institution> <address> NY, </address> <month> May </month> <year> 1997. </year>
Reference-contexts: The thrust of this thesis is to improve understanding of human attention by analysis of a particular model of selective visual object recognition. It is often claimed that the brain contains position-independent object representations [32] facilitated by a mechanism of attention <ref> [17, 22, 28] </ref>, so this thesis attempts to provide a concrete implementation of the type of processing that could happen between the retina and the hypothesized position-independent representation by extending the work of Tsotsos et al.[37, 38]. <p> To accommodate distortion due to transformation invariance, a first-order Taylor series model can be implemented <ref> [28] </ref>. A second hierarchy of predictors is introduced that represents object position. The image becomes factored into a representation of the learned image plus a shift.
Reference: [29] <author> Rajesh P. N. Rao and Dana h. Ballard. </author> <title> The visual cortex as a hierarchical predictor. </title> <type> Technical Report 96.4, </type> <institution> National Resource Laboratory for the Study of Brain and Behavior, Department of Computer Science, University of Rochester, </institution> <month> September </month> <year> 1996. </year>
Reference-contexts: Unsupervised Hebbian learning occurs by presenting a single object to the network and performing only feed-forward computations. All feedback and lateral connections in the network are fixed, not learned. Chapter 2. Background 20 2.4.4 Hierarchical Predictor Rao and Ballard <ref> [29] </ref> model the bottom-up and top-down interactions of the visual cortex as a hierarchical predictive network. High level processes attempt to predict that which is being registered by lower level processes. The predictions are carried along the feedback connections.
Reference: [30] <author> Paul-Antoine Salin and Jean Bullier. </author> <title> Corticocortical connections in the visual system: Structure and function. </title> <journal> Physiological Reviews, </journal> <volume> 75(1), </volume> <month> January </month> <year> 1995. </year>
Reference-contexts: But it does seem to be the case that extent of the feedback signals reciprocates the extent of feed-forward signals in the visual cortex <ref> [30] </ref>. That is to say, although individual feedback connections may not reciprocate feed-forward connections, the spatial extent of feedback connections from a neuron is that of neurons that feed into it. 2.2.3 Task-Modulated Cell Recordings Various neural correlates of selective attention have been found.
Reference: [31] <author> Eero P. Simoncelli, William T. Freeman, Edward H. Adelson, and David J. Heeger. </author> <title> Shiftable multi-scale transforms. </title> <journal> IEEE Transactions on Information Theory, Special Issue on Wavelets, </journal> <year> 1992. </year>
Reference-contexts: For example, Gaussian and Laplacian pyramids allow low-pass and band-pass analysis, respectively, of images at various scales [3]. Image orientation may also be analyzed at various scales via steerable pyramids <ref> [31] </ref>. A large amount of data can be analyzed at various scales for efficient analysis of texture segmentation, motion, or stereo correspondence over the entire image [1]. at the top of this pyramid. Figure 2.2 shows a processing pyramid with several top-level features. <p> Design of Weights 52 Since each level effectively subsamples from a convolution with the level below, the convolution kernel must not contain high frequency components or else aliasing will occur. Kernels might contain high-frequency components if information is not lost; e.g., wavelet pyramids under-sample while preserving information [20]. See <ref> [31] </ref> for a discussion of shiftable transforms.
Reference: [32] <author> Keiji Tanaka, Hide-Aki Saito, Yoshiro Fukada, and Madoka Moriya. </author> <title> Coding visual images of objects in the inferotemporal cortex of the macaque monkey. </title> <journal> Journal of Neurophysiology, </journal> <volume> 66(1) </volume> <pages> 170-189, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: The thrust of this thesis is to improve understanding of human attention by analysis of a particular model of selective visual object recognition. It is often claimed that the brain contains position-independent object representations <ref> [32] </ref> facilitated by a mechanism of attention [17, 22, 28], so this thesis attempts to provide a concrete implementation of the type of processing that could happen between the retina and the hypothesized position-independent representation by extending the work of Tsotsos et al.[37, 38]. <p> An area of the brain exhibiting central processing properties is anterior inferotemporal cortex (IT) in which cells are maximally activated by specific complex shapes or specific conjunctions of texture or colour with shape <ref> [32] </ref>. <p> The inferotemporal cortex (IT) of the macaque monkey is thought to play an important role in the "what" of object discrimination and recognition. Tanaka et al. <ref> [32] </ref> have investigated the optimal stimulus for individual cells in IT by the use of single-cell recordings of brain cells of anesthetized monkeys. <p> However, these face cells were not finely tuned, responding well to many different faces. The authors propose that each of the cells were responding to a partial feature of the face; the collection of responses is required for a complete representation <ref> [32] </ref>.
Reference: [33] <author> Anne Treisman. </author> <title> Features and objects: The fourteenth Bartlett memorial lecture. </title> <journal> The Quarterly Journal of Experimental Psychology, </journal> <volume> 40A(2):201-237, </volume> <year> 1988. </year>
Reference-contexts: A review of early experiments can be found by Treisman <ref> [33] </ref>, and recent perspectives are provided by Duncan [7] and Nakayama and Joseph [23]. Search seems easy when target items can be discriminated from non-targets by measuring a single dimension of colour contrast, curvature, orientation, direction of motion, or depth. <p> More recently, however, on the basis of experiments showing easy search for primitive features, parallel processes prior to attention have been theorized to compute only basic features that are then assembled into figures by the attentive mechanisms. In Treisman's feature integration theory <ref> [35, 33] </ref>, attention can be directed by inhibition of irrelevant feature maps in preattentive feature modules or by selection of a region of space. Modules are hypothesized to be responsible for measurements of different dimensions of sensory features, such as colour, orientation, direction of motion, etc. <p> Treisman points out that although cells in V1 are tuned to conjunctions of low-level features (i.e., particular orientation, colour, and spatial frequency), such conjunctions cannot be used as the basis of conjunctive search, presumably because the conjunctions are discarded at higher levels of processing <ref> [33] </ref>. The spotlight metaphor is commonly invoked to explain how information may be routed to a central location. Suppose that cells in the central location are indirectly connected to the entire visual field. Attention is explained to be akin to a spotlight that Chapter 2.
Reference: [34] <author> Anne Treisman and Hilary Schmidt. </author> <title> Illusory conjunctions in the perception of objects. </title> <journal> Cognitive Psychology, </journal> <volume> 14 </volume> <pages> 107-141, </pages> <year> 1982. </year>
Reference-contexts: A loose binding of features can result from coarsely tuned attention, resulting in illusory conjunctions, a real phenomenon in which subjects incorrectly perceive the attributes of one object on another <ref> [34] </ref>. Koch and Ullman model the preattentive process as a parallel bank of filters that computes a set of simple, local features [17]. A saliency map is computed from a weighted sum of the simple feature maps. <p> This is considered acceptable in the bottom-up pass because of similarity to certain psychophysical phenomena; human observers can be made to perceive illusory conjunctions in which attributes of one object are transplanted onto another <ref> [34] </ref>. Although permissible in bottom-up processing, a mechanism of attention must attempt to correct such mistakes. of red feature and X feature. Gating units are connected to either red or X feature maps. <p> And certainly there is an independence of representation that could, if time is not allocated for convergence, allow simultaneous activation of properties from different objects, resulting in an illusory conjunction <ref> [34] </ref> of features. This example also suggests that neurons for different features, such as colour and motion, could be represented in different locations of the brain, but the various maps could equally well be interleaved. Chapter 5. Demonstrations 64 recognize different sizes of hollow squares.
Reference: [35] <author> Anne M. Treisman and Garry Gelade. </author> <title> A feature-integration theory of attention. </title> <journal> Cognitive Psychology, </journal> <volume> 12 </volume> <pages> 97-136, </pages> <year> 1980. </year> <note> BIBLIOGRAPHY 108 </note>
Reference-contexts: More recently, however, on the basis of experiments showing easy search for primitive features, parallel processes prior to attention have been theorized to compute only basic features that are then assembled into figures by the attentive mechanisms. In Treisman's feature integration theory <ref> [35, 33] </ref>, attention can be directed by inhibition of irrelevant feature maps in preattentive feature modules or by selection of a region of space. Modules are hypothesized to be responsible for measurements of different dimensions of sensory features, such as colour, orientation, direction of motion, etc.
Reference: [36] <author> John K. Tsotsos. </author> <title> Analyzing vision at the complexity level. </title> <journal> Behavioral and Brain Sciences, </journal> <volume> 13 </volume> <pages> 423-469, </pages> <year> 1990. </year> <title> Includes peer commentaries and a response from the author. </title>
Reference-contexts: Computationally, the general vision problem is intractable, but the use of attention allows tractable vision tasks to be performed <ref> [36] </ref>. So whether the limitation is communication channel capacity or algorithm complexity, selective filtering of stimulus is required. <p> Hence, the mechanism of selective attention must allow units to be put to work on different tasks. Tsotsos has used computational complexity arguments to justify the processing pyramid with inhibitory attentional beam as a model of the visual cortex <ref> [36] </ref>. The inhibitory beam model allows processing units to be put to work on different (or restricted) regions of the image, thereby resulting in an attentional allocation of resources to a location. <p> Perhaps a better explanation is that high residuals, being less well explained, are more likely to attract attention. 2.4.5 Selective Tuning Model To describe the selective tuning model, it is useful to begin with the description of an inhibitory attentional beam <ref> [36] </ref>. In an input abstraction hierarchy, a top-level unit has a large receptive field. The attentional beam allows processing to be directed to a portion of that receptive field (the pass zone) by inhibiting the remainder of the receptive field (the inhibit zone). <p> Tsotsos says that multiple attentional beams may exist in different feature types, e.g., shape and colour, provided they converge on the same location of the image <ref> [36] </ref>. But it is not explained how this constraint is to be implemented, or how the beam may operate for complex features that combine information from various feature types.
Reference: [37] <author> John K. Tsotsos. </author> <title> An inhibitory beam for attentional selection. </title> <editor> In Harris and Jenkin, editors, </editor> <booktitle> Spatial Vision in Humns and Robots, </booktitle> <pages> pages 313-331. </pages> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1993. </year>
Reference-contexts: The selection is applied recursively over all of the levels, analogous to a beam of light shining through several translucent planes. An inhibitory beam may be implemented by a hierarchy of assemblies of processing units, each assembly implementing a winner-take-all (WTA) process <ref> [37] </ref>. Within each assembly are an interpretive unit, a set of gating units, and a gating control unit. Each gating unit takes its initial value from the interpretive output of an assembly in a lower level of the pyramid.
Reference: [38] <author> John K. Tsotsos, Sean M. Culhane, Winky Yan Kei Wai, Yuzhong Lai, Neal Davis, and Fernando Nuflo. </author> <title> Modeling visual attention via selective tuning. </title> <journal> Artificial Intelligence, </journal> <volume> 78 </volume> <pages> 507-545, </pages> <year> 1995. </year>
Reference-contexts: How do we decide what to look at? How do we have confidence in our interpretation of what we are looking at? How do we search for objects of interest while filtering out irrelevant information? The Selective Tuning Model <ref> [38] </ref> has properties that promise to help us better understand attention. Within a hierarchy of computational units, stimulus is attended by top-down spatial selection of prominent features at each level, forming an attentional beam. Additionally, top-down bias units permit task-directed inhibition of entire classes of stimuli. <p> The set of units providing feedforward connections to a particular unit is its support set. <ref> [38] </ref> When units in an input abstraction hierarchy have overlapping receptive fields, they are prone to context effects, blurring, cross-talk between objects, and boundary effects. The inhibitory beam reduces these effects by effectively narrowing a cell's receptive field and inhibiting the surrounding stimulus that would cause confusion. <p> The inhibitory beam reduces these effects by effectively narrowing a cell's receptive field and inhibiting the surrounding stimulus that would cause confusion. The selective tuning model <ref> [38] </ref> extends the implementation of the inhibitory beam by including top-down bias for weighting input stimulus according to the task. For each interpretive unit there is a bias unit which obtains its value from bias units in the level above and provides multiplicative inhibition to the interpretive unit. <p> It should be able to recognize an object in a variety of poses anywhere within a reasonably large region of the image, and it should be insensitive to noise and distractors in other regions of the image. The winner-take-all formulation of Tsotsos et al. <ref> [38] </ref> provides a top-down method of defining a selective pass beam to include strong input stimuli. At each level the beam recursively selects the strongest children of units in the pass beam. <p> The post-WTA bias for position (section 3.3.2) allows choices made by shape features to have an influence. 3.4 Details of the Model This section gives precise definition to the selective object recognition network. The following is based on the model of Tsotsos et al. <ref> [38] </ref>, but incorporates changes to implement the improvements discussed in section 3.3. 3.4.1 Pyramid Nomenclature The attention network is organized as an image pyramid, with each level in the pyramid containing a number of feature planes of the same size. <p> Chapter 3. The Selective Object Recognition Network 35 "left-end-stop" kernel applied to the "horizontal line" feature plus a "top-end-stop" kernel applied to the "vertical line" feature at the same location. Specifying these kernels defines the connectivity of all units in the network. 3.4.2 Definitions Following Tsotsos et al. <ref> [38] </ref>, the pyramid is composed of the following processing units.
Reference: [39] <author> Jeremy M. Wolfe. </author> <title> "Effortless" texture segmentation and "parallel" visual search are not the same thing. </title> <journal> Vision Research, </journal> <volume> 32(4) </volume> <pages> 757-763, </pages> <year> 1992. </year>
Reference-contexts: It is possible that there are two distinct mechanisms at work. The idea of two mechanisms is supported by Wolfe's note that there are easy-search stimuli that do not support texture segmentation and there are easy-segmentation features that are difficult to find in search <ref> [39] </ref>. Without a doubt, more precise judgements may be made about an object which is under attention. Attention is required when measuring not only conjunctions of features, but also arrangement of features.
Reference: [40] <author> Richard S. Zemel, Michael C. Mozer, and Geoffrey E. Hinton. </author> <title> Traffic: Recognizing objects using hierarchical reference frame transformations. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Segmentation and recognition of a particular object requires a consistent transformation to be applied to all features of the object [14]. A bottom-up approach to estimating the reference frame occurs in the hierarchical TRAFFIC scheme of Zemel et al. <ref> [40] </ref>. At each level in a bottom-up process, transformation parameters and object confidence are estimated together. Features are chosen from the level below that consistently predict the same transformation and the inconsistent Chapter 2. Background 15 features are suppressed. <p> Scale can also be estimated from the data. As in the TRAFFIC model <ref> [40] </ref>, each Chapter 5. Demonstrations 78 assembly at each level can estimate the scale of the object and vote on what the scale should be.
Reference: [41] <author> Steven W. Zucker. </author> <title> Vertical and horizontal processes in low level vision. </title> <editor> In Allen R. Hanson and Edward M. Riseman, editors, </editor> <booktitle> Computer Vision Systems, </booktitle> <pages> pages 187-195. </pages> <publisher> Academic Press Inc., </publisher> <address> New York, </address> <year> 1978. </year> <note> BIBLIOGRAPHY 109 </note>
Reference-contexts: If attention were to depend only on the region-filling process, however, objects like the Kanisza square would present particular difficulty. Hence, some cooperation between vertical shape and horizontal region-filling is necessary (see <ref> [41] </ref>). 7.2.3 Multi-Level Constraints on Features: Springs As discussed in section 5.3.4, it would be helpful to allow selection at a high level in the pyramid to govern selection in lower levels.
References-found: 41

