URL: ftp://ftp.isi.edu/isi-pubs/rs-96-447.ps.Z
Refering-URL: http://www.isi.edu/isi-technical-reports.html
Root-URL: http://www.isi.edu
Email: email: gil@isi.edu  
Title: Planning Experiments: Resolving Interactions between Two Planning Spaces  
Author: Yolanda Gil 
Date: May 29-31, 1996,  
Note: In Proceedings of the Third International Conference on Artificial Intelligence Planning Systems (AIPS-96),  
Address: 4676 Admiralty Way Marina del Rey, CA 90292  Scotland.  
Affiliation: USC/Information Sciences Institute  Edinburgh,  
Abstract: Learning from experimentation allows a system to acquire planning domain knowledge by correcting its knowledge when an action execution fails. Experiments are designed and planned to bring the world to a state where a hypothesis (e.g., that an operator is missing a precondition) can be tested. When planning an experiment, the planner must take into account the interactions between the execution of the main plan and the execution of the experiment plans, since after the experiment it must continue to carry on its main task. In order for planners to work in such environments where they can be given several tasks, they must take into account the interactions between them. A usual assumption in current planning systems is that they are given a single task (or set of goals to achieve). However, a plan that may seem adequate for a task in isolation may make other tasks harder (or even impossible) to achieve. Different tasks may compete for resources, execute irreversible actions that make other tasks unachievable, or set the world in undesirable states. This paper discusses what these interactions are and presents how the problem was adressed in EXPO, an implemented system that acquires domain knowledge for planning through experimentation. 
Abstract-found: 1
Intro-found: 1
Reference: [ Carbonell et al., 1991 ] <author> Carbonell, Jaime G., Craig A. Knoblock, and Steven Minton. </author> <year> 1991. </year> <title> prodigy: An integrated architecture for planning and learning. In Architectures for Intelligence, </title> <editor> ed. Kurt VanLehn. </editor> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: Our approach to learning by experimentation has been implemented in a system called EXPO. EXPO's underlying planning architecture is the prodigy system <ref> [ Carbonell et al., 1991 ] </ref> which provides a robust, expressive, and efficient planner. <p> An example of a policy is to avoid using irreversible operators, since they can bring the world to a state where the main task cannot be achieved. Each policy in EXPO is implemented as a control rule in PRODIGY's language <ref> [ Carbonell et al., 1991, Gil, 1992 ] </ref> . Control rules are used during the search at each decision point: to choose a node to expand, to choose a goal to achieve, to choose an operator to achieve a goal, and to choose bindings for the operator's parameters.
Reference: [ Cheng, 1990 ] <author> Cheng, Peter C-H. </author> <year> 1990. </year> <title> Modelling Scientific Discovery. </title> <type> PhD thesis, </type> <institution> The Open University, Milton Keynes, </institution> <address> England. </address>
Reference-contexts: An important component of our approach is the ability to design experiments to gather additional information that is not available to the learner and yet is needed to acquire the missing knowledge. Experimentation is vital for effective learning and is a very powerful tool to refine scientific theories <ref> [ Cheng, 1990, Rajamoney, 1993 ] </ref> , but other research on learning planning knowledge from the environment does not address the issue of experiment formulation and design [ Shen, 1993, Kedar et al., 1991 ] .
Reference: [ Gil, 1992 ] <author> Gil, Y. </author> <title> Acquiring Domain Knowledge for Planning by Experimentation. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <year> 1992. </year>
Reference-contexts: Our approach combines selective and continuous monitoring of the environment to detect knowledge faults with directed manipulation through experiments that lead to the missing knowledge. The first part of this paper summarizes our work on autonomous refinement of incomplete planning domains through experimentation <ref> [ Gil, 1994, Gil, 1993, Gil, 1992 ] </ref> and presents empirical results of its effectiveness and efficiency in improving the planner's domain knowledge when initial domain knowledge is up to 50% incomplete. <p> EXPO was tested with a complex process planning domain of dozens of operators, and a STRIPS-like robot planning domain that is widely used by planning researchers (both domains are described in <ref> [ Gil, 1992 ] </ref> ). 2 Learning from the Environment by Experimenta tion This section describes how EXPO detects that it is missing domain knowledge, then constructs a set of hypotheses of possible fixes, determines the more promising ones, and finally designs and executes experiments to determine the fix needed by <p> More details can be found in <ref> [ Gil, 1994, Gil, 1993, Gil, 1992 ] </ref> . 2.1 Detecting the Need to Learn EXPO is given a suite of representative planning problems in the domain. <p> Table 1 summarizes hypothesis generation for these two cases. Other possible fixes to the domain knowledge include acquiring data about the state of the external world and acquiring new operators <ref> [ Gil, 1992, Gil, 1994 ] </ref> . 3 heuristic description locality of actions objects affected by the action are likely to be already present in the operator's parameters structural similarity similar operators are likely to have similar preconditions generalization of experience necessary conditions have been present in all past successful executions <p> Such generalization of the planner's past experience is useful to guide our search for the missing condition, because it contains the conditions that were common to all the states when the action 4 was successfully executed before. The heuristics are described in more detail in <ref> [ Gil, 1992, Gil, 1993 ] </ref> . 2.4 Designing and Executing Experiments Each hypothesis is tested with an experiment. <p> be continued, and EXPO continues to watch for learning opportunities to correct its domain knowledge. 3 Empirical Results EXPO was tested in two different domains: a robot planning domain frequently used in the planning literature, and a complex process planning domain with dozens of operators and states of large size. <ref> [ Gil, 1992 ] </ref> describes these domains in detail as well as other empirical results not shown here. To control the amount of missing knowledge that EXPO was given in the tests, we first wrote a complete domain D with all the operators with all their corresponding conditions and effects. <p> The number of experiments needed with this strategy is comparable to (but still larger than) the number needed for our combined heuristics. However, for each experiment the planner must achieve many more additional goals because more conditions are tested in each experiment setup <ref> [ Gil, 1992, Gil, 1993 ] </ref> . 6 failures none g l s gls 5 215 168 50 94 10 With 50 percent of the preconditions missing, the cumulative number of experiments needed was: failures none g l s gls 5 205 172 27 118 40 17 728 370 201 325 <p> An example of a policy is to avoid using irreversible operators, since they can bring the world to a state where the main task cannot be achieved. Each policy in EXPO is implemented as a control rule in PRODIGY's language <ref> [ Carbonell et al., 1991, Gil, 1992 ] </ref> . Control rules are used during the search at each decision point: to choose a node to expand, to choose a goal to achieve, to choose an operator to achieve a goal, and to choose bindings for the operator's parameters.
Reference: [ Gil, 1993 ] <author> Gil, Y. </author> <title> Efficient domain-independent experimentation. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Leaning, </booktitle> <address> Amherst, MA. </address> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Our approach combines selective and continuous monitoring of the environment to detect knowledge faults with directed manipulation through experiments that lead to the missing knowledge. The first part of this paper summarizes our work on autonomous refinement of incomplete planning domains through experimentation <ref> [ Gil, 1994, Gil, 1993, Gil, 1992 ] </ref> and presents empirical results of its effectiveness and efficiency in improving the planner's domain knowledge when initial domain knowledge is up to 50% incomplete. <p> More details can be found in <ref> [ Gil, 1994, Gil, 1993, Gil, 1992 ] </ref> . 2.1 Detecting the Need to Learn EXPO is given a suite of representative planning problems in the domain. <p> Such generalization of the planner's past experience is useful to guide our search for the missing condition, because it contains the conditions that were common to all the states when the action 4 was successfully executed before. The heuristics are described in more detail in <ref> [ Gil, 1992, Gil, 1993 ] </ref> . 2.4 Designing and Executing Experiments Each hypothesis is tested with an experiment. <p> The number of experiments needed with this strategy is comparable to (but still larger than) the number needed for our combined heuristics. However, for each experiment the planner must achieve many more additional goals because more conditions are tested in each experiment setup <ref> [ Gil, 1992, Gil, 1993 ] </ref> . 6 failures none g l s gls 5 215 168 50 94 10 With 50 percent of the preconditions missing, the cumulative number of experiments needed was: failures none g l s gls 5 205 172 27 118 40 17 728 370 201 325
Reference: [ Gil, 1994 ] <author> Gil, Y. </author> <title> Learning by Experimentation: Incremental Refinement of Incomplete Planning Domains. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Leaning, </booktitle> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann, </publisher> <year> 1994 </year>
Reference-contexts: The imperfections of the domain knowledge have been closely related to planning and/or execution failures [ Hammond, 1986, Huffman et al., 1992 ] , but they can also cause unexpected successful executions <ref> [ Gil, 1994 ] </ref> . Planning systems that model a physical system and are given the ability to interact with it can directly examine the actual behavior of the physical system that the domain is supposed to model. <p> Our approach combines selective and continuous monitoring of the environment to detect knowledge faults with directed manipulation through experiments that lead to the missing knowledge. The first part of this paper summarizes our work on autonomous refinement of incomplete planning domains through experimentation <ref> [ Gil, 1994, Gil, 1993, Gil, 1992 ] </ref> and presents empirical results of its effectiveness and efficiency in improving the planner's domain knowledge when initial domain knowledge is up to 50% incomplete. <p> More details can be found in <ref> [ Gil, 1994, Gil, 1993, Gil, 1992 ] </ref> . 2.1 Detecting the Need to Learn EXPO is given a suite of representative planning problems in the domain. <p> Table 1 summarizes hypothesis generation for these two cases. Other possible fixes to the domain knowledge include acquiring data about the state of the external world and acquiring new operators <ref> [ Gil, 1992, Gil, 1994 ] </ref> . 3 heuristic description locality of actions objects affected by the action are likely to be already present in the operator's parameters structural similarity similar operators are likely to have similar preconditions generalization of experience necessary conditions have been present in all past successful executions
Reference: [ Hammond, 1986 ] <author> Hammond, Chris J. </author> <year> 1986. </year> <title> Case-based Planning: An Integrated Theory of Planning, Learning, and Memory. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <address> New Haven, CN. </address>
Reference-contexts: In a planning system, the inaccuracies of the knowledge base may render problems unsolvable or produce plans that yield unsuccessful executions. The imperfections of the domain knowledge have been closely related to planning and/or execution failures <ref> [ Hammond, 1986, Huffman et al., 1992 ] </ref> , but they can also cause unexpected successful executions [ Gil, 1994 ] .
Reference: [ Huffman et al., 1992 ] <author> Huffman, Scott B., Douglas J. Pearson, and John E. Laird. </author> <year> 1992. </year> <title> Correcting imperfect domain theories: A knowledge-level analysis. In Machine Learning: Induction, Analogy and Discovery. </title> <address> Boston, MA: </address> <publisher> Kluman Academic Press. </publisher> <pages> 13 </pages>
Reference-contexts: In a planning system, the inaccuracies of the knowledge base may render problems unsolvable or produce plans that yield unsuccessful executions. The imperfections of the domain knowledge have been closely related to planning and/or execution failures <ref> [ Hammond, 1986, Huffman et al., 1992 ] </ref> , but they can also cause unexpected successful executions [ Gil, 1994 ] .
Reference: [ Kedar et al., 1991 ] <author> Kedar, Smadar T., John L. Bresina, and C. Lisa Dent. </author> <year> 1991. </year> <title> The blind leading the blind: Mutual refinement of approximate theories. </title> <booktitle> In Proceedings of the Eight Machine Learning Workshop. </booktitle> <address> Evanston, IL. </address>
Reference-contexts: Experimentation is vital for effective learning and is a very powerful tool to refine scientific theories [ Cheng, 1990, Rajamoney, 1993 ] , but other research on learning planning knowledge from the environment does not address the issue of experiment formulation and design <ref> [ Shen, 1993, Kedar et al., 1991 ] </ref> . Previous work on learning by experimentation has not addressed the issue of how to choose good experiments, and much research on learning from failure has relied on background knowledge to build explanations that pinpoint directly the causes of failures.
Reference: [ Kulkarni, 1988 ] <author> Kulkarni, Deepak S. </author> <year> 1988. </year> <title> The Process of Scientific Research: The Strategy of Experimentation. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA. </address>
Reference: [ Maes and Brooks, 1990 ] <author> Maes, Pattie and Rodney A. Brooks. </author> <year> 1990. </year> <title> Learning to coordinate behaviors. </title> <booktitle> In Proceedings of the Eight National Conference on Artificial Intelligence. </booktitle> <address> Boston, MA. </address>
Reference-contexts: The planner in turn provides a performance element to measure any improvements in the knowledge base. This is a closed-loop integration of planning and learning by experimentation. Research in the area of acquiring action models is mostly subsymbolic <ref> [ Mahadevan and Connell, 1992, Maes and Brooks, 1990 ] </ref> . An important component of our approach is the ability to design experiments to gather additional information that is not available to the learner and yet is needed to acquire the missing knowledge.
Reference: [ Mahadevan and Connell, 1992 ] <author> Mahadevan, S. and Connell, J. </author> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <journal> Artificial Intelligence 55(2-3):311-365, </journal> <year> 1992. </year>
Reference-contexts: The planner in turn provides a performance element to measure any improvements in the knowledge base. This is a closed-loop integration of planning and learning by experimentation. Research in the area of acquiring action models is mostly subsymbolic <ref> [ Mahadevan and Connell, 1992, Maes and Brooks, 1990 ] </ref> . An important component of our approach is the ability to design experiments to gather additional information that is not available to the learner and yet is needed to acquire the missing knowledge.
Reference: [ Minton et al., 1989 ] <author> Minton, Steve, Craig A. Knoblock, Dan R. Kuokka, Yolanda Gil, Robert L. Joseph, and Jaime G. Carbonell. </author> <year> 1989. </year> <title> Prodigy 2.0: The Manual and Tutorial. </title> <type> Technical Report CMU-CS-89-146, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA. </address>
Reference: [ Rajamoney, 1993 ] <author> Rajamoney, Shankar A. </author> <year> 1993. </year> <title> The design of discrimination experiments. </title> <journal> Machine Learning, </journal> <volume> 12(1/2/3), </volume> <year> 1993. </year>
Reference-contexts: An important component of our approach is the ability to design experiments to gather additional information that is not available to the learner and yet is needed to acquire the missing knowledge. Experimentation is vital for effective learning and is a very powerful tool to refine scientific theories <ref> [ Cheng, 1990, Rajamoney, 1993 ] </ref> , but other research on learning planning knowledge from the environment does not address the issue of experiment formulation and design [ Shen, 1993, Kedar et al., 1991 ] .
Reference: [ Shen, 1993 ] <author> Shen, W. M. </author> <title> Discovery as autonomous learning from the environment. </title> <journal> Machine Learning, </journal> <volume> 12(1/2/3), </volume> <year> 1993. </year> <month> 14 </month>
Reference-contexts: Experimentation is vital for effective learning and is a very powerful tool to refine scientific theories [ Cheng, 1990, Rajamoney, 1993 ] , but other research on learning planning knowledge from the environment does not address the issue of experiment formulation and design <ref> [ Shen, 1993, Kedar et al., 1991 ] </ref> . Previous work on learning by experimentation has not addressed the issue of how to choose good experiments, and much research on learning from failure has relied on background knowledge to build explanations that pinpoint directly the causes of failures.
References-found: 14

