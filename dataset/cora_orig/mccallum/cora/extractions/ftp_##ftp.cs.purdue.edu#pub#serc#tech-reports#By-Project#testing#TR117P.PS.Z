URL: ftp://ftp.cs.purdue.edu/pub/serc/tech-reports/By-Project/testing/TR117P.PS.Z
Refering-URL: http://www.cs.gatech.edu/people/home/jmankoff/dataflow.html
Root-URL: 
Title: A Time/Structure Based Model for Estimating Software Reliability  
Author: Mei-Hwa Chen Joseph R. Horgan Aditya P. Mathur Vernon J. Rego 
Date: December 4, 1992  
Abstract: Estimation of the probability of software failure during a specified exposure period, also known as reliability, has long been an important subject of research. Several models for estimating software reliability exist. The Goel-Okumoto model and the Musa execution time model are two well known time-based models. Common characteristics of time-based models include neglect of program structure and use of time dependent data in the estimation of software reliability. In this paper we point out some fundamental problems with such an approach. We proposed two distinct approaches to reliability modeling. The approach described here leads to a class of models known as time/structure based models which extend the existing time-based models by using the notion of useless testing effort. The other approach, described elsewhere, is purely structure based and makes use of the notion of code coverage and other fault related parameters. A common characteristic of both the approaches is that they incorporate features of testing e.g. data flow or mutation testing, during the integration test phase of the software development cycle. While the pure structure based approach requires the use of certain testing methods, the time/structure based approach does not. We expect these approaches to lead to new and novel ways of estimating software reliability. Keywords: Data flow testing, structural coverage, mutation testing, software reliability, time based models.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. H. Chen, J. R. Horgan, A. P. Mathur, and V. J. Rego, </author> <title> "A Structure Based Model for Software Reliability Estimation," </title> <note> In preparation. </note>
Reference-contexts: This model is simple to use and retains the basic structure of existing models. A pure structure based model which is expected to produce more accurate reliability estimates but is more difficult to use than the time/structure based model is described in <ref> [1] </ref>. We begin by defining the notion of useful testing effort. A testing effort E k is useful if and only if it increases some type of coverage. Note that the definition of usefulness does not specify which coverage should be increased for a test effort to be useful. <p> It is important to note that the use of time-structure based models does not require a change in the testing method used. They do require the measurement of coverage such as block, decision, or data flow coverage. The pure structure based models <ref> [1] </ref> remove the notion of time in estimating the reliability. They, however, do require the use of specific testing methods such as data flow and mutation testing.
Reference: [2] <author> R. C. Cheung, </author> <title> "A User Oriented Software Reliability Model," </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> Vol. SE-6, No. 2, </volume> <month> March </month> <year> 1980, </year> <pages> pp 118-125. </pages>
Reference-contexts: Definitions of all the well know data flow criteria are provided in [5]. 6 7 uses the total CPU-time spent executing the program under test. Some researchers have used the number of test cases <ref> [2] </ref> as the independent variable. In our discussion below, we consider these quantities as indicators of testing effort. Thus, as testing effort increases, faults are discovered and removed. This results in an increase in program reliability.
Reference: [3] <author> T. A. Budd, </author> <title> "Mutation Analysis of Program Test Data," </title> <type> Dissertation, </type> <institution> Yale University, </institution> <month> May, </month> <year> 1980. </year>
Reference-contexts: This saturation effect has been illustrated for several structure based methods by a few empirical studies in the past <ref> [3, 9, 22] </ref>. Due to its imprecise nature, a proof that saturation effect holds for functional testing is not possible, although Howden's work [14] does provide some empirical justification. Below we present empirical justification based on another study with two relatively larger programs than the ones considered by Howden.
Reference: [4] <author> B. J. Choi, R. A. DeMillo, E. W. Krauser, A. P. Mathur, R. J. Martin, A. J. Offutt, H. Pan, and E. H. Spafford, </author> <title> "The Mothra Toolset," </title> <booktitle> Proceedings of Hawaii International Conference on System Sciences, </booktitle> <address> HI, </address> <month> January 3-6, </month> <year> 1989. </year>
Reference-contexts: It requires that one or more methods of obtaining some form of code coverage be used during testing. We assume that a program is under integration test [8]. We consider four different testing methods, namely functional or black-box [14], decision coverage, data flow [5], and mutation <ref> [4] </ref> testing. Below, we often refer to these four methods as B, D, F, and M, respectively. Of these four, functional testing is the most ill-defined, though most often used, method of software testing. <p> M is considered equivalent to P if 8d 2 D; P (d) = M (d). Mutation testing requires a tester to generate test data that distinguishes all non-equivalent mutants of P . Further details of mutation testing may be found in <ref> [4] </ref>. Note that each of the four testing methods provides an adequacy criterion against which a test set can be evaluated.
Reference: [5] <author> L. A. Clarke, A. Podgruski, D. J. Richardson, and S. Zeil, </author> <title> "A Formal Evaluation of Data Flow Path Selection Criteria," </title> <journal> IEEE Trans. on Software Engineering, November 1989, </journal> <volume> Vol. 15, No 11, </volume> <pages> pp 1318-1332. </pages>
Reference-contexts: It requires that one or more methods of obtaining some form of code coverage be used during testing. We assume that a program is under integration test [8]. We consider four different testing methods, namely functional or black-box [14], decision coverage, data flow <ref> [5] </ref>, and mutation [4] testing. Below, we often refer to these four methods as B, D, F, and M, respectively. Of these four, functional testing is the most ill-defined, though most often used, method of software testing. <p> Each of the above adequacy criteria is precise and measurable. Note that functional testing does not provide any such precise and measurable criteria. It can be shown formally that if a test set is p-use or c-use adequate, then it is also decision adequate <ref> [5] </ref>. We therefore say that data flow coverage subsumes decision coverage. No such relationship can be proven amongst functional, data flow, and mutation testing. <p> Musa's execution time model [20] 3 w.r.t: with respect to. 4 There are several other data flow criteria that we have not mentioned in this paper. Definitions of all the well know data flow criteria are provided in <ref> [5] </ref>. 6 7 uses the total CPU-time spent executing the program under test. Some researchers have used the number of test cases [2] as the independent variable. In our discussion below, we consider these quantities as indicators of testing effort. Thus, as testing effort increases, faults are discovered and removed.
Reference: [6] <author> S. Dalal and C. L. Mallows, </author> <title> "Some Graphical Aids for Deciding When to Stop Testing Soft--ware," </title> <journal> IEEE Journal of Selected Areas in Communications, </journal> <volume> Vol. 8, No. 2, </volume> <month> February </month> <year> 1990, </year> <pages> pp 169-175. </pages>
Reference-contexts: In practice, a variety of criteria, both formal (e.g. a reliability estimate) and informal (e.g. market pressure), are applied to terminate functional testing. As no other form of testing is generally used, this also terminates testing of the product. We point out that Dalal et al <ref> [6] </ref> have a formal method to decide when to stop testing. It is reasonable to assume that as functional testing proceeds, the reliability of the software being tested grows because faults found are removed. However, once its limit has been reached, no additional faults are found.
Reference: [7] <author> R. A. DeMillo and A. P. Mathur, </author> <title> "On the Use of Software Artifacts to Evaluate the Effectiveness of Mutation Analysis for Detecting Errors in Production Software," </title> <type> Technical Report, </type> <institution> SERC-TR-42-P, Software Engineering. Research Center, Purdue University, W. Lafayette, </institution> <note> IN 47907, </note> <year> 1991. </year>
Reference-contexts: As indicated in [16], TRIPTEST exercises T E X in several ways that may be highly improbable in practice. Knuth has also documented [18] all the errors discovered during the debugging and use of T E X . An examination <ref> [7] </ref> of this list of over 850 errors of various kinds indicates that in spite of fiendish amount of functional testing, errors have persisted in T E X .
Reference: [8] <author> C. Ghezzi, M. Jazayeri, and D. Mandrioli, </author> <title> Fundamentals of Software Engineering, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1991. </year>
Reference-contexts: The rationale underlying our model, however, does not depend on which particular white-box testing method is used 2 . It requires that one or more methods of obtaining some form of code coverage be used during testing. We assume that a program is under integration test <ref> [8] </ref>. We consider four different testing methods, namely functional or black-box [14], decision coverage, data flow [5], and mutation [4] testing. Below, we often refer to these four methods as B, D, F, and M, respectively.
Reference: [9] <author> M. R. Girgis and M. R. Woodward, </author> <title> "An Experimental Comparison of the Error Exposing Ability of Program Testing Criteria," </title> <booktitle> Proc. of the Workshop on Software Testing, Validation, and Analysis, </booktitle> <address> Banff, Canada, </address> <month> July 15-17, </month> <year> 1986. </year>
Reference-contexts: This saturation effect has been illustrated for several structure based methods by a few empirical studies in the past <ref> [3, 9, 22] </ref>. Due to its imprecise nature, a proof that saturation effect holds for functional testing is not possible, although Howden's work [14] does provide some empirical justification. Below we present empirical justification based on another study with two relatively larger programs than the ones considered by Howden.
Reference: [10] <author> A. L. Goel, </author> <title> "Software Reliability Models: Assumptions, Limitations, and Applicability," </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> Vol. SE-11, No. 2, </volume> <month> December </month> <year> 1985, </year> <pages> pp 1411-1423. </pages>
Reference-contexts: More formally, we have R = PrfP (d) is correct for any d 2 Dg (3) This definition can be cast in terms of the time-based definition currently in vogue. Let the cumulative effort S k be defined as: S k = i=1 In the literature <ref> [10] </ref>, reliability is defined as the probability that a software system will not fail during the next x time units during operation in a specified environment. Here x is known as the exposure period. <p> R (xjt) ! R as x ! 1 if the test inputs are operationally significant. 3 Basic Assumptions Most of the models rely on certain assumptions that are often not satisfied in practice. Goel has identified these assumptions for various models <ref> [10] </ref>. Several researchers have examined the validity of these assumptions. Here we examine one fundamental assumption, namely the assumption that testing is carried out in accordance with the operational profile. This implies that the testers know and make use of the operational profile of the inputs. <p> The example below illustrates this relationship. Thus, filtering leads to a more pessimistic reliability estimate than the approach that uses unfiltered data. The filtered data can also be applied to any of the other models such as the Goel-Okumoto model <ref> [10] </ref>. 14 Example 1 To show that filtered inter-failure data results in pessimistic reliability estimates, we generated hypothetical data corresponding to E k . These data were filtered and reliability estimates computed using the Musa execution time model [20]. The method for doing so is described below.
Reference: [11] <author> A. L. Goel and K. Okumoto, </author> <title> "A Time Dependent Error Detection Rate Model for Software Reliability and Other Performance Measures," </title> <journal> IEEE Trans. on Reliability, </journal> <volume> Vol. R-28, </volume> <pages> pp 206-211, </pages> <year> 1979. </year>
Reference-contexts: However, once its limit has been reached, no additional faults are found. A tester, not knowing that the limit is reached, continues testing without discovering any more faults. If existing models for reliability estimation are used, e.g. the NHPP model of Goel and Okumoto <ref> [11] </ref>, then as functional testing proceeds beyond its limit the computed reliability estimate improves even though the true reliability of the program remains fixed. The reliability estimate can be improved to any arbitrary limit by increasing the number of test cases executed in the saturation region.
Reference: [12] <author> J. R. Horgan and S. A. </author> <title> London, "ATAC- Automatic Test Analysis for C Programs," </title> <type> Memorandum, </type> <institution> Bell Communications Research (Bellcore) internal memorandum, </institution> <address> TM-TSV-017980, 1990, Morristown, NJ. </address>
Reference-contexts: Further details of data flow testing may be found in <ref> [12] </ref>. Mutation testing helps a tester design test cases based on a notion very different from that of path-oriented testing strategies such as the ones described above. Given a program P , mutation testing generates several syntactically correct mutants of P . <p> However, errors continue to crop up, though with decreasing frequency, in AWK . We used T E X and AWK to determine how much structural coverage one obtains using test data that has been derived from several years of functional testing. Using a data flow testing tool named ATAC <ref> [12] </ref>, we decided to compute various coverage measures when T E X , Version 3.0, is executed on TRIPTEST. Table 1 lists these coverages. Notice that none of the four structural coverages is 100%.
Reference: [13] <author> W. E. Howden, </author> <title> "Reliability of the Path Testing Strategy," </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> Vol. SE-2, No. 3, </volume> <pages> pp 208-215, </pages> <month> September </month> <year> 1976. </year>
Reference: [14] <author> W. E. Howden, </author> <title> "Functional Testing," </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> Vol. SE-6, No. 2, </volume> <pages> pp 162-169, </pages> <month> March </month> <year> 1980. </year>
Reference-contexts: It requires that one or more methods of obtaining some form of code coverage be used during testing. We assume that a program is under integration test [8]. We consider four different testing methods, namely functional or black-box <ref> [14] </ref>, decision coverage, data flow [5], and mutation [4] testing. Below, we often refer to these four methods as B, D, F, and M, respectively. Of these four, functional testing is the most ill-defined, though most often used, method of software testing. <p> Empirical evidence, presented in Section 5.3 below, also suggests that even after a significant effort has been spent in functional, testing the test data so developed is not data flow adequate, and hence not mutation adequate. On the contrary, it has been shown <ref> [14] </ref> that for several types of errors, structural testing is not sufficient, but functional testing is. <p> This saturation effect has been illustrated for several structure based methods by a few empirical studies in the past [3, 9, 22]. Due to its imprecise nature, a proof that saturation effect holds for functional testing is not possible, although Howden's work <ref> [14] </ref> does provide some empirical justification. Below we present empirical justification based on another study with two relatively larger programs than the ones considered by Howden. T E X [17] is a widely used program in the public domain. <p> Thus, for example, if a test set covers a decision, we assume that any other test case that once again exercises the same decision is useless. It is easy to show by examples, and has also been shown empirically <ref> [14] </ref>, that such a test case may indeed reveal faults. This implies that what we consider as a test case amounting to useless effort may indeed be a useful test case that, when run successfully on P , has shown the non-existence of a fault.
Reference: [15] <author> B. W. Kernighan, </author> <type> Personal Communication. </type>
Reference-contexts: An examination [7] of this list of over 850 errors of various kinds indicates that in spite of fiendish amount of functional testing, errors have persisted in T E X . The above observation is indeed true for yet another UNIX utility, namely AWK <ref> [15] </ref>, which has been tested for several years by Kernighan based on its functionality. However, errors continue to crop up, though with decreasing frequency, in AWK .
Reference: [16] <author> D. E. Knuth, </author> <title> "A Torture Test for T E X ," Technical Report No. </title> <address> STAAN-CS-84-1027, </address> <year> 1984, </year> <institution> Department of Computer Science, Stanford University, Stanford, </institution> <address> CA. </address>
Reference-contexts: T E X [17] is a widely used program in the public domain. It has been tested thoroughly for several years by Knuth [18] and, as a result, a widely distributed test set <ref> [16] </ref>, named TRIPTEST, is available for testing T E X . Prior to installation, it is recommended that TRIPTEST be used to ensure that T E X indeed functions as intended by its author. TRIPTEST has been devised by Knuth primarily to test the functionality of T E X . <p> Prior to installation, it is recommended that TRIPTEST be used to ensure that T E X indeed functions as intended by its author. TRIPTEST has been devised by Knuth primarily to test the functionality of T E X . As indicated in <ref> [16] </ref>, TRIPTEST exercises T E X in several ways that may be highly improbable in practice. Knuth has also documented [18] all the errors discovered during the debugging and use of T E X . <p> Knuth does mention the fact that some parts of T E X that are related to such error conditions are not exercised by TRIPTEST <ref> [16] </ref>. To ensure that not all of the uncovered structural elements of T E X correspond to error conditions, we examined the uncovered blocks and decisions and identified a few that are not related to processing error conditions arising at run-time.
Reference: [17] <author> D. E. Knuth, </author> <title> "T E X : The Program," </title> <publisher> Addison Wesley, </publisher> <address> Reading, MA, </address> <year> 1986. </year>
Reference-contexts: Due to its imprecise nature, a proof that saturation effect holds for functional testing is not possible, although Howden's work [14] does provide some empirical justification. Below we present empirical justification based on another study with two relatively larger programs than the ones considered by Howden. T E X <ref> [17] </ref> is a widely used program in the public domain. It has been tested thoroughly for several years by Knuth [18] and, as a result, a widely distributed test set [16], named TRIPTEST, is available for testing T E X .
Reference: [18] <author> D. E. Knuth, </author> <title> "The Errors of T E X ," Software Practice and Experience, </title> <journal> Vol. </journal> <volume> 19, No. 7, </volume> <month> July </month> <year> 1989, </year> <pages> pp 607-685. </pages>
Reference-contexts: Below we present empirical justification based on another study with two relatively larger programs than the ones considered by Howden. T E X [17] is a widely used program in the public domain. It has been tested thoroughly for several years by Knuth <ref> [18] </ref> and, as a result, a widely distributed test set [16], named TRIPTEST, is available for testing T E X . Prior to installation, it is recommended that TRIPTEST be used to ensure that T E X indeed functions as intended by its author. <p> TRIPTEST has been devised by Knuth primarily to test the functionality of T E X . As indicated in [16], TRIPTEST exercises T E X in several ways that may be highly improbable in practice. Knuth has also documented <ref> [18] </ref> all the errors discovered during the debugging and use of T E X . An examination [7] of this list of over 850 errors of various kinds indicates that in spite of fiendish amount of functional testing, errors have persisted in T E X .
Reference: [19] <author> A. P. Mathur, </author> <title> "On the Relative Strengths of Data Flow and Mutation Testing," </title> <booktitle> Proceedings of the Ninth Annual Pacific Northwest Software Quality Conference, </booktitle> <month> October 7-8, </month> <year> 1991, </year> <note> Portland, OR (to appear). </note>
Reference-contexts: It can be shown formally that if a test set is p-use or c-use adequate, then it is also decision adequate [5]. We therefore say that data flow coverage subsumes decision coverage. No such relationship can be proven amongst functional, data flow, and mutation testing. However, empirical evidence <ref> [19] </ref> suggests that test data which is mutation adequate is very likely to be data flow adequate whereas a data flow adequate test set is less likely to be mutation adequate.
Reference: [20] <author> J. D. Musa, A. Iannino, and K. Okumoto, </author> <title> Software Reliability: Measurement, Prediction, Application, </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: Mathur, and Vernon J. Rego are with the Software Engineering Research Center, Department of Computer Sciences, Purdue University, W. Lafayette, IN 47907, (317) 494-7822. This research was supported in part by NSF award CCR-9102331 and also in part by NATO award 5-2-05/RG No. 900108. 1 working software <ref> [20] </ref>. The accuracy of these models, as measured against the predicted versus actual software failure, has varied from one project to another. <p> This implies that the testers know and make use of the operational profile of the inputs. A knowledge of the operational profile, hereafter referred to as profile, implies knowing the frequency distribution with which test inputs are expected to be encountered when the software operates in its intended environment <ref> [20] </ref>. This frequency may presumably be used to decide which test inputs must be used during testing and in what order. Obviously, test cases not encountered during the monitoring of the environment, will correspond to a frequency of zero in the profile. <p> A reliability model often uses one of these quantities as the independent variable. Musa's execution time model <ref> [20] </ref> 3 w.r.t: with respect to. 4 There are several other data flow criteria that we have not mentioned in this paper. Definitions of all the well know data flow criteria are provided in [5]. 6 7 uses the total CPU-time spent executing the program under test. <p> These data were filtered and reliability estimates computed using the Musa execution time model <ref> [20] </ref>. The method for doing so is described below. Here we assume that the effort is measured in terms of CPU time. 1. Assume that the per-fault hazard rate = 0:05 and the total number of expected failures V = 100. 2.
Reference: [21] <author> J. D. Musa and K. Okumoto, </author> <title> "A Logarithmic Poisson Execution Time Model for Software Reliability Measurement," </title> <booktitle> Proc. 7th International Conference on Software Engineering, </booktitle> <address> Orlando, Fl., </address> <month> March </month> <year> 1983, </year> <pages> pp 230-237. </pages>
Reference: [22] <author> P. J. Walsh, </author> <title> "A Measure of Test Case Completeness," </title> <type> Dissertation, </type> <institution> State University of New York, Binghamton, </institution> <address> NY, </address> <year> 1985. </year>
Reference-contexts: This saturation effect has been illustrated for several structure based methods by a few empirical studies in the past <ref> [3, 9, 22] </ref>. Due to its imprecise nature, a proof that saturation effect holds for functional testing is not possible, although Howden's work [14] does provide some empirical justification. Below we present empirical justification based on another study with two relatively larger programs than the ones considered by Howden.
Reference: [23] <author> S. Yamada and S. Osaki, </author> <title> "Software Reliability Growth Modelling: Models and Applications," </title> <journal> IEEE Trans. on Reliability, </journal> <volume> Vol. SE-11, No. 12, </volume> <month> December </month> <year> 1985, </year> <pages> pp 1431-1437. 24 </pages>
Reference-contexts: Here x is known as the exposure period. A more precise definition is given by Yamada et al <ref> [23] </ref> using E k 1 and S k as follows: R (xjt) PrfE k &gt; xjS k1 = tg (5) where R (xjt) denotes the reliability during the next failure interval of x units given the failure history during t units.
References-found: 23

