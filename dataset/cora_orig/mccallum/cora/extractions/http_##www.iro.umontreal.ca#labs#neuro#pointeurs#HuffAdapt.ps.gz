URL: http://www.iro.umontreal.ca/labs/neuro/pointeurs/HuffAdapt.ps.gz
Refering-URL: http://www.iro.umontreal.ca/labs/neuro/other.html
Root-URL: http://www.iro.umontreal.ca
Email: -pigeon,bengioy-@iro.umontreal.ca  
Author: Steven Pigeon Yoshua Bengio 
Address: 6/19/97  
Affiliation: Dpartement dInformatique et de Recherche oprationelle (DIRO) Universit de Montral  
Pubnum: %fi1IQSV]i)JJMGMIRXfi,YJJQERfi%HETXMZIfi'SHMRK %PKSVMXLQfiJSVfi:IV]fi0EVKIfi7IXWfiSJfi7]QFSPW  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> D.A. </author> <title> Huffman A method for the construction of minimum redundancy codes in. </title> <booktitle> Proceedings of the I.R.E. </booktitle> , <address> v40 (1951) p.1098-1101 </address>
Reference-contexts: 1. Introduction and motivation. Of all compression algorithms, it is certainly Huffmans algorithm <ref> [1] </ref> for generating minimum redundancy codes that is the most popular.
Reference: [2] <author> R.G. </author> <title> Gallager Variation on a theme by Huffman IEEE Trans. </title> <booktitle> on Information Theory, IT-24, </booktitle> <address> v6 (nov 1978) p. </address> <pages> 668-674 </pages>
Reference: [3] <author> N. </author> <title> Faller An adaptive system for data compression records of the 7 th Asilomar conference on Circuits, </title> <booktitle> Systems & Computers, 1973, p. </booktitle> <pages> 393-397 </pages>
Reference: [4] <author> D.E. </author> <title> Knuth Dynamic Huffman Coding Journal of Algorithms, </title> <booktitle> v6 1983 p. </booktitle> <pages> 163-180 </pages>
Reference: [5] <author> J.S. </author> <title> Vitter Design and analysis of Dynamic Huffman Codes Journal of the ACM, </title> <booktitle> v34 # 4 (octobre 1987) p. </booktitle> <pages> 823-843 </pages>
Reference-contexts: The algorithm is also making sure that leaves are as close to ( ) log ()p s nodes away from the root as possible. However, this algorithm can be quite bad is some cases as Vitter <ref> [5] </ref> shows. The algorithm can encode the data with as much as (2H+1)t bits, where H is not H (S) but the average code length obtained by the static Huffman algorithm, and t is the total number of symbols (the length of the data sequence). <p> The FGK algorithm also uses a special zero-node which is used to introduce the symbols that have not been seen so far in the sequence. An update is always made in O (-log p (s) ) operations. Vitter, in <ref> [5] </ref>, also proposes an algorithm for adaptive Huffman codes. This algorithm, algorithm L, to follow Vitters own denomination, offers a better bound on compression performance than the FGK algorithm. <p> Vitter shows that his algorithm is guaranteed to encode a data sequence within [H (S),H (S)+1+p+0.086) of the entropy for a sequence sufficiently long. In this technical report, we only present an overview of the algorithm, but the reader may consult <ref> [5] </ref> and [10] for details and a Pascal language implementation. It also updates in O (-log p (s) ) in any case, but the bound on compression is clearly better and thus justify the complexification of the algorithm (which is not too great, either). <p> For algorithm L, we find a column named algorithm L*. This is Paul Howards version of algorithm L (see <ref> [5] </ref> and Acknowledgments). In this program the actual encoding is helped by an arithmetic coder.
Reference: [6] <author> T.C. Bell, J.G. Cleary, I.H. </author> <title> Witten Text compression Prentice-Hall, </title> <address> 1990 (QA76.9 T48B45). </address>
Reference-contexts: Huffmans algorithm generates a set of codes for which the average code length is bounded by the interval [ ( ), ( ) . )H S H S p+ + 0 086 , where p is the largest probability of all symbols s in S <ref> [6, p. 107] </ref>. While being rather efficient, this algorithm has several practical disadvantages. The first is that it is a static code. <p> We finally discuss about various initialization schemes and compare our results against the Calgary Corpus and static Huffman coding (the ordinary Huffman algorithm). 2. Huffmans Algorithm for Minimum-Redundancy Codes. Huffmans algorithm is already described in detail in many textbooks like the excellent <ref> [6] </ref>. Huffmans procedure, quite simple, to construct the code book for a set S and a probability function p (s) is shown in Algorithm 1 and fig. 1. <p> Huffman algorithms strategy is to build a tree with weights that are as balanced as possible. What is unusual in Huffmans algorithm is that instead of building this tree from the root down to the leaf (which would then be Shannon-Fanos algorithm! <ref> [6] </ref>) it is built from the leaves up to the root. The first step is to build a list which contains only leaves. Each leaf has an associated symbol s and a weight, p (s). We pick, out of that list, the two element with the smallest weights. <p> The Shannon-Fano isnt optimal (as Huffmans algorithm is) but it produces codes within [H (S), H (S)+1) of the entropy of the set S=-K 1 ,K 2 ,...,K m - <ref> [6] </ref>. 2 However, one may want to put in the set K i all symbols of approximately the same probability. In that case, the bounds change because the rounding of the codes will be different.
Reference: [7] <author> G. Seroussi, M.J. </author> <title> Weinberger On Adaptive Strategies for an Extended Family of Golomb-type Codes in DCC 1997, Snowbird, </title> <publisher> IEEE Computer Society Press. </publisher>
Reference: [8] <author> Alistair Moffat and J. </author> <title> Katajainen In place Calculation of minimum redundancy codes in Proceedings of the Workshop on Algorithms and Data Structures, </title> <institution> Kingston University, </institution> <address> 1995, </address> <publisher> LNCS 995 Springer Verlag. </publisher>
Reference: [9] <author> Alistair Moffat et Andrew Turpin, </author> <title> On the implementation of minimum-redundancy prefix codes (extended abstract), </title> <booktitle> in DCC 1996, p. </booktitle> <pages> 171-179. </pages>
Reference: [10] <author> J.S. </author> <title> Vitter Dynamic Huffman Coding Manuscript from the Internet with a Pascal source. </title>
Reference-contexts: Vitter shows that his algorithm is guaranteed to encode a data sequence within [H (S),H (S)+1+p+0.086) of the entropy for a sequence sufficiently long. In this technical report, we only present an overview of the algorithm, but the reader may consult [5] and <ref> [10] </ref> for details and a Pascal language implementation. It also updates in O (-log p (s) ) in any case, but the bound on compression is clearly better and thus justify the complexification of the algorithm (which is not too great, either).
Reference: [11] <institution> Steven Pigeon A Fast Image Compression Method Based on the Fast Hartley Transform AT&T Research, </institution> <note> Speech and Image Processing Lab 6 Technical Report (number </note> ???) 
Reference-contexts: In the JPEG still image compression standard, the image is first decomposed by a discrete cosine transform and the coefficients are then quantized. Once quantization is done, a scheme, not unlike the one in Fig. 2. <ref> [11] </ref>, is used to represent the possible values of the coefficients. There are however fewer values possible in JPEG, but still a few thousands. There is plenty of other situations were a large number of symbols is needed.
Reference: [12] <author> Douglas W. </author> <title> Jones Application of Splay Trees to Data Compression CACM, </title> <note> V31 #8 (August 1988) p. 998-1007 </note>
References-found: 12

