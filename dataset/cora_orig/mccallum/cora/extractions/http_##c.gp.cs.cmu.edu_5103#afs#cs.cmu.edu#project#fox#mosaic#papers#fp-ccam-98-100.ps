URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/project/fox/mosaic/papers/fp-ccam-98-100.ps
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/project/fox/mosaic/papers.html
Root-URL: http://www.cs.cmu.edu
Title: Run-time Code Generation and Modal-ML  
Author: Philip Wickline Peter Lee Frank Pfenning 
Note: The views and conclusions contained in this document are those of the authors and should not be interpreted as representing official policies, either expressed or implied, of the National Science Foundation, the Advanced Research Projects Agency, or the U.S. Government  
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Date: January, 1998  
Pubnum: CMU-CS-98-100  
Abstract: Also appears as CMU-CS-FOX-98-01. A revised version of this paper will appear in the 1998 ACM SIGPLAN Conference on Programming Language Design and Implementation. This research was sponsored in part by the Advanced Research Projects Agency CSTO under the title "The Fox Project: Advanced Languages for Systems Software", ARPA Order No. C533, issued by ESC/ENS under Contract No. F19628-95-C-0050, and in part by the National Science Foundation under grant#CCR-9619832 . 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Charles Consel and Fran~cois Noel. </author> <title> A general approach for run-time specialization and its application to C. </title> <booktitle> In Conference Record of POPL '96: The 23 rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <pages> pages 145-156, </pages> <month> 21-24 January </month> <year> 1996. </year>
Reference-contexts: More recent work has extended the partial evaluation framework to account for multiple computation stages [7]. In recent years, several researchers have studied the use of run-time code generation (RTCG) to exploit staged computation <ref> [1, 3, 9, 10, 12] </ref>. One advantage of RTCG is that opens the possibility of low-level code optimizations (such as register allocation, instruction selection, loop unrolling, array-bounds checking removal, and so on) to take advantage of values that are not known until run time. <p> Determining this staging information is not a simple matter, however. While automatic binding-time analyses have been used by partial evaluators and some compilers (notably the Tempo system <ref> [1] </ref>), we are interested here in developing a programming language that supports a systematic method for describing the computation stages. <p> Using values obtained in earlier computations, these generating extensions create code specialized to perform later computations. While several different schemes for run-time code generation have been used in other systems <ref> [4, 3, 13, 12, 1] </ref> Fabius is able to achieve a remarkably low instruction-executed-to-instruction-specialized ratio by a unique combination of features. * Generating extensions produced by Fabius never manipulate source-level terms at run time. Instead machine language programs are synthesized directly from machine language programs. <p> Instead machine language programs are synthesized directly from machine language programs. Fabius in not unique in this respect: the Synthesis kernel [13, 12] and Tempo compiler <ref> [1] </ref> also share this property. * Fabius encodes terms to be specialized directly into the instruction stream, usually in the form of immediate operands to instructions. This is in contrast to systems which copy templates and fill in holes at run time, such as Tempo and the Synthesis kernel.
Reference: [2] <author> Rowan Davies and Frank Pfenning. </author> <title> A modal analysis of staged computation. </title> <booktitle> In Conference Record of POPL '96: The 23 rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <pages> pages 258-270, </pages> <month> 21-24 January </month> <year> 1996. </year>
Reference-contexts: We propose that an extension of the SML language and type system can be used as a clear and expressive notation for staged computation. Drawing on previous work on the language 2 <ref> [2] </ref> which is based on the modal logic S4, and on the interpretation of this language for run-time code generation described in [18], we present an implementation of a prototype compiler for a version of the SML language (without modules) that uses modal operators to specify early and late stages of <p> M j lift M j let cogen u = M in N Contexts ::= j ; x : A j ; u : A 2 The Modal Lambda-Calculus We briefly introduce the language 2 which is a simplification of the explicit version of ML 2 described in Davies and Pfenning <ref> [2] </ref>. Although we present only 2 here because of space considerations, the compilation technique described in the section 5 extends easily to all core SML constructs. <p> In contrast to code this prohibits all optimizations during code generation. As noted in Davies and Pfenning <ref> [2] </ref>, lift is definable in ML 2 for base types, but its general form has no logical foundation.
Reference: [3] <author> Dawson R. Engler, Wilson C. Hsieh, and M. Frans Kaashoek. </author> <title> `C: A language for high-level, efficient, and machine-independent dynamic code generation. </title> <booktitle> In Conference Record of POPL '96: The 23 rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <pages> pages 131-144, </pages> <month> 21-24 January </month> <year> 1996. </year>
Reference-contexts: More recent work has extended the partial evaluation framework to account for multiple computation stages [7]. In recent years, several researchers have studied the use of run-time code generation (RTCG) to exploit staged computation <ref> [1, 3, 9, 10, 12] </ref>. One advantage of RTCG is that opens the possibility of low-level code optimizations (such as register allocation, instruction selection, loop unrolling, array-bounds checking removal, and so on) to take advantage of values that are not known until run time. <p> The idea of using a programming notation for staging is far from new. The backquote and antiquote notation of Lisp macros, for example, provides an intuitive though highly error-prone approach to staged computation. More recent annotation schemes used by RTCG systems include that of `C <ref> [3] </ref> and Fabius [10]. These languages allow the programmer to communicate his intentions to the compiler in a relatively straightforward manner. Unfortunately, in the case of the Fabius system, the annotation scheme is extremely simple, thus limiting the ability of the programmer to express staging decisions. <p> Using values obtained in earlier computations, these generating extensions create code specialized to perform later computations. While several different schemes for run-time code generation have been used in other systems <ref> [4, 3, 13, 12, 1] </ref> Fabius is able to achieve a remarkably low instruction-executed-to-instruction-specialized ratio by a unique combination of features. * Generating extensions produced by Fabius never manipulate source-level terms at run time. Instead machine language programs are synthesized directly from machine language programs.
Reference: [4] <author> Dawson R. Engler and Todd A. Proebsting. </author> <title> DCG: An efficient, retargetable dynamic code generation system. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI ), pages 263-272. </booktitle> <publisher> ACM Press, </publisher> <month> October </month> <year> 1994. </year>
Reference-contexts: Using values obtained in earlier computations, these generating extensions create code specialized to perform later computations. While several different schemes for run-time code generation have been used in other systems <ref> [4, 3, 13, 12, 1] </ref> Fabius is able to achieve a remarkably low instruction-executed-to-instruction-specialized ratio by a unique combination of features. * Generating extensions produced by Fabius never manipulate source-level terms at run time. Instead machine language programs are synthesized directly from machine language programs.
Reference: [5] <author> Dawson R. Engler, Deborah Wallach, and M. Frans Kaashoek. </author> <title> Efficient, safe, application-specific message processing. </title> <type> Technical Memorandum MIT/LCS/TM533, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: As demonstrated by several researchers, run-time code generation can eliminate the overhead of interpretation by specializing the interpreter to each packet filter program as it is installed. This has the effect of compiling each packet filter into safe native code <ref> [5, 10, 13, 17] </ref>.
Reference: [6] <author> G.Cousineau, P.-L. Curien, and M. Mauny. </author> <title> The categorical abstract machine. </title> <booktitle> Science of Computer Programming, </booktitle> <pages> pages 173-202, </pages> <year> 1987. </year>
Reference-contexts: In order to demonstrate these advantages, we have implemented a prototype compiler for the Standard ML language (without modules), extended with modal operators and types. The compiler generates code for a version of the Categorial Abstract Machine <ref> [6] </ref>, called the CCAM, which is extended with a facility for emitting fresh code at run time. We begin the paper with a brief introduction to the 2 language, on which our dialect of SML is based. <p> Furthermore, generic memoization routines could be written that can easily accomodate most common memoization needs. 4 The CCAM ' In this section we present the CCAM, an ad-hoc extension of the CAM <ref> [6] </ref> which provides facilities for run-time code generation and which we use as the target of the compiler detailed in the next section. 4.1 Fabius and Run-Time Code Gen eration The Fabius compiler [10] delivers dramatic speedups over conventional compilers for some programs by compiling selected functions in its input to <p> The rules for translating applications, non-modal variables, and abstractions in a non-code-generating context are the same as those in <ref> [6] </ref>. We compile code expressions to generating extensions, which are functions from arenas to arenas. An extension emits its code into its argument arena, and returns that transformed arena.
Reference: [7] <author> Robert Gluck and Jesper Jtrgensen. </author> <title> Efficient multi-level generating extensions. </title> <booktitle> In Programming Languages, Implementations, Logics and Programs (PLILP'95), volume 1181 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: This information is used to synthesize a generating extension that will generate specialized code for the late stages of the computation when given the first-stage inputs. More recent work has extended the partial evaluation framework to account for multiple computation stages <ref> [7] </ref>. In recent years, several researchers have studied the use of run-time code generation (RTCG) to exploit staged computation [1, 3, 9, 10, 12].
Reference: [8] <author> Neil D. Jones, Carsten K. Gomard, and Peter Sestoft. </author> <title> Partial Evaluation and Automatic Program Generation. </title> <publisher> Prentice-Hall, </publisher> <year> 1993. </year>
Reference-contexts: To achieve this effect, programmers often stage their program manually, using ad hoc methods; there have also been some attempts to make such staging transformations more systematic [16]. Another approach, used in partial evaluation <ref> [8] </ref>, is to automate the staging of programs according to a programmer-supplied indication of which program inputs will be available in the first stage of computation.
Reference: [9] <author> David Keppel, Susan J. Eggers, and Robert R. Henry. </author> <title> Evaluating runtime-compiled value-specific optimizations. </title> <type> Technical Report 93-11-02, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: More recent work has extended the partial evaluation framework to account for multiple computation stages [7]. In recent years, several researchers have studied the use of run-time code generation (RTCG) to exploit staged computation <ref> [1, 3, 9, 10, 12] </ref>. One advantage of RTCG is that opens the possibility of low-level code optimizations (such as register allocation, instruction selection, loop unrolling, array-bounds checking removal, and so on) to take advantage of values that are not known until run time.
Reference: [10] <author> Peter Lee and Mark Leone. </author> <title> Optimizing ML with run-time code generation. </title> <booktitle> In Proceedings of the ACM SIGPLAN '96 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 137-148, </pages> <address> Philadelphia, Pennsylvania, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: More recent work has extended the partial evaluation framework to account for multiple computation stages [7]. In recent years, several researchers have studied the use of run-time code generation (RTCG) to exploit staged computation <ref> [1, 3, 9, 10, 12] </ref>. One advantage of RTCG is that opens the possibility of low-level code optimizations (such as register allocation, instruction selection, loop unrolling, array-bounds checking removal, and so on) to take advantage of values that are not known until run time. <p> The idea of using a programming notation for staging is far from new. The backquote and antiquote notation of Lisp macros, for example, provides an intuitive though highly error-prone approach to staged computation. More recent annotation schemes used by RTCG systems include that of `C [3] and Fabius <ref> [10] </ref>. These languages allow the programmer to communicate his intentions to the compiler in a relatively straightforward manner. Unfortunately, in the case of the Fabius system, the annotation scheme is extremely simple, thus limiting the ability of the programmer to express staging decisions. <p> We then apply compilation techniques patterned after those developed for the Fabius system <ref> [10] </ref> in order to compile programs into code that performs RTCG according to the mode of each subexpression in the program. <p> As demonstrated by several researchers, run-time code generation can eliminate the overhead of interpretation by specializing the interpreter to each packet filter program as it is installed. This has the effect of compiling each packet filter into safe native code <ref> [5, 10, 13, 17] </ref>. <p> 4 The CCAM ' In this section we present the CCAM, an ad-hoc extension of the CAM [6] which provides facilities for run-time code generation and which we use as the target of the compiler detailed in the next section. 4.1 Fabius and Run-Time Code Gen eration The Fabius compiler <ref> [10] </ref> delivers dramatic speedups over conventional compilers for some programs by compiling selected functions in its input to generating extensions. Using values obtained in earlier computations, these generating extensions create code specialized to perform later computations.
Reference: [11] <author> Mark Leone and Peter Lee. </author> <title> Dynamic specialization in the fabius system. </title> <booktitle> ACM Computing Surveys 1998 Symposium on Partial Evaluation, </booktitle> <year> 1998. </year>
Reference-contexts: A compiler is free to augment the staging requirements from a hand-staged program using any other means at its disposal. * The language naturally handles situations in which more than two stages are desired, such as Fabius-style multi-stage specialization <ref> [11] </ref>. This arises, for example, when dynamically generated code can used to compute values that are used in the dynamic specialization of yet more code. In order to demonstrate these advantages, we have implemented a prototype compiler for the Standard ML language (without modules), extended with modal operators and types.
Reference: [12] <author> Henry Massalin. </author> <title> Synthesis: An Efficient Implementation of Fundamental Operating System Services. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Columbia University, </institution> <year> 1992. </year>
Reference-contexts: More recent work has extended the partial evaluation framework to account for multiple computation stages [7]. In recent years, several researchers have studied the use of run-time code generation (RTCG) to exploit staged computation <ref> [1, 3, 9, 10, 12] </ref>. One advantage of RTCG is that opens the possibility of low-level code optimizations (such as register allocation, instruction selection, loop unrolling, array-bounds checking removal, and so on) to take advantage of values that are not known until run time. <p> Using values obtained in earlier computations, these generating extensions create code specialized to perform later computations. While several different schemes for run-time code generation have been used in other systems <ref> [4, 3, 13, 12, 1] </ref> Fabius is able to achieve a remarkably low instruction-executed-to-instruction-specialized ratio by a unique combination of features. * Generating extensions produced by Fabius never manipulate source-level terms at run time. Instead machine language programs are synthesized directly from machine language programs. <p> Instead machine language programs are synthesized directly from machine language programs. Fabius in not unique in this respect: the Synthesis kernel <ref> [13, 12] </ref> and Tempo compiler [1] also share this property. * Fabius encodes terms to be specialized directly into the instruction stream, usually in the form of immediate operands to instructions.
Reference: [13] <author> Henry Massalin and Calton Pu. </author> <title> Threads and input/output in the Synthesis kernel. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 191-201, </pages> <month> De-cember </month> <year> 1989. </year> <month> 12 </month>
Reference-contexts: As demonstrated by several researchers, run-time code generation can eliminate the overhead of interpretation by specializing the interpreter to each packet filter program as it is installed. This has the effect of compiling each packet filter into safe native code <ref> [5, 10, 13, 17] </ref>. <p> Using values obtained in earlier computations, these generating extensions create code specialized to perform later computations. While several different schemes for run-time code generation have been used in other systems <ref> [4, 3, 13, 12, 1] </ref> Fabius is able to achieve a remarkably low instruction-executed-to-instruction-specialized ratio by a unique combination of features. * Generating extensions produced by Fabius never manipulate source-level terms at run time. Instead machine language programs are synthesized directly from machine language programs. <p> Instead machine language programs are synthesized directly from machine language programs. Fabius in not unique in this respect: the Synthesis kernel <ref> [13, 12] </ref> and Tempo compiler [1] also share this property. * Fabius encodes terms to be specialized directly into the instruction stream, usually in the form of immediate operands to instructions.
Reference: [14] <author> Steven McCanne and Van Jacobson. </author> <title> The BSD packet filter: A new architecture for user-level packet capture. </title> <booktitle> In The Winter 1993 USENIX Conference, </booktitle> <pages> pages 259-269. </pages> <publisher> USENIX Association, </publisher> <month> January </month> <year> 1993. </year>
Reference-contexts: As a result, many useless packets may be delivered, with a consequent degradation of performance. A commonly adopted solution to this problem is to allow user-level processes to install a program that implements a selection predicate into the kernel's address space <ref> [15, 14] </ref>. In order to ensure that the selection predicate will not corrupt internal kernel structures, the predicate must be expressed in a "safe" programming language. Unfortunately, this approach has a substantial overhead, since the safe programming language is typically implemented by a simple (and therefore easy-to-trust) interpreter. <p> This has the effect of compiling each packet filter into safe native code [5, 10, 13, 17]. To demonstrate this idea in our language, consider the following excerpt of the implementation of a simple interpreter for the BSD packet filter language <ref> [14] </ref> in SML. (* val evalpf : instruction array * * int array * * int * int * int -&gt; int * Return 1 to select packet, 0 to reject, * ~1 if error *) fun evalpf (filter, pkt, A, X, pc) = if pc &gt; length filter then ~1
Reference: [15] <author> Jeffrey C. Mogul, Richard F. Rashid, and Michael J. Accetta. </author> <title> The packet filter: An efficient mechanism for user-level network code. </title> <booktitle> In ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 39-51. </pages> <publisher> ACM Press, </publisher> <month> November </month> <year> 1987. </year> <note> An updated version is available as DEC WRL Research Report 87/2. </note>
Reference-contexts: As a result, many useless packets may be delivered, with a consequent degradation of performance. A commonly adopted solution to this problem is to allow user-level processes to install a program that implements a selection predicate into the kernel's address space <ref> [15, 14] </ref>. In order to ensure that the selection predicate will not corrupt internal kernel structures, the predicate must be expressed in a "safe" programming language. Unfortunately, this approach has a substantial overhead, since the safe programming language is typically implemented by a simple (and therefore easy-to-trust) interpreter.
Reference: [16] <author> Ulrik Jtrring and William L. Scherlis. </author> <title> Compilers and staging transformations. </title> <booktitle> In Conference Record of POPL '86: The th th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <pages> pages 86-96, </pages> <month> 21-24 January </month> <year> 1986. </year>
Reference-contexts: To achieve this effect, programmers often stage their program manually, using ad hoc methods; there have also been some attempts to make such staging transformations more systematic <ref> [16] </ref>. Another approach, used in partial evaluation [8], is to automate the staging of programs according to a programmer-supplied indication of which program inputs will be available in the first stage of computation.
Reference: [17] <author> Emin Gun Sirer, Stefan Savage, Przemyslaw Pardyak, Greg P. DeFouw, and Brian N. Ber-shad. </author> <title> Writing an operating system with Modula-3. </title> <booktitle> In The Inaugural Workshop on Compiler Support for Systems Software, </booktitle> <pages> pages 134-140, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: As demonstrated by several researchers, run-time code generation can eliminate the overhead of interpretation by specializing the interpreter to each packet filter program as it is installed. This has the effect of compiling each packet filter into safe native code <ref> [5, 10, 13, 17] </ref>.
Reference: [18] <author> Philip Wickline, Peter Lee, Frank Pfenning, and Rowan Davies. </author> <title> Modal types as staging specifications for run-time code generation. </title> <booktitle> ACM Computing Surveys 1998 Symposium on Partial Evaluation, </booktitle> <year> 1998. </year> <month> 13 </month>
Reference-contexts: Drawing on previous work on the language 2 [2] which is based on the modal logic S4, and on the interpretation of this language for run-time code generation described in <ref> [18] </ref>, we present an implementation of a prototype compiler for a version of the SML language (without modules) that uses modal operators to specify early and late stages of a program's computation.
References-found: 18

