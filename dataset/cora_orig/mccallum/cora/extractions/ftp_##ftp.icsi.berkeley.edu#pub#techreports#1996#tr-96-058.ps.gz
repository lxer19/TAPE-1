URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1996/tr-96-058.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1996.html
Root-URL: http://www.icsi.berkeley.edu
Title: Recognition of Handwritten Digits and Human Faces by Convolutional Neural Networks  
Phone: (510) 643-9153 FAX (510) 643-7684  
Author: Claus Neubauer 
Date: December 1996  
Address: I 1947 Center St. Suite 600 Berkeley, California 94704-1198  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  
Pubnum: TR-96-058  
Abstract: Convolutional neural networks provide an efficient method to constrain the complexity of feedforward neural networks by weightsharing. This network topology has been applied in particular to image classification when raw images are to be classified without preprocessing. In this paper two variations of convolutional networks - Neocognitron and Neoperceptron are compared with classifiers based on fully connected feedforward layers (i.e. Multilayerperceptron, Nearest Neighbor Classifier, Autoencoding network) with respect to their visual recognition performance. Beside the original Neocognitron a modification called Neoperceptron is proposed which combines neurons from Perceptron with the localized network structure of Neocognitron. Instead of training convolutional networks by time-consuming error backpropagation in this work a modular procedure is applied, whereby layers are trained sequentially from the input to the output layer in order to recognize features of increasing complexity. For a quantitative experimental comparison with standard classifiers two very different recognition tasks have been chosen: handwritten digit recognition and face recognition. In the first example on handwritten digit recognition the generalization of convolutional networks is compared to fully connected networks. In several experiments the influence of variations of position, size and orientation of digits is determined and the relation between training sample size and validation error is observed. In the second example recognition of human faces is investigated under constrained and variable conditions with respect to face orientation and illumination and the limitations of convolutional networks are discussed. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N.M. Allinson, A.W. Ellis, B. Flude, A. Luckman, </author> <title> "A Connectionist Model of Familiar Face Recognition", </title> <booktitle> in IEE Colloquium on Machine Storage and Recognition of Faces, </booktitle> <pages> pp. </pages> <address> 5.1-5.9, </address> <year> 1992 </year>
Reference-contexts: Alternatively the threelayered fully connected feedforward architecture is trained with two other training strategies. Here instead of overall supervised learning the hidden layer is first trained unsupervised and afterwards the output layer is trained supervised by least mean square error minimization. For this approach the Selforganizing Feature Map <ref> [1] </ref>, Autoencoding network [6] and Eigenfaces [24] have been proposed for unsupervised training of the first hidden layer. Beside pure feedforward classifiers there exist also iterative approaches like dynamic link architecture [15], which perform a graph matching between image and model, using simulated annealing for optimization.
Reference: [2] <author> D. Beymer, T. Poggio, </author> <title> "Image Representations for Visual Learning", </title> <journal> Science, </journal> <volume> vol. 272, </volume> <pages> pp. 1905-1909, </pages> <year> 1996 </year>
Reference-contexts: Based on these features it should be possible to learn a new person by a few images only and nevertheless 14 good generalization on unknown instances and poses should be obtained. In this case for example the generation of virtual views proposed by <ref> [2] </ref> can help to further improve identification accuracy. Beside the applications discussed here convolutional networks will be integrated as classifiers for automatic x-ray inspection of solder joints in electronic production, where three-dimensional datasets have to be evaluated.
Reference: [3] <author> C. Bishop, </author> <title> "Neural Networks for Pattern Recognition", </title> <publisher> Oxford Press, </publisher> <year> 1995. </year>
Reference-contexts: Weightsharing can also be considered as an alternative to weight elimination [26] in order to reduce the number of weights <ref> [3] </ref>. Moreover networks with local topology can more effectively be migrated to a locally connected parallel computer than fully connected feedforward networks [14]. The Neocognitron [8], [9], [10], [11], which can be considered as the first realization of convolutional networks has been introduced by Fukuskima. <p> In this case unsupervised algorithms for feature extraction are applied like principle component analysis or autoencoding learning [6], [23]. Both unsupervised algorithms give similar results since it has been shown that the weights in the hidden layer of a threelayered autoencoding network converge to principal components <ref> [3] </ref>, [5]. a l (-; ; ^ k) = q l c l (-)u cl1 (^n + -; ) (3) 3 Classification of Handwritten Digits First the performance of convolutional neural networks and fully connected networks is compared for recognition of handwritten digits.
Reference: [4] <author> H. Bouattour, F. Fogelman Soulie, E. Viennet, </author> <title> "Solving the Human Face Recognition Task using Neural Nets", in Artificial Neural Networks II, </title> <editor> I. Alexander, J. Taylor, eds, </editor> <publisher> North- Holland, Amsterdam, </publisher> <pages> pp. 1595-1598, </pages> <year> 1992 </year>
Reference-contexts: Fukushima applied the Neocognitron primarily to handwritten digit recognition. Later variants of convolutional networks have been applied for example to large scale zip code recognition and face recognition [16], <ref> [4] </ref>. Within a convolutional architecture there are several possibilities to combine different kinds of neurons and learning rules. One method is to use Mc-Culloch Pitts neurons, which calculate a weighted sum plus sigmoid nonlinearity, and to train the whole network by error backpropagation [27].
Reference: [5] <author> H. Bourlard, Y. Kamp, </author> <title> "Autoassoziation by Multilayerperceptrons and Singular Value Decomposition", </title> <journal> Biological Cybernetics, </journal> <volume> vol. 59, </volume> <pages> pp. 291-294, </pages> <year> 1988 </year>
Reference-contexts: In this case unsupervised algorithms for feature extraction are applied like principle component analysis or autoencoding learning [6], [23]. Both unsupervised algorithms give similar results since it has been shown that the weights in the hidden layer of a threelayered autoencoding network converge to principal components [3], <ref> [5] </ref>. a l (-; ; ^ k) = q l c l (-)u cl1 (^n + -; ) (3) 3 Classification of Handwritten Digits First the performance of convolutional neural networks and fully connected networks is compared for recognition of handwritten digits.
Reference: [6] <author> G.W. Cottrell, "EMPATH: </author> <title> Face, Emotion, and Gender Recognition Using Holons", </title> <booktitle> in Advances in Neural Information Processing Systems, </booktitle> <publisher> Morgan Kaufmann, vol. </publisher> <pages> 3, pp. 564-571, </pages> <year> 1991 </year>
Reference-contexts: For a problem like face recognition on the other hand it is difficult to train intermediate layers supervised since it is not known which higher level features are most important. In this case unsupervised algorithms for feature extraction are applied like principle component analysis or autoencoding learning <ref> [6] </ref>, [23]. <p> Here instead of overall supervised learning the hidden layer is first trained unsupervised and afterwards the output layer is trained supervised by least mean square error minimization. For this approach the Selforganizing Feature Map [1], Autoencoding network <ref> [6] </ref> and Eigenfaces [24] have been proposed for unsupervised training of the first hidden layer. Beside pure feedforward classifiers there exist also iterative approaches like dynamic link architecture [15], which perform a graph matching between image and model, using simulated annealing for optimization.
Reference: [7] <author> J.G. Daugman, </author> <title> "High Confidence Visual Recognition of Persons by a Test of Statistical Independence", </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 15, </volume> <pages> pp. 1148-1161, </pages> <year> 1993 </year>
Reference-contexts: There exist several reliable methods for identification of persons like iris diagnosis <ref> [7] </ref> or fingerprint analysis but they are quite uncomfortable so that identification by face images is preferred if the accuracy is sufficient.
Reference: [8] <author> K. Fukushima, </author> <title> "Neocognitron: A Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position", </title> <journal> Biological Cybernetics, </journal> <volume> vol. 36, </volume> <pages> pp. 193-202, </pages> <year> 1980 </year>
Reference-contexts: Weightsharing can also be considered as an alternative to weight elimination [26] in order to reduce the number of weights [3]. Moreover networks with local topology can more effectively be migrated to a locally connected parallel computer than fully connected feedforward networks [14]. The Neocognitron <ref> [8] </ref>, [9], [10], [11], which can be considered as the first realization of convolutional networks has been introduced by Fukuskima. In the Neocognitron model for the first time receptive fields are used extensively, which have been discovered in the cat's visual cortex by Hubel and Wiesel [12], [13]. <p> A comprehensive evaluation considering the influence of varying training sample size and other key parameters is subject of this paper (see also [22]). 1 2 Convolutional Neural Networks: Neocognitron and Neop- erceptron 2.1 Network Structure A detailed description of the Neocognitron architecture can be found in <ref> [8] </ref>. In Fig. 1 the topology of convolutional networks used in this work is shown. The raw image is feed into the input layer (1C) and determines the size of the inputvector. <p> Neurons perform local feature extraction and therefore each neuron is connected by a receptive field to a small area of the previous layer. In this work a four layered network is applied. The hidden layers consist of S-sublayer and C-sublayer (see <ref> [8] </ref> for details) and each sublayer itself consists of several planes. The input layer is first mapped onto multiple planes of the 2S-sublayer. Each plane of a layer contains neurons which are extracting a particular local feature like a oriented bar or edge.
Reference: [9] <author> K. Fukushima, </author> <title> "A Neural Network Model for Selective Attention in Visual Pattern recognition", </title> <journal> Biological Cybernetics, </journal> <volume> vol. 55, </volume> <pages> pp. 5-15, </pages> <year> 1986 </year>
Reference-contexts: Weightsharing can also be considered as an alternative to weight elimination [26] in order to reduce the number of weights [3]. Moreover networks with local topology can more effectively be migrated to a locally connected parallel computer than fully connected feedforward networks [14]. The Neocognitron [8], <ref> [9] </ref>, [10], [11], which can be considered as the first realization of convolutional networks has been introduced by Fukuskima. In the Neocognitron model for the first time receptive fields are used extensively, which have been discovered in the cat's visual cortex by Hubel and Wiesel [12], [13].
Reference: [10] <author> K. Fukushima, </author> <title> "Analysis of the Process of Visual Pattern Recognition by the Neocog-nitron", </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 2, </volume> <pages> pp. 413-421, </pages> <year> 1989 </year>
Reference-contexts: Weightsharing can also be considered as an alternative to weight elimination [26] in order to reduce the number of weights [3]. Moreover networks with local topology can more effectively be migrated to a locally connected parallel computer than fully connected feedforward networks [14]. The Neocognitron [8], [9], <ref> [10] </ref>, [11], which can be considered as the first realization of convolutional networks has been introduced by Fukuskima. In the Neocognitron model for the first time receptive fields are used extensively, which have been discovered in the cat's visual cortex by Hubel and Wiesel [12], [13]. <p> This approach has been applied for zip code recognition [16]. In contrast within the Neocognitron neurons calculate a weighted sum normalized by the incoming signal which results in a normalized convolution <ref> [10] </ref>. Furthermore weights are trained layer by layer independently by reinforcement and a winner takes all rule. This approach has been shown to be feasible for binarized images of digits but has not been verified on large data sets. <p> The same topology is used for the Neoperceptron but with a different kind of S-neuron. 2.2 Model of Neurons In the Neocognitron the S-neurons (together with the V-neurons) are described by (1). Is has been shown in <ref> [10] </ref> that (1) including the normalization term (V-neurons u vl ) approximates a convolution normalized by the length of the weightvector and the inputvector.
Reference: [11] <author> K. Fukushima, T. Imagawa, </author> <title> "Recognition and Segmentation of Connected Characters with Selective Attention", </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 6, </volume> <pages> pp. 33-41, </pages> <year> 1993 </year> <month> 15 </month>
Reference-contexts: Weightsharing can also be considered as an alternative to weight elimination [26] in order to reduce the number of weights [3]. Moreover networks with local topology can more effectively be migrated to a locally connected parallel computer than fully connected feedforward networks [14]. The Neocognitron [8], [9], [10], <ref> [11] </ref>, which can be considered as the first realization of convolutional networks has been introduced by Fukuskima. In the Neocognitron model for the first time receptive fields are used extensively, which have been discovered in the cat's visual cortex by Hubel and Wiesel [12], [13].
Reference: [12] <author> D.H. Hubel, </author> <title> T.N. Wiesel, "Receptive Fields, Binocular Interaction and Functional Architecture in the Cat's Visual Cortex", </title> <journal> J. Physiology, </journal> <volume> vol. 160, </volume> <pages> pp. 106-154, </pages> <year> 1962 </year>
Reference-contexts: The Neocognitron [8], [9], [10], [11], which can be considered as the first realization of convolutional networks has been introduced by Fukuskima. In the Neocognitron model for the first time receptive fields are used extensively, which have been discovered in the cat's visual cortex by Hubel and Wiesel <ref> [12] </ref>, [13]. Fukushima applied the Neocognitron primarily to handwritten digit recognition. Later variants of convolutional networks have been applied for example to large scale zip code recognition and face recognition [16], [4]. Within a convolutional architecture there are several possibilities to combine different kinds of neurons and learning rules.
Reference: [13] <author> D.H. Hubel, </author> <title> T.N. Wiesel, "Receptive Fields and Functional Architecture in Two Non-striate Visual Areas (18 und 19) of the Cat", </title> <journal> J. Neurophysiology, </journal> <volume> vol. 28, </volume> <pages> pp. 229-289, </pages> <year> 1965 </year>
Reference-contexts: The Neocognitron [8], [9], [10], [11], which can be considered as the first realization of convolutional networks has been introduced by Fukuskima. In the Neocognitron model for the first time receptive fields are used extensively, which have been discovered in the cat's visual cortex by Hubel and Wiesel [12], <ref> [13] </ref>. Fukushima applied the Neocognitron primarily to handwritten digit recognition. Later variants of convolutional networks have been applied for example to large scale zip code recognition and face recognition [16], [4]. Within a convolutional architecture there are several possibilities to combine different kinds of neurons and learning rules.
Reference: [14] <author> T. Ito, K. Fukushima, </author> <title> "Realization of a Neural Network Model Neocognitron on a Hypercube Parallel Computer", </title> <journal> Int. J. of High Speed Computing, </journal> <volume> vol. 2, </volume> <pages> pp. 1-16, </pages> <year> 1990 </year>
Reference-contexts: Weightsharing can also be considered as an alternative to weight elimination [26] in order to reduce the number of weights [3]. Moreover networks with local topology can more effectively be migrated to a locally connected parallel computer than fully connected feedforward networks <ref> [14] </ref>. The Neocognitron [8], [9], [10], [11], which can be considered as the first realization of convolutional networks has been introduced by Fukuskima.
Reference: [15] <author> M. Lades, J.C. Vorbruggen, J. Buhmann, J. Lange, C. von der Malsburg, R.P. Wurtz, W. Konen, </author> <title> "Distortion Invariant Object Recognition in the Dynamic Link Architecture", </title> <journal> IEEE Trans. on Computers, </journal> <volume> vol. 42, </volume> <pages> pp. 300-311, </pages> <year> 1993 </year>
Reference-contexts: For this approach the Selforganizing Feature Map [1], Autoencoding network [6] and Eigenfaces [24] have been proposed for unsupervised training of the first hidden layer. Beside pure feedforward classifiers there exist also iterative approaches like dynamic link architecture <ref> [15] </ref>, which perform a graph matching between image and model, using simulated annealing for optimization. By this concept it is also possible to compensate for small deformations, but this method is quite slow due to the simulated annealing optimization necessary during classification.
Reference: [16] <author> Y. Le Cun, B. Boser, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard, L.D. Jackel, </author> <title> "Backpropagation Applied to Handwritten Zip Code Recognition", </title> <journal> Neural Computation, </journal> <volume> vol. 1, </volume> <pages> pp. 541-551, </pages> <year> 1989 </year>
Reference-contexts: Fukushima applied the Neocognitron primarily to handwritten digit recognition. Later variants of convolutional networks have been applied for example to large scale zip code recognition and face recognition <ref> [16] </ref>, [4]. Within a convolutional architecture there are several possibilities to combine different kinds of neurons and learning rules. One method is to use Mc-Culloch Pitts neurons, which calculate a weighted sum plus sigmoid nonlinearity, and to train the whole network by error backpropagation [27]. <p> One method is to use Mc-Culloch Pitts neurons, which calculate a weighted sum plus sigmoid nonlinearity, and to train the whole network by error backpropagation [27]. This approach has been applied for zip code recognition <ref> [16] </ref>. In contrast within the Neocognitron neurons calculate a weighted sum normalized by the incoming signal which results in a normalized convolution [10]. Furthermore weights are trained layer by layer independently by reinforcement and a winner takes all rule. <p> The Neoperceptron performs a convolution without normalization resulting in a simpler function which is equivalent to a weighted sum between inputvector (within the receptive field) and weightvector of the neuron. In the Neoperceptron a sigmoid function is used as nonlinearity. 3 The main difference to the concept used in <ref> [16] </ref> is based on a sequential learning strategy. u sl (n; k) = ( a l (-; ; k)u cl1 (n + -; )) (2) 2.3 Learning Rules In this approach the convolutional neural networks are trained layer by layer starting from the first hidden layer.
Reference: [17] <author> Y. LeCun, Y. Bengio, </author> <title> "Convolutional Networks for Images, Speech, and Time Series" in The Handbook of Brain Science and Neural Networks, </title> <publisher> MIT Press, M. Arbib ed., </publisher> <pages> pp. 255-258, </pages> <year> 1995 </year>
Reference-contexts: 1 Introduction Convolutional neural networks with local weightsharing topology gained considerable interest both in the field of speech and image analysis <ref> [17] </ref>. Their topology is more similar to biological networks based on receptive fields and improves tolerance to local distortions. Additionally the model complexity and the number of weights is efficiently reduced by weightsharing.
Reference: [18] <author> C. Neubauer, </author> <title> "Fast Detection and Classification of Defects on Treated Metal Surfaces using a Backpropagation Neural Network", </title> <booktitle> in Proc. of IJCNN, Singapore, </booktitle> <pages> pp. 1148-1153, </pages> <year> 1991 </year>
Reference-contexts: Additionally the model complexity and the number of weights is efficiently reduced by weightsharing. This is an advantage when images with highdimensional inputvectors are to be presented directly to the network instead of explicit feature extraction and data reduction which is usually applied before classification <ref> [18] </ref>, [20], [25]. Weightsharing can also be considered as an alternative to weight elimination [26] in order to reduce the number of weights [3]. Moreover networks with local topology can more effectively be migrated to a locally connected parallel computer than fully connected feedforward networks [14].
Reference: [19] <author> C. Neubauer, </author> <title> "Shape, Position and Size Invariant Visual Pattern Recognition Based on Principles of Neocognitron and Perceptron", in Artificial Neural Networks, </title> <editor> I. Alexander, J. Taylor, eds, North-Holland, </editor> <booktitle> Amsterdam, </booktitle> <volume> vol. 2, </volume> <pages> pp. 833-837, </pages> <year> 1992 </year>
Reference-contexts: This approach has been shown to be feasible for binarized images of digits but has not been verified on large data sets. In this work a third method is proposed which combines advantages of both approaches by using McCulloch Pitts neurons instead of more complicated neurons based on Neocognitron <ref> [19] </ref>. The network is trained layer by layer similarly to Neocognitron and thus time-consuming error backpropagation is avoided. This approach is called Neoperceptron in this context.
Reference: [20] <author> C. Neubauer, </author> <title> "Segmentation of Defects in Textile Fabric", </title> <booktitle> in Proc. of ICPR, Den Haag, </booktitle> <pages> pp. 688-691, </pages> <year> 1992 </year>
Reference-contexts: Additionally the model complexity and the number of weights is efficiently reduced by weightsharing. This is an advantage when images with highdimensional inputvectors are to be presented directly to the network instead of explicit feature extraction and data reduction which is usually applied before classification [18], <ref> [20] </ref>, [25]. Weightsharing can also be considered as an alternative to weight elimination [26] in order to reduce the number of weights [3]. Moreover networks with local topology can more effectively be migrated to a locally connected parallel computer than fully connected feedforward networks [14].
Reference: [21] <author> C. Neubauer, R. Hanke, </author> <title> "Improving X-Ray Inspection of Printed Circuit Boards by Integration of Neural Network Classifiers", </title> <booktitle> in Proc. of IEMT, </booktitle> <address> Santa Clara, </address> <pages> pp. 14-18, </pages> <year> 1993 </year>
Reference-contexts: Beside the applications discussed here convolutional networks will be integrated as classifiers for automatic x-ray inspection of solder joints in electronic production, where three-dimensional datasets have to be evaluated. Neural networks have already been successfully applied for defect detection and classification of solder joints <ref> [21] </ref> and convolutional networks combined with computer tomography will help to further improve quality control of Printed Circuit Boards.
Reference: [22] <author> C. Neubauer, </author> <title> "Modellierung visueller Erkennungsvorgange mit neuronalen Netzen", </title> <type> Ph.D. dissertation, </type> <institution> University Erlangen-Nurnberg, Dept. Technische Elektronik, </institution> <year> 1995 </year>
Reference-contexts: Thus one goal of this work is, to compare the Neocognitron with several fully connected networks and with the Neoperceptron. A comprehensive evaluation considering the influence of varying training sample size and other key parameters is subject of this paper (see also <ref> [22] </ref>). 1 2 Convolutional Neural Networks: Neocognitron and Neop- erceptron 2.1 Network Structure A detailed description of the Neocognitron architecture can be found in [8]. In Fig. 1 the topology of convolutional networks used in this work is shown.
Reference: [23] <author> D.Rumelhart, J. McClelland, </author> <title> "Parallel Distributed Processing: Exploration in the Mi-crostructure of Cognition", </title> <publisher> MIT-Press, </publisher> <year> 1986 </year>
Reference-contexts: For a problem like face recognition on the other hand it is difficult to train intermediate layers supervised since it is not known which higher level features are most important. In this case unsupervised algorithms for feature extraction are applied like principle component analysis or autoencoding learning [6], <ref> [23] </ref>.
Reference: [24] <author> M. Turk, A. Pentland, </author> <title> "Eigenfaces for Recognition", </title> <journal> Journal of Cognitive Neuroscience, </journal> <volume> vol. 3, </volume> <pages> pp. 71-86, </pages> <year> 1991 </year>
Reference-contexts: Here instead of overall supervised learning the hidden layer is first trained unsupervised and afterwards the output layer is trained supervised by least mean square error minimization. For this approach the Selforganizing Feature Map [1], Autoencoding network [6] and Eigenfaces <ref> [24] </ref> have been proposed for unsupervised training of the first hidden layer. Beside pure feedforward classifiers there exist also iterative approaches like dynamic link architecture [15], which perform a graph matching between image and model, using simulated annealing for optimization.
Reference: [25] <author> U. Schramm, T. Wagner, S. Schmolz, K. Spinnler, F. Bobel, R. Haas, H. Haken, </author> <title> "A Practical Comparison of Synergetic Computer, Restricted Coulomb Energy Networks and Multilayer Perceptron", </title> <booktitle> in Proc. WCNN 93, Portland, vol.3, </booktitle> <pages> pp. 657-660, </pages> <year> 1993 </year> <month> 16 </month>
Reference-contexts: Additionally the model complexity and the number of weights is efficiently reduced by weightsharing. This is an advantage when images with highdimensional inputvectors are to be presented directly to the network instead of explicit feature extraction and data reduction which is usually applied before classification [18], [20], <ref> [25] </ref>. Weightsharing can also be considered as an alternative to weight elimination [26] in order to reduce the number of weights [3]. Moreover networks with local topology can more effectively be migrated to a locally connected parallel computer than fully connected feedforward networks [14].
Reference: [26] <author> A.S. Weigend, D. Rumelhart, B. </author> <title> Huberman , "Generalization by Weight-Elimination with Application to Forecasting", </title> <booktitle> in Advances in Neural Information Processing, Mor-gan Kaufmann, </booktitle> <volume> vol. 3, </volume> <pages> pp. 875-882, </pages> <year> 1991 </year>
Reference-contexts: This is an advantage when images with highdimensional inputvectors are to be presented directly to the network instead of explicit feature extraction and data reduction which is usually applied before classification [18], [20], [25]. Weightsharing can also be considered as an alternative to weight elimination <ref> [26] </ref> in order to reduce the number of weights [3]. Moreover networks with local topology can more effectively be migrated to a locally connected parallel computer than fully connected feedforward networks [14].
Reference: [27] <author> P.J. Werbos, </author> <title> "Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences", </title> <type> in Ph.D. </type> <institution> Dept. of Applied Mathematics, Havard University, </institution> <address> Cambridge, </address> <year> 1974 </year>
Reference-contexts: Within a convolutional architecture there are several possibilities to combine different kinds of neurons and learning rules. One method is to use Mc-Culloch Pitts neurons, which calculate a weighted sum plus sigmoid nonlinearity, and to train the whole network by error backpropagation <ref> [27] </ref>. This approach has been applied for zip code recognition [16]. In contrast within the Neocognitron neurons calculate a weighted sum normalized by the incoming signal which results in a normalized convolution [10]. Furthermore weights are trained layer by layer independently by reinforcement and a winner takes all rule.
References-found: 27

