URL: ftp://ftp.inria.fr/INRIA/publication/publi-ps-gz/RR/RR-2355.ps.gz
Refering-URL: http://www.cs.umd.edu/~keleher/bib/dsmbiblio/node8.html
Root-URL: 
Title: Shared Virtual Memory and Message Passing Programming on a Finite Element Application  
Author: Rudolf Berrendorf and Michael Gerndt, Zakaria Lahjomri and Thierry Priol N 
Note: PROGRAMME 1  
Date: Mars 1995  
Affiliation: INSTITUT NATIONAL DE RECHERCHE EN INFORMATIQUE ET EN AUTOMATIQUE  
Abstract-found: 0
Intro-found: 1
Reference: [1] <institution> Technical Summary. Kendall Square Research, Waltham, Massachusetts, </institution> <address> edition, </address> <year> 1992. </year>
Reference-contexts: computation ... do iq=1,8 rld (ni) = rld (ni) + ... ... i = ... cmatrx (i,ni) = cmatrx (i,ni) + ... enddo enddo enddo C combine partial sums of matrix elements call dfmdscatter_add (sched1,cmatrx (nnp+1,1), + cmatrx (1,1),maxnp,jband) call dfscatter_add (sched1, rld (nnp+1), rld (1)) C$ann [DoShared (``BLOCK")] C$ann <ref> [VGlobal (DSUM, tmprld, 1, nnp)] </ref> do m=1,nel ... local computation ... do iq=1,8 tmprld (ni) = tmprld (ni) + ... do jq=1,8 tmprld (ni) = tmprld (ni) + ... i = ... <p> This guarantees that memory regions are accessed by the same processors thus reducing page conflicts. Affinity regions have the same aim for loop tiling over several loops. For a full description of the hardware and software details see <ref> [1] </ref>. 5.2 Parallelizing for the KSR As the general parallelization strategy for the KSR and KOAN/Fortran-S is similar we will describe only the relevant differences for the KSR compared to the work done for KOAN/Fortran-S.
Reference: [2] <author> Rudolf Berrendorf, Michael Gerndt, Zakaria Lahjomri, Thierry Priol, and Philippe d'Anfray. </author> <title> Evaluation of numerical applications running with shared virtual memory. </title> <type> Internal Report KFA-ZAM-IB-9315, </type> <institution> KFA Research Centre Juelich, </institution> <year> 1993. </year>
Reference-contexts: More results can be found in <ref> [2] </ref>. In some columns for the large data set, results are missing due to insufficient memory for all the data.
Reference: [3] <author> F. Bodin, L. Kervella, and T. Priol. Fortran-s: </author> <title> a fortran interface for shared virtual memory architectures. </title> <booktitle> In Supercomputing'93, </booktitle> <pages> pages 274-283, </pages> <publisher> IEEE, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: The Fortran-S code generator creates a process for each processor for the entire duration of the computation. There is no dynamic creation of processes during the execution. A description of Fortran-S is given in <ref> [3] </ref>. A parallelizer, called PARKA, can generate Fortran-S code from a sequential fortran-77 code. 4.2 Matrix Assembly This section outlines the parallelization of the matrix assembly using the KOAN/Fortran-S programming environment. A simplified version of the matrix assembly algorithm is shown in Fig. 3.
Reference: [4] <author> F. Darema-Rodgers, V.A. Norton, </author> <title> and G.F. Pfister. Using A Single-Program-Multiple-Data Computational Model for Parallel Execution of Scientific Applications. </title> <type> Technical Report RC11552, </type> <institution> IBM T.J Watson Research Center, </institution> <month> November </month> <year> 1985. </year>
Reference-contexts: Therefore no extension to the language syntax has been made. A set of annotations provides the user with a simple programming model based on shared array variables and parallel loops. One of the main features of Fortran-S is its SPMD (Single Program Multiple Data) execution model <ref> [4] </ref> that minimizes the overhead due to the management of parallel processes. The Fortran-S code generator creates a process for each processor for the entire duration of the computation. There is no dynamic creation of processes during the execution. A description of Fortran-S is given in [3].
Reference: [5] <author> R. Das and J. Saltz. </author> <title> A Manual for Parti Runtime Primitives revision 2. Internal Research Report, </title> <institution> ICASE, </institution> <year> 1992. </year>
Reference-contexts: Porting such applications on distributed memory parallel computers requires to handle arbitrary data distributions as the data accesses are unknown at compile time. The PARTI subroutine package developed at NASA/ICASE by J. Saltz et al. <ref> [5] </ref> has been designed for that purpose. It allows the computation of processor-local indices, the analysis of communication patterns and the communication of non-local array elements. By using PARTI, few modifications to the sequential application are necessary to get a parallel version. <p> RR n-2355 6 Rudolf Berrendorf and Michael Gerndt, Zakaria Lahjomri and Thierry Priol 3.2 PARTI Subroutines The implementation is based on a subroutine package developed at NASA/ICASE by Joel Saltz et al. called PARTI <ref> [5] </ref>. It supports arbitrary distributions of arrays, computation of processor-local indices, analysis of communication patterns, and communication of non-local array elements. Distributions are specified in each processor via a list of the global array indices assigned to the processor. Based on the distributions, communication patterns and local indices are computed.
Reference: [6] <author> Z. Lahjomri and T. Priol. Koan: </author> <title> a shared virtual memory for the ipsc/2 hypercube. </title> <booktitle> In CONPAR/VAPP92, </booktitle> <month> September </month> <year> 1992. </year> <note> RR n-2355 14 Rudolf Berrendorf and Michael Gerndt, Zakaria Lahjomri and Thierry Priol </note>
Reference-contexts: A SVM hides the physical local memories and provides to the user a virtual address space made of pages that move on demand among processors. Each local memory acts as large cache. This concept can be implemented within the operating system such as the KOAN SVM <ref> [6] </ref> or by specialized hardware devices as done in the KSR1 of Kendall Square Research. However, using an SVM will add additional overhead to the parallel execution caused by several factors like a distribution overhead, cache coherency, etc. <p> enddo axvec (j)=temp enddo C$ann [DoShared ("BLOCK")] do j=1, nnp temp=0. do i=1, eintrz temp=temp+cmatrx (i,j)*vec (gnojcn (i,j)) enddo axvec (j)=temp enddo RR n-2355 8 Rudolf Berrendorf and Michael Gerndt, Zakaria Lahjomri and Thierry Priol KOAN is a Shared Virtual Memory (SVM) embedded in the operating system of the iPSC/2 <ref> [6] </ref>. It provides to the user an abstraction from an underlying memory architecture [8]. It provides a virtual address space that is shared by a number of processes running on different processors of a distributed memory parallel computer (DMPC). <p> For this purpose, Fortran-S provides two synchronization mechanisms: critical section and atomic update. Critical section is not well suited for synchronizing the update since it is implemented with a distributed algorithm using message passing <ref> [6] </ref>. It is mainly targeted for synchronizing large grain computations which is not our case. Atomic update is an efficient synchronizing mechanism based on the locking of pages into the cache of each processor.
Reference: [7] <author> Leslie Lamport. </author> <title> How to make a multiprocessor computer that correctly executes mul-tiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(6):313-348, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: Fig. 4 shows the parallel version of the matrix vector multiply. 5 Shared Virtual Memory on the KSR 5.1 Overview of the KSR The KSR1 of Kendall Square Research is a parallel machine with hardware-embedded SVM, called ALLCACHE TM , implementing sequential consistency <ref> [7] </ref>. The unit of coherence is a subpage of size 128 bytes. Parallel constructs are available to the user on several levels. On the most basic level are POSIX pthreads. On the next higher level are PRESTO routines, which dynamically evaluate runtime decisions to improve performance.
Reference: [8] <author> Kai Li. </author> <title> Shared Virtual Memory on Loosely Coupled Multiprocessors. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <month> September </month> <year> 1986. </year>
Reference-contexts: It provides to the user an abstraction from an underlying memory architecture <ref> [8] </ref>. It provides a virtual address space that is shared by a number of processes running on different processors of a distributed memory parallel computer (DMPC).
Reference: [9] <author> H. Vereecken, G. Lindenmayr, A. Kuhr, D. H. Welte, and A. Basermann. </author> <title> Numerical Modelling of Field Scale Transport in Heterogeneous Variably Saturated Porous Media. Internal Report KFA/ICG-4 No. </title> <type> 500393, </type> <institution> Forschungszentrum Julich, </institution> <year> 1993. </year>
Reference-contexts: Section 6 discusses several issues for the two programming models. We conclude in section 7. 2 ParFEM: A Finite Element Application The ParFEM application has been developed by Harry Vereecken et al. (Institute for Petrol and Organic Geochemistry) <ref> [9] </ref> at KFA. This application models transport and chemical processes in heterogeneous 3D porous media. Accounting for the heterogeneity of the porous medium results in grid size of more than 10 6 nodal points.

References-found: 9

