URL: http://www.cs.ucl.ac.uk/staff/a.hunter/auf2.ps
Refering-URL: http://www.cs.ucl.ac.uk/staff/A.Hunter/papers.html
Root-URL: http://www.cs.ucl.ac.uk
Phone: 2  
Title: A Review of Uncertainty Handling Formalisms  
Author: Simon Parsons and Anthony Hunter 
Address: London, London, E1 4NS, United Kingdom.  Gower Street, London, WC1E 6BT, United Kingdom.  
Affiliation: 1 Department of Electronic Engineering, Queen Mary and Westfield College, University of  Department of Computer Science, University College London,  
Abstract: Many different formal techniques, both numerical and symbolic, have been developed over the past two decades for dealing with incomplete and uncertain information. In this paper we review some of the most important of these formalisms, describing how they work, and in what ways they differ from one another. We also consider heterogeneous approaches which incorporate two or more approximate reasoning mechanisms within a single reasoning system. These have been proposed to address limitations in the use of individual formalisms. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> S. K. Andersen, F. V. Jensen, K. G. Olesen, and F. Jensen. </author> <title> HUGIN|a shell for building Bayesian belief universes for expert systems. </title> <booktitle> In Proceedings of the 11th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 783-791, </pages> <address> San Mateo, CA, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Another network-based method that has received wide attention is that of Lauritzen and Spiegel-halter [52] which has been used as the basis of the expert system shell HUGIN <ref> [1] </ref>, and the paper in this volume by Magni et al. [56] makes use of a graphical representation similar to that discussed above. 2.3 Evidence theory Evidence theory is the term commonly used to refer to the body of work carried out by Arthur Dempster [17, 18] and Glenn Shafer [82] <p> The basis of the measure of uncertainty is a probability mass function m () that assigns zero mass to the empty set, m (;) = 0, and a value in <ref> [0; 1] </ref> to each element of 2 fi , the total mass distributed being 1 so that: X m (A) = 1 Since we deal with all possible subsets of the set of all base propositions, rather than the propositions themselves as in probability theory, we can apportion the probability mass <p> The certainty factor approach assigns a numerical weight, the certainty factor, to the consequent of every If hevidencei then hhypothesis i rule in a rule-based system. The value of the certainty factor, which lies in the interval <ref> [-1, 1] </ref>, is assessed by the domain expert from the degree, between 0 and 1, to which a given piece of evidence causes her belief and disbelief in the hypothesis to be increased.
Reference: 2. <author> J. A. Barnett. </author> <title> Computational methods for a mathematical theory of evidence. </title> <booktitle> In Proceedings of the 7th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 868-875, </pages> <address> Los Altos, CA, 1981. </address> <publisher> William Kaufmann. </publisher>
Reference-contexts: The problems of the computational complexity of Dempster's rule have been discussed by several authors. Barnett <ref> [2] </ref> showed that the apparent exponential time requirement of the theory could be reduced to simple polynomial time if the theory was applied to single hypotheses, rather than sets of hypotheses, and the evidence combined in an orderly fashion.
Reference: 3. <author> S. Benferhat, D. Dubois, and H. Prade. </author> <title> Argumentative inference in uncertain and inconsistent knowledge bases. </title> <booktitle> In Proceedings of the 9th Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 411-419. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: As an example, suppose all maximally consistent subsets of imply OE, and so all arguments for OE are relatively safe, yet a more preferred conclusion is a formula that follows from the intersection of the maximally consistent subsets of . This approach to argumentation has been developed in <ref> [3, 29] </ref>. A number of other approaches to argumentation, including [70, 72, 97], focus on default reasoning by incorporating default connectives (which can be used to build up default statements similar to the default rules in default logic) into their languages together with associated machinery.
Reference: 4. <author> S. Benferhat and L. Garcia. </author> <title> A local handling of inconsistent knowledge and default bases. </title> <editor> In A. Hunter and S. Parsons, editors, </editor> <booktitle> Applications of Uncertainty Formalisms (this volume). </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1998. </year>
Reference-contexts: A number of different implementations are given in [31], and a particular approach to assumption-based reasoning is described by Haenni [38]. In addition, more sophisticated truth maintenance systems will emerge from advanced theoretical frameworks such as that described by Benferhat and Garcia <ref> [4] </ref>.
Reference: 5. <author> J. Bigham. </author> <title> Computing beliefs according to Dempster-Shafer and possibilistic logic. </title> <booktitle> In Proceedings of the 3rd International Conference on Information Processing and Management of Uncertainty, Paris, </booktitle> <pages> pages 59-61, </pages> <year> 1990. </year>
Reference-contexts: This is commonly done by using a logical technique to establish a set of possible hypotheses from a larger initial set of exhaustive hypotheses, and then using a numerical techniques to rank the plausible set. Typical of such systems are those of Provan [73], Bigham <ref> [5] </ref> and Laskey and Lehner [51]. In all three of these systems, the semantic equivalence of the ATMS [15] and the Dempster-Shafer method, proved by Provan, is exploited ensuring that no information is lost in the initial round of inference.
Reference: 6. <author> J. Bigham. </author> <title> Exploiting uncertain and temporal information in correlation. </title> <editor> In A. Hunter and S. Parsons, editors, </editor> <booktitle> Applications of Uncertainty Formalisms (this volume). </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1998. </year>
Reference-contexts: This combination of possibility theory and logic into possibilistic logic has been investigated at length by Dubois and Prade [23, 24]. Possibilistic logic is one of the techniques explored by Bigham in his paper in this volume <ref> [6] </ref>, and both the contributions of Ramalho [75] and Saffiotti [79] make use of fuzzy inference of the kind discussed above, while Bosc et al. [7] consider the application of fuzzy techniques to databases. 2.5 Other approaches There are a number of other numerical techniques which, although we do not have <p> Bigham <ref> [6] </ref> has extended this work by adapting the possibilistic ATMS to take account of temporal information. Furthermore, the possibilistic ATMS allows inconsistent knowledge bases to be revised using the principles of epistemic entrenchment [34].
Reference: 7. <author> P. Bosc, L. Lietard, and H. Prade. </author> <title> An ordinal approach to the processing of fuzzy queries with flexible quantifiers. </title> <editor> In A. Hunter and S. Parsons, editors, </editor> <booktitle> Applications of Uncertainty Formalisms (this volume). </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1998. </year>
Reference-contexts: Possibilistic logic is one of the techniques explored by Bigham in his paper in this volume [6], and both the contributions of Ramalho [75] and Saffiotti [79] make use of fuzzy inference of the kind discussed above, while Bosc et al. <ref> [7] </ref> consider the application of fuzzy techniques to databases. 2.5 Other approaches There are a number of other numerical techniques which, although we do not have space to consider them in any detail, are worth mentioning for their particular historical or theoretical interest.
Reference: 8. <author> F. Brazier, J. Engelfreit, and J. Truer. </author> <title> Analysis of multi-interpretable ecological monitoring information. </title> <editor> In A. Hunter and S. Parsons, editors, </editor> <booktitle> Applications of Uncertainty Formalisms (this volume). </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1998. </year> <title> 5 For a more detailed survey, </title> <note> see [43, 48, 64]. </note>
Reference-contexts: However this situation is beginning to change. This volume includes a paper by Nicolas and Schaub [62] which describes a system on which to build default logic applications, while Brazier et al. <ref> [8] </ref> have applied default logic to a problem from ecology. 3.3 Argumentation Argumentation is the process by which arguments are constructed and compared.
Reference: 9. <author> T. Chard. </author> <title> Qualitative probability versus quantitative probability in clinical diag-nosis: a study using a computer simulation. Medical Decision Making, </title> <booktitle> 11 </booktitle> <pages> 38-41, </pages> <year> 1991. </year>
Reference-contexts: then, as though there is no clear cut winner in this argument; the moral appears to be: "if you can obtain the numbers to your satisfaction, then use them." As a final word, it is worth mentioning that it has also been convincingly argued in several places (see for example <ref> [9, 71] </ref>) that even if the numbers are available, they make little difference to the business of weighing up the evidence. This, however, is a different argument altogether, and we will say no more about it.
Reference: 10. <author> P. Cheeseman. </author> <title> Probabilistic vs. fuzzy reasoning. </title> <editor> In L. N. Kanal and J. F. Lem-mer, editors, </editor> <booktitle> Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 85-102. </pages> <publisher> Elsevier Science Publishers, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1986. </year>
Reference: 11. <author> P. Cheeseman. </author> <title> Discussion of the paper by Lauritzen and Spiegelhalter. </title> <journal> Journal of the Royal Statistical Society, B, </journal> <volume> 50:203, </volume> <year> 1988. </year>
Reference-contexts: and possibility theory . . . (this) may be because the idea does not improve on any of the difficult methodological problems that beset probability theory, such as the assessment of prior probabilities" he is restating an argument that has been made time and again, perhaps most tersely by Cheeseman <ref> [11] </ref> who asked: "where are all the numbers coming from?" Obtaining the "numbers", be they probabilities, possibilities, or mass distributions does seem to be a major problem. Clearly, without good numerical assessments sophisticated computational mechanisms are of little value.
Reference: 12. <author> P. Cheeseman. </author> <title> An inquiry into computer understanding. </title> <journal> Computational Intelligence, </journal> <volume> 4 </volume> <pages> 58-142, </pages> <year> 1988. </year>
Reference: 13. <author> D. A. Clark. </author> <title> Numerical and symbolic approaches to uncertainty management in AI. </title> <journal> Artificial Intelligence Review, </journal> <volume> 4 </volume> <pages> 109-146, </pages> <year> 1990. </year>
Reference: 14. <author> P. R. Cohen. </author> <title> Heuristic Reasoning about Uncertainty: An Artificial Intelligence Approach. </title> <publisher> Pitman, </publisher> <address> London, UK, </address> <year> 1985. </year>
Reference-contexts: Applying Dempster's rule. what is going on here we will consider a simple example of the use of Dempster's rule in combining evidence. Consider a world <ref> [14] </ref> with only four car manufacturers, Nissan, Toyota, GM and Chrysler, all trying to break into a new car market. We are interested in who will dominate the market. There are four singleton hypotheses corresponding to the assertions that each of the four manufacturers will dominate the market. <p> The first is perhaps the simplest. When Cohen <ref> [14] </ref> criticises possibility theory saying: "relatively little has been made of the idea of fuzzy sets and possibility theory . . . (this) may be because the idea does not improve on any of the difficult methodological problems that beset probability theory, such as the assessment of prior probabilities" he is
Reference: 15. <author> J. de Kleer. </author> <title> An assumption-based TMS. </title> <journal> Artificial Intelligence, </journal> <volume> 28 </volume> <pages> 127-162, </pages> <year> 1986. </year>
Reference-contexts: A truth maintenance system (TMS) records information about each inference that is generated from a set of assumptions. The two main types of truth maintenance system are the justification-based truth maintenance system (JTMS) [20] and the assumption-based truth maintenance system (ATMS) <ref> [15] </ref>. A JTMS records a single set of consistent facts and all the inference which may be proved form them. When an inconsistency is detected some external system (which may be the user) is invoked to resolve the inconsistency and the JTMS then retracts the necessary inferences. <p> Typical of such systems are those of Provan [73], Bigham [5] and Laskey and Lehner [51]. In all three of these systems, the semantic equivalence of the ATMS <ref> [15] </ref> and the Dempster-Shafer method, proved by Provan, is exploited ensuring that no information is lost in the initial round of inference.
Reference: 16. <author> J. de Kleer and B. C. Williams. </author> <title> Diagnosing multiple faults. </title> <journal> Artificial Intelligence, </journal> <volume> 32 </volume> <pages> 97-130, </pages> <year> 1987. </year>
Reference-contexts: Bigham's system is particularly interesting in that it includes an extension of the clause based approach of McAllester's logic-based truth maintenance system (LTMS) [59] as a symbolic inference engine, and also permits beliefs based on possibility theory to be propagated. A similar system is de Kleer and Williams' <ref> [16] </ref> GDE for fault diagnosis.
Reference: 17. <author> A. P. Dempster. </author> <title> Upper and lower probabilities induced by a multi-valued mapping. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 38 </volume> <pages> 325-339, </pages> <year> 1967. </year>
Reference-contexts: the expert system shell HUGIN [1], and the paper in this volume by Magni et al. [56] makes use of a graphical representation similar to that discussed above. 2.3 Evidence theory Evidence theory is the term commonly used to refer to the body of work carried out by Arthur Dempster <ref> [17, 18] </ref> and Glenn Shafer [82] to remedy some of what 1 Singly-connected networks are those in which for every pair of nodes there is at most one path along arcs which joins them.
Reference: 18. <author> A. P. Dempster. </author> <title> A generalisation of Bayesian inference (with discussion). </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 30 </volume> <pages> 205-232, </pages> <year> 1968. </year>
Reference-contexts: the expert system shell HUGIN [1], and the paper in this volume by Magni et al. [56] makes use of a graphical representation similar to that discussed above. 2.3 Evidence theory Evidence theory is the term commonly used to refer to the body of work carried out by Arthur Dempster <ref> [17, 18] </ref> and Glenn Shafer [82] to remedy some of what 1 Singly-connected networks are those in which for every pair of nodes there is at most one path along arcs which joins them.
Reference: 19. <author> A. P. Dempster. </author> <title> Comments on `An inquiry into computer understanding' by Peter Cheeseman. </title> <journal> Computational Intelligence, </journal> <volume> 4 </volume> <pages> 72-73, </pages> <year> 1988. </year>
Reference-contexts: When assessing connectedness, arcs may be traversed both directions, but any arc may only be traversed once. they saw as the limitations of probability theory, in particular <ref> [19] </ref> disposing with the "completeness" axiom of probability theory [42].
Reference: 20. <author> J. Doyle. </author> <title> A truth maintenance system. </title> <journal> Artificial Intelligence, </journal> <volume> 12 </volume> <pages> 231-272, </pages> <year> 1979. </year>
Reference-contexts: A truth maintenance system (TMS) records information about each inference that is generated from a set of assumptions. The two main types of truth maintenance system are the justification-based truth maintenance system (JTMS) <ref> [20] </ref> and the assumption-based truth maintenance system (ATMS) [15]. A JTMS records a single set of consistent facts and all the inference which may be proved form them.
Reference: 21. <author> D. Driankov. </author> <title> A calculus for belief-intervals representation of uncertainty. </title> <editor> In B. Bouchon-Meunier and R. R. Yager, editors, </editor> <booktitle> Uncertainty in Knowledge-Based Systems, </booktitle> <pages> pages 205-216. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany, </address> <year> 1986. </year>
Reference-contexts: Thus the belief originally attributed to A is transfered to that part of A not eliminated by the new evidence, thus giving the system its name. Another interesting proposal is due to Driankov <ref> [21] </ref>.
Reference: 22. <author> D. Dubois, J. Lang, and H. Prade. </author> <title> A possibilistic assumption-based truth maintenance system with uncertain justifications, and its application to belief revision. </title> <editor> In J. P. Martins and M. Reinfrank, editors, </editor> <booktitle> Truth Maintenance Systems, </booktitle> <pages> pages 87-106, </pages> <address> Berlin, Germany, 1990. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: It is also possible to use possibility measures with an assumption-based truth maintenance system instead of belief functions or probabilities. This is exactly the course followed by Dubois, Lang and Prade in their possibilistic ATMS <ref> [22] </ref>.
Reference: 23. <author> D. Dubois, J. Lang, and H. Prade. </author> <title> Fuzzy sets in approximate reasoning, Part 2: Logical approaches. </title> <journal> Fuzzy Sets and Systems, </journal> <volume> 40 </volume> <pages> 203-244, </pages> <year> 1991. </year>
Reference-contexts: This calculation decreases the possibility that Y is B fl is true, the further A fl is from A. This combination of possibility theory and logic into possibilistic logic has been investigated at length by Dubois and Prade <ref> [23, 24] </ref>.
Reference: 24. <author> D. Dubois and H. Prade. </author> <title> Necessity measures and the resolution principle. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 17 </volume> <pages> 474-478, </pages> <year> 1987. </year>
Reference-contexts: This calculation decreases the possibility that Y is B fl is true, the further A fl is from A. This combination of possibility theory and logic into possibilistic logic has been investigated at length by Dubois and Prade <ref> [23, 24] </ref>.
Reference: 25. <author> D. Dubois and H. Prade. </author> <title> Modelling uncertainty and inductive inference: a survey of recent non-additive probability systems. </title> <journal> Acta Psychologica, </journal> <volume> 68 </volume> <pages> 53-78, </pages> <year> 1988. </year>
Reference-contexts: Dubois and Prade <ref> [25] </ref> point out that a weak theoretical connection exists since for all A, N (A) Pr (A) (A) where N (A) is the necessity of A, defined by: N (A) = 1 (:A) It is possible to extend these ideas to possibility distributions that depend on more than one attribute, and
Reference: 26. <author> D. Dubois and H. Prade. </author> <title> Possibility Theory: An Approach to Computerized Processing of Uncertainty. </title> <publisher> Plenum Press, </publisher> <address> New York, NY, </address> <year> 1988. </year>
Reference-contexts: Zadeh stresses the fact that possibility and probability are different concepts with the 2 The use of maximum and minimum is not compulsory. For further discussion of this point, see <ref> [26] </ref>. example of Hans' breakfast. Consider the statement "Hans ate X eggs for break-fast" with X 2 f1; : : : ; 8g.
Reference: 27. <author> D. Dubois and H. Prade. </author> <title> Processing of imprecision and uncertainty in expert system reasoning models. </title> <editor> In C. Ernst, editor, </editor> <booktitle> Management Expert Systems, </booktitle> <pages> pages 67-88. </pages> <publisher> Addison Wesley, </publisher> <year> 1988. </year>
Reference-contexts: Possibility has been applied to reasoning with vague statements <ref> [27, 104] </ref>. For example, suppose we have the following statement. If the clothes are dirty then wash them in hot water Both the concepts "dirty" and "hot" are vague or fuzzy in this context.
Reference: 28. <author> R. O. Duda, P. E. Hart, and N. J. Nilsson. </author> <title> Subjective Bayesian methods for a rule-based inference system. </title> <booktitle> In Proceedings of the National Computer Conference, </booktitle> <pages> pages 1075-1082, </pages> <year> 1976. </year>
Reference: 29. <author> M. Elvang-Goransson and A. Hunter. </author> <title> Argumentative logics: Reasoning from classically inconsistent information. </title> <journal> Data and Knowledge Engineering Journal, </journal> <volume> 16 </volume> <pages> 125-145, </pages> <year> 1995. </year>
Reference-contexts: As an example, suppose all maximally consistent subsets of imply OE, and so all arguments for OE are relatively safe, yet a more preferred conclusion is a formula that follows from the intersection of the maximally consistent subsets of . This approach to argumentation has been developed in <ref> [3, 29] </ref>. A number of other approaches to argumentation, including [70, 72, 97], focus on default reasoning by incorporating default connectives (which can be used to build up default statements similar to the default rules in default logic) into their languages together with associated machinery.
Reference: 30. <author> D. W. Etherington. </author> <title> Reasoning with Incomplete Information. </title> <publisher> Pitman, </publisher> <address> London, UK, </address> <year> 1988. </year>
Reference-contexts: (~x) and to replace the semi-normal default: ff (~x) : fi (~x) ^ fl (~x) by the normal default: ff (~x) : fi (~x) ^ fl (~x) The first is non-controversial, but the second, despite being applicable for a large range of practically occurring defaults, has some rather alarming exceptions <ref> [30] </ref>.
Reference: 31. <author> K. D. Forbus and J. de Kleer. </author> <title> Building Problem Solvers. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: The question of computational viability is then dependent upon the balance between on inferencing (consistency-checking and theorem proving) versus storage requirements (consistent subsets of data and inferential interdependencies). The aim of a TMS is to find the most parsimonious choice. A number of different implementations are given in <ref> [31] </ref>, and a particular approach to assumption-based reasoning is described by Haenni [38]. In addition, more sophisticated truth maintenance systems will emerge from advanced theoretical frameworks such as that described by Benferhat and Garcia [4].
Reference: 32. <author> J. Fox. </author> <title> Three arguments for extending the framework of probability. </title> <editor> In L. N. Kanal and J. F. Lemmer, editors, </editor> <booktitle> Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 447-458. </pages> <publisher> Elsevier Science Publishers, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1986. </year>
Reference-contexts: This has been used by many (see for example <ref> [32] </ref>) to argue against the use of probability theory for dealing with uncertainty.
Reference: 33. <author> J. Fox and S. Parsons. </author> <title> Arguing about beliefs and actions. </title> <editor> In A. Hunter and S. Parsons, editors, </editor> <booktitle> Applications of Uncertainty Formalisms (this volume). </booktitle> <publisher> Springer Ver-lag, </publisher> <address> Berlin, </address> <year> 1998. </year>
Reference-contexts: This approach is described in more detail in this volume <ref> [33] </ref>, and elsewhere [46], and its historical development is charted in [65]. It also forms the basis for one of the applications case studies in this book [47]. 3.4 Truth maintenance systems When reasoning with inconsistent information, questions of belief in assumptions and belief in conclusions arise.
Reference: 34. <author> P. Gardenfors. </author> <title> Knowledge in flux: Modelling the Dynamics of Epistemic States. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference-contexts: Bigham [6] has extended this work by adapting the possibilistic ATMS to take account of temporal information. Furthermore, the possibilistic ATMS allows inconsistent knowledge bases to be revised using the principles of epistemic entrenchment <ref> [34] </ref>. Another set of interesting developments which bridge the gap between symbolic and numerical techniques is the discovery of relationships between default logic and evidence theory. Wilson [99] considers the similarities between belief functions and default logic. He shows that, despite their initial dissimilarities they are, in fact, closely related.
Reference: 35. <author> J. Gebhardt and R. Kruse. </author> <title> Background to and perspectives on possibilistic graphical models. </title> <editor> In A. Hunter and S. Parsons, editors, </editor> <booktitle> Applications of Uncertainty Formalisms (this volume). </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1998. </year>
Reference-contexts: Explanations of these concepts will be omitted in the interests of saving space (but see [103]), but it should be noted that the kind of graphical structures discussed above in relation to probability theory can be adapted for use with possibility theory as well <ref> [35] </ref>. Possibility has been applied to reasoning with vague statements [27, 104]. For example, suppose we have the following statement. If the clothes are dirty then wash them in hot water Both the concepts "dirty" and "hot" are vague or fuzzy in this context.
Reference: 36. <author> M. Ginsberg. </author> <title> Non-monotonic reasoning using Dempster's rule. </title> <booktitle> In Proceedings of the 4th National Conference on Artificial Intelligence, </booktitle> <pages> pages 112-119, </pages> <address> Los Altos, CA, 1984. </address> <publisher> William Kaufmann. </publisher>
Reference-contexts: Both of these papers can be seen as an extension of the work of Rich [77] and Ginsberg <ref> [36] </ref>, who considered ways of applying numerical certainty measures to logical inference rules. It is also possible to use argumentation to combine symbolic and numerical reasoning. For instance, Fox and Krause [49] discuss a simple inference mechanism, based on argumentation, which is suitable for joint symbolic and numerical reasoning.
Reference: 37. <author> J. Gordon and E. H. Shortliffe. </author> <title> A method for managing evidential reasoning in a hierarchical hypothesis space. </title> <journal> Artificial Intelligence, </journal> <volume> 26 </volume> <pages> 323-357, </pages> <year> 1985. </year>
Reference-contexts: Barnett [2] showed that the apparent exponential time requirement of the theory could be reduced to simple polynomial time if the theory was applied to single hypotheses, rather than sets of hypotheses, and the evidence combined in an orderly fashion. Gordon and Shortliffe <ref> [37] </ref> extended Barnett's approach to compute approximate beliefs in a space of hierarchically organised sets of hypotheses in linear time.
Reference: 38. <author> R. Haenni. </author> <title> Modelling uncertainty in propositional assumption-based systems. </title> <editor> In A. Hunter and S. Parsons, editors, </editor> <booktitle> Applications of Uncertainty Formalisms (this volume). </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1998. </year>
Reference-contexts: The aim of a TMS is to find the most parsimonious choice. A number of different implementations are given in [31], and a particular approach to assumption-based reasoning is described by Haenni <ref> [38] </ref>. In addition, more sophisticated truth maintenance systems will emerge from advanced theoretical frameworks such as that described by Benferhat and Garcia [4].
Reference: 39. <author> D. Heckerman and M. Wellman. </author> <title> Bayesian networks. </title> <journal> Communications of the ACM, </journal> <volume> 38 </volume> <pages> 27-30, </pages> <year> 1995. </year>
Reference: 40. <author> D. E. Heckerman. </author> <title> Probability interpretation for MYCIN's certainty factors. </title> <editor> In L. N. Kanal and J. F. Lemmer, editors, </editor> <booktitle> Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 167-196. </pages> <publisher> Elsevier Science Publishers, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1986. </year>
Reference-contexts: Recently several people have challenged the validity of the certainty factor model. For instance, Heckerman <ref> [40] </ref> has shown that the original definition of the model is flawed since the belief in a hypothesis given two pieces of evidence will depend upon the order in which the effect of the pieces of evidence is computed.
Reference: 41. <author> M. Henrion, G. Provan, B. Del Favero, and G. Sanders. </author> <title> An experimental comparison of numerical and qualitative probabilistic reasoning. </title> <booktitle> In Proceedings of the 10th Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 319-326, </pages> <address> San Francisco, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: 42. <author> E. J. Horvitz, D. E. Heckerman, and C. P. Langlotz. </author> <title> A framework for comparing alternative formalisms for plausible reasoning. </title> <booktitle> In Proceedings of the 5th National Conference on Artificial Intelligence, </booktitle> <pages> pages 210-214, </pages> <address> Los Altos, CA, 1986. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: When assessing connectedness, arcs may be traversed both directions, but any arc may only be traversed once. they saw as the limitations of probability theory, in particular [19] disposing with the "completeness" axiom of probability theory <ref> [42] </ref>. The theory deals with the so-called frame of discernment, the set of base elements fi = f 1 ; :::; n g in which we are interested, and its power set 2 fi , which is the set of all subsets of the interesting elements.
Reference: 43. <author> A. Hunter. </author> <title> Uncertainty in Information Systems. </title> <publisher> McGraw-Hill, </publisher> <address> London, UK, </address> <year> 1996. </year>
Reference: 44. <author> D. J. Israel. </author> <title> Some remarks on the place of logic in knowledge representation. </title> <editor> In N. Cercone and G. McCalla, editors, </editor> <booktitle> The Knowledge Frontier: Essays in the Representation of Knowledge, </booktitle> <pages> pages 80-91. </pages> <publisher> Springer Verlag, </publisher> <address> New York, NY, </address> <year> 1987. </year>
Reference-contexts: This is unfortunately a naive impression, and there are many problems that beset the use of classical logic, especially when attempting to model the kind of "commonsense" reasoning which human beings excel at. Israel <ref> [44] </ref> credits Minsky with being the first to consider the matter, pointing out that there are two particular properties of first order logic that are at odds with commonsense human behaviour. The first results in the so called qualification problem.
Reference: 45. <author> G. J. Klir. </author> <title> Where do we stand on measures of uncertainty, ambiguity, </title> <journal> fuzziness, and the like? Fuzzy Sets and Systems, </journal> <volume> 24 </volume> <pages> 141-160, </pages> <year> 1987. </year>
Reference: 46. <author> P. Krause, S. Ambler, M. Elvang-Goransson, and J. Fox. </author> <title> A logic of argumentation for reasoning under uncertainty. </title> <journal> Computational Intelligence, </journal> <volume> 11 </volume> <pages> 113-131, </pages> <year> 1995. </year>
Reference-contexts: This approach is described in more detail in this volume [33], and elsewhere <ref> [46] </ref>, and its historical development is charted in [65]. It also forms the basis for one of the applications case studies in this book [47]. 3.4 Truth maintenance systems When reasoning with inconsistent information, questions of belief in assumptions and belief in conclusions arise.
Reference: 47. <author> P. Krause, J. Fox, P. Judson, and M. Patel. </author> <title> Qualitative risk assessment fulfills a need. </title> <editor> In A. Hunter and S. Parsons, editors, </editor> <booktitle> Applications of Uncertainty Formalisms (this volume). </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1998. </year>
Reference-contexts: This approach is described in more detail in this volume [33], and elsewhere [46], and its historical development is charted in [65]. It also forms the basis for one of the applications case studies in this book <ref> [47] </ref>. 3.4 Truth maintenance systems When reasoning with inconsistent information, questions of belief in assumptions and belief in conclusions arise. These questions include [58]: Inferences from beliefs. How do new beliefs follow from existing beliefs? Default beliefs.
Reference: 48. <author> P. J. Krause and D. A. Clark. </author> <title> Representing Uncertain Knowledge: An Artificial Intelligence Approach. </title> <publisher> Intellect, Oxford, </publisher> <address> UK, </address> <year> 1993. </year>
Reference: 49. <author> P. J. Krause and J. Fox. </author> <title> Combining symbolic and numerical methods for reasoning under uncertainty. </title> <editor> In D. J. Hand, editor, </editor> <booktitle> AI and Computer Power; The Impact on Statistics, </booktitle> <pages> pages 99-114. </pages> <publisher> Chapman and Hall, </publisher> <address> London, UK, </address> <year> 1994. </year>
Reference-contexts: Both of these papers can be seen as an extension of the work of Rich [77] and Ginsberg [36], who considered ways of applying numerical certainty measures to logical inference rules. It is also possible to use argumentation to combine symbolic and numerical reasoning. For instance, Fox and Krause <ref> [49] </ref> discuss a simple inference mechanism, based on argumentation, which is suitable for joint symbolic and numerical reasoning. Nonmonotonic reasoning about Tweety's ability to fly is handled in the following way.
Reference: 50. <author> M. Lalmas. </author> <title> Modelling information retrieval with Dempster-Shafer's theory of evidence. </title> <editor> In A. Hunter and S. Parsons, editors, </editor> <booktitle> Applications of Uncertainty Formalisms (this volume). </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1998. </year>
Reference-contexts: More recent advances are explored in [61, 101]. The application of evidence theory is the subject of three papers in this volume. Lalmas <ref> [50] </ref> uses it as a means of deciding which document to retrieve, van Dam [96] uses it to control a radio communication system, and Duncan Wilson [98] considers how to apply it to the classification of faults in automated inspection. 2.4 Possibility theory A formal theory of possibilities, based on the
Reference: 51. <author> K. B. Laskey and P. E. Lehner. </author> <title> Assumptions, beliefs and probabilities. </title> <journal> Artificial Intelligence, </journal> <volume> 32 </volume> <pages> 65-77, </pages> <year> 1990. </year>
Reference-contexts: Typical of such systems are those of Provan [73], Bigham [5] and Laskey and Lehner <ref> [51] </ref>. In all three of these systems, the semantic equivalence of the ATMS [15] and the Dempster-Shafer method, proved by Provan, is exploited ensuring that no information is lost in the initial round of inference.
Reference: 52. <author> S. L. Lauritzen and D. J. Spiegelhalter. </author> <title> Local computations on graphical structures, and their application to expert systems. </title> <journal> Journal of the Royal Statistical Society, B, </journal> <volume> 50 </volume> <pages> 157-224, </pages> <year> 1988. </year>
Reference-contexts: introduction to the use of probabilistic causal networks, along with an efficient scheme for the propagation of probabilities in singly-connected networks 1 between every that is based on autonomous message passing. Another network-based method that has received wide attention is that of Lauritzen and Spiegel-halter <ref> [52] </ref> which has been used as the basis of the expert system shell HUGIN [1], and the paper in this volume by Magni et al. [56] makes use of a graphical representation similar to that discussed above. 2.3 Evidence theory Evidence theory is the term commonly used to refer to the
Reference: 53. <author> D. V. Lindley. </author> <title> Making Decisions. </title> <publisher> John Wiley & Sons, </publisher> <address> Chichester, UK, </address> <year> 1975. </year>
Reference: 54. <author> W. Lukaszewicz. </author> <title> Two results on default logic. </title> <booktitle> In Proceedings of the 9th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 459-461, </pages> <address> Los Altos, CA, 1985. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Thus default rules alone do not generate inconsistent extensions. Many authors have worked on default logic in the years since it was first introduced. One those whose work is worth considering is Lukaszewicz, who proposed two important extensions to the original formulation. The first of these <ref> [54] </ref> takes the form of translations between different types of default, in particular to replace the general default: ff (~x) : fi (~x) by the semi-normal default: ff (~x) : fi (~x) ^ fl (~x) and to replace the semi-normal default: ff (~x) : fi (~x) ^ fl (~x) by the
Reference: 55. <author> W. Lukaszewicz. </author> <title> Considerations on default logic: an alternative approach. </title> <journal> Computational Intelligence, </journal> <volume> 4 </volume> <pages> 1-16, </pages> <year> 1988. </year>
Reference-contexts: By using both translations sequentially, we can replace the eminently sensible: has motive (x ) : guilty (x ) suspect (x ) by the rather unreasonable: has motive (x ) : suspect (x ) ^ guilty (x ) suspect (x ) ^ guilty (x ) In a further paper, Lukaszewicz <ref> [55] </ref> generalises default logic, providing an alternative formalisation of an extension, and proving that semi-normal default theories are guaranteed such extensions. He also shows that semi-normal default theories are semi-monotonic, that is monotonic with respect to default rules. <p> Wilson [99] considers the similarities between belief functions and default logic. He shows that, despite their initial dissimilarities they are, in fact, closely related. Indeed, in Lukaszewicz's <ref> [55] </ref> modification of default logic, the extensions of general closed default theories correspond to the sets of formulae whose beliefs, calculated by the theory of evidence, tend to 1 when the reliability of the sources of evidence tend to 1.
Reference: 56. <author> P. Magni, R. Bellazi, and F. Locatelli. </author> <title> Using uncertainty management techniques in medical therapy planning: a decision theoretic approach. </title> <editor> In A. Hunter and S. Parsons, editors, </editor> <booktitle> Applications of Uncertainty Formalisms (this volume). </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1998. </year>
Reference-contexts: Another network-based method that has received wide attention is that of Lauritzen and Spiegel-halter [52] which has been used as the basis of the expert system shell HUGIN [1], and the paper in this volume by Magni et al. <ref> [56] </ref> makes use of a graphical representation similar to that discussed above. 2.3 Evidence theory Evidence theory is the term commonly used to refer to the body of work carried out by Arthur Dempster [17, 18] and Glenn Shafer [82] to remedy some of what 1 Singly-connected networks are those in
Reference: 57. <author> P. Magrez and Ph. Smets. </author> <title> Fuzzy modus ponens: A new model suitable for applications in knowledge-based systems. </title> <journal> International Journal of Intelligent Systems, </journal> <volume> 4 </volume> <pages> 181-200, </pages> <year> 1975. </year>
Reference-contexts: For reasoning with fuzzy statements such as the one above about dirty clothes, we need to develop a notion of modus ponens which can handle fuzzy concepts. Generalized modus ponens is such a development <ref> [57] </ref>. For example, suppose the clothes are "not very dirty", then "not very dirty" does not directly match with "dirty". We need to adapt the statement to allow the data "not very dirty" to apply. This means changing the consequence in some way, perhaps to "warm water".
Reference: 58. <author> J. Martins and S. Shapiro. </author> <title> A model of belief revision. </title> <journal> Artificial Intelligence, </journal> <volume> 35 </volume> <pages> 25-79, </pages> <year> 1988. </year>
Reference-contexts: It also forms the basis for one of the applications case studies in this book [47]. 3.4 Truth maintenance systems When reasoning with inconsistent information, questions of belief in assumptions and belief in conclusions arise. These questions include <ref> [58] </ref>: Inferences from beliefs. How do new beliefs follow from existing beliefs? Default beliefs. How do we record that a belief depends on the absence of other beliefs? Dependency recording.
Reference: 59. <author> D. A. McAllester. </author> <title> An outlook on truth maintenance. </title> <type> AI Memo 551, </type> <institution> AI Laboratory, MIT, </institution> <year> 1980. </year>
Reference-contexts: Bigham's system is particularly interesting in that it includes an extension of the clause based approach of McAllester's logic-based truth maintenance system (LTMS) <ref> [59] </ref> as a symbolic inference engine, and also permits beliefs based on possibility theory to be propagated. A similar system is de Kleer and Williams' [16] GDE for fault diagnosis.
Reference: 60. <author> J. McCarthy. </author> <title> Circumscription|a form of non-monotonic reasoning. </title> <journal> Artificial Intelligence, </journal> <volume> 13 </volume> <pages> 27-39, </pages> <year> 1980. </year>
Reference-contexts: Two key approaches are Re-iter's default logic [76] and McCarthy's circumscription <ref> [60] </ref>. In this section we start with a discussion of the limitations of first order logic as a basis for practical reasoning systems, introducing the notions of retraction, monotonicity and defeasibility.
Reference: 61. <author> S. Moral and N. Wilson. </author> <title> Importance sampling Monte-Carlo algorithms for the calculation of Dempster-Shafer belief. </title> <booktitle> In Proceedings of the 6th International Conference on Information Processing and the Management of Uncertainty, </booktitle> <pages> pages 1337-1344, </pages> <year> 1996. </year>
Reference-contexts: Wilson has also proposed an approximate calculation, based on a Monte-Carlo simulation, which gives results that are arbitrarily close to the exact solution, and which can be performed in linear time. More recent advances are explored in <ref> [61, 101] </ref>. The application of evidence theory is the subject of three papers in this volume.
Reference: 62. <author> P. Nicolas and T. Schaub. </author> <title> The XRay system: an implementation platform for local query-answering in default logics. </title> <editor> In A. Hunter and S. Parsons, editors, </editor> <booktitle> Applications of Uncertainty Formalisms (this volume). </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1998. </year>
Reference-contexts: However this situation is beginning to change. This volume includes a paper by Nicolas and Schaub <ref> [62] </ref> which describes a system on which to build default logic applications, while Brazier et al. [8] have applied default logic to a problem from ecology. 3.3 Argumentation Argumentation is the process by which arguments are constructed and compared.
Reference: 63. <author> N. J. Nilsson. </author> <title> Probabilistic logic. </title> <journal> Artificial Intelligence, </journal> <volume> 28 </volume> <pages> 71-87, </pages> <year> 1986. </year>
Reference: 64. <author> S. Parsons. </author> <title> Qualitative approaches to reasoning under uncertainty. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1998. </year>
Reference: 65. <author> S. Parsons and J. Fox. </author> <title> Argumentation and decision making: a position paper. </title> <booktitle> In Formal and Applied Practical Reasoning, </booktitle> <pages> pages 705-709, </pages> <address> Berlin, Germany, 1996. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: This approach is described in more detail in this volume [33], and elsewhere [46], and its historical development is charted in <ref> [65] </ref>. It also forms the basis for one of the applications case studies in this book [47]. 3.4 Truth maintenance systems When reasoning with inconsistent information, questions of belief in assumptions and belief in conclusions arise. These questions include [58]: Inferences from beliefs.
Reference: 66. <author> J. Pearl. </author> <title> How to do with probabilities what people say you can't. </title> <type> Technical Report CSD-850031, </type> <institution> Cognitive Systems Laboratory, Computer Science Department UCLA, </institution> <year> 1985. </year>
Reference: 67. <author> J. Pearl. </author> <title> Fusion, propagation and structuring belief networks. </title> <journal> Artificial Intelligence, </journal> <volume> 29 </volume> <pages> 241-288, </pages> <year> 1986. </year>
Reference: 68. <author> J. Pearl. </author> <title> Bayesian decision methods. </title> <booktitle> In Encyclopedia of Artificial Intelligence, </booktitle> <pages> pages 48-56. </pages> <publisher> John Wiley, </publisher> <year> 1987. </year>
Reference: 69. <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference: 70. <author> G. Pinkas and R. Loui. </author> <title> Reasoning from inconsistency: A taxonomy of principles for resolving conflict. </title> <booktitle> In Principles of Knowledge Representation and Reasoning: Proceedings of the Third International Conference. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: This approach to argumentation has been developed in [3, 29]. A number of other approaches to argumentation, including <ref> [70, 72, 97] </ref>, focus on default reasoning by incorporating default connectives (which can be used to build up default statements similar to the default rules in default logic) into their languages together with associated machinery.
Reference: 71. <author> M. Pradhan, M. Henrion, G. Provan, B. Del Favero, and K. Huang. </author> <title> The sensitivity of belief networks to imprecise probabilities: an experiemntal investigation. </title> <journal> Artificial Intelligence, </journal> <volume> 85 </volume> <pages> 363-397, </pages> <year> 1996. </year>
Reference-contexts: then, as though there is no clear cut winner in this argument; the moral appears to be: "if you can obtain the numbers to your satisfaction, then use them." As a final word, it is worth mentioning that it has also been convincingly argued in several places (see for example <ref> [9, 71] </ref>) that even if the numbers are available, they make little difference to the business of weighing up the evidence. This, however, is a different argument altogether, and we will say no more about it.
Reference: 72. <author> H. Prakken. </author> <title> An argumentation framework for default reasoning. </title> <journal> Annals of Mathematics and Artificial Intelligence, </journal> <volume> 9, </volume> <year> 1993. </year>
Reference-contexts: This approach to argumentation has been developed in [3, 29]. A number of other approaches to argumentation, including <ref> [70, 72, 97] </ref>, focus on default reasoning by incorporating default connectives (which can be used to build up default statements similar to the default rules in default logic) into their languages together with associated machinery.
Reference: 73. <author> G. M. Provan. </author> <title> Solving diagnostic problems using extended assumption-based truth maintenance systems: foundations. </title> <type> Technical Report 88-10, </type> <institution> Department of Computer Science, University of British Columbia, </institution> <year> 1988. </year>
Reference-contexts: This is commonly done by using a logical technique to establish a set of possible hypotheses from a larger initial set of exhaustive hypotheses, and then using a numerical techniques to rank the plausible set. Typical of such systems are those of Provan <ref> [73] </ref>, Bigham [5] and Laskey and Lehner [51]. In all three of these systems, the semantic equivalence of the ATMS [15] and the Dempster-Shafer method, proved by Provan, is exploited ensuring that no information is lost in the initial round of inference.
Reference: 74. <author> J. R. Quinlan. </author> <title> INFERNO: a cautious approach to uncertain inference. </title> <journal> Computer Journal, </journal> <volume> 26 </volume> <pages> 255-269, </pages> <year> 1983. </year>
Reference: 75. <author> M. Ramalho. </author> <title> Uncertainty measures associated with fuzzy rules for connection admission control in ATM networks. </title> <editor> In A. Hunter and S. Parsons, editors, </editor> <booktitle> Applications of Uncertainty Formalisms (this volume). </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1998. </year>
Reference-contexts: This combination of possibility theory and logic into possibilistic logic has been investigated at length by Dubois and Prade [23, 24]. Possibilistic logic is one of the techniques explored by Bigham in his paper in this volume [6], and both the contributions of Ramalho <ref> [75] </ref> and Saffiotti [79] make use of fuzzy inference of the kind discussed above, while Bosc et al. [7] consider the application of fuzzy techniques to databases. 2.5 Other approaches There are a number of other numerical techniques which, although we do not have space to consider them in any detail,
Reference: 76. <author> R. Reiter. </author> <title> A logic for default reasoning. </title> <journal> Artificial Intelligence, </journal> <volume> 13 </volume> <pages> 81-132, </pages> <year> 1980. </year>
Reference-contexts: Two key approaches are Re-iter's default logic <ref> [76] </ref> and McCarthy's circumscription [60]. In this section we start with a discussion of the limitations of first order logic as a basis for practical reasoning systems, introducing the notions of retraction, monotonicity and defeasibility. <p> Cautious systems on the other hand are only prepared to accept conclusions which cannot be contradicted. As a result if they can hypothesise both OE and :OE, they conclude neither, even though one must be true. 3.2 Default logic Default logic, introduced by Reiter in <ref> [76] </ref>, models prototypical reasoning by allowing special inference rules, known as default rules, to be added to a standard first order logic.
Reference: 77. <author> E. Rich. </author> <title> Default reasoning as likelihood reasoning. </title> <booktitle> In Proceedings of the 3rd National Conference on Artificial Intelligence, </booktitle> <pages> pages 348-351, </pages> <address> Los Altos, CA, 1983. </address> <publisher> William Kaufmann. </publisher>
Reference-contexts: Both of these papers can be seen as an extension of the work of Rich <ref> [77] </ref> and Ginsberg [36], who considered ways of applying numerical certainty measures to logical inference rules. It is also possible to use argumentation to combine symbolic and numerical reasoning.
Reference: 78. <author> A. Saffiotti. </author> <title> An AI view of the treatment of uncertainty. </title> <journal> The Knowledge Engineering Review, </journal> <volume> 2 </volume> <pages> 75-97, </pages> <year> 1987. </year>
Reference: 79. <author> A. Saffiotti. </author> <title> Handling uncertainty in control of autonomous robots. </title> <editor> In A. Hunter and S. Parsons, editors, </editor> <booktitle> Applications of Uncertainty Formalisms (this volume). </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1998. </year>
Reference-contexts: This combination of possibility theory and logic into possibilistic logic has been investigated at length by Dubois and Prade [23, 24]. Possibilistic logic is one of the techniques explored by Bigham in his paper in this volume [6], and both the contributions of Ramalho [75] and Saffiotti <ref> [79] </ref> make use of fuzzy inference of the kind discussed above, while Bosc et al. [7] consider the application of fuzzy techniques to databases. 2.5 Other approaches There are a number of other numerical techniques which, although we do not have space to consider them in any detail, are worth mentioning
Reference: 80. <author> B. Schweitzer and A. Sklar. </author> <title> Associative functions and abstract semigroups. </title> <journal> Pub-licationes Mathematicae Debrecen, </journal> <volume> 10 </volume> <pages> 69-81, </pages> <year> 1963. </year>
Reference-contexts: is possible that: Bel (A) + Bel (:A) &gt; 1 These ideas lead to the definition of a calculus of belief intervals, where a belief interval for A is [Bel (A); Pl (A)], in which combination is carried out by a family of general functions called triangular norms and conorms <ref> [80] </ref>, and explicit reasoning about the degree to which a proposition is believed and disbelieved is possible. 2.6 Limitations of numerical techniques As one might expect, none of the systems mentioned in preceding sections is perfect, and there are a number of problems common to all numerical formalisms.
Reference: 81. <author> G. L. S. Shackle. </author> <title> Decision, order and time in human affairs. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, UK, </address> <year> 1961. </year>
Reference-contexts: However, the concept of using the notion of possibilities as an alternative to probabilities was mooted much earlier. The economist G. L. S. Shackle <ref> [81] </ref>, unhappy with the use of subjective probability for handling uncertainty, proposed an alternative formalism. This formalism was the calculus of potential surprise where uncertainty about an event is characterised by a subjective measure of the degree to which the observer in question would be surprised by its occurrence.
Reference: 82. <author> G. Shafer. </author> <title> A Mathematical Theory of Evidence. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1976. </year>
Reference-contexts: [1], and the paper in this volume by Magni et al. [56] makes use of a graphical representation similar to that discussed above. 2.3 Evidence theory Evidence theory is the term commonly used to refer to the body of work carried out by Arthur Dempster [17, 18] and Glenn Shafer <ref> [82] </ref> to remedy some of what 1 Singly-connected networks are those in which for every pair of nodes there is at most one path along arcs which joins them.
Reference: 83. <author> G. Shafer. </author> <title> Comments on `An inquiry into computer understanding' by Peter Cheeseman. </title> <journal> Computational Intelligence, </journal> <volume> 4 </volume> <pages> 121-124, </pages> <year> 1988. </year>
Reference-contexts: This has been used by many (see for example [32]) to argue against the use of probability theory for dealing with uncertainty. However, the personalist and necessarian <ref> [83] </ref> schools of probabilists argue that probabilities may always be obtained, either from rational human reasoning, or because they exist as a measure of the degree to which sets of propositions confirm one another.
Reference: 84. <author> G. Shafer and R. Logan. </author> <title> Implementing Dempster's rule for hierarchical evidence. </title> <journal> Artificial Intelligence, </journal> <volume> 33 </volume> <pages> 271-298, </pages> <year> 1987. </year>
Reference-contexts: Gordon and Shortliffe [37] extended Barnett's approach to compute approximate beliefs in a space of hierarchically organised sets of hypotheses in linear time. This approach was then subsumed by that of Shafer and Logan <ref> [84] </ref>, who provided an exact algorithm for hierarchically organised sets of hypotheses that is also linear in time whilst being slightly more general than that of Gordon and Shortliffe.
Reference: 85. <author> P. P. Shenoy and G. Shafer. </author> <title> Axioms for probability and belief function propagation. </title> <editor> In R. D. Shachter, T. S. Levitt, L. N. Kanal, and J. F. Lemmer, editors, </editor> <booktitle> Uncertainty in Artificial Intelligence 4, </booktitle> <pages> pages 169-198. </pages> <publisher> North-Holland, </publisher> <address> Amster-dam, The Netherlands, </address> <year> 1990. </year>
Reference-contexts: This approach was then subsumed by that of Shafer and Logan [84], who provided an exact algorithm for hierarchically organised sets of hypotheses that is also linear in time whilst being slightly more general than that of Gordon and Shortliffe. More recently, Shenoy and Shafer <ref> [85] </ref> have introduced a method for the efficient propagation of belief functions in networks by means of local computations, and Nic Wilson [100] has proposed a method in which the explicit use of Dempster's rule of combination is avoided.
Reference: 86. <author> E. H. Shortliffe. </author> <title> Computer-Based Medical Consultations: MYCIN. </title> <publisher> Elsevier, </publisher> <address> New York, NY, </address> <year> 1976. </year>
Reference-contexts: Certainty factors <ref> [86] </ref>, perhaps because of their simplicity and intuitive appeal, have been widely used to handle uncertainty. The certainty factor approach assigns a numerical weight, the certainty factor, to the consequent of every If hevidencei then hhypothesis i rule in a rule-based system.
Reference: 87. <author> Ph. Smets. </author> <title> Belief functions. </title> <editor> In Ph. Smets, E. H. Mamdani, D. Dubois, and H. Prade, editors, </editor> <booktitle> Non-Standard Logics for Automated Reasoning, </booktitle> <pages> pages 253-275. </pages> <publisher> Academic Press, </publisher> <address> London, UK, </address> <year> 1988. </year>
Reference-contexts: Smets has adapted evidence theory as introduced by Dempster and Shafer in two important ways <ref> [87, 90] </ref>. The first was to relax the assumption that all hypotheses have been identified before the evidence is considered. Instead Smets makes an open-world assumption that the frame of discernment does not necessarily contain an exhaustive set of hypotheses.
Reference: 88. <author> Ph. Smets. </author> <title> Belief functions versus probability functions. </title> <editor> In B Bouchon-Meunier, L. Saitta, and R. R. Yager, editors, </editor> <booktitle> Uncertainty and Intelligent Systems, </booktitle> <pages> pages 17-24. </pages> <publisher> Springer Verlag, </publisher> <address> Berlin, Germany, </address> <year> 1988. </year>
Reference: 89. <author> Ph. Smets and Y-T. Hsia. </author> <title> Default reasoning and the transferable belief model. </title> <editor> In P. P. Bonissone, M. Henrion, L. N. Kanal, and J. F. Lemmer, editors, </editor> <booktitle> Uncertainty in Artificial Intelligence 6, </booktitle> <pages> pages 495-504. </pages> <publisher> Elsevier Science Publishers, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1991. </year>
Reference-contexts: The existence of a strong relationship between default logic and the theory of evidence is borne out by Smets and Hsia <ref> [89] </ref> who demonstrate how to represent normal defaults (both with and without prerequisites) using the transferable belief model. Both of these papers can be seen as an extension of the work of Rich [77] and Ginsberg [36], who considered ways of applying numerical certainty measures to logical inference rules.
Reference: 90. <author> Ph. Smets and R. Kennes. </author> <title> The transferable belief model. </title> <journal> Artificial Intelligence, </journal> <volume> 66 </volume> <pages> 191-234, </pages> <year> 1994. </year>
Reference-contexts: Smets has adapted evidence theory as introduced by Dempster and Shafer in two important ways <ref> [87, 90] </ref>. The first was to relax the assumption that all hypotheses have been identified before the evidence is considered. Instead Smets makes an open-world assumption that the frame of discernment does not necessarily contain an exhaustive set of hypotheses.
Reference: 91. <author> M. Smithson. </author> <title> Ignorance and Uncertainty: Emerging Paradigms. </title> <publisher> Springer Verlag, </publisher> <address> New York, NY, </address> <year> 1989. </year>
Reference: 92. <author> L. E. Sucar, D. F. Gillies, and D. A. Gillies. </author> <title> Objective probabilities in expert systems. </title> <journal> Artificial Intelligence, </journal> <volume> 61 </volume> <pages> 187-208, </pages> <year> 1993. </year>
Reference: 93. <author> A. Tawfik and E. Neufeld. </author> <title> Model-based diagnosis: a probabilistic extension. </title> <editor> In A. Hunter and S. Parsons, editors, </editor> <booktitle> Applications of Uncertainty Formalisms (this volume). </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1998. </year>
Reference: 94. <author> S. Toulmin. </author> <title> The uses of argument. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, UK., </address> <year> 1957. </year>
Reference-contexts: Following Toulmin <ref> [94] </ref>, an argument can be structured so that from facts a qualified claim (a conclusion) can be argued (inferred) if and only if: 1. there is some warrant (some further assumptions) that can be used with the facts to logically derive the claim, and 2. there is no other argument that
Reference: 95. <author> A. Tversky and D. Kahneman. </author> <title> Judgement under uncertainty: Heuristics and biases. </title> <journal> Science, </journal> <volume> 185 </volume> <pages> 1124-1131, </pages> <year> 1974. </year>
Reference-contexts: This, however, is a different argument altogether, and we will say no more about it. A second problem stems from the use of numbers; the interpretation of the results of applying a numerical formalism given the notorious irrationality that human beings exhibit when dealing with numbers <ref> [95] </ref>. All the techniques generate results as numerical values. These values, however, have been generated in different ways, and thus measure different things, although they are just num-bers and may be compared and contrasted by the uninitiated as though they represented the same thing.
Reference: 96. <author> K. van Dam. </author> <title> Using uncertainty techniques in radio communication systems. </title> <editor> In A. Hunter and S. Parsons, editors, </editor> <booktitle> Applications of Uncertainty Formalisms (this volume). </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1998. </year>
Reference-contexts: More recent advances are explored in [61, 101]. The application of evidence theory is the subject of three papers in this volume. Lalmas [50] uses it as a means of deciding which document to retrieve, van Dam <ref> [96] </ref> uses it to control a radio communication system, and Duncan Wilson [98] considers how to apply it to the classification of faults in automated inspection. 2.4 Possibility theory A formal theory of possibilities, based on the notion of a fuzzy set [102], was first introduced by Zadeh [103]. <p> This measurement leads to new information which, when entered, leads to further symbolic computation. Thus the numerical computation sparks off another round of symbolic inference, and the cycle continues until the fault is found. In contrast to these ATMS-based approaches, van Dam <ref> [96] </ref> uses a JTMS in combination with Dempster-Shafer theory. It is also possible to use possibility measures with an assumption-based truth maintenance system instead of belief functions or probabilities. This is exactly the course followed by Dubois, Lang and Prade in their possibilistic ATMS [22].
Reference: 97. <author> G. Vreeswijk. </author> <title> Abstract argumentation systems. </title> <editor> In M de Glas and D Gabbay, editors, </editor> <booktitle> Proceedings of the First World Conference on Fundamentals of Artificial Intelligence. </booktitle> <publisher> Angkor, </publisher> <year> 1991. </year>
Reference-contexts: This approach to argumentation has been developed in [3, 29]. A number of other approaches to argumentation, including <ref> [70, 72, 97] </ref>, focus on default reasoning by incorporating default connectives (which can be used to build up default statements similar to the default rules in default logic) into their languages together with associated machinery.
Reference: 98. <author> D. Wilson, A. Greig, John Gilby, and Robert Smith. </author> <title> Some problems in trying to implement uncertainty techniques in automated inspection. </title> <editor> In A. Hunter and S. Parsons, editors, </editor> <booktitle> Applications of Uncertainty Formalisms (this volume). </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1998. </year>
Reference-contexts: More recent advances are explored in [61, 101]. The application of evidence theory is the subject of three papers in this volume. Lalmas [50] uses it as a means of deciding which document to retrieve, van Dam [96] uses it to control a radio communication system, and Duncan Wilson <ref> [98] </ref> considers how to apply it to the classification of faults in automated inspection. 2.4 Possibility theory A formal theory of possibilities, based on the notion of a fuzzy set [102], was first introduced by Zadeh [103].

References-found: 98

