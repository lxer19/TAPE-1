URL: http://www.cs.umn.edu/Users/dept/users/kumar/backprop.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Email: kumar@cs.umn.edu shekhar@cs.umn.edu amin@cs.umn.edu  
Title: A Scalable Parallel Formulation of the Backpropagation Algorithm for Hypercubes and Related Architectures  
Author: Vipin Kumar, Shashi Shekhar and Minesh B. Amin 
Address: Minneapolis, MN 55455  
Affiliation: Department of Computer Science, University of Minnesota,  
Abstract: In this paper, we present a new technique for mapping the backpropagation algorithm on hypercubes and related architectures. A key component of this technique is a network partitioning scheme which is called checkerboarding. Checkerboarding allows us to replace the all-to-all broadcast operation performed by the commonly used vertical network partitioning scheme, with operations that are much faster on the hypercubes and related architectures. Checker-boarding can be combined with the pattern partitioning technique to form a hybrid scheme which performs better than either one of these schemes. Theoretical analysis and experimental results on nCUBE2 TM y and CM5 TM z show that our scheme performs better than the other schemes, both for uniform and non-uniform networks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representations by error propagation, chapter 8. </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference: [2] <author> T. J. Sejnowski and C. R. Rosenburg. Nettalk: </author> <title> a parallel network that learns to read aloud. </title> <type> Technical Report JHU/EECS-86/01, </type> <institution> Johns Hopkins University - EE/Csci, </institution> <year> 1986. </year>
Reference-contexts: 1 Introduction The Backpropagation algorithm (BP)[1] is one of the most popular neural network learning algorithms. It has been used in a large number of applications <ref> [2, 3, 4, 5] </ref>. This algorithm is computation intensive and as a result there has been a great interest in developing parallel formulations of this algorithm for a variety of parallel computers. BP can be parallelized either by network partitioning or by pattern partitioning. <p> Non-uniform networks are interesting, since these are used in many applications <ref> [2, 25] </ref>. Let I 0 ; I 1 ; : : :I J be the number of nodes in the different layers of the network. The network is uniform iff I 0 = I 1 = = I J ; otherwise, the network is called a non-uniform network.
Reference: [3] <author> S. Shekhar and M. B. Amin. </author> <title> Genralization performance of feed-forward neural networks. </title> <journal> IEEE Trans. On Knowledge and Data Engineering, </journal> <month> April </month> <year> 1992. </year>
Reference-contexts: 1 Introduction The Backpropagation algorithm (BP)[1] is one of the most popular neural network learning algorithms. It has been used in a large number of applications <ref> [2, 3, 4, 5] </ref>. This algorithm is computation intensive and as a result there has been a great interest in developing parallel formulations of this algorithm for a variety of parallel computers. BP can be parallelized either by network partitioning or by pattern partitioning. <p> We will focus on the set-training regime in this paper. 2.2 Application Domains of Backpropagation The applications of neural network learning algorithms can broadly be divided into two groups: recognition and generalization 3 <ref> [3] </ref>. In recognition problems, a set of learning patterns are used to train the network. The trained network is expected to recall the output part of a previously seen learning pattern, when the input portion of the same learning pattern is presented. <p> The number of learning samples should be an order of magnitude larger than the number of weights to be learned. This is required for reasonable confidence in the learned weights for generalization over unseen testing patterns. These problems can be characterized by L &gt; 10I 2 J <ref> [3, 4] </ref>. 2.3 Parallel Formulations of Backpropagation Here we survey existing schemes to parallelize BP for the fully connected multi-layer networks. In these networks, the adjacent layers are completely connected. For schemes that primarily deal with randomly sparse networks, see [7, 26, 27, 28, 29].
Reference: [4] <author> S. Dutta and S. Shekhar. Bond-rating: </author> <title> A non-conservative application of neural networks. </title> <booktitle> In IEEE Intl. Conf. on Neural Networks, </booktitle> <month> July </month> <year> 1988. </year>
Reference-contexts: 1 Introduction The Backpropagation algorithm (BP)[1] is one of the most popular neural network learning algorithms. It has been used in a large number of applications <ref> [2, 3, 4, 5] </ref>. This algorithm is computation intensive and as a result there has been a great interest in developing parallel formulations of this algorithm for a variety of parallel computers. BP can be parallelized either by network partitioning or by pattern partitioning. <p> On the other hand, in generalization problems neural networks are tested with input portions of new testing patterns. The network is expected to predict the output portions of the testing pattern, which have not been shown to the network during the learning phase. Generalization problems include assignment of bond-rating <ref> [4] </ref> and economic prediction [5]. Usually a large neural network is used in recognition problems. Furthermore, recognition problems have a small number of learning samples. For example, in the protein-folding problem, L = 14 and I = 1000 [25]. <p> The number of learning samples should be an order of magnitude larger than the number of weights to be learned. This is required for reasonable confidence in the learned weights for generalization over unseen testing patterns. These problems can be characterized by L &gt; 10I 2 J <ref> [3, 4] </ref>. 2.3 Parallel Formulations of Backpropagation Here we survey existing schemes to parallelize BP for the fully connected multi-layer networks. In these networks, the adjacent layers are completely connected. For schemes that primarily deal with randomly sparse networks, see [7, 26, 27, 28, 29].
Reference: [5] <author> H. White. </author> <title> Economic prediction using neural networks: The case of ibm daily stock returns. </title> <booktitle> In IEEE Intl. Conf. on Neural Networks, </booktitle> <month> July </month> <year> 1988. </year>
Reference-contexts: 1 Introduction The Backpropagation algorithm (BP)[1] is one of the most popular neural network learning algorithms. It has been used in a large number of applications <ref> [2, 3, 4, 5] </ref>. This algorithm is computation intensive and as a result there has been a great interest in developing parallel formulations of this algorithm for a variety of parallel computers. BP can be parallelized either by network partitioning or by pattern partitioning. <p> The network is expected to predict the output portions of the testing pattern, which have not been shown to the network during the learning phase. Generalization problems include assignment of bond-rating [4] and economic prediction <ref> [5] </ref>. Usually a large neural network is used in recognition problems. Furthermore, recognition problems have a small number of learning samples. For example, in the protein-folding problem, L = 14 and I = 1000 [25]. The recognition problems can be characterized by L o I 2 .
Reference: [6] <author> W. Allen and A. Saha. </author> <title> Parallel neural network simulation using backpropagation for the es-kit environment. </title> <booktitle> In Proceedings of the 1989 Conference on Hypercubes, Concurrent Computers and Applications, </booktitle> <pages> pages 1097-1102, </pages> <year> 1989. </year>
Reference-contexts: In pattern partitioning, individual weight changes due to various learning patterns are computed concurrently [17, 18, 19, 20]. Pattern partitioning and network partitioning can also be combined <ref> [6, 11, 12, 14, 20, 21] </ref> to form hybrid schemes. Several machine architectures including linear arrays, meshes and hypercubes have been explored to implement parallel BP. In this paper, we present a new technique for mapping the backpropagation algorithm on hy-percubes and related architectures. <p> In this paper, we present a new technique for mapping the backpropagation algorithm on hy-percubes and related architectures. A key component of this technique is a network partitioning scheme which is called checkerboarding. The major communication-intensive operation in the commonly used vertical sectioning scheme <ref> [6, 9, 10, 11, 12, 13, 14, 15, 17, 22] </ref> is the all-to-all broadcast. In this operation, each of the P processors has to broadcast its local m units of information to all the other processors. <p> The network partitioning schemes take advantage of the parallelism in the computation of node activations and node errors by distributing both the nodes and the weights of the network among different processors. Nodes can be partitioned in different ways. Complete partitioning assigns one node per processor <ref> [6, 8] </ref>. Vertical sectioning 3 Many researchers use alternative categories, namely classification and regression. <p> Vertical sectioning 3 Many researchers use alternative categories, namely classification and regression. We choose recognition and generalization categories to establish the dominance regions of alternative parallel formulations. 5 divides the nodes in a layer among the processors, and each processor gets some nodes from each layer <ref> [6, 9, 10, 11, 12, 13, 14, 15, 17, 22] </ref>. The weights can be partitioned in four different ways: complete partitioning, inset grouping, outset grouping and checkerboarding. Complete partitioning [8, 16] allocates one processor per weight in the network. <p> The inset of a node is allocated to the processor possessing the node. Inset grouping reduces the need for communication in computing node activations during the forward propagation. It has been used in <ref> [6, 11, 12, 14] </ref>. Outset grouping schemes form sets, which represent the collection of weights on the outgoing edges from a specific node. The outset of a node is allocated to a processor possessing the node. <p> This scheme is preferred for problems with a large set of learning patterns on machines supporting efficient broadcast operation. This scheme has been used in [18, 19, 17, 20]. The computation in different layers of BP can be pipelined <ref> [6, 14] </ref>; i.e., while one pattern is being processed for some layer, a different pattern can be processed for a preceding layer. Even though pipelining partitions the network horizontally, it is more appropriate to view it as pattern partitioning. Hybrid schemes combine pattern partitioning with network partitioning. <p> Even though pipelining partitions the network horizontally, it is more appropriate to view it as pattern partitioning. Hybrid schemes combine pattern partitioning with network partitioning. For example, pipelin-ing can be combined with vertical sectioning <ref> [6, 14] </ref>. Other examples include the combination of vertical sectioning with simple pattern partitioning [12]. We can view the forward and backward phase computations for a pattern as a sequence of weight-matrix to activation/error-vector products. <p> The vertical-sectioning of nodes with duplicate inset/outset weight partitioning is used for mesh-connected transputers in [9, 10]. Mesh-based machines have been explored by many researchers. A matrix-matrix multiplication-based formulation is given in [21] for mesh architectures. Other schemes for mesh include those proposed in <ref> [13, 6, 16] </ref>. Techniques for hypercube and related architectures have been explored in [8, 11, 12, 15, 18, 19]. 6 3 Hypercubes and Backpropagation In a hypercube-connected parallel computer, each processor is directly connected to log (P ) other processors whose addresses differ by exactly one bit.
Reference: [7] <author> W. M. Lin, V. K. Prasanna, and K. W. Przytula. </author> <title> Algorithmic mapping of neural network models onto parallel simd machines. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 40(12) </volume> <pages> 1390-1401, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: In these networks, the adjacent layers are completely connected. For schemes that primarily deal with randomly sparse networks, see <ref> [7, 26, 27, 28, 29] </ref>. Parallelization schemes for BP can be classified into three broad categories: network partitioning schemes, pattern partitioning schemes, and hybrid schemes. <p> We would also like to generalize our schemes by utilizing different lengths and breadths of processor grids for different layers of the network. It would also be interesting to compare the parallelization techniques for randomly sparse networks <ref> [7, 26, 27, 28, 29] </ref>. with our scheme for non-uniform networks. 9 Acknowledgement We would like to thank Prof. Joydeep Ghosh (University of Texas at Austin), George Karypis and Ananth Grama for their useful comments during the drafting of this paper, and Dr.
Reference: [8] <author> G. Blelloch and C. R. Rosenberg. </author> <title> Network learning on the connection machine. </title> <type> Technical report, </type> <institution> MIT, </institution> <month> November </month> <year> 1986. </year>
Reference-contexts: Most previous parallel formulations of BP on hypercube or on related architectures used vertical sectioning [15], pattern partitioning [18, 19], or a hybrid of vertical sectioning and pattern partitioning [11, 12]. For hypercubes, the authors are aware of only one exception <ref> [8] </ref> in which each node and weight of the BP network is mapped onto a separate processor of CM2 T Mk . As discussed in [11, 12], this method incurs too much communication overhead. The paper is organized as follows. <p> The network partitioning schemes take advantage of the parallelism in the computation of node activations and node errors by distributing both the nodes and the weights of the network among different processors. Nodes can be partitioned in different ways. Complete partitioning assigns one node per processor <ref> [6, 8] </ref>. Vertical sectioning 3 Many researchers use alternative categories, namely classification and regression. <p> The weights can be partitioned in four different ways: complete partitioning, inset grouping, outset grouping and checkerboarding. Complete partitioning <ref> [8, 16] </ref> allocates one processor per weight in the network. It allows maximum concurrency in the computation of the various terms of the node activation and node error. However, it requires communication to accumulate the terms. Inset and outset grouping schemes are often used in conjunction with the vertical-sectioning scheme. <p> Mesh-based machines have been explored by many researchers. A matrix-matrix multiplication-based formulation is given in [21] for mesh architectures. Other schemes for mesh include those proposed in [13, 6, 16]. Techniques for hypercube and related architectures have been explored in <ref> [8, 11, 12, 15, 18, 19] </ref>. 6 3 Hypercubes and Backpropagation In a hypercube-connected parallel computer, each processor is directly connected to log (P ) other processors whose addresses differ by exactly one bit.
Reference: [9] <author> H. Yoon and J. H. Nang. </author> <title> Multilayer neural networks on distributed-memory multiprocessors. </title> <booktitle> In Proc. Intl. Jt. Conf. on Neural Networks (IEEE/EEC), </booktitle> <pages> pages 669-672, </pages> <year> 1991. </year>
Reference-contexts: In this paper, we present a new technique for mapping the backpropagation algorithm on hy-percubes and related architectures. A key component of this technique is a network partitioning scheme which is called checkerboarding. The major communication-intensive operation in the commonly used vertical sectioning scheme <ref> [6, 9, 10, 11, 12, 13, 14, 15, 17, 22] </ref> is the all-to-all broadcast. In this operation, each of the P processors has to broadcast its local m units of information to all the other processors. <p> Vertical sectioning 3 Many researchers use alternative categories, namely classification and regression. We choose recognition and generalization categories to establish the dominance regions of alternative parallel formulations. 5 divides the nodes in a layer among the processors, and each processor gets some nodes from each layer <ref> [6, 9, 10, 11, 12, 13, 14, 15, 17, 22] </ref>. The weights can be partitioned in four different ways: complete partitioning, inset grouping, outset grouping and checkerboarding. Complete partitioning [8, 16] allocates one processor per weight in the network. <p> Outset grouping eliminates the need for communication to compute error vectors during the backward phase. It has been used in [15]. Both inset and outset grouping can be used together to improve the efficiency of both forward and backward phases <ref> [9, 10, 13] </ref>. However, this scheme duplicates each weight on two processors, increasing the work during weight updates. It can also be shown that the communication cost for inset grouping is the same as the communication cost incurred by the replication scheme, through the use of multiply-accumulate-rotate operation [11]. <p> Complete node partitioning and inset weight partitioning with pipelin-ing have been explored for linear arrays in [14]. The vertical-sectioning of nodes with duplicate inset/outset weight partitioning is used for mesh-connected transputers in <ref> [9, 10] </ref>. Mesh-based machines have been explored by many researchers. A matrix-matrix multiplication-based formulation is given in [21] for mesh architectures. Other schemes for mesh include those proposed in [13, 6, 16]. <p> The processors also contain the incoming weights associated with the nodes that reside in them. Each processor computes node activations, node errors, and weight changes for one pattern at a time. As an example, consider the scheme given in [11]. A similar scheme is given in <ref> [9, 10] </ref>.
Reference: [10] <author> H. Yoon and J. H. Nang. </author> <title> A distributed backpropagation algorithm of neural networks on distributed-memory multiprocessors. </title> <booktitle> In Proc. Intl. Conf. on Parallel Processing, </booktitle> <pages> pages 358-363, </pages> <year> 1991. </year>
Reference-contexts: In this paper, we present a new technique for mapping the backpropagation algorithm on hy-percubes and related architectures. A key component of this technique is a network partitioning scheme which is called checkerboarding. The major communication-intensive operation in the commonly used vertical sectioning scheme <ref> [6, 9, 10, 11, 12, 13, 14, 15, 17, 22] </ref> is the all-to-all broadcast. In this operation, each of the P processors has to broadcast its local m units of information to all the other processors. <p> Vertical sectioning 3 Many researchers use alternative categories, namely classification and regression. We choose recognition and generalization categories to establish the dominance regions of alternative parallel formulations. 5 divides the nodes in a layer among the processors, and each processor gets some nodes from each layer <ref> [6, 9, 10, 11, 12, 13, 14, 15, 17, 22] </ref>. The weights can be partitioned in four different ways: complete partitioning, inset grouping, outset grouping and checkerboarding. Complete partitioning [8, 16] allocates one processor per weight in the network. <p> Outset grouping eliminates the need for communication to compute error vectors during the backward phase. It has been used in [15]. Both inset and outset grouping can be used together to improve the efficiency of both forward and backward phases <ref> [9, 10, 13] </ref>. However, this scheme duplicates each weight on two processors, increasing the work during weight updates. It can also be shown that the communication cost for inset grouping is the same as the communication cost incurred by the replication scheme, through the use of multiply-accumulate-rotate operation [11]. <p> Complete node partitioning and inset weight partitioning with pipelin-ing have been explored for linear arrays in [14]. The vertical-sectioning of nodes with duplicate inset/outset weight partitioning is used for mesh-connected transputers in <ref> [9, 10] </ref>. Mesh-based machines have been explored by many researchers. A matrix-matrix multiplication-based formulation is given in [21] for mesh architectures. Other schemes for mesh include those proposed in [13, 6, 16]. <p> The processors also contain the incoming weights associated with the nodes that reside in them. Each processor computes node activations, node errors, and weight changes for one pattern at a time. As an example, consider the scheme given in [11]. A similar scheme is given in <ref> [9, 10] </ref>.
Reference: [11] <author> Xiru Zhang. </author> <title> An efficient implementation of the backpropagation algorithm on the connection machine cm-2. </title> <type> Technical Report RL89-1, </type> <institution> Thinking Machines Corporation, </institution> <month> August 29 </month> <year> 1989. </year>
Reference-contexts: In pattern partitioning, individual weight changes due to various learning patterns are computed concurrently [17, 18, 19, 20]. Pattern partitioning and network partitioning can also be combined <ref> [6, 11, 12, 14, 20, 21] </ref> to form hybrid schemes. Several machine architectures including linear arrays, meshes and hypercubes have been explored to implement parallel BP. In this paper, we present a new technique for mapping the backpropagation algorithm on hy-percubes and related architectures. <p> In this paper, we present a new technique for mapping the backpropagation algorithm on hy-percubes and related architectures. A key component of this technique is a network partitioning scheme which is called checkerboarding. The major communication-intensive operation in the commonly used vertical sectioning scheme <ref> [6, 9, 10, 11, 12, 13, 14, 15, 17, 22] </ref> is the all-to-all broadcast. In this operation, each of the P processors has to broadcast its local m units of information to all the other processors. <p> Most previous parallel formulations of BP on hypercube or on related architectures used vertical sectioning [15], pattern partitioning [18, 19], or a hybrid of vertical sectioning and pattern partitioning <ref> [11, 12] </ref>. For hypercubes, the authors are aware of only one exception [8] in which each node and weight of the BP network is mapped onto a separate processor of CM2 T Mk . As discussed in [11, 12], this method incurs too much communication overhead. <p> pattern partitioning [18, 19], or a hybrid of vertical sectioning and pattern partitioning <ref> [11, 12] </ref>. For hypercubes, the authors are aware of only one exception [8] in which each node and weight of the BP network is mapped onto a separate processor of CM2 T Mk . As discussed in [11, 12], this method incurs too much communication overhead. The paper is organized as follows. Section 2 overviews the serial BP algorithm and its existing parallel formulations. Section 3 discusses existing parallel formulations of BP for hypercube and presents our new parallel formulations. <p> Vertical sectioning 3 Many researchers use alternative categories, namely classification and regression. We choose recognition and generalization categories to establish the dominance regions of alternative parallel formulations. 5 divides the nodes in a layer among the processors, and each processor gets some nodes from each layer <ref> [6, 9, 10, 11, 12, 13, 14, 15, 17, 22] </ref>. The weights can be partitioned in four different ways: complete partitioning, inset grouping, outset grouping and checkerboarding. Complete partitioning [8, 16] allocates one processor per weight in the network. <p> The inset of a node is allocated to the processor possessing the node. Inset grouping reduces the need for communication in computing node activations during the forward propagation. It has been used in <ref> [6, 11, 12, 14] </ref>. Outset grouping schemes form sets, which represent the collection of weights on the outgoing edges from a specific node. The outset of a node is allocated to a processor possessing the node. <p> However, this scheme duplicates each weight on two processors, increasing the work during weight updates. It can also be shown that the communication cost for inset grouping is the same as the communication cost incurred by the replication scheme, through the use of multiply-accumulate-rotate operation <ref> [11] </ref>. Checkerboard partitions the weights by grouping the rows and columns of the weight matrix. It has been used for systolic arrays [16], and for transputers [21] connected in a mesh configuration. Given enough processors, checkerboarding reduces to a complete partitioning of weights. <p> Mesh-based machines have been explored by many researchers. A matrix-matrix multiplication-based formulation is given in [21] for mesh architectures. Other schemes for mesh include those proposed in [13, 6, 16]. Techniques for hypercube and related architectures have been explored in <ref> [8, 11, 12, 15, 18, 19] </ref>. 6 3 Hypercubes and Backpropagation In a hypercube-connected parallel computer, each processor is directly connected to log (P ) other processors whose addresses differ by exactly one bit. <p> The processors also contain the incoming weights associated with the nodes that reside in them. Each processor computes node activations, node errors, and weight changes for one pattern at a time. As an example, consider the scheme given in <ref> [11] </ref>. A similar scheme is given in [9, 10].
Reference: [12] <author> Xiru Zhang and Michael Mckenna. </author> <title> The backpropagation algorithm on grid and hypercube architectures. </title> <type> Technical Report RL90-9, </type> <institution> Thinking Machines Corporation, </institution> <year> 1990. </year>
Reference-contexts: In pattern partitioning, individual weight changes due to various learning patterns are computed concurrently [17, 18, 19, 20]. Pattern partitioning and network partitioning can also be combined <ref> [6, 11, 12, 14, 20, 21] </ref> to form hybrid schemes. Several machine architectures including linear arrays, meshes and hypercubes have been explored to implement parallel BP. In this paper, we present a new technique for mapping the backpropagation algorithm on hy-percubes and related architectures. <p> In this paper, we present a new technique for mapping the backpropagation algorithm on hy-percubes and related architectures. A key component of this technique is a network partitioning scheme which is called checkerboarding. The major communication-intensive operation in the commonly used vertical sectioning scheme <ref> [6, 9, 10, 11, 12, 13, 14, 15, 17, 22] </ref> is the all-to-all broadcast. In this operation, each of the P processors has to broadcast its local m units of information to all the other processors. <p> Most previous parallel formulations of BP on hypercube or on related architectures used vertical sectioning [15], pattern partitioning [18, 19], or a hybrid of vertical sectioning and pattern partitioning <ref> [11, 12] </ref>. For hypercubes, the authors are aware of only one exception [8] in which each node and weight of the BP network is mapped onto a separate processor of CM2 T Mk . As discussed in [11, 12], this method incurs too much communication overhead. <p> pattern partitioning [18, 19], or a hybrid of vertical sectioning and pattern partitioning <ref> [11, 12] </ref>. For hypercubes, the authors are aware of only one exception [8] in which each node and weight of the BP network is mapped onto a separate processor of CM2 T Mk . As discussed in [11, 12], this method incurs too much communication overhead. The paper is organized as follows. Section 2 overviews the serial BP algorithm and its existing parallel formulations. Section 3 discusses existing parallel formulations of BP for hypercube and presents our new parallel formulations. <p> Vertical sectioning 3 Many researchers use alternative categories, namely classification and regression. We choose recognition and generalization categories to establish the dominance regions of alternative parallel formulations. 5 divides the nodes in a layer among the processors, and each processor gets some nodes from each layer <ref> [6, 9, 10, 11, 12, 13, 14, 15, 17, 22] </ref>. The weights can be partitioned in four different ways: complete partitioning, inset grouping, outset grouping and checkerboarding. Complete partitioning [8, 16] allocates one processor per weight in the network. <p> The inset of a node is allocated to the processor possessing the node. Inset grouping reduces the need for communication in computing node activations during the forward propagation. It has been used in <ref> [6, 11, 12, 14] </ref>. Outset grouping schemes form sets, which represent the collection of weights on the outgoing edges from a specific node. The outset of a node is allocated to a processor possessing the node. <p> Even though pipelining partitions the network horizontally, it is more appropriate to view it as pattern partitioning. Hybrid schemes combine pattern partitioning with network partitioning. For example, pipelin-ing can be combined with vertical sectioning [6, 14]. Other examples include the combination of vertical sectioning with simple pattern partitioning <ref> [12] </ref>. We can view the forward and backward phase computations for a pattern as a sequence of weight-matrix to activation/error-vector products. The hybrid schemes that process multiple patterns simultaneously can be viewed as a sequence of matrix-matrix products. <p> Mesh-based machines have been explored by many researchers. A matrix-matrix multiplication-based formulation is given in [21] for mesh architectures. Other schemes for mesh include those proposed in [13, 6, 16]. Techniques for hypercube and related architectures have been explored in <ref> [8, 11, 12, 15, 18, 19] </ref>. 6 3 Hypercubes and Backpropagation In a hypercube-connected parallel computer, each processor is directly connected to log (P ) other processors whose addresses differ by exactly one bit. <p> Hence, in this paper set-training iteration has been used as the benchmark to compare pattern partitioning and network partitioning of BP as is done by <ref> [12, 21] </ref> among other papers. One full-iteration of the set-training regime is used as the benchmark task to compare the alternative parallel formulations. <p> The serial time refers to the execution time of the benchmark task on a single processor. We provide a translation of the speedup measurements into weight updates per second and connections per second to be able to compare our results to related results <ref> [12, 22] </ref> in the literature. Parallel formulations speed up the execution of BP by accelerating the execution of each iteration.
Reference: [13] <author> F. Baiardi, R. Mussard, R. Serr, and G. Valastro. </author> <title> Feedforward neural networks on message passing parallel computers. </title> <booktitle> In Proc. 2nd Italian Workshop on Parallel Architectures and Neural Networks. </booktitle> <editor> (Ed. E. R. </editor> <publisher> Caianielo) World Scientific, </publisher> <address> Singapore, </address> <year> 1990. </year> <month> 20 </month>
Reference-contexts: In this paper, we present a new technique for mapping the backpropagation algorithm on hy-percubes and related architectures. A key component of this technique is a network partitioning scheme which is called checkerboarding. The major communication-intensive operation in the commonly used vertical sectioning scheme <ref> [6, 9, 10, 11, 12, 13, 14, 15, 17, 22] </ref> is the all-to-all broadcast. In this operation, each of the P processors has to broadcast its local m units of information to all the other processors. <p> Vertical sectioning 3 Many researchers use alternative categories, namely classification and regression. We choose recognition and generalization categories to establish the dominance regions of alternative parallel formulations. 5 divides the nodes in a layer among the processors, and each processor gets some nodes from each layer <ref> [6, 9, 10, 11, 12, 13, 14, 15, 17, 22] </ref>. The weights can be partitioned in four different ways: complete partitioning, inset grouping, outset grouping and checkerboarding. Complete partitioning [8, 16] allocates one processor per weight in the network. <p> Outset grouping eliminates the need for communication to compute error vectors during the backward phase. It has been used in [15]. Both inset and outset grouping can be used together to improve the efficiency of both forward and backward phases <ref> [9, 10, 13] </ref>. However, this scheme duplicates each weight on two processors, increasing the work during weight updates. It can also be shown that the communication cost for inset grouping is the same as the communication cost incurred by the replication scheme, through the use of multiply-accumulate-rotate operation [11]. <p> The vertical-sectioning of nodes with duplicate inset/outset weight partitioning is used for mesh-connected transputers in [9, 10]. Mesh-based machines have been explored by many researchers. A matrix-matrix multiplication-based formulation is given in [21] for mesh architectures. Other schemes for mesh include those proposed in <ref> [13, 6, 16] </ref>. Techniques for hypercube and related architectures have been explored in [8, 11, 12, 15, 18, 19]. 6 3 Hypercubes and Backpropagation In a hypercube-connected parallel computer, each processor is directly connected to log (P ) other processors whose addresses differ by exactly one bit.
Reference: [14] <author> M. Marchesi, G. Orlandi, F. Piazza, and A. Uncini. </author> <title> Linear array architecture implementing the back-propagation neural network. </title> <booktitle> In Proc. 2nd Italian Workshop on Parallel Architectures and Neural Networks. </booktitle> <editor> (Ed. E. R. </editor> <publisher> Caianielo) World Scientific, </publisher> <address> Singapore, </address> <year> 1990. </year>
Reference-contexts: In pattern partitioning, individual weight changes due to various learning patterns are computed concurrently [17, 18, 19, 20]. Pattern partitioning and network partitioning can also be combined <ref> [6, 11, 12, 14, 20, 21] </ref> to form hybrid schemes. Several machine architectures including linear arrays, meshes and hypercubes have been explored to implement parallel BP. In this paper, we present a new technique for mapping the backpropagation algorithm on hy-percubes and related architectures. <p> In this paper, we present a new technique for mapping the backpropagation algorithm on hy-percubes and related architectures. A key component of this technique is a network partitioning scheme which is called checkerboarding. The major communication-intensive operation in the commonly used vertical sectioning scheme <ref> [6, 9, 10, 11, 12, 13, 14, 15, 17, 22] </ref> is the all-to-all broadcast. In this operation, each of the P processors has to broadcast its local m units of information to all the other processors. <p> Vertical sectioning 3 Many researchers use alternative categories, namely classification and regression. We choose recognition and generalization categories to establish the dominance regions of alternative parallel formulations. 5 divides the nodes in a layer among the processors, and each processor gets some nodes from each layer <ref> [6, 9, 10, 11, 12, 13, 14, 15, 17, 22] </ref>. The weights can be partitioned in four different ways: complete partitioning, inset grouping, outset grouping and checkerboarding. Complete partitioning [8, 16] allocates one processor per weight in the network. <p> The inset of a node is allocated to the processor possessing the node. Inset grouping reduces the need for communication in computing node activations during the forward propagation. It has been used in <ref> [6, 11, 12, 14] </ref>. Outset grouping schemes form sets, which represent the collection of weights on the outgoing edges from a specific node. The outset of a node is allocated to a processor possessing the node. <p> This scheme is preferred for problems with a large set of learning patterns on machines supporting efficient broadcast operation. This scheme has been used in [18, 19, 17, 20]. The computation in different layers of BP can be pipelined <ref> [6, 14] </ref>; i.e., while one pattern is being processed for some layer, a different pattern can be processed for a preceding layer. Even though pipelining partitions the network horizontally, it is more appropriate to view it as pattern partitioning. Hybrid schemes combine pattern partitioning with network partitioning. <p> Even though pipelining partitions the network horizontally, it is more appropriate to view it as pattern partitioning. Hybrid schemes combine pattern partitioning with network partitioning. For example, pipelin-ing can be combined with vertical sectioning <ref> [6, 14] </ref>. Other examples include the combination of vertical sectioning with simple pattern partitioning [12]. We can view the forward and backward phase computations for a pattern as a sequence of weight-matrix to activation/error-vector products. <p> An orthogonal way of classifying the approaches to parallel BP is based on the architectures on which they are implemented. Complete node partitioning and inset weight partitioning with pipelin-ing have been explored for linear arrays in <ref> [14] </ref>. The vertical-sectioning of nodes with duplicate inset/outset weight partitioning is used for mesh-connected transputers in [9, 10]. Mesh-based machines have been explored by many researchers. A matrix-matrix multiplication-based formulation is given in [21] for mesh architectures. Other schemes for mesh include those proposed in [13, 6, 16].
Reference: [15] <author> D. S. Newhall and J. C. Horvath. </author> <title> Analysis of text using a neural network: A hypercube implementation. </title> <booktitle> In Proceedings of the 1989 Conference on Hypercubes, Concurrent Computers and Applications, </booktitle> <pages> pages 1119-1122, </pages> <year> 1989. </year>
Reference-contexts: In this paper, we present a new technique for mapping the backpropagation algorithm on hy-percubes and related architectures. A key component of this technique is a network partitioning scheme which is called checkerboarding. The major communication-intensive operation in the commonly used vertical sectioning scheme <ref> [6, 9, 10, 11, 12, 13, 14, 15, 17, 22] </ref> is the all-to-all broadcast. In this operation, each of the P processors has to broadcast its local m units of information to all the other processors. <p> The communication pattern used in our scheme is significantly different from those of other checkerboarding schemes [16, 21], leading to improved performance on hypercubes -. Most previous parallel formulations of BP on hypercube or on related architectures used vertical sectioning <ref> [15] </ref>, pattern partitioning [18, 19], or a hybrid of vertical sectioning and pattern partitioning [11, 12]. For hypercubes, the authors are aware of only one exception [8] in which each node and weight of the BP network is mapped onto a separate processor of CM2 T Mk . <p> Vertical sectioning 3 Many researchers use alternative categories, namely classification and regression. We choose recognition and generalization categories to establish the dominance regions of alternative parallel formulations. 5 divides the nodes in a layer among the processors, and each processor gets some nodes from each layer <ref> [6, 9, 10, 11, 12, 13, 14, 15, 17, 22] </ref>. The weights can be partitioned in four different ways: complete partitioning, inset grouping, outset grouping and checkerboarding. Complete partitioning [8, 16] allocates one processor per weight in the network. <p> The outset of a node is allocated to a processor possessing the node. Outset grouping eliminates the need for communication to compute error vectors during the backward phase. It has been used in <ref> [15] </ref>. Both inset and outset grouping can be used together to improve the efficiency of both forward and backward phases [9, 10, 13]. However, this scheme duplicates each weight on two processors, increasing the work during weight updates. <p> Mesh-based machines have been explored by many researchers. A matrix-matrix multiplication-based formulation is given in [21] for mesh architectures. Other schemes for mesh include those proposed in [13, 6, 16]. Techniques for hypercube and related architectures have been explored in <ref> [8, 11, 12, 15, 18, 19] </ref>. 6 3 Hypercubes and Backpropagation In a hypercube-connected parallel computer, each processor is directly connected to log (P ) other processors whose addresses differ by exactly one bit.
Reference: [16] <author> S. Y. Kung and J.N. Hwang. </author> <title> A unified systolic architecture for artificial neural networks. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 6 </volume> <pages> 358-387, </pages> <year> 1989. </year>
Reference-contexts: A summary of results of this paper appear in [23]. The checkerboarding scheme has been used in other parallel formulations of BP for the mesh architecture [21] and systolic arrays <ref> [16] </ref>. But if these formulations are mapped onto hypercube, then their performance approaches that of vertical sectioning (in the case of [16]) or the hybrid of vertical sectioning and pattern partitioning (in the case of [21]). <p> The checkerboarding scheme has been used in other parallel formulations of BP for the mesh architecture [21] and systolic arrays <ref> [16] </ref>. But if these formulations are mapped onto hypercube, then their performance approaches that of vertical sectioning (in the case of [16]) or the hybrid of vertical sectioning and pattern partitioning (in the case of [21]). The communication pattern used in our scheme is significantly different from those of other checkerboarding schemes [16, 21], leading to improved performance on hypercubes -. <p> The communication pattern used in our scheme is significantly different from those of other checkerboarding schemes <ref> [16, 21] </ref>, leading to improved performance on hypercubes -. Most previous parallel formulations of BP on hypercube or on related architectures used vertical sectioning [15], pattern partitioning [18, 19], or a hybrid of vertical sectioning and pattern partitioning [11, 12]. <p> Section 6 shows the performance of the proposed schemes on non-uniform networks. Section 7 considers scalability and optimality issues. Section 8 contains concluding remarks. x iPSC is a trademark of the Intel Scientific Computers In Kung's scheme <ref> [16] </ref>, multiplication of a I fi I matrix to a I fi 1 vector takes fl (I) time on a I fi I mesh-connected systolic array. <p> This is possible only with the set-training regime, and is most effective when the number of training patterns is fl (I) (where I is the number of inputs). In particular, with the per-pattern training regime, Kung's scheme <ref> [16] </ref> is strictly inferior to our scheme on hypercube, as well as to adaptation of our scheme on a mesh computer with worm-hole routing. k CM2 is a trademark of the Thinking Machines corporation 2 2 Backpropagation Learning Algorithm BP trains a given feedforward neural network for a given set of <p> The weights can be partitioned in four different ways: complete partitioning, inset grouping, outset grouping and checkerboarding. Complete partitioning <ref> [8, 16] </ref> allocates one processor per weight in the network. It allows maximum concurrency in the computation of the various terms of the node activation and node error. However, it requires communication to accumulate the terms. Inset and outset grouping schemes are often used in conjunction with the vertical-sectioning scheme. <p> Checkerboard partitions the weights by grouping the rows and columns of the weight matrix. It has been used for systolic arrays <ref> [16] </ref>, and for transputers [21] connected in a mesh configuration. Given enough processors, checkerboarding reduces to a complete partitioning of weights. Pattern partitioning replicates the network nodes and weights at each processor. It divides the pattern set equally among all processors. <p> The vertical-sectioning of nodes with duplicate inset/outset weight partitioning is used for mesh-connected transputers in [9, 10]. Mesh-based machines have been explored by many researchers. A matrix-matrix multiplication-based formulation is given in [21] for mesh architectures. Other schemes for mesh include those proposed in <ref> [13, 6, 16] </ref>. Techniques for hypercube and related architectures have been explored in [8, 11, 12, 15, 18, 19]. 6 3 Hypercubes and Backpropagation In a hypercube-connected parallel computer, each processor is directly connected to log (P ) other processors whose addresses differ by exactly one bit.
Reference: [17] <author> K. Joe, Y. Mori, and S. Miyake. </author> <title> Simulation of a large-scale neural network on a parallel computer. </title> <booktitle> In Proceedings of the 1989 Conference on Hypercubes, Concurrent Computers and Applications, </booktitle> <pages> pages 1111-1118, </pages> <year> 1989. </year>
Reference-contexts: In pattern partitioning, individual weight changes due to various learning patterns are computed concurrently <ref> [17, 18, 19, 20] </ref>. Pattern partitioning and network partitioning can also be combined [6, 11, 12, 14, 20, 21] to form hybrid schemes. Several machine architectures including linear arrays, meshes and hypercubes have been explored to implement parallel BP. <p> In this paper, we present a new technique for mapping the backpropagation algorithm on hy-percubes and related architectures. A key component of this technique is a network partitioning scheme which is called checkerboarding. The major communication-intensive operation in the commonly used vertical sectioning scheme <ref> [6, 9, 10, 11, 12, 13, 14, 15, 17, 22] </ref> is the all-to-all broadcast. In this operation, each of the P processors has to broadcast its local m units of information to all the other processors. <p> Vertical sectioning 3 Many researchers use alternative categories, namely classification and regression. We choose recognition and generalization categories to establish the dominance regions of alternative parallel formulations. 5 divides the nodes in a layer among the processors, and each processor gets some nodes from each layer <ref> [6, 9, 10, 11, 12, 13, 14, 15, 17, 22] </ref>. The weights can be partitioned in four different ways: complete partitioning, inset grouping, outset grouping and checkerboarding. Complete partitioning [8, 16] allocates one processor per weight in the network. <p> Each processor also accumulates the weight changes according to the local patterns. Then, the processors communicate to accumulate the weight changes for updating weights. This scheme is preferred for problems with a large set of learning patterns on machines supporting efficient broadcast operation. This scheme has been used in <ref> [18, 19, 17, 20] </ref>. The computation in different layers of BP can be pipelined [6, 14]; i.e., while one pattern is being processed for some layer, a different pattern can be processed for a preceding layer.
Reference: [18] <author> M. Witbrock and M. Zagha. </author> <title> An implementation of back-propagation learning on gf11, a large simd parallel computer. </title> <type> Technical Report CMU-CS-89-208, CMU, </type> <month> December </month> <year> 1989. </year>
Reference-contexts: In pattern partitioning, individual weight changes due to various learning patterns are computed concurrently <ref> [17, 18, 19, 20] </ref>. Pattern partitioning and network partitioning can also be combined [6, 11, 12, 14, 20, 21] to form hybrid schemes. Several machine architectures including linear arrays, meshes and hypercubes have been explored to implement parallel BP. <p> The communication pattern used in our scheme is significantly different from those of other checkerboarding schemes [16, 21], leading to improved performance on hypercubes -. Most previous parallel formulations of BP on hypercube or on related architectures used vertical sectioning [15], pattern partitioning <ref> [18, 19] </ref>, or a hybrid of vertical sectioning and pattern partitioning [11, 12]. For hypercubes, the authors are aware of only one exception [8] in which each node and weight of the BP network is mapped onto a separate processor of CM2 T Mk . <p> Each processor also accumulates the weight changes according to the local patterns. Then, the processors communicate to accumulate the weight changes for updating weights. This scheme is preferred for problems with a large set of learning patterns on machines supporting efficient broadcast operation. This scheme has been used in <ref> [18, 19, 17, 20] </ref>. The computation in different layers of BP can be pipelined [6, 14]; i.e., while one pattern is being processed for some layer, a different pattern can be processed for a preceding layer. <p> Mesh-based machines have been explored by many researchers. A matrix-matrix multiplication-based formulation is given in [21] for mesh architectures. Other schemes for mesh include those proposed in [13, 6, 16]. Techniques for hypercube and related architectures have been explored in <ref> [8, 11, 12, 15, 18, 19] </ref>. 6 3 Hypercubes and Backpropagation In a hypercube-connected parallel computer, each processor is directly connected to log (P ) other processors whose addresses differ by exactly one bit.
Reference: [19] <author> J. Bourrley. </author> <title> Parallelization of a neural learning algorithm. </title> <booktitle> In Proc. 1st Euro. Workshop on Hypercube and Distributed Computers. </booktitle> <editor> (Ed. F. Andre) North-Holland, </editor> <year> 1989. </year>
Reference-contexts: In pattern partitioning, individual weight changes due to various learning patterns are computed concurrently <ref> [17, 18, 19, 20] </ref>. Pattern partitioning and network partitioning can also be combined [6, 11, 12, 14, 20, 21] to form hybrid schemes. Several machine architectures including linear arrays, meshes and hypercubes have been explored to implement parallel BP. <p> The communication pattern used in our scheme is significantly different from those of other checkerboarding schemes [16, 21], leading to improved performance on hypercubes -. Most previous parallel formulations of BP on hypercube or on related architectures used vertical sectioning [15], pattern partitioning <ref> [18, 19] </ref>, or a hybrid of vertical sectioning and pattern partitioning [11, 12]. For hypercubes, the authors are aware of only one exception [8] in which each node and weight of the BP network is mapped onto a separate processor of CM2 T Mk . <p> Each processor also accumulates the weight changes according to the local patterns. Then, the processors communicate to accumulate the weight changes for updating weights. This scheme is preferred for problems with a large set of learning patterns on machines supporting efficient broadcast operation. This scheme has been used in <ref> [18, 19, 17, 20] </ref>. The computation in different layers of BP can be pipelined [6, 14]; i.e., while one pattern is being processed for some layer, a different pattern can be processed for a preceding layer. <p> Mesh-based machines have been explored by many researchers. A matrix-matrix multiplication-based formulation is given in [21] for mesh architectures. Other schemes for mesh include those proposed in [13, 6, 16]. Techniques for hypercube and related architectures have been explored in <ref> [8, 11, 12, 15, 18, 19] </ref>. 6 3 Hypercubes and Backpropagation In a hypercube-connected parallel computer, each processor is directly connected to log (P ) other processors whose addresses differ by exactly one bit.
Reference: [20] <author> B. K. Mak and O. Egecioglu. </author> <title> Communication parameter tests and parallel backpropagation algorithms on ipsc/2 hypercube multiprocessor. </title> <booktitle> In IEEE Frontier, </booktitle> <pages> pages 1353-1364, </pages> <year> 1990. </year>
Reference-contexts: In pattern partitioning, individual weight changes due to various learning patterns are computed concurrently <ref> [17, 18, 19, 20] </ref>. Pattern partitioning and network partitioning can also be combined [6, 11, 12, 14, 20, 21] to form hybrid schemes. Several machine architectures including linear arrays, meshes and hypercubes have been explored to implement parallel BP. <p> In pattern partitioning, individual weight changes due to various learning patterns are computed concurrently [17, 18, 19, 20]. Pattern partitioning and network partitioning can also be combined <ref> [6, 11, 12, 14, 20, 21] </ref> to form hybrid schemes. Several machine architectures including linear arrays, meshes and hypercubes have been explored to implement parallel BP. In this paper, we present a new technique for mapping the backpropagation algorithm on hy-percubes and related architectures. <p> Each processor also accumulates the weight changes according to the local patterns. Then, the processors communicate to accumulate the weight changes for updating weights. This scheme is preferred for problems with a large set of learning patterns on machines supporting efficient broadcast operation. This scheme has been used in <ref> [18, 19, 17, 20] </ref>. The computation in different layers of BP can be pipelined [6, 14]; i.e., while one pattern is being processed for some layer, a different pattern can be processed for a preceding layer.
Reference: [21] <author> A. Petrowski, L. Personnaz, G. Dreyfus, and C. Girault. </author> <title> Parallel implementations of neural network simulations. </title> <booktitle> In First European Workshop on hypercube and distributed computers, </booktitle> <pages> pages 205-218. </pages> <publisher> Elsevier Science Publishers B.V. (North-Holland), </publisher> <year> 1989. </year>
Reference-contexts: In pattern partitioning, individual weight changes due to various learning patterns are computed concurrently [17, 18, 19, 20]. Pattern partitioning and network partitioning can also be combined <ref> [6, 11, 12, 14, 20, 21] </ref> to form hybrid schemes. Several machine architectures including linear arrays, meshes and hypercubes have been explored to implement parallel BP. In this paper, we present a new technique for mapping the backpropagation algorithm on hy-percubes and related architectures. <p> A summary of results of this paper appear in [23]. The checkerboarding scheme has been used in other parallel formulations of BP for the mesh architecture <ref> [21] </ref> and systolic arrays [16]. But if these formulations are mapped onto hypercube, then their performance approaches that of vertical sectioning (in the case of [16]) or the hybrid of vertical sectioning and pattern partitioning (in the case of [21]). <p> used in other parallel formulations of BP for the mesh architecture <ref> [21] </ref> and systolic arrays [16]. But if these formulations are mapped onto hypercube, then their performance approaches that of vertical sectioning (in the case of [16]) or the hybrid of vertical sectioning and pattern partitioning (in the case of [21]). The communication pattern used in our scheme is significantly different from those of other checkerboarding schemes [16, 21], leading to improved performance on hypercubes -. <p> The communication pattern used in our scheme is significantly different from those of other checkerboarding schemes <ref> [16, 21] </ref>, leading to improved performance on hypercubes -. Most previous parallel formulations of BP on hypercube or on related architectures used vertical sectioning [15], pattern partitioning [18, 19], or a hybrid of vertical sectioning and pattern partitioning [11, 12]. <p> Checkerboard partitions the weights by grouping the rows and columns of the weight matrix. It has been used for systolic arrays [16], and for transputers <ref> [21] </ref> connected in a mesh configuration. Given enough processors, checkerboarding reduces to a complete partitioning of weights. Pattern partitioning replicates the network nodes and weights at each processor. It divides the pattern set equally among all processors. <p> The hybrid schemes that process multiple patterns simultaneously can be viewed as a sequence of matrix-matrix products. This allows the possibility of using parallel algorithms for computing a sequence of matrix operations <ref> [30, 21] </ref>. An orthogonal way of classifying the approaches to parallel BP is based on the architectures on which they are implemented. Complete node partitioning and inset weight partitioning with pipelin-ing have been explored for linear arrays in [14]. <p> Complete node partitioning and inset weight partitioning with pipelin-ing have been explored for linear arrays in [14]. The vertical-sectioning of nodes with duplicate inset/outset weight partitioning is used for mesh-connected transputers in [9, 10]. Mesh-based machines have been explored by many researchers. A matrix-matrix multiplication-based formulation is given in <ref> [21] </ref> for mesh architectures. Other schemes for mesh include those proposed in [13, 6, 16]. <p> Hence, in this paper set-training iteration has been used as the benchmark to compare pattern partitioning and network partitioning of BP as is done by <ref> [12, 21] </ref> among other papers. One full-iteration of the set-training regime is used as the benchmark task to compare the alternative parallel formulations.
Reference: [22] <author> D.A.Pomerleau G.L.Gusciora, H.T.Kung and D.S.Touretzky. </author> <title> Neural network simulation at warp speed: How we got 17 million connections per second. </title> <booktitle> In IEEE 2nd Intl. Conf. on Neural Networks, </booktitle> <pages> pages 143-50, </pages> <year> 1988. </year>
Reference-contexts: In this paper, we present a new technique for mapping the backpropagation algorithm on hy-percubes and related architectures. A key component of this technique is a network partitioning scheme which is called checkerboarding. The major communication-intensive operation in the commonly used vertical sectioning scheme <ref> [6, 9, 10, 11, 12, 13, 14, 15, 17, 22] </ref> is the all-to-all broadcast. In this operation, each of the P processors has to broadcast its local m units of information to all the other processors. <p> Vertical sectioning 3 Many researchers use alternative categories, namely classification and regression. We choose recognition and generalization categories to establish the dominance regions of alternative parallel formulations. 5 divides the nodes in a layer among the processors, and each processor gets some nodes from each layer <ref> [6, 9, 10, 11, 12, 13, 14, 15, 17, 22] </ref>. The weights can be partitioned in four different ways: complete partitioning, inset grouping, outset grouping and checkerboarding. Complete partitioning [8, 16] allocates one processor per weight in the network. <p> The serial time refers to the execution time of the benchmark task on a single processor. We provide a translation of the speedup measurements into weight updates per second and connections per second to be able to compare our results to related results <ref> [12, 22] </ref> in the literature. Parallel formulations speed up the execution of BP by accelerating the execution of each iteration.
Reference: [23] <author> V. Kumar, S. Shekhar, and M. B. Amin. </author> <title> A highly parallel formulation of backpropagation on hyper-cubes: A summary of results. </title> <booktitle> In Proc. Intl. Conf. on Neural Networks (IJCNN), </booktitle> <month> November </month> <year> 1992. </year>
Reference-contexts: For example, an unoptimized version of our hybrid parallel formulation on a 256 processor CM5 (without vector units) performs over 50 million weight changes per second (or over 160 million connections per second) for nonuniform networks. A summary of results of this paper appear in <ref> [23] </ref>. The checkerboarding scheme has been used in other parallel formulations of BP for the mesh architecture [21] and systolic arrays [16].
Reference: [24] <author> D. E. Rumelhart J. L. McClelland. </author> <title> Explorations in Parallel Distributed Processing. </title> <publisher> MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: Many neural network software (e.g. <ref> [24] </ref>) support an alternative training regime, called per-pattern training. In this training regime, the set of patterns are permuted to create an ordering among the learning patterns. The patterns are then examined in sequence. <p> Robert Benner of Sandia labs for giving access to nCUBE2. Anonymous refrees provided valuable comments to improve the presentation. Ms. Christine McCarthy, Dept. of Composition, University of Minnesota provided editorial help to improve grammatical and stylistic aspects. We also thank <ref> [24] </ref>, which was an excellent source on BP.
Reference: [25] <author> G. L. Wilcox, M. Poliac, and M. Liebman. </author> <title> Protein tertiary structure prediction using a large backpropagation network. </title> <booktitle> In IJCNN (IEEE/EEC), </booktitle> <pages> pages 365-369, </pages> <year> 1990. </year>
Reference-contexts: In recognition problems, a set of learning patterns are used to train the network. The trained network is expected to recall the output part of a previously seen learning pattern, when the input portion of the same learning pattern is presented. Pattern recognition applications <ref> [25] </ref> are examples of recognition problems. On the other hand, in generalization problems neural networks are tested with input portions of new testing patterns. The network is expected to predict the output portions of the testing pattern, which have not been shown to the network during the learning phase. <p> Generalization problems include assignment of bond-rating [4] and economic prediction [5]. Usually a large neural network is used in recognition problems. Furthermore, recognition problems have a small number of learning samples. For example, in the protein-folding problem, L = 14 and I = 1000 <ref> [25] </ref>. The recognition problems can be characterized by L o I 2 . In contrast, generalization problems are characterized by a large number of learning samples. The number of learning samples should be an order of magnitude larger than the number of weights to be learned. <p> Non-uniform networks are interesting, since these are used in many applications <ref> [2, 25] </ref>. Let I 0 ; I 1 ; : : :I J be the number of nodes in the different layers of the network. The network is uniform iff I 0 = I 1 = = I J ; otherwise, the network is called a non-uniform network.
Reference: [26] <author> B. W. Wah and L. Chu. </author> <title> Efficient mapping of neural networks on multicomputers. </title> <booktitle> IEEE International Conference on Parallel Processing, </booktitle> <pages> pages 234-241, </pages> <year> 1990. </year>
Reference-contexts: In these networks, the adjacent layers are completely connected. For schemes that primarily deal with randomly sparse networks, see <ref> [7, 26, 27, 28, 29] </ref>. Parallelization schemes for BP can be classified into three broad categories: network partitioning schemes, pattern partitioning schemes, and hybrid schemes. <p> We would also like to generalize our schemes by utilizing different lengths and breadths of processor grids for different layers of the network. It would also be interesting to compare the parallelization techniques for randomly sparse networks <ref> [7, 26, 27, 28, 29] </ref>. with our scheme for non-uniform networks. 9 Acknowledgement We would like to thank Prof. Joydeep Ghosh (University of Texas at Austin), George Karypis and Ananth Grama for their useful comments during the drafting of this paper, and Dr.
Reference: [27] <author> M. Misra and V. K. Prasanna Kumar. </author> <title> Neural network simulation on a reduced mesh of tree organization. </title> <booktitle> In SPIE/SPSE Symposium on Electronic Images, </booktitle> <year> 1990. </year>
Reference-contexts: In these networks, the adjacent layers are completely connected. For schemes that primarily deal with randomly sparse networks, see <ref> [7, 26, 27, 28, 29] </ref>. Parallelization schemes for BP can be classified into three broad categories: network partitioning schemes, pattern partitioning schemes, and hybrid schemes. <p> We would also like to generalize our schemes by utilizing different lengths and breadths of processor grids for different layers of the network. It would also be interesting to compare the parallelization techniques for randomly sparse networks <ref> [7, 26, 27, 28, 29] </ref>. with our scheme for non-uniform networks. 9 Acknowledgement We would like to thank Prof. Joydeep Ghosh (University of Texas at Austin), George Karypis and Ananth Grama for their useful comments during the drafting of this paper, and Dr.
Reference: [28] <author> M. Misra. </author> <title> Implementation of neural networks on parallel architectures. </title> <type> In Tech. Report 295. </type> <institution> Dept. of EE Systems hi, Univ. of Soutern California, </institution> <year> 1992. </year>
Reference-contexts: In these networks, the adjacent layers are completely connected. For schemes that primarily deal with randomly sparse networks, see <ref> [7, 26, 27, 28, 29] </ref>. Parallelization schemes for BP can be classified into three broad categories: network partitioning schemes, pattern partitioning schemes, and hybrid schemes. <p> We would also like to generalize our schemes by utilizing different lengths and breadths of processor grids for different layers of the network. It would also be interesting to compare the parallelization techniques for randomly sparse networks <ref> [7, 26, 27, 28, 29] </ref>. with our scheme for non-uniform networks. 9 Acknowledgement We would like to thank Prof. Joydeep Ghosh (University of Texas at Austin), George Karypis and Ananth Grama for their useful comments during the drafting of this paper, and Dr.
Reference: [29] <author> J. Ghosh and K. Hwang. </author> <title> Mapping neural networks onto message passing multicomputers. </title> <editor> Jr. </editor> <booktitle> Parallel and Distributed Computing, </booktitle> <month> April </month> <year> 1989. </year>
Reference-contexts: In these networks, the adjacent layers are completely connected. For schemes that primarily deal with randomly sparse networks, see <ref> [7, 26, 27, 28, 29] </ref>. Parallelization schemes for BP can be classified into three broad categories: network partitioning schemes, pattern partitioning schemes, and hybrid schemes. <p> We would also like to generalize our schemes by utilizing different lengths and breadths of processor grids for different layers of the network. It would also be interesting to compare the parallelization techniques for randomly sparse networks <ref> [7, 26, 27, 28, 29] </ref>. with our scheme for non-uniform networks. 9 Acknowledgement We would like to thank Prof. Joydeep Ghosh (University of Texas at Austin), George Karypis and Ananth Grama for their useful comments during the drafting of this paper, and Dr.
Reference: [30] <author> Anshul Gupta and Vipin Kumar. </author> <title> On the scalability of Matrix Multiplication Algorithms on parallel computers. </title> <type> Technical Report TR 91-54, </type> <institution> Computer Science Department, University of Minnesota, </institution> <address> Minneapolis, MN 55455, </address> <year> 1991. </year> <note> Revised, </note> <month> Sep. </month> <year> 1992. </year>
Reference-contexts: The hybrid schemes that process multiple patterns simultaneously can be viewed as a sequence of matrix-matrix products. This allows the possibility of using parallel algorithms for computing a sequence of matrix operations <ref> [30, 21] </ref>. An orthogonal way of classifying the approaches to parallel BP is based on the architectures on which they are implemented. Complete node partitioning and inset weight partitioning with pipelin-ing have been explored for linear arrays in [14].
Reference: [31] <author> D. P. Bertsekas and J. N. </author> <title> Tsitsiklis. </title> <booktitle> Parallel and Distributed Computation, </booktitle> <pages> pages 50-55. </pages> <publisher> Prentice-Hall Inc, </publisher> <year> 1989. </year>
Reference-contexts: A message containing m words can be broadcast from a processor to all other processors in time (t s + t w m) fl log (P ), as a binary tree of depth log (P ) can be mapped on to a hypercube containing P processors <ref> [31] </ref> 4 . <p> These can be used to improve the performance of our new scheme and pattern partitioning. For simplicity, we only consider the standard broadcasting algorithm from <ref> [31] </ref>. The relative performance of different schemes will be similar even with the new broadcast algorithms. 5 In steps 2 and 3, when weight changes are accumulated and reset, an addition is done for each weight. <p> patterns is: Parallel Runtime = L JI 2 t c + 2LJP (t s + P 6 All-to-all broadcast can be carried out in fl ( P log (P) [t s + I P t w ]) on a hypercube if simultaneous communication along all channels of hypercube is allowed <ref> [31] </ref>. 8 3.3 Our Parallel Formulation In the vertical-sectioning schemes, one processor performs the computation related to one or more nodes in each layer. In contrast, in our scheme, a set of processors is used to perform these operations. <p> This increases the degree of concurrency of our scheme over the vertical-sectioning scheme. Our formulation is essentially an adaptation of the matrix vector multiplication scheme for hypercube presented in <ref> [31] </ref>. ? * ? ffi - 14151312 23 10 Our scheme uses a well known embedding [34] of a p p P grid of processor on a P (= 2 2k ) processor hypercube.
Reference: [32] <author> S. Lennart Johnsson and Ching-Tien Ho. </author> <title> Optimum broadcasting and personalized communication in hypercubes. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(9):1249 - 1268, </volume> <month> September </month> <year> 1989. </year>
Reference-contexts: Hence, the parallel runtime for the BP algorithm for one iteration and for L patterns is given by: L The main features of this scheme are as follows: 4 Johnson and Ho present faster broadcast algorithms for hypercubes in <ref> [32] </ref>. These can be used to improve the performance of our new scheme and pattern partitioning. For simplicity, we only consider the standard broadcasting algorithm from [31].
Reference: [33] <author> Y. Saad and M. H. Schultz. </author> <title> Topological properties of hypercubes. </title> <journal> IEEE Trans. on Computers, </journal> <pages> pages 867-872, </pages> <year> 1988. </year>
Reference-contexts: In the forward phase, to compute activation values for any node i in layer j, it is necessary to have the activation values of all nodes from layer (j 1). Thus, the all-to-all broadcast operation <ref> [33] </ref>, in which each processor needs to send its I P activations to all other P processors, needs to be performed. This operation takes P [t s + I P t w ] for each of the J layers.
Reference: [34] <author> J. Jenq and S. Sahni. </author> <title> All pairs shortest paths on a hypercube multiprocessor. </title> <booktitle> In Int. Conf. on Parallel Processing, </booktitle> <pages> pages 713-716, </pages> <year> 1987. </year>
Reference-contexts: This increases the degree of concurrency of our scheme over the vertical-sectioning scheme. Our formulation is essentially an adaptation of the matrix vector multiplication scheme for hypercube presented in [31]. ? * ? ffi - 14151312 23 10 Our scheme uses a well known embedding <ref> [34] </ref> of a p p P grid of processor on a P (= 2 2k ) processor hypercube. The processors in each row and column of the grid form a 2 k = p processor hypercube.
Reference: [35] <author> Vipin Kumar and Anshul Gupta. </author> <title> Analyzing scalability of parallel algorithms and architectures. </title> <type> Technical report, </type> <institution> TR-91-18, Computer Science Department, University of Minnesota, </institution> <month> June </month> <year> 1991. </year> <note> A short version of the paper appears in the Proceedings of the 1991 International Conference on Supercomputing, Germany, and as an invited paper in the Proc. of 29th Annual Allerton Conference on Communuication, Control and Computing, Urbana,IL, </note> <month> October </month> <year> 1991. </year>
Reference-contexts: In that sense, all the schemes are scalable. But the degree of scalability of these schemes is actually quite different from each other. A number of metrics have been developed for characterizing the scalability of different parallel algorithms and architectures <ref> [35] </ref>. For fixed processor utilization, the rate of change of problem size as a function of number of processors is a good characterization of the scalability of an algorithm [36, 37, 38, 39].
Reference: [36] <author> Anshul Gupta and Vipin Kumar. </author> <title> On the scalability of fft on parallel computers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <note> 1993 (to appear). available as a technical report TR 90-20, </note> <institution> Computer Science Department, University of Minnesota. </institution>
Reference-contexts: A number of metrics have been developed for characterizing the scalability of different parallel algorithms and architectures [35]. For fixed processor utilization, the rate of change of problem size as a function of number of processors is a good characterization of the scalability of an algorithm <ref> [36, 37, 38, 39] </ref>. An algorithm that requires a smaller change in problem size to obtain fixed efficiency is considered more scalable. The problem size (i.e., serial run time) for each iteration of BP is LJI 2 t c .
Reference: [37] <author> Vipin Kumar and Vineet Singh. </author> <title> Scalability of Parallel Algorithms for the All-Pairs Shortest Path Problem. </title> <journal> Journal of Parallel and Distributed Processing (special issue on massively parallel computation), </journal> <volume> 13(2) </volume> <pages> 124-138, </pages> <month> October </month> <year> 1991. </year> <note> A short version appears in the Proceedings of the International Conference on Parallel Processing, </note> <year> 1990. </year>
Reference-contexts: A number of metrics have been developed for characterizing the scalability of different parallel algorithms and architectures [35]. For fixed processor utilization, the rate of change of problem size as a function of number of processors is a good characterization of the scalability of an algorithm <ref> [36, 37, 38, 39] </ref>. An algorithm that requires a smaller change in problem size to obtain fixed efficiency is considered more scalable. The problem size (i.e., serial run time) for each iteration of BP is LJI 2 t c .
Reference: [38] <author> S. Ranka and S. Sahni. </author> <title> Hypercube Algorithms for Image Processing and Pattern Recognition. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: A number of metrics have been developed for characterizing the scalability of different parallel algorithms and architectures [35]. For fixed processor utilization, the rate of change of problem size as a function of number of processors is a good characterization of the scalability of an algorithm <ref> [36, 37, 38, 39] </ref>. An algorithm that requires a smaller change in problem size to obtain fixed efficiency is considered more scalable. The problem size (i.e., serial run time) for each iteration of BP is LJI 2 t c .
Reference: [39] <author> Vineet Singh, Vipin Kumar, Gul Agha, and Chris Tomlinson. </author> <title> Scalability of parallel sorting on mesh multicomputers. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 20 (2), </volume> <year> 1991. </year> <note> A short version of this paper appears in the Proceedings of the Fifth International Parallel Processing Symposium, </note> <year> 1991. </year>
Reference-contexts: A number of metrics have been developed for characterizing the scalability of different parallel algorithms and architectures [35]. For fixed processor utilization, the rate of change of problem size as a function of number of processors is a good characterization of the scalability of an algorithm <ref> [36, 37, 38, 39] </ref>. An algorithm that requires a smaller change in problem size to obtain fixed efficiency is considered more scalable. The problem size (i.e., serial run time) for each iteration of BP is LJI 2 t c .
Reference: [40] <author> David J Kuck. </author> <title> A survey of parallel machine organization and programming. </title> <journal> ACM Computing Survey, </journal> <volume> 9(1), </volume> <month> March </month> <year> 1977. </year> <month> 22 </month>
Reference-contexts: In such an environment, the lower bound on the parallel computation time for evaluating an arithmetic expression with N atomic operations is dlog 2 (N )e for any number of processors <ref> [40] </ref>. The number of arithmetic operations in the expression computed at the output nodes of a feed-forward neural network at the end of forward propagation is fl (LI 2 ), assuming J to be a constant.
References-found: 40

