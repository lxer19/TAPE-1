URL: ftp://synapse.cs.byu.edu/pub/papers/cgc_93a.ps
Refering-URL: ftp://synapse.cs.byu.edu/pub/papers/details.html
Root-URL: 
Title: Using Precepts to Augment Training Set Learning an input whose value is don't-care in some
Author: Christophe Giraud-Carrier and Tony Martinez j 
Note: j  unchanged has the same output as p. Informally, this says  
Address: Provo, UT 84602  
Affiliation: Department of Computer Science, Brigham Young University,  
Abstract: are used in turn to approximate A. Empirical studies show that good results can be achieved with TSL [8, 11]. However, TSL has several drawbacks. Training set learners (e.g., backpropagation) are typically slow as they may require many passes over the training set. Also, there is no guarantee that, given an arbitrary training set, the system will find enough good critical features to get a reasonable approximation of A. Moreover, the number of features to be searched is exponential in the number of inputs, and TSL becomes computationally expensive [1]. Finally, the scarcity of interesting positive theoretical results suggests the difficulty of learning without sufficient a priori knowledge. The goal of learning systems is to generalize. Generalization is commonly based on the set of critical features the system has available. Training set learners typically extract critical features from a random set of examples. While this approach is attractive, it suffers from the exponential growth of the number of features to be searched. We propose to extend it by endowing the system with some a priori knowledge, in the form of precepts. Advantages of the augmented system are speedup, improved generalization, and greater parsimony. This paper presents a precept-driven learning algorithm. Its main features include: 1) distributed implementation, 2) bounded learning and execution times, and 3) ability to handle both correct and incorrect precepts. Results of simulations on real-world data demonstrate promise. This paper presents precept-driven learning (PDL). PDL is intended to overcome some of TSL's weaknesses. In PDL, the training set is augmented by a small set of precepts. A pair p = (i, o) in I O is called an example. A precept is an example in which some of the i-entries (inputs) are set to the special value don't-care. An input whose value is not don't-care is said to be asserted. If i has no effect on the value of the output. The use of the special value don't-care is therefore as a shorthand. A pair containing don't-care inputs represents as many examples as the product of the sizes of the input domains of its don't-care inputs. 1. Introduction 
Abstract-found: 1
Intro-found: 1
Reference: [2] <author> Dietterich, T.G., and Michalski, </author> <title> R.S. A Comparative Review of Selected Methods for Learning from Examples. </title> <editor> In Michalski, R.S., Carbonell, J.G., and Mitchell, T.M., (Eds.), </editor> <booktitle> Machine Learning: </booktitle> <publisher> An Artificial Intelligence Approach . Morgan Kaufmann Publishers, </publisher> <address> Inc., </address> <note> Vol. I, 1983, Chapter 3. </note>
Reference-contexts: The final network is in Figure 8. PDLA uses only the dropping condition generalization rule (see <ref> [2] </ref>). This rule is applied whenever two pairs are concordant, near_match, and one has 0 or 1 more asserted input than the other (variable Diff). The differing input is replaced by the value don't-care in the pair that has the largest size, as long as num_asserted&gt;1.
Reference: [3] <author> Hall, L.O., and Romaniuk, S.G. </author> <title> A Hybrid Connectionist, Symbolic Learning System. </title> <booktitle> In Proceedings of the 8th National Conference on Artificial Intelligence , 1990, </booktitle> <pages> 783-788. </pages>
Reference-contexts: Pairs that are not equal, and do not satisfy the subset, superset or discriminated relations, are said to overlap. The majority of learning algorithms do training set learning. Several models exist that allow the presentation of precepts such as ASOCS [4, 5, 6] and ScNets <ref> [3] </ref>. PDL seeks to overcome ASOCS rigid order-dependency in dealing with inconsistency and to improve generalization. Two pairs are said to be concordant if their outputs are equal; otherwise, they are discordant.

Reference: [7] <author> Michalski, </author> <title> R.S. A Theory and Methodology of Inductive Learning. </title> <booktitle> Artificial Intelligence , 20 , 1983, </booktitle> <pages> 111-161. </pages> <note> mushroom 100 4062 1499 (unnecessary) (unnecessary) Table 1 - PDLA-Simulation Results </note>
Reference-contexts: Moreover, PDL speeds up learning by pruning the input space and improves generalization by using precepts. num_asserted: the number of asserted inputs. specificity: the ratio num_asserted/number of inputs. Examples have specificity=1 (maximally specific), while precepts have specificity&lt;1 (more general). A pair can be generalized in many different ways (see <ref> [7] </ref> for a list of generalization rules). However, the result is always the same. If a pair p represents a set S of examples, then a pair p' that generalizes p (if such a generalization exists) represents a set S' of examples such that S S'.
Reference: [8] <author> Mooney, R., Shavlik, J., Towell, G., and Gove, A. </author> <title> An Experimental Comparison of Symbolic and Connectionist Learning Algorithms. </title> <booktitle> In Proceedings of the 11th International Joint Conference on Artificial Intelligence , 1989, </booktitle> <month> 775-780. </month> <title> Table 1 shows that the addition of simple precepts improves generalization performance and significantly reduces the size of the network (greater parsimony). On average, PA is increased by 4.3%, while NS is reduced by 30%. Table 1 also shows that PDLA has some ability to deal with bad precepts. In most cases, the "recovery" is good, but is obtained at the expense of a larger network. </title>
Reference: [9] <author> Murphy, P.M., and Aha, D.W. </author> <title> UCI Repository of machine learning databases . Irvine, </title> <institution> CA: University of California, Department of Information and Computer Science, </institution> <year> 1992. </year>
Reference-contexts: Simulation results PDLA was tested on several applications, drawn from the Irvine machine learning database <ref> [9] </ref>, with two modifications. All real-valued attributes are made discrete Figure 7 - Network After Presentation of p7 (using a K-means algorithm on each input), and all don't- know values are treated as don't-care values.
Reference: [10] <author> Rudolph, G., and Martinez T.R. </author> <title> An Efficient Static Topology for Modeling ASOCS. </title> <editor> In Kohonen, et al., (Eds.), </editor> <booktitle> Artificial Neural Networks . Elsevier Science Publishing, </booktitle> <year> 1991, </year> <pages> 729-734. </pages> <month> 4. </month> <title> Conclusion This paper introduces precept-driven learning and describes a precept-driven learning algorithm (PDLA). Results of simulations on several data sets are reported. PDLA's main features include: </title>
Reference-contexts: When a node is added, its position is determined so as to maximize breadth. When a node is deleted, transformations are applied to keep the tree balanced. Hence, the tree is maintained as full as possible. For details on such maintenance mechanisms, see <ref> [10] </ref>. 2. A precept-driven learning algorithm 2.2. Example In this section, we describe a precept-driven learning algorithm (PDLA). Before going into the details of it, we give a few preliminary definitions illustrated by examples. Pairs refer to either examples or precepts. Let p1=(&lt;0,0,*,1,*&gt;,&lt;0&gt;), p2=(&lt;0,0,0,1,0&gt;,&lt;1&gt;), p3= (&lt;0,*,*,1,1&gt;,&lt;1&gt;), p4=(&lt;0,0,1,1,1&gt;,&lt;0&gt;).
Reference: [11] <author> Sejnowski, T.J., and Rosenberg, C.R. </author> <title> Parallel Networks that Learn to Pronounce English Text. </title> <journal> Complex Systems , 1, </journal> <year> 1987, </year> <month> 145-168. </month> <title> Distributed implementation facilitating the use of massive parallelism and self adaptation, Bounded learning and execution times complexity, Use of precepts resulting in faster learning, increased parsimony, and improved generalization, Ability to deal with incorrect precepts. </title>
References-found: 7

