URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P544.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts95.htm
Root-URL: http://www.mcs.anl.gov
Title: High Performance Computational Chemistry:(II) A Scalable SCF Program  
Author: Robert J. Harrison, Martyn F. Guest, Rick A. Kendall, David E. Bernholdt, Adrian T. Wong, Mark Stave, James L. Anchell, Anthony C. Hess, Rik J. Littlefield, George L. Fann, Jaroslaw Nieplocha, Greg S. Thomas, and David Elwood Jeff Tilson, Ron L. Shepard, Albert F. Wagner, Ian T. Foster, Ewing Lusk and Rick Stevens 
Address: 1 P.O. Box 999, Richland WA 99352.  Argonne, IL 60439.  
Affiliation: Pacific Northwest Laboratory  Argonne National Laboratory,  
Note: Draft May 16, 1994. For submission to the Journal of Computational Chemistry.  Institute under contract DE-AC06-76RL0 1830.  
Abstract: We discuss issues in developing scalable parallel algorithms and focus in particular on the distribution, as opposed to the replication, of key data structures. Replication of large data structures limits the maximum calculation size by imposing a low ratio of processors to memory. Only applications which distribute both data and computation across processors are truly scalable. The use of shared data structures that may be independently accessed by each process even in a distributed-memory environment greatly simplifies development and provides a significant performance enhancement. We describe tools we have developed to support this programming paradigm. These tools are used to develop a highly efficient and scalable algorithm to perform self-consistent field calculations on molecular systems. A simple and classical strip-mining algorithm suffices to achieve an efficient and scalable Fock-matrix construction in which all matrices are fully distributed. By stripmining over atoms we also exploit all available sparsity and pave the way to adopting more sophisticated methods for summation of the Coulomb and exchange interactions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Almlof J, Faegri K, and Korsell K. </author> <year> 1982. </year> <note> J. Comp. Chem., 3:385. </note>
Reference-contexts: In this article we discuss the design of scalable parallel algorithms that emphasize the cost-effective use of both processors and memory. We use the direct <ref> [1] </ref> closed-shell self-consistent field (SCF) method for molecules as an example. Efficient SCF computations are required for our research, and for more accurate theories. Also, SCF is representative of more sophisticated theories in using irregular data-access patterns and accumulation of results into large data structures. <p> The input/output (I/O) problem of storing and repetitively processing the two-electron integrals is avoided by computing the integrals as required <ref> [1] </ref>. Each integral may take several hundred or more floating point operations (FLOPs) to compute.
Reference: [2] <author> Panas I and Almlof J. </author> <year> 1992. </year> <title> Int. </title> <journal> J. Quant. Chem., </journal> <volume> 42 </volume> <pages> 1073-1089. </pages>
Reference-contexts: To make more substantial improvements, it is necessary to incorporate more sophisticated algorithms with lower growth rates, for example, replacing an O (N 4 atom ) algorithm with one with an effective scaling of O (N atom )-O (N 2 atom ) <ref> [2, 3, 4, 5] </ref>. Parallelization of such algorithms will be the subject of 2 Computers capable of a TERAFLOP, or 1012 floating point operations per second High-Performance Comp. Chem. : : : 2 future publications. <p> High-Performance Comp. Chem. : : : 24 large molecules. Once improved algorithms are adopted for the Fock matrix construction <ref> [2, 3, 4, 5] </ref> the diagonalization will be dominant. Several approaches have been proposed for eliminating this bottleneck [17, 28]. <p> An efficiency of 97% is measured for a large molecular system on 512 processors. A production version of this algorithm would include dynamic adjustment of the granularity in response to the basis set size and machine performance parameters. Far greater gains are realizable, however, by first pursuing alternative algorithms <ref> [2, 3, 4, 5] </ref>. The current programming model, mixing globally-addressable arrays with message passing is both portable and efficient. In particular, machines such as the Kendall Square Research and the Cray T3D which have hardware support for shared memory should prove particularly effective in this model. High-Performance Comp.
Reference: [3] <author> Dovesi R, Saunders VR, and Roetti C. </author> <title> Crystal92 user's manual. </title> <institution> University of Torino, Torino, </institution> <year> 1992. </year>
Reference-contexts: To make more substantial improvements, it is necessary to incorporate more sophisticated algorithms with lower growth rates, for example, replacing an O (N 4 atom ) algorithm with one with an effective scaling of O (N atom )-O (N 2 atom ) <ref> [2, 3, 4, 5] </ref>. Parallelization of such algorithms will be the subject of 2 Computers capable of a TERAFLOP, or 1012 floating point operations per second High-Performance Comp. Chem. : : : 2 future publications. <p> High-Performance Comp. Chem. : : : 24 large molecules. Once improved algorithms are adopted for the Fock matrix construction <ref> [2, 3, 4, 5] </ref> the diagonalization will be dominant. Several approaches have been proposed for eliminating this bottleneck [17, 28]. <p> An efficiency of 97% is measured for a large molecular system on 512 processors. A production version of this algorithm would include dynamic adjustment of the granularity in response to the basis set size and machine performance parameters. Far greater gains are realizable, however, by first pursuing alternative algorithms <ref> [2, 3, 4, 5] </ref>. The current programming model, mixing globally-addressable arrays with message passing is both portable and efficient. In particular, machines such as the Kendall Square Research and the Cray T3D which have hardware support for shared memory should prove particularly effective in this model. High-Performance Comp.
Reference: [4] <editor> Ringald MN, Belhadj M, and Friesner RA. </editor> <year> 1990. </year> <journal> J. Chem. Phys., </journal> <volume> 93 </volume> <pages> 3397-3347. </pages>
Reference-contexts: To make more substantial improvements, it is necessary to incorporate more sophisticated algorithms with lower growth rates, for example, replacing an O (N 4 atom ) algorithm with one with an effective scaling of O (N atom )-O (N 2 atom ) <ref> [2, 3, 4, 5] </ref>. Parallelization of such algorithms will be the subject of 2 Computers capable of a TERAFLOP, or 1012 floating point operations per second High-Performance Comp. Chem. : : : 2 future publications. <p> High-Performance Comp. Chem. : : : 24 large molecules. Once improved algorithms are adopted for the Fock matrix construction <ref> [2, 3, 4, 5] </ref> the diagonalization will be dominant. Several approaches have been proposed for eliminating this bottleneck [17, 28]. <p> An efficiency of 97% is measured for a large molecular system on 512 processors. A production version of this algorithm would include dynamic adjustment of the granularity in response to the basis set size and machine performance parameters. Far greater gains are realizable, however, by first pursuing alternative algorithms <ref> [2, 3, 4, 5] </ref>. The current programming model, mixing globally-addressable arrays with message passing is both portable and efficient. In particular, machines such as the Kendall Square Research and the Cray T3D which have hardware support for shared memory should prove particularly effective in this model. High-Performance Comp.
Reference: [5] <author> Vahtras O, Almlof J, and Feyereisen MW. </author> <year> 1993. </year> <institution> Chem. Phys. Lett., 231:514. </institution>
Reference-contexts: To make more substantial improvements, it is necessary to incorporate more sophisticated algorithms with lower growth rates, for example, replacing an O (N 4 atom ) algorithm with one with an effective scaling of O (N atom )-O (N 2 atom ) <ref> [2, 3, 4, 5] </ref>. Parallelization of such algorithms will be the subject of 2 Computers capable of a TERAFLOP, or 1012 floating point operations per second High-Performance Comp. Chem. : : : 2 future publications. <p> High-Performance Comp. Chem. : : : 24 large molecules. Once improved algorithms are adopted for the Fock matrix construction <ref> [2, 3, 4, 5] </ref> the diagonalization will be dominant. Several approaches have been proposed for eliminating this bottleneck [17, 28]. <p> An efficiency of 97% is measured for a large molecular system on 512 processors. A production version of this algorithm would include dynamic adjustment of the granularity in response to the basis set size and machine performance parameters. Far greater gains are realizable, however, by first pursuing alternative algorithms <ref> [2, 3, 4, 5] </ref>. The current programming model, mixing globally-addressable arrays with message passing is both portable and efficient. In particular, machines such as the Kendall Square Research and the Cray T3D which have hardware support for shared memory should prove particularly effective in this model. High-Performance Comp.
Reference: [6] <author> Clementi E, Corongiu G, Detrich J, Chin S, and Domingo L. </author> <year> 1984. </year> <title> Int. </title> <journal> J. Quant. Chem. Sym., </journal> <volume> 18 </volume> <pages> 601-618. </pages> <institution> High-Performance Comp. Chem. </institution> : : : <month> 26 </month>
Reference-contexts: Parallelization of such algorithms will be the subject of 2 Computers capable of a TERAFLOP, or 1012 floating point operations per second High-Performance Comp. Chem. : : : 2 future publications. The first parallel self-consistent field (SCF) programs <ref> [6, 7, 8, 9, 10, 11, 12] </ref> were based on the "replicated data" model, in which each processor in the parallel system held its own complete copy of the Fock and density matrices.
Reference: [7] <author> Dupuis M and Watts JD. </author> <year> 1987. </year> <journal> Theor. Chim. Acta, </journal> <volume> 71 </volume> <pages> 91-103. </pages>
Reference-contexts: Parallelization of such algorithms will be the subject of 2 Computers capable of a TERAFLOP, or 1012 floating point operations per second High-Performance Comp. Chem. : : : 2 future publications. The first parallel self-consistent field (SCF) programs <ref> [6, 7, 8, 9, 10, 11, 12] </ref> were based on the "replicated data" model, in which each processor in the parallel system held its own complete copy of the Fock and density matrices.
Reference: [8] <author> Guest MF, Smith W, and Harrison RJ. </author> <year> 1992. </year> <title> Chem. Design Auto. </title> <journal> News, </journal> <volume> 7 </volume> <pages> 12-18. </pages>
Reference-contexts: Parallelization of such algorithms will be the subject of 2 Computers capable of a TERAFLOP, or 1012 floating point operations per second High-Performance Comp. Chem. : : : 2 future publications. The first parallel self-consistent field (SCF) programs <ref> [6, 7, 8, 9, 10, 11, 12] </ref> were based on the "replicated data" model, in which each processor in the parallel system held its own complete copy of the Fock and density matrices. <p> Each processor computes part of the integral list adding them into its own local matrix. Subsequently, the complete Fock is obtained by combining the partial matrices with a global summation operation. The algorithm is perfectly parallel (assuming some rudimentary load balancing <ref> [8, 12, 19] </ref>) and the global summation of the Fock matrix can be done efficiently (e.g., on the Intel Touchstone Delta a global summation of 10 6 numbers High-Performance Comp. Chem. : : : 7 takes about 3 seconds). The poor scaling inherent in this approach is readily apparent.
Reference: [9] <author> Burkhardt A, Wedig U, and v. Schnering HG. </author> <year> 1993. </year> <journal> Theor. Chim. Acta, </journal> <volume> 86 </volume> <pages> 497-510. </pages>
Reference-contexts: Parallelization of such algorithms will be the subject of 2 Computers capable of a TERAFLOP, or 1012 floating point operations per second High-Performance Comp. Chem. : : : 2 future publications. The first parallel self-consistent field (SCF) programs <ref> [6, 7, 8, 9, 10, 11, 12] </ref> were based on the "replicated data" model, in which each processor in the parallel system held its own complete copy of the Fock and density matrices. <p> By abandoning replicated data algorithms, we remove the rigid constraint on problem size and enable the usage of machines with a far more cost-effective ratio of processors to memory, a point raised by Hillis [20] in justifying the design of the CM-2. Burkhardt et al <ref> [9] </ref> distributed the integral computation across many small Transputer-based nodes, keeping the Fock matrix on just one processor with a large memory. While this addresses the cost issue the algorithm is intrinsically not scalable, since accumulation of the integrals into the Fock matrix is a sequential bottleneck.
Reference: [10] <author> Luthi HP, Mertz JE, Feyereisen MW, and Almlof JE. </author> <year> 1992. </year> <journal> J. Comp. Chem., </journal> <volume> 13 </volume> <pages> 160-164. </pages>
Reference-contexts: Parallelization of such algorithms will be the subject of 2 Computers capable of a TERAFLOP, or 1012 floating point operations per second High-Performance Comp. Chem. : : : 2 future publications. The first parallel self-consistent field (SCF) programs <ref> [6, 7, 8, 9, 10, 11, 12] </ref> were based on the "replicated data" model, in which each processor in the parallel system held its own complete copy of the Fock and density matrices.
Reference: [11] <author> Otto P and Fruchtl H. </author> <year> 1993. </year> <journal> Computers in Chemistry, </journal> <volume> 17 </volume> <pages> 229-239. </pages>
Reference-contexts: Parallelization of such algorithms will be the subject of 2 Computers capable of a TERAFLOP, or 1012 floating point operations per second High-Performance Comp. Chem. : : : 2 future publications. The first parallel self-consistent field (SCF) programs <ref> [6, 7, 8, 9, 10, 11, 12] </ref> were based on the "replicated data" model, in which each processor in the parallel system held its own complete copy of the Fock and density matrices.
Reference: [12] <author> Feyereisen M and Kendall RA. </author> <year> 1993. </year> <journal> Theor. Chim. Acta, </journal> <volume> 84 </volume> <pages> 289-299. </pages>
Reference-contexts: Parallelization of such algorithms will be the subject of 2 Computers capable of a TERAFLOP, or 1012 floating point operations per second High-Performance Comp. Chem. : : : 2 future publications. The first parallel self-consistent field (SCF) programs <ref> [6, 7, 8, 9, 10, 11, 12] </ref> were based on the "replicated data" model, in which each processor in the parallel system held its own complete copy of the Fock and density matrices. <p> Below we continue our previous work [15] and explore an approach based on a novel communication strategy. The Fock matrix construction is parallelized using a dynamic load-balancing approach, as with the replicated data model <ref> [12] </ref>. However, the Fock and density matrices are distributed across processors and are accessed during the Fock-build using a High-Performance Comp. <p> Each processor computes part of the integral list adding them into its own local matrix. Subsequently, the complete Fock is obtained by combining the partial matrices with a global summation operation. The algorithm is perfectly parallel (assuming some rudimentary load balancing <ref> [8, 12, 19] </ref>) and the global summation of the Fock matrix can be done efficiently (e.g., on the Intel Touchstone Delta a global summation of 10 6 numbers High-Performance Comp. Chem. : : : 7 takes about 3 seconds). The poor scaling inherent in this approach is readily apparent.
Reference: [13] <author> Colvin ME, Janssen CL, Whiteside RA, and Tong CH. </author> <year> 1993. </year> <journal> Theor. Chim. Acta, </journal> <volume> 84 </volume> <pages> 301-314. </pages>
Reference-contexts: Instead, it is both necessary and cost-effective to distribute the Fock and density matrices across the memories of all processors. Previous distributed SCF algorithms <ref> [13, 14] </ref> had the advantage of being based on regular communication patterns that were compatible with conventional "two-sided" message passing semantics (cooperative sender and receiver). They suffered, however, from inefficiencies arising from performing redundant computation, limited scalability and/or large process waiting times caused by load imbalance and frequent synchronization. <p> While this addresses the cost issue the algorithm is intrinsically not scalable, since accumulation of the integrals into the Fock matrix is a sequential bottleneck. Fully distributed algorithms <ref> [13, 14, 15] </ref> are required. One key to understanding the design and performance of fully distributed algorithms is an understanding of how to manage efficiently the memory hierarchy of parallel computers. <p> The only truly portable MIMD programming model is message passing, for which a standard interface has been recently proposed [22] . It is, however, very hard to develop applications with fully distributed data structures using the message-passing model <ref> [13, 14] </ref>. What is needed is support for one-sided access to data structures (here limited to one- and two-dimensional arrays) in the spirit of shared memory.
Reference: [14] <author> Furlani TR and King HF. </author> <year> 1993. </year> <note> J. Comp. Chem., submitted for publication. </note>
Reference-contexts: Instead, it is both necessary and cost-effective to distribute the Fock and density matrices across the memories of all processors. Previous distributed SCF algorithms <ref> [13, 14] </ref> had the advantage of being based on regular communication patterns that were compatible with conventional "two-sided" message passing semantics (cooperative sender and receiver). They suffered, however, from inefficiencies arising from performing redundant computation, limited scalability and/or large process waiting times caused by load imbalance and frequent synchronization. <p> While this addresses the cost issue the algorithm is intrinsically not scalable, since accumulation of the integrals into the Fock matrix is a sequential bottleneck. Fully distributed algorithms <ref> [13, 14, 15] </ref> are required. One key to understanding the design and performance of fully distributed algorithms is an understanding of how to manage efficiently the memory hierarchy of parallel computers. <p> The only truly portable MIMD programming model is message passing, for which a standard interface has been recently proposed [22] . It is, however, very hard to develop applications with fully distributed data structures using the message-passing model <ref> [13, 14] </ref>. What is needed is support for one-sided access to data structures (here limited to one- and two-dimensional arrays) in the spirit of shared memory. <p> Since the computation is a quartic function of the block size while the communication is only a quadratic function, small block sizes suffice to make the computation time dominate the communication time. This is the same principle independently used by Furlani and King <ref> [14] </ref>, but, in constrast to the complexities of their message-passing algorithm, the support for "one-sided" access to distributed arrays enables a very simple, efficient and readily modified implementation. We presently stripmine by grouping basis functions according to their (usually atomic) centers.
Reference: [15] <author> Foster IT, Tilson JL, Wagner AF, Shepard R, Harrison RJ, Kendall RA, Littlefield RJ, Bernholdt DE, and Anchel J. </author> <year> 1994. </year> <note> J. Comp. Chem., </note> ??:?? 
Reference-contexts: We wish to understand not merely how to write a fast and scalable SCF program, but to isolate the principles that make this task easier, and then apply those ideas to other algorithms of computational chemistry. Below we continue our previous work <ref> [15] </ref> and explore an approach based on a novel communication strategy. The Fock matrix construction is parallelized using a dynamic load-balancing approach, as with the replicated data model [12]. However, the Fock and density matrices are distributed across processors and are accessed during the Fock-build using a High-Performance Comp. <p> While this addresses the cost issue the algorithm is intrinsically not scalable, since accumulation of the integrals into the Fock matrix is a sequential bottleneck. Fully distributed algorithms <ref> [13, 14, 15] </ref> are required. One key to understanding the design and performance of fully distributed algorithms is an understanding of how to manage efficiently the memory hierarchy of parallel computers. <p> local (1:ihi-ilo+1, 1:jhi-jlo+1) The difference is that this single HPF assignment would be executed in a data-parallel fashion, whereas the global array put operation will execute in MIMD parallel mode such that each process may reference different array patches. 5 The distributed SCF algorithm Our initial distributed SCF prototype code <ref> [15] </ref> parallelized only the two-electron contribution to the Fock matrix construction. This effort was successful in that an acceptably efficient parallel Fock matrix construction was achieved. It was apparent, however, that the remainder of the program, which was still using replicated data methods, needed complete rewriting. <p> Each task requires fetching up to six blocks of the density matrix and accumulating into the corresponding blocks of the Fock matrix. Following our previous analysis <ref> [15] </ref>, communication may be reduced by up to a factor of High-Performance Comp. Chem. : : : 16 two by defining a task as all lat for given iat, jat, and kat.
Reference: [16] <author> Szabo A and Ostlund NS. </author> <title> Modern Quantum Chemistry: Introduction to Advanced Electronic Structure Theory. 1st Ed. Revised. </title> <publisher> McGraw-Hill, Inc., </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: A simple performance model of the algorithm is discussed and results are presented. 2 Self-Consistent Field (SCF) | basic theory A very brief summary of the non-relativistic SCF method in the Born-Oppenheimer approximation is given here, for more detailed references please refer to <ref> [16] </ref>. The restricted Hartree-Fock wavefunction for a closed-shell N-electron system is an antisymmetric product of N/2 doubly-occupied, orthonormal, molecular orbitals (MO-s).
Reference: [17] <author> Shepard R. </author> <year> 1993. </year> <journal> Theor. Chim. Acta, </journal> <volume> 84 </volume> <pages> 343-351. </pages>
Reference-contexts: For simplicity we limit consideration to real MO coeffi cients. High-Performance Comp. Chem. : : : 4 The MO coefficients are chosen to minimize the energy subject to the orthonormality constraints. With the MO coefficients as parameters the SCF problem is a non-linear optimization problem with constraints. Alternatively <ref> [17] </ref>, the parameters may be chosen to be orbital rotations, which results in an unconstrained non-linear optimization problem. The MO coefficients C i are specified as a rotation of some initial orthonormal set of orbitals C i : The matrix K is antisymmetric which ensures the rotation is unitary. <p> High-Performance Comp. Chem. : : : 24 large molecules. Once improved algorithms are adopted for the Fock matrix construction [2, 3, 4, 5] the diagonalization will be dominant. Several approaches have been proposed for eliminating this bottleneck <ref> [17, 28] </ref>. <p> Once improved algorithms are adopted for the Fock matrix construction [2, 3, 4, 5] the diagonalization will be dominant. Several approaches have been proposed for eliminating this bottleneck [17, 28]. We are adopting a variation of the second-order convergent approach proposed by Shepard <ref> [17] </ref>, in part because of the wide range of properties that may be computed from the orbital Hessian. 6 Conclusions A simple and classical strip-mining algorithm suffices to achieve an efficient and scalable Fock-matrix construction in which all matrices are fully distributed.
Reference: [18] <author> P. Pulay. </author> <year> 1982. </year> <note> J. Comp. Chem., 3:556. </note>
Reference-contexts: Repeat from step 2 until converged. The third step is the computational bottleneck on sequential computers. A conventional SCF program implements the fourth step by obtaining the new MO-s as the eigenvectors of the Fock matrix (possibly also employing damping, level-shifting and other methods <ref> [18] </ref>). For N basis basis functions there are O (N 4 basis ) two-electron integrals. Locality in the AO basis causes the number of integrals to decrease to approximately O (N 2 basis ) with the use of screening for very large, spatially-extended systems.
Reference: [19] <author> Harrison RJ and Kendall RA. </author> <year> 1991. </year> <journal> Theor. Chim. Acta, </journal> <volume> 79 </volume> <pages> 337-347. </pages>
Reference-contexts: Each processor computes part of the integral list adding them into its own local matrix. Subsequently, the complete Fock is obtained by combining the partial matrices with a global summation operation. The algorithm is perfectly parallel (assuming some rudimentary load balancing <ref> [8, 12, 19] </ref>) and the global summation of the Fock matrix can be done efficiently (e.g., on the Intel Touchstone Delta a global summation of 10 6 numbers High-Performance Comp. Chem. : : : 7 takes about 3 seconds). The poor scaling inherent in this approach is readily apparent.
Reference: [20] <author> Hillis WD. </author> <title> The Connection Machine. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1985. </year>
Reference-contexts: By abandoning replicated data algorithms, we remove the rigid constraint on problem size and enable the usage of machines with a far more cost-effective ratio of processors to memory, a point raised by Hillis <ref> [20] </ref> in justifying the design of the CM-2. Burkhardt et al [9] distributed the integral computation across many small Transputer-based nodes, keeping the Fock matrix on just one processor with a large memory.
Reference: [21] <institution> High Performance Fortran Forum. </institution> <type> Technical Report Version 1.0, </type> <institution> Rice University, </institution> <year> 1993. </year> <title> High-Performance Comp. </title> <journal> Chem. </journal> : : : <volume> 27 </volume>
Reference-contexts: The method we use to reduce communication in the parallel Fock matrix construction is identical to that used for this simple matrix multiplication example. 3.2 One-sided memory access No emerging standards for parallel programming languages (notably just High Performance Fortran, HPF-1 <ref> [21] </ref>) provide extensive support for MIMD programming. The only truly portable MIMD programming model is message passing, for which a standard interface has been recently proposed [22] . It is, however, very hard to develop applications with fully distributed data structures using the message-passing model [13, 14]. <p> In particular HPF <ref> [21] </ref> will certainly provide the basis for future standards definition of distributed arrays in FORTRAN. The basic functionality described above (create, fetch, store, accumulate, gather, scatter, data-parallel operations) all may be expressed as single statements using FORTRAN-90 array notation and the data-distribution directives of HPF.
Reference: [22] <author> The MPI Forum. </author> <booktitle> In Proceddings of Supercomputing '93, </booktitle> <pages> pages 878-883. </pages> <publisher> IEEE computer Society Press, Los Alamitos, </publisher> <address> California, </address> <year> 1993 </year> <month> November, </month> <year> 1993. </year>
Reference-contexts: The only truly portable MIMD programming model is message passing, for which a standard interface has been recently proposed <ref> [22] </ref> . It is, however, very hard to develop applications with fully distributed data structures using the message-passing model [13, 14]. What is needed is support for one-sided access to data structures (here limited to one- and two-dimensional arrays) in the spirit of shared memory.
Reference: [23] <author> Nieplocha J, Harrison RJ, and Littlefield RJ. </author> <title> Asynchronous global arrays, Supercomputing 1994, </title> <note> submitted for publication. </note>
Reference-contexts: What is needed is support for one-sided access to data structures (here limited to one- and two-dimensional arrays) in the spirit of shared memory. With some effort this can be done portably <ref> [23] </ref> and in return for this investment we gain a much easier programming environment that speeds code development and improves extensibility and maintainability. We also gain a significant performance enhancement from increased asynchrony of execution of processes [24]. <p> be efficiently supported. 4 Prototype support for distributed globally address able arrays Consideration of the requirements of the SCF algorithm discussed below, and also the parallel COLUMBUS configuration interaction program [25], second-order Moller-Plesset Perturbation theory and parallel Coupled-Cluster methods [26] led to the design and implementation of some preliminary tools <ref> [23] </ref> to support one-sided access to distributed one- and two-dimensional arrays.
Reference: [24] <author> Carriero N and Gelernter D. </author> <title> How To Write Parallel Programs. A First Course. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: With some effort this can be done portably [23] and in return for this investment we gain a much easier programming environment that speeds code development and improves extensibility and maintainability. We also gain a significant performance enhancement from increased asynchrony of execution of processes <ref> [24] </ref>. Message passing forces processes to cooperate (e.g., by responding to requests for a particular datum). Inevitably, this involves waiting for a collaborating process to reach the same point in the algorithm, which is only partially reduced by the use of complex buffering and asynchronous communication strategies.
Reference: [25] <author> Schuler M, Kovar T, Lischka H, Shepard R, and Harrison RJ. </author> <year> 1993. </year> <journal> Theor. Chim. Acta, </journal> <volume> 84 </volume> <pages> 489-509. </pages>
Reference-contexts: Both types of parallelism must be efficiently supported. 4 Prototype support for distributed globally address able arrays Consideration of the requirements of the SCF algorithm discussed below, and also the parallel COLUMBUS configuration interaction program <ref> [25] </ref>, second-order Moller-Plesset Perturbation theory and parallel Coupled-Cluster methods [26] led to the design and implementation of some preliminary tools [23] to support one-sided access to distributed one- and two-dimensional arrays.
Reference: [26] <author> Rendell AP, Guest MF, and Kendall RA. </author> <year> 1993. </year> <journal> J. Comp. Chem., </journal> <volume> 14 </volume> <pages> 1429-1439. </pages>
Reference-contexts: Both types of parallelism must be efficiently supported. 4 Prototype support for distributed globally address able arrays Consideration of the requirements of the SCF algorithm discussed below, and also the parallel COLUMBUS configuration interaction program [25], second-order Moller-Plesset Perturbation theory and parallel Coupled-Cluster methods <ref> [26] </ref> led to the design and implementation of some preliminary tools [23] to support one-sided access to distributed one- and two-dimensional arrays.
Reference: [27] <author> SCALAPACK, </author> <title> scalable linear algebra package, code and documents available through netlib. </title>
Reference-contexts: These libraries may internally use any form of parallelism appropriate to the computer system, such as cooperative message passing or shared memory. * standard and generalized real symmetric eigensolver; and * linear equation solver (interface to SCALAPACK <ref> [27] </ref>. 4.3 Sample code fragment This interface has been designed in the light of emerging standards. In particular HPF [21] will certainly provide the basis for future standards definition of distributed arrays in FORTRAN.
Reference: [28] <author> Pollard WT and Friesner RA. </author> <year> 1993. </year> <institution> J. Chem. Phys., 99:6742. </institution>
Reference-contexts: High-Performance Comp. Chem. : : : 24 large molecules. Once improved algorithms are adopted for the Fock matrix construction [2, 3, 4, 5] the diagonalization will be dominant. Several approaches have been proposed for eliminating this bottleneck <ref> [17, 28] </ref>.
References-found: 28

