URL: ftp://ftp.cis.upenn.edu/pub/pelachaud/ijcai93/ijcai93.ps.Z
Refering-URL: http://www.cis.upenn.edu/~hms/publications.html
Root-URL: 
Email: pelachau@graphics.cis.upenn.edu  luce@bora.inria.fr  hussein@bora.inria.fr  
Title: Rule-Structured Facial Animation System  
Author: Catherine Pelachaud Marie-Luce Viaud Hussein Yahia 
Address: Pennsylvania  
Affiliation: University of  INRIA  INRIA  
Abstract: Our overall goal is to produce as automatic as possible facial expressions with wrinkles from spoken input. We focus on two aspects of this problem: integration of the expressive wrinkles and generation of synchronized speech-animation. Our facial model integrates facial muscles deformations and bulges. We have produced a high level programming language to automatically drive 3D animation of facial expressions from speech. Our system embodies rule-governed translation from speech and utterance meaning to facial expressions. We are concerned primarily with expressions conveying information correlated with the intonation of the voice, some of which are also correlated with affect or emotion. We apply our automatic animation model to a new facial animation system which integrates effects of the facial motion as expressive wrinkles and muscles dependencies. We obtain then with this subtle criteria of modeling and motion an animation much more expressive and natural.
Abstract-found: 1
Intro-found: 1
Reference: [ Argyle and Cook, 1976 ] <author> M. Argyle and M. Cook. </author> <title> Gaze and Mutual gaze. </title> <publisher> Cambridge University Press, </publisher> <year> 1976. </year>
Reference-contexts: A boundary point (such as a comma) will be underlined by slow movement and a final pause will coincide with stillness [ Hadar et al., 1983 ] . Eyeblinks can occur also during pauses <ref> [ Argyle and Cook, 1976 ] </ref> . REGULATORS: they help the interaction between speaker-listener, they control the flow of speech. Head and eye movements are coordinated to synchronize speech and organize communication. Speaker-Turn system groups them into four sets [ Duncan, 1974 ] . <p> We supposed that when an action occurs (mutual gaze, breaking eye contact, and so on) head and eyes follow the same behavior. This means we interpret eye position as head position. Otherwise, when no action is occurring, the eyes will scan the listener's face in a random way <ref> [ Argyle and Cook, 1976 ] </ref> . 6 Our Facial Model The goal of the facial animation model we are presented in this paper is the generation of a more expressive and natural animation of the human face.
Reference: [ Dectalk, 1985 ] <institution> Digital Equipment Corporation. Dectalk and DTC03 Text-To-Speech System Owner's, </institution> <year> 1985. </year>
Reference-contexts: Furthermore, we assume that the input contains the decomposition of the utterance into its bracketed elements, the placement and type of accents. The sentence is written as a list of strings corresponding to the phonetic representation of the utterance and whose notation is compatible with Dectalk's ascii-keyboard notation <ref> [ Dectalk, 1985 ] </ref> . 5.2 Computation for each Channel The program runs through the set of rules for each channel computing the value of the two parameters of actions (see Fig. 2). The program retrieves all valid rules corresponding to the intonational pattern defined in the input file.
Reference: [ Duncan, 1974 ] <author> S. Duncan. </author> <title> Nonverbal Communication, Some signals and rules for taking speaking turns in conversations. </title> <publisher> Oxford University Press, </publisher> <year> 1974. </year>
Reference-contexts: Eyeblinks can occur also during pauses [ Argyle and Cook, 1976 ] . REGULATORS: they help the interaction between speaker-listener, they control the flow of speech. Head and eye movements are coordinated to synchronize speech and organize communication. Speaker-Turn system groups them into four sets <ref> [ Duncan, 1974 ] </ref> . MANIPULATORS: they correspond to the biological needs of the face (like blinking to keep the eyes wet). emotional channel : Emotions are mainly displayed on the face and through the voice.
Reference: [ Ekman and Friesen, 1978 ] <author> P. Ekman and W. Friesen. </author> <title> Facial Action Coding System. </title> <publisher> Consulting Psychologists Press, Inc., </publisher> <year> 1978. </year>
Reference-contexts: We then present our facial model. We define in our context wrinkles and bulges. We describe how we have included then in our animation system. 1.1 Facial Notation We are using the FACS notation (Facial Action Coding System) created by P. Ekman and W. Friesen <ref> [ Ekman and Friesen, 1978 ] </ref> . It is based on anatomical studies. Since facial movement is due to muscle action, a system was derived to describe visible facial expressions (emotional or conversational) at the muscle level.
Reference: [ Ekman, 1979 ] <author> P. Ekman. </author> <title> Human ethology: claims and limits of a new discipline: contributions to the Colloquium, </title> <booktitle> About brows: emotional and conversational signals, </booktitle> <pages> pages 169-248. </pages> <publisher> Cambridge University Press, </publisher> <address> Cambridge, Eng-land; New-York, </address> <year> 1979. </year>
Reference-contexts: Some can highlight a word, underline a pause; some are tied to the intonation of the voice. Our first step was to elaborate a repertory of such facial actions called channels. 2.1 Facial Channel Facial actions can be clusterized depending on their functionality <ref> [ Ekman, 1979 ] </ref> . phonemic channel : lip shapes and coarticulation. intonational channel : CONVERSATIONAL SIGNALS: when occurring on accented items or on emphatic segments, they clarify and support what is being said. <p> MANIPULATORS: they correspond to the biological needs of the face (like blinking to keep the eyes wet). emotional channel : Emotions are mainly displayed on the face and through the voice. Six emotions (anger, disgust, fear, happiness, sadness and surprise) were found to have universal facial expressions <ref> [ Ekman, 1979 ] </ref> corresponding to prototypes. We have chosen to study all of them [ Pelachaud et al., 1991a ] .
Reference: [ Hadar et al., 1983 ] <author> U. Hadar, T.J. Steiner, E.C. Grant, and F. Clifford Rose. </author> <title> Kinematics of head movements accompanying speech during conversation. </title> <booktitle> Human Movement Science, </booktitle> <volume> 2 </volume> <pages> 35-46, </pages> <year> 1983. </year>
Reference-contexts: PUNCTUATORS: they can reduce the ambiguity of speech, grouping or separating sequence of words into discrete unit phrases. A boundary point (such as a comma) will be underlined by slow movement and a final pause will coincide with stillness <ref> [ Hadar et al., 1983 ] </ref> . Eyeblinks can occur also during pauses [ Argyle and Cook, 1976 ] . REGULATORS: they help the interaction between speaker-listener, they control the flow of speech. Head and eye movements are coordinated to synchronize speech and organize communication.
Reference: [ Hill et al., 1988 ] <author> D.R. Hill, A. Pearce, and B. Wyvill. </author> <title> Animating speech: an automated approach using speech synthesized by rules. </title> <journal> The Visual Computer, </journal> <volume> 3 </volume> <pages> 277-289, </pages> <year> 1988. </year>
Reference-contexts: Multi-layer approach [ Kalra et al., 1991 ] or addition of some speech parameters <ref> [ Hill et al., 1988 ] </ref> , [ Lewis and Parke, 1987 ] , [ Nahas, 1988 ] were included in animation systems to produce automatic lip synchronization.
Reference: [ Jeffers and Barley, 1971 ] <author> J. Jeffers and M. Barley. </author> <title> Speechreading (lipreading). </title> <type> C.C. </type> <institution> Thomas, </institution> <year> 1971. </year>
Reference-contexts: The chosen rules define the type and timing of occurrence of facial actions for each channel. After reading in the input file, the computation of lip shapes is done (see Fig. 1). We apply speechreading technique <ref> [ Jeffers and Barley, 1971 ] </ref> to group phonemes depending on their lip shapes. This clustering varies with the speech-rate. Coarticulation rules are then performed (see section 4.3). To each emotion corresponds a facial expression which serves as a base with whom the other facial actions are combined.
Reference: [ Kalra et al., 1991 ] <author> P. Kalra, A. Mangili, N. Magnenat-Thalmann, and D. Thalmann. Smile: </author> <title> A multilayered facial animation system. In T.L. </title> <editor> Kunii, editor, </editor> <booktitle> Modeling in Computer Graphics. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: Multi-layer approach <ref> [ Kalra et al., 1991 ] </ref> or addition of some speech parameters [ Hill et al., 1988 ] , [ Lewis and Parke, 1987 ] , [ Nahas, 1988 ] were included in animation systems to produce automatic lip synchronization.
Reference: [ Kent and Minifie, 1977 ] <author> R.D. Kent and F.D. Minifie. </author> <title> Coar-ticulation in recent speech production models. </title> <journal> Journal of Phonetics, </journal> <volume> 5 </volume> <pages> 115-135, </pages> <year> 1977. </year>
Reference-contexts: Coarticulation occurs due to the overlap of phonemic segments during their production. The boundary between segments are blurred. No complete set of rules exists. We have implemented look-ahead model <ref> [ Kent and Minifie, 1977 ] </ref> which considers articulatory adjustment on a sequence of consonants followed or preceded by a vowel. To counterbalance some of the unsolved problems due to the incompleteness of this model, geometric and temporal constraints were integrated in the set of rules.
Reference: [ Lewis and Parke, 1987 ] <author> J.P. Lewis and F.I. Parke. </author> <title> Automated lip-synch and speech synthesis for character animation. </title> <booktitle> CHI + GI, </booktitle> <pages> pages 143-147, </pages> <year> 1987. </year>
Reference-contexts: Multi-layer approach [ Kalra et al., 1991 ] or addition of some speech parameters [ Hill et al., 1988 ] , <ref> [ Lewis and Parke, 1987 ] </ref> , [ Nahas, 1988 ] were included in animation systems to produce automatic lip synchronization.
Reference: [ Nahas, 1988 ] <author> M. Nahas, H. Huitric, and M. Saintourens. </author> <title> Animation of b-spline figure. </title> <journal> The Visual Computer, </journal> <volume> 3 </volume> <pages> 272-276, </pages> <year> 1988. </year>
Reference-contexts: Multi-layer approach [ Kalra et al., 1991 ] or addition of some speech parameters [ Hill et al., 1988 ] , [ Lewis and Parke, 1987 ] , <ref> [ Nahas, 1988 ] </ref> were included in animation systems to produce automatic lip synchronization. Their approach was to express the animation by a set of rules where every element (of the sound and of the face) can be modified interactively through the use of parameters.
Reference: [ Parke, 1989 ] <author> F.I. Parke. </author> <title> Parameterized models for facial animation revisited. Siggraph'89 Course Notes, State in the Art in Facial Animation, </title> <year> 1989. </year>
Reference-contexts: It is the accumulation of such synchronized details which give expression and life to synthetic animation. Earlier systems <ref> [ Parke, 1989 ] </ref> used a set of conformation and expression parameters to perform the animation. Simulation of muscles of the face and of the propagation of their movement [ Platt, 1985 ] , [ Waters, 1987 ] produce more accurate facial actions.
Reference: [ Pelachaud et al., 1991a ] <author> C. Pelachaud, N.I. Badler, and M. Steedman. </author> <title> Linguistic issues in facial animation. </title> <editor> In N. Magnenat-Thalmann and D. Thalmann, editors, </editor> <booktitle> Computer Animation '91, </booktitle> <pages> pages 15-30. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: Six emotions (anger, disgust, fear, happiness, sadness and surprise) were found to have universal facial expressions [ Ekman, 1979 ] corresponding to prototypes. We have chosen to study all of them <ref> [ Pelachaud et al., 1991a ] </ref> .
Reference: [ Pelachaud, 1991b ] <author> C. Pelachaud. </author> <title> Communication and Coarticulation in Facial Animation. </title> <type> PhD thesis, </type> <institution> Computer and Information Science Department, University of Penn-sylvania, </institution> <address> Philadelphia, Pennsylvania, </address> <year> 1991. </year>
Reference-contexts: This is the basic principle which regulates the computa-tion of our facial animation. 3.2 Modularity The computation over the different channels is performed by a set of rules. These rules are derived from physiological, psychological, and linguistic considerations <ref> [ Pelachaud, 1991b ] </ref> . This scheme allows the user to modify, add, or delete a rule for one channel without modifying the other part of the system. Considering different channels for facial movement offers the possibility for researchers to experiment with their individual significance. <p> constraint) consider if the current speech posture has time to relax before the next speech posture (or, respectively, to contract after the previous one) otherwise the next speech posture is influenced by the relaxation of the current one (some action of the current speech posture remains over the next one) <ref> [ Pelachaud, 1991b ] </ref> . 4.4 Attenuation Rules The final animation is obtained by adding the list of facial action (set of AUS) occurring for each channel (AUs as defined in FACS work in an additive manner). Channels might involve the same facial regions. <p> The type of movement varies with the emotion (for comma, frown will be chosen in the case of anger) or with the type of pause (period will be marked by a frown, while a question mark with a raise of the eyebrows, especially when the question is not stated linguistically) <ref> [ Pelachaud, 1991b ] </ref> . When a blink occurs as conversational signal or punctuator (called voluntary blink) the program follows again the principle of synchrony to find its starting point.
Reference: [ Pierrehumbert and Hischberg, 1987 ] <author> J. Pierrehumbert and J. Hischberg. </author> <title> The meaning of intonational contours in the interpretation of discourse. </title> <type> Technical memorandum, </type> <institution> AT&T Bell Laboratories, </institution> <year> 1987. </year>
Reference-contexts: of a sound, pitch is a subjective one), loudness (the perceived intensity of a sound), pitch contour (the global envelope of the pitch), tempo (rate of speech) and pause. 2.2.1 Specification of the Intonation To define the syntactic structure of intonation, we are using an extension of Janet Pierrehumbert's notation <ref> [ Pierrehumbert and Hischberg, 1987 ] </ref> . Under this definition, intonation consists of a linear sequence of accents made from two tones (H and L for high and low tones respectively). Utterances are decomposed into intonational and intermediate phrases.
Reference: [ Pieper, 1992 ] <author> Steven D. </author> <title> Pieper CAPS: Computer-Aided Plastic Surgery. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, Cambridge, Massachusetts, </institution> <year> 1992. </year>
Reference-contexts: Modelling fl CIS department, Philadelphia, PA 19104 U.S.A y Domaine de Voluceau, BP 105, 78153 Le Chesnay, France z Domaine de Voluceau, BP 105, 78153 Le Chesnay, France the external and interface structures of the skin leads to a computer simulation of some plastic surgery operations <ref> [ Pieper, 1992 ] </ref> . Multi-layer approach [ Kalra et al., 1991 ] or addition of some speech parameters [ Hill et al., 1988 ] , [ Lewis and Parke, 1987 ] , [ Nahas, 1988 ] were included in animation systems to produce automatic lip synchronization.
Reference: [ Platt, 1985 ] <author> S.M. Platt. </author> <title> A Structural Model of the Human Face. </title> <type> PhD thesis, </type> <institution> Computer and Information Science Department, University of Pennsylvania, </institution> <address> Philadelphia, Penn-sylvania, </address> <year> 1985. </year>
Reference-contexts: It is the accumulation of such synchronized details which give expression and life to synthetic animation. Earlier systems [ Parke, 1989 ] used a set of conformation and expression parameters to perform the animation. Simulation of muscles of the face and of the propagation of their movement <ref> [ Platt, 1985 ] </ref> , [ Waters, 1987 ] produce more accurate facial actions.
Reference: [ Steedman, 1991 ] <author> M. Steedman. </author> <title> Structure and intonation. </title> <booktitle> Language, </booktitle> <volume> 67 </volume> <pages> 260-296, </pages> <year> 1991. </year>
Reference-contexts: The choice of intonational bracketing is obtained by the context the sentence is uttered and on the meaning of the utterance (what the speaker wants to focus on, what s/he considers as new information versus old) <ref> [ Steedman, 1991 ] </ref> . The input utterance "Julia prefers popcorn" has the following bracketing and set of accents in the context defined by the question: Question: Well, what about JUlia? What does SHE prefer? Answer: (JUlia prefers) (POPcorn).
Reference: [ Terzopoulos and Waters, 1991 ] <author> D. Terzopoulos and K. Waters. </author> <title> Techniques for realistic facial modelling and animation. </title> <editor> In N. Magnenat-Thalmann and D. Thalmann, editors, </editor> <booktitle> Computer Animation '91, </booktitle> <pages> pages 45-58. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: To gain higher-level control over facial animation, techniques of extracting contours over designated areas on a real actor's face recorded on video were developed <ref> [ Terzopoulos and Waters, 1991 ] </ref> . Facial expressions are often very subtle. The observation of details such as expressive wrinkles, and conversational signals in the speech, is an important feature of understanding human face.
Reference: [ Viaud and Yahia, 1992 ] <author> M.L.. Viaud and H. Yahia. </author> <title> Facial Animation with Wrinkles. </title> <address> Eurographics'92, </address> <month> September </month> <year> 1992. </year>
Reference-contexts: of the links between muscles are given by a real number varying 1 We have defined in an earlier publication how a database of the face can be rebuilt such that the isolines of the spline surface associated to the face are blended into the wrinkles lines of the face <ref> [ Viaud and Yahia, 1992 ] </ref> . We will assume the spline surface has these characteristics. between 0 (no link) and 1 (fully linked).
Reference: [ Waters, 1987 ] <author> K. Waters. </author> <title> A muscle model for animating three-dimensional facial expression. </title> <journal> Computer Graphics, </journal> <volume> 21(4) </volume> <pages> 17-24, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: Earlier systems [ Parke, 1989 ] used a set of conformation and expression parameters to perform the animation. Simulation of muscles of the face and of the propagation of their movement [ Platt, 1985 ] , <ref> [ Waters, 1987 ] </ref> produce more accurate facial actions.
References-found: 22

