URL: http://www.cs.berkeley.edu/~xiaoye/simax95.ps.gz
Refering-URL: http://www.cs.berkeley.edu/~xiaoye/
Root-URL: 
Title: A SUPERNODAL APPROACH TO SPARSE PARTIAL PIVOTING  
Author: JAMES W. DEMMEL STANLEY C. EISENSTAT JOHN R. GILBERT XIAOYE S. LI AND JOSEPH W. H. LIU 
Keyword: Key words. sparse matrix algorithms; unsymmetric linear systems; supernodes; column elimination tree; partial pivoting  
Note: AMS subject classifications. 65F05, 65F50  
Abstract: We investigate several ways to improve the performance of sparse LU factorization with partial pivoting, as used to solve unsymmetric linear systems. We introduce the notion of unsymmetric supernodes to perform most of the numerical computation in dense matrix kernels. We introduce unsymmetric supernode-panel updates and two-dimensional data partitioning to better exploit the memory hierarchy. We use Gilbert and Peierls's depth-first search with Eisenstat and Liu's symmetric structural reductions to speed up symbolic factorization. We have developed a sparse LU code using all these ideas. We present experiments demonstrating that it is significantly faster than earlier partial pivoting codes. We also compare performance with UMFPACK, which uses a multifrontal approach; our code is very competitive in time and storage requirements, especially for large problems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Agarwal, F. Gustavson, P. Palkar, and M. Zubair, </author> <title> A performance analysis of the subroutines in the ESSL/LAPACK call conversion interface (CCI). </title> <institution> IBM T.J. Watson Research Center, Yorktown Heights, </institution> <year> 1994. </year>
Reference-contexts: In the inner loops of our sparse code, we call the two Level 2 BLAS routines DTRSV (triangular solve) and DGEMV (matrix-vector multiply) provided in the IBM ESSL library [32], whose BLAS-3 matrix-matrix multiply routine (DGEMM) achieves about 250 Mflops when the dimension of the matrix is larger than 60 <ref> [1] </ref>. In our sparse algorithm, we find that DGEMV typically accounts for more than 80% of the floating-point operations. As shown in the second to last column of Table 4, this percentage is higher than 95% for many matrices.
Reference: [2] <author> P. R. Amestoy and I. Duff, MUPS: </author> <title> a parallel package for solving sparse unsymmetric sets of linear equations, </title> <type> tech. report, </type> <institution> CERFACS, Toulouse, France, </institution> <year> 1994. </year>
Reference-contexts: Submatrix methods typically use some form of Markowitz ordering with threshold pivoting, in which each stage's pivot element is chosen from the uneliminated submatrix by criteria that attempt to balance numerical quality and preservation of sparsity. Recent submatrix codes include Amestoy and Duff's symmetric pattern multifrontal code MUPS <ref> [2] </ref>, and Davis and Duff's unsymmetric multifrontal code UMFPACK [7]. Column methods, by contrast, typically use ordinary partial pivoting. 1 The pivot is chosen from the current column according to numerical considerations alone; the columns may be preordered before factorization to preserve sparsity.
Reference: [3] <author> E. Anderson et al., </author> <note> LAPACK User's Guide, Second Edition, SIAM, Philadelphia, </note> <year> 1995. </year>
Reference-contexts: Our techniques are successful in reducing the solution times for this type of problem. For a dense 1000 fi 1000 matrix, our code achieves 117 Mflops. This compares with 168 Mflops reported in the LAPACK manual <ref> [3] </ref> on a matrix of this size, and 236 Mflops reported in the online Linpack benchmark files [36]. 5.3. Comparison with previous column LU algorithms. <p> These are all based on the dense matrix routines in LAPACK <ref> [3] </ref>. In addition, SuperLU includes a Matlab mex-file interface, so that our factor and solve routines can be called as alternatives to those built into Matlab. 6.2. Effect of the matrix on performance. The supernodal approach reduces both symbolic and numeric computation time. <p> one panel, and perform the panel update to A (1: n; n d + 1: n). (One might want to split this panel up for better cache behavior.) Then the reduced matrix at the bottom right corner can be factored by calling an efficient dense code, for example, from LAPACK <ref> [3] </ref>. The dense code does not spend time on symbolic structure prediction and pruning, thus streamlining the numeric computation. We believe that, for large problems, the final dense submatrix will be big enough to make the switch beneficial.
Reference: [4] <author> C. Ashcraft and R. Grimes, </author> <title> The influence of relaxed supernode partitions on the multifrontal method, </title> <journal> ACM Trans. Mathematical Software, </journal> <volume> 15 (1989), </volume> <pages> pp. 291-309. </pages>
Reference-contexts: But the column ordering is fixed, for sparsity, before numeric factorization; what can we do? In symmetric Cholesky factorization, one type of supernodes|the "fundamental" supernodes|can can be made contiguous by permuting the matrix (symmetrically) according to a postorder on its elimination tree <ref> [4] </ref>. <p> In practice, the best values of r are generally between 4 and 8, and yield improvements in running time of 5% to 15%. Artificial supernodes are a special case of relaxed supernodes, which Duff and Reid [15] and Ashcraft and Grimes <ref> [4] </ref> have used in the context of multifrontal methods for systems with symmetric nonzero structure. They allow a small number of zeros in the structure of any supernode, thus relaxing the condition that the columns must have strictly nested structures.
Reference: [5] <author> C. Ashcraft, R. Grimes, J. Lewis, B. Peyton, and H. Simon, </author> <title> Progress in sparse matrix methods for large sparse linear systems on vector supercomputers, </title> <journal> Intern. J. of Supercomputer Applications, </journal> <volume> 1 (1987), </volume> <pages> pp. 10-30. </pages>
Reference-contexts: Normally, the solution process is performed in two phases: * Symbolic determination of the nonzero structure of the Cholesky factor; * Numeric factorization and solution. Elimination trees [35] and compressed subscripts [41] reduce the time and space for symbolic factorization. Supernodal <ref> [5] </ref> and multifrontal [15] elimination allow the use of dense vector operations for nearly all of the floating-point computation, thus reducing the symbolic overhead in numeric factorization. Overall, the Megaflop rates of modern sparse Cholesky codes are nearly comparable to those of dense solvers [37, 38, 39]. <p> Finally, Section 6 presents conclusions and open questions. 2. Unsymmetric supernodes. The idea of a supernode is to group together columns with the same nonzero structure, so they can be treated as a dense matrix for storage and computation. Supernodes were originally used for (symmetric) sparse Cholesky factorization <ref> [5, 15] </ref>. <p> Ng and Peyton reported that a sparse Cholesky algorithm based on sup-sup updates typically runs 2.5 to 4.5 times as fast as a col-col algorithm. Indeed, supernodes have become a standard tool in sparse Cholesky factorization <ref> [5, 37, 38, 43] </ref>. To sum up, supernodes as the source of updates help because: 1. The inner loop (over rows) has no indirect addressing. (Sparse Level 1 BLAS is replaced by dense Level 1 BLAS.) 2.
Reference: [6] <author> J. Bilmes, K. Asanovic, J. Demmel, D. Lam, and C.-W. Chin, </author> <title> Optimizing matrix multiply using PHiPAC: a portable, high-performance, ANSI C coding methodology, </title> <institution> Computer Science Dept. </institution> <type> Technical Report CS-96-326, </type> <institution> University of Tennessee, Knoxville, </institution> <month> May </month> <year> 1996. </year> <note> (LAPACK Working Note #111). </note>
Reference-contexts: The DGEMM and DGEMV Mflop rates were measured using vendor-supplied BLAS libraries. (Exception: Sun does not supply a BLAS library, so we report the DGEMM speed from PHiPAC <ref> [6] </ref>. PHiPAC does not include DGEMV.) Our UltraSparc-I has less physical memory than the other machines, so some large problems could not be tested on this machine. 5.2. Performance of SuperLU on an IBM RS/6000-590. Table 4 presents the performance of SuperLU on this system.
Reference: [7] <author> T. A. Davis, </author> <title> User's guide for the Unsymmetric-pattern MultiFrontal Package (UMFPACK), </title> <type> Tech. Report TR-95-004, </type> <institution> Computer and Information Sciences Department, University of Florida, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: Recent submatrix codes include Amestoy and Duff's symmetric pattern multifrontal code MUPS [2], and Davis and Duff's unsymmetric multifrontal code UMFPACK <ref> [7] </ref>. Column methods, by contrast, typically use ordinary partial pivoting. 1 The pivot is chosen from the current column according to numerical considerations alone; the columns may be preordered before factorization to preserve sparsity. <p> NumSym is the fraction of nonzeros matched by equal values in the symmetric locations. 30 present paper. Rather, to locate the performance of SuperLU in the constellation of linear solvers, we compare it with in detail with one alternative: UMFPACK version 2.1 <ref> [7, 8, 9] </ref>. This is a modern code that, like SuperLU, emphasizes unsym-metric structure and robustness for general problems. (A recent report [30] compares SuperLU and GP with some unsymmetric iterative algorithms.) UMFPACK uses a multifrontal algorithm. <p> UMFPACK is written in Fortran; we compiled it with the AIX xlf compiler with -O3 optimization, and linked it with the IBM ESSL library for BLAS calls. We used the parameter settings recommended by UMFPACK's authors <ref> [7] </ref>. UMFPACK does not include an initial column ordering step. For the initial column ordering in SuperLU, we ran Liu's multiple minimum degree algorithm [34] on the structure of A T A. We report times for ordering and factorization separately.
Reference: [8] <author> T. A. Davis and I. S. Duff, </author> <title> A combined unifrontal/multifrontal method for unsymmetric sparse matrices, </title> <type> Tech. Report TR-95-020, </type> <institution> Computer and Information Sciences Department, University of Florida, </institution> <year> 1995. </year> <title> [9] , An unsymmetric-pattern multifrontal method for sparse LU factorization, </title> <journal> SIAM J. Matrix Analysis and Applications, </journal> <volume> 18 (1997), </volume> <pages> pp. 140-158. </pages>
Reference-contexts: NumSym is the fraction of nonzeros matched by equal values in the symmetric locations. 30 present paper. Rather, to locate the performance of SuperLU in the constellation of linear solvers, we compare it with in detail with one alternative: UMFPACK version 2.1 <ref> [7, 8, 9] </ref>. This is a modern code that, like SuperLU, emphasizes unsym-metric structure and robustness for general problems. (A recent report [30] compares SuperLU and GP with some unsymmetric iterative algorithms.) UMFPACK uses a multifrontal algorithm.
Reference: [10] <author> T. A. Davis, J. R. Gilbert, S. I. Larimore, and E. Ng, </author> <title> Approximate minimum degree ordering for unsymmetric matrices. </title> <booktitle> SIAM Symposium on Applied Linear Algebra, </booktitle> <address> Snowbird, UT, </address> <month> October </month> <year> 1997. </year>
Reference-contexts: It should be noted that our current approach to ordering can be improved. For example, the column minimum degree algorithm used in Matlab [26] implements the minimum degree algorithm on A T A without explicitly forming the structure of A T A. In recent work, Davis et al. <ref> [10] </ref> are investigating better minimum degree algorithms for unsymmetric matrices that we expect to improve both fill and runtime. For 9 of the 13 problems whose dimensions are at least 10000, SuperLU outperforms UMFPACK both in factorization time and in memory. 6. Remarks. 6.1. The rest of the SuperLU package.
Reference: [11] <author> J. W. Demmel, J. R. Gilbert, and X. S. Li, </author> <title> An asynchronous parallel supernodal algorithm for sparse gaussian elimination, </title> <type> Tech. Report UCB//CSD-97-943, </type> <institution> Computer Science Division, U.C. Berkeley, </institution> <month> Feburary </month> <year> 1997. </year> <title> [12] , SuperLU User's Guide, </title> <type> Tech. Report UCB//CSD-97-944, </type> <institution> Computer Science Division, U.C. Berkeley, </institution> <month> February </month> <year> 1997. </year>
Reference-contexts: Again, it might be possible to use information about the Cholesky supernodes of A T A to guide this grouping. We are also developing a parallel sparse LU algorithm based on SuperLU <ref> [11, 33] </ref>. In this context, we target large problems, especially those too big to be solved on a uniprocessor system. Therefore, we plan to parallelize the 2-D blocked supernode 35 panel algorithm, which has very good asymptotic behavior for large problems.
Reference: [13] <author> D. Dodson and J. Lewis, </author> <title> Issues relating to extension of the Basic Linear Algebra Subprograms, </title> <journal> ACM SIGNUM Newsletter, </journal> <volume> 20 (1985), </volume> <pages> pp. 2-18. </pages>
Reference-contexts: Figure 1 sketches a generic left-looking column LU factorization. 2 Notice that the bulk of the numeric computation occurs in column-column updates, or, to use BLAS terminology <ref> [13, 14] </ref>, in sparse AXPYs. In column methods, the preordering for sparsity is completely separate from the factorization, just as in the symmetric case. This is an advantage when several matrices with the same nonzero structure but different numerical values must be factored.
Reference: [14] <author> J. J. Dongarra, J. D. Croz, S. Hammarling, and R. J. Hanson, </author> <title> An extended set of basic linear algebra subroutines, </title> <journal> ACM Trans. Mathematical Software, </journal> <volume> 14 (1988), </volume> <pages> pp. 1-17, 18-32. </pages>
Reference-contexts: Figure 1 sketches a generic left-looking column LU factorization. 2 Notice that the bulk of the numeric computation occurs in column-column updates, or, to use BLAS terminology <ref> [13, 14] </ref>, in sparse AXPYs. In column methods, the preordering for sparsity is completely separate from the factorization, just as in the symmetric case. This is an advantage when several matrices with the same nonzero structure but different numerical values must be factored.
Reference: [15] <author> I. Duff and J. Reid, </author> <title> The multifrontal solution of indefinite sparse symmetric linear equations, </title> <journal> ACM Trans. Mathematical Software, </journal> <volume> 9 (1983), </volume> <pages> pp. 302-325. </pages>
Reference-contexts: Normally, the solution process is performed in two phases: * Symbolic determination of the nonzero structure of the Cholesky factor; * Numeric factorization and solution. Elimination trees [35] and compressed subscripts [41] reduce the time and space for symbolic factorization. Supernodal [5] and multifrontal <ref> [15] </ref> elimination allow the use of dense vector operations for nearly all of the floating-point computation, thus reducing the symbolic overhead in numeric factorization. Overall, the Megaflop rates of modern sparse Cholesky codes are nearly comparable to those of dense solvers [37, 38, 39]. <p> Finally, Section 6 presents conclusions and open questions. 2. Unsymmetric supernodes. The idea of a supernode is to group together columns with the same nonzero structure, so they can be treated as a dense matrix for storage and computation. Supernodes were originally used for (symmetric) sparse Cholesky factorization <ref> [5, 15] </ref>. <p> In practice, the best values of r are generally between 4 and 8, and yield improvements in running time of 5% to 15%. Artificial supernodes are a special case of relaxed supernodes, which Duff and Reid <ref> [15] </ref> and Ashcraft and Grimes [4] have used in the context of multifrontal methods for systems with symmetric nonzero structure. They allow a small number of zeros in the structure of any supernode, thus relaxing the condition that the columns must have strictly nested structures.
Reference: [16] <author> I. Duff and J. K. Reid, </author> <title> The design of MA48, a code for the direct solution of sparse un-symmetric linear systems of equations, </title> <journal> ACM Trans. Mathematical Software, </journal> <volume> 22 (1996), </volume> <pages> pp. 187-226. </pages>
Reference-contexts: The Harwell library code MA48 <ref> [18, 16] </ref> employs such a switch to dense code, which has a significant and beneficial effect on performance. To enhance SuperLU's performance on small and extremely sparse problems, it would be possible to make a choice at runtime whether to use supernode-panel, supernode-column, or column-column updates.
Reference: [17] <author> I. S. Duff, R. Grimes, and J. Lewis, </author> <title> Sparse matrix test problems, </title> <journal> ACM Trans. Mathematical Software, </journal> <volume> 15 (1989), </volume> <pages> pp. 1-14. 36 </pages>
Reference-contexts: Unsymmetric supernodes seem harder to characterize, but they also are related to dense subma-trices arising from fill. We measured the supernodes according to each definition for 126 unsymmetric matrices from the Harwell-Boeing sparse matrix test collection <ref> [17] </ref> under various column orderings. Table 1 shows, for each definition, the fraction of nonzeros of L that are not in the first column of a supernode; this measures how much row index storage is saved by using supernodes. <p> We compare the performance of SuperLU, our supernode-panel code, with its predecessors, and with one other approach to sparse LU factorization. 5.1. Experimental setup. Table 2 lists 23 matrices with some characteristics of their nonzero structures. Some of the matrices are from the Harwell-Boeing collection <ref> [17] </ref>.
Reference: [18] <author> I. S. Duff and J. K. Reid, MA48, </author> <title> a Fortran code for direct solution of sparse unsymmetric linear systems of equations, </title> <type> Tech. Report RAL-93-072, </type> <institution> Rutherford Appleton Laboratory, Oxon, UK, </institution> <year> 1993. </year>
Reference-contexts: The Harwell library code MA48 <ref> [18, 16] </ref> employs such a switch to dense code, which has a significant and beneficial effect on performance. To enhance SuperLU's performance on small and extremely sparse problems, it would be possible to make a choice at runtime whether to use supernode-panel, supernode-column, or column-column updates.
Reference: [19] <author> S. C. Eisenstat, J. R. Gilbert, and J. W. Liu, </author> <title> A supernodal approach to a sparse partial pivoting code, </title> <booktitle> in Householder Symposium XII, </booktitle> <year> 1993. </year>
Reference-contexts: The blocking parameters for SuperLU are w = 8; t = 100 and b = 200. column code GP by Gilbert and Peierls [29] (Figure 1), GP-Mod by Eisenstat and Liu [21] (Section 4.2), and SupCol by Eisenstat, Gilbert and Liu <ref> [19] </ref> (Figure 6). GP and GP-Mod were written in Fortran. SupCol was first written in Fortran, and later translated literally into C; no changes in algorithms or data structures were made in this translation.
Reference: [20] <author> S. C. Eisenstat and J. W. H. Liu, </author> <title> Exploiting structural symmetry in sparse unsymmetric symbolic factorization, </title> <journal> SIAM J. Matrix Analysis and Applications, </journal> <volume> 13 (1992), </volume> <pages> pp. </pages> <month> 202-211. </month> <title> [21] , Exploiting structural symmetry in a sparse partial pivoting code, </title> <journal> SIAM J. Scientific and Statistical Computing, </journal> <volume> 14 (1993), </volume> <pages> pp. 253-257. </pages>
Reference-contexts: Pruning the symbolic structure. We can speed up the depth-first search traversals by using a reduced graph in place of G, the reverse of the graph of L (: ; J ). We have explored this idea in a series of papers <ref> [20, 21, 25] </ref>. Any graph H can be substituted for G, provided that i H =) j if and only if i G =) j. The traversals are more efficient if H has fewer edges; but any gain in efficiency must be traded off against the cost of computing H. <p> An extreme choice of H is the elimination dag (directed acyclic graph) [25], which is the transitive reduction of G, or the minimal subgraph of G that preserves paths. However, the elimination dag is expensive to compute. The symmetric reduction <ref> [20] </ref> is a subgraph that has (in general) fewer edges than G but more edges than the elimination dag, and that is much less expensive to compute.
Reference: [22] <author> J. A. George and E. Ng, </author> <title> An implementation of Gaussian elimination with partial pivoting for sparse systems, </title> <journal> SIAM J. Scientific and Statistical Computing, </journal> <volume> 6 (1985), </volume> <pages> pp. </pages> <month> 390-409. </month> <title> [23] , Symbolic factorization for sparse Gaussian elimination with partial pivoting, </title> <journal> SIAM J. Scientific and Statistical Computing, </journal> <volume> 8 (1987), </volume> <pages> pp. 877-898. </pages>
Reference-contexts: Thus column codes must do some symbolic factorization at each stage; typically this amounts to predicting the structure of each column of the factors immediately before computing it. (George and Ng <ref> [22, 23] </ref> described ways to obtain upper bounds on the structure of the factors based only on the nonzero structure of the original matrix.) A disadvantage of the column methods is that they do not reorder the columns dynamically, so there may be more fill. <p> T5. Supernodes of A T A: A supernode is a range (r: s) of columns of L corresponding to a Cholesky supernode of the symmetric matrix A T A. T5 super-nodes are motivated by George and Ng's observation <ref> [22] </ref> that (with suitable 4 T1 T2 T3 T4 median 0.236 0.345 0.326 0.006 mean 0.284 0.365 0.342 0.052 Table 1 Fraction of nonzeros not in first column of supernode. 0 B B B B B B B 1 * * * 3 * * * * 7 * * *
Reference: [24] <author> J. R. Gilbert, </author> <title> Predicting structure in sparse matrix computations, </title> <journal> SIAM J. Matrix Analysis and Applications, </journal> <volume> 15 (1994), </volume> <pages> pp. 62-79. </pages>
Reference-contexts: The notation i M =) j means that there is a directed path from i to j in the directed graph of M . Such a path may have length zero; that is, i =) i always holds. Theorem 4.1. <ref> [24] </ref> f ij is nonzero (equivalently, i F ! j) if and only if i L (:;J) A for some k i. 13 This result implies that the symbolic factorization of column j can be obtained as follows.
Reference: [25] <author> J. R. Gilbert and J. W. H. Liu, </author> <title> Elimination structures for unsymmetric sparse LU factors, </title> <journal> SIAM J. Matrix Analysis and Applications, </journal> <volume> 14 (1993), </volume> <pages> pp. 334-352. </pages>
Reference-contexts: Pruning the symbolic structure. We can speed up the depth-first search traversals by using a reduced graph in place of G, the reverse of the graph of L (: ; J ). We have explored this idea in a series of papers <ref> [20, 21, 25] </ref>. Any graph H can be substituted for G, provided that i H =) j if and only if i G =) j. The traversals are more efficient if H has fewer edges; but any gain in efficiency must be traded off against the cost of computing H. <p> The traversals are more efficient if H has fewer edges; but any gain in efficiency must be traded off against the cost of computing H. An extreme choice of H is the elimination dag (directed acyclic graph) <ref> [25] </ref>, which is the transitive reduction of G, or the minimal subgraph of G that preserves paths. However, the elimination dag is expensive to compute.
Reference: [26] <author> J. R. Gilbert, C. Moler, and R. Schreiber, </author> <title> Sparse matrices in Matlab: Design and implementation, </title> <journal> SIAM J. Matrix Analysis and Applications, </journal> <volume> 13 (1992), </volume> <pages> pp. 333-356. </pages>
Reference-contexts: We now study the reasons and remedies for this. To implement loops (3-7) of Figure 7, we first expand the nonzeros of the panel columns of A into an n by w full working array, called the SPA (for sparse accumulator <ref> [26] </ref>). This allows random access to the entries of the active panel. A temporary array stores the results of the BLAS operations, and the updates are scattered into the SPA. At the end of panel factorization, the data in the SPA are copied into storage for L and U . <p> The reason for this order will be described in more detail in Section 5.4. This paper does not address the performance of column preordering for sparsity. We simply use the existing ordering algorithms provided by Matlab <ref> [26] </ref>. For all matrices, except 1, 15 and 21, the columns were permuted by Matlab's minimum degree ordering of A T A, also known as "column minimum degree". <p> GP and GP-Mod were written in Fortran. SupCol was first written in Fortran, and later translated literally into C; no changes in algorithms or data structures were made in this translation. SuperLU is written in C. (Matlab contains C implementations of GP and GP-Mod <ref> [26] </ref>, which we did not test here.) For the Fortran codes, we use Fortran 77 compilers; for the C codes, we use ANSI C compilers. In all cases, we use highest possible optimization provided by each compiler. Both SupCol and SuperLU call Level 2 BLAS routines. <p> Fig. 19. Compare SuperLU to UMF-PACK, when MMD ordering time is in cluded. the MMD ordering takes significantly more time than factorization. It should be noted that our current approach to ordering can be improved. For example, the column minimum degree algorithm used in Matlab <ref> [26] </ref> implements the minimum degree algorithm on A T A without explicitly forming the structure of A T A. In recent work, Davis et al. [10] are investigating better minimum degree algorithms for unsymmetric matrices that we expect to improve both fill and runtime.
Reference: [27] <author> J. R. Gilbert and E. Ng, </author> <title> Predicting structure in nonsymmetric sparse matrix factorizations, in Graph Theory and Sparse Matrix Computation, </title> <editor> A. George, J. R. Gilbert, and J. W. H. Liu, eds., </editor> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: The column etree of A is the (symmetric) elimination tree of the column intersection graph of A, or equivalently the elimination tree of A T A provided there is no cancellation in computing A T A. See Gilbert and Ng <ref> [27] </ref> for complete definitions. The column etree can be computed from A in time almost linear in the number of nonzeros of A by a variation of an algorithm of Liu [35]. <p> Note that column i updates column j in LU factorization if and only if u ij 6= 0. Theorem 2.1 (Column Elimination Tree). <ref> [27] </ref> Let A be a square, nonsingular, possibly unsymmetric matrix, and let P A = LU be any factorization of A with pivoting by row interchanges. Let T be the column elimination tree of A. 1.
Reference: [28] <author> J. R. Gilbert, E. G. Ng, and B. W. Peyton, </author> <title> An efficient algorithm to compute row and column counts for sparse Cholesky factorization, </title> <journal> SIAM J. Matrix Analysis and Applications, </journal> <volume> 15 (1994), </volume> <pages> pp. 1075-1091. </pages>
Reference-contexts: The choice would depend on the size of the matrix A and the expected properties of its supernodes; it might be based on an efficient symbolic computation of the density and supernode distribution of the Cholesky factor of A T A <ref> [28] </ref>. Could we make supernode-panel updates more effective by improving the similarity between the row structures of the columns in a panel? We believe this could be accomplished with a more sophisticated column permutation strategy.
Reference: [29] <author> J. R. Gilbert and T. Peierls, </author> <title> Sparse partial pivoting in time proportional to arithmetic operations, </title> <journal> SIAM J. Scientific and Statistical Computing, </journal> <volume> 9 (1988), </volume> <pages> pp. 862-874. </pages>
Reference-contexts: An early example of such a code is Sherman's NSPIV [42] (which is actually a row code). Gilbert and Peierls <ref> [29] </ref> showed how to use depth-first search and topological ordering to get the structure of each factor column. <p> Indeed, F (: ; j) has the same structure as the solution vector for the following triangular system <ref> [29] </ref>: @ @ @ @ @ L (: ; J ) I A straightforward way to compute the structure of F (: ; j) from the structures of L (: ; J ) and A (: ; j) is to simulate the numerical computation. <p> These updates must be applied in an order consistent with a topological ordering of G. We use depth-first search to perform the traversal, which makes it possible to generate a topological order (specifically, reverse postorder) on the nonzeros of U (: ; j) as they are located <ref> [29] </ref>. Another consequence of the path theorem is the following corollary. It says that if we divide each column of U into segments, one per supernode, then within each segment the column of U just consists of a consecutive sequence of nonzeros. <p> The blocking parameters for SuperLU are w = 8; t = 100 and b = 200. column code GP by Gilbert and Peierls <ref> [29] </ref> (Figure 1), GP-Mod by Eisenstat and Liu [21] (Section 4.2), and SupCol by Eisenstat, Gilbert and Liu [19] (Figure 6). GP and GP-Mod were written in Fortran.
Reference: [30] <author> J. R. Gilbert and S. Toledo, </author> <title> An assesment of incomplete lu preconditioners for nonsymmetric linear systems, 1997. </title> <type> Manuscript. </type>
Reference-contexts: Rather, to locate the performance of SuperLU in the constellation of linear solvers, we compare it with in detail with one alternative: UMFPACK version 2.1 [7, 8, 9]. This is a modern code that, like SuperLU, emphasizes unsym-metric structure and robustness for general problems. (A recent report <ref> [30] </ref> compares SuperLU and GP with some unsymmetric iterative algorithms.) UMFPACK uses a multifrontal algorithm.
Reference: [31] <author> A. Gupta and V. Kumar, </author> <title> Optimally scalable parallel sparse cholesky factorization, </title> <booktitle> in The 7th SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <year> 1995, </year> <pages> pp. 442-447. </pages>
Reference-contexts: Therefore, we plan to parallelize the 2-D blocked supernode 35 panel algorithm, which has very good asymptotic behavior for large problems. The 2-D block-oriented layout has been shown to scale well for parallel sparse Cholesky factorization <ref> [31, 40] </ref>. Acknowledgements. We thank Rob Schreiber and Ed Rothberg for very helpful discussions during the development of SuperLU. Rob pushed us to find a way to do the supernode-panel update; Ed suggested the ops-per-ref statistic, and also provided access to the SGI MIPS R8000.
Reference: [32] <institution> International Business Machines Corporation Engineering and Scientific Subroutine Library, </institution> <note> Guide and Reference. Version 2 Release 2, Order No. SC23-0526-01, </note> <year> 1994. </year>
Reference-contexts: All floating-point computations are performed in double precision. In the inner loops of our sparse code, we call the two Level 2 BLAS routines DTRSV (triangular solve) and DGEMV (matrix-vector multiply) provided in the IBM ESSL library <ref> [32] </ref>, whose BLAS-3 matrix-matrix multiply routine (DGEMM) achieves about 250 Mflops when the dimension of the matrix is larger than 60 [1]. In our sparse algorithm, we find that DGEMV typically accounts for more than 80% of the floating-point operations.
Reference: [33] <author> X. S. Li, </author> <title> Sparse Gaussian elimination on high performance computers, </title> <type> Tech. Report UCB//CSD-96-919, </type> <institution> Computer Science Division, U.C. Berkeley, </institution> <month> September </month> <year> 1996. </year> <type> Ph.D dissertation. </type>
Reference-contexts: Again, it might be possible to use information about the Cholesky supernodes of A T A to guide this grouping. We are also developing a parallel sparse LU algorithm based on SuperLU <ref> [11, 33] </ref>. In this context, we target large problems, especially those too big to be solved on a uniprocessor system. Therefore, we plan to parallelize the 2-D blocked supernode 35 panel algorithm, which has very good asymptotic behavior for large problems.
Reference: [34] <author> J. W. Liu, </author> <title> Modification of the minimum degree algorithm by multiple elimination, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 11 (1985), </volume> <pages> pp. 141-153. </pages>
Reference-contexts: We used the parameter settings recommended by UMFPACK's authors [7]. UMFPACK does not include an initial column ordering step. For the initial column ordering in SuperLU, we ran Liu's multiple minimum degree algorithm <ref> [34] </ref> on the structure of A T A. We report times for ordering and factorization separately. <p> In addition to the LU factorization kernel described in this paper, we have developed a suite of supporting routines to solve linear systems, all of which are available from Netlib. 4 The complete SuperLU package [12] includes column preordering for sparsity, based on Liu's multiple minimum degree algorithm <ref> [34] </ref> applied to the structure of A T A. (As noted above, we plan to replace this with a new code that does not form A T A.) SuperLU also includes condition number estimation, iterative refinement of solutions, and componentwise error bounds for the refined solutions.
Reference: [35] <author> J. W. H. Liu, </author> <title> The role of elimination trees in sparse factorization, </title> <journal> SIAM J. Matrix Analysis and Applications, </journal> <volume> 11 (1990), </volume> <pages> pp. 134-172. </pages>
Reference-contexts: The problem of solving sparse symmetric positive definite sys tems of linear equations on sequential and vector processors is fairly well understood. Normally, the solution process is performed in two phases: * Symbolic determination of the nonzero structure of the Cholesky factor; * Numeric factorization and solution. Elimination trees <ref> [35] </ref> and compressed subscripts [41] reduce the time and space for symbolic factorization. Supernodal [5] and multifrontal [15] elimination allow the use of dense vector operations for nearly all of the floating-point computation, thus reducing the symbolic overhead in numeric factorization. <p> This postorder is an example of what Liu calls an equivalent reordering <ref> [35] </ref>, which does not change the sparsity of the 6 1 2 8 B @ s 1 s 1 s 1 s 1 C A 6 10 B @ s 2 s 2 C A 8 0 B B s 3 s 3 s 3 s 3 s 3 s 3 <p> See Gilbert and Ng [27] for complete definitions. The column etree can be computed from A in time almost linear in the number of nonzeros of A by a variation of an algorithm of Liu <ref> [35] </ref>. The following theorem says that the column etree represents potential dependencies among columns in LU factorization, and that (for strong Hall matrices) no stronger information is obtainable from the nonzero structure of A. <p> Then P T LP and P T U P are respectively unit lower triangular and upper triangular, so A = (P T LP )(P T U P ) is also an LU factorization. Remark 2.3. Liu <ref> [35] </ref> attributes to F. Peters a result similar to part (3) for the symmetric positive definite case, concerning the Cholesky factor and the (usual, symmetric) elimination tree. Proof. Part (1) is immediate from the definition of P . Part (2) follows from Corollary 6.2 in Liu [35], with the symmetric structure <p> Remark 2.3. Liu <ref> [35] </ref> attributes to F. Peters a result similar to part (3) for the symmetric positive definite case, concerning the Cholesky factor and the (usual, symmetric) elimination tree. Proof. Part (1) is immediate from the definition of P . Part (2) follows from Corollary 6.2 in Liu [35], with the symmetric structure of the column intersection graph of our matrix A taking the place of Liu's symmetric matrix A. (Liu exhibits the isomorphism explicitly in the proof of his Theorem 6.1.) Now we prove part (3).
Reference: [36] <author> PDS: </author> <title> The performance database server, </title> <note> http://performance.netlib.org/performance/, May 1995. </note>
Reference-contexts: For a dense 1000 fi 1000 matrix, our code achieves 117 Mflops. This compares with 168 Mflops reported in the LAPACK manual [3] on a matrix of this size, and 236 Mflops reported in the online Linpack benchmark files <ref> [36] </ref>. 5.3. Comparison with previous column LU algorithms.
Reference: [37] <author> E. G. Ng and B. W. Peyton, </author> <title> Block sparse Cholesky algorithms on advanced uniprocessor computers, </title> <journal> SIAM J. Scientific and Statistical Computing, </journal> <volume> 14 (1993), </volume> <pages> pp. 1034-1056. </pages>
Reference-contexts: Supernodal [5] and multifrontal [15] elimination allow the use of dense vector operations for nearly all of the floating-point computation, thus reducing the symbolic overhead in numeric factorization. Overall, the Megaflop rates of modern sparse Cholesky codes are nearly comparable to those of dense solvers <ref> [37, 38, 39] </ref>. For unsymmetric systems, where pivoting is required to maintain numerical sta bility, progress has been less satisfactory. Recent research has concentrated on two fl Computer Science Division, University of California, Berkeley, CA 94720 (dem-mel@cs.berkeley.edu). <p> Rothberg and Gupta [38, 39] and Ng and Peyton <ref> [37] </ref> analyzed the effect of super-nodes in Cholesky factorization on modern uniprocessor machines with memory hierarchies and vector or superscalar hardware. All the updates from columns of a supernode are summed into a dense vector before the sparse update is performed. <p> Ng and Peyton reported that a sparse Cholesky algorithm based on sup-sup updates typically runs 2.5 to 4.5 times as fast as a col-col algorithm. Indeed, supernodes have become a standard tool in sparse Cholesky factorization <ref> [5, 37, 38, 43] </ref>. To sum up, supernodes as the source of updates help because: 1. The inner loop (over rows) has no indirect addressing. (Sparse Level 1 BLAS is replaced by dense Level 1 BLAS.) 2. <p> As described in Section 4, the symbolic phase determines the value of k, that is, the position of the first nonzero in the segment U (r: s; j). The advantages of using supernode-column updates are similar to those in the symmetric case <ref> [37] </ref>. Efficient Level 2 BLAS matrix-vector kernels can be used for the triangular solve and matrix-vector multiply. Furthermore, all the updates from the supernodal columns can be collected in a dense vector before doing a single scatter into the target vector. This reduces the amount of indirect addressing. 3.2. <p> This is analogous to loop tiling techniques used in optimizing compilers to improve cache behavior for two-dimensional arrays with regular stride. It is also somewhat analogous to the supernode-supernode updates that Ng and Peyton <ref> [37] </ref> and Rothberg and Gupta [38] have used in symmetric Cholesky factorization. The structure of each supernode-column update is the same as in the supernode-column algorithm.
Reference: [38] <author> E. Rothberg and A. Gupta, </author> <title> Efficient sparse matrix factorization on high-performance workstations exploiting the memory hierarchy, </title> <journal> ACM Trans. Mathematical Software, </journal> <volume> 17 (1991), </volume> <pages> pp. </pages> <month> 313-334. </month> <title> [39] , An evaluation of left-looking, right-looking and multifrontal approaches to sparse Cholesky factorization on hierarchical-memory machines, </title> <journal> Int. J. High Speed Computing, </journal> <volume> 5 (1993), </volume> <pages> pp. 537-593. </pages>
Reference-contexts: Supernodal [5] and multifrontal [15] elimination allow the use of dense vector operations for nearly all of the floating-point computation, thus reducing the symbolic overhead in numeric factorization. Overall, the Megaflop rates of modern sparse Cholesky codes are nearly comparable to those of dense solvers <ref> [37, 38, 39] </ref>. For unsymmetric systems, where pivoting is required to maintain numerical sta bility, progress has been less satisfactory. Recent research has concentrated on two fl Computer Science Division, University of California, Berkeley, CA 94720 (dem-mel@cs.berkeley.edu). <p> Rothberg and Gupta <ref> [38, 39] </ref> and Ng and Peyton [37] analyzed the effect of super-nodes in Cholesky factorization on modern uniprocessor machines with memory hierarchies and vector or superscalar hardware. All the updates from columns of a supernode are summed into a dense vector before the sparse update is performed. <p> Ng and Peyton reported that a sparse Cholesky algorithm based on sup-sup updates typically runs 2.5 to 4.5 times as fast as a col-col algorithm. Indeed, supernodes have become a standard tool in sparse Cholesky factorization <ref> [5, 37, 38, 43] </ref>. To sum up, supernodes as the source of updates help because: 1. The inner loop (over rows) has no indirect addressing. (Sparse Level 1 BLAS is replaced by dense Level 1 BLAS.) 2. <p> This is analogous to loop tiling techniques used in optimizing compilers to improve cache behavior for two-dimensional arrays with regular stride. It is also somewhat analogous to the supernode-supernode updates that Ng and Peyton [37] and Rothberg and Gupta <ref> [38] </ref> have used in symmetric Cholesky factorization. The structure of each supernode-column update is the same as in the supernode-column algorithm.
Reference: [40] <author> E. E. Rothberg and A. Gupta, </author> <title> An efficient block-oriented approach to parallel sparse cholesky factorization, </title> <booktitle> in Supercomputing, </booktitle> <month> November </month> <year> 1993, </year> <pages> pp. 503-512. </pages>
Reference-contexts: Therefore, we plan to parallelize the 2-D blocked supernode 35 panel algorithm, which has very good asymptotic behavior for large problems. The 2-D block-oriented layout has been shown to scale well for parallel sparse Cholesky factorization <ref> [31, 40] </ref>. Acknowledgements. We thank Rob Schreiber and Ed Rothberg for very helpful discussions during the development of SuperLU. Rob pushed us to find a way to do the supernode-panel update; Ed suggested the ops-per-ref statistic, and also provided access to the SGI MIPS R8000.
Reference: [41] <author> A. H. Sherman, </author> <title> On the efficient solution of sparse systems of linear and nonlinear equations, </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <year> 1975. </year> <title> [42] , Algorithm 533: NSPIV, a FORTRAN subroutine for sparse Gaussian elimination with partial pivoting, </title> <journal> ACM Trans. Mathematical Software, </journal> <volume> 4 (1978), </volume> <pages> pp. 391-398. </pages>
Reference-contexts: Normally, the solution process is performed in two phases: * Symbolic determination of the nonzero structure of the Cholesky factor; * Numeric factorization and solution. Elimination trees [35] and compressed subscripts <ref> [41] </ref> reduce the time and space for symbolic factorization. Supernodal [5] and multifrontal [15] elimination allow the use of dense vector operations for nearly all of the floating-point computation, thus reducing the symbolic overhead in numeric factorization. <p> This is similar to the effect of compressed subscripts in the symmetric case <ref> [41] </ref>.
Reference: [43] <author> H. Simon, P. Vu, and C. Yang, </author> <title> Performance of a supernodal general sparse solver on the CRAY Y-MP: 1.68 GFLOPS with autotasking, </title> <type> Tech. Report TR SCA-TR-117, </type> <institution> Boeing Computer Services, </institution> <year> 1989. </year>
Reference-contexts: Ng and Peyton reported that a sparse Cholesky algorithm based on sup-sup updates typically runs 2.5 to 4.5 times as fast as a col-col algorithm. Indeed, supernodes have become a standard tool in sparse Cholesky factorization <ref> [5, 37, 38, 43] </ref>. To sum up, supernodes as the source of updates help because: 1. The inner loop (over rows) has no indirect addressing. (Sparse Level 1 BLAS is replaced by dense Level 1 BLAS.) 2.
Reference: [44] <author> S. A. Vavasis, </author> <title> Stable finite elements for problems with wild coefficients, </title> <type> Tech. Report 93-1364, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, NY, </address> <year> 1993. </year> <note> To appear in SIAM J. Numerical Analysis. 37 </note>
Reference-contexts: Wang3 is from solving a coupled nonlinear PDE system in a 3-D (30 fi 30 fi 30 uniform mesh) semiconductor device simulation, as provided by Song Wang of the University of New South Wales, Sydney. Vavasis3 is an unsymmetric augmented matrix for a 2-D PDE with highly varying coefficients <ref> [44] </ref>. Dense1000 is a dense 1000 fi 1000 random matrix.
References-found: 38

