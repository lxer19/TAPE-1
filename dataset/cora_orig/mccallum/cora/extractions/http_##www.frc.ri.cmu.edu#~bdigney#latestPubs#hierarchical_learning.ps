URL: http://www.frc.ri.cmu.edu/~bdigney/latestPubs/hierarchical_learning.ps
Refering-URL: http://www.frc.ri.cmu.edu/~bdigney/latestPubs/
Root-URL: 
Email: bdigney@ri.cmu.edu  
Phone: Phone (412) 268-7084  
Title: Learning Hierarchical Control Structures for Multiple Tasks and Changing Environments  
Author: Bruce L. Digney 
Address: Pittsburgh, PA, 15232, USA  
Affiliation: Robotics Institute Carnegie Mellon University  
Abstract: While the need for hierarchies within control systems is apparent, it is also clear to many researchers that such hierarchies should be learned. Learning both the structure and the component behaviors is a difficult task. The benefit of learning the hierarchical structures of behaviors is that the decomposition of the control structure into smaller transportable chunks allows previously learned knowledge to be applied to new but related tasks. Presented in this paper are improvements to Nested Q-learning (NQL) that allow more realistic learning of control hierarchies in reinforcement environments. Also presented is a simulation of a simple robot performing a series of related tasks that is used to compare both hierarchical and non-hierarchal learning techniques. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A.G. Sutton, R. and Watkins, C. </author> <year> (1989). </year> <title> Learning and sequential decision making. </title> <note> In COINS Technical Report. </note>
Reference-contexts: Previous work in Nested Q-learning (NQL) (Digney, 1996) demonstrated the autonomous construction of a control hierarchy able to learn both the structure and the behaviors using a reinforcement learning technique based on Q-learning <ref> (Barto and Watkins, 1989) </ref>.
Reference: <author> Dayan, P. and Hinton, G. </author> <year> (1993). </year> <title> Feudal reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems 5, </booktitle> <address> San Mateco, CA. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: By abstracting away many details of a complex world to lower level behaviors, learning can more easily be implemented in the higher levels. Many researchers have recognized the need for hierarchical structures in learning control systems (Maes and Brooks, 1990) (Long-Ji, 1993) <ref> (Dayan and Hinton, 1993) </ref> (Singh, 1992). These approaches are all similar in the respect that the structures are hand designed and individual components are learned or vise versa.
Reference: <author> Digney, B. </author> <year> (1996). </year> <title> Emergent hierarchical control structures: Learning reactive/hierarchical relationships in reinforcement environments. </title> <editor> In Materic, editor, </editor> <booktitle> From animals to animats 4: </booktitle> <volume> SAB 96, </volume> <pages> pages 363-373, </pages> <address> Cape Cod, USA. </address> <publisher> MIT Press-Bradford Books. </publisher>
Reference-contexts: Although the imposed hierarchies result in more tractable problems, they also impose the designers preconceived notions on the control system. It is desirable that the control system able to generate its own hierarchical structure. Recently, work on Nested Q-learning (NQL) <ref> (Digney, 1996) </ref> has shown that a hierarchical structure can be learned in a reinforcement learning environment. Although this method generated hierarchical structures, it was considered to be seriously handicapped by the need to classify every distinct sensory state as a feature. <p> These approaches are all similar in the respect that the structures are hand designed and individual components are learned or vise versa. Previous work in Nested Q-learning (NQL) <ref> (Digney, 1996) </ref> demonstrated the autonomous construction of a control hierarchy able to learn both the structure and the behaviors using a reinforcement learning technique based on Q-learning (Barto and Watkins, 1989). <p> Now that the two criteria for useful feature emergence have been developed, the extension to NQL can be discussed. The core of the NQL algorithm remains very much as previously described <ref> (Digney, 1996) </ref>. However, instead of having all possible behaviors discovered and available for learning and use very soon after the animat begins operation, features emerge and are learned and used continuously throughout the life of the animat. <p> Features may emerge at any time during operation and their corresponding behaviors are added to the evolving behavior pool. Incorporating these newly introduced behaviors into the action selection mechanism is required. This makes action selection more difficult than in the action selection mechanism described in previous work <ref> (Digney, 1996) </ref>. For the top-down goal directed action/behavior selection, a random based exploration policy is used. Although this is not the most effective exploration policy, it is easily implemented and other more efficient forms of exploration have been studied in depth elsewhere (Thurn, 1992). <p> The bottom-up or sensory based action selection mechanism and how it fits with the top-down action selection to result in a flexible reactive control hierarchy is described elsewhere <ref> (Digney, 1996) </ref> and is not explored in this paper. 4. Simulation To evaluate the NQL algorithm and it's capabilities, (the emergent features and the subsequent learning of a hierarchical structure of behaviors), an animat and the two dimensional environment of Figure 2 were used. <p> The animat was placed in a grid world 6 by 10 units in size as shown in Figure 2 (b). This grid world was bounded by an impassable barrier and additional barriers could be erected within the world in any configuration desired. Different from related work <ref> (Digney, 1996) </ref> in which the animat had a number of different sensors, the animat in this study was only capable of perceiving its location within the grid world. <p> Two goals, A and B, were activated and goal C was left undetectable. The control system initially had no behaviors available to it as shown in Figure 3 (a) and its actions were thoroughly random and undirected. This can be contrasted to previous work <ref> (Digney, 1996) </ref> in which the control system was quickly flooded with potential useful behaviors and then had to sort out the useful from the many irrelevant. Once placed in its world, the animat randomly moved about undi-rected.
Reference: <author> Digney, B. </author> <year> (1998). </year> <title> Learning in continuous domains with delayed rewards. </title> <note> In SAB 98 (submitted). </note>
Reference-contexts: Most notably, the technique described in this paper still uses discrete spaces, states, actions and behaviors and generalization within these states, actions and behaviors is not possible. Operation within continuous space and generalization methods for NQL are currently being pursued <ref> (Digney, 1998) </ref> . This should allow the operation of applications in more realistic situations with real sensors and actuators. Also, the results presented indicated that the animat was beginning to view its world at a rudimentary level of abstrac tion.
Reference: <author> Long-Ji, L. </author> <year> (1993). </year> <title> Hierarchical learning of robot skills. </title> <booktitle> In IEEE International Conference on Neural Networks, </booktitle> <pages> pages 181-186, </pages> <address> San Francisco. </address> <publisher> IEEE Press. </publisher>
Reference: <author> Maes, P. and Brooks, R. </author> <year> (1990). </year> <title> Learning to coordinate behaviors. </title> <booktitle> In Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 796-802. </pages>
Reference-contexts: By abstracting away many details of a complex world to lower level behaviors, learning can more easily be implemented in the higher levels. Many researchers have recognized the need for hierarchical structures in learning control systems <ref> (Maes and Brooks, 1990) </ref> (Long-Ji, 1993) (Dayan and Hinton, 1993) (Singh, 1992). These approaches are all similar in the respect that the structures are hand designed and individual components are learned or vise versa.
Reference: <author> Singh, P. </author> <year> (1992). </year> <title> Transfer of learning by composing solutions of elemental sequential tasks. </title> <journal> Machine Learning, </journal> 8(3/4):323-339. 
Reference-contexts: By abstracting away many details of a complex world to lower level behaviors, learning can more easily be implemented in the higher levels. Many researchers have recognized the need for hierarchical structures in learning control systems (Maes and Brooks, 1990) (Long-Ji, 1993) (Dayan and Hinton, 1993) <ref> (Singh, 1992) </ref>. These approaches are all similar in the respect that the structures are hand designed and individual components are learned or vise versa.
Reference: <author> Thurn, S. </author> <year> (1992). </year> <title> Efficient exploration in reinforcement learning. </title> <note> In Technical Report CMU-CS-92-102. </note> <institution> Carnegie Mellon University, School of Computer Science. </institution>
Reference-contexts: For the top-down goal directed action/behavior selection, a random based exploration policy is used. Although this is not the most effective exploration policy, it is easily implemented and other more efficient forms of exploration have been studied in depth elsewhere <ref> (Thurn, 1992) </ref>.
Reference: <author> Tyrrel, T. </author> <year> (1992). </year> <title> The use of hierarchies for action selection. </title> <booktitle> In From animals to animats 2: </booktitle> <volume> SAB 92, </volume> <pages> pages 138-148, </pages> <address> Massachussets. </address> <publisher> MIT Press-Bradford Books. </publisher>
Reference-contexts: As only features with a high probability of relevance emerge learning will be much faster. A hierarchical learning control system is developed and tested against a non-hierarchical learning system. 2. Background An issue closely linked to robot learning is the architectures in which the control strategies are implemented <ref> (Tyrrel, 1992) </ref>. The two main ideologies are flat and hierarchical. Figure 1 shows the flat and the hierarchical architectures schematically. Flat architectures have direct connections from sensors to actions, through a single level of control. When some situation is perceived all behaviors compete for control with the strongest response winning.
References-found: 9

