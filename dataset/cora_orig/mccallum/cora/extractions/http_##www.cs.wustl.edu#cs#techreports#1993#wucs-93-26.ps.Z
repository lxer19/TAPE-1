URL: http://www.cs.wustl.edu/cs/techreports/1993/wucs-93-26.ps.Z
Refering-URL: http://www.cs.wustl.edu/cs/cs/publications.html
Root-URL: 
Title: TRAINREC: A System for Training Feedforward Simple Recurrent Networks Efficiently and Correctly  
Author: Barry L. Kalman and Stan C. Kwasny 
Note: This material is based upon work supported by the National Science Foundation under Grant No. IRI-9201987.  
Date: May 1993  
Pubnum: WUCS-93-26  
Abstract-found: 0
Intro-found: 0
Reference: [B92] <author> E. Barnard, </author> <title> Optimization for training neural nets, </title> <journal> IEEE Trans. Neural Networks, </journal> <volume> Vol. 3, No. 2, </volume> <month> March </month> <year> 1992, </year> <pages> pp. 232-241. </pages>
Reference-contexts: Others (see, for example <ref> [B92] </ref>) have also found it very useful. Because of quadratic conver 17 gence it uses fewer epochs (often many fewer) than backprop. The CGA requires linear storage (in weights) while Newton-style quadratically convergent methods require quadratic storage. CGAs generalization properties are very close to those of backprop.
Reference: [E88] <author> J. L. Elman, </author> <title> Finding Structure in Time, </title> <type> CRL Technical Report 8801, </type> <institution> Center for Research in Language, University of California, </institution> <address> San Diego, </address> <year> 1988. </year>
Reference-contexts: Table 2 summarizes these networks. The middle number for each network size is the number of hidden units. The percentage of the variables taken up with skip connections reects the degree of linearity present in the problem. For our recurrent networks we use only feedback from the hidden layer <ref> [E88] </ref>. We call the nodes to which the outputs of the hidden units are fed back the pseudo input layer. We connect the pseudo input layer just as we do the ordinary input layer.
Reference: [GGJ91] <author> Shelly D. D. Goggin, Karl E. Gustafson and Kristina M. Johnson, </author> <title> An Asymptotic Singular Value Decomposition Analysis of Nonlinear Multilayer Neural Networks, </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, </booktitle> <month> July </month> <year> 1991, </year> <editor> v. </editor> <volume> 1, </volume> <pages> pp. 785-790. </pages>
Reference: [GL83] <author> Gene H. Golub and Charles F. Van Loan, </author> <booktitle> Matrix Computations, </booktitle> <pages> pp 16-20 and 285-295, </pages> <publisher> The Johns Hopkins University Press, </publisher> <year> 1983. </year>
Reference: [GL89] <author> Gene H. Golub and Charles F. Van Loan, </author> <title> Matrix Computations 2d Edition, page 42, </title> <publisher> The Johns Hopkins University Press, </publisher> <year> 1989. </year>
Reference-contexts: We believe we have created data structures for d, b, q and w vectors that place their values at a unit stride <ref> [GL89] </ref> apart so that our code may be nearly optimized for this critical section of derivative computation for a machine architecture that allows pipelined oating point computation.
Reference: [JKK93] <author> Sahnny Johnson, Stan C. Kwasny and Barry L. </author> <title> Kalman, An Adaptive Neural Network Parser, </title> <publisher> forthcoming. </publisher>
Reference-contexts: In our NETTalk experiment, there were 5,523 patterns, but only 5,208 of those led to unique states. Therefore, no more than 13 feedback units should be required, which we have verified empirically. In training a recurrent neural network to be a deterministic parser for English <ref> [JKK93] </ref>, we found that the 7,659 patterns resulted in 5,433 unique states which suggested an initial choice of 13 feedback units. This too has been verified experimentally.
Reference: [K90] <author> Barry L. </author> <title> Kalman, Superlinear Learning in Back-Propagation Neural Networks, </title> <type> Technical Report WUCS-90-21, </type> <institution> Washington University, </institution> <month> June, </month> <year> 1990. </year>
Reference-contexts: Because of quadratic conver 17 gence it uses fewer epochs (often many fewer) than backprop. The CGA requires linear storage (in weights) while Newton-style quadratically convergent methods require quadratic storage. CGAs generalization properties are very close to those of backprop. We originally reported on the behavior of CGA in <ref> [K90] </ref>. Code for the CGA is discussed in [P71 and PFTV88].
Reference: [KH91] <author> Gary M. Kuhn and Norman P. Herzberg, </author> <title> Some Variations on Training of Recurrent Networks, </title> <publisher> Academic Press, </publisher> <year> 1991. </year>
Reference: [KK91] <author> Barry L. Kalman and Stan C. Kwasny, </author> <title> A Superior Error Function for Training Neural Networks, </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, </booktitle> <month> July </month> <year> 1991, </year> <editor> v. </editor> <volume> 2, </volume> <pages> pp. 49-52. 29 </pages>
Reference-contexts: These are the Kalman-Kwasny error function, skip connections, singular value decomposition of the inputs and use of vector best match as a correctness criterion. 2.1 The Kalman-Kwasny Error Function We first presented the Kalman-Kwasny error function in <ref> [KK91] </ref> and we gave its derivation along with the importance of the hyperbolic tangent sigmoidal in [KK92]. Here we summarize those ideas based on [KK92]. First we present the desirable behaviors of an error function on a finite interval. Next we show the derivation of the error function. <p> In <ref> [KK91] </ref> we reported on the performance of our error function on the problem of training a feedforward network to be a deterministic parser (see Marcus [M80]) for a medium s pk ki pi b += pk pk 1 s - s' l 1 s -( )= s x ( ) lx <p> The network had 53 inputs, 22 outputs, 30 hidden units and no skip connections. The data set consisted of 113 training patterns. Table 1 contains a summary of results from <ref> [KK91] </ref>. These results are strong evidence for using the Kalman-Kwasny error function. 2.2 Using Skip Connections Most problems have a significant linear component in their solutions. Some require no non-linear component and are therefore solvable by a perceptron.
Reference: [KK92] <author> Barry L. Kalman and Stan C. Kwasny, </author> <title> Why Tanh: Choosing a Sigmoidal Function, </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, </booktitle> <month> June </month> <year> 1992, </year> <editor> v. </editor> <booktitle> IV, </booktitle> <pages> pp 578-581. </pages>
Reference-contexts: Kalman-Kwasny error function, skip connections, singular value decomposition of the inputs and use of vector best match as a correctness criterion. 2.1 The Kalman-Kwasny Error Function We first presented the Kalman-Kwasny error function in [KK91] and we gave its derivation along with the importance of the hyperbolic tangent sigmoidal in <ref> [KK92] </ref>. Here we summarize those ideas based on [KK92]. First we present the desirable behaviors of an error function on a finite interval. Next we show the derivation of the error function. Finally, we review how to scale the sigmoidal function for maximum fairness. <p> of the inputs and use of vector best match as a correctness criterion. 2.1 The Kalman-Kwasny Error Function We first presented the Kalman-Kwasny error function in [KK91] and we gave its derivation along with the importance of the hyperbolic tangent sigmoidal in <ref> [KK92] </ref>. Here we summarize those ideas based on [KK92]. First we present the desirable behaviors of an error function on a finite interval. Next we show the derivation of the error function. Finally, we review how to scale the sigmoidal function for maximum fairness. <p> finite interval force the following differential equation to hold: (EQ 8) For the interval [-1, 1], the only functional form which satisfies EQ 8 is: (EQ 9) If e pk = (t pk - a pk ), this relation leads to the error function: (EQ 10) We have shown in <ref> [KK92] </ref> that selecting the value: (EQ 11) maintains an equitable scaling of weight layers for training purposes.
Reference: [KKA93] <author> Barry L. Kalman, Stan C. Kwasny and Aurorita Abella, </author> <title> Decomposing Input Patterns to Facilitate Training, </title> <booktitle> Proceedings of the World Congress on Neural Networks, </booktitle> <address> Portland, OR, </address> <month> July </month> <year> 1993, </year> <month> forthcoming. </month>
Reference-contexts: TABLE 1. Results for Medium Grammar Problem Error Function Optimization Method Epochs Presentations CPU Time (sec) Sum of Squares Backprop Kalman-Kwasny Backprop 13000 146900 230185 Sum of Squares Conjugate Gradient 639 72151 6001 Kalman-Kwasny Conjugate Gradient 237 26781 2300 12 In <ref> [KKA93] </ref> we reported on several problems on which we used TRAINREC. We employed skip connections in all of these cases. <p> We connect the pseudo input layer just as we do the ordinary input layer. This is illustrated in FIGURE 2 ABOUT HERE 2.3 Singular Value Decomposition for Preprocessing Input We discussed the use of singular value decomposition (SVD) to preprocess input in <ref> [KKA93] </ref>. We reported that the use of SVD along with an affine transformation to place the inputs in the interval [-1, 1] often allowed training to proceed when it was previously impossible. <p> After training it is useful to transform the parameters of the network to a network which uses the original input. We show in <ref> [KKA93] </ref> that for weights, , on connections which involve the input units, weights that perform equivalently on the original patterns can be obtained as follows: (EQ 18) A UGV = I' I i 1 k p ki s max Q = i s r - d c 2 i i A'
Reference: [KKEW92] <author> Stan C. Kwasny, Barry L. Kalman, A. Maynard Engebretson, and Weilan Wu, </author> <title> Identifying Language from Speech: An Example of High-Level, Statistically-Based Feature Extraction, </title> <booktitle> Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society, </booktitle> <month> July, </month> <year> 1992, </year> <pages> pp. 909-914. </pages>
Reference-contexts: By reducing the frequency of derivative computations, we improve overall efficiency greatly. Use of settling and voting. In training a SRN for tasks involving noisy data (for example, our work on identifying which of two languages is being spoken <ref> [KKEW92] </ref> and [WKKE93]) we found that allowing the network to settle over a prefix of patterns is very helpful.
Reference: [KKC93] <author> Stan C. Kwasny, Barry L. Kalman and Nancy Chang, </author> <title> Distributed Patterns as Hierarchical Structures, </title> <booktitle> Proceedings of the World Congress on Neural Networks, </booktitle> <address> Port-land, OR, </address> <month> July </month> <year> 1993, </year> <month> forthcoming. </month>
Reference-contexts: Because the growth of the recurrent iterations is , we are careful to limit the growth of the hidden layer. Here we analyze RAAMs where O = H + I. The SRN version of RAAMs that we use is described in <ref> [KKC93] </ref>. By definition, we are not able to use skip connections with our RAAMs.
Reference: [LH92] <author> Samuel E. Lee and Bradley R. Holt, </author> <title> Regression Analysis of Spectroscopic Process Data Using a Combined Architecture of Linear and Nonlinear Artificial Neural Networks, </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, </booktitle> <month> June </month> <year> 1992, </year> <editor> v. </editor> <booktitle> IV, </booktitle> <pages> pp 549-554. </pages>
Reference-contexts: Some require no non-linear component and are therefore solvable by a perceptron. Connection of input units to output units places the linear component of the solution in those connections while the non-linear component is isolated in the hidden layer weights <ref> [LH92] </ref>. For a simple example, consider the simplest architecture capable of learning XOR. Connecting inputs to outputs is required to give a 2-1-1 network. Other evidence comes from work on genetic algorithms used to prune neural nets [WB90] which shows that connections from input to output are important.
Reference: [M80] <author> Mitchell Marcus, </author> <title> A Theory of Syntactic Recognition for Natural Language, </title> <publisher> MIT Press, </publisher> <year> 1980. </year>
Reference-contexts: In [KK91] we reported on the performance of our error function on the problem of training a feedforward network to be a deterministic parser (see Marcus <ref> [M80] </ref>) for a medium s pk ki pi b += pk pk 1 s - s' l 1 s -( )= s x ( ) lx ( )tanh= g e 2 pk - l 1.5= 11 sized subset of English grammar.
Reference: [MGB74] <author> Alexander M. Mood, Franklin A. Graybill and Duane C. Boes, </author> <title> Introduction to the Theory of Statistics, </title> <publisher> 3rd ed., McGraw-Hill, </publisher> <year> 1974. </year>
Reference-contexts: This measure tends to be high when performance is distributed over both frequent and infrequent categories. Unfortunately it can be high under other conditions. The confusion matrix is the same as a two-way contingency table discussed in <ref> [MGB74] </ref>. Construction of the confusion matrix is discussed in Section 2.4. The third measure concerns maximizing worst case performance. We save the network whenever the observed lowest fraction correct in the test set categories increases.
Reference: [P71] <author> E. Polak, </author> <title> Computational Methods in Optimization: A Unified Approach, </title> <publisher> Academic Press, </publisher> <year> 1971. </year> <month> 30 </month>
Reference: [P90] <author> Jordan Pollack, </author> <title> Recursive Distributed Representations, </title> <booktitle> Artificial Intelligence 46, </booktitle> <pages> 77-105. </pages>
Reference-contexts: This means that the forward connection from a context node only exists with the hidden node that spawned it. More general architec tures are desired. Static targets for Recursive Auto-Associative Memories. RAAMs cleverly use auto-asso ciativity to permit representation of complex structures as distributed patterns (see <ref> [P90] </ref> and [KK93]). In our experience, the effect of the variability of the target is usu ally not properly considered under standard implementations of backprop. 1.2 Primary Discoveries In building TRAINREC we made several discoveries which are helping us improve the efficiency of training.
Reference: [PFTV88] <author> William H. Press, Brian P. Flannery, Saul A. Teukolsky and William T. Vetter-ing, </author> <title> Numerical Recipes in C, </title> <publisher> Cambridge University Press, </publisher> <year> 1988. </year>
Reference-contexts: The results in Table 1 also give strong evidence for the use of CGA over ordinary backprop. 3.2 Derivative-Free Line Search The CGA requires a line search algorithm to find a local minimum in a particular direction for each iteration. Two line search algorithms are Brent and Dbrent <ref> [PFTV88] </ref>. Dbrent requires the magnitude of the gradient in the search direction for each inner iteration. Brent only requires the value of the error function for each inner iteration. First we analyze training for ordinary feedforward networks to see the effect of using DFLS. <p> The algorithm mnbrak <ref> [PFTV88] </ref> locates such a region. It requires three points along the search direction to initialize it. At the beginning of CGA we use the ratio of the error function to the magnitude of the gradient as a start for mnbrak. <p> If making the correct prediction for the smaller category is important such a network will be a failure. The second measure is a computation (as specified in <ref> [PFTV88] </ref>) on the confusion matrix. This measure tends to be high when performance is distributed over both frequent and infrequent categories. Unfortunately it can be high under other conditions. The confusion matrix is the same as a two-way contingency table discussed in [MGB74].
Reference: [SR87] <author> Terrence J. Sejnowski and Charles R. Rosenberg, </author> <title> Parallel Networks that Learn to Pronounce English Text, </title> <journal> Complex Systems 1, </journal> <year> 1987, </year> <pages> pp 145-168. </pages>
Reference-contexts: VBM works especially well when one output unit is designated as a catchall for cases where the predicted category is none of the those which come from the model. This is because every vector is orthogonal to the zero vector. VBM is used quite effectively, for example, in NetTalk <ref> [SR87] </ref>. c 16 In our work on RAAMs, we have both distributed patterns and category vectors as outputs. To test which symbol is represented by the output vector that is extracted we use VBM. The choice of the empty symbol is very important in this work.
Reference: [SCM88] <author> David Servan-Schreiber, Axel Cleeremans and James L. McClelland, </author> <title> Encoding Sequential Structure in Simple Recurrent Networks, </title> <type> Technical Report CMU-CS-88-183, </type> <institution> Carnegie Mellon University, </institution> <month> November, </month> <year> 1988. </year>
Reference: [VB93] <author> Jacques de Villiers and Etienne Barnard, </author> <title> Backpropagation Neural Nets with One and Two Hidden Layers, </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol 4., No. 1, </volume> <month> January </month> <year> 1993, </year> <pages> pp 136-141. </pages>
Reference-contexts: De Villiers and Barnard found that with nearly equal numbers of weights feedforward networks with one and two hidden layers performed equally well and networks with two hidden layers had more frequent occurrences of local extrema during training <ref> [VB93] </ref>. If one is testing for the optimal number of hidden units for a network then the case of zero hidden units is a natural limiting case. Of course, TRAINREC permits disconnection of inputs and outputs by user choice. This is an essential choice, of course, for RAAM networks.
Reference: [WB90] <author> Darrell Whitley and Christopher Bogart, </author> <title> The Evolution of Connectivity: Pruning Neural Networks Using Genetic Algorithms, </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, </booktitle> <month> January </month> <year> 1990, </year> <editor> v. </editor> <volume> 1, </volume> <pages> pp 134-7. </pages>
Reference-contexts: For a simple example, consider the simplest architecture capable of learning XOR. Connecting inputs to outputs is required to give a 2-1-1 network. Other evidence comes from work on genetic algorithms used to prune neural nets <ref> [WB90] </ref> which shows that connections from input to output are important. Without skip connections more nodes are needed in the hidden layer to account for the linear component of the solution. We use one hidden layer because it is the smallest number for which non-linear separation can be effected.
Reference: [WK90] <author> Sholom M. Weiss and Casimir A. </author> <title> Kulikowski, Computer Systems That Learn, </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1990. </year>
Reference: [WKKE93] <author> Weilan Wu, Stan C. Kwasny, Barry L. Kalman and A. Maynard Engebretson, </author> <title> Identifying Language from Raw Speech: An Application of Recurrent Neural Networks, </title> <booktitle> Proceedings of the Midwest Artificial Intelligence and Cognitive Science Conference, </booktitle> <month> April </month> <year> 1993, </year> <pages> pp 53-57. 31 </pages>
Reference-contexts: By reducing the frequency of derivative computations, we improve overall efficiency greatly. Use of settling and voting. In training a SRN for tasks involving noisy data (for example, our work on identifying which of two languages is being spoken [KKEW92] and <ref> [WKKE93] </ref>) we found that allowing the network to settle over a prefix of patterns is very helpful. <p> But for a sequence of events in which each event is supposed to predict the same output, we can think of the N column as being an upper bound of the length of sequence needed for virtually always correct prediction. In <ref> [WKKE93] </ref> we reported on a recurrent network which was trained by TRAINREC to identify which of two languages a speaker was using. The overall performance on patterns in the test set was 78.9%. The worst case was 100/190 where 96/190 would have been TABLE 3.

References-found: 25

