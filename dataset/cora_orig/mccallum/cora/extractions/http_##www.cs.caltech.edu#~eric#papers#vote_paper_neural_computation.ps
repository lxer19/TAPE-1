URL: http://www.cs.caltech.edu/~eric/papers/vote_paper_neural_computation.ps
Refering-URL: http://www.cs.caltech.edu/~eric/papers/papers.html
Root-URL: http://www.cs.caltech.edu
Email: (eric@cs.caltech.edu).  
Title: Validation of Voting Committees  
Author: Eric Bax 
Keyword: Key words machine learning, learning theory, fusion, committees, voting, Vapnik-Chervonenkis, linear programming.  
Address: ifornia, 91125  
Affiliation: Computer Science Department, California Institute of Technology 256-80, Pasadena, Cal  
Date: June 2, 1997  
Abstract: This paper contains a method to bound the test errors of voting committees with members chosen from a pool of trained classifiers. There are so many prospective committees that validating them directly does not achieve useful error bounds. Because there are fewer classifiers than prospective committees, it is better to validate the classifiers individually, then use linear programming to infer committee error bounds. We test the method using credit card data. Also, we extend the method to infer bounds for classifiers in general. 
Abstract-found: 1
Intro-found: 1
Reference: [Abu-Mostafa 1996] <author> Y. Abu-Mostafa, </author> <title> What you need to know about the VC Inequality, Class notes from CS156, </title> <institution> California Institute of Technology, </institution> <year> 1996. </year>
Reference-contexts: The first bound is weaker, but it is smooth. We will use it for analysis. The second bound is stronger. We will use it for tests on real data. For the first bound, we modify a simplified treatment of VC error bounds <ref> [Vapnik and Chervonenkis 1971, Abu-Mostafa 1996] </ref>. Suppose we have M classifiers. Let d be the number of validation examples, and let d 0 be the number of test examples. Let -m be the validation error of classifier m, and let -0 m be the test error.
Reference: [Bax, Cataltepe, and Sill 1997] <author> E. Bax, Z. Cataltepe, and J. Sill, </author> <title> Alternative error bounds for the classifier chosen by early stopping, </title> <publisher> CalTech-CS-TR-97-08. </publisher>
Reference-contexts: Now, identify the snapshot with minimum validation error. It will be delivered as the result of training. To bound its test error, use the linear program with contraints provided by the uniformly bounded classifiers. For suggested methods of choosing the constraint classifiers, see <ref> [Bax, Cataltepe, and Sill 1997] </ref>. The linear program yields strong bounds when the constraint classifiers have low error rates, so the method works best with trained constraint classifiers. Disagreement among constraint classifiers encourages strong bounds. <p> Also, having one or more constraint classifiers with high rates of agreement with the classifier being error-bounded yields strong bounds. This occurs when the constraint classifiers are drawn from the training sequence <ref> [Bax, Cataltepe, and Sill 1997] </ref> to bound the classifier chosen by early stopping. It also occurs for stacking and fusion, both through data-fitting and by design [Breiman 1992]. This paper focused on binary classification problems.
Reference: [Breiman 1992] <author> L. Breiman, </author> <title> Stacked regressions, </title> <type> Tech. Rep. No. 367, </type> <institution> Statistics Dept., Univ. of California at Berkeley (1992). </institution>
Reference-contexts: This occurs when the constraint classifiers are drawn from the training sequence [Bax, Cataltepe, and Sill 1997] to bound the classifier chosen by early stopping. It also occurs for stacking and fusion, both through data-fitting and by design <ref> [Breiman 1992] </ref>. This paper focused on binary classification problems. It would be interesting to extend the error bounding method to other problem types, e.g., regression problems. 16 8 Acknowledgements Thanks to Joel Franklin for advice, teaching, and encouragement.
Reference: [Feller 1968] <author> W. Feller, </author> <title> An Introduction to Probability Theory and Its Applications, </title> <publisher> John Wiley and Sons, Inc. </publisher> <year> 1968. </year>
Reference-contexts: Validating individual classifiers to use the linear program gives O ( p ln n). Validating committees directly gives O ( q n 2 + 1, for example, then n q n 2 n (see <ref> [Feller 1968] </ref> p. 180), and * grows as O ( p If the committee size is fixed, then n k! .
Reference: [Franklin 1980] <author> J. Franklin, </author> <title> Methods of Mathematical Economics pp.68-79, </title> <publisher> Springer-Verlag New York, Inc. </publisher> <year> 1980. </year>
Reference-contexts: then the distribution of agreements can be estimated by random sampling of inputs. (Note that example outputs are not needed to compute the distribution of agreements.) Estimation errors will affect the solution of the linear program; the relationship between the errors and the solution can be calculated by perturbation methods <ref> [Franklin 1980] </ref>. 10 5 General Classifier Bound The linear programming method can be extended to compute a test error bound for any classifier, not just a voting committee. Let g 1 ; : : : ; g n be classifiers trained without reference to some validation data.
Reference: [Garey and Johnson 1979] <author> M. R. Gaery and D. S. Johnson, </author> <title> Computers and Intractability, A Guide to the Theory of NP-Completeness, </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <address> New York 1979. p.245. </address>
Reference-contexts: It may be difficult to solve for large problems, since ILP is NP-hard <ref> [Garey and Johnson 1979, Karp 1972] </ref>. Relaxing the integer constraints on the variables to 0 x j 1 8 j 2 f1; : : : ; d 0 g (22) produces a linear program.
Reference: [Hoeffding 1963] <author> W. Hoeffding, </author> <title> Probability inequalities for sums of bounded random variables, </title> <journal> Am. Stat. Assoc. J., </journal> (1963):13-30. 
Reference-contexts: Let m be the error rate over the entire input distribution, i.e. the expected error rate for a random data set. For a single classifier selected without reference to the validation or test data, Hoeffding's inequality <ref> [Hoeffding 1963, Vapnik 1982] </ref> implies Prf m -m + *g e 2* 2 d for * &gt; 0 (1) Prf-0 for * &gt; 0 (2) Using both bounds, Prf-0 m m + 2 * g 2e 1 where D = min (d; d 0 ).
Reference: [Karp 1972] <author> R. M. Karp, </author> <title> Reducibility among combinatorial problems, </title> <editor> in R. E. Miller and J. W. Thatcher (eds.), </editor> <title> Complexity of Computer Computations, </title> <publisher> Plenum Press, </publisher> <address> New York 1972. </address> <pages> pp. 85-103. </pages>
Reference-contexts: It may be difficult to solve for large problems, since ILP is NP-hard <ref> [Garey and Johnson 1979, Karp 1972] </ref>. Relaxing the integer constraints on the variables to 0 x j 1 8 j 2 f1; : : : ; d 0 g (22) produces a linear program.
Reference: [Kim and Bartlett 1995] <author> K. Kim and E. B. Bartlett, </author> <title> Error estimation by series association for neural network systems, </title> <journal> Neural Computation, </journal> <volume> 7(1995) </volume> <pages> 799-808. </pages>
Reference: [Sill and Abu-Mostafa 1997] <author> J. Sill and Y. Abu-Mostafa, </author> <title> Monotonicity hints, </title> <booktitle> to appear in Advances in Neural Information Processing Systems, </booktitle> <volume> 9. </volume> <year> 1997. </year>
Reference-contexts: The discrete-valued traits were removed, leaving the six continous-valued traits. Of the 690 examples in the original database, 24 examples had at least one trait missing. These examples were removed, leaving 666 examples. The data were cleaned by Joseph Sill. For further information, see <ref> [Sill and Abu-Mostafa 1997] </ref>. In each test, the 666 examples were randomly partitioned into 444 training examples, d = 111 validation examples, and d 0 = 111 test examples. In each test, a pool of classifiers was trained by early stopping.
Reference: [Sridhar, Seagrave, and Bartlett 1996] <author> D. V. Sridhar, R. C. Seagrave, and E. B. Bartlett, </author> <title> Process modeling using stacked neural networks, </title> <journal> AIChE Journal, </journal> <volume> 42(9) </volume> (1996):2529-2539. 
Reference: [Vapnik 1982] <author> V. N. Vapnik, </author> <title> Estimation of Dependences Based on Empirical Data p.31, </title> <publisher> Springer-Verlag New York, Inc. </publisher> <year> 1982. </year>
Reference-contexts: Let m be the error rate over the entire input distribution, i.e. the expected error rate for a random data set. For a single classifier selected without reference to the validation or test data, Hoeffding's inequality <ref> [Hoeffding 1963, Vapnik 1982] </ref> implies Prf m -m + *g e 2* 2 d for * &gt; 0 (1) Prf-0 for * &gt; 0 (2) Using both bounds, Prf-0 m m + 2 * g 2e 1 where D = min (d; d 0 ).
Reference: [Vapnik and Chervonenkis 1971] <author> V. N. Vapnik and A. Chervonenkis, </author> <title> On the uniform convergence of relative frequencies of events to their probabilities, </title> <journal> Theory Prob. Appl., </journal> <volume> 16(1971) </volume> <pages> 264-280. </pages>
Reference-contexts: In the next section, which focuses on validation, we derive VC-type <ref> [Vapnik and Chervonenkis 1971] </ref> uniform bounds on test error. To achieve useful error bounds through validation, the number of classifiers must be small. Since the classifier pool is much smaller than the number of prospective commitees, the classifier errors can be bounded more precisely than the committee errors. <p> The first bound is weaker, but it is smooth. We will use it for analysis. The second bound is stronger. We will use it for tests on real data. For the first bound, we modify a simplified treatment of VC error bounds <ref> [Vapnik and Chervonenkis 1971, Abu-Mostafa 1996] </ref>. Suppose we have M classifiers. Let d be the number of validation examples, and let d 0 be the number of test examples. Let -m be the validation error of classifier m, and let -0 m be the test error. <p> reduces the waste by using uniform bound (4) only over the pool of classifiers, then using linear programming to infer uniform bounds over the prospective committees. 5 Now we develop the second error bound, which is a generalization of the bound at the heart of the original VC bound proof <ref> [Vapnik and Chervonenkis 1971] </ref>. For a single classifier, condition the bound on a given multiset of b = d + d 0 inputs composing the validation and test inputs. Since the inputs are drawn i.i.d., each partition of the inputs into validation and test sets is equally likely.
Reference: [Wolpert 1992] <author> D. H. Wolpert, </author> <title> Stacked generalization, </title> <booktitle> Neural Networks, </booktitle> <volume> 5(1992) </volume> <pages> 241-259. </pages>
References-found: 14

