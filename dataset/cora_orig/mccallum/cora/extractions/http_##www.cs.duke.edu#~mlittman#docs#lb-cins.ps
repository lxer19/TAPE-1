URL: http://www.cs.duke.edu/~mlittman/docs/lb-cins.ps
Refering-URL: 
Root-URL: 
Title: Reinforcement Learning for Selfish Load Balancing in a Distributed Memory Environment  
Author: Stephen M. Majercik and Michael L. Littman 
Date: 20 December 1996  
Abstract: Load balancing is a difficult problem whose solution can greatly increase the speedup one achieves in a parallel distributed memory environment. The necessity for load balancing can arise not only from the structure or dynamics of one's problem, but from the need to compete for processor time with other users. Given a lengthy computation, the ability to exploit changes in processor loads when allocating work or deciding whether to reallocate work is critical in making the computation time-feasible. We show how this aspect of the load balancing problem can be formulated as a Markov decision process (MDP), and describe some preliminary attempts to solve this MDP using guided off-line Q-learning and a linear value-function approximator. In particular, we describe difficulties with value-function approximator divergence and techniques we applied to correct this problem. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. A. Board, Z. S. Hakura, W. D. Elliott, D. C. Gray, W. J. Blanke, and Jr. J. F. Leathrum. </author> <title> Scalable implementations of multipole-accelerated algorithms for molecular dynamics. </title> <type> Technical Report 94-002, </type> <institution> Duke University, Department of Electrical Engineering, </institution> <year> 1994. </year>
Reference-contexts: such as large molecular dynamics simulations, the benefits of load balancing can greatly outweigh the costs. 2 Molecular Dynamics We will focus on selfish load balancing for the Molecular Dynamics Multipole Algorithm (MDMA), an efficient multipole-based algorithm for computing accurate nonbonded forces among particles in a molecular dynamics (MD) simulation <ref> [1] </ref>. This focus is appropriate since the calculation of nonbonded forces consumes approximately 90% of the execution time in an MD simulation [2]. 1 The MDMA constructs a tree based on a hierarchical decomposition of the simulation space. <p> Thus, we always have a balanced tree with the appropriate level of granularity, so we never have to rebuild the tree. (If a buffer zone is constructed around the simulation volume, the tree usually does not need to be rebuilt for hundreds or thousands of time steps <ref> [1] </ref>, making this assumption somewhat realistic.) * We assume that the distributed system is small enough to allow a centralized load balancer, i.e. the overhead associated with collecting the processor information necessary to load balance is significantly smaller than the potential gain from load balancing. <p> Given this simplified molecular dynamics problem, most interprocessor communication is eliminated if cells from the third level of the oct-tree (along with all their descendants) are assigned to the available processors <ref> [1] </ref>. <p> Thus, a decision to rebalance in the middle of an MD step incurs the additional cost of the wasted computation up to that point. Since an MD step for a 100,000 particle simulation can take on the order of an hour <ref> [1] </ref> on a dedicated processor, however, the decision to rebalance in the middle of an MD step might make sense in some situations. 4 Load Balancing as an MDP Given the description of the simulation model in Section 3, the MDP formulation is relatively straightforward.
Reference: [2] <author> Y. Hwang, R. Das, J. H. Saltz, M. Hodoscek, and B. R. Brooks. </author> <title> Parallelizing molecular dynamics programs for distributed-memory machines. </title> <journal> IEEE Computational Science and Engineering, </journal> <volume> Summer:18-29, </volume> <year> 1995. </year>
Reference-contexts: This focus is appropriate since the calculation of nonbonded forces consumes approximately 90% of the execution time in an MD simulation <ref> [2] </ref>. 1 The MDMA constructs a tree based on a hierarchical decomposition of the simulation space. Starting at the root, levels of the tree represent increasingly fine decompositions of the simulation space.
Reference: [3] <author> S. P. Singh, A. G. Barto, R. Grupen, and C. Con nolly. </author> <title> Robust reinforcement learning in motion planning. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 655-662. </pages> <publisher> Morgan Kauf-mann, </publisher> <year> 1992. </year>
Reference-contexts: This idea of framing actions as policy applications and forming a more complex policy from the simpler action policies has been used before <ref> [3] </ref>, although we do not allow policies to be mixed to form hybrid actions. Costs are both explicit and implicit.
Reference: [4] <author> J. N. Tsitsiklis and B. Van Roy. </author> <title> An analysis of temporal-difference learning with function approximation. </title> <type> Technical Report LIDS-P-2322, </type> <institution> Mas-sachusetts Institute of Technology, </institution> <month> March </month> <year> 1996. </year> <month> 4 </month>
Reference-contexts: c i + ffx i dv; (3) where ff is the learning rate, x i is the i th feature of the current state vector, and dv is the difference between the target value and the value of the current state (standard delta rule). 3 Previous work with function approximation <ref> [4] </ref> suggests that using a learning rate which decays at an appropriate rate is important for ensuring convergence of the function approximator. We used a decaying learning rate over 1000 epochs of training, where the learning rate ff m for epoch m was ff m = 0:001=(m+1).
References-found: 4

