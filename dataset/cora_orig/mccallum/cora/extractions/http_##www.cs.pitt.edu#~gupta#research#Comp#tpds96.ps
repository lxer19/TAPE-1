URL: http://www.cs.pitt.edu/~gupta/research/Comp/tpds96.ps
Refering-URL: http://www.cs.pitt.edu/~gupta/research/dm.html
Root-URL: 
Email: gong@cs.pitt.edu melhem@cs.pitt.edu gupta@cs.pitt.edu  
Title: Loop Transformations for Fault Detection in Regular Loops on Massively Parallel Systems  
Author: Chun Gong, Rami Melhem and Rajiv Gupta 
Note: Pittsburgh.  
Date: April 17, 1996  
Address: Pittsburgh Pittsburgh, PA 15260  
Affiliation: Department of Computer Science The University of  
Abstract: Distributed-memory systems can incorporate thousands of processors at a reasonable cost. However, with an increasing number of processors in a system, fault detection and fault tolerance become critical issues. By replicating the computation on more than one processor and comparing the results produced by these processors, errors can be detected. During the execution of a program, due to data dependencies, typically not all of the processors in a multiprocessor system are busy at all times. Therefore processor schedules contain idle time slots and it is the goal of this work to exploit these idle time slots to schedule duplicated computation for the purpose of fault detection. We propose a compiler-assisted approach to fault detection in regular loops on distributed-memory systems. This approach achieves fault detection by duplicating the execution of statement instances. After carefully analyzing the data dependencies of a regular loop, selected instances of loop statements are duplicated in a way that ensures the desired fault coverage. We first present duplication strategies for fault detection and show that these strategies use idle processor times for executing replicated statements, whenever possible. Next, we present loop transformations to implement these fault-detection strategies. Also, a general framework for selecting appropriate loop transformations is developed. Experimental results performed on the CRAY-T3D show that the overhead of adding the fault detection capability is usually less than 25%, and is less than 10% when communication overhead is reduced by grouping messages. y Supported, in part, by an MPC grant ASC-8092826 and National Science Foundation through a Presidential Young Investigator Award CCR-9157371 to the Univ. of
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Alewine, S. Chen, C. Li, W. Fuchs, and W. Hwu. </author> <title> Branch recovery with compiler-assisted multiple instruction retry. </title> <booktitle> In Proc. the 22nd Annual International Symposium on Fault-Tolerant Computing, </booktitle> <year> 1992, </year> <pages> pp. 66-73. </pages>
Reference-contexts: Techniques for compiler-assisted fault detection and recovery have been developed in previous research. A compiler-assisted scheme to enable a process to quickly recover from transient faults is developed in <ref> [1] </ref> and a method that utilizes the VLIW compiler to insert redundant operations into idle functional units for fault detection purposes is presented in [7]. Compiler techniques are used in [24] to insert checkpoints into a program so that both the desired checkpoint intervals and reproducible locations are maintained.
Reference: [2] <author> J.M. Anderson and M.S. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proc. the SIGPLAN Conf. on Programming Language Design and Implementation, </booktitle> <year> 1993, </year> <pages> pp. 112-125. </pages>
Reference-contexts: The compiler automatically selects : Heuristics are employed for the selection of in conjunction with data distribution. The goal of the heuristic is to balance parallelism and communication in order to achieve good performance <ref> [2] </ref>. In this paper, we assume that is given by the user. Furthermore, the function specifies scheduling of loop 2 iterations rather than individual statements. However, the techniques presented here can also be applied to the other methods described above [19].
Reference: [3] <author> T. Anderson, P. Barrett, D. Halliwell, and M. </author> <title> Moulding. Software fault tolerance: an evaluation. </title> <journal> IEEE trans. on Software Engneering, </journal> <volume> SE-11, </volume> <pages> pp. 1502-1510, </pages> <month> Dec. </month> <year> 1985. </year>
Reference-contexts: Thus, techniques for fault detection and fault tolerance on distributed-memory systems is an important area of research. Fault detection and tolerance can be achieved by introducing redundancy in a system at either the hardware level [6] [8] [27] or the software level <ref> [3] </ref> [4] [14] [28]. Recently, it has been realized that the computing power of a multiprocessor system is rarely completely utilized. This has resulted in the development of techniques that achieve fault detection by replicating computations and comparing the results of these computations [16] [29].
Reference: [4] <author> A. Avizienis and J. Kelly. </author> <title> Fault tolerance by design diversity: concepts and experiments. </title> <journal> Computer, </journal> <volume> 17, </volume> <pages> pp. 67-80, </pages> <month> Aug. </month> <year> 1984. </year>
Reference-contexts: Thus, techniques for fault detection and fault tolerance on distributed-memory systems is an important area of research. Fault detection and tolerance can be achieved by introducing redundancy in a system at either the hardware level [6] [8] [27] or the software level [3] <ref> [4] </ref> [14] [28]. Recently, it has been realized that the computing power of a multiprocessor system is rarely completely utilized. This has resulted in the development of techniques that achieve fault detection by replicating computations and comparing the results of these computations [16] [29].
Reference: [5] <author> V. Balasubramanian and P. Banerjee. </author> <title> Compiler-assisted synthesis of algorithm-based checking in multiprocessors. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 39, </volume> <pages> pp. 436-446, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Compiler techniques are used in [24] to insert checkpoints into a program so that both the desired checkpoint intervals and reproducible locations are maintained. A source-to-source restructuring compiler for the synthesis of low-cost checks for scientific programs using the notion of algorithm-based checking is described in <ref> [5] </ref>. The compiler-assisted approaches are appealing since the compilers can apply a variety of analysis techniques to efficiently allocate resources in multiprocessor systems. Furthermore, adding fault detection capabilities to massively parallel systems is usually tedious and error prone. Thus, systematic techniques that can be implemented through compilers are most appropriate. <p> In this paper, we describe a compiler-assisted approach for achieving fault detection in regular loops on distributed-memory systems by duplicating computations at the statement level. This approach allows the compiler to utilize idle resources for the purpose of fault detection. It differs from the approach of <ref> [5] </ref> in that it presents a systematic approach to utilizing the spare processors' capacity for introducing redundancy in distributed memory environments. It also provides a means for analyzing the resulting overhead. <p> This reduced communication-overhead is at the expense of memory overhead since it uses a two dimensional array T emp (i; j). Finally we can borrow the idea from <ref> [5] </ref> to delay the checking to the end of the loop and instead of exchanging all the results, the processors exchange a single value which is the sum of all the results that are to be compared, see Figure 8b. Thus, the overhead of communication is further reduced. <p> A fault is detected only at the end of the whole computation irrespective of the time at which the fault occurs. For computation that needs long time to complete, this is undesirable. The approach of <ref> [5] </ref> is also a statement level duplication approach. The authors extend the algorithm-based checking techniques to deal with more general applications by exploiting linearity property of Fortran Do loops. <p> For loops that satisfy the linearity property, the two sides of equation (12) can be computed on two different processors and compared to detect a fault. We compare the approach of <ref> [5] </ref> and the approach of this paper as follows. 1. Class of loops: While the approach proposed in this paper deals with the class of regular loops, the approach of [5] deals with the class of loops with linearity property. <p> We compare the approach of <ref> [5] </ref> and the approach of this paper as follows. 1. Class of loops: While the approach proposed in this paper deals with the class of regular loops, the approach of [5] deals with the class of loops with linearity property. These two classes are not the same and neither of these two classes is a superset of the other. 2. Overhead: For loops that are regular and satisfy the linearity property, we can compare the two approaches as follows. <p> Overhead: For loops that are regular and satisfy the linearity property, we can compare the two approaches as follows. The approach proposed in this paper achieves low cost by exploiting the idle time slots during the execution of a regular loop. The approach of <ref> [5] </ref> does not explicitly exploit idle processors that might exist during the execution of a program. Instead, it tries to reduce the overhead through carefully choosing the checking variable to reduce the duplicated computations and data exchanges. <p> This technique achieves low cost fault-detection only for special applications such as matrix multiplication. For example, the image processing loop given in the last section satisfies the 21 linearity property. Even though there are many idle time slots during the execution of the loop, the approach of <ref> [5] </ref> will not be able to exploit them since it has to compute the two sides of equation (12) at the end of the loop. Therefore, for these kind of loops, the approach of this paper will achieve lower overhead than the approach of [5]. <p> of the loop, the approach of <ref> [5] </ref> will not be able to exploit them since it has to compute the two sides of equation (12) at the end of the loop. Therefore, for these kind of loops, the approach of this paper will achieve lower overhead than the approach of [5]. The advantage of the approach of [5] is in dealing with fully parallel loops, since it can detect transient faults with less than 100% overhead. It also pioneered the using of delayed checking technique. 3. Roundoff error: Another problem of the approach of [5] is that the roundoff errors accumulate <p> Therefore, for these kind of loops, the approach of this paper will achieve lower overhead than the approach of <ref> [5] </ref>. The advantage of the approach of [5] is in dealing with fully parallel loops, since it can detect transient faults with less than 100% overhead. It also pioneered the using of delayed checking technique. 3. Roundoff error: Another problem of the approach of [5] is that the roundoff errors accumulate in different ways for the two sides <p> lower overhead than the approach of <ref> [5] </ref>. The advantage of the approach of [5] is in dealing with fully parallel loops, since it can detect transient faults with less than 100% overhead. It also pioneered the using of delayed checking technique. 3. Roundoff error: Another problem of the approach of [5] is that the roundoff errors accumulate in different ways for the two sides of equation (12), thus making it highly unlikely that the equality is preserved exactly [12]. This problem does not exist in the approach proposed in this paper since exactly the same computations are duplicated. <p> From the above discussion, we can see that the two approaches are complementary to each other. It would be a good idea to combine the approach of this paper and the approach of <ref> [5] </ref> to develop a more powerful system that can (1) deal with both regular loops and loops with linearity property; (2) deal with both loops that produce many idle time slots at execution time and loops that produce few or no idle time slots. 9 Concluding Remarks We proposed a compiler-assisted
Reference: [6] <author> D. Blough and G. </author> <title> Masson. Performance analysis of a generalized concurrent error detection procedure. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 39, </volume> <pages> pp. 47-62, </pages> <month> Jan. </month> <year> 1990. </year>
Reference-contexts: Thus, techniques for fault detection and fault tolerance on distributed-memory systems is an important area of research. Fault detection and tolerance can be achieved by introducing redundancy in a system at either the hardware level <ref> [6] </ref> [8] [27] or the software level [3] [4] [14] [28]. Recently, it has been realized that the computing power of a multiprocessor system is rarely completely utilized.
Reference: [7] <author> D. Blough and A. Nicolau. </author> <title> Fault tolerance in super-scalar and VLIW processors. </title> <booktitle> In Proc. IEEE Workshop on Fault-Tolerant Parallel and Distributed Systems, IEEE, </booktitle> <year> 1992, </year> <pages> pp. 193-200. </pages>
Reference-contexts: A compiler-assisted scheme to enable a process to quickly recover from transient faults is developed in [1] and a method that utilizes the VLIW compiler to insert redundant operations into idle functional units for fault detection purposes is presented in <ref> [7] </ref>. Compiler techniques are used in [24] to insert checkpoints into a program so that both the desired checkpoint intervals and reproducible locations are maintained. A source-to-source restructuring compiler for the synthesis of low-cost checks for scientific programs using the notion of algorithm-based checking is described in [5].
Reference: [8] <author> M. Breuer and A. Ismaeel. </author> <title> Roving emulation as a fault detection mechanism. </title> <journal> IEEE Trans. on Computers, </journal> <volume> c-35, </volume> <pages> pp. 933-939, </pages> <month> Nov. </month> <year> 1986. </year>
Reference-contexts: Thus, techniques for fault detection and fault tolerance on distributed-memory systems is an important area of research. Fault detection and tolerance can be achieved by introducing redundancy in a system at either the hardware level [6] <ref> [8] </ref> [27] or the software level [3] [4] [14] [28]. Recently, it has been realized that the computing power of a multiprocessor system is rarely completely utilized.
Reference: [9] <author> D. Callahan and K. Kennedy. </author> <title> Compiling programs for distributed-memory multiprocessors. </title> <journal> The Journal of Supercomputing, </journal> <pages> pp. 151-169, </pages> <month> Feb. </month> <year> 1988. </year>
Reference-contexts: Therefore, many languages have been developed which provide the users with the conceptually easier programming model based upon a globally shared name space. The compiler is responsible for inserting necessary communication primitives in this model <ref> [9] </ref>. Examples of such languages include CM Fortran [13], C* [25], Vienna Fortran [10], and HPF [21]. The compiler-assisted methodology introduced in this paper achieves fault detection on distributed-memory systems when these systems are programmed using the shared-name-space model.
Reference: [10] <author> B. M. Chapman, P. Mehrotra, and H. P. Zima. </author> <title> Programming in Vienna Fortran. </title> <booktitle> Scientific Programming, </booktitle> <pages> pp. 31-50, </pages> <month> Jan. </month> <year> 1992. </year> <month> 23 </month>
Reference-contexts: Therefore, many languages have been developed which provide the users with the conceptually easier programming model based upon a globally shared name space. The compiler is responsible for inserting necessary communication primitives in this model [9]. Examples of such languages include CM Fortran [13], C* [25], Vienna Fortran <ref> [10] </ref>, and HPF [21]. The compiler-assisted methodology introduced in this paper achieves fault detection on distributed-memory systems when these systems are programmed using the shared-name-space model.
Reference: [11] <author> B. M. Chapman, P. Mehrotra, and H. P. Zima. </author> <title> High performance Fortran without templates: an alternative model for distribution and alignment. </title> <booktitle> In Proc. of the Fourth ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming, </booktitle> <year> 1993, </year> <pages> pp. 92-101. </pages>
Reference: [12] <author> A. R. Chowdhury and P. Banerjee. </author> <title> Tolerance determination for algorithm-based check using simplified error analysis techniques. </title> <booktitle> In Proc. of 1993 FTCS, </booktitle> <month> June 22-24, </month> <year> 1993, </year> <pages> pp. 290-298. </pages>
Reference-contexts: It also pioneered the using of delayed checking technique. 3. Roundoff error: Another problem of the approach of [5] is that the roundoff errors accumulate in different ways for the two sides of equation (12), thus making it highly unlikely that the equality is preserved exactly <ref> [12] </ref>. This problem does not exist in the approach proposed in this paper since exactly the same computations are duplicated. From the above discussion, we can see that the two approaches are complementary to each other.
Reference: [13] <institution> CM Fortran user's guide for the CM-5. Thinking Machines, </institution> <year> 1992. </year>
Reference-contexts: Therefore, many languages have been developed which provide the users with the conceptually easier programming model based upon a globally shared name space. The compiler is responsible for inserting necessary communication primitives in this model [9]. Examples of such languages include CM Fortran <ref> [13] </ref>, C* [25], Vienna Fortran [10], and HPF [21]. The compiler-assisted methodology introduced in this paper achieves fault detection on distributed-memory systems when these systems are programmed using the shared-name-space model.
Reference: [14] <author> E. Cooper. </author> <title> Replicated distributed programs. </title> <booktitle> In Proc. 10th ACM Symposium on Operating System Principles, </booktitle> <month> Dec. </month> <pages> 1-4, </pages> <year> 1985, </year> <pages> pp. 63-78. </pages>
Reference-contexts: Thus, techniques for fault detection and fault tolerance on distributed-memory systems is an important area of research. Fault detection and tolerance can be achieved by introducing redundancy in a system at either the hardware level [6] [8] [27] or the software level [3] [4] <ref> [14] </ref> [28]. Recently, it has been realized that the computing power of a multiprocessor system is rarely completely utilized. This has resulted in the development of techniques that achieve fault detection by replicating computations and comparing the results of these computations [16] [29]. <p> For example, it could be done at process level [29], at transaction level [28], at procedure level <ref> [14] </ref>, or at statement level as we proposed in this paper. While replication of computation at a coarse-grained level incurs smaller overhead, it has a serious drawback. A fault is detected only at the end of the whole computation irrespective of the time at which the fault occurs.
Reference: [15] <author> R. Cytron and J. Ferrante. </author> <title> What's in a name? -or- The value of renaming for parallelism detection and storage allocation. </title> <booktitle> In Proc. International Conf. on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1987, </year> <pages> pp. 19-27. </pages>
Reference-contexts: A statement instance s i is antidependent upon s j , if s i writes into a memory location after s j reads from the same memory location. Since output and anti-dependencies can be eliminated through renaming <ref> [15] </ref>, we assume that the loop contains only flow dependencies.
Reference: [16] <author> A. Dahbura, K. Sabnani, and W. Hery. </author> <title> Spare capacity as a means of fault detection and diagnosis in multiprocessor systems. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 38, </volume> <pages> pp. 881-891, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Recently, it has been realized that the computing power of a multiprocessor system is rarely completely utilized. This has resulted in the development of techniques that achieve fault detection by replicating computations and comparing the results of these computations <ref> [16] </ref> [29]. Ideally by utilizing the spare capacity of the system to execute replicated computations, fault detection and tolerance can be achieved at a reduced cost. Techniques for compiler-assisted fault detection and recovery have been developed in previous research.
Reference: [17] <author> J. Feo. </author> <title> An analysis of the computational and parallel complexity of the livermore loops. </title> <booktitle> Parallel Computing, </booktitle> <pages> pp. 163-185, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: Therefore we applied the transformation t IP of Figure 9. The experiment results, which are given in Figure 12, are consistent with the previous results. We applied loop transformations for fault detection to all the regular loops from the Livermore Loop benchmark <ref> [17] </ref>. The processor number is fixed as N = 64, and inner loop iteration is fixed as K = 65536.
Reference: [18] <author> A. Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. Sunderam. </author> <title> PVM 3 user's guide and reference manual. </title> <institution> Prepared by the Oak Ridge National Laborary, Oak Ridge, TN 37831. </institution>
Reference-contexts: So we can exploit the spare capacity for the purposes of fault detection. 7 Performance Measurements In order to empirically estimate the overhead of the compiler-assisted fault-detection approach, we have developed a testing environment (TE) on the multiprocessor computer CRAY-T3D using PVM <ref> [18] </ref>. At this time, the TE does not have a full-fledged compiler for the parallel programming model. Instead, programs are translated into parallel intermediate code which is executed by interperters running on each processors of the CRAY-T3D. Interprocessor communication is carried out by the PVM primitives.
Reference: [19] <author> C. Gong, R. Melhem, and R. Gupta. </author> <title> Compiler assisted fault detection for distributed-memory systems. </title> <booktitle> In Proc. of the 1994 Scalable High Performance Computing Conference, </booktitle> <address> Knoxville, TN, U.S.A., </address> <month> May 23-25, </month> <year> 1994, </year> <pages> pp. 373-380. </pages>
Reference-contexts: In this paper, we assume that is given by the user. Furthermore, the function specifies scheduling of loop 2 iterations rather than individual statements. However, the techniques presented here can also be applied to the other methods described above <ref> [19] </ref>. We focus on loops since most of the idle time can be found in executing loops. We first define the notation used to describe data dependencies. There are three kinds of data dependencies. <p> Our goal is to reduce, as much as possible, the overhead incurred by the fault-detection mechanism. Before we discuss duplication strategies we discuss the fault model used in this paper. We consider only single transient faults. The case of permanent faults is simpler to handle and is discussed in <ref> [19] </ref>. We assume that a processor is faulty if and only if it produces wrong results for all input data. We also assume that interprocessor communication is fault free. If errors resulting from transient faults are to be detected, then every statement instance should be duplicated.
Reference: [20] <author> C. Gong. </author> <title> Compiler-assisted approaches to fault detection on distributed-memory systems. </title> <type> Ph.D Thesis. </type>
Reference-contexts: Communication delay may also create idle slots during execution. The efficient utilization of those slots for scheduling duplicated computations is studied in detail in <ref> [20] </ref>. Furthermore, the work of this paper can be expanded in as follows. First, the extension of the proposed approach to fault location and error masking, specifically, by triplicating computation instances on three processors.
Reference: [21] <author> High Performance Fortran Forum. </author> <title> DRAFT High Performance Fortran language specification. Version 1.0. </title> <type> Technical Report, </type> <month> Jan. </month> <year> 1993, </year> <institution> Rice University. </institution>
Reference-contexts: The compiler is responsible for inserting necessary communication primitives in this model [9]. Examples of such languages include CM Fortran [13], C* [25], Vienna Fortran [10], and HPF <ref> [21] </ref>. The compiler-assisted methodology introduced in this paper achieves fault detection on distributed-memory systems when these systems are programmed using the shared-name-space model.
Reference: [22] <author> S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, and C. Tseng. </author> <title> An overview of the Fortran D programming system. </title> <booktitle> In Proc. of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <year> 1991, </year> <pages> pp. </pages> <month> e1-e17. </month>
Reference: [23] <author> C. Koelbel, P. Mehrotra, and J. Rosendale. </author> <title> Supporting shared data structure on distributed memory architectures. </title> <booktitle> In Proc. of the Second ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming, SIGPLAN, ACM, </booktitle> <year> 1990, </year> <pages> pp. 177-186. </pages>
Reference-contexts: The following methods are commonly used to specify : 1. The user specifies : Languages such as Kali and the Cray T3D Fortran allow the specification of for scheduling of loop iterations on specific processors <ref> [23] </ref>. 2. The compiler chooses according to the "owner computes rule": In compilers for languages such as Fortran D and HPF, the user specifies data distribution and is chosen such that each statement is executed by the processor that owns the variable whose value is being computed. 3.
Reference: [24] <author> J. Long, W. Fuchs, and J. Abraham. </author> <title> Compiler-assisted static checkpoint insertion. </title> <booktitle> In Proc. the 22nd Annual International Symposium on Fault-Tolerant Computing, </booktitle> <year> 1992, </year> <pages> pp. 58-65. </pages>
Reference-contexts: A compiler-assisted scheme to enable a process to quickly recover from transient faults is developed in [1] and a method that utilizes the VLIW compiler to insert redundant operations into idle functional units for fault detection purposes is presented in [7]. Compiler techniques are used in <ref> [24] </ref> to insert checkpoints into a program so that both the desired checkpoint intervals and reproducible locations are maintained. A source-to-source restructuring compiler for the synthesis of low-cost checks for scientific programs using the notion of algorithm-based checking is described in [5].
Reference: [25] <author> M. Quinn, P. Hatcher, and K. Jourdenais. </author> <title> Compiling C* programs for a hypercube multicomputer. </title> <booktitle> In Proc. ACM/SIGPLAN PPEALS, </booktitle> <month> July, </month> <year> 1988, </year> <pages> pp. 57-65. </pages>
Reference-contexts: Therefore, many languages have been developed which provide the users with the conceptually easier programming model based upon a globally shared name space. The compiler is responsible for inserting necessary communication primitives in this model [9]. Examples of such languages include CM Fortran [13], C* <ref> [25] </ref>, Vienna Fortran [10], and HPF [21]. The compiler-assisted methodology introduced in this paper achieves fault detection on distributed-memory systems when these systems are programmed using the shared-name-space model.
Reference: [26] <author> M. Quinn and P. Hatcher. </author> <title> Compiling SIMD programs for MIMD architecthres. </title> <booktitle> In Proc. of International Conference on Computer Languages, </booktitle> <year> 1990, </year> <pages> pp. 291-296. </pages>
Reference: [27] <author> L. Shombert and D. Siewiorek. </author> <title> Using redundancy for concurrent testing and repairing of systolic arrays. </title> <booktitle> In Proc. the Seventeenth International Symposium on Fault-Tolerant Computing. IEEE, </booktitle> <year> 1987, </year> <pages> pp. 244-249. </pages>
Reference-contexts: Thus, techniques for fault detection and fault tolerance on distributed-memory systems is an important area of research. Fault detection and tolerance can be achieved by introducing redundancy in a system at either the hardware level [6] [8] <ref> [27] </ref> or the software level [3] [4] [14] [28]. Recently, it has been realized that the computing power of a multiprocessor system is rarely completely utilized. This has resulted in the development of techniques that achieve fault detection by replicating computations and comparing the results of these computations [16] [29].
Reference: [28] <author> T. P. Ng. </author> <title> Replicated transactions. </title> <booktitle> In Proc. 9th International Conference on Distributed Computing Systems, </booktitle> <year> 1988, </year> <pages> pp. 474-481. </pages>
Reference-contexts: Thus, techniques for fault detection and fault tolerance on distributed-memory systems is an important area of research. Fault detection and tolerance can be achieved by introducing redundancy in a system at either the hardware level [6] [8] [27] or the software level [3] [4] [14] <ref> [28] </ref>. Recently, it has been realized that the computing power of a multiprocessor system is rarely completely utilized. This has resulted in the development of techniques that achieve fault detection by replicating computations and comparing the results of these computations [16] [29]. <p> For example, it could be done at process level [29], at transaction level <ref> [28] </ref>, at procedure level [14], or at statement level as we proposed in this paper. While replication of computation at a coarse-grained level incurs smaller overhead, it has a serious drawback.
Reference: [29] <author> S. Tridandapani, A. Somani, and U. Sandadi. </author> <title> Low overhead multiprocessor allocation strategies exploiting system spare capacity for fault detection and location. </title> <journal> IEEE Trans. on Computers, </journal> <pages> pp. 865-877, </pages> <month> July, </month> <year> 1995. </year>
Reference-contexts: Recently, it has been realized that the computing power of a multiprocessor system is rarely completely utilized. This has resulted in the development of techniques that achieve fault detection by replicating computations and comparing the results of these computations [16] <ref> [29] </ref>. Ideally by utilizing the spare capacity of the system to execute replicated computations, fault detection and tolerance can be achieved at a reduced cost. Techniques for compiler-assisted fault detection and recovery have been developed in previous research. <p> For example, it could be done at process level <ref> [29] </ref>, at transaction level [28], at procedure level [14], or at statement level as we proposed in this paper. While replication of computation at a coarse-grained level incurs smaller overhead, it has a serious drawback.
Reference: [30] <author> M. Wolfe. </author> <title> Optimizing supercompiler for supercomputers. </title> <publisher> The MIT Press, </publisher> <year> 1989. </year> <month> 24 </month>
Reference-contexts: There may be dependencies among different iterations of a loop. If a statement instance at the ith iteration of a loop is dependent upon a statement instance at the kth iteration, then this particular dependence is said to have a distance of (i k) <ref> [30] </ref>. For multiply-nested loops, a vector of dependence distances is used to represent the dependencies. If all the dependencies of a loop can be described by a set of dependence distances that are independent of the loop variables, then the loop is said to be a regular loop.
References-found: 30

