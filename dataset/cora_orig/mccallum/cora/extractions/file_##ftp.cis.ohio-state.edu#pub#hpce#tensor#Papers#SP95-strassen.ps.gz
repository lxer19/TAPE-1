URL: file://ftp.cis.ohio-state.edu/pub/hpce/tensor/Papers/SP95-strassen.ps.gz
Refering-URL: http://www.cis.ohio-state.edu/~chh/Publication/tensor-papers.html
Root-URL: 
Title: A Tensor Product Formulation of Strassen's Matrix Multiplication Algorithm  
Author: B. Kumar, C.-H. Huang, and P. Sadayappan R.W. Johnson 
Keyword: Strassen's matrix multiplication algorithm, Tensor product, Block recursive algorithm, Par allel architecture, Vector processors.  
Note: This work was supported in part by ARPA and monitered by NIST.  
Affiliation: Department of Computer and Information Science The Ohio State University  Department of Computer Science St. Cloud State University  
Abstract: In this paper, we present a program generation strategy of Strassen's matrix multiplication algorithm using a programming methodology based on tensor product formulas. In this methodology, block recursive programs such as the fast Fourier Transforms and Strassen's matrix multiplication algorithm are expressed as algebraic formulas involving tensor products and other matrix operations. Such formulas can be systematically translated to high-performance parallel/vector codes for various architectures. In this paper, we present a non-recursive implementation of Strassen's algorithm for shared memory vector processors such as the Cray Y-MP. A previous implementation of Strassen's algorithm synthesized from tensor product formulas required working storage of size O(7 n ) for multiplying 2 n fi 2 n matrices. We present a modified formulation in which the working storage requirement is reduced to O(4 n ). The modified formulation exhibits sufficient parallelism for efficient implementation on a shared-memory multiprocessor. Performance results on a Cray Y-MP8/64 are presented. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D.H. Bailey. </author> <title> Extra High Speed Matrix Multiplication on the Cray-2. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 9(3) </volume> <pages> 603-607, </pages> <year> 1988. </year>
Reference-contexts: A recursive application of this algorithm for multiplying 2 n fi 2 n matrices requires only O (7 n ) operations, compared to O (8 n ) for conventional matrix multiplication. Efficient parallel implementations of this algorithm have been described in <ref> [1, 10] </ref>. This algorithm has been used for fast matrix multiplication in implementing level 3 BLAS [9] and linear algebra routines [2]. In this paper, we describe the tensor product formulation of Strassen's matrix multiplication algorithm, and discuss program generation for shared memory vector processors such as the Cray Y-MP.
Reference: [2] <author> D.H. Bailey, K. Lee, and H.D. Simon. </author> <title> Using Strassen's Algorithm to Accelerate the Solution of Linear Systems. </title> <journal> Journal of Supercomputing, </journal> <volume> 4(4) </volume> <pages> 357-371, </pages> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: Efficient parallel implementations of this algorithm have been described in [1, 10]. This algorithm has been used for fast matrix multiplication in implementing level 3 BLAS [9] and linear algebra routines <ref> [2] </ref>. In this paper, we describe the tensor product formulation of Strassen's matrix multiplication algorithm, and discuss program generation for shared memory vector processors such as the Cray Y-MP.
Reference: [3] <author> A. Borodin and I. Munro. </author> <title> The Computational Complexity of Algebraic and Numeric Problems. </title> <publisher> American Elsevier Publishing Co., </publisher> <year> 1975. </year>
Reference-contexts: We present a strategy for automatic code synthesis from tensor product formulas containing a selection operator. The modified formulation exhibits sufficient parallelism for efficient implementation on a vector-parallel machine such as the Cray Y-MP. In addition, we express Winograd's variation <ref> [3] </ref> using our notation and describe its translation to a programming code. Winograd's variation uses the same number of multiplications, but a smaller number of additions than the original Strassen's algorithm. This paper is organized as follows. Section 2 contains an overview of the tensor product notation. <p> Winograd presented a more efficient algorithm which uses 15 scalar additions and 7 scalar multiplications <ref> [3] </ref>.
Reference: [4] <author> R.P. Brent. </author> <title> Algorithms for matrix multiplication. </title> <type> Technical Report CS 157, </type> <institution> Computer Science Dept., Stanford University, Palo Alto, Calif., </institution> <year> 1970. </year>
Reference-contexts: A common technique used is to pad the matrices with rows and columns of zeros to increase the matrix sizes to the next higher powers of two, compute the extended matrix product, and then extract the desired result [17]. Another approach <ref> [4] </ref> is drop the last rows and columns from the computation to achieve even dimensions, and compute the partial matrix product.
Reference: [5] <author> J.W. Brewer. </author> <title> Kronecker Products and Matrix Calculus in System Theory. </title> <journal> IEEE Transactions on Circuits and Systems, </journal> <volume> 25 </volume> <pages> 772-781, </pages> <year> 1978. </year>
Reference-contexts: 1 Introduction Tensor products (Kronecker products) have been used to model algorithms with a recursive computational structure which occur in application areas such as digital signal processing [6, 15], image processing [16], linear system design <ref> [5] </ref>, and statistics [7]. In recent years, a programming methodology based on tensor products has been successfully used to design and implement high-performance algorithms to compute fast Fourier Transforms (FFT) [12, 14] and matrix multiplication [10, 13] for shared-memory vector multiprocessors.
Reference: [6] <author> J. Granta, M. Conner, and R. Tolimieri. </author> <title> Recursive Fast Algorithms and the Role of Tensor Products. </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 40(12) </volume> <pages> 2921-2930, </pages> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Tensor products (Kronecker products) have been used to model algorithms with a recursive computational structure which occur in application areas such as digital signal processing <ref> [6, 15] </ref>, image processing [16], linear system design [5], and statistics [7]. In recent years, a programming methodology based on tensor products has been successfully used to design and implement high-performance algorithms to compute fast Fourier Transforms (FFT) [12, 14] and matrix multiplication [10, 13] for shared-memory vector multiprocessors.
Reference: [7] <author> F.A. Graybill. </author> <title> Matrices, with Applications in Statistics. </title> <booktitle> Wadsworth International Group, </booktitle> <address> Belmont, CA, </address> <year> 1983. </year>
Reference-contexts: 1 Introduction Tensor products (Kronecker products) have been used to model algorithms with a recursive computational structure which occur in application areas such as digital signal processing [6, 15], image processing [16], linear system design [5], and statistics <ref> [7] </ref>. In recent years, a programming methodology based on tensor products has been successfully used to design and implement high-performance algorithms to compute fast Fourier Transforms (FFT) [12, 14] and matrix multiplication [10, 13] for shared-memory vector multiprocessors.
Reference: [8] <author> H.V. Henderson and S.R. Searle. </author> <title> The Vec-Permutation Matrix, The Vec Operator and Kronecker Products: A Review. </title> <journal> Linear and Multilinear Algebra, </journal> <volume> 9 </volume> <pages> 271-288, </pages> <year> 1981. </year>
Reference-contexts: The notation A corresponds exactly to the vec (A) notation <ref> [8] </ref>, however, we shall use the former for readability purposes. The matrices S a , S b and S c are termed as basic operators, and do not have to be explicitly generated, but specify which operations have to be performed on specific components of the input vectors.
Reference: [9] <author> N.J. Higham. </author> <title> Exploiting fast matrix multiplication within the level 3 BLAS. </title> <journal> In ACM Transactions on Mathematical Software, </journal> <volume> volume 16, </volume> <pages> pages 352-368, </pages> <month> Dec. </month> <year> 1990. </year>
Reference-contexts: Efficient parallel implementations of this algorithm have been described in [1, 10]. This algorithm has been used for fast matrix multiplication in implementing level 3 BLAS <ref> [9] </ref> and linear algebra routines [2]. In this paper, we describe the tensor product formulation of Strassen's matrix multiplication algorithm, and discuss program generation for shared memory vector processors such as the Cray Y-MP.
Reference: [10] <author> C.-H. Huang, J.R. Johnson, and R.W. Johnson. </author> <title> A Tensor Product Formulation of Strassen's Matrix Multiplication Algorithm. In App. </title> <journal> Math. Lett., </journal> <volume> volume 3, </volume> <pages> pages 67-71, </pages> <year> 1990. </year>
Reference-contexts: In recent years, a programming methodology based on tensor products has been successfully used to design and implement high-performance algorithms to compute fast Fourier Transforms (FFT) [12, 14] and matrix multiplication <ref> [10, 13] </ref> for shared-memory vector multiprocessors. A set of multilinear algebra operations such as tensor product and matrix multiplication are used to express block recursive algorithms. These algebraic operations can be systematically translated into high-level programming language constructs such as sequential composition, iteration, and parallel / vector operations. <p> A recursive application of this algorithm for multiplying 2 n fi 2 n matrices requires only O (7 n ) operations, compared to O (8 n ) for conventional matrix multiplication. Efficient parallel implementations of this algorithm have been described in <ref> [1, 10] </ref>. This algorithm has been used for fast matrix multiplication in implementing level 3 BLAS [9] and linear algebra routines [2]. In this paper, we describe the tensor product formulation of Strassen's matrix multiplication algorithm, and discuss program generation for shared memory vector processors such as the Cray Y-MP. <p> We show how the tensor product formula of Strassen's algorithm can be manipulated to operate on full vectors with unit stride. An important feature of the generated code is that it employs no recursion. The initial formulation presented in <ref> [10] </ref> required a working array of size O (7 n ) for the multiplication of 2 n fi 2 n matrices. We present a modified formulation that significantly reduces the size of working array to O (4 n ). This reduction is made possible through the reuse of working storage. <p> Let A, B, and C be the l-level block recursive representations of 2 n fi 2 n matrices A, B, and C. The computation of C is described by the following formulation <ref> [10] </ref>: C = S n;l a (A) fl nl S b (B)] where S n;l N l Q l1 S b = i=0 S b I 4 nl = i=0 (I 7 i S b I 4 n (i+1) ) c = i=0 S c I 4 nl = i=n1 (I
Reference: [11] <author> C.-H. Huang, J.R. Johnson, and R.W. Johnson. </author> <title> Generating Parallel Programs from Tensor Product Formulas: A Case Study of Strassen's Matrix Multiplication Algorithm. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <volume> volume 3, </volume> <pages> pages 104-108, </pages> <year> 1992. </year> <month> 17 </month>
Reference-contexts: The tensor product formula to convert a 2 n fi 2 n matrix from a column major form to a k-level block recursive form is given by <ref> [11] </ref>: R n;k = i=0 2 I 2 n+ki1 ) (I 2 nk L 2 n R n;k is termed as a conversion operator. There are two ways in which storage conversion can be implemented.
Reference: [12] <author> J.R. Johnson, R.W. Johnson, D. Rodriguez, and R. Tolimieri. </author> <title> A Methodology for Designing, Modifying and Implementing Fourier Transform Algorithms on Various Architectures. </title> <journal> Circuits Systems Signal Process, </journal> <volume> 9(4) </volume> <pages> 450-500, </pages> <year> 1990. </year>
Reference-contexts: In recent years, a programming methodology based on tensor products has been successfully used to design and implement high-performance algorithms to compute fast Fourier Transforms (FFT) <ref> [12, 14] </ref> and matrix multiplication [10, 13] for shared-memory vector multiprocessors. A set of multilinear algebra operations such as tensor product and matrix multiplication are used to express block recursive algorithms.
Reference: [13] <author> B. Kumar, C. H. Huang, J. Johnson, R. W. Johnson, and P. Sadayappan. </author> <title> A tensor product formulation of Strassen's matrix multiplication algorithm with memory reduction. </title> <booktitle> In Seventh International Parallel Processing Symposium, </booktitle> <pages> pages 582-588, </pages> <month> Apr. </month> <year> 1993. </year>
Reference-contexts: In recent years, a programming methodology based on tensor products has been successfully used to design and implement high-performance algorithms to compute fast Fourier Transforms (FFT) [12, 14] and matrix multiplication <ref> [10, 13] </ref> for shared-memory vector multiprocessors. A set of multilinear algebra operations such as tensor product and matrix multiplication are used to express block recursive algorithms. These algebraic operations can be systematically translated into high-level programming language constructs such as sequential composition, iteration, and parallel / vector operations.
Reference: [14] <author> C. Van Loan. </author> <title> Computational Frameworks for the Fast Fourier Transform. </title> <publisher> SIAM Publications, </publisher> <year> 1992. </year>
Reference-contexts: In recent years, a programming methodology based on tensor products has been successfully used to design and implement high-performance algorithms to compute fast Fourier Transforms (FFT) <ref> [12, 14] </ref> and matrix multiplication [10, 13] for shared-memory vector multiprocessors. A set of multilinear algebra operations such as tensor product and matrix multiplication are used to express block recursive algorithms.
Reference: [15] <author> P.A. Regalia and S.K. Mitra. </author> <title> Kronecker Products, Unitary Matrices and Signal Processing Applications. </title> <journal> SIAM Review, </journal> <volume> 31(4) </volume> <pages> 586-613, </pages> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Tensor products (Kronecker products) have been used to model algorithms with a recursive computational structure which occur in application areas such as digital signal processing <ref> [6, 15] </ref>, image processing [16], linear system design [5], and statistics [7]. In recent years, a programming methodology based on tensor products has been successfully used to design and implement high-performance algorithms to compute fast Fourier Transforms (FFT) [12, 14] and matrix multiplication [10, 13] for shared-memory vector multiprocessors.
Reference: [16] <author> G.X. Ritter and P.D. Gader. </author> <title> Image Algebra Techniques and Parallel Image Processing. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 4 </volume> <pages> 7-44, </pages> <year> 1987. </year>
Reference-contexts: 1 Introduction Tensor products (Kronecker products) have been used to model algorithms with a recursive computational structure which occur in application areas such as digital signal processing [6, 15], image processing <ref> [16] </ref>, linear system design [5], and statistics [7]. In recent years, a programming methodology based on tensor products has been successfully used to design and implement high-performance algorithms to compute fast Fourier Transforms (FFT) [12, 14] and matrix multiplication [10, 13] for shared-memory vector multiprocessors.

References-found: 16

