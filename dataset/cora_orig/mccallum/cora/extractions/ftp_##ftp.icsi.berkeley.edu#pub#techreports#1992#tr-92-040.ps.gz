URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1992/tr-92-040.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1992.html
Root-URL: http://www.icsi.berkeley.edu
Title: Robot Shaping: Developing Situated Agents through Learning  
Author: Marco Dorigo Marco Colombetti 
Address: Parallelo", subproject  Milano, Piazza Leonardo da Vinci, 32, 20133 Milano, Italy  Berkeley, CA 94704,  Milano, Piazza Leonardo da Vinci, 32, 20133 Milano, Italy  
Note: This work has been submitted to the Artificial Intelligence Journal and has been partly supported by the Italian National Research Council, under the "Progetto Finalizzato  2 "Processori dedicati", and under the "Progetto Finalizzato Robotica", subproject 2 "Tema: ALPI".  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  Sistemi Informatici e Calcolo  Progetto di Intelligenza Artificiale e Robotica, Dipartimento di Elettronica e Informazione, Politecnico di  International Computer Science Institute,  and Progetto di Intelligenza Artificiale e Robotica, Dipartimento di Elettronica e Informazione, Politecnico di  
Pubnum: TR-92-040  
Email: (e-mail: colombet@ipmel2.elet.polimi.it).  (e-mail: dorigo@icsi.berkeley.edu).  
Date: Revised April 1993  
Abstract: Learning plays a vital role in the development of situated agents. In this paper, we explore the use of reinforcement learning to "shape" a robot to perform a predefined target behavior. We connect both simulated and real robots to A LECSYS, a parallel implementation of a learning classifier system with an extended genetic algorithm. After classifying different kinds of Animat-like behaviors, we explore the effects on learning of different types of agent's architecture (monolithic, flat and hierarchical) and of training strategies. In particular, hierarchical architecture requires the agent to learn how to coordinate basic learned responses. We show that the best results are achieved when both the agent's architecture and the training strategy match the structure of the behavior pattern to be learned. We report the results of a number of experiments carried out both in simulated and in real environments, and show that the results of simulations carry smoothly to real robots. While most of our experiments deal with simple reactive behavior, in one of them we demonstrate the use of a simple and general memory mechanism. As a whole, our experimental activity demonstrates that classifier systems with genetic algorithms can be practically employed to develop autonomous agents. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Agre, P. E., & D. Chapman, </author> <year> 1987. </year> <title> Pengi: An implementation of a theory of activity. </title> <booktitle> Proceedings of the 6th National Conference on Artificial Intelligence, AAAI 87, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, </address> <month> 268272. </month>
Reference: <author> Agre, P. E., & I. Horswill, </author> <title> this volume. Cultural support for improvisation. </title>
Reference: <author> Arkin, R. C., </author> <year> 1990. </year> <title> Integrating behavioral, perceptual, and world knowledge in reactive navigation. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> 6, </volume> <pages> 1-2, 105122. </pages>
Reference: <author> Barto, A. G., S. J. Bradtke & S. P. Singh, </author> <title> this volume. Learning to act using real-time dynamic programming. </title>
Reference: <author> Barto, A. G., R. S. Sutton & C. W. Anderson, </author> <year> 1983. </year> <title> Neuronlike elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 13, 834 - 846. </volume>
Reference: <author> Beer, R. D., </author> <title> this volume. A dynamical systems perspective on autonomous agents. </title>
Reference: <author> Beer, R. D., & J. C. Gallagher, </author> <year> 1992. </year> <title> Evolving dynamical neural networks for adaptive behavior. Adaptive Behavior , 1, 1, </title> <publisher> MIT Press, </publisher> <pages> 92122. </pages>
Reference: <author> Booker, L., </author> <year> 1988. </year> <title> Classifier Systems that Learn Internal World Models. </title> <journal> Machine Learning , 3, </journal> <volume> 2 - 3, </volume> <pages> 161192. </pages>
Reference: <author> Booker, L., D. E. Goldberg & J. H. Holland, </author> <year> 1989. </year> <title> Classifier Systems and Genetic Algorithms. </title> <booktitle> Artificial Intelligence , 40, </booktitle> <pages> 1-3, 235282. </pages>
Reference-contexts: The learning system Here we briefly illustrate some characteristics of A LECSYS, a parallel learning classifier system allowing for the implementation of hierarchies of classifier systems, which can be exploited to build modular agents. ALECSYS introduces some major improvements in the standard model of learning classifier systems (CS) <ref> (Booker, Goldberg & Holland, 1989) </ref>. First, A LECSYS permits to distribute a CS on any number of transputers (Dorigo & Sirtori, 1991; Dorigo, 1992a, 1992c) . Second, it gives the learning system designer the possibility to use many concurrent CSs, each one specialized in learning a specific behavioral pattern.
Reference: <author> Brooks, R. A., </author> <year> 1990a. </year> <title> Elephants don't play chess. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> 6, </volume> <pages> 1-2, 3 16. </pages>
Reference: <author> Brooks, R. A., </author> <year> 1990b. </year> <title> The behavior language: User's guide. </title> <publisher> MIT A.I. </publisher> <address> Lab Memo 1227, </address> <month> April. </month>
Reference-contexts: Brooks (1991b) has recently discussed the possibility to use genetic algorithms to evolve programs written in GEN, a high level language especially designed to produce programs which can be easily evolved by the genetic algorithm. (GEN can then be compiled into the Behavior Language <ref> (Brooks, 1990b) </ref>, a rule-based parallel programming language which compiles into the subsumption architecture.) This idea is, to the authors' knowledge, still under development (Brooks, 1991b) and no results have been published yet.
Reference: <author> Brooks, R. A., </author> <year> 1991a. </year> <title> Intelligence without representation. </title> <journal> Artificial Intelligence, </journal> <volume> 47, </volume> <pages> 1-3, 139 159. </pages>
Reference: <author> Brooks, R. A., </author> <year> 1991b. </year> <title> Artificial Life and Real Robots. </title> <booktitle> Proceedings of the First European Conference on Artificial Life (ECAL) , MIT Press, </booktitle> <pages> 310. </pages>
Reference-contexts: a high level language especially designed to produce programs which can be easily evolved by the genetic algorithm. (GEN can then be compiled into the Behavior Language (Brooks, 1990b), a rule-based parallel programming language which compiles into the subsumption architecture.) This idea is, to the authors' knowledge, still under development <ref> (Brooks, 1991b) </ref> and no results have been published yet. An approach similar to that proposed by Brooks was taken by Koza & Rice, which used the Genetic Programming paradigm (Koza, 1992) to evolve Lisp programs to control an autonomous robot (Koza & Rice, 1992).
Reference: <author> Camilli, A., R. Di Meglio, F. Baiardi, M. Vanneschi, D. Montanari & R. Serra, </author> <year> 1990. </year> <title> Classifier systems parallelization on MIMD architectures. </title> <type> Technical Report 3-17, </type> <institution> CNR, Italy. </institution>
Reference: <author> Clouse, J.A., & P.E. Utgoff, </author> <year> 1992. </year> <title> A teaching method for reinforcement learning. </title> <booktitle> Proceedings of the Ninth Conference on Machine Learning, </booktitle> <address> Aberdeen, Scotland, </address> <month> 92101. </month>
Reference-contexts: To develop more interesting interactions, we are currently moving to environments with richer dynamics. We are also considering the possibility of developing multi-agent, cooperative behaviors. Recent results in reinforcement learning and training <ref> (Clouse & Utgoff, 1992) </ref> suggest that the design of the reinforcement program, which currently requires a substantial designer's effort, could be replaced by direct interaction with a human trainer.
Reference: <author> Colombetti, M. & M. Dorigo, </author> <year> 1992. </year> <title> Learning to Control an Autonomous Robot by Distributed Genetic Algorithms. </title> <booktitle> Proceedings of From Animals To Animats, Second International Conference on Simulation of Adaptive Behavior (SAB92) , Honolulu. </booktitle>
Reference: <author> Colombetti, M. & M. Dorigo, </author> <year> 1993. </year> <title> Learning to plan. Forthcoming . ROBOT SHAPING 55 Compiani, </title> <editor> M., D. Montanari, R. Serra & G. Valastro, </editor> <year> 1989. </year> <title> Classifier systems and neural networks. </title> <editor> In E. R. Caianiello (ed.), </editor> <booktitle> Parallel architectures and neural networks , World Scientific. </booktitle>
Reference-contexts: Combination : flat architecture with integrated outputs (Figure 8b), or hierarchical architecture. Suppression : switch architecture (remember that the switch architecture is a special kind of hierarchical architecture). Sequence <ref> (not treated in this paper, see Colombetti & Dorigo, 1993) </ref>: hierarchical architecture. How to design an architecture: Quantitative criteria In Section 4 we stressed that the main reason for introducing architecture is speeding up learning of complex behavior patterns.
Reference: <author> Dorigo, M., </author> <year> 1993. </year> <title> Genetic and nongenetic operators in A LECSYS. </title> <note> To appear in Evolutionary Computation Journal </note> . 
Reference-contexts: Another difference is that we use learning classifier systems instead of Q-learning with statistical clustering. ROBOT SHAPING 10 The rule discovery module, which creates new classifiers according to an extended genetic algorithm <ref> (Dorigo, 1993) </ref>. Learning takes place at two distinct levels. First, the apportionment of credit can be viewed as a way of learning from experience the adaptive value of a number of given classifiers with respect to a predefined target behavior. <p> Combination : flat architecture with integrated outputs (Figure 8b), or hierarchical architecture. Suppression : switch architecture (remember that the switch architecture is a special kind of hierarchical architecture). Sequence <ref> (not treated in this paper, see Colombetti & Dorigo, 1993) </ref>: hierarchical architecture. How to design an architecture: Quantitative criteria In Section 4 we stressed that the main reason for introducing architecture is speeding up learning of complex behavior patterns. <p> more complex hierarchical structures? Memory : how can the agent solve problems that require it to remember what happened in the past? Simulation/Real world: are there major differences between the real and simulated worlds? Some other important questions, like the learning of basic behaviors, were discussed in a previous paper <ref> (Dorigo & Schnepf, 1993) </ref>. This section is organized as follows. First, we explain our experimental methodology. Second, we illustrate the simulated environments we used to carry out our experiments. Third, we report experiments that try to answer the first four questions (about architecture, shaping and learning). <p> We do not know yet whether our learning algorithm can manage tasks in which delayed reinforcement is a must. First results are contradictory <ref> (see Dorigo & Schnepf, 1993) </ref> and ROBOT SHAPING 53 further research is needed. Clearly, this issue is fundamental for developing more complex dynamic behavior, beyond the present limits of S-R responses. (ii) Quite a large amount of work is to be put in the architectural design.
Reference: <author> Dorigo, M., </author> <year> 1992a. </year> <title> Using transputers to increase speed and flexibility of genetics-based machine learning systems. </title> <journal> Microprocessing and Microprogramming Journal, </journal> <volume> 34, </volume> <pages> 147152. </pages>
Reference: <author> Dorigo, M., </author> <year> 1992b. </year> <title> A LECSYS and the AutonoMouse: Learning to control a real robot by distributed classifier systems. </title> <type> Technical Report 92011, </type> <institution> Dipartimento di Elettronica e Informazione, Politecnico di Milano, Milan, Italy. </institution>
Reference-contexts: All of the experiments showed that the AutonoMouse, although with some degraded performance, was still capable of achieving the goal of approaching the light source. A thorough discussion of these experiments can be found in Dorigo <ref> (1992b, see also Colombetti & Dorigo, 1992) </ref>. As an example we report here in Figure 29 the result of the one blind eye experiment.
Reference: <author> Dorigo, M., </author> <year> 1992c. </year> <title> Optimization, learning and natural algorithms , Ph. </title> <address> D. </address> <institution> Dissertation, Politecnico di Milano, Milan, Italy. </institution>
Reference-contexts: The Chameleon/Chase environment was introduced to study the composition of two independent behaviors. In this environment the agent learns to follow a light source and to change its color according to the background color (see Figure 11b). Results obtained in this environment were very satisfying <ref> (see Dorigo, 1992c) </ref>; using the flat architecture (Figure 8a) the agent was able to learn to follow the light source and to change its color correctly. (After 80,000 iterations, the average performance in the last 1,000 iterations was: 0.97 for the Chase behavior, 0.95 for the Chameleon behavior, and 0.92 for
Reference: <author> Dorigo, M., & U. Schnepf, </author> <year> 1993. </year> <title> Genetics-based machine learning and behavior-based robotics: </title>
Reference-contexts: Another difference is that we use learning classifier systems instead of Q-learning with statistical clustering. ROBOT SHAPING 10 The rule discovery module, which creates new classifiers according to an extended genetic algorithm <ref> (Dorigo, 1993) </ref>. Learning takes place at two distinct levels. First, the apportionment of credit can be viewed as a way of learning from experience the adaptive value of a number of given classifiers with respect to a predefined target behavior. <p> Combination : flat architecture with integrated outputs (Figure 8b), or hierarchical architecture. Suppression : switch architecture (remember that the switch architecture is a special kind of hierarchical architecture). Sequence <ref> (not treated in this paper, see Colombetti & Dorigo, 1993) </ref>: hierarchical architecture. How to design an architecture: Quantitative criteria In Section 4 we stressed that the main reason for introducing architecture is speeding up learning of complex behavior patterns. <p> more complex hierarchical structures? Memory : how can the agent solve problems that require it to remember what happened in the past? Simulation/Real world: are there major differences between the real and simulated worlds? Some other important questions, like the learning of basic behaviors, were discussed in a previous paper <ref> (Dorigo & Schnepf, 1993) </ref>. This section is organized as follows. First, we explain our experimental methodology. Second, we illustrate the simulated environments we used to carry out our experiments. Third, we report experiments that try to answer the first four questions (about architecture, shaping and learning). <p> We do not know yet whether our learning algorithm can manage tasks in which delayed reinforcement is a must. First results are contradictory <ref> (see Dorigo & Schnepf, 1993) </ref> and ROBOT SHAPING 53 further research is needed. Clearly, this issue is fundamental for developing more complex dynamic behavior, beyond the present limits of S-R responses. (ii) Quite a large amount of work is to be put in the architectural design.
References-found: 22

