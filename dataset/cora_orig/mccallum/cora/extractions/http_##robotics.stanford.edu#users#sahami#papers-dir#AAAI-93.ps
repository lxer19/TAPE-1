URL: http://robotics.stanford.edu/users/sahami/papers-dir/AAAI-93.ps
Refering-URL: http://robotics.stanford.edu/users/sahami/papers.html
Root-URL: 
Email: sahami@cs.Stanford.EDU  
Title: Learning NonLinearly Separable Boolean Functions With Linear Threshold Unit Trees and Madaline-Style Networks  
Author: Mehran Sahami 
Affiliation: Department of Computer Science Stanford University  
Address: Menlo Park, CA:  Stanford, CA 94305  
Note: In AAAI-93: Proceedings of the 11th National Conference on Artificial Intelligence, 335-41.  AAAI Press.  
Abstract: This paper investigates an algorithm for the construction of decisions trees comprised of linear threshold units and also presents a novel algorithm for the learning of non-linearly separable boolean functions using Madaline-style networks which are isomorphic to decision trees. The construction of such networks is discussed, and their performance in learning is compared with standard BackPropagation on a sample problem in which many irrelevant attributes are introduced. Littlestone's Winnow algorithm is also explored within this architecture as a means of learning in the presence of many irrelevant attributes. The learning ability of this Madaline-style architecture on nonoptimal (larger than necessary) networks is also explored. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Brent, R. P. </author> <year> 1990. </year> <title> Fast training algorithms for multilayer neural nets. Numerical Analysis Project Manuscript NA-90-03, </title> <institution> Dept. of Computer Science, Stanford Univ. </institution>
Reference-contexts: Recently, Brodley and Utgoff (1992) have also shown that the use of multivariate tests at each node of a decision tree often provides greater generalization when learning concepts in which there are irrelevant attributes. Furthermore, as presented in <ref> (Brent 1990) </ref>, we show how such an LTU tree can be transformed into a three-layer neural network with two hidden layers and one output layer (the input layer is not counted) and can often be trained much more quickly than the standard BackPropagation algorithm applied to an entire network (Rumelhart, Hinton, <p> Two examples are given below. As pointed out in <ref> (Brent 1990) </ref>, it is more efficient to do classifications using the tree structure than the corresponding network since the only computations which must be performed are those which lie on a single path from the root of the tree to a leaf.
Reference: <author> Brodley, C. E., and Utgoff, P. E. </author> <year> 1992. </year> <title> Multivariate Versus Univariate Decision Trees. </title> <type> COINS Technical Report 92-8, </type> <institution> Dept. of Computer Science, Univ. of Mass. </institution>
Reference-contexts: We set lrate = 1.0 and momentum = 0.5 in our experiments. There are many possible extensions to this LTU tree-building algorithm including irrelevant attribute elimination <ref> (Brodley & Utgoff 1992) </ref>, producing several hyperplanes at each node using different weight updating procedures and selecting the hyperplane which causes the fewest number of incorrect classifications, using Bayesian analysis to determine instance separations (Langley 1992), postprocessing of the tree to reduce its size, etc.
Reference: <author> Duda, R. O., and Hart, P. E. </author> <year> 1973. </year> <title> Pattern Classification and Scene Analysis. </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: We contrast how it performs in learning our sample nonlinearly separable function with the classical fixed increment (Perceptron) updating method <ref> (Duda & Hart 1973) </ref>. We also examine the effectiveness of such learning procedures in "nonoptimal" Madaline-style networks, and comment on possible future extensions of this learning architecture.
Reference: <author> Langley, P. </author> <year> 1992. </year> <title> Induction of Recursive Bayesian Classifiers. </title> <publisher> Forthcoming. </publisher>
Reference-contexts: There are many possible extensions to this LTU tree-building algorithm including irrelevant attribute elimination (Brodley & Utgoff 1992), producing several hyperplanes at each node using different weight updating procedures and selecting the hyperplane which causes the fewest number of incorrect classifications, using Bayesian analysis to determine instance separations <ref> (Langley 1992) </ref>, postprocessing of the tree to reduce its size, etc. These modifications are beyond the scope of this paper however, and generally are only fine tunings to the underlying learning architecture which is not changed by them.
Reference: <author> Littlestone, N. </author> <year> 1988. </year> <title> Learning quickly when irrelevant attributes abound: a new linear-threshold algorithm. </title> <booktitle> Machine Learning 2 </booktitle> <pages> 285-318. </pages>
Reference-contexts: Being primarily interested in functions in which many irrelevant attributes exist, we also explore the performance of the Winnow algorithm <ref> (Littlestone 1988, 1991) </ref> (which has proven effective in learning linearly separable functions in the presence of many irrelevant attributes) within the Madaline-style learning architecture. We contrast how it performs in learning our sample nonlinearly separable function with the classical fixed increment (Perceptron) updating method (Duda & Hart 1973).
Reference: <author> Littlestone, N. </author> <year> 1991. </year> <title> Redundant noisy attributes, attribute errors, and linear-threshold learning using Winnow. </title> <booktitle> In Proceedings of the Fourth Annual Workshop of Computational Learning Theory, </booktitle> <pages> 147-156. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference: <author> Nilsson, N. J. </author> <year> 1965. </year> <title> Learning machines. </title> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference: <author> Quinlan, J. R. </author> <year> 1986. </year> <title> Induction of decision trees. </title> <booktitle> Machine Learning 1 </booktitle> <pages> 81-106. </pages>
Reference: <author> Ridgway, W. C., </author> <year> 1962. </year> <title> An Adaptive Logic System with Generalizing Properties. </title> <type> Stanford Electronics Laboratories Technical Report 1556-1, </type> <institution> prepared under Air Force Contract AF 33(616)-7726, Stanford Univ. </institution>
Reference-contexts: After examining this transformation, a new incremental learning algorithm, based on a Madaline-style architecture <ref> (Ridgway 1962, Widrow & Winter 1988) </ref>, is presented in which learning is performed using such three-layer networks. The effectiveness of this algorithm is assessed on a sample nonlinearly separable boolean function in order to perform comparisons with the LTU tree algorithm and a similar network trained using standard BackPropagation.
Reference: <author> Rumelhart, D. E.; Hinton, G. E.; and Williams, R. J. </author> <year> 1986. </year> <title> Learning internal representations by error propagation. </title> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol. 1, </volume> <pages> eds. </pages>
Reference-contexts: in (Brent 1990), we show how such an LTU tree can be transformed into a three-layer neural network with two hidden layers and one output layer (the input layer is not counted) and can often be trained much more quickly than the standard BackPropagation algorithm applied to an entire network <ref> (Rumelhart, Hinton, & Williams 1986) </ref>. After examining this transformation, a new incremental learning algorithm, based on a Madaline-style architecture (Ridgway 1962, Widrow & Winter 1988), is presented in which learning is performed using such three-layer networks.
Reference: <editor> D. E. Rumelhart and J. L. McClelland, </editor> <address> 318-62. Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <editor> Rumelhart, D. E. and McClelland, J. L. eds. </editor> <booktitle> 1986. Parallel Distributed Processing, </booktitle> <volume> Vol. </volume> <pages> 1. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: in (Brent 1990), we show how such an LTU tree can be transformed into a three-layer neural network with two hidden layers and one output layer (the input layer is not counted) and can often be trained much more quickly than the standard BackPropagation algorithm applied to an entire network <ref> (Rumelhart, Hinton, & Williams 1986) </ref>. After examining this transformation, a new incremental learning algorithm, based on a Madaline-style architecture (Ridgway 1962, Widrow & Winter 1988), is presented in which learning is performed using such three-layer networks.
Reference: <author> Sahami, M. </author> <year> 1993. </year> <title> An Experimental Study of Learning NonLinearly Separable Boolean Functions With Trees of Linear Threshold Units. </title> <publisher> Forthcoming. </publisher>
Reference-contexts: are as follows: if n 25 then E = 25% if n &gt; 25 & n 100 then E = 12% else E = 6% Initial testing was performed within this LTU tree architecture using a variety of methods for learning the linear discriminant at each node of the tree <ref> (Sahami 1993) </ref>. Wishing to minimize the number of erroneous classifications made at each node in the tree, BackPropagation appeared to be the most promising of these weight updating procedures.
Reference: <author> Utgoff, P. E. </author> <year> 1988. </year> <title> Perceptron Trees: A Case Study in Hybrid Concept Representation. </title> <booktitle> In AAAI-88 Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> 601-6. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Introduction We initially examine a non-incremental algorithm that learns binary classification tasks by producing decision trees of linear threshold units (LTU trees). This decision tree bears some similarity to the decision trees produced by ID3 (Quinlan 1983) and Perceptron Trees <ref> (Utgoff 1988) </ref>, yet it seems to promise more generality as each node in our tree implements a separate linear discriminant function while only the leaves of a Perceptron Tree have this generality and the remaining nodes in both the Perceptron Tree and the trees produced by ID3 perform a test on
Reference: <author> Widrow, B., and Winter, R. G. </author> <year> 1988. </year> <title> Neural Nets for Adaptive Filtering and Adaptive Pattern Recognition. </title> <publisher> IEEE Computer, March:25-39. </publisher>
Reference: <author> Winston, P. </author> <year> 1992. </year> <booktitle> Artificial Intelligence, third edition. </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
References-found: 16

