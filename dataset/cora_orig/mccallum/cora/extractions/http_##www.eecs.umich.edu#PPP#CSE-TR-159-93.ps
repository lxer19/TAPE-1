URL: http://www.eecs.umich.edu/PPP/CSE-TR-159-93.ps
Refering-URL: http://www.eecs.umich.edu/PPP/publist.html
Root-URL: http://www.cs.umich.edu
Email: email: kahhaleh@eecs.umich.edu  
Phone: Phone: (313) 763-6970 FAX: (313) 763-4617  
Title: Analysis of Memory Latency Factors and their Impact on KSR1 MPP Performance  
Author: Bassam Kahhaleh 
Note: On sabbatical leave from University of Jordan, Amman, Jordan. This work uses facilities partially funded by NSF grant CDA-92-14296.  
Date: Apr. 15, 1993  
Address: 1301 Beal Avenue, Room 2312 EECS  Ann Arbor, MI 48109-2122  
Affiliation: Advanced Computer Architecture Lab. Department of Electrical Engineering and Computer Science  The University of Michigan  
Abstract: The Kendall Square Research KSR1 MPP system has a shared address space, which spreads over physically distributed memory modules. Thus, memory access time can vary over a wide range even when accessing the same variable, depending on how this variable is being referenced and updated by the various processors. Since the processor stalls during this access time, the KSR1 performance depends considerably on the program's locality of reference. The KSR1 provides two novel features to reduce such long memory latencies: prefetch and post-store instructions. This paper analyzes the various memory latency factors which stalls the processor during program execution. A suitable model for evaluating these factors is developed for the execution of FORTRAN DO-loops parallelized with the Tile construct using the Slice strategy. The DO-loops used in the benchmark program perform sparse matrix-vector multiply, vector-vector dot product, and vector-vector addition, which are typically executed in an iterative sparse solver. Memory references generated by such loops are analyzed and their memory latencies are experimentally evaluated. Thus, the performance of the KSR1 and its unique memory system is determined. Furthermore, the prefetch and post-store operations are evaluated and their effects on performance and memory latencies are determined. The limited size of the prefetch queue is shown to stall the processor for a long period of time, which reduces the benefit of prefetch considerably. The post-store operation is evaluated with two placements: immediate and delayed post-store. In both cases, the post-store operation has a high overhead. However, it is shown that delaying the post-store operation improved performance considerably. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Chatterjee, J. Jin, and J. Volakis, </author> <title> "Application of edge-based finite elements and ABCs to 3-D scattering," </title> <note> To appear in the IEEE Transactions on Antennas and Propagation, </note> <year> 1993. </year>
Reference-contexts: The benchmark program used is a Finite Element Method (FEM) radiation backscatter modeling application, using the diagonal-preconditioned symmetric biconjugate gradient method to solve a system of complex linear equations <ref> [1] </ref>. All DO-loops in this program were tiled with the slice strategy. Figure ?? shows the main loop outlining the performed vector operations. The execution time of these operations is analyzed to determine the effect of memory references on performance.
Reference: [2] <author> T. H. Dunigan, </author> <title> "Kendall square multiprocessor: Early experiences and performance," </title> <type> Technical Report ORNL/TM-12065, </type> <institution> Oak Ridge National Laboratory, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: The ALLCACHE ENGINE is capable of a peak data transfer rate of 1 GByte/sec, with a packet length of 128 data bytes and a 16-byte header [6]. However, the maximum achievable data transfer bandwidth is reported as 731 MBytes/sec <ref> [2] </ref>. Each cell consists of a 64-bit superscalar processor, 0.25 MB instruction `sub-cache', 0.25 MB data `sub-cache', and 32 MB `local-cache'. The processor is clocked at the rate of 20 MHz, and can perform a maximum of two floating point operations per clock.
Reference: [3] <author> J. R. Goodman and P. J. Woest, </author> <title> "The Wisconsin multicube: A new large-scale cache-coherent multiprocessor," </title> <booktitle> in Proceedings of the Fifteenth Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 422-431, </pages> <year> 1988. </year> <month> 16 </month>
Reference-contexts: With the main memory of each processor made as a cache memory, the system is considered as a Cache-Only Memory Architecture (COMA) [4], with guaranteed cache coherency and automatic data movement between caches. Other cache protocols also employ automatic update feature, for example <ref> [3] </ref>. Once a program is loaded in the KSR1 memory and starts to run, a reference to a data element is usually satisfied by the processor's own data sub-cache.
Reference: [4] <author> E. Hagersten, A. Landin, and S. Haridi, </author> <title> "DDM | a cache-only memory architecture," </title> <booktitle> Computer, </booktitle> <month> September </month> <year> 1992. </year>
Reference-contexts: Storage is allocated in 16 KBytes `page' units, while data movement 2 between local caches is based on 128-Byte `subpage' units. With the main memory of each processor made as a cache memory, the system is considered as a Cache-Only Memory Architecture (COMA) <ref> [4] </ref>, with guaranteed cache coherency and automatic data movement between caches. Other cache protocols also employ automatic update feature, for example [3]. Once a program is loaded in the KSR1 memory and starts to run, a reference to a data element is usually satisfied by the processor's own data sub-cache.
Reference: [5] <institution> KSR1 Principles of Operation, Kendall Square Research Corporation, </institution> <year> 1991. </year>
Reference-contexts: The peak performance of each cell is rated at 20 MIPS (VLIW instructions) and 40 MFLOPS, with 64 floating point registers, 32 integer registers, and 32 address registers <ref> [5] </ref>. The two sub-caches are organized as a 64-set, 2-way set associative caches with random replacement strategy. Storage is allocated with tag descriptors in 2 KBytes `block' units, while data movement between sub-cache and local cache is based on 64-Byte `subblock' units. <p> This broadcast is especially useful if other cells have invalid copies of the broadcasted subpage and they are not too busy to ignore it <ref> [5] </ref>. Thus, subsequent references to this data element by those processors which utilized this broadcast would avoid the long stall time normally incurred in bringing the new value from a remote cache on demand. <p> When the processor issues a post-store request to its local cache, the processor stalls for a period of time 8 clocks, at the same time the local cache resource becomes unavailable for a period of time 24 <ref> [5] </ref>. Thus, a subsequent reference to the local cache might stall the processor until the local cache is free. Figure ?? shows the savings in step (1) and the cost of using the single post-store approach in step (8).
Reference: [6] <institution> KSR1 Technical Summary, Kendall Square Research Corporation, </institution> <year> 1992. </year>
Reference-contexts: The second level, `ALLCACHE ENGINE:1' or AE:1, can connect a maximum of 34 AE:0 rings together. The ALLCACHE ENGINE is capable of a peak data transfer rate of 1 GByte/sec, with a packet length of 128 data bytes and a 16-byte header <ref> [6] </ref>. However, the maximum achievable data transfer bandwidth is reported as 731 MBytes/sec [2]. Each cell consists of a 64-bit superscalar processor, 0.25 MB instruction `sub-cache', 0.25 MB data `sub-cache', and 32 MB `local-cache'. <p> Different tiling strategies are provided to handle various loop constructs, loop dependencies, and vector or matrix accessing patterns. The strategy may be static (determined at compile time) like the slice, mod, and wave strategies, or dynamic (determined at run time) like the grab strategy <ref> [6] </ref>. The slice strategy slices the iteration space of the DO-loop so that each pthread executes the DO-loop over a certain part of the iteration space.
Reference: [7] <institution> IEEE Technical Committee on Operating Systems, "Threads execution for portable operating systems," </institution> <note> draft P1003.4a/D4, </note> <year> 1990. </year>
Reference-contexts: These constructs provide the necessary high level interface to pthreads <ref> [7] </ref>. The parallel section construct is used to execute multiple blocks of code in parallel. The parallel region construct is used to execute multiple instances of a block of code in parallel.
Reference: [8] <author> D. Windheiser, E. Boyd, E.Hao, S. Abraham, and E. Davidson, </author> <title> "KSR1 multiprocessor: Analysis of latency hiding techniques in a sparse solver," </title> <booktitle> The 7th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1993. </year> <month> 17 </month>
Reference-contexts: All DO-loops in this program were tiled with the slice strategy. Figure ?? shows the main loop outlining the performed vector operations. The execution time of these operations is analyzed to determine the effect of memory references on performance. In an early study <ref> [8] </ref>, the performance of KSR1 executing this benchmark was reported with emphasis on the effects of using latency hiding techniques. The observed performance improvement due to employing prefetch and post-store techniques was much reduced by the unexpected high overhead of using these two instructions. <p> The memory latency for a remote subblock reference is 135 clocks depending on the ring load <ref> [8] </ref>. Since the same DO-loop in an iterative solver is executed repeatedly, some of the referenced instruction and local data subblocks may have remained in sub-cache from last iteration.
References-found: 8

