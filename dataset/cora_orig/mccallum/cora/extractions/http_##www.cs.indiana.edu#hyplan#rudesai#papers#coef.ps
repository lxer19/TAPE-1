URL: http://www.cs.indiana.edu/hyplan/rudesai/papers/coef.ps
Refering-URL: http://www.cs.indiana.edu/hyplan/rudesai/research.html
Root-URL: http://www.cs.indiana.edu
Title: High-Dimensional Wavelet Modeling  
Author: Amitava Dhar and Rutvik Desai 
Date: January 26, 1996  
Abstract: Wavelets can be thought of as a set of well-localized basis functions with very good approximation properties. The difficulty in applying wavelet approximation to high-dimensional data is that the number of basis functions increases exponentially with the number of dimensions, making the application of standard mathematical methods for determining coefficients diffic ult. We propose a modeling methodology that uses multidimensional cubic B-spline wavelets whose co efficients are determined by a nonlinear optimization procedure that combines simulated anneali ng with hill climbing. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Courant and D. </author> <title> Hilbert, </title> <journal> "Methods of Mathematical Physics," </journal> <volume> Vol. 1, </volume> <publisher> Interscience Publishers, </publisher> <address> New York, </address> <year> 1955. </year>
Reference-contexts: This technique generates the high dimensional wavelet basis functions from the product of the one dimensional truncated wavelet series representations. The procedure is analogous to that proposed 1 2 by Courant and Hilbert <ref> [1] </ref> for generating multidimensional orthonormal bases from one dimensional orthonormal basis functions. The number of wavelet basis functions increase exponentially with the increase in number of dimensions or input variables. A nonlinear optimization technique may be required to determine the coefficients if the number of bases is very large.
Reference: [2] <author> C. K. Chui, </author> <title> "An Introduction to Wavelets," </title> <publisher> Academic Press Inc., </publisher> <address> San Diego, California, </address> <year> 1992. </year>
Reference-contexts: which this is accomplished is best described in the context of a multiresolution analysis. 2.2 Multiresolution Analysis A multiresolution analysis of data can be performed by considering a sequence of approximation spaces V m that satisfy certain properties, some of which are discussed below (a detailed description is available in <ref> [2] </ref>). The spaces V m are related to each other as follows: :::: ae V 2 ae V 1 ae V 0 ae V 1 ae V 2 ::: (2) There is a special function OE (x), called a scaling function, which lies in V 0 .
Reference: [3] <author> I. Daubechies, </author> <title> "Ten Lectures on Wavelets," </title> <publisher> SIAM, </publisher> <address> Philadelphia, Penn-sylvania, </address> <year> 1992. </year>
Reference-contexts: The local basis functions with certain other attractive approximation properties have made the wavelet analysis a better option for many applications such as signal processing, data compression, etc. <ref> [3] </ref>. There could be several ways of using wavelets for approximation in high dimensions, however, the proposed methodology adopts the technique suggested by Daubechies [3]. This technique generates the high dimensional wavelet basis functions from the product of the one dimensional truncated wavelet series representations. <p> basis functions with certain other attractive approximation properties have made the wavelet analysis a better option for many applications such as signal processing, data compression, etc. <ref> [3] </ref>. There could be several ways of using wavelets for approximation in high dimensions, however, the proposed methodology adopts the technique suggested by Daubechies [3]. This technique generates the high dimensional wavelet basis functions from the product of the one dimensional truncated wavelet series representations. The procedure is analogous to that proposed 1 2 by Courant and Hilbert [1] for generating multidimensional orthonormal bases from one dimensional orthonormal basis functions. <p> In equation (1), the powers of two have been used to generate the family of wavelets, however, any other integer or fraction could also have been used <ref> [3] </ref>. It may be noted that if the parent wavelet (x) decays quickly, then linear combinations of functions from the family defined in equation (1) will retain this property and have good localization characteristics. <p> m x k) 2 g for (m; k)2Z 2 (10) m;k (x) = 0:251477 2 m cosf2:570935f2 (2 m x k) 1ggfi expf0:222759f2 (2 m x k) 1g 2 g for (m; k)2Z 2 (11) Multidimensional wavelets can be derived from one dimensional wavelets by the method proposed by Daubechies <ref> [3] </ref>. To start with, let us consider the 5 case of two input variables.
Reference: [4] <author> S. Haykin, </author> <title> "Neural Networks," </title> <publisher> Macmillan College Publishing Company, Inc., </publisher> <address> New York. </address>
Reference-contexts: It is derived from Monte Carlo methods in statistical mechanics and attempts to avoid local minima by taking non-locally optimal steps in the search space, when needed. The details of a conventional simulated annealing procedure is available in <ref> [4] </ref>. L. Ingber [6] proposed an improved method for annealing, which can use a significantly faster temperature annealing schedule than that for a conventional simulated annealing. This method is known as Very Fast Simulated Re-annealing (VFSR).
Reference: [5] <author> S. Kirkpatrick, C. D. Gelatt and M. P. Vecchi, </author> <title> "Optimization by Simulated Annealing," </title> <journal> Science, </journal> <volume> 220, </volume> <pages> pp. 671-680, </pages> <year> 1983. </year>
Reference-contexts: Once the cost-function is formulated, the optimization can be carried out by the procedure described in the next section. 3 Simulated Annealing and SALO 3.1 Simulated Annealing Simulated annealing <ref> [5] </ref> is an optimization technique with several advantages. It can process cost functions with arbitrary degrees of nonlinearities, discontinuities and stochasticity. Also, it takes care of arbitrary boundary conditions and constraints imposed on these cost functions. The methodology can be implemented quite easily and guarantees statistically optimal solution.
Reference: [6] <author> L. Ingber, </author> <title> "Very Fast Simulated Re-Annealing," </title> <journal> Mathl. Comput. Mod-elling, </journal> <volume> 12, </volume> <pages> pp. 967-973, </pages> <year> 1989. </year>
Reference-contexts: This improves the speed of convergence of the optimization procedure significantly when compared to a pure simulated annealing. Also, Desai and Patil [11] has shown that combining adaptive simulated annealing <ref> [6] </ref> with hill climbing [10] outperforms Parallel Genetic Algorithm and ASA on a variety of functions with different characteristics. <p> It is derived from Monte Carlo methods in statistical mechanics and attempts to avoid local minima by taking non-locally optimal steps in the search space, when needed. The details of a conventional simulated annealing procedure is available in [4]. L. Ingber <ref> [6] </ref> proposed an improved method for annealing, which can use a significantly faster temperature annealing schedule than that for a conventional simulated annealing. This method is known as Very Fast Simulated Re-annealing (VFSR).
Reference: [7] <author> L. Ingber, </author> <title> Adaptive Simulated Annealing (ASA) [ftp.caltech.edu: /pub/ingber/asa.Z], Software package documentation, </title> <year> 1995. </year>
Reference-contexts: L. Ingber [6] proposed an improved method for annealing, which can use a significantly faster temperature annealing schedule than that for a conventional simulated annealing. This method is known as Very Fast Simulated Re-annealing (VFSR). We propose the use of a modified VFSR known as Adaptive Simulated Annealing (ASA) <ref> [7] </ref>. 3.2 Local Optimization While a local optimizer such as hill climber is an efficient method for optimization in simple, unimodal spaces it easily gets stuck in non-optimal regions in real world problems.
Reference: [8] <author> M. Unser and A. Aldroubi, </author> <title> "Polynomial Splines and Wavelets- A Signal Processing Perspective,"Wavelets: A Tutorial in Theory and Applications, </title> <editor> Editor C. K. Chui, </editor> <publisher> Academic Press Inc., </publisher> <address> San Diego, California. </address>
Reference-contexts: complete expression of the best approximation to f (x) at the resolution level (m 1): +1 X ff m;k OE m;k (x) + k=1 2.3 Wavelets in Higher Dimensions The one dimensional cubic B-spline scaling functions and wavelets at any scale m and location k have the following general expressions <ref> [8] </ref>: OE m;k (x) = 0:690988 2 m expf1:5 (2 m x k) 2 g for (m; k)2Z 2 (10) m;k (x) = 0:251477 2 m cosf2:570935f2 (2 m x k) 1ggfi expf0:222759f2 (2 m x k) 1g 2 g for (m; k)2Z 2 (11) Multidimensional wavelets can be derived from
Reference: [9] <author> P. Winston, </author> <booktitle> Artificial Intelligence, </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, third edition, </address> <year> 1992. </year>
Reference-contexts: The main reasons for the failure of the hill climber are (1) local optima (the foothill problem); (2) flat surfaces (the plateau problem) and/or (3) ridges <ref> [9] </ref>.
Reference: [10] <author> D. Yuret, </author> <title> "From Genetic Algorithms to Efficient Optimization," </title> <type> Masters Thesis, </type> <institution> Dept. of Electrical Engineering, MIT, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: This improves the speed of convergence of the optimization procedure significantly when compared to a pure simulated annealing. Also, Desai and Patil [11] has shown that combining adaptive simulated annealing [6] with hill climbing <ref> [10] </ref> outperforms Parallel Genetic Algorithm and ASA on a variety of functions with different characteristics. <p> The main reasons for the failure of the hill climber are (1) local optima (the foothill problem); (2) flat surfaces (the plateau problem) and/or (3) ridges [9]. The local optimization algorithm proposed in <ref> [10] </ref> attempts to tackle some of these problems while trying to maintain the efficiency by employing the following ideas: (1) Adjust the size 7 of the probing steps to suit the nature of the terrain, shrinking when probes do poorly and growing when probes do well. (2) Keep track of the
Reference: [11] <author> R. Desai and R. Patil, "SALO: </author> <title> Combining simulated annealing and local optimization for efficient global optimization," </title> <type> Technical Report LA-UR [95-2862], </type> <institution> Los Alamos National Laboratory, </institution> <year> 1995. </year>
Reference-contexts: This improves the speed of convergence of the optimization procedure significantly when compared to a pure simulated annealing. Also, Desai and Patil <ref> [11] </ref> has shown that combining adaptive simulated annealing [6] with hill climbing [10] outperforms Parallel Genetic Algorithm and ASA on a variety of functions with different characteristics. <p> &gt; f (~x) else if iter = 0 ~x ~x + ~v; ~v 2~u; else if f (~x + ~u + ~v) &lt; f (~x) ~x ~x + ~u + ~v; ~v 2~u; else ~x ~x + ~v; ~v 2~v; return ~x 3.3 SALO SALO (Simulated Annaling with Local Optimization) <ref> [11] </ref> algorithm attempts to combine the ability of the SA process to get out of local optima and the efficiency of the local optimizer in relatively "simple" regions of the space. <p> In <ref> [11] </ref>, SALO was shown to outperform a Parallel Genetic Algorithm and ASA by a significant margin on a variety of functions with different chracteristics. 4 Conclusions The methodology to model with wavelet-based cost-function which is optimized by a combined simulated annealing hill climbing algorithm needs to be applied to different problems
References-found: 11

