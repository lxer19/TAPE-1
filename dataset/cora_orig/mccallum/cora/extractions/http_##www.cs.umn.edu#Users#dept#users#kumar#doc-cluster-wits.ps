URL: http://www.cs.umn.edu/Users/dept/users/kumar/doc-cluster-wits.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Title: Web Page Categorization and Feature Selection Using Association Rule and Principal Component Clustering  
Author: Jerome Moore, Eui-Hong (Sam) Han, Daniel Boley, Maria Gini, Robert Gross, Kyle Hastings, George Karypis, Vipin Kumar, and Bamshad Mobasher 
Keyword: clustering, information retrieval, world wide web, association rules, data mining, intelligent software agents.  
Address: Minneapolis, MN 55455  
Affiliation: Department of Computer Science and Engineering University of Minnesota,  
Abstract: Clustering techniques have been used by many intelligent software agents in order to retrieve, filter, and categorize documents available on the World Wide Web. Clustering is also useful in extracting salient features of related web documents to automatically formulate queries and search for other similar documents on the Web. Traditional clustering algorithms either use a priori knowledge of document structures to define a distance or similarity among these documents, or use probabilistic techniques such as Bayesian classification. Many of these traditional algorithms, however, falter when the dimensionality of the feature space becomes high relative to the size of the document space. In this paper, we introduce two new clustering algorithms that can effectively cluster documents, even in the presence of a very high dimensional feature space. These clustering techniques. which are based on generalizations of graph partitioning, do not require pre-specified ad hoc distance functions, and are capable of automatically discovering document similarities or associations. We conduct several experiments on real Web data using various feature selection heuristics, and compare our clustering schemes to standard distance-based techniques, such as hierarchical agglomeration clustering, and Bayesian classification methods, AutoClass. 
Abstract-found: 1
Intro-found: 1
Reference: [AMS + 96] <author> A. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and A.I. Verkamo. </author> <title> Fast discovery of association rules. In U.M. </title> <editor> Fayyad, G. Piatetsky-Shapiro, P. Smith, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 307-328. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: This method first finds set of items that occur frequently together in transactions using association rule discovery methods <ref> [AMS + 96] </ref>. These frequent item sets are then used to group items into hypergraph edges, and a hypergraph partitioning algorithm [KAKS97] is used to find the item clusters. The similarity among items is captured implicitly by the frequent item sets. <p> In this model, the set of vertices V corresponds to the documents, and each hyperedge e 2 E corresponds to a set of related documents found. The weight of a hyperedge is calculated as the average confidence <ref> [AMS + 96] </ref> of all the association rules involving the related documents of the hyperedge. For example, if fd 1 ; d 2 ; d 3 g is a frequent item set, then the hypergraph contains a hyperedge that connects d 1 , d 2 and d 3 . <p> Similarly, this method can be applied to word clustering. In this setting, each word corresponds to an item and each document corresponds to a transaction. This method uses the Apriori algorithm <ref> [AMS + 96] </ref> which has been shown to be very efficient in finding frequent item sets and HMETIS [KAKS97] which can partition very large hypergraphs (of size &gt; 100K nodes) in minutes on personal computers. 2.2 Principal Component Partitioning Algorithm In the principal component algorithm, each document is represented by an
Reference: [Ber76] <author> C. Berge. </author> <title> Graphs and Hypergraphs. </title> <publisher> American Elsevier, </publisher> <year> 1976. </year>
Reference-contexts: A frequent item sets found using the association rule discovery algorithm corresponds to a set of documents that have a sufficiently large number of features in common. These frequent item sets are mapped into hyperedges in a hypergraph. A hypergraph <ref> [Ber76] </ref> H = (V; E) consists of a set of vertices V and a set of hyperedges E. A hypergraph is an extension of a graph in the sense that each hyperedge can connect more than two vertices.
Reference: [BGM97] <author> Andrei Z. Broder, Steven C. Glassman, and Mark S. Manasse. </author> <title> Syntactic clustering of the Web. </title> <booktitle> In 6th International World Wide Web Conference, </booktitle> <month> April </month> <year> 1997. </year> <note> http://proceedings.www6conf.org/HyperNews/get/PAPER205.html. </note>
Reference: [BGMZ97] <author> A. Z. Broder, S. C. Glassman, M. S. Manasse, and G Zweig. </author> <title> Syntactic clustering of the web. </title> <booktitle> In Proc. of 6th International World Wide Web Conference, </booktitle> <year> 1997. </year> <month> - 9 </month> - 
Reference-contexts: Table 2 also specifies these sub-categories for each cluster. 4 Related Work A number of Web agents use various information retrieval techniques [FBY92] and characteristics of open hypertext Web documents to automatically retrieve, filter, and categorize these documents <ref> [CH97, BGMZ97, MS96, WP97, WVS + 96] </ref>. For example, HyPursuit [WVS + 96] uses semantic information embedded in link structures as well as document content to create cluster hierarchies of hypertext documents and structure an information space.
Reference: [BSY95] <author> Marko Balabanovic, Yoav Shoham, and Yeogirl Yun. </author> <title> An adaptive agent for automated Web browsing. Journal of Visual Communication and Image Representation, </title> <type> 6(4), </type> <year> 1995. </year> <note> http://www-diglib.stanford.edu/cgi-bin/WP/get/SIDL-WP-1995-0023. </note>
Reference-contexts: Syskill & Webert [PMB96] represents an HTML page with a Boolean feature vector, and then uses naive Bayesian classification to find web pages that are similar, but for only a given single user profile. Also, Balabanovic <ref> [BSY95] </ref> presents a system that uses a single well-defined profile to find similar web documents for a user. Candidate web pages are - 8 - located using best-first search, comparing their word vectors against a user profile vector, and returning the highest -scoring pages.
Reference: [CH97] <author> C. Chang and C. Hsu. </author> <title> Customizable multi-engine search tool with clustering. </title> <booktitle> In Proc. of 6th International World Wide Web Conference, </booktitle> <year> 1997. </year>
Reference-contexts: Table 2 also specifies these sub-categories for each cluster. 4 Related Work A number of Web agents use various information retrieval techniques [FBY92] and characteristics of open hypertext Web documents to automatically retrieve, filter, and categorize these documents <ref> [CH97, BGMZ97, MS96, WP97, WVS + 96] </ref>. For example, HyPursuit [WVS + 96] uses semantic information embedded in link structures as well as document content to create cluster hierarchies of hypertext documents and structure an information space.
Reference: [CS96] <author> P. Cheeseman and J. Stutz. </author> <title> Baysian classification (autoclass): Theory and results. In U.M. </title> <editor> Fayyad, G. Piatetsky-Shapiro, P. Smith, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 153-180. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: If the dimensionality is high, then the calculated mean values do not differ significantly from one cluster to the next. Hence the clustering based on these mean values does not always produce very good clusters. Similarly, probabilistic methods such as Bayesian classification used in AutoClass <ref> [CS96] </ref>, do not perform well when the size of the feature space is much larger than the size of the sample set. This type of data distribution seems to be characteristic of document categorization applications on the Web, such as categorizing a bookmark file. <p> For our evaluation, we compare these algorithms to two well-known methods: Bayesian classification as used by AutoClass <ref> [CS96] </ref> and hierarchical agglomeration clustering (HAC) based on the use of a distance function [DH73]. - 2 - AutoClass is based on the probabilistic mixture modeling [TSM85], and given a data set it finds maximum parameter values for a specific probability distribution functions of the clusters.
Reference: [DH73] <author> Richard O. Duda and Peter E. Hart. </author> <title> Pattern Classification and scene analysis. </title> <publisher> John Wiley & Sons, </publisher> <year> 1973. </year>
Reference-contexts: For our evaluation, we compare these algorithms to two well-known methods: Bayesian classification as used by AutoClass [CS96] and hierarchical agglomeration clustering (HAC) based on the use of a distance function <ref> [DH73] </ref>. - 2 - AutoClass is based on the probabilistic mixture modeling [TSM85], and given a data set it finds maximum parameter values for a specific probability distribution functions of the clusters. The clustering results provide the full description of each cluster in terms of probability distribution of each attributes.
Reference: [FBY92] <author> W. B. Frakes and R. Baeza-Yates. </author> <title> Information Retrieval Data Structures and Algorithms. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1992. </year>
Reference-contexts: We show that partitioning clustering methods perform better than traditional distance based clustering. - 1 - 2 Clustering Methods Most of the existing approaches to document clustering are based on either probabilistic methods, or distance and similarity measures (see <ref> [FBY92] </ref>). Distance-based methods such as k-means analysis, hierarchical clustering [JD88] and nearest-neighbor clustering [LF78] use a selected set of words (features) appearing in different documents as the dimensions. Each such feature vector, representing a document, can be viewed as a point in this multidimensional space. <p> To remain unbiased we assigned the finer labels before starting any of the experiments. Table 2 also specifies these sub-categories for each cluster. 4 Related Work A number of Web agents use various information retrieval techniques <ref> [FBY92] </ref> and characteristics of open hypertext Web documents to automatically retrieve, filter, and categorize these documents [CH97, BGMZ97, MS96, WP97, WVS + 96].
Reference: [Fra92] <author> W. B. Frakes. </author> <title> Stemming algorithms. </title> <editor> In W. B. Frakes and R. Baeza-Yates, editors, </editor> <booktitle> Information Retrieval Data Structures and Algorithms, </booktitle> <pages> pages 131-160. </pages> <publisher> Prentice Hall, </publisher> <year> 1992. </year>
Reference-contexts: This ensures a stable data sample since some pages are fairly dynamic in content. Table 1: Setup of experiments. The word lists from all documents were filtered with a stop-list and "stemmed" using Porter's suffix-stripping algorithm [Por80] as implemented by <ref> [Fra92] </ref>. We derived seven experiments and clustered the documents using the four algorithms described earlier. Our objective is to reduce the dimensionality of the clustering problem and retain the important - 5 - features of the documents.
Reference: [HKKM97a] <author> E.H. Han, G. Karypis, V. Kumar, and B. Mobasher. </author> <title> Clustering based on association rule hypergraphs. </title> <booktitle> In Workshop on Research Issues on Data Mining and Knowledge Discovery, </booktitle> <pages> pages 9-13, </pages> <address> Tucson, Arizona, </address> <year> 1997. </year>
Reference-contexts: The HAC method starts with trivial clusters, each containing one document and iteratively combines smaller clusters that are sufficiently "close" based on a distance metric. 2.1 Association Rule Hypergraph Partitioning Algorithm In <ref> [HKKM97a, HKKM97b] </ref>, a new method was proposed for clustering related items in transaction-based databases, such as supermarket bar code data, using association rules and hypergraph partitioning. This method first finds set of items that occur frequently together in transactions using association rule discovery methods [AMS + 96].
Reference: [HKKM97b] <author> E.H. Han, G. Karypis, V. Kumar, and B. Mobasher. </author> <title> Clustering based on association rule hypergraphs. </title> <type> Technical Report TR-97-019, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, </institution> <year> 1997. </year>
Reference-contexts: The HAC method starts with trivial clusters, each containing one document and iteratively combines smaller clusters that are sufficiently "close" based on a distance metric. 2.1 Association Rule Hypergraph Partitioning Algorithm In <ref> [HKKM97a, HKKM97b] </ref>, a new method was proposed for clustering related items in transaction-based databases, such as supermarket bar code data, using association rules and hypergraph partitioning. This method first finds set of items that occur frequently together in transactions using association rule discovery methods [AMS + 96].
Reference: [JD88] <author> A.K. Jain and R. C. Dubes. </author> <title> Algorithms for Clustering Data. </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: We show that partitioning clustering methods perform better than traditional distance based clustering. - 1 - 2 Clustering Methods Most of the existing approaches to document clustering are based on either probabilistic methods, or distance and similarity measures (see [FBY92]). Distance-based methods such as k-means analysis, hierarchical clustering <ref> [JD88] </ref> and nearest-neighbor clustering [LF78] use a selected set of words (features) appearing in different documents as the dimensions. Each such feature vector, representing a document, can be viewed as a point in this multidimensional space.
Reference: [KAKS97] <author> G. Karypis, R. Aggarwal, V. Kumar, and S. Shekhar. </author> <title> Multilevel hypergraph partitioning: Application in VLSI domain. </title> <booktitle> In Proceedings ACM/IEEE Design Automation Conference, </booktitle> <year> 1997. </year>
Reference-contexts: This method first finds set of items that occur frequently together in transactions using association rule discovery methods [AMS + 96]. These frequent item sets are then used to group items into hypergraph edges, and a hypergraph partitioning algorithm <ref> [KAKS97] </ref> is used to find the item clusters. The similarity among items is captured implicitly by the frequent item sets. In document clustering, each document corresponds to an item and each possible feature corresponds to a transaction. <p> Similarly, this method can be applied to word clustering. In this setting, each word corresponds to an item and each document corresponds to a transaction. This method uses the Apriori algorithm [AMS + 96] which has been shown to be very efficient in finding frequent item sets and HMETIS <ref> [KAKS97] </ref> which can partition very large hypergraphs (of size &gt; 100K nodes) in minutes on personal computers. 2.2 Principal Component Partitioning Algorithm In the principal component algorithm, each document is represented by an N -vector, where N is the number of features to be used for the selection.
Reference: [LF78] <author> S.Y. Lu and K.S. Fu. </author> <title> A sentence-to-sentence clustering procedure for pattern analysis. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 8 </volume> <pages> 381-389, </pages> <year> 1978. </year>
Reference-contexts: Distance-based methods such as k-means analysis, hierarchical clustering [JD88] and nearest-neighbor clustering <ref> [LF78] </ref> use a selected set of words (features) appearing in different documents as the dimensions. Each such feature vector, representing a document, can be viewed as a point in this multidimensional space. There are a number of problems with clustering in a multi-dimensional space using traditional distance- or probability-based methods.
Reference: [MS96] <author> Y. S. Maarek and I.Z. Ben Shaul. </author> <title> Automatically organizing bookmarks per content. </title> <booktitle> In Proc. of 5th International World Wide Web Conference, </booktitle> <year> 1996. </year>
Reference-contexts: Table 2 also specifies these sub-categories for each cluster. 4 Related Work A number of Web agents use various information retrieval techniques [FBY92] and characteristics of open hypertext Web documents to automatically retrieve, filter, and categorize these documents <ref> [CH97, BGMZ97, MS96, WP97, WVS + 96] </ref>. For example, HyPursuit [WVS + 96] uses semantic information embedded in link structures as well as document content to create cluster hierarchies of hypertext documents and structure an information space.
Reference: [PMB96] <author> Michael Pazzani, Jack Muramatsu, and Daniel Billsus. Syskill & Webert: </author> <title> Identifying interesting Web sites. </title> <booktitle> In National Conference on Artificial Intelligence, </booktitle> <pages> pages 54-61, </pages> <month> August </month> <year> 1996. </year> <note> http://www.ics.uci.edu/ pazzani/RTF/AAAI.html. </note>
Reference-contexts: Pattern recognition methods and word clustering using the Hartigan's K-means partitional clustering algorithm are used in [WP97] to discover salient HTML document features (words) that can be used in finding similar HTML documents on the Web. Syskill & Webert <ref> [PMB96] </ref> represents an HTML page with a Boolean feature vector, and then uses naive Bayesian classification to find web pages that are similar, but for only a given single user profile.
Reference: [Por80] <author> M. F. Porter. </author> <title> An algorithm for suffix stripping. </title> <booktitle> Program, </booktitle> <volume> 14(3) </volume> <pages> 130-137, </pages> <year> 1980. </year>
Reference-contexts: This ensures a stable data sample since some pages are fairly dynamic in content. Table 1: Setup of experiments. The word lists from all documents were filtered with a stop-list and "stemmed" using Porter's suffix-stripping algorithm <ref> [Por80] </ref> as implemented by [Fra92]. We derived seven experiments and clustered the documents using the four algorithms described earlier. Our objective is to reduce the dimensionality of the clustering problem and retain the important - 5 - features of the documents.
Reference: [SM83] <author> Gerard Salton and Michael J. McGill. </author> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill, </publisher> <year> 1983. </year>
Reference-contexts: Some words are more frequent in a document than other words. Simple frequency of the occurrence of words is not adequate, as some documents are larger than others. Furthermore, some words may occur frequently across documents. Techniques such as TFIDF <ref> [SM83] </ref> have been proposed precisely to deal with some of these problems. Second, the number of all the words in all the documents can be very large. Distance-based schemes generally require the calculation of the mean of document clusters. <p> The values of the components of the N -vector are computed using the traditional TFIDF method <ref> [SM83] </ref>. The algorithm proceeds by cutting the entire space of documents with a hyperplane passing through the overall arithmetic mean of the documents.
Reference: [TSM85] <author> D.M. Titterington, A.F.M. Smith, and U.E. Makov. </author> <title> Statistical Analysis of Finite Mixture Distributions. </title> <publisher> John Wiley & Sons, </publisher> <year> 1985. </year>
Reference-contexts: For our evaluation, we compare these algorithms to two well-known methods: Bayesian classification as used by AutoClass [CS96] and hierarchical agglomeration clustering (HAC) based on the use of a distance function [DH73]. - 2 - AutoClass is based on the probabilistic mixture modeling <ref> [TSM85] </ref>, and given a data set it finds maximum parameter values for a specific probability distribution functions of the clusters. The clustering results provide the full description of each cluster in terms of probability distribution of each attributes.
Reference: [WP97] <author> Marilyn R. Wulfekuhler and William F. Punch. </author> <title> Finding salient features for personal Web page categories. </title> <booktitle> In 6th International World Wide Web Conference, </booktitle> <month> April </month> <year> 1997. </year> <note> http://proceedings.www6conf.org/HyperNews/get/PAPER118.html. </note>
Reference-contexts: Table 2 also specifies these sub-categories for each cluster. 4 Related Work A number of Web agents use various information retrieval techniques [FBY92] and characteristics of open hypertext Web documents to automatically retrieve, filter, and categorize these documents <ref> [CH97, BGMZ97, MS96, WP97, WVS + 96] </ref>. For example, HyPursuit [WVS + 96] uses semantic information embedded in link structures as well as document content to create cluster hierarchies of hypertext documents and structure an information space. <p> For example, HyPursuit [WVS + 96] uses semantic information embedded in link structures as well as document content to create cluster hierarchies of hypertext documents and structure an information space. Pattern recognition methods and word clustering using the Hartigan's K-means partitional clustering algorithm are used in <ref> [WP97] </ref> to discover salient HTML document features (words) that can be used in finding similar HTML documents on the Web.
Reference: [WVS + 96] <author> Ron Weiss, Bienvenido Velez, Mark A. Sheldon, Chanathip Nemprempre, Peter Szilagyi, An-drzej Duda, and David K. Gifford. Hypursuit: </author> <title> A hierarchical network search engine that exploits content-link hypertext clustering. </title> <booktitle> In Seventh ACM Conference on Hypertext, </booktitle> <month> March </month> <year> 1996. </year> <note> http://paris.lcs.mit.edu/ rweiss/. - 10 </note> - 
Reference-contexts: Table 2 also specifies these sub-categories for each cluster. 4 Related Work A number of Web agents use various information retrieval techniques [FBY92] and characteristics of open hypertext Web documents to automatically retrieve, filter, and categorize these documents <ref> [CH97, BGMZ97, MS96, WP97, WVS + 96] </ref>. For example, HyPursuit [WVS + 96] uses semantic information embedded in link structures as well as document content to create cluster hierarchies of hypertext documents and structure an information space. <p> Table 2 also specifies these sub-categories for each cluster. 4 Related Work A number of Web agents use various information retrieval techniques [FBY92] and characteristics of open hypertext Web documents to automatically retrieve, filter, and categorize these documents [CH97, BGMZ97, MS96, WP97, WVS + 96]. For example, HyPursuit <ref> [WVS + 96] </ref> uses semantic information embedded in link structures as well as document content to create cluster hierarchies of hypertext documents and structure an information space.
References-found: 22

