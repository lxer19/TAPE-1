URL: ftp://ftp.cc.gatech.edu/pub/people/cga/partigame.ps.gz
Refering-URL: http://www.cs.gatech.edu/fac/Chris.Atkeson/publications.html
Root-URL: 
Email: awm@cs.cmu.edu  cga@cc.gatech.edu  
Title: The Parti-game Algorithm for Variable Resolution Reinforcement Learning in Multidimensional State-spaces  
Author: ANDREW W. MOORE CHRISTOPHER G. ATKESON 
Keyword: Reinforcement Learning, Curse of Dimensionality, Learning Control, Robotics, kd-trees  
Address: Pittsburgh, PA 15213  Atlanta, GA 30332  
Affiliation: School of Computer Science, Carnegie Mellon University,  Georgia Institute of Technology, College of Computing,  
Note: Machine Learning, 1-36 (To appear) c To appear Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Abstract: Parti-game is a new algorithm for learning feasible trajectories to goal regions in high dimensional continuous state-spaces. In high dimensions it is essential that learning does not plan uniformly over a state-space. Parti-game maintains a decision-tree partitioning of state-space and applies techniques from game-theory and computational geometry to efficiently and adaptively concentrate high resolution only on critical areas. The current version of the algorithm is designed to find feasible paths or trajectories to goal regions in high dimensional spaces. Future versions will be designed to find a solution that optimizes a real-valued criterion. Many simulated problems have been tested, ranging from two-dimensional to nine-dimensional state-spaces, including mazes, path planning, non-linear dynamics, and planar snake robots in restricted spaces. In all cases, a good solution is found in less than ten trials and a few minutes. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> M. Akian, J.P. Chancelier, and J.P. Quadrat. </author> <title> Dynamic Programming Complexity and Application. </title> <booktitle> In Proceedings of the 27th Conference on Decision and Control, </booktitle> <address> Austin, Texas, </address> <year> 1988. </year>
Reference-contexts: These approaches subdivide a cell by splitting in all dimensions simultaneously. [2] describe adaptive triangulation approaches. Multigrid approaches have been used for dynamic programming in solving for the value function specified by Bellman's equation [11], <ref> [1] </ref>, [9]. As with the robot motion planning approaches described above, it is not yet clear 28 A.W. MOORE AND C.G. ATKESON in the cell.
Reference: 2. <author> A.S. Arcilla, J. Hauser, P.R. Eiseman, and J.F. Thompson. </author> <title> Numerical Grid Generation in Computational Fluid Dynamics and Related Fields. </title> <publisher> North-Holland, </publisher> <year> 1991. </year>
Reference-contexts: Adaptive multigrid methods that allow variations in resolution across the space typically use quad-tree or oct-tree data structures [18]. These approaches subdivide a cell by splitting in all dimensions simultaneously. <ref> [2] </ref> describe adaptive triangulation approaches. Multigrid approaches have been used for dynamic programming in solving for the value function specified by Bellman's equation [11], [1], [9]. As with the robot motion planning approaches described above, it is not yet clear 28 A.W. MOORE AND C.G. ATKESON in the cell.
Reference: 3. <author> A. G. Barto, S. J. Bradtke, and S. P. Singh. </author> <title> Real-time Learning and Control using Asynchronous Dynamic Programming. </title> <journal> AI Journal, </journal> <note> to appear (also published as UMass Amherst Technical Report 91-57 in 1991), </note> <year> 1994. </year>
Reference-contexts: 1. Reinforcement Learning Reinforcement learning [19], [27], [29], <ref> [3] </ref> is a promising method for robots to program and improve themselves. This paper addresses one of reinforcement learning's biggest stumbling blocks: the curse of dimensionality [5], in which costs increase exponentially with the number of state variables. <p> A more common formulation, especially in discrete spaces, is the optimization of the expected long term sum of state dependent rewards. The state transitions are typically stochastic. This more conventional reinforcement learning formalism is described in many references such as [4], [29], [13], <ref> [3] </ref>, [21]. Parti-game is restricted to a smaller class of tasks, but within that class is designed to attack reinforcement learning problems of much higher dimensionality than previous algorithms. It is hoped that future versions of parti-game will be applicable to stochastic systems and arbitrary reward functions. <p> Techniques like this are described in [28], [23], [21], <ref> [3] </ref>.
Reference: 4. <author> A. G. Barto, R. S. Sutton, and C. W. Anderson. </author> <title> Neuronlike Adaptive elements that that can learn difficult Control Problems. </title> <journal> IEEE Trans. on Systems Man and Cybernetics, </journal> <volume> 13(5) </volume> <pages> 835-846, </pages> <year> 1983. </year>
Reference-contexts: The obvious way to transform such state-spaces into discrete problems involves quantizing them: partitioning the state-space into a multidimensional grid, and treating each box within the grid as an atomic object. Although this can be effective (see, for instance, the pole balancing experiments of [19], <ref> [4] </ref>), the naive grid approach has a number of dangers which will be detailed in this paper. 2 A.W. MOORE AND C.G. ATKESON This paper studies the pitfalls of discretization during reinforcement learning and then introduces the parti-game algorithm. <p> A more common formulation, especially in discrete spaces, is the optimization of the expected long term sum of state dependent rewards. The state transitions are typically stochastic. This more conventional reinforcement learning formalism is described in many references such as <ref> [4] </ref>, [29], [13], [3], [21]. Parti-game is restricted to a smaller class of tasks, but within that class is designed to attack reinforcement learning problems of much higher dimensionality than previous algorithms. It is hoped that future versions of parti-game will be applicable to stochastic systems and arbitrary reward functions.
Reference: 5. <author> R. E. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1957. </year>
Reference-contexts: 1. Reinforcement Learning Reinforcement learning [19], [27], [29], [3] is a promising method for robots to program and improve themselves. This paper addresses one of reinforcement learning's biggest stumbling blocks: the curse of dimensionality <ref> [5] </ref>, in which costs increase exponentially with the number of state variables. These costs include both the computational effort required for planning and the physical amount of data that the control system must gather. <p> Depending upon the application other sticking detectors are possible, such as an obstacle sensor on a mobile robot. Algorithm (1) works by constructing a discrete, deterministic Markov decision task (MDT) <ref> [5] </ref>, [6] in which the discrete MDT states correspond to cells. Actions correspond to neighbors thus: action k in cell i corresponds to starting at the center of cell i and greedily aiming at the center of cell k. <p> i, denoted by J SP (i), is determined by solving the set of equations: J SP (i) = &lt; 0 if i = GOAL 1 + k 2 NEIGHS (i) J SP (NEXT (i; k)) Otherwise (3) The equations are solved by a shortest-path method such as dynamic program ming <ref> [5] </ref>, [6] or Dijkstra's algorithm [15]. 3 The following policy is returned: Always aim for the neighbor with the lowest J SP . THE PARTI-GAME ALGORITHM 5 that they will enter cell 6 if we aim for the center of 6. Thus p 6 56 = 0:65.
Reference: 6. <author> D. P. Bertsekas and J. N. Tsitsiklis. </author> <title> Parallel and Distributed Computation. </title> <publisher> Prentice Hall, </publisher> <year> 1989. </year>
Reference-contexts: Depending upon the application other sticking detectors are possible, such as an obstacle sensor on a mobile robot. Algorithm (1) works by constructing a discrete, deterministic Markov decision task (MDT) [5], <ref> [6] </ref> in which the discrete MDT states correspond to cells. Actions correspond to neighbors thus: action k in cell i corresponds to starting at the center of cell i and greedily aiming at the center of cell k. <p> denoted by J SP (i), is determined by solving the set of equations: J SP (i) = &lt; 0 if i = GOAL 1 + k 2 NEIGHS (i) J SP (NEXT (i; k)) Otherwise (3) The equations are solved by a shortest-path method such as dynamic program ming [5], <ref> [6] </ref> or Dijkstra's algorithm [15]. 3 The following policy is returned: Always aim for the neighbor with the lowest J SP . THE PARTI-GAME ALGORITHM 5 that they will enter cell 6 if we aim for the center of 6. Thus p 6 56 = 0:65.
Reference: 7. <author> R. A. Brooks and T. Lozano-Perez. </author> <title> A Subdivision Algorithm in Configuration Space for Findpath with rotation. </title> <booktitle> In Proceedings of the 8th International Conference on Artifical Intelligence, </booktitle> <year> 1983. </year>
Reference-contexts: The price parti-game pays is that it is limited to geometric abstractions, whereas both Kaelbling's and Dayan's methods may eventually be applicable to other abstraction hierarchies. Geometric Decompositions have also been used fairly extensively in Robot Motion Planning (e.g. <ref> [7] </ref>, [14]), summarized in [17]. The principal difference is that the Robot Motion Planning methods all assume that a model of the environment (typically in the form of a pre-programmed list of polygons) is supplied to the system in advance so that there is no learning or exploration capability. <p> The principal difference is that the Robot Motion Planning methods all assume that a model of the environment (typically in the form of a pre-programmed list of polygons) is supplied to the system in advance so that there is no learning or exploration capability. The experiments in <ref> [7] </ref> involve a 3-degree-of-freedom navigation problem and in [14], a fairly difficult 2-dimensional maze. Finally, some relation can be seen between parti-game and adaptive multigrid methods used to accelerate the convergence of solutions to partial differential equations.
Reference: 8. <author> D. Chapman and L. P. Kaelbling. </author> <title> Learning from Delayed Reinforcement In a Complex Domain. </title> <type> Technical Report, </type> <institution> Teleos Research, </institution> <year> 1991. </year>
Reference-contexts: MOORE AND C.G. ATKESON This paper studies the pitfalls of discretization during reinforcement learning and then introduces the parti-game algorithm. Some earlier work [26], [20], <ref> [8] </ref>, [10] considered recursively partitioning state-space while learning from delayed rewards. <p> This enabled conventional dynamic programming to be performed in real-valued multivariate state-spaces where straightforward discretization would fall prey to the curse of dimensionality. This is another approach to partitioning state-space but has the drawback that, unlike parti-game, it requires a guess at an initially valid trajectory through state-space. <ref> [8] </ref> proposed an interesting algorithm, which used more sophisticated statistics to decide which attributes to split. Their objectives were very hard because they wished to avoid remembering transitions between cells and they did not assume continuous paths through state-space, and so they obtained only limited empirical success. <p> If the randomness is something that occasionally teleports the system to a random place (breaking the assumption of paths being continuous through state-space), then parti-game would probably need an entirely different splitting criterion. One possibility is a version of the "G" splitting rule of <ref> [8] </ref>. 6.8. The curse of dimensionality We finish by noting a promising sign involving a series of snake robot experiments with different numbers of links (but fixed total length).
Reference: 9. <author> C. S. Chow. </author> <title> Multigrid algorithms and complexity results for discrete-time stochastic control and related fixed-point problems. </title> <type> Technical report, </type> <institution> M.I.T. Laboratory for Information and Decision Sciences, </institution> <year> 1990. </year>
Reference-contexts: These approaches subdivide a cell by splitting in all dimensions simultaneously. [2] describe adaptive triangulation approaches. Multigrid approaches have been used for dynamic programming in solving for the value function specified by Bellman's equation [11], [1], <ref> [9] </ref>. As with the robot motion planning approaches described above, it is not yet clear 28 A.W. MOORE AND C.G. ATKESON in the cell. Only one succeeded. how multigrid approaches can be adapted to handle the learning problem in which the system dynamics are not completely known in advance. 6.
Reference: 10. <author> P. Dayan and G. E. Hinton. </author> <title> Feudal Reinforcement Learning. </title> <editor> In S. J. Hanson, J. D Cowan, and C. L. Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year> <note> 36 A.W. MOORE AND C.G. ATKESON </note>
Reference-contexts: MOORE AND C.G. ATKESON This paper studies the pitfalls of discretization during reinforcement learning and then introduces the parti-game algorithm. Some earlier work [26], [20], [8], <ref> [10] </ref> considered recursively partitioning state-space while learning from delayed rewards. <p> Their objectives were very hard because they wished to avoid remembering transitions between cells and they did not assume continuous paths through state-space, and so they obtained only limited empirical success. In <ref> [10] </ref> a 2-dimensional hierarchical partitioning was used on a grid with 64 discrete squares, and [12] gives another hierarchical algorithm. These references both attempt a different goal than parti-game: they try to accelerate Q-learning [29] by providing it with a pre-programmed abstraction of the world.
Reference: 11. <author> R. H. W. Hoppe. </author> <title> Multi-Grid Methods for Hamilton-Jacobi-Bellman Equations. </title> <journal> Numerical Mathematics, </journal> <volume> 49, </volume> <year> 1986. </year>
Reference-contexts: These approaches subdivide a cell by splitting in all dimensions simultaneously. [2] describe adaptive triangulation approaches. Multigrid approaches have been used for dynamic programming in solving for the value function specified by Bellman's equation <ref> [11] </ref>, [1], [9]. As with the robot motion planning approaches described above, it is not yet clear 28 A.W. MOORE AND C.G. ATKESON in the cell.
Reference: 12. <author> L. Kaelbling. </author> <title> Hierarchicial Learning in Stochastic Domains: Preliminary Results. </title> <booktitle> In Machine Learning: Proceedings of the Tenth International Workshop. </booktitle> <publisher> Morgan Kaufmann, </publisher> <month> June </month> <year> 1993. </year>
Reference-contexts: Their objectives were very hard because they wished to avoid remembering transitions between cells and they did not assume continuous paths through state-space, and so they obtained only limited empirical success. In [10] a 2-dimensional hierarchical partitioning was used on a grid with 64 discrete squares, and <ref> [12] </ref> gives another hierarchical algorithm. These references both attempt a different goal than parti-game: they try to accelerate Q-learning [29] by providing it with a pre-programmed abstraction of the world.
Reference: 13. <author> L. P. Kaelbling. </author> <title> Learning in Embedded Systems. </title> <type> PhD. Thesis; Technical Report No. </type> <institution> TR-90-04, Stanford University, Department of Computer Science, </institution> <month> June </month> <year> 1990. </year>
Reference-contexts: A more common formulation, especially in discrete spaces, is the optimization of the expected long term sum of state dependent rewards. The state transitions are typically stochastic. This more conventional reinforcement learning formalism is described in many references such as [4], [29], <ref> [13] </ref>, [3], [21]. Parti-game is restricted to a smaller class of tasks, but within that class is designed to attack reinforcement learning problems of much higher dimensionality than previous algorithms. It is hoped that future versions of parti-game will be applicable to stochastic systems and arbitrary reward functions. <p> MOORE AND C.G. ATKESON THE PARTI-GAME ALGORITHM 17 18 A.W. MOORE AND C.G. ATKESON In the first experiments, the discrete algorithms were not told the location of the goal. The exploration mechanism was "optimism in the face of uncertainty" <ref> [13] </ref>, [28] in which any unvisited state-action pair was assumed to be zero steps from the goal. In a deterministic problem, this strategy is guaranteed to find the optimal path [16]. The deterministic assumption also permitted Q-learning to use a learning rate ff = 1.
Reference: 14. <author> Subbarao Kambhampati and Larry S. Davis. </author> <title> Multiresolution Path Planning for Mobile Robots. </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> Vol. RA-2, No. 3, 2(3), </volume> <year> 1986. </year>
Reference-contexts: The price parti-game pays is that it is limited to geometric abstractions, whereas both Kaelbling's and Dayan's methods may eventually be applicable to other abstraction hierarchies. Geometric Decompositions have also been used fairly extensively in Robot Motion Planning (e.g. [7], <ref> [14] </ref>), summarized in [17]. The principal difference is that the Robot Motion Planning methods all assume that a model of the environment (typically in the form of a pre-programmed list of polygons) is supplied to the system in advance so that there is no learning or exploration capability. <p> The experiments in [7] involve a 3-degree-of-freedom navigation problem and in <ref> [14] </ref>, a fairly difficult 2-dimensional maze. Finally, some relation can be seen between parti-game and adaptive multigrid methods used to accelerate the convergence of solutions to partial differential equations. Adaptive multigrid methods that allow variations in resolution across the space typically use quad-tree or oct-tree data structures [18].
Reference: 15. <author> D. E. Knuth. </author> <title> Sorting and Searching. </title> <publisher> Addison Wesley, </publisher> <year> 1973. </year>
Reference-contexts: (i), is determined by solving the set of equations: J SP (i) = &lt; 0 if i = GOAL 1 + k 2 NEIGHS (i) J SP (NEXT (i; k)) Otherwise (3) The equations are solved by a shortest-path method such as dynamic program ming [5], [6] or Dijkstra's algorithm <ref> [15] </ref>. 3 The following policy is returned: Always aim for the neighbor with the lowest J SP . THE PARTI-GAME ALGORITHM 5 that they will enter cell 6 if we aim for the center of 6. Thus p 6 56 = 0:65. This simple algorithm has immediate drawbacks. <p> Call such a cell a losing cell. 2 The following policy is returned: Always aim for the neighbor with the lowest J W C () value. The J W C (:) function can be computed by a standard minimax algorithm <ref> [15] </ref>, which is in turn closely related to deterministic dynamic programming algorithms.
Reference: 16. <author> S. Koenig and R.G. Simmons. </author> <title> Complexity Analysis of Reinforcement Learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Artificial Intelligence (AAAI-93). </booktitle> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: The exploration mechanism was "optimism in the face of uncertainty" [13], [28] in which any unvisited state-action pair was assumed to be zero steps from the goal. In a deterministic problem, this strategy is guaranteed to find the optimal path <ref> [16] </ref>. The deterministic assumption also permitted Q-learning to use a learning rate ff = 1. Prioritized sweeping was allowed 200 backups per transition. The results are shown in Table 1. Parti-game has considerably fewer steps for exploration and fewer backups than either of the other algorithms.
Reference: 17. <author> J. Latombe. </author> <title> Robot Motion Planning. </title> <publisher> Kluwer, </publisher> <year> 1991. </year>
Reference-contexts: The price parti-game pays is that it is limited to geometric abstractions, whereas both Kaelbling's and Dayan's methods may eventually be applicable to other abstraction hierarchies. Geometric Decompositions have also been used fairly extensively in Robot Motion Planning (e.g. [7], [14]), summarized in <ref> [17] </ref>. The principal difference is that the Robot Motion Planning methods all assume that a model of the environment (typically in the form of a pre-programmed list of polygons) is supplied to the system in advance so that there is no learning or exploration capability.
Reference: 18. <author> S. F. McCormick. </author> <title> Multilevel Adaptive Methods for Partial Differential Equations. </title> <publisher> SIAM, </publisher> <year> 1989. </year>
Reference-contexts: Finally, some relation can be seen between parti-game and adaptive multigrid methods used to accelerate the convergence of solutions to partial differential equations. Adaptive multigrid methods that allow variations in resolution across the space typically use quad-tree or oct-tree data structures <ref> [18] </ref>. These approaches subdivide a cell by splitting in all dimensions simultaneously. [2] describe adaptive triangulation approaches. Multigrid approaches have been used for dynamic programming in solving for the value function specified by Bellman's equation [11], [1], [9].
Reference: 19. <author> D. Michie and R. A. Chambers. </author> <title> BOXES: An Experiment in Adaptive Control. </title> <editor> In E. Dale and D. Michie, editors, </editor> <booktitle> Machine Intelligence 2. </booktitle> <publisher> Oliver and Boyd, </publisher> <year> 1968. </year>
Reference-contexts: 1. Reinforcement Learning Reinforcement learning <ref> [19] </ref>, [27], [29], [3] is a promising method for robots to program and improve themselves. This paper addresses one of reinforcement learning's biggest stumbling blocks: the curse of dimensionality [5], in which costs increase exponentially with the number of state variables. <p> The obvious way to transform such state-spaces into discrete problems involves quantizing them: partitioning the state-space into a multidimensional grid, and treating each box within the grid as an atomic object. Although this can be effective (see, for instance, the pole balancing experiments of <ref> [19] </ref>, [4]), the naive grid approach has a number of dangers which will be detailed in this paper. 2 A.W. MOORE AND C.G. ATKESON This paper studies the pitfalls of discretization during reinforcement learning and then introduces the parti-game algorithm.
Reference: 20. <author> A. W. Moore. </author> <title> Variable Resolution Dynamic Programming: Efficiently Learning Action Maps in Multivariate Real-valued State-spaces. </title> <editor> In L. Birnbaum and G. Collins, editors, </editor> <booktitle> Machine Learning: Proceedings of the Eighth International Workshop. </booktitle> <publisher> Morgan Kaufmann, </publisher> <month> June </month> <year> 1991. </year>
Reference-contexts: MOORE AND C.G. ATKESON This paper studies the pitfalls of discretization during reinforcement learning and then introduces the parti-game algorithm. Some earlier work [26], <ref> [20] </ref>, [8], [10] considered recursively partitioning state-space while learning from delayed rewards. <p> An early example was [26] who applied it to 3-degree-of-freedom force control. Their method gradually learned by recording cumulative statistics of performance in cells. More recently, we produced a variable resolution dynamic programming method <ref> [20] </ref>. This enabled conventional dynamic programming to be performed in real-valued multivariate state-spaces where straightforward discretization would fall prey to the curse of dimensionality. <p> Christopher Atkeson acknowledges support provided under Air Force Office of Scientific Research grant F49-6209410362, by the ATR Human Information Processing Research Laboratories, and by a National Science Foundation Presidential Young Investigator Award. Notes 1. Another method is to increase the resolution along the trajectory <ref> [20] </ref>. 2. In practice, parti-game's solution was shorter than the optimal path in the discretized maze. This is because the continuous actions of parti-game permit travel in arbitrary directions, not just North, East, South and West. 3.
Reference: 21. <author> A. W. Moore and C. G. Atkeson. </author> <title> Prioritized Sweeping: Reinforcement Learning with Less Data and Less Real Time. </title> <journal> Machine Learning, </journal> <volume> 13, </volume> <year> 1993. </year>
Reference-contexts: A more common formulation, especially in discrete spaces, is the optimization of the expected long term sum of state dependent rewards. The state transitions are typically stochastic. This more conventional reinforcement learning formalism is described in many references such as [4], [29], [13], [3], <ref> [21] </ref>. Parti-game is restricted to a smaller class of tasks, but within that class is designed to attack reinforcement learning problems of much higher dimensionality than previous algorithms. It is hoped that future versions of parti-game will be applicable to stochastic systems and arbitrary reward functions. <p> Techniques like this are described in [28], [23], <ref> [21] </ref>, [3]. <p> Comparison with other Algorithms In an attempt to compare the parti-game algorithm with other reinforcement learning methods, we discretized the maze of Figure 9 and applied two discrete learning algorithms: Q-learning [29] and prioritized sweeping <ref> [21] </ref>, [23]. The discretization, shown in Figure 14, was at the coarsest resolution (50 fi 50) which still permitted a path from start to goal.
Reference: 22. <author> N. J. Nilsson. </author> <booktitle> Problem-solving Methods in Artificial Intelligence. </booktitle> <publisher> McGraw Hill, </publisher> <year> 1971. </year>
Reference-contexts: For systems which can neither be characterized as geometric motion planning problems nor dynamics problems, it is also possible that optimality might be provable. Future research into this might incorporate admissible heuristics: a classical method in AI for formally reasoning about the optimality of proposed solutions <ref> [22] </ref>. 6.6. Multiple goals Because it builds an explicit model of all the possible state transitions between cells, it is a trivial matter for parti-game to change to a new goal. We have performed a number of experiments (not reported here) that confirm this. 6.7.
Reference: 23. <author> J. Peng and R. J. Williams. </author> <title> Efficient Learning and Planning Within the Dyna Framework. </title> <booktitle> In Proceedings of the Second International Conference on Simulation of Adaptive Behavior. </booktitle> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: If real time constraints do not permit full recomputation of J W C after an outcome set has changed, then the J W C updates can take place incrementally in a series of finite time intervals interleaved with real time control decisions. Techniques like this are described in [28], <ref> [23] </ref>, [21], [3]. <p> Comparison with other Algorithms In an attempt to compare the parti-game algorithm with other reinforcement learning methods, we discretized the maze of Figure 9 and applied two discrete learning algorithms: Q-learning [29] and prioritized sweeping [21], <ref> [23] </ref>. The discretization, shown in Figure 14, was at the coarsest resolution (50 fi 50) which still permitted a path from start to goal.
Reference: 24. <author> A. P. Sage and C. C. White. </author> <title> Optimum Systems Control. </title> <publisher> Prentice Hall, </publisher> <year> 1977. </year>
Reference-contexts: In the case of navigation problems, a proof would involve geometric assumptions. In dynamics problems a proof might need to assume local linearizability of the dynamics within cells, and could use then use Linear Quadratic Gaussian (LQG) local control design (see, for example, <ref> [24] </ref>). For systems which can neither be characterized as geometric motion planning problems nor dynamics problems, it is also possible that optimality might be provable. Future research into this might incorporate admissible heuristics: a classical method in AI for formally reasoning about the optimality of proposed solutions [22]. 6.6.
Reference: 25. <author> S. Schaal and C. G. Atkeson. </author> <title> Assessing the Quality of Local Linear Models. </title> <booktitle> In Advances in Neural Information Processing Systems 6. </booktitle> <publisher> Morgan Kaufmann, </publisher> <month> April </month> <year> 1994. </year>
Reference-contexts: This can be done with relative ease, both in a statistical and computational sense. It is particularly easy given our working assumption of deterministic system dynamics, but even in stochastic cases, developing a local linear model from data may not be hard <ref> [25] </ref>. 6.4. Dealing with an unknown goal state There is no difficulty for parti-game in removing the assumption that the location of the goal state is known. Convergence will be considerably slowed down if it is not given, but this is not the fault of the algorithm.
Reference: 26. <author> J. Simons, H. Van Brussel, J. De Schutter, and J. Verhaert. </author> <title> A Self-Learning Automaton with Variable Resolution for High Precision Assembly by Industrial Robots. </title> <journal> IEEE Trans. on Automatic Control, </journal> <volume> 27(5) </volume> <pages> 1109-1113, </pages> <month> October </month> <year> 1982. </year>
Reference-contexts: MOORE AND C.G. ATKESON This paper studies the pitfalls of discretization during reinforcement learning and then introduces the parti-game algorithm. Some earlier work <ref> [26] </ref>, [20], [8], [10] considered recursively partitioning state-space while learning from delayed rewards. <p> Related work A few other researchers in Reinforcement Learning have attempted to overcome dimensionality problems by decompositions of state-space. An early example was <ref> [26] </ref> who applied it to 3-degree-of-freedom force control. Their method gradually learned by recording cumulative statistics of performance in cells. More recently, we produced a variable resolution dynamic programming method [20].
Reference: 27. <author> R. S. Sutton. </author> <title> Temporal Credit Assignment in Reinforcement Learning. </title> <type> Phd. thesis, </type> <institution> University of Massachusetts, Amherst, </institution> <year> 1984. </year>
Reference-contexts: 1. Reinforcement Learning Reinforcement learning [19], <ref> [27] </ref>, [29], [3] is a promising method for robots to program and improve themselves. This paper addresses one of reinforcement learning's biggest stumbling blocks: the curse of dimensionality [5], in which costs increase exponentially with the number of state variables.
Reference: 28. <author> R. S. Sutton. </author> <title> Integrated Architecture for Learning, Planning, and Reacting Based on Approximating Dynamic Programming. </title> <booktitle> In Proceedings of the 7th International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: The curse of dimensionality is also a problem in other areas of artificial intelligence, such as planning and supervised learning. Much work has been performed with discrete state-spaces: in particular a class of Markov decision tasks known as grid worlds [29], <ref> [28] </ref>. Most potentially useful applications of reinforcement learning, however, take place in multidimensional continuous state-spaces. The obvious way to transform such state-spaces into discrete problems involves quantizing them: partitioning the state-space into a multidimensional grid, and treating each box within the grid as an atomic object. <p> If real time constraints do not permit full recomputation of J W C after an outcome set has changed, then the J W C updates can take place incrementally in a series of finite time intervals interleaved with real time control decisions. Techniques like this are described in <ref> [28] </ref>, [23], [21], [3]. <p> MOORE AND C.G. ATKESON THE PARTI-GAME ALGORITHM 17 18 A.W. MOORE AND C.G. ATKESON In the first experiments, the discrete algorithms were not told the location of the goal. The exploration mechanism was "optimism in the face of uncertainty" [13], <ref> [28] </ref> in which any unvisited state-action pair was assumed to be zero steps from the goal. In a deterministic problem, this strategy is guaranteed to find the optimal path [16]. The deterministic assumption also permitted Q-learning to use a learning rate ff = 1.
Reference: 29. <author> C. J. C. H. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD. Thesis, </type> <institution> King's College, University of Cambridge, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: 1. Reinforcement Learning Reinforcement learning [19], [27], <ref> [29] </ref>, [3] is a promising method for robots to program and improve themselves. This paper addresses one of reinforcement learning's biggest stumbling blocks: the curse of dimensionality [5], in which costs increase exponentially with the number of state variables. <p> The curse of dimensionality is also a problem in other areas of artificial intelligence, such as planning and supervised learning. Much work has been performed with discrete state-spaces: in particular a class of Markov decision tasks known as grid worlds <ref> [29] </ref>, [28]. Most potentially useful applications of reinforcement learning, however, take place in multidimensional continuous state-spaces. The obvious way to transform such state-spaces into discrete problems involves quantizing them: partitioning the state-space into a multidimensional grid, and treating each box within the grid as an atomic object. <p> A more common formulation, especially in discrete spaces, is the optimization of the expected long term sum of state dependent rewards. The state transitions are typically stochastic. This more conventional reinforcement learning formalism is described in many references such as [4], <ref> [29] </ref>, [13], [3], [21]. Parti-game is restricted to a smaller class of tasks, but within that class is designed to attack reinforcement learning problems of much higher dimensionality than previous algorithms. It is hoped that future versions of parti-game will be applicable to stochastic systems and arbitrary reward functions. <p> Figures 12 and 13 show what happens when we now start the system inside this barrier. 4.1.1. Comparison with other Algorithms In an attempt to compare the parti-game algorithm with other reinforcement learning methods, we discretized the maze of Figure 9 and applied two discrete learning algorithms: Q-learning <ref> [29] </ref> and prioritized sweeping [21], [23]. The discretization, shown in Figure 14, was at the coarsest resolution (50 fi 50) which still permitted a path from start to goal. <p> In [10] a 2-dimensional hierarchical partitioning was used on a grid with 64 discrete squares, and [12] gives another hierarchical algorithm. These references both attempt a different goal than parti-game: they try to accelerate Q-learning <ref> [29] </ref> by providing it with a pre-programmed abstraction of the world. The abstraction, it is noted in both cases, may sometimes indeed lead to faster learning and can improve Q-learning if there are multiple goals in the problem.
References-found: 29

