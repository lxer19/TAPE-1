URL: ftp://ftp.cs.umass.edu/pub/mckinley/weaver.ps.gz
Refering-URL: http://celestial.cs.umass.edu/~mckinley/papers.html
Root-URL: 
Email: weaver@cs.umass.edu  
Phone: (413) 545-1249 (fax)  
Title: Compiler Representations for Heterogeneous Processing  
Author: Glen Weaver 
Date: September 30, 1995  
Address: Amherst, MA 01003-4610  
Affiliation: Department of Computer Science University of Massachusetts  
Note: Dissertation Proposal  
Abstract: The emergence of heterogeneous parallel systems opens the possibility of higher performance for complex, heterogeneous applications. Unfortunately, heterogeneous parallel systems are even more complex to program than homogeneous parallel systems. Programmers should not have to handle all the added complexity of these systems. Instead, compilers should be extended to automatically handle as much of this complexity as possible. In previous work [50, 48], we argue that a compiler for heterogeneous systems requires a substantially more flexible software architecture than found in current compilers. Achieving this flexibility requires changes in the compiler's intermediate representation (IR). This proposal presents an intermediate representation, Score, designed to support heterogeneity. We identify four capabilities than an IR for heterogeneous systems should support (i.e., reordering of transformations, representing of multiple models of parallelism, extending to new models of parallelism, and reusing transformations for different models of parallelism). We survey existing intermediate representations with respect to their support for these capabilities. We then present our IR design which adapts ideas found in our survey section to suit the needs of heterogeneity. Score's adaptations for heterogeneity support several existing models of parallelism, facilitate extension to new models of parallelism, and allow low and high-level transformations to be interleaved. Finally, we describe our approach for evaluating Score.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, R. Sethi, and J. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <note> second edition, </note> <year> 1986. </year>
Reference-contexts: For each IR, we discuss the goals of the system in which the IR is used (if applicable), the design goals of the IR itself, and its support for the IR requirements listed in Table 1. Several well-known representation techniques occur repeatedly in our survey. Abstract syntax trees (ASTs) <ref> [1] </ref> represent a program by treating source constructs as operators and operands, and organizing these nodes into trees where the root is generally a procedure or an entire program. ASTs are popular in source-to-source translators. Scalar compilers, on the other hand, often use a control flow graph (CFG) [1] as their <p> trees (ASTs) <ref> [1] </ref> represent a program by treating source constructs as operators and operands, and organizing these nodes into trees where the root is generally a procedure or an entire program. ASTs are popular in source-to-source translators. Scalar compilers, on the other hand, often use a control flow graph (CFG) [1] as their primary representation. CFGs represent programs as a graph of basic blocks. Edges in a CFG represent control flow (e.g., decisions, loops, jumps), and basic blocks contain sequentially executed code. Program dependence graphs (PDG) [32] connect nodes together with control dependence edges and data dependence edges. <p> Each AST node represents an operation, and the node's children represent the operation's operands <ref> [1, 47] </ref>. The type of an AST node corresponds to a construct from the source language such as a minus operation, while loop, or variable declaration. ASTs are also used in other types of systems such as compiler toolkits, software engineering environments, and distribution formats.
Reference: [2] <author> Alfred V. Aho, Mahadevan Ganapathi, and Steven W. K. Tjiang. </author> <title> Code generation using tree matching and dynamic programming. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 11(4) </volume> <pages> 491-516, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Instead, transformations need to know the data and control dependences between each IR node. For ASTs, this dependence information has traditionally been obtained through analysis and either woven into the AST or stored in a separate structure. * Though some research has investigated generating code from trees <ref> [2, 19, 4, 39] </ref>, ASTs are not adequate for low-level optimizations. ASTs do not capture the features of the target hardware (e.g., the instruction pipeline and register set). SUIF and ANDF attack this problem in two separate ways.
Reference: [3] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison Wesley, </publisher> <address> Reading, MAf, 1 edition, </address> <year> 1986. </year>
Reference-contexts: Treating them as identical ensures proper program behavior when variables are aliased but diminishes the precision of the SSA graph when the alias relationship is unwarranted. Alias analysis tries to disprove that variables are aliases and thereby improve the precision of the representation <ref> [3] </ref>. Specializing Phi Nodes Alpern et al. [5] propose a modest change to SSA to support detecting equality of variables. Their algorithm for detecting variables with equal values is similar to value numbering but works across control structures such as conditionals and loops.
Reference: [4] <author> Philippe Aigrain, Susan L. Graham, Robert R. Henry, Marshall Kirk McKusick, and Eduardo Pelegri'-Llopart. </author> <title> Experience with a graham-glanville style code generator. </title> <booktitle> In Proceedings of the SIGPLAN '84 Symposium on Compiler Construction, </booktitle> <pages> pages 13-24, </pages> <address> Montreal, Canada, </address> <month> June </month> <year> 1984. </year>
Reference-contexts: Instead, transformations need to know the data and control dependences between each IR node. For ASTs, this dependence information has traditionally been obtained through analysis and either woven into the AST or stored in a separate structure. * Though some research has investigated generating code from trees <ref> [2, 19, 4, 39] </ref>, ASTs are not adequate for low-level optimizations. ASTs do not capture the features of the target hardware (e.g., the instruction pipeline and register set). SUIF and ANDF attack this problem in two separate ways.
Reference: [5] <author> B. Alpern, M. Wegman, and K. Zadeck. </author> <title> Detecting equality of variables in programs. </title> <booktitle> In Proceedings of the Fifteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> San Diego, CA, </address> <month> January </month> <year> 1988. </year>
Reference-contexts: Alias analysis tries to disprove that variables are aliases and thereby improve the precision of the representation [3]. Specializing Phi Nodes Alpern et al. <ref> [5] </ref> propose a modest change to SSA to support detecting equality of variables. Their algorithm for detecting variables with equal values is similar to value numbering but works across control structures such as conditionals and loops. The authors define congruence which is a subset of equivalence.
Reference: [6] <author> S. Amarasinghe, J. Anderson, M. Lam, and A. Lim. </author> <title> An overview of a compiler for scalable parallel machines. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: However, these representations do not use inheritance, so transformations must operate on specific node types. SUIF Stanford University Intermediate Format (SUIF) is a compiler framework that can be used as either a source-to-C translator or a native code compiler <ref> [69, 6, 38] </ref> for Fortran 77 and C. SUIF serves as a research tool for investigating automatic parallelization of sequential programs. SUIF's IR (which is also called SUIF) includes both high and low-level nodes to enable code generation.
Reference: [7] <author> David F. Bacon, Susan L. Graham, and Oliver J. Sharp. </author> <title> Compiler transformations for high-performance computing. </title> <journal> ACM Computing Surveys, </journal> <volume> 28(4) </volume> <pages> 345-420, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: For example, a typical transformation is strip mining which tailors a vectorizable loop for a vector register of a specific size. Figure 1 shows a strip mined version of the loop (this example is taken from Bacon et al. <ref> [7] </ref>). The strip mining transformation could be explicitly coded to handle both types of loops and recoded when another type is introduced, but a more extensible approach is for the transformation to be written such that it operates over attributes of a node rather than the node's operation. <p> For low-level optimizations to be applied to SUIF, high-SUIF must first be translated to low-SUIF which prevents further high-level transformations and related analyses. Loops Loops are an important construct for compiler manipulation as evidenced by the plethora of loop transformations <ref> [7] </ref>. Due to the importance of loops, researchers have developed techniques for extracting loops and related information from low-level representations [70]. Given these techniques, our compiler will find loops through analysis rather than through interacting with the compiler's front end.
Reference: [8] <author> Vasanth Bala, Jeanne Ferrante, and Larry Carter. </author> <title> Explicit data placement (XDP): A methodology for explicit compile-time representation and optimization of data movement. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 139-148, </pages> <month> MAY </month> <year> 1993. </year>
Reference-contexts: The authors expect that at some point during compilation the YIL subgraph will be ignored and subsequent optimization will use only the XIL subgraph. Hence, the separation of graphs hinders reordering. Explicit Data Placement Bala et al. <ref> [8] </ref> describe Explicit Data Placement (XDP), which is an IR extension for representing communication. The authors designed XDP to give compilers the power of manipulating both data and ownership transfer operations, with the loosest possible semantics. <p> Though this is the first attempt at building an IR that supports so many diverse models of parallelism, the nodes can largely be borrowed from other systems such as XDP <ref> [8] </ref>, the D System [63], and ParaScope [24]. It is unclear if we need to investigate new nodes for some models of parallelism or simply assemble existing ones.
Reference: [9] <author> R. Ballance, A. Maccabe, and K. Ottenstein. </author> <title> The program dependence web: a repreesentation supporting control-, data- and demand-driven interpretation of imperative languages. </title> <booktitle> In Proceedings of the SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 257-271, </pages> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Switch and merge nodes are inserted symmetrically so that the graph can be traversed in either direction. Merge nodes are equivalent to -nodes in SSA form. Program Dependence Web Ballance et al. combine many of the preceding features into a single representation, the Program Dependence Web (PDW) <ref> [9] </ref>. The PDW consists of SSA form on top of a PDG with modifications to make it interpretable and provide support for data flow parallelism. The PDW is built in three steps. The first step uses the standard algorithm [25] to build SSA form on a PDG. <p> TGSA extends high-level SSA form (i.e., GSA) to unstructured code and removes information from GSA that is unnecessary for symbolic analysis. Supporting unstructured code requires constructing a directed, acyclic graph (DAG) of fl-nodes rather than a tree (as Ballance et al. <ref> [9] </ref> does). Thinning refers to the removal of information the authors do not need. For example, -nodes for which only one argument will ever be selected are eliminated. Discussion SSA form complements rather than competes with other intermediate representations such as ASTs and PDGs.
Reference: [10] <author> W. Baxter and H. R. Bauer, III. </author> <title> The program dependence graph and vectorization. </title> <booktitle> In Proceedings of the Sixteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Austin, TX, </address> <month> January </month> <year> 1989. </year> <month> 23 </month>
Reference-contexts: In contrast, CFGs and ASTs capture constraints that are artifacts of the original program's linear order. PDGs simplify the coding of transformations by connecting computationally relevant parts of the program and capturing the relationships among nodes that are most important to transformations. 6 PDGs and Vectorization Baxter et al. <ref> [10] </ref> demonstrate the effectiveness of PDGs for high-level optimizations by using loop innermosting (the shifting/interchanging of loops with no associated data dependences to the deepest nesting level) and vectorization to vectorize loop nests automatically.
Reference: [11] <author> David A. Berson, Rajiv Gupta, and Mary Lou Soffa. GURRR: </author> <title> A global unified resource requirements represen-tations. </title> <booktitle> In Workshop on Intermediate Representations, </booktitle> <address> San Francisco, CA, </address> <month> January </month> <year> 1995. </year>
Reference-contexts: The PDG's unification of control and data dependence allows a compiler to delay the decision of whether or not to apply if-conversion. Control dependences are also more natural than control flow for transforming conditionals because each control dependence explicitly links statements with their controlling condition. GURRR Berson et al. <ref> [11] </ref> use a PDG as the basis of an IR, Global Unified Resource Requirements Representation (GURRR), that supports reordering of (very) low-level optimizations as well as high level transformations.
Reference: [12] <author> B. Blume, R. Eigenmann, K. Faigin, J. Grout, J. Hoeflinger, D. Padua, P. Peterson, B. Pottenger, L. Rauchwerger, P. Tu, and S. Weatherford. </author> <title> Polaris: The next generation in parallelizing compilers. </title> <booktitle> In Proceedings of the Seventh Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, NY, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Optimistist IRs are those representations that ignore most control or control flow dependences, and instead rely heavily on data dependences to define the relationships between nodes. Section 2.5 includes miscellaneous IRs which do not fit neatly in this progression. 2.1 Abstract Syntax Trees Many parallelizing compilers <ref> [23, 12, 13, 69, 54] </ref> use an abstract syntax tree (AST) because it is simple to build, retains the original structure of the program, and readily supports high-level transformations. Each AST node represents an operation, and the node's children represent the operation's operands [1, 47]. <p> Transformations in ParaScope may be reordered and even applied to sections of a program during interactive editing. Some transformations incrementally update analysis data, but implementing incremental update is solely the responsibility of the individual transformations. Polaris Polaris from the University of Illinois, is an optimizing source-to-source translator <ref> [12, 52, 30] </ref> for Fortran derivatives. The authors have two major goals for Polaris: to automatically parallelize sequential programs for distributed shared memory machines and to be a production quality compiler in terms of reliability and speed.
Reference: [13] <author> F. Bodin, P. Beckman, D. Gannon, J. Gotwals, S. Narayana, S. Srinivas, and B. Winnicka. Sage++: </author> <title> An object-oriented toolkit and class library for building Fortran and C++ restructuring tools. </title> <booktitle> In Second Object-Oriented Numerics Conference, </booktitle> <year> 1994. </year>
Reference-contexts: Optimistist IRs are those representations that ignore most control or control flow dependences, and instead rely heavily on data dependences to define the relationships between nodes. Section 2.5 includes miscellaneous IRs which do not fit neatly in this progression. 2.1 Abstract Syntax Trees Many parallelizing compilers <ref> [23, 12, 13, 69, 54] </ref> use an abstract syntax tree (AST) because it is simple to build, retains the original structure of the program, and readily supports high-level transformations. Each AST node represents an operation, and the node's children represent the operation's operands [1, 47]. <p> Once analysis data has been computed, Polaris does not ensure that the data is still accurate when subsequent transformations access it. Sage++ Sage++ from Indiana University is a toolkit for building source-to-source translators <ref> [13] </ref>. Sage++ seeks to facilitate the construction of source-to-source translators for Fortran and C++ derivatives by packaging common routines such as parsers and data dependence testing.
Reference: [14] <author> P. Briggs. </author> <title> The massively scalar compiler project. </title> <type> Technical report, </type> <institution> Dept. of Computer Science, Rice University, </institution> <note> In Progress. </note>
Reference-contexts: This feature meets our requirement of supporting compilation to multiple models of parallelism. 3.3 Additional Compiler Support Click built his system on top of utilities provided by the Massively Scalar Compiler Project (MSCP) <ref> [15, 14] </ref>. MSCP includes Fortran and C compilers that output ILOC, a low-level intermediate representation. Click's system reads in ILOC, builds his representation internally, performs various optimizations, and outputs ILOC. MSCP also has a machine simulator that executes ILOC and collects statistics.
Reference: [15] <author> P. Briggs and T. Harvey. </author> <title> ILOC '93. </title> <type> Technical Report CRPC-TR93323, </type> <institution> Dept. of Computer Science, Rice University, </institution> <year> 1993. </year>
Reference-contexts: This feature meets our requirement of supporting compilation to multiple models of parallelism. 3.3 Additional Compiler Support Click built his system on top of utilities provided by the Massively Scalar Compiler Project (MSCP) <ref> [15, 14] </ref>. MSCP includes Fortran and C compilers that output ILOC, a low-level intermediate representation. Click's system reads in ILOC, builds his representation internally, performs various optimizations, and outputs ILOC. MSCP also has a machine simulator that executes ILOC and collects statistics.
Reference: [16] <author> James H. Burrill. </author> <title> The class library for the iua: Tutorial. </title> <type> Technical report, </type> <institution> Amerinex Artifical Intelligence, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: We have a simulator <ref> [16] </ref> for the IUA CAAPP [67], which is a SIMD machine. We expect to obtain access to a distributed shared-memory MIMD at one of the national supercomputing centers. We are currently designing a compiler testbed, of which Score will be part.
Reference: [17] <author> Ralph Butler and Ewing Lusk. </author> <title> Monitors, messages, and clusters: the p4 parallel programming system. </title> <journal> Parallel Computing, </journal> <volume> 20(4) </volume> <pages> 547-564, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Section 5 explains the project's timeline, which is actually shown in Appendix A. Finally, we summarize in Section 6. 1.1 Heterogeneous processing Recent product introductions (e.g., Meiko CS-2, IBM SP-2) and research (e.g., IUA [66], PVM [61], p4 <ref> [17] </ref>, and MPI [34]) demonstrate growing interest in heterogeneous parallelism. Previous efforts in parallelism focused on homogeneous parallel machines, which implement a single model of parallelism throughout the machine.
Reference: [18] <author> Brad Calder and Dirk Grunwald. </author> <title> Reducing iindirect function call overhead in c++ programs. </title> <booktitle> In Proceedings of the Twenty-first Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 397-408, </pages> <address> Portland, OR, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: Type information is not shown. and pointer indirections. The indirect call inherent in a method invocation may be optimized by deducing constraints on the actual method called <ref> [29, 18, 26, 31, 53] </ref>. This deduction requires high-level knowledge about the method invocation. Score distinguishes between four different types of calls: direct calls, indirect calls for languages with function pointers, closure calls for languages with nested procedures, and method calls for object oriented languages.
Reference: [19] <author> R. G. G. Cattell. </author> <title> Automatic derivation of code generators from machine descriptions. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 2(2) </volume> <pages> 173-190, </pages> <month> April </month> <year> 1980. </year>
Reference-contexts: Instead, transformations need to know the data and control dependences between each IR node. For ASTs, this dependence information has traditionally been obtained through analysis and either woven into the AST or stored in a separate structure. * Though some research has investigated generating code from trees <ref> [2, 19, 4, 39] </ref>, ASTs are not adequate for low-level optimizations. ASTs do not capture the features of the target hardware (e.g., the instruction pipeline and register set). SUIF and ANDF attack this problem in two separate ways.
Reference: [20] <author> Cliff Click. </author> <title> Combining Analyses, Combining Optimizations. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <year> 1995. </year>
Reference-contexts: The existence of speculatively executed code increases possibilities to combine code, improves flexibility in assigning registers, and creates opportunities for moving the definition and uses of a value closer together. In the CFG Click <ref> [22, 20] </ref> describes a simple SSA form intermediate representation. Click's IR is based on the CFG, but he represents basic blocks by a single node, called a region node. <p> Section 3.1 begins by describing Click's intermediate representation, which is the basis for Score. Section 3.2 presents our extensions to Click's representation that enable Score to support compilation for heterogeneous systems. Finally, Section 3.3 lists other enhancements to Click's IR for handling parallelism. 3.1 Click's Representation Click <ref> [22, 20] </ref> describes an IR with simple nodes appropriate for representing instructions for a uniprocessor RISC machine (e.g., load, store, and conditional branch). His representation uses SSA form on top of a CFG, but each CFG block is explicitly represented by an IR node, called a region node.
Reference: [21] <author> Cliff Click. </author> <title> Global code motion, global value numbering. </title> <booktitle> In ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <volume> volume 30, </volume> <month> June </month> <year> 1995. </year>
Reference-contexts: Our strategy will be to convert the PDG to a CFG [58], generate a legal ordering <ref> [21] </ref>, run analyses, and then convert back to a PDG. 4 Experiments 4.1 Infrastructure Our tests will include four models of parallelism: uniprocessor, SIMD, message passing MIMD, and distributed shared-memory MIMD. We have a simulator [16] for the IUA CAAPP [67], which is a SIMD machine.
Reference: [22] <author> Cliff Click and Michael Paleczny. </author> <title> A simple graph-based intermediate representation. </title> <booktitle> In ACM SIGPLAN Workshop on Intermediate Representations, </booktitle> <address> San Francisco, CA, </address> <month> January </month> <year> 1995. </year>
Reference-contexts: The existence of speculatively executed code increases possibilities to combine code, improves flexibility in assigning registers, and creates opportunities for moving the definition and uses of a value closer together. In the CFG Click <ref> [22, 20] </ref> describes a simple SSA form intermediate representation. Click's IR is based on the CFG, but he represents basic blocks by a single node, called a region node. <p> Section 3.1 begins by describing Click's intermediate representation, which is the basis for Score. Section 3.2 presents our extensions to Click's representation that enable Score to support compilation for heterogeneous systems. Finally, Section 3.3 lists other enhancements to Click's IR for handling parallelism. 3.1 Click's Representation Click <ref> [22, 20] </ref> describes an IR with simple nodes appropriate for representing instructions for a uniprocessor RISC machine (e.g., load, store, and conditional branch). His representation uses SSA form on top of a CFG, but each CFG block is explicitly represented by an IR node, called a region node.
Reference: [23] <author> K. Cooper, M. W. Hall, R. T. Hood, K. Kennedy, K. S. McKinley, J. M. Mellor-Crummey, L. Torczon, and S. K. Warren. </author> <title> The ParaScope parallel programming environment. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2) </volume> <pages> 244-263, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Optimistist IRs are those representations that ignore most control or control flow dependences, and instead rely heavily on data dependences to define the relationships between nodes. Section 2.5 includes miscellaneous IRs which do not fit neatly in this progression. 2.1 Abstract Syntax Trees Many parallelizing compilers <ref> [23, 12, 13, 69, 54] </ref> use an abstract syntax tree (AST) because it is simple to build, retains the original structure of the program, and readily supports high-level transformations. Each AST node represents an operation, and the node's children represent the operation's operands [1, 47]. <p> The type of an AST node corresponds to a construct from the source language such as a minus operation, while loop, or variable declaration. ASTs are also used in other types of systems such as compiler toolkits, software engineering environments, and distribution formats. ParaScope Rice University's ParaScope <ref> [23, 42] </ref> is a source-to-source translator for Fortran. ParaScope was built to explore automatic parallelization and has been used for a variety of research projects. It provides sophisticated global program analysis and a rich set of program transformations.
Reference: [24] <author> K. Cooper, M. W. Hall, R. T. Hood, K. Kennedy, K. S. McKinley, J. M. Mellor-Crummey, L. Torczon, and S. K. Warren. </author> <title> The ParaScope parallel programming environment. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2) </volume> <pages> 244-263, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Though this is the first attempt at building an IR that supports so many diverse models of parallelism, the nodes can largely be borrowed from other systems such as XDP [8], the D System [63], and ParaScope <ref> [24] </ref>. It is unclear if we need to investigate new nodes for some models of parallelism or simply assemble existing ones.
Reference: [25] <author> R. Cytron, J. Ferrante, B. Rosen, M. Wegman, and K. Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The control dependence edges in PDGs reflect less of a source program's syntactic structure than the control flow edges in CFGs or the syntax based edges in ASTs. Any of these three graph representations can be put into static single assignment (SSA) form <ref> [25] </ref>. In SSA form, no variable is assigned a value in more than one location. We divide the IRs in our survey into five groups by focusing on the extent to which each IR constrains 3 the ordering of its nodes. <p> Similarly, Static Single Assignment (SSA) form increases opportunities for optimization by modifying data dependences. In SSA form, a variable use has exactly one definition <ref> [25] </ref>. To meet this requirement, every place a variable is defined or two control paths merge, a new variable name is created. Conceptually, pseudo-assignments, called -nodes, are inserted when two control paths merge to combine the values from each of the incoming control paths. <p> In addition, converting to SSA form creates an extraordinary number of new variable names; many of which can and should be removed (Cytron et al. <ref> [25] </ref> suggest using a graph coloring algorithm for removal). Aliases play an important role in the construction of SSA form. When several memory accesses are potentially aliased, they must be treated as identical. Hence, an update to any one of them is the same as an update to them all. <p> The PDW consists of SSA form on top of a PDG with modifications to make it interpretable and provide support for data flow parallelism. The PDW is built in three steps. The first step uses the standard algorithm <ref> [25] </ref> to build SSA form on a PDG. The second step translates an SSA-form PDG into Gated Single Assignment (GSA) form, which is essentially equivalent to Alpern et al.'s gated SSA form.
Reference: [26] <author> Jeffery Dean, David Grove, and Craig Chambers. </author> <title> Optimization of object-oriented programs using static class hierarchy analysis. </title> <type> Technical Report 94-12-01, </type> <institution> University of Washington, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: Type information is not shown. and pointer indirections. The indirect call inherent in a method invocation may be optimized by deducing constraints on the actual method called <ref> [29, 18, 26, 31, 53] </ref>. This deduction requires high-level knowledge about the method invocation. Score distinguishes between four different types of calls: direct calls, indirect calls for languages with function pointers, closure calls for languages with nested procedures, and method calls for object oriented languages.
Reference: [27] <institution> Defense Research Agency. TDF specification. </institution> <type> Technical report, </type> <institution> Defence Research Agency, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: For example, the declaration of a local variable will have as one of its arguments, the routine's body. ANDF attempts to straddle the line between languages and machines. Besides linkage concerns, the representation consists largely of types, conditions, expressions (including simple control structures), and exceptions <ref> [27] </ref>. This level of abstraction is barely above that of an idealized uniprocessor. ANDF is only intended to support uniprocessors, so it does not consider multiple models of parallelism. ANDF also has no mechanism to protect the rest of the compiler from changes to the IR.
Reference: [28] <author> Arundhati Dhagat. ANN: </author> <title> A software tool for annotatin IRIS-Ada graphs. </title> <type> Master's Project, </type> <month> June </month> <year> 1995. </year>
Reference-contexts: These same algebraic simplifications can apply to user defined data types, if the compiler is able to tag nodes corresponding to user defined constructs with the appropriate attributes. In separate research, we are investigating an annotation tool for conveying such information to the compiler <ref> [28] </ref>. 3.2.2 Supporting Reordering A compiler's compilation strategy determines the order in which transformations are performed. Different targets often require different compilation strategies, which means different transformations and orderings. <p> We plan to use a Fortran 77 front end and a combined C and C++ front end from Eddison Design Group (EDG). We will build a tool to translate EDG's intermediate language into Score. We will also revise our annotation tool <ref> [28] </ref> to work on either the EDG intermediate language or Score. In addition, we are constructing a native code generator for the Alpha family of microprocessors. Click's IR is written in C++, so Score will also be written in C++. The annotation tool is currently written in C.
Reference: [29] <author> Amer Diwan. </author> <title> Analyzing statically-typed object-oriented programs for modern processors. </title> <booktitle> In Progress. </booktitle>
Reference-contexts: Type information is not shown. and pointer indirections. The indirect call inherent in a method invocation may be optimized by deducing constraints on the actual method called <ref> [29, 18, 26, 31, 53] </ref>. This deduction requires high-level knowledge about the method invocation. Score distinguishes between four different types of calls: direct calls, indirect calls for languages with function pointers, closure calls for languages with nested procedures, and method calls for object oriented languages. <p> The closure call, which does not apply to C, is also not shown but would be similar to a direct call. Field References Analysis of object oriented code benefits from a high-level representation for references to record fields <ref> [29] </ref>. Therefore, Score includes an annotation which holds the necessary high-level information. This annotation will be similar to the one for procedure calls. 3.2.3 Supporting Multiple Models Our third contribution is to assemble the IR nodes necessary for compiling to multiple models of parallelism within a single IR.
Reference: [30] <author> Keith A. Faigin, Jay P. Hoeflinger, David A. Padua, Paul M. Petersen, and Stephen A. Weatherford. </author> <title> The polaris internal representation. </title> <institution> Center for Supercomputing Research and Development CSRD-1317, University of Illinois at Urbana-Champaign, </institution> <month> February </month> <year> 1994. </year> <month> 24 </month>
Reference-contexts: Transformations in ParaScope may be reordered and even applied to sections of a program during interactive editing. Some transformations incrementally update analysis data, but implementing incremental update is solely the responsibility of the individual transformations. Polaris Polaris from the University of Illinois, is an optimizing source-to-source translator <ref> [12, 52, 30] </ref> for Fortran derivatives. The authors have two major goals for Polaris: to automatically parallelize sequential programs for distributed shared memory machines and to be a production quality compiler in terms of reliability and speed.
Reference: [31] <author> Mary F. Fernandez. </author> <title> Simple and effective link-time optimization of modula-3 programs. </title> <booktitle> In Proceedings of the SIGPLAN '95 Conference on Programming Language Design and Implementation, </booktitle> <address> La Jolla, CA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Type information is not shown. and pointer indirections. The indirect call inherent in a method invocation may be optimized by deducing constraints on the actual method called <ref> [29, 18, 26, 31, 53] </ref>. This deduction requires high-level knowledge about the method invocation. Score distinguishes between four different types of calls: direct calls, indirect calls for languages with function pointers, closure calls for languages with nested procedures, and method calls for object oriented languages.
Reference: [32] <author> J. Ferrante, K. Ottenstein, and J. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: Scalar compilers, on the other hand, often use a control flow graph (CFG) [1] as their primary representation. CFGs represent programs as a graph of basic blocks. Edges in a CFG represent control flow (e.g., decisions, loops, jumps), and basic blocks contain sequentially executed code. Program dependence graphs (PDG) <ref> [32] </ref> connect nodes together with control dependence edges and data dependence edges. The control dependence edges in PDGs reflect less of a source program's syntactic structure than the control flow edges in CFGs or the syntax based edges in ASTs. <p> SUIF includes low-level nodes as well as high-level nodes and eventually linearizes the tree. ANDF tries to find a balance between high and low-level representations. 2.2 Program Dependence Graphs Ferrante et al.introduce program dependence graphs (PDG) which overcomes some of the limitations of ASTs <ref> [32] </ref>. PDGs eliminate the infiltration of the source language's syntax into the IR and supports a broader range of transformations. As in ASTs, PDG nodes represent program actions (usually operators and operands), but in a PDG nodes are connected via data and control dependences.
Reference: [33] <author> Kari Forester. </author> <title> IRIS-Ada reference manual. </title> <institution> Arcadia UM-90-07, University of Massachusetts, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: Reprise, IRIS-Ada, IRIS-C++ The Arcadia project investigates software engineering tools and techniques. Though not a compiler, Arcadia software represents programs in an AST and can derive other compiler representations (e.g., CFGs) from the AST. Arcadia uses IRIS-Ada <ref> [33] </ref> to represent Ada programs and IRIS-C++ [62] to represent C++ programs. IRIS-C++ descends from both IRIS-Ada and Reprise [56], a representation for C++ from AT&T. These IRs use a separate node to represent each semantic concept (e.g., variable attributes) in the source language.
Reference: [34] <author> Message Passing Interface Forum. </author> <title> MPI: A message-passing interface standard, </title> <type> v1.0. Technical report, </type> <institution> University of Tennessee, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Section 5 explains the project's timeline, which is actually shown in Appendix A. Finally, we summarize in Section 6. 1.1 Heterogeneous processing Recent product introductions (e.g., Meiko CS-2, IBM SP-2) and research (e.g., IUA [66], PVM [61], p4 [17], and MPI <ref> [34] </ref>) demonstrate growing interest in heterogeneous parallelism. Previous efforts in parallelism focused on homogeneous parallel machines, which implement a single model of parallelism throughout the machine.
Reference: [35] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report TR90-141, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: ParaScope's core intermediate representation is an AST with other analysis data (e.g., data dependences, control dependences, and SSA form) woven into it. ParaScope has been used to compile to several models of parallelism (e.g., shared-memory, control-parallel MIMD [49] and distributed memory, data-parallel MIMD <ref> [35] </ref>), and has IR nodes to represent parallel constructs. Modifying ParaScope's IR is difficult because the implementation does not use data hiding, so all the components in ParaScope directly access the AST. Transformations in ParaScope may be reordered and even applied to sections of a program during interactive editing.
Reference: [36] <author> Arif Ghafoor and Jaehyung Yang. </author> <title> A distributed heterogeneous supercomputing management system. </title> <journal> Computer, </journal> <volume> 26(6) </volume> <pages> 78-86, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: These sections must either be parallelized inefficiently or executed sequentially. Heterogeneous systems can deliver consistent high performance by incorporating multiple models of parallelism within one machine or across machines, creating a virtual machine. Heterogeneous processing <ref> [59, 64, 65, 43, 36] </ref> is the well-orchestrated use of heterogeneous hardware to execute a single application [43]. When an application encompasses subtasks that employ different models of parallelism, the application benefits from using disparate hardware architectures that match the inherent parallelism of each subtask.
Reference: [37] <author> Milind B. Girkar and Constantine Polychronopoulos. </author> <title> The hierarchical task graph as a universal intermediate representation. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 22(5), </volume> <year> 1994. </year>
Reference-contexts: Parafrase-2 is unique in that it focuses on control parallelism as well as data parallelism. For control parallelism, Parafrase-2 partitions a program into parallel tasks while minimizing dependences between tasks, and for data parallelism, it parallelizes the execution of loops. Girkar and Polychronopoulos <ref> [37] </ref> designed an intermediate representation to meet the needs of Parafrase-2: Hierarchical Task Graphs (HTG). HTGs are hierarchical graphs much like the output of interval analysis. The leaves in an HTG represent simple statements and subroutine calls. Interior nodes represent loops and basic blocks in the flow graph.
Reference: [38] <author> Stanford Compiler Group. </author> <title> The SUIF library. </title> <type> Technical report, </type> <institution> Stanford University, </institution> <year> 1994. </year>
Reference-contexts: However, these representations do not use inheritance, so transformations must operate on specific node types. SUIF Stanford University Intermediate Format (SUIF) is a compiler framework that can be used as either a source-to-C translator or a native code compiler <ref> [69, 6, 38] </ref> for Fortran 77 and C. SUIF serves as a research tool for investigating automatic parallelization of sequential programs. SUIF's IR (which is also called SUIF) includes both high and low-level nodes to enable code generation.
Reference: [39] <author> Philip J. Hatcher and Thomas W. Christopher. </author> <title> High-quality code generation via bottom-up tree pattern matching. </title> <booktitle> In Thirteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 119-130, </pages> <address> St. Petersburg Beach, FL, </address> <month> January </month> <year> 1986. </year>
Reference-contexts: Instead, transformations need to know the data and control dependences between each IR node. For ASTs, this dependence information has traditionally been obtained through analysis and either woven into the AST or stored in a separate structure. * Though some research has investigated generating code from trees <ref> [2, 19, 4, 39] </ref>, ASTs are not adequate for low-level optimizations. ASTs do not capture the features of the target hardware (e.g., the instruction pipeline and register set). SUIF and ANDF attack this problem in two separate ways.
Reference: [40] <author> P. Havlak. </author> <title> Construction of thinned gate single-assignment form. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: The authors use the data flow IPG in their efforts to compile to data flow architectures. The other IPGs are appropriate for compiling to other models of parallelism. Thinned Gated Single Assignment Havlak <ref> [40] </ref> describes a minor variation of Ballance, et al.'s GSA form, called Thinned Gated Single Assignment (TGSA). TGSA extends high-level SSA form (i.e., GSA) to unstructured code and removes information from GSA that is unnecessary for symbolic analysis.
Reference: [41] <author> R. Johnson and K. Pingali. </author> <title> Dependence-based program analysis. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Unlike a -node, both control flow paths leading to a -node are executed. Hence, -nodes allow the explicit representation of control parallelism. Dataflow and SSA Even with use-def chains as Stoltz et al. [60] recommend, SSA form does not support solving backward data flow problems. Johnson and Pingali <ref> [41] </ref> devised the dependence flow graph (DFG) to support forward and backward data flow. The algorithm to construct DFGs finds single-entry single-exit (SESE) regions in the CFG. The single entry edge and the single exit edge of a SESE regions have the same control dependences. <p> Hence, a single -node in SSA form becomes a tree of specialized -nodes in GSA form. The third step converts from GSA form to a PDW by inserting switch nodes (similar to those in Johnson and Pingali <ref> [41] </ref>). Switch nodes are inserted symmetrically to fl-nodes. The authors do not expect transforms to operate over the entire PDW. Instead, they define an interface that provides subgraphs called Interpretable Program Graphs (IPGs).
Reference: [42] <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Analysis and transformation in an interactive parallel programming tool. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 5(7) </volume> <pages> 575-602, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: The type of an AST node corresponds to a construct from the source language such as a minus operation, while loop, or variable declaration. ASTs are also used in other types of systems such as compiler toolkits, software engineering environments, and distribution formats. ParaScope Rice University's ParaScope <ref> [23, 42] </ref> is a source-to-source translator for Fortran. ParaScope was built to explore automatic parallelization and has been used for a variety of research projects. It provides sophisticated global program analysis and a rich set of program transformations.
Reference: [43] <author> Ashfaq A. Khokhar, Viktor K. Prasanna, Muhammad E. Shaaaban, and Cho-Li Wang. </author> <title> Heterogeneous computing: Challenges and opportunities. </title> <journal> Computer, </journal> <volume> 26(6) </volume> <pages> 18-27, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: These sections must either be parallelized inefficiently or executed sequentially. Heterogeneous systems can deliver consistent high performance by incorporating multiple models of parallelism within one machine or across machines, creating a virtual machine. Heterogeneous processing <ref> [59, 64, 65, 43, 36] </ref> is the well-orchestrated use of heterogeneous hardware to execute a single application [43]. When an application encompasses subtasks that employ different models of parallelism, the application benefits from using disparate hardware architectures that match the inherent parallelism of each subtask. <p> Heterogeneous systems can deliver consistent high performance by incorporating multiple models of parallelism within one machine or across machines, creating a virtual machine. Heterogeneous processing [59, 64, 65, 43, 36] is the well-orchestrated use of heterogeneous hardware to execute a single application <ref> [43] </ref>. When an application encompasses subtasks that employ different models of parallelism, the application benefits from using disparate hardware architectures that match the inherent parallelism of each subtask.
Reference: [44] <author> Alan E. Klietz, Andrei V. Malevsky, and Ken Chin-Purcell. </author> <title> A case study in metacomputing: Distributed simulations of mixing in turbulent convection. </title> <booktitle> In Workshop on Heterogeneous Processing, </booktitle> <pages> pages 101-106. </pages> <publisher> IEEE, </publisher> <month> April </month> <year> 1993. </year>
Reference: [45] <author> Stavros Macrakis. </author> <title> From UNCOL to ANDF: Progress in standard intermediate languages. </title> <type> Technical report, </type> <institution> Open Software Foundation Research Institute, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: The SUIF compiler does not reorder transformations during compilation. SUIF exchanges data between passes through an annotation mechanism, which is very flexible but provides no support for ensuring their accuracy. ANDF Architecture-Neutral Distribution Format (ANDF) [54, 46] is a distribution format for portable software <ref> [45] </ref>. ANDF tries for a clean break of the compiler into a language-dependent, machine-independent front end called a producer, and a language-independent, machine-dependent back end called an installer.
Reference: [46] <author> Stavros Macrakis. </author> <title> The structure of ANDF: Principles and examples. </title> <type> Technical report, </type> <institution> Open Software Foundation Research Institute, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: The SUIF compiler does not reorder transformations during compilation. SUIF exchanges data between passes through an annotation mechanism, which is very flexible but provides no support for ensuring their accuracy. ANDF Architecture-Neutral Distribution Format (ANDF) <ref> [54, 46] </ref> is a distribution format for portable software [45]. ANDF tries for a clean break of the compiler into a language-dependent, machine-independent front end called a producer, and a language-independent, machine-dependent back end called an installer. <p> As a distribution format, ANDF must protect against reverse engineering. Therefore, ANDF removes all identifiers and replaces them with tags. Tags are integers, and though they do not obey the source language's scoping, tags are only valid within their declaration <ref> [46] </ref>. Therefore, a declaration must carry the code for which the declaration is valid. For example, the declaration of a local variable will have as one of its arguments, the routine's body. ANDF attempts to straddle the line between languages and machines.
Reference: [47] <author> J. McCarthy. </author> <title> Towards a mathematical science of computation. </title> <booktitle> In Information Processing, </booktitle> <pages> pages 21-28, </pages> <address> Holland, </address> <year> 1962. </year>
Reference-contexts: Each AST node represents an operation, and the node's children represent the operation's operands <ref> [1, 47] </ref>. The type of an AST node corresponds to a construct from the source language such as a minus operation, while loop, or variable declaration. ASTs are also used in other types of systems such as compiler toolkits, software engineering environments, and distribution formats.
Reference: [48] <author> K. McKinley, S. Singhai, G. Weaver, and C. Weems. </author> <title> Compiling for heterogeneous systems: A survey and an approach. </title> <institution> Computer Science TR-95-59, University of Massachusetts, </institution> <month> July </month> <year> 1995. </year> <note> http://osl-www.cs.umass.edu/- ~oos/papers.html. </note>
Reference: [49] <author> K. S. McKinley. </author> <title> Automatic and Interactive Parallelization. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: ParaScope's core intermediate representation is an AST with other analysis data (e.g., data dependences, control dependences, and SSA form) woven into it. ParaScope has been used to compile to several models of parallelism (e.g., shared-memory, control-parallel MIMD <ref> [49] </ref> and distributed memory, data-parallel MIMD [35]), and has IR nodes to represent parallel constructs. Modifying ParaScope's IR is difficult because the implementation does not use data hiding, so all the components in ParaScope directly access the AST.
Reference: [50] <author> Kathryn S. McKinley, Sharad K. Singhai, Glen E. Weaver, and Charles C. Weems. </author> <title> Compiler architectures for heterogeneous systems. </title> <booktitle> In Proceedings of the Eighth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Columbus, OH, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Instead of modifying programs, the complexity of heterogeneous systems should be automatically handled by a compiler. With certain modifications to a compiler's software architecture, compilers could use transformations to adjust a program to execute efficiently on a heterogeneous machine <ref> [50] </ref>. A compiler for heterogeneous systems is unique in that it must simultaneously compile a single program to diverse targets 1 . <p> If a transformation is totally unaware of the node types on which it operates, then it will work for any model of parallelism. However, most interesting transformations need more knowledge of the underlying program. 3 Score Our long term goal is to build a compiler for heterogeneous systems <ref> [50] </ref>. This section describes our intermediate representation, Score, which is the first step in building this compiler. In Section 1 we observed that the variety, variability, and high performance of heterogeneous systems demand a compiler that generates efficient code for a variety of hardware models.
Reference: [51] <author> Kevin O'Brien, Kathryn M. O'Brien, Martin Hopkins, Arvin Shepherd, and Ron Unrau. XIL and YIL: </author> <booktitle> The intermediate languages of TOBEY. In Workshop on Intermediate Representations, </booktitle> <address> San Francisco, CA, </address> <month> January </month> <year> 1995. </year>
Reference-contexts: Much of the semantics of loops and calls are captured by the dependence and control information, further separating the programming language from the representation. XIL/YIL The XIL and YIL intermediate representations <ref> [51] </ref> jointly comprise the intermediate representation for IBM's TOBEY compiler. Initially, TOBEY's design included only traditional, low-level optimizations performed on XIL. However, when high-level transformations were added to TOBEY, they developed YIL to 11 work with XIL. XIL nodes represent instructions typically found in RISC processors.
Reference: [52] <author> David A. Padua, Rudolf Eigenmann, Jay Hoeflinger, Paul Petersen, Peng Tu, Stephen Weatherford, and Keith Faigin. </author> <title> Polaris: A new-generation parallelizing compiler for MPPs. </title> <institution> Center for Supercomputing Research and Development CSRD-1306, University of Illinois at Urbana-Champaign, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Transformations in ParaScope may be reordered and even applied to sections of a program during interactive editing. Some transformations incrementally update analysis data, but implementing incremental update is solely the responsibility of the individual transformations. Polaris Polaris from the University of Illinois, is an optimizing source-to-source translator <ref> [12, 52, 30] </ref> for Fortran derivatives. The authors have two major goals for Polaris: to automatically parallelize sequential programs for distributed shared memory machines and to be a production quality compiler in terms of reliability and speed.
Reference: [53] <author> Jens Palsberg and Michael I. Schwartzbach. </author> <title> Object-oriented. </title> <booktitle> In Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, </booktitle> <address> Pheonix, AZ, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Type information is not shown. and pointer indirections. The indirect call inherent in a method invocation may be optimized by deducing constraints on the actual method called <ref> [29, 18, 26, 31, 53] </ref>. This deduction requires high-level knowledge about the method invocation. Score distinguishes between four different types of calls: direct calls, indirect calls for languages with function pointers, closure calls for languages with nested procedures, and method calls for object oriented languages.
Reference: [54] <author> N. E. Peeling. </author> <title> ANDF features and benefits. </title> <type> Technical report, </type> <institution> Defense Research Agency, </institution> <year> 1992. </year>
Reference-contexts: Optimistist IRs are those representations that ignore most control or control flow dependences, and instead rely heavily on data dependences to define the relationships between nodes. Section 2.5 includes miscellaneous IRs which do not fit neatly in this progression. 2.1 Abstract Syntax Trees Many parallelizing compilers <ref> [23, 12, 13, 69, 54] </ref> use an abstract syntax tree (AST) because it is simple to build, retains the original structure of the program, and readily supports high-level transformations. Each AST node represents an operation, and the node's children represent the operation's operands [1, 47]. <p> The SUIF compiler does not reorder transformations during compilation. SUIF exchanges data between passes through an annotation mechanism, which is very flexible but provides no support for ensuring their accuracy. ANDF Architecture-Neutral Distribution Format (ANDF) <ref> [54, 46] </ref> is a distribution format for portable software [45]. ANDF tries for a clean break of the compiler into a language-dependent, machine-independent front end called a producer, and a language-independent, machine-dependent back end called an installer.
Reference: [55] <author> Constantine Polychronopoulos, Milind B. Girkar, Mohammad R. Haghighat, Chia L. Lee, Bruce P. Leung, and Dale A. Schouten. </author> <title> Parafrase-2: An environment for parallelizing, partitioning, synchronizing, and scheduling programs on multiprocessors. </title> <journal> International Journal of High Speed Computing, </journal> <volume> 1(1), </volume> <year> 1989. </year>
Reference-contexts: Explicit Data Placement focuses solely on representing communication. Hierarchical Task Graphs The University of Illinois' Parafrase-2 is a source-to-source translator designed for investigating compiler support for multiple languages and target architectures <ref> [55] </ref>. Parafrase-2 is unique in that it focuses on control parallelism as well as data parallelism. For control parallelism, Parafrase-2 partitions a program into parallel tasks while minimizing dependences between tasks, and for data parallelism, it parallelizes the execution of loops.
Reference: [56] <author> David S. Rosenblum and Alexander L. Wolf. </author> <title> Representing semantically analyzed C++ code with reprise. </title> <booktitle> In Proceedings of the USENIX C++ Conference, </booktitle> <pages> pages 119-134, </pages> <address> DC, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Though not a compiler, Arcadia software represents programs in an AST and can derive other compiler representations (e.g., CFGs) from the AST. Arcadia uses IRIS-Ada [33] to represent Ada programs and IRIS-C++ [62] to represent C++ programs. IRIS-C++ descends from both IRIS-Ada and Reprise <ref> [56] </ref>, a representation for C++ from AT&T. These IRs use a separate node to represent each semantic concept (e.g., variable attributes) in the source language.
Reference: [57] <author> Erik Ruf. </author> <title> Optimizing sparse representations for dataflow analysis. </title> <booktitle> In Workshop on Intermediate Representations, </booktitle> <address> San Francisco, CA, </address> <month> January </month> <year> 1995. </year>
Reference-contexts: Click takes a different approach to optimization ordering by combining optimistic optimizations within a single data flow framework. In a PDG Weise et al. present the Value Dependence Graph (VDG) for representing imperative programs <ref> [68, 57] </ref>. The VDG grew out of Fuse, a representation for functional programs, and VDG retains a functional flavor. The VDG is much like a PDG coupled with SSA form. <p> Finally, dPDGs are sequentialized using one of the standard algorithms for sequentializing PDGs. Though the VDG should be easy to extend for other models of parallelism, it currently supports only uniprocessor targets. Ruf <ref> [57] </ref> presents transformations that produce a sparse form of the VDG tailored 10 for specific analyses. He reuses the VDG structure, so the compiler does not need an entirely different representation for this specialized IR. The use of specialized representations may aid compilation to different models.
Reference: [58] <author> B. Simons and J. Ferrante. </author> <title> An efficient algorithm for constructing a control flow graph for parallel code. </title> <type> Technical Report TR 03.465, </type> <institution> IBM Corp., Santa Teresa Laboratory, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: Our strategy will be to convert the PDG to a CFG <ref> [58] </ref>, generate a legal ordering [21], run analyses, and then convert back to a PDG. 4 Experiments 4.1 Infrastructure Our tests will include four models of parallelism: uniprocessor, SIMD, message passing MIMD, and distributed shared-memory MIMD.
Reference: [59] <author> Larry Smarr and Charles E. Catlett. </author> <title> Metacomputing. </title> <journal> Communications of the ACM, </journal> <volume> 35(6) </volume> <pages> 45-52, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: These sections must either be parallelized inefficiently or executed sequentially. Heterogeneous systems can deliver consistent high performance by incorporating multiple models of parallelism within one machine or across machines, creating a virtual machine. Heterogeneous processing <ref> [59, 64, 65, 43, 36] </ref> is the well-orchestrated use of heterogeneous hardware to execute a single application [43]. When an application encompasses subtasks that employ different models of parallelism, the application benefits from using disparate hardware architectures that match the inherent parallelism of each subtask.
Reference: [60] <author> Eric Stoltz, Michael P. Gerlek, and Michael Wolfe. </author> <title> Extended SSA with Factored Use-Def Chains to Support Optimization and Parallelism. </title> <booktitle> In Proc. Hawaii International Conference on Systems Sciences, </booktitle> <address> Maui, Hawaii, </address> <month> Jan 94. </month>
Reference-contexts: High-level -nodes combine high and low-level information in a single representation which in turn enables a broader range of transformations to be used on the representation. 8 Extended SSA Stoltz et al. <ref> [60] </ref> describe the intermediate form they use in their parallelizing Fortran 90 compiler, Nascent. Rather than incur the cost of actually creating many new variable names, most systems use factored def-use chains. <p> Unlike a -node, both control flow paths leading to a -node are executed. Hence, -nodes allow the explicit representation of control parallelism. Dataflow and SSA Even with use-def chains as Stoltz et al. <ref> [60] </ref> recommend, SSA form does not support solving backward data flow problems. Johnson and Pingali [41] devised the dependence flow graph (DFG) to support forward and backward data flow. The algorithm to construct DFGs finds single-entry single-exit (SESE) regions in the CFG.
Reference: [61] <author> V.S. Sunderam, G.A. Geist, J. Dongarra, and P. Manchek. </author> <title> The PVM concurrent computing system: Evolution, experiences, and trends. </title> <journal> Parallel Computing, </journal> <volume> 20(4) </volume> <pages> 531-545, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Section 5 explains the project's timeline, which is actually shown in Appendix A. Finally, we summarize in Section 6. 1.1 Heterogeneous processing Recent product introductions (e.g., Meiko CS-2, IBM SP-2) and research (e.g., IUA [66], PVM <ref> [61] </ref>, p4 [17], and MPI [34]) demonstrate growing interest in heterogeneous parallelism. Previous efforts in parallelism focused on homogeneous parallel machines, which implement a single model of parallelism throughout the machine.
Reference: [62] <author> Peri Tarr, Alexander Wise, and Glen Weaver. IRIS-C++ 1.0 vs. IRIS-Ada 2.0: </author> <title> A comparison of the internal representations. </title> <type> Arcadia Technical Report UM-94-02, </type> <institution> University of Massachusetts, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: Reprise, IRIS-Ada, IRIS-C++ The Arcadia project investigates software engineering tools and techniques. Though not a compiler, Arcadia software represents programs in an AST and can derive other compiler representations (e.g., CFGs) from the AST. Arcadia uses IRIS-Ada [33] to represent Ada programs and IRIS-C++ <ref> [62] </ref> to represent C++ programs. IRIS-C++ descends from both IRIS-Ada and Reprise [56], a representation for C++ from AT&T. These IRs use a separate node to represent each semantic concept (e.g., variable attributes) in the source language.
Reference: [63] <author> C. Tseng. </author> <title> An Optimizing Fortran D Compiler for MIMD Distributed-Memory Machines. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Though this is the first attempt at building an IR that supports so many diverse models of parallelism, the nodes can largely be borrowed from other systems such as XDP [8], the D System <ref> [63] </ref>, and ParaScope [24]. It is unclear if we need to investigate new nodes for some models of parallelism or simply assemble existing ones.
Reference: [64] <author> L. H. Turcotte. </author> <title> A survey of software environments for exploiting networked computing resources. </title> <type> Technical Report MSSU-EIRS-ERC-93-2, </type> <institution> NSF Engineering Research Center, Mississippi State University, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: These sections must either be parallelized inefficiently or executed sequentially. Heterogeneous systems can deliver consistent high performance by incorporating multiple models of parallelism within one machine or across machines, creating a virtual machine. Heterogeneous processing <ref> [59, 64, 65, 43, 36] </ref> is the well-orchestrated use of heterogeneous hardware to execute a single application [43]. When an application encompasses subtasks that employ different models of parallelism, the application benefits from using disparate hardware architectures that match the inherent parallelism of each subtask.
Reference: [65] <author> L. H. Turcotte. </author> <title> Cluster computing. </title> <editor> In Albert Y. Zomaya, editor, </editor> <booktitle> Parallel and Distributed Computing Handbook, chapter 26. </booktitle> <publisher> McGraw-Hill, </publisher> <month> October </month> <year> 1995. </year> <note> ISBN 0-07-073020-2. </note>
Reference-contexts: These sections must either be parallelized inefficiently or executed sequentially. Heterogeneous systems can deliver consistent high performance by incorporating multiple models of parallelism within one machine or across machines, creating a virtual machine. Heterogeneous processing <ref> [59, 64, 65, 43, 36] </ref> is the well-orchestrated use of heterogeneous hardware to execute a single application [43]. When an application encompasses subtasks that employ different models of parallelism, the application benefits from using disparate hardware architectures that match the inherent parallelism of each subtask.
Reference: [66] <author> Charles Weems, Steven Levitan, Allen Hanson, Edward Riseman, J. Gregory Nash, and David Shu. </author> <title> The image understanding architecture. </title> <journal> International Journal of Computer Vision, </journal> <volume> 2(3) </volume> <pages> 251-282, </pages> <year> 1989. </year>
Reference-contexts: Section 5 explains the project's timeline, which is actually shown in Appendix A. Finally, we summarize in Section 6. 1.1 Heterogeneous processing Recent product introductions (e.g., Meiko CS-2, IBM SP-2) and research (e.g., IUA <ref> [66] </ref>, PVM [61], p4 [17], and MPI [34]) demonstrate growing interest in heterogeneous parallelism. Previous efforts in parallelism focused on homogeneous parallel machines, which implement a single model of parallelism throughout the machine.
Reference: [67] <author> Charles C. Weems, Steven P. Levitan, Allen R. Hanson, Edward M. Riseman, David B. Shu, and J. Grefory Nash. </author> <title> The image understanding architecture. </title> <journal> Internation Journal of Computer Vision, </journal> <volume> 2 </volume> <pages> 251-282, </pages> <year> 1989. </year>
Reference-contexts: We have a simulator [16] for the IUA CAAPP <ref> [67] </ref>, which is a SIMD machine. We expect to obtain access to a distributed shared-memory MIMD at one of the national supercomputing centers. We are currently designing a compiler testbed, of which Score will be part. Though the design is not finalized, some details appear to be settled.
Reference: [68] <author> Daniel Weise, Roger F. Crew, Michael Ernst, and Bjarne Steensgaard. </author> <title> Value dependence graphs: Representation without taxation. </title> <booktitle> In Proceedings of the Twenty-first Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 297-310, </pages> <address> Portland, OR, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: Click takes a different approach to optimization ordering by combining optimistic optimizations within a single data flow framework. In a PDG Weise et al. present the Value Dependence Graph (VDG) for representing imperative programs <ref> [68, 57] </ref>. The VDG grew out of Fuse, a representation for functional programs, and VDG retains a functional flavor. The VDG is much like a PDG coupled with SSA form.
Reference: [69] <author> Robert P. Wilson, Robert S. French, Christopher S. Wilson, Saman P. Amarasinghe, Jennifer M. Anderson, Steve W. K. Tjiang, Shih-Wei Liao, Chau-Wen Tseng, Mary W. Hall, Monica S. Lam, and John L. Hennessy. </author> <title> The SUIF compiler system: A parallelizing and optimizing research compiler. </title> <journal> SIGPLAN, </journal> <volume> 29(12), </volume> <month> December </month> <year> 1994. </year>
Reference-contexts: Optimistist IRs are those representations that ignore most control or control flow dependences, and instead rely heavily on data dependences to define the relationships between nodes. Section 2.5 includes miscellaneous IRs which do not fit neatly in this progression. 2.1 Abstract Syntax Trees Many parallelizing compilers <ref> [23, 12, 13, 69, 54] </ref> use an abstract syntax tree (AST) because it is simple to build, retains the original structure of the program, and readily supports high-level transformations. Each AST node represents an operation, and the node's children represent the operation's operands [1, 47]. <p> However, these representations do not use inheritance, so transformations must operate on specific node types. SUIF Stanford University Intermediate Format (SUIF) is a compiler framework that can be used as either a source-to-C translator or a native code compiler <ref> [69, 6, 38] </ref> for Fortran 77 and C. SUIF serves as a research tool for investigating automatic parallelization of sequential programs. SUIF's IR (which is also called SUIF) includes both high and low-level nodes to enable code generation.
Reference: [70] <author> Michael Wolfe. </author> <title> Beyond induction variables. </title> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 162-174, </pages> <address> San Francisco, CA, </address> <year> 1992. </year> <month> 26 </month>
Reference-contexts: Loops Loops are an important construct for compiler manipulation as evidenced by the plethora of loop transformations [7]. Due to the importance of loops, researchers have developed techniques for extracting loops and related information from low-level representations <ref> [70] </ref>. Given these techniques, our compiler will find loops through analysis rather than through interacting with the compiler's front end.
References-found: 70

