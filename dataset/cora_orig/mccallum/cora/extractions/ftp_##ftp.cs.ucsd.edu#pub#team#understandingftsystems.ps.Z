URL: ftp://ftp.cs.ucsd.edu/pub/team/understandingftsystems.ps.Z
Refering-URL: http://www.cs.ucsd.edu/users/flaviu/
Root-URL: http://www.cs.ucsd.edu
Email: flaviu@cs.ucsd.edu  
Title: Understanding Fault-Tolerant Distributed Systems  
Author: Flaviu Cristian 
Date: May 25, 1993  
Address: La Jolla, CA 92093-0114  
Affiliation: Computer Science and Engineering University of California, San Diego  
Abstract: We propose a small number of basic concepts that can be used to explain the architecture of fault-tolerant distributed systems and we discuss a list of architectural issues that we find useful to consider when designing or examining such systems. For each issue we present known solutions and design alternatives, we discuss their relative merits and we give examples of systems which adopt one approach or the other. The aim is to introduce some order in the complex discipline of designing and understanding fault-tolerant distributed systems. 
Abstract-found: 1
Intro-found: 1
Reference: [AL81] <author> T. Anderson, P. Lee: </author> <title> Fault-tolerance- Principles and Practice, </title> <publisher> Prentice Hall, </publisher> <year> 1981. </year>
Reference-contexts: Some examples of commercially successful coarse granularity architectures are Tandem [B81], the DEC VAX Cluster [KLS86] and the IBM MVS/XRF [IBM87]. Examples of fine granularity architectures are Stratus [TW89] and Sequoia [B88]. Other examples of fault-tolerant architectures can be found in <ref> [AL81] </ref>, [La90], and [S90]. The Tandem processor architecture packages CPU, memory, bus and I/O controller servers into single replaceable units as illustrated in Figure 2. These units can communicate among themselves via a dual bus called Dynabus.
Reference: [A89] <author> A. Avizienis: </author> <title> Software Fault Tolerance, </title> <booktitle> IFIP Computer Congress, </booktitle> <address> San Fran-cisco, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: Since physical faults tend to occur independently in independent processors, such faults are likely to cause minority application group failures. Experience with the systems [Ha78], [HLD88] and [Wa78] confirms that these can effectively mask the consequences of physical faults. Recent work on diverse software design <ref> [A89] </ref> pursues the goal of producing a diverse software design methodology that would ensure that groups of application servers running diverse programs do not suffer majority failures despite the existence of residual design faults in these programs. <p> Perhaps because there are no accepted techniques to estimate with confidence the increase in reliability that results from the use of diverse programming with voting, the work on design diversity has generated considerable controversy to date <ref> [A89] </ref>, [KA89]. 4.2 How is the specified software server failure semantics implemented? The problems related to the implementation of atomic transactions on persistent data despite processors with crash/performance failure semantics, disks with omission failure semantics, and communications with omission/performance communication failures have been investigated intensively for more than twenty years by
Reference: [Aa85] <author> A. Avizienis, P. Gunningberg, J. Kelly, L. Strigini, P. Traverse, K. Tso, U. Voges: </author> <title> The UCLA Dedix System: a Distributed Testbed for Multi-version Software, </title> <booktitle> 15th Int. Conf. on Fault-tolerant Computing, </booktitle> <address> Ann Arbor, Mi 1985. </address>
Reference-contexts: Some special-purpose systems do not assume that their component servers have "nice" failure semantics, such as crash/omission/performance <ref> [Aa85] </ref>, [Ha78], [HLD88] and [Wa78]. By assuming that group management services such as clock synchronization, atomic broadcast, and voting work correctly, such systems can mask a minority of server failures in application groups built from members with arbitrary failure semantics. <p> Depending on the assumptions made about the faults which cause the failures, the systems can be classified in two classes: those that attempt to tolerate only physical faults [Ha78], [HLD88] and [Wa78], and those that attempt to also tolerate design faults <ref> [Aa85] </ref>. The first class of system replicates the application servers on different, physically independent processors. Since physical faults tend to occur independently in independent processors, such faults are likely to cause minority application group failures.
Reference: [ASC85] <author> A. E. Abbadi, D. Skeen, F. Cristian: </author> <title> An Efficient Fault-Tolerant Protocol for 40 Replicated Data Management, </title> <booktitle> 4th ACM Conf. on Principles of Database Systems, </booktitle> <year> 1985. </year>
Reference-contexts: Moreover, if the group state is replicated at several members, these members need to update the replicas by using special group protocols that ensure replica consistency in the presence of process and communication failures. There already exist a significant number of protocols that have been proposed for group communication <ref> [ASC85] </ref>, [B81], [Ba89], [BD85], [BJ87], [Ca85], [CASD85], [CM84], [Cri89], [CZ85], [GS89], [KT90], [KGR89], [L89], [LG90], [LLS90], [OL88], [MMA90], [S88], [SSCA87], and [VRB89]. This area of research is presently one of the most active.
Reference: [B81] <author> J. Bartlett: </author> <title> A NonStop Kernel, </title> <booktitle> 8th Symp. on Operating System Principles, </booktitle> <month> Dec. </month> <year> 1981. </year>
Reference-contexts: For example, a user u of a primary/backup server group that sends its service requests directly to the primary might detect a primary server failure as a transient service failure and might explicitly attempt to mask the failure by re-sending the last service 10 request <ref> [B81] </ref>. Even if the service request were automatically resent for u by some underlying group communication mechanism which matches service requests with replies and automatically detects missing answers, it is likely that u contains exception handling code to deal with the situation when the entire primary/backup group fails. <p> In a coarse granularity architecture, some replaceable units package together several elementary hardware servers such as CPUs, memory, I/O controllers and communication controllers. In a fine granularity architecture, each elementary hardware server is a replaceable unit by itself. Some examples of commercially successful coarse granularity architectures are Tandem <ref> [B81] </ref>, the DEC VAX Cluster [KLS86] and the IBM MVS/XRF [IBM87]. Examples of fine granularity architectures are Stratus [TW89] and Sequoia [B88]. Other examples of fault-tolerant architectures can be found in [AL81], [La90], and [S90]. <p> The operating system uses a combination of hierarchical and group masking techniques to mask hardware resource failures from users: if one of the active hardware resources on the active path fails, then the operating system uses the other path to continue to provide service to the affected users <ref> [B81] </ref>. For example, if a software server interpreting user commands uses a disk via a certain disk controller, and the controller fails, the operating system masks the failure by re-directing further disk accesses through the other disk controller. <p> If a failed server is a primary in a group implementing an operating system service, such as disk I/O, its failure is automatically masked from higher level user servers by the promotion of its backup <ref> [B81] </ref> to the role of primary. Since user level application servers are generally not implemented by process pairs, to avoid the complications associated with programming periodic check-pointing [G86], the failure of user application servers is visible to human users which have to wait until these servers are re-started. <p> A fourth reason might be that it is not a priori clear whether low overhead groups of members with crash/performance failure semantics can provide effective tolerance to residual software design faults. Recent statistics [G86] show that software server group management mechanism designed for servers with crash/performance failure semantics <ref> [B81] </ref> can be realistically effective in masking server failures caused by hardware faults as well as residual design faults left in production quality software after extensive reviews and testing. <p> For real-time applications, if the response time required is smaller than the time needed to detect a member failure and to absorb the primary/backup processing lag, close synchronization has to be used. Examples of loosely synchronized server groups are discussed in <ref> [B81] </ref>, [Ba89], [BJ87], [CDD90], 31 [IBM87], and [OL88]. Close and loose synchronization as described above are just two end points of a continuum of synchronization policies. <p> There already exist a significant number of protocols that have been proposed for group communication [ASC85], <ref> [B81] </ref>, [Ba89], [BD85], [BJ87], [Ca85], [CASD85], [CM84], [Cri89], [CZ85], [GS89], [KT90], [KGR89], [L89], [LG90], [LLS90], [OL88], [MMA90], [S88], [SSCA87], and [VRB89]. This area of research is presently one of the most active. <p> One possible approach to enforcing a certain group availability policy, illustrated in 33 <ref> [B81] </ref> is to implement in each group member mechanisms for reaching agreement on the c- and s-rankings in the group, as well as mechanisms for detecting member failures and procedures for handling new member joins and promotions to higher s-ranks (when the group is loosely synchronized).
Reference: [B88] <author> P. Bernstein: </author> <title> Sequoia: a Fault-tolerant Tightly Coupled Multiprocessor for Transaction Processing, </title> <booktitle> IEEE Computer, </booktitle> <month> February </month> <year> 1988. </year>
Reference-contexts: In a fine granularity architecture, each elementary hardware server is a replaceable unit by itself. Some examples of commercially successful coarse granularity architectures are Tandem [B81], the DEC VAX Cluster [KLS86] and the IBM MVS/XRF [IBM87]. Examples of fine granularity architectures are Stratus [TW89] and Sequoia <ref> [B88] </ref>. Other examples of fault-tolerant architectures can be found in [AL81], [La90], and [S90]. The Tandem processor architecture packages CPU, memory, bus and I/O controller servers into single replaceable units as illustrated in Figure 2. These units can communicate among themselves via a dual bus called Dynabus.
Reference: [Ba89] <author> A. Borg, W. Blau, W. Graetsch, F. Herrmann, W. Oberle: </author> <title> Fault-Tolerance under Unix, </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> Vol. 7, No. 1, </volume> <month> Feb </month> <year> 1989. </year>
Reference-contexts: Statistical measurements of run-time overhead in practical systems confirm the general rule that the cost of group management mechanisms is higher when the failure semantics of group members is weaker: while the run-time cost of managing server-pair groups with crash/performance failure semantics can sometimes be as low as 15% <ref> [Ba89] </ref>, the cost of managing groups with arbitrary failure semantics can be as high as 80% of the total throughput of a system [PB85]. 11 Since it is more expensive to build servers with stronger failure semantics, but it is cheaper to handle the failure behavior of such servers at higher <p> For real-time applications, if the response time required is smaller than the time needed to detect a member failure and to absorb the primary/backup processing lag, close synchronization has to be used. Examples of loosely synchronized server groups are discussed in [B81], <ref> [Ba89] </ref>, [BJ87], [CDD90], 31 [IBM87], and [OL88]. Close and loose synchronization as described above are just two end points of a continuum of synchronization policies. <p> There already exist a significant number of protocols that have been proposed for group communication [ASC85], [B81], <ref> [Ba89] </ref>, [BD85], [BJ87], [Ca85], [CASD85], [CM84], [Cri89], [CZ85], [GS89], [KT90], [KGR89], [L89], [LG90], [LLS90], [OL88], [MMA90], [S88], [SSCA87], and [VRB89]. This area of research is presently one of the most active.
Reference: [BD85] <author> O. Babaoglu, R. Drumond: </author> <title> Streets of Byzantium: Network Architectures for Fast Reliable Broadcast, </title> <journal> IEEE Tr. on Software Engineering, </journal> <volume> Vol. SE-11, No. 6, </volume> <year> 1985. </year>
Reference-contexts: There already exist a significant number of protocols that have been proposed for group communication [ASC85], [B81], [Ba89], <ref> [BD85] </ref>, [BJ87], [Ca85], [CASD85], [CM84], [Cri89], [CZ85], [GS89], [KT90], [KGR89], [L89], [LG90], [LLS90], [OL88], [MMA90], [S88], [SSCA87], and [VRB89]. This area of research is presently one of the most active.
Reference: [BGMS89] <author> D. Barbara, H. Garcia-Molina, A. Spauster: </author> <title> Increasing Availability under Mutual Exclusion Constraints with Dynamic Vote Reassignment, </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> Vol. 7, No. 4, </volume> <month> Nov </month> <year> 1989. </year>
Reference-contexts: If no quorum of correct processors is present, one possibility is to ask the human operator for permission to go ahead [KLS86]. Another possibility is to attempt to modify quorums dynamically <ref> [BGMS89] </ref>. Designers of distributed fault-tolerant systems are thus faced with the following choices: attempt to ensure the existence of an upper bound d on message delays or accept unbounded message delays.
Reference: [BHG87] <author> P. Bernstein, V. Hadzilacos, N. Goodman: </author> <title> Concurrency Control and Recovery in Database Systems, </title> <publisher> Addison-Wesley, </publisher> <year> 1987. </year>
Reference-contexts: Monographs and books, such as <ref> [BHG87] </ref> and [G78] treat the subject in great detail. To separate concerns, these monographs assume that the programs used in implementing transaction atomicity are at least partially correct. <p> These servers rely on techniques such as locking, logging, disk mirroring, and atomic commit <ref> [BHG87] </ref>, [G78], and [LS81]. 4.3 How are software server failures masked? All of the commercial systems discussed earlier use hierarchical masking techniques to mask hardware and certain software server failures.
Reference: [BJ87] <author> K. Birman, T. Joseph: </author> <title> Reliable Communication in the Presence of Failures, </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> Vol. 5, No. 1, </volume> <month> February </month> <year> 1987. </year>
Reference-contexts: For real-time applications, if the response time required is smaller than the time needed to detect a member failure and to absorb the primary/backup processing lag, close synchronization has to be used. Examples of loosely synchronized server groups are discussed in [B81], [Ba89], <ref> [BJ87] </ref>, [CDD90], 31 [IBM87], and [OL88]. Close and loose synchronization as described above are just two end points of a continuum of synchronization policies. <p> There already exist a significant number of protocols that have been proposed for group communication [ASC85], [B81], [Ba89], [BD85], <ref> [BJ87] </ref>, [Ca85], [CASD85], [CM84], [Cri89], [CZ85], [GS89], [KT90], [KGR89], [L89], [LG90], [LLS90], [OL88], [MMA90], [S88], [SSCA87], and [VRB89]. This area of research is presently one of the most active. <p> According to whether they assume or not the existence of a bound d on message delays, existing membership and atomic broadcast protocols can be divided in synchronous protocols, such as [C88], [CASD85], and [Cri89], and asynchronous protocols, such as <ref> [BJ87] </ref>, [Ca85] and [CM84]. In general, synchronous protocols assume that processor clocks are synchronized within some constant maximum deviation epsilon, while asynchronous protocols do not require clocks to be synchronized. <p> Similarly, a message that was atomically broadcast to a group might need an unbounded amount of time to reach some group members. Second, most asynchronous protocols make the activities of membership computation and broadcast interfere, so that broadcasts that occur during a membership change have to be aborted <ref> [BJ87] </ref>, [Ca85], [CM84]. Third, the asynchronous protocols specifically designed to tolerate partitions require that the correctly working processors of a system form a quorum before any work can be done [CM84], [KLS86].
Reference: [C85] <author> F. Cristian: </author> <title> A Rigorous Approach to Fault-tolerant Programming, </title> <journal> IEEE Tr. on Software Eng., </journal> <volume> Vol. SE 11, No. 1, </volume> <year> 1985. </year>
Reference-contexts: upon detection of a server failure depend on the likely failure behaviors of the server, in a fault-tolerant system one has to extend the standard specification of servers to include, in addition to their familiar failure-free 6 semantics (the set of failure-free behaviors), their likely failure behaviors, or failure semantics <ref> [C85] </ref>. If the specification of a server s prescribes that the failure behaviors likely to be observed by s users should be in class F, we say that "s has F failure semantics" (a discussion of what we mean by "likely" is deferred to Section 2.6). <p> Any verification that the design satisfies u's own standard and failure functional specifications S u and F u also relies on S r and F r <ref> [C85] </ref>. <p> For example, CPU and disk controllers with crash failure semantics and disks with read/write omission failure semantics enable the implementation of a higher level stable storage service <ref> [C85] </ref>, [LS81] with write operations that are atomic with respect to crashes: any stable storage write interrupted by a crash is either carried out to completion or is not performed at all.
Reference: [C88] <author> F. Cristian: </author> <title> Agreeing on Who is Present and Who is Absent in a Synchronous Distributed System, </title> <booktitle> 18th Int Conf on Fault-Tolerant Computing, </booktitle> <address> Tokyo, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: Let r (s) be the degree of server replication for s, h (s) denote the set of host processors for s, and members denote the processor membership <ref> [C88] </ref>, that is, the set of correctly functioning processors that agree to communicate among themselves in order to collectively provide to clients all services that are enabled in the system. <p> A protocol that ensures agreement on a unique temporal sequence of successive processor group and group memberships members 1 , members 2 , ..., members i , ... is termed a processor membership protocol <ref> [C88] </ref>. <p> According to whether they assume or not the existence of a bound d on message delays, existing membership and atomic broadcast protocols can be divided in synchronous protocols, such as <ref> [C88] </ref>, [CASD85], and [Cri89], and asynchronous protocols, such as [BJ87], [Ca85] and [CM84]. In general, synchronous protocols assume that processor clocks are synchronized within some constant maximum deviation epsilon, while asynchronous protocols do not require clocks to be synchronized.
Reference: [C89] <author> F. Cristian: </author> <title> Exception Handling, in "Dependability of Resilient Computers", </title> <editor> T. Anderson, Ed., </editor> <publisher> Blackwell Scientific Publications, Oxford, </publisher> <year> 1989. </year>
Reference-contexts: In hierarchical systems relying on such servers, exception handling provides a convenient way to propagate information about failure detections across abstraction levels and to mask low level failures from higher level 8 servers <ref> [C89] </ref>. The pattern is as follows. Let i and j be two levels of abstraction, so that a server u at j depends on the service implemented by the lower level i. <p> Servers which, for any initial state and input, either provide their standard service or signal an exception without changing their state (termed "atomic with respect to exceptions" in <ref> [C89] </ref>) simplify fault-tolerant programming because they provide their users with a simple-to-understand omission failure semantics. We illustrate the hierarchical failure masking pattern described above by an IBM MVS operating system example running on a processor with several CPUs (other examples of hierarchical masking can be found in [W75]). <p> To implement atomic transaction or simply crash failure semantics, the commercial systems described previously assume that the programs that implement the operations exported by software servers are totally, or at least partially, correct <ref> [C89] </ref>. A program is totally correct if it behaves as specified in response to any input as long as the services it uses do not fail. A partially correct program may suffer a crash or a performance failure for certain inputs even when the services it uses do not fail.
Reference: [Ca85] <author> R. Carr: </author> <title> The Tandem Global Update Protocol, </title> <journal> Tandem Systems Review, </journal> <volume> Vol. 1, No. 2, </volume> <month> June </month> <year> 1985. </year>
Reference-contexts: There already exist a significant number of protocols that have been proposed for group communication [ASC85], [B81], [Ba89], [BD85], [BJ87], <ref> [Ca85] </ref>, [CASD85], [CM84], [Cri89], [CZ85], [GS89], [KT90], [KGR89], [L89], [LG90], [LLS90], [OL88], [MMA90], [S88], [SSCA87], and [VRB89]. This area of research is presently one of the most active. <p> The goal of such location-transparent naming services is to mask from clients the effects of individual server failures in a group. Several examples of fault-tolerant location-transparent name services that have been implemented in real-systems can be found in <ref> [Ca85] </ref>, [CDD90], [KLS86]. A recent survey of the issues involved in naming can be found in [CP89]. 4.3.4 How to enforce group availability policies automatically? The synchronization and replication policies defined for a service implemented by a server group constitute the availability policy for that service (or group). <p> According to whether they assume or not the existence of a bound d on message delays, existing membership and atomic broadcast protocols can be divided in synchronous protocols, such as [C88], [CASD85], and [Cri89], and asynchronous protocols, such as [BJ87], <ref> [Ca85] </ref> and [CM84]. In general, synchronous protocols assume that processor clocks are synchronized within some constant maximum deviation epsilon, while asynchronous protocols do not require clocks to be synchronized. <p> Similarly, a message that was atomically broadcast to a group might need an unbounded amount of time to reach some group members. Second, most asynchronous protocols make the activities of membership computation and broadcast interfere, so that broadcasts that occur during a membership change have to be aborted [BJ87], <ref> [Ca85] </ref>, [CM84]. Third, the asynchronous protocols specifically designed to tolerate partitions require that the correctly working processors of a system form a quorum before any work can be done [CM84], [KLS86].
Reference: [CASD85] <author> F. Cristian, H. Aghili, R. Strong, D. Dolev: </author> <title> Atomic Broadcast: From Simple Diffusion to Byzantine Agreement, </title> <booktitle> 15th Int. Conf. on Fault-tolerant Computing, </booktitle> <address> Ann Arbor, Mi 1985. </address>
Reference-contexts: If that clock is used by a higher level communication server that is specified to associate different timestamps with different messages it sends at different real times, then the communication server may be classed as experiencing an arbitrary failure <ref> [CASD85] </ref>. As illustrated above, failure propagation among servers situated at different abstraction levels of the "depends upon" hierarchy can be a complex phenomenon. <p> Other illustrations of the rule that group management cost increases as the failure semantics of group members and communication services becomes weaker are given in <ref> [CASD85] </ref> and [ES86], where families of solutions to a group communication problem are studied under increasingly weaker group member and communication failure semantics assumptions. <p> There already exist a significant number of protocols that have been proposed for group communication [ASC85], [B81], [Ba89], [BD85], [BJ87], [Ca85], <ref> [CASD85] </ref>, [CM84], [Cri89], [CZ85], [GS89], [KT90], [KGR89], [L89], [LG90], [LLS90], [OL88], [MMA90], [S88], [SSCA87], and [VRB89]. This area of research is presently one of the most active. <p> A protocol that ensures that all broadcasts originated by processors in a group members i are received in the same order by all correct processors in members i is termed an atomic broadcast protocol <ref> [CASD85] </ref>. 36 The requirements for both membership and atomic broadcast consist of a number of safety and timeliness properties. Safety properties are invariants which must be true at all points in real time. <p> According to whether they assume or not the existence of a bound d on message delays, existing membership and atomic broadcast protocols can be divided in synchronous protocols, such as [C88], <ref> [CASD85] </ref>, and [Cri89], and asynchronous protocols, such as [BJ87], [Ca85] and [CM84]. In general, synchronous protocols assume that processor clocks are synchronized within some constant maximum deviation epsilon, while asynchronous protocols do not require clocks to be synchronized.
Reference: [CDD90] <author> F. Cristian, R. Dancey, J. Dehn: </author> <title> Fault-tolerance in the Advanced Automation System, </title> <booktitle> 20th Int. Conf. on Fault-tolerant Computing, </booktitle> <institution> Newcastle upon Tyne, </institution> <address> 41 England, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Closely synchronized groups of software servers are used by all systems that attempt to tolerate arbitrary server failures, such as [Ha78], [HLD88] and [Wa78]. Examples of closely synchronized groups of members with crash/performance failure semantics are described in <ref> [CDD90] </ref> and [Co85]. A number of rules for transforming non fault-tolerant services implemented by nonredundant application programs into fault-tolerant services implemented by closely synchronized server groups have been proposed in [L84] and are discussed further in [S86]. <p> For real-time applications, if the response time required is smaller than the time needed to detect a member failure and to absorb the primary/backup processing lag, close synchronization has to be used. Examples of loosely synchronized server groups are discussed in [B81], [Ba89], [BJ87], <ref> [CDD90] </ref>, 31 [IBM87], and [OL88]. Close and loose synchronization as described above are just two end points of a continuum of synchronization policies. <p> The goal of such location-transparent naming services is to mask from clients the effects of individual server failures in a group. Several examples of fault-tolerant location-transparent name services that have been implemented in real-systems can be found in [Ca85], <ref> [CDD90] </ref>, [KLS86]. A recent survey of the issues involved in naming can be found in [CP89]. 4.3.4 How to enforce group availability policies automatically? The synchronization and replication policies defined for a service implemented by a server group constitute the availability policy for that service (or group). <p> An alternative approach, adopted in a recent distributed fault-tolerant system built for air traffic control <ref> [CDD90] </ref>, requires group members to implement only the application specific get-state and check-point (in case the group is s-ranked) procedures needed for local member state synchronization at join and check-point times.
Reference: [Cl85] <author> D. Clark: </author> <title> The Structuring of Systems using Up-calls, </title> <booktitle> 10th ACM Symp. on Operating Systems Principles, </booktitle> <year> 1985. </year>
Reference-contexts: When the I/O is completed, the I/O server will typically notify the file server f by using an "up-call" <ref> [Cl85] </ref> which might interrupt f. If a processor p interprets the programs of f, s, and d these "depend" on p (although p "executes" 4 them). A distributed system consists of software servers which depend on processor and communication services.
Reference: [CM84] <author> J.M. Chang, N. Maxemchuck: </author> <title> Reliable Broadcast Protocols, </title> <journal> ACM Tr. on Computer Systems, </journal> <volume> Vol. 2, No. 3, </volume> <month> August </month> <year> 1984. </year>
Reference-contexts: There already exist a significant number of protocols that have been proposed for group communication [ASC85], [B81], [Ba89], [BD85], [BJ87], [Ca85], [CASD85], <ref> [CM84] </ref>, [Cri89], [CZ85], [GS89], [KT90], [KGR89], [L89], [LG90], [LLS90], [OL88], [MMA90], [S88], [SSCA87], and [VRB89]. This area of research is presently one of the most active. <p> According to whether they assume or not the existence of a bound d on message delays, existing membership and atomic broadcast protocols can be divided in synchronous protocols, such as [C88], [CASD85], and [Cri89], and asynchronous protocols, such as [BJ87], [Ca85] and <ref> [CM84] </ref>. In general, synchronous protocols assume that processor clocks are synchronized within some constant maximum deviation epsilon, while asynchronous protocols do not require clocks to be synchronized. <p> Second, most asynchronous protocols make the activities of membership computation and broadcast interfere, so that broadcasts that occur during a membership change have to be aborted [BJ87], [Ca85], <ref> [CM84] </ref>. Third, the asynchronous protocols specifically designed to tolerate partitions require that the correctly working processors of a system form a quorum before any work can be done [CM84], [KLS86]. <p> activities of membership computation and broadcast interfere, so that broadcasts that occur during a membership change have to be aborted [BJ87], [Ca85], <ref> [CM84] </ref>. Third, the asynchronous protocols specifically designed to tolerate partitions require that the correctly working processors of a system form a quorum before any work can be done [CM84], [KLS86]. This requirement, needed to prevent divergence among processors in distinct partitions, affects adversely system availability, since a quorum of processors needs to be functioning before any work can be done.
Reference: [Co85] <author> E. Cooper: </author> <title> Replicated Distributed Programs, </title> <type> PhD thesis, </type> <institution> UC Berkeley, </institution> <year> 1985. </year>
Reference-contexts: Closely synchronized groups of software servers are used by all systems that attempt to tolerate arbitrary server failures, such as [Ha78], [HLD88] and [Wa78]. Examples of closely synchronized groups of members with crash/performance failure semantics are described in [CDD90] and <ref> [Co85] </ref>. A number of rules for transforming non fault-tolerant services implemented by nonredundant application programs into fault-tolerant services implemented by closely synchronized server groups have been proposed in [L84] and are discussed further in [S86].
Reference: [CP85] <author> D. Comer, L. Peterson: </author> <title> Understanding Naming in Distributed Systems, </title> <journal> Distributed Computing, </journal> <volume> Vol. 3, </volume> <pages> pp. 51-60, </pages> <year> 1989. </year>
Reference: [Cr89] <author> F. Cristian: </author> <title> Probabilistic Clock Synchronization, </title> <journal> Distributed Computing, </journal> <volume> Vol. 3, </volume> <pages> pp. 146-158, </pages> <year> 1989. </year>
Reference-contexts: This will ensure strong timeliness properties and will eliminate the need to worry about quorums. Moreover, if d' is sufficiently large compared to the median message delay, it is likely that few messages delays will exceed d' time units <ref> [Cr89] </ref>, and transient partitions will therefore be rare. On the other hand, if the cost of compensating for inconsistent actions taken as a consequence of transient partition occurrences is higher than the cost of missing recovery deadlines, one can adopt an asynchronous approach with timeout delay d'.
Reference: [Cri89] <author> F. Cristian: </author> <title> Synchronous Atomic Broadcast for Redundant Broadcast Channels, </title> <type> IBM Research Report RJ 7203, </type> <month> Dec </month> <year> 1989. </year>
Reference-contexts: There already exist a significant number of protocols that have been proposed for group communication [ASC85], [B81], [Ba89], [BD85], [BJ87], [Ca85], [CASD85], [CM84], <ref> [Cri89] </ref>, [CZ85], [GS89], [KT90], [KGR89], [L89], [LG90], [LLS90], [OL88], [MMA90], [S88], [SSCA87], and [VRB89]. This area of research is presently one of the most active. <p> According to whether they assume or not the existence of a bound d on message delays, existing membership and atomic broadcast protocols can be divided in synchronous protocols, such as [C88], [CASD85], and <ref> [Cri89] </ref>, and asynchronous protocols, such as [BJ87], [Ca85] and [CM84]. In general, synchronous protocols assume that processor clocks are synchronized within some constant maximum deviation epsilon, while asynchronous protocols do not require clocks to be synchronized.
Reference: [CZ85] <author> D. Cheriton, W. Zwaenepoel: </author> <title> Distributed Process Groups in the V Kernel, </title> <journal> ACM Tr. on Comp. Systems, </journal> <volume> Vol. 3, No. 2, </volume> <month> May </month> <year> 1985. </year>
Reference-contexts: There already exist a significant number of protocols that have been proposed for group communication [ASC85], [B81], [Ba89], [BD85], [BJ87], [Ca85], [CASD85], [CM84], [Cri89], <ref> [CZ85] </ref>, [GS89], [KT90], [KGR89], [L89], [LG90], [LLS90], [OL88], [MMA90], [S88], [SSCA87], and [VRB89]. This area of research is presently one of the most active.
Reference: [D71] <author> E. Dijkstra: </author> <title> Hierarchical Ordering of Sequential Processes, </title> <journal> Acta Informatica, </journal> <volume> Vol 1, </volume> <pages> pp. 115-138, </pages> <year> 1971. </year>
Reference-contexts: This relation is often represented as an acyclic graph in which nodes denote servers and arrows represent the "depends" relation. Since it is customary to represent graphically a user u of a resource r above r, u is said to be at a level of abstraction "higher" than r <ref> [D71] </ref>, [P79], [R75]. For example, a file server f, which uses the services provided by a disk space allocation server s and a disk I/O server d to provide file creation, access, update, and deletion service, depends on s and d (see figure 1).
Reference: [ES86] <author> P. Ezhilchelvan, S. Shrivastava: </author> <title> A Characterization of Faults in Systems, </title> <booktitle> 5th Symp. on Reliability in Dist. Softw. and Database systems, </booktitle> <address> Los Angeles, </address> <month> January </month> <year> 1986. </year>
Reference-contexts: Other illustrations of the rule that group management cost increases as the failure semantics of group members and communication services becomes weaker are given in [CASD85] and <ref> [ES86] </ref>, where families of solutions to a group communication problem are studied under increasingly weaker group member and communication failure semantics assumptions.
Reference: [G78] <author> J. Gray: </author> <booktitle> Notes on Database Operating Systems, Operating Systems An Advanced Course, Lecture Notes in Computer Science, Springer Verlag, </booktitle> <volume> Vol 60, </volume> <year> 1978. </year>
Reference-contexts: Such stable storage service can then be used by other higher level servers to implement atomic transactions by relying on known database recovery techniques such as write-ahead logging and two phase commit <ref> [G78] </ref>. Similarly, lower level data gram communication services with omission/performance failure semantics enable the implementation of higher level virtual-circuits with crash failure semantics [T81]. <p> An atomic transaction, which is a sequence of operations on persistent data usually stored in databases, must change a consistent database state into another consistent database state or must not change the database state at all when a lower level processor service crash occurs <ref> [G78] </ref>, [LS81]. For software servers, such as low level I/O and communication controllers which typically do not have persistent state, one is usually content with amnesia crash failure semantics: after a failure these servers must properly re-initialize themselves and accept new service requests. <p> Monographs and books, such as [BHG87] and <ref> [G78] </ref> treat the subject in great detail. To separate concerns, these monographs assume that the programs used in implementing transaction atomicity are at least partially correct. <p> These servers rely on techniques such as locking, logging, disk mirroring, and atomic commit [BHG87], <ref> [G78] </ref>, and [LS81]. 4.3 How are software server failures masked? All of the commercial systems discussed earlier use hierarchical masking techniques to mask hardware and certain software server failures.
Reference: [G86] <author> J. Gray: </author> <title> Why do computers stop and what can be done about it? 5th Symp. on Reliability in Dist. Softw. and Database systems, </title> <address> Los Angeles, </address> <month> January </month> <year> 1986. </year>
Reference-contexts: Since user level application servers are generally not implemented by process pairs, to avoid the complications associated with programming periodic check-pointing <ref> [G86] </ref>, the failure of user application servers is visible to human users which have to wait until these servers are re-started. <p> The duplication of hardware resources needed by software servers makes the Tandem architecture single-fault tolerant: any single hardware replaceable unit failure can be tolerated, provided no second replaceable unit failure occurs. As reported in <ref> [G86] </ref>, the use of server pair groups to implement operating system services also enables the masking of a significant fraction of the operating system server failures caused by residual software design faults. <p> Although the choice of masking hardware server failures at the operating system level can provide tolerance not only to hardware, but also to operating system server failures <ref> [G86] </ref>, it does not solve the problem of how to ensure fault-tolerance of application services. The use of redundant application software server groups allows both hardware, operating system and software server failures to be masked at the highest level of abstraction: the application level. <p> A fourth reason might be that it is not a priori clear whether low overhead groups of members with crash/performance failure semantics can provide effective tolerance to residual software design faults. Recent statistics <ref> [G86] </ref> show that software server group management mechanism designed for servers with crash/performance failure semantics [B81] can be realistically effective in masking server failures caused by hardware faults as well as residual design faults left in production quality software after extensive reviews and testing. An example reported in [G86], which is <p> Recent statistics <ref> [G86] </ref> show that software server group management mechanism designed for servers with crash/performance failure semantics [B81] can be realistically effective in masking server failures caused by hardware faults as well as residual design faults left in production quality software after extensive reviews and testing. An example reported in [G86], which is based on a sample set of 2000 Tandem systems representing over 10 million system hours, indicates that for the primary/backup spooling process group used in Tandem's distributed operating system, only 0.7% of the failures affecting the group were double server failures, that is, group failures.
Reference: [GS89] <author> H. Garcia-Molina, A. Spauster: </author> <title> Message Ordering in a Multicast Environment, </title> <booktitle> 9th Int. Conf. on Distributed Systems, </booktitle> <address> Newport Beach, California, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: There already exist a significant number of protocols that have been proposed for group communication [ASC85], [B81], [Ba89], [BD85], [BJ87], [Ca85], [CASD85], [CM84], [Cri89], [CZ85], <ref> [GS89] </ref>, [KT90], [KGR89], [L89], [LG90], [LLS90], [OL88], [MMA90], [S88], [SSCA87], and [VRB89]. This area of research is presently one of the most active.
Reference: [Ha78] <author> A. Hopkins, B. Smith, J. Lala: </author> <title> FTMP-A highly reliable fault-tolerant multiprocessor for aircraft, </title> <journal> Proceedings IEEE, </journal> <volume> Vol. 66, </volume> <month> Oct </month> <year> 1978. </year>
Reference-contexts: Examples of processor architectures designed for highly critical 22 application areas were, because of ultra-high stochastic dependability requirements, designers had to assume that their elementary hardware servers have arbitrary failure semantics can be found in <ref> [Ha78] </ref>, [HLD88], and [Wa78]. 3.3 How is the specified hardware failure semantics imple mented? In order to detect failures in buses, communication lines, memory and disk servers, all the previously discussed architectures use error detecting codes. This hardware failure detection technique is well understood. <p> the hardware apparently has worked properly. 3.4 How are replaceable hardware unit failures masked? It is possible to implement the redundancy management mechanisms which mask hardware server failures directly in hardware, for example by using group masking techniques such as triplexing physical hardware servers with arbitrary failure semantics and voting <ref> [Ha78] </ref> or duplexing hardware servers with crash failure semantics (which in their turn can be implemented by server pairs based on duplication and matching [TW89]). <p> Some special-purpose systems do not assume that their component servers have "nice" failure semantics, such as crash/omission/performance [Aa85], <ref> [Ha78] </ref>, [HLD88] and [Wa78]. By assuming that group management services such as clock synchronization, atomic broadcast, and voting work correctly, such systems can mask a minority of server failures in application groups built from members with arbitrary failure semantics. <p> Depending on the assumptions made about the faults which cause the failures, the systems can be classified in two classes: those that attempt to tolerate only physical faults <ref> [Ha78] </ref>, [HLD88] and [Wa78], and those that attempt to also tolerate design faults [Aa85]. The first class of system replicates the application servers on different, physically independent processors. Since physical faults tend to occur independently in independent processors, such faults are likely to cause minority application group failures. <p> The first class of system replicates the application servers on different, physically independent processors. Since physical faults tend to occur independently in independent processors, such faults are likely to cause minority application group failures. Experience with the systems <ref> [Ha78] </ref>, [HLD88] and [Wa78] confirms that these can effectively mask the consequences of physical faults. <p> Closely synchronized groups of software servers are used by all systems that attempt to tolerate arbitrary server failures, such as <ref> [Ha78] </ref>, [HLD88] and [Wa78]. Examples of closely synchronized groups of members with crash/performance failure semantics are described in [CDD90] and [Co85].
Reference: [HLD88] <author> R. Harper, J. Lala, J. Deyst: </author> <title> Fault Tolerant Parallel Processor Architecture Overview, </title> <booktitle> 18th Int Conf on Fault-Tolerant Computing, </booktitle> <address> Tokyo, </address> <month> June </month> <year> 1988. </year> <month> 42 </month>
Reference-contexts: Examples of processor architectures designed for highly critical 22 application areas were, because of ultra-high stochastic dependability requirements, designers had to assume that their elementary hardware servers have arbitrary failure semantics can be found in [Ha78], <ref> [HLD88] </ref>, and [Wa78]. 3.3 How is the specified hardware failure semantics imple mented? In order to detect failures in buses, communication lines, memory and disk servers, all the previously discussed architectures use error detecting codes. This hardware failure detection technique is well understood. <p> Some special-purpose systems do not assume that their component servers have "nice" failure semantics, such as crash/omission/performance [Aa85], [Ha78], <ref> [HLD88] </ref> and [Wa78]. By assuming that group management services such as clock synchronization, atomic broadcast, and voting work correctly, such systems can mask a minority of server failures in application groups built from members with arbitrary failure semantics. <p> Depending on the assumptions made about the faults which cause the failures, the systems can be classified in two classes: those that attempt to tolerate only physical faults [Ha78], <ref> [HLD88] </ref> and [Wa78], and those that attempt to also tolerate design faults [Aa85]. The first class of system replicates the application servers on different, physically independent processors. Since physical faults tend to occur independently in independent processors, such faults are likely to cause minority application group failures. <p> The first class of system replicates the application servers on different, physically independent processors. Since physical faults tend to occur independently in independent processors, such faults are likely to cause minority application group failures. Experience with the systems [Ha78], <ref> [HLD88] </ref> and [Wa78] confirms that these can effectively mask the consequences of physical faults. <p> Closely synchronized groups of software servers are used by all systems that attempt to tolerate arbitrary server failures, such as [Ha78], <ref> [HLD88] </ref> and [Wa78]. Examples of closely synchronized groups of members with crash/performance failure semantics are described in [CDD90] and [Co85].
Reference: [IBM87] <institution> IBM International Technical Support Centers: </institution> <note> IMS/VS Extended Recovery Facility (XRF): Technical Reference, </note> <year> 1987. </year>
Reference-contexts: In a fine granularity architecture, each elementary hardware server is a replaceable unit by itself. Some examples of commercially successful coarse granularity architectures are Tandem [B81], the DEC VAX Cluster [KLS86] and the IBM MVS/XRF <ref> [IBM87] </ref>. Examples of fine granularity architectures are Stratus [TW89] and Sequoia [B88]. Other examples of fault-tolerant architectures can be found in [AL81], [La90], and [S90]. The Tandem processor architecture packages CPU, memory, bus and I/O controller servers into single replaceable units as illustrated in Figure 2. <p> For real-time applications, if the response time required is smaller than the time needed to detect a member failure and to absorb the primary/backup processing lag, close synchronization has to be used. Examples of loosely synchronized server groups are discussed in [B81], [Ba89], [BJ87], [CDD90], 31 <ref> [IBM87] </ref>, and [OL88]. Close and loose synchronization as described above are just two end points of a continuum of synchronization policies.
Reference: [JZ87] <author> D. Johnson, W. Zwaenepoel: </author> <title> Sender Based Message Logging, </title> <booktitle> 17th Int Conf on Fault-Tolerant Computing, </booktitle> <address> Tokyo, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: Variants of this check-pointing rollback masking technique have also been developed for the case when the primary server is implemented by a set of distributed processes which check-point and log service requests independently <ref> [JZ87] </ref>, [KT87], [SY85]. For groups composed of servers with performance failure semantics, the main advantage of loose over close synchronization is that only primary servers make full use of their share of the required replicated service resources, while backups make only a reduced use.
Reference: [KA89] <author> J. Knight, P. Amann: </author> <title> Issues Influencing the Use of N-version Programming, </title> <booktitle> Proceedings IFIP Congress, </booktitle> <address> San Francisco, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: Perhaps because there are no accepted techniques to estimate with confidence the increase in reliability that results from the use of diverse programming with voting, the work on design diversity has generated considerable controversy to date [A89], <ref> [KA89] </ref>. 4.2 How is the specified software server failure semantics implemented? The problems related to the implementation of atomic transactions on persistent data despite processors with crash/performance failure semantics, disks with omission failure semantics, and communications with omission/performance communication failures have been investigated intensively for more than twenty years by researchers
Reference: [KT90] <author> F. Kaashoek, A. Tanenbaum: </author> <title> Fault-tolerance Using Group Communication, </title> <booktitle> 4th ACM SIGOPS European Workshop, </booktitle> <address> Bologna, </address> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: There already exist a significant number of protocols that have been proposed for group communication [ASC85], [B81], [Ba89], [BD85], [BJ87], [Ca85], [CASD85], [CM84], [Cri89], [CZ85], [GS89], <ref> [KT90] </ref>, [KGR89], [L89], [LG90], [LLS90], [OL88], [MMA90], [S88], [SSCA87], and [VRB89]. This area of research is presently one of the most active.
Reference: [KGR89] <author> H. Kopetz, G. Grunsteidl, J. Reisinger: </author> <title> Fault-tolerant Membership in a Synchronous Real-time System, </title> <booktitle> IFIP Working Conference on "Dependable Computing for Critical Applications", </booktitle> <address> Santa Barbara, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: There already exist a significant number of protocols that have been proposed for group communication [ASC85], [B81], [Ba89], [BD85], [BJ87], [Ca85], [CASD85], [CM84], [Cri89], [CZ85], [GS89], [KT90], <ref> [KGR89] </ref>, [L89], [LG90], [LLS90], [OL88], [MMA90], [S88], [SSCA87], and [VRB89]. This area of research is presently one of the most active.
Reference: [KLS86] <author> N. Kronenberg, H. Levy, W. Strecker: VAXclusters: </author> <title> A Closely-Coupled Distributed System, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 4, No. 2, </volume> <year> 1986. </year>
Reference-contexts: In a fine granularity architecture, each elementary hardware server is a replaceable unit by itself. Some examples of commercially successful coarse granularity architectures are Tandem [B81], the DEC VAX Cluster <ref> [KLS86] </ref> and the IBM MVS/XRF [IBM87]. Examples of fine granularity architectures are Stratus [TW89] and Sequoia [B88]. Other examples of fault-tolerant architectures can be found in [AL81], [La90], and [S90]. <p> The goal of such location-transparent naming services is to mask from clients the effects of individual server failures in a group. Several examples of fault-tolerant location-transparent name services that have been implemented in real-systems can be found in [Ca85], [CDD90], <ref> [KLS86] </ref>. A recent survey of the issues involved in naming can be found in [CP89]. 4.3.4 How to enforce group availability policies automatically? The synchronization and replication policies defined for a service implemented by a server group constitute the availability policy for that service (or group). <p> Third, the asynchronous protocols specifically designed to tolerate partitions require that the correctly working processors of a system form a quorum before any work can be done [CM84], <ref> [KLS86] </ref>. This requirement, needed to prevent divergence among processors in distinct partitions, affects adversely system availability, since a quorum of processors needs to be functioning before any work can be done. <p> If no quorum of correct processors is present, one possibility is to ask the human operator for permission to go ahead <ref> [KLS86] </ref>. Another possibility is to attempt to modify quorums dynamically [BGMS89]. Designers of distributed fault-tolerant systems are thus faced with the following choices: attempt to ensure the existence of an upper bound d on message delays or accept unbounded message delays.
Reference: [KT87] <author> R. Koo, S. Toueg: </author> <title> Check-pointing and Rollback Recovery for Distributed Systems, </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. SE-13, No. 1, </volume> <year> 1986. </year>
Reference-contexts: Variants of this check-pointing rollback masking technique have also been developed for the case when the primary server is implemented by a set of distributed processes which check-point and log service requests independently [JZ87], <ref> [KT87] </ref>, [SY85]. For groups composed of servers with performance failure semantics, the main advantage of loose over close synchronization is that only primary servers make full use of their share of the required replicated service resources, while backups make only a reduced use.
Reference: [L84] <author> L. Lamport: </author> <title> Using Time Instead of Time-outs in Fault-Tolerant Systems, </title> <journal> ACM Trans on Programming Languages and Systems, </journal> <volume> vol. 6, no. 2, </volume> <year> 1984. </year>
Reference-contexts: Examples of closely synchronized groups of members with crash/performance failure semantics are described in [CDD90] and [Co85]. A number of rules for transforming non fault-tolerant services implemented by nonredundant application programs into fault-tolerant services implemented by closely synchronized server groups have been proposed in <ref> [L84] </ref> and are discussed further in [S86].
Reference: [L89] <author> L. Lamport: </author> <title> The Part Time Parliament, </title> <note> DEC SRC Report 49, </note> <month> Sept </month> <year> 1989. </year>
Reference-contexts: There already exist a significant number of protocols that have been proposed for group communication [ASC85], [B81], [Ba89], [BD85], [BJ87], [Ca85], [CASD85], [CM84], [Cri89], [CZ85], [GS89], [KT90], [KGR89], <ref> [L89] </ref>, [LG90], [LLS90], [OL88], [MMA90], [S88], [SSCA87], and [VRB89]. This area of research is presently one of the most active.
Reference: [La89] <author> J. C. Laprie: </author> <title> Dependability: A unifying concept for reliable computing and fault-tolerance, </title> <editor> T. Anderson, Ed., </editor> <publisher> Blackwell Scientific Publications, Oxford, </publisher> <year> 1989. </year>
Reference: [La90] <author> J. C. Laprie, J. Arlat, C. Beounes, K. Kanoun: </author> <title> Definition and Analysis of Hardware and Software-Fault-Tolerant Architectures, </title> <booktitle> IEEE Computer, </booktitle> <month> July </month> <year> 1990. </year>
Reference-contexts: Some examples of commercially successful coarse granularity architectures are Tandem [B81], the DEC VAX Cluster [KLS86] and the IBM MVS/XRF [IBM87]. Examples of fine granularity architectures are Stratus [TW89] and Sequoia [B88]. Other examples of fault-tolerant architectures can be found in [AL81], <ref> [La90] </ref>, and [S90]. The Tandem processor architecture packages CPU, memory, bus and I/O controller servers into single replaceable units as illustrated in Figure 2. These units can communicate among themselves via a dual bus called Dynabus. Disk, tape, communication, 15 and terminal controller servers are field replaceable units by themselves.
Reference: [LG90] <author> S. Luan, V. Gligor: </author> <title> A Fault-tolerant Protocol for Atomic Broadcast, </title> <booktitle> 10th Int Conf on Distributed Computing Systems, </booktitle> <address> Paris, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: There already exist a significant number of protocols that have been proposed for group communication [ASC85], [B81], [Ba89], [BD85], [BJ87], [Ca85], [CASD85], [CM84], [Cri89], [CZ85], [GS89], [KT90], [KGR89], [L89], <ref> [LG90] </ref>, [LLS90], [OL88], [MMA90], [S88], [SSCA87], and [VRB89]. This area of research is presently one of the most active.
Reference: [LL89] <author> G. Le Lann: </author> <title> Critical Issues in Distributed Real-Time Computing, </title> <booktitle> Proceedings of "ESTEC Workshop on Communication Networks and Distributed Operating Systems within the Space Environment", European Space Agency Report WPP-10, Noordwijk, </booktitle> <address> Oct.24-26, </address> <year> 1989. </year>
Reference-contexts: To ensure that a local area network has omission failure semantics, one typically uses network access mechanisms that guarantee bounded access delays and real-time executives that guarantee upper bounds on message transmission and processing delays <ref> [LL89] </ref>. <p> More design effort is required to build a real-time operating system that provides to software servers processor service with crash failure semantics, than to build a standard multi-user operating system, such as Unix, which provides processor service with only crash/performance failure semantics <ref> [LL89] </ref>. 2.4 Hierarchical failure propagation and masking A failure behavior can be classified only with respect to a certain server specification, at a certain level of abstraction.
Reference: [LLS90] <author> R. Ladin, B. Liskov, L. Shrira: </author> <title> Lazy Replication: a Method for Managing Replicated Data, </title> <booktitle> 9th Annual ACM Symposium on Principles of Distributed Comput 43 ing, </booktitle> <month> August </month> <year> 1990. </year>
Reference-contexts: There already exist a significant number of protocols that have been proposed for group communication [ASC85], [B81], [Ba89], [BD85], [BJ87], [Ca85], [CASD85], [CM84], [Cri89], [CZ85], [GS89], [KT90], [KGR89], [L89], [LG90], <ref> [LLS90] </ref>, [OL88], [MMA90], [S88], [SSCA87], and [VRB89]. This area of research is presently one of the most active.
Reference: [LS81] <author> B. Lampson, H. Sturgis: </author> <title> Atomic Transactions, in "Distributed Systems: An Advanced Course", </title> <booktitle> Lecture Notes in Computer Science Vol. </booktitle> <volume> 105, </volume> <publisher> Springer Verlag, </publisher> <year> 1981. </year>
Reference-contexts: All previously mentioned systems make these assumptions. Such strong hardware failure semantics enable system designers to use known hierarchical masking techniques to mask hardware server failures, such as storage du-plexing to mask loss of data replicated on two memory or disk servers with read/write omission failure semantics <ref> [LS81] </ref> or virtual-circuits to mask omission or performance communication failures by using time-outs, sequence numbers, acknowledgements and retries [T81]. <p> For example, CPU and disk controllers with crash failure semantics and disks with read/write omission failure semantics enable the implementation of a higher level stable storage service [C85], <ref> [LS81] </ref> with write operations that are atomic with respect to crashes: any stable storage write interrupted by a crash is either carried out to completion or is not performed at all. <p> An atomic transaction, which is a sequence of operations on persistent data usually stored in databases, must change a consistent database state into another consistent database state or must not change the database state at all when a lower level processor service crash occurs [G78], <ref> [LS81] </ref>. For software servers, such as low level I/O and communication controllers which typically do not have persistent state, one is usually content with amnesia crash failure semantics: after a failure these servers must properly re-initialize themselves and accept new service requests. <p> These servers rely on techniques such as locking, logging, disk mirroring, and atomic commit [BHG87], [G78], and <ref> [LS81] </ref>. 4.3 How are software server failures masked? All of the commercial systems discussed earlier use hierarchical masking techniques to mask hardware and certain software server failures.
Reference: [M82] <author> E. McCluskey: </author> <title> Fault-Tolerant Systems, </title> <institution> Technical Report CSL-199 Stanford University, </institution> <year> 1982. </year>
Reference-contexts: Indeed, while for CPUs and I/O controllers based on error detecting codes there is a possibility that the data written to a bus or storage during the last "few" cycles before a failure detection is erroneous, duplication and matching by using self-checking <ref> [M82] </ref> comparator circuits virtually eliminates the possibility of such damage. The cost for this excellent failure detection capability is that two physical hardware servers plus the comparison logic are needed instead of only one elementary server augmented with error detecting circuitry.
Reference: [MMA90] <author> M. Melliar-Smith, L. Moser, V. Agrawala: </author> <title> Broadcast Protocols for Distributed Systems, </title> <journal> IEEE Tr on Parallel and Distributed Systems, </journal> <volume> Vol. 1, No. 1, </volume> <month> Jan </month> <year> 1990. </year>
Reference-contexts: There already exist a significant number of protocols that have been proposed for group communication [ASC85], [B81], [Ba89], [BD85], [BJ87], [Ca85], [CASD85], [CM84], [Cri89], [CZ85], [GS89], [KT90], [KGR89], [L89], [LG90], [LLS90], [OL88], <ref> [MMA90] </ref>, [S88], [SSCA87], and [VRB89]. This area of research is presently one of the most active.
Reference: [OL88] <author> B. Oki, B. Liskov: </author> <title> Viewstamped Replication: a New Primary Copy Method to Support Highly Available Distributed Systems, </title> <booktitle> 7th ACM Symp. on Principles of Distributed Computing, </booktitle> <month> August </month> <year> 1988. </year>
Reference-contexts: For real-time applications, if the response time required is smaller than the time needed to detect a member failure and to absorb the primary/backup processing lag, close synchronization has to be used. Examples of loosely synchronized server groups are discussed in [B81], [Ba89], [BJ87], [CDD90], 31 [IBM87], and <ref> [OL88] </ref>. Close and loose synchronization as described above are just two end points of a continuum of synchronization policies. <p> There already exist a significant number of protocols that have been proposed for group communication [ASC85], [B81], [Ba89], [BD85], [BJ87], [Ca85], [CASD85], [CM84], [Cri89], [CZ85], [GS89], [KT90], [KGR89], [L89], [LG90], [LLS90], <ref> [OL88] </ref>, [MMA90], [S88], [SSCA87], and [VRB89]. This area of research is presently one of the most active.
Reference: [P79] <author> D. Parnas: </author> <title> Designing Software for Ease of Extension and Contraction, </title> <journal> IEEE Tr. on Software Engineering, </journal> <volume> Vol. SE-5, No. 2, </volume> <month> March </month> <year> 1979. </year>
Reference-contexts: Since it is customary to represent graphically a user u of a resource r above r, u is said to be at a level of abstraction "higher" than r [D71], <ref> [P79] </ref>, [R75]. For example, a file server f, which uses the services provided by a disk space allocation server s and a disk I/O server d to provide file creation, access, update, and deletion service, depends on s and d (see figure 1).
Reference: [Pa89] <author> D. Powell: </author> <title> La Tolerance aux Fautes Dans les Systemes Repartis: Les Hypotheses d'Erreur et leur Importance, </title> <type> LAAS Research report 89-258, </type> <month> September, </month> <year> 1989. </year>
Reference: [PB85] <author> D. Palumbo, R. Butler: </author> <title> Measurement of SIFT operating system overhead, </title> <type> NASA Technical Memo 86322, </type> <year> 1985. </year>
Reference-contexts: the failure semantics of group members is weaker: while the run-time cost of managing server-pair groups with crash/performance failure semantics can sometimes be as low as 15% [Ba89], the cost of managing groups with arbitrary failure semantics can be as high as 80% of the total throughput of a system <ref> [PB85] </ref>. 11 Since it is more expensive to build servers with stronger failure semantics, but it is cheaper to handle the failure behavior of such servers at higher levels of abstraction, a key issue in designing multi-layered fault-tolerant systems is how to balance the amounts of failure detection, recovery, and masking
Reference: [PW72] <author> W. Peterson, E. Weldon: </author> <title> Error Correcting Codes, </title> <publisher> MIT Press, </publisher> <address> Massachusetts, </address> <year> 1972. </year>
Reference-contexts: This hardware failure detection technique is well understood. There is a rich literature specializing in error detecting codes and this subject seems to have reached a fairly mature state <ref> [PW72] </ref>, [W78]. To detect failures in hardware servers, such as CPU, I/O and communication controllers, systems such as IBM, VAX and high-end Tandem use error detecting codes while newer systems such as Stratus, Sequoia, Tandem CLX and DEC VAXft 3000 use lock-step duplication with comparison.
Reference: [R75] <author> B. Randell: </author> <title> System Structure for Software Fault-Tolerance, </title> <journal> IEEE Trans. on Software Eng., </journal> <volume> Vol. SE-1, No. 2, </volume> <year> 1975. </year>
Reference-contexts: Since it is customary to represent graphically a user u of a resource r above r, u is said to be at a level of abstraction "higher" than r [D71], [P79], <ref> [R75] </ref>. For example, a file server f, which uses the services provided by a disk space allocation server s and a disk I/O server d to provide file creation, access, update, and deletion service, depends on s and d (see figure 1).
Reference: [S90] <author> D. Siewiorek: </author> <title> Fault-tolerance in Commercial Computers, </title> <booktitle> IEEE Computer, </booktitle> <month> July </month> <year> 1990. </year>
Reference-contexts: Some examples of commercially successful coarse granularity architectures are Tandem [B81], the DEC VAX Cluster [KLS86] and the IBM MVS/XRF [IBM87]. Examples of fine granularity architectures are Stratus [TW89] and Sequoia [B88]. Other examples of fault-tolerant architectures can be found in [AL81], [La90], and <ref> [S90] </ref>. The Tandem processor architecture packages CPU, memory, bus and I/O controller servers into single replaceable units as illustrated in Figure 2. These units can communicate among themselves via a dual bus called Dynabus. Disk, tape, communication, 15 and terminal controller servers are field replaceable units by themselves.
Reference: [S86] <author> F. Schneider: </author> <title> The State Machine Approach: a tutorial, </title> <type> TR 86-800 Cornell Univ., </type> <year> 1986. </year>
Reference-contexts: A number of rules for transforming non fault-tolerant services implemented by nonredundant application programs into fault-tolerant services implemented by closely synchronized server groups have been proposed in [L84] and are discussed further in <ref> [S86] </ref>.
Reference: [S88] <author> F. Schmuck: </author> <title> The Use of Efficient Broadcast Protocols in Asynchronous Distributed Systems, </title> <type> PhD thesis, </type> <institution> TR88-928 Cornell Univ., </institution> <year> 1988. </year>
Reference-contexts: There already exist a significant number of protocols that have been proposed for group communication [ASC85], [B81], [Ba89], [BD85], [BJ87], [Ca85], [CASD85], [CM84], [Cri89], [CZ85], [GS89], [KT90], [KGR89], [L89], [LG90], [LLS90], [OL88], [MMA90], <ref> [S88] </ref>, [SSCA87], and [VRB89]. This area of research is presently one of the most active.
Reference: [SRC84] <author> J. Saltzer, D. Reed, D. Clark: </author> <title> End-to-end Arguments in System Design, </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> Vol. 2, No. 4, </volume> <month> Nov, </month> <year> 1984. </year> <month> 44 </month>
Reference-contexts: Similar cost/effectiveness "end-to-end" arguments in layered implementations of fault-tolerant communication services have been discussed in <ref> [SRC84] </ref>. 2.6 On choosing the right failure semantics When is the probability that a server r suffers failures outside a given failure class F small enough to be considered "negligible"? In other terms, when is it justified to assume that the only "likely" failure behaviors of r are in class F?
Reference: [SSCA87] <author> R. Strong, D. Skeen, F. Cristian, H. Aghili: </author> <title> Handshake Protocols, </title> <booktitle> 7th Int. Conf. on Distributed Computing Systems, </booktitle> <address> Berlin, </address> <month> September </month> <year> 1987. </year>
Reference-contexts: There already exist a significant number of protocols that have been proposed for group communication [ASC85], [B81], [Ba89], [BD85], [BJ87], [Ca85], [CASD85], [CM84], [Cri89], [CZ85], [GS89], [KT90], [KGR89], [L89], [LG90], [LLS90], [OL88], [MMA90], [S88], <ref> [SSCA87] </ref>, and [VRB89]. This area of research is presently one of the most active. <p> For example, after transient partition occurrences, servers might disagree on the order of messages broadcast and on group membership. Protocols for detecting transient partitions and reconciling diverging server views have been investigated in <ref> [SSCA87] </ref>. Such a posteriori partition detection and recovery protocols need to compensate for any damage done while disagreement existed among group members. In contrast with synchronous protocols, one can build asynchronous protocols that never violate their safety requirements, even when communication delays are unbounded and communication partitions occur.
Reference: [SY85] <author> R. Strom, S. Yemini: </author> <title> Optimistic Recovery in Distributed Systems, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 3, No. 3, </volume> <year> 1985. </year>
Reference-contexts: Variants of this check-pointing rollback masking technique have also been developed for the case when the primary server is implemented by a set of distributed processes which check-point and log service requests independently [JZ87], [KT87], <ref> [SY85] </ref>. For groups composed of servers with performance failure semantics, the main advantage of loose over close synchronization is that only primary servers make full use of their share of the required replicated service resources, while backups make only a reduced use.
Reference: [T81] <author> A. Tanenbaum: </author> <title> Computer Networks, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year>
Reference-contexts: to use known hierarchical masking techniques to mask hardware server failures, such as storage du-plexing to mask loss of data replicated on two memory or disk servers with read/write omission failure semantics [LS81] or virtual-circuits to mask omission or performance communication failures by using time-outs, sequence numbers, acknowledgements and retries <ref> [T81] </ref>. Moreover, when masking is not possible, strong hardware server failure semantics such as omission and crash enable system programmers to ensure that the operating system and communication services they implement have a strong failure semantics. <p> Similarly, lower level data gram communication services with omission/performance failure semantics enable the implementation of higher level virtual-circuits with crash failure semantics <ref> [T81] </ref>. Beyond these restrictive hypotheses about the failure semantics of the basic hardware replaceable units, the complexity and cost of detecting, diagnosing, and recovering from elementary hardware server failures increases to levels that many regard as unacceptable for most commercial applications in on-line transaction processing and telecommunications.
Reference: [T82] <author> K. Trivedi: </author> <title> Probability and Statistics with Reliability, Queuing and Computer Science Applications, </title> <publisher> Prentice Hall, </publisher> <year> 1982. </year>
Reference-contexts: To check that u satisfies its stochastic specifications, a designer has to rely on s r , c r and stochastic modelling/simulation/testing techniques 12 <ref> [T82] </ref> to ensure that the probability of observing S u behaviors at run-time is at least s u and the probability of observing unspecified (potentially catastrophic) behaviors outside S u or F u is smaller than c u .
Reference: [TW89] <author> D. Taylor and G. Wilson: </author> <title> The Stratus System Architecture, in "Dependability of Resilient Computers", </title> <editor> T. Anderson, Ed., </editor> <publisher> Blackwell Scientific Publications, Oxford, </publisher> <year> 1989. </year>
Reference-contexts: processor service with crash failure semantics, one can use duplication and matching, that is, use two physically independent processors that execute in parallel the same sequence of instructions and that compare their results after each instruction execution, so that a crash occurs when a disagreement between processor outputs is detected <ref> [TW89] </ref>. In general, the stronger a specified failure semantics is, the more expensive and complex it is to build a server that implements it. 7 The following examples illustrate this general rule of fault-tolerant computing. A processor that achieves crash failure semantics by using duplication and matching, as discussed in [TW89], <p> <ref> [TW89] </ref>. In general, the stronger a specified failure semantics is, the more expensive and complex it is to build a server that implements it. 7 The following examples illustrate this general rule of fault-tolerant computing. A processor that achieves crash failure semantics by using duplication and matching, as discussed in [TW89], is more expensive to build than an elementary processor which does not use any form of redundancy to prevent users from seeing arbitrary failure behaviors. <p> In a fine granularity architecture, each elementary hardware server is a replaceable unit by itself. Some examples of commercially successful coarse granularity architectures are Tandem [B81], the DEC VAX Cluster [KLS86] and the IBM MVS/XRF [IBM87]. Examples of fine granularity architectures are Stratus <ref> [TW89] </ref> and Sequoia [B88]. Other examples of fault-tolerant architectures can be found in [AL81], [La90], and [S90]. The Tandem processor architecture packages CPU, memory, bus and I/O controller servers into single replaceable units as illustrated in Figure 2. These units can communicate among themselves via a dual bus called Dynabus. <p> mask hardware server failures directly in hardware, for example by using group masking techniques such as triplexing physical hardware servers with arbitrary failure semantics and voting [Ha78] or duplexing hardware servers with crash failure semantics (which in their turn can be implemented by server pairs based on duplication and matching <ref> [TW89] </ref>). This allows single processor failures to be masked from higher software levels of abstraction and increases the mean time between failures for the raw processor service.
Reference: [VRB89] <author> P. Verissimo, L. Rodrigues, M. Baptista: </author> <title> AMp: A Highly Parallel Atomic Multicast Protocol, </title> <booktitle> in Proceedings, ACM SIGCOM'89, </booktitle> <address> Austin, Texas, </address> <month> Sept 89. </month>
Reference-contexts: There already exist a significant number of protocols that have been proposed for group communication [ASC85], [B81], [Ba89], [BD85], [BJ87], [Ca85], [CASD85], [CM84], [Cri89], [CZ85], [GS89], [KT90], [KGR89], [L89], [LG90], [LLS90], [OL88], [MMA90], [S88], [SSCA87], and <ref> [VRB89] </ref>. This area of research is presently one of the most active.
Reference: [W78] <author> J. Wakerly: </author> <title> Error Detecting Codes, Self-checking Circuits, and Applications, </title> <publisher> Elsevier North-Holland, Inc., </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: This hardware failure detection technique is well understood. There is a rich literature specializing in error detecting codes and this subject seems to have reached a fairly mature state [PW72], <ref> [W78] </ref>. To detect failures in hardware servers, such as CPU, I/O and communication controllers, systems such as IBM, VAX and high-end Tandem use error detecting codes while newer systems such as Stratus, Sequoia, Tandem CLX and DEC VAXft 3000 use lock-step duplication with comparison.
Reference: [Wa78] <author> J. Wensley, L. Lamport, J. Goldberg, M. Green, K. Levitt, M. Melliar-Smith, R. Shostak, C. Weinstock: SIFT: </author> <title> Design and Analysis of a Fault tolerant Computer for Aircraft Control, </title> <journal> Proc IEEE Vol. </journal> <volume> 66, </volume> <month> Oct </month> <year> 1978. </year>
Reference-contexts: Examples of processor architectures designed for highly critical 22 application areas were, because of ultra-high stochastic dependability requirements, designers had to assume that their elementary hardware servers have arbitrary failure semantics can be found in [Ha78], [HLD88], and <ref> [Wa78] </ref>. 3.3 How is the specified hardware failure semantics imple mented? In order to detect failures in buses, communication lines, memory and disk servers, all the previously discussed architectures use error detecting codes. This hardware failure detection technique is well understood. <p> Some special-purpose systems do not assume that their component servers have "nice" failure semantics, such as crash/omission/performance [Aa85], [Ha78], [HLD88] and <ref> [Wa78] </ref>. By assuming that group management services such as clock synchronization, atomic broadcast, and voting work correctly, such systems can mask a minority of server failures in application groups built from members with arbitrary failure semantics. <p> Depending on the assumptions made about the faults which cause the failures, the systems can be classified in two classes: those that attempt to tolerate only physical faults [Ha78], [HLD88] and <ref> [Wa78] </ref>, and those that attempt to also tolerate design faults [Aa85]. The first class of system replicates the application servers on different, physically independent processors. Since physical faults tend to occur independently in independent processors, such faults are likely to cause minority application group failures. <p> The first class of system replicates the application servers on different, physically independent processors. Since physical faults tend to occur independently in independent processors, such faults are likely to cause minority application group failures. Experience with the systems [Ha78], [HLD88] and <ref> [Wa78] </ref> confirms that these can effectively mask the consequences of physical faults. <p> Closely synchronized groups of software servers are used by all systems that attempt to tolerate arbitrary server failures, such as [Ha78], [HLD88] and <ref> [Wa78] </ref>. Examples of closely synchronized groups of members with crash/performance failure semantics are described in [CDD90] and [Co85].
Reference: [W75] <author> W. Wulf: </author> <booktitle> Reliable Hardware-Software Architecture, 1975 Int. Conf. on Reliable Software, SIGPLAN 10, </booktitle> <volume> No. 6, </volume> <year> 1975. </year> <title> General Terms: Design, Reliability Additional Key Words and Phrases: Exception Handling, Failure, Failure Classification, Failure Masking, Failure Semantics, Fault-Tolerant System, Group Communication, Redundancy, Server Group, System Architecture. </title> <type> 45 </type>
Reference-contexts: We illustrate the hierarchical failure masking pattern described above by an IBM MVS operating system example running on a processor with several CPUs (other examples of hierarchical masking can be found in <ref> [W75] </ref>). When an attempt at reading a CPU register results in a parity check exception detection, there is an automatic CPU retry from the last saved CPU state.
References-found: 67

