URL: http://www.cs.technion.ac.il/~shaulm/papers/similarity.ps
Refering-URL: http://www.cs.technion.ac.il/~shaulm/research.html
Root-URL: 
Email: dagan@bimacs.cs.biu.ac.il  
Title: Contextual Word Similarity and Estimation from Sparse Data  
Author: Ido Dagan Shaul Marcus Shaul Markovitch 
Date: February 9, 1995  
Address: Ramat Gan 52900, Israel  Haifa 32000, Israel  Haifa 32000, Israel  
Affiliation: Department of Mathematics and Computer Science Bar-Ilan University  Department of Computer Science Technion Israel Institute of Technology  Department of Computer Science Technion Israel Institute of Technology  
Abstract: In recent years there is much interest in word cooccurrence relations, such as n-grams, verb-object combinations, or cooccurrence within a limited context. This paper discusses how to estimate the likelihood of cooccurrences that do not occur in the training data. We present a method that makes local analogies between each specific unobserved cooccurrence and other cooccurrences that contain similar words. These analogies are based on the assumption that similar word cooccurrences have similar values of mutual information. Accordingly, the word similarity metric captures similarities between vectors of mutual information values. Our evaluation suggests that this method performs better than existing, frequency based, smoothing methods, and may provide an alternative to class based models. A background survey is included, covering issues of lexical cooccurrence, data sparseness and smoothing, word similarity and clustering, and mutual information. 
Abstract-found: 1
Intro-found: 1
Reference: <author> David W. Aha, Dennis Kibler, and M. K. Albert. </author> <year> 1991. </year> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 37-66. </pages>
Reference: <author> James Allen. </author> <year> 1995. </year> <title> Natural Language Understanding. </title> <publisher> Benjamin/Cummings Publishing Company, </publisher> <address> Inc., </address> <note> second edition. </note>
Reference: <author> Eric Brill, David Magerman, Mitchell Marcus, and Beatrice Santorini. </author> <year> 1990. </year> <title> Deducing linguistic structure from the statistics of large corpora. </title> <booktitle> In DARPA Speech and Natural Language Workshop, </booktitle> <month> June. </month>
Reference: <author> P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer. </author> <year> 1991. </year> <title> Word sense disambiguation using statistical methods. </title> <booktitle> In Proc. of the Annual Meeting of the ACL, </booktitle> <pages> pages 264-270. </pages> <note> 34 Peter Brown, </note> <author> Vincent Della Pietra, Peter deSouza, Jenifer Lai, and Robert Mercer. </author> <year> 1992. </year> <title> Class-based n-gram models of natural language. </title> <journal> Computational Linguistics, </journal> <volume> 18(4). </volume>
Reference-contexts: Variants of the latter type of relationship were found useful in two other works. Brown et al. <ref> (Brown et al., 1991) </ref> use a part of speech tagger to identify relations such as "the first verb to the right" or "the first noun to the left", and then use these relations for sense disambiguation in machine translation.
Reference: <author> Kenneth W. Church and William A. Gale. </author> <year> 1991. </year> <title> A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 5 </volume> <pages> 19-54. </pages>
Reference: <author> Kenneth W. Church and Patrick Hanks. </author> <year> 1990. </year> <title> Word association norms, mutual information, and lexicography. </title> <journal> Computational Linguistics, </journal> <volume> 16(1) </volume> <pages> 22-29. </pages>
Reference: <author> Kenneth W. Church and Robert L. Mercer. </author> <year> 1993. </year> <title> Introduction to the special issue in computational linguistics using large corpora. </title> <journal> Computational Linguistics, </journal> <volume> 19(1). </volume>
Reference: <author> Ido Dagan and Alon Itai. </author> <year> 1991. </year> <title> A statistical filter for resolving pronoun references. </title>
Reference: <editor> In Y. A. Feldman and A. Bruckstein, editors, </editor> <booktitle> Artificial Intelligence and Computer Vision, </booktitle> <pages> pages 125-135. </pages> <address> Elsevier Science Publishers B.V. </address> <booktitle> (The Proc. of the 7th Israeli Sym. on Artificial Intelligence and Computer Vision, </booktitle> <year> 1990). </year>
Reference-contexts: Statistical data about these various cooccurrence relations are employed for a variety of applications, such as speech recognition <ref> (Jelinek, 1990) </ref>, language generation (Smadja and McKeown, 1990), lexicography (Church and Hanks, 1990), machine translation (Brown et al., 1992; Sadler, 1989), semantic clustering (Hindle, 1990), information retrieval (Maarek and Smadja, 1989) and various disambiguation tasks (Grishman et al., 1986; Hindle and Rooth, 1991; Dagan et al., 1991; Dagan and Itai, 1991; <p> Statistical data about these various cooccurrence relations are employed for a variety of applications, such as speech recognition (Jelinek, 1990), language generation (Smadja and McKeown, 1990), lexicography (Church and Hanks, 1990), machine translation (Brown et al., 1992; Sadler, 1989), semantic clustering <ref> (Hindle, 1990) </ref>, information retrieval (Maarek and Smadja, 1989) and various disambiguation tasks (Grishman et al., 1986; Hindle and Rooth, 1991; Dagan et al., 1991; Dagan and Itai, 1991; Gale et al., 1992b). <p> Table 1, taken from <ref> (Hindle, 1990) </ref>, demonstrates the utility of the mutual information measure. The table lists objects of the verb drink, along with the count and the mutual information of their cooccurrence with this verb. <p> COUNT MUTUAL INFORMATION brunch beer 2 12.34 tea 4 11.75 Pepsi 2 11.75 champagne 4 11.75 liquid 2 10.53 beer 5 10.20 wine 2 9.34 water 7 7.65 anything 3 5.15 much 3 1.25 it 3 1.25 SOME AMOUNT 2 1.22 Table 1: The objects of the verb drink (from <ref> (Hindle, 1990) </ref>) 3 An Estimation Method Based on Word Simi larity In the previous section we have discussed the importance of lexical relationships, and focused on the sparse data problem. This section presents an estimation method that uses a measure of word similarity to reduce the sparse data problem. <p> In this case w 1 and w 3 will not necessarily be similar. The intransitivity of the metric expresses our intended deviation from the approach of clustering words to equivalence classes, as was stated in the introduc tion. 3.3.3 Comparison with other similarity metrics Hindle <ref> (Hindle, 1990) </ref> defines similarity between nouns according to their cooccurrence with verbs in verb-object and subject-verb patterns, as identified by a syntactic parser. The formula used by Hindle to define the similarity of two nouns is very similar to the numerator of definition 8. <p> as adjacent to each other, such as 14 It should be noted that our method does take into account word frequencies, by the use of mutual information (see equation 5). 15 Hindle, for example, attributes the failure of early work on automatic classification to the lack of a robust parser <ref> (Hindle, 1990) </ref>. Many other methods also rely on a parsed corpus in identifying lexical relations (see section 2).
Reference: <author> Ido Dagan, Alon Itai, and Ulrike Schwall. </author> <year> 1991. </year> <title> Two languages are more informative than one. </title> <booktitle> In Proc. of the Annual Meeting of the ACL, </booktitle> <pages> pages 130-137. </pages>
Reference: <author> Ido Dagan, Shaul Marcus, and Shaul Markovitch. </author> <year> 1993. </year> <title> Contextual word similarity and estimation from sparse data. </title> <booktitle> In Proc. of the Annual Meeting of the ACL, </booktitle> <pages> pages 164-171. </pages>
Reference: <author> Ido Dagan, Fernando Pereira, and Lillian Lee. </author> <year> 1994. </year> <title> Similarity-based estimation of word cooccurrence probabilities. </title> <booktitle> In Proc. of the Annual Meeting of the ACL, </booktitle> <pages> pages 272-278. </pages>
Reference: <author> Ido Dagan. </author> <year> 1992. </year> <title> Multilingual Statistical Approaches for Natural Language Disambiguation. </title> <type> Ph.D. thesis, </type> <institution> Computer Science Department, Technion - Israel Institute of Technology, Haifa, </institution> <month> May. </month> <note> (in Hebrew). </note>
Reference: <author> Ute Essen and Volker Steinbiss. </author> <year> 1992. </year> <title> Coocurrence smoothing for stochastic language modeling. </title> <booktitle> In Proceedings of ICASSP, </booktitle> <volume> volume I, </volume> <pages> pages 161-164. </pages> <publisher> IEEE. </publisher>
Reference: <author> R. Fano. </author> <year> 1961. </year> <title> Transmission of Information. </title> <address> Cambridge, Mass., </address> <publisher> MIT Press. </publisher>
Reference-contexts: It reflects the strength of relationship between words by comparing their actual cooccurrence probability with the probability that would be expected by chance. More precisely, the mutual information of two events x and y is defined as follows <ref> (Fano, 1961) </ref>: I (x; y) = log 2 P (x)P (y) where P (x) and P (y) are the probabilities of the events, and P (x; y) is the probability of the joint event.
Reference: <author> William Gale, Kenneth Church, and David Yarowsky. </author> <year> 1992a. </year> <title> A method for disambiguating word senses in a large corpus. </title> <type> Technical Report Statistical Research Report, No. 104, </type> <institution> AT&T Bell Laboratories. </institution>
Reference: <author> William Gale, Kenneth Church, and David Yarowsky. </author> <year> 1992b. </year> <title> Work on statistcal methods for word sense disambiguation. </title> <booktitle> In Working Notes, AAAI Fall Symposium Series, Probabilistic Approaches to Natural Language, </booktitle> <pages> pages 54-60. </pages> <note> 35 I. </note> <author> J. Good. </author> <year> 1953. </year> <title> The population frequencies of species and the estimation of popu-lation parameters. </title> <journal> Biometrika, </journal> <volume> 40 </volume> <pages> 237-264. </pages>
Reference: <author> R. Grishman, L. Hirschman, and Ngo Thanh Nhan. </author> <year> 1986. </year> <title> Discovery procedures for sublanguage selectional patterns initial experiments. </title> <journal> Computational Linguistics, </journal> <volume> 12 </volume> <pages> 205-214. </pages>
Reference: <author> V. Gupta, M. Lennig, and P. Mermelstein. </author> <year> 1992. </year> <title> A language model for very large-vocabulary speech recognition. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 6 </volume> <pages> 331-344. </pages>
Reference-contexts: All these methods, as well as the methods in (Jelinek et al., 1992) and <ref> (Gupta et al., 1992) </ref>, smooth the observed frequencies by making generalizations, which are based solely on the frequencies of the involved objects: either the frequency of cooccurrence (e.g. the frequency of a bigram or a trigram), or the frequency of the individual words which compose the cooccurrence.
Reference: <author> Zelig S. Harris. </author> <year> 1968. </year> <title> Mathematical structures of language. </title> <publisher> Wiley. </publisher>
Reference: <author> D. Hindle and M. Rooth. </author> <year> 1991. </year> <title> Structural ambiguity and lexical relations. </title> <booktitle> In Proc. of the Annual Meeting of the ACL, </booktitle> <pages> pages 229-236. </pages>
Reference-contexts: Statistical data on such cooccurrence relations can be viewed as a statistical alternative to traditional notions of selectional constraints and semantic preferences (Wilks, 1975). As such, these relations were used successfully for various broad coverage disambiguation tasks: prepositional phrase attachment <ref> (Hindle and Rooth, 1991) </ref>, pronoun reference resolution (Dagan and Itai, 1991) and word sense disambiguation 5 for machine translation (Dagan et al., 1991; Dagan, 1992).
Reference: <author> D. Hindle. </author> <year> 1983. </year> <title> Deterministic parsing of syntactic non-fluencies. </title> <booktitle> In Proc. of the Annual Meeting of the ACL. </booktitle>
Reference-contexts: Hindle (1990), also motivated by the distributional hypothesis, attributes the failure of previous implementations to the lack of a sufficiently robust syntactic parser that would identify relationships between words. He therefore uses a robust parser that extracts grammatical structures from unrestricted text <ref> (Hindle, 1983) </ref>. His paper proposes a new similarity measure, that reflects the similarity between cooccurrence patterns of nouns in subject-verb and verb-object relations. The measure is based on the concept of mutual information (see subsection 2.4), which identifies the degree of association between each noun and verb.
Reference: <author> D. Hindle. </author> <year> 1990. </year> <title> Noun classification from predicate-argument structures. </title> <booktitle> In Proc. of the Annual Meeting of the ACL, </booktitle> <pages> pages 268-275. </pages>
Reference-contexts: Statistical data about these various cooccurrence relations are employed for a variety of applications, such as speech recognition (Jelinek, 1990), language generation (Smadja and McKeown, 1990), lexicography (Church and Hanks, 1990), machine translation (Brown et al., 1992; Sadler, 1989), semantic clustering <ref> (Hindle, 1990) </ref>, information retrieval (Maarek and Smadja, 1989) and various disambiguation tasks (Grishman et al., 1986; Hindle and Rooth, 1991; Dagan et al., 1991; Dagan and Itai, 1991; Gale et al., 1992b). <p> Table 1, taken from <ref> (Hindle, 1990) </ref>, demonstrates the utility of the mutual information measure. The table lists objects of the verb drink, along with the count and the mutual information of their cooccurrence with this verb. <p> COUNT MUTUAL INFORMATION brunch beer 2 12.34 tea 4 11.75 Pepsi 2 11.75 champagne 4 11.75 liquid 2 10.53 beer 5 10.20 wine 2 9.34 water 7 7.65 anything 3 5.15 much 3 1.25 it 3 1.25 SOME AMOUNT 2 1.22 Table 1: The objects of the verb drink (from <ref> (Hindle, 1990) </ref>) 3 An Estimation Method Based on Word Simi larity In the previous section we have discussed the importance of lexical relationships, and focused on the sparse data problem. This section presents an estimation method that uses a measure of word similarity to reduce the sparse data problem. <p> In this case w 1 and w 3 will not necessarily be similar. The intransitivity of the metric expresses our intended deviation from the approach of clustering words to equivalence classes, as was stated in the introduc tion. 3.3.3 Comparison with other similarity metrics Hindle <ref> (Hindle, 1990) </ref> defines similarity between nouns according to their cooccurrence with verbs in verb-object and subject-verb patterns, as identified by a syntactic parser. The formula used by Hindle to define the similarity of two nouns is very similar to the numerator of definition 8. <p> as adjacent to each other, such as 14 It should be noted that our method does take into account word frequencies, by the use of mutual information (see equation 5). 15 Hindle, for example, attributes the failure of early work on automatic classification to the lack of a robust parser <ref> (Hindle, 1990) </ref>. Many other methods also rely on a parsed corpus in identifying lexical relations (see section 2).
Reference: <author> L. Hirschman. </author> <year> 1986. </year> <title> Discovering sublanguage structures. </title> <editor> In R. Grishman and R. Kit-tredge, editors, </editor> <booktitle> Analyzing Language in Restricted Domains: Sublanguage Description and Processing, </booktitle> <pages> pages 211-234. </pages> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> F. Jelinek and R. Mercer. </author> <year> 1985. </year> <title> Probability distribution estimation from sparse data. </title> <journal> IBM Technical Disclosure Bulletin, </journal> <volume> 28 </volume> <pages> 2591-2594. </pages>
Reference: <author> Frederick Jelinek, Robert L. Mercer, and Salim Roukos. </author> <year> 1992. </year> <title> Principles of lexiacal language modeling for speech recognition. </title> <editor> In Sadaoki Furui and M. Mohan Sondhi, editors, </editor> <booktitle> Advances in Speech Signal Processing, </booktitle> <pages> pages 651-699. </pages> <publisher> Mercer Dekker, Inc. </publisher>
Reference: <author> Frederick Jelinek. </author> <year> 1990. </year> <title> Self-organized language modeling for speech recognition. </title> <editor> In Alex Waibel and Kai-Fu Lee, editors, </editor> <booktitle> Readings in Speech Recognition, </booktitle> <pages> pages 450-506. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, California. </address>
Reference: <author> Slava M. Katz. </author> <year> 1987. </year> <title> Estimation of probabilities from sparse data for the language model component of a speech recognizer. </title> <journal> IEEE Transactions on Acoustics, speech, and Signal Processing, </journal> <volume> 35(3) </volume> <pages> 400-401. </pages>
Reference: <author> Raymond Lau, Ronald Rosenfeld, and Salim Roukos. </author> <year> 1993. </year> <title> Adaptive language modeling using the maximum entropy principle. </title> <booktitle> In ARPA. </booktitle>
Reference: <author> Yoelle Maarek and Frank Smadja. </author> <year> 1989. </year> <title> Full text indexing based on lexical relations An application: </title> <booktitle> Software libraries. In Proc. of SIGIR. </booktitle> <volume> 36 W.J.R. </volume> <editor> Martin, B.P.F. Al, and P.J.G. van Sterkenburg. </editor> <year> 1983. </year> <title> On the processing of text corpus: from textual data to lexicographical information. </title> <editor> In R.R.K. Hart-man, editor, </editor> <booktitle> Lexicography: Principles and Practice, Applied Language Studies Series. </booktitle> <publisher> Academic Press, London. </publisher>
Reference: <author> Fernando Pereira and Naftali Tishby. </author> <year> 1992. </year> <title> Distributional similarity, phase transitions and hierarchical clustering. </title> <booktitle> In Working Notes, AAAI Fall Symposium on Probabilistic Approaches to Natural Language. </booktitle>
Reference: <author> Fernando Pereira, Naftali Tishby, and Lillian Lee. </author> <year> 1993. </year> <title> Distributional clustering of English words. </title> <booktitle> In Proc. of the Annual Meeting of the ACL. </booktitle>
Reference: <author> Philip Resnik. </author> <year> 1992. </year> <title> Wordnet and distributional analysis: A class-based approach to lexical discovery. </title> <booktitle> In AAAI Workshop on Statistically-based Natural Language Processing Techniques, </booktitle> <month> July. </month>
Reference: <author> V. Sadler. </author> <year> 1989. </year> <title> Working with analogical semantics: Disambiguation techniques in DLT. </title> <publisher> Foris Publications. </publisher> <address> Hinrich Schutze. </address> <year> 1993. </year> <title> Word space. </title> <editor> In S. J. Hanson, J. D. Cowan, and C. L. Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 895-902. </pages> <publisher> Morgan Kaufman Publishers, </publisher> <address> San Mateo CA. </address>
Reference: <author> Frank Smadja and Katheleen McKeown. </author> <year> 1990. </year> <title> Automatically extracting and representing collocations for language generation. </title> <booktitle> In Proc. of the Annual Meeting of the ACL. </booktitle>
Reference: <author> Yorick Wilks. </author> <year> 1975. </year> <title> An intelligent analyzer and understander of english. </title> <journal> Communications of the ACM, </journal> <volume> 18(5) </volume> <pages> 264-274. </pages> <note> reprinted in RNLP. 37 </note>
References-found: 36

