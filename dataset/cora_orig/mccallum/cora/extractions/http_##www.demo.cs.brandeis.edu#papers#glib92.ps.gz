URL: http://www.demo.cs.brandeis.edu/papers/glib92.ps.gz
Refering-URL: http://www.demo.cs.brandeis.edu/papers/long.html
Root-URL: http://www.cs.brandeis.edu
Title: Abstract  
Abstract: In this paper 1 we describe a genetic algorithm capable of evolving large programs by exploiting two new genetic operators which construct and deconstruct parameterized subroutines. These subroutines protect useful partial solutions and help to solve the scaling problem for a class of genetic problem solving methods. We demonstrate that our algorithm acquires useful subroutines by evolving a modular program from scratch to play and win at Tic-Tac-Toe against a awed expert. This work also serves to amplify our previous note (Pollack, 1991) that a phase transition is the principle behind induction in dynamical cognitive models. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Belew, R., McInerney, J. and Schraudolph, N., </author> <year> 1991, </year> <title> Evolving Networks: Using the Genetic Algorithm with Connectionist Learning, </title> <booktitle> In Artificial Life II, </booktitle> <address> C. </address>
Reference: <editor> Langton, C. Taylor, J. D. Farmer and S. Rasmussen (eds.), </editor> <address> Reading, MA: </address> <publisher> Addison-Wesley Publishing Company, Inc. </publisher>
Reference: <author> Dawkins R., </author> <year> 1987, </year> <title> The Blind Watchmaker, </title> <address> New York, </address> <publisher> W. </publisher> <editor> W. </editor> <title> Norton and Co. routines. See text for explanation. Note graphs are not equally scaled. </title> <editor> (a) (b) (c) Goldberg, D., </editor> <year> 1989a, </year> <title> Genetic Algorithms in Search, Optimization, </title> <booktitle> and Machine Learning, </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley Publishing Company, Inc. </publisher>
Reference: <author> Goldberg, D., </author> <year> 1989b, </year> <title> Zen and the Art of Genetic Algorithms, </title> <booktitle> In Proceedings of the Third International Conference on Genetic Algorithms, </booktitle> <editor> J. Schaffer (ed), </editor> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: When this occurs, we say that the subroutine is evolutionarily viable. Our task now is to insure that good subroutines will be copied generously into subsequent generations while inappropriate ones will be suppressed. The enlightening guidelines provided for genetic algorithm design in <ref> (Goldberg, 1989b) </ref> suggest one should never be too clever when dealing with genetic algorithms as a frontal assault to the solution of a design problem usually defeats the inherent non-linear interactions. Thus, one should practice prudence when possible.
Reference: <author> Holland, J., </author> <year> 1975, </year> <booktitle> Adaptation in Natural and Artificial Systems, </booktitle> <address> Ann Arbor, MI: </address> <publisher> The University of Michigan Press. </publisher>
Reference-contexts: The point mutation operator alters the value of a single position of a single parent string to create offspring. The schema theorem <ref> (Holland, 1975) </ref>, often called the Fundamental Theorem of Genetic Algorithms, illustrates the power behind these search methods. Holland defines a schema to be a class of binary strings which share a collection of subsequences. <p> Similarly, the defining length of 1.0.#.#.0.1 is 5. The schema theorem proves that above average schemata with small defining lengths will be copied an exponential number of times in the generations subsequent to their appearance <ref> (Holland, 1975) </ref>. For a more detailed introduction to genetic algorithms, the schema theorem and its implications see (Goldberg, 1989a). While simple genetic algorithms can be used to evolve solutions to a wide range of tasks two problems prevent their scaling to more interesting tasks.
Reference: <author> Jacobs, R., Jordan, M., Nowlan, S., & Hinton, G., </author> <year> 1991, </year> <title> Adaptive Mixtures of Local Experts, </title> <journal> Neural Com putation, </journal> <volume> 3, 79 - 87. </volume>
Reference: <author> Jacobs, S., & Jordan, M., </author> <year> 1991, </year> <title> A Competitive Modular Connectionist Architecture, </title> <booktitle> In Advances in Neural Information Processing 3, </booktitle> <editor> R. Lippmann, J. Moody, </editor> <address> D. </address>
Reference: <editor> Touretzky (eds.), </editor> <address> San Mateo CA: </address> <publisher> Morgan Kaufmann Publications, Inc. </publisher>
Reference: <author> Koza, J., </author> <year> 1992, </year> <title> Genetic Evolution and Co-Evolution of Computer Programs, </title> <booktitle> In Artificial Life II, </booktitle> <editor> C. Langton, C. Taylor, J. Farmer and S. Rasmussen (eds.), Read ing, </editor> <address> MA: </address> <publisher> Addison-Wesley Publishing Company, Inc. </publisher>
Reference-contexts: Recently, Koza has described an exciting advance in genetic algorithms. In his Genetic Programming Paradigm (GPP), Koza uses a hierarchy of primitive functions rather than a fixed-length string to represent potential solutions <ref> (Koza, 1992, Koza, 1990) </ref>. These hierarchies are interpreted as programs written in a language defined by the primitive functions which when executed compute the solution to the task. Kozas genetic operators exchange subtrees of the hierarchies rather than substrings.
Reference: <author> Koza, J., </author> <year> 1990, </year> <title> Genetic Programming: A Paradigm for Genetically Breeding Populations of Computer Programs to Solve Problems, </title> <type> Technical Report No. </type> <institution> STAN-CS-90-1314, Computer Science Department, Stanford University. </institution>
Reference-contexts: We ran GLiB with a population size of 1000 using the described expert and scoring method as the fitness function. In this run we applied the compression operator to 10 percent of the population each generation. All other parameters were as set in <ref> (Koza, 1990) </ref> for the ant experiment. The best evolved program after 200 generations had an average score of 16.5 points for the 4 games it played against the expert to determine its fitness.
Reference: <author> Nowlan, S., & Hinton, G., </author> <year> 1991, </year> <title> Evaluation of Adaptive Mixtures of Competing Experts, </title> <booktitle> In Advances in Neural Information Processing 3, </booktitle> <editor> R. Lippmann, </editor> <publisher> J. </publisher>
Reference: <editor> Moody, D. Touretzky (eds.), </editor> <address> San Mateo CA: </address> <publisher> Morgan Kaufmann Publications, Inc. </publisher>
Reference: <author> Pollack, J., </author> <year> 1991, </year> <title> The Induction of Dynamical Recog nizers, </title> <journal> Machine Learning (7), </journal> <volume> 227 - 252. </volume>
Reference-contexts: An analysis of the frequency of subroutine calls shows an exponential growth and decay of subroutine usage as they are induced or expelled from the language, leading us to name this phenomenon evolutionary induction, an amplification of our earlier principle of induction by phase transition <ref> (Pollack, 1991) </ref>. Genetic Algorithms Background The genetic algorithm (Holland, 1975; Goldberg 1989a) is a form of problem solving search analogous to natural selection, and is a surprisingly adept search method in even very large ill-formed problem spaces. <p> First, it is apparent that we have been able to capture useful schemata in our subroutines by the exponential-like rises in the subroutine call curves. Second this work amplifies our previous note <ref> (Pollack, 1991) </ref> that a phase transition is the principle behind induction in dynamical cognitive models. We call this method of random selection and evolutionary evaluation of subroutines evolutionary induction. But there is more to the story than a simple attachment to Hollands powerful theorem.
Reference: <author> Saunders, G., Kolen, J., Angeline, P. & Pollack, J., </author> <year> 1992, </year> <title> Additive Modular Learning in Preemptrons, </title> <booktitle> In Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society, </booktitle> <address> Bloomington, Indiana, Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates, Inc. </publisher>
Reference: <author> Simon, H. A., </author> <year> 1969, </year> <booktitle> The Sciences of the Artificial, Cam bridge, </booktitle> <address> MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: We call this the defining length problem. 2 As an empirical indication of this problem, we note that the largest evolved program Koza reports is only 48 nodes. These scaling difficulties call to mind Simons parable of the two watchmakers Tempus and Hora <ref> (Simon, 1969) </ref>. In this parable, the two watchmakers build products of similar complexity (1000 parts) using differing design philosophies. Tempus constructs the entire watch directly from the primitive components, much like GPP constructs programs.
References-found: 15

