URL: ftp://ftp.cs.orst.edu/pub/tgd/papers/ml98-maxq.ps.gz
Refering-URL: http://www.cs.orst.edu/~tgd/cv/pubs.html
Root-URL: 
Email: tgd@cs.orst.edu  
Title: The MAXQ Method for Hierarchical Reinforcement Learning  
Author: Thomas G. Dietterich 
Address: Corvallis, Oregon 97331  
Affiliation: Department of Computer Science Oregon State University  
Abstract: This paper presents a new approach to hierarchical reinforcement learning based on the MAXQ decomposition of the value function. The MAXQ decomposition has both a procedural semanticsas a subroutine hierarchyand a declarative semanticsas a representation of the value function of a hierarchical policy. MAXQ unifies and extends previous work on hierarchical reinforcement learning by Singh, Kaelbling, and Dayan and Hinton. Conditions under which the MAXQ decomposition can represent the optimal value function are derived. The paper defines a hierarchical Q learning algorithm, proves its convergence, and shows experimentally that it can learn much faster than ordinary flat Q learning. Finally, the paper discusses some interesting issues that arise in hierarchical reinforcement learning including the hierarchical credit assignment problem and non-hierarchical execution of the MAXQ hierarchy.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bertsekas, D. P. </author> <year> (1995). </year> <title> Dynamic programming and optimal control. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, MA. </address>
Reference-contexts: This is equivalent to computing the one-step greedy lookahead policy given the current value function. If the hierarchical policy is not optimal, then this one-step greedy policy will be closer to an optimal policy, because it corresponds to one step of policy improvement in the policy iteration algorithm <ref> (Bertsekas, 1995) </ref>. This informally proves the following: Theorem 4 For all states s, the value of the policy computed by polling execution of the MAXQ hierarchy is the value of the policy computed by hierarchical execution.
Reference: <author> Bertsekas, D. P., & Tsitsiklis, J. N. </author> <year> (1996). </year> <title> Neuro-Dynamic Programming. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, MA. </address>
Reference-contexts: By applying a stochastic approximation result <ref> (Proposition 4.5 from Bertsekas and Tsitsiklis, 1996) </ref>, we can prove that node j will converge to a locally optimal policy. Hence, by induction, we can prove that the entire hierarchy converges to a recursively optimal policy. End of Proof Sketch.
Reference: <author> Dayan, P., & Hinton, G. </author> <year> (1993). </year> <title> Feudal reinforcement learning. </title> <journal> NIPS, </journal> <volume> 5, </volume> <pages> pp. 271278. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA. </address>
Reference: <author> Dean, T., & Lin, S.-H. </author> <year> (1995). </year> <title> Decomposition techniques for planning in stochastic domains. </title> <type> Tech. rep. </type> <institution> CS-95-10, Dept. of Computer Science, Brown University, </institution> <address> Providence, Rhode Island. </address>
Reference: <author> Jaakkola, T., Jordan, M. I., & Singh, S. P. </author> <year> (1994). </year> <title> On the convergence of stochastic iterative dynamic programming algorithms. </title> <journal> Neur. Comp., </journal> <volume> 6(6), </volume> <pages> 11851201. </pages>
Reference: <author> Kaelbling, L. P. </author> <year> (1993). </year> <title> Hierarchical reinforcement learning: Preliminary results. </title> <booktitle> ICML-93, </booktitle> <pages> pp. </pages> <address> 167173 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Parr, R., & Russell, S. </author> <year> (1998). </year> <title> Reinforcement learning with hierarchies of machines. </title> <booktitle> NIPS, Vol. </booktitle> <address> 10 Cambridge, MA. </address> <publisher> MIT Press. </publisher>
Reference: <author> Peng, J., & Williams, R. J. </author> <year> (1996). </year> <title> Incremental multi-step Q-learning. </title> <journal> Mach. Learn., </journal> <volume> 22, </volume> <pages> 283290. </pages>
Reference-contexts: Of course, both methods could be improved by employing techniques for accelerating Q learning, such as eligibility traces <ref> (e.g., Peng & Williams, 1996) </ref>. 5 Non-Hierarchical Execution We have shown that the MAXQ hierarchy can learn an optimal policy for an MDP if that policy is a recursively optimal hierarchical. However, there are situations in which the optimal policy is almostbut not quitehierarchical.
Reference: <author> Singh, S., Jaakkola, T., Littman, M. L., & Szpesvari, C. </author> <year> (1998). </year> <title> Convergence results for single-step on-policy reinforcement-learning algorithms. </title> <type> Tech. rep., </type> <institution> University of Colorado, Dept. Comp. Sci. </institution>
Reference: <author> Singh, S. P. </author> <year> (1992). </year> <title> Transfer of learning by composing solutions of elemental sequential tasks. </title> <journal> Mach. Learn., </journal> <volume> 8, </volume> <pages> 323339. </pages>
Reference: <author> Sutton, R. S., Precup, D., & Singh, S. </author> <year> (1998). </year> <title> Between MDPs and Semi-MDPs: Learning, planning, and representing knowledge at multiple temporal scales. </title> <type> Tech. rep., </type> <institution> University of Mass., Dept. Comp. Inf. Sci., </institution> <address> Amherst, MA. </address>
References-found: 11

