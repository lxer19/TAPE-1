URL: ftp://ftp.cs.umass.edu/pub/anw/pub/crites/root.ps.Z
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00193.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: LARGE-SCALE DYNAMIC OPTIMIZATION USING TEAMS OF REINFORCEMENT LEARNING AGENTS  
Author: ROBERT HARRY CRITES 
Degree: A Dissertation Presented by  Submitted to the Graduate School of the  in partial fulfillment of the requirements for the degree of DOCTOR OF PHILOSOPHY  
Date: September 1996  
Affiliation: University of Massachusetts Amherst  Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Aicardi, M., Davoli, F., and Minciardi, R. </author> <title> Decentralized optimal control of Markov chains with a common past information set. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 32 </volume> <pages> 1028-1031, </pages> <year> 1987. </year>
Reference-contexts: Hsu & Marcus [49] used this result in 1982 to reduce the one step delay problem to a centralized problem. The common information becomes the more complex state in the equivalent centralized problem. Classical methods could then be used to solve for the optimal policy. Aicardi et al <ref> [1] </ref> extended this result in 1987 to k step delay problems by finding a sufficient statistic that captured all the relevant information from the common information. Sufficient statistics are used to respond to the problem that the dimension of the state (the information vector) grows as new measurements come in.
Reference: [2] <author> Astrom, K. J. </author> <title> Optimal control of Markov processes with incomplete state information. </title> <journal> Journal of Mathematical Analysis and Applications, </journal> <volume> 10 </volume> <pages> 174-205, </pages> <year> 1965. </year>
Reference-contexts: Decentralized control problems are thus treated as centralized problems with incomplete state information. The standard approach in problems with incomplete state information is to reduce them to problems with perfect state information, where the new state consists of all the information that is available <ref> [2] </ref>. The problem of delayed sharing patterns, where the controllers share their own private information after k steps of delay, is one example in decentralized control where this technique has been 23 used.
Reference: [3] <author> Aumann, R. J. </author> <title> Survey of repeated games. </title> <booktitle> In Essays in Game Theory and Mathematical Economics. </booktitle> <publisher> Bibliographisches Institut, Mannheim, </publisher> <address> Germany, </address> <year> 1981. </year>
Reference-contexts: The literature on RGs has been surveyed by Sabourian [93] and Aumann <ref> [3] </ref>. Much of the work on RGs addresses competitive games, and is thus not directly applicable to team problems. An RG can be defined as a one-shot game repeated many times. If a one-shot game is repeated an infinite number of times it is called a supergame.
Reference: [4] <author> Axelrod, R. M. </author> <title> The Evolution of Cooperation. </title> <publisher> Basic Books, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: This can be proved by constructing strategies that punish any player that deviates from the norm. Axelrod <ref> [4] </ref> uses the repeated Prisoner's Dilemma to study how cooperation can develop implicitly. He shows that in that game, there is no best strategy independent of the strategy used by the other player. He ran several round robin tournaments to determine what types of strategies would do well.
Reference: [5] <author> Baird, L. C. </author> <title> Advantage updating. </title> <type> Technical report, </type> <institution> Wright-Patterson Air Force Base, </institution> <year> 1993. </year>
Reference-contexts: This would allow more of the state variables to be considered in the decision making process. The decision about whether to close the doors could potentially be made at each instant, and so a continuous time algorithm such as advantage updating <ref> [5] </ref> could be used. Of course, up-peak traffic is seldom completely pure. Some method must be used to deal with any down hall calls. Making down stops increases the round trip time but allows passengers to be carried in both directions.
Reference: [6] <author> Bao, G., Cassandras, C. G., Djaferis, T. E., Gandhi, A. D., and Looze, D. P. </author> <title> Elevator dispatchers for down peak traffic. </title> <type> ECE Department Technical Report, </type> <institution> University of Massachusetts, </institution> <year> 1994. </year>
Reference-contexts: The goal of the zoning approach is to keep the cars reasonably well separated and thus keep the interval down. The drawback of this approach seems to be a significant loss of flexibility. Several papers include zoning in a comparison with other strategies <ref> [6, 16] </ref>. Sakai & Kurosawa [94] of Hitachi describe a concept called area control that is related to zoning. If possible, it assigns a hall call to a car that already must stop at that floor due to a car call. <p> Closed-loop control is achieved by re-calculating a new open-loop plan after every event. The weaknesses of this approach are its computational demands, and its lack of consideration of future arrivals. Examples of receding horizon controllers are Finite Intervisit Minimization (FIM) and 53 Empty the System Algorithm (ESA) <ref> [6] </ref>. FIM attempts to minimize squared waiting times and ESA attempts to minimize the length of the current busy period, the same objective as Levy et al [60]. 4.2.3 Rule-Based Approaches In some sense, all control policies could be considered rule-based: IF situation THEN action. <p> where the amount of sand left indicates the remaining waiting time. 54 4.2.4 Other Heuristic Approaches The Longest Queue First (LQF) algorithm assigns upward moving cars to the longest waiting queue, and the Highest Unanswered Floor First (HUFF) algorithm assigns upward moving cars to the highest queue with people waiting <ref> [6] </ref>. Both of these algorithms are designed specifically for down-peak traffic. They assign downward moving cars to any unassigned hall calls they encounter. <p> Levy et al [60] use dynamic programming (DP) o*ine to minimize the expected time needed for completion of the current busy period. The idea of minimizing the busy period also appears in the receding horizon controller ESA (Empty the System Algorithm) <ref> [6] </ref>. K (b; d) is the time needed for completion of the busy period given state b and decision d, and all subsequent decisions optimal. This definition, with its state-action pair, seems almost prophetic of the Q-learning algorithm developed 12 years later by Watkins [119]. <p> The simulator was written by Lewis [61]. Passenger arrivals at each floor are assumed to be Poisson, with arrival rates that 58 vary during the course of the day. Our simulations use a traffic profile <ref> [6] </ref> which dictates arrival rates for every 5-minute interval during a typical afternoon down-peak rush hour. Table 4.1 shows the mean number of passengers arriving at each of floors 2 through 10 during each 5-minute interval who are headed for the lobby. <p> ESA uses queue length information that would not be available in a real elevator system. ESA/nq is a version of ESA that uses arrival rate information to estimate the queue lengths. For more details, see <ref> [6] </ref>. RLp and RLd denote the RL controllers, parallel and decentralized. The RL controllers were each trained on 60,000 hours of simulated elevator time, which took four days on a 100 MIPS workstation.
Reference: [7] <author> Barto, A. G. </author> <title> Learning by statistical cooperation of self-interested neuron-like adaptive elements. </title> <journal> Human Neurobiology, </journal> <volume> 4 </volume> <pages> 229-256, </pages> <year> 1985. </year>
Reference-contexts: Barto <ref> [7] </ref> studied the learning behavior of networks of A RP units and showed empirical results on their ability to learn nonlinear tasks, including an exclusive-or problem and a multiplexor problem.
Reference: [8] <author> Barto, A. G. </author> <title> From chemotaxis to cooperativity: Abstract exercises in neuronal learning strategies. </title> <editor> In Durbin, R., Miall, C., and Mitchison, G., editors, </editor> <booktitle> The Computing Neuron. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Wokingham, England, </address> <year> 1989. </year>
Reference-contexts: Such local reinforcement architectures have the potential to greatly increase the speed of learning, but they will require much more knowledge on the part of whatever is producing the reinforcement signals <ref> [8] </ref>. Finally, it is important to find effective methods of allowing the possibility of explicit communication among the agents. 92 CHAPTER 8 CONCLUSIONS Multi-agent control systems are often required because of spatial or geographic distribution, or in situations where centralized information is not available or is not practical.
Reference: [9] <author> Barto, A. G. </author> <title> Some learning tasks from a control perspective. </title> <type> COINS Technical Report 90-122, </type> <institution> University of Massachusetts, </institution> <year> 1990. </year>
Reference-contexts: Associative learning automata receive the context as an additional input, and must learn a mapping from contexts to actions that maximizes the expected reinforcement. Such a task is known as an associative reinforcement learning task <ref> [9] </ref> because an association must be made between contexts and actions. One way to address this would be to have a group of learning automata, and assign one to each possible context.
Reference: [10] <author> Barto, A. G. and Anandan, P. </author> <title> Pattern recognizing stochastic learning automata. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 15 </volume> <pages> 360-375, </pages> <year> 1985. </year>
Reference-contexts: One way to address this would be to have a group of learning automata, and assign one to each possible context. Such a 31 lookup-table approach is flexible in terms of the mappings it can do, but it would not allow any generalization across different contexts. Barto & Anandan <ref> [10] </ref> introduced a learning algorithm for associative learning automata that does allow generalization. They called it the associative reward-penalty, or A RP algorithm. An A RP unit operates in discrete time. At step t, it receives the context through inputs y 1 (t) to y n (t).
Reference: [11] <author> Barto, A. G., Bradtke, S. J., and Singh, S. P. </author> <title> Learning to act using real-time dynamic programming. </title> <type> CMPSCI Technical Report 93-02, </type> <institution> University of Massachusetts, </institution> <year> 1993. </year>
Reference-contexts: Starting with an initial estimate ^ V fl , repeatedly update this estimate by performing "backup" operations as follows: ^ V fl (x) = max [h (x; u) + fl z2X Traditionally, these backup operations are ordered to form repeated sweeps through the state set. (For alternatives to this, see <ref> [11, 20] </ref>). Regardless of the initial estimate ^ V fl , this process converges to the optimal value function V fl . Once the optimal value function V fl has been approximated closely enough, it can be used to easily find an optimal policy. <p> In addition, it requires a probability model representing the state transition probabilities and reward structure of the environment, which may be difficult to obtain. A number of sequential RL algorithms have been developed that approximate DP on an incremental basis <ref> [11] </ref>. Unlike traditional DP algorithms, these algorithms can perform with or without a model of the environment, and they can be used online as well as o*ine.
Reference: [12] <author> Barto, A. G. and Jordan, M. I. </author> <title> Gradient following without back-propagation in layered networks. </title> <booktitle> In Proceedings of the IEEE First Annual Conference on Neural Networks, </booktitle> <year> 1987. </year> <month> 96 </month>
Reference-contexts: The long-term behavior of these networks was analyzed by Phansalkar & Thathatchar [88] using weak convergence techniques. Networks of associative learning automata perform a statistical analogue to the type of gradient descent carried out by error backpropagation [91]. Barto & Jordan <ref> [12] </ref> compared the backpropagation algorithm with a network of modified A RP units that handle real-valued reinforcement and use a batching method to increase learning efficiency. Backpropagation is faster since it can compute the gradient directly, while the A RP units must estimate it in the presence of noise.
Reference: [13] <author> Barto, A. G. and Sutton, R. S. </author> <title> Learning By Interaction: An Introduction to Modern Reinforcement Learning. </title> <publisher> Forthcoming. </publisher>
Reference-contexts: Thus, these algorithms are a computationally tractable way of approximating DP on very large problems. The same focusing phenomenon can also be achieved with simulated online training. One can often construct a simulation model without ever explicitly determining the state transition probabilities for an environment <ref> [13, 34] </ref>. (For an example of such a simulation model, see section 4.3.) There are several advantages to this use of a simulation model if it is sufficiently accurate. <p> Dynamic programming (DP) can be used to find an optimal policy if the reward function and state transition probabilities are known a priori. In this case, we say that the agent has a probability model of its environment <ref> [13] </ref>. Although uncertainties exist, they are already well understood. 21 2.3.4 Dynamic Programming A DP technique called value iteration can be used to calculate the optimal value function. <p> In many cases it is possible to generate sample realizations of a process without an explicit probability model. One can often construct a simulation model of an environment using a high-level structural view of the environment without ever explicitly determining the state transition probabilities <ref> [13, 34] </ref>. (For an example of 36 such a simulation model, see section 4.3.) Sample backups can then be performed along trajectories through state space generated by the simulation model.
Reference: [14] <author> Barto, A. G., Sutton, R. S., and Anderson, C. W. </author> <title> Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 13 </volume> <pages> 835-846, </pages> <year> 1983. </year>
Reference-contexts: Of course, it is possible to use value-based RL algorithms in non-sequential problems, for example, Q-learning with fl = 0. Likewise, there are policy-based sequential RL algorithms, such as actor/critic algorithms <ref> [14, 15, 105] </ref>, where the actor implements a stochastic policy that maps states to action probability vectors, and the critic attempts to estimate the value of each state under the current policy in order to provide more useful feedback to the actor.
Reference: [15] <author> Barto, A. G., Sutton, R. S., and Watkins, C. J. C. H. </author> <title> Learning and sequential decision making. </title> <type> COINS Technical Report 89-95, </type> <institution> University of Massachusetts, </institution> <year> 1989. </year>
Reference-contexts: Of course, it is possible to use value-based RL algorithms in non-sequential problems, for example, Q-learning with fl = 0. Likewise, there are policy-based sequential RL algorithms, such as actor/critic algorithms <ref> [14, 15, 105] </ref>, where the actor implements a stochastic policy that maps states to action probability vectors, and the critic attempts to estimate the value of each state under the current policy in order to provide more useful feedback to the actor.
Reference: [16] <author> Benmakhlouf, S. M. and Khator, S. K. Smart lifts: </author> <title> Control design and performance evaluation. </title> <journal> Computers and Industrial Engineering, </journal> <volume> 25 </volume> <pages> 175-178, </pages> <year> 1993. </year>
Reference-contexts: Another commonly used up-peak strategy involves selecting a dwell time, which is the amount of time a car will spend waiting for the next passenger to arrive <ref> [16] </ref>. It is reinitialized every time a passenger enters the car. Finding an optimal threshold or dwell time for a given traffic intensity would involve minimizing a function (the expected wait time) of one variable (the threshold or dwell time). <p> The goal of the zoning approach is to keep the cars reasonably well separated and thus keep the interval down. The drawback of this approach seems to be a significant loss of flexibility. Several papers include zoning in a comparison with other strategies <ref> [6, 16] </ref>. Sakai & Kurosawa [94] of Hitachi describe a concept called area control that is related to zoning. If possible, it assigns a hall call to a car that already must stop at that floor due to a car call.
Reference: [17] <author> Berry, D. A. and Fristedt, B. </author> <title> Bandit Problems. </title> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1985. </year>
Reference-contexts: The "Cumulative Best Response" algorithm they present works fairly well in the convention game, but rather poorly in the Prisoner's Dilemma. It is a relatively greedy algorithm that could not be expected to perform well in more general situations such as bandit problems <ref> [17] </ref>, and they do not compare it with existing RL algorithms. In behavior-based robotics, the conditions under which behaviors should be activated are generally fixed at design-time by hand.
Reference: [18] <author> Bertsekas, D. P. </author> <title> Dynamic Programming and Stochastic Control. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1976. </year>
Reference-contexts: One wants to reduce this information to the data actually needed for control purposes. Sufficient statistics are ideally of a smaller dimension, and yet summarize all the information that is relevant to control <ref> [18, 19] </ref>.
Reference: [19] <author> Bertsekas, D. P. </author> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1987. </year>
Reference-contexts: One wants to reduce this information to the data actually needed for control purposes. Sufficient statistics are ideally of a smaller dimension, and yet summarize all the information that is relevant to control <ref> [18, 19] </ref>.
Reference: [20] <author> Bertsekas, D. P. and Tsitsiklis, J. N. </author> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: Starting with an initial estimate ^ V fl , repeatedly update this estimate by performing "backup" operations as follows: ^ V fl (x) = max [h (x; u) + fl z2X Traditionally, these backup operations are ordered to form repeated sweeps through the state set. (For alternatives to this, see <ref> [11, 20] </ref>). Regardless of the initial estimate ^ V fl , this process converges to the optimal value function V fl . Once the optimal value function V fl has been approximated closely enough, it can be used to easily find an optimal policy.
Reference: [21] <author> Bertsekas, D. P. and Tsitsiklis, J. N. </author> <title> Neuro-Dynamic Programming. </title> <publisher> Athena Scientific Press, </publisher> <address> Belmont, MA, </address> <year> 1996. </year>
Reference-contexts: By using compact function approximators such as artificial neural networks, they can represent value functions that are too massive to store as lookup tables. This allows generalization that can speed up learning, but that also removes most theoretical guarantees of convergence <ref> [21, 22, 115] </ref>. When a probability model of the environment is not available, two basic approaches are generally used. Indirect approaches first estimate an explicit probability model of the environment, and then use model-based techniques. Direct approaches do not learn an explicit probability model of the environment.
Reference: [22] <author> Boyan, J. A., Moore, A. W., and Sutton, R. S. </author> <title> Proceedings of the workshop on value function approximation, Machine Learning Conference 1995. </title> <type> Technical Report CMU-CS-95-206, </type> <institution> Carnegie Mellon University, </institution> <year> 1995. </year>
Reference-contexts: By using compact function approximators such as artificial neural networks, they can represent value functions that are too massive to store as lookup tables. This allows generalization that can speed up learning, but that also removes most theoretical guarantees of convergence <ref> [21, 22, 115] </ref>. When a probability model of the environment is not available, two basic approaches are generally used. Indirect approaches first estimate an explicit probability model of the environment, and then use model-based techniques. Direct approaches do not learn an explicit probability model of the environment.
Reference: [23] <author> Bradtke, S. J. </author> <title> Distributed adaptive optimal control of flexible structures, 1993. </title> <type> Unpublished manuscript. </type>
Reference-contexts: The parallel Q-learning architecture is able to handle a problem with 55 million possible action combinations that would be unthinkable using a centralized architecture. Tham & Prager [111] briefly mention the same idea of having a separate Q-network independently control each actuator. Bradtke <ref> [23] </ref> describes some initial experiments using RL for the decentralized control of a flexible beam. The task is to efficiently damp out disturbances of a beam by applying forces at discrete locations and times. He uses 10 independent adaptive controllers distributed along the beam.
Reference: [24] <author> Bradtke, S. J. and Duff, M. O. </author> <title> Reinforcement learning methods for continuous-time Markov decision problems. </title> <editor> In Tesauro, G., Touretzky, D., and Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1995. </year>
Reference-contexts: In such systems, the constant discount factor fl used in most discrete-time reinforcement learning algorithms is inadequate. This problem can be approached using a variable discount factor that depends on the amount of time between events <ref> [24] </ref>. <p> Bradtke and Duff <ref> [24] </ref> consider the case where c t is constant between events. We extend their formulation to the case where c t is quadratic, since the goal is to minimize squared wait times.
Reference: [25] <author> Caruana, R. </author> <title> Learning many related tasks at the same time with backpropagation. </title> <editor> In Tesauro, G., Touretzky, D., and Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1995. </year>
Reference-contexts: RLds and RLps refer to the distributed and parallel RL architectures with separate action networks. Tables 6.12, 6.13, and 6.14 show a comparison of the performance of the combined versus separate action networks, trained under the same conditions. In spite of experiments <ref> [25] </ref> that seem to suggest that combined networks should be superior, in this particular problem that does not appear to be the case. 85 Table 6.12 Combined versus separate action network results for down-peak profile with down traffic only.
Reference: [26] <author> Cassandras, C. G. </author> <title> Discrete Event Systems: Modeling and Performance Analysis. </title> <publisher> Aksen Associates, Homewood, </publisher> <address> IL, </address> <year> 1993. </year> <month> 97 </month>
Reference-contexts: Then we describe the algorithm, the feedforward networks used to store the Q-values, and the distinction between parallel and distributed versions of the algorithm. 5.1 Discrete-Event Reinforcement Learning Elevator systems can be modeled as discrete event systems <ref> [26] </ref>, where significant events (such as passenger arrivals) occur at discrete times, but the amount of time between events is a real-valued variable. In such systems, the constant discount factor fl used in most discrete-time reinforcement learning algorithms is inadequate.
Reference: [27] <author> Chandrasekaran, B. </author> <title> Natural and social system metaphors for distributed prob-lem solving: Introduction to the issue. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 11 </volume> <pages> 1-5, </pages> <year> 1981. </year>
Reference-contexts: Dawkins [38] argues that decisions 1 based on local information are most efficient if made locally. Chandrasekaran <ref> [27] </ref> notes that for "complex information processing systems involving very large numbers of sensors and effectors, a central processor will require very large bandwidths for responding to sensors or activating effectors." As he says, it is difficult to imagine "an army whose commanding general alone is authorized to make all the
Reference: [28] <author> Chapman, D. and Kaelbling, L. P. </author> <title> Input generalization in delayed reinforcement learning: An algorithm and performance comparisons. </title> <booktitle> In Proceedings of IJCAI, </booktitle> <year> 1991. </year>
Reference-contexts: One difficulty with this approach is that the number of underlying states needed in the HMM is not known a priori. Chrisman [29] and McCallum [74] both add states incrementally by splitting existing ones. State-splitting was also used by Chapman & Kaelbling <ref> [28] </ref> in a reinforcement learning context. They built an agent to play a videogame called Amazon. The visual system of their agent had 100 input bits.
Reference: [29] <author> Chrisman, L. </author> <title> Planning for closed-loop execution using partially observable Markovian decision processes. </title> <booktitle> In AAAI Spring Symposium Series: Control of Selective Sensing, </booktitle> <year> 1992. </year>
Reference-contexts: There is also a problem with inherently stochastic environments, as the architecture may try to keep adding more and more units. 43 There are several new methods that deal with hidden state by constructing hidden Markov models (HMMs). In the Perceptual Distinctions Approach <ref> [29] </ref> developed by Chrisman, the agent learns to make predictions about the environment by estimating the underlying state of the environment using an HMM. The HMM estimates the state based on not only the current perceptions, but also on estimates of the previous state. <p> One difficulty with this approach is that the number of underlying states needed in the HMM is not known a priori. Chrisman <ref> [29] </ref> and McCallum [74] both add states incrementally by splitting existing ones. State-splitting was also used by Chapman & Kaelbling [28] in a reinforcement learning context. They built an agent to play a videogame called Amazon. The visual system of their agent had 100 input bits.
Reference: [30] <author> Chrisman, L., Caruana, R., and Carriker, W. </author> <title> Intelligent agent design issues: Internal agent state and incomplete perception. </title> <booktitle> In AAAI Fall Symposium Series: Sensory Aspects of Robotic Intelligence, </booktitle> <year> 1991. </year>
Reference-contexts: Examples of this include work by Whitehead & Ballard [123] and Tan [106]. One disadvantage of the algorithms they present is the assumption of a deterministic environment. A limitation of sensor-based approaches in general is that they cannot handle what Chrisman calls incomplete perception <ref> [30] </ref>. This refers to situations where no perceptual actions can be taken to disambiguate world states. In these situations, the only chance at being able to distinguish certain states is by remembering past perceptions and actions. This is called the memory-based approach.
Reference: [31] <author> Craven, M. W. and Shavlik, J. W. </author> <title> Learning symbolic rules using artificial neural networks. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <year> 1993. </year>
Reference-contexts: The cars should also have the ability to "park" at various floors during periods of light traffic. It would also be useful to be able to interpret or summarize the policies the RL controllers have learned, e.g., <ref> [31] </ref>. There may or may not be a simple explanation of the strategies they have learned, but this is difficult to ascertain because the state space has such a high dimension.
Reference: [32] <author> Crick, F. </author> <title> The recent excitement about neural networks. </title> <journal> Nature, </journal> <volume> 337 </volume> <pages> 129-132, </pages> <year> 1989. </year>
Reference-contexts: Backpropagation is faster since it can compute the gradient directly, while the A RP units must estimate it in the presence of noise. However, networks of learning automata have a biological plausibility that backpropagation seems to lack <ref> [32] </ref>, and they are applicable to problems more difficult than supervised learning. The two approaches can be effectively combined by using a backpropagation network with associative learning automata as output units. Gullapalli [45] applied this architecture successfully to a difficult robotics problem.
Reference: [33] <author> Crites, R. H. and Barto, A. G. </author> <title> An actor/critic algorithm that is equivalent to Q-learning. </title> <editor> In Tesauro, G., Touretzky, D., and Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1995. </year>
Reference-contexts: For further insight into the relationship between value-based and policy-based RL algorithms, see Crites & Barto <ref> [33] </ref>. 3.2.3 Sequential Multi-Agent Reinforcement Learning Starting in approximately 1993, a number of researchers began to investigate applying sequential RL algorithms in multi-agent contexts.
Reference: [34] <author> Crites, R. H. and Barto, A. G. </author> <title> Forming control policies from simulation models using reinforcement learning. </title> <booktitle> In Proceedings of the Ninth Yale Workshop on Adaptive and Learning Systems. </booktitle> <year> 1996. </year>
Reference-contexts: Thus, these algorithms are a computationally tractable way of approximating DP on very large problems. The same focusing phenomenon can also be achieved with simulated online training. One can often construct a simulation model without ever explicitly determining the state transition probabilities for an environment <ref> [13, 34] </ref>. (For an example of such a simulation model, see section 4.3.) There are several advantages to this use of a simulation model if it is sufficiently accurate. <p> In many cases it is possible to generate sample realizations of a process without an explicit probability model. One can often construct a simulation model of an environment using a high-level structural view of the environment without ever explicitly determining the state transition probabilities <ref> [13, 34] </ref>. (For an example of 36 such a simulation model, see section 4.3.) Sample backups can then be performed along trajectories through state space generated by the simulation model.
Reference: [35] <author> Crites, R. H. and Barto, A. G. </author> <title> Improving elevator performance using reinforcement learning. </title> <editor> In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference-contexts: At the same time, RL agents adapt to a top-down global reinforcement signal, which guides their behavior toward the achievement of complex specific goals. As a result, very robust systems for complex problems can be created with a minimum of human effort <ref> [35] </ref>. RL also combines the heuristic, satisficing nature of AI with the principled nature of control theory, by approximating DP in an incremental manner.
Reference: [36] <author> Davies, P., </author> <title> editor. The American Heritage Dictionary of the English Language. </title> <publisher> Dell Publishing, </publisher> <year> 1980. </year>
Reference-contexts: INTRODUCTION 1.1 Multiple Agents An agent is an entity that has the power to act <ref> [36] </ref>. An agent is rarely found in isolation. Its environment usually contains other agents, which is one reason why the world is such a dynamic place. Multi-agent systems pervade human experience on many different levels.
Reference: [37] <author> Davis, R. and Smith, R. G. </author> <title> Negotiation as a metaphor for distributed problem solving. </title> <journal> Artificial Intelligence, </journal> <volume> 20 </volume> <pages> 63-109, </pages> <year> 1983. </year>
Reference-contexts: They iteratively exchange tentative partial solutions in order to construct global solutions. Coordination may also be achieved by means of negotiation. For example, the contract net protocol <ref> [37] </ref> assigns tasks to agents on the basis of a bidding mechanism. Agents respond to task announcements with bids indicating how well they believe they can perform the task. The Clarke tax [42] is a technique for ensuring that agents reveal their true preferences.
Reference: [38] <author> Dawkins, R. </author> <title> Hierarchical organisation: A candidate principle for ethology. </title> <editor> In Bateson, P. and Hinde, R., editors, </editor> <title> Growing Points in Ethology. </title> <publisher> Cambridge University Press, </publisher> <year> 1976. </year>
Reference-contexts: But even when a distributed approach is not required, multiple agents may still provide an excellent way of scaling up to approximate solutions for very large problems by streamlining the search through the space of possible policies. Dawkins <ref> [38] </ref> argues that decisions 1 based on local information are most efficient if made locally.
Reference: [39] <author> Dayan, P. and Hinton, G. E. </author> <title> Feudal reinforcement learning. </title> <editor> In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year> <month> 98 </month>
Reference-contexts: As Bradtke notes, this result depends on the structure inherent in the problem. That is, changes 39 in a component of the state vector depend primarily on the influence of neighboring components. Dayan & Hinton <ref> [39] </ref> propose a managerial hierarchy they call Feudal RL. In their scheme, higher-level managers set tasks for lower level managers, and reward them as they see fit. Since the rewards may be different at different levels of the hierarchy, this is not strictly a team.
Reference: [40] <author> Durfee, E. H. </author> <title> Coordination of Distributed Problem Solvers. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: Research in multi-agent systems deals with interactions between self-motivated autonomous agents with conflicting objectives. One of the key issues in DAI research has been finding efficient mechanisms for coordination among multiple intelligent agents. For example, in partial global planning (PGP) <ref> [40] </ref>, after determining their own goals and plans, agents exchange information about how their plans interact and modify their plans in order to better coordinate their activities. They iteratively exchange tentative partial solutions in order to construct global solutions. Coordination may also be achieved by means of negotiation.
Reference: [41] <author> Durfee, E. </author> <title> H. </title> <journal> The distributed artificial intelligence melting pot. IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 21 </volume> <pages> 1301-1306, </pages> <year> 1991. </year>
Reference-contexts: DAI researchers have proposed another answer: build a system consisting of smaller, more manageable components that can communicate and cooperate [55], possibly representing a diverse collection of capabilities and expertise <ref> [41] </ref>. However, one may ask whether even these components are truly manageable, since they are regarded in most DAI research as sophisticated, intelligent agents, themselves.
Reference: [42] <author> Ephrati, E. and Rosenschein, J. S. </author> <title> The Clarke tax as a consensus mechanism among automated agents. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 173-178, </pages> <year> 1991. </year>
Reference-contexts: Coordination may also be achieved by means of negotiation. For example, the contract net protocol [37] assigns tasks to agents on the basis of a bidding mechanism. Agents respond to task announcements with bids indicating how well they believe they can perform the task. The Clarke tax <ref> [42] </ref> is a technique for ensuring that agents reveal their true preferences. Each agent pays a tax corresponding to the portion of its bid that makes a difference to the outcome. Recently, there has been a great deal of interest in incorporating learning into DAI systems [122].
Reference: [43] <author> Fujino, A., Tobita, T., and Yoneda, K. </author> <title> An on-line tuning method for multi-objective control of elevator group. </title> <booktitle> In Proceedings of the International Conference on Industrial Electronics, Control, Instrumentation, and Automation, </booktitle> <pages> pages 795-800, </pages> <year> 1992. </year>
Reference-contexts: Their system adjusts the parameters by evaluating alternative candidate parameters with the neural network. They do not explain what control algorithm is actually used, what its parameters are, or how the network is trained. Hitachi researchers <ref> [43, 112] </ref> use a greedy RSR-like control algorithm that combines multiple objectives such as wait time, travel time, crowding, and power consumption. The weighting of these objectives is accomplished using parameters that are tuned online.
Reference: [44] <author> Guha, R. V. and Lenat, D. B. </author> <title> CYC: A midterm report. </title> <journal> AI Magazine, </journal> <volume> 11 </volume> <pages> 32-59, </pages> <year> 1990. </year>
Reference-contexts: AI systems are often quite brittle, in the sense that they break down badly when faced with problems outside of their often narrow areas of expertise. The traditional answer in AI has been to add more knowledge into the system, including commonsense knowledge <ref> [44] </ref>, but there is increasing skepticism about the feasibility of this approach. DAI researchers have proposed another answer: build a system consisting of smaller, more manageable components that can communicate and cooperate [55], possibly representing a diverse collection of capabilities and expertise [41].
Reference: [45] <author> Gullapalli, V. </author> <title> Reinforcement Learning and Its Application to Control. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <year> 1992. </year> <type> COINS Technical Report 92-10. </type>
Reference-contexts: However, networks of learning automata have a biological plausibility that backpropagation seems to lack [32], and they are applicable to problems more difficult than supervised learning. The two approaches can be effectively combined by using a backpropagation network with associative learning automata as output units. Gullapalli <ref> [45] </ref> applied this architecture successfully to a difficult robotics problem.
Reference: [46] <author> Harsanyi, J. C. </author> <title> Games with incomplete information played by Bayesian players, </title> <booktitle> part 1. Management Science, </booktitle> <volume> 14 </volume> <pages> 159-182, </pages> <year> 1967. </year>
Reference-contexts: Player 2 cooperate defect Player cooperate (3; 3) (0; 5) 1 defect (5; 0) (1; 1) strategy combination which is the worst for the players as a whole, namely, where both defect. 2.1.2 Information Availability Harsanyi <ref> [46] </ref> makes a distinction between imperfect and incomplete information. Imperfect information refers to a lack of knowledge the players have about any previous moves (including their own, those of the other players, or any chance moves).
Reference: [47] <author> Ho, Y.-C. </author> <title> Team decision theory and information structures. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 68 </volume> <pages> 644-654, </pages> <year> 1980. </year>
Reference-contexts: Traditional control theory approaches decentralized control from a cooperative point of view. As Ho puts it <ref> [47] </ref>, "we permit any kind of communication and agreement among the decision makers beforehand." This dissertation, however, focuses on the non-cooperative case, where cooperation must be learned. In strictly competitive (zero-sum) games, it is impossible for the players to receive mutual benefit from cooperation. <p> They can also be connected into networks allowing the actions of one automaton to form part of the context input for other automata, thus allowing a form of explicit communication. The next two sections discuss these configurations. 3.1.4 Teams of Associative Learning Automata In static team decision problems <ref> [114, 47, 71] </ref>, each agent i has a finite set of possible observations Y i and possible actions U i .
Reference: [48] <author> Holusha, J. </author> <title> Want the 99th floor? Getting there may be half the fun. The New York Times, </title> <type> Sunday, </type> <month> January 22, </month> <year> 1995. </year>
Reference-contexts: The Schindler Elevator Company recently developed a system that has destination buttons at all of the hall landings <ref> [48] </ref>. There are no buttons inside the cars, and that helps to offset the cost of the additional keypads at the landings.
Reference: [49] <author> Hsu, K. and Marcus, S. </author> <title> Decentralized control of finite state Markov processes. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 27 </volume> <pages> 426-431, </pages> <year> 1982. </year>
Reference-contexts: The reason is that the common information might contain not only information about the state, but also about the actions selected between time t-k and time t. However, they proved the conjecture to be true for one step delays. Hsu & Marcus <ref> [49] </ref> used this result in 1982 to reduce the one step delay problem to a centralized problem. The common information becomes the more complex state in the equivalent centralized problem. Classical methods could then be used to solve for the optimal policy.
Reference: [50] <author> Humphrys, M. W-learning: </author> <title> Competition among selfish Q-learners. </title> <type> Computer Laboratory Technical Report 362, </type> <institution> University of Cambridge, </institution> <year> 1995. </year>
Reference-contexts: Each behavior's learning algorithm tests the condition bits one at a time for correlation with positive 17 or negative reinforcement. The insect eventually learns a tripod gait, where it lifts three legs at a time. Humphrys <ref> [50] </ref> presents a behavior-based algorithm where the behaviors of a robot are considered to be agents competing for control of the robot. The agents each perform Q-learning [119] with their own separate reward functions.
Reference: [51] <author> Imasaki, N., Kiji, J., and Endo, T. </author> <title> A fuzzy neural network and its application to elevator group control. </title> <editor> In Terano, T., Sugeno, M., Mukaidono, M., and Shigemasu, K., editors, </editor> <title> Fuzzy Engineering Toward Human Friendly Systems. </title> <publisher> IOS Press, </publisher> <address> Amsterdam, </address> <year> 1992. </year>
Reference-contexts: It also raises the question of why Q-learning should be used at all, when the heuristics could be implemented directly. 55 4.2.5 Adaptive and Learning Approaches Imasaki et al <ref> [51] </ref> of Toshiba use a fuzzy neural network to predict passenger waiting time distributions for various sets of control parameters. Their system adjusts the parameters by evaluating alternative candidate parameters with the neural network.
Reference: [52] <author> Jaakkola, T., Singh, S. P., and Jordan, M. I. </author> <title> Reinforcement learning algorithm for partially observable Markov decision problems. </title> <editor> In Tesauro, G., Touretzky, D., and Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1995. </year>
Reference-contexts: It is a blessing when it confounds states where the same action is best, since it reduces the size of the policy space to be searched. However, it is a curse when it confounds states where different actions are best. Jaakkola et al <ref> [52] </ref> present an algorithm that does not attempt to estimate the underlying state, but rather seeks to find a stochastic policy that can work well in spite of perceptual aliasing. The algorithm iterates Monte Carlo policy evaluations with policy improvement steps.
Reference: [53] <author> Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. </author> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 79-87, </pages> <year> 1991. </year> <month> 99 </month>
Reference-contexts: We were able to obtain good elevator performance in spite of ignoring the non-stationarity of the passenger arrival patterns. However, there are several ways one might explicitly address this non-stationarity. One could train different networks for different traffic profiles, and use a gating architecture <ref> [53] </ref> to select the appropriate network. Another possibility would be to provide the networks with additional information about the traffic context or the time of day. The custom in Japanese elevator systems is to signal the car assignment as soon as a hall call has been registered.
Reference: [54] <author> Jagannathan, V. and Dodhiawala, R. </author> <title> Distributed artificial intelligence: An annotated bibliography. </title> <journal> ACM SIGART Newsletter, </journal> <volume> 95 </volume> <pages> 44-56, </pages> <year> 1986. </year>
Reference-contexts: Multi-agent systems are often categorized by the sophistication of their constituent agents. The distributed artificial intelligence (DAI) research community <ref> [58, 59, 54] </ref> has generally focused on building systems that employ very sophisticated local decision makers.
Reference: [55] <author> Jennings, N. R. </author> <title> Controlling cooperative problem solving in industrial multia-gent systems using joint intentions. </title> <journal> Artificial Intelligence, </journal> <volume> 75 </volume> <pages> 195-240, </pages> <year> 1995. </year>
Reference-contexts: The traditional answer in AI has been to add more knowledge into the system, including commonsense knowledge [44], but there is increasing skepticism about the feasibility of this approach. DAI researchers have proposed another answer: build a system consisting of smaller, more manageable components that can communicate and cooperate <ref> [55] </ref>, possibly representing a diverse collection of capabilities and expertise [41]. However, one may ask whether even these components are truly manageable, since they are regarded in most DAI research as sophisticated, intelligent agents, themselves.
Reference: [56] <author> Lakshmivarahan, S. and Narendra, K. S. </author> <title> Learning algorithms for two-person zero-sum stochastic games with incomplete information. </title> <journal> Mathematics of Operations Research, </journal> <volume> 6 </volume> <pages> 379-386, </pages> <year> 1981. </year>
Reference-contexts: The automata are faced with an extreme case of incomplete information. They are not even aware that they are participating 30 in a game. In spite of this handicap, their asymptotic behavior is quite impressive, as shown by a number of analyses. * Two-Person Zero-Sum Games: Lakshmivarahan & Narendra <ref> [56, 57] </ref> show that if a two-person zero-sum game has a unique pure (i.e. deterministic) strategy equilibrium, two automata using the L RI scheme will converge to that solution.
Reference: [57] <author> Lakshmivarahan, S. and Narendra, K. S. </author> <title> Learning algorithms for two-person zero-sum stochastic games with incomplete information: A unified approach. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 20 </volume> <pages> 541-552, </pages> <year> 1982. </year>
Reference-contexts: The automata are faced with an extreme case of incomplete information. They are not even aware that they are participating 30 in a game. In spite of this handicap, their asymptotic behavior is quite impressive, as shown by a number of analyses. * Two-Person Zero-Sum Games: Lakshmivarahan & Narendra <ref> [56, 57] </ref> show that if a two-person zero-sum game has a unique pure (i.e. deterministic) strategy equilibrium, two automata using the L RI scheme will converge to that solution.
Reference: [58] <author> Lesser, V., </author> <title> editor. </title> <booktitle> Proceedings of the First International Conference on Multi-Agent Systems. </booktitle> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1995. </year>
Reference-contexts: Multi-agent systems are often categorized by the sophistication of their constituent agents. The distributed artificial intelligence (DAI) research community <ref> [58, 59, 54] </ref> has generally focused on building systems that employ very sophisticated local decision makers.
Reference: [59] <author> Lesser, V. and Corkill, D. </author> <title> Distributed problem solving. </title> <editor> In Shapiro, S. and Eckroth, D., editors, </editor> <booktitle> Encyclopedia of Artificial Intelligence. </booktitle> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: Multi-agent systems are often categorized by the sophistication of their constituent agents. The distributed artificial intelligence (DAI) research community <ref> [58, 59, 54] </ref> has generally focused on building systems that employ very sophisticated local decision makers.
Reference: [60] <author> Levy, D., Yadin, M., and Alexandrovitz, A. </author> <title> Optimal control of elevators. </title> <journal> International Journal of Systems Science, </journal> <volume> 8 </volume> <pages> 301-320, </pages> <year> 1977. </year>
Reference-contexts: However, the number of states that it can distinguish is vast, making the search for an optimal solution even more complex. One drawback of this hardware is that a passenger can misuse it by pressing more than one button at a time <ref> [60] </ref>. With an appropriate state representation, it should be possible in principle to control such a system by applying the same reinforcement learning techniques we describe in this dissertation. <p> Examples of receding horizon controllers are Finite Intervisit Minimization (FIM) and 53 Empty the System Algorithm (ESA) [6]. FIM attempts to minimize squared waiting times and ESA attempts to minimize the length of the current busy period, the same objective as Levy et al <ref> [60] </ref>. 4.2.3 Rule-Based Approaches In some sense, all control policies could be considered rule-based: IF situation THEN action. However, here we are more narrowly considering the type of production systems commonly used in Artificial Intelligence. Ujihara & Tsuji [117] of Mitsubishi describe the AI-2100 system. <p> The best parameters are then used to control the system. Searching the entire parameter space would be prohibitively expensive, so heuristics are used about which parameter sets to test. Levy et al <ref> [60] </ref> use dynamic programming (DP) o*ine to minimize the expected time needed for completion of the current busy period. The idea of minimizing the busy period also appears in the receding horizon controller ESA (Empty the System Algorithm) [6].
Reference: [61] <author> Lewis, J. </author> <title> A Dynamic Load Balancing Approach to the Control of Multiserver Polling Systems with Applications to Elevator System Dispatching. </title> <institution> Ece department, University of Massachusetts, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: The elevator domain provides an opportunity to compare parallel and distributed control architectures where each controller controls one elevator car, and to monitor the amount of degradation that occurs as the controllers face increasing levels of incomplete state information. A schematic diagram of an elevator system <ref> [61] </ref> is presented in figure 4.1. The elevators cars are represented as filled boxes in the diagram. '+' represents a hall call or someone wanting to enter a car. '' represents a car call or someone wanting to leave a car. <p> They assign downward moving cars to any unassigned hall calls they encounter. The Dynamic Load Balancing (DLB) algorithm attempts to keep the cars evenly spaced by assigning contiguous non-overlapping sectors to each car in a way that balances their loads <ref> [61] </ref>. DLB is a non-greedy algorithm because it reassigns sectors after every event. Pepyne [85] describes a Q-learning elevator dispatching algorithm that uses lookup tables rather than neural networks, and has a drastically reduced state representation containing less than twelve thousand states. <p> The simulator was written by Lewis <ref> [61] </ref>. Passenger arrivals at each floor are assumed to be Poisson, with arrival rates that 58 vary during the course of the day. Our simulations use a traffic profile [6] which dictates arrival rates for every 5-minute interval during a typical afternoon down-peak rush hour.
Reference: [62] <author> Lin, L.-J. and Mitchell, T. M. </author> <title> Memory approaches to reinforcement learning in non-Markovian domains. </title> <type> Technical Report CMU-CS-92-138, </type> <institution> Carnegie Mellon University, </institution> <year> 1992. </year>
Reference-contexts: In these situations, the only chance at being able to distinguish certain states is by remembering past perceptions and actions. This is called the memory-based approach. An example of the memory-based approach is the window-Q architecture of Lin & Mitchell <ref> [62] </ref>. It is an attempt to extend the Q-learning algorithm to non-Markovian domains. For input, it uses not only the current observation, but also a sliding window over the past N observations and actions. <p> In addition, the size of the training problem grows 42 exponentially in the size of the window. Still, this may be a useful method in cases where it is known that only a small memory depth is needed. Lin & Mitchell <ref> [62] </ref> also describe two additional architectures that attempt to distill relevant features out of the past observations and actions (much like sufficient statistics in control theory). Both architectures use recurrent neural networks trained with gradient descent.
Reference: [63] <author> Littman, M. and Boyan, J. </author> <title> A distributed reinforcement learning scheme for network routing. </title> <type> Technical Report CMU-CS-93-165, </type> <institution> Carnegie Mellon University, </institution> <year> 1993. </year>
Reference-contexts: The global reinforcement signal is determined by the deviation from the desired path of the block. It is not clear why Q-learning was used, since this is a non-sequential RL problem where the goal is to maximize the immediate reinforcement at each step. Littman & Boyan <ref> [63] </ref> describe an unusual distributed reinforcement learning algorithm for packet routing based on the asynchronous Bellman-Ford algorithm. Their scheme uses a single Q-function, where each state entry in the Q-function is assigned to a node in the network which is responsible for storing and updating the value of that entry.
Reference: [64] <author> Littman, M. L. </author> <title> Markov games as a framework for multi-agent reinforcement learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: A more recent example is Tesauro's TD-Gammon program [108, 109, 110], which has learned to play strong Master level backgammon. These types of programs are often trained using self-play, and they can generally be viewed as single agents. Littman <ref> [64, 65] </ref> provides a detailed discussion of RL applied to zero-sum games, both in the case where the agents alternate their actions and where they take them simultaneously. Very little work has been done on multi-agent RL in more general non-zero-sum games.
Reference: [65] <author> Littman, M. L. </author> <title> Algorithms for Sequential Decision Making. </title> <type> PhD thesis, </type> <institution> Brown University, </institution> <month> March </month> <year> 1996. </year>
Reference-contexts: The motivation is the necessity of decentralized control in large scale systems where centralized information is not available or is not practical. In operations research, control problems with incomplete state information are refered to as partially observable Markovian decision problems (POMDP's). For an overview, see <ref> [65, 66, 76] </ref>. As is the case in the control literature, a non-adaptive viewpoint is usually taken, where the set of underlying states, transition probabilities, and observation probabilities are assumed to be known a priori. <p> A more recent example is Tesauro's TD-Gammon program [108, 109, 110], which has learned to play strong Master level backgammon. These types of programs are often trained using self-play, and they can generally be viewed as single agents. Littman <ref> [64, 65] </ref> provides a detailed discussion of RL applied to zero-sum games, both in the case where the agents alternate their actions and where they take them simultaneously. Very little work has been done on multi-agent RL in more general non-zero-sum games.
Reference: [66] <author> Lovejoy, W. S. </author> <title> A survey of algorithmic methods for partially observed Markov decision processes. </title> <journal> Annals of Operations Research, </journal> <volume> 28 </volume> <pages> 47-66, </pages> <year> 1991. </year>
Reference-contexts: The motivation is the necessity of decentralized control in large scale systems where centralized information is not available or is not practical. In operations research, control problems with incomplete state information are refered to as partially observable Markovian decision problems (POMDP's). For an overview, see <ref> [65, 66, 76] </ref>. As is the case in the control literature, a non-adaptive viewpoint is usually taken, where the set of underlying states, transition probabilities, and observation probabilities are assumed to be known a priori.
Reference: [67] <author> Luce, R. D. and Raiffa, H. </author> <title> Games and Decisions. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1957. </year>
Reference-contexts: Once again, the situation is more complex for general non-zero-sum games. It is possible for all players to benefit from cooperation. However, there are also cases where pre-game communication can involve threats. Luce & Raiffa <ref> [67] </ref> give an interesting example of this: Table 2.2 An example involving threats.
Reference: [68] <author> Maes, P. and Brooks, R. A. </author> <title> Learning to coordinate behaviors. </title> <booktitle> In AAAI-90 Proceedings, </booktitle> <year> 1990. </year> <month> 100 </month>
Reference-contexts: In behavior-based robotics, the conditions under which behaviors should be activated are generally fixed at design-time by hand. However, when the behaviors and the environment are complex, it becomes difficult for the designer to specify a strategy for all contingencies. Maes & Brooks <ref> [68] </ref> present a distributed learning algorithm that learns to coordinate the behaviors necessary for six-legged walking in an insect-like robot. Each of the robot's six built-in swing-leg-forward behaviors can be considered as an agent. Each agent observes the six perceptual bits that specify which legs are touching the ground.
Reference: [69] <author> Markey, K. L. </author> <title> Efficient learning of multiple degree-of-freedom control problems with quasi-independent Q-agents. </title> <editor> In Mozer, M. C., Smolensky, P., Touret-zky, D. S., Elman, J. L., and Weigend, A. S., editors, </editor> <booktitle> Proceedings of the 1993 Connectionist Models Summer School. </booktitle> <publisher> Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1994. </year>
Reference-contexts: Although much of the work 38 has been in simplistic domains such as grid worlds, several interesting applications have appeared that have pointed to the promise of sequential multi-agent RL. Markey <ref> [69] </ref> applies parallel Q-learning to the problem of controlling a vocal tract model with 10 degrees of freedom. He discusses two architectures equivalent to the distributed and parallel architectures described in section 5.4.
Reference: [70] <author> Markon, S., Kita, H., and Nishikawa, Y. </author> <title> Adaptive optimal elevator group control by use of neural networks. </title> <journal> Transactions of the Institute of Systems, Control, and Information Engineers, </journal> <volume> 7 </volume> <pages> 487-497, </pages> <year> 1994. </year>
Reference-contexts: Markon et al <ref> [70] </ref> have devised a system that trains a neural network to perform immediate call allocation. There are three phases of training. <p> The custom in Japanese elevator systems is to signal the car assignment as soon as a hall call has been registered. This requirement could be satisfied by taking the architecture for immediate call assignments proposed by Markon et al <ref> [70] </ref> (see section 4.2.5), and training it with a sequential RL algorithm. The outputs of the network could be trained to learn Q-values. It would be fascinating to compare the performance of such a centralized immediate call assignment architecture with the architecture we have described.
Reference: [71] <author> Marschak, J. and Radner, R. </author> <title> Economic Theory of Teams. </title> <publisher> Yale University Press, </publisher> <address> New Haven, </address> <year> 1972. </year>
Reference-contexts: They can also be connected into networks allowing the actions of one automaton to form part of the context input for other automata, thus allowing a form of explicit communication. The next two sections discuss these configurations. 3.1.4 Teams of Associative Learning Automata In static team decision problems <ref> [114, 47, 71] </ref>, each agent i has a finite set of possible observations Y i and possible actions U i .
Reference: [72] <author> Mataric, M. J. </author> <title> Issues and approaches in the design of collective autonomous agents. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> 16 </volume> <pages> 321-331, </pages> <year> 1995. </year>
Reference-contexts: Artificial life and behavior-based AI researchers have generally taken the opposite approach, 2 combining large numbers of relatively unsophisticated agents in a bottom-up manner, and seeing what emerges when they are put together into a group <ref> [72, 73] </ref>. The elevator control application developed in this dissertation employs relatively unsophisticated, homogeneous agents that do not communicate. However, these characteristics should not be interpreted as restrictions. <p> Unfortunately, Humphrys does not compare the fitness of his evolved agents with the fitness that could be achieved by using this fitness measure as the direct basis for a reward structure for a Q-learning robot. Mataric <ref> [72] </ref> studies the collective behavior of groups of behavior-based robots. For example, flocking behavior can be obtained by having each robot sum the outputs of its individual avoidance, aggregation, and wandering behaviors. Foraging behavior can be obtained from a more complex combination of behaviors that are sensitive to sensory conditions.
Reference: [73] <editor> Mataric, M. J. </editor> <booktitle> Learning in multi-robot systems. In Proceedings of the IJCAI-95 Workshop on Adaptation and Learning in Multiagent Systems, </booktitle> <year> 1995. </year>
Reference-contexts: Artificial life and behavior-based AI researchers have generally taken the opposite approach, 2 combining large numbers of relatively unsophisticated agents in a bottom-up manner, and seeing what emerges when they are put together into a group <ref> [72, 73] </ref>. The elevator control application developed in this dissertation employs relatively unsophisticated, homogeneous agents that do not communicate. However, these characteristics should not be interpreted as restrictions. <p> Foraging behavior can be obtained from a more complex combination of behaviors that are sensitive to sensory conditions. Mataric has also adapted some reinforcement learning techniques to enable learning in these systems <ref> [73] </ref>. Behaviors take the place of actions, states are clustered into conditions, and shaping speeds the learning process. These changes amount mostly to incorporating some domain knowledge in order to speed up learning.
Reference: [74] <author> McCallum, R. A. </author> <title> Overcoming incomplete perception with utile distinction memory. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <year> 1993. </year>
Reference-contexts: These agents are driven by their perceptions, and in most cases they are not able to perceive the complete state of their environments. Perceptual aliasing 41 [123] occurs when a perception does not uniquely identify a state of the world. As McCallum points out <ref> [74] </ref>, perceptual aliasing can be a blessing or a curse. It is a blessing when it confounds states where the same action is best, since it reduces the size of the policy space to be searched. However, it is a curse when it confounds states where different actions are best. <p> One difficulty with this approach is that the number of underlying states needed in the HMM is not known a priori. Chrisman [29] and McCallum <ref> [74] </ref> both add states incrementally by splitting existing ones. State-splitting was also used by Chapman & Kaelbling [28] in a reinforcement learning context. They built an agent to play a videogame called Amazon. The visual system of their agent had 100 input bits. <p> In order for the scheme to work, the bits had to be relevant in isolation. In Chrisman's approach, states are split incrementally based on statistical tests about environmental predictions. McCallum <ref> [74] </ref> developed an approach similar to Chrisman's that he calls Utile Distinction Memory (UDM). The main difference is that states are split in order to increase the ability to predict rewards rather than perceptions. In a stochastic environment, there will be a large amount of variance in the rewards.
Reference: [75] <author> McCallum, R. A. </author> <title> Instance-based state identification for reinforcement learning. </title> <editor> In Tesauro, G., Touretzky, D., and Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1995. </year>
Reference-contexts: The relevance of pieces of information must be detectable in isolation. This limits the feasibility of UDM in environments where there is extended concealment of crucial features. McCallum <ref> [75] </ref> has developed one of the most promising approaches to RL with hidden state, called instance-based state identification, which applies memory-based learning techniques such as nearest-neighbor to perception-action-reward sequences. He reports orders of magnitude faster learning than previous memory-based hidden state techniques.
Reference: [76] <author> Monahan, G. </author> <title> A survey of partially observable Markov decision processes. </title> <journal> Management Science, </journal> <volume> 28 </volume> <pages> 1-16, </pages> <year> 1982. </year>
Reference-contexts: The motivation is the necessity of decentralized control in large scale systems where centralized information is not available or is not practical. In operations research, control problems with incomplete state information are refered to as partially observable Markovian decision problems (POMDP's). For an overview, see <ref> [65, 66, 76] </ref>. As is the case in the control literature, a non-adaptive viewpoint is usually taken, where the set of underlying states, transition probabilities, and observation probabilities are assumed to be known a priori.
Reference: [77] <author> Moore, A. W. and Atkeson, C. G. </author> <title> Memory-based reinforcement learning: Efficient computation with prioritized sweeping. </title> <editor> In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: This interleaves the process of building and using the model. As the model improves, it naturally becomes more useful in performing backups. Sutton has discussed this approach and calls it the "Dyna" architecture [105, 84]. Moore & Atkeson's "Prioritized Sweeping" algorithm <ref> [77] </ref> builds a forward and backward model incrementally based on observations of the environment. After each real-world transition, the model is updated and some number of backups are performed. A priority queue is used to keep track of the areas of state space where backups would be most useful.
Reference: [78] <author> Nagendra Prasad, M. V. and Lesser, V. R. </author> <title> Learning situation-specific coordination in generalized partial global planning. </title> <booktitle> In AAAI Spring Symposium on Adaptation, Co-Evolution, and Learning in Multiagent Systems, </booktitle> <year> 1996. </year>
Reference-contexts: Whenever performance suffers, they replay traces of their inferences and make changes. Learning is done using heuristic rules for recognizing costly incoherent behavior, identifying the decisions that led to the behavior, and modifying those decisions in the future. 15 Nagendra Prasad & Lesser <ref> [78] </ref> describe a system that learns situation-specific co-ordination strategies. During each problem solving instance, the agents communicate their local views to form a common global view of the situation, agree upon a coordination strategy, and measure its performance.
Reference: [79] <author> Nagendra Prasad, M. V., Lesser, V. R., and Lander, S. E. </author> <title> Learning experiments in a heterogeneous multi-agent system. </title> <booktitle> In Proceedings of the IJCAI-95 Workshop on Adaptation and Learning in Multiagent Systems, </booktitle> <year> 1995. </year>
Reference-contexts: After the learning phase, the agents again communicate to form a global picture of the situation, and use a nearest neighbor technique to select the most appropriate coordination strategy. Nagendra Prasad et al <ref> [79] </ref> present a multi-agent parametric design system called L-TEAM where a set of heterogeneous agents learn appropriate organizational roles. The system is applied to steam condenser design. Agents can choose to initiate a new design, or extend or critique an existing partial design. During learning, roles are chosen probabilistically.
Reference: [80] <author> Narendra, K. S. and Thathachar, M. A. L. </author> <title> Learning Automata: An Introduction. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: Narendra & Thathachar <ref> [80] </ref> give an excellent introduction to this field. 3.1.1 Learning Automata A variable-structure automaton is a quadruple (ff; fi; p; T ) where * ff is the output or action set of the automaton, * fi is the set of possible reinforcement signals it can receive, * p (t) is the <p> It might be particularly useful in nonstationary environments, such as those containing other adaptive agents. The performance of L R*P may also benefit by annealing over the course of learning. 3.1.2 Games of Automata Chapter 8 of <ref> [80] </ref> summarizes much of the work on games of automata. Let G be an N-person game in strategic form. The automata participate in the repeated game G 1 , where the stage-game G is repeated infinitely often. <p> Further, two automata using the L R*P scheme will converge to an equilibrium point of the game whether or not it has a pure strategy equilibrium. * Team Problems: The results of Narendra & Wheeler and Thathachar & Ra-makrishnan are also summarized in <ref> [80] </ref>.
Reference: [81] <author> Ovaska, S. J. </author> <booktitle> Electronics and information technology in high-range elevator systems. Mechatronics, </booktitle> <volume> 2 </volume> <pages> 89-99, </pages> <year> 1992. </year> <month> 101 </month>
Reference-contexts: For this reason, it is difficult to ascertain the relative performance levels of many of these algorithms, and there is no accepted definition of the current state of the art <ref> [81] </ref>. 4.2.1 Zoning Approaches The Otis Elevator Company has used zoning as a starting point in dealing with various traffic patterns [102]. Each car is assigned a zone of the building. It answers hall calls within its zone, and parks there when it is idle.
Reference: [82] <author> Pang, G. K. H. </author> <title> Elevator scheduling system using blackboard architecture. </title> <journal> IEE Proceedings-D, </journal> <volume> 138 </volume> <pages> 337-346, </pages> <year> 1991. </year>
Reference-contexts: RSR evaluates the amount of time each car would take to serve a new hall call given the car's current state and current hall call assignments. It generally selects the car that can respond most quickly. Pang <ref> [82] </ref> advances an extremely simplistic blackboard system with only five rules that pursues essentially the same greedy strategy, assigning calls to the cars that can respond most quickly.
Reference: [83] <author> Papadimitriou, C. H. and Tsitsiklis, J. N. </author> <title> The complexity of Markov decision processes. </title> <journal> Mathematics of Operations Research, </journal> <volume> 12 </volume> <pages> 441-450, </pages> <year> 1987. </year>
Reference-contexts: However, in many problems of interest, the 22 state space is so large that even polynomial time is completely infeasible. Although deterministic versions of an MDP can be solved very fast in parallel, Papadimitriou & Tsitsiklis <ref> [83] </ref> show that in general MDPs are P-complete, and thus most likely cannot be solved significantly faster with highly parallel algorithms. 2.3.5 Non-Classical Information Patterns Most of control theory has dealt with the classical information pattern. <p> Still, the number of points in the new state space that must be considered generally grows exponentially in the length of the observation history, and so all but the smallest of these problems are intractable. Papadimitriou & Tsitsiklis <ref> [83] </ref> show that finite-horizon POMDPs are PSPACE-complete, and that "most likely, it is not possible to have an efficient on-line implementation (involving polynomial time on-line computations and memory) of an optimal policy, even if an arbitrary amount of precomputation is allowed." Finally, they do not even consider infinite horizon POMDPs because
Reference: [84] <author> Peng, J. and Williams, R. J. </author> <title> Efficient search control in Dyna. </title> <booktitle> In Proceedings of the Second International Conference on Simulation of Adaptive Behavior. </booktitle> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: This interleaves the process of building and using the model. As the model improves, it naturally becomes more useful in performing backups. Sutton has discussed this approach and calls it the "Dyna" architecture <ref> [105, 84] </ref>. Moore & Atkeson's "Prioritized Sweeping" algorithm [77] builds a forward and backward model incrementally based on observations of the environment. After each real-world transition, the model is updated and some number of backups are performed.
Reference: [85] <author> Pepyne, D. L. </author> <title> Application of Q-learning to elevator dispatching. </title> <institution> Ece department, University of Massachusetts, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: The Dynamic Load Balancing (DLB) algorithm attempts to keep the cars evenly spaced by assigning contiguous non-overlapping sectors to each car in a way that balances their loads [61]. DLB is a non-greedy algorithm because it reassigns sectors after every event. Pepyne <ref> [85] </ref> describes a Q-learning elevator dispatching algorithm that uses lookup tables rather than neural networks, and has a drastically reduced state representation containing less than twelve thousand states.
Reference: [86] <author> Pepyne, D. L. and Cassandras, C. G. </author> <title> Concurrent estimation for on-line adaptive control of elevator systems during uppeak traffic. </title> <note> In Preparation. </note>
Reference-contexts: Evaluations of the function could be performed using Monte Carlo simulations, which would not be at all computationally prohibitive. Pepyne & Cassandras <ref> [86] </ref> suggest an even more efficient method called concurrent estimation which is based on the theory of perturbation analysis and involves using a single sequence of events to drive a set of simulators each running under a different value of the control variable.
Reference: [87] <author> Pepyne, D. L. and Cassandras, C. G. </author> <title> Optimal dispatching control for elevator systems during uppeak traffic. </title> <note> Submitted to CDC-96. </note>
Reference-contexts: Once the doors have closed, there is really no 49 choice about the next actions: the car calls registered by the passengers must be serviced in ascending order and the empty car must then return to the lobby. Pepyne & Cassandras <ref> [87] </ref> show that the optimal policy for handling pure up-peak traffic is a threshold-based policy that closes the doors after an optimal number of passengers have entered the car.
Reference: [88] <author> Phansalkar, V. V. and Thathachar, M. A. L. </author> <title> Local and global optimization algorithms for generalized learning automata. </title> <journal> Neural Computation, </journal> <volume> 7 </volume> <pages> 950-973, </pages> <year> 1995. </year>
Reference-contexts: Williams [124] proved that for a class of learning algorithms that includes networks of A RI units, their expected update direction in weight space follows the gradient of the expected reinforcement as a function of the weights. The long-term behavior of these networks was analyzed by Phansalkar & Thathatchar <ref> [88] </ref> using weak convergence techniques. Networks of associative learning automata perform a statistical analogue to the type of gradient descent carried out by error backpropagation [91].
Reference: [89] <author> Rabiner, L. R. and Juang, B. H. </author> <title> An introduction to hidden Markov models. </title> <journal> IEEE ASSP Magazine, </journal> <volume> 3 </volume> <pages> 4-16, </pages> <year> 1986. </year>
Reference-contexts: The influence of the previous state estimate implicitly incorporates a memory of past perceptions and actions. The main idea behind this approach is that if the HMM is able to make accurate predictions, then its state representation should also be sufficient to restore the Markov property. Rabiner & Juang <ref> [89] </ref> provide an excellent overview of HMMs.
Reference: [90] <author> Ring, M. </author> <title> Learning sequential tasks by incrementally adding higher orders. </title> <editor> In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: Another problem with both recurrent architectures is that it is again difficult to know what the size of the networks should be ahead of time. Ring <ref> [90] </ref> describes a non-recurrent network architecture for learning sequential tasks. The architecture incrementally adds "high level" units as training progresses. A unit is added whenever a weight is being pulled strongly in both directions.
Reference: [91] <author> Rumelhart, D. E., McClelland, J. L., </author> <title> and the PDP Research Group. Parallel Distributed Processing: Explorations in the Microstructure of Cognition. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: The long-term behavior of these networks was analyzed by Phansalkar & Thathatchar [88] using weak convergence techniques. Networks of associative learning automata perform a statistical analogue to the type of gradient descent carried out by error backpropagation <ref> [91] </ref>. Barto & Jordan [12] compared the backpropagation algorithm with a network of modified A RP units that handle real-valued reinforcement and use a batching method to increase learning efficiency. <p> Let x y and t x t y . Go to step 1. 5.3 The Networks Used to Store the Q-Values Using lookup tables to store the Q-values was ruled out for such a large system. Instead, we used feedforward neural networks trained with the error backpropagation algorithm <ref> [91] </ref>. The networks receive some of the state information as input, and produce Q-value estimates as output. The Q-value estimates can be written as ^ Q (x; a; ), where is a vector of the parameters or weights of the networks.
Reference: [92] <author> Rummery, G. A. and Niranjan, M. </author> <title> On-line Q-learning using connectionist systems. </title> <type> CUED/F-INFENG/TR 166, </type> <institution> Cambridge University Engineering Department, </institution> <year> 1994. </year>
Reference-contexts: In practice, it is useful to balance the need for exploration with the need to select the best actions. There are several methods for doing this, including the selection of actions according to a Boltzmann distribution or the methods discussed by Sato et al [97]. 37 Sarsa <ref> [92] </ref> is a modification of the Q-learning rule based more closely on temporal difference learning [104]. <p> Rummery & Niran-jan <ref> [92] </ref> argue that if the amount of exploration is reduced as training proceeds, the greedy action will eventually be chosen at each step, and the Q-function will converge to the optimal values. 3.2.2 Policy-Based RL Techniques Most of the sequential RL algorithms discussed above attempt to learn the values of states
Reference: [93] <author> Sabourian, H. </author> <title> Repeated games: A survey. </title> <editor> In Hahn, F., editor, </editor> <title> The Economics of Missing Markets, Information, and Games. </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1989. </year>
Reference-contexts: The repeated play of non-cooperative games allows the implicit development of cooperation, and the repeated play of games of incomplete information allows the gathering of information about some of the rules of the game (such as one's own payoff structure). The literature on RGs has been surveyed by Sabourian <ref> [93] </ref> and Aumann [3]. Much of the work on RGs addresses competitive games, and is thus not directly applicable to team problems. An RG can be defined as a one-shot game repeated many times. If a one-shot game is repeated an infinite number of times it is called a supergame. <p> An RG can be defined as a one-shot game repeated many times. If a one-shot game is repeated an infinite number of times it is called a supergame. RGs can possess a large number of equilibria. The Folk Theorem of RGs <ref> [93] </ref> states that any norm of behavior that guarantees payoffs to players more than their security levels (the minimum payoffs within their own control) can be supported as an equilibrium of a supergame. This can be proved by constructing strategies that punish any player that deviates from the norm.
Reference: [94] <author> Sakai, Y. and Kurosawa, K. </author> <title> Development of elevator supervisory group control system with artificial intelligence. </title> <journal> Hitachi Review, </journal> <volume> 33 </volume> <pages> 25-30, </pages> <year> 1984. </year>
Reference-contexts: The goal of the zoning approach is to keep the cars reasonably well separated and thus keep the interval down. The drawback of this approach seems to be a significant loss of flexibility. Several papers include zoning in a comparison with other strategies [6, 16]. Sakai & Kurosawa <ref> [94] </ref> of Hitachi describe a concept called area control that is related to zoning. If possible, it assigns a hall call to a car that already must stop at that floor due to a car call.
Reference: [95] <author> Samuel, A. L. </author> <title> Some studies in machine learning using the game of checkers. </title> <editor> In Feigenbaum, E. and Feldman, J., editors, </editor> <booktitle> Computers and Thought. </booktitle> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1963. </year>
Reference-contexts: In addition to the multi-agent RL research concerned with team problems, a significant amount of work has focused on zero-sum games, where a single agent learns to play against an opponent. One of the earliest examples of this is Samuel's checker-playing program <ref> [95] </ref>. A more recent example is Tesauro's TD-Gammon program [108, 109, 110], which has learned to play strong Master level backgammon. These types of programs are often trained using self-play, and they can generally be viewed as single agents.
Reference: [96] <author> Sandholm, T. W. and Crites, R. H. </author> <title> Multiagent reinforcement learning in the iterated prisoner's dilemma. </title> <journal> Biosystems, </journal> <volume> 37 </volume> <pages> 147-166, </pages> <year> 1996. </year> <month> 102 </month>
Reference-contexts: Littman [64, 65] provides a detailed discussion of RL applied to zero-sum games, both in the case where the agents alternate their actions and where they take them simultaneously. Very little work has been done on multi-agent RL in more general non-zero-sum games. Sandholm & Crites <ref> [96] </ref> study the behavior of multi-agent RL in the context of the iterated prisoner's dilemma (see section 2.1). They show that Q-learning agents are able to learn the optimal strategy against the fixed opponent Tit-for-Tat.
Reference: [97] <author> Sato, M., Abe, K., and Takeda, H. </author> <title> Learning control of finite Markov chains with explicit tradeoff between estimation and control. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 18 </volume> <pages> 677-684, </pages> <year> 1988. </year>
Reference-contexts: In practice, it is useful to balance the need for exploration with the need to select the best actions. There are several methods for doing this, including the selection of actions according to a Boltzmann distribution or the methods discussed by Sato et al <ref> [97] </ref>. 37 Sarsa [92] is a modification of the Q-learning rule based more closely on temporal difference learning [104].
Reference: [98] <author> Sen, S., Sekaran, M., and Hale, J. </author> <title> Learning to coordinate without sharing information. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 426-431, </pages> <year> 1994. </year>
Reference-contexts: Some of the hunters receive sensory information from each other or from scouts. Others use and update a single shared policy or exchange their policies or experiences, speeding up the learning process. Sen & Sekaran <ref> [98] </ref> apply distributed Q-learning to a simple block pushing problem. Each agent independently selects the direction and magnitude of the force it will exert on a block. The resultant force is determined by vector addition.
Reference: [99] <author> Shoham, Y. and Tennenholtz, M. </author> <title> Emergent conventions in multi-agent systems: Initial experimental results and observations. </title> <booktitle> In Proceedings of KR-92. </booktitle>
Reference-contexts: The approach is to build less sophisticated agents, and see what emerges when they are put together into a group. Shoham & Tennenholtz <ref> [99, 100] </ref> investigate the social behavior that can emerge from agents with simple learning rules. They focus on two simple n-k-g iterative games, where n agents meet k at a time (randomly) to play game g.
Reference: [100] <author> Shoham, Y. and Tennenholtz, M. </author> <title> Co-learning and the evolution of coordinated multi-agent activity. </title> <year> 1993. </year>
Reference-contexts: The approach is to build less sophisticated agents, and see what emerges when they are put together into a group. Shoham & Tennenholtz <ref> [99, 100] </ref> investigate the social behavior that can emerge from agents with simple learning rules. They focus on two simple n-k-g iterative games, where n agents meet k at a time (randomly) to play game g.
Reference: [101] <author> Siikonen, M.-L. </author> <title> Elevator traffic simulation. </title> <journal> Simulation, </journal> <volume> 61 </volume> <pages> 257-267, </pages> <year> 1993. </year>
Reference-contexts: The large number of stops will cause significantly longer round-trip times than in heavy down traffic, where each car may fill up after only a few stops at upper floors. For this reason, down-peak handling capacity is much greater than up-peak capacity. Siikonen <ref> [101] </ref> illustrates these differences in an excellent graph obtained through extensive simulations. Since up-peak handling capacity is a limiting factor, elevator systems are designed by predicting the heaviest likely up-peak demand in a building, and then determining a configuration that can accomodate that demand. <p> If up-peak capacity is sufficient, then down-peak generally will be also. Up-peak traffic is the easiest type to analyze, since all passengers enter cars at the lobby, their destination floors are serviced in ascending order, and empty cars then return to the lobby. The standard capacity calculations <ref> [102, 101] </ref> assume that each car leaves the lobby with M passengers (80 to 100 percent of its capacity) and that the average passenger's likelihood of selecting each destination floor is known. Then probability theory is used to determine the average number of stops needed on each round trip. <p> For this reason, a down-peak traffic pattern is used as a testbed for this dissertation. Before describing the testbed in detail, we review various elevator control strategies from the literature. 4.2 Elevator Control Strategies The oldest relay-based automatic controllers used the principle of collective control <ref> [102, 101] </ref>, where cars always stop at the nearest call in their running direction. <p> Greedy algorithms give up some measure of performance due to their lack of flexibility, but also require less computation time. In western countries, an arriving car generally signals waiting passengers when it begins to decelerate <ref> [101] </ref>, allowing the use of a non-greedy algorithm. The custom in Japan is to signal the car assignment immediately upon call registration. This type of signalling requires the use of a greedy algorithm.
Reference: [102] <author> Strakosch, G. R. </author> <title> Vertical Transportation: </title> <editor> Elevators and Escalators. </editor> <publisher> Wiley and Sons, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: If up-peak capacity is sufficient, then down-peak generally will be also. Up-peak traffic is the easiest type to analyze, since all passengers enter cars at the lobby, their destination floors are serviced in ascending order, and empty cars then return to the lobby. The standard capacity calculations <ref> [102, 101] </ref> assume that each car leaves the lobby with M passengers (80 to 100 percent of its capacity) and that the average passenger's likelihood of selecting each destination floor is known. Then probability theory is used to determine the average number of stops needed on each round trip. <p> Assuming that the cars are evenly spaced, the average waiting time is one half the interval. In reality, the average wait is somewhat longer. Dividing a building into zones significantly increases the up-peak capacity, but at the expense of much longer intervals, and thus longer average wait times. Strakosch <ref> [102] </ref> provides an example of this in a 12 floor building with 6 cars. By dividing the building into an upper and a lower zone, and assigning 3 cars to each zone, the number of stops needed and thus the round trip times can be reduced. <p> For this reason, a down-peak traffic pattern is used as a testbed for this dissertation. Before describing the testbed in detail, we review various elevator control strategies from the literature. 4.2 Elevator Control Strategies The oldest relay-based automatic controllers used the principle of collective control <ref> [102, 101] </ref>, where cars always stop at the nearest call in their running direction. <p> this reason, it is difficult to ascertain the relative performance levels of many of these algorithms, and there is no accepted definition of the current state of the art [81]. 4.2.1 Zoning Approaches The Otis Elevator Company has used zoning as a starting point in dealing with various traffic patterns <ref> [102] </ref>. Each car is assigned a zone of the building. It answers hall calls within its zone, and parks there when it is idle. The goal of the zoning approach is to keep the cars reasonably well separated and thus keep the interval down.
Reference: [103] <author> Sugawara, T. and Lesser, V. </author> <title> Learning coordination plans in distributed problem-solving environments. </title> <booktitle> In Twelfth International Workshop on Distributed Artificial Intelligence, </booktitle> <year> 1993. </year>
Reference-contexts: Each agent pays a tax corresponding to the portion of its bid that makes a difference to the outcome. Recently, there has been a great deal of interest in incorporating learning into DAI systems [122]. Weiss has provided a useful bibliography [121]. Sugawara & Lesser <ref> [103] </ref> have developed a system of knowledge-based local area network diagnosis agents that learn situation-specific coordination strategies. The agents start by acting based only on their local views. Whenever performance suffers, they replay traces of their inferences and make changes.
Reference: [104] <author> Sutton, R. S. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: There are several methods for doing this, including the selection of actions according to a Boltzmann distribution or the methods discussed by Sato et al [97]. 37 Sarsa [92] is a modification of the Q-learning rule based more closely on temporal difference learning <ref> [104] </ref>.
Reference: [105] <author> Sutton, R. S. </author> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <year> 1990. </year>
Reference-contexts: This interleaves the process of building and using the model. As the model improves, it naturally becomes more useful in performing backups. Sutton has discussed this approach and calls it the "Dyna" architecture <ref> [105, 84] </ref>. Moore & Atkeson's "Prioritized Sweeping" algorithm [77] builds a forward and backward model incrementally based on observations of the environment. After each real-world transition, the model is updated and some number of backups are performed. <p> Of course, it is possible to use value-based RL algorithms in non-sequential problems, for example, Q-learning with fl = 0. Likewise, there are policy-based sequential RL algorithms, such as actor/critic algorithms <ref> [14, 15, 105] </ref>, where the actor implements a stochastic policy that maps states to action probability vectors, and the critic attempts to estimate the value of each state under the current policy in order to provide more useful feedback to the actor.
Reference: [106] <author> Tan, M. </author> <title> Learning a cost-sensitive internal representation for reinforcement learning. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning (ML91), </booktitle> <year> 1991. </year>
Reference-contexts: There are two main approaches that attempt to identify hidden states. The first could be called the sensor-based approach. The idea is to use additional perceptual actions to disambiguate the state. Examples of this include work by Whitehead & Ballard [123] and Tan <ref> [106] </ref>. One disadvantage of the algorithms they present is the assumption of a deterministic environment. A limitation of sensor-based approaches in general is that they cannot handle what Chrisman calls incomplete perception [30]. This refers to situations where no perceptual actions can be taken to disambiguate world states.
Reference: [107] <author> Tan, M. </author> <title> Multi-agent reinforcement learning: Independent vs. </title> <booktitle> cooperative agents. In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <year> 1993. </year>
Reference-contexts: The motivation for the architecture is to allow learning to occur at multiple levels of resolution. One key question is whether the organizational structure must be built-in ahead of time or whether it could be learned. Tan <ref> [107] </ref> reports on some simple hunter-prey experiments with multi-agent RL. His focus is on the sharing of sensory information, policies, and experience among the agents. Some of the hunters receive sensory information from each other or from scouts.
Reference: [108] <author> Tesauro, G. </author> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 257-277, </pages> <year> 1992. </year>
Reference-contexts: In addition, one need not be concerned about the performance level of a simulated system during training. A successful example of simulated online training is found in Tesauro's TD-Gammon system <ref> [108, 109, 110] </ref>, which used RL techniques to learn to play strong master-level backgammon. RL algorithms can be used as components of multi-agent algorithms. If the members of a group of agents each employs one of these algorithms, a new collective algorithm emerges for the group as a whole. <p> One of the earliest examples of this is Samuel's checker-playing program [95]. A more recent example is Tesauro's TD-Gammon program <ref> [108, 109, 110] </ref>, which has learned to play strong Master level backgammon. These types of programs are often trained using self-play, and they can generally be viewed as single agents. <p> At the beginning of the learning process, the agents are all extremely inept. With gradual annealing they are all able to raise their performance levels in parallel. Tesauro <ref> [108, 109, 110] </ref> notes a slightly different but related phenomenon in the context of zero-sum games, where training with self-play allows an agent to learn with a well-matched opponent during each stage of its development. 6.4 Omniscient Versus Online Reinforcements This section examines the relative performance of the omniscient and online
Reference: [109] <author> Tesauro, G. </author> <title> TD-Gammon, a self-teaching backgammon program, achieves master-level play. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 215-219, </pages> <year> 1994. </year>
Reference-contexts: In addition, one need not be concerned about the performance level of a simulated system during training. A successful example of simulated online training is found in Tesauro's TD-Gammon system <ref> [108, 109, 110] </ref>, which used RL techniques to learn to play strong master-level backgammon. RL algorithms can be used as components of multi-agent algorithms. If the members of a group of agents each employs one of these algorithms, a new collective algorithm emerges for the group as a whole. <p> One of the earliest examples of this is Samuel's checker-playing program [95]. A more recent example is Tesauro's TD-Gammon program <ref> [108, 109, 110] </ref>, which has learned to play strong Master level backgammon. These types of programs are often trained using self-play, and they can generally be viewed as single agents. <p> At the beginning of the learning process, the agents are all extremely inept. With gradual annealing they are all able to raise their performance levels in parallel. Tesauro <ref> [108, 109, 110] </ref> notes a slightly different but related phenomenon in the context of zero-sum games, where training with self-play allows an agent to learn with a well-matched opponent during each stage of its development. 6.4 Omniscient Versus Online Reinforcements This section examines the relative performance of the omniscient and online
Reference: [110] <author> Tesauro, G. </author> <title> Temporal difference learning and TD-Gammon. </title> <journal> Communications of the ACM, </journal> <volume> 38 </volume> <pages> 58-68, </pages> <year> 1995. </year>
Reference-contexts: In addition, one need not be concerned about the performance level of a simulated system during training. A successful example of simulated online training is found in Tesauro's TD-Gammon system <ref> [108, 109, 110] </ref>, which used RL techniques to learn to play strong master-level backgammon. RL algorithms can be used as components of multi-agent algorithms. If the members of a group of agents each employs one of these algorithms, a new collective algorithm emerges for the group as a whole. <p> One of the earliest examples of this is Samuel's checker-playing program [95]. A more recent example is Tesauro's TD-Gammon program <ref> [108, 109, 110] </ref>, which has learned to play strong Master level backgammon. These types of programs are often trained using self-play, and they can generally be viewed as single agents. <p> At the beginning of the learning process, the agents are all extremely inept. With gradual annealing they are all able to raise their performance levels in parallel. Tesauro <ref> [108, 109, 110] </ref> notes a slightly different but related phenomenon in the context of zero-sum games, where training with self-play allows an agent to learn with a well-matched opponent during each stage of its development. 6.4 Omniscient Versus Online Reinforcements This section examines the relative performance of the omniscient and online
Reference: [111] <author> Tham, C. and Prager, R. </author> <title> A modular Q-learning architecture for manipulator task decomposition. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year> <month> 103 </month>
Reference-contexts: Their goal is to learn a sequence of actions corresponding to a particular articulatory trajectory. The parallel Q-learning architecture is able to handle a problem with 55 million possible action combinations that would be unthinkable using a centralized architecture. Tham & Prager <ref> [111] </ref> briefly mention the same idea of having a separate Q-network independently control each actuator. Bradtke [23] describes some initial experiments using RL for the decentralized control of a flexible beam. The task is to efficiently damp out disturbances of a beam by applying forces at discrete locations and times.
Reference: [112] <author> Tobita, T., Fujino, A., Inaba, H., Yoneda, K., and Ueshima, T. </author> <title> An elevator characterized group supervisory control system. </title> <booktitle> In Proceedings of IECON, </booktitle> <pages> pages 1972-1976, </pages> <year> 1991. </year>
Reference-contexts: Tobita et al <ref> [112] </ref> of Hitachi describe a system where car assignment occurs when a hall button is pressed. They indicate that their main contribution was to attempt to reduce not only the wait time, but also travel time and crowding, and to let the building manager decide the priorities. <p> Their system adjusts the parameters by evaluating alternative candidate parameters with the neural network. They do not explain what control algorithm is actually used, what its parameters are, or how the network is trained. Hitachi researchers <ref> [43, 112] </ref> use a greedy RSR-like control algorithm that combines multiple objectives such as wait time, travel time, crowding, and power consumption. The weighting of these objectives is accomplished using parameters that are tuned online.
Reference: [113] <author> Tsetlin, M. L. </author> <title> Automaton Theory and Modeling of Biological Systems. </title> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1973. </year>
Reference-contexts: "complex information processing systems involving very large numbers of sensors and effectors, a central processor will require very large bandwidths for responding to sensors or activating effectors." As he says, it is difficult to imagine "an army whose commanding general alone is authorized to make all the field decisions." Tsetlin <ref> [113] </ref> makes the same observation: "If one assumes that all control proceeds from the top down to a specific address, then the system becomes very complex.... but if the conditions of a game are given, then the automata find the required actions by themselves.
Reference: [114] <author> Tsitsiklis, J. N. and Athans, M. </author> <title> On the complexity of decentralized decision making and detection problems. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 30 </volume> <pages> 440-446, </pages> <year> 1985. </year>
Reference-contexts: They can also be connected into networks allowing the actions of one automaton to form part of the context input for other automata, thus allowing a form of explicit communication. The next two sections discuss these configurations. 3.1.4 Teams of Associative Learning Automata In static team decision problems <ref> [114, 47, 71] </ref>, each agent i has a finite set of possible observations Y i and possible actions U i . <p> Tsitsiklis & Athans <ref> [114] </ref> have shown that this problem is NP hard, even in the case where there are only two agents, each with only two possible actions, and where the payoff function r only takes on the values 0 and 1.
Reference: [115] <author> Tsitsiklis, J. N. and Van Roy, B. </author> <title> An analysis of temporal-difference learning with function approximation. </title> <type> Technical Report LIDS-P-2322, </type> <institution> MIT, </institution> <year> 1996. </year>
Reference-contexts: By using compact function approximators such as artificial neural networks, they can represent value functions that are too massive to store as lookup tables. This allows generalization that can speed up learning, but that also removes most theoretical guarantees of convergence <ref> [21, 22, 115] </ref>. When a probability model of the environment is not available, two basic approaches are generally used. Indirect approaches first estimate an explicit probability model of the environment, and then use model-based techniques. Direct approaches do not learn an explicit probability model of the environment.
Reference: [116] <author> Ujihara, H. and Amano, M. </author> <title> The latest elevator group-control system. </title> <journal> Mitsubishi Electric Advance, </journal> <volume> 67 </volume> <pages> 10-12, </pages> <year> 1994. </year>
Reference-contexts: Displays placed in the lobby indicate the floors each car will be servicing. This reduces the wait times and the travel times, particularly to the highest floors, by reducing the number of stops each car needs to make. Mitsubishi's latest elevator system <ref> [116] </ref> also adds destination buttons in the lobby. When a passenger presses a destination button, the controller immediately allocates the call to an appropriate car, activates its hall lantern and chime to show the passenger where to go, and updates the service area indicators. <p> The discrepancies were then analyzed by the experts, whose knowledge about solving such problems was used to create fuzzy control rules. The fuzziness lies in the IF part of the rules. Ujihara & Amano <ref> [116] </ref> describe the latest changes to the AI-2100 system. A previous version used a fixed evaluation formula based on the current car positions and call locations. A more recent version considers future car positions and probable future hall calls.
Reference: [117] <author> Ujihara, H. and Tsuji, S. </author> <title> The revolutionary AI-2100 elevator-group control system and the new intelligent option series. </title> <journal> Mitsubishi Electric Advance, </journal> <volume> 45 </volume> <pages> 5-8, </pages> <year> 1988. </year>
Reference-contexts: However, here we are more narrowly considering the type of production systems commonly used in Artificial Intelligence. Ujihara & Tsuji <ref> [117] </ref> of Mitsubishi describe the AI-2100 system. It uses expert-system and fuzzy-logic technologies. They claim that experts in group-supervisory control have the experience and knowledge necessary to shorten waiting times under various traffic conditions, but admit that expert knowledge is fragmentary, hard to organize, and difficult to incorporate.
Reference: [118] <author> Varaiya, P. and Walrand, J. </author> <title> On delayed sharing patterns. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 23 </volume> <pages> 443-445, </pages> <year> 1978. </year>
Reference-contexts: Witsenhausen [126] conjectured in 1971 that the optimal control strategy in such cases could be made to depend on the common information only through the conditional distribution of the state given the common information. Varaiya & Wal-rand <ref> [118] </ref> gave a counterexample to this in 1978 for delays of more than one step. The reason is that the common information might contain not only information about the state, but also about the actions selected between time t-k and time t.
Reference: [119] <author> Watkins, C. J. C. H. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge University, </institution> <year> 1989. </year>
Reference-contexts: The insect eventually learns a tripod gait, where it lifts three legs at a time. Humphrys [50] presents a behavior-based algorithm where the behaviors of a robot are considered to be agents competing for control of the robot. The agents each perform Q-learning <ref> [119] </ref> with their own separate reward functions. Each agent suggests an action with some weight based on its Q-values and the robot executes the action with the highest weight. <p> A priority queue is used to keep track of the areas of state space where backups would be most useful. If an observation is surprising, then that state's predecessors are promoted to the top of the queue. Watkins developed a direct approach called "Q-learning" <ref> [119] </ref>. Instead of learning the value of states, this algorithm learns the value of state-action pairs. Q (x; a) is defined to be the expected discounted return for performing action a in state x, and performing optimally thereafter. <p> K (b; d) is the time needed for completion of the busy period given state b and decision d, and all subsequent decisions optimal. This definition, with its state-action pair, seems almost prophetic of the Q-learning algorithm developed 12 years later by Watkins <ref> [119] </ref>. No discount factor is used, since it is assumed that the values will all be finite. The major difference between this and Q-learning is that it must be performed o*ine since it uses a model of the transition probabilities of the system and performs sweeps of the state space. <p> Therefore, we employ a discrete event version of the Q-learning algorithm since it considers only events encountered in actual system trajectories and does not require a model of the state transition probabilities. The Q-learning update rule <ref> [119] </ref> takes on the following discrete event form: ^ Q (x; a) = ff [ t x b where action a is taken from state x at time t x , the next decision is required from state y at time t y , ff is the learning rate parameter, and
Reference: [120] <author> Weiss, G. </author> <title> Some studies in distributed machine learning and organizational design. </title> <type> FKI 189-94, </type> <institution> Institut fur Informatik, Technische Universitat Muenchen, </institution> <year> 1994. </year>
Reference-contexts: During learning, roles are chosen probabilistically. At the end of each problem solving episode, a credit assignment process determines the rating for each role selection. After learning, each agent chooses its highest rated role for each situation. In an effort to combine distributed learning and organizational design, Weiss <ref> [120] </ref> considers a blocks world with heterogeneous agents, where each agent is able to perform only a limited number of operations. For example, one agent may only be able to put block F on block A or block G on block B.
Reference: [121] <author> Weiss, G. </author> <title> Adaptation and learning in multi-agent systems: Some remarks and a bibliography. </title> <booktitle> In Adaptation and Learning in Multi-Agent Systems, </booktitle> <address> Berlin, </address> <year> 1996. </year> <title> Springer Verlag. </title> <booktitle> Lecture Notes in Artificial Intelligence, </booktitle> <volume> Volume 1042. </volume>
Reference-contexts: Each agent pays a tax corresponding to the portion of its bid that makes a difference to the outcome. Recently, there has been a great deal of interest in incorporating learning into DAI systems [122]. Weiss has provided a useful bibliography <ref> [121] </ref>. Sugawara & Lesser [103] have developed a system of knowledge-based local area network diagnosis agents that learn situation-specific coordination strategies. The agents start by acting based only on their local views. Whenever performance suffers, they replay traces of their inferences and make changes.
Reference: [122] <author> Weiss, G. and Sen, S. </author> <title> Adaptation and Learning in Multi-Agent Systems. </title> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1996. </year> <booktitle> Lecture Notes in Artificial Intelligence, </booktitle> <volume> Volume 1042. </volume>
Reference-contexts: There also tends to be less of an emphasis on learning in such systems, though this is starting to change <ref> [122] </ref>. Artificial life and behavior-based AI researchers have generally taken the opposite approach, 2 combining large numbers of relatively unsophisticated agents in a bottom-up manner, and seeing what emerges when they are put together into a group [72, 73]. <p> The Clarke tax [42] is a technique for ensuring that agents reveal their true preferences. Each agent pays a tax corresponding to the portion of its bid that makes a difference to the outcome. Recently, there has been a great deal of interest in incorporating learning into DAI systems <ref> [122] </ref>. Weiss has provided a useful bibliography [121]. Sugawara & Lesser [103] have developed a system of knowledge-based local area network diagnosis agents that learn situation-specific coordination strategies. The agents start by acting based only on their local views.
Reference: [123] <author> Whitehead, S. D. and Ballard, D. H. </author> <title> Active perception and reinforcement learning. </title> <journal> Neural Computation, </journal> <volume> 2 </volume> <pages> 409-419, </pages> <year> 1990. </year>
Reference-contexts: These agents are driven by their perceptions, and in most cases they are not able to perceive the complete state of their environments. Perceptual aliasing 41 <ref> [123] </ref> occurs when a perception does not uniquely identify a state of the world. As McCallum points out [74], perceptual aliasing can be a blessing or a curse. <p> There are two main approaches that attempt to identify hidden states. The first could be called the sensor-based approach. The idea is to use additional perceptual actions to disambiguate the state. Examples of this include work by Whitehead & Ballard <ref> [123] </ref> and Tan [106]. One disadvantage of the algorithms they present is the assumption of a deterministic environment. A limitation of sensor-based approaches in general is that they cannot handle what Chrisman calls incomplete perception [30].
Reference: [124] <author> Williams, R. J. </author> <title> Toward a theory of reinforcement-learning connectionist systems. </title> <type> Technical Report NU-CCS-88-3, </type> <institution> Northeastern University, </institution> <year> 1988. </year>
Reference-contexts: Barto [7] studied the learning behavior of networks of A RP units and showed empirical results on their ability to learn nonlinear tasks, including an exclusive-or problem and a multiplexor problem. Williams <ref> [124] </ref> proved that for a class of learning algorithms that includes networks of A RI units, their expected update direction in weight space follows the gradient of the expected reinforcement as a function of the weights.
Reference: [125] <author> Witsenhausen, H. S. </author> <title> A counterexample in stochastic optimum control. </title> <journal> SIAM Journal of Control, </journal> <volume> 6 </volume> <pages> 138-147, </pages> <year> 1968. </year>
Reference-contexts: Under the classical pattern, a wide variety of powerful results have been found for controlling linear systems with quadratic cost functions (called LQ problems). Unfortunately, decentralized control is much more difficult than centralized control. For example, in 1968, Witsenhausen <ref> [125] </ref> showed that under a non-classical information pattern, an optimal controller for an LQ problem is no longer necessarily linear in the state of the system.
Reference: [126] <author> Witsenhausen, H. S. </author> <title> Separation of estimation and control for discrete time systems. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 59 </volume> <pages> 1557-1566, </pages> <year> 1971. </year> <month> 104 </month>
Reference-contexts: 1 i K: I i An example of a more restrictive information pattern is one where the control stations do not have access to each other's actions, i.e., for each i, 1 i K: I i 0 ; : : : ; u i A control station has perfect recall <ref> [126] </ref> if it never forgets any information. In a classical information pattern, all the control stations receive the same data and have perfect recall. In a strictly classical pattern, there is a single control station (K = 1) with perfect recall. <p> The problem of delayed sharing patterns, where the controllers share their own private information after k steps of delay, is one example in decentralized control where this technique has been 23 used. Witsenhausen <ref> [126] </ref> conjectured in 1971 that the optimal control strategy in such cases could be made to depend on the common information only through the conditional distribution of the state given the common information.
References-found: 126

