URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1990/tr-90-015.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1990.html
Root-URL: http://www.icsi.berkeley.edu
Title: Learning Feature-based Semantics with Simple Recurrent Networks  
Phone: 1-415-642-4274 FAX 1-415-643-7684  
Author: Andreas Stolcke 
Date: April 1990  
Address: I 1947 Center Street Suite 600 Berkeley, California 94704  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  
Pubnum: TR-90-015  
Abstract: The paper investigates the possibilities for using simple recurrent networks as transducers which map sequential natural language input into non-sequential feature-based semantics. The networks perform well on sentences containing a single main predicate (encoded by transitive verbs or prepositions) applied to multiple-feature objects (encoded as noun-phrases with adjectival modifiers), and shows robustness against ungrammatical inputs. A second set of experiments deals with sentences containing embedded structures. Here the network is able to process multiple levels of sentence-final embeddings but only one level of center-embedding. This turns out to be a consequence of the network's inability to retain information that is not reflected in the outputs over intermediate phases of processing. Two extensions to Elman's [9] original recurrent network architecture are introduced. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Robert B. Allen. </author> <title> Sequential connectionist networks for answering simple questions about a microworld. </title> <booktitle> In COGSCI [5], </booktitle> <pages> pages 489-495. </pages>
Reference-contexts: The sequential nature of these networks makes them a natural candidate for natural language processing tasks, as has been variously demonstrated <ref> [1, 22, 23, 11, 13, 19] </ref>. The application of SRNs presented here purports to use a task and training environment that is `natural' in several ways. <p> Allen <ref> [1, 3, 2] </ref> uses networks were both the word input and a representation of the `world' (corresponding to our semantic vectors) are given as inputs and the network typically has to give a yes/no answer regarding the correctness of the linguistic description.
Reference: [2] <author> Robert B. Allen and Selma M. Kaufman. </author> <title> Identifying and discriminating temporal events with connectionist language users. </title> <booktitle> In IEEE Conference on Artificial Neural Networks, </booktitle> <address> London, </address> <month> October </month> <year> 1989. </year>
Reference-contexts: Allen <ref> [1, 3, 2] </ref> uses networks were both the word input and a representation of the `world' (corresponding to our semantic vectors) are given as inputs and the network typically has to give a yes/no answer regarding the correctness of the linguistic description.
Reference: [3] <author> Robert B. Allen and Mark E. Riecken. </author> <title> Reference in connectionist language users. </title> <editor> In R. Pfeifer, Z. Schreter, F. Fogelman-Soulie, and L. Steels, editors, </editor> <booktitle> Connectionism in Perspective, </booktitle> <pages> pages 301-308. </pages> <publisher> Elsevier (North-Holland), </publisher> <year> 1989. </year>
Reference-contexts: Allen <ref> [1, 3, 2] </ref> uses networks were both the word input and a representation of the `world' (corresponding to our semantic vectors) are given as inputs and the network typically has to give a yes/no answer regarding the correctness of the linguistic description.
Reference: [4] <author> Axel Cleeremans, David Servan-Schreiber, and James L. McClelland. </author> <title> Finite state automata and simple recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 1(3) </volume> <pages> 372-381, </pages> <year> 1989. </year>
Reference-contexts: The left half of the network operates and is trained as a next-word predictor, just like originally conceived by El-man [9]. This training setup is known to build hidden layer representations which are closely related to the finite-state structure of the input domain <ref> [4] </ref>. The design is such that the representations in this `state' feedback loop can then control the information flowing through a separate `memory' feedback loop and the semantics output, but not vice-versa. <p> This confirmed that the network is not able to retain information which is not correlated with the output target for a number of intermediate processing steps, a limitation of SRNs previously found by Cleeremans et. al. <ref> [4] </ref>. 5 Related work A number of researchers have explored sequential recurrent networks (or related models) for language processing tasks.
Reference: [5] <institution> Proceedings of the 10th Annual Conference of the Cognitive Science Society, </institution> <address> Montreal, Quebec, Canada, </address> <month> August </month> <year> 1988. </year>
Reference: [6] <institution> Proceedings of the 11th Annual Conference of the Cognitive Science Society, University of Michigan, Ann Arbor, Mich., </institution> <month> August </month> <year> 1989. </year>
Reference: [7] <author> Garrison W. Cottrell and Fu-Sheng Tsung. </author> <title> Learning simple arithmetic procedures. </title> <booktitle> In COGSCI [6], </booktitle> <pages> pages 58-65. 7 </pages>
Reference-contexts: The resulting recurrent architecture combines features of both the Elman and the Jordan [15] type. 2 Although the total number of links was not re-duced this way, the smaller number of hidden units (15 instead of 25 in this experiment) sped up convergence. As various researchers have noted <ref> [10, 7] </ref>, backpropagation learning on large training sets with complex underlying structure is helped by presenting the training set incrementally, in stages of increasing size and/or complexity.
Reference: [8] <author> J. Diederich. </author> <title> Instruction and higher-level learning in connectionist networks. </title> <journal> Connection Science, </journal> <volume> 1(2) </volume> <pages> 163-182, </pages> <year> 1989. </year>
Reference-contexts: further progress towards learning of both natural lan 7 Due to this second reason, the problem is not solved by simply replacing the local representation with a distributed one. guage syntax and semantics will crucially depend on advances in the connectionist treatment of hierarchical representations, be they localist, structured models <ref> [8] </ref> or PDP mechanisms [14]. 7 Acknowledgements I wish to thank Jerry Feldman and Subutai Ah-mad for many fruitful discussions, as well as comments on earlier versions of this paper.
Reference: [9] <author> Jeffrey L. Elman. </author> <title> Finding structure in time. </title> <type> CRL Technical Report 8801, </type> <institution> Center for Research in Language, University of California at San Diego, La Jolla, Calif., </institution> <month> April </month> <year> 1988. </year>
Reference-contexts: 1 Introduction Since their introduction by Elman <ref> [9] </ref> backpropagation networks with one time step of hidden layer recurrence (simple recurrent networks, SRNs) have been extensively exploited as a simple, efficient, and yet surprisingly powerful architecture for dealing with sequential input and output patterns. <p> Its structure anticipates the need for a finite state control, directing the information passing through the network. The left half of the network operates and is trained as a next-word predictor, just like originally conceived by El-man <ref> [9] </ref>. This training setup is known to build hidden layer representations which are closely related to the finite-state structure of the input domain [4].
Reference: [10] <author> Jeffrey L. Elman. </author> <title> Representation and structure in connectionist models. </title> <type> CRL Technical Report 8903, </type> <institution> Center for Research in Language, University of Califor-nia at San Diego, La Jolla, Calif., </institution> <month> August </month> <year> 1989. </year>
Reference-contexts: The resulting recurrent architecture combines features of both the Elman and the Jordan [15] type. 2 Although the total number of links was not re-duced this way, the smaller number of hidden units (15 instead of 25 in this experiment) sped up convergence. As various researchers have noted <ref> [10, 7] </ref>, backpropagation learning on large training sets with complex underlying structure is helped by presenting the training set incrementally, in stages of increasing size and/or complexity.
Reference: [11] <author> Jeffrey L. Elman. </author> <title> Structured representations and connectionist models. </title> <booktitle> In COGSCI [6], </booktitle> <pages> pages 17-25. </pages>
Reference-contexts: The sequential nature of these networks makes them a natural candidate for natural language processing tasks, as has been variously demonstrated <ref> [1, 22, 23, 11, 13, 19] </ref>. The application of SRNs presented here purports to use a task and training environment that is `natural' in several ways. <p> First, the goal for the network is to act as a surface-to-semantic transducer, i.e. to map a sequence of words into a representation of the corresponding semantics. This contrasts with other approaches which have the network predict next words <ref> [11] </ref>, or fill in the unaltered word patterns into slots [19]. Second, the representations are sequential at the input (word) level, but parallel at the output (semantic) level.
Reference: [12] <author> Jerome A. Feldman, George Lakoff, An-dreas Stolcke, and Susan Hollbach Weber. </author> <title> Miniature language acquisition: A touchstone for cognitive science. </title> <type> Technical Report TR-90-009, </type> <institution> International Computer Science Institute, Berkeley, Calif., </institution> <month> March </month> <year> 1990. </year> <booktitle> Also appeared in the Proceedings of the 12th Annual Conference of the Cognitive Science Society, </booktitle> <pages> pp. 686-693. </pages>
Reference-contexts: The learning environment is largely consistent with the miniature language acquisition problem recently proposed as a framework for interdisciplinary research by Feldman et. al <ref> [12] </ref>, although it addresses only a subset of the learn S ! NP jNP VP ! VT NP P1 ! fModg P NP ! Det N1 VI ! is VT ! touches Det ! a N ! circle jsquare jtriangle Adj ! small jmedium jlarge j light jdark P ! above <p> relations are expressed as complete sentences, with the predicate expressed either by a transitive verb (touches) or the head of a prepositional phrase (PP), optionally modified by far : 1 Note that the fragment represents a a slightly restricted version of the L 0 language specified by Feldman et. al <ref> [12] </ref>. a light circle touches a small square a large dark square is far to the left of a small triangle Note that isolated NPs are allowed as sentences, denoting minimal subsets of the domain (i.e., objects and unary predicates).
Reference: [13] <author> Catherine L. Harris and Jeffrey L. Elman. </author> <title> Representing variable information with simple recurrent networks. </title> <booktitle> In COGSCI [6], </booktitle> <pages> pages 635-642. </pages>
Reference-contexts: The sequential nature of these networks makes them a natural candidate for natural language processing tasks, as has been variously demonstrated <ref> [1, 22, 23, 11, 13, 19] </ref>. The application of SRNs presented here purports to use a task and training environment that is `natural' in several ways. <p> Several studies investigate the use of sequential networks for the recognition of scripts (i.e. schematic event sequences). Harris and Elman <ref> [13] </ref> use the word prediction training scheme to address the question how well SRNs can capture correlations between temporally distant script variables. Miikkulainen and Dyer [19] use the same training target principle as adopted here, namely the network is required to transform a sequential information into a static structure.
Reference: [14] <author> Geoffrey E. Hinton. </author> <title> Representing part-whole hierarchies in connectionist networks. </title> <booktitle> In COGSCI [5], </booktitle> <pages> pages 48-54. </pages>
Reference-contexts: of both natural lan 7 Due to this second reason, the problem is not solved by simply replacing the local representation with a distributed one. guage syntax and semantics will crucially depend on advances in the connectionist treatment of hierarchical representations, be they localist, structured models [8] or PDP mechanisms <ref> [14] </ref>. 7 Acknowledgements I wish to thank Jerry Feldman and Subutai Ah-mad for many fruitful discussions, as well as comments on earlier versions of this paper.
Reference: [15] <author> M. I. Jordan. </author> <title> Attractor dynamics and parallelism in a connectionist sequential machine. </title> <booktitle> In Proceedings of the 8th Annual Conference of the Cognitive Science Society, </booktitle> <address> Amherst, Mass., </address> <month> August </month> <year> 1986. </year>
Reference-contexts: The resulting recurrent architecture combines features of both the Elman and the Jordan <ref> [15] </ref> type. 2 Although the total number of links was not re-duced this way, the smaller number of hidden units (15 instead of 25 in this experiment) sped up convergence.
Reference: [16] <author> Ronald M. Kaplan and Joan Bresnan. </author> <title> Lexical functional grammar: A formal system for grammatical representation. </title> <editor> In Joan Bresnan, editor, </editor> <booktitle> The Mental Representation of Grammatical Relations, </booktitle> <pages> pages 173-281. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1982. </year>
Reference-contexts: The network updates its outputs constantly according to the semantics expected from the current input. Contrary to a view held by `lexical' theories of language and grammar (such as, e.g., Lexical Functional Grammar <ref> [16] </ref>) where meanings are localized in individual lexicon entries, the predicate `right' is not locally attached to the word right. Instead, it is shared by the syntagmatic environment of right.
Reference: [17] <author> George Lakoff. Women, </author> <title> Fire and Dangerous Things. What Categories Reveal about the Mind. </title> <publisher> University of Chicago Press, </publisher> <address> Chicago, </address> <year> 1986. </year>
Reference-contexts: Simple recurrent networks have proven to be quite effective in extracting syntactic structure as it is manifest in distributional properties of the input. However, according to cognitively oriented linguistic theories <ref> [18, 17] </ref>, the semantic and conceptual domain provides not only a target for language learning, but also constrains syntactic form to be motivated by the conceptual structure it conveys.
Reference: [18] <author> Ronald Langacker. </author> <booktitle> Foundations of Cognitive Grammar. </booktitle> <volume> Vol. 1: </volume> <booktitle> Theoretical Prerequisites. </booktitle> <publisher> Stanford University Press, </publisher> <address> Stan-ford, </address> <year> 1985. </year>
Reference-contexts: Simple recurrent networks have proven to be quite effective in extracting syntactic structure as it is manifest in distributional properties of the input. However, according to cognitively oriented linguistic theories <ref> [18, 17] </ref>, the semantic and conceptual domain provides not only a target for language learning, but also constrains syntactic form to be motivated by the conceptual structure it conveys. <p> However, according to cognitively oriented linguistic theories [18, 17], the semantic and conceptual domain provides not only a target for language learning, but also constrains syntactic form to be motivated by the conceptual structure it conveys. For example, Lan-gacker <ref> [18] </ref> suggests that there exists an iconic relationship between the syntactic unity of a noun phrase and the conceptualized unity of the properties it encodes (such as shape, size, color).
Reference: [19] <author> Risto Miikkulainen and Michael G. Dyer. </author> <title> A modular neural network architecture for sequential paraphrasing of script-based stories. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> volume 2, </volume> <pages> pages 49-56, </pages> <address> Washington, D.C., </address> <month> June </month> <year> 1989. </year>
Reference-contexts: The sequential nature of these networks makes them a natural candidate for natural language processing tasks, as has been variously demonstrated <ref> [1, 22, 23, 11, 13, 19] </ref>. The application of SRNs presented here purports to use a task and training environment that is `natural' in several ways. <p> First, the goal for the network is to act as a surface-to-semantic transducer, i.e. to map a sequence of words into a representation of the corresponding semantics. This contrasts with other approaches which have the network predict next words [11], or fill in the unaltered word patterns into slots <ref> [19] </ref>. Second, the representations are sequential at the input (word) level, but parallel at the output (semantic) level. <p> Several studies investigate the use of sequential networks for the recognition of scripts (i.e. schematic event sequences). Harris and Elman [13] use the word prediction training scheme to address the question how well SRNs can capture correlations between temporally distant script variables. Miikkulainen and Dyer <ref> [19] </ref> use the same training target principle as adopted here, namely the network is required to transform a sequential information into a static structure.
Reference: [20] <author> Steven Pinker. </author> <title> Language Learnability and Language Development. </title> <publisher> Harvard University Press, </publisher> <address> Cambridge, Mass., </address> <year> 1984. </year>
Reference-contexts: The general idea that a combination of semantic grounding and distributional induction leads to syntactic structure is also consistent with theories of language acquisition and development <ref> [20] </ref>. Clearly the localist, flat feature encoding used here, although initially convenient and efficient for our purposes, does not have enough structure to explore any of those possibly crucial relationships between syntax and semantics.
Reference: [21] <author> David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In David E. Rumelhart and James L. McClel-land, editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, volume 1: Foundations, </booktitle> <pages> pages 318-362. </pages> <publisher> Bradford Books (MIT Press), </publisher> <address> Cambridge, Mass., </address> <year> 1986. </year>
Reference-contexts: Although this particular phenomenon may be in rough accordance with psycholinguistic data, a general inability to handle embedded structures is clearly not acceptable. Geoff Hin-ton (personal communication) has recently suggested applying full backpropagation in time <ref> [21] </ref> to this language learning problem, but it remains to be seen whether this is sufficient to learn to solve the center-embedding problem. Simple recurrent networks have proven to be quite effective in extracting syntactic structure as it is manifest in distributional properties of the input.
Reference: [22] <author> Josep Maria Sopena. </author> <title> Verbal description of visual blocks world using neural networks. </title> <type> Technical Report UB-DPB-88-10, </type> <institution> Depar-tament de Psicologia Basica, Universitat de Barcelona, </institution> <address> Barcelona, Spain, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: The sequential nature of these networks makes them a natural candidate for natural language processing tasks, as has been variously demonstrated <ref> [1, 22, 23, 11, 13, 19] </ref>. The application of SRNs presented here purports to use a task and training environment that is `natural' in several ways. <p> This idealizes a situation where there are no temporal clues concerning the correlation between input elements and target elements. This approach contrasts with others where the network is trained to map the sequential input into sequences of outputs with a temporal structure that is essentially isomorphic <ref> [22] </ref>. It also differs from schemes which present inputs sequentially, but encode some of the sequential structure redundantly in the input patterns themselves, or preprocess inputs into phrase-level chunks, thus avoiding even simple syntactic complexities such as adjective-noun combinations and function words [23]. <p> Allen [1, 3, 2] uses networks were both the word input and a representation of the `world' (corresponding to our semantic vectors) are given as inputs and the network typically has to give a yes/no answer regarding the correctness of the linguistic description. Sopena <ref> [22] </ref> applies sequential networks to a domain very similar to ours (visual scenes), but his networks perform transductions of input to output sequences which closely parallel each other. The changing semantic representations are meant to model an attentional mechanism which shifts focus as the corresponding verbal sequences are processed. St.

References-found: 22

