URL: ftp://ftp.cis.upenn.edu/pub/msingh/ml96_1.ps.Z
Refering-URL: http://www.cis.upenn.edu/~msingh/frames/papers_list.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: msingh@gradient.cis.upenn.edu  provan@camis.stanford.edu  
Title: Efficient Learning of Selective Bayesian Network Classifiers  
Author: Moninder Singh Gregory M. Provan 
Address: Philadelphia, PA 19104-6389  2164 Staunton Court Palo Alto, CA, 94306  
Affiliation: Dept. of Computer and Information Science University of Pennsylvania  Institute for the Study of Learning and Expertise  
Note: Proceedings of the 13th International Conference on Machine Learning 1996: Morgan Kaufmann.  
Abstract: In this paper, we present a computation-ally efficient method for inducing selective Bayesian network classifiers. Our approach is to use information-theoretic metrics to efficiently select a subset of attributes from which to learn the classifier. We explore three conditional, information-theoretic met-rics that are extensions of metrics used extensively in decision tree learning, namely Quin-lan's gain and gain ratio metrics and Man-taras's distance metric. We experimentally show that the algorithms based on gain ratio and distance metric learn selective Bayesian networks that have predictive accuracies as good as or better than those learned by existing selective Bayesian network induction approaches (K2-AS), but at a significantly lower computational cost. We prove that the subset-selection phase of these information-based algorithms has polynomial complexity, as compared to the worst-case exponential time complexity of the corresponding phase in K2-AS.
Abstract-found: 1
Intro-found: 1
Reference: <author> Andreassen, S., Woldbye, M., Falck, B., and Andersen, S. </author> <year> (1987). </year> <title> A causal probabilistic network for interpretation of electromyographic findings. </title> <booktitle> In Proc. IJCAI, </booktitle> <pages> pages 366-372. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In terms of learning, one can use this structure to explicitly encode priors, a feature absent in many learning frameworks. Bayesian networks are being increasingly used in many real-world domains like medical diagnosis <ref> (Andreassen et al., 1987) </ref>, telecommunications (Ezawa and Norton, 1995), information retrieval (Fung and Favero, 1995), system troubleshooting (Hecker-man et al., 1994a) and vision (Levitt et al., 1989). However, the use of Bayesian networks has often been limited by the NP-hard complexity of probabilistic inference (Cooper, 1990).
Reference: <author> Buntine, W. and Niblett, T. </author> <year> (1992). </year> <title> A further comparison of splitting rules for decision-tree induction. </title> <journal> Machine Learning, </journal> <volume> 7. </volume>
Reference-contexts: However, this choice was 7 Further information on these databases can be obtained from the UCI repository by anonymous ftp to ics.uci.edu. The voting1 database was derived from the voting database by deleting the most significant attribute physician-fee-freeze <ref> (Buntine and Niblett, 1992) </ref>. arbitrary and one could as well have chosen any other method for learning BN's, e.g. (Heckerman et al., 1994b), etc.
Reference: <author> Cooper, G. </author> <year> (1990). </year> <title> The computational complexity of probabilistic inference using Belief networks. </title> <journal> Artificial Intelligence, </journal> <volume> 42 </volume> <pages> 393-405. </pages>
Reference-contexts: However, the use of Bayesian networks has often been limited by the NP-hard complexity of probabilistic inference <ref> (Cooper, 1990) </ref>. In practice, network topology (especially maximum clique size) and the number of variables are two of the most significant parameters that govern inference complexity.
Reference: <author> Cooper, G. and Herskovits, E. </author> <year> (1992). </year> <title> A Bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 309-347. </pages>
Reference: <author> Cowell, R., Dawid, P., and Spiegelhalter, D. </author> <year> (1993). </year> <title> Sequential model criticism in probabilistic expert systems. </title> <journal> IEEE Transactions of Pattern Analysis and Machine Intelligence, </journal> <volume> 15(3) </volume> <pages> 209-219. </pages>
Reference-contexts: Note that even though a given model may be "correct" in the sense of generating the data, it need not be the best model when it comes to making predictions <ref> (Cowell et al., 1993) </ref>. significantly better than naive-AS but was not statistically different from naive-ALL. 5.2.2 Comparison with Bayesian Networks We also compared Info-AS with non-selective Bayesian networks learned using the CB algorithm as well as with selective Bayesian networks learned using K2-AS and K2-AS&lt;.
Reference: <author> Ezawa, K. and Norton, S. </author> <year> (1995). </year> <title> Knowledge discovery in telecommunication services data using Bayesian network models. </title> <booktitle> In Proc. 1st Int. Conf. on Knowledge Discovery and Data Mining. </booktitle>
Reference-contexts: In terms of learning, one can use this structure to explicitly encode priors, a feature absent in many learning frameworks. Bayesian networks are being increasingly used in many real-world domains like medical diagnosis (Andreassen et al., 1987), telecommunications <ref> (Ezawa and Norton, 1995) </ref>, information retrieval (Fung and Favero, 1995), system troubleshooting (Hecker-man et al., 1994a) and vision (Levitt et al., 1989). However, the use of Bayesian networks has often been limited by the NP-hard complexity of probabilistic inference (Cooper, 1990).
Reference: <author> Fung, R. and Favero, B. D. </author> <year> (1995). </year> <title> Applying Bayesian networks to information retrieval. </title> <journal> Communications of the ACM, </journal> <volume> 38(3). </volume>
Reference-contexts: In terms of learning, one can use this structure to explicitly encode priors, a feature absent in many learning frameworks. Bayesian networks are being increasingly used in many real-world domains like medical diagnosis (Andreassen et al., 1987), telecommunications (Ezawa and Norton, 1995), information retrieval <ref> (Fung and Favero, 1995) </ref>, system troubleshooting (Hecker-man et al., 1994a) and vision (Levitt et al., 1989). However, the use of Bayesian networks has often been limited by the NP-hard complexity of probabilistic inference (Cooper, 1990).
Reference: <author> Heckerman, D., Breese, J. S., and Rommelse, K. </author> <year> (1994a). </year> <title> Troubleshooting under uncertainty. </title> <type> Technical Report MSR-TR-94-07, </type> <institution> Microsoft Research, </institution> <address> Redmond, WA. </address>
Reference: <author> Heckerman, D., Geiger, D., and Chickering, M. </author> <year> (1994b). </year> <title> Learning Bayesian networks: the combination of knowledge and statistical data. </title> <type> Technical Report MSR-TR-94-09, </type> <institution> Microsoft research, </institution> <address> Redmond, WA. </address>
Reference-contexts: The voting1 database was derived from the voting database by deleting the most significant attribute physician-fee-freeze (Buntine and Niblett, 1992). arbitrary and one could as well have chosen any other method for learning BN's, e.g. <ref> (Heckerman et al., 1994b) </ref>, etc.
Reference: <author> Heckerman, D. and Shachter, R. </author> <year> (1994). </year> <title> A decision-based view of causality. </title> <booktitle> In Proc. Conf. Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 302-310. </pages>
Reference: <author> Kononenko, I. </author> <year> (1990). </year> <title> Comparison of inductive and noise Bayesian learning approaches to automatic knowledge acquisition. </title> <editor> In et al., B. W., editor, </editor> <title> Current Trends in Knowledge Acquisition. </title> <publisher> IOS Press, Amsterdam. </publisher>
Reference: <author> Langley, P. </author> <year> (1993). </year> <title> Induction of recursive Bayesian classifiers. </title> <booktitle> In Proc. European Conf. on Machine Learning, </booktitle> <pages> pages 153-164. </pages> <publisher> Springer Verlag. </publisher>
Reference: <author> Langley, P. and Sage, S. </author> <year> (1994). </year> <title> Induction of selective Bayesian classifiers. </title> <booktitle> In Proc. Conf. on Uncertainty in AI. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The counts let the classifier estimate P (c k ) and P (z j jc k ) for each class c k . The performance of naive Bayesian classifiers can deteriorate for domains with many correlated attributes. The selective naive Bayesian classifier <ref> (Langley and Sage, 1994) </ref> is an extension to the naive Bayesian classifier designed to perform better in such domains.
Reference: <author> Lauritzen, S. L. and Spiegelhalter, D. J. </author> <year> (1988). </year> <title> Local computations with probabilities on graphical structures and their application to expert systems. </title> <journal> Journal of the Royal Statistical Society (Series B), </journal> <volume> 50 </volume> <pages> 157-224. </pages>
Reference: <author> Levitt, T., Mullin, J., and Binford, T. </author> <year> (1989). </year> <title> Model-based influence diagrams for machine vision. </title> <booktitle> In Proc. Fifth Workshop on Uncertainty in AI, </booktitle> <pages> pages 233-244. </pages>
Reference-contexts: Bayesian networks are being increasingly used in many real-world domains like medical diagnosis (Andreassen et al., 1987), telecommunications (Ezawa and Norton, 1995), information retrieval (Fung and Favero, 1995), system troubleshooting (Hecker-man et al., 1994a) and vision <ref> (Levitt et al., 1989) </ref>. However, the use of Bayesian networks has often been limited by the NP-hard complexity of probabilistic inference (Cooper, 1990). In practice, network topology (especially maximum clique size) and the number of variables are two of the most significant parameters that govern inference complexity.
Reference: <author> Lopez de Mantaras, R. </author> <year> (1991). </year> <title> A distance-based attribute selection measure for decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 81-92. </pages>
Reference: <author> Murphy, P. and Aha, D. </author> <year> (1992). </year> <title> UCI repository of machine learning databases. </title> <institution> Department of Information and Computer Science, University of California, Irvine. </institution>
Reference-contexts: above result. 2 6 Cooper and Herskovits (1992) used a similar technique to calculate P (B S ; D) efficiently. 5 EXPERIMENTAL COMPARISON OF INFO-AS WITH OTHER INDUCTION METHODS In our experiments we used a variety of databases acquired from the University of California, Irvine, repository of Machine Learning databases <ref> (Murphy and Aha, 1992) </ref>.
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kauf-man, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Provan, G. M. and Singh, M. </author> <year> (1996). </year> <title> Learning Bayesian networks using feature selection. </title> <editor> In Fisher, D. and Lenz, H., editors, </editor> <booktitle> Learning from Data: AI and Statistics V, Lecture Notes in Statistics, </booktitle> <pages> pages 291-300. </pages> <publisher> Springer Verlag. </publisher>
Reference-contexts: Moreover, K2-AS, by selecting a subset of attributes prior to learning the networks, not only significantly improves the inference efficiency of the resulting networks, but also achieves a predictive accuracy comparable to Bayesian networks learned using the full set of attributes <ref> (Provan and Singh, 1996) </ref>. However, one of the main drawbacks of this approach is that the attribute selection phase is computationally intensive.
Reference: <author> Quinlan, J. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106. </pages>
Reference: <author> Singh, M. and Provan, G. M. </author> <year> (1995). </year> <title> A comparison of induction algorithms for selective and non-selective Bayesian classifiers. </title> <booktitle> In Proc. 12th Intl. Conference on Machine Learning, </booktitle> <pages> pages 497-505. </pages>
Reference-contexts: The second method, learning Bayesian network classifiers from a subset of the attributes (selective Bayesian networks), may be a good compromise between naive Bayesian classifiers and Bayesian networks that model all attributes. The K2-AS algorithm <ref> (Singh and Provan, 1995) </ref> was an attempt in this direction and has been shown to outperform both selective as well as non-selective naive Bayesian classifiers. <p> Attributes are added as long as the resultant accuracy increases. 4 The second phase then uses the subset of selected attributes, , to learn the final network. The learning algorithm that they use (in both phases), called CB <ref> (Singh and Valtorta, 1995) </ref>, is a modified version of K2.
Reference: <author> Singh, M. and Valtorta, M. </author> <year> (1995). </year> <title> Construction of Bayesian network structures from data: a brief survey and an efficient algorithm. </title> <journal> International Journal of Approximate Reasoning, </journal> <volume> 12 </volume> <pages> 111-131. </pages>
Reference-contexts: The second method, learning Bayesian network classifiers from a subset of the attributes (selective Bayesian networks), may be a good compromise between naive Bayesian classifiers and Bayesian networks that model all attributes. The K2-AS algorithm <ref> (Singh and Provan, 1995) </ref> was an attempt in this direction and has been shown to outperform both selective as well as non-selective naive Bayesian classifiers. <p> Attributes are added as long as the resultant accuracy increases. 4 The second phase then uses the subset of selected attributes, , to learn the final network. The learning algorithm that they use (in both phases), called CB <ref> (Singh and Valtorta, 1995) </ref>, is a modified version of K2.
Reference: <author> White, A. and Liu, W. </author> <year> (1994). </year> <title> Bias in information-based measures in decision tree induction. </title> <booktitle> Machine Learning, </booktitle> <pages> pages 321-329. </pages>
References-found: 23

