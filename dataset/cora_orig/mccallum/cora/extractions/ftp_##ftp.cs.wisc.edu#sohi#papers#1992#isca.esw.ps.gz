URL: ftp://ftp.cs.wisc.edu/sohi/papers/1992/isca.esw.ps.gz
Refering-URL: http://www.cs.wisc.edu/~sohi/sohi.html
Root-URL: 
Title: THE EXPANDABLE SPLIT WINDOW PARADIGM FOR EXPLOITING FINE-GRAIN PARALLELISM  
Author: Manoj Franklin and Gurindar S. Sohi 
Address: 1210 W. Dayton Street Madison, WI 53706  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Abstract: We propose a new processing paradigm, called the Expandable Split Window (ESW) paradigm, for exploiting fine-grain parallelism. This paradigm considers a window of instructions (possibly having dependencies) as a single unit, and exploits fine-grain parallelism by overlapping the execution of multiple windows. The basic idea is to connect multiple sequential processors, in a decoupled and decentralized manner, to achieve overall multiple issue. This processing paradigm shares a number of properties of the restricted dataflow machines, but was derived from the sequential von Neumann architecture. We also present an implementation of the Expandable Split Window execution model, and preliminary performance results. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, R. Sethi, and J. D. Ullman, </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1986. </year>
Reference-contexts: Notice that data from several stages can simultaneously be traveling to subsequent stages in a pipelined fashion. Queues are used to facilitate this forwarding. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 2 All optimizing compilers invariably do dataflow analysis <ref> [1, 7] </ref>; the create and use masks are similar to the def and use variables computed by these compilers for each basic block, except that the former pair represent architectural registers and the latter pair represent variables of the source program. 3 Since most of the register instances are used up
Reference: [2] <author> T. M. Austin and G. S. Sohi, </author> <title> ``Dynamic Dependency Analysis of Ordinary Programs,'' </title> <booktitle> Proc. 19th Annual International Symposium on Computer Architecture, </booktitle> <year> 1992. </year>
Reference-contexts: An inspection of the dynamic dataflow graph of many sequential programs reveals that there exists a large amount of theoretically exploitable instruction-level parallelism <ref> [2, 3, 5, 13, 24] </ref>, i.e., a large number of computation nodes that can be executed in parallel, provided a suitable processor model with a suitable communication mechanism backed up by a suitable temporary storage mechanism exists.
Reference: [3] <author> M. Butler, T. Yeh, Y. Patt, M. Alsup, H. Scales, and M. Shebanow, </author> <title> ``Single Instruction Stream Parallelism Is Greater than Two,'' </title> <booktitle> Proc. 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 276-286, </pages> <year> 1991. </year>
Reference-contexts: An inspection of the dynamic dataflow graph of many sequential programs reveals that there exists a large amount of theoretically exploitable instruction-level parallelism <ref> [2, 3, 5, 13, 24] </ref>, i.e., a large number of computation nodes that can be executed in parallel, provided a suitable processor model with a suitable communication mechanism backed up by a suitable temporary storage mechanism exists. <p> c c c c c c c c c c c c c c c c c c c c c c c c c c c In comparing our results to other results in the literature, we see that we are achieving issue rates similar to Butler, et. al. <ref> [3] </ref>, with similar resources but larger window sizes (our results are slightly better in several cases), and much better than the issue rates achieved by Smith, et. al [21].
Reference: [4] <author> R. P. Colwell, R. P. Nix, J. J. O'Donnell, D. B. Papworth, and P. K. Rodman, </author> <title> ``A VLIW Architecture for a Trace Scheduling Compiler,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 37, </volume> <pages> pp. 967-979, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Sequential execution can be augmented to exploit the ``irregular'' type of parallelism found in most non-numeric applications. Superscalar processors [10, 12, 15, 18] and VLIW processors <ref> [4] </ref> do exactly this; they stay within the realm of sequential execution, but attempt to execute multiple operations every cycle. For achieving this, superscalars scan through a window of (sequential) operations every cycle and dynamically detect independent operations to be issued in a cycle.
Reference: [5] <author> D. E. Culler and Arvind, </author> <title> ``Resource Requirements of Dataflow Programs,'' </title> <booktitle> Proc. 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 141-150, </pages> <year> 1988. </year>
Reference-contexts: An inspection of the dynamic dataflow graph of many sequential programs reveals that there exists a large amount of theoretically exploitable instruction-level parallelism <ref> [2, 3, 5, 13, 24] </ref>, i.e., a large number of computation nodes that can be executed in parallel, provided a suitable processor model with a suitable communication mechanism backed up by a suitable temporary storage mechanism exists. <p> It also suffers from the inability to express critical sections and imperative operations that are essential for the efficient execution of operating system functions, such as resource management <ref> [5, 17] </ref>. (Of late, more restricted forms of dataflow architecture have been proposed [9, 14, 16, 17].) This is where the power of sequentiality comes in.
Reference: [6] <author> J. S. Emer and R. P. Nix, </author> <type> ``Personal Communication,'' </type> <month> June </month> <year> 1991. </year>
Reference-contexts: Studies are also needed to determine the optimum number of stages and the maximum number of hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 4 There is an ongoing independent work on hardware support for dynamic disambiguation of memory references in the context of a VLIW processor in industry <ref> [6] </ref>. instructions to be issued from each IE unit in a cycle. Another issue worth investigating is the benefit of out-of-order execution within an IE unit when it contains a straight-line piece of code.
Reference: [7] <author> C. N. Fischer and R. J. LeBlanc, Jr., </author> <title> Crafting A Compiler. </title> <address> Menlo Park, CA: </address> <publisher> The Benjamin/Cummings Publishing Company, Inc., </publisher> <year> 1988. </year>
Reference-contexts: Notice that data from several stages can simultaneously be traveling to subsequent stages in a pipelined fashion. Queues are used to facilitate this forwarding. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 2 All optimizing compilers invariably do dataflow analysis <ref> [1, 7] </ref>; the create and use masks are similar to the def and use variables computed by these compilers for each basic block, except that the former pair represent architectural registers and the latter pair represent variables of the source program. 3 Since most of the register instances are used up
Reference: [8] <author> J. R. Goodman and P. J. Woest, </author> <title> ``The Wisconsin Multi-cube: A New Large-Scale Cache-Coherent Multiprocessor,'' </title> <booktitle> Proc. 15th Annual Symposium on Computer Architecture, </booktitle> <pages> pp. 422-431, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: If the transferred window is a loop, the L1 caches of the subsequent stages can also grab the window in parallel, much like the snarfing (read broadcast) scheme proposed for multiprocessor caches <ref> [8] </ref>. If the request misses in the L2 cache, it is forwarded to main memory.
Reference: [9] <author> R. A. </author> <title> Iannucci, ``Toward a Dataflow / von Neumann Hybrid Architecture,'' </title> <booktitle> Proc. 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 131-140, </pages> <year> 1988. </year>
Reference-contexts: It also suffers from the inability to express critical sections and imperative operations that are essential for the efficient execution of operating system functions, such as resource management [5, 17]. (Of late, more restricted forms of dataflow architecture have been proposed <ref> [9, 14, 16, 17] </ref>.) This is where the power of sequentiality comes in. By ordering the computation nodes in a suitable manner, sequentiality can be used to introduce (and exploit) different kinds of temporal locality to minimize the costs of communication and intermediate result storage.
Reference: [10] <author> N. P. Jouppi and D. W. Wall, </author> <title> ``Available Instruction-Level Parallelism for Superscalar and Superpipelined Machines,'' </title> <booktitle> Proc. ASPLOS III, </booktitle> <pages> pp. 272-282, </pages> <year> 1989. </year>
Reference-contexts: Sequential execution can be augmented to exploit the ``irregular'' type of parallelism found in most non-numeric applications. Superscalar processors <ref> [10, 12, 15, 18] </ref> and VLIW processors [4] do exactly this; they stay within the realm of sequential execution, but attempt to execute multiple operations every cycle.
Reference: [11] <author> D. R. Kaeli and P. G. Emma, </author> <title> ``Branch History Table Prediction of Moving Target Branches Due to Subroutine Returns,'' </title> <booktitle> Proc. 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 34-42, </pages> <year> 1991. </year>
Reference-contexts: For effectively predicting the return addresses of procedure calls, there is a stack-like mechanism similar to the one discussed in <ref> [11] </ref>, with a stack depth of 20.
Reference: [12] <author> K. Murakami, N. Irie, M. Kuga, and S. Tomita, </author> <title> ``SIMP (Single Instruction Stream / Multiple Instruction Pipelin-ing): A Novel High-Speed Single-Processor Architecture,'' </title> <booktitle> Proc. 16th Annual Symposium on Computer Architecture, </booktitle> <pages> pp. 78-85, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Sequential execution can be augmented to exploit the ``irregular'' type of parallelism found in most non-numeric applications. Superscalar processors <ref> [10, 12, 15, 18] </ref> and VLIW processors [4] do exactly this; they stay within the realm of sequential execution, but attempt to execute multiple operations every cycle.
Reference: [13] <author> A. Nicolau and J. A. Fisher, </author> <title> ``Measuring the Parallelism Available for Very Long Instruction Word Architectures,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-33, </volume> <pages> pp. 968-976, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: An inspection of the dynamic dataflow graph of many sequential programs reveals that there exists a large amount of theoretically exploitable instruction-level parallelism <ref> [2, 3, 5, 13, 24] </ref>, i.e., a large number of computation nodes that can be executed in parallel, provided a suitable processor model with a suitable communication mechanism backed up by a suitable temporary storage mechanism exists.
Reference: [14] <author> R. S. Nikhil and Arvind, </author> <title> ``Can Dataflow Subsume von Neumann Computing?,'' </title> <booktitle> Proc. 16th International Symposium on Computer Architecture, </booktitle> <pages> pp. 262-272, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: It also suffers from the inability to express critical sections and imperative operations that are essential for the efficient execution of operating system functions, such as resource management [5, 17]. (Of late, more restricted forms of dataflow architecture have been proposed <ref> [9, 14, 16, 17] </ref>.) This is where the power of sequentiality comes in. By ordering the computation nodes in a suitable manner, sequentiality can be used to introduce (and exploit) different kinds of temporal locality to minimize the costs of communication and intermediate result storage.
Reference: [15] <author> R. R. Oehler and R. D. Groves, </author> <title> ``IBM RISC System/6000 Processor Architecture,'' </title> <journal> IBM Journal of Research and Development, </journal> <volume> vol. 34, </volume> <pages> pp. 23-36, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Sequential execution can be augmented to exploit the ``irregular'' type of parallelism found in most non-numeric applications. Superscalar processors <ref> [10, 12, 15, 18] </ref> and VLIW processors [4] do exactly this; they stay within the realm of sequential execution, but attempt to execute multiple operations every cycle.
Reference: [16] <author> G. M. Papadopoulos and D. E. Culler, ``Monsoon: </author> <title> An Explicit Token-Store Architecture,'' </title> <booktitle> Proc. 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 82-91, </pages> <month> May, </month> <year> 1990. </year>
Reference-contexts: It also suffers from the inability to express critical sections and imperative operations that are essential for the efficient execution of operating system functions, such as resource management [5, 17]. (Of late, more restricted forms of dataflow architecture have been proposed <ref> [9, 14, 16, 17] </ref>.) This is where the power of sequentiality comes in. By ordering the computation nodes in a suitable manner, sequentiality can be used to introduce (and exploit) different kinds of temporal locality to minimize the costs of communication and intermediate result storage.
Reference: [17] <author> G. M. Papadopoulos and K. R. Traub, </author> <title> ``Multithreading: A Revisionist View of Dataflow Architectures,'' </title> <booktitle> Proc. 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 342-351, </pages> <year> 1991. </year>
Reference-contexts: It also suffers from the inability to express critical sections and imperative operations that are essential for the efficient execution of operating system functions, such as resource management <ref> [5, 17] </ref>. (Of late, more restricted forms of dataflow architecture have been proposed [9, 14, 16, 17].) This is where the power of sequentiality comes in. <p> It also suffers from the inability to express critical sections and imperative operations that are essential for the efficient execution of operating system functions, such as resource management [5, 17]. (Of late, more restricted forms of dataflow architecture have been proposed <ref> [9, 14, 16, 17] </ref>.) This is where the power of sequentiality comes in. By ordering the computation nodes in a suitable manner, sequentiality can be used to introduce (and exploit) different kinds of temporal locality to minimize the costs of communication and intermediate result storage.
Reference: [18] <author> Y. N. Patt, W. W. Hwu, and M. Shebanow, ``HPS, </author> <title> A New Microarchitecture: Rationale and Introduction,'' </title> <booktitle> Proc. 18th Annual Workshop on Microprogramming, </booktitle> <pages> pp. 103-108, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: Sequential execution can be augmented to exploit the ``irregular'' type of parallelism found in most non-numeric applications. Superscalar processors <ref> [10, 12, 15, 18] </ref> and VLIW processors [4] do exactly this; they stay within the realm of sequential execution, but attempt to execute multiple operations every cycle.
Reference: [19] <author> J. E. Smith, </author> <title> ``A Study of Branch Prediction Strategies,'' </title> <booktitle> Proc. 8th International Symposium on Computer Architecture, </booktitle> <pages> pp. 135-148, </pages> <month> May </month> <year> 1981. </year>
Reference-contexts: is 4 cycles. g Each stage has a 4Kword L1 instruction cache. g The L2 instruction cache has not been included in the simulator; instead, we assume 100% hit ratio for the L2 instruction cache. g The branch prediction mechanism for conditional branches uses the 3-bit counter scheme proposed in <ref> [19] </ref>. For effectively predicting the return addresses of procedure calls, there is a stack-like mechanism similar to the one discussed in [11], with a stack depth of 20.
Reference: [20] <author> J. E. Smith and A. R. Pleszkun, </author> <title> ``Implementing Precise Interrupts in Pipelined Processors,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 37, </volume> <pages> pp. 562-573, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: Distributed Future File In our proposed design, each stage has a separate register file called a future file. These distributed future files are the working files used by the functional units in the IE units. In that sense, they work similar in spirit to the future file discussed in <ref> [20] </ref> for implementing precise interrupts in pipelined processors. As we will see in section 3.5, the distributed future file simplifies the task of recovery when incorrect branch predictions are encountered.
Reference: [21] <author> M. D. Smith, M. Johnson, and M. A. Horowitz, </author> <title> ``Limits on Multiple Instruction Issue,'' </title> <booktitle> Proc. ASPLOS III, </booktitle> <pages> pp. 290-302, </pages> <year> 1989. </year>
Reference-contexts: 3.3 how we enforce the register dependencies without dynamically decoding the instructions.) Because the task of the control unit is relatively straightforward, it does not become a potential bottleneck. (Control units with instruction decoders that feed centralized windows are a major impediment to performance in superscalar processors, as shown in <ref> [21] </ref>.) The active stages, the ones from the head to the tail, together constitute the large dynamic window of operations, and the stages contain basic windows, in the sequential order in which the windows appear in the dynamic instruction stream. <p> comparing our results to other results in the literature, we see that we are achieving issue rates similar to Butler, et. al. [3], with similar resources but larger window sizes (our results are slightly better in several cases), and much better than the issue rates achieved by Smith, et. al <ref> [21] </ref>.
Reference: [22] <author> G. S. Sohi and M. Franklin, </author> <title> ``High-Bandwidth Data Memory Systems for Superscalar Processors,'' </title> <booktitle> Proc. AS-PLOS IV, </booktitle> <pages> pp. 53-62, </pages> <year> 1991. </year>
Reference-contexts: Distributed Data Memory System When a processor attempts to issue and execute many instructions in parallel, it is imperative that the memory system should be capable of supporting multiple memory references per cycle, preferably with small latencies <ref> [22] </ref>. The latency can be reduced by using a data cache. In line with our objective of expandability, the memory system also has to be decentralized. Decentralizing the memory system is harder than most of the other parts. <p> Interleaved Data Cache As seen from the discussion so far, conventional data caches are not appropriate for the ESW paradigm; the exact nature of the data caches is a subject of our future research. Currently we propose to use an interleaved non-blocking cache similar to the one proposed in <ref> [22] </ref>. 3.5. Enforcing Control Dependencies Dynamic branch prediction is used (if required) to fetch new basic windows. When a conditional branch instruction or a return instruction is executed in an IE unit, its outcome is compared with the earlier prediction.
Reference: [23] <author> R. M. Tomasulo, </author> <title> ``An Efficient Algorithm for Exploiting Multiple Arithmetic Units,'' </title> <journal> IBM Journal of Research and Development, </journal> <pages> pp. 25-33, </pages> <month> January </month> <year> 1967. </year>
Reference-contexts: As we will see in section 3.5, the distributed future file simplifies the task of recovery when incorrect branch predictions are encountered. If out-of-order execution within IE units is desired, then each IE unit should also have some means of forwarding results within the unit (possibly reservation stations <ref> [23] </ref>). Another advantage of the distributed future file structure in that case is that it allows independent register renaming for each stage.
Reference: [24] <author> D. W. Wall, </author> <title> ``Limits of Instruction-Level Parallelism,'' </title> <booktitle> Proc. ASPLOS IV, </booktitle> <pages> pp. 176-188, </pages> <year> 1991. </year>
Reference-contexts: An inspection of the dynamic dataflow graph of many sequential programs reveals that there exists a large amount of theoretically exploitable instruction-level parallelism <ref> [2, 3, 5, 13, 24] </ref>, i.e., a large number of computation nodes that can be executed in parallel, provided a suitable processor model with a suitable communication mechanism backed up by a suitable temporary storage mechanism exists. <p> Consequently, the big window is a sliding or continuous window, and not a fixed big window, a feature that allows more parallelism to be exploited <ref> [24] </ref>. The major parts of the ESW implementation are described below. 3.1. Distributed Issue and Execution Units Each stage has as its heart an Issue and Execution (IE) unit, which takes operations from a local instruction cache and pumps them to its functional units after resolving their data dependencies.
References-found: 24

