URL: http://cag-www.lcs.mit.edu/~robertb/thesis.ps
Refering-URL: http://www.cag.lcs.mit.edu/~robertb/dissertation.html
Root-URL: 
Title: The Meerkat Multicomputer: Tradeoffs in Multicomputer Architecture  
Author: by Robert C. Bedichek 
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy  Approved by (Co-Chairperson of Supervisory Committee) (Co-Chairperson of Supervisory Committee)  
Note: Program Authorized to Offer Degree Date  
Date: 1994  
Affiliation: University of Washington  
Abstract-found: 0
Intro-found: 1
Reference: [1] <institution> Advanced Micro Devices, Inc., </institution> <type> 901 Thompson Place, </type> <address> PO Box 3453, Sunnyvale, Ca. </address> <month> 94088-3453. </month> <title> PAL Device Data Book and Design Guide, </title> <year> 1993. </year>
Reference-contexts: However, we were unable to find a controller chip that could give us the performance we wanted. Therefore, we designed our own controller using Programmable Array Logic (PALs) <ref> [1] </ref>. These devices sense S-Bus signals and generate the Row Address Strobe (RAS), Column Address Strobe (CAS), and write signals for the two banks of DRAM. CMMUs read or write one or four words per bus transaction.
Reference: [2] <author> Anant Agarwal, David Chaiken, Godfrey D'Souza, Kirk Johnson, David Kranz, John Kubiatowicz, Kiyoshi Kurihara, Beng-Hong Lim, Gino Maa, Dan Nussbaum, Mike Parkin, and Donald Yeung. </author> <title> The MIT Alewife machine: A large-scale distributed-memory multiprocessor. </title> <type> Technical Report MIT/LCS Memo TM-454, </type> <institution> Laboratory for Computer Science, MIT, </institution> <year> 1991. </year>
Reference-contexts: Multiprocessors are generally more difficult to design and build than multicomputers. However, many researchers believe that multiprocessors are easier to program [85, 93, 52]. This has led a number of researchers to study how multiprocessors with hundreds of processors should be designed <ref> [58, 2, 12, 8] </ref>. Multiprocessors require hardware either to keep track of the location and status of cache lines as they move through the system, or to effect extremely low latency internode communication, as in the Cray T3D. Software can make multicomputers perform as multiprocessors [59].
Reference: [3] <author> T. E. Anderson, H. M. Levy, B. N. Bershad, and E. D. Lazowska. </author> <title> The interaction of architecture and operating system design. </title> <booktitle> Proceedings of the Fourth International Conference of Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: This level of indirection hurts performance in several ways. First, transferring control to the operating system in order to send or receive data causes context switches. Anderson et al. argue that the relative cost of context switches is increasing with advances in processor technology <ref> [3] </ref>. Second, the operating system must 103 check that the communication is valid. Third, the operating system is written for the general case: its general code is slower than the tailored code in user-space programs.
Reference: [4] <author> Thomas E. Anderson, Brian N. Bershad, Edward D. Lazowska, and Henry M. Levy. </author> <title> Scheduler activations: Effective kernel support for the user-level management of parallelism. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 95-109, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: If they are active, the operating system lets the application execute until it is finished its internode operation and the interface registers are no longer active. This technique is used to prevent context switches of user-level threads that are in critical sections <ref> [4] </ref>. On the receiving node, a context-switch interrupt may occur after data starts filling the FIFO and before the receiving node loads the receive address register. If the FIFO overflows because the context-switch interrupt takes too long, the sender is signaled with a negative acknowledgement.
Reference: [5] <author> Ramune Arlauskas. </author> <title> iPSC/2 System: A second generation hypercube. </title> <booktitle> In Proceedings of Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 38-42, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: We chose the node numbers (zero and three) so that the communication requires a two-bus connection. We execute the sequence above for message sizes ranging from four to 256k bytes. function of message size. We include in these graphs the bandwidth measured on the Intel Hypercube iPSC/2 and iPSC/860 <ref> [5] </ref>. In this test the bandwidths reported by the Meerkat simulator and the hardware differed by about one percent. Therefore, the Meerkat curve 79 can be viewed both as a measurement of a real system and as a simulation result.
Reference: [6] <author> Emmanuel A. Arnould, Francois J. Bitz, Eric C. Cooper, H.T. Kung, Robert D. Sansom, and Peter A. Steenkiste. </author> <title> The design of Nectar: A network backplane for heterogeneous multicomputers. </title> <booktitle> In Proceedings of the Third International Conference of Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> April </month> <year> 1989. </year>
Reference-contexts: This allows a wide variety of interface styles to be exposed to the processor. Examples of systems with a protocol processor that handles internode traffic include the FLASH multiprocessor [55], Intel's Delta, and Nectar <ref> [6] </ref>. DMA interfaces allow the full bandwidth of system memory to be coupled to the interconnect, but they also present complexities that FIFO interfaces do not. FIFO interfaces usually require less overhead to transmit and receive short messages.
Reference: [7] <author> C. Ashcraft, S. Eisenstat, J. Liu, and A. Sherman. </author> <title> A comparison of three distributed sparse factorization schemes. In SIAM Symposium on Sparse Matrices, </title> <month> May </month> <year> 1989. </year> <month> 123 </month>
Reference-contexts: Many regular problems, such as FFT and SOR (described in the next two sections), present the same computational load to each node, regardless of the input data. Other algorithms, however, require variable amounts of computation. In Cholesky factorization <ref> [44, 7] </ref>, each processor is assigned a fixed portion of the matrix on which to operate. However, the number of operations performed on each portion is a function of the input data. Some processors may be idle, while others are busy.
Reference: [8] <author> Luiz Andre' Barroso and Michel Dubois. </author> <title> The performance of cache-coherent ring-based multiprocessors. </title> <type> Technical Report CENG-92-19, </type> <institution> University of Southern Cali-fornia, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: Multiprocessors are generally more difficult to design and build than multicomputers. However, many researchers believe that multiprocessors are easier to program [85, 93, 52]. This has led a number of researchers to study how multiprocessors with hundreds of processors should be designed <ref> [58, 2, 12, 8] </ref>. Multiprocessors require hardware either to keep track of the location and status of cache lines as they move through the system, or to effect extremely low latency internode communication, as in the Cray T3D. Software can make multicomputers perform as multiprocessors [59]. <p> While convenient for the logic designer, this rule 19 is not necessary. Some researchers have taken this rule to be one that is intrinsic to buses: Barroso and Dubois motivate small rings for cache-coherent multiprocessors by listing this as a fundamental problem with buses <ref> [8] </ref>. 2.3.2 Beyond Steady State: High Performance Bus Signalling If the steady-state design rule is used, bus clock speed decreases as bus length increases. However, if the system designer circumvents this rule, the bus can be clocked at a high rate that is mostly independent of bus length.
Reference: [9] <author> Robert Bedichek. </author> <title> Some efficient architecture simulation techniques. </title> <booktitle> In Proceedings of the Winter 1990 USENIX Conference, </booktitle> <pages> pages 53-63, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: No other simulator that we are aware of has this combination of features. We achieved simulation efficiency by starting with a fast, threaded-code simulator and adding only those timing models needed to achieve accuracy. This approach resembles that used to gain semantic accuracy in a previous simulator <ref> [9] </ref>. To measure timing accuracy, we ran a suite of benchmarks on the simulator and the Meerkat prototype. Test results guided our grafting of timing models onto the threaded-code simulator base. We introduce the roles of simulation in Section 4.1. <p> Section 4.7 concludes the chapter with a description of the simulator's execution profiling feature. 4.1 Introduction Computer architecture simulators vary widely in their application. They are used by processor architects to evaluate uniprocessor design tradeoffs [26], operating system authors to debug their code <ref> [9] </ref> and to evaluate operating system performance [23, 82], parallel 47 system architects to assess the performance of large systems [18, 75], and by end users to execute programs written for one system on a different host system [62, 86, 87]. <p> An earlier version of the simulator kept just the DECIP in a host register and calculated IP when its value was needed <ref> [9] </ref>. The earlier version was written for a host with a small number of registers, and it therefore made sense to calculate the IP this way. <p> The table lookup corresponds to the address decoders found in hardware between the processor address bus and the I/O device select signals. An older version of our simulator used a hashing scheme to speed the lookup <ref> [9] </ref>. However, the Meerkat simulator has a small table of devices, and the lookup time is not significant.
Reference: [10] <author> Robert Bedichek and Curtis Brown. </author> <title> The Meerkat multicomputer. </title> <booktitle> In Proceedings Fifth Annual International Symposium on Parallel and Distributed Systems, </booktitle> <month> December </month> <year> 1993. </year>
Reference-contexts: This approach moves function from hardware to software, reducing design time and production cost, and increasing flexibility. Our approach also limits scaling range to allow the use of simple, high-performance hardware technology. We use a concrete design, Meerkat <ref> [10] </ref>, as an example of our approach. We demonstrate that Meerkat can scale to hundreds of processors, has a short design time, is simple to construct, and is applicable to common parallel workloads. Several design assumptions are key to this thesis.
Reference: [11] <author> James R. Bell. </author> <title> Threaded code. </title> <journal> Communications of the ACM (CACM), </journal> <volume> 16(2) </volume> <pages> 370-372, </pages> <month> June </month> <year> 1973. </year>
Reference-contexts: In addition, it had to model timing accurately. We could not afford to spend years constructing a complex simulator or waiting for results from a slow one. For these reasons, we wrote a simulator that translates instructions to threaded code <ref> [11, 56] </ref>, which is then executed. The threaded code is cached, so that the price of translation for most instructions is paid just once, the first time they are encountered in the code stream. The result is a simulator that has a slow-down of about 100 per simulated processor. <p> Its 50 timing is close enough to the prototype's that we can use it to run large programs and make meaningful measurements. 4.4 Structure of the Meerkat Simulator through a symbolic debugger, called gdb [88]. The simulator consists of an instruction translator, a threaded-code interpreter <ref> [11] </ref>, cache models, a TLB model, a physical memory system model, and I/O models. Meerkat programs are compiled, assembled, and linked with a set of GNU cross-development tools on a Sparcstation host. The simulator, called mg88 (a contraction of "Meerkat", "gdb", and "88000"), is run on the resulting executable image.
Reference: [12] <author> John K. Bennett, Sandhya Dwarkadas, Jay Greenwood, and Evan Speight. </author> <title> Willow: A scalable shared memory multiprocessor. </title> <booktitle> Proceedings. Supercomputing '92, </booktitle> <pages> pages 336-345, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Multiprocessors are generally more difficult to design and build than multicomputers. However, many researchers believe that multiprocessors are easier to program [85, 93, 52]. This has led a number of researchers to study how multiprocessors with hundreds of processors should be designed <ref> [58, 2, 12, 8] </ref>. Multiprocessors require hardware either to keep track of the location and status of cache lines as they move through the system, or to effect extremely low latency internode communication, as in the Cray T3D. Software can make multicomputers perform as multiprocessors [59].
Reference: [13] <author> Brian N. Bershad, David D. Redell, and John R. Ellis. </author> <title> Fast mutual exclusion for uniprocessors. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 223-233, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: If the send is interrupted, the sending processor will have to reload the user-send registers only once. Both cases require little overhead to support context switching. The send-register loading code resembles a restartable atomic sequence, in that it will be retried if a context switch occurs during its execution <ref> [13] </ref>. Another possible solution is called roll-forward. With this technique, the operating system ensures that it does not context-switch while the network interface registers are in use. It does this by testing for active interface registers when it is ready to context switch.
Reference: [14] <author> Andrew D. Birrell and Bruce Jay Nelson. </author> <title> Implementing remote procedure calls. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(1) </volume> <pages> 39-59, </pages> <month> February </month> <year> 1984. </year>
Reference-contexts: Although all of our experiments use message-passing benchmarks, the Meerkat-1 and Meerkat-2 interfaces are not specific to this programming model. These interfaces could be programmed to support efficiently remote procedure call <ref> [14] </ref>, distributed-shared-memory [59], or remote write [27]. Meerkat-msg and Meerkat-infinite, on the other hand, directly support message passing and thus would be less well suited to other programming styles. 6.2 Meerkat-2 Network Interface Architecture This section identifies problems encountered with the Meerkat-1 interface that gave rise to Meerkat-2.
Reference: [15] <author> Roberto Bisiani and Mosur Ravishankar. </author> <title> Plus: A distributed shared-memory system. </title> <booktitle> In Proc. 17th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: Software can make multicomputers perform as multiprocessors [59]. However, performance on many workloads will not be as good as it would be with hardware support. Some researchers propose intermediate systems that support a shared address space through a combination of hardware and software <ref> [15] </ref>. Multiprocessor design is interesting and important, however our investigation concerned simple, quick-to-design parallel computers.
Reference: [16] <author> T. Blank. </author> <title> The MasPar MP-1 architecture. </title> <booktitle> In Thirty-Fifth IEEE Computer Society Internation Conference, </booktitle> <pages> pages 20-24, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: Expensive processors generally require expensive support components: while their numbers will be lower, their cost will increase. Some experimental parallel systems, such as Mosaic [80], the J-Machine [30], and the Connection Machine [46], use large numbers of small custom processors. Maspar and Thinking Machines Corporation offered the MP-1 <ref> [16] </ref> and the CM-2, respectively, both of which used many small custom processors. These systems can yield high performance on regular problems with high parallelism. However, their performance suffers on irregular workloads and those without sufficient parallelism.
Reference: [17] <author> Gaetano Borriello. </author> <type> Personal communication, </type> <month> April </month> <year> 1994. </year>
Reference-contexts: High clock rates, in turn, require expensive coaxial or fiber-optic cables and GaAs drivers [40]. Thus, planar interconnects using printed circuit wiring are intrinsically cheaper per unit bandwidth. The performance of GaAs is not improving as rapidly as CMOS <ref> [17] </ref>. Therefore, we expect the relative advantage of CMOS-driven interconnects to increase. With recent advances in CMOS technology, it is possible today to fabricate a single, low-cost CMOS gate array that drives over 100 terminated bus wires at 100 MHz [42].
Reference: [18] <author> Eric A. Brewer, Chrysanthos N. Dellacrocas, Adrian Colbrook, and William E. Weihl. Proteus: </author> <title> A high-performance parallel-architecture simulator. </title> <type> Technical Report MIT/LCS/TR-516, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1991. </year> <month> 124 </month>
Reference-contexts: They are used by processor architects to evaluate uniprocessor design tradeoffs [26], operating system authors to debug their code [9] and to evaluate operating system performance [23, 82], parallel 47 system architects to assess the performance of large systems <ref> [18, 75] </ref>, and by end users to execute programs written for one system on a different host system [62, 86, 87]. We used the Meerkat simulator to debug and tune operating system code and to evaluate Meerkat's performance.
Reference: [19] <author> Eric A. Brewer and William E. Weihl. </author> <title> Developing parallel applications using high-performance simulation. ACM/ONR Workshop on Parallel and Dist. Debugging. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 28(12) </volume> <pages> 158-168, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: It is also used in some processors to translate an instruction set that programmers see into a more RISC-like form that is more efficient to execute [25, 34]. 4.3.3 Direct Execution A popular approach to efficient architecture simulation is to execute the target program directly on the host <ref> [19] </ref>. The target program is encased in an environment that makes it 49 execute as though it were on the simulated system. This requires that either the host system have the same instruction set as the target or that the program be recompiled.
Reference: [20] <author> Michael Carlton and Alvin Despain. </author> <title> Aquarius project. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 80-83, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The sender owns the bus (es) for the duration of the connection through which it transmits information to the receiver. Meerkat's architecture (Figure 2.2) resembles those of the Wisconsin Multicube [41] and Aquarius Multi-multi <ref> [20] </ref>. However, these two machines put their buses to different uses than does Meerkat. The Multicube and Aquarius implement a coherent shared memory system in hardware. Therefore, transactions on their buses are initiated by memory reference instructions and cache coherence operations.
Reference: [21] <author> John B. Carter, John K. Bennet, and Willy Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Networks of workstations connected by Ethernet [65] are sometimes used as parallel computers. Ethernet allows workstations to communicate, albeit with low bandwidth and high latency. System software such as PVM [89], Munin <ref> [21] </ref>, Emerald [47], and Amber [22] provides application support on such parallel computers. Some applications require neither high bandwidth nor low latency, and these are well served by networks of workstations. On a global scale, the Internet can be considered a parallel computer.
Reference: [22] <author> Jeffrey S. Chase, Franz G. Amador, Edward D. Lazowska, Henry M. Levy, and Richard J. Littlefield. </author> <title> The Amber System: Parallel programming on a network of multiprocessors. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 147-158, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Networks of workstations connected by Ethernet [65] are sometimes used as parallel computers. Ethernet allows workstations to communicate, albeit with low bandwidth and high latency. System software such as PVM [89], Munin [21], Emerald [47], and Amber <ref> [22] </ref> provides application support on such parallel computers. Some applications require neither high bandwidth nor low latency, and these are well served by networks of workstations. On a global scale, the Internet can be considered a parallel computer. It provides an even lower bandwidth and higher latency interconnect than Ethernet. <p> While some research systems have supported this model <ref> [22] </ref>, it has not gained wide acceptance. * User-level access to the network interface (see Chapter 6) is more complicated, because much of the network interface state must be duplicated for each processor. * Physical node size will increase to accommodate the additional processors, which affects node performance.
Reference: [23] <author> J. Bradley Chen and Brian N. Bershad. </author> <title> The impact of operating system performance on memory system performance. </title> <booktitle> Proceedings of the 14th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 120-133, </pages> <month> Dec </month> <year> 1993. </year>
Reference-contexts: They are used by processor architects to evaluate uniprocessor design tradeoffs [26], operating system authors to debug their code [9] and to evaluate operating system performance <ref> [23, 82] </ref>, parallel 47 system architects to assess the performance of large systems [18, 75], and by end users to execute programs written for one system on a different host system [62, 86, 87].
Reference: [24] <author> Christopher Cheng and Leo Yuan. </author> <title> Electrical design of a 1 GByte/sec high performance backplane using low voltage swing CMOS (GTL). </title> <booktitle> In Proceedings of Research on Integrated Systems, </booktitle> <pages> pages 291-299. </pages> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: In such a system, two nodes driving the same bus will cause the bus to have the wired-OR of the two values being driven, but no electrical problems result. The bus arbitration protocol can handle the wired-OR condition. The XDBus <ref> [24] </ref>, used in high-end SUN SPARC systems, is an example of a bus that uses single-rail drivers.
Reference: [25] <author> D. W. Clark. </author> <title> Pipelining and performance in the VAX-8800 processor. </title> <booktitle> Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> Oct </month> <year> 1987. </year>
Reference-contexts: This idea has been used in a variety of simulators for a number of applications [26, 32, 61, 62, 86]. It is also used in some processors to translate an instruction set that programmers see into a more RISC-like form that is more efficient to execute <ref> [25, 34] </ref>. 4.3.3 Direct Execution A popular approach to efficient architecture simulation is to execute the target program directly on the host [19]. The target program is encased in an environment that makes it 49 execute as though it were on the simulated system.
Reference: [26] <author> Robert F. Cmelik and David Keppel. Shade: </author> <title> A fast instruction-set simulator for execution profiling. </title> <booktitle> In 1994 ACM SIGMETRICS Conference on Modeling and Measurement of Computer Systems, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: The development of timing models and simulator timing accuracy are discussed in Section 4.6. Section 4.7 concludes the chapter with a description of the simulator's execution profiling feature. 4.1 Introduction Computer architecture simulators vary widely in their application. They are used by processor architects to evaluate uniprocessor design tradeoffs <ref> [26] </ref>, operating system authors to debug their code [9] and to evaluate operating system performance [23, 82], parallel 47 system architects to assess the performance of large systems [18, 75], and by end users to execute programs written for one system on a different host system [62, 86, 87]. <p> Instead of decoding the operation fields each time an instruction is executed, we can translate the instruction once into a form that is faster to execute. This idea has been used in a variety of simulators for a number of applications <ref> [26, 32, 61, 62, 86] </ref>. <p> This requires that either the host system have the same instruction set as the target or that the program be recompiled. Instructions that cannot execute directly on the host are replaced with procedure calls to simulator code. If every instruction were rewritten, the simulator would resemble Shade <ref> [26] </ref>, which translates every target instruction to a sequence of host instructions. Direct execution simulators are usually capable of executing only user-space code. To evaluate tradeoffs in multicomputer architecture, we needed to simulate both user and kernel code. It is possible to use direct execution techniques to simulate kernel code. <p> There are a number of ways in which the simulator could be made faster while maintaining its accuracy. First, native host code could be generated, as in Shade <ref> [26] </ref>. Second, the simulator could be parallelized and run on a multicomputer 1 . The simulator could be made more accurate while sacrificing minimal performance by making the instruction cache model accurate and by modelling the single register write-back port. Currently, the simulator models instruction cache cold misses only.
Reference: [27] <author> Douglas Comer and James Griffioen. </author> <title> A new design for distributed systems: The remote memory model. </title> <booktitle> In Proceedings of the Summer 1990 USENIX Conference, </booktitle> <pages> pages 127-135, </pages> <month> June </month> <year> 1990. </year> <month> 125 </month>
Reference-contexts: Although all of our experiments use message-passing benchmarks, the Meerkat-1 and Meerkat-2 interfaces are not specific to this programming model. These interfaces could be programmed to support efficiently remote procedure call [14], distributed-shared-memory [59], or remote write <ref> [27] </ref>. Meerkat-msg and Meerkat-infinite, on the other hand, directly support message passing and thus would be less well suited to other programming styles. 6.2 Meerkat-2 Network Interface Architecture This section identifies problems encountered with the Meerkat-1 interface that gave rise to Meerkat-2.
Reference: [28] <author> J.W. Cooley and J.W. Tukey. </author> <title> An algorithm for the machine calculation of complex Fourier series. </title> <journal> Math. Computation, </journal> <volume> 19 </volume> <pages> 297-301, </pages> <year> 1965. </year>
Reference-contexts: Figure 5.8 shows Meerkat's and Delta's speedup achieved on a one-dimensional FFT <ref> [28] </ref> as a function of the number of nodes applied to the problem. Curves for two input sizes are shown: 4096 and 32768 points. The input points are evenly spread amongst the nodes; each point consists of a pair double precision floating point values.
Reference: [29] <author> W. Crowley, </author> <title> C.P. Hendrickson, and T.I. Luby. The SIMPLE code. </title> <type> Technical Report UCID-17715, </type> <year> 1978. </year>
Reference-contexts: Delta's 90 natural advantage on this communication pattern is not large enough to compensate for its less-efficient interconnect. 5.11 Performance of SIMPLE: A Fluid Dynamics Benchmark Crowley et al. introduced SIMPLE in 1977 as a benchmark to evaluate new computers <ref> [29] </ref>. SIMPLE models the hydrodynamics of a pressurized fluid inside a spherical shell. It has been widely studied for a variety of purposes, including programming environment evaluation, computer performance evaluation, and portability studies [60]. The message-passing implementation we used modeled 4096 points spread evenly across the nodes.
Reference: [30] <author> William Dally. </author> <title> The J-Machine System, volume 1. </title> <publisher> MIT Press, </publisher> <address> Cambridge Mass., </address> <year> 1990. </year>
Reference-contexts: In the past, multicomputer designers focused their efforts on the interconnect and node processor. The network interface, seen as significant to the operating system only, was 7 deemed incidental to performance. More recently, designers have recognized the benefits of streamlining network access <ref> [30] </ref> and allowing application code direct access to the network interface [45, 38]. Our Meerkat-2 study is in line with this research: we show that the choice of network interface can dominate system performance. For the systems we modelled, this choice becomes more important as message sizes decrease. <p> Expensive processors generally require expensive support components: while their numbers will be lower, their cost will increase. Some experimental parallel systems, such as Mosaic [80], the J-Machine <ref> [30] </ref>, and the Connection Machine [46], use large numbers of small custom processors. Maspar and Thinking Machines Corporation offered the MP-1 [16] and the CM-2, respectively, both of which used many small custom processors. These systems can yield high performance on regular problems with high parallelism. <p> The network interface, seen as significant to the operating system only, was deemed incidental to performance. More recently, designers have recognized the benefits of streamlining network access <ref> [30] </ref> and allowing application code direct access to the network interface [45, 38]. To evaluate tradeoffs in network interface designs, we use the Meerkat simulator and the same suite of parallel applications described in Chapter 5.
Reference: [31] <author> William J. Dally. </author> <title> Wire-efficient VLSI multiprocessor communication networks. </title> <editor> In Paul Losleben, editor, </editor> <booktitle> Proceedings Stanford Conference on Advanced Research in VLSI, </booktitle> <pages> pages 391-415, </pages> <address> Cambridge Mass., 1987. </address> <publisher> MIT Press. </publisher>
Reference-contexts: One or both of these effects will occur in a given implementation. While our prototype uses multiple-processor nodes, we envision future Meerkat imple mentations with single-processor nodes. 15 2.1.6 Interconnect Planarity Multi interconnect performance is limited by the density of the wires that connect nodes <ref> [31] </ref>. Meerkat's interconnect is planar, which means that the wires connecting nodes can be routed in a limited number of wiring planes. These limits allow an inexpensive implementation with a conventional, controlled-impedance backplane driven by CMOS integrated circuits.
Reference: [32] <author> Peter Deutsch and Alan M. Schiffman. </author> <title> Efficient implementation of the Smalltalk-80 system. </title> <booktitle> 11th Annual Symposium on Principles of Programming Languages, </booktitle> <pages> pages 297-302, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: Instead of decoding the operation fields each time an instruction is executed, we can translate the instruction once into a form that is faster to execute. This idea has been used in a variety of simulators for a number of applications <ref> [26, 32, 61, 62, 86] </ref>.
Reference: [33] <author> Digital Equipment Corporation, </author> <title> Workstation Systems Engineering. PMADD-AA Tur-boChannel Ethernet Module Functional Specification, </title> <journal> Rev 1.2., </journal> <month> August </month> <year> 1990. </year>
Reference-contexts: The DEC AN1 controller [79] is a recent example of such a controller. * Limited-DMA. This resembles asynchronous-DMA, except that the processor can access only a small, dedicated region of memory located on the controller. It is typically hundreds of kilobytes in length. The DECstation PMADD-AA TurboChannel Ethernet interface <ref> [33] </ref> is an example of a limited-DMA interface. * Separate-DMA. This resembles limited-DMA, except that the processor cannot access the small buffer memory that the controller can access directly. Data move between the separate buffer memory and system memory by remote-DMA.
Reference: [34] <author> David R. Ditzel, Hubert R. MeLellan, and Alan D. Berenbaum. </author> <title> The hardware architecture of the CRISP microprocessor. </title> <booktitle> Proceedings of the 14th Annual International Symposium on Computer Architecture; Computer Architecture News, </booktitle> <volume> 15(2) </volume> <pages> 309-319, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: This idea has been used in a variety of simulators for a number of applications [26, 32, 61, 62, 86]. It is also used in some processors to translate an instruction set that programmers see into a more RISC-like form that is more efficient to execute <ref> [25, 34] </ref>. 4.3.3 Direct Execution A popular approach to efficient architecture simulation is to execute the target program directly on the host [19]. The target program is encased in an environment that makes it 49 execute as though it were on the simulated system.
Reference: [35] <author> Dan Dobberpuhl, R. Witek, R. Allmon, R. Anglin, D. Bertucci, S. Britton, L. Chao, R. Conrad, D. Dever, B. Gieseke, S. Hassoun, G. Hoeppner, J. Kowaleski, K. Kuchler, M. Ladd, M. Leary, L. Madden, E. McLellan, D. Meyer, J. Montanaro, D. Priore, V. Rajagopalan, S. Samudrala, and S. Santhanam. </author> <title> A 200MHz 64 bit dual issue CMOS microprocessor. </title> <booktitle> In International Solid-State Circuits Conference 1992, </booktitle> <month> February </month> <year> 1992. </year>
Reference-contexts: Every year the price per unit of performance and per bit drops significantly. Kendall Square Research made the mistake of not using commercial processors and is suffering as a result. Cray Research for years designed their own processors, but has now switched to the DEC Alpha <ref> [35] </ref> as the core of their multiprocessors. We concluded that a viable machine would use commercial microprocessors and memory. We envisioned a multicomputer of nodes composed of commodity components, running an operating system that supports message passing, at a minimum.
Reference: [36] <author> Peter Druschel and Larry L. Peterson. Fbufs: </author> <title> A high-bandwidth cross-domain transfer facility. </title> <booktitle> In Proceedings of the 14th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 189-202, </pages> <month> December </month> <year> 1993. </year> <month> 126 </month>
Reference-contexts: The former reduces the number of physical discontinuities in the user space's virtual map. The latter is then able to avoid the few remaining discontinuities with little effort. Druschel and Peterson describe an operating system facility to support a user-level allocation function for I/O and inter-domain buffers <ref> [36] </ref>. Their facility resembles ours in that its goals are high-performance I/O, and it involves a careful user-level buffer allocator working in concert with the operating system. It differs from our approach in that its buffers are passed between domains, whereas our allocator simply avoids physical discontinuities in user buffers.
Reference: [37] <author> G.M. Papadopoulos et al. </author> <title> *T: Integrated building blocks for parallel computing. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 624-635, </pages> <year> 1993. </year>
Reference-contexts: For example, the Stanford DASH [58] uses MIPS R3000s, the Cray T3D uses DEC Alphas, and the Intel Paragon uses Intel i860s. Some experimental systems use modified commercial parts. For example, the MIT Alewife uses modified Sparc microprocessors, while the MIT *T planned to use modified Motorola 88110s <ref> [37] </ref>. We conclude that commodity microprocessors are the processors of choice in multis where the price/performance ratio is important.
Reference: [38] <author> Edward W. Felten. </author> <title> Protocol Compilation: High-Performance Communication for Parallel Programs. </title> <type> Ph.D. thesis, </type> <institution> Department of Computer Science and Enginerring, University of Washington, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: The network interface, seen as significant to the operating system only, was 7 deemed incidental to performance. More recently, designers have recognized the benefits of streamlining network access [30] and allowing application code direct access to the network interface <ref> [45, 38] </ref>. Our Meerkat-2 study is in line with this research: we show that the choice of network interface can dominate system performance. For the systems we modelled, this choice becomes more important as message sizes decrease. <p> The network interface, seen as significant to the operating system only, was deemed incidental to performance. More recently, designers have recognized the benefits of streamlining network access [30] and allowing application code direct access to the network interface <ref> [45, 38] </ref>. To evaluate tradeoffs in network interface designs, we use the Meerkat simulator and the same suite of parallel applications described in Chapter 5. We augment the original Meerkat 94 simulator with device models for three additional network interfaces.
Reference: [39] <author> FORE Systems, </author> <title> 1000 Gamma Drive, Pittsburgh PA 15238. TCA-100 TURBOchannel ATM Computer Interface, User's Manual, </title> <year> 1992. </year>
Reference-contexts: The processor reads and writes a single location that maps to the ends of a pair of FIFOs. The FIFOs buffer data between the processor and the network. The Intel Delta [50] and Fore System's ATM interface <ref> [39] </ref> use this type of interface. * Fixed-register. The processor can access a small array of memory in the interface. The processor writes data into this array and then touches a location to start the transfer. The entire contents of the array are transmitted.
Reference: [40] <institution> GigaBit Logic, Inc., 1908 Oak Terrace Lane, Newbury Park, </institution> <address> CA 91320. </address> <note> GigaBit Logic GaAs IC Data Book and Designer's Guide, </note> <year> 1991. </year>
Reference-contexts: Instead, nodes of these systems are connected by links that use cables, which have relatively few wires, often just one. To maintain the same data rates, these cables must use much higher clock rates. High clock rates, in turn, require expensive coaxial or fiber-optic cables and GaAs drivers <ref> [40] </ref>. Thus, planar interconnects using printed circuit wiring are intrinsically cheaper per unit bandwidth. The performance of GaAs is not improving as rapidly as CMOS [17]. Therefore, we expect the relative advantage of CMOS-driven interconnects to increase.
Reference: [41] <author> James R. Goodman and Philip J. Woest. </author> <title> The Wisconsin Multicube: A new large-scale cache-coherent multiprocessor. </title> <booktitle> In Proceedings 17th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 422-431, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Every node is a potential cross-point for two-bus connections between nodes that share its vertical and horizontal buses. The sender owns the bus (es) for the duration of the connection through which it transmits information to the receiver. Meerkat's architecture (Figure 2.2) resembles those of the Wisconsin Multicube <ref> [41] </ref> and Aquarius Multi-multi [20]. However, these two machines put their buses to different uses than does Meerkat. The Multicube and Aquarius implement a coherent shared memory system in hardware. Therefore, transactions on their buses are initiated by memory reference instructions and cache coherence operations.
Reference: [42] <author> Bill Gunning, Leo Yuan, Trung Nguyen, and Tony Wong. </author> <title> A CMOS low-voltage-swing transmission-line transceiver. </title> <booktitle> IEEE International Solid-State Circuits Conference, </booktitle> <pages> pages 58-59, </pages> <year> 1992. </year>
Reference-contexts: Therefore, we expect the relative advantage of CMOS-driven interconnects to increase. With recent advances in CMOS technology, it is possible today to fabricate a single, low-cost CMOS gate array that drives over 100 terminated bus wires at 100 MHz <ref> [42] </ref>. Fat-tree networks [57] are planar in theory, but in practice manufacturers such as Thinking Machines are not able to take advantage of this property. The CM-5's fat-tree network, for example, uses coaxial cables to connect nodes of the fat tree. <p> This optimistic protocol allows data transmission and arbitration to proceed concurrently. This technique can also be used in high performance parallel buses. * Gunning Transceiver Logic (GTL) <ref> [42] </ref>. This technique requires a fraction of the signalling energy of older methods, such as ECL, BTL and CMOS. GTL transceivers can be fabricated with conventional CMOS transistors. GTL is used in the new XDBus from SUN Microsystems and Xerox Corp [84]. <p> nodes per bus * Internode bus lengths of 75 cm, stub lengths of 5 cm or less * A maximum of sixteen taps (each node having a tap) per internode bus * Internode buses clocked at 100 MHz with one 32-bit word transferred per clock tick * Use of GTL <ref> [42] </ref> logic levels for internode communication These characteristics yield a system that can have 256 nodes and a raw, per-bus internode transfer rate of 400 MB/sec. We estimate that under heavy load, such a system would provide 20 MB/sec per node, assuming that most communication requires two buses.
Reference: [43] <author> D. Gustavson and J. Theus. </author> <title> Wire-Or logic on transmission lines. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 51-55, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: A typical bound for the bus clock period is: T cycle &gt; 2T prop + T setup + T skew The 2T prop term represents the worst case time for a wired-OR glitch to dissipate <ref> [43] </ref>. T setup is the time that the receiver's latch requires the data to be stable before it is latched. T skew is the maximum uncertainty in the clock signal that tells the receiver's latch when to sample the value on the bus wire. <p> A bus that goes from such an idle state into a driven state will not experience a wired-OR glitch and thus can avoid having to wait for a bus round-trip propagation to let such a glitch settle <ref> [43] </ref>. When a bus goes from being owned by one node to being owned by 20 another, it will have to wait for the wired-OR glitch to settle.
Reference: [44] <author> M.T. Heath, E. Ng, and B.W. Peyton. </author> <title> Parallel algorithms for sparse linear systems. </title> <journal> SIAM Review, </journal> <volume> 33, </volume> <year> 1991. </year>
Reference-contexts: Many regular problems, such as FFT and SOR (described in the next two sections), present the same computational load to each node, regardless of the input data. Other algorithms, however, require variable amounts of computation. In Cholesky factorization <ref> [44, 7] </ref>, each processor is assigned a fixed portion of the matrix on which to operate. However, the number of operations performed on each portion is a function of the input data. Some processors may be idle, while others are busy.
Reference: [45] <author> Dana S. Henry and Christopher F. Joerg. </author> <title> A tightly-coupled processor-network interface. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: The network interface, seen as significant to the operating system only, was 7 deemed incidental to performance. More recently, designers have recognized the benefits of streamlining network access [30] and allowing application code direct access to the network interface <ref> [45, 38] </ref>. Our Meerkat-2 study is in line with this research: we show that the choice of network interface can dominate system performance. For the systems we modelled, this choice becomes more important as message sizes decrease. <p> The network interface, seen as significant to the operating system only, was deemed incidental to performance. More recently, designers have recognized the benefits of streamlining network access [30] and allowing application code direct access to the network interface <ref> [45, 38] </ref>. To evaluate tradeoffs in network interface designs, we use the Meerkat simulator and the same suite of parallel applications described in Chapter 5. We augment the original Meerkat 94 simulator with device models for three additional network interfaces. <p> Examples of systems with network interfaces that are not user-safe include workstations, Intel parallel systems, and Meerkat-1. The CM-5 and Meerkat-2 have user-safe interfaces. Henry and Joerg proposes extensions to their network interface to make it user-safe <ref> [45] </ref>. A second way to categorize network interfaces is by the way in which data are moved into them. We present a number of such categories: * Asynchronous-DMA. The processor programs the controller to read or write a single block of system memory.
Reference: [46] <author> W. Daniel Hillis. </author> <title> The Connection Machine. </title> <publisher> MIT Press, </publisher> <address> Cambridge Mass., </address> <year> 1985. </year>
Reference-contexts: Expensive processors generally require expensive support components: while their numbers will be lower, their cost will increase. Some experimental parallel systems, such as Mosaic [80], the J-Machine [30], and the Connection Machine <ref> [46] </ref>, use large numbers of small custom processors. Maspar and Thinking Machines Corporation offered the MP-1 [16] and the CM-2, respectively, both of which used many small custom processors. These systems can yield high performance on regular problems with high parallelism.
Reference: [47] <author> Norman C. Hutchinson. </author> <title> Emerald: An Object-Based Language for Distributed Programming. </title> <type> Ph.D. thesis, </type> <institution> University of Washington, </institution> <month> January </month> <year> 1987. </year> <institution> Department of Computer Science technical report 87-01-01. </institution> <month> 127 </month>
Reference-contexts: Networks of workstations connected by Ethernet [65] are sometimes used as parallel computers. Ethernet allows workstations to communicate, albeit with low bandwidth and high latency. System software such as PVM [89], Munin [21], Emerald <ref> [47] </ref>, and Amber [22] provides application support on such parallel computers. Some applications require neither high bandwidth nor low latency, and these are well served by networks of workstations. On a global scale, the Internet can be considered a parallel computer.
Reference: [48] <institution> Intel Corp, </institution> <address> 2065 Bowers Avenue, Santa Clara, California 95051. </address> <note> Touchstone Delta C System Calls Reference Manual, </note> <year> 1991. </year>
Reference-contexts: Delta that complicate the comparison of the two interconnect architectures: * The 40 MHz Intel i860 processors in Delta are about twice as fast as the 20 MHz Motorola 88100 processors in Meerkat. * Meerkat's message-passing code is written in carefully crafted assembler, while Delta runs the NX/M operating system <ref> [48] </ref>, written in C. * The Meerkat internode interface copies to and from memory, whereas the Delta interface requires the processor to load and store each byte moved through the interface (i.e., programmed I/O). * The Delta test program runs in an address space separate from NX/M and thus incurs context-switching
Reference: [49] <institution> Intel Corp, </institution> <address> 2065 Bowers Avenue, Santa Clara, California 95051. </address> <note> A Touchstone Delta System Description, </note> <month> February </month> <year> 1991. </year>
Reference-contexts: Chapter 5 A PERFORMANCE ANALYSIS OF MEERKAT This chapter compares the performance of programs running on both the Meerkat prototype and Meerkat simulator 1 with those running on Intel's Touchstone Delta <ref> [49] </ref>. These comparisons demonstrate that Meerkat is an effective multicomputer architecture when compared to Delta, a heavily used, commercially built multicomputer.
Reference: [50] <institution> Intel Corp, </institution> <address> 2065 Bowers Avenue, Santa Clara, California 95051. </address> <note> Touchstone Delta System User's Guide, </note> <year> 1991. </year>
Reference-contexts: The processor reads and writes a single location that maps to the ends of a pair of FIFOs. The FIFOs buffer data between the processor and the network. The Intel Delta <ref> [50] </ref> and Fore System's ATM interface [39] use this type of interface. * Fixed-register. The processor can access a small array of memory in the interface. The processor writes data into this array and then touches a location to start the transfer. The entire contents of the array are transmitted.
Reference: [51] <institution> Intel Supercomputer Systems Division. Paragon XP/S Product Overview, </institution> <year> 1991. </year>
Reference-contexts: Well-funded laboratories are willing to pay these high costs for fast systems that can solve numerical problems requiring either frequent interprocessor communication and/or the fastest single-stream performance. Between these extremes of both cost and communication performance are systems such as Intel's Paragon <ref> [51] </ref>, Thinking Machine's CM-5 [92], and Cray's T3D. These systems use conventional microprocessors, which lowers their design cost and reduces their physical size (compared to systems, such as the older Crays, whose processors are composed of many chips). <p> There is 12 no lack of demand for the greater computational power of larger systems. Rather, size is limited by economics. Nodes of commercial multis, such as the Thinking Machine's CM-5 [92] and the Intel Paragon <ref> [51] </ref>, use powerful microprocessors and large memories, and therefore cost thousands of dollars. A 256-node system, for example, would cost millions of dollars.
Reference: [52] <author> Alan H. Karp. </author> <title> Programming for parallelism. </title> <journal> IEEE Computer, </journal> <volume> 20(5) </volume> <pages> 43-57, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: The most common programming model used on these systems is called message passing, because parallel programs written for these machines explicitly send and receive messages. Multiprocessors are generally more difficult to design and build than multicomputers. However, many researchers believe that multiprocessors are easier to program <ref> [85, 93, 52] </ref>. This has led a number of researchers to study how multiprocessors with hundreds of processors should be designed [58, 2, 12, 8].
Reference: [53] <author> Peter B. Kessler. </author> <title> Fast breakpoints: </title> <booktitle> Design and implementation. In Proc. of the ACM SIGPLAN '90 Conference on Programming Language Design and Implementation; SIGPLAN Notices, </booktitle> <volume> volume 25, </volume> <pages> pages 78-84, </pages> <address> White Plains, NY, USA, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: We patch a branch to this code into the text where we would otherwise have inserted a breakpoint instruction. Fast breakpoint condition evaluation has been implemented before <ref> [53] </ref>. What we report here is new in that we are doing it in a system debugger, we found it useful in this environment, and that it took only a few days to implement. G88 generates Motorola 88100 code to evaluate the conditional expression on the target for most expressions. <p> Operating system debuggers usually have a larger cost of communicating with the debugee than do conventional debuggers. As a result, the benefit gained from evaluating the breakpoint on the target is even higher than in the systems that Kessler considered <ref> [53] </ref>. 4.9 Summary Simulators vary widely in their application, structure, accuracy, and performance. We outlined several simulator applications and the simulators typically used. The Meerkat simulator is unique in its combination of speed, efficient use of memory, fine modelling detail, portability, user/supervisor modelling, and modelling of address translation.
Reference: [54] <author> John Kubiatowicz and Anant Agarwal. </author> <title> Anatomy of a message in the Alewife multiprocessor. </title> <booktitle> In 7th ACM International Conference on Supercomputing, </booktitle> <year> 1993. </year>
Reference-contexts: The inclusion of immediate registers also allows the building of an interface without scatter/gather. Scatter/gather is often used to send a few control words followed by a mass of data. Meerkat-2 resembles the Alewife message interface <ref> [54] </ref>, except that the latter is more general. Meerkat-2 has a fixed number of registers and cannot perform scatter/gather. The Alewife interface has: (1) an array of registers that can hold immediate data, and (2) address/length pairs that describe regions of memory.
Reference: [55] <author> Jeffrey Kuskin, David Ofelt, Mark Heinrich, John Heinlein, Richard Simoni, Kourosh Charachorloo, John Chapin, David Hakahira, Joel Bater, Mark Horowitz, Anoop Gupta, Mendel Rosenblum, and John Hennessy. </author> <title> The Stanford FLASH multiprocessor. </title> <booktitle> In 21th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: However, there are several practical and theoretical problems with having multiple, general-purpose processors per node: * The design time is significantly longer. The designers of the Stanford FLASH multiprocessor chose to use one processor per node <ref> [55] </ref>, citing design complexity of multiple processor nodes as the principal reason. * The operating system is more difficult to write and debug, because it must support multiple nodes and multiple processors per node. <p> A separate processor dedicated to handling network traffic mediates between the node's main processor and the network. This allows a wide variety of interface styles to be exposed to the processor. Examples of systems with a protocol processor that handles internode traffic include the FLASH multiprocessor <ref> [55] </ref>, Intel's Delta, and Nectar [6]. DMA interfaces allow the full bandwidth of system memory to be coupled to the interconnect, but they also present complexities that FIFO interfaces do not. FIFO interfaces usually require less overhead to transmit and receive short messages.
Reference: [56] <author> T.G. Lang, J.T. O'Quin, and R.O. Simpson. </author> <title> Threaded code interpreter for object code. </title> <journal> IBM Technical Disclosure Bulletin, </journal> <pages> pages 4238-4241, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: In addition, it had to model timing accurately. We could not afford to spend years constructing a complex simulator or waiting for results from a slow one. For these reasons, we wrote a simulator that translates instructions to threaded code <ref> [11, 56] </ref>, which is then executed. The threaded code is cached, so that the price of translation for most instructions is paid just once, the first time they are encountered in the code stream. The result is a simulator that has a slow-down of about 100 per simulated processor.
Reference: [57] <author> Charles E. Leiserson. Fat-trees: </author> <title> Universal networks for hardware-efficient supercomputing. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-34(10):892-901, </volume> <month> October </month> <year> 1985. </year>
Reference-contexts: Therefore, we expect the relative advantage of CMOS-driven interconnects to increase. With recent advances in CMOS technology, it is possible today to fabricate a single, low-cost CMOS gate array that drives over 100 terminated bus wires at 100 MHz [42]. Fat-tree networks <ref> [57] </ref> are planar in theory, but in practice manufacturers such as Thinking Machines are not able to take advantage of this property. The CM-5's fat-tree network, for example, uses coaxial cables to connect nodes of the fat tree.
Reference: [58] <author> Daniel Lenoski, James Laudon, Kourosh Gharachorloo, Anoop Gupta, and John Hen-nessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Proc. 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year> <month> 128 </month>
Reference-contexts: This may be why Thinking Machines' more recent system, the CM-5, uses the popular Sparc microprocessor rather than the tiny-custom-processor approach. Many experimental and commercial multis use commodity microprocessors. For example, the Stanford DASH <ref> [58] </ref> uses MIPS R3000s, the Cray T3D uses DEC Alphas, and the Intel Paragon uses Intel i860s. Some experimental systems use modified commercial parts. For example, the MIT Alewife uses modified Sparc microprocessors, while the MIT *T planned to use modified Motorola 88110s [37]. <p> Multiprocessors are generally more difficult to design and build than multicomputers. However, many researchers believe that multiprocessors are easier to program [85, 93, 52]. This has led a number of researchers to study how multiprocessors with hundreds of processors should be designed <ref> [58, 2, 12, 8] </ref>. Multiprocessors require hardware either to keep track of the location and status of cache lines as they move through the system, or to effect extremely low latency internode communication, as in the Cray T3D. Software can make multicomputers perform as multiprocessors [59]. <p> The current cost for these items and the requisite integrated circuits is about $90 per node. Compared to the total cost of a node, which is $7,500, this figure is insignificant. The overhead of the interconnect and distributed shared memory hardware in DASH <ref> [58] </ref> is about 20 percent of the system. While that amount is reasonable for a machine of its class, it represents a substantially higher system cost than Meerkat's two percent overhead.
Reference: [59] <author> Kai Li. </author> <title> Shared Virtual Memory on Loosely Coupled Multiprocessors. </title> <type> Ph.D. thesis, </type> <institution> Yale University, </institution> <month> September </month> <year> 1986. </year> <note> Technical Report YALEU/DCS/RR-492. </note>
Reference-contexts: Multiprocessors require hardware either to keep track of the location and status of cache lines as they move through the system, or to effect extremely low latency internode communication, as in the Cray T3D. Software can make multicomputers perform as multiprocessors <ref> [59] </ref>. However, performance on many workloads will not be as good as it would be with hardware support. Some researchers propose intermediate systems that support a shared address space through a combination of hardware and software [15]. <p> Chapter 5 discusses the structure and performance of a message-passing system we implemented. While we have not yet implemented DSM for Meerkat, we believe that Kai Li's success with DSM on systems with slower interconnects <ref> [59] </ref> will be repeated with even better results on systems such as Meerkat. This section discusses some of the software issues that are specific to Meerkat. 2.4.1 Software-Controlled Interconnect Meerkat's interconnect is software-controlled. <p> Although all of our experiments use message-passing benchmarks, the Meerkat-1 and Meerkat-2 interfaces are not specific to this programming model. These interfaces could be programmed to support efficiently remote procedure call [14], distributed-shared-memory <ref> [59] </ref>, or remote write [27]. Meerkat-msg and Meerkat-infinite, on the other hand, directly support message passing and thus would be less well suited to other programming styles. 6.2 Meerkat-2 Network Interface Architecture This section identifies problems encountered with the Meerkat-1 interface that gave rise to Meerkat-2.
Reference: [60] <author> Calvin Lin and Lawrence Snyder. </author> <title> A portable implementation of SIMPLE. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 20(5) </volume> <pages> 363-401, </pages> <year> 1991. </year>
Reference-contexts: SIMPLE models the hydrodynamics of a pressurized fluid inside a spherical shell. It has been widely studied for a variety of purposes, including programming environment evaluation, computer performance evaluation, and portability studies <ref> [60] </ref>. The message-passing implementation we used modeled 4096 points spread evenly across the nodes. The computation is similar to that of SOR: communication is all nearest-neighbor and alternates between computation and communication phases.
Reference: [61] <author> Peter S. Magnusson. </author> <title> A design for efficient simulation of a multiprocessor. </title> <booktitle> MASCOTS '93 Proceedings of the 1993 Western Simulation Multiconference on International Workshop on Modeling, Analysis, </booktitle> <institution> and Simulation of Computer and Telecommunication Systems, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Instead of decoding the operation fields each time an instruction is executed, we can translate the instruction once into a form that is faster to execute. This idea has been used in a variety of simulators for a number of applications <ref> [26, 32, 61, 62, 86] </ref>.
Reference: [62] <author> Cathy May. </author> <title> Mimic: A Fast S/370 Simulator. </title> <booktitle> In Proceedings of the ACM SIGPLAN 1987 Symposium on Interpreters and Interpretive Techniques; SIGPLAN Notices, </booktitle> <volume> volume 22, </volume> <pages> pages 1-13, </pages> <address> St. Paul, MN, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: evaluate uniprocessor design tradeoffs [26], operating system authors to debug their code [9] and to evaluate operating system performance [23, 82], parallel 47 system architects to assess the performance of large systems [18, 75], and by end users to execute programs written for one system on a different host system <ref> [62, 86, 87] </ref>. We used the Meerkat simulator to debug and tune operating system code and to evaluate Meerkat's performance. Simulators also vary in their performance and the level of detail they can model. <p> Instead of decoding the operation fields each time an instruction is executed, we can translate the instruction once into a form that is faster to execute. This idea has been used in a variety of simulators for a number of applications <ref> [26, 32, 61, 62, 86] </ref>. <p> Instead, instructions are first translated to decoded instructions, which are cached in structures called decoded instruction pages. Only instructions encountered during execution are translated and cached, so unlike Mimic <ref> [62] </ref>, there is little startup overhead. Decoded instructions contain up to six fields. The first is always a pointer to the instruction's handler, the code that interprets the instruction. This makes it easy to dispatch decoded instructions.
Reference: [63] <institution> MC88100 RISC Microprocessor User's Manual. </institution> <address> 2900 South Diablo Way, Tempe, Arizona 85282. </address>
Reference-contexts: The wide boxes labeled "S-Bus slot 0" and "S-Bus slot 1" are connectors that allow connection of I/O adapters. The top box in Figure 3.2 shows the four Motorola MC88100 processors <ref> [63] </ref> and eight MC88200 Cache and Memory Management Units (CMMUs) that support them. Each CMMU contains 16 KB of cache. Four of the CMMUs are for code, and four are for data. Thus, each processor has one code CMMU and one data CMMU.
Reference: [64] <institution> MC88200 Cache/Memory Management User's Manual. </institution> <address> 2900 South Diablo Way, Tempe, Arizona 85282. </address>
Reference-contexts: Hence, the incremental cost of the Meerkat interconnect would be low. 35 3.4 Meerkat's Memory System Meerkat's memory system is the most complex part of the prototype's design. The MC88100 processors are each connected to a pair of MC88200 CMMUS <ref> [64] </ref>) that cache data and translate addresses. Each MC88200 within a node is also connected to a single MBus. The MBus is the CMMU's connection to memory and to other CMMUs for snooping. The MBus is translated into the S-Bus, which, in turn is connected to memory. <p> The timing of the data cache is modelled by keeping track of what the tag state of a real MC88200 cache <ref> [64] </ref> would be. We model the MC88200's Least Recently Used (LRU) behavior by keeping the cycle count of the most recent access to each cache line. While this takes more storage than the LRU bit scheme that the hardware uses, it is simpler to understand and faster to execute.
Reference: [65] <author> R.M. Metcalfe and D.R. Boggs. </author> <title> Ethernet: Distributed packet switching for local computer networks. </title> <journal> Communications of the ACM, </journal> <volume> 19(7) </volume> <pages> 395-404, </pages> <month> July </month> <year> 1976. </year>
Reference-contexts: Networks of workstations connected by Ethernet <ref> [65] </ref> are sometimes used as parallel computers. Ethernet allows workstations to communicate, albeit with low bandwidth and high latency. System software such as PVM [89], Munin [21], Emerald [47], and Amber [22] provides application support on such parallel computers. <p> This is the technique that Ethernet employs so that senders can transmit without knowing that they have exclusive access to the bus <ref> [65] </ref>. This optimistic protocol allows data transmission and arbitration to proceed concurrently. This technique can also be used in high performance parallel buses. * Gunning Transceiver Logic (GTL) [42]. This technique requires a fraction of the signalling energy of older methods, such as ECL, BTL and CMOS. <p> For both of these reasons, Meerkat's low-level arbitration software releases the first bus in a failed two-bus arbitration and tries again a short time later. The length of the back-off interval is random to prevent live-lock. This back-off method resembles that used by Ethernet <ref> [65] </ref>, except that it is done in software and the Meerkat bus arbitration circuit grants ownership of a particular bus to one node at a time. In Ethernet, collisions occur when two nodes try to use the wire at the same time.
Reference: [66] <institution> Mitsubishi Electric, </institution> <address> 1050 East Arques Avenue, Sunnyvale, CA 93086. </address> <note> Target Spec: Fast Page Mode 4194304-it Dynamic RAM, </note> <year> 1990. </year>
Reference-contexts: The banks are interleaved, so that every other word is stored in a given bank. One word is transferred to or from memory per clock tick for sequentially accessed memory locations. Each bank of DRAM has its own control, address, and data paths <ref> [66] </ref>. 3.4.1 DRAM Control and Interleaving DRAMs require special control signals and are often connected to specialized chips that generate these signals [83]. However, we were unable to find a controller chip that could give us the performance we wanted.
Reference: [67] <author> Motorola Corporation, </author> <title> 2900 South Diablo Way, Tempe, Arizona 85282. HM88K HYPERmodule 32-bit RISC Processor Mezzanine Module User's Manual, </title> <month> March </month> <year> 1990. </year>
Reference-contexts: The processors and cache are on a Motorola Hypermodule daughter board <ref> [67] </ref>. The Hypermodule connects to the node board via three, 100-pin surface mounted connectors. The 32 MB of DRAM is in the form of eight, 4Mx9 SIMMs that plug into connectors at one end of the node board.
Reference: [68] <institution> National Semiconductor Corporation. DP8390C/NS32490C Network Interface Controller, </institution> <year> 1986. </year>
Reference-contexts: Separate-DMA network interfaces support a memory-to-memory copy operation to copy data between the separate buffer and system memory. Although the processor cannot access the separate buffer memory, it must manage allocation of this memory. The National Semiconductor NS8390 Ethernet controller <ref> [68] </ref> supports both DMA into an isolated memory buffer and DMA between this buffer and system memory. * FIFO. The processor reads and writes a single location that maps to the ends of a pair of FIFOs. The FIFOs buffer data between the processor and the network.
Reference: [69] <institution> IEEE standard for a simple 32-bit backplane bus: </institution> <address> NuBus. New York, NY, USA, </address> <month> August </month> <year> 1988. </year> <month> 129 </month>
Reference-contexts: The mesh has a per-hop latency that can dominate communication time in fine-grain machines, such as the MIT Alewife. It seemed that the simplest structure was a set of horizontal and vertical buses. We first thought about using multiple NuBUS's <ref> [69] </ref>. But like most existing buses, NuBUS is a backplane bus for connecting a small number of processors to a small number of I/O and memory devices. Its peak bandwidth is low (40 MB/sec), and its protocol is more complex than we needed.
Reference: [70] <author> J. M. Ortega and R. G. Voight. </author> <title> Solution of partial differential equations on vector and parallel computers. </title> <journal> SIAM Review, </journal> <volume> 27(149), </volume> <year> 1985. </year>
Reference-contexts: The differences in these ratios derive from Delta's more powerful processors and Meerkat's faster interconnect. 89 5.10 Performance of SOR Iterative parallel algorithms for solving systems of equations represent an important class of applications. We measured the speedup of one such algorithm: Red/Black Successive Over Relaxation <ref> [70] </ref>.
Reference: [71] <institution> Paradise Systems, Inc., </institution> <address> 99 South Hill Drive, Brisbane, CA 94005. Paradis Printer Controller, </address> <month> January </month> <year> 1988. </year>
Reference-contexts: A fourth PAL generates bus grant lines for the two S-Bus slots, the cycle-interrupt initialization FPGA, and various control signals. 41 3.9 Host-Meerkat Communication The Sparcstation host is connected to the host-Meerkat interface via a Centronics parallel port in the Sparc with an attached parallel cable <ref> [71] </ref>. The host-Meerkat interface uses a Xilinx 3090 FPGA to interpret the Centronics signals and to control the Meerkat array. A special device driver in the Sparcstation controls the parallel port in response to commands from mg88, the Meerkat cross-debugger.
Reference: [72] <author> Paul Pierce. </author> <title> The NX/2 operating system. </title> <booktitle> In Proceedings of Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 384-390. </pages> <publisher> ACM Press, </publisher> <year> 1988. </year>
Reference-contexts: We used several low-level benchmarks to measure raw internode communication performance. The benchmarks use a subset of the message-passing functions of NX, the Intel operating system <ref> [72] </ref>, so that we could run them both on Meerkat and on Intel's parallel computers. Measurements on the prototype and simulator showed a high sustained bandwidth on long messages, and a low round-trip latency between node pairs. Simulator measurements also showed high bisection throughput for long messages on 256-node systems.
Reference: [73] <author> Carl Ponder. </author> <type> Personal communication, </type> <month> February </month> <year> 1994. </year>
Reference-contexts: They can also execute short sequences of code, enabling designers to evaluate architectural features and debug microcode. Microarchitecture simulators typically have a slow-down of 20,000 to one <ref> [73] </ref>. While useful, they are too slow for debugging all but the shortest sequences of code. 4.3.2 Macroarchitecture Simulation Macroarchitecture simulators (also called macro simulators or instruction-set-architecture simulators) can execute longer-running programs. They are used for studying cache performance and debugging operating system code in advance of a chip's availability.
Reference: [74] <author> Justin Ratner. </author> <title> Talk given at the University of Washington, </title> <year> 1993. </year>
Reference-contexts: There are several techniques that can be used to achieve this higher rate: * Phase-tolerant signalling, such as that being used in the next generation of 2-D mesh routers from Intel's Scientific Systems Division <ref> [74] </ref>. This kind of signalling helps compensate for clock skew, thus allowing the receivers to know with great precision when they should sample the bus wires. * Carrier Sense (CS), Multiple Access with Collision Detection (MACD), and Collision Enforcement (CE).
Reference: [75] <author> Steven K. Reinhardt, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, and David A. Wood. </author> <title> The Wisconsin Wind Tunnel: Virtual prototyping of parallel computers. </title> <journal> Performance Evaluation Review, </journal> <volume> 21(1) </volume> <pages> 48-60, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: They are used by processor architects to evaluate uniprocessor design tradeoffs [26], operating system authors to debug their code [9] and to evaluate operating system performance [23, 82], parallel 47 system architects to assess the performance of large systems <ref> [18, 75] </ref>, and by end users to execute programs written for one system on a different host system [62, 86, 87]. We used the Meerkat simulator to debug and tune operating system code and to evaluate Meerkat's performance.
Reference: [76] <author> Lee W. Ritchey. </author> <title> Controlled impedance PCB design. </title> <booktitle> Printed Circuit Design, </booktitle> <pages> pages 23-28, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Rather, clock jitter and bus reflections from the taps that each node places on the internode buses will the primary constraints. The latter effect can be helped by keeping stub lengths as short as possible, using a controlled-impedance backplane <ref> [76] </ref>, and using GTL. Any pair of horizontal and vertical buses can be connected through cross-point switches. This requires that all buses be clocked at the rate of the slowest bus. In an optimal design, all of the buses would reach their limit at the same clock frequency.
Reference: [77] <author> Edward Rothberg, Jaswinder Pal Singh, and Anoop Gupta. </author> <title> Working sets, cache sizes, and node granularity issues for large-scale multiprocessors. </title> <booktitle> In Proc. 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 14-25, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The principal application characteristics that determine performance on Meerkat are: (1) the ratio of computation to communication, (2) message size, and (3) load balance. The first characteristic, the comp/comm ratio, is the ratio of the instructions executed to the amount of data sent between nodes <ref> [77] </ref>. For example, an application that executes one million double-precision floating point instructions on each of 256 processors, and which passes two million double words between nodes, has a comp/comm ratio of 128. <p> The next section shows that measured speedup of FFT on Meerkat and Delta are consistent with this analysis and our assumption of a comp/comm ratio of 100. 86 J.P. Singh gives large-message ratios for several commercial multicomputers <ref> [77] </ref>. These are reproduced in Table 5.3, along with the values we measured on Delta and Meerkat. While we have listed our figures with Singh's, we note that his were derived from simple calculations, while ours started with measured bisection bandwidths.
Reference: [78] <author> Youcef Saad and Martin H. Schultz. </author> <title> Topological properties of hypercube. </title> <booktitle> In Proceedings of the Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <volume> vol. 1, </volume> <pages> pages 867-872, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: Non-planar interconnects must use cables to connect the node circuit boards. The wiring density of cables is approximately 10 wires per inch. In addition to their order-of-magnitude advantage in density, printed circuit wires are less expensive and more reliable than cables. Nonplanar interconnect topologies, such as the hypercube <ref> [78] </ref>, cannot be wired using low-cost, high-density printed circuit wires. Instead, nodes of these systems are connected by links that use cables, which have relatively few wires, often just one. To maintain the same data rates, these cables must use much higher clock rates.
Reference: [79] <author> Michael D. Schroeder, Andrew D. Birrell, Michael Burrows, Hal Murray, Roger M. Needham, Thomas L. Rodeheffer, Edwin H. Satterthwaite, and Charles P. Thacker. Autonet: </author> <title> A high-speed, self-configuring local area network using point-to-point links. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 9(8) </volume> <pages> 1318-1335, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: No interrupt is needed, because the processor knows that the transfer is complete when it continues past the DMA-initiating point. The Meerkat-1 interface has this property. * Scatter/gather-DMA. This resembles asynchronous-DMA, except that the controller can move data to and from multiple regions of memory. The DEC AN1 controller <ref> [79] </ref> is a recent example of such a controller. * Limited-DMA. This resembles asynchronous-DMA, except that the processor can access only a small, dedicated region of memory located on the controller. It is typically hundreds of kilobytes in length.
Reference: [80] <author> Charles L. Seitz, Nanett J. Boden, Jakov Seizovic, and Wen-King Su. </author> <title> The design of the Caltech Mosaic C multicomputer. </title> <booktitle> Research on Integrated Systems, </booktitle> <pages> pages 1-22, </pages> <year> 1993. </year> <month> 130 </month>
Reference-contexts: Expensive processors generally require expensive support components: while their numbers will be lower, their cost will increase. Some experimental parallel systems, such as Mosaic <ref> [80] </ref>, the J-Machine [30], and the Connection Machine [46], use large numbers of small custom processors. Maspar and Thinking Machines Corporation offered the MP-1 [16] and the CM-2, respectively, both of which used many small custom processors. These systems can yield high performance on regular problems with high parallelism.
Reference: [81] <author> Charles L. Seitz and Wen-King Su. </author> <title> A family of routing and communication chips based on the Mosaic. </title> <booktitle> Research on Integrated Systems, </booktitle> <pages> pages 320-337, </pages> <year> 1993. </year>
Reference-contexts: Both Meerkat and Delta are limited on long messages by their different abilities to drive their interconnects. Meerkat's internode bandwidth reaches 83 percent of its peak rate of 80 MB/sec. Delta reaches 10 percent of its theoretical rate, which is also 80 MB/sec <ref> [81] </ref>. Delta's ability to drive its interconnect in this test is limited by its network interface and the speed of the node processor. The network interface requires that the processor manipulate each byte sent through the interconnect.
Reference: [82] <author> Margo Selzer, Peter Chen, and John Ousterhout. </author> <title> Disk scheduling revisited. </title> <booktitle> In Proceedings of the Winter 1990 USENIX Conference, </booktitle> <pages> pages 313-324, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: They are used by processor architects to evaluate uniprocessor design tradeoffs [26], operating system authors to debug their code [9] and to evaluate operating system performance <ref> [23, 82] </ref>, parallel 47 system architects to assess the performance of large systems [18, 75], and by end users to execute programs written for one system on a different host system [62, 86, 87].
Reference: [83] <institution> Signetics Corp, 811 E. </institution> <address> Arques Avenue, Sunnyvale, CA 94088-3409. </address> <booktitle> Signetics FAST Logic Data Handbook, </booktitle> <year> 1989. </year>
Reference-contexts: Each bank of DRAM has its own control, address, and data paths [66]. 3.4.1 DRAM Control and Interleaving DRAMs require special control signals and are often connected to specialized chips that generate these signals <ref> [83] </ref>. However, we were unable to find a controller chip that could give us the performance we wanted. Therefore, we designed our own controller using Programmable Array Logic (PALs) [1].
Reference: [84] <author> Pradeep Sindhu, Jean-Marc Frailong, Jean Gastinel, Michel Cekleov, Leo Yuan, Bill Gunning, and Don Curry. XDBus: </author> <title> a high-performance, consistent, packet-switched VLSI bus. </title> <journal> IEEE, </journal> <pages> pages 338-344, </pages> <year> 1993. </year>
Reference-contexts: This technique requires a fraction of the signalling energy of older methods, such as ECL, BTL and CMOS. GTL transceivers can be fabricated with conventional CMOS transistors. GTL is used in the new XDBus from SUN Microsystems and Xerox Corp <ref> [84] </ref>. It allows many parallel bits to be sent by a small number of packages. This, in turn, makes it easier to keep stub lengths short, clock skew short, and settling times due to SSO (Simultaneous Switching Output) short. * An idle-bus-undriven protocol.
Reference: [85] <author> J. P. Singh. </author> <title> Talk given at the University of Washington, </title> <year> 1993. </year>
Reference-contexts: The most common programming model used on these systems is called message passing, because parallel programs written for these machines explicitly send and receive messages. Multiprocessors are generally more difficult to design and build than multicomputers. However, many researchers believe that multiprocessors are easier to program <ref> [85, 93, 52] </ref>. This has led a number of researchers to study how multiprocessors with hundreds of processors should be designed [58, 2, 12, 8].
Reference: [86] <author> Richard L. Sites, Anton Chernoff, Mathew B. Kerk, Maurice P. Marks, and Scott G. Robinson. </author> <title> Binary translation. </title> <journal> Communications of the ACM, </journal> <volume> 36(2) </volume> <pages> 69-81, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: evaluate uniprocessor design tradeoffs [26], operating system authors to debug their code [9] and to evaluate operating system performance [23, 82], parallel 47 system architects to assess the performance of large systems [18, 75], and by end users to execute programs written for one system on a different host system <ref> [62, 86, 87] </ref>. We used the Meerkat simulator to debug and tune operating system code and to evaluate Meerkat's performance. Simulators also vary in their performance and the level of detail they can model. <p> Instead of decoding the operation fields each time an instruction is executed, we can translate the instruction once into a form that is faster to execute. This idea has been used in a variety of simulators for a number of applications <ref> [26, 32, 61, 62, 86] </ref>.
Reference: [87] <institution> Insignia Solutions. SoftPC Product Information, </institution> <year> 1991. </year>
Reference-contexts: evaluate uniprocessor design tradeoffs [26], operating system authors to debug their code [9] and to evaluate operating system performance [23, 82], parallel 47 system architects to assess the performance of large systems [18, 75], and by end users to execute programs written for one system on a different host system <ref> [62, 86, 87] </ref>. We used the Meerkat simulator to debug and tune operating system code and to evaluate Meerkat's performance. Simulators also vary in their performance and the level of detail they can model.
Reference: [88] <author> Richard M. Stallman and Roland H. Pesch. </author> <title> Using GDB: The GNU Source-Level Debugger. Free Software Foundation, </title> <type> 545 Tech Square, </type> <address> Cambridge, Ma. 02139, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: The result is a simulator that has a slow-down of about 100 per simulated processor. Its 50 timing is close enough to the prototype's that we can use it to run large programs and make meaningful measurements. 4.4 Structure of the Meerkat Simulator through a symbolic debugger, called gdb <ref> [88] </ref>. The simulator consists of an instruction translator, a threaded-code interpreter [11], cache models, a TLB model, a physical memory system model, and I/O models. Meerkat programs are compiled, assembled, and linked with a set of GNU cross-development tools on a Sparcstation host.
Reference: [89] <author> V. S. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: Networks of workstations connected by Ethernet [65] are sometimes used as parallel computers. Ethernet allows workstations to communicate, albeit with low bandwidth and high latency. System software such as PVM <ref> [89] </ref>, Munin [21], Emerald [47], and Amber [22] provides application support on such parallel computers. Some applications require neither high bandwidth nor low latency, and these are well served by networks of workstations. On a global scale, the Internet can be considered a parallel computer.
Reference: [90] <institution> Tektronix, Inc., P.O. Box 1000, Wilsonville, Oregon 97070. XD88/12 Engineering Requirements Specification, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: Two of the wide boxes in the middle are Field Programmable Gate Arrays (FPGAs) [95]. The FPGAs: * Control internode arbitration and signalling. 1 Our node design started as the modified core of a Tektronix workstation <ref> [90] </ref>.
Reference: [91] <author> Chandramohan A. Thekkath and Henry M. Levy. </author> <title> Limits to low-latency communication on high-speed networks. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(2), </volume> <month> May </month> <year> 1993. </year>
Reference-contexts: Third, the operating system is written for the general case: its general code is slower than the tailored code in user-space programs. However, Thekkath and Levy propose injecting tailored code into the operating system to mitigate the slowness <ref> [91] </ref>. This is not always easy to do, and it lessens only one of the costs of non-user-safe network interfaces. Examples of systems with network interfaces that are not user-safe include workstations, Intel parallel systems, and Meerkat-1. The CM-5 and Meerkat-2 have user-safe interfaces.
Reference: [92] <institution> Thinking Machines Corp., </institution> <address> 245 First St., Cambridge MA 02142. </address> <note> CM-5 Technical Summary. 131 </note>
Reference-contexts: Well-funded laboratories are willing to pay these high costs for fast systems that can solve numerical problems requiring either frequent interprocessor communication and/or the fastest single-stream performance. Between these extremes of both cost and communication performance are systems such as Intel's Paragon [51], Thinking Machine's CM-5 <ref> [92] </ref>, and Cray's T3D. These systems use conventional microprocessors, which lowers their design cost and reduces their physical size (compared to systems, such as the older Crays, whose processors are composed of many chips). However, these multicomputers are still expensive compared to networks of workstations for two primary reasons. <p> There is 12 no lack of demand for the greater computational power of larger systems. Rather, size is limited by economics. Nodes of commercial multis, such as the Thinking Machine's CM-5 <ref> [92] </ref> and the Intel Paragon [51], use powerful microprocessors and large memories, and therefore cost thousands of dollars. A 256-node system, for example, would cost millions of dollars.
Reference: [93] <author> Zvonko G. Vranesic, Michael Stumm, David M. Lewis, and Ron White. Hector: </author> <title> a hierachically structured shared memory multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 24(1) </volume> <pages> 72-79, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: The most common programming model used on these systems is called message passing, because parallel programs written for these machines explicitly send and receive messages. Multiprocessors are generally more difficult to design and build than multicomputers. However, many researchers believe that multiprocessors are easier to program <ref> [85, 93, 52] </ref>. This has led a number of researchers to study how multiprocessors with hundreds of processors should be designed [58, 2, 12, 8].
Reference: [94] <author> Jr. William R. </author> <title> Blood. MECL System Design Handbook. </title> <address> 2900 South Diablo Way, Tempe, Arizona 85282, </address> <year> 1983. </year>
Reference-contexts: This section discusses these relationships. Characteristic impedance is: Z 0 = L 0 where C d is the distributed tap capacitance per unit length, C 0 is the bus wire capacitance per unit length, and L 0 is the bus wire inductance per unit length <ref> [94] </ref>. A bus with no taps corresponds to C d = 0. Propagation speed is: T 0 q and is independent of impedance. Note that both propagation speed and impedance are functions of the per-unit inductance and capacitance. The distributed gate capacitance lowers propagation speed.
Reference: [95] <author> Xilinx. </author> <title> The Programmable Gate Array Data Book, </title> <year> 1993. </year>
Reference-contexts: It also generates the clock signals that are distributed to the nodes. rows of boxes represent the memory system and the internode data path. Two of the wide boxes in the middle are Field Programmable Gate Arrays (FPGAs) <ref> [95] </ref>. The FPGAs: * Control internode arbitration and signalling. 1 Our node design started as the modified core of a Tektronix workstation [90].
Reference: [96] <author> E.L. Zapata, J.A. Lamas, F.F. Rivera, and G. Plata. </author> <title> Modified Gram-Schmidt QR factorization on hypercube SIMD computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12 </volume> <pages> 60-69, </pages> <year> 1991. </year>
Reference-contexts: Some algorithms, such as Cholesky factorization, use messages with sizes that are also a function of the input data. Other algorithms, e.g., modified Gram-Schmidt with partial pivoting <ref> [96] </ref>, have a comp/comm ratio that is a function of the input data. Estimating the ratio for these applications may be difficult. Applications that have very high comp/comm ratios, e.g., over a million to one, will perform well on almost any parallel computer.
References-found: 96

