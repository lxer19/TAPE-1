URL: http://www.tue.nl/tm/vakgr/it/mwerkers/twe/ijcai97.ps
Refering-URL: http://www.tue.nl/tm/vakgr/it/mwerkers/twe/publ_ton.htm
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Avoiding Overfitting with BP-SOM  
Author: Ton Weijters, H. Jaap van den Herik, Antal van den Bosch, and Eric Postma 
Address: PO Box 616, NL-6200 MD Maastricht, The Netherlands  
Affiliation: Universiteit Maastricht  
Abstract: Overfitting is a well-known problem in the fields of symbolic and connectionist machine learning. It describes the deterioration of gen-eralisation performance of a trained model. In this paper, we investigate the ability of a novel artificial neural network, bp-som, to avoid overfitting. bp-som is a hybrid neural network which combines a multi-layered feed-forward network (mfn) with Kohonen's self-organising maps (soms). During training, supervised back-propagation learning and unsupervised som learning cooperate in finding adequate hidden-layer representations. We show that bp-som outperforms standard backpropagation, and also back-propagation with a weight decay when dealing with the problem of overfitting. In addition, we show that bp-som succeeds in preserving generalisation performance under hidden-unit pruning, where both other methods fail.
Abstract-found: 1
Intro-found: 1
Reference: [ Cun et al., 1990 ] <author> Y. Le Cun, J. Denker, and S. Solla. </author> <title> Optimal brain damage. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 598-605. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Ma-teo, CA, </address> <year> 1990. </year>
Reference: [ Fahlman and Lebiere, 1990 ] <author> S. E. Fahlman and C. Lebiere. </author> <title> The cascade-correlation learning architecture. </title> <type> Technical Report CMU-CS-90-100, </type> <institution> School of Computer Science, Carnegie-Mellon University, </institution> <address> Pitts-burgh, PA, </address> <year> 1990. </year>
Reference: [ Hassibi et al., 1992 ] <author> B. Hassibi, D. G. Stork, and G. J. Wolff. </author> <title> Optimal brain surgeon and general network pruning. </title> <type> Technical Report CRC-TR-9235, </type> <institution> RICOH California Research Centre, </institution> <year> 1992. </year>
Reference: [ Hinton, 1986 ] <author> G. E. Hinton. </author> <title> Learning distributed representation of concepts. </title> <booktitle> In Proceedings of the Eighth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 1-12, </pages> <address> Hillsdale, NJ, 1986. </address> <publisher> Erlbaum. </publisher>
Reference: [ Jordan and Bishop, 1996 ] <author> M. I. Jordan and C. M. Bishop. </author> <title> Neural networks. </title> <type> Technical Report A.I. Memo No. 1562, </type> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1996. </year>
Reference: [ Kohonen, 1984 ] <author> T. Kohonen. </author> <title> Self-organisation and Associative Memory, </title> <booktitle> volume 8 of Series in Information Sciences. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1984. </year>
Reference: [ Mozer and Smolensky, 1989 ] <author> M. C. Mozer and P. Smolensky. </author> <title> Using relevance to reduce network size automatically. </title> <journal> Connection Science, </journal> <volume> 1 </volume> <pages> 3-16, </pages> <year> 1989. </year>
Reference: [ Norris, 1989 ] <author> D. Norris. </author> <title> How to build a connectionist idiot (savant). </title> <journal> Cognition, </journal> <volume> 35 </volume> <pages> 277-291, </pages> <year> 1989. </year>
Reference: [ Prechelt, 1994 ] <author> L. Prechelt. Proben1: </author> <title> A set of neural network benchmark problems and benchmarking rules. </title> <type> Technical Report 19/94, </type> <institution> Fakultat fur Infor-matik, Universitat Karlsruhe, Germany, </institution> <year> 1994. </year>
Reference: [ Quinlan, 1993 ] <author> J. R. Quinlan. c4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference: [ Rissanen, 1983 ] <author> J. Rissanen. </author> <title> A universal prior for integers and estimation by minimum description length. </title> <journal> Annals of Statistics, </journal> <volume> 11 </volume> <pages> 416-431, </pages> <year> 1983. </year>
Reference: [ Rumelhart et al., 1986 ] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart and J. L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, volume 1: Foundations, </booktitle> <pages> pages 318-362. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference: [ Shaffer, 1993 ] <author> C. Shaffer. </author> <title> Overfitting avoidance as bias. </title> <journal> Machine Learning, </journal> <volume> 10 </volume> <pages> 153-178, </pages> <year> 1993. </year>
Reference: [ Thornton, 1995 ] <author> C. Thornton. </author> <title> Measuring the difficulty of specific learning problems. </title> <journal> Connection Science, </journal> <volume> 7 </volume> <pages> 81-92, </pages> <year> 1995. </year>
Reference: [ Weigend et al., 1990 ] <author> A. S. Weigend, D. E. Rumelhart, and B. A. Huberman. </author> <title> Generalization by weight-elimination with application to forecasting. </title> <editor> In R. P. Lippman, J. E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 3, </volume> <pages> pages 875-882. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Ma-teo, CA, </address> <year> 1990. </year>
Reference: [ Weigend, 1994 ] <author> A. S. Weigend. </author> <title> On overfitting and the effective number of hidden units. </title> <editor> In M. C. Mozer, P. Smolensky, D. S. Touretzky, J. L. Elman, and A. S. Weigend, editors, </editor> <booktitle> Proceedings of the 1993 Connectionist Models Summer School, </booktitle> <pages> pages 335-342, </pages> <year> 1994. </year>
Reference: [ Weijters, 1995 ] <author> A. Weijters. </author> <title> The BP-SOM architecture and learning rule. </title> <journal> Neural Processing Letters, </journal> <volume> 2 </volume> <pages> 13-16, </pages> <year> 1995. </year>
Reference: [ Wolpert, 1992 ] <author> D. H. Wolpert. </author> <title> On overfitting avoidance as bias. </title> <type> Technical Report SFI TR 92-03-5001, </type> <institution> The Santa Fe Institute, </institution> <year> 1992. </year>
References-found: 18

