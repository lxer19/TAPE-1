URL: http://www.isi.edu/isd/rickel/animagents97.ps
Refering-URL: http://www.isi.edu/isd/rickel/publications.html
Root-URL: http://www.isi.edu
Email: rickel, johnson@isi.edu  
Title: Steve: An Animated Pedagogical Agent for Procedural Training in Virtual Environments (Extended Abstract)  
Author: Jeff Rickel and W. Lewis Johnson 
Web: http://www.isi.edu/isd/VET/vet.html  
Address: 4676 Admiralty Way, Marina del Rey, CA 90292-6695  
Affiliation: Information Sciences Institute Computer Science Department University of Southern California  
Abstract: Virtual reality can broaden the types of interaction between students and computer tutors. As in conventional simulation-based training, the computer can watch students practice tasks, responding to questions and offering advice. However, immersive virtual environments also allow the computer tutor to inhabit the virtual world with the student. Unlike previous, disembodied computer tutors, such a "pedagogical agent" can "physically" collaborate with students, enabling new types of interaction. To illustrate the possibilities, this paper describes Steve, a pedagogical agent for virtual environments that helps students learn procedural tasks. After providing an overview of Steve's capabilities, the paper focuses on the benefits and challenges of graphically representing Steve in the virtual environment.
Abstract-found: 1
Intro-found: 1
Reference: [ Badler et al., 1993 ] <author> Norman I. Badler, Cary B. Phillips, and Bonnie Lynn Webber. </author> <title> Simulating Humans. </title> <publisher> Oxford University Press, </publisher> <address> New York, </address> <year> 1993. </year>
Reference-contexts: Figure 5 shows the latter two together: one Steve agent, represented by a human figure, watches (via dynamic gaze control) as another Steve agent, represented by a hand, demonstrates a task. To implement the full human figure, we use the Jack soft ware <ref> [ Badler et al., 1993 ] </ref> developed at the University of Pennsylvania. Jack can be used two different ways. The Jack software would allow course authors to create a variety of animation sequences, and these could be dynamically strung together by Steve during task execution.
Reference: [ Cassell et al., 1994 ] <author> Justine Cassell, Cather-ine Pelachaud, Norman Badler, Mark Steedman, Brett Achorn, Tripp Becket, Brett Douville, Scott Prevost, and Matthew Stone. </author> <title> Animated conversation: Rule-based generation of facial expression, gesture and spoken intonation for multiple conversational agents. </title> <booktitle> In Proceedings of ACM SIGGRAPH '94, </booktitle> <year> 1994. </year>
Reference: [ Collins et al., 1989 ] <author> Allan Collins, John Seely Brown, and Susan E. Newman. </author> <title> Cognitive apprenticeship: Teaching the crafts of reading, writing, and mathematics. In L.B. Resnick, editor, Knowing, Learning, and Instruction: </title> <booktitle> Essays in Honor of Robert Glaser. </booktitle> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1989. </year>
Reference-contexts: Figure 1 shows a snapshot of the virtual HPAC and its surrounding room. Steve teaches students how to perform procedural tasks, like operating the HPAC. Our goal is to support the apprenticeship model of learning <ref> [ Collins et al., 1989 ] </ref> . This requires two capabilities: Steve must be able to demonstrate and explain tasks, and he must be able to monitor students performing tasks, providing assistance when it is needed. All of Steve's instruction and assistance is situated in the performance of domain tasks.
Reference: [ Durlach and Mavor, 1995 ] <author> Nathaniel I. Durlach and Anne S. Mavor, </author> <title> editors. Virtual Reality: Scientific and Technological Challenges. </title> <publisher> National Academy Press, </publisher> <address> Washington, D.C., </address> <year> 1995. </year>
Reference-contexts: However, virtual reality can provide more realistic perceptual stimuli (e.g., visual, auditory, and haptic) than earlier technologies, thereby providing an adequate simulation for a wider range of situations <ref> [ Durlach and Mavor, 1995 ] </ref> . In addition, using networked virtual reality systems, multiple students (possibly at different work sites) can learn to perform collaborative or competitive tasks together. Virtual reality also offers exciting opportunities and challenges for intelligent tutoring systems.
Reference: [ Geib et al., 1994 ] <author> Christopher Geib, Libby Levison, and Michael B. Moore. Sodajack: </author> <title> An architecture for agents that search and manipulate objects. </title> <type> Technical Report MS-CIS-94-16/LINC LAB 265, </type> <institution> Department of Computer and Information Science, University of Pennsylvania, </institution> <year> 1994. </year>
Reference-contexts: Jack makes it awkward to use another program to control him. (Jack was primarily designed for interactive control by a person via a graphical user interface.) However, work at the University of Pennsylvania has shown the potential for using Jack to autonomously move around a virtual world and manipulate objects <ref> [ Geib et al., 1994; Levison and Badler, 1994 ] </ref> , and we believe this approach holds more promise than handcrafted animation sequences. There are many useful types of nonverbal feedback that a pedagogical agent could give to a student beyond those currently used by Steve.
Reference: [ Johnson et al., ] <author> W. Lewis Johnson, Jeff Rickel, Randy Stiles, and Allen Munro. </author> <title> Integrating pedagogical agents into virtual environments. Presence. </title> <publisher> Forthcoming. </publisher>
Reference-contexts: Using this information, Steve uses a method analogous to partial order planning [ Weld, 1994 ] to keep track of which steps of the task are still relevant to completing it <ref> [ Rickel and Johnson, 1997; Johnson et al., ] </ref> . <p> The cognitive module, implemented in Soar [ Laird et al., 1987; Newell, 1990 ] handles the normal duties of an intelligent tutoring system, and it outputs motor commands (e.g., press a button or look at an object) to a sensorimotor module <ref> [ Rickel and Johnson, 1997; Johnson et al., ] </ref> . The sensorimotor module translates these motor commands into lower-level graphical commands to move Steve's body and, if necessary, commands to the simulator to affect the virtual world (e.g., simulate the button and its effects).
Reference: [ Johnson, 1994 ] <author> W. Lewis Johnson. </author> <title> Agents that learn to explain themselves. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94), </booktitle> <pages> pages 1257-1263, </pages> <address> Menlo Park, CA, 1994. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Thus, unlike videos or scripted demonstrations, Steve can adapt domain procedures to the state of the virtual world. room. When demonstrating a task, Steve maintains an episodic memory of situations in which he performs actions, using Johnson's Debrief software <ref> [ Johnson, 1994 ] </ref> . After the demonstration, the student can ask Steve to rationalize any of his actions. Steve recalls the situation and explains his action in terms of its relevant effect.
Reference: [ Laird et al., 1987 ] <author> John E. Laird, Allen Newell, and Paul S. Rosenbloom. </author> <title> Soar: An architecture for general intelligence. </title> <journal> Artificial Intelligence, </journal> <volume> 33(1) </volume> <pages> 1-64, </pages> <year> 1987. </year>
Reference-contexts: Therefore, we wanted an architecture for Steve that would allow us to evaluate the tradeoffs between different representations. To experiment with different graphical representations for Steve, we designed him as two cooperating modules. The cognitive module, implemented in Soar <ref> [ Laird et al., 1987; Newell, 1990 ] </ref> handles the normal duties of an intelligent tutoring system, and it outputs motor commands (e.g., press a button or look at an object) to a sensorimotor module [ Rickel and Johnson, 1997; Johnson et al., ] .
Reference: [ Levison and Badler, 1994 ] <author> Libby Levison and Nor-man I. Badler. </author> <title> How animated agents perform tasks: Connecting planning and manipulation through object-specific reasoning. In Toward Physical Interaction and Manipulation, </title> <booktitle> AAAI Spring Symposium Series, </booktitle> <year> 1994. </year>
Reference-contexts: Jack makes it awkward to use another program to control him. (Jack was primarily designed for interactive control by a person via a graphical user interface.) However, work at the University of Pennsylvania has shown the potential for using Jack to autonomously move around a virtual world and manipulate objects <ref> [ Geib et al., 1994; Levison and Badler, 1994 ] </ref> , and we believe this approach holds more promise than handcrafted animation sequences. There are many useful types of nonverbal feedback that a pedagogical agent could give to a student beyond those currently used by Steve.
Reference: [ Maybury, 1993 ] <author> Mark T. </author> <title> Maybury, editor. Intelligent Multimedia Interfaces. </title> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1993. </year>
Reference-contexts: By controlling their own field of view, students in immersive virtual environments learn to navigate around their work environment, and they can view objects from different angles. In contrast, most tutoring systems, and even multimedia presentation systems <ref> [ Maybury, 1993 ] </ref> , assume they can design and control the student's view. The Vista software allows Steve to control the student's field of view when necessary. However, to avoid losing the benefits of having students control their own view, we have ignored that option.
Reference: [ McAllester and Rosenblitt, 1991 ] <author> David McAllester and David Rosenblitt. </author> <title> Systematic nonlinear planning. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence (AAAI-91), </booktitle> <pages> pages 634-639, </pages> <address> Menlo Park, CA, 1991. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Steve's demonstrations are not canned. Steve is given knowledge of tasks in the form of hierarchical plans. These plans include a set of steps (each either a primitive action or a subtask), a set of ordering constraints, and a set of causal links. The causal links <ref> [ McAllester and Rosenblitt, 1991 ] </ref> tell Steve the role of each step in the task; each causal link states that one step (e.g., pulling out a dipstick) achieves a goal (e.g., dipstick out) that is either an end goal or a precondition for another step (e.g., checking the oil level).
Reference: [ Munro et al., 1993 ] <author> A. Munro, M.C. Johnson, D.S. Sur-mon, and J.L. Wogulis. </author> <title> Attribute-centered simulation authoring for instruction. </title> <booktitle> In Proceedings of the World Conference on Artificial Intelligence in Education (AI-ED '93), </booktitle> <pages> pages 82-89. </pages> <booktitle> Association for the Advancement of Computing in Education, </booktitle> <year> 1993. </year>
Reference-contexts: Students interact with the virtual world using a 3D mouse or data gloves. Sensors on the mouse and gloves keep track of the student's hands, and the Vista software sends out messages when the student touches virtual objects. These messages are received and handled by the RIDES software <ref> [ Munro et al., 1993 ] </ref> , which controls the behavior of the virtual world. (The RIDES software makes it easy for course authors to create simulation behaviors.) Currently, we are applying the VET system to training Navy personnel to operate a high-pressure air compressor (HPAC) on board a ship.
Reference: [ Newell, 1990 ] <author> Allen Newell. </author> <title> Unified Theories of Cognition. </title> <publisher> Harvard University Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: Therefore, we wanted an architecture for Steve that would allow us to evaluate the tradeoffs between different representations. To experiment with different graphical representations for Steve, we designed him as two cooperating modules. The cognitive module, implemented in Soar <ref> [ Laird et al., 1987; Newell, 1990 ] </ref> handles the normal duties of an intelligent tutoring system, and it outputs motor commands (e.g., press a button or look at an object) to a sensorimotor module [ Rickel and Johnson, 1997; Johnson et al., ] .
Reference: [ Perlin and Goldberg, 1996 ] <author> Ken Perlin and Athomas Goldberg. Improv: </author> <title> A system for scripting interactive actors in virtual worlds. </title> <journal> Computer Graphics, </journal> <volume> 29(3), </volume> <year> 1996. </year>
Reference-contexts: Some Steve agents will play the role of missing team members, while others will assist students learning their role. Second, in addition to our use of the Jack software, we plan to experiment with other human figures, such as the Otto character developed at New York University <ref> [ Perlin and Goldberg, 1996 ] </ref> . Third, in collaboration with Clark Elliott of DePaul University, we are developing an emotional subsystem for Steve, to test the benefits of a more personal relationship with students.
Reference: [ Rickel and Johnson, 1997 ] <author> Jeff Rickel and W. Lewis Johnson. </author> <title> Integrating pedagogical capabilities in a virtual environment agent. </title> <booktitle> In Proceedings of the First International Conference on Autonomous Agents, </booktitle> <month> February </month> <year> 1997. </year>
Reference-contexts: Using this information, Steve uses a method analogous to partial order planning [ Weld, 1994 ] to keep track of which steps of the task are still relevant to completing it <ref> [ Rickel and Johnson, 1997; Johnson et al., ] </ref> . <p> The cognitive module, implemented in Soar [ Laird et al., 1987; Newell, 1990 ] handles the normal duties of an intelligent tutoring system, and it outputs motor commands (e.g., press a button or look at an object) to a sensorimotor module <ref> [ Rickel and Johnson, 1997; Johnson et al., ] </ref> . The sensorimotor module translates these motor commands into lower-level graphical commands to move Steve's body and, if necessary, commands to the simulator to affect the virtual world (e.g., simulate the button and its effects).
Reference: [ Stiles et al., 1995 ] <author> Randy Stiles, Laurie McCarthy, and Michael Pontecorvo. </author> <title> Training studio: A virtual environment for training. </title> <booktitle> In Workshop on Simulation and Interaction in Virtual Environments (SIVE-95), </booktitle> <address> Iowa City, IW, July 1995. </address> <publisher> ACM Press. </publisher>
Reference-contexts: Each student's interface to the virtual world is provided by special-purpose hardware and Lockheed Martin's Vista Viewer software <ref> [ Stiles et al., 1995 ] </ref> . Students get a 3D, immersive view of the world through a head-mounted display (HMD). Vista uses data from a position and orientation sensor on the HMD to update the student's view as they move around.
Reference: [ Stone and Lester, 1996 ] <author> Brian A. Stone and James C. Lester. </author> <title> Dynamically sequencing an animated pedagogical agent. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96), </booktitle> <pages> pages 424-431, </pages> <address> Menlo Park, CA, 1996. </address> <publisher> AAAI Press/MIT Press. </publisher>
Reference: [ Weld, 1994 ] <author> Daniel S. Weld. </author> <title> An introduction to least commitment planning. </title> <journal> AI Magazine, </journal> <volume> 15(4) </volume> <pages> 27-61, </pages> <year> 1994. </year>
Reference-contexts: When demonstrating tasks, Steve continually monitors the state of the world, keeping track of whether task goals are satisfied (both end goals and intermediate goals). Using this information, Steve uses a method analogous to partial order planning <ref> [ Weld, 1994 ] </ref> to keep track of which steps of the task are still relevant to completing it [ Rickel and Johnson, 1997; Johnson et al., ] .
References-found: 18

