URL: ftp://hpsl.cs.umd.edu/pub/papers/scalib.ps.Z
Refering-URL: http://www.cs.umd.edu/projects/hpsl/papers.brandnew/LocalResources/tech-10-23.htm
Root-URL: 
Email: alsg@cs.umd.edu  
Title: Applying the CHAOS/PARTI Library to Irregular Problems in Computational Chemistry and Computational Aerodynamics  
Author: Raja Das, Yuan-Shin Hwang, Mustafa Uysal, Joel Saltz, Alan Sussman fraja, shin, uysal, saltz, 
Address: College Park, MD 20742  
Affiliation: Department of Computer Science University of Maryland  
Abstract: This paper describes a number of optimizations that can be used to support the efficient execution of irregular problems on distributed memory parallel machines. We describe software primitives that (1) coordinate in-terprocessor data movement, (2) manage the storage of, and access to, copies of off-processor data, (3) minimize interprocessor communication requirements and (4) support a shared name space. The performance of the primitives is characterized by examination of kernels from real applications and from a full implementation of a large unstructured adaptive application (the molecular dynamics code CHARMM). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Harry Berryman, Joel Saltz, and Jeffrey Scroggs. </author> <title> Execution time support for adaptive scientific algorithms on distributed memory machines. </title> <journal> Con-currency: Practice and Experience, </journal> <volume> 3(3) </volume> <pages> 159-178, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: this paper we give a detailed description of the communication optimizations that prove to be useful for optimizing irregular problem performance but do not further address compilation issues. 1.1 Overview of PARTI In this section, we give an overview of the functionality of the PARTI primitives described in previous publications <ref> [1, 15, 16] </ref>. In many algorithms data produced or input during program initialization plays a large role in determining the nature of the subsequent computation. In the PARTI approach, when the data structures that define a computation have been initialized, a preprocessing phase follows. <p> We thus build a translation table which, for each array element, lists the host processor address. The problems we are dealing with all consist of a sequence of clearly demarcated concurrent computational phases where data access patterns cannot be anticipated until runtime. They are called static irregular concurrent computations <ref> [1] </ref>. In these problems, once runtime information is available, 1) data access patterns are known before each computational phase and 2) the same data access patterns occur many times. Adaptive problems can fall into this class of problems as long as data access patterns change relatively infrequently. <p> The executor loop then uses the information from the inspector to implement the actual computation. We have developed a suite of primitives that can be used directly by programmers to generate inspector/executor pairs. These primitives are named PARTI <ref> [1, 5] </ref>; they carry out the distribution and retrieval of globally indexed but irregularly distributed data-sets over the numerous local processor memories.
Reference: [2] <author> Shahid Bokhari. </author> <title> Communication overhead on the Intel iPSC-860 hypercube. </title> <type> Technical Report ICASE Interim Report 10, </type> <institution> ICASE, NASA Lang-ley Research Center, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: This kind of optimization is sometimes called commu-nication coalescing. The object of communication coalescing is to reduce the number of message startups. For many distributed memory systems there is a substantial latency associated with message passing. For instance, Bokhari <ref> [2] </ref> measured the time to communicate a message of size k (bytes) between two nodes of an Intel iPSC/860 as: T = 65.0 + 0.425k + 10.0h, for 0 &lt; k 100, and where T is the time in secs and h is the number of hops between the communicating processors.
Reference: [3] <author> B. R. Brooks, R. E. Bruccoleri, B. D. Olafson, D. J. States, S. Swaminathan, and M. Karplus. Charmm: </author> <title> A program for macromolecular energy, minimization, and dynamics calculations. </title> <journal> Journal of Computational Chemistry, </journal> <volume> 4:187, </volume> <year> 1983. </year>
Reference-contexts: In Section 2.1 we describe a representative kernel from the explicit Euler solver [5, 12] and Section 2.2 briefly describes the kernel from the molecular dynamics code CHARMM <ref> [3, 7] </ref>. 2.1 Unstructured Euler Kernel Unstructured meshes provide a great deal of flexibility in discretizing complex domains and offer the possibility of easily performing adaptive meshing. However, unstructured meshes result in random datasets and large sparse matrices that require runtime preprocessing to be executed on a distributed memory machine.
Reference: [4] <author> B. R. Brooks and M. Hodoscek. </author> <title> Parallelization of charmm for mimd machines. Chemical Design Automation News, </title> <address> 7:16, </address> <year> 1992. </year>
Reference-contexts: Data Replication Procs Unweighted Weighted Replicated 1 (sec/iter) (sec/iter) (sec/iter) 16 5.38 4.86 4.95 64 2.27 1.35 1.53 1 Results from <ref> [4] </ref> ing application codes. These results are comparable to all other implementations of which we are aware [4]. 6 Conclusion This paper has described and systematically evaluated a number of new communication optimizations aimed at irregular problems. These optimizations reduce communication latency and volume. <p> Data Replication Procs Unweighted Weighted Replicated 1 (sec/iter) (sec/iter) (sec/iter) 16 5.38 4.86 4.95 64 2.27 1.35 1.53 1 Results from <ref> [4] </ref> ing application codes. These results are comparable to all other implementations of which we are aware [4]. 6 Conclusion This paper has described and systematically evaluated a number of new communication optimizations aimed at irregular problems. These optimizations reduce communication latency and volume.
Reference: [5] <author> R. Das, D. J. Mavriplis, J. Saltz, S. Gupta, and R. Ponnusamy. </author> <title> The design and implementation of a parallel unstructured Euler solver using software primitives, </title> <booktitle> AIAA-92-0562. In Proceedings of the 30th Aerospace Sciences Meeting, </booktitle> <month> January </month> <year> 1992. </year>
Reference-contexts: The executor loop then uses the information from the inspector to implement the actual computation. We have developed a suite of primitives that can be used directly by programmers to generate inspector/executor pairs. These primitives are named PARTI <ref> [1, 5] </ref>; they carry out the distribution and retrieval of globally indexed but irregularly distributed data-sets over the numerous local processor memories. <p> In this section we briefly describe two kernels that have been extracted from these codes and how they stress the primitives quite differently. In Section 2.1 we describe a representative kernel from the explicit Euler solver <ref> [5, 12] </ref> and Section 2.2 briefly describes the kernel from the molecular dynamics code CHARMM [3, 7]. 2.1 Unstructured Euler Kernel Unstructured meshes provide a great deal of flexibility in discretizing complex domains and offer the possibility of easily performing adaptive meshing.
Reference: [6] <author> R. Das, R. Ponnusamy, J. Saltz, and D. Mavriplis. </author> <title> Distributed memory compiler methods for irregular problems data copy reuse and runtime partitioning. In Compilers and Runtime Software for Scalable Multiprocessors, </title> <editor> J. Saltz and P. Mehro-tra Editors, </editor> <address> Amsterdam, The Netherlands, 1992. </address> <publisher> Elsevier. </publisher>
Reference-contexts: The gather on each processor fetches all the necessary y references that reside off-processor. The scatter add call accumulates the off-processor x values. A detailed description of the functionality of these primitives is given in <ref> [6] </ref>. 2 Real Problem Kernels We have ported a number of scientific codes to parallel machines using the PARTI primitives. In this section we briefly describe two kernels that have been extracted from these codes and how they stress the primitives quite differently.
Reference: [7] <author> R. Das and J. Saltz. </author> <title> Parallelizing molecular dynamics codes using the Parti software. </title> <booktitle> In Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 187-192. </pages> <publisher> SIAM, </publisher> <month> March </month> <year> 1993. </year>
Reference-contexts: In Section 2.1 we describe a representative kernel from the explicit Euler solver [5, 12] and Section 2.2 briefly describes the kernel from the molecular dynamics code CHARMM <ref> [3, 7] </ref>. 2.1 Unstructured Euler Kernel Unstructured meshes provide a great deal of flexibility in discretizing complex domains and offer the possibility of easily performing adaptive meshing. However, unstructured meshes result in random datasets and large sparse matrices that require runtime preprocessing to be executed on a distributed memory machine.
Reference: [8] <author> R. Das, J. Saltz, D. Mavriplis, and R. Ponnusamy. </author> <title> The incremental scheduler. In Unstructured Scientific Computation on Scalable Multiprocessors, </title> <address> Cambridge Mass, 1992. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Memory considerations make it clear that it is not always feasible to place a copy of the translation table on each processor. A translation table can be distributed between processors. Earlier versions of PARTI supported a translation table that was partitioned between processors in a blocked fashion <ref> [8, 16] </ref>. This was accomplished by putting the first N/P elements on the first processor, the second N/P elements of the table on the second processor, etc., where P is the number of processors.
Reference: [9] <author> Raja Das, Joel Saltz, and Reinhard von Hanxle--den. </author> <title> Slicing analysis and indirect access to distributed arrays. </title> <booktitle> In Proceedings of the 6th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: As long as it is known that the values assigned to off-processor memory locations remain unmodified, it is possible to reuse stored off-processor data. A mixture of compile-time and run-time analysis can be used to generate efficient code for irregular problems <ref> [9, 10, 13] </ref>. <p> In either case, the same off-processor data may be accessed multiple times, but only a single copy of that data must be fetched from off-processor. In this paper, we do not address issues associated with identifying when these prefetches can be carried out; See von Hanxleden [10] and Das <ref> [9] </ref> for a detailed discussion.
Reference: [10] <author> R. v. Hanxleden, K. Kennedy, C. Koelbel, R. Das, and J. Saltz. </author> <title> Compiler analysis for irregular problems in Fortran D. </title> <booktitle> In Proceedings of the 5th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: As long as it is known that the values assigned to off-processor memory locations remain unmodified, it is possible to reuse stored off-processor data. A mixture of compile-time and run-time analysis can be used to generate efficient code for irregular problems <ref> [9, 10, 13] </ref>. <p> In either case, the same off-processor data may be accessed multiple times, but only a single copy of that data must be fetched from off-processor. In this paper, we do not address issues associated with identifying when these prefetches can be carried out; See von Hanxleden <ref> [10] </ref> and Das [9] for a detailed discussion.
Reference: [11] <author> S. Hiranandani, J. Saltz, P. Mehrotra, and H. Berryman. </author> <title> Performance of hashed cache data migration schemes on multicomputers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12 </volume> <pages> 415-422, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: The primitives that build schedules use hash tables to generate communication calls that, for each loop nest, transmit only a single copy of each off-processor datum <ref> [11, 16] </ref>. The schedules are used in the executor by PARTI primitives to gather, scatter and accumulate data to/from off-processor memory locations. In this paper, the idea of eliminating duplicates has been taken a step further.
Reference: [12] <author> D. J. Mavriplis. </author> <title> Three dimensional multigrid for the Euler equations. </title> <journal> AIAA paper 91-1549CP, </journal> <pages> pages 824-831, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: In this section we briefly describe two kernels that have been extracted from these codes and how they stress the primitives quite differently. In Section 2.1 we describe a representative kernel from the explicit Euler solver <ref> [5, 12] </ref> and Section 2.2 briefly describes the kernel from the molecular dynamics code CHARMM [3, 7]. 2.1 Unstructured Euler Kernel Unstructured meshes provide a great deal of flexibility in discretizing complex domains and offer the possibility of easily performing adaptive meshing.
Reference: [13] <author> Ravi Ponnusamy, Joel Saltz, and Alok Choud-hary. </author> <title> Runtime-compilation techniques for data partitioning and communication schedule reuse. </title> <institution> Technical Report CS-TR-3055 and UMIACS-TR-93-32, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> April </month> <year> 1993. </year> <note> To appear in proceedings of Supercomputing '93. </note>
Reference-contexts: As long as it is known that the values assigned to off-processor memory locations remain unmodified, it is possible to reuse stored off-processor data. A mixture of compile-time and run-time analysis can be used to generate efficient code for irregular problems <ref> [9, 10, 13] </ref>.
Reference: [14] <author> A. Pothen, H. D. Simon, and K. P. Liou. </author> <title> Partitioning sparse matrices with eigenvectors of graphs. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 11 </volume> <pages> 430-452, </pages> <year> 1990. </year>
Reference-contexts: When the data and the indirection arrays are block partitioned the number of off-processor data items is extremely high, and almost every processor has to communicate with all others. When the data is partitioned based on the connectivity of the mesh (e.g. by spectral bisection <ref> [14] </ref>), we get low volumes of communication between neighboring processors. We have used such geometric partitioners with a fair amount of success. 2.2 Molecular Dynamics This kernel from the molecular dynamics code CHARMM has a very different data access pattern from that of the Euler code kernel.
Reference: [15] <author> J. Saltz, H. Berryman, and J. Wu. </author> <title> Multiprocessors and run-time compilation. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 3(6) </volume> <pages> 573-592, </pages> <year> 1991. </year>
Reference-contexts: this paper we give a detailed description of the communication optimizations that prove to be useful for optimizing irregular problem performance but do not further address compilation issues. 1.1 Overview of PARTI In this section, we give an overview of the functionality of the PARTI primitives described in previous publications <ref> [1, 15, 16] </ref>. In many algorithms data produced or input during program initialization plays a large role in determining the nature of the subsequent computation. In the PARTI approach, when the data structures that define a computation have been initialized, a preprocessing phase follows.
Reference: [16] <author> J. Wu, J. Saltz, S. Hiranandani, and H. Berry-man. </author> <title> Runtime compilation methods for multi-computers. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 26-30, </pages> <year> 1991. </year>
Reference-contexts: this paper we give a detailed description of the communication optimizations that prove to be useful for optimizing irregular problem performance but do not further address compilation issues. 1.1 Overview of PARTI In this section, we give an overview of the functionality of the PARTI primitives described in previous publications <ref> [1, 15, 16] </ref>. In many algorithms data produced or input during program initialization plays a large role in determining the nature of the subsequent computation. In the PARTI approach, when the data structures that define a computation have been initialized, a preprocessing phase follows. <p> The primitives that build schedules use hash tables to generate communication calls that, for each loop nest, transmit only a single copy of each off-processor datum <ref> [11, 16] </ref>. The schedules are used in the executor by PARTI primitives to gather, scatter and accumulate data to/from off-processor memory locations. In this paper, the idea of eliminating duplicates has been taken a step further. <p> Memory considerations make it clear that it is not always feasible to place a copy of the translation table on each processor. A translation table can be distributed between processors. Earlier versions of PARTI supported a translation table that was partitioned between processors in a blocked fashion <ref> [8, 16] </ref>. This was accomplished by putting the first N/P elements on the first processor, the second N/P elements of the table on the second processor, etc., where P is the number of processors.
References-found: 16

