URL: ftp://ftp.idsia.ch/pub/nic/nips95.ps.gz
Refering-URL: http://www.cnl.salk.edu/~schraudo/pubs.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: nici@evotec.de  terry@salk.edu  
Title: Tempering Backpropagation Networks: Not All Weights are Created Equal approach yields hitherto unparalleled performance on
Author: Nicol N. Schraudolph Terrence J. Sejnowski 
Note: This  
Address: Grandweg 64 22529 Hamburg, Germany  San Diego, CA 92186-5800, USA  
Affiliation: EVOTEC BioSystems GmbH  Computational Neurobiology Lab The Salk Institute for Biol. Studies  
Abstract: Backpropagation learning algorithms typically collapse the network's structure into a single vector of weight parameters to be optimized. We suggest that their performance may be improved by utilizing the structural information instead of discarding it, and introduce a framework for tempering each weight accordingly. In the tempering model, activation and error signals are treated as approximately independent random variables. The characteristic scale of weight changes is then matched to that of the residuals, allowing structural properties such as a node's fan-in and fan-out to affect the local learning rate and backpropagated error. The model also permits calculation of an upper bound on the global learning rate for batch updates, which in turn leads to different update rules for bias vs. non-bias weights. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Amari, S.-I. </author> <year> (1995). </year> <title> Learning and statistical inference. </title> <editor> In Arbib, M. A., editor, </editor> <booktitle> The Handbook of Brain Theory and Neural Networks, </booktitle> <pages> pages 522-526. </pages> <publisher> MIT Press, </publisher> <address> Cambridge. </address>
Reference-contexts: Formally, this approach may be interpreted as a diagonal approximation of the inverse Fischer information matrix <ref> (Amari, 1995) </ref>. We implement (4) by deriving an upper bound for the left-hand side which is then equated with the right-hand side.
Reference: <author> Battiti, T. </author> <year> (1992). </year> <title> First- and second-order methods for learning: Between steepest descent and Newton's method. </title> <journal> Neural Computation, </journal> <volume> 4(2) </volume> <pages> 141-166. </pages>
Reference-contexts: This serves to decouple the learning process from network design and makes a large body of function optimization techniques directly applicable to backpropagation learning. But what if the discarded structural information holds valuable clues for efficient weight optimization? Adaptive step size and second-order gradient techniques <ref> (Battiti, 1992) </ref> may fl Preprint to appear in Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E. (eds.), Advances in Neural Information Processing Systems 8, MIT Press, Cambridge 1996. recover some of it, at considerable computational expense.
Reference: <author> Haykin, S. </author> <year> (1994). </year> <title> Neural Networks: A Comprehensive Foundation. </title> <publisher> Macmillan, </publisher> <address> New York. </address>
Reference: <author> Hinton, G. </author> <year> (1986). </year> <title> Learning distributed representations of concepts. </title> <booktitle> In Proceedings of the Eighth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 1-12, </pages> <address> Amherst 1986. </address> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale. </address>
Reference-contexts: We expect online implementations to perform best when both input and error signals are centered so as to improve the stochastic approximation. 4 eff :25 :05 6 Experimental Setup We tested these ideas on the family relations task <ref> (Hinton, 1986) </ref>: a backpropagation network is given examples of a family member and relationship as input, and must indicate on its output which family members fit the relational description according to an underlying family tree.
Reference: <author> Jacobs, R. </author> <year> (1988). </year> <title> Increased rates of convergence through learning rate adaptation. </title> <booktitle> Neural Networks, </booktitle> <volume> 1 </volume> <pages> 295-307. </pages>
Reference-contexts: We expect tempering to be applicable to a variety of backpropagation learning algorithms; here we present first results for batch learning with momentum and the delta-bar-delta rule <ref> (Jacobs, 1988) </ref>. Both algorithms were tested under three conditions: conventional, tempered (as described in Sections 2 and 3), and tempered with error shunting.
Reference: <author> Krogh, A., Thorbergsson, G., and Hertz, J. A. </author> <year> (1990). </year> <title> A cost function for internal representations. </title> <editor> In Touretzky, D. S., editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 733-740, </pages> <address> Denver, CO, 1989. </address> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address>
Reference-contexts: What about the remainder of the network? Unlike <ref> (Krogh et al., 1990) </ref>, we do not wish to prescribe definite targets (and hence residuals) for hidden nodes. Instead we shall use our bounds and independence arguments to scale backpropagated error signals to roughly appropriate magnitude.
Reference: <author> LeCun, Y. </author> <year> (1993). </year> <title> Efficient learning & second-order methods. Tutorial given at Neural Information Processing Systems 6, </title> <address> Denver, </address> <publisher> CO. </publisher>
Reference: <author> LeCun, Y., Kanter, I., and Solla, S. A. </author> <year> (1991). </year> <title> Second order properties of error surfaces: Learning time and generalization. </title> <editor> In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 3, </volume> <pages> pages 918-924, </pages> <address> Denver, CO, 1990. </address> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address>
Reference-contexts: Indeed for batch update centering errors is equivalent to centering inputs, which is known to assist learning by removing a large eigenvalue of the Hessian <ref> (LeCun et al., 1991) </ref>.
Reference: <author> O'Reilly, R. C. </author> <year> (1996). </year> <title> Biologically plausible error-driven learning using local activation differences: The generalized recirculation algorithm. </title> <booktitle> Neural Computation, </booktitle> <pages> 8. </pages>
Reference: <author> Plaut, D., Nowlan, S., and Hinton, G. </author> <year> (1986). </year> <title> Experiments on learning by back propagation. </title> <type> Technical Report CMU-CS-86-126, </type> <institution> Department of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA. </address> <month> 7 </month>
Reference-contexts: S., Mozer, M. C., and Hasselmo, M. E. (eds.), Advances in Neural Information Processing Systems 8, MIT Press, Cambridge 1996. recover some of it, at considerable computational expense. Ad hoc attempts to incorporate structural information such as the fan-in <ref> (Plaut et al., 1986) </ref> into local learning rates have become a familiar part of backpropagation lore; here we derive a more comprehensive framework which we call tempering and demonstrate its effectiveness. Tempering is based on modeling the activities and error signals in a backpropagation network as independent random variables. <p> assume independence and derive ~ j from the range of their activation functions: ~ j = i2A j p (f i ) 2 ; where p (f i ) max u Note that when all nodes use the same activation function f , we obtain the well-known p fan-in heuristic <ref> (Plaut et al., 1986) </ref> as a special case of (8). 3 Error Backpropagation In deriving local learning rates above we have tacitly used the error signal as a stand-in for the residual proper, i.e. the distance to the target.
References-found: 10

