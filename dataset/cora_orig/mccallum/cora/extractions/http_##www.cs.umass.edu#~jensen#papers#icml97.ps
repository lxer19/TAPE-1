URL: http://www.cs.umass.edu/~jensen/papers/icml97.ps
Refering-URL: http://www.cs.umass.edu/~jensen/papers/ida97.html
Root-URL: 
Email: oates@cs.umass.edu  jensen@cs.umass.edu  
Title: The Effects of Training Set Size on Decision Tree Complexity  
Author: Tim Oates David Jensen 
Address: Box 34610 Amherst, MA 01003-4610  Box 34610 Amherst, MA 01003-4610  
Affiliation: Computer Science Department, LGRC University of Massachusetts  Computer Science Department, LGRC University of Massachusetts  
Abstract: This paper presents experiments with 19 datasets and 5 decision tree pruning algorithms that show that increasing training set size often results in a linear increase in tree size, even when that additional complexity results in no significant increase in classification accuracy. Said differently, removing randomly selected training instances often results in trees that are substantially smaller and just as accurate as those built on all available training instances. This implies that decreases in tree size obtained by more sophisticated data reduction techniques should be decomposed into two parts: that which is due to reduction of training set size, and the remainder, which is due to how the method selects instances to discard. We perform this decomposition for one recent data reduction technique, John's robust-c4.5 (John 1995), and show that a large percentage of its effect on tree size is attributable to the fact that it simply reduces the size of the training set. We conclude that random data reduction is a baseline against which more sophisticated data reduction techniques should be compared. Finally, we examine one possible cause of the pathological relationship between tree size and training set size.
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L.; Friedman, J.; Olshen, R.; and Stone, C. </author> <year> 1984. </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth International. </publisher>
Reference-contexts: for the UC Irvine australian dataset. c4.5 was used to generate the trees (Quinlan 1993) and each plot corresponds to a different pruning mechanism: error-based (ebp the c4.5 default) (Quinlan 1993), reduced error (rep) (Quinlan 1987), minimum description length (mdl) (Quinlan & Rivest 1989), cost-complexity with the 1se rule (ccp1se) <ref> (Breiman et al. 1984) </ref>, and cost-complexity without the 1se rule (ccp0se). On the left-hand side of the graphs, no training instances are available and the best one can do with test instances is to assign them a class label at random. <p> The crx dataset was omitted because it is roughly the same as the australian dataset, pruning methods are error-based (ebp the c4.5 de-fault) (Quinlan 1993), reduced error (rep) (Quinlan 1987), minimum description length (mdl) (Quinlan & Rivest 1989), cost-complexity with the 1se rule (ccp1se) <ref> (Breiman et al. 1984) </ref>, and cost-complexity without the 1se rule (ccp0se).
Reference: <author> Brodley, C. E., and Friedl, M. A. </author> <year> 1996. </year> <title> Identifying and eliminating mislabeled training instances. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence. </booktitle>
Reference-contexts: Brodley and Friedl developed a method to remove instances deemed mislabeled (e.g. by transcription errors) in an effort to boost accuracy. They observe that such filtering, as an unanticipated side-effect, leads to substantially smaller trees <ref> (Brodley & Friedl 1996) </ref>. In this paper we argue that, under a broad range of circumstances, all data reduction techniques will result in some decrease in tree size with little impact on accuracy.
Reference: <author> Catlett, J. </author> <year> 1991. </year> <title> Overpruning large decision trees. </title> <booktitle> In Proceedings of the 12th Internation Joint Conference on Artificial Intelligence, </booktitle> <pages> 764-769. </pages>
Reference-contexts: The linear relationship between training set size and tree size was identified previously in a limited context. Catlett <ref> (Catlett 1991) </ref> found that trees build from extremely large datasets (e.g., datasets containing thousands of instances) were both significantly larger and more accurate than trees built on datasets half that size. The increase in accuracy was attributed, in part, to better attribute selection made possible by additional training instances.
Reference: <author> Cestnik, B., and Bratko, I. </author> <year> 1991. </year> <title> On estimating probabilities in tree pruning. </title> <booktitle> In Proceedings of the Fifth European Working Session on Learning, </booktitle> <pages> 138-150. </pages>
Reference: <author> Cohen, P. R. </author> <year> 1995. </year> <title> Empirical Methods for Artificial Intelligence. </title> <publisher> The MIT Press. </publisher>
Reference-contexts: Then for 1 i k, a tree is built on the instances in D D i and tested on the instances in D i , and the results are averaged over all k folds <ref> (Cohen 1995) </ref>. That procedure was augmented for this paper by building trees on subsets of D D i of various sizes, and testing them on D i .
Reference: <author> Holte, R. C. </author> <year> 1993. </year> <title> Very simple classification rules perform well on most commonly used dataset. </title> <booktitle> Machine Learning 11 </booktitle> <pages> 63-90. </pages>
Reference-contexts: What do these results mean? First, it is clear that tree sizes obtained through random data reduction should serve as a baseline against which other data reduction techniques measure their success, much as default accuracy or Holte's one-rules serve as a baseline for classification accuracy <ref> (Holte 1993) </ref>. If a data reduction technique improves accuracy, or obtains smaller trees relative to trees built by eliminating a comparable number of randomly selected instances, then our confidence in that technique's ability to identify "bad" instances is boosted.
Reference: <author> Jensen, D. </author> <year> 1997. </year> <title> Adjusting for multiple testing in decision tree pruning. </title> <booktitle> In Preliminary Papers of the Sixth International Workshop on Artificial Intelligence and Statistics. </booktitle>
Reference-contexts: One of the authors has identified multiple testing in tree construction and pruning as one source of problems, and has implemented a promising solution <ref> (Jensen 1997) </ref>. Also, decision trees are but one type of model, and we intend to investigate the extent to which other model construction algorithms fall victim to a pathological relationship between model complexity and the amount of data used to build the model.
Reference: <author> John, G. H. </author> <year> 1995. </year> <title> Robust decision trees: Removing outliers from databases. </title> <booktitle> In Proceedings of the First International Conference on Knowledge Discovery and Data Mining. </booktitle>
Reference-contexts: Whether the explicit goal of any given technique is increased accuracy or smaller trees, the latter is invariably observed. John's robust-c4.5 treats misclassified training instances as outliers, iteratively removing them and building a new tree <ref> (John 1995) </ref>. The result over a large number of datasets is trees that are much smaller than those built by c4.5, but that have roughly equivalent accuracy. Brodley and Friedl developed a method to remove instances deemed mislabeled (e.g. by transcription errors) in an effort to boost accuracy. <p> in training set size have strong and predictable effects on tree size. 2.1 Experimental Method The relationship between training set size and tree size was explored with 5 pruning methods and 19 datasets taken from the UC Irvine repository. 1 The 1 The datasets are the same ones used in <ref> (John 1995) </ref> with two exceptions. <p> In this section, we investigate that question for one of the data reduction methods mentioned earlier, John's robust-c4.5 (rc4.5) <ref> (John 1995) </ref>. The idea behind rc4.5 is that when a pruning algorithm turns a test node into a leaf, it is in effect making a local decision to ignore those instances that don't belong to the majority class.
Reference: <author> Mingers, J. </author> <year> 1989. </year> <title> An empirical comparison of pruning methods for decision tree induction. </title> <booktitle> Machine Learning 4 </booktitle> <pages> 227-243. </pages>
Reference: <author> Quinlan, J. R., and Rivest, R. </author> <year> 1989. </year> <title> Inferring decision trees using the minimum description length principle. </title> <booktitle> Information and Computation 80 </booktitle> <pages> 227-248. </pages>
Reference-contexts: size and accuracy as a function of training set size for the UC Irvine australian dataset. c4.5 was used to generate the trees (Quinlan 1993) and each plot corresponds to a different pruning mechanism: error-based (ebp the c4.5 default) (Quinlan 1993), reduced error (rep) (Quinlan 1987), minimum description length (mdl) <ref> (Quinlan & Rivest 1989) </ref>, cost-complexity with the 1se rule (ccp1se) (Breiman et al. 1984), and cost-complexity without the 1se rule (ccp0se). <p> The crx dataset was omitted because it is roughly the same as the australian dataset, pruning methods are error-based (ebp the c4.5 de-fault) (Quinlan 1993), reduced error (rep) (Quinlan 1987), minimum description length (mdl) <ref> (Quinlan & Rivest 1989) </ref>, cost-complexity with the 1se rule (ccp1se) (Breiman et al. 1984), and cost-complexity without the 1se rule (ccp0se).
Reference: <author> Quinlan, J. R. </author> <year> 1987. </year> <title> Simplifying decision trees. </title> <journal> International Journal of Man-Machine Studies 27 </journal> <pages> 221-234. </pages>
Reference-contexts: The figure shows plots of tree size and accuracy as a function of training set size for the UC Irvine australian dataset. c4.5 was used to generate the trees (Quinlan 1993) and each plot corresponds to a different pruning mechanism: error-based (ebp the c4.5 default) (Quinlan 1993), reduced error (rep) <ref> (Quinlan 1987) </ref>, minimum description length (mdl) (Quinlan & Rivest 1989), cost-complexity with the 1se rule (ccp1se) (Breiman et al. 1984), and cost-complexity without the 1se rule (ccp0se). <p> The crx dataset was omitted because it is roughly the same as the australian dataset, pruning methods are error-based (ebp the c4.5 de-fault) (Quinlan 1993), reduced error (rep) <ref> (Quinlan 1987) </ref>, minimum description length (mdl) (Quinlan & Rivest 1989), cost-complexity with the 1se rule (ccp1se) (Breiman et al. 1984), and cost-complexity without the 1se rule (ccp0se).
Reference: <author> Quinlan, J. R. </author> <year> 1993. </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The figure shows plots of tree size and accuracy as a function of training set size for the UC Irvine australian dataset. c4.5 was used to generate the trees <ref> (Quinlan 1993) </ref> and each plot corresponds to a different pruning mechanism: error-based (ebp the c4.5 default) (Quinlan 1993), reduced error (rep) (Quinlan 1987), minimum description length (mdl) (Quinlan & Rivest 1989), cost-complexity with the 1se rule (ccp1se) (Breiman et al. 1984), and cost-complexity without the 1se rule (ccp0se). <p> The figure shows plots of tree size and accuracy as a function of training set size for the UC Irvine australian dataset. c4.5 was used to generate the trees <ref> (Quinlan 1993) </ref> and each plot corresponds to a different pruning mechanism: error-based (ebp the c4.5 default) (Quinlan 1993), reduced error (rep) (Quinlan 1987), minimum description length (mdl) (Quinlan & Rivest 1989), cost-complexity with the 1se rule (ccp1se) (Breiman et al. 1984), and cost-complexity without the 1se rule (ccp0se). <p> The crx dataset was omitted because it is roughly the same as the australian dataset, pruning methods are error-based (ebp the c4.5 de-fault) <ref> (Quinlan 1993) </ref>, reduced error (rep) (Quinlan 1987), minimum description length (mdl) (Quinlan & Rivest 1989), cost-complexity with the 1se rule (ccp1se) (Breiman et al. 1984), and cost-complexity without the 1se rule (ccp0se).
References-found: 12

