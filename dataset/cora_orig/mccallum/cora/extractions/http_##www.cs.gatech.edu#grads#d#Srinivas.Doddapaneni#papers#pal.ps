URL: http://www.cs.gatech.edu/grads/d/Srinivas.Doddapaneni/papers/pal.ps
Refering-URL: http://www.cs.gatech.edu/grads/d/Srinivas.Doddapaneni/pubs.html
Root-URL: 
Title: A New Algorithm for Global Optimization for Parallelism and Locality  
Author: Bill Appelbe, Srinivas Doddapaneni and Charles Hardnett 
Address: Atlanta, GA 30332  
Affiliation: College of Computing, Georgia Institute of Technology,  
Abstract: Converting sequential programs to execute on parallel computers is difficult because of the need to globally optimize for both parallelism and data locality. The choice of which loop nests to parallelize, and how, drastically affects data locality. Similarly, data distribution directives, such as DISTRIBUTE in High Performance Fortran (HPF), affects available parallelism and locality. What is needed is a systematic approach to converting programs to parallel form, based upon analysis that identifies opportunities for both parallelism and locality in one representation. This paper presents a global framework for optimizing parallelism and locality, based upon constraint solving for locality between potentially parallel loop nests. We outline the theory behind the framework, and provide a global algorithm for parallelizing programs while optimizing for locality. We also give results from applying the algorithm to parallelizing the Perfect benchmarks, targeted at the KSR-1, and analyze the results. Unlike other approaches, we do not assume an explicit distribution of data to processors. The distribution is inferred from locality constraints and available parallelism. This approach works well for machines such as the KSR-1, where there is no explicit distribution of data. However, our approach could be used to generate code for distributed memory processors (such as generating HPF) with explicit data distribution.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Anderson, J., and Lam, M. S. </author> <title> Global optimzations for parallelism and locality on scalable parallel machines. </title> <booktitle> In SIGPLAN Programming Language Design and Implementation (1993), </booktitle> <pages> pp. 112-125. </pages>
Reference-contexts: Next, analyze the array accesses in the parallel loops to determine the data and computation distributions that optimize the locality of reference (PARADIGM [7], and ADG [6]). But the optimality can only be obtained if subproblems are treated together <ref> [1] </ref>. Our approach brings both program transformations for parallel and determining distributions of data and computation to processors into a single framework. The closest related work is that of Anderson and Lam [1]. They use explicit data distributions and iteration to processor mappings represented as linear transformation matrices. <p> But the optimality can only be obtained if subproblems are treated together <ref> [1] </ref>. Our approach brings both program transformations for parallel and determining distributions of data and computation to processors into a single framework. The closest related work is that of Anderson and Lam [1]. They use explicit data distributions and iteration to processor mappings represented as linear transformation matrices. Unlike our approach, they rely upon iteration space transformations rather than initially using statement transformations. Our representation of partitions is more precise than theirs (which relies on basis vectors).
Reference: 2. <author> Appelbe, B., Doddapaneni, S., Hardnett, C., and Smith, K. </author> <title> Determining transformation sequences for loop parallelization. </title> <type> Tech. Rep. </type> <institution> GIT-ICS-92/59, Georgia Institute of Technology, </institution> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: These transformations can be applied to each level of a loop nest, including loops containing conditional statements <ref> [2] </ref>. An alternative approach is to remove dependences by skewing and other unimodular transformations (transforming the iteration space rather than statements). In an N-level loop this can result in N-1 levels of parallelism [11]. However, skewing introduces a communication overhead (which can later be minimized by block skewing). <p> removing dependences. 1 However, replication (for removing anti-dependences) 1 More accurately, statement alignment can never degrade locality, as alignment shifts all accesses of an array variable to the same iteration, and hence the same processor. has a computational overhead (making copies), although strip mining can be used to reduce copying <ref> [2] </ref>. Similarly, distribution introduces an overhead although it may improve parallelism in a given dimension. Hence, in our model we identify available parallelism rather than actually performing the transformations before analyzing for locality.
Reference: 3. <author> Appelbe, B., Hardnett, C., and Doddapaneni, S. </author> <title> Aligning data structures for parallelism and locality. </title> <type> Tech. Rep. </type> <institution> GIT-CC-94-20, Georgia Institute of Technology, </institution> <month> Feb. </month> <year> 1994. </year>
Reference: 4. <author> Appelbe, B., and Smith, K. </author> <title> Determining transformation sequences for loop parallelization. </title> <booktitle> In Fifth Workshop on Languages and Compilers for Parallel Computing (July 1993). </booktitle>
Reference-contexts: Thus, available parallelism simply means loops which are not serial. Loop carried dependences can be systematically removed by statement transformations (alignment, replication, substitution) <ref> [4] </ref> and by standard transformations for privatization of local variables. If there are no recurrences (more precisely, no cycles in the dependence graph for that loop level with all dependence directions positive), then all dependences can be removed, otherwise recurrences can be isolated in serial loops using loop distribution.
Reference: 5. <author> Callahan, D. </author> <title> A Global Approach to Detection of Parallelism. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <year> 1987. </year> <type> Rice Tech Report, </type> <institution> COMP TR87-50. </institution>
Reference-contexts: Shared memory machines introduce further problems due to false sharing. Most previous work has focused upon subproblems such as optimizing locality for a single loop nest [12, 9, 10]. Other authors have focused upon the problem of scheduling loop iterations and transforming loops for maximum parallelism <ref> [5, 11] </ref> or optimizing data distribution [6]. All these approaches do not generalize easily to the problem of globally opti-mizing for both parallelism and locality. The fundamental insight to overcoming these limitations is that parallelism and locality cannot be optimized independently.
Reference: 6. <author> Chatterjee, S., Gilbert, J., and Schreiber, R. </author> <title> The alignment-distribution graph. </title> <booktitle> In Sixth Workshop on Languages and Compilers for Parallel Computing (July 1993), </booktitle> <pages> pp. 234-252. </pages>
Reference-contexts: Most previous work has focused upon subproblems such as optimizing locality for a single loop nest [12, 9, 10]. Other authors have focused upon the problem of scheduling loop iterations and transforming loops for maximum parallelism [5, 11] or optimizing data distribution <ref> [6] </ref>. All these approaches do not generalize easily to the problem of globally opti-mizing for both parallelism and locality. The fundamental insight to overcoming these limitations is that parallelism and locality cannot be optimized independently. <p> First, transform the program to expose the parallelism in the program. Next, analyze the array accesses in the parallel loops to determine the data and computation distributions that optimize the locality of reference (PARADIGM [7], and ADG <ref> [6] </ref>). But the optimality can only be obtained if subproblems are treated together [1]. Our approach brings both program transformations for parallel and determining distributions of data and computation to processors into a single framework. The closest related work is that of Anderson and Lam [1]. <p> Their model is based upon explicit data distribution, and mesh decomposition. They use constraint solution, and do not have an explicit phase for identifying available parallelism. Other authors use graph representations for finding data distributions that optimize communication for a parallel program. The Alignment-Distribution Graph (ADG <ref> [6] </ref>) is similar in spirit to our PAL Graph.
Reference: 7. <author> Gupta, M., and Banerjee, P. </author> <title> Paradigm: A compiler for automatic data distri-bution on multicomputers. </title> <booktitle> In International Conference on Supercomputing (June 1993), </booktitle> <pages> pp. 87-96. </pages>
Reference-contexts: First, transform the program to expose the parallelism in the program. Next, analyze the array accesses in the parallel loops to determine the data and computation distributions that optimize the locality of reference (PARADIGM <ref> [7] </ref>, and ADG [6]). But the optimality can only be obtained if subproblems are treated together [1]. Our approach brings both program transformations for parallel and determining distributions of data and computation to processors into a single framework. The closest related work is that of Anderson and Lam [1]. <p> However, in many cases both approaches would derive the same mappings. In addition, they use a communication graph corresponding to the PAL graph above. However, their communication graph is less precise (does not include dependences or constraints). Other related work includes PARADIGM <ref> [7] </ref>. Their model is based upon explicit data distribution, and mesh decomposition. They use constraint solution, and do not have an explicit phase for identifying available parallelism. Other authors use graph representations for finding data distributions that optimize communication for a parallel program.
Reference: 8. <author> Haghighat, M., and Polychronopoulos, C. </author> <title> Symbolic analysis for parallelizing compilers. </title> <type> Tech. rep., </type> <institution> University of Illinois, </institution> <year> 1994. </year>
Reference-contexts: The key question is how to solve sets of constraints for the entire program, how to compromise when all constraints cannot be satisfied, and how to optimize communication. Prior to global partitioning, we assume that a number of standard optimizations have been performed, and that symbolic analysis <ref> [8] </ref> has been done to simplify the program and determine the number of loop iterations. In practice, for the benchmarks below, we have used runtime traces to determine loop iterations.
Reference: 9. <author> Kennedy, K., and McKinley, K. </author> <title> Optimizing for parallelism and data locality. </title> <booktitle> In International Conference on Supercomputing (July 1992), </booktitle> <pages> pp. 323-334. </pages>
Reference-contexts: Nevertheless, the communication costs are a significant overhead in practice, as the results in section 4 show. Shared memory machines introduce further problems due to false sharing. Most previous work has focused upon subproblems such as optimizing locality for a single loop nest <ref> [12, 9, 10] </ref>. Other authors have focused upon the problem of scheduling loop iterations and transforming loops for maximum parallelism [5, 11] or optimizing data distribution [6]. All these approaches do not generalize easily to the problem of globally opti-mizing for both parallelism and locality.
Reference: 10. <author> Ramanujam, J., and Sadayappan, P. </author> <title> Tiling multidimensional iteration spaces for nonshared memory machines. </title> <note> In Supecomputing '91 (Nov. </note> <year> 1991), </year> <pages> pp. 111-121. </pages>
Reference-contexts: Nevertheless, the communication costs are a significant overhead in practice, as the results in section 4 show. Shared memory machines introduce further problems due to false sharing. Most previous work has focused upon subproblems such as optimizing locality for a single loop nest <ref> [12, 9, 10] </ref>. Other authors have focused upon the problem of scheduling loop iterations and transforming loops for maximum parallelism [5, 11] or optimizing data distribution [6]. All these approaches do not generalize easily to the problem of globally opti-mizing for both parallelism and locality.
Reference: 11. <author> Wolf, M. E., and Lam, M. S. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In SIGPLAN Programming Language Design and Implementation (1991), </booktitle> <pages> pp. 30-44. </pages>
Reference-contexts: Shared memory machines introduce further problems due to false sharing. Most previous work has focused upon subproblems such as optimizing locality for a single loop nest [12, 9, 10]. Other authors have focused upon the problem of scheduling loop iterations and transforming loops for maximum parallelism <ref> [5, 11] </ref> or optimizing data distribution [6]. All these approaches do not generalize easily to the problem of globally opti-mizing for both parallelism and locality. The fundamental insight to overcoming these limitations is that parallelism and locality cannot be optimized independently. <p> An alternative approach is to remove dependences by skewing and other unimodular transformations (transforming the iteration space rather than statements). In an N-level loop this can result in N-1 levels of parallelism <ref> [11] </ref>. However, skewing introduces a communication overhead (which can later be minimized by block skewing).
Reference: 12. <author> Wolf, M. E., and Lam, M. S. </author> <title> A loop transformation theory and an algorithm to maximize parallelism. </title> <journal> IEEE Transactions on Parallel and Distributed Systems 2, </journal> <month> 4 (October </month> <year> 1991), </year> <pages> 452-482. </pages>
Reference-contexts: Nevertheless, the communication costs are a significant overhead in practice, as the results in section 4 show. Shared memory machines introduce further problems due to false sharing. Most previous work has focused upon subproblems such as optimizing locality for a single loop nest <ref> [12, 9, 10] </ref>. Other authors have focused upon the problem of scheduling loop iterations and transforming loops for maximum parallelism [5, 11] or optimizing data distribution [6]. All these approaches do not generalize easily to the problem of globally opti-mizing for both parallelism and locality.
Reference: 13. <author> Zima, H., and Chapman, B. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> ACM Press, </publisher> <address> New York, New York, </address> <year> 1990. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: Algorithms for minimizing the number of barriers have been developed by other authors (e.g., the Kennedy/Callahan algorithm <ref> [13] </ref>). The Kennedy/Callahan algorithm needs to be modified for our application, as it results in a barrier between any two parallel loops with a reaching dependence (as it assumes no processor to iteration mapping). We need to insert a barrier only if the resulting locality constraint is not satisfied.
References-found: 13

