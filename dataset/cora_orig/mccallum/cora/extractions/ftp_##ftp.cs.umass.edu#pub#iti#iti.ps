URL: ftp://ftp.cs.umass.edu/pub/iti/iti.ps
Refering-URL: http://www-ml.cs.umass.edu/iti/index.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: utgoff@cs.umass.edu  neil@corvid.com  clouse@ncat.edu  
Title: Decision Tree Induction Based on Efficient Tree Restructuring  
Author: PAUL E. UTGOFF NEIL C. BERKMAN JEFFERY A. CLOUSE Editor: Doug Fisher 
Keyword: decision tree, incremental induction, direct metric, binary test, example incorporation, missing value, tree transposition, installed test, virtual pruning, update cost.  
Address: Amherst, MA 01003  779 West St., Carlisle, MA 01741  Greensboro, NC 27411  
Affiliation: Department of Computer Science, University of Massachusetts,  Corvid Corp.,  Department of Computer Science, North Carolina A&T State University,  
Note: 1-42 c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Abstract: The ability to restructure a decision tree efficiently enables a variety of approaches to decision tree induction that would otherwise be prohibitively expensive. Two such approaches are described here, one being incremental tree induction (ITI), and the other being non-incremental tree induction using a measure of tree quality instead of test quality (DMTI). These approaches and several variants offer new computational and classifier characteristics that lend themselves to particular applications. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. </author> <year> (1984). </year> <title> Classification and regression trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: Schlimmer and Fisher's (1986) ID4 demonstrated incremental tree induction through test revision and discarding of subtrees. Crawford (1989) has constructed an incremental version of the CART algorithm <ref> (Breiman, Friedman, Olshen & Stone, 1984) </ref>. When a new example is received, if a new test would be picked at a decision node, a new subtree with the new test is constructed by building the new subtree from scratch from the corresponding subset of the training examples.
Reference: <author> Cockett, J. R. B., & Herrera, J. A. </author> <year> (1990). </year> <title> Decision tree reduction. </title> <journal> Journal of the ACM, </journal> <volume> 37, </volume> <pages> 815-842. </pages>
Reference: <author> Crawford, S. L. </author> <year> (1989). </year> <title> Extensions to the CART algorithm. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 31, </volume> <pages> 197-217. </pages>
Reference: <author> Fayyad, U. M. </author> <year> (1991). </year> <title> On the induction of decision trees for multiple concept learning. </title> <type> Doctoral dissertation, </type> <institution> Computer Science and Engineering, University of Michigan. </institution>
Reference: <author> Fayyad, U. M., & Irani, K. B. </author> <year> (1992). </year> <title> On the handling of continuous-valued attributes in decision tree generation. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 87-102. </pages>
Reference-contexts: The possible cutpoints and the merit of each one can be computed efficiently during a single pass over the sorted list of tagged values. When average class entropy is the metric for test selection, one needs only to consider those cutpoints that separate two values from different classses <ref> (Fayyad & Irani, 1992) </ref>. 2.3. Incorporating a Training Example A basic operation in tree revision is to change the set of examples on which the tree is based.
Reference: <author> Fisher, D. </author> <year> (1996). </year> <title> Iterative optimization and simplification of hierarchical clusterings. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4, </volume> <pages> 147-178. </pages>
Reference: <author> Kalles, D., & Morris, T. </author> <year> (1996). </year> <title> Efficient incremental induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 24, </volume> <pages> 231-242. </pages>
Reference: <author> Kohavi, R. </author> <year> (1995). </year> <title> The power of decision tables. </title> <booktitle> Proceedings of the European Conference on Machine Learning. </booktitle>
Reference-contexts: The cross-validated accuracy is the percentage of classifications that were correct. With efficient tree revision, leave-one-out cross validation is practical for decision tree classification <ref> (Kohavi, 1995) </ref>. One first builds a tree from all the examples using ITI in incremental or batch mode. Then for each example, one subtracts it from the tree, classifies it, and adds it to the tree.
Reference: <author> Lovell, B. C., & Bradley, A. P. </author> <year> (1996). </year> <title> The multiscale classifer. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 18, </volume> <pages> 124-137. </pages>
Reference: <author> Mooney, R., Shavlik, J., Towell, G., & Gove, A. </author> <year> (1989). </year> <title> An experimental comparison of symbolic and connectionist learning algorithms. </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 775-780). </pages> <address> Detroit, Michigan: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Murphy, P. M., & Aha, D. W. </author> <year> (1994). </year> <title> UCI repository of machine learning databases, </title> <address> Irvine, CA: </address> <institution> University of California, Department of Information and Computer Science. </institution>
Reference-contexts: Finally, the direct metric expected-misclassification-cost measures the penalty that one would pay when misclassifying an example, assuming that testing examples are drawn according to the same probability distribution as training examples. Often, tree induction algorithms embody the assumption that all classification errors incur the same cost <ref> (Pazzani, Merz, Murphy, Ali, Hume & Brunk, 1994) </ref>. To be more comprehensive, one can include an explicit cost matrix that specifies the cost of labelling an example with class X when it should have been class Y. <p> The DM variant uses minimum description length, the DE variant uses expected number of tests for classification, and the DL variant uses the number of leaves. 5.3. Tasks Thirty-nine of the tasks were taken from the UCI <ref> (Murphy & Aha, 1994) </ref> repository, and the remaining seven were acquired or produced elsewhere. From all the available UCI tasks, one was generally taken from UCI and included here if it was not extraordinarily large and if there was a clearly defined class label.
Reference: <author> Pazzani, M., Merz, C., Murphy, P., Ali, K., Hume, T., & Brunk, C. </author> <year> (1994). </year> <title> Reducing misclas-sification costs. </title> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference (pp. </booktitle> <pages> 217-225). </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Finally, the direct metric expected-misclassification-cost measures the penalty that one would pay when misclassifying an example, assuming that testing examples are drawn according to the same probability distribution as training examples. Often, tree induction algorithms embody the assumption that all classification errors incur the same cost <ref> (Pazzani, Merz, Murphy, Ali, Hume & Brunk, 1994) </ref>. To be more comprehensive, one can include an explicit cost matrix that specifies the cost of labelling an example with class X when it should have been class Y.
Reference: <author> Quinlan, J. R., & Rivest, R. L. </author> <year> (1989). </year> <title> Inferring decision trees using the minimum description length principle. </title> <journal> Information and Computation, </journal> <volume> 80, </volume> <pages> 227-248. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for machine learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For example, if the value set for the variable color were fred, green, blueg then the possible binary tests would be (color = red), (color = green), and (color = blue). For numeric variables, the conversion to a binary test is done as it is by C4.5 <ref> (Quinlan, 1993) </ref>, by finding a cutpoint and incorporating it into a threshold test, for example (x &lt; cutpoint). The outcome of a test is either that the value of the variable in the example satisfies the test, or that it does not.
Reference: <author> Rissanen, J. </author> <year> (1978). </year> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14, </volume> <pages> 465-471. </pages>
Reference-contexts: All of the approaches that maintain a separate pruning set are oxymoronic for incremental tree induction. For ITI, a suitable approach is based on the minimum description length principle <ref> (Rissanen, 1978, Quinlan & Rivest, 1989) </ref>.
Reference: <author> Schlimmer, J. C., & Fisher, D. </author> <year> (1986). </year> <title> A case study of incremental concept induction. </title> <booktitle> Proceedings of the Fifth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 496-501). </pages> <address> Philadelpha, PA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Steel, R. G. D., & Torrie, J. H. </author> <year> (1960). </year> <title> Principles and procedures of statistics. </title> <address> New York, NY: </address> <publisher> McGraw-Hill. </publisher>
Reference-contexts: No subgroup is tested when a group is not significantly different. Any two means that are underscored by any common line are not significantly different. Any two means that are not underscored by a common line are significantly different <ref> (Steel & Torrie, 1960) </ref>. Finally, to reduce experimental error, the eight algorithm variants are run as a group with pruning turned off, and then again as a separate group with pruning turned on. There is no discussion here of the relative merits of pruning versus not pruning. 5.2.
Reference: <author> Tan, M., & Schlimmer, J. C. </author> <year> (1990). </year> <title> Two case studies in cost-sensitive concept acquisition. </title> <booktitle> Proceedings of the Eighth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 854-860). </pages> <address> Boston, MA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The metric expected-classification-cost is identical to expected-number-of-tests except that each test has a specified evaluation cost, instead of the implied uniform evaluation cost. In some applications, such as diagnosis, some tests are much more expensive than others, and the cost of producing the answer is an important factor <ref> (Tan & Schlimmer, 1990) </ref>. Finally, the direct metric expected-misclassification-cost measures the penalty that one would pay when misclassifying an example, assuming that testing examples are drawn according to the same probability distribution as training examples.
Reference: <author> Utgoff, P. E. </author> <year> (1989). </year> <title> Incremental induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 4, </volume> <pages> 161-186. </pages>
Reference-contexts: It does however bring some inefficiency when merging two AVL-trees, for example two sorted lists of tagged values, making the cost O (n log n) instead of O (n). 9. Related Work The incremental tree induction algorithm ID5R <ref> (Utgoff, 1989) </ref> demonstrated the basic process of tree revision. It did not handle numeric variables, multiclass tasks, or missing values, and did not include any prepruning or postpruning method for avoiding overfitting.
Reference: <author> Utgoff, P. E. </author> <year> (1994). </year> <title> An improved algorithm for incremental induction of decision trees. </title> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference (pp. </booktitle> <pages> 318-325). </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Related Work The incremental tree induction algorithm ID5R (Utgoff, 1989) demonstrated the basic process of tree revision. It did not handle numeric variables, multiclass tasks, or missing values, and did not include any prepruning or postpruning method for avoiding overfitting. The first version of ITI <ref> (Utgoff, 1994) </ref> had an awkward manner for handling numeric variables and for handling missing values, that have been replaced here. Schlimmer and Fisher's (1986) ID4 demonstrated incremental tree induction through test revision and discarding of subtrees.
Reference: <author> Utgoff, P. E., & Clouse, J. A. </author> <year> (1996). </year> <title> A Kolmogorov-Smirnoff metric for decision tree induction, </title> <type> (Technical Report 96-3), </type> <institution> Amherst, MA: University of Massachusetts, Department of Computer Science. </institution>
Reference-contexts: Several useful operators have been implemented that have not been discussed here, such as save-tree and restore-tree. In addition, a Kolmogorov-Smirnoff distance attribute selection metric can be selected as the indirect metric <ref> (Utgoff & Clouse, 1996) </ref> instead of the default. In the implementation, every set of information items kept at a decision node is maintained as an AVL tree, which is an almost-balanced binary search tree (Wirth, 1976). This organization provides O (log n) insert, delete and lookup.
Reference: <author> Van de Velde, W. </author> <year> (1990). </year> <title> Incremental induction of topologically minimal trees. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning (pp. </booktitle> <pages> 66-74). </pages> <address> Austin, TX: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Walpole, R. E. </author> <year> (1974). </year> <title> Introduction to statistics. </title> <address> New York: </address> <publisher> Macmillan. </publisher>
Reference-contexts: One cannot simply apply a test for each comparison, because the chance of a false difference rises with each test, much like repeatedly 15 rolling a twenty-sided die that has one face marked `significant'. A variety of multiple comparison procedures have been devised, and Duncan's Multiple Range Test <ref> (Walpole, 1974) </ref> is used here. To use the Duncan Multiple Range Test (DMRT), one first isolates the variance that cannot be attributed to any treatment, which is called the error variance. To do this, one proceeds with the initial stages of an analysis of variance.
Reference: <author> White, A. P., & Liu, W. Z. </author> <year> (1994). </year> <title> Bias in information-based measures in decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 15, </volume> <pages> 321-329. </pages>
Reference-contexts: The first is that there can be no bias among tests that is due to the tests having a different number of possible outcomes. This is important because many common methods for selecting a test are biased in this manner <ref> (White & Liu, 1994) </ref>. Second, choosing a binary 3 split at a decision node is a conservative approach to partitioning, because a block of examples is divided into at most two smaller blocks.
Reference: <author> Wirth, N. </author> <year> (1976). </year> <title> Algorithms + data structures = programs. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference-contexts: In addition, a Kolmogorov-Smirnoff distance attribute selection metric can be selected as the indirect metric (Utgoff & Clouse, 1996) instead of the default. In the implementation, every set of information items kept at a decision node is maintained as an AVL tree, which is an almost-balanced binary search tree <ref> (Wirth, 1976) </ref>. This organization provides O (log n) insert, delete and lookup. Specifically, the set of variables is maintained at a decision node as an attached AVL-tree of variables.
References-found: 25

