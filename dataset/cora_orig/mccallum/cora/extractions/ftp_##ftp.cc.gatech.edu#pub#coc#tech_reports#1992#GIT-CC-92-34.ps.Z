URL: ftp://ftp.cc.gatech.edu/pub/coc/tech_reports/1992/GIT-CC-92-34.ps.Z
Refering-URL: http://www.cs.gatech.edu/fac/Mustaque.Ahamad/pubs.html
Root-URL: 
Title: The Power of Processor Consistency  
Author: Mustaque Ahamad Rida A. Bazzi Ranjit John Prince Kohli Gil Neiger 
Note: This work was supported in part by the National Science Foundation under grants CCR-8619886, CCR-8909663, and CCR-9106627. This author was supported in part by a scholarship from the Hariri Foundation.  
Address: Atlanta, Georgia 30332-0280  
Affiliation: College of Computing Georgia Institute of Technology  
Date: December 31, 1992 Revised: July 8, 1993  
Pubnum: GIT-CC-92/34  
Abstract: Shared memories that provide weaker consistency guarantees than the traditional sequentially consistent or atomic memories have been claimed to provide the key to building scalable systems. One influential memory model, processor consistency, has been cited widely in the literature but, due to the lack of a precise and formal definition, contradictory claims have been made regarding its power. We use a formal model to give two distinct definitions of processors consistency: one corresponding to Goodman's original proposal and the other corresponding that given by the implementors of the DASH system. These definitions are nonoperational and can be easily related to other types of memories. To illustrate the power of processor consistency, we exhibit a non-cooperative solution to the mutual exclusion problem that is correct with processor consistency. As a contrast, we show that Lamport's Bakery algorithm is not correct with processor consistency. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Weak ordering | a new definition. </title> <booktitle> In Proceedings of the Seventeenth Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14, </pages> <year> 1990. </year>
Reference: [2] <author> Mustaque Ahamad, James E. Burns, Phillip W. Hutto, and Gil Neiger. </author> <title> Causal memory. </title> <editor> In S. Toueg, P. G. Spirakis, and L. Kirousis, editors, </editor> <booktitle> Proceedings of the Fifth International Workshop on Distributed Algorithms, volume 579 of Lecture Notes on Computer Science, </booktitle> <pages> pages 9-30. </pages> <publisher> Springer-Verlag, </publisher> <month> October </month> <year> 1991. </year>
Reference-contexts: We present here a non-operational definition. As noted above, Goodman's definition of processor consistency was a combination of PRAM and coherence. The DASH definition (which we refer to as PCD) combines coherence with "semi-causality" (this is similar to the causality relation used to define causal memory <ref> [2] </ref>). Since all PCD histories are coherent, there is a legal serialization S x of operations on each location x. The semi-causality relation is defined as follows. First, a weaker notion of program order is used; this is because reads are allowed to "bypass" earlier writes by the same processor. <p> We prove that Peterson's algorithm for mutual exclusion [25], which does not require cooperation, is correct with both forms of processor consistency defined in the previous section. We then show that Lamport's Bakery algorithm [19], another such algorithm, is not correct with processor consistency. 14 shared need <ref> [2] </ref> : boolean /* Initially false */ turn : integer local other : boolean whose : integer while true do w (need [i])true w (turn)- repeat whose = r (turn ) other = r (need [-]) until (whose = i or not other ) Critical section w (need [i])false Remainder section <p> While this technique has been used by other researchers to define shared memories [5,16,21,24], it has never been used to define processor consistency. (Other researchers [13,14] have used a formal automaton-based method for defining shared memories.) We have also used this method to define another shared memory, called causal memory <ref> [2] </ref>. Like processor consistency, causal memory lies between between PRAM and sequential consistency in a hierarchy of shared memories. However, processor consistency and causal memory are incomparable; there are histories of each that are not histories of the other.
Reference: [3] <author> Mustaque Ahamad, Phillip W. Hutto, and Ranjit John. </author> <title> Implementing and programming causal distributed shared memory. </title> <booktitle> In Proceedings of the Eleventh International Conference on Distributed Computing, </booktitle> <pages> pages 274-281, </pages> <month> May </month> <year> 1991. </year>
Reference: [4] <author> Hagit Attiya, </author> <month> August </month> <year> 1992. </year> <type> Personal communication. </type>
Reference-contexts: For example, Peterson extended the algorithm given in Figure 11 to one that solves the n-processor problem. This extension is correct with either form of processor consistency. Attiya and Friedman understood processor consistency to not include coherence <ref> [4] </ref>. Under this assumption, mutual exclusion without cooperation is indeed impossible. Implementations of coherent memory [12] require processor cooperation at a lower level. It is not hard to see that, to provide mutual exclusion, cooperation is necessary at some level.
Reference: [5] <author> Hagit Attiya and Roy Friedman. </author> <title> A correctness condition for high performance multiprocessors. </title> <booktitle> In Proceedings of the Twenty-Fourth ACM Symposium on Theory of Computing, </booktitle> <pages> pages 679-690. </pages> <publisher> ACM Press, </publisher> <month> May </month> <year> 1992. </year>
Reference-contexts: However, it is not as strong as sequential consistency, as it may, like cache consistency, permit some writes to different locations to appear in different orders at different processors. There is some disagreement as to whether or not Goodman meant for processor consistency to include coherence. Attiya and Friedman <ref> [5] </ref> and Bitar [6] both assumed that coherence was not part of processor consistency. <p> Attiya and Friedman <ref> [5] </ref> claimed that processor consistency lacked the power to implement mutual exclusion without cooperation (recall that they did not attribute coherence to processor consistency).
Reference: [6] <author> Philip Bitar. </author> <title> MIMD synchronization and coherence. </title> <type> Technical Report 90/605, </type> <institution> Department of Computer Science, University of California at Berkeley, </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: There is some disagreement as to whether or not Goodman meant for processor consistency to include coherence. Attiya and Friedman [5] and Bitar <ref> [6] </ref> both assumed that coherence was not part of processor consistency.
Reference: [7] <author> Michel Dubois, Christoph Scheurich, and Faye Briggs. </author> <title> Memory access buffering in multiprocessors. </title> <booktitle> In Proceedings of the Thirteenth International Symposium on Computer Architecture, </booktitle> <pages> pages 434-442, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: Because Goodman's term for coherence was "cache consistency," others have taken "weak ordering" to mean that of Dubois, Scheurich, and Briggs <ref> [7] </ref>. However, this type of memory requires a distinction between "strong" and "weak" accesses, which Goodman does not attribute to processor consistency; it thus seems unlikely that this was his intention.
Reference: [8] <author> Roy Friedman, </author> <month> March </month> <year> 1993. </year> <type> Personal communication. </type>
Reference-contexts: We are currently working on a modification of PCD that matches the DASH implementation. The DASH implementation of processor consistency is designed for a multiprocessor. It is not clear whether or not the memory can scale well to large distributed systems. In fact, Friedman <ref> [8] </ref> has observed that any implementation of PCG or PCD would require access latencies on the order of the longest communication delay in the system. This suggests that processor consistency would require access latencies similar to those incurred by sequential consistency without providing as strong consistency guarantees.
Reference: [9] <author> Kourosh Gharachorloo, </author> <month> October </month> <year> 1992. </year> <type> Personal communication. </type>
Reference: [10] <author> Kourosh Gharachorloo, Sarita V. Adve, Anoop Gupta, John L. Hennessy, and Mark D. Hill. </author> <title> Programming for different memory consistency models. </title> <journal> Journal of Parallel and Distributed Systems, </journal> <volume> 15(4) </volume> <pages> 399-407, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Researchers are attempting to develop a similar model for release consistency with processor consistent synchronization (RCpc) or simple processor consistency <ref> [10] </ref>. We have recently learned [11] that the version of processor consistency implemented in the DASH system is weaker than that described by Gharachorloo et al. [12] and 18 defined here as PCD. That is, the DASH system does actually not implement PCD.
Reference: [11] <author> Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> Revision to "Memory consistency and event ordering in scalable shared-memory multiprocessors". </title> <type> Unpublished manuscript, </type> <month> March </month> <year> 1993. </year>
Reference-contexts: Researchers are attempting to develop a similar model for release consistency with processor consistent synchronization (RCpc) or simple processor consistency [10]. We have recently learned <ref> [11] </ref> that the version of processor consistency implemented in the DASH system is weaker than that described by Gharachorloo et al. [12] and 18 defined here as PCD. That is, the DASH system does actually not implement PCD.
Reference: [12] <author> Kourosh Gharachorloo, Daniel Lenoski, James Laudon, Phillip Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the Seventeenth International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year> <month> 20 </month>
Reference-contexts: One influential weakly consistent memory was proposed by Goodman [15] and is called processor consistency. The DASH system implements a type of processor consistent memory that is distinct from Goodman's definition; this was described by Gharachorloo et al. <ref> [12] </ref>. Processor consistency has been cited elsewhere in the literature [5,6,10,14]. Unfortunately, the original proposal did not provide a detailed specification of processor consistency, and only an operational definition is provided by Gharachorloo et al. As a result, various researchers have developed different interpretations leading to confusion and contradictory claims. <p> Goodman's definition of cache consistency requires that the above hold only on a per-location basis: processors need not agree on the order of two writes to two different locations. Following Gharachorloo et al. <ref> [12] </ref>, we use the term coherence to refer to the property of every data location being strongly ordered. Goodman observes that strong consistency is costly to implement and that cache consistency alone is inadequate for many applications. <p> As noted above, there are two such formulations. One was originally presented by Goodman [15] and is referred to here as PCG. The other was described by the implementors of the DASH system <ref> [12] </ref> and is referred to here as PCD. 5.1 Goodman's Processor Consistency Goodman's definition of processor consistency is, in a sense, a combination of PRAM and coherence. <p> Thus, PCG is strictly weaker than sequential consistency. 8 5.2 Processor Consistency in the DASH System The DASH system also implements a form of processor consistency. As we will see, this definition is incomparable with Goodman's; it is neither stronger nor weaker. Ghara-chorloo et al. <ref> [12] </ref> gave an operational definition, using the notion of an operation being "performed" at a processor (see Section 2). We present here a non-operational definition. As noted above, Goodman's definition of processor consistency was a combination of PRAM and coherence. <p> and place those histories in the structure of memories. 12 7 3 PRAM Coherence PCD SC p: r (x)1 w (y)1 5.4 Disallowing Anomalous Executions in Processor Consis tency The definitions of processor consistency given above correctly capture the definitions originally given by Goodman [15] and by Gharachorloo et al. <ref> [12] </ref>. However, they may allow certain anomalous executions. <p> This extension is correct with either form of processor consistency. Attiya and Friedman understood processor consistency to not include coherence [4]. Under this assumption, mutual exclusion without cooperation is indeed impossible. Implementations of coherent memory <ref> [12] </ref> require processor cooperation at a lower level. It is not hard to see that, to provide mutual exclusion, cooperation is necessary at some level. Thus, memories that do not require cooperation in their implementation (e.g., slow memory, PRAM, or causal memory) cannot provide mutual exclusion without explicit cooperation. <p> Researchers are attempting to develop a similar model for release consistency with processor consistent synchronization (RCpc) or simple processor consistency [10]. We have recently learned [11] that the version of processor consistency implemented in the DASH system is weaker than that described by Gharachorloo et al. <ref> [12] </ref> and 18 defined here as PCD. That is, the DASH system does actually not implement PCD.
Reference: [13] <author> Phillip B. Gibbons and Michael Merritt. </author> <title> Specifying nonblocking shared memories (extended abstract). </title> <booktitle> In Proceedings of the Fourth Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 306-315. </pages> <publisher> ACM Press, </publisher> <month> June </month> <year> 1992. </year>
Reference-contexts: The DASH implementation of processor consistency uses a partial program order (see Section 5.2 below). In addition, there has been recent interest in the formalization of systems with non-blocking reads, in which the order of local operations may be partial <ref> [13] </ref>. 2 It is assumed that each location has some initial value that is returned by a read of a location with no preceding write.
Reference: [14] <author> Phillip B. Gibbons, Michael Merritt, and Kourosh Gharachorloo. </author> <title> Proving sequential consistency of high-performance shared memories (extended abstract). </title> <booktitle> In Proceedings of the Third Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 292-303. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1991. </year>
Reference: [15] <author> James R. Goodman. </author> <title> Cache consistency and sequential consistency. </title> <type> Technical Report 61, </type> <institution> IEEE Scalable Coherent Interface Working Group, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: Other research [1,5,7,12] has proposed memories that are "hybrids" of weaker and stronger forms of consistency, allowing a program to use a strongly consistent memory only when it is necessary to do so. One influential weakly consistent memory was proposed by Goodman <ref> [15] </ref> and is called processor consistency. The DASH system implements a type of processor consistent memory that is distinct from Goodman's definition; this was described by Gharachorloo et al. [12]. Processor consistency has been cited elsewhere in the literature [5,6,10,14]. <p> This fact raises further questions about the nature and utility of processor consistency, which we discuss in a concluding section. 1 2 2 Background This section discusses Goodman's original definition of processor consistency <ref> [15] </ref> and some interpretations of it [5,6,12]. Goodman's work concentrated on three memory consistency models: strong consistency, cache consistency, and processor consistency. <p> As noted above, there are two such formulations. One was originally presented by Goodman <ref> [15] </ref> and is referred to here as PCG. The other was described by the implementors of the DASH system [12] and is referred to here as PCD. 5.1 Goodman's Processor Consistency Goodman's definition of processor consistency is, in a sense, a combination of PRAM and coherence. <p> to histories shown in earlier figures and place those histories in the structure of memories. 12 7 3 PRAM Coherence PCD SC p: r (x)1 w (y)1 5.4 Disallowing Anomalous Executions in Processor Consis tency The definitions of processor consistency given above correctly capture the definitions originally given by Goodman <ref> [15] </ref> and by Gharachorloo et al. [12]. However, they may allow certain anomalous executions.
Reference: [16] <author> Maurice P. Herlihy and Jeannette M. Wing. </author> <title> Linearizability: A correctness condition for concurrent objects. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12(3) </volume> <pages> 463-492, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: To this end, Lamport [21] defined a memory model called sequential consistency that provided such properties. Lamport later defined a stronger form of memory, called atomic memory [22]; this memory has a correctness condition that was later called linearizability by Herlihy and Wing <ref> [16] </ref>. Unfortunately, it is costly to implement strongly consistent memories such as these in large systems; a simple argument [23] can be used to show that these memories cannot retain low access latency in large systems. This fact represents a significant efficiency problem in distributed shared-memory systems. <p> In Section 5, we precisely characterize the differences between the two definitions. 3 Definitions, Terminology, and Notation This section formally describes the system that underlies our definitions and results. We use a model derived from those used by Misra [24] and by Herlihy and Wing <ref> [16] </ref>. In Sections 4 and 5, we use this model to define various forms of shared memory, including two formulations of processor consistency. We define a system to be a finite set of processors that interact via a shared memory that consists of a finite set of locations. <p> Figure 3 gives a history that is PRAM but not coherent. 3 In a sense, a coherent memory is sequentially consistent on per-location basis. Maintaining this kind of consistency is not sufficient to guarantee full sequential consistency. This contrasts with linearizable memory <ref> [16] </ref>. Linearizable memory is local, meaning that, if each location is linearizable, then the entire memory is.
Reference: [17] <author> Phillip W. Hutto and Mustaque Ahamad. </author> <title> Slow memory: Weakening consistency to enhance concurrency in distributed shared memories. </title> <booktitle> In Proceedings of the Tenth International Conference on Distributed Computing Systems, </booktitle> <month> May </month> <year> 1990. </year> <note> A complete version appears as Technical Report 89/39, </note> <institution> School of Information and Computer Science, Georgia Institute of Technology. </institution>
Reference: [18] <author> Prince Kohli, Gil Neiger, and Mustaque Ahamad. </author> <title> A characterization of scalable shared memories. </title> <type> Technical Report 93/04, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: A further exploration of the relative merits of the two memories are a subject for future research. We have generalized our method <ref> [18] </ref> to encompass more complex memory models such as release consistency. Doing so enabled us to formally show that the programming model developed for RCsc is not adequate for RCpc.
Reference: [19] <author> Leslie Lamport. </author> <title> A new solution of Dijkstra's concurrent programming problem. </title> <journal> Communications of the ACM, </journal> <volume> 17(8) </volume> <pages> 453-455, </pages> <month> August </month> <year> 1974. </year>
Reference-contexts: We prove that Peterson's algorithm for mutual exclusion [25], which does not require cooperation, is correct with both forms of processor consistency defined in the previous section. We then show that Lamport's Bakery algorithm <ref> [19] </ref>, another such algorithm, is not correct with processor consistency. 14 shared need [2] : boolean /* Initially false */ turn : integer local other : boolean whose : integer while true do w (need [i])true w (turn)- repeat whose = r (turn ) other = r (need [-]) until (whose
Reference: [20] <author> Leslie Lamport. </author> <title> Time, clocks, and the ordering of events in a distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 21(7) </volume> <pages> 558-565, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: In a sense, this history exhibits a circularity in causal relationships that should not be allowed. We now formalize this idea. We can define a causal "happens-before" relation between operations similar to that used by Lamport <ref> [20] </ref> in studying systems with message passing. It is a combination of a writes-before relation (similar to, but stronger than, the weak writes-before relation of Section 5.2) and the (appropriate) program order relation.
Reference: [21] <author> Leslie Lamport. </author> <title> How to make a multiprocessor computer that correct executes mul-tiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> Septem-ber </month> <year> 1979. </year>
Reference-contexts: 1 Introduction The abstraction of a memory shared across distributed hardware is attractive because it allows uniform access to local and remote information, thereby simplifying programming. Ideally, a distributed shared memory should provide all the consistency properties of a true shared memory. To this end, Lamport <ref> [21] </ref> defined a memory model called sequential consistency that provided such properties. Lamport later defined a stronger form of memory, called atomic memory [22]; this memory has a correctness condition that was later called linearizability by Herlihy and Wing [16]. <p> Goodman's work concentrated on three memory consistency models: strong consistency, cache consistency, and processor consistency. Strong consistency is sequential consistency as defined by Lamport <ref> [21] </ref>; it guarantees that there is some sequential order of all memory operations that is consistent with the individual executions of all the processors. <p> Hjx (respectively, H p+w jx) contains the subsequences of H (respectively, H p+w ) consisting only of operations on location x. 4 Earlier Memory Models Given the formalism developed in the preceding section, we can define a variety of memory consistency models. This section defines Lamport's sequential consistency <ref> [21] </ref>, the PRAM of Lipton and Sandberg [23], and coherence. The next section uses the same formalism to define two forms of processor consistency.
Reference: [22] <author> Leslie Lamport. </author> <title> On interprocess communication; part I: Basic formalism. </title> <journal> Distributed Computing, </journal> <volume> 1(2) </volume> <pages> 77-85, </pages> <year> 1986. </year>
Reference-contexts: Ideally, a distributed shared memory should provide all the consistency properties of a true shared memory. To this end, Lamport [21] defined a memory model called sequential consistency that provided such properties. Lamport later defined a stronger form of memory, called atomic memory <ref> [22] </ref>; this memory has a correctness condition that was later called linearizability by Herlihy and Wing [16].
Reference: [23] <author> Richard J. Lipton and Jonathan S. Sandberg. </author> <title> PRAM: A scalable shared memory. </title> <type> Technical Report 180-88, </type> <institution> Department of Computer Science, Princeton University, </institution> <month> September </month> <year> 1988. </year>
Reference-contexts: Lamport later defined a stronger form of memory, called atomic memory [22]; this memory has a correctness condition that was later called linearizability by Herlihy and Wing [16]. Unfortunately, it is costly to implement strongly consistent memories such as these in large systems; a simple argument <ref> [23] </ref> can be used to show that these memories cannot retain low access latency in large systems. This fact represents a significant efficiency problem in distributed shared-memory systems. <p> Another argument that processor consistency includes coherence is the fact that processor consistency without coherence would be identical to the PRAM of Lipton and Sandberg <ref> [23] </ref>. Again, it is doubtful that Goodman meant this. Finally, Goodman himself includes a sample execution that he states should not be allowed by processor consistency. This example is given in coherence, then, contrary to the claim by Goodman, this execution would be allowed. <p> This section defines Lamport's sequential consistency [21], the PRAM of Lipton and Sandberg <ref> [23] </ref>, and coherence. The next section uses the same formalism to define two forms of processor consistency.
Reference: [24] <author> Jayadev Misra. </author> <title> Axioms for memory access in asynchronous hardware systems. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(1) </volume> <pages> 142-153, </pages> <month> Jan-uary </month> <year> 1986. </year> <title> The Power of Processor Consistency 21 </title>
Reference-contexts: In Section 2, we discuss a possible basis of such confusions. In this paper, we present a precise characterization of processor consistency by developing non-operational definitions. We do so by using a formal method similar to those used by Misra <ref> [24] </ref> and by Herlihy and Wing for defining strongly consistent memories. This method allows us to explicitly demonstrate how the two definitions of processor consistency relate to each other and to other types of shared memory. <p> In Section 5, we precisely characterize the differences between the two definitions. 3 Definitions, Terminology, and Notation This section formally describes the system that underlies our definitions and results. We use a model derived from those used by Misra <ref> [24] </ref> and by Herlihy and Wing [16]. In Sections 4 and 5, we use this model to define various forms of shared memory, including two formulations of processor consistency.
Reference: [25] <author> Gary L. Peterson. </author> <title> Myths about the mutual exclusion problem. </title> <journal> Information Processing Letters, </journal> <volume> 12(3) </volume> <pages> 115-116, </pages> <month> June </month> <year> 1981. </year>
Reference-contexts: Attiya and Friedman [5] claimed that processor consistency lacked the power to implement mutual exclusion without cooperation (recall that they did not attribute coherence to processor consistency). We prove that Peterson's algorithm for mutual exclusion <ref> [25] </ref>, which does not require cooperation, is correct with both forms of processor consistency defined in the previous section.
References-found: 25

