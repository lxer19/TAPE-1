URL: http://polaris.cs.uiuc.edu/reports/1388.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Email: Email: [torrella,koufaty]@csrd.uiuc.edu  
Title: Comparing the Performance and Programmability of the DASH and Cedar Multiprocessors for Scientific Loads  
Author: Josep Torrellas and David Koufaty 
Keyword: Shared-Memory Multiprocessors, Performance Evaluation, Machine Organization, Experimental Analysis, Parallel Program Performance.  
Address: IL 61801  
Affiliation: Center for Supercomputing Research and Development and Computer Science Department University of Illinois at Urbana-Champaign,  
Abstract: Although the performance of scalable shared-memory multiprocessors is now better understood, systems designers still make surprisingly different choices on major issues like support for cache coherence, loop versus task model of parallelism, or organization of the processors in the machine. To gain insight into the impact of some of these choices, this paper compares DASH and Cedar. We focus on the performance and programmability impact of the different cache coherence support, model of parallelism emphasized, and support for clusters of processors. For the comparison, we use small kernels, parallelized versions of old serial codes, and parallel scientific applications. We observe that the lack of hardware support for cache coherence is often not a major problem for common medium-grain scientific applications. Often, applications only need simple manual changes, probably soon doable by compilers, involving data copying and compacting. More irregular symbolic applications, however, need extensive rewriting. Furthermore, while similarly tuned codes written using loop- or task-based parallelism perform similarly, loop-based parallelism has the advantage that very simple codes often perform equally well. In addition, it is possible to parallelize serial codes using loop-based parallelism, and not have to understand the serial code. Finally, we conclude that the cluster organization of Cedar and DASH is at best difficult to exploit. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> R. Alverson et al. </editor> <booktitle> The Tera Computer System. In Proceedings of the 1990 International Conference on Supercomputing, </booktitle> <pages> pages 1-6, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Several such machines have been built or are currently being built, for example RP3 [15], Cedar [11], KSR1 [10], DASH [12], DDM1 [8], Alewife [5], NYU Ultracomputer [7], or the Tera computer system <ref> [1] </ref>. While all these machines support the shared-memory paradigm, they have substantial differences. For example, some of them use hardware schemes to maintain the caches coherent, while others rely on the compiler or the programmer to do so.
Reference: [2] <author> J. B. Andrews. </author> <title> Parallelization of TRFD. Internal Document, </title> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: Consequently, the Cedar versions used are the best a parallelizing compiler can generate [6]. For DASH, the versions have been extensively rewritten and hand-optimized: MDG is the Splash [18] code tuned for DASH; TRFD is the DASH port of a hand-parallelized and optimized version of the serial code <ref> [2] </ref>. The generation 9 of all these versions involved heavy use of program profiling. In the following, we first examine the programming effort involved and then compare the performance achieved. 5.1 Programming Effort Surprisingly, the lack of hardware support for cache coherence in Cedar introduces little programming complexity.
Reference: [3] <author> C. Baldwin and E. Gallopoulos. </author> <title> High Accuracy Solution of Parabolic Partial Differential Equations. </title> <note> In preparation, </note> <month> January </month> <year> 1994. </year>
Reference-contexts: Each of the two programs was parallelized independently for the two machines by different people. Both MDG and TRFD run the problem size provided by Perfect Club. However, we reduced the number of iterations in MDG from 100 to 10. PPDE is a parabolic partial differential equations solver <ref> [3] </ref>. The four parallel kernels, MatInc, SOR, LU, and MatMul, were written by us. Both MatInc and SOR divide a matrix into fixed per-processor regions. In MatInc, there is no communication among the processors; in SOR, processors communicate in the boundaries of their regions without synchronizing.
Reference: [4] <author> M. Berry et al. </author> <title> The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference-contexts: Kernel LU 300 LU decomposition of a dense matrix. MatMul 200 Matrix multipl. of dense matrices using blocks and data copying. second set of applications are large parallel applications. MDG and TRFD are parallelized versions of two Perfect Club codes <ref> [4] </ref>. Each of the two programs was parallelized independently for the two machines by different people. Both MDG and TRFD run the problem size provided by Perfect Club. However, we reduced the number of iterations in MDG from 100 to 10. PPDE is a parabolic partial differential equations solver [3]. <p> We focus on how these two issues affects the parallelization. The applications that we choose are two Perfect Club <ref> [4] </ref> applications: MDG and TRFD. For our analysis, we decided to use the best versions that programmers could produce under the software environments available in the two machines as of January 1994. Consequently, the Cedar versions used are the best a parallelizing compiler can generate [6].
Reference: [5] <author> D. Chaiken, J. Kubiatowicz, and A. Agarwal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 224-234, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Scalable shared-memory multiprocessors are attractive because they achieve the benefits of large-scale processing without surrendering much programmability. Several such machines have been built or are currently being built, for example RP3 [15], Cedar [11], KSR1 [10], DASH [12], DDM1 [8], Alewife <ref> [5] </ref>, NYU Ultracomputer [7], or the Tera computer system [1]. While all these machines support the shared-memory paradigm, they have substantial differences. For example, some of them use hardware schemes to maintain the caches coherent, while others rely on the compiler or the programmer to do so.
Reference: [6] <author> R. Eigenmann, J. Hoeflinger, G. Jaxon, and D. Padua. </author> <title> The Cedar Fortran Project. </title> <type> Technical Report 1262, </type> <institution> Center for Supercomputing Research and Development, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: For our analysis, we decided to use the best versions that programmers could produce under the software environments available in the two machines as of January 1994. Consequently, the Cedar versions used are the best a parallelizing compiler can generate <ref> [6] </ref>. For DASH, the versions have been extensively rewritten and hand-optimized: MDG is the Splash [18] code tuned for DASH; TRFD is the DASH port of a hand-parallelized and optimized version of the serial code [2]. The generation 9 of all these versions involved heavy use of program profiling.
Reference: [7] <author> J. Elder, A. Gottlieb, C. K. Kruskal, K. P. McAuliffe, L. Rudolph, M. Snir, P. Teller, and J. Wilson. </author> <title> 22 Issues Related to MIMD, Shared-Memory Computers: The NYU Ultracomputer Approach. </title> <booktitle> In Proceedings of the 12th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 126-135, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: 1 Introduction Scalable shared-memory multiprocessors are attractive because they achieve the benefits of large-scale processing without surrendering much programmability. Several such machines have been built or are currently being built, for example RP3 [15], Cedar [11], KSR1 [10], DASH [12], DDM1 [8], Alewife [5], NYU Ultracomputer <ref> [7] </ref>, or the Tera computer system [1]. While all these machines support the shared-memory paradigm, they have substantial differences. For example, some of them use hardware schemes to maintain the caches coherent, while others rely on the compiler or the programmer to do so.
Reference: [8] <author> E. Hagersten, A. Landin, and S. Haridi. </author> <title> DDM a Cache-Only Memory Architecture. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 44-54, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Scalable shared-memory multiprocessors are attractive because they achieve the benefits of large-scale processing without surrendering much programmability. Several such machines have been built or are currently being built, for example RP3 [15], Cedar [11], KSR1 [10], DASH [12], DDM1 <ref> [8] </ref>, Alewife [5], NYU Ultracomputer [7], or the Tera computer system [1]. While all these machines support the shared-memory paradigm, they have substantial differences. For example, some of them use hardware schemes to maintain the caches coherent, while others rely on the compiler or the programmer to do so. <p> While all these machines support the shared-memory paradigm, they have substantial differences. For example, some of them use hardware schemes to maintain the caches coherent, while others rely on the compiler or the programmer to do so. Among the former, some are all-cache architectures <ref> [8, 10] </ref> while others are not. Furthermore, some machines organize the processors in clusters, while others do not. In addition to these hardware issues, these fl This work was supported in part by the National Science Foundation under grant nos.
Reference: [9] <author> J. Hoeflinger. </author> <title> Cedar Fortran Programmer's Handbook. </title> <type> Technical Report 1157, </type> <institution> Center for Supercomputing Research and Development, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: Such support includes a fast synchronization bus enabled with advance and await instructions, and a shared cache that allows processors to communicate in 3 cycles. Furthermore, Cedar relies to a certain extent on compilers that parallelize the loops in the code. Finally, a parallel Fortran dialect called Cedar Fortran <ref> [9] </ref> has been developed, with heavy emphasis on parallel loop constructs. DASH is currently used almost exclusively under a task-based model. The machine does have a per-cluster synchronization bus. However, it is slow. <p> In our experiments, we consider two problem sizes per application: one that fits in the caches of both machines and one that does not fit in any of the two. Overall, all DASH applications use the ANL m4 macros [14], while all Cedar applications use Cedar Fortran <ref> [9] </ref>, a Fortran dialect with parallel loop constructs. 4 Comparing the Performance with Microkernels We start the comparison of the two machines by examining the performance of three basic architectural primitives: floating and fixed point arithmetic, synchronization activity, and the memory system. <p> This approach, although attractive, is difficult to use successfully. Indeed, it requires the programmer to carefully partition and tune the application. This can be made easier by the programming language. In Cedar, for example, Cedar Fortran has the SDOALL (spread doall) and CDOALL (cluster doall) constructs <ref> [9] </ref>. SDOALL passes a set of loop iterations to each cluster. The programmer can then declare per-cluster private variables. 17 CDOALL spreads the work among the processors in the cluster. The processors in the cluster can use the per-cluster private variables and share more data among themselves.
Reference: [10] <author> H. Burkhardt III et al. </author> <title> Overview of the KSR1 Computer System. </title> <type> Technical Report 9202001, </type> <institution> Kendall Square Research, </institution> <address> Boston, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Scalable shared-memory multiprocessors are attractive because they achieve the benefits of large-scale processing without surrendering much programmability. Several such machines have been built or are currently being built, for example RP3 [15], Cedar [11], KSR1 <ref> [10] </ref>, DASH [12], DDM1 [8], Alewife [5], NYU Ultracomputer [7], or the Tera computer system [1]. While all these machines support the shared-memory paradigm, they have substantial differences. <p> While all these machines support the shared-memory paradigm, they have substantial differences. For example, some of them use hardware schemes to maintain the caches coherent, while others rely on the compiler or the programmer to do so. Among the former, some are all-cache architectures <ref> [8, 10] </ref> while others are not. Furthermore, some machines organize the processors in clusters, while others do not. In addition to these hardware issues, these fl This work was supported in part by the National Science Foundation under grant nos.
Reference: [11] <author> D. Kuck et al. </author> <title> The Cedar System and an Initial Performance Study. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 213-224, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Scalable shared-memory multiprocessors are attractive because they achieve the benefits of large-scale processing without surrendering much programmability. Several such machines have been built or are currently being built, for example RP3 [15], Cedar <ref> [11] </ref>, KSR1 [10], DASH [12], DDM1 [8], Alewife [5], NYU Ultracomputer [7], or the Tera computer system [1]. While all these machines support the shared-memory paradigm, they have substantial differences. <p> Such cache is called remote access cache (RAC) [12]. We will study its effectiveness. 2.2 The Cedar Machine Cedar is a 4-cluster vector multiprocessor developed at the Center for Supercomputing Research and Development, University of Illinois <ref> [11] </ref>. Each cluster is an 8-processor bus-based Alliant FX/8. Each processor has a 16 Kbyte direct-mapped instruction cache. In addition, all processors in a cluster share a 512 Kbyte direct-mapped cache and 64 Mbytes of memory visible only to the cluster. <p> Machine Fix. Add Fix. Mult. Fix. Divide Scalar Vector Scalar Vector Scalar Vector (ns) (ns) (ns) (ns) (ns) (ns) DASH 166 - 554 - 1233 - Cedar 1930 600 2440 595 5609 2180 4.2 Synchronization Performance While both Cedar and DASH have support for advanced synchronization <ref> [11, 12] </ref>, most applications do not use it. Hence, our Sync microkernel only tests the latency of simple test and set operations. This is done by measuring the time to perform a lock acquire followed by a release in a tight loop executed by several processors.
Reference: [12] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam. </author> <title> The Stanford Dash Multiprocessor. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Scalable shared-memory multiprocessors are attractive because they achieve the benefits of large-scale processing without surrendering much programmability. Several such machines have been built or are currently being built, for example RP3 [15], Cedar [11], KSR1 [10], DASH <ref> [12] </ref>, DDM1 [8], Alewife [5], NYU Ultracomputer [7], or the Tera computer system [1]. While all these machines support the shared-memory paradigm, they have substantial differences. <p> Throughout this paper, we consider a 32-processor configuration of the two machines. 2.1 The DASH Machine DASH is a cluster-based machine developed at Stanford University <ref> [12, 13] </ref>. Each cluster is a 4-processor bus-based Silicon Graphics Power System/340, with 32 Mbytes of memory visible to all processors in the machine. The clusters are interconnected with a 2-D mesh. <p> While the directory-based protocol has many types of transactions and optimizations, we only note 2 here that the machine has a cluster cache designed to increase the effectiveness of the clusters. Such cache is called remote access cache (RAC) <ref> [12] </ref>. We will study its effectiveness. 2.2 The Cedar Machine Cedar is a 4-cluster vector multiprocessor developed at the Center for Supercomputing Research and Development, University of Illinois [11]. Each cluster is an 8-processor bus-based Alliant FX/8. Each processor has a 16 Kbyte direct-mapped instruction cache. <p> These are the "base" performance gains of clustering. Both DASH and Cedar benefit from them. In addition, the two machines have different, extra hardware to further to exploit clustering. Indeed, DASH supports local cache to cache or remote access cache (RAC) to cache sharing <ref> [12] </ref> and remote request combining [12]. We will call these effects local sharing. Cedar has shared caches and synchronization buses. In DASH, local sharing allows a processor in a cluster to access from the cache of another processor in the same cluster, data whose home is remote. <p> These are the "base" performance gains of clustering. Both DASH and Cedar benefit from them. In addition, the two machines have different, extra hardware to further to exploit clustering. Indeed, DASH supports local cache to cache or remote access cache (RAC) to cache sharing <ref> [12] </ref> and remote request combining [12]. We will call these effects local sharing. Cedar has shared caches and synchronization buses. In DASH, local sharing allows a processor in a cluster to access from the cache of another processor in the same cluster, data whose home is remote. The access occurs without notifying the home. <p> Machine Fix. Add Fix. Mult. Fix. Divide Scalar Vector Scalar Vector Scalar Vector (ns) (ns) (ns) (ns) (ns) (ns) DASH 166 - 554 - 1233 - Cedar 1930 600 2440 595 5609 2180 4.2 Synchronization Performance While both Cedar and DASH have support for advanced synchronization <ref> [11, 12] </ref>, most applications do not use it. Hence, our Sync microkernel only tests the latency of simple test and set operations. This is done by measuring the time to perform a lock acquire followed by a release in a tight loop executed by several processors.
Reference: [13] <author> D. Lenoski, J. Laudon, T. Joe, D. Nakahira, L. Stevens, A. Gupta, and J. Hennessy. </author> <title> The DASH Prototype: Implementation and Performance. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 92-103, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Throughout this paper, we consider a 32-processor configuration of the two machines. 2.1 The DASH Machine DASH is a cluster-based machine developed at Stanford University <ref> [12, 13] </ref>. Each cluster is a 4-processor bus-based Silicon Graphics Power System/340, with 32 Mbytes of memory visible to all processors in the machine. The clusters are interconnected with a 2-D mesh. <p> The benefits of DASH's approach are, of course, ease of programming: the programmer can largely ignore data placement/transfer issues and still achieve high performance in most of the cases. The disadvantage is the complex hardware for cache coherence (about 16% of the logic gates in DASH <ref> [13] </ref>). This makes the machine harder to design and debug. Cedar's approach, instead, has no hardware cost. However, it requires a potentially high programming effort moving data between memory levels.
Reference: [14] <editor> E. Lusk, R. Overbeek, et al. </editor> <title> Portable Programs for Parallel Processors. </title> <publisher> Holt, Rinehart, and Winston, Inc., </publisher> <address> New York, NY, </address> <year> 1987. </year>
Reference-contexts: In our experiments, we consider two problem sizes per application: one that fits in the caches of both machines and one that does not fit in any of the two. Overall, all DASH applications use the ANL m4 macros <ref> [14] </ref>, while all Cedar applications use Cedar Fortran [9], a Fortran dialect with parallel loop constructs. 4 Comparing the Performance with Microkernels We start the comparison of the two machines by examining the performance of three basic architectural primitives: floating and fixed point arithmetic, synchronization activity, and the memory system. <p> For DASH, TRFD was rewritten keeping an explicit loop-based structure, while MDG had been completely restructured and written in a task-based form. We felt that the TRFD approach was much easier. It implied using getsub, the distributed loop construct of the ANL macros <ref> [14] </ref>, and adding extra synchronization and if statements in the serial sections to make sure that only the master processor executed them. The changes still required a good understanding of the program, to make sure that variables in the serial and parallel sections were consistent.
Reference: [15] <author> G. Pfister, W. Brantley, D. George, S. Harvey, W. Kleinfelder, K. McAuliffe, E. Melton, A. Norton, and J. Weiss. </author> <title> The IBM Research Parallel Processor Prototype (RP3): Introduction and Architecture. </title> <booktitle> In Proceedings of the 1985 International Conference on Parallel Processing, </booktitle> <pages> pages 764-771, </pages> <year> 1985. </year>
Reference-contexts: 1 Introduction Scalable shared-memory multiprocessors are attractive because they achieve the benefits of large-scale processing without surrendering much programmability. Several such machines have been built or are currently being built, for example RP3 <ref> [15] </ref>, Cedar [11], KSR1 [10], DASH [12], DDM1 [8], Alewife [5], NYU Ultracomputer [7], or the Tera computer system [1]. While all these machines support the shared-memory paradigm, they have substantial differences.
Reference: [16] <author> R. H. Saavedra, R. S. Gaines, and M. J. Carlton. </author> <title> Micro Benchmark Analysis of the KSR1. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 202-213, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Because of the magnitude of the differences, we would like to see how these features impact the performance of the machines. For example, the effectiveness of the all-cache feature has been studied elsewhere <ref> [16, 17] </ref>. The approach that we follow is to compare real machines whose design involved very different choices. Studies comparing real machines, however, are challenging. Firstly, they require porting the same application benchmarks with similar degree of optimization to all the machines compared. <p> We suspect that this unfairness affects several locks that have high contention in our applications. 4.3 Memory System Performance Our goal is to determine the latency of accesses to the different layers of the memory hierarchy. To do so, we use a methodology introduced by Saavedra et al. <ref> [16] </ref>: one processor reads the elements of a 1-dimension array with a given stride in a loop. In the beginning, we will be hitting in the cache. 6 However, as we increase the stride and the length of the array, we will be causing cache misses and eventually TLB misses. <p> In Chart (b), they are distributed among the 8 DASH clusters; in (d) and (f), they are in Cedar's global memory. We first consider the DASH charts ((a)-(b)), although since they were discussed by Saavedra et al. <ref> [16] </ref>, we will not delve into much detail. In levels A and B, the data is supplied by the first and second level caches respectively. Therefore, the difference between levels B and A is the latency of an access that misses in the first-level cache and hits in the second.
Reference: [17] <author> J. P. Singh, T. Joe, A. Gupta, and J. L. Hennessy. </author> <title> An Empirical Comparison of the Kendall Square Research KSR-1 and Stanford DASH Multiprocessors. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 214-225, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Because of the magnitude of the differences, we would like to see how these features impact the performance of the machines. For example, the effectiveness of the all-cache feature has been studied elsewhere <ref> [16, 17] </ref>. The approach that we follow is to compare real machines whose design involved very different choices. Studies comparing real machines, however, are challenging. Firstly, they require porting the same application benchmarks with similar degree of optimization to all the machines compared.
Reference: [18] <author> J. P. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: Consequently, the Cedar versions used are the best a parallelizing compiler can generate [6]. For DASH, the versions have been extensively rewritten and hand-optimized: MDG is the Splash <ref> [18] </ref> code tuned for DASH; TRFD is the DASH port of a hand-parallelized and optimized version of the serial code [2]. The generation 9 of all these versions involved heavy use of program profiling. <p> These conclusions are likely to hold for a sizable number of these programs. However, they do not apply to symbolic applications. Indeed, we have tried similar changes on LocusRoute from Splash <ref> [18] </ref>; porting LocusRoute to a machine with non-coherent local memories like Cedar requires significant programmer effort. 7 Clusters of Processors The final issue that we examine is the different way in which the two machines organize the processors in clusters.
Reference: [19] <author> J. Torrellas and Z. Zhang. </author> <title> The Performance of the Cedar Multistage Switching Network. </title> <booktitle> In Supercomputing '94, </booktitle> <month> November </month> <year> 1994. </year> <month> 23 </month>
Reference-contexts: In addition, all processors in a cluster share a 512 Kbyte direct-mapped cache and 64 Mbytes of memory visible only to the cluster. All processors in the machine are connected to 64 Mbytes of shared memory via forward and return omega networks <ref> [19] </ref>. Processors cycle at 170 ns, while networks and shared-memory modules cycle at 85 ns. Each cluster of Cedar has a synchronization bus. This bus provides fast synchronization that is used to speed-up the self-scheduling of iterations in parallel loops 1 . In addition, each processors has a prefetch buffer.
Reference: [20] <author> W. Weber and A. Gupta. </author> <title> Analysis of Cache Invalidation Patterns in Multiprocessors. </title> <booktitle> In Proceedings of the 3rd International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 243-256, </pages> <month> April </month> <year> 1989. </year> <month> 24 </month>
Reference-contexts: As a result, the codes for Cedar and DASH are very similar. * Data structures that had more than one access pattern were decomposed and handled differently. For example, if one field of the data structure was fine-grained shared and another was shared in a migratory <ref> [20] </ref> manner, the former was left in global memory, while the latter was successively copied among cluster memories. 6.2.2 Performance Results The performance of the tuned task-based codes for DASH and tuned loop-based codes for Cedar is compared in Figure 6.
References-found: 20

