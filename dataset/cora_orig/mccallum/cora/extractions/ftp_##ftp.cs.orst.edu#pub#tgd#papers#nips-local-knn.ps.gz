URL: ftp://ftp.cs.orst.edu/pub/tgd/papers/nips-local-knn.ps.gz
Refering-URL: http://www.cs.orst.edu/~tgd/cv/pubs.html
Root-URL: 
Title: Locally Adaptive Nearest Neighbor Algorithms  
Author: Dietrich Wettschereck and Thomas Dietterich Cowan, J.D., Tesauro, G., and Alspector, J., 
Date: 1994  
Note: To appear in: Advances in Neural Information Processing Systems 6 Edited by  Morgan Kaufmann Publishers,  
Address: Corvallis, OR 97331-3202  San Mateo, CA,  
Affiliation: Department of Computer Science Oregon State University  
Abstract-found: 0
Intro-found: 1
Reference: <author> Aha, D.W. </author> <year> (1990). </year> <title> A Study of Instance-Based Algorithms for Supervised Learning Tasks. </title> <type> Technical Report, </type> <institution> University of California, Irvine. </institution>
Reference: <author> Bottou, L., Vapnik, V. </author> <year> (1992). </year> <title> Local Learning Algorithms. </title> <journal> Neural Computation, </journal> <volume> 4(6), </volume> <pages> 888-900. </pages>
Reference: <author> Dasarathy, B.V. </author> <year> (1991). </year> <title> Nearest Neighbor(NN) Norms: NN Pattern Classification Techniques. </title> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: 1 Introduction The k-nearest neighbor algorithm <ref> (kNN, Dasarathy, 1991) </ref> is one of the most venerable algorithms in machine learning. The entire training set is stored in memory. A new example is classified with the class of the majority of the k nearest neighbors among all stored training examples.
Reference: <author> Detrano, R., Janosi, A., Steinbrunn, W., Pfisterer, M., Schmid, K., Sandhu, S., Guppy, K., Lee, S. & Froelicher, V. </author> <year> (1989). </year> <title> Rapid searches for complex patterns in biological molecules. </title> <journal> American Journal of Cardiology, </journal> <volume> 64, </volume> <pages> 304-310. </pages>
Reference: <author> Murphy, P.M. & Aha, D.W. </author> <year> (1991). </year> <title> UCI Repository of machine learning databases [Machine-readable data repository]. </title> <type> Technical Report, </type> <institution> University of California, Irvine. </institution>
Reference-contexts: Data sets for these domains were obtained from the UC-Irvine repository of machine learning databases <ref> (Murphy & Aha, 1991, Aha, 1990, Detrano et al., 1989) </ref>. Results displayed in Figure 2 indicate that in most data sets which are commonly used to evaluate machine learning algorithms, local nearest neighbor methods have only minor impact on the performance of kNN.
Reference: <author> Weiss, </author> <title> S.M., & Kulikowski, </title> <address> C.A. </address> <year> (1991). </year> <title> Computer Systems that learn. </title> <address> San Mateo California: </address> <publisher> Morgan Kaufmann Publishers, INC. </publisher>
Reference-contexts: The procedure was repeated a total of 25 times to reduce statistical variation. In each experiment, the algorithms being compared were trained (and tested) on identical data sets to ensure that differences in performance were due entirely to the algorithms. Leave-one-out cross-validation <ref> (Weiss & Kulikowski, 1991) </ref> was employed in all experiments to estimate optimal settings for free parameters such as k in kNN and M in localKNN. 1 Rival Penalized Competitive Learning is a straightforward modification of the well known k-means clustering algorithm.
Reference: <author> Xu, L., Krzyzak, A., & Oja, E. </author> <year> (1993). </year> <title> Rival Penalized Competitive Learning for Clustering Analysis, RBF Net, and Curve Detection IEEE Transactions on Neural Networks, 4(4),636-649. displayed curves (i.e. all data points lie on either of the two curves). Class labels were flipped with increasing probabilities to a maximum noise level of approximately 45% at the respective ends of the two lines. Listed at the bottom is performance of kNN and localKNN unrestricted within different regions of the input space and for the entire input space. </title>
Reference-contexts: Otherwise, q is assigned to class C 2 . Generalization of that procedure to any number of output classes is straightforward. * localKNN one k per cluster An unsupervised cluster algorithm <ref> (RPCL, 1 Xu et al., 1993) </ref> is used to determine clusters of input data. A single k value is determined for each cluster.
References-found: 7

