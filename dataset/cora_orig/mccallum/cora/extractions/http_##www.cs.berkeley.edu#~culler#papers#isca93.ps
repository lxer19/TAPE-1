URL: http://www.cs.berkeley.edu/~culler/papers/isca93.ps
Refering-URL: http://www.cs.berkeley.edu/~culler/papers/
Root-URL: 
Email: fellens,billdg@ai.mit.edu ftamg@cs.berkeley.edu  
Phone: 617-253-7710, 617-253-6043 510-643-7572  
Title: Evaluation of Mechanisms for Fine-Grained Parallel Programs in the J-Machine and the CM-5  
Author: Ellen Spertus Seth Copen Goldstein William J. Dally Klaus Erik Schauser Thorsten von Eicken David E. Culler 
Address: 545 Technology Square  Cambridge, MA 02139 Berkeley, CA 94720  
Affiliation: MIT Artifical Intelligence Laboratory Computer Science Division EECS  University of California  
Abstract: Experimental and commercial parallel machines have matured to a point where it is possible to quantify the performance enhancement due to the novel mechanisms supporting fine-grain parallel programs in the experimental machines. The MIT J-Machine provides a register-level message send, autonomous message receive and dispatch, a prioritized scheduler with multiple contexts, and tagged memory with fast traps. The Thinking Machines CM-5 is essentially a collection of conventional processors with user-level messages. However, it is difficult to establish a meaningful workload for a comparison that adequately accounts for compiler optimizations which specifically address the lack of available mechanisms. This paper uses an abstract machine approach in a relative evaluation of the J-Machine and CM-5. High-level parallel programs are translated by a single optimizing compiler to a fine-grained abstract parallel machine, TAM. The final compilation step is unique to each architecture and optimizes for specifics of the architecture. Determining the cost of each primitive on the machines weighted by their dynamic frequency in parallel programs, provides a quantitative analysis of the effectiveness of the mechanisms. The analysis addresses the individual mechanisms and their synergy. Efficient processor/network coupling proves valuable. The J-Machine message dispatch is reduced in value by the lack of support for atomic operations necerssary for the scheduling levels to co-operate. Multiple hardware contexts are of small value when the contexts co-operate and the compiler can partition the register set. Tagged memory provides little gain. The performance of the overall system is strongly influenced by the performance of the memory system. Fine-grain parallel programs are control intensive. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, B. Lim, D. Kranz, and J. Kubiatowicz. </author> <month> April: </month> <title> A processor architecture for multiprocessing. </title> <booktitle> In Proc. 17th Annual Int'l Symp. on Comp. Arch., </booktitle> <pages> pages 104-114, </pages> <address> Seattle, WA, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Several experimental parallel architectures have been developed in recent years to demonstrate novel hardware mechanisms that may enhance the performance of programs written in emerging parallel languages. For example, Monsoon [17] focuses on Id90 [15], J-Machine [10, 11] on CST [13], Alewife <ref> [1] </ref> on Mul-T, CM-5 [20] on Fortran90, and Dash [14] and KSR-1 on extensions to C and Fortran.
Reference: [2] <author> Arvind and K. Ekanadham. </author> <title> Future Scientific Programming on Parallel Machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5(5) </volume> <pages> 460-493, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: The input is a list of random numbers. Gamteb is a Monte Carlo neutron transport code [6]. It is highly recursive with many conditionals. Paraffins [3] enumerates the distinct isomers of paraffins. Simple is a hydrodynamics and heat conduction code widely used as an application benchmark, rewritten in Id90 <ref> [7, 2] </ref>. Speech determines cepstral coefficients for speech processing. MMT is a simple matrix operation test using 4x4 blocks; two double precision identity matrices are created, multiplied, and subtracted from a third. The final column shows the arithmetic mean of the distributions.
Reference: [3] <author> Arvind, S. K. Heller, and R. S. Nikhil. </author> <title> Programming Generality and Parallel Computers. </title> <booktitle> In Proc. of the Fourth Int. Symp. on Biological and Artificial Intelligence Systems, </booktitle> <pages> pages 255-286. </pages> <address> ESCOM (Leider), Trento, Italy, </address> <month> September </month> <year> 1988. </year>
Reference-contexts: Six benchmark programs ranging from 50 to 1,100 lines are used. QS is a simple quick-sort using accumulation lists. The input is a list of random numbers. Gamteb is a Monte Carlo neutron transport code [6]. It is highly recursive with many conditionals. Paraffins <ref> [3] </ref> enumerates the distinct isomers of paraffins. Simple is a hydrodynamics and heat conduction code widely used as an application benchmark, rewritten in Id90 [7, 2]. Speech determines cepstral coefficients for speech processing.
Reference: [4] <author> D. H. Bailey and et. al. </author> <title> The NAS Parallel Benchmarks Summary and Preliminary Results. </title> <booktitle> In Proc. Supercomputing '91, </booktitle> <month> November </month> <year> 1991. </year> <month> November 6, </month> <note> 1992 - 14 : 59 DRAFT 24 </note>
Reference-contexts: There is not even a consensus on the programming languages of choice. Where benchmarks exist, they have been developed specifically for the machine which they are intended to evaluate [19, 12] or specifically avoid emerging languages which novel mechanism may bring into practical reach <ref> [4] </ref>. It is also difficult to obtain high-quality compilers for such new languages on more than one machine, yet it is well understood that the architectural support can only be evaluated in the context of sophisticated compilation, rather than direct execution of high level constructs.
Reference: [5] <author> Paul S. Barth, Rishiyur S. Nikhil, and Arvind. M-Structures: </author> <title> Extending a Parallel, Non-strict, Functional Language with State. </title> <booktitle> In Conf. on Functional Programming Languages and Computer Architecture, </booktitle> <address> Cambridge, MA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: On each processor, all frames holding enabled threads are linked into a ready queue. Global data structures in TAM provide synchronization on a per-element basis to support I-structure and M-structure semantics <ref> [5, 15] </ref>. In particular, reads of empty elements are deferred until the corresponding write occurs. I-structures support write-once semantics while M-structures allow multiple writes to an element. Accesses to the data structures are split-phase and send a request message to the processor containing the data.
Reference: [6] <author> P. J. Burns, M. Christon, R. Schweitzer, O. M. Lubeck, H. J. Wasserman, M. L. Simmons, and D. V. Pryor. </author> <title> Vectorization of Monte-Carlo Particle Transport: An Architectural Study using the LANL Benchmark Gamteb. </title> <booktitle> In Proc. Supercomputing '89. IEEE Computer Society and ACM SIGARCH, </booktitle> <address> New York, NY, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: Six benchmark programs ranging from 50 to 1,100 lines are used. QS is a simple quick-sort using accumulation lists. The input is a list of random numbers. Gamteb is a Monte Carlo neutron transport code <ref> [6] </ref>. It is highly recursive with many conditionals. Paraffins [3] enumerates the distinct isomers of paraffins. Simple is a hydrodynamics and heat conduction code widely used as an application benchmark, rewritten in Id90 [7, 2]. Speech determines cepstral coefficients for speech processing.
Reference: [7] <author> W. P. Crowley, C. P. Hendrickson, and T. E. Rudy. </author> <title> The SIMPLE code. </title> <type> Technical Report UCID 17715, </type> <institution> Lawrence Livermore Laboratory, </institution> <month> February </month> <year> 1978. </year>
Reference-contexts: The input is a list of random numbers. Gamteb is a Monte Carlo neutron transport code [6]. It is highly recursive with many conditionals. Paraffins [3] enumerates the distinct isomers of paraffins. Simple is a hydrodynamics and heat conduction code widely used as an application benchmark, rewritten in Id90 <ref> [7, 2] </ref>. Speech determines cepstral coefficients for speech processing. MMT is a simple matrix operation test using 4x4 blocks; two double precision identity matrices are created, multiplied, and subtracted from a third. The final column shows the arithmetic mean of the distributions.
Reference: [8] <author> D. Culler, A. Sah, K. Schauser, T. von Eicken, and J. Wawrzynek. </author> <title> Fine-grain Parallelism with Minimal Hardware Support: A Compiler-Controlled Threaded Abstract Machine. </title> <booktitle> In Proc. of 4th Int. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Santa-Clara, CA, </address> <month> April </month> <year> 1991. </year> <note> (Also available as Technical Report UCB/CSD 91/591, </note> <institution> CS Div., University of California at Berkeley). </institution>
Reference-contexts: The compiler performs a variety of high-level optimizations in translating the language down to code for a simple abstract machine, TAM <ref> [8] </ref>. The TAM code is identical for the two machines, controlling for effects of high level optimizations. The translator from TAM code to machine language, however, employs a variety of machine specific optimizations reflecting the most advantageous use of the available mechanisms.
Reference: [9] <author> D. E. Culler. </author> <title> Managing Parallelism and Resources in Scientific Dataflow Programs. </title> <type> Technical Report 446, </type> <institution> MIT Lab for Comp. Sci., </institution> <month> March </month> <year> 1990. </year> <type> (PhD Thesis, </type> <institution> Dept. of EECS, MIT). </institution>
Reference-contexts: Program code is placed on every processor, but a given code-block invocation takes place on a single processor. Because the compiler may pull loops out into separate code-blocks, these can be spread across the machine to implement parallel loops <ref> [9] </ref>. The memory on each processor is divided into two areas. One holds small arrays and activation frames. The other holds large arrays, which are spread across all the processors such that logically consecutive elements are on different processors.
Reference: [10] <author> William J. Dally. </author> <title> The J-Machine system. </title> <editor> In Patrick Winston and Sarah A. Shellard, editors, </editor> <booktitle> Artificial Intelligence at MIT: Expanding Frontiers, chapter 21, </booktitle> <pages> pages 536-569. </pages> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: 1 Introduction Several experimental parallel architectures have been developed in recent years to demonstrate novel hardware mechanisms that may enhance the performance of programs written in emerging parallel languages. For example, Monsoon [17] focuses on Id90 [15], J-Machine <ref> [10, 11] </ref> on CST [13], Alewife [1] on Mul-T, CM-5 [20] on Fortran90, and Dash [14] and KSR-1 on extensions to C and Fortran.
Reference: [11] <author> William J. Dally, Roy Davidson, J. A. Stuart Fiske, Greg Fyler, John S. Keen, Richard A. Lethin, Michael Noakes, and Peter R. Nuth. </author> <title> The Message-Driven Processor: A Multicomputer Processing Node with Efficient Mechanisms. </title> <booktitle> IEEE Micro, </booktitle> <month> April </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Several experimental parallel architectures have been developed in recent years to demonstrate novel hardware mechanisms that may enhance the performance of programs written in emerging parallel languages. For example, Monsoon [17] focuses on Id90 [15], J-Machine <ref> [10, 11] </ref> on CST [13], Alewife [1] on Mul-T, CM-5 [20] on Fortran90, and Dash [14] and KSR-1 on extensions to C and Fortran.
Reference: [12] <author> J. Gustafson, G. Montry, and Benner R. </author> <title> Development of Parallel Methods for a 1024-Processor Hypercube. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9, </volume> <year> 1988. </year>
Reference-contexts: There is not even a consensus on the programming languages of choice. Where benchmarks exist, they have been developed specifically for the machine which they are intended to evaluate <ref> [19, 12] </ref> or specifically avoid emerging languages which novel mechanism may bring into practical reach [4].
Reference: [13] <author> Waldemar Horwat. </author> <title> Concurrent Smalltalk on the Message-Driven Processor. </title> <type> Technical Report Technical Report 1321, </type> <institution> MIT Artificial Intelligence Lab, </institution> <type> 545 Tech. </type> <institution> Square, </institution> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: 1 Introduction Several experimental parallel architectures have been developed in recent years to demonstrate novel hardware mechanisms that may enhance the performance of programs written in emerging parallel languages. For example, Monsoon [17] focuses on Id90 [15], J-Machine [10, 11] on CST <ref> [13] </ref>, Alewife [1] on Mul-T, CM-5 [20] on Fortran90, and Dash [14] and KSR-1 on extensions to C and Fortran.
Reference: [14] <author> D. Lenoski, J. Laudon, K Gharachorloo, A Gupta, and J. Hennessy. </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <address> Sealttle, Washington, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: For example, Monsoon [17] focuses on Id90 [15], J-Machine [10, 11] on CST [13], Alewife [1] on Mul-T, CM-5 [20] on Fortran90, and Dash <ref> [14] </ref> and KSR-1 on extensions to C and Fortran.
Reference: [15] <author> R. S. Nikhil. </author> <title> ID Language Reference Manual Version 90.1. </title> <type> Technical Report CSG Memo 284-2, </type> <institution> MIT Lab for Comp. Sci., </institution> <type> 545 Tech. </type> <institution> Square, </institution> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: 1 Introduction Several experimental parallel architectures have been developed in recent years to demonstrate novel hardware mechanisms that may enhance the performance of programs written in emerging parallel languages. For example, Monsoon [17] focuses on Id90 <ref> [15] </ref>, J-Machine [10, 11] on CST [13], Alewife [1] on Mul-T, CM-5 [20] on Fortran90, and Dash [14] and KSR-1 on extensions to C and Fortran. <p> On each processor, all frames holding enabled threads are linked into a ready queue. Global data structures in TAM provide synchronization on a per-element basis to support I-structure and M-structure semantics <ref> [5, 15] </ref>. In particular, reads of empty elements are deferred until the corresponding write occurs. I-structures support write-once semantics while M-structures allow multiple writes to an element. Accesses to the data structures are split-phase and send a request message to the processor containing the data.
Reference: [16] <author> R. S. Nikhil, G. M. Papadopoulos, and Arvind. </author> <title> *T: A Killer Micro for A Brave New World. </title> <type> Technical Report CSG Memo 325, </type> <institution> MIT Lab for Comp. Sci., </institution> <type> 545 Tech. </type> <institution> Square, </institution> <address> Cambridge, MA, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: These are combined with the machine cost coefficients to obtain the average cycles per TAM instruction. We look forward to a much broader set of studies following a similar methodology using additional machines and additional language frameworks. Clearly it should be possible to evaluate proposals such at *T <ref> [16] </ref> in this framework. Although other parallel language implementations may differ from TAM in many ways, the primary ingredients are likely to be similar: message exchange, remote references, synchronization, control flow, and dynamic scheduling.
Reference: [17] <author> G. M. Papadopoulos and D. E. Culler. Monsoon: </author> <title> an Explicit Token-Store Architecture. </title> <booktitle> In Proc. of the 17th Annual Int. Symp. on Comp. Arch., </booktitle> <address> Seattle, Washington, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Several experimental parallel architectures have been developed in recent years to demonstrate novel hardware mechanisms that may enhance the performance of programs written in emerging parallel languages. For example, Monsoon <ref> [17] </ref> focuses on Id90 [15], J-Machine [10, 11] on CST [13], Alewife [1] on Mul-T, CM-5 [20] on Fortran90, and Dash [14] and KSR-1 on extensions to C and Fortran.
Reference: [18] <author> R. H. Saavedra-Barrera and A. J. Smith. </author> <title> Benchmarking and The Abstract Machine Characterization Model. </title> <type> Technical Report UCB/CSD 90/607, </type> <institution> U.C. Berkeley, Computer Science Div., </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: We follow a method of analysis similar to the Abstract Machine Characterization Model used to evaluate a wide range of conventional machines and benchmarks <ref> [18] </ref>. We normalize for software effects, including programming language, programming style, and high-level compiler optimizations by using a common low-level representation of each program in terms of a Threaded Abstract Machine. Machine specific optimizations are realized in compiling the TAM code to each machine.
Reference: [19] <author> J. P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Stanford University, </institution> <year> 1991. </year>
Reference-contexts: There is not even a consensus on the programming languages of choice. Where benchmarks exist, they have been developed specifically for the machine which they are intended to evaluate <ref> [19, 12] </ref> or specifically avoid emerging languages which novel mechanism may bring into practical reach [4].
Reference: [20] <institution> Thinking Machines Corporation,Cambridge, Massachusetts. </institution> <note> The Connection Machine CM-5 Technical Summary, </note> <month> January </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Several experimental parallel architectures have been developed in recent years to demonstrate novel hardware mechanisms that may enhance the performance of programs written in emerging parallel languages. For example, Monsoon [17] focuses on Id90 [15], J-Machine [10, 11] on CST [13], Alewife [1] on Mul-T, CM-5 <ref> [20] </ref> on Fortran90, and Dash [14] and KSR-1 on extensions to C and Fortran.
References-found: 20

