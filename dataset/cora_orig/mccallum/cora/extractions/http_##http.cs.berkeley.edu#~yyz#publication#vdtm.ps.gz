URL: http://http.cs.berkeley.edu/~yyz/publication/vdtm.ps.gz
Refering-URL: http://http.cs.berkeley.edu/~yyz/publication/publication.html
Root-URL: 
Email: debevec@cs.berkeley.edu  
Title: Efficient View-Dependent Image-Based Rendering with Projective Texture-Mapping  
Author: Paul Debevec, Yizhou Yu, and George Borshukov 
Address: Berkeley  
Affiliation: Univeristy of California at  
Abstract: This paper presents how the image-based rendering technique of view-dependent texture-mapping (VDTM) can be efficiently implemented using projective texture mapping, a feature commonly available in polygon graphics hardware. VDTM is a technique for generating novel views of a scene with approximately known geometry making maximal use of a sparse set of original views. The original presentation of VDTM in by Debevec, Taylor, and Malik required significant per-pixel computation and did not scale well with the number of original images. In our technique, we precompute for each polygon the set of original images in which it is visibile and create a view map data structure that encodes the best texture map to use for a regularly sampled set of possible viewing directions. To generate a novel view, the view map for each polygon is queried to determine a set of no more than three original images to blended together in order to render the polygon with projective texture-mapping. Invisible triangles are shaded using an object-space hole-filling method. We show how the rendering process can be streamlined for implementation on standard polygon graphics hardware. We present results of using the method to render a large-scale model of the Berkeley bell tower and its surrounding campus enironment.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> DANA, K. J., GINNEKEN, B., NAYAR, S. K., AND KOENDERINK, J. J. </author> <title> Reflectance and texture of real-world surfaces. </title> <booktitle> In Proc. IEEE Conf. on Comp. Vision and Patt. </booktitle> <address> Recog. </address> <year> (1997), </year> <pages> pp. 151-157. </pages>
Reference-contexts: Lastly, it seems as if it would be more efficient to analyze the set of available views of each polygon and distill a unified view-dependent function of its appearance, rather than the raw set of original views. One such representation is the Bidirectional texture function, presented in <ref> [1] </ref>, or a yet-to-be-presented form of compressed light field. Both techniques will require new rendering methods in order to render the distilled representations in real time. Extensions of techniques such as model-based stereo [2] might be Fig. 8.
Reference: 2. <author> DEBEVEC, P. E., TAYLOR, C. J., AND MALIK, J. </author> <title> Modeling and rendering architecture from photographs: A hybrid geometry- and image-based approach. </title> <booktitle> In SIGGRAPH '96 (August 1996), </booktitle> <pages> pp. 11-20. </pages>
Reference-contexts: A desirable quality of such a rendering algorithm is to make judicious use of all the available views, including when a particular surface is seen from different directions in several images. This problem was addressed in <ref> [2] </ref>, which presented view-dependent texture mapping as a means to render each pixel in the novel view as a blend of its corresponding pixels in the original views. <p> However, the work did not concentrate on investigating how to combining appearance information from multiple images to optimally produce novel views. View-Dependent Texture Mapping (VDTM) was presented in <ref> [2] </ref> as a method of rendering interactively constructed 3D architectural scenes using images of the scene taken from several locations. <p> As a result, the view-dependent texture mapping approach allowed renderings to be considerably more realistic than static texture-mapping allowed, since it better represented non-diffuse reflectance and can simulate the appearance of unmodeled geometry. The implementation of VDTM in <ref> [2] </ref> computed texture weighting on a per-pixel basis, required visibility calculations to be performed at rendering time, examined every original view to produce every novel view, and only blended between the two closest viewpoints available. <p> Instead of taking more and more photographs, we decided to compose some colors for those black holes from its surrounding area, a precessed called hole filling. Previous hole-filling algorithms <ref> [12, 2] </ref> have operated in image space, which can cause flickering in animations since the manner of the hole filling will change with each frame. Object-space filling can guarantee the color for each invisible polygon is consistent at different frames. <p> The problem is most likely to be noticeable near the frame boundaries of the original images, or near a shadow boundary of an image, where polygons lying on one side of the boundary include an image in their view maps but the polygons on the other side do not. <ref> [2] </ref> suggested feathering the influence of images in image-space toward their boundaries and near shadow boundaries to reduce the appearance of such seams; with some consideration this technique should be adaptable to the object-space method presented here. <p> One such representation is the Bidirectional texture function, presented in [1], or a yet-to-be-presented form of compressed light field. Both techniques will require new rendering methods in order to render the distilled representations in real time. Extensions of techniques such as model-based stereo <ref> [2] </ref> might be Fig. 8. Multi-pass rendering display loop. able to perform a better job than linear blending of interpolating between the various views. 10 Images and Animations Images and Animations of the Berkeley campus model may be found at: http://www.cs.berkeley.edu/~debevec/Campanile
Reference: 3. <author> FOLEY, J. D., VAN DAM, A., FEINER, S. K., AND HUGHES, J. F. </author> <title> Computer Graphics: </title> <booktitle> principles and practice. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: This gives rise to the necessity to clip polygons in the scene against the edges of this texture region. For rendering performance, we wish to minimize the number of polygons resulting from visibility processing. Traditional object-space algorithms for hidden surface removal <ref> [3, 11] </ref> often generate too many polygons or run very slowly. We propose an efficient visibility algorithm for the above purposes. This algorithm operates in both image space and object space to get better performance. It is summarized as follows: 1. Give each original polygon an id number.
Reference: 4. <author> GORTLER, S. J., GRZESZCZUK, R., SZELISKI, R., AND COHEN, M. F. </author> <booktitle> The Lumigraph. In SIGGRAPH '96 (1996), </booktitle> <pages> pp. 43-54. </pages>
Reference-contexts: In [6], blending was performed amongst a dense regular sampling of images in order to generate novel views. Since scene geometry was not used, a very large number of images was necessary in order to produce relatively low-resolution renderings. <ref> [4] </ref> was similar to [6] but used approximate scene geometry derived from object silhouettes. Both of these methods restricted the viewpoint to be outside the convex hull of an object or inside a convex empty region of space.
Reference: 5. <author> LAVEAU, S., AND FAUGERAS, O. </author> <title> 3-D scene representation as a collection of images. </title> <booktitle> In Proceedingsof 12th International Conferenceon Pattern Recognition (1994), </booktitle> <volume> vol. 1, </volume> <pages> pp. 689-691. </pages>
Reference-contexts: and known imaging geom- etry) is available * The photographs are taken in the same lighting conditions * The photographs generally observe each surface of the scene from a few different angles * Surfaces in the scene are not extremely specular 2 Previous Work Early image-based modeling and rendering work <ref> [12, 5, 7] </ref>, presented methods of using image depth or image correspondences to reproject the pixels from one camera position to the viewpoint of another. However, the work did not concentrate on investigating how to combining appearance information from multiple images to optimally produce novel views.
Reference: 6. <author> LEVOY, M., AND HANRAHAN, P. </author> <title> Light field rendering. </title> <booktitle> In SIGGRAPH '96 (1996), </booktitle> <pages> pp. 31-42. </pages>
Reference-contexts: This paper uses visibility preprocessing, polygon view maps, and projective texture mapping to overcome these limitations. Other image-based modeling and rendering work has addressed the problem of blending between available views of the scene in order to produce renderings. In <ref> [6] </ref>, blending was performed amongst a dense regular sampling of images in order to generate novel views. Since scene geometry was not used, a very large number of images was necessary in order to produce relatively low-resolution renderings. [4] was similar to [6] but used approximate scene geometry derived from object <p> In <ref> [6] </ref>, blending was performed amongst a dense regular sampling of images in order to generate novel views. Since scene geometry was not used, a very large number of images was necessary in order to produce relatively low-resolution renderings. [4] was similar to [6] but used approximate scene geometry derived from object silhouettes. Both of these methods restricted the viewpoint to be outside the convex hull of an object or inside a convex empty region of space.
Reference: 7. <author> MCMILLAN, L., AND BISHOP, G. </author> <title> Plenoptic Modeling: An image-based rendering system. </title> <note> In SIGGRAPH '95 (1995). </note>
Reference-contexts: and known imaging geom- etry) is available * The photographs are taken in the same lighting conditions * The photographs generally observe each surface of the scene from a few different angles * Surfaces in the scene are not extremely specular 2 Previous Work Early image-based modeling and rendering work <ref> [12, 5, 7] </ref>, presented methods of using image depth or image correspondences to reproject the pixels from one camera position to the viewpoint of another. However, the work did not concentrate on investigating how to combining appearance information from multiple images to optimally produce novel views.
Reference: 8. <author> SEGAL, M., KOROBKIN, C., VAN WIDENFELT, R., FORAN, J., AND HAEBERLI, P. </author> <title> Fast shadows and lighting effects using texture mapping. </title> <booktitle> In SIGGRAPH '92 (July 1992), </booktitle> <pages> pp. 249-252. </pages>
Reference-contexts: Render the polygon using alpha-blending of the three textures with projective texture mapping. 4 Projective Texture Mapping To take advantage of current graphics hardware, we make use of projective texture mapping. Projective texture mapping was introduced in <ref> [8] </ref> and is now part of the OpenGL graphics standard. Although the original paper used it only for shadows and lighting effects, it is extremely useful in image-based rendering because it can simulate the inverse projection of taking photographs with a camera.
Reference: 9. <author> TELLER, S. J., AND SEQUIN, C. H. </author> <title> Visibility preprocessing for interactive walkthroughs. </title> <booktitle> In SIGGRAPH '91 (1991), </booktitle> <pages> pp. 61-69. </pages>
Reference-contexts: The algorithm as we have presented it requires all the available images of the scene to fit within the main memory of the rendering computer. For a very large-scale environment, this is unreasonable to expect. To solve this problem, spatial partitioning schemes such as those presented in <ref> [9] </ref> could be adapted for this purpose. As we have presented the algorithm, it is only appropriate for models that can be broken into polygonal patches.
Reference: 10. <author> WEGHORST, H., HOOPER, G., AND GREENBERG, D. P. </author> <title> Improved computational methods for ray tracing. </title> <journal> ACM Transactions on Graphics 3, </journal> <month> 1 (January </month> <year> 1984), </year> <pages> 52-69. </pages>
Reference-contexts: Clip each polygon with its list of occluders in object-space. 7. Associate with each polygon a list of photographs to which it is totally visible. Using identification (id) numbers to retrieve objects from Z-buffer is similar to the item buffer technique introduced in <ref> [10] </ref>. The image-space steps in the algorithm can quickly obtain the list of occluders for each polygon.
Reference: 11. <author> WEILER, K., AND ATHERTON, P. </author> <title> Hidden surface removal using polygon area sorting. </title> <booktitle> In SIGGRAPH '77 (1977), </booktitle> <pages> pp. 214-222. </pages>
Reference-contexts: This gives rise to the necessity to clip polygons in the scene against the edges of this texture region. For rendering performance, we wish to minimize the number of polygons resulting from visibility processing. Traditional object-space algorithms for hidden surface removal <ref> [3, 11] </ref> often generate too many polygons or run very slowly. We propose an efficient visibility algorithm for the above purposes. This algorithm operates in both image space and object space to get better performance. It is summarized as follows: 1. Give each original polygon an id number.
Reference: 12. <author> WILLIAMS, L., AND CHEN, E. </author> <title> View interpolation for image synthesis. In SIGGRAPH '93 (1993). (b) (d) Fig. 9. The different rendering passes in producing a frame from the photorealistic renderings of the Berkeley campus virtual fly-by. (a) The campus buildings and terrain; these areas were seen from only one viewpoint and are thus rendered before the VDTM passes. (b) The Berkeley tower after the first pass of view-dependent texture mapping. (c) The Berkeley tower after the second pass of view-dependent texture mapping. (d) The complete rendering of the scene. </title>
Reference-contexts: and known imaging geom- etry) is available * The photographs are taken in the same lighting conditions * The photographs generally observe each surface of the scene from a few different angles * Surfaces in the scene are not extremely specular 2 Previous Work Early image-based modeling and rendering work <ref> [12, 5, 7] </ref>, presented methods of using image depth or image correspondences to reproject the pixels from one camera position to the viewpoint of another. However, the work did not concentrate on investigating how to combining appearance information from multiple images to optimally produce novel views. <p> Instead of taking more and more photographs, we decided to compose some colors for those black holes from its surrounding area, a precessed called hole filling. Previous hole-filling algorithms <ref> [12, 2] </ref> have operated in image space, which can cause flickering in animations since the manner of the hole filling will change with each frame. Object-space filling can guarantee the color for each invisible polygon is consistent at different frames.
References-found: 12

