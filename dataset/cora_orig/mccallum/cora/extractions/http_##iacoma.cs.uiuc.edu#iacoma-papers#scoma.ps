URL: http://iacoma.cs.uiuc.edu/iacoma-papers/scoma.ps
Refering-URL: http://iacoma.cs.uiuc.edu/papers.html
Root-URL: http://www.cs.uiuc.edu
Email: basu,torrella@cs.uiuc.edu  
Title: Enhancing Memory Use in Simple Coma: Multiplexed Simple Coma 1  
Author: Sujoy Basu and Josep Torrellas 
Web: http://iacoma.cs.uiuc.edu/iacoma/  
Address: IL 61801  
Affiliation: Computer Science Department University of Illinois at Urbana-Champaign,  
Abstract: Scalable shared-memory multiprocessors that are designed as Cache-Only Memory Architectures (Coma) allow automatic replication and migration of data in the main memory. This enhances programmability by hopefully eliminating the need for data distribution strategies and page migration schemes. Recently, a variant of Coma called Simple Coma has been proposed as a lower-cost alternative to hardware-intensive systems like Flat Coma. However, we find that Simple Coma is quite slower than Flat Coma. The main reason is the high page mapping, unmapping, and transfer overhead caused by memory fragmentation in Simple Coma. This paper proposes a solution to the memory fragmentation problem that we call Multiplexed Simple Coma. The idea is to allow multiple virtual pages to map into the same physical page at the same time, therefore compressing the page working set of the application. Multiplexed Simple Coma requires very little support over Simple Coma and reduces its execution time by about 40%. We find that Multiplexed Simple Coma can be very easily implemented with off-the-shelf processors. In addition, there is no need to be selective when choosing what virtual pages are to share the same physical page. Overall, although Multiplexed Simple Coma is still slower than Flat Coma, since it is cheaper to implement, it represents a good cost-performance design point. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Basu and J. Torrellas. </author> <title> Enhancing Memory Use in Simple Coma: Multiplexed Simple Coma. </title> <type> Technical Report 1528, </type> <institution> Department of Computer Science, University of Illinois, </institution> <month> December </month> <year> 1997. </year>
Reference-contexts: In that case, a second scheme that we evaluate uses conditional prefetch-ing: if more than N umM ux=2 pages multiplexed into that physical page are using the subpage, we do not prefetch the subpage. The hardware support required to determine sub-page use is presented in <ref> [1] </ref>. We do not present it here because we are about to show that it is not cost-effective. Mux8, the impact of unconditional prefetching (M4Pref and M8Pref) and conditional prefetching (M4Cond and M8Cond). Unconditional prefetching works acceptably well, reducing the execution time of the applications by an average of 7%. <p> The new page is compatible only if the number of collisions is less than or equal to a threshold that we call CollThresh. An efficient implementation of the use vectors and collision analysis is discussed in <ref> [1] </ref>. We do not include it here because we are about to show that selectivity is unnecessary. The value of CollThresh can range from 0 to 8. A low value means that we are selective. High selectivity has a twofold impact on the execution time.
Reference: [2] <author> A. L. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Raja-mony, and W. Zwaenepoel. </author> <title> Software Versus Hardware Shared Memory Implementation: A Case Study. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 106-117, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Unlike in Distributed Virtual Shared Memory (DVSM) systems <ref> [2, 11] </ref>, however, the unit of coherence in Simple Coma is still the memory line, and traditional implementations support a hardware cache coherence protocol similar to the one in Flat Coma.
Reference: [3] <author> B. Falsafi and D. Wood. </author> <title> Reactive NUMA: A Design for Unifying S-COMA and CC-NUMA. </title> <booktitle> In Proceedings of the 24th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 229-240, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Overall, the conclusions from Section 5.1 still hold for 45-50% memory pressure. Multiplexed Simple Coma is significantly more cost-effective than Simple Coma for this memory pressure. 7 Related Work Three other pieces of work bear some relation to the work presented here. Falsafi and Wood <ref> [3] </ref> describe a hybrid architecture called Reactive NUMA in which a page is initially mapped in Numa mode, and gets remapped in Simple Coma mode when the number of conflict misses from that page exceeds a threshold.
Reference: [4] <author> S. Goldschmidt. </author> <title> Simulation of Multiprocessors: Accuracy and Performance. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: The size of the pages is 4 Kbytes. 4.2 Applications Run and Simulation System We trace seven applications from the Splash-2 suite [17]. Table 3 summarizes some of application-specific information relevant for this paper. Our simulations are performed within the execution-driven simulation framework of TangoLite <ref> [4] </ref>. The applications are instrumented so that all memory accesses result in calls to our memory simulator. Table 3: Characteristics of the applications traced.
Reference: [5] <author> E. Hagersten, A. Landin, and S. Haridi. </author> <title> DDM a Cache-Only Memory Architecture. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 44-54, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: As a result, accesses to remote memory modules now involve large latencies. This effect degrades the performance of many applications, especially those with working sets that overflow the caches. One organization of shared-memory multiprocessors that addresses this problem is the Cache-Only Memory Architecture (Coma) <ref> [5, 9, 10, 13, 14] </ref>. In Coma, main memory is organized as a cache, known as attraction memory. The physical address of data is no longer associated with a fixed location in memory. <p> Indeed, we only need to check the directory entry in the corresponding home node to determine where to find a copy of the line. This is unlike in older Coma machines like KSR1 [9] and DDM1 <ref> [5] </ref>. To reduce conflicts, the attraction memory in Flat Coma is made set-associative, and each entry is given a tag that contains state and address information for the line.
Reference: [6] <author> E. Hagersten, A. Saulsbury, and A. Landin. </author> <title> Simple COMA Node Implementations. </title> <booktitle> In Proceedings of the 27th Annual Hawaii International Conference of System Sciences, </booktitle> <pages> pages 522-533, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: 2 compares Simple Coma to Flat Coma; Section 3 presents Multiplexed Simple Coma; Section 4 discusses the experimental setup used for our evaluation; Section 5 evaluates the performance of the base Multiplexed Simple Coma; and Section 6 examines variations of the design. 2 Simple Coma Versus Flat Coma Simple Coma <ref> [6, 13] </ref> migrates some of the support for data coherence from the hardware to the virtual memory manager in the operating system.
Reference: [7] <author> H. A. Jamrozik, M. J. Feeley, G. M. Voelker, J. E. II, A. R. Karlin, H. M. Levy, and M. K. Vernon. </author> <title> Reducing Network Latency Using Subpages in a Global Memory Environment. </title> <booktitle> In Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 258-267, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: The work by Tal-luri et al [15, 16] discusses superpages. Superpages are a way of expanding the coverage of the TLB at the possible expense of memory fragmentation. Finally, the work by Jamrozik et al <ref> [7] </ref> discusses operating system-induced page transfers in a network of workstations. While the environment is totally different to ours, they use the concept of subpages in data transfers. We also use subpages for the prefetching scheme. 8 Conclusions Cache-only memory architectures are an interesting class of shared-memory multiprocessors.
Reference: [8] <author> T. Joe and J. L. Hennessy. </author> <title> Evaluating the Memory Overhead Required for COMA Architectures. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 82-93, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: If the tag of the displaced line indicates that this is the last copy of the line, the hardware injects the line into another node. This is necessary to ensure that last copies of lines are never lost <ref> [8] </ref>. Recently, a variant of Coma called Simple Coma [13] has been proposed as a lower-cost alternative to hardware-intensive systems like Flat Coma. The idea is to migrate part of the complexity of Coma from hardware into software. <p> This has three advantages. One of them is that it simplifies handling the replacement of last copies of data. Recall that, in Flat Coma, the cache coherence protocol must be extended to make sure that last copies of lines are not purged from the system <ref> [8] </ref>. This requires the handling of several corner cases which complicate the protocol. In Simple Coma, all replacements are handled by the operating system, irrespective of whether or not they handle last copies. Specifically, the last-copy problem is handled as follows. <p> We will call this a line collision. In that case, one line displaces the other from the node. If the displaced line is the last copy in the machine, it has to be stored in another node <ref> [8] </ref>. To handle line collisions easily, we simplify the cache coherence protocol as follows. Each virtual page that is multiplexed in one or more nodes in the machine will have a non-multiplexed mapping in one node. The mapping that is not multiplexed is called the repository copy of the page.
Reference: [9] <institution> Kendall Square Research. </institution> <type> KSR1 Technical Summary. </type> <address> Waltham, MA, </address> <year> 1992. </year>
Reference-contexts: As a result, accesses to remote memory modules now involve large latencies. This effect degrades the performance of many applications, especially those with working sets that overflow the caches. One organization of shared-memory multiprocessors that addresses this problem is the Cache-Only Memory Architecture (Coma) <ref> [5, 9, 10, 13, 14] </ref>. In Coma, main memory is organized as a cache, known as attraction memory. The physical address of data is no longer associated with a fixed location in memory. <p> Consequently, while memory lines are allowed to migrate freely, they can still be located quickly. Indeed, we only need to check the directory entry in the corresponding home node to determine where to find a copy of the line. This is unlike in older Coma machines like KSR1 <ref> [9] </ref> and DDM1 [5]. To reduce conflicts, the attraction memory in Flat Coma is made set-associative, and each entry is given a tag that contains state and address information for the line.
Reference: [10] <author> A. Landin and F. Dahlgren. </author> <title> Bus-Based COMA Reducing Traffic in Shared-Bus Multiprocessors. </title> <booktitle> In Proceedings of the 2nd International Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 95-105, </pages> <month> Feb-ruary </month> <year> 1996. </year>
Reference-contexts: As a result, accesses to remote memory modules now involve large latencies. This effect degrades the performance of many applications, especially those with working sets that overflow the caches. One organization of shared-memory multiprocessors that addresses this problem is the Cache-Only Memory Architecture (Coma) <ref> [5, 9, 10, 13, 14] </ref>. In Coma, main memory is organized as a cache, known as attraction memory. The physical address of data is no longer associated with a fixed location in memory.
Reference: [11] <author> K. Li and P. Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Unlike in Distributed Virtual Shared Memory (DVSM) systems <ref> [2, 11] </ref>, however, the unit of coherence in Simple Coma is still the memory line, and traditional implementations support a hardware cache coherence protocol similar to the one in Flat Coma.
Reference: [12] <author> S. Reinhardt, J. R. Larus, and D. A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325-336, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: When the message reaches the destination node, another translation into the local address in that node is required. Unfortunately, all these translations are performed in hardware. This is expensive and must rely on fast TLB-like tables. Some of these tables are sometimes referred to as Reverse TLB tables <ref> [12] </ref>. Since these tables can overflow, they need to be backed up by page tables in the operating system. A second consequence of the different physical address spaces is that Simple Coma is easier to build from commodity components like workstations and operating systems.
Reference: [13] <author> A. Saulsbury, T. Wilkinson, J. Carter, and A. Landin. </author> <title> An Argument for Simple COMA. </title> <booktitle> In Proceedings of the First IEEE Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 276-285, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: As a result, accesses to remote memory modules now involve large latencies. This effect degrades the performance of many applications, especially those with working sets that overflow the caches. One organization of shared-memory multiprocessors that addresses this problem is the Cache-Only Memory Architecture (Coma) <ref> [5, 9, 10, 13, 14] </ref>. In Coma, main memory is organized as a cache, known as attraction memory. The physical address of data is no longer associated with a fixed location in memory. <p> If the tag of the displaced line indicates that this is the last copy of the line, the hardware injects the line into another node. This is necessary to ensure that last copies of lines are never lost [8]. Recently, a variant of Coma called Simple Coma <ref> [13] </ref> has been proposed as a lower-cost alternative to hardware-intensive systems like Flat Coma. The idea is to migrate part of the complexity of Coma from hardware into software. <p> 2 compares Simple Coma to Flat Coma; Section 3 presents Multiplexed Simple Coma; Section 4 discusses the experimental setup used for our evaluation; Section 5 evaluates the performance of the base Multiplexed Simple Coma; and Section 6 examines variations of the design. 2 Simple Coma Versus Flat Coma Simple Coma <ref> [6, 13] </ref> migrates some of the support for data coherence from the hardware to the virtual memory manager in the operating system. <p> A final consequence of having independent physical addresses in each node is that Simple Coma can use the TLB lookup step to locate the desired line in the attraction memory <ref> [13] </ref>. If, instead, we had a single physical address space like in Flat Coma, remote lines would have to be stored into fixed locations in the local attraction memory.
Reference: [14] <author> P. Stenstrom, T. Joe, and A. Gupta. </author> <title> Comparative Performance Evaluation of Cache-Coherent NUMA and COMA Architectures. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 80-91, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: As a result, accesses to remote memory modules now involve large latencies. This effect degrades the performance of many applications, especially those with working sets that overflow the caches. One organization of shared-memory multiprocessors that addresses this problem is the Cache-Only Memory Architecture (Coma) <ref> [5, 9, 10, 13, 14] </ref>. In Coma, main memory is organized as a cache, known as attraction memory. The physical address of data is no longer associated with a fixed location in memory. <p> An example of a high-performance hardware-only Coma organization is Flat Coma <ref> [14] </ref>. In Flat Coma, the directory information for a line is kept in a fixed home node. Consequently, while memory lines are allowed to migrate freely, they can still be located quickly.
Reference: [15] <author> M. Talluri and M. D. Hill. </author> <title> Surpassing the TLB Performance of Superpages with Less Operating System Support. </title> <booktitle> In Proceedings of the 6th International Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 171-182, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: The number of page faults is heavily dependent on the architecture parameters and algorithms used. We believe that a major reason why we observe more page faults in our system is that we use a higher memory pressure in the memory. The work by Tal-luri et al <ref> [15, 16] </ref> discusses superpages. Superpages are a way of expanding the coverage of the TLB at the possible expense of memory fragmentation. Finally, the work by Jamrozik et al [7] discusses operating system-induced page transfers in a network of workstations.
Reference: [16] <author> M. Talluri, S. Kong, M. D. Hill, and D. A. Patterson. </author> <title> Tradeoffs in Supporting Two Page Sizes. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 415-424, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: The number of page faults is heavily dependent on the architecture parameters and algorithms used. We believe that a major reason why we observe more page faults in our system is that we use a higher memory pressure in the memory. The work by Tal-luri et al <ref> [15, 16] </ref> discusses superpages. Superpages are a way of expanding the coverage of the TLB at the possible expense of memory fragmentation. Finally, the work by Jamrozik et al [7] discusses operating system-induced page transfers in a network of workstations.
Reference: [17] <author> S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, and A. Gupta. </author> <title> The SPLASH-2 Programs: Characterization and Methodological Considerations. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: In this section, we discuss the basic idea, the algorithms implemented, and the hardware and operating system support required. 3.1 The Basic Idea To gain insight into the magnitude of the memory fragmentation problem, we measured, for seven Splash2 applications <ref> [17] </ref>, what fraction of a page is used. <p> For page allocation, we use first-touch after the parallel section starts. For this first allocation, no cost is charged. The size of the pages is 4 Kbytes. 4.2 Applications Run and Simulation System We trace seven applications from the Splash-2 suite <ref> [17] </ref>. Table 3 summarizes some of application-specific information relevant for this paper. Our simulations are performed within the execution-driven simulation framework of TangoLite [4]. The applications are instrumented so that all memory accesses result in calls to our memory simulator. Table 3: Characteristics of the applications traced.
References-found: 17

