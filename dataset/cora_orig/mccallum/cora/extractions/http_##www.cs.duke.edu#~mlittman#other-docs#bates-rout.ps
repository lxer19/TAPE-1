URL: http://www.cs.duke.edu/~mlittman/other-docs/bates-rout.ps
Refering-URL: http://www.cs.duke.edu/~mlittman/topics/routing-page.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: jbates@encore.com  
Title: Packet Routing and Reinforcement Learning: Estimating Shortest Paths in Dynamic Graphs  
Author: John W. Bates 
Date: April 19, 1995  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> J. Boyan and M. Littman. </author> <title> A distributed reinforcement learning scheme for network routing. </title> <type> Technical Report CMU-CS-93-165, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1993. </year>
Reference-contexts: Congestion can result in huge delays and dropped messages. For a detailed description of the routing problem in various types of networks, see [6]. This paper evaluates the "Q-routing" algorithm described in Boyan and Littman's papers <ref> [1, 2] </ref>, which uses a reinforcement learning algorithm to attempt to learn routing policies. By examining the results of simulations in which real-world restrictions have been relaxed, it is shown that Q-routing performs well, with only a small margin for improvement.
Reference: [2] <author> J. Boyan and M. Littman. </author> <title> Packet routing in dynamically changing networks: A reinforcement learning approach. </title> <note> Available for ftp from ftp.cs.cmu.edu, </note> <year> 1995. </year>
Reference-contexts: Congestion can result in huge delays and dropped messages. For a detailed description of the routing problem in various types of networks, see [6]. This paper evaluates the "Q-routing" algorithm described in Boyan and Littman's papers <ref> [1, 2] </ref>, which uses a reinforcement learning algorithm to attempt to learn routing policies. By examining the results of simulations in which real-world restrictions have been relaxed, it is shown that Q-routing performs well, with only a small margin for improvement. <p> Due to the rapid propagation of information about network state, nodes were able to maintain a rough approximation of the optimal policies. Delivery rates were largely constant up until the point at which blow-up occurred. It should be noted that in <ref> [2] </ref>, Boyan and Littman describe a similar algorithm, which they call "full-echo". Their results show that full-echo performs significantly worse than greedy learning, but close examination revealed a minor difference in the algorithms. Full-echo only propagates information when processing a packet, while constant pinging occurs at every time step.
Reference: [3] <author> M. </author> <title> DeGroot. Probability and Statistics. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> second edition, </address> <year> 1986. </year> <month> 10 </month>
Reference-contexts: Ideally, the list would be infinite, but space and time requirements force it to be maintained as a window of past responses. The distribution of the sample is assumed to be normal. Further discussion of the mathematics can be found in most statistics textbooks, with <ref> [3] </ref> being a good example. The basic Q-routing algorithm is modified to be as follows: 1. Select a packet from the queue. 2. For each node n i in the set of all neighbors N, compute l (n i ; d) as in equation (2). 8 3.
Reference: [4] <author> J. Hertz, A. Krogh, and R. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1991. </year>
Reference-contexts: Also, a supplementary algorithm is presented which exhibits better performance under heavy load, at the cost of decreased efficiency under lighter loads. For a detailed description of the routing problem in various types of networks, see [6]. For discussion of reinforcement learning, see <ref> [4, 5] </ref>. Performance improvements are demonstrated the by use of an algorithm which makes decisions based upon a statistical history of the results of actions taken. <p> The gains, however, are offset by an decrease in the rate at 1 which learning occurs, resulting in greater high-end performance, but sig-nificant problems under medium loads. For a discussion of reinforcement learning, see <ref> [4, 5] </ref>. 2 Reinforcement Learning for Routing 2.1 Q-routing In Q-routing, each node in a network, when given a packet with some destination node d, decides on a neighbor to which to send it based on statistics gathered about the network.
Reference: [5] <author> M. McFarland and T. Bosser. </author> <title> Intelligent Behavior in Animals and Robots. </title> <publisher> The MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: Also, a supplementary algorithm is presented which exhibits better performance under heavy load, at the cost of decreased efficiency under lighter loads. For a detailed description of the routing problem in various types of networks, see [6]. For discussion of reinforcement learning, see <ref> [4, 5] </ref>. Performance improvements are demonstrated the by use of an algorithm which makes decisions based upon a statistical history of the results of actions taken. <p> The gains, however, are offset by an decrease in the rate at 1 which learning occurs, resulting in greater high-end performance, but sig-nificant problems under medium loads. For a discussion of reinforcement learning, see <ref> [4, 5] </ref>. 2 Reinforcement Learning for Routing 2.1 Q-routing In Q-routing, each node in a network, when given a packet with some destination node d, decides on a neighbor to which to send it based on statistics gathered about the network.
Reference: [6] <author> F. Leighton Thomson. </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays, Trees, Hypercubes. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1992. </year> <note> 11 12 was fixed at 2.0. 13 </note>
Reference-contexts: The process by which the next neighbor is selected is known as "routing", and an inefficient routing policy can have devastating effects on network communications. Congestion can result in huge delays and dropped messages. For a detailed description of the routing problem in various types of networks, see <ref> [6] </ref>. This paper evaluates the "Q-routing" algorithm described in Boyan and Littman's papers [1, 2], which uses a reinforcement learning algorithm to attempt to learn routing policies. <p> Also, a supplementary algorithm is presented which exhibits better performance under heavy load, at the cost of decreased efficiency under lighter loads. For a detailed description of the routing problem in various types of networks, see <ref> [6] </ref>. For discussion of reinforcement learning, see [4, 5]. Performance improvements are demonstrated the by use of an algorithm which makes decisions based upon a statistical history of the results of actions taken.
References-found: 6

