URL: http://www.cs.columbia.edu/~sal/hpapers/kdd98-bridging.ps.gz
Refering-URL: http://www.cs.columbia.edu:80/~sal/recent-papers.html
Root-URL: 
Email: fandreas,salg@cs.columbia.edu  (andreas@cs.columbia.edu)  
Title: Mining databases with different schemas: Integrating incompatible classifers  
Author: Andreas L. Prodromidis Salvatore Stolfo Andreas L. Prodromidis 
Keyword: bridging agents, incompatible classifiers, database schema, distributed data mining, meta-learning.  
Note: Contact Author:  This research is supported by the Intrusion Detection Program (BAA9603) from DARPA (F30602-96 1-0311), NSF (IRI-96-32225 and CDA-96-25374) and NYSSTF (423115-445). Supported in part by an IBM fellowship  
Date: March 16, 1998  
Address: 1214 Amsterdam Ave. Mail Code 0401 New York, NY 10027  
Affiliation: Department of Computer Science Columbia University  
Abstract: Distributed data mining systems aim to discover (and combine) usefull information that is distributed across multiple databases. The JAM system, for example, applies machine learning algorithms to compute models over distributed data sets and employs meta-learning techniques to combine the multiple models. Occasionally, however, these models (or classifiers) are induced from databases that have (moderately) different schemas and hence are incompatible. In this paper, we systematically investigate the problem of combining multiple models computed over distributed data sets with different schemas. Through experiments performed on actual credit card data provided by two different financial institutions, we evaluate the effectiveness of the proposed approaches and demonstrate their potential utility. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P.K. Chan A. L. Prodromidis, S. Stolfo. </author> <title> Pruning classifiers in a distributed meta-learning system. </title> <note> Submitted for publication in KDD-98, </note> <year> 1998. </year>
Reference-contexts: Both evaluation metrics and pruning algorithms are described in detail in a companion paper <ref> [1] </ref>. The main purpose of the experiment was to establish the feasibility of overcoming the incompatible schema problem. <p> The new First Union meta-classifier is superior even to the best "pure" First Union meta-classifiers (i.e. the meta-classifier composed by local base-classifiers alone) as reported in companion paper <ref> [1] </ref> improving total accuracy by 1.2%, T P F P spread by 5.8% and savings by $20K. 5 Conclusion Distributed data-mining systems, need to be flexible enough to accommodate incompatible yet comparable databases (i.e. similar databases but of different schemas).
Reference: [2] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, CA, </address> <year> 1984. </year>
Reference-contexts: To be more specific, by deploying regression methods (e.g. Cart <ref> [2] </ref>, locally weighted regression [3], linear regression fit [11], MARS [10]) for continuous attributes and machine learning algorithms for categorical attributes, data site A can compute one or more auxiliary classifier agents C Aj 0 that predict the value of attribute A n+1 based on the common attributes A 1 ; <p> Learning algorithms Five inductive learning algorithms are used in our experiments. ID3, its successor C4.5, and Cart <ref> [2] </ref> are decision tree based algorithms, Bayes, described in [8], is a naive Bayesian classifier and Ripper [6] is a rule induction algorithm based on IREP [9].
Reference: [3] <author> A. W. Moore C. G. Atkeson, S. A. Schaal. </author> <title> Locally weighted learning. </title> <journal> AI Review, </journal> <note> To Appear, </note> <year> 1997. </year>
Reference-contexts: To be more specific, by deploying regression methods (e.g. Cart [2], locally weighted regression <ref> [3] </ref>, linear regression fit [11], MARS [10]) for continuous attributes and machine learning algorithms for categorical attributes, data site A can compute one or more auxiliary classifier agents C Aj 0 that predict the value of attribute A n+1 based on the common attributes A 1 ; :::; An.
Reference: [4] <author> P. Chan and S. Stolfo. </author> <title> Meta-learning for multistrategy and parallel learning. </title> <booktitle> In Proc. Second Intl. Work. Multistrategy Learning, </booktitle> <pages> pages 150-165, </pages> <year> 1993. </year>
Reference-contexts: In section 3 we detail our techniques for bridging the differences among databases and we describe how JAM can be extended to accommodate incompatible, yet comparable databases. Section 4 presents the experiments performed and the performance results collected, and section 5 concludes this paper. 2 Database Compatibility Meta-learning <ref> [4] </ref> is a technique that addresses the scaling problem of machine learning, i.e. the problem of learning useful information from large and inherently distributed databases.
Reference: [5] <author> P. Chan and S. Stolfo. </author> <title> Sharing learned models among remote database partitions by local meta-learning. </title> <booktitle> In Proc. Second Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <pages> pages 2-7, </pages> <year> 1996. </year>
Reference-contexts: These strategies address the incompatible schema problem and meta-learning over these models should subsequently proceed in a straightforward manner <ref> [5] </ref>. 4 Experiments and Evaluation In this chapter we describe in detail our experiments in combining seemingly incompatible classification models using these techniques. Learning algorithms Five inductive learning algorithms are used in our experiments.
Reference: [6] <author> W. Cohen. </author> <title> Fast effective rule induction. </title> <booktitle> In Proc. 12th Intl. Conf. Machine Learning, </booktitle> <pages> pages 115-123, </pages> <year> 1995. </year>
Reference-contexts: Learning algorithms Five inductive learning algorithms are used in our experiments. ID3, its successor C4.5, and Cart [2] are decision tree based algorithms, Bayes, described in [8], is a naive Bayesian classifier and Ripper <ref> [6] </ref> is a rule induction algorithm based on IREP [9]. Learning tasks Two data sets of real credit card transactions were used in our experiments provided by the Chase and First Union Banks, members of the FSTC (Financial Services Technology Consortium). Each bank supplied .5 million records spanning one year.
Reference: [7] <author> T.G. Dietterich. </author> <title> Machine learning research: Four current directions. </title> <journal> AI Magazine, </journal> <volume> 18(4) </volume> <pages> 97-136, </pages> <year> 1997. </year>
Reference-contexts: One approach of mining and combining useful information that is distributed across distributed databases is to apply various machine learning programs to discover patterns exhibited in the data and then combine the computed descriptive representations. Combining multiple classification models has been receiving increased attention <ref> [7] </ref>.
Reference: [8] <author> C. </author> <type> Elkan. </type> <institution> Boosting and naive bayesian learning [http://www-cse.ucsd.edu/~elkan/papers/bnb.ps]. Department of Computer Science and Engineering, Univ. of California, </institution> <address> San Diego, CA, </address> <year> 1997. </year>
Reference-contexts: Learning algorithms Five inductive learning algorithms are used in our experiments. ID3, its successor C4.5, and Cart [2] are decision tree based algorithms, Bayes, described in <ref> [8] </ref>, is a naive Bayesian classifier and Ripper [6] is a rule induction algorithm based on IREP [9]. Learning tasks Two data sets of real credit card transactions were used in our experiments provided by the Chase and First Union Banks, members of the FSTC (Financial Services Technology Consortium).
Reference: [9] <author> J. Furnkranz and G. </author> <title> Widmer. Incremental reduced error pruning. </title> <booktitle> In Proc. 11th Intl. Conf. Mach. Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: Learning algorithms Five inductive learning algorithms are used in our experiments. ID3, its successor C4.5, and Cart [2] are decision tree based algorithms, Bayes, described in [8], is a naive Bayesian classifier and Ripper [6] is a rule induction algorithm based on IREP <ref> [9] </ref>. Learning tasks Two data sets of real credit card transactions were used in our experiments provided by the Chase and First Union Banks, members of the FSTC (Financial Services Technology Consortium). Each bank supplied .5 million records spanning one year.
Reference: [10] <author> J.H.Friedman. </author> <title> Multivariate adaptive regression splines. </title> <journal> The Annals of Statistics, </journal> <volume> 19(1) </volume> <pages> 1-141, </pages> <year> 1991. </year>
Reference-contexts: To be more specific, by deploying regression methods (e.g. Cart [2], locally weighted regression [3], linear regression fit [11], MARS <ref> [10] </ref>) for continuous attributes and machine learning algorithms for categorical attributes, data site A can compute one or more auxiliary classifier agents C Aj 0 that predict the value of attribute A n+1 based on the common attributes A 1 ; :::; An.
Reference: [11] <author> R.H. Myers. </author> <title> Classical and Modern Regression with Applications. </title> <publisher> Duxbury, </publisher> <address> Boston, MA, </address> <year> 1986. </year>
Reference-contexts: To be more specific, by deploying regression methods (e.g. Cart [2], locally weighted regression [3], linear regression fit <ref> [11] </ref>, MARS [10]) for continuous attributes and machine learning algorithms for categorical attributes, data site A can compute one or more auxiliary classifier agents C Aj 0 that predict the value of attribute A n+1 based on the common attributes A 1 ; :::; An.
Reference: [12] <author> A. L. Prodromidis. </author> <title> On the management of distributed learning agents. </title> <type> Technical Report CUCS-032-97 (PhD Thesis proposal), </type> <institution> Department of Computer Science, Columbia University, </institution> <address> New York, NY, </address> <year> 1997. </year>
Reference-contexts: On a couple of occasions, the (T P F P ) spread remains negative despite the additional information provided by the bridging classifiers. In such cases, the classifier evaluation methods, the meta-learning phase and meta-classifier pruning algorithms <ref> [12] </ref> serve to prune away these poor performers. After being matched and evaluated, the base-classifier/bridging classifier pairs can be 8 (right) of First Union meta-classifiers combined both Chase and First Union base classifiers. integrated into new meta-classifier hierarchies.
Reference: [13] <institution> StatSci Division, MathSoft, Seattle. Splus, </institution> <note> Version 3.4, </note> <year> 1996. </year>
Reference-contexts: In this experiment, we tested two different regression methods, the linear regression fit and regression trees both available by Splus <ref> [13] </ref>. We applied the two techniques on the different subsets of the Chase data to generate several bridging models and we matched each (original) Chase classifier with one bridging classifier according to their performance on a First Union validation set.
Reference: [14] <author> S. Stolfo, A. Prodromidis, S. Tselepis, W. Lee, W. Fan, and P. Chan. </author> <title> JAM: Java agents for meta-learning over distributed databases. </title> <booktitle> In Proc. 3rd Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <pages> pages 74-81, </pages> <year> 1997. </year> <month> 10 </month>
Reference-contexts: Although the J AM system <ref> [14] </ref> addresses the later by employing meta-learning techniques, integrating classification models derived from distinct and distributed databases may not always be feasible. In all cases considered so far, all classification models are assumed to originate from databases of identical schemas. <p> From their predictions, the meta-learner will detect the properties, the behavior and performance of the base-classifiers and compute a meta-classifier that represents a model of the "global" data set. The J AM system <ref> [14] </ref> is designed to implement meta-learning.
References-found: 14

