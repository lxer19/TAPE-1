URL: http://www.cs.unm.edu/~bap/papers/nc-hessian.ps.gz
Refering-URL: http://www.cs.unm.edu/~bap/publications.html
Root-URL: http://www.cs.unm.edu
Email: bap@learning.siemens.com  
Title: Fast Exact Multiplication by the Hessian  
Author: Barak A. Pearlmutter 
Note: To appear in Neural Computation  
Date: June 9, 1993  
Address: 755 College Road East Princeton, NJ 08540  
Affiliation: Siemens Corporate Research  
Abstract: Just storing the Hessian H (the matrix of second derivatives 2 E=w i w j of the error E with respect to each pair of weights) of a large neural network is difficult. Since a common use of a large matrix like H is to compute its product with various vectors, we derive a technique that directly calculates Hv, where v is an arbitrary vector. To calculate Hv, we first define a differential operator R v ff (w)g = (=r) f (w + rv)j r=0 , note that R v fr w g = Hv and R v fwg = v, and then apply R v fg to the equations used to compute r w . The result is an exact and numerically stable procedure for computing Hv, which takes about as much computation, and is about as local, as a gradient evaluation. We then apply the technique to a one pass gradient calculation algorithm (backpropagation), a relaxation gradient calculation algorithm (recurrent backpropagation), and two stochastic gradient calculation algorithms (Boltzmann Machines and weight perturbation). Finally, we show that this technique can be used at the heart of many iterative techniques for computing various properties of H, obviating any need to calculate the full Hessian. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ackley, D. H., Hinton, G. E., and Sejnowski, T. J. </author> <year> (1985). </year> <title> A learning algorithm for Boltzmann Machines. </title> <journal> Cognitive Science, </journal> <volume> 9 </volume> <pages> 147-169. </pages>
Reference-contexts: as the relaxation equations for computing r w are linear even though those for computing y and E are not, these new relaxation equations are linear. 4.3 Stochastic Boltzmann Machines One might ask whether this technique can be used to derive a Hessian multiplication algorithm for a classic Boltzmann Machine <ref> (Ackley et al., 1985) </ref>, which is discrete and stochastic, unlike its continuous and deterministic cousin to which application of Rfg is simple. <p> statistics are sampled because, at equilibrium, G = p + ij =T (14) where p ij = hs i s j i, G is the asymmetric divergence, an information theoretic measure of the difference between the environmental distribution over the output units and that of the network, as used in <ref> (Ackley et al., 1985) </ref>, T is the temperature, and the + and superscripts indicate the environmental distribution, + for waking and for hallucinating.
Reference: <author> Almeida, L. B. </author> <year> (1987). </year> <title> A learning rule for asynchronous perceptrons with feedback in a combinatorial environment. In (Caudill and Butler, </title> <booktitle> 1987), </booktitle> <pages> pages 609-618. </pages> <note> 11 Alspector, </note> <author> J., Meir, R., Yuhas, B., and Jayakumar, A. </author> <year> (1993). </year> <title> A parallel gradient descent method for learning in analog VLSI neural networks. </title> <editor> In (Hanson et al., </editor> <year> 1993), </year> <pages> pages 836-844. </pages>
Reference: <author> Becker, S. and le Cun, Y. </author> <year> (1989). </year> <title> Improving the convergence of back-propagation learning with second order methods. </title> <editor> In Touretzky, D. S., Hinton, G. E., and Sejnowski, T. J., editors, </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School. </booktitle> <address> Morgan Kaufmann. </address> <note> Also published as Technical Report CRG-TR-88-5, </note> <institution> Department of Computer Science, University of Toronto. </institution>
Reference: <author> Bishop, C. </author> <year> (1992). </year> <title> Exact calculation of the Hessian matrix for the multilayer perceptron. </title> <journal> Neural Computation, </journal> <volume> 4(4) </volume> <pages> 494-501. </pages>
Reference: <author> Buntine, W. and Weigend, A. </author> <year> (1991). </year> <title> Calculating second derivatives on feedforward networks. </title> <journal> IEEE Transactions on Neural Networks. </journal> <note> In submission. </note>
Reference: <author> Caudill, M. and Butler, C., </author> <title> editors (1987). </title> <booktitle> IEEE First International Conference on Neural Networks, </booktitle> <address> San Diego, CA. </address>
Reference: <author> Cauwenberghs, G. </author> <year> (1993). </year> <title> A fast stochastic error-descent algorithm for supervised learning and optimization. </title> <editor> In (Hanson et al., </editor> <year> 1993), </year> <pages> pages 244-251. </pages>
Reference: <author> Flower, B. and Jabri, M. </author> <year> (1993). </year> <title> Summed weight neuron perturbation: An O(n) improvement over weight perturbation. </title>
Reference-contexts: The same technique applies equally well to other perturbative procedures, such as unit perturbation <ref> (Flower and Jabri, 1993) </ref>, and a similar derivation can be used to find the diagonal elements of H, without the need for any additional globally broadcast values. 5 Practical Applications The Rfg technique makes it possible to calculate Hv efficiently.
Reference: <editor> In (Hanson et al., </editor> <year> 1993), </year> <pages> pages 212-219. </pages>
Reference: <editor> Hanson, S. J., Cowan, J. D., and Giles, C. L., editors (1993). </editor> <booktitle> Advances in Neural Information Processing Systems 5. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Hassibi, B. and Stork, D. G. </author> <year> (1993). </year> <title> Second order derivatives for network pruning: Optimal brain surgeon. </title> <editor> In (Hanson et al., </editor> <year> 1993), </year> <pages> pages 164-171. </pages>
Reference: <author> Hinton, G. E. </author> <year> (1987). </year> <title> Connectionist learning procedures. </title> <type> Technical Report CMU-CS-87-115, </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA 15213. </address>
Reference-contexts: We assume that e i depends only on y i , and not on any y j for j i. This is true of most common error measures, such as squared error or cross entropy <ref> (Hinton, 1987) </ref>. 3 We can thus write e i (y i ) as a simple function. <p> However, the technique is immediately applicable to higher order Boltzmann Machines <ref> (Hinton, 1987) </ref>, as well as to Boltzmann Machines with non-binary units (Movellan and McClelland, 1991). 4.4 Weight Perturbation In weight perturbation (Jabri and Flower, 1991; Alspector et al., 1993; Flower and Jabri, 1993; Kirk et al., 1993; Cauwenberghs, 1993) the gradient r w is approximated using only the globally broadcast result
Reference: <author> Jabri, M. and Flower, B. </author> <year> (1991). </year> <title> Weight perturbation: An optimal architecture and learning technique for analog VLSI feedforward and recurrent multilayer networks. </title> <journal> Neural Computation, </journal> <volume> 3(4) </volume> <pages> 546-565. </pages>

Reference: <author> MacKay, D. J. C. </author> <year> (1991). </year> <title> A practical Bayesian framework for back-prop networks. </title> <journal> Neural Computation, </journal> <volume> 4(3) </volume> <pages> 448-472. </pages>
Reference-contexts: There exist algorithms for calculating the full Hessian H (the matrix of second derivative terms 2 E=w i w j of the error E with respect to the weights w) of a backpropagation network (Bishop, 1 1992; Werbos, 1992; Buntine and Weigend, 1991), or reasonable estimates thereof <ref> (MacKay, 1991) </ref>but even storing the full Hessian is impractical for large networks. There is also an algorithm for efficiently computing just the diagonal of the Hessian (Becker and le Cun, 1989; le Cun et al., 1990).
Reference: <author> Mller, M. </author> <year> (1993a). </year> <title> Exact calculation of the product of the Hessian matrix of feed-forward network error functions and a vector in O(n) time. </title> <institution> Daimi PB-432, Computer Science Department, Aarhus University, Denmark. </institution>
Reference-contexts: The same algorithm was also discovered, with yet another derivation, by <ref> (Mller, 1993a) </ref>. For convenience, we will now change our notation for indexing the weights w. Let w be the weights, now doubly indexed by their source and destination units' indices, as in w ij , the weight from unit i to unit j. <p> The first order information used is simply r w (w), while the second-order information is precisely Hv, calculated with the one-sided finite difference approximation of equation (1). It can thus benefit immediately from the exact calculation of Hv. In fact, the Rfbackpropg procedure was independently discovered for that application <ref> (Mller, 1993a) </ref>. The SCG line search proceeds as follows.
Reference: <author> Mller, M. </author> <year> (1993b). </year> <title> A scaled conjugate gradient algorithm for fast supervised learning. </title> <booktitle> Neural Networks, </booktitle> <volume> 6(4) </volume> <pages> 525-533. </pages>
Reference-contexts: Some are approximate while others attempt to find an exact constrained minimum, and some use only the value of the error, while others also make use of the gradient. In particular, the line search used within the Scaled Conjugate Gradient (SCG) optimization procedure, in both its deterministic <ref> (Mller, 1993b) </ref> and stochastic (Mller, 1993c) incarnations, makes use of both first- and second-order information at w to determine how far to move. The first order information used is simply r w (w), while the second-order information is precisely Hv, calculated with the one-sided finite difference approximation of equation (1).
Reference: <author> Mller, M. </author> <year> (1993c). </year> <title> Supervised learning on large redundant training sets. </title> <journal> International Journal of Neural Systems, </journal> <volume> 4(1). </volume>
Reference-contexts: In particular, the line search used within the Scaled Conjugate Gradient (SCG) optimization procedure, in both its deterministic (Mller, 1993b) and stochastic <ref> (Mller, 1993c) </ref> incarnations, makes use of both first- and second-order information at w to determine how far to move. The first order information used is simply r w (w), while the second-order information is precisely Hv, calculated with the one-sided finite difference approximation of equation (1).
Reference: <author> Moody, J. E. </author> <year> (1992). </year> <title> The effective number of parameters: an analysis of generalization and regularization in nonlinear learning systems. </title> <editor> In (Moody et al., </editor> <year> 1992), </year> <pages> pages 847-854. </pages>
Reference: <editor> Moody, J. E., Hanson, S. J., and Lippmann, R. P., editors (1992). </editor> <booktitle> Advances in Neural Information Processing Systems 4. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Movellan, J. R. and McClelland, J. L. </author> <year> (1991). </year> <title> Learning continuous probability distributions with the contrastive Hebbian algorithm. </title> <type> Technical Report PDP.CNS.91.2, </type> <institution> Carnegie Mellon University Dept. of Psychology, </institution> <address> Pittsburgh, PA. </address>
Reference-contexts: However, the technique is immediately applicable to higher order Boltzmann Machines (Hinton, 1987), as well as to Boltzmann Machines with non-binary units <ref> (Movellan and McClelland, 1991) </ref>. 4.4 Weight Perturbation In weight perturbation (Jabri and Flower, 1991; Alspector et al., 1993; Flower and Jabri, 1993; Kirk et al., 1993; Cauwenberghs, 1993) the gradient r w is approximated using only the globally broadcast result of the computation of E (w).
Reference: <author> Pearlmutter, B. A. </author> <year> (1992). </year> <title> Gradient descent: Second-order momentum and saturating error. </title> <editor> In (Moody et al., </editor> <year> 1992), </year> <pages> pages 887-894. </pages>
Reference: <author> Pineda, F. </author> <year> (1987). </year> <title> Generalization of back-propagation to recurrent neural networks. </title> <journal> Physical Review Letters, </journal> 19(59) 2229-2232. 
Reference: <author> Press, W. H., Flannery, B. P., Teukolsky, S. A., and Verrerling, W. T. </author> <year> (1988). </year> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press. </publisher> <address> 12 Skilling, J. </address> <year> (1989). </year> <title> The eigenvalues of mega-dimensional matrices. </title> <editor> In Skilling, J., editor, </editor> <booktitle> Maximum Entropy and Bayesian Methods, </booktitle> <pages> pages 455-466. </pages> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: This squares the condition number, but if H is known to be positive definite, one can instead minimize x T Hx=2 + x b, which does not square the condition number <ref> (Press et al., 1988, page 78) </ref>.
Reference: <author> Watrous, R. </author> <year> (1987). </year> <title> Learning algorithms for connectionist networks: Applied gradient methods of nonlinear optimization. </title>
Reference-contexts: learning algorithms (Widrow et al., 1979; le Cun et al., 1991; Pearlmutter, 1992); in some techniques for predicting generalization rates in neural networks (MacKay, 1991; Moody, 1992); in techniques for enhancing generalization by weight elimination (le Cun et al., 1990; Hassibi and Stork, 1993); and in full second-order optimization methods <ref> (Watrous, 1987) </ref>.
Reference: <author> In (Caudill and Butler, </author> <year> 1987), </year> <pages> pages 619-627. </pages>
Reference-contexts: We assume that e i depends only on y i , and not on any y j for j i. This is true of most common error measures, such as squared error or cross entropy <ref> (Hinton, 1987) </ref>. 3 We can thus write e i (y i ) as a simple function. <p> However, the technique is immediately applicable to higher order Boltzmann Machines <ref> (Hinton, 1987) </ref>, as well as to Boltzmann Machines with non-binary units (Movellan and McClelland, 1991). 4.4 Weight Perturbation In weight perturbation (Jabri and Flower, 1991; Alspector et al., 1993; Flower and Jabri, 1993; Kirk et al., 1993; Cauwenberghs, 1993) the gradient r w is approximated using only the globally broadcast result
Reference: <author> Werbos, P. J. </author> <year> (1974). </year> <title> Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. </title> <type> PhD thesis, </type> <institution> Harvard University. </institution>
Reference-contexts: As is usual, quantities which occur on the left sides of the equations are treated computationally as variables, and calculated in topological order, which is assumed to exist because the weights, regarded as a connection matrix, is zero-diagonal and can be put into triangular form <ref> (Werbos, 1974) </ref>. The forward computation of the network is 2 x i = j 2 This compact form of the backpropagation equations, due to Fernando Pineda, unifies the special cases of input units, hidden units, and output units.
Reference: <author> Werbos, P. J. </author> <year> (1988). </year> <title> Backpropagation: Past and future. </title> <booktitle> In IEEE International Conference on Neural Networks, </booktitle> <volume> volume I, </volume> <pages> pages 343-353, </pages> <address> San Diego, CA. </address>
Reference-contexts: The Rfbackpropg algorithm was independently discovered by <ref> (Werbos, 1988, eq. 14) </ref>, who derived it as a backpropagation process to calculate Hv = r w (v r w E), where r w E is also calculated by backpropagation. <p> This application of fast exact multiplication by the Hessian, in particular Rfbackpropg, was independently noted in <ref> (Werbos, 1988) </ref>. 9 5.3 Step Size and Line Search Many optimization techniques repeatedly choose a direction v, and then proceed along that direction some distance m, which takes the system to the constrained minimum of E (w + mv).
Reference: <author> Werbos, P. J. </author> <year> (1992). </year> <title> Neural networks, system identification, and control in the chemical processindustries. In White, </title> <editor> D. A. and Sofge, D. A., editors, </editor> <booktitle> Handbook of Intelligent ControlNeural, Fuzzy, and Adaptive approaches, chapter 10, </booktitle> <pages> pages 283-356. </pages> <note> Van Norstrand Reinhold. see section 10.7. </note>
Reference: <author> Widrow, B., McCool, J. M., Larimore, M. G., and Johnson, Jr., C. R. </author> <year> (1979). </year> <title> Stationary and nonstationary learning characteristics of the LMS adaptive filter. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 64 </volume> <pages> 1151-1162. 13 </pages>
References-found: 29

