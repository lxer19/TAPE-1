URL: http://ftp.ics.uci.edu/pub/smyth/papers/nips96.ps.Z
Refering-URL: http://www.ics.uci.edu/~mlearn/MLPapers.html
Root-URL: 
Email: smyth@ics.uci.edu  
Title: Clustering Sequences with Hidden Markov Models  
Author: Padhraic Smyth 
Address: CA 92697-3425  
Affiliation: Information and Computer Science University of California, Irvine  
Abstract: This paper discusses a probabilistic model-based approach to clustering sequences, using hidden Markov models (HMMs). The problem can be framed as a generalization of the standard mixture model approach to clustering in feature space. Two primary issues are addressed. First, a novel parameter initialization procedure is proposed, and second, the more difficult problem of determining the number of clusters K, from the data, is investigated. Experimental results indicate that the proposed techniques are useful for revealing hidden cluster structure in data sets of sequences. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Baldi, P. and Y. Chauvin, </author> <title> `Hierarchical hybrid modeling, HMM/NN architectures, and protein applications,' </title> <journal> Neural Computation, </journal> <volume> 8(6), </volume> <pages> 1541-1565, </pages> <year> 1996. </year>
Reference: <author> Krogh, A. et al., </author> <title> `Hidden Markov models in computational biology: applications to protein modeling,' </title> <journal> it J. Mol. Bio., </journal> <volume> 235 </volume> <pages> 1501-1531, </pages> <year> 1994. </year>
Reference: <author> Juang, B. H., and L. R. Rabiner, </author> <title> `A probabilistic distance measure for hidden Markov models,' </title> <journal> AT&T Technical Journal, vol.64, no.2, </journal> <month> February </month> <year> 1985. </year>
Reference-contexts: The "symmetrized distance" L ij = 1=2 (L (S i jM j ) + L (S j jM i )) can be shown to be an appropriate measure of dissimilarity between models M i and M j <ref> (Juang and Rabiner 1985) </ref>. For the results described in this paper, hierarchical clustering was used to generate K clusters from the symmetrized distance matrix. The "furthest-neighbor" merging heuristic was used to encourage compact clusters and worked well empirically, although there is no particular reason to use only this method.
Reference: <author> Rabiner, L. R., C. H. Lee, B. H. Juang, and J. G. Wilpon, </author> <title> `HMM clustering for connected word recognition,' </title> <booktitle> Proc. Int. Conf. Ac. Speech. Sig. Proc, </booktitle> <publisher> IEEE Press, </publisher> <pages> 405-408, </pages> <year> 1989. </year>
Reference: <author> Shao, J., </author> <title> `Linear model selection by cross-validation,' </title> <journal> J. Am. Stat. Assoc., </journal> <volume> 88(422), </volume> <pages> 486-494, </pages> <year> 1993. </year>
Reference-contexts: In Smyth (1996) it was found that for standard multivariate Gaussian mixture modeling, the standard v-fold cross-validation techniques (with say v = 10) performed poorly in terms of selecting the correct model on simulated data. Instead Monte-Carlo cross-validation <ref> (Shao, 1993) </ref> was found to be much more stable: the data are partitioned into a fraction fi for testing and 1 fi for training, and this procedure is repeated M times where the partitions are randomly chosen on each run (i.e., need not be disjoint).
Reference: <author> Smyth, P., </author> <title> `Clustering using Monte-Carlo cross validation,' </title> <booktitle> in Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press, </publisher> <address> pp.126-133, </address> <year> 1996. </year>
Reference-contexts: In general, as the total amount of data increases relative to the model complexity, the optimal fi becomes larger. For the mixture clustering problem fi = 0:5 was found empirically to work well <ref> (Smyth, 1996) </ref> and is used in the results reported here. 3.3 Experimental Results The same data set as described earlier was used where now K is not known a priori. The 40 sequences were randomly partitioned 20 times into training and test cross-validation sets. <p> If the data were not generated by the assumed form of the model, the posterior distribution on K will tend to be peaked about the model size which is closest (in K-L distance) to the true model. Results in the context of Gaussian mixture clustering <ref> (Smyth 1996) </ref> have shown that the Monte Carlo cross-validation technique performs as well as the better Bayesian approximation methods and is more robust then penalized likelihood methods such as BIC. In conclusion, we have shown that model-based probabilistic clustering can be generalized from feature-space clustering to sequence clustering.
Reference: <author> Zeevi, A. J., Meir, R., Adler, R., </author> <title> `Time series prediction using mixtures of experts,' </title> <booktitle> in this volume, </booktitle> <year> 1997. </year>
References-found: 7

