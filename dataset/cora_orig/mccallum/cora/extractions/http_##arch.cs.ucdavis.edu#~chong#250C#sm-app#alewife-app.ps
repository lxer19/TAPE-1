URL: http://arch.cs.ucdavis.edu/~chong/250C/sm-app/alewife-app.ps
Refering-URL: http://arch.cs.ucdavis.edu/~chong/250C/sm-app/
Root-URL: http://www.cs.ucdavis.edu
Title: Application Performance on the MIT Alewife Multiprocessor  
Author: Frederic T. Chong Beng-Hong Lim Ricardo Bianchini John Kubiatowicz and Anant Agarwal 
Keyword: distributed shared memory, multiprocessor, performance metrics, applications, fine grain  
Affiliation: Dept. of Computer Science, University of California at Davis IBM T.J. Watson Research Center COPPE Systems Engineering, UFRJ/Brazil Lab. for Computer Science, Massachusetts Institute of Technology  
Abstract: This study reports on the performance of several applications on the Alewife machine, focusing on emerging applications and evolving architectural mechanisms. It shows that low-latency cache miss handling mechanisms for both local and remote accesses in Alewife make these emerging applications viable candidates for shared-memory parallel processing. The results show that efficient shared memory is an excellent communication mechanism, even for fine-grain applications that do not re-use data. Such applications are thought to favor message-passing. As expected, traditional coarse-grain applications perform well with Alewife's mechanisms. The results also confirm that hardware support for limited sharing is adequate for a broad range of applications, even on large numbers of processors. Additionally, modeling local cache-miss behavior is important for machines such as Alewife, where remote-miss latencies are only five times longer than local miss latencies. We introduce two novel performance metrics that account for the effect of local misses and are more accurate than previously proposed metrics. We conclude that most applications perform well on Alewife. In particular, fine-grain applications can take advantage of Alewife's high integration and efficiency to achieve a new level of performance on scalable shared-memory machines. 
Abstract-found: 1
Intro-found: 1
Reference: [ABC + 95] <author> Anant Agarwal, Ricardo Bianchini, David Chaiken, Kirk Johnson, David Kranz, John Ku-biatowicz, Beng-Hong Lim, Ken Mackenzie, and Donald Yeung. </author> <title> The MIT Alewife machine: Architecture and performance. </title> <booktitle> In Proc. 22nd Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Benchmark suites and architectural mechanisms constantly evolve from the dynamics of the architecture-applications symbiosis. This study reports on the performance of several applications on the Alewife machine <ref> [ABC + 95] </ref> (see Sidebar A), focusing on fine-grain applications and evolving architectural mechanisms. The results show that low-latency miss-handling mechanisms for both local and remote accesses in Alewife make fine-grain applications viable candidates for shared-memory parallel processing.
Reference: [AG88] <author> Anant Agarwal and Anoop Gupta. </author> <title> Memory-Reference Characteristics of Multiprocessor Applications under MACH. </title> <booktitle> In Proceedings of ACM SIGMETRICS 1988, </booktitle> <pages> pages 215-225, </pages> <month> May </month> <year> 1988. </year>
Reference: [Bai94] <author> D. Bailey et al. </author> <title> The NAS Parallel Benchmarks. </title> <type> Technical Report RNR-94-007, </type> <institution> NASA Ames Research Center, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Table 1 provides a short description of each of the applications and their input parameters. MP3D, Barnes, Locus, Chol, and Water are from the SPLASH suite [SWG92]. Appbt and MG are part of the NAS parallel benchmarks <ref> [Bai94] </ref>. The rest of the applications are engineering-type kernels from the University of Rochester, MIT [CA96], and Berkeley [CDG + 93]. We categorize the applications into traditional, coarse-grain applications and emerging, fine-grain applications. The traditional, coarse-grain applications are applications that appear in most studies of shared-memory applications and architectures.
Reference: [CA96] <author> Frederic T. Chong and Anant Agarwal. </author> <title> Shared memory versus message passing for iterative solution of sparse, irregular problems. </title> <type> Technical Report MIT/LCS/TR-697, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> September </month> <year> 1996. </year>
Reference-contexts: MP3D, Barnes, Locus, Chol, and Water are from the SPLASH suite [SWG92]. Appbt and MG are part of the NAS parallel benchmarks [Bai94]. The rest of the applications are engineering-type kernels from the University of Rochester, MIT <ref> [CA96] </ref>, and Berkeley [CDG + 93]. We categorize the applications into traditional, coarse-grain applications and emerging, fine-grain applications. The traditional, coarse-grain applications are applications that appear in most studies of shared-memory applications and architectures. These include Appbt, Barnes , CGRID, Chol, FFT , Gauss, Locus, MG, Msort, and Water.
Reference: [CDG + 93] <author> David E. Culler, Andrea Dusseau, Seth Copen Goldstein, Arvind Krishnamurthy, Steven Lumetta, Thorsten von Eicken, and Katherine Yelick. </author> <title> Parallel programming in Split-C. </title> <booktitle> In Supercomputing, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: MP3D, Barnes, Locus, Chol, and Water are from the SPLASH suite [SWG92]. Appbt and MG are part of the NAS parallel benchmarks [Bai94]. The rest of the applications are engineering-type kernels from the University of Rochester, MIT [CA96], and Berkeley <ref> [CDG + 93] </ref>. We categorize the applications into traditional, coarse-grain applications and emerging, fine-grain applications. The traditional, coarse-grain applications are applications that appear in most studies of shared-memory applications and architectures. These include Appbt, Barnes , CGRID, Chol, FFT , Gauss, Locus, MG, Msort, and Water. <p> As discussed earlier, local cache behavior accounts for much of the good speedup. On 32 Alewife processors, each edge takes 107 cycles of computation. For comparison, only a few results are available in the literature. A Berkeley Split-C study <ref> [CDG + 93] </ref> achieved 56 cycles 7 per edge with message-passing implementation on the Thinking Machines CM-5 on 64 processors. A Wisconsin study [CLR94] simulates shared-memory and message-passing machines with hardware configurations based closely on the CM-5.
Reference: [CLR94] <author> Satish Chandra, James R. Larus, and Anne Rogers. </author> <title> Where is time spent in message-passing and shared-memory programs. </title> <booktitle> In ASPLOS VI, </booktitle> <pages> pages 61-73, </pages> <address> San Jose, California, </address> <year> 1994. </year>
Reference-contexts: On 32 Alewife processors, each edge takes 107 cycles of computation. For comparison, only a few results are available in the literature. A Berkeley Split-C study [CDG + 93] achieved 56 cycles 7 per edge with message-passing implementation on the Thinking Machines CM-5 on 64 processors. A Wisconsin study <ref> [CLR94] </ref> simulates shared-memory and message-passing machines with hardware configurations based closely on the CM-5. It finds that an invalidation-based shared-memory implementation (172 cycles/edge) is twice as slow as a message-passing implementation (86 cycles/edge). Alewife performs substantially better than previous shared-memory results and competitively with message-passing implementations.
Reference: [DRPS87] <author> F. Darema-Rogers, G. F. Pfister, and K. </author> <title> So. Memory Access Patterns of Parallel Scientific Programs. </title> <booktitle> In Proceedings of ACM SIGMETRICS 1987, </booktitle> <pages> pages 46-58, </pages> <month> May </month> <year> 1987. </year>
Reference: [EK88] <author> S. J. Eggers and R. H. Katz. </author> <title> A Characterization of Sharing in Parallel Programs and Its Application to Coherency Protocol Evaluation. </title> <booktitle> In Proceedings of the 15th International Symposium on Computer Architecture, </booktitle> <address> New York, </address> <month> June </month> <year> 1988. </year> <note> IEEE. </note>
Reference: [LLG + 92] <author> Daniel Lenoski, James Laudon, Kourosh Gharachorloo, Wolf-Dietrich Weber, Anoop Gupta, John Hennessy, Mark Horowitz, and Monica S. Lam. </author> <title> The stanford Dash multiprocessor. </title> <journal> Computer, </journal> <volume> 25(3) </volume> <pages> 63-80, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: While the particles in the simulation are represented by data that are statically assigned to processors, the wind tunnel through which the particles move is represented by data that migrate frequently. Figure 4 compares the performance of MP3D on Alewife with Stanford DASH <ref> [LLG + 92] </ref>. Alewife achieves substantially higher speedups than DASH on MP3D. The primary reason is Alewife's coherence protocol that is better suited for migratory data.
Reference: [SWG92] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford parallel applications for shared-memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Computation is usually partitioned and scheduled statically among the threads that synchronize with locks and barriers. Table 1 provides a short description of each of the applications and their input parameters. MP3D, Barnes, Locus, Chol, and Water are from the SPLASH suite <ref> [SWG92] </ref>. Appbt and MG are part of the NAS parallel benchmarks [Bai94]. The rest of the applications are engineering-type kernels from the University of Rochester, MIT [CA96], and Berkeley [CDG + 93]. We categorize the applications into traditional, coarse-grain applications and emerging, fine-grain applications.

References-found: 10

