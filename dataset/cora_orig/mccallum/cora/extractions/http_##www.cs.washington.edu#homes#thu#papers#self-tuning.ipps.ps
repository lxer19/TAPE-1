URL: http://www.cs.washington.edu/homes/thu/papers/self-tuning.ipps.ps
Refering-URL: http://www.cs.washington.edu/homes/thu/papers/pps.abstract.html
Root-URL: 
Title: Maximizing Speedup through Self-Tuning of Processor Allocation  
Author: Thu D. Nguyen, Raj Vaswani, and John Zahorjan 
Date: 1996  
Note: Appears in Proceedings of the International Parallel Processing Symposium  
Address: Box 352350  Seattle, WA 98195-2350 USA  
Affiliation: Department of Computer Science and Engineering,  University of Washington,  
Abstract: We address the problem of maximizing application speedup through runtime, self-selection of an appropriate number of processors on which to run. Automatic, run-time selection of processor allocations is important because many parallel applications exhibit peak speedups at allocations that are data or time dependent. We propose the use of a runtime system that: (a) dynamically measures job efficiencies at different allocations, (b) uses these measurements to calculate speedups, and (c) automatically adjusts a job's processor allocation to maximize its speedup. Using a set of 10 applications that includes both hand-coded parallel programs and compiler-parallelized sequential programs, we show that our runtime system can reliably determine dynamic allocations that match the best possible static allocation, and that it has the potential to find dynamic allocations that outperform any static allocation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Berry, D. Chen, P. Koss, D. Kuck, S. Lo, Y. Pang, L. Pointer, R. Roloff, A. Sameh, E. Clementi, S. Chin, D. Schneider, G. Fox, P. Messina, D. Walker, C. Hsiung, J. Scharzmeier, K. Lue, S. Orszag, F. Seidl, O. Johnson, R. Goodrum, and J. Martin. </author> <title> The PERFECT Club benchmarks: Effective performance evaluation of supercomputers. </title> <journal> The International Journal of Supercomputer Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <year> 1989. </year>
Reference-contexts: In what follows, we develop four self-tuning approaches of increasing sophistication, and assess their performance using 10 parallel applications that include both hand-coded parallel programs from the SPLASH benchmark suite [16] and compiler-parallelized sequential programs from the PERFECT Club benchmark suite <ref> [1] </ref>. The remainder of the paper is organized as follows. Section 2 discusses the applications that we use and documents our experimental platform. Section 3 discusses the runtime measures used by our search procedure. Section 4 defines the basic self-tuning schemes we employ, while Section 5 examines their performance experimentally. <p> We evaluate self-tuning using a set of 10 applications that include both hand-coded parallel and compiler-parallelized sequential programs. Hand-coded applications include a local benchmark, Grav, and 2 SPLASH benchmarks [16], Barnes and MP3D. Compiler-parallelized sequential programs include 6 Perfect Club benchmarks <ref> [1] </ref> that we were 2 In [14], we found that five of the ten SPLASH applications and all seven of the Perfect Club applications we could compile were iterative. <p> A basic Self-Tuning algorithm We start with two assumptions: * Speedup is a single variable function, S (p) : I ! R, with domain <ref> [1; P ] </ref>. * S (p) can be calculated using equation 2 for any p, 1 p P , by measuring E (p) for any one iteration. <p> Since measured speedups can never be super-linear (a direct consequence of equations 1 and 2), we know that the globally best number of processors must fall in the interval [S (P ); P ]. Our search therefore starts in this interval instead of the interval <ref> [1; P ] </ref>. Our search within [S (P ); P ] is identical to MGS, with the exception, of course, that we must address non-unimodal speedup functions.
Reference: [2] <author> E. C. Cooper and R. P. Draves. </author> <title> C Threads. </title> <type> Technical Report CMU-CS-88-154, </type> <institution> Department of Computer Science, Carnegie-Mellon University, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: The number of iterations in these applications ranges from 50 to 2000. We use the KSR KAP preprocessor to parallelize sequential applications, and the KSR PRESTO runtime system and CThreads <ref> [2] </ref>, an efficient user-level threads package, as the vehicles of parallelism. We instrument CThreads using the KSR-2's event monitors, allowing us to perform runtime efficiency measurements. We rely on applications to call into the runtime system at the beginning of each iteration.
Reference: [3] <author> D. Ghosal, G. Serazzi, and S. K. Tripathi. </author> <title> The processor working set and its use in scheduling multiprocessor systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(5) </volume> <pages> 443-53, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: A compiler-parallelized job's processor allocation should then be changed only at these phase boundaries. 7. Related work Many researchers have studied the use of application characteristics by processor schedulers of multiprogrammed systems <ref> [8, 7, 3, 15] </ref>. This body of work differs from ours in that its goal is to determine an appropriate allocation to each of several simultaneously scheduled jobs, typically with the goal of minimizing average response time.
Reference: [4] <author> T. E. Jeremiassen. </author> <title> Using Compile-Time Analysis and Transformation to Reduce False Sharing on Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Washington, </institution> <year> 1995. </year>
Reference-contexts: We rely on applications to call into the runtime system at the beginning of each iteration. For this study, we have inserted these calls by hand. In many instances, however, we believe that it would be possible for compilers to detect iterative behavior and automatically insert such calls <ref> [4] </ref>. We also modify all hand-coded applications to dynamically match the number of threads to the number of processors at the beginning of each iteration to avoid loss of efficiency due to dynamic adjustment of processor allocations.
Reference: [5] <institution> Kendall Square Research Inc., </institution> <address> 170 Tracer Lane, Waltham, MA 02154. </address> <booktitle> KSR/Series Principles of Operation, </booktitle> <year> 1994. </year>
Reference-contexts: Section 6 extends the three basic schemes to allow more fine grained scheduling decisions and examines the implications to performance. Section 7 discusses related work. Section 8 concludes the paper. 2. Experimental environment All measurements were done on a Kendall Square Research KSR-2 COMA shared memory multiprocessor <ref> [5] </ref>. We rely on the hardware monitoring unit available on each node of the KSR-2, the event monitor, to perform runtime measurements of application efficiency.
Reference: [6] <author> S. Kirkpatrick, J. C. D. Gellat, and M. P. Vecchi. </author> <title> Optimization by simulated annealing. </title> <journal> Science, </journal> <volume> 220(4598) </volume> <pages> 671-680, </pages> <month> May </month> <year> 1983. </year>
Reference-contexts: To account for the relationship of the performance in one phase to the allocation decisions made for other phases, a more complicated search procedure is required. In particular, we implemented a randomized search technique, called inter-dependent multi-phase self-tuning, based on simulated annealing <ref> [6] </ref> and a heuristic-based approach to choosing the initial vector (p 1 ; p 2 ; :::; p N ). We refer the reader to the full version of this paper [13] for a more detailed description of inter-dependent multi-phase self-tuning.
Reference: [7] <author> S. T. Leutenegger and M. K. Vernon. </author> <title> The performance of multiprogrammed multiprocessor scheduling policies. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference, </booktitle> <pages> pages 226-236, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: A compiler-parallelized job's processor allocation should then be changed only at these phase boundaries. 7. Related work Many researchers have studied the use of application characteristics by processor schedulers of multiprogrammed systems <ref> [8, 7, 3, 15] </ref>. This body of work differs from ours in that its goal is to determine an appropriate allocation to each of several simultaneously scheduled jobs, typically with the goal of minimizing average response time.
Reference: [8] <author> S. Majumdar, D. L. Eager, and R. B. Bunt. </author> <title> Scheduling in Multiprogrammed Parallel Systems. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference, </booktitle> <pages> pages 104-113, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: A compiler-parallelized job's processor allocation should then be changed only at these phase boundaries. 7. Related work Many researchers have studied the use of application characteristics by processor schedulers of multiprogrammed systems <ref> [8, 7, 3, 15] </ref>. This body of work differs from ours in that its goal is to determine an appropriate allocation to each of several simultaneously scheduled jobs, typically with the goal of minimizing average response time.
Reference: [9] <author> C. McCann, R. Vaswani, and J. Zahorjan. </author> <title> A dynamic processor allocation policy for multiprogrammed shared-memory multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(2) </volume> <pages> 146-178, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Furthermore, all of these studies assume that accurate historical performance data is provided to the scheduler simultaneously with job submission. McCann et. al. <ref> [9] </ref> have proposed a dynamic scheduler that uses application provided runtime idleness information to dynamically adjust processor allocations to improve processor utilization.
Reference: [10] <author> G. P. McCormick. </author> <title> Nonlinear Programming. </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1983. </year>
Reference-contexts: Our runtime system relies on appropriate hardware support to dynamically measure application efficiencies at different processor allocations. It uses these measurements to calculate speedup, and then employs an optimization technique that is an adaptation of the method of golden sections (MGS) <ref> [10] </ref> to find the best allocation. MGS is a simple search procedure that finds the maximum of a unimodal function over a finite interval by iteratively using computed function values to narrow the interval in which the maximum may occur 1 . <p> Thus, multiple evaluations of S (p) at p's far away from the optimal value can lengthen the very execution that we are trying to minimize. As previously mentioned, we base our current implementation of self-tuning on a simple optimization technique, MGS <ref> [10] </ref>, which searches for the maximum of a unimodal function over a finite interval by iteratively using computed function values to narrow the interval in which the maximum may occur. Our adaptation of this technique begins by executing one iteration using all P available processors.
Reference: [11] <author> P. Moller-Nielsen and J. Staunstrup. Problem-heap: </author> <title> a paradigm for multiprocessor algorithms. </title> <journal> Parallel Computing, </journal> <volume> 4(1) </volume> <pages> 63-74, </pages> <month> Feb. </month> <year> 1987. </year>
Reference-contexts: Furthermore, we believe that, with some modifications, our algorithm could also be applicable to non-iterative applications that use general runtime work organization paradigms, such as work-heap <ref> [11, 18] </ref>. able to compile (ADM, ARC2D, DYFESM, FLO52, QCD, and TRACK), and an industrial fluids-dynamic application, USAero (obtained from Analytical Methods, Inc.).
Reference: [12] <author> A. J. Musciano and T. L. Sterling. </author> <title> Efficient dynamic scheduling of medium-grained tasks for general purpose parallel processing. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <year> 1988. </year>
Reference-contexts: In this study, we have chosen to use efficiency because it is directly related to speedup (which itself is not directly measurable). It is well known that loss of efficiency in shared memory systems arises from parallelization overhead, system overhead, idleness, and communication <ref> [12] </ref>. We have shown previously that we can accurately predict application efficiency by measuring only system overhead, idleness, and processor stall 3 ; parallelization overhead is typically small [14].
Reference: [13] <author> T. D. Nguyen, R. Vaswani, and J. Zahorjan. </author> <title> Maximizing speedup through self-tuning of processor allocation. </title> <type> Technical Report UW-CSE-95-09-02, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> Sept. 95. </month>
Reference-contexts: Space limitations preclude our providing graphs for all applications these graphs are available in the long version of this paper <ref> [13] </ref>. From these representative graphs, we make the following observations. Self-tuning imposes very little overhead. Applications that do not experience significant slowdown within the available number of processors exhibit comparable speedup under all forms of self-tuning as under no-tuning (e.g. Barnes). Basic self-tuning can significantly improve performance over no-tuning. <p> In particular, we implemented a randomized search technique, called inter-dependent multi-phase self-tuning, based on simulated annealing [6] and a heuristic-based approach to choosing the initial vector (p 1 ; p 2 ; :::; p N ). We refer the reader to the full version of this paper <ref> [13] </ref> for a more detailed description of inter-dependent multi-phase self-tuning. Multi-phase self-tuning obviously requires that applications' gross iterations exhibit internal phase structure. Of the hand-coded applications, only Grav has multiple significant phases of execution within a single iteration.
Reference: [14] <author> T. D. Nguyen, R. Vaswani, and J. Zahorjan. </author> <title> On scheduling implications of application characteristics. </title> <type> Technical report, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <note> in preparation. </note>
Reference-contexts: Empirical evidence shows that successive iterations tend to behave similarly, so that measurements taken for a particular iteration are good predictors of near future behavior <ref> [14] </ref>. Therefore, for such applications, we equate a measurement interval to an application iteration, providing a basis by which to reasonably compare a job's performance as its processor allocation is varied. We call the process by which our runtime system determines the best number of processors to use self-tuning. <p> We evaluate self-tuning using a set of 10 applications that include both hand-coded parallel and compiler-parallelized sequential programs. Hand-coded applications include a local benchmark, Grav, and 2 SPLASH benchmarks [16], Barnes and MP3D. Compiler-parallelized sequential programs include 6 Perfect Club benchmarks [1] that we were 2 In <ref> [14] </ref>, we found that five of the ten SPLASH applications and all seven of the Perfect Club applications we could compile were iterative. <p> It is well known that loss of efficiency in shared memory systems arises from parallelization overhead, system overhead, idleness, and communication [12]. We have shown previously that we can accurately predict application efficiency by measuring only system overhead, idleness, and processor stall 3 ; parallelization overhead is typically small <ref> [14] </ref>. Thus, we require only estimates of the other three components to accurately assess efficiency On the KSR-2, we rely on a combination of hardware and software support to measure overheads. <p> Our search within [S (P ); P ] is identical to MGS, with the exception, of course, that we must address non-unimodal speedup functions. Fortunately, measurements of a large number of parallel benchmarks show that, except for local variations, most speedup functions are unimodal over substantial ranges of processors <ref> [14] </ref>. Thus, we employ a simple greedy heuristic to deal with non-unimodal speedup functions. <p> On the other hand, long term changes in speedup can lead to noticeable performance degradation. As can be seen in Figure 1, the performance of USAero, an application whose speedup does change with time <ref> [14] </ref>, is indeed enhanced by change-driven self-tuning. Time-driven self-tuning is not useful for the applications we studied. Time-driven self-tuning is meant to address the situation in which a job's speedup changes in the middle of self-tuning but stabilizes before self-tuning completes, possibly trapping change-driven self-tuning into a poor allocation choice.
Reference: [15] <author> E. Rosti, E. Smirni, L. W. Dowdy, G. Serazzi, and B. M. Carl-son. </author> <title> Robust partitioning policies of multiprocessor systems. Performance Evaluation, </title> <address> 19(2-3):141-65, </address> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: A compiler-parallelized job's processor allocation should then be changed only at these phase boundaries. 7. Related work Many researchers have studied the use of application characteristics by processor schedulers of multiprogrammed systems <ref> [8, 7, 3, 15] </ref>. This body of work differs from ours in that its goal is to determine an appropriate allocation to each of several simultaneously scheduled jobs, typically with the goal of minimizing average response time.
Reference: [16] <author> J. P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared-memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <year> 1992. </year>
Reference-contexts: We call the process by which our runtime system determines the best number of processors to use self-tuning. In what follows, we develop four self-tuning approaches of increasing sophistication, and assess their performance using 10 parallel applications that include both hand-coded parallel programs from the SPLASH benchmark suite <ref> [16] </ref> and compiler-parallelized sequential programs from the PERFECT Club benchmark suite [1]. The remainder of the paper is organized as follows. Section 2 discusses the applications that we use and documents our experimental platform. Section 3 discusses the runtime measures used by our search procedure. <p> We evaluate self-tuning using a set of 10 applications that include both hand-coded parallel and compiler-parallelized sequential programs. Hand-coded applications include a local benchmark, Grav, and 2 SPLASH benchmarks <ref> [16] </ref>, Barnes and MP3D. Compiler-parallelized sequential programs include 6 Perfect Club benchmarks [1] that we were 2 In [14], we found that five of the ten SPLASH applications and all seven of the Perfect Club applications we could compile were iterative.
Reference: [17] <author> A. Torn and A. Zilinskas. </author> <title> Global Optimization. </title> <publisher> Springer-Verlag, </publisher> <year> 1987. </year>
Reference-contexts: We found that the first assumption was never violated in our experimental environment. Therefore, although it was theoretically possible to extend self-tuning in a number of ways to better deal with non-unimodal speedup func-tions <ref> [17] </ref>, it would have been impossible to evaluate their cost-effectiveness. For this reason, we did not implement any of these extensions. On the other hand, we found that applications can indeed violate our latter two assumptions.
Reference: [18] <author> M. Vandevoorde and E. Roberts. WorkCrews: </author> <title> an abstraction for controlling parallelism. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 17(4) </volume> <pages> 347-66, </pages> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: Furthermore, we believe that, with some modifications, our algorithm could also be applicable to non-iterative applications that use general runtime work organization paradigms, such as work-heap <ref> [11, 18] </ref>. able to compile (ADM, ARC2D, DYFESM, FLO52, QCD, and TRACK), and an industrial fluids-dynamic application, USAero (obtained from Analytical Methods, Inc.).
References-found: 18

