URL: ftp://ftp.cs.indiana.edu/pub/techreports/TR424.ps.Z
Refering-URL: http://www.cs.indiana.edu/ftp/techreports/index.html
Root-URL: http://www.cs.indiana.edu
Title: PARALLEL DYNAMIC PROGRAMMING  
Author: by Phillip Gnassi Bradford 
Degree: Submitted to the faculty of the Graduate School in partial fulfillment of the requirements for the degree Doctor of Philosophy in the  
Date: December 15, 1994  
Affiliation: Department of Computer Science Indiana University  
Abstract-found: 0
Intro-found: 1
Reference: <author> A. Aggarwal, M. M. Klawe, S. Moran, P. Shor, and R. Wilbur: </author> <title> "Geometric Applications of a Matrix Searching Algorithm," </title> <journal> Algorithmica, </journal> <volume> Vol. 2, </volume> <pages> 195-208, </pages> <year> 1987. </year>
Reference-contexts: As before, begin assuming the inductive invariant. Also, all jumpers in the next algorithm are jumpers that get their sp values from the inner band where the jumpers themselves are in unit rows or columns of the outer band. The following algorithm is striking similar to those discussed in <ref> (Aggarwal et al., 1987) </ref> and (Aggarwal and Park, 1988). This key observation leads to some complexity improvements.
Reference: <author> A. Aggarwal and J. K. Park: </author> <title> "Notes on Searching Multidimensional Monotone Arrays," </title> <booktitle> Proceedings of the 29th Annual IEEE Symposium on the Foundations of Computer Science (FOCS), </booktitle> <publisher> IEEE Press, </publisher> <pages> 497-512, </pages> <year> 1988. </year> <note> Full versions to appear in the Journal of Algorithms. </note>
Reference-contexts: Also, all jumpers in the next algorithm are jumpers that get their sp values from the inner band where the jumpers themselves are in unit rows or columns of the outer band. The following algorithm is striking similar to those discussed in (Aggarwal et al., 1987) and <ref> (Aggarwal and Park, 1988) </ref>. This key observation leads to some complexity improvements. <p> Hence, by the results in <ref> (Aggarwal and Park, 1988) </ref> and in (Atallah, 1991) our algorithm runs in O ((lg 2 n)(lg lg n)) time using n=lg lg n processors on a common-CRCW PRAM or in O (lg 2 n) time using n processors on a EREW PRAM.
Reference: <author> A. V. Aho, J. E. Hopcroft, and J. D. Ullman: </author> <title> The Design and Analysis of Computer Algorithms, </title> <publisher> Addison-Wesley, </publisher> <year> 1975. </year>
Reference: <author> R. Anderson and E. W. Mayr: </author> <title> "Parallelism and the Minimal Path Problem," </title> <journal> Information Processing Letters, </journal> <volume> Vol. 24, </volume> <pages> 121-126, </pages> <year> 1987. </year>
Reference: <author> R. Anderson and E. W. Mayr: </author> <title> "Parallelism and Greedy Algorithms," </title> <booktitle> Advances in Computing Research, </booktitle> <volume> Vol. 4, </volume> <pages> 17-38, </pages> <publisher> JAI Press, </publisher> <year> 1987. </year>
Reference: <author> A. Apostolico, M. J. Atallah, L. L. Larmore, and S. H. McFaddin: </author> <title> "Efficient Parallel Algorithms for String Editing and Related Problems," </title> <journal> SIAM Journal on Computing, </journal> <volume> Vol. 19, No. 5, </volume> <pages> 968-988, </pages> <month> Oct. </month> <year> 1990. </year> <title> 126 BIBLIOGRAPHY 127 T. Archibald: "Parallel Dynamic Programming," </title> <booktitle> in Advances in Parallel Algorithms, </booktitle> <editor> L. Kronsjo and D. Shumsheruddin (Editors), </editor> <publisher> John Wiley and Sons, </publisher> <pages> 343-367, </pages> <year> 1992. </year>
Reference: <author> M. J. Atallah, S. R. Kosaraju, L. L. Larmore, G. L. Miller, and S.-H. Teng: </author> <title> "Constructing Trees in Parallel," </title> <booktitle> Proc. 1st Symp. on Parallel Algorithms and Architectures (SPAA), </booktitle> <publisher> ACM Press, </publisher> <pages> 499-513, </pages> <year> 1989. </year>
Reference-contexts: dissertation represents strides in this area, hopefully there will be many more. 122 Directions of Further Research and Conclusions 123 9.1.1 Optimal Binary Search Trees The development of algorithms for building optimal binary search trees in polylog parallel time with low processor complexity is cited as an open problem in <ref> (Atallah et al., 1989) </ref> and (Larmore, Przytycka, and Rytter, 1993). By low processor complexity we mean n k for some constant k &lt; 6, The best sequential algorithm for building optimal binary search trees takes fi (n 2 ) time (Knuth, 1973). <p> This is given as a dynamic programming problem in Chapter 1 and a O (lg 2 n) time and n 6 =lg n processor solution is given in Chapter 4. 9.1.2 Previous Results In <ref> (Atallah et al., 1989) </ref>, among other things, the authors give a nearly optimal algorithm for building optimal alphabetic binary search trees in O (lg 2 n) time and using n 2 =lg 2 n processors.
Reference: <author> M. J. Atallah and S. R. Kosaraju: </author> <title> "An Efficient Parallel Algorithm for the Row Minima of a Totally Monotone Matrix," </title> <booktitle> Proceedings of the 1st Symposium on Discerete Algorithms (SODA), </booktitle> <pages> 394-403, </pages> <publisher> ACM Press, </publisher> <year> 1991. </year>
Reference-contexts: Hence, by the results in (Aggarwal and Park, 1988) and in <ref> (Atallah, 1991) </ref> our algorithm runs in O ((lg 2 n)(lg lg n)) time using n=lg lg n processors on a common-CRCW PRAM or in O (lg 2 n) time using n processors on a EREW PRAM.
Reference: <author> M. J. Atallah and S. R. Kosaraju: </author> <title> "An Efficient Parallel Algorithm for the Row Minima of a Totally Monotone Matrix," </title> <journal> Journal of Algorithms, </journal> <volume> Vol. 13, </volume> <pages> 394-413, </pages> <year> 1992. </year>
Reference: <author> S. Baase: </author> <title> Computer Algorithms, Second Edition, </title> <publisher> Addison-Wesley, </publisher> <year> 1988. </year>
Reference: <author> J. L. Balcazar, J. Diaz, and J. Gabarro: </author> <title> Structural Complexity I and II, Vols. </title> <booktitle> 11 and 22 of EATCS Monographs on Theoretical Computer Science, </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1988 </year> <month> and </month> <year> 1990. </year>
Reference-contexts: Here the main focus was PSpace and the polynomial hierarchy to describe a notion of parallelism for Turing machines. This work inspired much work showing the asymptotic equivalence of parallel time and sequential space on the Turing machine model. Definitions and Foundations 18 The authors of <ref> (Balcazar et al., 1988 and 1990) </ref> say that the conference paper (Chandra and Stockmeyer, 1976) is where the parallel computation hypothesis was first explicitly mentioned by name. In the same conference (Kozen, 1976) gave a similar rendition of a parallel Turing machine model.
Reference: <author> R. Bellman: </author> <title> Dynamic Programming, </title> <publisher> Princeton University Press, </publisher> <year> 1957. </year>
Reference-contexts: Conversely, the greedy principle basically is that if a substructure is optimal, then it is in some optimal superstructure. In some sense this is a bottom up design method. Some foundations for the principle of optimality and the greedy principle can be 19 Sequential Dynamic Programming 20 found in <ref> (Bellman, 1957) </ref> and (Cormen et al., 1990; Korte et al., 1991), respectively. 3.2 The Matrix Chain Ordering Problem This section contains an in-depth discussion of the matrix chain ordering problem (MCOP). Its focus is on the classical sequential solution.
Reference: <author> O. Berkman, D. Breslauer, Z. Galil, B. Schieber, and U. Vishkin: </author> <title> "Highly Paralleliz-able Problems," </title> <booktitle> Symposium on the Theory of Computing (STOC) , ACM Press, </booktitle> <pages> 309-319, </pages> <year> 1989. </year>
Reference-contexts: Further, critical nodes indicate where magnitude can overtake order in the row and column equations. Special D n Graphs for the MCOP 53 By solving the ANSV problem we can compute all critical nodes of a D n graph. In our nomenclature, <ref> (Berkman et al., 1989) </ref> shows on a common-CRCW PRAM and in (Chen 1990; Kim 1990) on a EREW PRAM: Theorem 7 On a common-CRCW PRAM all critical nodes can be computed in O (lg lg n) and O (lg n) time using n=lg lg n and n=lg n processors, respectively. <p> This algorithm is built by combining results of (Chin, 1979), and (Hu and Shing, 1981) with those of <ref> (Berkman et al., 1989) </ref>. This algorithm approximates the MCOP to within 15:5% of optimality. In addition, the processor-time product of this algorithm is linear.
Reference: <author> O. Berkman, B. Schieber, and U. Vishkin: </author> <title> "Optimal Doubly Logarithmic Parallel Algorithms Based on Finding All Nearest Smaller Values," </title> <journal> Journal of Algorithms, </journal> <volume> Vol. 14, </volume> <pages> 344-370, </pages> <year> 1993. </year> <note> BIBLIOGRAPHY 128 P. </note> <author> G. Bradford: </author> <title> "Efficient Parallel Dynamic Programming," </title> <type> Technical Report # 352, </type> <institution> Indiana University, </institution> <month> April </month> <year> 1992. </year>
Reference: <author> P. G. Bradford: </author> <title> "Efficient Parallel Dynamic Programming," </title> <booktitle> The 30th Allerton Conference on Communication, Control, and Computation, </booktitle> <institution> University of Illinois, </institution> <month> 185-194, </month> <year> 1992. </year>
Reference-contexts: Theorem 24 also applies to the optimal convex triangulation problem with the standard triangle cost metrics (Cormen et al., 1990; Hu and Shing, 1982). 7.4 Historical Notes The algorithm outlined here is basically from <ref> (Bradford, 1992) </ref>. An extended abstract was published in (Bradford, 1992a) and the full version is in (Bradford, 1992b). The full version was available and circulated since May of 1992. <p> An O (lg 2 n) Time and n Processor Algorithm 121 8.6 Historical Notes The algorithms given in this chapter were motivated by algorithms given in <ref> (Bradford, 1992) </ref> and (Bradford, 1992a; Bradford, 1992b). The algorithms given here improve the algorithms given by many people. <p> In particular, both the extended abstract in (Ramanan, 1992) and the later full version (Ramanan, 1993) give an O (lg 4 n) time and n processor CREW PRAM MCOP algorithm building directly on the work of Hu and Shing. On the other hand, the full version <ref> (Bradford et al., 1992) </ref> (versions circulated since the late summer of 1992) gives an O (lg 4 n) time and n=lg n processor common-CRCW PRAM MCOP algorithm using the model given in this dissertation and originally in (Bradford, 1992). <p> On the other hand, the full version (Bradford et al., 1992) (versions circulated since the late summer of 1992) gives an O (lg 4 n) time and n=lg n processor common-CRCW PRAM MCOP algorithm using the model given in this dissertation and originally in <ref> (Bradford, 1992) </ref>. Of course, the algorithms outlined in this chapter can run in O (lg 2 n) time using n processors or in in O (lg 3 n) time using n=lg n processors on an EREW PRAM.
Reference: <author> P. G. Bradford: </author> <title> "Efficient Parallel Dynamic Programming," </title> <note> Full Version Manuscript May, 1992: Version with very minor revisions in Revised TR # 352, Indiana University. </note>
Reference-contexts: Theorem 24 also applies to the optimal convex triangulation problem with the standard triangle cost metrics (Cormen et al., 1990; Hu and Shing, 1982). 7.4 Historical Notes The algorithm outlined here is basically from <ref> (Bradford, 1992) </ref>. An extended abstract was published in (Bradford, 1992a) and the full version is in (Bradford, 1992b). The full version was available and circulated since May of 1992. <p> An O (lg 2 n) Time and n Processor Algorithm 121 8.6 Historical Notes The algorithms given in this chapter were motivated by algorithms given in <ref> (Bradford, 1992) </ref> and (Bradford, 1992a; Bradford, 1992b). The algorithms given here improve the algorithms given by many people. <p> In particular, both the extended abstract in (Ramanan, 1992) and the later full version (Ramanan, 1993) give an O (lg 4 n) time and n processor CREW PRAM MCOP algorithm building directly on the work of Hu and Shing. On the other hand, the full version <ref> (Bradford et al., 1992) </ref> (versions circulated since the late summer of 1992) gives an O (lg 4 n) time and n=lg n processor common-CRCW PRAM MCOP algorithm using the model given in this dissertation and originally in (Bradford, 1992). <p> On the other hand, the full version (Bradford et al., 1992) (versions circulated since the late summer of 1992) gives an O (lg 4 n) time and n=lg n processor common-CRCW PRAM MCOP algorithm using the model given in this dissertation and originally in <ref> (Bradford, 1992) </ref>. Of course, the algorithms outlined in this chapter can run in O (lg 2 n) time using n processors or in in O (lg 3 n) time using n=lg n processors on an EREW PRAM.
Reference: <author> P. G. Bradford, G. J. E. Rawlins, and G. E. Shannon: </author> <title> "Matrix Chain Ordering in Polylog Time with n=lg n Processors," </title> <type> Technical Report # 360, </type> <institution> Indiana University, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: Theorem 24 also applies to the optimal convex triangulation problem with the standard triangle cost metrics (Cormen et al., 1990; Hu and Shing, 1982). 7.4 Historical Notes The algorithm outlined here is basically from <ref> (Bradford, 1992) </ref>. An extended abstract was published in (Bradford, 1992a) and the full version is in (Bradford, 1992b). The full version was available and circulated since May of 1992. <p> An O (lg 2 n) Time and n Processor Algorithm 121 8.6 Historical Notes The algorithms given in this chapter were motivated by algorithms given in <ref> (Bradford, 1992) </ref> and (Bradford, 1992a; Bradford, 1992b). The algorithms given here improve the algorithms given by many people. <p> In particular, both the extended abstract in (Ramanan, 1992) and the later full version (Ramanan, 1993) give an O (lg 4 n) time and n processor CREW PRAM MCOP algorithm building directly on the work of Hu and Shing. On the other hand, the full version <ref> (Bradford et al., 1992) </ref> (versions circulated since the late summer of 1992) gives an O (lg 4 n) time and n=lg n processor common-CRCW PRAM MCOP algorithm using the model given in this dissertation and originally in (Bradford, 1992). <p> On the other hand, the full version (Bradford et al., 1992) (versions circulated since the late summer of 1992) gives an O (lg 4 n) time and n=lg n processor common-CRCW PRAM MCOP algorithm using the model given in this dissertation and originally in <ref> (Bradford, 1992) </ref>. Of course, the algorithms outlined in this chapter can run in O (lg 2 n) time using n processors or in in O (lg 3 n) time using n=lg n processors on an EREW PRAM.
Reference: <author> P. G. Bradford, G. J. E. Rawlins, and G. E. Shannon: </author> <title> "Matrix Chain Ordering in Polylog Time with Linear Processors (Extended Abstract)," </title> <booktitle> Proceedings of the 8th Annual IEEE International Parallel Processing Symposium (IPPS), Cancun Mexico, </booktitle> <editor> H. J. Siegel editor, </editor> <publisher> IEEE Press, </publisher> <pages> 234-241, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Of course, the algorithms outlined in this chapter can run in O (lg 2 n) time using n processors or in in O (lg 3 n) time using n=lg n processors on an EREW PRAM. An extended abstract of the algorithm given in this section is in <ref> (Bradford et al., 1994) </ref> and the full version has been submitted. Chapter 9 Directions of Further Research and Conclusions This chapter contains conclusions and directions of further research. Some extensions to the work in this dissertation have already come to fruition, see (Bradford et al., 1993b; Bradford et al., 1995).
Reference: <author> P. G. Bradford, G. J. E. Rawlins, and G. E. Shannon: </author> <title> "Matrix Chain Ordering in Polylog Time with Linear Processors," </title> <note> Full Version Submitted to SIAM Journal on Computing. </note>
Reference: <author> P. G. Bradford, V. Choppella, and G. J. E. Rawlins: </author> <title> "On Lower Bounds for the Matrix Chain Ordering Problem," </title> <note> Full version to be Submitted, Earlier version: Technical Report # 391, </note> <institution> Indiana University, </institution> <month> October </month> <year> 1993. </year>
Reference: <author> P. G. Bradford, V. Choppella, and G. J. E. Rawlins: </author> <title> "On Lower Bounds for the Matrix Chain Ordering Problem," Extended Abstract to appear in the proceedings of LATIN '95, </title> <publisher> Springer Verlag, </publisher> <year> 1995. </year> <title> BIBLIOGRAPHY 129 D. </title> <editor> Z. Chen: </editor> <title> "Efficient Geometric Algorithms on the EREW PRAM," </title> <booktitle> Proceedings of the 28th Allerton Conference on Communication, Control, and Computation, </booktitle> <address> Monticello, </address> <publisher> Illinois, </publisher> <pages> 818-827, </pages> <year> 1990. </year> <note> Full version accepted to IEEE Trans. on Parallel and Distributed Systems. </note>
Reference: <author> F. Y. Chin: </author> <title> "An O(n) Algorithm for Determining Near-Optimal Computation Order of Matrix Chain Products," </title> <journal> Communications of the ACM, </journal> <volume> Vol. 21, No. 7, </volume> <pages> 544-549, </pages> <month> July </month> <year> 1978. </year>
Reference: <author> A. K. Chandra: </author> <title> "Computing Matrix Chain Products in Near Optimal Time," </title> <institution> IBM Research Report RC-5625, IBM T. J. Watson Research Center, </institution> <month> Oct. </month> <year> 1975. </year>
Reference-contexts: The cost of multiplying these two matrices is written f (i; j; k) = d i d j+1 d k+1 . Generalizations of this matrix product function reflecting asymptotically better matrix multiplication algorithms, such as those in (Pan, 1984; Coppersmith and Winograd, 1990), are also acceptable, see <ref> (Chandra, 1975) </ref>. <p> The depth of the parentheses provides an approximation to within 15:5% of optimal for the MCOP. This is due to <ref> (Chandra, 1975) </ref>, (Chin, 1979), and (Hu and Shing, 1981). <p> This algorithm is built on the greedy principle more than the dynamic programming paradigm. In terms of the processor-time product, this algorithm is optimal. 6.2 Historical Notes Both <ref> (Chandra, 1975) </ref> and (Chin, 1979) gave approximations for the MCOP that were not quite as good as the 1:155 of Theorem 15. Theorem 15 was conjectured in (Chin, 1979) and finally proved in (Hu and Shing, 1981).
Reference: <author> A. K. Chandra and L. J. Stockmeyer: </author> <title> "Alternation," </title> <booktitle> Proceedins of the 17th Anual IEEE Symposium on the Foundations of Computing (FOCS), </booktitle> <publisher> IEEE Press, </publisher> <pages> 98-108, </pages> <year> 1976. </year>
Reference-contexts: This work inspired much work showing the asymptotic equivalence of parallel time and sequential space on the Turing machine model. Definitions and Foundations 18 The authors of (Balcazar et al., 1988 and 1990) say that the conference paper <ref> (Chandra and Stockmeyer, 1976) </ref> is where the parallel computation hypothesis was first explicitly mentioned by name. In the same conference (Kozen, 1976) gave a similar rendition of a parallel Turing machine model. Together, both (Chandra and Stockmeyer, 1976) and (Kozen, 1976) lead to the journal article (Chandra et al., 1981). <p> Foundations 18 The authors of (Balcazar et al., 1988 and 1990) say that the conference paper <ref> (Chandra and Stockmeyer, 1976) </ref> is where the parallel computation hypothesis was first explicitly mentioned by name. In the same conference (Kozen, 1976) gave a similar rendition of a parallel Turing machine model. Together, both (Chandra and Stockmeyer, 1976) and (Kozen, 1976) lead to the journal article (Chandra et al., 1981).
Reference: <author> A. K. Chandra, D. Kozen, and L. J. Stockmeyer: </author> <title> "Alternation," </title> <journal> Journal of the ACM, </journal> <volume> Vol. 28, </volume> <pages> 114-133, </pages> <year> 1981. </year>
Reference-contexts: More recent results have lowered this processor complexity. This dissertation shows that some of these problems have efficient parallel solutions. 2.5 Historical Notes The PRAM model was first given in (Fortune and Wyllie, 1978). One of the first renditions of the parallel computation hypothesis was in <ref> (Chandra et al., 1981) </ref> in terms of alternating Turing machines. Here the main focus was PSpace and the polynomial hierarchy to describe a notion of parallelism for Turing machines. This work inspired much work showing the asymptotic equivalence of parallel time and sequential space on the Turing machine model. <p> In the same conference (Kozen, 1976) gave a similar rendition of a parallel Turing machine model. Together, both (Chandra and Stockmeyer, 1976) and (Kozen, 1976) lead to the journal article <ref> (Chandra et al., 1981) </ref>. The authors of (Karp and Ramachandran, 1990) point out that the parallel computation hypothesis is quite robust in that most theoretical parallel models of computation abide by it such as those from circuit complexity, parallel vector machines, alternating Turing machines, etc.
Reference: <author> S. A. Cook: </author> <title> "An Observation on Time-Storage Trade Off," </title> <journal> Journal of Computer and System Sciences, </journal> <volume> Vol. 9, No. 3, </volume> <pages> 308-316, </pages> <year> 1974. </year>
Reference-contexts: In terms of parallel computation, this log-space transformation can be replaced by an N C algorithm as a consequence of the parallel computation hypothesis. The first log-space complete problem in P was given in <ref> (Cook, 1974) </ref>. Such problems are in some sense among the "hardest" in P in that solving them takes as much space as does solving any other problem in P. <p> The class N C stands for "Nick's Class" in honor of Nick Pippenger. The first P-Complete problem was given in <ref> (Cook, 1974) </ref> while the author was discussing the question of the space complexity of parsing context-free grammars relative to the space complexity of all problems in P.
Reference: <author> S. A. Cook: </author> <title> "A Taxonomy of Problems with Fast Parallel Algorithms," </title> <journal> Information and Control, </journal> <volume> Vol. 64, </volume> <pages> 2-22, </pages> <year> 1985. </year>
Reference: <author> D. Coppersmith and S. Winograd: </author> <title> "Matrix Multiplication via Arithmetic Progressions," </title> <journal> Journal of Symbolic Computation, </journal> <volume> Vol. 9, </volume> <pages> 251-280, </pages> <year> 1990. </year>
Reference: <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest: </author> <title> Introduction to Algorithms, </title> <publisher> McGraw Hill, </publisher> <year> 1990. </year> <title> BIBLIOGRAPHY 130 A. Czumaj: "An Optimal Parallel Algorithm for Computing a Near-Optimal Order of Matrix Multiplications," </title> <booktitle> Scandinavian Workshop on Algorithms Theory (SWAT), </booktitle> <publisher> Springer Verlag, </publisher> <pages> LNCS # 621 , 62-72, </pages> <year> 1992. </year>
Reference-contexts: A (min; +)-matrix multiplication based shortest path algorithm is a matrix multiplication algorithm where the usual matrix operations of (+; fi) are replaced by (min; +). That is, the vector sums are replaced by minimizations and the dot-products are replaced by dot-sums, see <ref> (Cormen et al., 1990) </ref>. A (min; +)-matrix multiplication based shortest path algorithm is easily run in parallel on a common-CRCW PRAM in O (lg 2 n) time using n 3 =lg n processors (JaJa, 1992; Kumar et al., 1994). <p> For example, given two strings of n characters each, find the longest common subsequence of them. The longest common subsequence of two strings is the longest string that is a subsequence of both given strings, see for example <ref> (Cormen et al., 1990) </ref>. Applications of this problem can be found in computational biology and operating systems. 4.3 Minimum Cost Parenthesizations This section contains a generalization of the G n graphs that accounts for split paren-thesizations. <p> Jumper lengths are the basis of the arguments in this section. In order to construct a D n graph by starting with a weighted digraph G n and then A Dynamic Graph Model 41 perform incremental path relaxation <ref> (Cormen et al., 1990) </ref> while adding new jumpers. A shortest path in D n is found simultaneously. This is done by using a variation of a (min; +)-matrix multiplication based all pairs shortest path algorithm. <p> Also, the minimum distances between all pairs of nodes in G n up to 2 nodes apart are now available. With this, construct all jumpers of length 2. For length 2 horizontal jumpers, this is done by relaxing <ref> (Cormen et al., 1990, Pages 520-527) </ref> them with the two horizontal edges they are directly above. In Another, (min; +)-matrix multiplication gives sp (i; j) for all (i; j) such that j i 2 2 1. Continue this process inductively.
Reference: <author> A. Czumaj: </author> <title> "Parallel Algorithm for the Matrix Chain Product and the Optimal Triangulation Problem (Extended Abstract)," </title> <booktitle> Symposium on Theoretical Aspects of Computer Science (STACS), </booktitle> <publisher> Springer Verlag, </publisher> <pages> LNCS # 665 , 294-305, </pages> <year> 1993. </year> <note> Full version submitted. </note>
Reference-contexts: Early examples of monotonicity properties used to improve sequential dynamic programming algorithms can be found in (Knuth, 1971) and (Yao, 1982). Some of the techniques of (Yao, 1982) have been made parallel by <ref> (Czumaj, 1993) </ref> for attacking the MCOP. 9.1.3 Some Comments on Solving the OBST on a D n Graph As we saw in Chapter 4, a D n graph suited for solving the problem of building an optimal binary search tree can be constructed in O (lg 2 n) time using n
Reference: <author> L. E. Deimel Jr. and T. A. Lampe: </author> <title> "An Invariance Theorem Concerning Optimal Computation of Matrix Chain Products," </title> <institution> North Carolina State Univ. Tech Report # TR79-14, </institution> <year> 1979. </year>
Reference-contexts: Theorem 5 <ref> (Deimel and Lampe, 1979) </ref> and (Hu and Shing, 1982) Given an instance of the MCOP with the weight list l 1 = w 1 ; w 2 ; : : : ; w n+1 and cyclically rotating it getting l 2 = w fl (1) ; w fl (2) ; : <p> &gt; &gt; w m &gt; w m+1 . 2 Take a D (i;t) (j;k) canonical graph, then both w i &lt; w i+1 &lt; &lt; w j and w k &gt; w k+1 &gt; &gt; w t+1 follow from Theorem 12. 5.4 Historical Notes Theorem 5 was first proved by <ref> (Deimel and Lampe, 1979) </ref> and a simpler proof was later given by (Hu and Shing, 1982). There seems to be two general approaches to improving the complexity of finding minimal paths in special graphs.
Reference: <author> S. Fortune and J. Wyllie: </author> <title> "Parallelism in Random Access Machines," </title> <booktitle> Proceedings of the 19th Annual IEEE Symposium on the Foundations of Computer Science (FOCS), </booktitle> <publisher> IEEE Press, </publisher> <pages> 114-118, </pages> <year> 1978. </year>
Reference-contexts: More recent results have lowered this processor complexity. This dissertation shows that some of these problems have efficient parallel solutions. 2.5 Historical Notes The PRAM model was first given in <ref> (Fortune and Wyllie, 1978) </ref>. One of the first renditions of the parallel computation hypothesis was in (Chandra et al., 1981) in terms of alternating Turing machines. Here the main focus was PSpace and the polynomial hierarchy to describe a notion of parallelism for Turing machines.
Reference: <author> Z. Galil and K. Park: </author> <title> "Dynamic Programming with Convexity, Concavity and Sparsity," </title> <journal> Theoretical Computer Science, </journal> <volume> Vol. 92, </volume> <pages> 49-76, </pages> <year> 1992. </year>
Reference-contexts: In any event, the algorithm in Figure 4 solves this recurrence. 3.5 Historical Notes A recent survey and classification of modern dynamic programming problems is <ref> (Galil and Park, 1992) </ref>. This paper classifies dynamic programming problems in terms of size of the dynamic programming table and the number of table entries a new table entry depends on.
Reference: <author> Z. Galil and K. Park: </author> <title> "Parallel Dynamic Programming," </title> <type> Manuscript, </type> <year> 1992. </year>
Reference-contexts: In any event, the algorithm in Figure 4 solves this recurrence. 3.5 Historical Notes A recent survey and classification of modern dynamic programming problems is <ref> (Galil and Park, 1992) </ref>. This paper classifies dynamic programming problems in terms of size of the dynamic programming table and the number of table entries a new table entry depends on.
Reference: <author> M. R. Garey and D. S. Johnson: Computers and Intractability, W. H. Freeman, </author> <year> 1979. </year>
Reference-contexts: These problems are often thought to be tractable on modern computers <ref> (Garey and Johnson, 1979) </ref>. Since a PRAM can only have a polynomial number of processors, a RAM can simulate a PRAM in polynomial time. This means the only problems that have any hope of being tractable on a PRAM are those that are tractable on a RAM.
Reference: <author> A. Gibbons and W. Rytter: </author> <title> Efficient Parallel Algorithms, </title> <publisher> Cambridge University Press, </publisher> <year> 1988. </year>
Reference: <author> S. S. Godbole: </author> <title> "An Efficient Computation of Matrix Chain Products," </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. C-22, </volume> <pages> 864-866, </pages> <year> 1973. </year>
Reference-contexts: Taking the definition of M i;j and applying the principle of optimality gives a dynamic programming algorithm with a polynomial time solution. This was observed by several researchers in the early 1970s. In 1973, the first polynomial time solutions of the MCOP were given independently in <ref> (Godbole, 1973) </ref> and in (Muraoka and Kuck, 1973). Godbole's algorithm has become the classical O (n 3 ) dynamic programming solution, and it is where we begin.
Reference: <author> R. Greenlaw: </author> <title> "Polynomial Completeness and Parallel Computation," 901-953, in Synthesis of Parallel Algorithms, </title> <editor> J. H. Reif (Editor), </editor> <publisher> Morgan-Kaufmann, </publisher> <year> 1993. </year> <note> BIBLIOGRAPHY 131 R. </note> <author> Greenlaw, H. J. Hoover, and W. L. Ruzzo: </author> <title> A Compendium of Problems Complete for P , Revision 1.46, </title> <type> Technical Report from Universities of Alberta (TR 91-11), </type> <institution> New Hampshire (TR91-14) and Washington (TR 91-05-01), </institution> <year> 1991. </year>
Reference: <author> J. Hopcroft and J. D. Ullman: </author> <title> Introduction to Automata Theory, Languages, and Computation, </title> <publisher> Addison-Wesley, </publisher> <year> 1979. </year>
Reference: <author> S.-H. S. Huang, H. Liu, and V. Viswanathan: </author> <title> "Parallel Dynamic Programming," </title> <booktitle> Proceedings of the 2nd IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> 497-500, </pages> <year> 1990. </year>
Reference-contexts: Furthermore, for various reasons it is important to have a re-characterization of Hu and Shing's work on the MCOP. Also, our model is general enough to capture more than just Hu and Shing's work on the MCOP. In 1990, the paper <ref> (Huang et al., 1990) </ref> improved Rytters result by giving an O (lg 2 n) time algorithm that uses n 6 =lg 5 n processors. Further, (Huang et al., 1992) gives an O ( p n lg n) time algorithm using O (n 3:5 =lg n) processors on a CREW PRAM.
Reference: <author> S.-H. S. Huang, H. Liu, and V. Viswanathan: </author> <title> "A Sublinear Parallel Algorithm for Some Dynamic Programming Problems," </title> <journal> Theoretical Computer Science, </journal> <volume> Vol. 106, </volume> <pages> 361-371, </pages> <year> 1992. </year>
Reference-contexts: Also, our model is general enough to capture more than just Hu and Shing's work on the MCOP. In 1990, the paper (Huang et al., 1990) improved Rytters result by giving an O (lg 2 n) time algorithm that uses n 6 =lg 5 n processors. Further, <ref> (Huang et al., 1992) </ref> gives an O ( p n lg n) time algorithm using O (n 3:5 =lg n) processors on a CREW PRAM.
Reference: <author> T. C. Hu: </author> <title> Combinatorial Algorithms, </title> <publisher> Addison-Wesley, </publisher> <year> 1982. </year>
Reference-contexts: Theorem 5 (Deimel and Lampe, 1979) and <ref> (Hu and Shing, 1982) </ref> Given an instance of the MCOP with the weight list l 1 = w 1 ; w 2 ; : : : ; w n+1 and cyclically rotating it getting l 2 = w fl (1) ; w fl (2) ; : : : ; w fl <p> The next theorem was originally proved by Hu and Shing in a different framework and follows from the properties of ANSV matches. Theorem 8 <ref> (Hu and Shing, 1982) </ref> In any instance of the matrix chain ordering problem all critical nodes are compatible. Proof: Suppose D n represents an instance of the matrix chain ordering problem with two non-compatible critical nodes (i; s) and (j; t). <p> For example, a monotone weight list does not have any critical nodes. The next lemma follows from the ANSV characterization of critical nodes. Lemma 5 <ref> (Hu and Shing, 1982) </ref> Any D n graph has at most n 1 critical nodes. Proof: Take a list of n + 1 weights w 1 ; w 2 ; : : : ; w n+1 making up the edge weights of some D n graph. <p> (i;t) (j;k) canonical graph, then both w i &lt; w i+1 &lt; &lt; w j and w k &gt; w k+1 &gt; &gt; w t+1 follow from Theorem 12. 5.4 Historical Notes Theorem 5 was first proved by (Deimel and Lampe, 1979) and a simpler proof was later given by <ref> (Hu and Shing, 1982) </ref>. There seems to be two general approaches to improving the complexity of finding minimal paths in special graphs. The first is to use specific properties of the graphs to get more efficient renditions of the (min; +)-matrix based shortest-path algorithms. <p> This generalizes to paths with more than one jumper. In our terminology the following result of <ref> (Hu and Shing, 1982, Corollary 3) </ref> can be stated as: Theorem 16 In any canonical graph, the sum of the two products w i w j+1 w k+1 + w j+1 w j+2 w k+1 where i &lt; j &lt; k, cannot contribute to the weight of any shortest path iff <p> In our terminology, Hu and Shing gave the following important theorem. Theorem 18 <ref> (Hu and Shing, 1982) </ref> A shortest path to a critical node (i; j) in a D (1;m) graph is either along a straight unit path to (i; j), along the path of critical nodes to (i; j), or along subpaths of critical nodes connected together by angular paths and finally to <p> In essence, this result gives some of the power of the greedy principle together with the principle of optimality. That is, this result allows the isolation of some substructures that are necessarily in an optimal superstructure. Theorem 19 <ref> (Hu and Shing, 1982) </ref> Given a weight list w 1 ; : : : ; w n+1 with the three smallest weights w 1 &lt; w i+1 &lt; w v+1 and i + 1 &lt; v, the products w 1 w i+1 and w 1 w v+1 are in some associative
Reference: <author> T. C. Hu and M. T. Shing: </author> <title> "Some Theorems about Matrix Multiplication", </title> <booktitle> Proceedings of the 21st Annual IEEE Symposium on the Foundations of Computer Science (FOCS), </booktitle> <publisher> IEEE Press, </publisher> <pages> 28-35, </pages> <year> 1980. </year>
Reference-contexts: This is interesting because we can solve any instance of the MCOP very efficiently if its weight list is monotonic. Special D n Graphs for the MCOP 49 ( ( ( ) ) ( ( ( ) ) ) ) As in <ref> (Hu and Shing, 1980) </ref> let kw i : w k k = j=i and all such pair-wise sums can be computed by performing one parallel partial prefix. <p> Also, a partial prefix sum can be computed easily in parallel with O (n) work and in O (lg n) time (JaJa, 1992; Kumar et al., 1994). Theorem 6 <ref> (Hu and Shing, 1980) </ref> The vector F [i] = kw 1 : w i k can be computed by a parallel partial prefix. <p> By solving the ANSV problem it is possible to isolate certain nodes that are central to finding shortest paths in D n graphs. The next definition is originally due to <ref> (Hu and Shing, 1980) </ref>, although they present it in a different framework: In D n , a critical node is a node (i; k) such that w j &gt; maxfw i ; w k+1 g for all i &lt; j k.
Reference: <author> T. C. Hu and M. T. Shing: </author> <title> "An O(n) Algorithm to Find a Near-Optimum Partition of a Convex Polygon," </title> <journal> Journal of Algorithms, </journal> <volume> Vol. 2, </volume> <pages> 122-138, </pages> <year> 1981. </year>
Reference-contexts: This algorithm is built by combining results of (Chin, 1979), and <ref> (Hu and Shing, 1981) </ref> with those of (Berkman et al., 1989). This algorithm approximates the MCOP to within 15:5% of optimality. In addition, the processor-time product of this algorithm is linear. <p> Theorem 13 <ref> (Hu and Shing, 1981) </ref> If w 1 w i w i+2 + w i w i+1 w i+2 &lt; w 1 w i w i+1 + w 1 w i+1 w i+2 (3) then the product (a i * a i+1 ) is in an optimal parenthesization. <p> Proof of this Theorem is left to the literature, see (Chin, 1979) and <ref> (Hu and Shing, 1981) </ref> for different proofs. When w i+1 &gt; maxfw i ; w i+2 g fails to hold Equation 3 cannot hold, so there is no gain in assuming w i+1 &gt; maxfw i ; w i+2 g. <p> The depth of the parentheses provides an approximation to within 15:5% of optimal for the MCOP. This is due to (Chandra, 1975), (Chin, 1979), and <ref> (Hu and Shing, 1981) </ref>. Approximating the MCOP 68 Theorem 15 (Hu and Shing, 1981) If a weight list w 1 ; w 2 ; ; w r+1 is reduced, then the MCOP can be solved to within a multiplicative factor of 1:155 from optimal in constant time using n processors. <p> The depth of the parentheses provides an approximation to within 15:5% of optimal for the MCOP. This is due to (Chandra, 1975), (Chin, 1979), and <ref> (Hu and Shing, 1981) </ref>. Approximating the MCOP 68 Theorem 15 (Hu and Shing, 1981) If a weight list w 1 ; w 2 ; ; w r+1 is reduced, then the MCOP can be solved to within a multiplicative factor of 1:155 from optimal in constant time using n processors. <p> In terms of the processor-time product, this algorithm is optimal. 6.2 Historical Notes Both (Chandra, 1975) and (Chin, 1979) gave approximations for the MCOP that were not quite as good as the 1:155 of Theorem 15. Theorem 15 was conjectured in (Chin, 1979) and finally proved in <ref> (Hu and Shing, 1981) </ref>. The algorithm given in this chapter appeared in (Bradford, 1992; Bradford, 1992a) and a very similar algorithm was given in (Czumaj, 1992).
Reference: <author> T. C. Hu and M. T. Shing: </author> <title> "Computation of Matrix Product Chains. Part I," </title> <journal> SIAM Journal on Computing, </journal> <volume> Vol. 11, No. 3, </volume> <pages> 362-373, </pages> <year> 1982. </year>
Reference-contexts: Theorem 5 (Deimel and Lampe, 1979) and <ref> (Hu and Shing, 1982) </ref> Given an instance of the MCOP with the weight list l 1 = w 1 ; w 2 ; : : : ; w n+1 and cyclically rotating it getting l 2 = w fl (1) ; w fl (2) ; : : : ; w fl <p> The next theorem was originally proved by Hu and Shing in a different framework and follows from the properties of ANSV matches. Theorem 8 <ref> (Hu and Shing, 1982) </ref> In any instance of the matrix chain ordering problem all critical nodes are compatible. Proof: Suppose D n represents an instance of the matrix chain ordering problem with two non-compatible critical nodes (i; s) and (j; t). <p> For example, a monotone weight list does not have any critical nodes. The next lemma follows from the ANSV characterization of critical nodes. Lemma 5 <ref> (Hu and Shing, 1982) </ref> Any D n graph has at most n 1 critical nodes. Proof: Take a list of n + 1 weights w 1 ; w 2 ; : : : ; w n+1 making up the edge weights of some D n graph. <p> (i;t) (j;k) canonical graph, then both w i &lt; w i+1 &lt; &lt; w j and w k &gt; w k+1 &gt; &gt; w t+1 follow from Theorem 12. 5.4 Historical Notes Theorem 5 was first proved by (Deimel and Lampe, 1979) and a simpler proof was later given by <ref> (Hu and Shing, 1982) </ref>. There seems to be two general approaches to improving the complexity of finding minimal paths in special graphs. The first is to use specific properties of the graphs to get more efficient renditions of the (min; +)-matrix based shortest-path algorithms. <p> This generalizes to paths with more than one jumper. In our terminology the following result of <ref> (Hu and Shing, 1982, Corollary 3) </ref> can be stated as: Theorem 16 In any canonical graph, the sum of the two products w i w j+1 w k+1 + w j+1 w j+2 w k+1 where i &lt; j &lt; k, cannot contribute to the weight of any shortest path iff <p> In our terminology, Hu and Shing gave the following important theorem. Theorem 18 <ref> (Hu and Shing, 1982) </ref> A shortest path to a critical node (i; j) in a D (1;m) graph is either along a straight unit path to (i; j), along the path of critical nodes to (i; j), or along subpaths of critical nodes connected together by angular paths and finally to <p> In essence, this result gives some of the power of the greedy principle together with the principle of optimality. That is, this result allows the isolation of some substructures that are necessarily in an optimal superstructure. Theorem 19 <ref> (Hu and Shing, 1982) </ref> Given a weight list w 1 ; : : : ; w n+1 with the three smallest weights w 1 &lt; w i+1 &lt; w v+1 and i + 1 &lt; v, the products w 1 w i+1 and w 1 w v+1 are in some associative
Reference: <author> T. C. Hu and M. T. Shing: </author> <title> "Computation of Matrix Product Chains. Part II," </title> <journal> SIAM Journal on Computing, </journal> <volume> Vol. 13, No. 2, </volume> <pages> 228-251, </pages> <year> 1984. </year>
Reference: <author> O. H. Ibarra, T.-C. Pong, and S. M. Sohn: </author> <title> "Hypercube Algorithms for Some String Comparison Problems," </title> <booktitle> Proceedings of the IEEE International Conference on Parallel Processing, </booktitle> <publisher> IEEE Press, </publisher> <pages> 190-193, </pages> <year> 1988. </year> <title> BIBLIOGRAPHY 132 J. JaJa: An Introduction to Parallel Algorithms, </title> <publisher> Addison-Wesley, </publisher> <year> 1992. </year>
Reference: <author> D. S. Johnson: </author> <title> "A Catalog of Complexity Classes," Chapter 2 in Handbook of Theoretical Computer Science, Vol. A: Algorithms and Complexity, </title> <editor> V. Van Leeuwen (Editor), </editor> <publisher> Elsevier, </publisher> <pages> 67-161, </pages> <year> 1990. </year>
Reference: <author> R. M. Karp and V. Ramachandran: </author> <title> "Parallel Algorithms for Shared Memory Machines," Chapter 17 in Handbook of Theoretical Computer Science, Vol. A, Algorithms and Complexity V. </title> <editor> Van Leeuwen (Editor), </editor> <publisher> Elsevier, </publisher> <pages> 869-942, </pages> <year> 1990. </year>
Reference-contexts: All of the above models can simulate each other within minor time and space factors <ref> (Karp and Ramachandran, 1990) </ref>. Because these parallel models are so closely related, we use the most convenient model at hand. 2.2 Efficient and Optimal Parallel Algorithms The set P is the class of problems that have polynomial time bounded algorithms on a RAM. <p> In the same conference (Kozen, 1976) gave a similar rendition of a parallel Turing machine model. Together, both (Chandra and Stockmeyer, 1976) and (Kozen, 1976) lead to the journal article (Chandra et al., 1981). The authors of <ref> (Karp and Ramachandran, 1990) </ref> point out that the parallel computation hypothesis is quite robust in that most theoretical parallel models of computation abide by it such as those from circuit complexity, parallel vector machines, alternating Turing machines, etc. <p> Therefore, choosing to do them simultaneously makes most sense. But, any number of nested bands must be merged in a way to avoid conflicts. This is easily done using the Euler tour technique for numbering appropriately for tree contraction, see <ref> (Karp and Ramachandran, 1990) </ref>. The next lemma shows that pruning allows the construction of a shortest path from (0; 0) to (1; n) in a tree made of leaf canonical graphs and isolated critical nodes. This is necessary for canonical subtrees as in Figure 22.
Reference: <author> S. K. Kim: </author> <title> "Optimal Parallel Algorithms on Sorted Intervals," </title> <type> TR 90-01-04, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <address> Seattle, WA, </address> <year> 1990. </year>
Reference: <author> D. G. Kirkpatrick and T. Przytycka: </author> <title> "Parallel Construction of Near Optimal Binary Search Trees," </title> <booktitle> Proceedings of the 2nd Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <pages> 234-243, </pages> <year> 1990. </year>
Reference: <author> P. N. Klein and J. H. Reif: </author> <title> "Parallel Time O(lg n) Acceptance of Deterministic CFLs on an Exclusive-Write P-RAM," </title> <journal> SIAM Journal on Computing, </journal> <volume> Vol. 17, </volume> <pages> 463-485, </pages> <year> 1988. </year>
Reference: <author> D. E. Knuth: </author> <title> "Optimum Binary Search Trees," </title> <journal> Acta Informatica, </journal> <volume> Vol. 5, </volume> <pages> 14-25, </pages> <year> 1971. </year>
Reference-contexts: Many of the parallel and sequential algorithms for solving problems amenable to dynamic programming solutions depend on monotonicity properties. Early examples of monotonicity properties used to improve sequential dynamic programming algorithms can be found in <ref> (Knuth, 1971) </ref> and (Yao, 1982). <p> Directions of Further Research and Conclusions 125 A D n graph adapted for solving the the OBST exhibits several interesting properties. It turns out that there are some definite relationships between the subgraphs that might be useful for a tree decomposition divide-and-conquer tool. In particular, Knuth's principle <ref> (Knuth, 1971) </ref> or something close to it, may give monotonicity conditions that lead to a good and cheap tree decomposition allowing efficient construction of OBSTs in parallel. 9.2 Conclusions In terms of algorithm design paradigms, Chapter 6 gives a greedy approximation algorithm.
Reference: <author> D. E. Knuth: </author> <title> The Art of Computer Programing, Volume 3: Searching and Sorting, </title> <publisher> Addison-Wesley, </publisher> <year> 1971. </year>
Reference-contexts: Many of the parallel and sequential algorithms for solving problems amenable to dynamic programming solutions depend on monotonicity properties. Early examples of monotonicity properties used to improve sequential dynamic programming algorithms can be found in <ref> (Knuth, 1971) </ref> and (Yao, 1982). <p> Directions of Further Research and Conclusions 125 A D n graph adapted for solving the the OBST exhibits several interesting properties. It turns out that there are some definite relationships between the subgraphs that might be useful for a tree decomposition divide-and-conquer tool. In particular, Knuth's principle <ref> (Knuth, 1971) </ref> or something close to it, may give monotonicity conditions that lead to a good and cheap tree decomposition allowing efficient construction of OBSTs in parallel. 9.2 Conclusions In terms of algorithm design paradigms, Chapter 6 gives a greedy approximation algorithm.
Reference: <author> B. Korte, L. Lovasz, and R. </author> <title> Schrader: </title> <publisher> Greedoids, Springer-Verlag, </publisher> <year> 1991. </year>
Reference: <author> D. Kozen: </author> <title> "On Parallelism in Turing Machines," </title> <booktitle> Proceedins of the 17th IEEE Symposium on the Foundations of Computing (FOCS), </booktitle> <publisher> IEEE Press, </publisher> <pages> 89-97, </pages> <year> 1976. </year> <note> BIBLIOGRAPHY 133 V. Kumar, </note> <author> A. Grama, A. Gupta, and G. Karypis: </author> <title> Introduction to Parallel Computing, </title> <address> Benjamin/Cummings, </address> <year> 1994. </year>
Reference-contexts: Definitions and Foundations 18 The authors of (Balcazar et al., 1988 and 1990) say that the conference paper (Chandra and Stockmeyer, 1976) is where the parallel computation hypothesis was first explicitly mentioned by name. In the same conference <ref> (Kozen, 1976) </ref> gave a similar rendition of a parallel Turing machine model. Together, both (Chandra and Stockmeyer, 1976) and (Kozen, 1976) lead to the journal article (Chandra et al., 1981). <p> In the same conference <ref> (Kozen, 1976) </ref> gave a similar rendition of a parallel Turing machine model. Together, both (Chandra and Stockmeyer, 1976) and (Kozen, 1976) lead to the journal article (Chandra et al., 1981).
Reference: <author> L. L. Larmore and W. Rytter: </author> <title> "Efficient Sublinear Time Parallel Algorithms for the Recognition of Context-Free Languages," </title> <booktitle> Scandinavian Workshop on Algorithms Theory, </booktitle> <address> (SWAT), </address> <publisher> Springer Verlag, LNCS #577, </publisher> <pages> 121-132, </pages> <year> 1991. </year>
Reference: <author> L. L. Larmore and T. M. Przytycka: </author> <title> "Constructing Huffman Trees in Parallel (Extended Abstract)," </title> <booktitle> Proceedings of the 3rd Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <pages> 71-80, </pages> <year> 1991. </year>
Reference: <author> L. L. Larmore, T. M. Przytycka, and W. Rytter: </author> <title> "Parallel Construction of Optimal Alphabetic Trees (Extended Abstract)," </title> <booktitle> Proceedings of the 5th Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <pages> 214-223, </pages> <year> 1993. </year>
Reference-contexts: area, hopefully there will be many more. 122 Directions of Further Research and Conclusions 123 9.1.1 Optimal Binary Search Trees The development of algorithms for building optimal binary search trees in polylog parallel time with low processor complexity is cited as an open problem in (Atallah et al., 1989) and <ref> (Larmore, Przytycka, and Rytter, 1993) </ref>. By low processor complexity we mean n k for some constant k &lt; 6, The best sequential algorithm for building optimal binary search trees takes fi (n 2 ) time (Knuth, 1973). <p> Directions of Further Research and Conclusions 124 Alphabetic trees are binary search trees that have their search keys only in the leaves. There are several interesting results for the building of alphabetic trees, take for example (Kirkpatrick and Przytycka, 1990; Larmore and Przytycka, 1992), and <ref> (Larmore, Przytycka, and Rytter, 1993) </ref>, and for alphabetic trees the best of which is (Larmore, Przytycka, and Rytter, 1993) giving a polylog time and n 2 =lg n processor algorithm. Although, the alphabetic tree construction problem can be solved sequentially in O (n lg n). <p> There are several interesting results for the building of alphabetic trees, take for example (Kirkpatrick and Przytycka, 1990; Larmore and Przytycka, 1992), and <ref> (Larmore, Przytycka, and Rytter, 1993) </ref>, and for alphabetic trees the best of which is (Larmore, Przytycka, and Rytter, 1993) giving a polylog time and n 2 =lg n processor algorithm. Although, the alphabetic tree construction problem can be solved sequentially in O (n lg n). Many of the parallel and sequential algorithms for solving problems amenable to dynamic programming solutions depend on monotonicity properties. <p> Unfortunately, the MCOP is quite different in character from the OBST, so it does not appear that the ANSV problem can be used in the same manner to solve the OBST efficiently in parallel. But, other possibilities exist, for example in the parallel algorithm of <ref> (Larmore, Przytycka, and Rytter, 1993) </ref> for building optimal alphabetic trees both monotonicity and tree contraction techniques are used. Directions of Further Research and Conclusions 125 A D n graph adapted for solving the the OBST exhibits several interesting properties.
Reference: <author> M. Marcus: </author> <title> Introduction to Modern Algebra, </title> <publisher> Marcel Dekker, </publisher> <year> 1978. </year>
Reference-contexts: Call a function f polylog iff f (n) = fi (lg k n) for some constant k &gt; 0. A function f is at most polylog iff f (n) = O (lg k n) for some constant k &gt; 0, etc. We will always 1 See, for example <ref> (Marcus, 1978) </ref>, for a definition of a function. Definitions and Foundations 12 attempt to build parallel algorithms that work in polylog time. Further, all polylog and polynomial functions are in terms of the input size n. <p> The generality given by the abstractions in this section allows broad applications of the graph model. A (finite) semigroupoid (S; R; *) is a nonempty finite set S, a binary relation R S fi S, and an associative binary operator "*" satisfying the following conditions <ref> (Marcus, 1978) </ref>: 1. If (a; b) 2 R, then a * b 2 S. 2. (a * b; c) 2 R iff (a; b * c) 2 R and (a * b) * c = a * (b * c). 3.
Reference: <author> Y. Muraoka and D. J. Kuck: </author> <title> "On the Time Required for a Sequence of Matrix Products," </title> <journal> Communications of the ACM, </journal> <volume> Vol. 16, No. 1, </volume> <pages> 22-26, </pages> <year> 1973. </year>
Reference-contexts: This was observed by several researchers in the early 1970s. In 1973, the first polynomial time solutions of the MCOP were given independently in (Godbole, 1973) and in <ref> (Muraoka and Kuck, 1973) </ref>. Godbole's algorithm has become the classical O (n 3 ) dynamic programming solution, and it is where we begin. Start by letting M [i; j] be the minimal cost of multiplying matrices i through j, therefore our final goal is to compute M [1; n]. <p> This paper also gives other interesting discussion of convexity, concavity and sparsity in dynamic programming tables. Sequential Dynamic Programming 28 According to the authors of (Aho et al., 1974) the MCOP first appeared in (God-bole, 1973) and <ref> (Muraoka and Kuck, 1973) </ref>. The authors of these papers on the MCOP were apparently motivated by the rich nature of the MCOP. As years passed the MCOP became a very popular problem for textbooks on algorithm design and analysis.
Reference: <author> J. O'Rourke: </author> <title> Computational Geometry in C, </title> <publisher> Cambridge University Press, </publisher> <year> 1994. </year>
Reference: <author> V. Pan: </author> <title> How to Multiply Matrices Faster, </title> <booktitle> Lecture Notes in Computer Science, </booktitle> # <volume> 179, </volume> <publisher> Springer Verlag, </publisher> <year> 1984. </year>
Reference: <author> C. Papadimitriou: </author> <title> Computational Complexity, </title> <publisher> Addison-Wesley, </publisher> <year> 1994. </year>
Reference-contexts: If some problem 1 has polynomial space complexity and there is a log-space transformation from 1 to 2 , then 2 is at least as hard as 1 in terms of the space it uses. (All log-space algorithms run in polynomial time, see <ref> (Papadimitriou, 1994) </ref>.) This is because any instance of 1 can be solved by transforming it, in log-space, to an instance of 2 , and then by solving this instance of 2 we have solved the initial instance of 1 .
Reference: <author> I. Parberry: </author> <title> Parallel Complexity Theory, </title> <booktitle> Research Notes in Theoretical Computer Science, </booktitle> <publisher> Pitman Publishers, </publisher> <address> London, </address> <year> 1987. </year>
Reference-contexts: That is, this hypothesis defines a "reasonable" parallel model in terms of a "reasonable" sequential model. The Turing machine model is polynomially equivalent to the RAM model, (Aho et al., 1974; Papadimitriou, 1994). Making a few assumptions about the processor word-sizes gives the next theorem <ref> (Parberry, 1987) </ref>, The time complexity of a problem on a PRAM is polynomially equivalent to the space complexity of the same problem on a RAM. Definitions and Foundations 14 As before, this assumes the amount of work on the PRAM is within a polynomial of the work on the RAM.
Reference: <author> P. W. Purdom Jr. and C. A. Brown: </author> <title> The Analysis of Algorithms, </title> <publisher> Holt, Rinehart and Winston, </publisher> <year> 1985. </year> <title> BIBLIOGRAPHY 134 R. Raman and U. Vishkin: "Optimal Randomized Parallel Algorithms for Computing the Row Minima of a Totally Monotone Matrix," </title> <booktitle> Proceedings of the Fifth Symposium on Discerete Algorithms (SODA), </booktitle> <publisher> ACM Press, </publisher> <pages> 613-621, </pages> <year> 1994. </year>
Reference: <author> P. Ramanan: </author> <title> "A New Lower Bound Technique and its Application: Tight Lower Bounds for a Polygon Triangularization Problem," </title> <booktitle> Proceedings of the Second Annual Symposium on Discrete Algorithms, (SODA), </booktitle> <publisher> ACM Press, </publisher> <pages> 281-290, </pages> <year> 1991. </year>
Reference: <author> P. Ramanan: </author> <title> "An Efficient Parallel Algorithm for Finding an Optimal Order of Computing a Matrix Chain Product," </title> <type> Technical Report, </type> <institution> WSUCS-92-2, Wichita State University, </institution> <month> June, </month> <year> 1992. </year>
Reference-contexts: The algorithms given here improve the algorithms given by many people. In particular, both the extended abstract in <ref> (Ramanan, 1992) </ref> and the later full version (Ramanan, 1993) give an O (lg 4 n) time and n processor CREW PRAM MCOP algorithm building directly on the work of Hu and Shing.
Reference: <author> P. Ramanan: </author> <title> "An Efficient Parallel Algorithm for the Matrix Chain Product Problem," </title> <type> Technical Report, </type> <institution> WSUCS-93-1, Wichita State University, </institution> <month> January, </month> <year> 1993. </year> <note> Submitted. </note>
Reference-contexts: The algorithms given here improve the algorithms given by many people. In particular, both the extended abstract in (Ramanan, 1992) and the later full version <ref> (Ramanan, 1993) </ref> give an O (lg 4 n) time and n processor CREW PRAM MCOP algorithm building directly on the work of Hu and Shing.
Reference: <author> P. Ramanan: </author> <title> "A New Lower Bound Technique and its Application: Tight Lower Bounds for a Polygon Triangularization Problem," </title> <journal> SIAM Journal on Computing, </journal> <volume> Vol. 23, No. 4, </volume> <pages> 834-851, </pages> <year> 1994. </year>
Reference: <author> G. J. E. Rawlins: </author> <title> Compared To What ?, Computer Science Press/W. </title> <editor> H. </editor> <publisher> Freeman, </publisher> <year> 1992. </year>
Reference: <author> J. H. Reif: </author> <title> "Depth First Search is Inherently Sequential," </title> <journal> Information Processing Letters, </journal> <volume> Vol. 20, No. 5, </volume> <pages> 229-234, </pages> <year> 1985. </year>
Reference-contexts: The problem of lexicographical depth first search is inherently sequential, though the general problem of depth first search is not known to be either inherently sequential or inherently parallel <ref> (Reif, 1985) </ref>. In (Anderson and Mayr, 1987; Anderson and Mayr, 1987a) it is shown that many problems that have elementary solutions by the lexicographical greedy principle are inherently sequential. This makes many problems that are solvable sequentially using lexicographical reachability intractable to solve in polylog time.
Reference: <editor> J. H. Reif (Editor): </editor> <title> Synthesis of Parallel Algorithms, </title> <publisher> Morgan-Kaufmann, </publisher> <year> 1993. </year>
Reference: <author> W. Rytter: </author> <title> "On Efficient Parallel Computation for Some Dynamic Programming Problems," </title> <journal> Theoretical Computer Science, </journal> <volume> Vol. 59, </volume> <pages> 297-307, </pages> <year> 1988. </year> <title> BIBLIOGRAPHY 135 S-H. Teng: "The Construction of Huffman-Equivalent Prefix Code in N C," </title> <journal> SIGACT News, </journal> <volume> 18(4), </volume> <pages> 54-61, </pages> <year> 1987. </year>
Reference-contexts: It is modified in that minimum paths are dynamically relaxed with new jumpers. This algorithm is basically the same as Rytter's algorithm <ref> (Rytter, 1988) </ref>. <p> Theorem 4 shows that D n graphs can be constructed quickly in parallel. These results are similar to those in <ref> (Rytter, 1988) </ref>. Any cost function f (i; j; k) can be used to solve the MPP as long as the shortest paths in the appropriate D n graph remain the same. 4.5 Historical Notes There is an interesting history of parallel solutions to the MCOP in the 1980s.
Reference: <author> L. G. Valiant, S. Skyum, S. Berkowitz, and C. Rackoff: </author> <title> "Fast Parallel Computation of Polynomials Using Few Processors," </title> <journal> SIAM Journal on Computing, </journal> <volume> Vol. 12, No. 4, </volume> <pages> 641-644, </pages> <month> Nov. </month> <year> 1983. </year>
Reference-contexts: This makes many problems that are solvable sequentially using lexicographical reachability intractable to solve in polylog time. Non-lexicographical greedyness is not necessarily bogged down in the same way. On the other hand, since <ref> (Valiant et al., 1983) </ref>, many problems amenable to dynamic programming were known to be N C, but their processor complexities were very high (asymptotically ninth degree polynomials). More recent results have lowered this processor complexity. <p> Any cost function f (i; j; k) can be used to solve the MPP as long as the shortest paths in the appropriate D n graph remain the same. 4.5 Historical Notes There is an interesting history of parallel solutions to the MCOP in the 1980s. In 1983, <ref> (Valiant et al., 1983) </ref> showed that many problems with simple O (n 3 ) sequential dynamic programming solutions are in the class N C. One of the problems they were considering was the MCOP.
Reference: <author> F. F. Yao: </author> <title> "Efficient Dynamic Programming Using Quadrangle Inequalities," </title> <booktitle> Symposium on the Theory on Computing (STOC), </booktitle> <publisher> ACM Press, </publisher> <pages> 429-435, </pages> <year> 1980. </year>
Reference: <author> F. F. Yao: </author> <title> "Speed-Up in Dynamic Programming," </title> <journal> SIAM Journal on Algebraic and Discrete Methods, </journal> <volume> Vol. 3, No. 4, </volume> <pages> 532-540, </pages> <year> 1982. </year>
Reference-contexts: Many of the parallel and sequential algorithms for solving problems amenable to dynamic programming solutions depend on monotonicity properties. Early examples of monotonicity properties used to improve sequential dynamic programming algorithms can be found in (Knuth, 1971) and <ref> (Yao, 1982) </ref>. Some of the techniques of (Yao, 1982) have been made parallel by (Czumaj, 1993) for attacking the MCOP. 9.1.3 Some Comments on Solving the OBST on a D n Graph As we saw in Chapter 4, a D n graph suited for solving the problem of building an optimal <p> Many of the parallel and sequential algorithms for solving problems amenable to dynamic programming solutions depend on monotonicity properties. Early examples of monotonicity properties used to improve sequential dynamic programming algorithms can be found in (Knuth, 1971) and <ref> (Yao, 1982) </ref>. Some of the techniques of (Yao, 1982) have been made parallel by (Czumaj, 1993) for attacking the MCOP. 9.1.3 Some Comments on Solving the OBST on a D n Graph As we saw in Chapter 4, a D n graph suited for solving the problem of building an optimal binary search tree can be constructed in
References-found: 77

