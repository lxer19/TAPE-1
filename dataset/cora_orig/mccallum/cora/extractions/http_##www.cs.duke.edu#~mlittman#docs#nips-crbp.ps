URL: http://www.cs.duke.edu/~mlittman/docs/nips-crbp.ps
Refering-URL: 
Root-URL: 
Title: Generalization and scaling in reinforcement learning  
Author: David H. Ackley Michael L. Littman 
Address: Morristown, NJ 07960  
Affiliation: Cognitive Science Research Group Bellcore  
Abstract: In associative reinforcement learning, an environment generates input vectors, a learning system generates possible output vectors, and a reinforcement function computes feedback signals from the input-output pairs. The task is to discover and remember input-output pairs that generate rewards. Especially difficult cases occur when rewards are rare, since the expected time for any algorithm can grow exponentially with the size of the problem. Nonetheless, if a reinforcement function possesses regularities, and a learning algorithm exploits them, learning time can be reduced below that of non-generalizing algorithms. This paper describes a neural network algorithm called complementary reinforcement back-propagation (CRBP), and reports simulation results on problems designed to offer differing opportunities for generalization.
Abstract-found: 1
Intro-found: 1
Reference: <author> Ackley, D.H. </author> <title> (1987) A connectionist machine for genetic hillclimbing. </title> <address> Boston, MA: </address> <publisher> Kluwer Academic Press. </publisher>
Reference: <author> Ackley, D.H. </author> <title> (1989) Associative learning via inhibitory search. </title> <editor> In D.S. Touretzky (ed.), </editor> <booktitle> Advances in Neural Information Processing Systems 1, </booktitle> <pages> 20-28. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Allen, R.B. </author> <title> (1989) Developing agent models with a neural reinforcement technique. </title> <booktitle> IEEE Systems, Man, and Cybernetics Conference. </booktitle> <address> Cambridge, MA. </address>
Reference: <author> Anderson, C.W. </author> <title> (1986) Learning and problem solving with multilayer connectionist systems. University of Mass. </title> <type> Ph.D. dissertation. COINS TR 86-50. </type> <address> Amherst, MA. </address>
Reference: <author> Barto, </author> <title> A.G. (1985) Learning by statistical cooperation of self-interested neuron-like computing elements. </title> <journal> Human Neurobiology, </journal> <volume> 4 </volume> <pages> 229-256. </pages>
Reference: <author> Barto, A.G., & Anandan, P. </author> <title> (1985) Pattern recognizing stochastic learning automata. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 15, </volume> <pages> 360-374. </pages>
Reference: <author> Rumelhart, D.E., Hinton, G.E., & Williams, </author> <title> R.J. (1986) Learning representations by back-propagating errors. </title> <journal> Nature, </journal> <volume> 323, </volume> <pages> 533-536. </pages>
Reference: <author> Sutton, </author> <title> R.S. (1984) Temporal credit assignment in reinforcement learning. University of Mass. </title> <type> Ph.D. dissertation. COINS TR 84-2. </type> <address> Amherst, MA. </address>
Reference: <author> Williams, </author> <title> R.J. (1988) Toward a theory of reinforcement-learning connectionist systems. </title> <institution> College of Computer Science of Northeastern University Technical Report NU-CCS-88-3. </institution> <address> Boston, MA. </address>
References-found: 9

