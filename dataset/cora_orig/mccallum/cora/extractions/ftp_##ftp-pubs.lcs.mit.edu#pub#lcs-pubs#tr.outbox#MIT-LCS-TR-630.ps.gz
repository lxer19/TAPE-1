URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/tr.outbox/MIT-LCS-TR-630.ps.gz
Refering-URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/listings/tr600.html
Root-URL: 
Title: Global Partitioning of Parallel Loops and Data Arrays for Caches and Distributed Memory in Multiprocessors  
Author: by Rajeev K. Barua A. Agarwal 
Degree: B.Tech., Computer Science and Engineering Indian  Submitted to the DEPARTMENT OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCE in partial fulfillment of the requirements for the degree of MASTER OF SCIENCE at the  All rights reserved Signature of Author:  Certified by:  Associate Professor of Computer Science and Engineering Thesis Supervisor Accepted by: F. R. Morgenthaler Chairman, Departmental Graduate Committee  
Note: c 1994  
Date: (1992)  May 1994  May 12, 1994  
Address: New Delhi  
Affiliation: Institute of Technology,  MASSACHUSETTS INSTITUTE OF TECHNOLOGY  Massachusetts Institute of Technology  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> S. G. Abraham and D. E. Hudak. </author> <title> Compile-time partitioning of iterative parallel loops to reduce cache coherency traffic. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 318-328, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Introduction The problem of loop and data partitioning for distributed memory multiprocessors with global address spaces has been studied by many researchers <ref> [1, 2, 4, 14] </ref>. The goal of loop partitioning for applications with nested loops that access data arrays is to divide the iteration space among the processors to get maximum reuse of data in the cache, subject to the constraint of having good load balance. <p> Their theory produces communication-free hyperplane partitions for loops with affine index expressions when such partitions exist. However, when communication-free partitions do not exist, they deal only with index expressions of the form variable plus a constant. Abraham and Hudak <ref> [1] </ref> look at the problem of automatic loop partitioning for cache locality only for the case when array accesses have simple index expressions. Their method uses a local per-loop analysis. 12 A more general framework was presented by Agarwal et. al. [2] for optimizing for cache locality.
Reference: [2] <author> Anant Agarwal, David Kranz, and Venkat Natarajan. </author> <title> Automatic Partitioning of Parallel Loops for Cache-Coherent Multiprocessors. </title> <booktitle> In 22nd International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1993. </year> <note> IEEE. A version of this paper appears as MIT/LCS TM-481, </note> <month> December </month> <year> 1992. </year>
Reference-contexts: Introduction The problem of loop and data partitioning for distributed memory multiprocessors with global address spaces has been studied by many researchers <ref> [1, 2, 4, 14] </ref>. The goal of loop partitioning for applications with nested loops that access data arrays is to divide the iteration space among the processors to get maximum reuse of data in the cache, subject to the constraint of having good load balance. <p> This can either be done by a programmer, or by a previous dependence analysis and paral-lelization phase. The algorithm presented is mainly directed towards cache-coherent multiprocessors with physically distributed memory. Initially, the basic algorithm for deriving loop partitions defined in <ref> [2] </ref> is used. These partitions are optimized for cache reuse without regard to data locality. This partition is used as an initial loop partition and the induced data partition is used as the initial data partition. <p> The NWO simulator [7] for the Alewife machine has been used in this process. We present the improvement in locality achieved as well as the impact on overall performance. 1.4 Overview of Results It was found that the heuristic method provided significant speedup over using the method described in <ref> [2] </ref>, which itself had significant speedup over using random data partitions with cache-optimized loop partitions. <p> Chapter 2 describes related work. Chapter 3 describes the framework and notation used by <ref> [2] </ref>, which we build on. Chapter 4 10 describes the cost model. Chapter 5 describes the heuristic method. Chapter 6 looks at the relative benefits of, and the tradeoffs between, optimizing for cache and data locality. Chapter 7 describes some experimental results. <p> Abraham and Hudak [1] look at the problem of automatic loop partitioning for cache locality only for the case when array accesses have simple index expressions. Their method uses a local per-loop analysis. 12 A more general framework was presented by Agarwal et. al. <ref> [2] </ref> for optimizing for cache locality. They handled fully general affine access functions, i.e. accesses of the form A [2i+j,j] and A [100-i,j] were handled. <p> This technique can be used before partitioning when the programming model is sequential to convert to parallel loops. 13 Chapter 3 Loop Partitioning Overview This chapter gives a brief summary of the method for loop partitioning to increase cache reuse given in <ref> [2] </ref>. We use this method as the starting point for loop and data partitioning. The method handles programs with loop nests where the array index expressions are affine functions of the loop variables. <p> This is because all data elements in the cumulative footprints have only their first time access not in cache, assuming a large enough cache, and no interference. <ref> [2] </ref> shows how L can be chosen to minimize the number of cache misses. In doing so, it shows how the combined footprints of a set of uniformly intersecting references can be characterized by a single offset vector ^a. <p> This vector is used in the cost model presented in the next chapter. The vector in a sense is the summary of all the offset vectors in a uniformly intersecting set. <ref> [2] </ref> presents a theorem giving the size of the cumulative footprint, which we reproduce here: Theorem 1 Given a hyperparallelepiped tile L and a unimodular reference matrix G, the size of the cumulative footprint with respect to a set of uniformly intersecting references specified by the reference matrix G and a <p> The sum of these expressions for all the arrays in a loop yields the total cumulative footprint of the loop. This can then be minimized by conventional methods to obtain the loop partitioning with best cache locality. A complete description of this technique appears in <ref> [2] </ref>. In the cost model we also refer to the data partition D. D represents how the data space is tiled. This is represented as a tile at the origin of the data space just like L is represented as a tile at the origin of the iteration space. <p> It returns an estimation of the cost of array references for the loop. 4.1 Derivation of the formula In this section we shall define the cost formula in terms of the variables used to define partitions in <ref> [2] </ref>. We shall begin in section 4.1.1 by stating a basic formula for total access time in a loop, in terms of the number of cache, local and remote accesses in the loop. <p> We refer to F f as the peripheral footprint. Although the exact details of how F f is computed is not important to understanding the heuristic method in the next chapter, <ref> [2] </ref> demonstrates that F f can be computed as: F f = k=1 j Det D k!^a j where ^a is the spread vector of references as mentioned in Chapter 3, and defined in [2]. 4.1.3 The Final Formula Theorem 2 The cumulative access time for all accesses in a loop <p> F f is computed is not important to understanding the heuristic method in the next chapter, <ref> [2] </ref> demonstrates that F f can be computed as: F f = k=1 j Det D k!^a j where ^a is the spread vector of references as mentioned in Chapter 3, and defined in [2]. 4.1.3 The Final Formula Theorem 2 The cumulative access time for all accesses in a loop with partitioning L, accessing an array having data partition D with reference matrix G in a uniformly intersecting set, is T total access = T R (R b + F f ) + T
Reference: [3] <author> A. Agarwal et al. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. </title> <booktitle> In Proceedings of Workshop on Scalable Shared Memory Multiprocessors. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year> <note> An extended version of this paper has been submitted for publication, and appears as MIT/LCS Memo TM-454, </note> <year> 1991. </year>
Reference-contexts: Hence we conclude that an initial cache optimized solution may not make a difference for some programs, and leads to gains in others. 42 Chapter 7 Results We have implemented the algorithm described in this thesis as part of our compiler for the Alewife <ref> [3] </ref> machine. The Alewife machine implements a shared global address space with distributed physical memory and coherent caches. The nodes contain slightly modified SPARC processors and are configured in a 2-dimensional mesh network.
Reference: [4] <author> Jennifer M. Anderson and Monica S. Lam. </author> <title> Global Optimizations for Parallelism and Locality on Scalable Parallel Machines. </title> <booktitle> In Proceedings of SIGPLAN '93, Conference on Programming Languages Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: Introduction The problem of loop and data partitioning for distributed memory multiprocessors with global address spaces has been studied by many researchers <ref> [1, 2, 4, 14] </ref>. The goal of loop partitioning for applications with nested loops that access data arrays is to divide the iteration space among the processors to get maximum reuse of data in the cache, subject to the constraint of having good load balance. <p> We borrow the concept of uniformly generated references from their work, which was used earlier in Wolf and Lam [17] and Gannon et. al. [8] also. However, they found local minima for each loop independently, giving possibly conflicting data partitioning requests across loops. The work of Anderson and Lam <ref> [4] </ref> does a global analysis across loops, but has the following differences with our method: (1) It does not take into account the effect of globally coherent caches. <p> Finally section 5.3 shows that the algorithm has polynomial time complexity. 5.1 Graph formulation A commonly used natural representation of a program with many loops accessing many arrays is a bipartite graph G = (V l ; V d ; E), as in <ref> [4] </ref>. We picture the loops as a set of nodes V l on the left hand side, and the data arrays as a set of nodes V d on the right. <p> As an example, we ran the conduct routine from the SIMPLE application , a hydrodynamics code from the Lawrence Livermore National Lab, on the Alewife machine simulator, known as NWO [7]. This is the same code used as the example in <ref> [4] </ref>. It has 20 loop nests. They used a problem size of 1K by 1K but, because we were using a simulator, we used a problem size about 50 times smaller. We use a static partition for the data. Combining the possibility of runtime data migration as in [4] with our <p> example in <ref> [4] </ref>. It has 20 loop nests. They used a problem size of 1K by 1K but, because we were using a simulator, we used a problem size about 50 times smaller. We use a static partition for the data. Combining the possibility of runtime data migration as in [4] with our algorithm might well improve performance. <p> The different partitions impact the percentage of local versus remote references quite dramatically as shown in Figure 7.2. In all cases the cache hit rate was around 88%. The overall speedups are not that large compared to those shown in <ref> [4] </ref> for four reasons. 1. The problem size we used is 50 times smaller resulting in more overhead asso ciated with barriers. 44 2. <p> Future compiler implementations could incorporate prefetching and our partitioning scheme, and measure how well they complement each other. We would like to add the possibility of copying data at runtime to avoid remote references as in <ref> [4] </ref>. This factor could be added to our cost model. We do however suspect that data relocation is probably less important in cache-coherent shared memory machines than in message passing machines. 48
Reference: [5] <author> E. A. Brewer, C. N. Dellarocas, A. Colbrook, and W. E. Weihl. Pro-teus: </author> <title> A high-performance parallel-architecture simulator. </title> <type> Technical Report MIT/LCS/TR-516, </type> <institution> Massachusetts Institute of Technology, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: In lieu of this data, we did present estimated total access times for a scaled-up conduct routine with and without an initial cache optimized solution in section 6.3. Simulating a larger problem size should not be a problem once the Proteus <ref> [5] </ref> simulator for Alewife is available. We see that the local version does pretty well. This is partly because the remote latency of the machine we simulated is quite low.
Reference: [6] <author> David Callahan, Ken Kennedy, and Allan Porterfield. </author> <title> Software Prefetching. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 40-52. </pages> <publisher> ACM, </publisher> <month> April </month> <year> 1991. </year>
Reference-contexts: Current results indicate that combining loop and data partitioning is important in obtaining the best performance on cache-coherent distributed memory multiprocessors. Prefetching is a promising technique for latency reduction in multiprocessors. Software-controlled prefetching <ref> [6, 11, 13, 15] </ref> involves placing prefetch instructions in the compiler generated code. Future compiler implementations could incorporate prefetching and our partitioning scheme, and measure how well they complement each other. We would like to add the possibility of copying data at runtime to avoid remote references as in [4].
Reference: [7] <author> David Chaiken. </author> <note> NWO User's Manual. ALEWIFE Memo No. 36, </note> <institution> Laboratory for Computer Science, Massachusetts Institute of Technology, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: We will describe the implementation and present some results using a part of the SIMPLE program. The NWO simulator <ref> [7] </ref> for the Alewife machine has been used in this process. <p> Local and remote accesses fetch cache lines (16 bytes). The last number will be larger for large machine configurations. As an example, we ran the conduct routine from the SIMPLE application , a hydrodynamics code from the Lawrence Livermore National Lab, on the Alewife machine simulator, known as NWO <ref> [7] </ref>. This is the same code used as the example in [4]. It has 20 loop nests. They used a problem size of 1K by 1K but, because we were using a simulator, we used a problem size about 50 times smaller. We use a static partition for the data.
Reference: [8] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 587-616, </pages> <year> 1988. </year> <month> 49 </month>
Reference-contexts: They handled fully general affine access functions, i.e. accesses of the form A [2i+j,j] and A [100-i,j] were handled. We borrow the concept of uniformly generated references from their work, which was used earlier in Wolf and Lam [17] and Gannon et. al. <ref> [8] </ref> also. However, they found local minima for each loop independently, giving possibly conflicting data partitioning requests across loops.
Reference: [9] <author> M. Gupta and P. Banerjee. </author> <title> Demonstration of Automatic Data Partitioning Tech--niques for Parallelizing Compilers on Multicomputers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(2) </volume> <pages> 179-193, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Our method of having a cost model can evaluate different competing alternatives, each having some amount of communication, and choose between them. (3) We guarantee a load balanced solution. Gupta and Banerjee <ref> [9] </ref> have developed an algorithm for partitioning doing a global analysis across loops. They allow simple index expression accesses of the form c 1 fl i + c 2 , but not general affine functions.
Reference: [10] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiling Fortran D for MIMD Distributed Memory Machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: One approach to solve this problem is to leave it to the user to specify data partitions explicitly in the program, as in Fortran-D <ref> [10, 16] </ref>. Loop partitions are usually determined by the owner computes rule. Though simple to implement, this requires the user to thoroughly understand the access patterns of the program, a task which is not trivial even for small programs.
Reference: [11] <author> Alexander C. Klaiber and Henry M. Levy. </author> <title> An Architecture for Software-Controlled Data Prefetching. </title> <booktitle> In Proceedings of the 18th International Conference on Computer Architecture, </booktitle> <address> Toronoto, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: Current results indicate that combining loop and data partitioning is important in obtaining the best performance on cache-coherent distributed memory multiprocessors. Prefetching is a promising technique for latency reduction in multiprocessors. Software-controlled prefetching <ref> [6, 11, 13, 15] </ref> involves placing prefetch instructions in the compiler generated code. Future compiler implementations could incorporate prefetching and our partitioning scheme, and measure how well they complement each other. We would like to add the possibility of copying data at runtime to avoid remote references as in [4].
Reference: [12] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam. </author> <title> The Stanford Dash Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: In all cases the cache hit rate was around 88%. The overall speedups are not that large compared to those shown in [4] for four reasons. 1. The problem size we used is 50 times smaller resulting in more overhead asso ciated with barriers. 44 2. The Stanford Dash <ref> [12] </ref> machine distributes data which does not fit in local memory across the network in the one-processor run case, which Alewife does not do. Hence their sequential run would be relatively slower than ours, giving a larger speedup for Dash. 3.
Reference: [13] <author> Todd Mowry and Anoop Gupta. </author> <title> Tolerating Latency Through Software-Controlled Prefetching in Shared-Memory Multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12(2) </volume> <pages> 87-106, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Current results indicate that combining loop and data partitioning is important in obtaining the best performance on cache-coherent distributed memory multiprocessors. Prefetching is a promising technique for latency reduction in multiprocessors. Software-controlled prefetching <ref> [6, 11, 13, 15] </ref> involves placing prefetch instructions in the compiler generated code. Future compiler implementations could incorporate prefetching and our partitioning scheme, and measure how well they complement each other. We would like to add the possibility of copying data at runtime to avoid remote references as in [4].
Reference: [14] <author> J. Ramanujam and P. Sadayappan. </author> <title> Compile-Time Techniques for Data Distribution in Distributed Memory Machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 472-482, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Introduction The problem of loop and data partitioning for distributed memory multiprocessors with global address spaces has been studied by many researchers <ref> [1, 2, 4, 14] </ref>. The goal of loop partitioning for applications with nested loops that access data arrays is to divide the iteration space among the processors to get maximum reuse of data in the cache, subject to the constraint of having good load balance. <p> For real medium-sized or large programs, the task is a very difficult one. Presence of fully general affine function accesses further complicates the process. Further, the user would need to be familiar with machine architecture and architectural parameters to understand the trade-offs involved. Ramanujam and Sadayappan <ref> [14] </ref> deal with data partitioning in multicomputers and use a matrix formulation; their results do not apply to multiprocessors with caches. Their theory produces communication-free hyperplane partitions for loops with affine index expressions when such partitions exist.
Reference: [15] <author> Monica S. Lam Todd C. Mowry and Anoop Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Proceedings of the Fifth ACM Int'l Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> Oct </month> <year> 1992. </year>
Reference-contexts: Current results indicate that combining loop and data partitioning is important in obtaining the best performance on cache-coherent distributed memory multiprocessors. Prefetching is a promising technique for latency reduction in multiprocessors. Software-controlled prefetching <ref> [6, 11, 13, 15] </ref> involves placing prefetch instructions in the compiler generated code. Future compiler implementations could incorporate prefetching and our partitioning scheme, and measure how well they complement each other. We would like to add the possibility of copying data at runtime to avoid remote references as in [4].
Reference: [16] <author> C.-W. Tseng. </author> <title> An Optimizing Fortran D compiler for MIMD Distributed-Memory Machines. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> Jan </month> <year> 1993. </year> <note> Published as Rice COMP TR93-199. </note>
Reference-contexts: One approach to solve this problem is to leave it to the user to specify data partitions explicitly in the program, as in Fortran-D <ref> [10, 16] </ref>. Loop partitions are usually determined by the owner computes rule. Though simple to implement, this requires the user to thoroughly understand the access patterns of the program, a task which is not trivial even for small programs.
Reference: [17] <author> M. Wolf and M. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the ACM SIGPLAN 91 Conference Programming Language Design and Implementation, </booktitle> <pages> pages 30-44, </pages> <year> 1991. </year>
Reference-contexts: They handled fully general affine access functions, i.e. accesses of the form A [2i+j,j] and A [100-i,j] were handled. We borrow the concept of uniformly generated references from their work, which was used earlier in Wolf and Lam <ref> [17] </ref> and Gannon et. al. [8] also. However, they found local minima for each loop independently, giving possibly conflicting data partitioning requests across loops.
Reference: [18] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A Loop Transformation Theory and an Algorithm to Maximize Parallelism. </title> <booktitle> In The Third Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1990. </year> <institution> Irvine, </institution> <address> CA. </address> <month> 50 </month>
Reference-contexts: They allow simple index expression accesses of the form c 1 fl i + c 2 , but not general affine functions. They do not allow for the possibility of hyperparallelepiped data tiles, and do not account for caches. The work of Wolf and Lam <ref> [18] </ref> complements ours. They deal with the problem of taking sequential loops and applying transformations to them to convert them to a set of parallel loops with at most one outer sequential loop.
References-found: 18

