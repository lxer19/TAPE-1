URL: http://www.daimi.aau.dk/~rmunk/progress.ps.gz
Refering-URL: http://www.daimi.aau.dk/~rmunk/publications.html
Root-URL: http://www.daimi.aau.dk
Title: Parallel Numerical Algorithms and Applications in Inverse Problems  
Date: 7, 1997  
Note: February  
Abstract: Progress report by: Rasmus Munk Larsen Department of Computer Science University of Aarhus Ny Munkegade, Building 540 DK-8000 Aarhus C, Denmark Abstract A description of the development in the architecture of supercomputers, and its inAEu-ence on the development of numerical algorithms used in scientioc computing is presented. We focus especially on the trade-ooe between numerical stability and eOEciency. Numerical algorithms for solving linear inverse problems and their implementation on parallel supercomputers are discussed, with emphasis on the iterative regularization algorithms based on the Lanczos process. Finally applications in helioseismology and image restoration are treated, and it is demonstrated how an iterative regularization method can be succesfully applied to both problems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. M. </author> <title> Amdahl, Validity of the single processor approach to achieving large-scale computing capabilities, </title> <booktitle> in AFIPS Conference Proceedings 30, </booktitle> <publisher> AFIPS Press, </publisher> <address> Montvale, NJ, </address> <year> 1967, </year> <month> 483485. </month>
Reference-contexts: One of the basic concepts is speedup S (and the related concept of eOEciency E) which attempts to capture the amount of parallelism in an algorithm by measuring the ratio between the execution times with 1 and P processors S P = T P S P : (1) See also <ref> [1] </ref>. In practice speedup can be somewhat misleading especially because people use the term speedup when referring to quite dioeerent measures.
Reference: [2] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, O. Ostrouchow & D. Sorensen, </author> <title> LAPACK Users' Guide, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1992. </year>
Reference-contexts: but for most numerical techniques used extensively in scientioc computing, such as numerical linear algebra, methods for solving partial and ordinary dioeer-ential equations, linear and non-linear optimization, quadrature, fast fourier transforms etc., many software libraries exist implementing the istate of the artj algorithms for a variety of platforms; see, e.g., <ref> [32, 2, 60, 52] </ref>. In the following it will be illustrated how the development in computer architecture has inAEuenced the algorithms used to solve large-scale numerical problem and vice-versa. <p> For many large-scale problems standard implementations of these algorithms found in software libraries such as LAPACK (see <ref> [2] </ref>), may not be applicable because they require the entire matrix to be stored explicitly in the computer.
Reference: [3] <author> E. F. D'Azevedo, V.L. Eijkhout & C.H. Romine, </author> <title> Reducing Communication Costs in the Conjugate Gradient Algorithm on Distributed Memory Multiprocessors, </title> <institution> University of Tennesee, CS-93-187, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: If, say, the vectors are distributed over the processors the updates can simply be done locally, but the global reduction involved in computing the inner products and broadcasting the result involves communication. It has been demonstrated in a number of publications (see, e.g., <ref> [71, 3, 74] </ref> and [30], Section 8) how to reorder the operations performed in the algorithms such that the communication can be overlapped with, e.g., the local vector updates without changing the numerical properties of the algorithm.
Reference: [4] <author> G. E. Backus & J. F. Gilbert, </author> <title> The resolving power of gross earth data, Geophys. </title> <journal> J. R. Astron. Soc. </journal> <volume> 16 (1968), </volume> <pages> 169205. </pages>
Reference-contexts: A mollioer method is a technique for computing the coeOEcients explicitly in such a way that averaging kernel K (x 0 ; x) have the desired properties. The Backus-Gilbert (BG) method (cf. <ref> [4] </ref>), also known as the method of optimally localized averages, has been used successfully in seismology, ([63]), helioseismology, [22] and other areas that involve inverse problems, [27].
Reference: [5] <author> D. H. Bailey, </author> <title> Twelve Ways to Fool the Masses When Giving Performance Results on Parallel Computers, </title> <type> RNR Technical Report RNR-91-020, </type> <year> 1991. </year>
Reference-contexts: This lack of consistency calls for a lot of care when comparing dioeerent presentations of parallel algorithms, cf. <ref> [5] </ref>. Furthermore it is questionable whether speedup makes sense for all architectures. Consider the execution of an algorithm solving a problem of oxed size on a parallel computer with a memory hierarchy.
Reference: [6] <author> S. M. Balle & P. C. Hansen, </author> <title> Block Algorithms for the Connection Machine, </title> <type> Technical Report UNIC-93-10, </type> <institution> UNI*C, </institution> <year> 1993. </year>
Reference-contexts: Even for algorithms that do allow a large amount of data-parallelism it takes a large eoeort to achieve a good performance on the massively parallel machines <ref> [6] </ref>. <p> This often involves a delicate trade-ooe between, e.g. granularity, cost of communication and load balancing, see <ref> [6] </ref>. Examples: The LU, QR and Cholesky factorization algorithms used in ScaLAPACK (and LAPACK) which are simply block versions of the corresponding scalar (point) algorithms, cf. [15]. 2. Develop completely new algorithms with better parallel performance and the same numerical stability as the standard serial algorithms. Few examples exist.
Reference: [7] <author> S. M. Balle, P. C. Hansen & N. J. Higham, </author> <title> A Strassen-Type Matrix Inversion Algorithm for the Connection Machine, </title> <type> Technical Report UNIC-93-11, </type> <institution> UNI*C, </institution> <year> 1993. </year>
Reference-contexts: See e.g. the method described in <ref> [7] </ref> 2.3.2 Examples In the following we shall go through an example of dioeerent parallelizations of a numerical algorithm and illustrate how some of the four techniques listed above are applied to develop an eOEcient parallel implementation. <p> Instead we could use it in the setting indicated by principle number 4 above, and e.g. apply iterative reonement (cf. [39, Section 3.5.3] to the approximate solution, see <ref> [7] </ref>. A true block algorithm can be derived from the partitioning A 21 A 22 = I 0 0 ~ A 22 ; (4) which leads to the following: 1. U 11 A 11 , U 12 A 12 2. <p> This should be compared with the error bound of the normal point LU factorization (or Cholesky factorization) which is kx ~xk 2 O (u)(A) ; See also <ref> [7] </ref> where the idea of using a fast but unstable matrix inversion based on block LU factorization to compute an approximate solution to Ax = b is taken even further by applying a Strassens-type matrix inversion combined with iterative reonement. 3 Algorithms for Inverse Problems 3.1 Introduction to Regularization As discussed
Reference: [8] <author> V. A. Barker, </author> <title> Iterative methods for sparse systems of linear equations, Institute for Numerical Analysis, </title> <year> 1992. </year>
Reference-contexts: To understand this, consider the case where we apply the LSQR algorithm with = 0 to the problem min kAx bk 2 : (23) It can be shown (e.g. by applying Theorem 7.6 of <ref> [8] </ref> and noticing that LSQR is equivalent to CG applied to the normal equations) that the kth iterate x (k) has the following two properties kx fl x (k) k T = min kx fl xk T ; x 2 spanfv 1 ; : : : ; v k g (24)
Reference: [9] <author> A. Basermann, M. Bcker, P. Weidner, P. C. Hansen & R. M. Larsen, </author> <title> Parallel Iterative Methods for Nonsymmetric Large-Scale Problems, </title> <booktitle> APPARC PaA5a Deliverable, Esprit BRA III Contract # 6634, </booktitle> <year> 1995 </year>
Reference-contexts: Therefore the number of data transports between layers in the memory hierarchy, e.g. caused by cache misses, is in many cases the most important performance measure to consider when designing a new algorithm for or when implementing a well known algorithm on a new machine <ref> [38, 9] </ref>. Table 2, which is based on ogure 1.16 in [47] Section 1.7, lists typical values of the key ogures for the memory hierarchy of a large workstation or a small server in 1996. <p> Using these quantities we could, e.g., compute an approximation to the truncated SVD solution in Equation (13); see <ref> [9] </ref>. Solution to the regularized least squares equation: Alternatively we can compute an approximate Tikhonov solution using the bidiagonalization algorithm described in Section 3.2.4 with the full bidiagonalization replaced by the Lanczos bidiagonalization (22), i.e. <p> A dioeerent approach is to use the Lanczos algorithm without re-orthogonalization and subsequently use a test to identify the spurious singular values. On such test, which we have implemented and used in <ref> [9] </ref> is described in [29], Section 4.5. The apparent instability of the Lanczos process in onite precision has a much smaller or rather: a less destructive inAEuence, when the method is used to solve linear equations or least squares problems as described in Section 3.3.1. <p> The main computational eoeort usually lies in computing the matrix-vector products with A and A T . How these operations should be implemented in parallel depends entirely on the properties of matrix A. See, e.g. <ref> [75, 9, 30, 71, 74] </ref>. When implementing the LSQR algorithm on a parallel computer, it is necessary to compute matrix-vector multiplications with both A and A T . <p> How these operations should be implemented in parallel depends entirely on the properties of matrix A. See, e.g. [75, 9, 30, 71, 74]. When implementing the LSQR algorithm on a parallel computer, it is necessary to compute matrix-vector multiplications with both A and A T . In <ref> [9] </ref> we have demonstrated how the Lanczos bidiagonalization can be reordered such that the multiplications with A and A T can be overlapped. If A is generated ion-the-AEyj this means that A has to be generated only once, alternatively, if A is stored (as in the test problem used in [9]) <p> <ref> [9] </ref> we have demonstrated how the Lanczos bidiagonalization can be reordered such that the multiplications with A and A T can be overlapped. If A is generated ion-the-AEyj this means that A has to be generated only once, alternatively, if A is stored (as in the test problem used in [9]) the number of cache-misses is halved. <p> The overlapping introduces a number of extra synchronization points in the algorithm which lowers the achieved speedup on the machine (Convex Exemplar: See <ref> [9, 26, 24] </ref>, for a description.), but reduces the execution time almost by a factor of 2 for large problems due to the reduction in the number of cache-misses. This is a demonstration of the fact that speedup is not necessarily a good performance measure on shared-memory multiprocessors. <p> The IBM SP-2 is well suited to this type of applications with large data-sets, due to the high memory-to-cache bandwidth of the processing nodes. Implementation of the approximate SVD computation is described in <ref> [9, 29] </ref>; see also [30, Section 6.4]. 4 Applications 4.1 Helioseismology 4.1.1 Introduction The study of stellar structure have always played a central part in astrophysics. This has several reasons. First and foremost the radiation emitted from stars is our main source of information about the universe around us.
Reference: [10] <author> A. Basermann & P. Weidner, </author> <title> A parallel algorithm for determining all eigenvalues oAEarge real symmetric tridiagonal matrices, </title> <booktitle> Parallel Computing, 18 (1992), </booktitle> <pages> 11291141. </pages>
Reference-contexts: Moreover clustering of the eigenvalues can also introduce some overhead in terms of bad load balance, but despite this parallel bisection using Sturm sequences was until recently used in e.g. ScaLAPACK for both SVD and eigenvalue computations, cf. <ref> [16, 10] </ref>. Another example is the use of stationary iterative methods (Jacobi, Gauss-Seidel or SOR) for solving sparse systems, e.g. in connection with partial dioeerential equations, cf. [74, Chapter 8]. 4.
Reference: [11] <author> F. Baskett & J. L. Hennesy, </author> <title> Microprocessors: From Desktops to Supercomputers, </title> <booktitle> Science, </booktitle> <month> 261 </month> <year> (1993), </year> <title> 864870. [12] . Bjrck, A bidiagonalization algorithm for solving large and sparse ill-posed systems of linear equations, </title> <note> BIT 28 (1988), 659670. [13] . Bjrck, </note> <author> T. Elfving & Z. Strakos. </author> <title> Stability of Conjugate Gradient-Type Methods for Linear Least Squares Problems, </title> <institution> LiTH-MAT-R-1995-26, Linkping University, </institution> <month> November </month> <year> 1995. </year>
Reference-contexts: i = P n 3 Matrix-matrix mult C ij = P n possible to develop microprocessors that achieve a performance that is substantially higher than that of small vector-processors (e.g. the CONVEX C3840) and is approaching that of a large vector-supercomputer (e.g. the Cray C90), but for less money (cf. <ref> [11] </ref>). The improvement in integrated circuit technology has resulted in shorter switching delays and decreased feature size and the resulting increase in the number of transistors per chip. <p> manufactured and sold today Thinking Machines Corporation went out of business in 1994. 10 MIMD machines ooeer the largest AEexibility of the machines in Flynn's taxonomy and has emerged in the ending of the 1980s and the beginning of he 1990s as the architecture of choice for general-purpose parallel computers <ref> [37, 47, 11] </ref>. There are several reason for that: 1. MIMDs can be build from ooe-the-shelf technology found in ordinary workstations which gives them a good cost/performance ratio.
Reference: [14] <author> D. Calvetti, L. Reichel & D. C. Sorensen, </author> <title> An Implicitly Restarted Lanczos Method for Large Symmetric Eigenvalue Problems, </title> <journal> Elec. Trans. on Num. Anal. </journal> <volume> 2 (1994), </volume> <pages> 121. </pages>
Reference: [15] <author> J. </author> <title> Choi et al.,The design and implementation of the ScaLAPACK LU, QR, AND Cholesky factorization routines, </title> <institution> ORNL/TM-12470, Oak Ridge National Laboraroty, Oak Ridge, Tennesee, </institution> <year> 1994. </year> <month> 41 </month>
Reference-contexts: This often involves a delicate trade-ooe between, e.g. granularity, cost of communication and load balancing, see [6]. Examples: The LU, QR and Cholesky factorization algorithms used in ScaLAPACK (and LAPACK) which are simply block versions of the corresponding scalar (point) algorithms, cf. <ref> [15] </ref>. 2. Develop completely new algorithms with better parallel performance and the same numerical stability as the standard serial algorithms. Few examples exist. One could mention Cuppens divide-and-conquer for the symmetric eigenvalue problem as a possi ble example, cf. [39, Section 8.6], [30, Section 6.3.3]. 3. <p> The coarser granularity does not come for free but involves a tradeooe between granularity and load balancing, which will be bad if the block size n b is too large, since only one processor column participates in step 1. This is the LU factorization used in ScaLAPACK <ref> [15] </ref> and it is just as stable as the scalar LU factorization with partial pivoting [31]. When moving to a true block version of the LU factorization in order to gain some speed, we no longer have an unconditionally stable method. <p> These matrix factorizations, which are some of the standard ibuilding blocksj in numerical linear algebra, can be implemented eOEciently on practically all of todays supercomputers using a partitioned block version of the (classical) serial algorithm; see <ref> [15, 74, 30, 37] </ref>. Direct methods are very well suited for solving small to medium size problems.
Reference: [16] <author> J. Choi et al.,ScaLAPACK: </author> <title> A Portable Linear Algebra Library for Distributed Memory Computers Design issues and Performance, </title> <booktitle> In proceedings of Fourth Symposium on the Frontiers of massively Parallel Computation (McLean, </booktitle> <address> Virginia), </address> <publisher> IEEE Computing Society Press, Los Alamitos, </publisher> <address> California, </address> <year> 1992. </year>
Reference-contexts: In addition to this much work has been done on PBLAS, BLACS and ScaLAPACK in an attempt to develop a set of basic computational primitives for linear algebra on distributed memory parallel computers and the results look promising <ref> [17, 35, 16] </ref>. 2.2.1 Parallel Architectures One characteristic thing of the area of parallel computing is the diversity of architectures and programming models. Below we will outline a few of the basic issues in connection with parallel architectures starting with Floyds taxonomy [72, Chapter 8]. <p> Moreover clustering of the eigenvalues can also introduce some overhead in terms of bad load balance, but despite this parallel bisection using Sturm sequences was until recently used in e.g. ScaLAPACK for both SVD and eigenvalue computations, cf. <ref> [16, 10] </ref>. Another example is the use of stationary iterative methods (Jacobi, Gauss-Seidel or SOR) for solving sparse systems, e.g. in connection with partial dioeerential equations, cf. [74, Chapter 8]. 4.
Reference: [17] <author> J. </author> <title> Choi et al.,A Proposal for a Set of Parallel Basic LinearAlgebra Subprograms, </title> <note> LAPACK Working Note 100, </note> <year> 1995 </year>
Reference-contexts: In addition to this much work has been done on PBLAS, BLACS and ScaLAPACK in an attempt to develop a set of basic computational primitives for linear algebra on distributed memory parallel computers and the results look promising <ref> [17, 35, 16] </ref>. 2.2.1 Parallel Architectures One characteristic thing of the area of parallel computing is the diversity of architectures and programming models. Below we will outline a few of the basic issues in connection with parallel architectures starting with Floyds taxonomy [72, Chapter 8]. <p> This type of mapping could be easily expressed in, e.g., High Performance Fortran or by using the PBLAS subroutines <ref> [17] </ref>.
Reference: [18] <editor> J. Christensen-Dalsgaard, </editor> <booktitle> Lecture Notes on Stellar Oscillations, </booktitle> <institution> Institute of Physics and Astronomy, University of Aarhus, </institution> <year> 1994. </year>
Reference-contexts: appreciable amplitude ! nl0 ! nlm = m ! nlm = m 0 0 The kernels K nlm (r; ) are determined by the oscillation eigenfunctions computed from a spherically symmetric (non-rotating) solar model, i.e. the frequency shifts are estimated as orst order order perturbations to ! nl0 , cf. <ref> [18] </ref>. 32 To discretize Equation (28) we could represent the rotation rate on a discrete set of points in radius r p ; p = 1; 2; : : :; n r and latitude q ; q = 1; 2; : : :; n . and use the approach described in
Reference: [19] <editor> J. Christensen-Dalsgaard et al., </editor> <booktitle> The current state of solar modelling, Science, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: a long way in understanding the overall nature of the processes that determines the structure and evolution of a star there are still a lot of unanswered questions concerning the details of the physics of stellar matter, which cannot be answered without observational information about the interior of stars, cf. <ref> [19] </ref>. This poses a serious challenge since we cannot (for obvious reasons) probe the interior of stars directly. One way to indirectly probe the interior of stars is by measuring stellar oscillations. Stars with periodic variations in luminosity have been known for many years.
Reference: [20] <author> J. Christensen-Dalsgaard, P. C. Hansen & M. J. Thompson, </author> <title> GSVD analysis of helioseis-mic inversions, Mon. Not. </title> <editor> R. Astr. </editor> <publisher> Soc. </publisher> <month> 264 </month> <year> (1993), </year> <month> 541465. </month>
Reference-contexts: This ill-conditioning is not a result of the discretiza-tion but is a property inherent in the continuous inverse problem, i.e. that Equation (5) does not contain enough information for us to recover the function f ; see <ref> [20, 79] </ref>. This does not mean that the problem can not be solved, but the standard algorithms for solving linear least squares problems are not applicable to these ill-posed problems in their original form. <p> the standard form min fl fl A 0 fl fl : (10) For details on how to compute the reduction to standard form see, e.g., [36, 44]. 3.2.2 SVD-based methods The singular value decomposition (SVD, cf. [39]) is another very valuable tool for analyzing and solving ill-conditioned problems; see, e.g., <ref> [20] </ref>. <p> This shows that we can only can only gain information by increasing the resolution to a certain point. This property was more carefully examined for the 1D radial inversion in <ref> [20] </ref>. The dependency on the number of points in latitude was examined in [21]. To solve Equation (29) we can (in principle) apply any of the methods described in Section 3.2. <p> The dependency on the number of points in latitude was examined in [21]. To solve Equation (29) we can (in principle) apply any of the methods described in Section 3.2. Many investigations have been made into the properties of solutions produced by dioeerent regularization methods; see, e.g., <ref> [22, 20, 67] </ref>. The two most popular methods are 1. Regularized least squares (RLS) with second derivative smoothing. 2. Optimally localized averages (In the SOLA variant for reasons of computational eOE ciency).
Reference: [21] <author> J. Christensen-Dalsgaard, R. M. Larsen, J. Schou & M. J. Thompson, </author> <title> Optimally localized kernels for 2D helioseismis inversion, </title> <booktitle> In Proc. GONG'94: Helio- and Astro-seismology from Earth and Space, </booktitle> <editor> eds Ulrich, R. K., Rhodes Jr, E. J., Dppen, W., Astron. </editor> <booktitle> Soc. of the Pacioc Conference Series, </booktitle> <address> San Francisco, </address> <month> 76 </month> <year> (1994), </year> <month> 70. </month>
Reference-contexts: This shows that we can only can only gain information by increasing the resolution to a certain point. This property was more carefully examined for the 1D radial inversion in [20]. The dependency on the number of points in latitude was examined in <ref> [21] </ref>. To solve Equation (29) we can (in principle) apply any of the methods described in Section 3.2. Many investigations have been made into the properties of solutions produced by dioeerent regularization methods; see, e.g., [22, 20, 67]. The two most popular methods are 1. <p> In helioseismology the mollioer methods like SOLA are very attractive because they allow us to control the shape of the averaging kernel explicitly. As shown in, e.g. <ref> [67, 21] </ref> the averaging kernels produced by the RLS procedure often have undesirable structure away from (r 0 ; 0 ). This is especially problematic if these spurious non-local contributions to (r 0 ; 0 ), e.g. come from areas where the physical properties are not well understood; see [22].
Reference: [22] <author> J. Christensen-Dalsgaard, J. Schou & M. J. Thompson, </author> <title> A comparison of methods for inverting helioseismic data, Mon. Not. </title> <editor> R. Astr. </editor> <publisher> Soc. </publisher> <month> 242 </month> <year> (1990), </year> <month> 353369. </month>
Reference-contexts: It is possible to deone averaging kernels for many regularization methods, and they are often used for comparing dioeerent techniques for a particular application; see <ref> [22] </ref>. A mollioer method is a technique for computing the coeOEcients explicitly in such a way that averaging kernel K (x 0 ; x) have the desired properties. <p> The Backus-Gilbert (BG) method (cf. [4]), also known as the method of optimally localized averages, has been used successfully in seismology, ([63]), helioseismology, <ref> [22] </ref> and other areas that involve inverse problems, [27]. <p> Often, a regularization term is also added in equation (17). A commonly used criteria function is J (x 0 ; x) = 12 (x x 0 ) 2 . In helioseismology this method produces well localized averaging kernels with little structure away from x 0 , see <ref> [22] </ref>. Unfortunately the method is very expensive, since the coeOEcient matrix of the least squares problem arising from Equation (17) depends on x 0 , and hence O (mn 2 ) AEoating point operations are required for each value of x 0 . <p> The dependency on the number of points in latitude was examined in [21]. To solve Equation (29) we can (in principle) apply any of the methods described in Section 3.2. Many investigations have been made into the properties of solutions produced by dioeerent regularization methods; see, e.g., <ref> [22, 20, 67] </ref>. The two most popular methods are 1. Regularized least squares (RLS) with second derivative smoothing. 2. Optimally localized averages (In the SOLA variant for reasons of computational eOE ciency). <p> This is especially problematic if these spurious non-local contributions to (r 0 ; 0 ), e.g. come from areas where the physical properties are not well understood; see <ref> [22] </ref>. The SOLA method has suoeered from the disadvantage, that even in the most eOEcient methods used to compute the SOLA solution so far (see [23]), it has been necessary to compute the SVD of A, and hence to store it explicitly (see the discussion in Section 3.3).
Reference: [23] <author> J. Christensen-Dalsgaard & M. J. Thompson, </author> <title> A preprocessing strategy for helioseismic inversions, </title> <journal> Astron. Astrophys., </journal> <volume> 272, </volume> <year> (1993), </year> <month> L1L4. </month>
Reference-contexts: The SOLA method has suoeered from the disadvantage, that even in the most eOEcient methods used to compute the SOLA solution so far (see <ref> [23] </ref>), it has been necessary to compute the SVD of A, and hence to store it explicitly (see the discussion in Section 3.3).
Reference: [24] <institution> Convex Computing Corporation, Convex Exemplar Programming Guide, </institution> <month> March </month> <year> 1994, </year> <month> DSW-067. </month>
Reference-contexts: The overlapping introduces a number of extra synchronization points in the algorithm which lowers the achieved speedup on the machine (Convex Exemplar: See <ref> [9, 26, 24] </ref>, for a description.), but reduces the execution time almost by a factor of 2 for large problems due to the reduction in the number of cache-misses. This is a demonstration of the fact that speedup is not necessarily a good performance measure on shared-memory multiprocessors.
Reference: [25] <author> Convex Computing Corporation, </author> <title> cps parallel compiler support library, Manual pages, </title> <month> March </month> <year> 1994. </year>
Reference: [26] <institution> Convex Computing Corporation, Exemplar Architecture, </institution> <month> November </month> <year> 1993, </year> <month> DHW-014. </month>
Reference-contexts: Examples of this architecture are the SGI PowerChallenge (bus-based) [69] and the Convex Exemplar (single hypernode, based on cross-bar switch) <ref> [26] </ref>. Distributed physical memory, single address space: Known as NUMA, non-uniform memory access, or DSM, distributed shared memory machines. More diOEcult to program than the above because of the non-uniform memory access time makes it more diOEcult to model the behavior of the program. <p> The overlapping introduces a number of extra synchronization points in the algorithm which lowers the achieved speedup on the machine (Convex Exemplar: See <ref> [9, 26, 24] </ref>, for a description.), but reduces the execution time almost by a factor of 2 for large problems due to the reduction in the number of cache-misses. This is a demonstration of the fact that speedup is not necessarily a good performance measure on shared-memory multiprocessors.
Reference: [27] <author> I. J. D. Craig & J. C. Brown, </author> <title> Inverse Problems in Astronomy, Adam Hilger, </title> <address> Bristol, </address> <year> 1986. </year>
Reference-contexts: The Backus-Gilbert (BG) method (cf. [4]), also known as the method of optimally localized averages, has been used successfully in seismology, ([63]), helioseismology, [22] and other areas that involve inverse problems, <ref> [27] </ref>. <p> using FFT-based matrix-vector multiplication The inverse problem deoned by equation (31) is just one example of a deconvolution problems which arise in many experiments where some measured value relates to the true value of the observable by a convolution with a function H characteristic of the instrument (s) used, see <ref> [44, 27] </ref>. This type of problems can often be handled analytically; in this case by means of the 2-dimensional Fourier Transform. In fact we can in theory deconvolve (the term inverse oltering is also used) f eOEciently using the 2d-FFT algorithm; see, e.g., [40, 74].
Reference: [28] <author> J. K. Cullum, </author> <title> Peaks and plateaus in Lanczos methods for solving nonsymmetric systems of equations Ax = b, </title> <type> Report RC 18084, </type> <institution> IBM Research Division, T. J. Watson Research Center, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: Some of these problems can be icuredj (as I will brieAEy discuss below), and since the Lanczos process and equivalently the conjugate gradient method do have very attractive convergence properties their behavior in onite precision has been studied intensely: see e.g. <ref> [28, 42, 29, 13] </ref>. The eoeects of onite precision arithmetic in the Lanczos process is of most concern when it is used for computing singular values (or eigenvalues).
Reference: [29] <author> J. K. Cullum & R. A. Willoughby, </author> <title> Lanczos Algorithms for Large Symmetric Eigenvalue Computations. Vol. I Theory, </title> <address> Birkhuser, Boston, </address> <year> 1985. </year>
Reference-contexts: form the Lanczos algorithm is used for solving the eigenvalue problem Ax = x, especially when only a few of the largest or smallest eigenvalues are needed, but variants of the algorithm can also be used for computing the SVD and solving linear equations or least squares problems; see, e.g., <ref> [29] </ref> and [39], Section 9.3. The most important variant for solving discrete ill-posed problems is the Lanczos bidiagonalization. <p> Some of these problems can be icuredj (as I will brieAEy discuss below), and since the Lanczos process and equivalently the conjugate gradient method do have very attractive convergence properties their behavior in onite precision has been studied intensely: see e.g. <ref> [28, 42, 29, 13] </ref>. The eoeects of onite precision arithmetic in the Lanczos process is of most concern when it is used for computing singular values (or eigenvalues). <p> A dioeerent approach is to use the Lanczos algorithm without re-orthogonalization and subsequently use a test to identify the spurious singular values. On such test, which we have implemented and used in [9] is described in <ref> [29] </ref>, Section 4.5. The apparent instability of the Lanczos process in onite precision has a much smaller or rather: a less destructive inAEuence, when the method is used to solve linear equations or least squares problems as described in Section 3.3.1. <p> most stable implementations for solving ill-conditioned problems. 3.3.3 Parallel Implementation Iterative algorithms are in some sense serial by nature, since the computations in the kth iteration usually involve recursions refering to the values of variables in a few of the previous steps, as in, e.g., the conjugate gradient algorithm (cf. <ref> [29] </ref>) which involves two two-term recurrences of the form Ap i = fi i (r i r i+1 ) Hence parallelizing an iterative method is usually done by parallelizing the iatomic op erationsj involved in each iteration, i.e. inner products (ff y T x) and vector updates (y ffx + y), <p> The IBM SP-2 is well suited to this type of applications with large data-sets, due to the high memory-to-cache bandwidth of the processing nodes. Implementation of the approximate SVD computation is described in <ref> [9, 29] </ref>; see also [30, Section 6.4]. 4 Applications 4.1 Helioseismology 4.1.1 Introduction The study of stellar structure have always played a central part in astrophysics. This has several reasons. First and foremost the radiation emitted from stars is our main source of information about the universe around us.
Reference: [30] <author> J. W. Demmel, M. T. Heath, H. A. van der Vorst, </author> <title> Parallel Numerical Linear Algebra, </title> <journal> Acta Numerica 1993, </journal> <volume> 111197. </volume>
Reference-contexts: Develop completely new algorithms with better parallel performance and the same numerical stability as the standard serial algorithms. Few examples exist. One could mention Cuppens divide-and-conquer for the symmetric eigenvalue problem as a possi ble example, cf. [39, Section 8.6], <ref> [30, Section 6.3.3] </ref>. 3. Use an algorithm with less favorable convergence properties but better parallel performance. It it often the case that ioldj algorithms have come to life because they have a simpler structure than a more sophisticated istate-of-the-artj serial algorithm and therefore are easier to parallelize. <p> It it often the case that ioldj algorithms have come to life because they have a simpler structure than a more sophisticated istate-of-the-artj serial algorithm and therefore are easier to parallelize. Examples: Parallel bisection based on Sturm sequences for the symmetric eigenvalue problem (cf. [39, Chapter 8], <ref> [30] </ref>) which is not as eOEcient as the QR algorithm on a serial computer. But the method based on Sturm sequences is embarrassingly parallel each processor can simply compute a separate part of the eigenvalue spectrum and the corresponding eigenvectors using inverse iteration. <p> These matrix factorizations, which are some of the standard ibuilding blocksj in numerical linear algebra, can be implemented eOEciently on practically all of todays supercomputers using a partitioned block version of the (classical) serial algorithm; see <ref> [15, 74, 30, 37] </ref>. Direct methods are very well suited for solving small to medium size problems. <p> If, say, the vectors are distributed over the processors the updates can simply be done locally, but the global reduction involved in computing the inner products and broadcasting the result involves communication. It has been demonstrated in a number of publications (see, e.g., [71, 3, 74] and <ref> [30] </ref>, Section 8) how to reorder the operations performed in the algorithms such that the communication can be overlapped with, e.g., the local vector updates without changing the numerical properties of the algorithm. The main computational eoeort usually lies in computing the matrix-vector products with A and A T . <p> The main computational eoeort usually lies in computing the matrix-vector products with A and A T . How these operations should be implemented in parallel depends entirely on the properties of matrix A. See, e.g. <ref> [75, 9, 30, 71, 74] </ref>. When implementing the LSQR algorithm on a parallel computer, it is necessary to compute matrix-vector multiplications with both A and A T . <p> The IBM SP-2 is well suited to this type of applications with large data-sets, due to the high memory-to-cache bandwidth of the processing nodes. Implementation of the approximate SVD computation is described in [9, 29]; see also <ref> [30, Section 6.4] </ref>. 4 Applications 4.1 Helioseismology 4.1.1 Introduction The study of stellar structure have always played a central part in astrophysics. This has several reasons. First and foremost the radiation emitted from stars is our main source of information about the universe around us.
Reference: [31] <author> J. W. Demmel, N. J. Higham & R. S. Schreiber, </author> <title> Stability of Block LU Factorization, </title> <journal> Num. Lin. Alg. Appl. </journal> <volume> Vol. </volume> <month> 2(2) </month> <year> (1995), </year> <month> 173190 </month>
Reference-contexts: Generally the partitioned have the same stability as the scalar algorithms whereas the block versions are not always stable, cf. <ref> [31] </ref>. <p> This is the LU factorization used in ScaLAPACK [15] and it is just as stable as the scalar LU factorization with partial pivoting <ref> [31] </ref>. When moving to a true block version of the LU factorization in order to gain some speed, we no longer have an unconditionally stable method. <p> Moreover this algorithm is rich in matrix multiplications which perform well on parallel architectures especially if U 1 11 is formed explicitly in step 2. This algorithms does not possess the same stability as the usual point LU factorization. In <ref> [31] </ref> it is shown that even for a matrix that is symmetric positive deonite (for which even (point) Gaussian elimination without pivoting is stable) the method is only stable if A is well-conditioned.
Reference: [32] <author> J. J. Dongarra, J. R. Bunch, C. B. Moler & G. W. Stewart, </author> <title> Linpack Users' Guide, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1979. </year> <month> 42 </month>
Reference-contexts: but for most numerical techniques used extensively in scientioc computing, such as numerical linear algebra, methods for solving partial and ordinary dioeer-ential equations, linear and non-linear optimization, quadrature, fast fourier transforms etc., many software libraries exist implementing the istate of the artj algorithms for a variety of platforms; see, e.g., <ref> [32, 2, 60, 52] </ref>. In the following it will be illustrated how the development in computer architecture has inAEuenced the algorithms used to solve large-scale numerical problem and vice-versa. <p> By using the BLAS it has become relatively easy to formulate algorithms that ivectorizes by deoni-tionj and therefore achieves a good performance across a wide range of dioeerent machines. The development and wide-spread use of, e.g., the LINPACK library has fully demonstrated the usefulness of this approach (cf. <ref> [32, 34] </ref>). 5 2.1.2 Memory hierarchies In most of todays computers the rate at which operands have to be fed to the processor signiocantly exceeds the rate that is permitted by the latency and bandwidth of the main memory. This has lead to a widespread use of multilayered memory hierarchies.
Reference: [33] <author> J. J. Dongarra, J. Du Croz, S. Hammerling & R. Hanson, </author> <title> A Proposal of Level 3 Linear Algebra Subprograms, </title> <institution> Argonne National Laboratory, ANL-MCS-TM-88, </institution> <year> 1987. </year>
Reference-contexts: The level 3 BLAS is a collection of linear algebra subroutines that perform O (n 3 ) operations on O (n 2 ) data; see <ref> [33, 53] </ref>. The algorithms implemented in the LAPACK library, which is used on most serial and shared memory multiprocessor machines today for computations involving dense matrix, use block-algorithms written in terms of calls to the level 3 BLAS subroutines to achieve good performance.
Reference: [34] <author> J. J. Dongarra, F. G. Gustavson & A. Karp, </author> <title> Implementing Algorithms for Dense Matrices on a Vector Pipeline Machine, </title> <note> Siam Review 26 (1984), 91112. </note>
Reference-contexts: The architectural parameters that determine the performance for a particular vector length include cycle time, the number of stages in the pipeline and the cost of preparing the functional units for doing the computation (cf. <ref> [34, 38] </ref>). Secondly, memory accesses should have a very regular structure for the memory systems to be able to keep up with the processor. If the distance separating the elements used in a single vector operation, the so-called stride, is dioeerent from 1 performance may degrade. <p> By using the BLAS it has become relatively easy to formulate algorithms that ivectorizes by deoni-tionj and therefore achieves a good performance across a wide range of dioeerent machines. The development and wide-spread use of, e.g., the LINPACK library has fully demonstrated the usefulness of this approach (cf. <ref> [32, 34] </ref>). 5 2.1.2 Memory hierarchies In most of todays computers the rate at which operands have to be fed to the processor signiocantly exceeds the rate that is permitted by the latency and bandwidth of the main memory. This has lead to a widespread use of multilayered memory hierarchies. <p> This suggests that algorithms should be analyzed in terms of the average vector length attainable and also stride plays a major role, cf. <ref> [34] </ref>, [47, Section B.6].
Reference: [35] <author> J. J. Dongarra & R. C. Whaley, </author> <title> A Users's Guide to the BLACS v1.0, </title> <note> LAPACK Working Note 94, </note> <year> 1995. </year>
Reference-contexts: In addition to this much work has been done on PBLAS, BLACS and ScaLAPACK in an attempt to develop a set of basic computational primitives for linear algebra on distributed memory parallel computers and the results look promising <ref> [17, 35, 16] </ref>. 2.2.1 Parallel Architectures One characteristic thing of the area of parallel computing is the diversity of architectures and programming models. Below we will outline a few of the basic issues in connection with parallel architectures starting with Floyds taxonomy [72, Chapter 8].
Reference: [36] <author> L. </author> <title> Eldn Algorithms for the regularization of ill-conditioned least-squares problems, </title> <note> BIT 17 (1977), 134145. </note>
Reference-contexts: There are many ways to implement the Tikhonov method. A common approach is to initially transforming the problem to the standard form min fl fl A 0 fl fl : (10) For details on how to compute the reduction to standard form see, e.g., <ref> [36, 44] </ref>. 3.2.2 SVD-based methods The singular value decomposition (SVD, cf. [39]) is another very valuable tool for analyzing and solving ill-conditioned problems; see, e.g., [20]. <p> Solve min fl fl B 0 fl fl : (21) This can be solved for by eliminating the 's under the main diagonal using 2n Givens rotations. 4. Compute the solution x = V : For further computational details see <ref> [36] </ref> and [39], Section 5.4.3. This algorithm has the advantage, that the most expensive step in the algorithm, the bidiagonalization of A which requires 2mn 2 + 2n 3 AEoating point operations, is independent of the value of . <p> This means that if is changed the new solution can be computed cheaply, which is essential when a parameter-choice method is used to ond the optimal value of , see <ref> [54, 44, 36] </ref>. Normal equations: 1. Compute the lower triangular portion of Q = (A T A + 2 I). 2. d = A T b 3. Compute the Cholesky factorization Q = CC T . 4. Solve Cy = d and C T x = y.
Reference: [37] <author> G. C. Fox, M. A. Johnson, G. A. Lyzenga, S. Otto, J. Salmon, D. Walker. </author> <title> Solving Problems on Concurrent Procccesors, Vol I: General techniques and Regular Problems, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Clioes, NJ, </address> <year> 1988. </year>
Reference-contexts: in typical applications, but limitations in hardware technology, e.g. the high cost of fast semiconductor memory, have also given rise to new machine architectures, e.g. memory hierarchies, forcing users to restructure or even discard commonly used algorithms in favor of other methods, that were earlier considered to be less eOEcient <ref> [37] </ref>. Machine dependent constants such as the relative speed of dioeerent instructions, the number of registers, memory latency and bandwidth, cache size etc. have always inAEuenced the design and implementation of algorithms on high-performance computers. <p> By combining a suOEcient number of high performance microprocessors built from standard technology, we can construct parallel computers with a potential performance comparable to or even higher than that of a traditional serial supercomputer, but at a much lower price <ref> [37, 59] </ref>. This is more true than ever considering the rapid developments in microprocessor architecture and technology. The architectural developments in serial computers mentioned in the previous section have not changed the basic structure or semantics of a program. <p> manufactured and sold today Thinking Machines Corporation went out of business in 1994. 10 MIMD machines ooeer the largest AEexibility of the machines in Flynn's taxonomy and has emerged in the ending of the 1980s and the beginning of he 1990s as the architecture of choice for general-purpose parallel computers <ref> [37, 47, 11] </ref>. There are several reason for that: 1. MIMDs can be build from ooe-the-shelf technology found in ordinary workstations which gives them a good cost/performance ratio. <p> To obtain such a knowledge a combination of theoretical predictions built on a model of the parallel architecture and an understanding of the algorithm is combined with experiments designed to determine the value of the various machine parameters; cf. [59], [74, Chapter 1] and <ref> [37, Chapter 3] </ref>. <p> Usually a simple model of the form t com = ff + fin is used for the communication, where ff is the communication latency, fi is the reciprocal of the bandwidth and n is the number of bytes, say, sent or received, cf. <ref> [37] </ref>. The values of ff and fi generally depend on the location of the data being moved. <p> q) fi (r; s); p = i mod P; q = j mod Q ; r = i Q ; (2) describes how the element (i; j) in A is mapped to element (r; s) on the processor in position (p; q) in the processor grid, see [74, Capter 4], <ref> [37, Chapter 20] </ref>. This type of mapping could be easily expressed in, e.g., High Performance Fortran or by using the PBLAS subroutines [17]. <p> These matrix factorizations, which are some of the standard ibuilding blocksj in numerical linear algebra, can be implemented eOEciently on practically all of todays supercomputers using a partitioned block version of the (classical) serial algorithm; see <ref> [15, 74, 30, 37] </ref>. Direct methods are very well suited for solving small to medium size problems.
Reference: [38] <author> K. A. Gallivan et al., </author> <title> Parallel Algorithms for Matrix Computations, </title> <publisher> Siam, </publisher> <address> Philadelphia, </address> <year> 1990. </year>
Reference-contexts: The architectural parameters that determine the performance for a particular vector length include cycle time, the number of stages in the pipeline and the cost of preparing the functional units for doing the computation (cf. <ref> [34, 38] </ref>). Secondly, memory accesses should have a very regular structure for the memory systems to be able to keep up with the processor. If the distance separating the elements used in a single vector operation, the so-called stride, is dioeerent from 1 performance may degrade. <p> Therefore the number of data transports between layers in the memory hierarchy, e.g. caused by cache misses, is in many cases the most important performance measure to consider when designing a new algorithm for or when implementing a well known algorithm on a new machine <ref> [38, 9] </ref>. Table 2, which is based on ogure 1.16 in [47] Section 1.7, lists typical values of the key ogures for the memory hierarchy of a large workstation or a small server in 1996. <p> Instead the stride issue mentioned earlier puts certain restrictions on the way memory should be accessed, and bank conAEicts is a crucial factor to consider in algorithms design and implementation; see [47] Section 5.6 and <ref> [38] </ref>. For machines with vector-registers, the reuse of data in the higher levels of the memory hierarchy, i.e. the vector-registers, does play a role and was the motivation for the development of the level 2 BLAS; see [38]. <p> factor to consider in algorithms design and implementation; see [47] Section 5.6 and <ref> [38] </ref>. For machines with vector-registers, the reuse of data in the higher levels of the memory hierarchy, i.e. the vector-registers, does play a role and was the motivation for the development of the level 2 BLAS; see [38]. In contrast to the introduction of pipelined vector-processors, which where mainly designed to speed up large scientioc applications performing linear algebra operations on large datasets, the development of memory hierarchies has been largely technology driven motivated by the high cost of fast memory. <p> Obviously t mem may be rather complex depending on whether memory is distributed and depending on the cache coherence protocol used. Hence the important quantity characterizing a given algorithm is the cache-miss rate that it gives rise to <ref> [38] </ref>. Often the cache misses is divided into those occuring as a result of cache coherency checks, and iordinaryj cache misses [47]. 2.3 Numerical Algorithms on Parallel Computers In the following we shall discuss some of the problems that arise in connection with implementing numerical algorithms on parallel computers. <p> In contrast to this most direct methods can be formulated in terms of level 3 BLAS (matrix-matrix) operations where the amount of reuse of data in, e.g., the cache is much higher <ref> [38] </ref>. For many large-scale problems standard implementations of these algorithms found in software libraries such as LAPACK (see [2]), may not be applicable because they require the entire matrix to be stored explicitly in the computer.
Reference: [39] <author> G. H. Golub & C. F. Van Loan, </author> <title> Matrix Computations, 2. </title> <editor> Ed., </editor> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference-contexts: A good example is matrix multiplication, which is blocked by transforming the computation into a version that operates on submatrices; see <ref> [39] </ref>, Section 1.4.9. The success of this approach is explained by Table 3: In an unblocked (point) matrix multiply C AB + C the inner-most loop will consist of a SAXPY operation updating a column of C. <p> Develop completely new algorithms with better parallel performance and the same numerical stability as the standard serial algorithms. Few examples exist. One could mention Cuppens divide-and-conquer for the symmetric eigenvalue problem as a possi ble example, cf. <ref> [39, Section 8.6] </ref>, [30, Section 6.3.3]. 3. Use an algorithm with less favorable convergence properties but better parallel performance. It it often the case that ioldj algorithms have come to life because they have a simpler structure than a more sophisticated istate-of-the-artj serial algorithm and therefore are easier to parallelize. <p> It it often the case that ioldj algorithms have come to life because they have a simpler structure than a more sophisticated istate-of-the-artj serial algorithm and therefore are easier to parallelize. Examples: Parallel bisection based on Sturm sequences for the symmetric eigenvalue problem (cf. <ref> [39, Chapter 8] </ref>, [30]) which is not as eOEcient as the QR algorithm on a serial computer. But the method based on Sturm sequences is embarrassingly parallel each processor can simply compute a separate part of the eigenvalue spectrum and the corresponding eigenvectors using inverse iteration. <p> When moving to a true block version of the LU factorization in order to gain some speed, we no longer have an unconditionally stable method. Instead we could use it in the setting indicated by principle number 4 above, and e.g. apply iterative reonement (cf. <ref> [39, Section 3.5.3] </ref> to the approximate solution, see [7]. A true block algorithm can be derived from the partitioning A 21 A 22 = I 0 0 ~ A 22 ; (4) which leads to the following: 1. U 11 A 11 , U 12 A 12 2. <p> A common approach is to initially transforming the problem to the standard form min fl fl A 0 fl fl : (10) For details on how to compute the reduction to standard form see, e.g., [36, 44]. 3.2.2 SVD-based methods The singular value decomposition (SVD, cf. <ref> [39] </ref>) is another very valuable tool for analyzing and solving ill-conditioned problems; see, e.g., [20]. <p> Solve min fl fl B 0 fl fl : (21) This can be solved for by eliminating the 's under the main diagonal using 2n Givens rotations. 4. Compute the solution x = V : For further computational details see [36] and <ref> [39] </ref>, Section 5.4.3. This algorithm has the advantage, that the most expensive step in the algorithm, the bidiagonalization of A which requires 2mn 2 + 2n 3 AEoating point operations, is independent of the value of . <p> Normal equations: 1. Compute the lower triangular portion of Q = (A T A + 2 I). 2. d = A T b 3. Compute the Cholesky factorization Q = CC T . 4. Solve Cy = d and C T x = y. For further details see <ref> [39] </ref>, Section 5.3. <p> For a description of the Lanczos algorithm its many applications and the connection to the conjugate gradient method see, e.g., <ref> [39] </ref>, Chapter 9-10. An excellent introduction to the conjugate gradient method is given in [68]. <p> Lanczos algorithm is used for solving the eigenvalue problem Ax = x, especially when only a few of the largest or smallest eigenvalues are needed, but variants of the algorithm can also be used for computing the SVD and solving linear equations or least squares problems; see, e.g., [29] and <ref> [39] </ref>, Section 9.3. The most important variant for solving discrete ill-posed problems is the Lanczos bidiagonalization. <p> 1 ; u 2 ; : : : ; u k+1 ); V k = (v 1 ; v 2 ; : : : ; v k ); we obtain a (k + 1) fi k bidiagonal matrix B k satisfying A = U k B k V T See <ref> [39, Chapter 9] </ref>. <p> iterations are performed, the cost of full reothogonalization is approximately k 2 (m + n) AEoating point operations and moreover all Lanczos vectors have to be stored which makes the method highly impractical for large problems, if many iterations are required to obtain a satisfactory solution; see the summary in <ref> [39] </ref>, Section 9.2. A dioeerent approach is to use the Lanczos algorithm without re-orthogonalization and subsequently use a test to identify the spurious singular values. On such test, which we have implemented and used in [9] is described in [29], Section 4.5. <p> Finally it should be mentioned, that the IR 1 IR 1 approach suggested in [66] could perhaps serve as a preconditioner (cf. <ref> [39, Section 10.3] </ref>) in the iterative method suggested above. The IR 1 IR 1 is based on the observation that the term f (2) (2) lm ( q ) in Equation (30) plays a minor role compared to the term f (1) (1) lm ( q ).
Reference: [40] <author> R. C. Gonzales & P. Wintz, </author> <title> Digital Image Processing, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mas-sachusetts, </address> <year> 1977. </year>
Reference-contexts: the images as vectors f; g 2 IR n 2 , by "stacking" the images row by row say, we can write the problem in matrix notation g = Kf ; (32) where the "blur-matrix" K is n 2 fi n 2 . and block circulant with circulant blocks, see <ref> [40] </ref>, Chapter 5. 2 The author would like to thank Mr. J. Nagy for kindly supplying these data. 36 0 50 100 150 200 250 0 0.4 0.8 x 10 -3 50 150 250 50 150 250 0.2 0.6 1 A surface plot illustrating the whole point spread function. <p> This type of problems can often be handled analytically; in this case by means of the 2-dimensional Fourier Transform. In fact we can in theory deconvolve (the term inverse oltering is also used) f eOEciently using the 2d-FFT algorithm; see, e.g., <ref> [40, 74] </ref>. <p> This is very similar to applying Wiener oltering, see <ref> [40, Equation (5.5-10)] </ref>. In [43] elements in FFT2 (H) with modulus smaller than o are simply set equal to 1. Using this preconditioner the iteration converges to the regularized solution in 2 iterations.
Reference: [41] <author> J. L. Gustavson, </author> <title> Reevaluating Amdahl's law, </title> <journal> Comm. of the ACM, </journal> <volume> 31 (1988), </volume> <pages> 532533. </pages>
Reference-contexts: do we know which algorithm that is?) A second problem is whether the speedup is measured for a oxed problem size, absolute speedup or whether it is the speedup achieved when one scales the problem such that the memory of the computer is used at all times, memory-scaled speedup; cf. <ref> [41] </ref>. This lack of consistency calls for a lot of care when comparing dioeerent presentations of parallel algorithms, cf. [5]. Furthermore it is questionable whether speedup makes sense for all architectures.
Reference: [42] <author> A. Greenbaum & Z. Strakos, </author> <title> Predicting the behavior of onite precision Lanczos and conjugate gradientcomputations, </title> <journal> Siam J. Matrix Anal. Appl., </journal> <volume> Vol. 13(1), 121137, </volume> <year> 1992. </year>
Reference-contexts: Some of these problems can be icuredj (as I will brieAEy discuss below), and since the Lanczos process and equivalently the conjugate gradient method do have very attractive convergence properties their behavior in onite precision has been studied intensely: see e.g. <ref> [28, 42, 29, 13] </ref>. The eoeects of onite precision arithmetic in the Lanczos process is of most concern when it is used for computing singular values (or eigenvalues).
Reference: [43] <author> M. Hanke & J. G. Nagy, </author> <title> Restoration of Atmospherically Blurred Images by Symmetric Indeonite Conjugate Gradient Techniques, Dep. Math. </title> <type> Tech. Rep. 95-3, </type> <institution> Southern Methodist University, </institution> <note> To appear in Inverse Problems sometime in 1996. </note>
Reference-contexts: requires only 2N z AEoating point operations, where N z is the number of non-zeroes in A; if A is an m fi m Toeplitz matrix computing y Ax requires 15 p log 2 p AEoating point operations, where p is the smallest power of 2 larger than m; see <ref> [61, 43] </ref>. <p> In this situation f looks like a ffi-function and hence the recorded image g is a good approximation to K. To illustrate this application a small example is included. This is a model problem used in <ref> [43] </ref> 2 in which regularizing CG-iterations are used to restore the image of a satellite taken from a ground based telescope, see Figure 4.2.1. The blurring is applied using a simulated point-spread function which is illustrated in ogure 4.2.1. <p> This observation makes it obvious to consider the use of an iterative regularization method. Since the matrix K is not necessarily positive deonite the normal conjugate gradient algorithm cannot be applied, 37 but instead a minimum residual Krylov subspace method called MR-II is used see <ref> [43] </ref>. <p> This is very similar to applying Wiener oltering, see [40, Equation (5.5-10)]. In <ref> [43] </ref> elements in FFT2 (H) with modulus smaller than o are simply set equal to 1. Using this preconditioner the iteration converges to the regularized solution in 2 iterations.
Reference: [44] <author> M. Hanke & P. C. Hansen, </author> <title> Regularization methods for large-scale problems, </title> <journal> Surv. Math. </journal> <note> Ind. 3 (1993), 253315. </note>
Reference-contexts: Instead additional information about the solution, e.g. that it is smooth, must be incorporated in order to stabilize the problem such that a single useful solution can be computed. This is called regularization (cf. <ref> [44] </ref>). A large number of regularization methods for solving discrete ill-posed problems exist. Methods such as Tikhonov regularization, truncated SVD and mollioer methods such as the method of optimally localized averages, are well-studied and widely used to solve a variety of inverse problems. Below the aforementioned methods will be described. <p> Below the aforementioned methods will be described. It will be discussed what numerical algorithms are used to implement them and how suited they are for implementation on modern high-performance computers. 3.2 Regularization methods 3.2.1 Tikhonov regularization In Tikhonov regularization (cf. <ref> [44] </ref>), also known as regularized least squares, information about the solution is incorporated into the problem by replacing the problem in Equation (7) with min kAx bk 2 2 + 2 kLxk 2 2 ; (8) or equivalently min fl fl A 0 fl fl : (9) The term kLxk measures <p> The parameter is called the regularization parameter and it controls the amount of regularization applied to the solution by determining the relative weighting of the two terms in Equation (8). Choosing a good value for is a very important in itsself, see <ref> [44] </ref>. There are many ways to implement the Tikhonov method. <p> There are many ways to implement the Tikhonov method. A common approach is to initially transforming the problem to the standard form min fl fl A 0 fl fl : (10) For details on how to compute the reduction to standard form see, e.g., <ref> [36, 44] </ref>. 3.2.2 SVD-based methods The singular value decomposition (SVD, cf. [39]) is another very valuable tool for analyzing and solving ill-conditioned problems; see, e.g., [20]. <p> noise-free case the coeOEcients u T i b decay at least as fast as the singular values, but in the presence of noise or rounding errors b, the coeOEcients u T i b do not decay to zero, but levels 20 of at some level * determined by the noise <ref> [44] </ref>. This means that the terms corresponding to large i are magnioed by the division with the small oe i , which usually result in meaningless, wildly oscillating solutions dominated by noise. <p> It is possible to characterize many dioeerent regularization methods in terms of their olter factors. For example the Tikhonov method in standard form corresponds to the olter factors f i = oe 2 (cf. <ref> [44] </ref>). 3.2.3 Mollioer methods The mollioer methods are based on the idea, that given the problem deoned by Equation (5) we can represent the solution in a given point x 0 by a linear combination of the measurements f est (x 0 ) = i=1 If f exact denotes the exact <p> This means that if is changed the new solution can be computed cheaply, which is essential when a parameter-choice method is used to ond the optimal value of , see <ref> [54, 44, 36] </ref>. Normal equations: 1. Compute the lower triangular portion of Q = (A T A + 2 I). 2. d = A T b 3. Compute the Cholesky factorization Q = CC T . 4. Solve Cy = d and C T x = y. <p> using FFT-based matrix-vector multiplication The inverse problem deoned by equation (31) is just one example of a deconvolution problems which arise in many experiments where some measured value relates to the true value of the observable by a convolution with a function H characteristic of the instrument (s) used, see <ref> [44, 27] </ref>. This type of problems can often be handled analytically; in this case by means of the 2-dimensional Fourier Transform. In fact we can in theory deconvolve (the term inverse oltering is also used) f eOEciently using the 2d-FFT algorithm; see, e.g., [40, 74].
Reference: [45] <author> P. C. Hansen, </author> <title> Experience with Regularizing CG Iterations, </title> <type> Report UNIC-94-05, </type> <institution> UNI*C, </institution> <year> 1994. </year>
Reference-contexts: To what extent the loss of orthogonality among the v i 's changes the regularizing eoeect is not well understood. Attempts to analyze this, partly by looking at the olter factors of LSQR, are presented in, e.g., [76] and <ref> [45] </ref> but more research is needed in this area. Finally I should mention that several dioeerent algorithms equivalent to LSQR exist.
Reference: [46] <author> P. C. Hansen, </author> <title> The Backus-Gilbert method: SVD analysis and fast implementation, Inverse Problems 10 (1994), </title> <type> 895904. </type>
Reference: [47] <author> J. L. Hennessy & D. A. Patterson, </author> <title> Computer Architecture: A Quantitative Approach, 2. </title> <editor> Ed., </editor> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, California, </address> <year> 1996. </year>
Reference-contexts: These operations form a very regular sequence of AEoating point operations. This regularity has been exploited in what has until recently been the dominant technique for constructing computers capable of executing 4 millions of AEoating point instructions per second: vector processors; see, e.g. <ref> [47] </ref>, Appendix B. Vector processors provide high-level operations that operate on entire vectors, i.e. arrays of AEoating point numbers, doing, e.g., componentwise addition or multiplication of two vectors. Moreover most high-end vector processors contains multiple functional units allowing several vector instructions to proceed in parallel. <p> Quite often it is the ability of the compiler to vectorize the code that decides how eOEcient the vector pipeline is used; see, e.g., the comparison in <ref> [47] </ref>, Section B.4. Maybe the most obvious example of the inAEuence from vector-processors on numerical software is seen in the development of the BLAS, which provides a set of computational primitives, that relate closely to the hardware primitives provided by vector processors. <p> By exploiting the temporal and spatial locality exhibited by many programs and their access to data, it is possible to reduce the average cost of a memory access to just a few cycles (cf., e.g., <ref> [47] </ref>, Figure 5.49). This introduces a new factor to consider in the analysis of algorithm complexity, namely the cost of moving data between the dioeerent levels of the memory hierarchy. Thus for a hierarchical memory to provide any gains in computational performance a large amount of data reuse is crucial. <p> Table 2, which is based on ogure 1.16 in <ref> [47] </ref> Section 1.7, lists typical values of the key ogures for the memory hierarchy of a large workstation or a small server in 1996. As mentioned above, large serial supercomputers using vector-processors, such as the vector-processors from CRAY, usually do not have a cache based memory hierarchy. <p> As mentioned above, large serial supercomputers using vector-processors, such as the vector-processors from CRAY, usually do not have a cache based memory hierarchy. Instead they use fast and very expensive SRAM for main memory (cf. <ref> [47] </ref>, Appendix B). This allows vector-processors to perform well even with programs that work on very large data-sets and where only a small amount of data reuse is possible. <p> Instead the stride issue mentioned earlier puts certain restrictions on the way memory should be accessed, and bank conAEicts is a crucial factor to consider in algorithms design and implementation; see <ref> [47] </ref> Section 5.6 and [38]. For machines with vector-registers, the reuse of data in the higher levels of the memory hierarchy, i.e. the vector-registers, does play a role and was the motivation for the development of the level 2 BLAS; see [38]. <p> is very eOEcient on a vector-processor, because it ensures that operations are performed on long vectors (Moreover the operation y i = ffx i + y i can be performed in one cycle using the technique called chaining, and it ensures that bank conAEicts are unlikely to occur; see, e.g., <ref> [47] </ref> p. B-24). On a machine with a memory hierarchy this approach is rather ineOEcient due to the low ratio of AEoating point operations to the number of memory references of the SAXPY operation as shown in Table 3. <p> For a description of these techniques see <ref> [47] </ref>, Chapters 3, 4 and 5. For a description of specioc processors see, e.g., [51] and [69]. <p> This is the primary reason why software for parallel supercomputers has not yet reached the same level of maturity that we have seen for e.g. serial supercomputers <ref> [59, 47, 74] </ref>. Having said this it is also true that recently certain standards are beginning to emerge in dioeerent areas of scientioc computing. <p> manufactured and sold today Thinking Machines Corporation went out of business in 1994. 10 MIMD machines ooeer the largest AEexibility of the machines in Flynn's taxonomy and has emerged in the ending of the 1980s and the beginning of he 1990s as the architecture of choice for general-purpose parallel computers <ref> [37, 47, 11] </ref>. There are several reason for that: 1. MIMDs can be build from ooe-the-shelf technology found in ordinary workstations which gives them a good cost/performance ratio. <p> By introducing a cache there is a need for a cache consistency mechanism; in bus based system it is common to use snooping caches implementing a write-once or write-invalidate protocol <ref> [47, Section 8.3] </ref>. Examples of this architecture are the SGI PowerChallenge (bus-based) [69] and the Convex Exemplar (single hypernode, based on cross-bar switch) [26]. Distributed physical memory, single address space: Known as NUMA, non-uniform memory access, or DSM, distributed shared memory machines. <p> More diOEcult to program than the above because of the non-uniform memory access time makes it more diOEcult to model the behavior of the program. Scales to a large number of processors. Memory consistency is usually based on a directory based protocol <ref> [47, Section 8.4] </ref>. Examples: Kendall Square Research KSR-1, Convex Exemplar (multiple hypernodes really a hybrid) and Cray T3D (provides no cache consistency, shared data are simply not cached). <p> This suggests that algorithms should be analyzed in terms of the average vector length attainable and also stride plays a major role, cf. [34], <ref> [47, Section B.6] </ref>. <p> Hence the important quantity characterizing a given algorithm is the cache-miss rate that it gives rise to [38]. Often the cache misses is divided into those occuring as a result of cache coherency checks, and iordinaryj cache misses <ref> [47] </ref>. 2.3 Numerical Algorithms on Parallel Computers In the following we shall discuss some of the problems that arise in connection with implementing numerical algorithms on parallel computers. First we will discuss the important trade-ooe between numerical stability, parallel eOEciency and speed of iconvergencej / serial complexity.
Reference: [48] <author> J. W. Harvey et al., </author> <title> TheGlobal Oscillation Network Group (GONG) Project, </title> <booktitle> Science, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Modes with higher frequencies are reAEected before they reach the core and carry no information about this part of the Sun. For a description of the GONG project and the techniques used for estimating the Solar eigenspectral parameters see <ref> [48, 49] </ref>. Another project with the aim of providing helioseismic data is the SOHO satellite project. The SOHO satellite carries 12 instruments, of which 3 are dedicated to helioseismic investigations. <p> The modes observed by the GONG and the SOHO projects have degrees l in the range from 0 to as high as 3500 and radial orders in the range from 1 to more than 40 <ref> [48] </ref>. We can expect to have measurements of the frequencies of in the order of 10 7 dioeerent modes, when data from GONG and SOHO are being combined.
Reference: [49] <author> F. Hill et al., </author> <title> GONG Estimates of Solar Eigenspectral Parameters, </title> <booktitle> Science, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Modes with higher frequencies are reAEected before they reach the core and carry no information about this part of the Sun. For a description of the GONG project and the techniques used for estimating the Solar eigenspectral parameters see <ref> [48, 49] </ref>. Another project with the aim of providing helioseismic data is the SOHO satellite project. The SOHO satellite carries 12 instruments, of which 3 are dedicated to helioseismic investigations.
Reference: [50] <author> W. D. Hillis & G. L. Steele, Jr., </author> <title> Data Parallel Algorithms, </title> <journal> Comm. of the ACM, </journal> <volume> 29 (1986), </volume> <pages> 11701183. </pages>
Reference-contexts: CM 200), where thousands of processors compute synchronously under the control of an external control processor broadcasting instructions to the slaves have lead to very high performances for a number large-scale scientioc applications where this type of data-parallelism has been applicable <ref> [50] </ref>. The massively parallel machines are unfortunately very inAEexible. If a portion of a given algorithm can not be cast in a data-parallel form performance is likely to be far below the peak performance of the machine because the scalar performance is very low. <p> MIMDs ooeer AEexibility since they can function both as single parallel inumber-crunchersj and as a multiprogrammed machines running a workload consisting of a collection of sequential programs. 4. Regarded as a single parallel machine a MIMD architecture also ooeers AEexibility since it supports both data-parallelism and control-parallelism <ref> [50] </ref>. Memory organization and interconnection topology: There are two sides to memory organization: We distinguish between centralized physical memory and distributed physical memory, and between a single address space and multiple address spaces.
Reference: [51] <institution> International Business Machines Corporation, PowerPC and Power2: Technical Aspects of the New IBM RISC System/6000, Austin, Texas, </institution> <year> 1994. </year>
Reference-contexts: For a description of these techniques see [47], Chapters 3, 4 and 5. For a description of specioc processors see, e.g., <ref> [51] </ref> and [69]. Maybe the single most important factor in the large increase in the performance of modern superscalar microprocessers is the developments in compiler technology that have occured in parallel with the development of the superscalar processors.
Reference: [52] <institution> IMSL Math/Library, </institution> <note> Version 2.0, </note> <year> 1991. </year>
Reference-contexts: but for most numerical techniques used extensively in scientioc computing, such as numerical linear algebra, methods for solving partial and ordinary dioeer-ential equations, linear and non-linear optimization, quadrature, fast fourier transforms etc., many software libraries exist implementing the istate of the artj algorithms for a variety of platforms; see, e.g., <ref> [32, 2, 60, 52] </ref>. In the following it will be illustrated how the development in computer architecture has inAEuenced the algorithms used to solve large-scale numerical problem and vice-versa.
Reference: [53] <author> B. Kgstrm & C. Van Loan, </author> <title> GEMM-Based Level-3 BLAS, </title> <type> Tech. rep. </type> <institution> CTC91TR47, Department of Computer Science, Cornell University, </institution> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: The level 3 BLAS is a collection of linear algebra subroutines that perform O (n 3 ) operations on O (n 2 ) data; see <ref> [33, 53] </ref>. The algorithms implemented in the LAPACK library, which is used on most serial and shared memory multiprocessor machines today for computations involving dense matrix, use block-algorithms written in terms of calls to the level 3 BLAS subroutines to achieve good performance.
Reference: [54] <author> R. M. Larsen & P. C. Hansen, </author> <title> EOEcient Implementations of the SOLA Mollioer Method, Accepted for publication in Astronomy & Astrophysics Supplement Series, </title> <month> April </month> <year> 1996. </year>
Reference-contexts: in the SOLA method is, instead of minimizing (17), to minimize the squared dioeerence between the averaging kernel and some chosen target function T (x 0 ; x) that has the desired form, Z 1 [K (x 0 ; x) T (x 0 ; x)] 2 dx ; (18) In <ref> [54] </ref> we have shown how the discretization of Equation (18) can be written as an equality constrained least squares problem where the coeOEcient matrix does not depend on x 0 min fl fl A T 0 fl fl subject to c T q (x 0 ) = 1 ; (19) where <p> Equation (19) can in a numerically stable way, using a single Householder transformation, be reduced to a standard regularized least squares problem in standard form and solved eOEciently using the bidiagonalization algorithm described in the next paragraph, see <ref> [54] </ref> for details. In the paper we also demonstrate that this approach is more eOEcient than the algorithm in the original presentation of SOLA [64] when solutions for many values of the regularization parameter must be computed in order to ond the optimal one. <p> This means that if is changed the new solution can be computed cheaply, which is essential when a parameter-choice method is used to ond the optimal value of , see <ref> [54, 44, 36] </ref>. Normal equations: 1. Compute the lower triangular portion of Q = (A T A + 2 I). 2. d = A T b 3. Compute the Cholesky factorization Q = CC T . 4. Solve Cy = d and C T x = y. <p> The iteration number k plays the role of a regularization parameter, and the computation of x (1) ; x (2) ; : : : corresponds to sweeping through a range of dioeerent regularization parameters; see, e.g., <ref> [54] </ref>. Methods based on the Lanczos algortihm, including the mathematically equivalent conjugate gradient method, have the abovementioned properties and many iterative regularization methods based on the Lanczos algorithm have been suggested (see, e.g., [12, 76]). <p> Doing this we can apply the iterative implementation of the SOLA method based on LSQR which was presented in <ref> [54] </ref>. Preliminary experiments on the Convex Exemplar has shown that this approach is feasible on a parallel machine, although the large amount of level 2 BLAS operations involved prevented the implementation to achieve a substantial fraction of the peak performance of the machine due to the relatively low memory bandwidth. <p> The comparison should include programming methodology, performance modelling, eOEciency of dioeerent algorithms and the eOEciency of the computational primitives in software libraries such as ScaLAPACK. 2. To test the algorithm developed in <ref> [54] </ref>, and described in Section 4.1.3, on large scale problems using the real Solar data from GONG. Work on the SOLA$Tikhonov connection: As mentioned in Section 3.2 it is not quite clear whether the constraint in Equation (16) is necessary in practice or not. <p> Work on the SOLA$Tikhonov connection: As mentioned in Section 3.2 it is not quite clear whether the constraint in Equation (16) is necessary in practice or not. If this can be removed the algorithms in <ref> [54] </ref> could be improved even further. As a orst attempt the above mentioned implementation could be used to test this on a series of numerical examples.
Reference: [55] <author> C. L. Lawson & R. J. Hanson, </author> <title> Solving Least Squares Problems, </title> <publisher> Prentice-Hall, </publisher> <year> 1974. </year>
Reference: [56] <author> C. Lawson, R. Hanson, D. Kinkaid & F. Krogh, </author> <title> Basic Linear Algebra Subprograms for Fortran Usage, </title> <journal> ACM Trans. on Math. Software, </journal> <volume> 5 (1979), </volume> <pages> 308323. </pages>
Reference-contexts: The BLAS has become a de facto standard and every modern supercomputer comes with a set of highly tuned BLAS routines. see <ref> [56, 57] </ref>. Numerical algorithms for serial computers has also reached a high level of sophistication.
Reference: [57] <author> C. Lawson, R. Hanson, D. Kinkaid & F. Krogh, </author> <title> Algorithm 539: Basic Linear Algebra Subprograms for Fortran Usage, </title> <journal> ACM Trans. on Math. Software, </journal> <volume> 5 (1979), </volume> <pages> 324325. </pages>
Reference-contexts: The BLAS has become a de facto standard and every modern supercomputer comes with a set of highly tuned BLAS routines. see <ref> [56, 57] </ref>. Numerical algorithms for serial computers has also reached a high level of sophistication.
Reference: [58] <author> A. K. Louis & P. Maass, </author> <title> A mollioer method for linear operator equations of the orst kind, Inverse Problems 6 (1990), </title> <type> 427440. </type>
Reference-contexts: The so-called subtractive optimally localized averages (SOLA) method, see e.g., <ref> [58, 64] </ref>, is a computationally more eOEcient variant of the BG method.
Reference: [59] <author> P. C . Messina, </author> <title> Parallel computing in the 1980s one person's view, </title> <journal> Concurrency: Pratice and Experience, </journal> <volume> Vol. 3 (6), 501524, </volume> <year> 1991. </year>
Reference-contexts: By combining a suOEcient number of high performance microprocessors built from standard technology, we can construct parallel computers with a potential performance comparable to or even higher than that of a traditional serial supercomputer, but at a much lower price <ref> [37, 59] </ref>. This is more true than ever considering the rapid developments in microprocessor architecture and technology. The architectural developments in serial computers mentioned in the previous section have not changed the basic structure or semantics of a program. <p> This is the primary reason why software for parallel supercomputers has not yet reached the same level of maturity that we have seen for e.g. serial supercomputers <ref> [59, 47, 74] </ref>. Having said this it is also true that recently certain standards are beginning to emerge in dioeerent areas of scientioc computing. <p> To obtain such a knowledge a combination of theoretical predictions built on a model of the parallel architecture and an understanding of the algorithm is combined with experiments designed to determine the value of the various machine parameters; cf. <ref> [59] </ref>, [74, Chapter 1] and [37, Chapter 3]. <p> This 13 measure is useful for scientioc applications because it is a direct measure of how much work is being done and how well the program is making use of the potential computational power of the machine, cf. <ref> [59, Section 7] </ref>. It does not guaranty however, that anything useful is being computed! A number of suggestions have been made as to which machine parameters should be used in characterizing the behavior of algorithms on dioeerent architectures.
Reference: [60] <author> NAG Fortran Library Manual, </author> <title> Mark 15, </title> <publisher> NAG Ltd., Oxford, </publisher> <year> 1991. </year>
Reference-contexts: but for most numerical techniques used extensively in scientioc computing, such as numerical linear algebra, methods for solving partial and ordinary dioeer-ential equations, linear and non-linear optimization, quadrature, fast fourier transforms etc., many software libraries exist implementing the istate of the artj algorithms for a variety of platforms; see, e.g., <ref> [32, 2, 60, 52] </ref>. In the following it will be illustrated how the development in computer architecture has inAEuenced the algorithms used to solve large-scale numerical problem and vice-versa.
Reference: [61] <author> C. C. Paige & M. A. Saunders, </author> <title> LSQR: an algorithm for sparse linear equations and sparse least squares, </title> <journal> ACM Trans. Math. </journal> <note> Software 8 (1982), 4371. </note>
Reference-contexts: requires only 2N z AEoating point operations, where N z is the number of non-zeroes in A; if A is an m fi m Toeplitz matrix computing y Ax requires 15 p log 2 p AEoating point operations, where p is the smallest power of 2 larger than m; see <ref> [61, 43] </ref>. <p> The LSQR algorithm by Page and Saunders implements this using a clever iteration scheme that updates a QR factorization of B k and thereby allows x (k) to be computed from x (k1) without ever storing U k and V k ; see <ref> [61, 62] </ref>. We notice that there are two levels of regularization involved here: An iouterj regularization controlled by the number of iterations k and an iinnerj regularization controlled by (see [12] for further discussion of this aspect). <p> Finally I should mention that several dioeerent algorithms equivalent to LSQR exist. In a recent paper by Bjrck, Elfving and Strakos, [13], it was shown that the variant known as CGLS 1 together with the LSQR algorithm by Paige and Saunders, <ref> [61, 62] </ref>, are the two most stable implementations for solving ill-conditioned problems. 3.3.3 Parallel Implementation Iterative algorithms are in some sense serial by nature, since the computations in the kth iteration usually involve recursions refering to the values of variables in a few of the previous steps, as in, e.g., the
Reference: [62] <author> C. C. Paige & M. A. Saunders, </author> <title> Algorithm 583. LSQR: sparse linear equations and least squares problems, </title> <journal> ACM Trans. Math. </journal> <note> Software 8 (1982), 195209. </note>
Reference-contexts: The LSQR algorithm by Page and Saunders implements this using a clever iteration scheme that updates a QR factorization of B k and thereby allows x (k) to be computed from x (k1) without ever storing U k and V k ; see <ref> [61, 62] </ref>. We notice that there are two levels of regularization involved here: An iouterj regularization controlled by the number of iterations k and an iinnerj regularization controlled by (see [12] for further discussion of this aspect). <p> Finally I should mention that several dioeerent algorithms equivalent to LSQR exist. In a recent paper by Bjrck, Elfving and Strakos, [13], it was shown that the variant known as CGLS 1 together with the LSQR algorithm by Paige and Saunders, <ref> [61, 62] </ref>, are the two most stable implementations for solving ill-conditioned problems. 3.3.3 Parallel Implementation Iterative algorithms are in some sense serial by nature, since the computations in the kth iteration usually involve recursions refering to the values of variables in a few of the previous steps, as in, e.g., the
Reference: [63] <author> R. L. Parker, </author> <title> Understanding inverse theory, </title> <journal> Ann. Rev. Earth Planet. Sci. </journal> <volume> 5 (1977), </volume> <pages> 3564. </pages>
Reference: [64] <author> F. P. Pijpers & M. J. Thompson, </author> <title> Faster formulation of the optimally localized averages method for helioseismic inversions, </title> <institution> Astr. Astrophys. </institution> <month> 262 </month> <year> (1992), </year> <month> L33L36. </month>
Reference-contexts: The so-called subtractive optimally localized averages (SOLA) method, see e.g., <ref> [58, 64] </ref>, is a computationally more eOEcient variant of the BG method. <p> In the paper we also demonstrate that this approach is more eOEcient than the algorithm in the original presentation of SOLA <ref> [64] </ref> when solutions for many values of the regularization parameter must be computed in order to ond the optimal one.
Reference: [65] <author> F. P. Pijpers & M. J. Thompson, </author> <title> The SOLA method for helioseismic inversion, </title> <journal> Astr. Astrophys. </journal> <volume> 281 (1994), </volume> <pages> 231240. </pages>
Reference: [66] <author> F. P. Pijpers & M. J. Thompson, </author> <title> A modioed IR 1 IR 1 method for helioseismic rotation inversions, Mon. Not. </title> <institution> R. Astron. Soc., </institution> <year> 1994. </year>
Reference-contexts: Finally it should be mentioned, that the IR 1 IR 1 approach suggested in <ref> [66] </ref> could perhaps serve as a preconditioner (cf. [39, Section 10.3]) in the iterative method suggested above.
Reference: [67] <author> J. Schou, J. Christensen-Dalsgaard, & M. J. Thompson, </author> <title> On Comparing Helioseismic 2-dimensional Inversion Methods, Astrophys. </title> <journal> J., </journal> <volume> 433 (1994), </volume> <pages> 389. </pages>
Reference-contexts: The discretization described here is not used in practice, but is chosen for ease of presentation. For a more elaborate description see <ref> [67] </ref>. 4.1.3 Computational aspects It turns out that the matrix in Equation (29) is generally very ill-conditioned and hence to solve the inverse problem we must apply some sort of regularization. In Figure 4.1.3 is a plot of the singular values for 3 dioeerent discretizations. <p> The dependency on the number of points in latitude was examined in [21]. To solve Equation (29) we can (in principle) apply any of the methods described in Section 3.2. Many investigations have been made into the properties of solutions produced by dioeerent regularization methods; see, e.g., <ref> [22, 20, 67] </ref>. The two most popular methods are 1. Regularized least squares (RLS) with second derivative smoothing. 2. Optimally localized averages (In the SOLA variant for reasons of computational eOE ciency). <p> In helioseismology the mollioer methods like SOLA are very attractive because they allow us to control the shape of the averaging kernel explicitly. As shown in, e.g. <ref> [67, 21] </ref> the averaging kernels produced by the RLS procedure often have undesirable structure away from (r 0 ; 0 ). This is especially problematic if these spurious non-local contributions to (r 0 ; 0 ), e.g. come from areas where the physical properties are not well understood; see [22]. <p> Up until now the RLS method has been used because it is possible to set up the normal equations, that require orders of magnitude less storage, without explicitly forming A; see <ref> [67, Appendix B] </ref>. In [67, Appendix B] we also ond the basis for a way of implementing the SOLA procedure without storing the matrix A explicitly. <p> Up until now the RLS method has been used because it is possible to set up the normal equations, that require orders of magnitude less storage, without explicitly forming A; see <ref> [67, Appendix B] </ref>. In [67, Appendix B] we also ond the basis for a way of implementing the SOLA procedure without storing the matrix A explicitly. <p> We observe, that the storage required for storing F 1 ; F 2 ; G 1 and G 2 is negligible compared to the size of A. For example if we calculate the storage required for the data used in <ref> [67, Section 5] </ref>. This is an artiocial dataset which resembles what can be expected from the GONG network.
Reference: [68] <author> J. R. Shewchuk, </author> <title> An Introduction to the Conjugate Gradient Method Without the Agonizing Pain, </title> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1994. </year>
Reference-contexts: For a description of the Lanczos algorithm its many applications and the connection to the conjugate gradient method see, e.g., [39], Chapter 9-10. An excellent introduction to the conjugate gradient method is given in <ref> [68] </ref>.
Reference: [69] <institution> Silicon Graphics Computer Systems, The advent of Powercomputing, Silicon Graphics Technical Report, California, </institution> <year> 1994. </year> <month> 44 </month>
Reference-contexts: For a description of these techniques see [47], Chapters 3, 4 and 5. For a description of specioc processors see, e.g., [51] and <ref> [69] </ref>. Maybe the single most important factor in the large increase in the performance of modern superscalar microprocessers is the developments in compiler technology that have occured in parallel with the development of the superscalar processors. <p> By introducing a cache there is a need for a cache consistency mechanism; in bus based system it is common to use snooping caches implementing a write-once or write-invalidate protocol [47, Section 8.3]. Examples of this architecture are the SGI PowerChallenge (bus-based) <ref> [69] </ref> and the Convex Exemplar (single hypernode, based on cross-bar switch) [26]. Distributed physical memory, single address space: Known as NUMA, non-uniform memory access, or DSM, distributed shared memory machines.
Reference: [70] <author> A. A. Stephanov, </author> <title> Inverse problem of the rotional splitting, </title> <institution> Institute of Physics and As--tronomy, University of Aarhus, </institution> <year> 1995. </year>
Reference-contexts: Once the Tikhonov solution is known, solutions corresponding to dioeerent shapes of the target function can be computed in O (qn) operations. This relation was derived in the continuous setting in <ref> [70] </ref>. Whether it is possible to replace the constraint in Equation (18) by the simple requirement that R 1 0 T (x 0 ; x) dx = 1 and how this will aoeect the solutions is yet uncertain.
Reference: [71] <author> E. de Sturler & H. A. van der Vorst, </author> <title> Reducing the eoeect of global communication in GMRES(m) and CG on parallel distributed memory computers, </title> <journal> Appl. Num. Math. </journal> <volume> 18 (1995), </volume> <pages> 441459. </pages>
Reference-contexts: If, say, the vectors are distributed over the processors the updates can simply be done locally, but the global reduction involved in computing the inner products and broadcasting the result involves communication. It has been demonstrated in a number of publications (see, e.g., <ref> [71, 3, 74] </ref> and [30], Section 8) how to reorder the operations performed in the algorithms such that the communication can be overlapped with, e.g., the local vector updates without changing the numerical properties of the algorithm. <p> The main computational eoeort usually lies in computing the matrix-vector products with A and A T . How these operations should be implemented in parallel depends entirely on the properties of matrix A. See, e.g. <ref> [75, 9, 30, 71, 74] </ref>. When implementing the LSQR algorithm on a parallel computer, it is necessary to compute matrix-vector multiplications with both A and A T .
Reference: [72] <author> A. S. Tannenbaum, </author> <title> Structured Computer Organization, Third ed., </title> <publisher> Prentice-Hall, </publisher> <year> 1990. </year>
Reference-contexts: Below we will outline a few of the basic issues in connection with parallel architectures starting with Floyds taxonomy <ref> [72, Chapter 8] </ref>. Nature of parallelism: The taxonomy proposed by Floyd is often used to characterize par allel architectures according to the number if instruction streams and data streams. 1. Single instruction stream, Single data stream (SISD): A serial processor, e.g. a workstation. 2.
Reference: [73] <author> M. J. Thompson et al., </author> <title> Dioeerential Rotation and Dynamics of the Solar Interior, </title> <booktitle> Science, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: This problem leads to very large computational problems. We start by giving a very simplioed illustration of how the frequencies of the solar oscillations can be used to infer the solar rotation rate. For a more detailed survey of the physics of solar rotation and inversion techniques see <ref> [73] </ref>.
Reference: [74] <author> E. F. Van de Velde, </author> <title> Concurrent Scientioc Computing, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: This is the primary reason why software for parallel supercomputers has not yet reached the same level of maturity that we have seen for e.g. serial supercomputers <ref> [59, 47, 74] </ref>. Having said this it is also true that recently certain standards are beginning to emerge in dioeerent areas of scientioc computing. <p> To obtain such a knowledge a combination of theoretical predictions built on a model of the parallel architecture and an understanding of the algorithm is combined with experiments designed to determine the value of the various machine parameters; cf. [59], <ref> [74, Chapter 1] </ref> and [37, Chapter 3]. <p> ScaLAPACK for both SVD and eigenvalue computations, cf. [16, 10]. Another example is the use of stationary iterative methods (Jacobi, Gauss-Seidel or SOR) for solving sparse systems, e.g. in connection with partial dioeerential equations, cf. <ref> [74, Chapter 8] </ref>. 4. <p> j) 7! (p; q) fi (r; s); p = i mod P; q = j mod Q ; r = i Q ; (2) describes how the element (i; j) in A is mapped to element (r; s) on the processor in position (p; q) in the processor grid, see <ref> [74, Capter 4] </ref>, [37, Chapter 20]. This type of mapping could be easily expressed in, e.g., High Performance Fortran or by using the PBLAS subroutines [17]. <p> a reduction has to take place along the processor column holding the kth column of A to compute the pivot element, and after this the row swapping 16 must take place and the pivot row must be broadcast in order to update the remaining active part of A (see, e.g., <ref> [74, Sections 4.2.2-4.2.3] </ref>. The (point) scattered distribution also makes it diOEcult to use the memory hierarchy eOEciently. All in all this straight forward parallelization is only eoeective if the ration t com =t calc is close to 1. <p> These matrix factorizations, which are some of the standard ibuilding blocksj in numerical linear algebra, can be implemented eOEciently on practically all of todays supercomputers using a partitioned block version of the (classical) serial algorithm; see <ref> [15, 74, 30, 37] </ref>. Direct methods are very well suited for solving small to medium size problems. <p> If, say, the vectors are distributed over the processors the updates can simply be done locally, but the global reduction involved in computing the inner products and broadcasting the result involves communication. It has been demonstrated in a number of publications (see, e.g., <ref> [71, 3, 74] </ref> and [30], Section 8) how to reorder the operations performed in the algorithms such that the communication can be overlapped with, e.g., the local vector updates without changing the numerical properties of the algorithm. <p> The main computational eoeort usually lies in computing the matrix-vector products with A and A T . How these operations should be implemented in parallel depends entirely on the properties of matrix A. See, e.g. <ref> [75, 9, 30, 71, 74] </ref>. When implementing the LSQR algorithm on a parallel computer, it is necessary to compute matrix-vector multiplications with both A and A T . <p> This type of problems can often be handled analytically; in this case by means of the 2-dimensional Fourier Transform. In fact we can in theory deconvolve (the term inverse oltering is also used) f eOEciently using the 2d-FFT algorithm; see, e.g., <ref> [40, 74] </ref>. <p> The conclusion is that som kind of regularization is needed. Equation (33) gives us a way of computing the matrix-vector product with the blur-matrix in O (n 2 log n) operations (cf., e.g., <ref> [74] </ref>) using the FFT algorithm (This should be compared to O (n 4 ) which we get if we naively multiply by K). This observation makes it obvious to consider the use of an iterative regularization method.
Reference: [75] <author> B. Van Rietdergen et al., </author> <title> A new method to determine trabecular bone elastic properties and loading using micromechanical onite-element models, </title> <journal> J. Biomech., </journal> <volume> Vol. 28 1 (1995), </volume> <pages> 6981. </pages>
Reference-contexts: This issue will be discussed further in Section 4.1.3; see also, e.g., <ref> [75] </ref>. <p> The main computational eoeort usually lies in computing the matrix-vector products with A and A T . How these operations should be implemented in parallel depends entirely on the properties of matrix A. See, e.g. <ref> [75, 9, 30, 71, 74] </ref>. When implementing the LSQR algorithm on a parallel computer, it is necessary to compute matrix-vector multiplications with both A and A T .
Reference: [76] <author> C. R. Vogel, </author> <title> Solving Ill-conditioned Linear Systems using the Conjugate Gradient Method </title>
Reference-contexts: Methods based on the Lanczos algortihm, including the mathematically equivalent conjugate gradient method, have the abovementioned properties and many iterative regularization methods based on the Lanczos algorithm have been suggested (see, e.g., <ref> [12, 76] </ref>). For a description of the Lanczos algorithm its many applications and the connection to the conjugate gradient method see, e.g., [39], Chapter 9-10. An excellent introduction to the conjugate gradient method is given in [68]. <p> Alternatively we could set = 0 and only rely on the regularizing eoeect that comes from the fact that the spectral components associated with the large singular values converge faster than the remaining components. This approach is studied in <ref> [76] </ref>. 3.3.2 Lanczos-based methods in onite precision In onite precision this procedure is made more diOEcult due to loss of orthogonality among the Lanczos vectors, i.e. the columns of U k and V k . <p> To what extent the loss of orthogonality among the v i 's changes the regularizing eoeect is not well understood. Attempts to analyze this, partly by looking at the olter factors of LSQR, are presented in, e.g., <ref> [76] </ref> and [45] but more research is needed in this area. Finally I should mention that several dioeerent algorithms equivalent to LSQR exist.
Reference: [77] <author> C. R. Vogel, </author> <title> Optimal choice of a truncation level for the truncated SVD solution of linear orst kind integral equations when the data are noisy, </title> <journal> SIAM J. Numer. Anal. </journal> <volume> 23 (1986), </volume> <pages> 109117 </pages>
Reference: [78] <author> L. Wang & A. Zunger, </author> <title> Large scale electronic structure calculations using the Lanczos method, </title> <booktitle> Computational Material Science 2 (1994), </booktitle> <pages> 326340. </pages>
Reference-contexts: This issue will be discussed further in Section 4.1.3; see also, e.g., [75]. Indirect formulation: If the matrix A represent some continuous operator, it may be possible to compute the result of the operator acting on a vector x without using the matrix representation of A; see, e.g., <ref> [78] </ref>. 25 In the following we will assume that A is large and sparse and/or structured. 3.3.1 Iterative Regularization The issues raised above are not in any way particular to the solution of ill-posed problems, but apply quite generally to applications where iterative algorithms are used for solving problems involving large
Reference: [79] <author> G. M. Wing, </author> <title> A Primer on Integral Equations of the First Kind, </title> <publisher> Siam, </publisher> <address> Philadelphia, </address> <year> 1991. </year> <month> 45 </month>
Reference-contexts: This ill-conditioning is not a result of the discretiza-tion but is a property inherent in the continuous inverse problem, i.e. that Equation (5) does not contain enough information for us to recover the function f ; see <ref> [20, 79] </ref>. This does not mean that the problem can not be solved, but the standard algorithms for solving linear least squares problems are not applicable to these ill-posed problems in their original form.
References-found: 77

