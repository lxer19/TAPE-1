URL: ftp://hpsl.cs.umd.edu/pub/papers/pldi95-gagan.ps.Z
Refering-URL: http://www.cs.umd.edu/projects/hpsl/papers.brandnew/LocalResources/tech-10-23.htm
Root-URL: 
Email: fgagan, saltz,rajag@cs.umd.edu  
Phone: (301)-405-2756  
Title: Interprocedural Partial Redundancy Elimination and Its Application To Distributed Memory Compilation 1 Length of the
Author: Gagan Agrawal Joel Saltz Raja Das 
Address: College Park, MD 20742  College Park, MD 20742  
Affiliation: Department of Computer Science, University of Maryland,  UMIACS and Dept. of Computer Sc., University of Maryland,  
Abstract: Partial Redundancy Elimination (PRE) is a general scheme for suppressing partial redundancies which encompasses traditional optimizations like loop invariant code motion and redundant code elimination. In this paper we address the problem of performing this optimization interprocedurally. We use interprocedural partial redundancy elimination for placement of communication and communication preprocessing statements while compiling for distributed memory parallel machines.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Gagan Agrawal, Alan Sussman, and Joel Saltz. </author> <title> An integrated runtime and compile-time approach for parallelizing structured and block structured applications. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <note> 1994. To appear. Also available as University of Maryland Technical Report CS-TR-3143 and UMIACS-TR-93-94. </note>
Reference-contexts: Compiler analysis has been developed for analyzing the data access patterns associated with a given parallel loop and inserting calls to appropriate communication preprocessing routines and collective communication routines <ref> [1, 9] </ref>. 2 Real X (m), Y (m) ! data arrays Integer IA (n) ! indirection array forall i = 1, n X (i) = X (i) + Y (IA (i)) C Build the required schedule DS = Sched (.. parameters ..) C Communicate data using the schedule build above Call
Reference: [2] <author> Michael Burke. </author> <title> An interval-based approach to exhaustive and incremental interprocedural data-flow analysis. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12(3) </volume> <pages> 341-395, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: FIAT has been introduced as a general framework for performing interprocedural analysis [13], but is more targeted towards flow-insensitive problems. Interval based approach for solving interprocedural data flow equations has been investigated in <ref> [2] </ref>. Recompilation in a compiler performing interprocedural analysis has been investigated in [3]. Program Foo Procedure P (x,y) a = 1 ..other computations .. Sched (a,b) End Do i = 1, 100 Call P (a,b) Call Q (c) Procedure Q (z) Enddo z = ...z...
Reference: [3] <author> Michael Burke and Linda Torczon. </author> <title> Interprocedural optimization: Eliminating unnecessary recompilation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 15(3) </volume> <pages> 367-399, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: FIAT has been introduced as a general framework for performing interprocedural analysis [13], but is more targeted towards flow-insensitive problems. Interval based approach for solving interprocedural data flow equations has been investigated in [2]. Recompilation in a compiler performing interprocedural analysis has been investigated in <ref> [3] </ref>. Program Foo Procedure P (x,y) a = 1 ..other computations .. Sched (a,b) End Do i = 1, 100 Call P (a,b) Call Q (c) Procedure Q (z) Enddo z = ...z...
Reference: [4] <author> D. Callahan. </author> <title> The program summary graph and flow-sensitive interprocedural data flow analysis. </title> <booktitle> In Proceedings of the SIGPLAN '88 Conference on Program Language Design and Implementation, </booktitle> <address> Atlanta, GA, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: The total number of nodes in SuperGraph can get very large and consequently the solution may take much longer time to converge. Several ideas in the design of our representation are similar to the ideas used in Callahan's Program Summary Graph <ref> [4] </ref> and Interprocedural Flow Graph used by Soffa et al. [16]. FIAT has been introduced as a general framework for performing interprocedural analysis [13], but is more targeted towards flow-insensitive problems. Interval based approach for solving interprocedural data flow equations has been investigated in [2].
Reference: [5] <author> K. Cooper and K. Kennedy. </author> <title> Interprocedural side-effect analysis in linear time. </title> <booktitle> In Proceedings of the SIGPLAN '88 Conference on Program Language Design and Implementation, </booktitle> <address> Atlanta, GA, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: In absence of aliasing, this information can easily be computed by flow-insensitive interprocedural analysis in time linear to the size of call graph of the program <ref> [5] </ref>. This information is used by the CMOD cs function defined later. 2 This is different from intraprocedural PRE in which placement is considered at beginning and end of node (basic block) of the graph. 6 4.2 Candidates for Placement We consider only the placement of pure functions.
Reference: [6] <author> K. Cooper, K. Kennedy, and L. Torczon. </author> <title> The impact of interprocedural analysis and optimization in the rn programming environment. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(4) </volume> <pages> 491-523, </pages> <month> October </month> <year> 1986. </year>
Reference: [7] <author> R. Das, D. J. Mavriplis, J. Saltz, S. Gupta, and R. Ponnusamy. </author> <title> The design and implementation of a parallel unstructured Euler solver using software primitives. </title> <journal> AIAA Journal, </journal> <volume> 32(3) </volume> <pages> 489-496, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: After such an initial analysis at a single parallel loop or a single procedure level, placement of these statements must be optimized interprocedurally. Large scientific and engineering applications often present opportunities for reusing communication schedule several times <ref> [7, 8] </ref> and it is important to do this optimization to obtain reasonable performance. We, therefore, identify two optimization problems, communication schedule generation placement and communication placement. Partial redundancy elimination can be applied interprocedurally for solving both these optimization problems. <p> We studied the effectiveness of our scheme in compiling an Euler solver for unstructured grids <ref> [7] </ref>, a code 13 Edge AVIN AVOUT PAVIN PAVOUT PPOUT PPIN DEL INSERT 1 ? ? ? ? &lt; a; b &gt; ? ? &lt; a; b &gt; 3 ? ? ? ? &lt; a; c &gt; ? ? &lt; a; c &gt; 5 &lt; a; b &gt; &lt; a; b
Reference: [8] <author> R. Das and J. Saltz. </author> <title> Parallelizing molecular dynamics codes using the Parti software primitives. </title> <booktitle> In Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 187-192. </pages> <publisher> SIAM, </publisher> <month> March </month> <year> 1993. </year>
Reference-contexts: After such an initial analysis at a single parallel loop or a single procedure level, placement of these statements must be optimized interprocedurally. Large scientific and engineering applications often present opportunities for reusing communication schedule several times <ref> [7, 8] </ref> and it is important to do this optimization to obtain reasonable performance. We, therefore, identify two optimization problems, communication schedule generation placement and communication placement. Partial redundancy elimination can be applied interprocedurally for solving both these optimization problems.
Reference: [9] <author> Raja Das, Joel Saltz, and Reinhard von Hanxleden. </author> <title> Slicing analysis and indirect access to distributed arrays. </title> <booktitle> In Proceedings of the 6th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 152-168. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1993. </year> <note> Also available as University of Maryland Technical Report CS-TR-3076 and UMIACS-TR-93-42. </note>
Reference-contexts: Compiler analysis has been developed for analyzing the data access patterns associated with a given parallel loop and inserting calls to appropriate communication preprocessing routines and collective communication routines <ref> [1, 9] </ref>. 2 Real X (m), Y (m) ! data arrays Integer IA (n) ! indirection array forall i = 1, n X (i) = X (i) + Y (IA (i)) C Build the required schedule DS = Sched (.. parameters ..) C Communicate data using the schedule build above Call <p> The existing compiler for irregular applications <ref> [9, 14] </ref> generated calls to PARTI routines for communication preprocessing and collective communication [21], but did not perform any interprocedural placement of these statements. The performance achieved by the compiled code (before interprocedural optimizations) and the code after interprocedural optimizations is presented in Figure 11.
Reference: [10] <author> D.M. Dhamdhere and H. Patil. </author> <title> An elimination algorithm for bidirectional data flow problems using edge placement. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 15(2) </volume> <pages> 312-336, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: More recently, it has been used for more complex code placement tasks like placement of communication statements while compiling for parallel machines [12, 15]. A number of schemes for partial redundancy elimination have been proposed in literature <ref> [10, 11, 19, 18, 22] </ref>, but are all restricted to optimizing code within a single procedure. All these schemes perform data flow analysis on Control Flow Graph (CFG) of the procedure. In this paper, we address the problem of performing partial redundancy elimination interprocedurally. <p> We believe that it can be applied interprocedurally to optimize placement of communication preprocessing statements and communication statements. 3 Intraprocedural Redundancy Elimination The details of interprocedural redundancy elimination we present are derived from the intrapro-cedural node based method of Dhamdhere <ref> [10] </ref>, also referred to as Modified Morel Renvoise Algorithm (MMRA). Detailed data flow equations and the meaning of the terms used are given in the appendix.
Reference: [11] <author> K. Drechsler and M. Stadel. </author> <title> A solution to a problem with Morel and Renvoise's "Global optimization by suppression of partial redundancies". </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(4) </volume> <pages> 635-640, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: More recently, it has been used for more complex code placement tasks like placement of communication statements while compiling for parallel machines [12, 15]. A number of schemes for partial redundancy elimination have been proposed in literature <ref> [10, 11, 19, 18, 22] </ref>, but are all restricted to optimizing code within a single procedure. All these schemes perform data flow analysis on Control Flow Graph (CFG) of the procedure. In this paper, we address the problem of performing partial redundancy elimination interprocedurally.
Reference: [12] <author> Manish Gupta, Edith Schonberg, and Harini Srinivasan. </author> <title> A unified data flow framework for optimizing communication. </title> <booktitle> In Proceedings of Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1994. </year> <month> 16 </month>
Reference-contexts: It encompasses traditional optimizations like invariant code motion and redundant computation elimination. It is widely used in optimizing compilers for performing common subexpression elimination and strength reduction. More recently, it has been used for more complex code placement tasks like placement of communication statements while compiling for parallel machines <ref> [12, 15] </ref>. A number of schemes for partial redundancy elimination have been proposed in literature [10, 11, 19, 18, 22], but are all restricted to optimizing code within a single procedure. All these schemes perform data flow analysis on Control Flow Graph (CFG) of the procedure.
Reference: [13] <author> Mary Hall, John M Mellor Crummey, Alan Carle, and Rene G Rodriguez. FIAT: </author> <title> A framework for interprocedural analysis and transformations. </title> <booktitle> In Proceedings of the 6th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 522-545. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1993. </year>
Reference-contexts: Several ideas in the design of our representation are similar to the ideas used in Callahan's Program Summary Graph [4] and Interprocedural Flow Graph used by Soffa et al. [16]. FIAT has been introduced as a general framework for performing interprocedural analysis <ref> [13] </ref>, but is more targeted towards flow-insensitive problems. Interval based approach for solving interprocedural data flow equations has been investigated in [2]. Recompilation in a compiler performing interprocedural analysis has been investigated in [3]. Program Foo Procedure P (x,y) a = 1 ..other computations ..
Reference: [14] <author> Reinhard v. Hanxleden. </author> <title> Handling irregular problems with Fortran D a preliminary report. </title> <booktitle> In Proceedings of the Fourth Workshop on Compilers for Parallel Computers, </booktitle> <address> Delft, The Netherlands, </address> <month> December </month> <year> 1993. </year> <note> Also available as CRPC Technical Report CRPC-TR93339-S. </note>
Reference-contexts: The existing compiler for irregular applications <ref> [9, 14] </ref> generated calls to PARTI routines for communication preprocessing and collective communication [21], but did not perform any interprocedural placement of these statements. The performance achieved by the compiled code (before interprocedural optimizations) and the code after interprocedural optimizations is presented in Figure 11.
Reference: [15] <author> Reinhard von Hanxleden and Ken Kennedy. </author> <title> Give-n-take a balanced code placement framework. </title> <booktitle> In Proceedings of the SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 107-120. </pages> <publisher> ACM Press, </publisher> <month> June </month> <year> 1994. </year> <journal> ACM SIGPLAN Notices, </journal> <volume> Vol. 29, No. </volume> <pages> 6. </pages>
Reference-contexts: It encompasses traditional optimizations like invariant code motion and redundant computation elimination. It is widely used in optimizing compilers for performing common subexpression elimination and strength reduction. More recently, it has been used for more complex code placement tasks like placement of communication statements while compiling for parallel machines <ref> [12, 15] </ref>. A number of schemes for partial redundancy elimination have been proposed in literature [10, 11, 19, 18, 22], but are all restricted to optimizing code within a single procedure. All these schemes perform data flow analysis on Control Flow Graph (CFG) of the procedure.
Reference: [16] <author> Mary Jean Harrold and Mary Lou Soffa. </author> <title> Efficient computation of interprocedural definition-use chains. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 16(2) </volume> <pages> 175-204, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Several ideas in the design of our representation are similar to the ideas used in Callahan's Program Summary Graph [4] and Interprocedural Flow Graph used by Soffa et al. <ref> [16] </ref>. FIAT has been introduced as a general framework for performing interprocedural analysis [13], but is more targeted towards flow-insensitive problems. Interval based approach for solving interprocedural data flow equations has been investigated in [2]. Recompilation in a compiler performing interprocedural analysis has been investigated in [3].
Reference: [17] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: N00014-93-1-0158. The authors assume all responsibility for the contents of the paper. 1 We have used interprocedural partial redundancy elimination for optimizing placement of communication statements and communication preprocessing statements in distributed memory compilation. We have implemented our scheme using the the existing Fortran D compilation system <ref> [17] </ref> as infrastructure. We have shown significant performance gains by optimizing placement of communication preprocessing statements. The rest of this paper is organized as follows. <p> The solution of data flow properties for the program shown in Figure 2 is shown in Figure 9. The optimized program is shown in Figure 10. 6 Discussion We have implemented a preliminary version of our scheme using the existing Fortran D compilation system developed at Rice University <ref> [17] </ref> as the necessary infrastructure.
Reference: [18] <author> J. Knoop, O. Ruthing, and B. Steffen. </author> <title> Lazy code motion. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Program Language Design and Implementation, </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: More recently, it has been used for more complex code placement tasks like placement of communication statements while compiling for parallel machines [12, 15]. A number of schemes for partial redundancy elimination have been proposed in literature <ref> [10, 11, 19, 18, 22] </ref>, but are all restricted to optimizing code within a single procedure. All these schemes perform data flow analysis on Control Flow Graph (CFG) of the procedure. In this paper, we address the problem of performing partial redundancy elimination interprocedurally.
Reference: [19] <author> E. Morel and C. </author> <title> Renvoise. Global optimization by suppression of partial redundancies. </title> <journal> Communications of the ACM, </journal> <volume> 22(2) </volume> <pages> 96-103, </pages> <month> February </month> <year> 1979. </year>
Reference-contexts: More recently, it has been used for more complex code placement tasks like placement of communication statements while compiling for parallel machines [12, 15]. A number of schemes for partial redundancy elimination have been proposed in literature <ref> [10, 11, 19, 18, 22] </ref>, but are all restricted to optimizing code within a single procedure. All these schemes perform data flow analysis on Control Flow Graph (CFG) of the procedure. In this paper, we address the problem of performing partial redundancy elimination interprocedurally.
Reference: [20] <author> E. Myers. </author> <title> A precise interprocedural data flow algorithm. </title> <booktitle> In Conference Record of the Eighth ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 219-230, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: We briefly compare our work with efforts on other flow-sensitive interprocedural problems. Several different program representations have been used for different flow-sensitive inter-procedural problems. Myer has suggested concept of SuperGraph <ref> [20] </ref> which is constructed by linking control flow graphs of subroutines by inserting edges from call site in the caller to start node in callee. The total number of nodes in SuperGraph can get very large and consequently the solution may take much longer time to converge.
Reference: [21] <author> Joel Saltz, Kathleen Crowley, Ravi Mirchandaney, and Harry Berryman. </author> <title> Run-time scheduling and execution of loops on message passing machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(4) </volume> <pages> 303-312, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: In these cases, communication can be optimized by placing a preprocessing statement, which determines the set of data elements to be communicated between the processors at runtime. The preprocessing statement stores this information in a data-structure called communication schedule <ref> [21] </ref>. A collective communication routine then performs the data movement, using the information in the communication schedule. This ensures that for a parallel loop, each processor packages the set of data elements it wants to send to any other processor in a single message. <p> The existing compiler for irregular applications [9, 14] generated calls to PARTI routines for communication preprocessing and collective communication <ref> [21] </ref>, but did not perform any interprocedural placement of these statements. The performance achieved by the compiled code (before interprocedural optimizations) and the code after interprocedural optimizations is presented in Figure 11. The figure also shows the performance obtained by a hand parallelized code.
Reference: [22] <author> A. Sorkin. </author> <title> Some comments on "A solution to a problem with Morel and Renvoise's `Global optimization by suppression of partial redundancies' ". ACM Transactions on Programming Languages and Systems, </title> <booktitle> 11(4) </booktitle> <pages> 666-668, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: More recently, it has been used for more complex code placement tasks like placement of communication statements while compiling for parallel machines [12, 15]. A number of schemes for partial redundancy elimination have been proposed in literature <ref> [10, 11, 19, 18, 22] </ref>, but are all restricted to optimizing code within a single procedure. All these schemes perform data flow analysis on Control Flow Graph (CFG) of the procedure. In this paper, we address the problem of performing partial redundancy elimination interprocedurally.
Reference: [23] <author> Mark Weiser. </author> <title> Program slicing. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 10 </volume> <pages> 352-357, </pages> <year> 1984. </year>
References-found: 23

