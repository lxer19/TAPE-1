URL: http://www.neci.nj.nec.com/homepages/pny/papers/string_compare/string_compare.ps
Refering-URL: http://www.neci.nj.nec.com/homepages/pny/papers/string_compare/main.html
Root-URL: 
Email: Email: sbuss@ucsd.edu.  Email: pny@research.nj.nec.com.  
Title: A Bipartite Matching Approach to Approximate String Comparison and Search  
Author: Samuel R. Buss Peter N. Yianilos 
Keyword: Approximate String Comparison, Approximate String Search, Text Search, Sequence Compari son, Bipartite Quasi-Convex Matching, Distance Metric, Natural Language Processing.  
Address: Diego, La Jolla, CA 92093-0112.  4 Independence Way, Princeton, NJ 08540  Princeton University, Princeton, NJ 08544.  
Affiliation: Department of Mathematics, University of California, San  NEC Research Institute,  and Department of Computer Science,  
Note: Supported in part by NSF grant DMS-9503247.  Earlier related works [4, 3] of the authors may be obtained by anonymous ftp from euclid.ucsd.edu, directory pub/sbuss/research, filenames quasiconvex.ps and quasiconvex.Ccode.ps.  
Abstract: Approximate string comparison and search is an important part of applications that range from natural language to the interpretation of DNA. This paper presents a bipartite weighted graph matching approach to these problems, based on the authors' linear time matching algorithms z . Our approach's tolerance to permutation of symbols or blocks, distinguishes it from the widely used edit distance and finite state machine methods. A close relationship with the earlier related `proximity comparison' method is established. Under the linear cost model, a simple O(1) time per position online algorithm is presented for comparing two strings given a fixed alignment. Heuristics are given for optimal alignment. In the approximate string search problem, one string advances in a fixed direction relative to the other with each time step. We introduce a new online algorithm for this setting which dynamically maintains an optimal bipartite weighted matching. We discuss the application of our algorithms to natural language text search, including prefilters to improve efficiency, and the use of polygraphic symbols to improve search quality. Our approach is used in the LikeIt text search utility now under development. Its overall design and objectives are summarized. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aggarwal, A. Bar-Noy, S. Khuller, D. Kravets, and B. Schieber, </author> <title> Efficient minimum cost matching using quadrangle inequality, </title> <booktitle> in Proceedings of the 33th Annual IEEE Symposium on Foundations of Computer Science, IEEE Computer Society, </booktitle> <year> 1992, </year> <pages> pp. 583-592. </pages>
Reference-contexts: We denote by ff (i) the symbol at position i of ff . Notice that only when a string's domain is <ref> [1; m] </ref> is ff (i) really the i -th symbol. Allowing domains more general than [1; m] makes it easier to describe the process of searching for an optimal alignment, and the process of searching for approximate occurrences of ff in fi . <p> We denote by ff (i) the symbol at position i of ff . Notice that only when a string's domain is <ref> [1; m] </ref> is ff (i) really the i -th symbol. Allowing domains more general than [1; m] makes it easier to describe the process of searching for an optimal alignment, and the process of searching for approximate occurrences of ff in fi . Let ff be a string with domain [k; m]. <p> For simplicity assume that ff and fi are both strings with domain <ref> [1; n] </ref>. k Let be a symbol which occurs at position i in ff , i.e., ff (i) = . Then level ff (i) is equal to the number of 's that occur in ff [i; i 1] minus the number that occur fi [1; i 1]. <p> Then level ff (i) is equal to the number of 's that occur in ff [i; i 1] minus the number that occur fi <ref> [1; i 1] </ref>. Similarly, if t = fi (j), then level fi (j) is equal to the number of t 's in ff [1; i] minus the number in fi [1; i 1] minus 1. <p> Then level ff (i) is equal to the number of 's that occur in ff [i; i 1] minus the number that occur fi [1; i 1]. Similarly, if t = fi (j), then level fi (j) is equal to the number of t 's in ff <ref> [1; i] </ref> minus the number in fi [1; i 1] minus 1. A simple, but important, fact about optimal matchings is: Proposition 2 ([1, Lemma 1] or [4, Lemma 4]) Let c be concave down and assume c (?) &gt; c (i) for all i . <p> is equal to the number of 's that occur in ff [i; i 1] minus the number that occur fi <ref> [1; i 1] </ref>. Similarly, if t = fi (j), then level fi (j) is equal to the number of t 's in ff [1; i] minus the number in fi [1; i 1] minus 1. A simple, but important, fact about optimal matchings is: Proposition 2 ([1, Lemma 1] or [4, Lemma 4]) Let c be concave down and assume c (?) &gt; c (i) for all i . <p> Frequently, however, it is desirable to allow the strings to be shifted relative to one another to find a better matching. An example of this would be matching the string ff = "SOUR" against fi = "DINOSAUR". If the strings are aligned so that both ff has domain <ref> [1; 4] </ref> and fi has domain [1; 8] , then the total cost will be c (4) + c (2) + c (4) + c (4). <p> An example of this would be matching the string ff = "SOUR" against fi = "DINOSAUR". If the strings are aligned so that both ff has domain [1; 4] and fi has domain <ref> [1; 8] </ref> , then the total cost will be c (4) + c (2) + c (4) + c (4). <p> Unfortunately, although the use Median for free realignment always provides an optimal free realignment, this does not mean that Realign must converge to an alignment which is even locally optimal. For example, suppose that ff ="BCA" with domain <ref> [1; 3] </ref>, that fi ="ACBXA" with domain [1; 5], that n = 0, and that is the optimal matching for this alignment with (1) = 3, (2) = 2 and (3) = 1. The (only) median value for this is m = 0. <p> Unfortunately, although the use Median for free realignment always provides an optimal free realignment, this does not mean that Realign must converge to an alignment which is even locally optimal. For example, suppose that ff ="BCA" with domain [1; 3], that fi ="ACBXA" with domain <ref> [1; 5] </ref>, that n = 0, and that is the optimal matching for this alignment with (1) = 3, (2) = 2 and (3) = 1. The (only) median value for this is m = 0. <p> Space does not allow a complete treatment of the proximity metric, but we do include its full definition and proofs of the main new results. For the rest of this section, we let ff and fi have equal length and both have domain <ref> [1; n] </ref>. Definition We let ff [i; j] denote the substring of ff from position i to position j . For each symbol , let Common (ff; fi) equal the minimum of the number of occurrences of in ff and the number in fi . <p> Let Common (ff; fi) = X Common (ff; fi) which is the number of (occurrences of) symbols common to both ff and fi . The proximity similarity of ff and fi is defined to equal sim (ff; fi) = i=1 Common (ff <ref> [1; i] </ref>; fi [1; i]) + n X Common (ff [i; n]; fi [i; n]): 13 Note that Common (ff [1; i]; fi [1; i]) is at most i. Therefore, sim (ff; fi) is at most n (n + 1). <p> Let Common (ff; fi) = X Common (ff; fi) which is the number of (occurrences of) symbols common to both ff and fi . The proximity similarity of ff and fi is defined to equal sim (ff; fi) = i=1 Common (ff <ref> [1; i] </ref>; fi [1; i]) + n X Common (ff [i; n]; fi [i; n]): 13 Note that Common (ff [1; i]; fi [1; i]) is at most i. Therefore, sim (ff; fi) is at most n (n + 1). <p> The proximity similarity of ff and fi is defined to equal sim (ff; fi) = i=1 Common (ff <ref> [1; i] </ref>; fi [1; i]) + n X Common (ff [i; n]; fi [i; n]): 13 Note that Common (ff [1; i]; fi [1; i]) is at most i. Therefore, sim (ff; fi) is at most n (n + 1). <p> The proximity similarity of ff and fi is defined to equal sim (ff; fi) = i=1 Common (ff <ref> [1; i] </ref>; fi [1; i]) + n X Common (ff [i; n]; fi [i; n]): 13 Note that Common (ff [1; i]; fi [1; i]) is at most i. Therefore, sim (ff; fi) is at most n (n + 1). <p> For the former, the unmatched symbols provide nothing to the "Common " values. For the latter, the two occurrences of increase Common (ff <ref> [1; i] </ref>; fi [1; i]) by 1 for all i j 1 and they increase Common (ff [i; n]; fi [i; n]) by 1 for all i i 1 . <p> For the former, the unmatched symbols provide nothing to the "Common " values. For the latter, the two occurrences of increase Common (ff <ref> [1; i] </ref>; fi [1; i]) by 1 for all i j 1 and they increase Common (ff [i; n]; fi [i; n]) by 1 for all i i 1 . <p> For the former, the unmatched symbols provide nothing to the "Common " values. For the latter, the two occurrences of increase Common (ff <ref> [1; i] </ref>; fi [1; i]) by 1 for all i j 1 and they increase Common (ff [i; n]; fi [i; n]) by 1 for all i j 1 . <p> For the former, the unmatched symbols provide nothing to the "Common " values. For the latter, the two occurrences of increase Common (ff <ref> [1; i] </ref>; fi [1; i]) by 1 for all i j 1 and they increase Common (ff [i; n]; fi [i; n]) by 1 for all i j 1 . <p> For an example consider the strings ff ="ABA" and fi ="BAB" and fl ="ABB". Here we have dist (ff; fi) = 6 dist (ff; fi) = 6 dist (ff; fl) = 6 dist (ff; fl) = 4 (To normalize these distances to the range <ref> [0; 1] </ref>, they should be divided by 12; e.g., dist (ff; fl) = 4 reflects the fact that ff and fl differ in exactly 1=3 (= 4=12) of their symbols.) But now one clearly feels that fl is more similar to ff than fi is. <p> To give a concrete example, LikeIt system has used to the following method of preprocessing a string ff with domain <ref> [1; m] </ref>: for each value i = 2; : : : ; m and each j such that 2 j min (i; 6), let G i;j be the j -gram comprising symbols ff (i j + 1) ff (i); then the polygrams G i;2 G i;min (i;6) are inserted into ff
Reference: [2] <author> S. R. Buss, </author> <title> Neighborhood metrics on n-dimensional blocks of characters, </title> <type> Tech. Rep. 00218-86, </type> <institution> Mathematical Sciences Research Institute, Berkeley, </institution> <month> September </month> <year> 1985. </year>
Reference-contexts: This metric and various generalizations, developed primarily by the second author, has been reported in <ref> [16, 13, 15, 2] </ref> and has been used extensively for natural language applications, especially spelling correction, by Proximity Technologies and a large number of word processing software publishers, and by Franklin Electronic Publishers in their hand-held solid-state electronic books.
Reference: [3] <author> S. R. Buss, K. G. Kanzelberger, D. Robinson, and P. N. Yianilos, </author> <title> Solving the minimum-cost matching problem for quasi-convex tours: An efficient ANSI C implementation, </title> <type> Tech. Rep. </type> <institution> CS94-370, U.C. </institution> <address> San Diego, </address> <year> 1994. </year>
Reference-contexts: However, by realigning the strings and shifting fi leftward by 4 (thereby letting it have domain <ref> [3; 4] </ref> , the total cost is reduced to c (0) + c (2) + c (0) + c (0) . We call the problem of finding the optimal shift of fi the realignment problem. We give below two heuristics for finding a good realignment. <p> Unfortunately, although the use Median for free realignment always provides an optimal free realignment, this does not mean that Realign must converge to an alignment which is even locally optimal. For example, suppose that ff ="BCA" with domain <ref> [1; 3] </ref>, that fi ="ACBXA" with domain [1; 5], that n = 0, and that is the optimal matching for this alignment with (1) = 3, (2) = 2 and (3) = 1. The (only) median value for this is m = 0.
Reference: [4] <author> S. R. Buss and P. N. Yianilos, </author> <title> Linear and O(n log n) time minimum-cost matching algorithms for quasi-convex tours. </title> <note> To appear in Siam J. Comput.. An extended abstract of this paper appeared in Proceedings of the 5th Annual ACM-SIAM Symposium on Discrete Algorithms, </note> <year> 1994, </year> <pages> pp. 65-76., 199? </pages>
Reference-contexts: These enhancements can be very useful in practice to improve the perceived quality of the optimal assignment; however, they make little difference to the algorithms described in this paper, so we shall use just the simple notion of cost as defined above. In <ref> [4] </ref> the authors have developed linear time and near-linear time algorithms for finding minimum cost bipartite matchings when the cost function is concave down. <p> Similarly, if t = fi (j), then level fi (j) is equal to the number of t 's in ff [1; i] minus the number in fi [1; i 1] minus 1. A simple, but important, fact about optimal matchings is: Proposition 2 ([1, Lemma 1] or <ref> [4, Lemma 4] </ref>) Let c be concave down and assume c (?) &gt; c (i) for all i . There is an optimal cost matching such that, for all i , level ff (i) = level fi ((i)) whenever (i) is defined. <p> Then a common shifting makes both strings begin with position 1. 5 A complete solution to the alternating matching problem, and thereby the matching problem, for concave-down cost functions was given by the authors in <ref> [4] </ref>. A particularly simple, yet useful, special case of this is when the cost function is linear. In this case, the following theorem explains how the algorithm of [4] is simplified. Theorem 3 Let the cost function be linear (and c (?) arbitrary). <p> complete solution to the alternating matching problem, and thereby the matching problem, for concave-down cost functions was given by the authors in <ref> [4] </ref>. A particularly simple, yet useful, special case of this is when the cost function is linear. In this case, the following theorem explains how the algorithm of [4] is simplified. Theorem 3 Let the cost function be linear (and c (?) arbitrary). Let an instance of the alternating matching problem be given as above. <p> Part (a) is a simple special case of the well-known Skier and Skis problem, see [7, 9]. Part (b) is the generalization to unbalanced matching. Proof Recall the notion of "jumper" from <ref> [4] </ref>. First suppose r = q . Since the cost function is linear, there is never any need to put a jumper, assuming ties are broken in favor of not adding a jumper. Therefore the optimal matching is the matching (i a ) = j a for all a . <p> Therefore the optimal matching is the matching (i a ) = j a for all a . Now suppose r = q + 1 . Following <ref> [4] </ref>, this unbalanced problem is reduced to a balanced quasiconvex tour by adding a new phantom occurrence of at j r ; the cost function is modified so that an edge : i a 7! j r contributes zero to the total cost (i.e., the new phantom node j r has <p> As before, ties may be broken in favor of not using jumpers. Therefore, by the methods of <ref> [4] </ref> the optimal matching has some i b such that (i b ) = j r and since there are no other jumpers, (i a ) = j a for a &lt; b and (i a ) = j a1 for a &gt; b. <p> The case (c) is dual to (b) and essentially the same proof works. 2 In <ref> [4] </ref> we proved that there is a linear time algorithm to solve the alternating matching problem of Theorem 3 (since a linear cost function clearly satisfies the weak crossover condition; the algorithm, as explained there, made two passes over the words from left to right. <p> Frequently, however, it is desirable to allow the strings to be shifted relative to one another to find a better matching. An example of this would be matching the string ff = "SOUR" against fi = "DINOSAUR". If the strings are aligned so that both ff has domain <ref> [1; 4] </ref> and fi has domain [1; 8] , then the total cost will be c (4) + c (2) + c (4) + c (4). <p> However, by realigning the strings and shifting fi leftward by 4 (thereby letting it have domain <ref> [3; 4] </ref> , the total cost is reduced to c (0) + c (2) + c (0) + c (0) . We call the problem of finding the optimal shift of fi the realignment problem. We give below two heuristics for finding a good realignment.
Reference: [5] <author> M. Damashek, </author> <title> Gauging similarity with n-grams: Language-independent categorization of text, </title> <booktitle> Science, 267 (1995), </booktitle> <pages> pp. 843-848. </pages>
Reference-contexts: The "Friendly Finder" program is an example of a proximity comparison based database retrieval system based entirely on 1 and 2-grams and their positional relationship. More recently Damashek and Huffman <ref> [5, 6] </ref> have developed a large scale text retrieval system based entirely on polygrams. It is convenient to imagine two strings ff and fi arranged on top of one another as in Figure 1. In this example the graph's nodes consist of unigrams only, i.e., single letters. <p> Unfortunately, although the use Median for free realignment always provides an optimal free realignment, this does not mean that Realign must converge to an alignment which is even locally optimal. For example, suppose that ff ="BCA" with domain [1; 3], that fi ="ACBXA" with domain <ref> [1; 5] </ref>, that n = 0, and that is the optimal matching for this alignment with (1) = 3, (2) = 2 and (3) = 1. The (only) median value for this is m = 0. <p> That is, ignore position entirely. This may be thought of as a crude projection of the matching model so as to use a cost function which assumes a constant value for all matching edges independent of edge length. It corresponds to the use of frequencies only in <ref> [5, 6] </ref>. Other model simplifications involve approximations we have already seen. Examples are the use of free realignment only, the CoG heuristic, and approximate solutions to each level's matching problem. Another simplification consists of dealing fewer polygram sizes.
Reference: [6] <author> S. Huffman and M. Damashek, Acquaintance: </author> <title> A novel vector-space n-gram technique for document categorization, </title> <booktitle> in Proceedings, Text REtrieval Conference (TREC-3), </booktitle> <address> Washington, D.C., </address> <year> 1995, </year> <pages> NIST, pp. 305-310. </pages>
Reference-contexts: The "Friendly Finder" program is an example of a proximity comparison based database retrieval system based entirely on 1 and 2-grams and their positional relationship. More recently Damashek and Huffman <ref> [5, 6] </ref> have developed a large scale text retrieval system based entirely on polygrams. It is convenient to imagine two strings ff and fi arranged on top of one another as in Figure 1. In this example the graph's nodes consist of unigrams only, i.e., single letters. <p> That is, ignore position entirely. This may be thought of as a crude projection of the matching model so as to use a cost function which assumes a constant value for all matching edges independent of edge length. It corresponds to the use of frequencies only in <ref> [5, 6] </ref>. Other model simplifications involve approximations we have already seen. Examples are the use of free realignment only, the CoG heuristic, and approximate solutions to each level's matching problem. Another simplification consists of dealing fewer polygram sizes.
Reference: [7] <author> R. M. Karp and S.-Y. R. Li, </author> <title> Two special cases of the assignment problem, </title> <journal> Discrete Mathematics, </journal> <volume> 13 (1975), </volume> <pages> pp. 129-142. </pages>
Reference-contexts: Part (a) is a simple special case of the well-known Skier and Skis problem, see <ref> [7, 9] </ref>. Part (b) is the generalization to unbalanced matching. Proof Recall the notion of "jumper" from [4]. First suppose r = q . Since the cost function is linear, there is never any need to put a jumper, assuming ties are broken in favor of not adding a jumper.
Reference: [8] <author> D. E. Knuth, J. James H. Morris, and V. R. Pratt, </author> <title> Fast pattern matching in strings, </title> <journal> SIAM Journal on Computing, </journal> <volume> 6 (1977), </volume> <pages> pp. 323-350. </pages>
Reference-contexts: That is: organize a set of strings for effective storage and retrieval of exact matches. Given a short string ff and a much longer one fi , one might also strive to efficiently locate occurrences of ff in fi . This is sometimes called the string matching problem <ref> [8] </ref>. When the comparison method is generalized to allow inexact match, the resulting ideas, algorithms, and data structures become somewhat more complicated. Despite the added complexity, this research direction is important because the resulting algorithms are frequently of greater practical value. <p> An example of this would be matching the string ff = "SOUR" against fi = "DINOSAUR". If the strings are aligned so that both ff has domain [1; 4] and fi has domain <ref> [1; 8] </ref> , then the total cost will be c (4) + c (2) + c (4) + c (4).
Reference: [9] <author> E. Lawler, </author> <title> Combinatorial Optimization: Networks and Matroids, </title> <publisher> Holt, Rinehart and Winston, </publisher> <year> 1976. </year>
Reference-contexts: Part (a) is a simple special case of the well-known Skier and Skis problem, see <ref> [7, 9] </ref>. Part (b) is the generalization to unbalanced matching. Proof Recall the notion of "jumper" from [4]. First suppose r = q . Since the cost function is linear, there is never any need to put a jumper, assuming ties are broken in favor of not adding a jumper.
Reference: [10] <author> U. Manber and S. Wu, GLIMPSE: </author> <title> A tool to search through entire file systems, </title> <booktitle> in Proceedings of the Winter 1994 USENIX Conference, </booktitle> <year> 1994, </year> <pages> pp. 23-32. </pages>
Reference-contexts: Given a query string, it is then possible to build a finite state machine (FSM) to detect it, or any match within the error bounds, within a second string. We will refer to this approach generalizations of it as FSM methods. The recent work of <ref> [10, 12] </ref> demonstrates that text can be scanned at very high speeds within this framework for comparison. Another well known approach to generalized string comparison computes edit-distance (ED), which measures the least costly transformation of one string into another using some set of primitive operations.
Reference: [11] <author> D. Sankoff and J. B. Kruskal, </author> <title> Time Warps, String Edits and Macromolecules: The Theory and Practice of Sequence Comparison, </title> <publisher> Addison-Wesley, </publisher> <year> 1983. </year>
Reference-contexts: The most common choices of primitive operations are insert, delete, substitute, and sometimes adjacent transposition. A simple dynamic program computes this distance in quadratic time, i.e. proportional to the product of string lengths. See <ref> [11] </ref> for a discussion of these and related algorithms which we will refer to as ED methods. Both the FSM and ED approaches rather strictly enforce temporal ordering. In most applications this is to some extent desirable.
Reference: [12] <author> S. Wu and U. Manber, </author> <title> Fast test searching allowing errors, </title> <journal> Communications of the ACM, </journal> <volume> 35 (1993), </volume> <pages> pp. 83-91. </pages>
Reference-contexts: Given a query string, it is then possible to build a finite state machine (FSM) to detect it, or any match within the error bounds, within a second string. We will refer to this approach generalizations of it as FSM methods. The recent work of <ref> [10, 12] </ref> demonstrates that text can be scanned at very high speeds within this framework for comparison. Another well known approach to generalized string comparison computes edit-distance (ED), which measures the least costly transformation of one string into another using some set of primitive operations.
Reference: [13] <author> P. N. Yianilos, </author> <title> The definition, computation and application of symbol string similarity functions, </title> <type> Master's thesis, </type> <institution> Emory University, </institution> <year> 1978. </year> <title> [14] , A dedicated comparator matches symbol strings fast and intelligently, </title> <journal> Electronics Magazine, </journal> <year> (1983). </year>
Reference-contexts: This metric and various generalizations, developed primarily by the second author, has been reported in <ref> [16, 13, 15, 2] </ref> and has been used extensively for natural language applications, especially spelling correction, by Proximity Technologies and a large number of word processing software publishers, and by Franklin Electronic Publishers in their hand-held solid-state electronic books. <p> Traditionally, the proximity metric has been a scaled version of sim ; namely, (ff; fi) = sim (ff; fi)=(n (n + 1)). The next theorem is proved in <ref> [13] </ref>, and we omit its proof here: Theorem 6 Fix n 0. Then dist (ff; fi) is a metric.
Reference: [15] <author> P. N. Yianilos and S. R. Buss, </author> <title> Associative memory circuit system and method, continuation-in-part. </title> <type> U.S. Patent #4490811, </type> <month> December </month> <year> 1984. </year> <month> 19 </month>
Reference-contexts: This metric and various generalizations, developed primarily by the second author, has been reported in <ref> [16, 13, 15, 2] </ref> and has been used extensively for natural language applications, especially spelling correction, by Proximity Technologies and a large number of word processing software publishers, and by Franklin Electronic Publishers in their hand-held solid-state electronic books.
Reference: [16] <author> P. N. Yianilos, R. A. Harbort, and S. R. Buss, </author> <title> The application of a pattern matching algorithm to searching medical record text, </title> <booktitle> in IEEE Symposium on Computer Applications in Medical Care, </booktitle> <year> 1978, </year> <pages> pp. 308-313. </pages>
Reference-contexts: This metric and various generalizations, developed primarily by the second author, has been reported in <ref> [16, 13, 15, 2] </ref> and has been used extensively for natural language applications, especially spelling correction, by Proximity Technologies and a large number of word processing software publishers, and by Franklin Electronic Publishers in their hand-held solid-state electronic books.
Reference: [17] <author> P. N. Yianilos and K. G. Kanzelberger, </author> <title> The LikeIt distributed Web search system. </title> <type> Manuscript, </type> <note> in preparation, 1995. 20 </note>
Reference-contexts: The mapping then attempts to capture linguistic relationships. 17 3.4 The LikeIt Text Search Engine: Summary Preview The ideas of this paper and the authors' earlier work have lead to the development of LikeIt; a new text search engine for the Web <ref> [17] </ref>. The weighted matching approach represents a single conceptual framework that deals with variations in word spelling, form, spacing, ordering, and proximity within the text.
References-found: 16

