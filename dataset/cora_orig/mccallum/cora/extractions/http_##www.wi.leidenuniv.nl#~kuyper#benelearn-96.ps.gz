URL: http://www.wi.leidenuniv.nl/~kuyper/benelearn-96.ps.gz
Refering-URL: http://www.wi.leidenuniv.nl/~kuyper/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: kuyper@wi.leidenuniv.nl  
Title: Artificial Neural Networks 149 Some remarks on the entropy of a neural network  
Author: Ida G. Sprinkhuizen-Kuyper 
Affiliation: Department of Computer Science Leiden University  
Abstract: The entropy of a neural network is defined in terms of the probabilities of the functions that can be implemented by the network. Supervised learning results in a monotonic reduction of the effective volume of the configuration space. The probabilities of the functions that are in conict with the training set become zero, while the other probabilities increase. In this paper we show that the entropy of a neural network is not always monotonically decreasing as function of the size of the training set.
Abstract-found: 1
Intro-found: 1
Reference: <author> Claas, W.J. </author> <year> (1996). </year> <title> Leren met terugkoppeling, het gemiddeld generaliserend vermogen van feedforward netwerken. </title> <type> Masters Thesis, Internal Report 96-01, </type> <institution> Department of Computer Science, Leiden University. </institution>
Reference-contexts: So for example the trivial function for all is represented if the weights satisfy the following inequalities: Restricting the weight space to a cube results in a volume of measure for the part of the weight space correctly representing the function with all outputs equal to zero <ref> (Claas, 1996) </ref>. We computed the volumes of the parts of the weight space corresponding to all representable boolean functions and their corresponding probabilities. Taking the limit resulted in the probabilities given in table 1.
Reference: <author> Denker, J., Schwartz, D., Wittner, B., Solla, S., Howard, R., Jackel, L., & Hopfield, J. </author> <year> (1987). </year> <title> Large Automatic Learning, Rule Extraction, and Generalisation. </title> <booktitle> Complex Systems 1, </booktitle> <pages> 877-922. </pages>
Reference: <editor> McEliece, </editor> <address> R.J. </address> <year> (1977). </year> <title> The Theory of Information and Coding. Vol. 3 of: </title> <editor> Rota, G.-C. (ed.): </editor> <booktitle> Encyclopedia of Mathematics and its Applications, </booktitle> <publisher> Addison-Wesley. </publisher>
Reference-contexts: We were very surprised when we indeed were able to construct a counter example. So S m will not in general decrease with m. The case is that S m is a so-called conditional entropy <ref> (McEliece, 1977) </ref>. On average S m will decrease, but presenting a special example can lead to an increase of the entropy. In section 2 the definition of entropy is given following Schwartz et al. (1990). In section 3 we give a simple example that the entropy can increase. <p> Then we get the new set of functions = -f 1 , f 2 , f 3 , f 4 , f 5 - with probabilities , . The corresponding entropy is: since it is clear that in this case . In <ref> (McEliece, 1977) </ref> the conditional entropy of X, given Y = y is defined as: and the conditional entropy is its expectation: Example 1.7 in (McEliece, 1977) also shows that the conditional entropy can be larger than the original entropy . <p> The corresponding entropy is: since it is clear that in this case . In <ref> (McEliece, 1977) </ref> the conditional entropy of X, given Y = y is defined as: and the conditional entropy is its expectation: Example 1.7 in (McEliece, 1977) also shows that the conditional entropy can be larger than the original entropy . <p> f 1 f 5 f 6 S m 1+ 1 log 5 log 5 2.32= = H X Y=y ( ) p x y ( ) logp x y ( ) x H X Y ( ) y H X Y=y ( ) 152 BENELEARN-96 is positive (theorem 1.3 in <ref> (McEliece, 1977) </ref>). So on the average (learning arbitrary functions) the entropy will decrease, while for learning some concrete function it is quite well possible that sometimes will increase. <p> Acknowledgements I like to thank W.J. Claas for his work and the discussions on this subject during the period he was working for his masters thesis. I also like to thank G. Cybenko (visiting our institute for the Kloosterman chair) who told me about the notion of conditional entropy <ref> (McEliece, 1977) </ref>. n the outputs of f n P 0 (f n ) 1 (0,0,0,1) 3 (0,0,1,1) 5 (0,1,0,1) 7 (0,1,1,1) 9 (1,0,0,1) 11 (1,0,1,1) 13 (1,1,0,1) 15 (1,1,1,1) 271 12 48 2 ( ) 484 12 48 2 ( ) 484 12 48 2 ( ) 532 12 48 2
Reference: <author> Solla, S. </author> <year> (1992). </year> <title> Supervised Learning: A Theoretical Framework. </title> <editor> In: Casdagli, M. & Eubank, S. (eds.): </editor> <booktitle> Nonlinear Modeling and Forecasting, SFI Studies in the Science of Complexity, Proc. </booktitle> <volume> Vol. XII, </volume> <publisher> Addison-Wesley: </publisher> <address> Redwood, CA. </address>
Reference: <author> Schwartz, D.B., Samalam, V.K., Solla, S.A., & Denker, J.S. </author> <year> (1990). </year> <title> Exhaustive Learning. </title> <booktitle> Neural Computation 2, </booktitle> <pages> 374-358. </pages>
References-found: 5

