URL: http://www.rpi.edu/~bennek/svmdt.ps
Refering-URL: http://www.rpi.edu/~bennek/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: bennek@rpi.edu  bluej@rpi.edu  
Title: A Support Vector Machine Approach to Decision Trees  
Author: K. P. Bennett J. A. Blue 
Address: Troy, NY 12180  Troy, NY 12180  
Affiliation: Mathematical Sciences Department Rensselaer Polytechnic Institute  Mathematical Sciences Department Rensselaer Polytechnic Institute  
Abstract: Key ideas from statistical learning theory and support vector machines are generalized to decision trees. A support vector machine is used for each decision in the tree. The "optimal" decision tree is characterized, and both a primal and dual space formulation for constructing the tree are proposed. The result is a method for generating logically simple decision trees with multivariate linear or nonlinear decisions. The preliminary results indicate that the method produces simple trees that generalize well with respect to other decision tree algorithms and single support vector machines.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. P. Bennett and J. </author> <title> Blue. Optimal decision trees. R.P.I. Math Report No. </title> <type> 214, </type> <institution> Rensselaer Polytechnic Institute, </institution> <address> Troy, New York, </address> <year> 1996. </year> <note> Revised. </note>
Reference-contexts: This is equivalent to the complementarity constraint, ( L i + i)( R i + t i ) = 0; i = 1; : : : ; m. A prior approach, Global Tree Optimization (GTO) <ref> [1] </ref>, uses the complementarity constraint as an error function. <p> Prior algorithms applied to GTO <ref> [1] </ref> can be readily applied to this problem. Our computational results show that GTO/SVM works better than GTO alone and produces much simpler trees than conventional DT algorithms. There are, however, some limitations. First, the complementarity error function is not a theoretically well-defined measure of error. <p> The primal Problem (4), GTO/SVM, is a nonconvex objective function minimized with respect to polyhedral constraints. In <ref> [1] </ref>, it was shown that the best solution to GTO/SVM in terms of the number of points misclassified is an extreme point of the polyhedral constraints. A hybrid extreme point tabu search (HEPTS) algorithm developed for GTO can be used to solve approximately GTO/SVM [2]. <p> Details of the method and the parameter settings used can be found in <ref> [1] </ref>. On the primal problem, GTO corresponds to setting = 0. For GTO/SVM, = 0:1 was used for all the data sets, except on Cancer and House where = 0:35 was used.
Reference: [2] <author> J. Blue and K. Bennett. </author> <title> Hybrid extreme point tabu search. R.P.I. Math Report No. </title> <type> 240, </type> <institution> Rensselaer Polytechnic Institute, </institution> <address> Troy, NY, </address> <year> 1996. </year> <note> To appear in the European Journal of Operations Research. </note>
Reference-contexts: In [1], it was shown that the best solution to GTO/SVM in terms of the number of points misclassified is an extreme point of the polyhedral constraints. A hybrid extreme point tabu search (HEPTS) algorithm developed for GTO can be used to solve approximately GTO/SVM <ref> [2] </ref>. The dual Problem (6) is known as a mathematical program with equilibrium constraints (MPEC). It has a nice convex quadratic objective but the constraints are combinatorial in nature. MPECs and their applications are a very active research topic [7]. Problem (6) is much larger than previously solved MPECs.
Reference: [3] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International, </booktitle> <address> California, </address> <year> 1984. </year>
Reference-contexts: Consider the two limiting cases, a tree with one flexible multivariate decision such as a SVM versus a univariate DT such as the one produced by CART <ref> [3] </ref> or C4.5 [9]. Given a sufficiently general discriminant function such as a 9th-order polynomial, a SVM can solve a large difficult problem using only one decision. One benefit of DTs is that logical interpretable rules are produced.
Reference: [4] <author> C. Burges and B. Scholkopf. </author> <title> Improving the accuracy and speed of support vector machines. </title> <editor> In M. Mozer, M. Jordan, and T. Petsche, editors, </editor> <booktitle> Neural Information Processing Systems, </booktitle> <volume> Vol. </volume> <pages> 9, </pages> <address> Cambridge, MA, 1999. </address> <publisher> MIT Press. In press. </publisher>
Reference-contexts: 1 Introduction Statistical Learning Theory [10] is both a powerful theoretical tool and a practical approach to learning problems <ref> [4] </ref>. Support Vector Machine (SVM) algorithms have been successfully applied to classification and regression problems. In this paper, we examine how three key ideas from SVMs can be extended to two-class decision trees (DTs).
Reference: [5] <author> C. Cortes and V. N. Vapnik. </author> <title> Support vector networks. </title> <journal> Machine Learning, </journal> <volume> 20 </volume> <pages> 273-297, </pages> <year> 1995. </year>
Reference-contexts: This is a minor variant of the formulations given in <ref> [5, 10] </ref>. The problem objective maximizes the margin with weight and minimizes the classification error with weight (1). The user must select 2 (0; 1) to control the trade-off between the classification rate and the confidence interval. The second key idea is the use of the dual problem.
Reference: [6] <author> F. Glover. </author> <title> Tabu search part I. </title> <journal> ORSA Journal of Computing, </journal> <volume> 1(3) </volume> <pages> 190-206, </pages> <year> 1989. </year>
Reference-contexts: HEPTS, our algorithm to solve GTO/SVM, is a hybrid of a local gradient descent method and a nonmonotonic heuristic search technique called tabu search (TS) <ref> [6] </ref>. The descent method finds a local minimum. TS is then done on the extreme points of the feasible region. TS searches around the local minimum and then uses long-term memory to diversify into a new area of the search space.
Reference: [7] <author> Z. Q. Luo, J. S. Pang, and D. Ralph. </author> <title> Mathematical Programs with Equilibrium Constraints. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, England, </address> <year> 1996. </year>
Reference-contexts: The dual Problem (6) is known as a mathematical program with equilibrium constraints (MPEC). It has a nice convex quadratic objective but the constraints are combinatorial in nature. MPECs and their applications are a very active research topic <ref> [7] </ref>. Problem (6) is much larger than previously solved MPECs. We are developing a new tabu search algorithm for approximately solving Problem (6).
Reference: [8] <author> S. Murthy, S. Kasif, and S. Salzberg. </author> <title> A system for induction of oblique decision trees. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 1-32, </pages> <year> 1994. </year>
Reference-contexts: This was done 5 times for each dimension, and the results given for each dimension are the average of the 5 runs. We compared GTO/SVM results with original GTO, the univariate DT algorithm C4.5 [9], the multivariate DT algorithm OC1 <ref> [8] </ref>, and a single SVM. For both C4.5 and OC1, the standard default settings were used. GTO used the same parameters as GTO/SVM. For the single SVM, we experimented with polynomials of degree 1 through 5 with various values of .
Reference: [9] <author> J. R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Consider the two limiting cases, a tree with one flexible multivariate decision such as a SVM versus a univariate DT such as the one produced by CART [3] or C4.5 <ref> [9] </ref>. Given a sufficiently general discriminant function such as a 9th-order polynomial, a SVM can solve a large difficult problem using only one decision. One benefit of DTs is that logical interpretable rules are produced. <p> This was done 5 times for each dimension, and the results given for each dimension are the average of the 5 runs. We compared GTO/SVM results with original GTO, the univariate DT algorithm C4.5 <ref> [9] </ref>, the multivariate DT algorithm OC1 [8], and a single SVM. For both C4.5 and OC1, the standard default settings were used. GTO used the same parameters as GTO/SVM. For the single SVM, we experimented with polynomials of degree 1 through 5 with various values of .
Reference: [10] <author> V. N. Vapnik. </author> <title> The Nature of Statistical Learning Theory. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: 1 Introduction Statistical Learning Theory <ref> [10] </ref> is both a powerful theoretical tool and a practical approach to learning problems [4]. Support Vector Machine (SVM) algorithms have been successfully applied to classification and regression problems. In this paper, we examine how three key ideas from SVMs can be extended to two-class decision trees (DTs). <p> In this paper we use a very simple tree one with three nonlinear multivariate decisions, but our techniques apply to trees of any size. We directly address the combinatorial aspects of the DT construction. By using structural risk minimization <ref> [10] </ref> we eliminate DT pruning. Ideally, the resulting DT will be logically simpler than those produced by other DT approaches, and the decisions in the tree will be much simpler than a single SVM. We begin with a review of three key ideas of SVMs. <p> Then we show how these ideas can be extended to DTs. Practical algorithms and preliminary computational results are then discussed. We conclude with future extensions and applications. 2 Key Ideas of Support Vector Machines We briefly review three key ideas from Support Vector Machines. Readers should consult <ref> [10] </ref> for details. In a SVM the input vectors are mapped nonlinearly to a very high-dimensional feature space. A linear discriminant is then constructed in the new space, resulting in a nonlinear discriminant in the original input space. <p> This is a minor variant of the formulations given in <ref> [5, 10] </ref>. The problem objective maximizes the margin with weight and minimizes the classification error with weight (1). The user must select 2 (0; 1) to control the trade-off between the classification rate and the confidence interval. The second key idea is the use of the dual problem. <p> So for example if we let K (u; v) = (u v + 1) d , the resulting discriminant surface is a polynomial of degree d. Consult <ref> [10] </ref> for more details on convolutions and definitions of other inner products for neural networks and radial basis functions. 3 SVM Formulation for Decision Trees 1 2 0 xw 0 b 0 1 xw 0 b 0 1 A 1 B 1 A 2 B 2 xw 2 = b 2
References-found: 10

