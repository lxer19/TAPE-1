URL: ftp://ftp.cs.rochester.edu/pub/papers/systems/93.SEDMS.false_sharing.ps.Z
Refering-URL: http://www.cs.rochester.edu/trs/systems-trs.html
Root-URL: 
Email: bolosky@microsoft.com  scott@cs.rochester.edu  
Title: False Sharing and its Effect on Shared Memory Performance  
Author: William J. Bolosky Michael L. Scott 
Address: One Microsoft Way, 9S/1049 Redmond, WA 98052-6399  Rochester, NY 14627-0226  
Affiliation: Microsoft Research Laboratory  Computer Science Department University of Rochester  
Abstract: False sharing occurs when processors in a shared-memory parallel system make references to different data objects within the same coherence block (cache line or page), thereby inducing "unnecessary" coherence operations. False sharing is widely believed to be a serious problem for parallel program performance, but a precise definition and quantification of the problem has proven to be elusive. We explain why. In the process, we present a variety of possible definitions for false sharing, and discuss the merits and drawbacks of each. Our discussion is based on experience gained during a four-year study of multiprocessor memory architecture and its effect on the behavior of applications in a sixteen-program suite. Using trace-based simulation, we present experimental evidence to support the claim that false sharing is a serious problem. Unfortunately, we find that the various computa-tionally tractable approaches to quantifying the problem are either heuristic in nature, or fail to agree with intuition. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. N. Bershad, E. D. Lazowska, H. M. Levy, and D. B. Wagner. </author> <title> An Open Environ ment for Building Parallel Programming Systems. </title> <booktitle> In Proceedings of the First ACM 13 Conference on Parallel Programming: Experience with Applications, Languages and Systems, </booktitle> <pages> pages 1-9, </pages> <address> New Haven, CT, </address> <month> 19-21 July </month> <year> 1988. </year> <journal> In ACM SIGPLAN Notices 23:9. </journal>
Reference-contexts: The applications are a scene-rendering program [11], a Presto <ref> [1] </ref> quicksort program, the Cholesky factorization program from the SPLASH suite [17], and our successive over-relaxation program. The machine parameters model a sequentially consistent cache-coherent (CC) multiprocessor, and a VM-based software coherence system running on a non-uniform memory access (NUMA) machine resembling the Cray T3D.
Reference: [2] <author> W. J. Bolosky, R. P. Fitzgerald, and M. L. Scott. </author> <title> Simple But Effective Techniques for NUMA Memory Management. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 19-31, </pages> <address> Litchfield Park, AZ, </address> <month> 3-6 December </month> <year> 1989. </year> <booktitle> In ACM SIGOPS Operating Systems Review 23:5. </booktitle>
Reference-contexts: For the purpose of maintaining coherence, memory is grouped into blocks. On a machine with hardware cache coherence, blocks are cache lines; on a machine with VM-based software coherence (i.e., a Non-Uniform Memory Architecture (NUMA) <ref> [2, 8, 13] </ref> or Distributed Shared Memory (DSM) [16] system), blocks are generally pages.
Reference: [3] <author> W. J. Bolosky, M. L. Scott, R. P. Fitzgerald, R. J. Fowler, and A. L. Cox. </author> <title> NUMA Policies and Their Relation to Memory Architecture. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 212-221, </pages> <address> Santa Clara, CA, </address> <month> 8-11 April </month> <year> 1991. </year> <journal> In ACM SIGARCH Computer Architecture News 19:2, ACM SIGOPS Operating Systems Review 25 (spe-cial issue), and ACM SIGPLAN Notices 26:4. </journal>
Reference-contexts: An important point in understanding the use of the optimal policy is that previous results <ref> [3] </ref> show that, at least for a certain class of architectures, straightforward on-line policies can closely approach optimal performance. 3 1.1 1.3 Page or Line Size M P 32 64 128 256 512 1K 2K 4K 8K * * * * * * fi fi fi fi fi fi * *
Reference: [4] <author> W. J. Bolosky and M. L. Scott. </author> <title> A Trace-Based Comparison of Shared Memory Multiprocessor Architectures. </title> <type> TR 432, </type> <institution> Computer Science Department, University of Rochester, </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: As noted above, the "optimal" policy doesn't actually optimize anything on the CC and DSM models, which lack remote reference capabilities. All five sets of parameters assume equivalent hardware technology. Further details appear in <ref> [4] </ref>. The total cost of memory references and coherence operations in the SOR program increases markedly as the block size is lowered toward one word.
Reference: [5] <author> W. J. Bolosky and M. L. Scott. </author> <title> Evaluation of Multiprocessor Memory Systems Using Off-Line Optimal Behavior. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 15(4) </volume> <pages> 382-398, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Section 4 uses observations from the cost component method to estimate false sharing in several example programs. Much of our discussion takes place in the context of a formal model of memory access cost, defined in previous work <ref> [5] </ref>. The model applies to invalidation-based coherence protocols on sequentially-consistent machines. It captures a program and its input in the form of a shared-memory reference trace, interleaved as the references occurred in practice on some parallel machine (in our studies, an 8-node IBM ACE [12] with uniform access-time memory). <p> Reference <ref> [5] </ref> presents the cost of several practical policies on a 16-program application suite. It also presents a tractable off-line policy that is provably optimal|that minimizes the total cost of memory accesses and coherence operations.
Reference: [6] <author> W. J. Bolosky. </author> <title> Software Coherence in Multiprocessor Memory Systems. </title> <type> Ph. D. Thesis, TR 456, </type> <institution> Computer Science Department, University of Rochester, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: The insights offered into the magnitude of the problem, however, indicate that if it could be solved in a general way, it would result in large improvements in parallel program performance, particularly on systems with large block sizes. An expanded version of this paper appears as chapter 7 of <ref> [6] </ref>. 2 Definitions of False Sharing Ideally, a definition of false sharing would have the following properties: * It would adequately capture the intuitive notion of false sharing. * It would be mathematically precise. * It would be practically applicable. <p> If we define S to be the false sharing component and F the fragmentation component of the difference in cost between runs with regular and single-word blocks, we can show <ref> [6] </ref> that the grouping components cancel out, and S = (o + bs)M s ( s o )F (1) where o is per-message overhead, b is per-byte overhead, s is block size, and M s and M 1 are the number of block moves performed by an optimal policy with block
Reference: [7] <author> J. B. Carter, J. K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <address> Pacific Grove, CA, </address> <month> 14-16 October </month> <year> 1991. </year> <booktitle> In ACM SIGOPS Operating Systems Review 25:5. </booktitle>
Reference-contexts: In practice, this lower bound is not very tight. Munin's <ref> [7] </ref> software implementation of release consistency takes a practical approach to heuristic interval selection. Its designers observe that with the proper use of locks, any 6 references made by a processor to an object for which it holds a lock will be inside a false sharing interval. <p> When combined with some knowledge of application semantics, these results suggest that the elimination of false sharing could result in order-of-magnitude improvements in performance for many programs. Relaxed models of memory consistency (as in DASH [14] or Munin <ref> [7] </ref>; see [15] for a survey) constitute one promising approach to reducing the impact of false sharing. In essence, relaxed consistency models suffer delays due to false sharing only at synchronization points.
Reference: [8] <author> A. L. Cox and R. J. Fowler. </author> <title> The Implementation of a Coherent Memory Abstraction on a NUMA Multiprocessor: Experiences with PLATINUM. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 32-44, </pages> <address> Litchfield Park, AZ, </address> <month> 3-6 December </month> <year> 1989. </year> <booktitle> In ACM SIGOPS Operating Systems Review 23:5. </booktitle>
Reference-contexts: For the purpose of maintaining coherence, memory is grouped into blocks. On a machine with hardware cache coherence, blocks are cache lines; on a machine with VM-based software coherence (i.e., a Non-Uniform Memory Architecture (NUMA) <ref> [2, 8, 13] </ref> or Distributed Shared Memory (DSM) [16] system), blocks are generally pages.
Reference: [9] <author> C. Dubnicki and T. J. LeBlanc. </author> <title> Adjustable Block Size Coherent Caches. </title> <booktitle> In Proceedings of the Nineteenth International Symposium on Computer Architecture, </booktitle> <pages> pages 170-180, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: In essence, relaxed consistency models suffer delays due to false sharing only at synchronization points. Other means of reducing false sharing include on-line adaptation of the block size <ref> [9] </ref>, hand tuning [10], and smart compilers.
Reference: [10] <author> S. J. Eggers and T. E. Jeremiassen. </author> <title> Eliminating False Sharing. </title> <booktitle> Proceedings of the 1991 International Conference on Parallel Processing, I, </booktitle> <address> Architecture:377-381, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: This is an informal statement of the false sharing problem. False sharing has been observed and commented on previously <ref> [10, 18] </ref>, but these studies do not provide sufficiently mathematically precise, convincing definitions. This paper considers several methods for transforming the intuitive idea of false sharing into a precise, usable definition that is able to show false sharing's performance impact on a particular program running on a particular machine. <p> While this definition is precise and complete, and describes something that can be called "false sharing," it fails to capture the real problem of false sharing. It is a valid definition of the wrong effect; the intuition criterion is not satisfied. 2.5 The Hand Tuning Method Eggers and Jeremiassen <ref> [10] </ref> defined false sharing to be the cost of cache coherence operations that were initiated by a reference to a word that was not modified by any processor since it was last present at the referencing processor. This definition has the difficulty (which was not noted in [10]) that true sharing <p> Eggers and Jeremiassen <ref> [10] </ref> defined false sharing to be the cost of cache coherence operations that were initiated by a reference to a word that was not modified by any processor since it was last present at the referencing processor. This definition has the difficulty (which was not noted in [10]) that true sharing may be masked by such coherence operations. <p> In essence, relaxed consistency models suffer delays due to false sharing only at synchronization points. Other means of reducing false sharing include on-line adaptation of the block size [9], hand tuning <ref> [10] </ref>, and smart compilers.
Reference: [11] <author> A. Garcia. </author> <title> Efficient Rendering of Synthetic Images. </title> <editor> Ph. D. </editor> <booktitle> thesis, </booktitle> <publisher> MIT, </publisher> <month> February </month> <year> 1988. </year>
Reference-contexts: The applications are a scene-rendering program <ref> [11] </ref>, a Presto [1] quicksort program, the Cholesky factorization program from the SPLASH suite [17], and our successive over-relaxation program. The machine parameters model a sequentially consistent cache-coherent (CC) multiprocessor, and a VM-based software coherence system running on a non-uniform memory access (NUMA) machine resembling the Cray T3D.
Reference: [12] <author> A. Garcia, D. Foster, and R. Freitas. </author> <title> The Advanced Computing Environment Mul tiprocessor Workstation. </title> <type> Technical Report RC-14419, </type> <institution> IBM T. J. Watson Research Center, </institution> <month> March </month> <year> 1989. </year> <month> 14 </month>
Reference-contexts: The model applies to invalidation-based coherence protocols on sequentially-consistent machines. It captures a program and its input in the form of a shared-memory reference trace, interleaved as the references occurred in practice on some parallel machine (in our studies, an 8-node IBM ACE <ref> [12] </ref> with uniform access-time memory).
Reference: [13] <author> R. P. LaRowe, Jr. and C. S. Ellis. </author> <title> Experimental Comparison of Memory Manage ment Policies for NUMA Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(4) </volume> <pages> 319-363, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: For the purpose of maintaining coherence, memory is grouped into blocks. On a machine with hardware cache coherence, blocks are cache lines; on a machine with VM-based software coherence (i.e., a Non-Uniform Memory Architecture (NUMA) <ref> [2, 8, 13] </ref> or Distributed Shared Memory (DSM) [16] system), blocks are generally pages.
Reference: [14] <author> D. Lenoski, J. Laudon, L. Stevens, T. Joe, D. Nakahira, A. Gupta, and J. Hennessy. </author> <title> The DASH Prototype: Implementation and Performance. </title> <booktitle> In Proceedings of the Nineteenth International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: When combined with some knowledge of application semantics, these results suggest that the elimination of false sharing could result in order-of-magnitude improvements in performance for many programs. Relaxed models of memory consistency (as in DASH <ref> [14] </ref> or Munin [7]; see [15] for a survey) constitute one promising approach to reducing the impact of false sharing. In essence, relaxed consistency models suffer delays due to false sharing only at synchronization points.
Reference: [15] <author> D. Mosberger. </author> <title> Memory Consistency Models. </title> <booktitle> ACM SIGOPS Operating Systems Re view, </booktitle> <volume> 27(1) </volume> <pages> 18-26, </pages> <month> January </month> <year> 1993. </year> <note> Relevant correspondence appears in Volume 27, Number 3; revised version available Technical Report 92/11, </note> <institution> Department of Computer Science, University of Arizona, </institution> <year> 1993. </year>
Reference-contexts: When combined with some knowledge of application semantics, these results suggest that the elimination of false sharing could result in order-of-magnitude improvements in performance for many programs. Relaxed models of memory consistency (as in DASH [14] or Munin [7]; see <ref> [15] </ref> for a survey) constitute one promising approach to reducing the impact of false sharing. In essence, relaxed consistency models suffer delays due to false sharing only at synchronization points. Other means of reducing false sharing include on-line adaptation of the block size [9], hand tuning [10], and smart compilers.
Reference: [16] <author> B. Nitzberg and V. Lo. </author> <title> Distributed Shared Memory: A Survey of Issues and Algo rithms. </title> <journal> Computer, </journal> <volume> 24(8) </volume> <pages> 52-60, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: For the purpose of maintaining coherence, memory is grouped into blocks. On a machine with hardware cache coherence, blocks are cache lines; on a machine with VM-based software coherence (i.e., a Non-Uniform Memory Architecture (NUMA) [2, 8, 13] or Distributed Shared Memory (DSM) <ref> [16] </ref> system), blocks are generally pages.
Reference: [17] <author> J. P. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <journal> ACM SIGARCH Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The applications are a scene-rendering program [11], a Presto [1] quicksort program, the Cholesky factorization program from the SPLASH suite <ref> [17] </ref>, and our successive over-relaxation program. The machine parameters model a sequentially consistent cache-coherent (CC) multiprocessor, and a VM-based software coherence system running on a non-uniform memory access (NUMA) machine resembling the Cray T3D. The "useful grouping" cost component can only result in increased cost with reduced block sizes.
Reference: [18] <author> J. Torrellas, M. S. Lam, and J. L. Hennessy. </author> <title> Shared Data Placement Optimizations to Reduce Multiprocessor Cache Miss Rates. </title> <booktitle> Proceedings of the 1990 International Conference on Parallel Processing, II software:266-270, </booktitle> <month> August </month> <year> 1990. </year> <month> 15 </month>
Reference-contexts: This is an informal statement of the false sharing problem. False sharing has been observed and commented on previously <ref> [10, 18] </ref>, but these studies do not provide sufficiently mathematically precise, convincing definitions. This paper considers several methods for transforming the intuitive idea of false sharing into a precise, usable definition that is able to show false sharing's performance impact on a particular program running on a particular machine.
References-found: 18

