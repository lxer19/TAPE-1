URL: http://www.cs.cornell.edu/Info/People/pearson/rsa.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/pearson/pearson.html
Root-URL: 
Email: pearson@cs.cornell.edu  
Title: A Parallel Implementation of RSA  
Author: David Pearson 
Date: July 22, 1996  
Address: Ithaca, NY 14853  
Affiliation: Computer Science Department Cornell University  
Abstract: Performing RSA public and private key operations fast is increasingly important. In this paper I describe an efficient implementation of RSA for a highly parallel computer. I present a new algorithm for modular multiplication using a residue number system (RNS) and a variation of Montgomery's method. The heart of the algorithm is a new method for converting from one RNS to another.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Markus A. Hitz and Erich Kaltofen. </author> <title> Integer division in residue number systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 44(8) </volume> <pages> 983-989, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: And finally, the RNS conversion algorithm allows us to fold the multiplies in steps 5 and 7 into the conversion process without any penalty, as we will see shortly. 3.3 Residue conversion Converting from one residue number system to another was discussed by Hitz and Kal-tofen <ref> [1] </ref> in the context of division in a RNS. Unfortunately, their algorithm is both fairly complex and computationally costly, essentially forming the full multiple-precision value 6 (in a mixed-radix system) of the number represented by the residues, then converting that to the output RNS.
Reference: [2] <author> Donald E. Knuth. </author> <booktitle> The Art of Computer Programming, volume 2: Seminumerical Algorithms. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1991. </year> <month> 9 </month>
Reference-contexts: These multiplies can all be performed and the results summed in parallel. Note that although there are asymptotically faster multiplication algorithms, including FFT and Karatsuba <ref> [2] </ref>, they do not appear to be competitive until much larger numbers are used than RSA [3]. Unlike the other methods, parallel multiply requires an architecture that supports fine-grained parallelism|it is not practical if the commu nication overhead between processors is much larger than the multiplication time.
Reference: [3] <author> C~ etin Ko~c. </author> <title> High-speed RSA implementation. </title> <type> Technical Report TR-201, </type> <institution> RSA Labo--ratories, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: These multiplies can all be performed and the results summed in parallel. Note that although there are asymptotically faster multiplication algorithms, including FFT and Karatsuba [2], they do not appear to be competitive until much larger numbers are used than RSA <ref> [3] </ref>. Unlike the other methods, parallel multiply requires an architecture that supports fine-grained parallelism|it is not practical if the commu nication overhead between processors is much larger than the multiplication time. A good parallel implementation of RSA will certainly incorporate the first three of these ideas.
Reference: [4] <author> F. Thomson Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays, Trees, Hypercubes. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: This interleaved method is inherently sequential, and has no obvious analogue in the parallel multiplication described above. There are parallel algorithms for division and remainder that are approximately as efficient as multiplication, and in fact use multiplication as a subroutine <ref> [4] </ref>. To divide two numbers one can use Newton's method to form an approximate reciprocal of the denominator and multiply it by the numerator. In the case of RSA, the reciprocal approximation need be computed only once.
Reference: [5] <author> Arjen Lenstra. </author> <title> Factoring. </title> <editor> In Gerard Tel and Paul Vitanyi, editors, </editor> <booktitle> Distributed Algorithms, </booktitle> <pages> pages 28-38. </pages> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1994. </year>
Reference-contexts: also secure traffic, is increasing even faster than computer speeds; and Moore's law is also helping the codebreakers and factoring algorithms, perhaps even more than it helps encryption, since they can use parallelism very effectively, as Lenstra and others have so convincingly demonstrated by breaking the original RSA challenge message <ref> [5] </ref>. Current sequential algorithms require O (n 3 ) time for private-key operations, so we use up our cycles with more costly operations. I view special-purpose hardware as simply another way to get parallelism.
Reference: [6] <author> Peter L. Montgomery. </author> <title> Modular multiplication without trial division. </title> <journal> Mathematics of computation, </journal> <volume> 44(170) </volume> <pages> 519-521, </pages> <month> April </month> <year> 1985. </year>
Reference-contexts: Montgomery <ref> [6] </ref> used an idea originally due to Hensel at the turn of the century, which is to zero instead the low-order bits of the product, and shift the remaining bits down.
Reference: [7] <author> Ronald L. Rivest, Adi Shamir, and Leonard Adleman. </author> <title> A method for obtaining digital signatures and public-key cryptosystems. </title> <journal> Communications of the ACM, </journal> <volume> 21(2) </volume> <pages> 120-126, </pages> <month> February </month> <year> 1978. </year>
Reference-contexts: I doubt there will be a time when every computer comes equipped with RSA in hardware, but I can imagine a day when every computer comes with several processors. 2 Opportunities for parallelism Recall the basic operations of RSA <ref> [7] </ref>: the public key (e; m) consists of an exponent and a modulus, and the encryption operation transforms a message t &lt; m into t e (mod m). The private key (d; m) uses the same modulus but a different exponent; the operation is the same, modular exponentiation.
Reference: [8] <author> Mark Shand and Jean Vuillemin. </author> <title> Fast implementations of RSA cryptography. </title> <booktitle> In Proceedings, 11th Symposium on Computer Arithmetic, </booktitle> <pages> pages 252-259. </pages> <publisher> IEEE, </publisher> <month> June </month> <year> 1993. </year>
Reference-contexts: A full decryption therefore requires about 15000 cycles, or 150 microseconds with a 100MHz clock. This is somewhat better than the fastest reported hardware implementation <ref> [8] </ref>, which can do a 512-bit private-key operation in about 850 sec (with a 40MHz clock|if the clock rate were 100MHz, the time would be 340 sec).
References-found: 8

