URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-92-1126/CS-TR-92-1126.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-92-1126/
Root-URL: http://www.cs.wisc.edu
Title: C flfl A Large-Grain, Object-Oriented, Data-Parallel Programming Language  
Author: James R. Larus, Brad Richards, and Guhan Viswanathan 
Note: 1 This work was supported by the National Science Foundation under grants CCR-9101035 and CDA 9024618.  
Date: November 24, 1992  
Address: 1210 West Dayton Street Madison, WI 53706 USA  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Pubnum: UW Technical Report #1126  
Abstract-found: 0
Intro-found: 0
Reference: [1] <author> Guy E. Blelloch. </author> <title> Vector Models for Data-Parallel Computing. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Hillis and Steele convincingly argued that data parallelism|which they defined as "simultaneous operations across large sets of data"|is a widely-applicable programming technique for massively parallel computers, by which they meant SIMD computers like the CM-2 [8]. Many data-parallel languages reflect the quirks of SIMD hardware <ref> [1, 12] </ref>. Nevertheless, data parallelism has grown beyond its SIMD origins because it offers the appealing advantages of a simple, explicable parallel semantics and nearly-deterministic, race-free 2 execution. 1 On the other hand, SIMD execution is a major limitation for executing programs that contain conditionals and loops with data-dependent behavior.
Reference: [2] <author> Guy E. Blelloch. NESL: </author> <title> A Nested Data-Parallel Language. </title> <type> Technical Report CMU-CS-92-103, </type> <institution> Department of Computer Science, Carnegie Mellon University, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: Although Paralation Lisp does not permit nested parallelism, subsequent work lifted this restriction [3]. 2.1.4 NESL Blelloch's language NESL is a strongly-typed, applicative data-parallel language designed to support nested parallelism <ref> [2] </ref>. In NESL, a programmer applies a pure function (i.e., without side-effects) to a one-dimensional aggregate (vector) that contains arbitrary elements. The function application results are collected into a new aggregate.
Reference: [3] <author> Guy E. Blelloch and Gary W. Sabot. </author> <title> Compiling Collection-Oriented Languages onto Massively Parallel Computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8 </volume> <pages> 119-134, </pages> <year> 1990. </year>
Reference-contexts: Although Paralation Lisp does not permit nested parallelism, subsequent work lifted this restriction <ref> [3] </ref>. 2.1.4 NESL Blelloch's language NESL is a strongly-typed, applicative data-parallel language designed to support nested parallelism [2]. In NESL, a programmer applies a pure function (i.e., without side-effects) to a one-dimensional aggregate (vector) that contains arbitrary elements. The function application results are collected into a new aggregate.
Reference: [4] <author> Timothy A. Budd. </author> <title> An APL Compiler for a Vector Processor. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 6(3) </volume> <pages> 297-313, </pages> <month> July </month> <year> 1984. </year>
Reference-contexts: If the operators are pure|compute a new result rather than modify an existing one|a compiler has great freedom to rearrange and execute them in parallel. Techniques such as APL dragthrough can eliminate many, if not all, intermediate results <ref> [4] </ref>. C ++ 's inlining facilities facilitate this process by making a function's code available at call sites. Unfortunately, C ++ does not provide a clean mechanism for a parallel function to create and return a composite result.
Reference: [5] <author> Craig M. Chase, Alex L. Cheung, Anthony P. Reeves, and Mark R. Smith. </author> <title> Paragon: A Parallel Programming Environment for Scientific Applications Using Communication Structures. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing (Vol. II Software), </booktitle> <pages> pages II-211-218, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: avoid conflicts by preventing an element from updating values in other elements, but read-write conflicts are still possible and may not be caught by the compiler 2 2.1.6 Paragon The Paragon programming environment is a collection of C++ classes that provide data-parallel array operations similar to those in Fortran 90 <ref> [5] </ref>. <p> Each element in an Aggregate object contains the member fields defined in the class. For example, the following declarations declare several 2-dimensional matrices of an indeterminate and two fixed sizes: class matrix -float value;[][]; class small_matrix -float value;- <ref> [5] </ref> [5]; class large_matrix -float value;- [100] [100]; The bounds of the indeterminate-sized matrix, matrix, are specified when allocating an instance: new matrix [100][100]. An Aggregate's rank is the number of dimensions listed in its class declaration. Rank is fixed by the declaration and cannot change. <p> Each element in an Aggregate object contains the member fields defined in the class. For example, the following declarations declare several 2-dimensional matrices of an indeterminate and two fixed sizes: class matrix -float value;[][]; class small_matrix -float value;- <ref> [5] </ref> [5]; class large_matrix -float value;- [100] [100]; The bounds of the indeterminate-sized matrix, matrix, are specified when allocating an instance: new matrix [100][100]. An Aggregate's rank is the number of dimensions listed in its class declaration. Rank is fixed by the declaration and cannot change.
Reference: [6] <author> Andrew A. Chien and William J. Dally. </author> <title> Concurrent Aggregates (CA). </title> <booktitle> In Second ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 187-196, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: Data parallel operations are invoked by sending a message to an aggregate, which redistributes the message to its elements, where the operation is actually applied. This model is similar to Chien's Concurrent Aggregates <ref> [6] </ref>.
Reference: [7] <author> Philip J. Hatcher and Michael J. Quinn. </author> <title> Data-Parallel Programming on MIMD Computers. </title> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: The original language described by Rose was C fl version 5. Hatcher and Quinn demonstrated that C fl , despite its origins, could be effectively compiled for shared-memory and message-passing MIMD parallel computers <ref> [7] </ref>. Their dialect, although called Dataparallel C (DPC), is nearly identical to C fl v5. Thinking Machines subsequently changed C fl into a new language, called C fl version 6 [16], that bears little resemblance to original C fl .
Reference: [8] <author> W. Daniel Hillis and Guy L. Steele, Jr. </author> <title> Data Parallel Algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1170-1183, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: Hillis and Steele convincingly argued that data parallelism|which they defined as "simultaneous operations across large sets of data"|is a widely-applicable programming technique for massively parallel computers, by which they meant SIMD computers like the CM-2 <ref> [8] </ref>. Many data-parallel languages reflect the quirks of SIMD hardware [1, 12].
Reference: [9] <author> Jenq Kuen Lee and Dennis Gannon. </author> <title> Object Oriented Parallel Programming Experiments and Results. </title> <booktitle> In Proceedings of Supercomputing 91, </booktitle> <pages> pages 273-282, </pages> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: semantically attractive, since it eliminates conflicts and data races due to imperative updates, but it shares implementation difficulties with functional languages. 2.1.5 PC++ Lee and Gannon described a new programming model called the distributed collection model and used it as the basis for PC++, their data parallel extension to C++ <ref> [9] </ref>. In their model, a collection is an aggregate that contains elements and is mapped to a set of processors by a distribution. Data parallel operations are invoked by sending a message to an aggregate, which redistributes the message to its elements, where the operation is actually applied.
Reference: [10] <author> Christos Papadimitriou. </author> <title> The Theory of Database Concurrency Control. </title> <publisher> Computer Science Press, </publisher> <year> 1986. </year>
Reference-contexts: While a parallel computation is running, its state is changed only by the computation itself, so it appears as if the computation executed sequentially. This suggests an analogy to database transactions, which use serializability as a correctness criterion <ref> [10] </ref>. Serializability, however, is not appropriate for data parallelism, because it requires equivalence to some sequential execution of the parallel tasks. For example, consider a stencil computation on a matrix. When a processor writes a location's average, seri-alizability requires neighboring computations to start computing anew with this value.
Reference: [11] <author> John R. Rose. </author> <title> C*: A C++-like Language for Data-Parallel Computation. </title> <type> Technical Report PL-87-8, </type> <institution> Thinking Machines Corporation, </institution> <month> December </month> <year> 1987. </year> <note> Appeared in Usenix C++ Workshop Proceedings. </note>
Reference-contexts: operations that does not include a higher-order apply function, issues of SIMD semantics and nested parallelism do not arise. 2.1.2 C fl C fl is a data-parallel programming language based on C (with a few ideas from C ++ ) that was originally designed for the SIMD CM-2 parallel computer <ref> [12, 11, 14, 15] </ref>. The original language described by Rose was C fl version 5. Hatcher and Quinn demonstrated that C fl , despite its origins, could be effectively compiled for shared-memory and message-passing MIMD parallel computers [7].
Reference: [12] <author> John R. Rose and Guy L. Steele Jr. </author> <title> C*: An Extended C Language for Data Parallel Programming. </title> <booktitle> In Proceedings of the Second International Conference on Supercomputing, </booktitle> <pages> pages 2-16, </pages> <address> Santa Clara, California, </address> <month> May </month> <year> 1987. </year>
Reference-contexts: Hillis and Steele convincingly argued that data parallelism|which they defined as "simultaneous operations across large sets of data"|is a widely-applicable programming technique for massively parallel computers, by which they meant SIMD computers like the CM-2 [8]. Many data-parallel languages reflect the quirks of SIMD hardware <ref> [1, 12] </ref>. Nevertheless, data parallelism has grown beyond its SIMD origins because it offers the appealing advantages of a simple, explicable parallel semantics and nearly-deterministic, race-free 2 execution. 1 On the other hand, SIMD execution is a major limitation for executing programs that contain conditionals and loops with data-dependent behavior. <p> operations that does not include a higher-order apply function, issues of SIMD semantics and nested parallelism do not arise. 2.1.2 C fl C fl is a data-parallel programming language based on C (with a few ideas from C ++ ) that was originally designed for the SIMD CM-2 parallel computer <ref> [12, 11, 14, 15] </ref>. The original language described by Rose was C fl version 5. Hatcher and Quinn demonstrated that C fl , despite its origins, could be effectively compiled for shared-memory and message-passing MIMD parallel computers [7].
Reference: [13] <author> Guy L. Steele Jr. </author> <title> Making Asynchronous Parallelism Safe for the World. </title> <booktitle> In Conference Record of the Seventeenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 218-231, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Another approach to preventing races is to prohibit conflicting dependences. Steele proposed a programming model in which operations that are not causally related (i.e., may execute in either order or simultaneously) must commute or are prohibited <ref> [13] </ref>. Violations of these semantics can only be detected during a program's execution. Steele described a complex memory system to detect these violations. Large-grain data parallelism takes a different approach to preventing interoperation data dependences from affecting a program's behavior.
Reference: [14] <institution> Thinking Machines Corporation. </institution> <note> C* Reference Manual, Version 4.0, </note> <month> August </month> <year> 1987. </year>
Reference-contexts: operations that does not include a higher-order apply function, issues of SIMD semantics and nested parallelism do not arise. 2.1.2 C fl C fl is a data-parallel programming language based on C (with a few ideas from C ++ ) that was originally designed for the SIMD CM-2 parallel computer <ref> [12, 11, 14, 15] </ref>. The original language described by Rose was C fl version 5. Hatcher and Quinn demonstrated that C fl , despite its origins, could be effectively compiled for shared-memory and message-passing MIMD parallel computers [7].
Reference: [15] <institution> Thinking Machines Corporation. </institution> <note> Supplement to C* Reference Manual, Version 4.3, </note> <month> May </month> <year> 1988. </year>
Reference-contexts: operations that does not include a higher-order apply function, issues of SIMD semantics and nested parallelism do not arise. 2.1.2 C fl C fl is a data-parallel programming language based on C (with a few ideas from C ++ ) that was originally designed for the SIMD CM-2 parallel computer <ref> [12, 11, 14, 15] </ref>. The original language described by Rose was C fl version 5. Hatcher and Quinn demonstrated that C fl , despite its origins, could be effectively compiled for shared-memory and message-passing MIMD parallel computers [7].
Reference: [16] <institution> Thinking Machines Corporation. </institution> <note> C* Reference Manual, Version 6.0 Pre-Beta, </note> <month> July </month> <year> 1990. </year> <month> 16 </month>
Reference-contexts: Their dialect, although called Dataparallel C (DPC), is nearly identical to C fl v5. Thinking Machines subsequently changed C fl into a new language, called C fl version 6 <ref> [16] </ref>, that bears little resemblance to original C fl . C fl , Version 5 C fl v5 extends the C language with domains. A domain declaration, like a C struct , specifies the fields in each aggregate element and, optionally, the aggregate's size.
References-found: 16

