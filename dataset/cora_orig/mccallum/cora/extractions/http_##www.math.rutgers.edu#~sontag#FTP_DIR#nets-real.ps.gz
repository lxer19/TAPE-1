URL: http://www.math.rutgers.edu/~sontag/FTP_DIR/nets-real.ps.gz
Refering-URL: http://www.math.rutgers.edu/~sontag/papers.html
Root-URL: 
Email: E-mail: siegelma@yoko.rutgers.edu  E-mail: sontag@control.rutgers.edu  
Title: ANALOG COMPUTATION VIA NEURAL NETWORKS contrast to classical computational models, the models studied here exhibit
Author: Hava T. Siegelmann Eduardo D. Sontag 
Note: In  
Address: New Brunswick, NJ 08903  New Brunswick, NJ 08903  
Affiliation: Department of Computer Science Rutgers University,  Department of Mathematics Rutgers University,  
Abstract: We pursue a particular approach to analog computation, based on dynamical systems of the type used in neural networks research. Our systems have a fixed structure, invariant in time, corresponding to an unchanging number of "neurons". If allowed exponential time for computation, they turn out to have unbounded power. However, under polynomial-time constraints there are limits on their capabilities, though being more powerful than Turing Machines. (A similar but more restricted model was shown to be polynomial-time equivalent to classical digital computation in the previous work [20].) Moreover, there is a precise correspondence between nets and standard non-uniform circuits with equivalent resources, and as a consequence one has lower bound constraints on what they can compute. This relationship is perhaps surprising since our analog devices do not change in any manner with input size. We note that these networks are not likely to solve polynomially NP-hard problems, as the equality "p = np " in our model implies the almost complete collapse of the standard polynomial hierarchy. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alspector J., R.B. Allen, </author> <title> "A neuromorphic VLSI learning system," </title> <booktitle> in Advanced Research in VLSI: Proceedings of the 1987 Stanford Conference, </booktitle> <editor> (P. Loseleben ed.,) </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1987: </year> <pages> 313-349. </pages>
Reference-contexts: Special purpose analog chips are being built to implement these solutions directly in hardware; see for instance <ref> [1] </ref>, [6]. However, very little work has been done in the direction of exploring the ultimate capabilities of such devices from a theoretical standpoint. <p> However, not every number in the range <ref> [0; 1] </ref> can appear in this manner. If the first digit to the right of the decimal point is 0, then the value of the encoding ranges in [0; 1 9 ]; if it is 2, the value ranges in [ 2 9 ; 3 9 ], and so forth. <p> The number cannot lie in any of the ranges 9 [ 2i1 9 ], for i = 1; 2; 3; 4. The second digit after the decimal point decides the possible range relative to the currently candidate range; see Figure 2. In summary, not every value in <ref> [0; 1] </ref> appears. The set of possible values is not continuous and has "holes". Such a set of values "with holes" is a Cantor set. <p> Let = f0; 2; 4; 6; 8g. Denote by C 9 the "Cantor 9-set," which consists of all those real numbers q which admit an expansion of the form q = i=1 9 i (6) with each ff i 2 . Let fl : IR ! <ref> [0; 1] </ref> be the function fl [x] := &gt; &lt; 0 if x &lt; 0 1 if x &gt; 1 : Let ffi : IR ! [0; 1] be the function ffi [x] := &gt; &lt; 0 if x &lt; 0 2 c if 0 x 1 (8) Note that, for <p> Let fl : IR ! <ref> [0; 1] </ref> be the function fl [x] := &gt; &lt; 0 if x &lt; 0 1 if x &gt; 1 : Let ffi : IR ! [0; 1] be the function ffi [x] := &gt; &lt; 0 if x &lt; 0 2 c if 0 x 1 (8) Note that, for each q = i=1 we may think of ffi [q] as the "select left" operation, since ffi [q] = ff 1 ; and of fl [q]
Reference: [2] <author> Atkinson K.E., </author> <title> An Introduction to Numerical Analysis, </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: Proof. We first measure the difference (error) between the activations of the corresponding internal processors of N 1 (n) and N at time t T (n). This calculation is analogous to that of the chop error in floating point computation, <ref> [2] </ref>.
Reference: [3] <author> Balcazar J.L., J. Diaz, J. Gabarro, </author> <title> Structural Complexity, </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1988. </year>
Reference-contexts: The class P (S), for a given sparse set S, is the class of all languages computed by Turing machines in polynomial time and using queries from the oracle S. From <ref> [3] </ref>, volume I, Theorem 5.5, pg 112, and Corollary 6.1, we conclude as follows: Corollary 6.2 net-p = [ S sparse P (S) : From [3], volume I, Theorem 5.11, pg 122 (originally, [14]), we conclude as follows: Corollary 6.3 net-exp includes all possible binary languages. <p> From <ref> [3] </ref>, volume I, Theorem 5.5, pg 112, and Corollary 6.1, we conclude as follows: Corollary 6.2 net-p = [ S sparse P (S) : From [3], volume I, Theorem 5.11, pg 122 (originally, [14]), we conclude as follows: Corollary 6.3 net-exp includes all possible binary languages. <p> Definition 8.8 Given a vector function f = ffi as above, we say that f is non-uniformly F (n)- approximable in time A f (n), if there is a Turing Machine M that computes T (n)-Chop (f) using an advice function (c.f. <ref> [3] </ref>, volume I, pg 99-115) in K [F (n), poly (T (n))]. 24 Example 8.9 Assume a generalized processor network D that computes in time T .
Reference: [4] <author> Blum L., M. Shub, and S. Smale, </author> <title> "On a theory of computation and complexity over the real numbers: NP completeness, recursive functions, and universal machines," </title> <journal> Bull. A.M.S. </journal> <volume> 21(1989): </volume> <pages> 1-46. </pages>
Reference-contexts: The work closest to ours seems to be that on real-number-based computation started by Blum, Shub and Smale (see e.g. <ref> [4] </ref>); we believe that our setup is far simpler, and is much more appropriate if one is interested in studying neural networks or distributed processor environments. <p> Of course, it follows from this that it is quite likely that net-np is strictly more powerful than net-p . 7 Complexity Over The Reals Blum, Shub, and Smale introduced in <ref> [4] </ref> a powerful model of computation over the real numbers. This model allows one possible formalization of the notion of analog computing.
Reference: [5] <author> Chandra A.K., L. Stockmeyer, U. Vishkin, </author> <title> "Constant depth reducibility," </title> <journal> SIAM J. Computing 13(1984): </journal> <pages> 423-439. </pages>
Reference-contexts: Using the carry-look-ahead method, [19], the summation can be computed via a subcircuit of depth O (log (T N )), width O (T 2 N ), and size O (T 2 N ). (This depth is of the same order as the lower bound of similar tasks, see <ref> [5] </ref>, [7].) As for the saturation, one gate, p u , is sufficient for the integer part.
Reference: [6] <author> Eberhardt S.P., T. Daud, D. A. Kerns, T. X. Brown, and A. P. Thakoor, </author> <title> "Competitive neural architecture for hardware solution to the assignment problem," </title> <booktitle> Neural Networks 4(1989): </booktitle> <pages> 431-442. </pages>
Reference-contexts: Special purpose analog chips are being built to implement these solutions directly in hardware; see for instance [1], <ref> [6] </ref>. However, very little work has been done in the direction of exploring the ultimate capabilities of such devices from a theoretical standpoint.
Reference: [7] <author> Furst M., J.B.Saxe, M. </author> <title> Sipser "Parity, circuits, and the polynomial-time hierarchy," </title> <booktitle> Proc. 22nd IEEE Symp. Foundations of Comp. Sci., </booktitle> <year> 1981: </year> <pages> 260-270. </pages>
Reference-contexts: Using the carry-look-ahead method, [19], the summation can be computed via a subcircuit of depth O (log (T N )), width O (T 2 N ), and size O (T 2 N ). (This depth is of the same order as the lower bound of similar tasks, see [5], <ref> [7] </ref>.) As for the saturation, one gate, p u , is sufficient for the integer part.
Reference: [8] <author> Giles, C. L., C.B. Miller, D. Chen, H.H. Chen, G.Z. Sun and Y.C. Lee, </author> <title> "Learning and Extracting Finite State Automata with Second-Order Recurrent Neural Networks," it Neural Computation, </title> <type> 4, </type> <year> 1992: </year> <pages> 393-405. </pages>
Reference-contexts: Note that networks with high order polynomials have appeared especially in the language recognition literature (see e.g. <ref> [8] </ref> and references there). We emphasize the relationship between these models: Let N 1 be neural network (of any order), which recognizes a language L in polynomial time. Then there is a first order network N 2 which recognizes the same language L in polynomial time.
Reference: [9] <author> Hong J.W., </author> <title> "On Connectionist Models," </title> <journal> Comm. on Pure and Applied Mathematics 41(1988): </journal> <pages> 1039-1050. </pages>
Reference-contexts: What changes in time are the activation values, or outputs of each processor, which are used in the next iteration. (A synchronous update model is used.) In this sense our model is very "uniform" in contrast with certain models used in the past, including those used in <ref> [9] </ref> or in the cellular automata literature, which allow the number of units to increase over time and often even the structure to change depending on the length of inputs being presented. The Meaning of (Non-Computable) Real Weights One may ask about the meaning of real weights.
Reference: [10] <author> Karp R.M., R. Lipton, </author> <title> "Turing Machines that take advice," </title> <journal> Enseign. Math. </journal> <volume> 28(1982): </volume> <pages> 191-209. </pages>
Reference-contexts: Thus, from <ref> [10] </ref> we conclude: Theorem 6 If net-np = net-p then the polynomial hierarchy collapses to 2 . The above result says that a theory of computation similar to that which arises in the classical case of Turing machine computation is also possible for our model of analog computation.
Reference: [11] <author> Kilian, J. and H.T. </author> <title> Siegelmann, </title> <booktitle> "On the power of sigmoid neural networks," Proc. Sixth ACM Workshop on Computational Learning Theory, </booktitle> <address> Santa Cruz, </address> <month> July, </month> <year> 1993. </year>
Reference-contexts: One step in understanding this issue, for first order nets, was taken in <ref> [11] </ref>.
Reference: [12] <author> MacLennan B.J., </author> <title> "Continuous symbol systems: The logic of connectionism," </title> <editor> in D.S. Levine and M. Aparicio IV (eds.), </editor> <title> Neural Networks for Knowledge Representation and Inference, </title> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, NJ, </address> <year> 1992. </year>
Reference-contexts: As pointed out by many authors, in particular in the work of <ref> [12] </ref>, the issue of understanding how macroscopic symbolic behavior arises from such a substrate is one of the most challenging ones in science.
Reference: [13] <author> Maass W., G. Schnitger, and E.D. Sontag, </author> <title> "On the computational power of sigmoid versus Boolean threshold circuits," </title> <booktitle> Proc. 32nd IEEE Symp. Foundations of Comp. Sci., </booktitle> <year> 1991: </year> <pages> 767-776. </pages>
Reference-contexts: In the related previous paper <ref> [13] </ref>, there were different models for each input size, the model allowed for no loops, and the emphasis was on comparisons with similar models made up of binary processors.
Reference: [14] <author> Muller D.E., </author> <title> "Complexity in electronic switching circuits," </title> <journal> IRE Trans. Electronic Comp. </journal> <volume> 5(1956): </volume> <pages> 15-19. </pages>
Reference-contexts: From [3], volume I, Theorem 5.5, pg 112, and Corollary 6.1, we conclude as follows: Corollary 6.2 net-p = [ S sparse P (S) : From [3], volume I, Theorem 5.11, pg 122 (originally, <ref> [14] </ref>), we conclude as follows: Corollary 6.3 net-exp includes all possible binary languages. Furthermore, most Boolean func tions require exponential time complexity. 20 Nondeterministic Neural Networks The concept of a nondeterministic circuit family is usually defined by means of an extra input, whose role is that of an oracle.
Reference: [15] <author> Muroga, S., </author> <title> Threshold Logic and its Applications, </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: Without loss of generality, one may assume that these constants can each be expressed in binary with at most n i log (n i ) bits; see <ref> [15] </ref>. If x i is on the bottom level, its input is the external input.
Reference: [16] <author> Parberry I., </author> <title> "Knowledge, understanding, and computational complexity," </title> <type> Technical Report CRPDC-92-2, </type> <institution> Center for Research in Parallel and Distributed Computing, Department of Computer Sciences, University of North Texas, </institution> <month> Feb. </month> <year> 1992. </year>
Reference-contexts: The qualifier "nonuniform" serves as a reminder that there is no requirement that circuit families be recursively described. It is this lack of classical computability that makes circuits a possible model of resource-bounded "computing," as emphasized in <ref> [16] </ref>. We will show that recurrent neural networks, although more "uniform" in the sense that they have an unchanging physical structure, share exactly the same power. If L is recognized by the formal net N in time T , we write N = L and T N = T . <p> It is interesting to point out that the work that we report here does allow for such non-Turing power, while keeping track of computational constraints -and thus embedding a possible answer to Penrose's challenge in more classical computer science. Note that Parberry, in <ref> [16] </ref>, also insists that possible non-Turing theories should take account of such constraints, though he suggests a different approach, namely the use of probabilistic computations within the theory of circuit complexity. 25 Finally, we remark that human cognition seems to be clearly based on "subsymbolic" or "analog" components and modes of
Reference: [17] <author> Parberry, I., </author> <title> The Computational and Learning Complexity of Neural Networks, </title> <type> draft. </type>
Reference-contexts: If x i is on the bottom level, its input is the external input. The function H is the threshold function H (z) = 1 z 0 (12) The relationships between threshold circuits and Boolean circuits are well studied. (See for example <ref> [17] </ref>.) They are known to be polynomial equivalent in size. We provide here an alternative direct relationship between threshold circuits and real networks, without passing through Boolean circuits. <p> Each gate of N 0 computes an addition of N m-bit numbers; then, it applies the function to it. Using a technique similar to the one provided in <ref> [17] </ref> pg 156-157, we show how 18 to simulate each gate of N 0 via a threshold circuit of size O (m) and depth 2. We achieve the simulation in two steps: First we add the N numbers and then we simulate the application of the saturation functions.
Reference: [18] <author> Penrose R., </author> <title> The Emperor's New Mind, </title> <publisher> Oxford University Press, Oxford, </publisher> <year> 1989. </year>
Reference-contexts: This is an easy consequence of the continuous dependence of the output on all the data. (A detailed proof involves defining precisely "perturbations of the activation function; we omit the details.) 2 9 Comments on Analog and non-Turing "Computation" In the recent, very popular -and very controversial- book <ref> [18] </ref>, Penrose has argued that the standard model of computing is not appropriate for modeling true biological intelligence. The author argues that physical processes, evolving at a quantum level, may result in computations which cannot be incorporated in Church's Thesis.
Reference: [19] <author> Savage J.E. </author> <title> The Complexity of Computing, New Tork, </title> <publisher> Wiley, </publisher> <year> 1976. </year>
Reference-contexts: Hardwiring the weights, we can say that each processor computes a sum of (T N + 2) (2T )-bit numbers. Using the carry-look-ahead method, <ref> [19] </ref>, the summation can be computed via a subcircuit of depth O (log (T N )), width O (T 2 N ), and size O (T 2 N ). (This depth is of the same order as the lower bound of similar tasks, see [5], [7].) As for the saturation, one
Reference: [20] <editor> Siegelmann H.T., E.D. Sontag, </editor> <booktitle> "On the computational power of neural nets," in Proc. Fifth ACM Workshop on Computational Learning Theory, </booktitle> <address> Pittsburgh, </address> <month> July </month> <year> 1992: </year> <pages> 440-449. </pages>
Reference-contexts: The robustness includes changes in the precise form of the activation function, in the weights of the network, and even an error in the update. In classical models of (digital) computation, this type of robustness can not even be properly defined. A Previous Related Result In our work <ref> [20] </ref>, we showed that if one restricts attention to nets all whose interconnection weights are rational numbers, which we call rational nets, then one obtains a model of computation that is polynomially related to Turing Machines. <p> We proceed as in <ref> [20] </ref> and define formal nets with two binary input lines. The first of these is a data line, and it is used to carry a binary input signal; when no signal is present, it defaults to zero.
Reference: [21] <author> Vergis A., K. Steiglitz, B. Dickinson, </author> <title> "The complexity of analog computation," in Math. </title> <booktitle> and Computers in Simulation 28(1986): </booktitle> <pages> 91-113. </pages>
Reference-contexts: Part of the problem is that, much interesting work notwithstanding, analog computation is hard to model, as difficult questions about precision of data and readout of results are immediately encountered |see for instance <ref> [21] </ref>, and the many references there. With the constraint of an unchanging structure, it is easy to see that classical McCulloch-Pitts|that is, binary|neurons would have no more power than finite automata, which is not an interesting situation from a theoretical complexity point of view.
Reference: [22] <author> Wolpert D., </author> <title> "A computationally universal field computer which is purely linear," </title> <institution> Los Alamos National Laboratory report LA-UR-91-2937. </institution>
Reference-contexts: This continuity in behavior is a basic characteristic of our model. In the paper <ref> [22] </ref>, the author studies a class of machines with just linear activation functions, and shows that this class is at least as powerful as any Turing Machine (and clearly has super-Turing capabilities as well). <p> It is essential in that model, however, that the number of "neurons" be allowed to be infinite |as a matter of fact, in <ref> [22] </ref> the number of such units is even uncountable| as the construction relies on using different neurons to encode different possible tape configurations in Turing Machines.
Reference: [23] <author> Yasuhara, A., </author> <title> Recursive Function Theory and Logic, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1971. </year> <month> 27 </month>
Reference-contexts: Analogously to Church's thesis of computability (see e.g. <ref> [23] </ref> p.98), our results suggest the following Thesis of Time-bounded Analog Computing: "Any reasonable analog computer will have no more power (up to polynomial time) than first-order recurrent networks." We consider dynamical systems -which we will call generalized processor networks- with far less restrictive structure than the recurrent neural network model
References-found: 23

