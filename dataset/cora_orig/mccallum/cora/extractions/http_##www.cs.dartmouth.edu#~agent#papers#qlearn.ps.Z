URL: http://www.cs.dartmouth.edu/~agent/papers/qlearn.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/~rus/report-dec96.html
Root-URL: http://www.cs.dartmouth.edu
Title: Q-Learning: A Tutorial and Extensions  
Author: George Cybenko Robert Gray Katsuhiro Moizumi 
Address: 8000 Cummings Hall  Hanover, NH 03755 USA  
Affiliation: Thayer School of Engineering  Dartmouth College  
Abstract: In the past decade, research in neurocomputing has been divided into two relatively well-defined tracks: one track dealing with cognition and the other with behavior. Cognition deals with organizing, classifying and recognizing sensory stimuli. Behavior is more dynamic, involving sequences of actions and changing interactions with an external environment. The mathematical techniques that apply to these areas, at least from the point of neurocomputing, appear to have been quite separate as well. The purpose of this paper is to give an overview of some recent powerful mathematical results in behavioral neurocomputing, specifically the concept of Q-learning due to C. Watkins, and some new extensions. Finally, we propose ways in which the mathematics of cognition and the mathematics of behavior can move closer to build more unified systems of information processing and action.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Bertsekas. </author> <title> Dynamic Programming and Optimal Control. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, MA, </address> <year> 1995. </year>
Reference-contexts: Only recently has the mathematics of controlled Markov chains been explored along with 2 solution techniques such as dynamic programming. A major breakthrough in learning optimal actions for such processes has been Watkins' Q-learning <ref> [3, 2, 4, 1] </ref>. In this paper, we give a quick overview of controlled Markov processes in Section 2. Section 3 presents Watkins' basic results and Section 4 extends those results to describe a system that learns and simultaneously converges to the optimal policy solution.
Reference: [2] <author> R. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1960. </year>
Reference-contexts: Only recently has the mathematics of controlled Markov chains been explored along with 2 solution techniques such as dynamic programming. A major breakthrough in learning optimal actions for such processes has been Watkins' Q-learning <ref> [3, 2, 4, 1] </ref>. In this paper, we give a quick overview of controlled Markov processes in Section 2. Section 3 presents Watkins' basic results and Section 4 extends those results to describe a system that learns and simultaneously converges to the optimal policy solution.
Reference: [3] <author> W.T. Miller III, R.S. Sutton, and P.J. Werbos. </author> <title> Neural Networks for Control. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: Only recently has the mathematics of controlled Markov chains been explored along with 2 solution techniques such as dynamic programming. A major breakthrough in learning optimal actions for such processes has been Watkins' Q-learning <ref> [3, 2, 4, 1] </ref>. In this paper, we give a quick overview of controlled Markov processes in Section 2. Section 3 presents Watkins' basic results and Section 4 extends those results to describe a system that learns and simultaneously converges to the optimal policy solution.
Reference: [4] <author> R.S. Sutton, A.G. Barto, and R.J. Williams. </author> <title> Reinforcement learning is direct adaptive control. </title> <journal> IEEE Control Systems Magazine, </journal> <pages> pages 19-22, </pages> <month> April </month> <year> 1992. </year> <month> 9 </month>
Reference-contexts: Only recently has the mathematics of controlled Markov chains been explored along with 2 solution techniques such as dynamic programming. A major breakthrough in learning optimal actions for such processes has been Watkins' Q-learning <ref> [3, 2, 4, 1] </ref>. In this paper, we give a quick overview of controlled Markov processes in Section 2. Section 3 presents Watkins' basic results and Section 4 extends those results to describe a system that learns and simultaneously converges to the optimal policy solution.
Reference: [5] <author> J. Tsitsiklis. </author> <title> Asynchronous stochastic approximation and Q-Learning. </title> <booktitle> Machine Learning, </booktitle> <address> 16:185--202, </address> <year> 1994. </year>
Reference-contexts: All other elements of Q i are merely copied from Q i1 without change. The parameters fi i ! 0 as i ! 1. Theorem (Watkins <ref> [6, 7, 5] </ref>) Let fi (x; ff)g be the set of indices for which the (x; ff) entry of the Q-tableau is updated. <p> If X i (x;ff) X fi 2 then Q i (x; ff) ! Q fl (x; ff) as i ! 1: Accordingly, ff i (x) = argmax ff Q i (x; ff) converges to the optimal action for state x. Proof See <ref> [7, 5] </ref>. This result is remarkable in that it demonstrates that a simple update rule on the Q-tableau results in a learning system which computes the optimal policy.
Reference: [6] <author> C.I.C.H. Watkins. </author> <title> Learning from delayed rewards. </title> <type> Ph.D. Dissertation, </type> <institution> University of Cambridge, </institution> <year> 1989. </year>
Reference-contexts: All other elements of Q i are merely copied from Q i1 without change. The parameters fi i ! 0 as i ! 1. Theorem (Watkins <ref> [6, 7, 5] </ref>) Let fi (x; ff)g be the set of indices for which the (x; ff) entry of the Q-tableau is updated.
Reference: [7] <author> C.I.C.H. Watkins and P. </author> <title> Dayan. </title> <journal> Q-Learning. Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292, </pages> <year> 1989. </year> <month> 10 </month>
Reference-contexts: All other elements of Q i are merely copied from Q i1 without change. The parameters fi i ! 0 as i ! 1. Theorem (Watkins <ref> [6, 7, 5] </ref>) Let fi (x; ff)g be the set of indices for which the (x; ff) entry of the Q-tableau is updated. <p> If X i (x;ff) X fi 2 then Q i (x; ff) ! Q fl (x; ff) as i ! 1: Accordingly, ff i (x) = argmax ff Q i (x; ff) converges to the optimal action for state x. Proof See <ref> [7, 5] </ref>. This result is remarkable in that it demonstrates that a simple update rule on the Q-tableau results in a learning system which computes the optimal policy.
References-found: 7

