URL: http://polaris.cs.uiuc.edu/reports/1449.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Email: moreira@watson.ibm.com fschouten,cdpg@csrd.uiuc.edu  
Title: The Performance Impact of Granularity Control and Functional Parallelism  
Author: Jos e E. Moreiray Dale Schoutenz Constantine Polychronopoulosz 
Keyword: dynamic scheduling, functional parallelism, task granularity, parallel processing, threads.  
Address: Yorktown Heights, NY 10598-0218  1308 W. Main St. Urbana, IL 61801-2307  
Affiliation: IBM T. J. Watson Research Center  Center for Supercomputing Research and Development, Coordinated Science Laboratory University of Illinois at Urbana-Champaign  
Abstract: CSRD Technical Report 1449 presented at the Eighth Workshop on Languages and Compilers for Parallel Computing Proceedings to be published by Springer-Verlag Abstract. Task granularity and functional parallelism are fundamental issues in the optimization of parallel programs. Appropriate granularity for exploitation of parallelism is affected by characteristics of both the program and the execution environment. In this paper we demonstrate the efficacy of dynamic granularity control. The scheme we propose uses dynamic runtime information to select the task size of exploited parallelism at various stages of the execution of a program. We also demonstrate that functional parallelism can be an important factor in improving the performance of parallel programs, both in the presence and absence of loop-level parallelism. Functional parallelism can increase the amount of large-grain parallelism as well as provide finer-grain parallelism that leads to better load balance. Analytical models and benchmark results quantify the impact of granularity control and functional parallelism. The underlying implementation for this research is a low-overhead threads model based on user-level scheduling. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Thomas Anderson, Edward Lazowska, and Henry Levy. </author> <title> The performance implications of thread management alternatives for shared-memory multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12), </volume> <month> December </month> <year> 1989. </year>
Reference-contexts: Granularity control can make a large difference in performance, especially when the available parallelism is not large (FFT2, SMM). In the case where granularity control performs worse (TRFD), the difference is small. 10 Related Work A variety of thread management issues and implementation alternatives are considered in <ref> [1] </ref>.
Reference: 2. <author> Thomas E. Anderson, Brian N. Bershad, Edward D. Lazowska, and Henry M. Levy. Sched-uler activations: </author> <title> Effective kernel support for the user-level management of parallelism. </title> <booktitle> In 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 95-109. </pages> <publisher> ACM Sigops, </publisher> <month> Octo-ber </month> <year> 1991. </year>
Reference-contexts: 1 Introduction The magnitude to which runtime overhead affects performance has been widely demonstrated <ref> [2, 3, 12] </ref>. In order to alleviate this problem [12] and other subsequent studies provided an environment that allows the user to control the number of parallel tasks a given parallel application generates. <p> In our scheme, however, the exact task size is determined at runtime and depends on program properties as well as runtime conditions. The latter depends largely on workload characteristics. Modern processor scheduling systems, such as Process Control [12] and Scheduler Activations <ref> [2] </ref>, space-share the processors in a multiprocessor to improve utilization and locality of reference. In this case, the number of processors allocated to a particular job depends on other jobs running at the same time. <p> The effect of granularity control. Scheduler Activations <ref> [2] </ref> and Process Control [12] both offer user-level scheduling and dynamic adaptation to changing environments. Scheduler activations supply an execution context that may be manipulated by a user-level scheduler or the kernel to allow for user-level scheduling with the flexibility and power of kernel-level threads.
Reference: 3. <author> Carl J. Beckmann. </author> <title> Hardware and Software for Functional and Fine Grain Parallelism. </title> <type> PhD thesis, </type> <institution> Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, </institution> <year> 1993. </year>
Reference-contexts: 1 Introduction The magnitude to which runtime overhead affects performance has been widely demonstrated <ref> [2, 3, 12] </ref>. In order to alleviate this problem [12] and other subsequent studies provided an environment that allows the user to control the number of parallel tasks a given parallel application generates. <p> Information on control and data dependences allows the exploitation of functional (task-level) parallelism, in addition to data (loop-level) parallelism. A brief summary of the properties of the HTG is given here and details can be found in <ref> [3, 8, 9, 10, 19] </ref>. 2 The hierarchical task graph is a directed acyclic graph HTG = (HV ; HE ) with unique nodes START and STOP 2 HV , the set of vertices. <p> The minimum size depends on the per task overhead of the system. The details of selecting an appropriate minimum size are beyond the scope of this paper. Static minimum granularity control can be implemented through task merging, a process described in <ref> [3] </ref>. Another aspect of static granularity control is to help prevent unnecessarily conservative dynamic granularity control decisions. When the overhead for task scheduling is negligible, as compared to the task size, there is little advantage in serializing the task.
Reference: 4. <author> Jyh-Herng Chow and Williams Ludwell Harrison. Switch-stacks: </author> <title> A scheme for microtasking nested parallel loops. </title> <booktitle> In Supercomputing 90, </booktitle> <pages> pages 190-199, </pages> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: Jade [21] is a high-level language designed for the exploitation of coarse-grain task (functional) parallelism. Concurrency is detected dynamically from data access specifications in each task. The language also supports the declaration of hierarchical tasks. Techniques to exploit nested parallelism include switch-stacks <ref> [4] </ref> and process con 15 trol blocks (PCBs) [13]. A PCB for a parallel loop is used to schedule the iterations of that loop. Reference [13] discusses heuristics that strike a balance between efficient allocation of PCBs versus load balancing problems that arise from barrier synchronization in nested parallel loops.
Reference: 5. <author> Peter Dinda, Thomas Gross, David O'Hallaron, Edward Segall, James Stichnoth, Jaspal Subhlok, Jon Webb, and Bwolen Yang. </author> <title> The CMU task parallel program suite. </title> <type> Technical Report CMU-CS-94-131, </type> <institution> School of Computer Science, Carnegie-Mellon University, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Two Dimensional Fast Fourier Transform (FFT2): Fortran code that performs a two dimensional fast Fourier transform was obtained from CMU's Task Parallel Suite <ref> [5] </ref> and hand-parallelized with the C++ nanoThreads library. There are two nested parallel loops that are exploited. There is also one routine performing matrix transpose that exploits functional parallelism. Perfect Benchmark TRFD (TRFD): A C++ version of the Perfect Club benchmark TRFD was implemented with the nanoThreads library.
Reference: 6. <author> Derek Eager and John Zahorjan. Chores: </author> <title> Enhanced run-time support for shared-memory parallel computing. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(1), </volume> <month> February </month> <year> 1993. </year>
Reference-contexts: The Psyche [16] system has facilities for user-level threads in which many tasks normally performed by the kernel, such as interrupt handling and preemptions, are handled at the user-level. Like nanoThreads, it relies on multiple virtual processors sharing the same address space. Chores <ref> [6] </ref> is a paradigm for the exploitation of loop and functional parallelism. It allows dynamic scheduling at the user-level and the expression of dependences between tasks without explicit synchronization. 11 Conclusion In this paper we have demonstrated the benefits of exploiting functional parallelism and performing dynamic granularity control.
Reference: 7. <author> Mike Galles and Eric Williams. </author> <title> Performance optimizations, implementation, and verification of the SGI Challenge multiprocessor. </title> <type> Silicon Graphics Technical Report. </type> <note> Available from http://www.sgi.com. </note>
Reference-contexts: However, information is less accurate than in the case of a centralized queue. A fully distributed queue was employed in the nanoThreads library. 6 Measurements Environment NanoThreads was implemented on an SGI Challenge shared-memory multiprocessor <ref> [7] </ref>. The machine on which measurements were obtained was configured with 32 R4400 processors and 2 Gigabytes of main memory. Each processor had a local cache with hardware support for cache coherence. Two different versions of nanoThreads were implemented on the SGI Challenge.
Reference: 8. <author> M. Girkar and C. D. Polychronopoulos. </author> <title> The HTG: An intermediate representation for programs based on control and data dependences. </title> <type> Technical Report 1046, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: Information on control and data dependences allows the exploitation of functional (task-level) parallelism, in addition to data (loop-level) parallelism. A brief summary of the properties of the HTG is given here and details can be found in <ref> [3, 8, 9, 10, 19] </ref>. 2 The hierarchical task graph is a directed acyclic graph HTG = (HV ; HE ) with unique nodes START and STOP 2 HV , the set of vertices.
Reference: 9. <author> Milind Girkar. </author> <title> Functional Parallelism: Theoretical Foundations and Implementation. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1992. </year>
Reference-contexts: Information on control and data dependences allows the exploitation of functional (task-level) parallelism, in addition to data (loop-level) parallelism. A brief summary of the properties of the HTG is given here and details can be found in <ref> [3, 8, 9, 10, 19] </ref>. 2 The hierarchical task graph is a directed acyclic graph HTG = (HV ; HE ) with unique nodes START and STOP 2 HV , the set of vertices.
Reference: 10. <author> Milind Girkar and Constantine Polychronopoulos. </author> <title> Automatic detection and generation of unstructured parallelism in ordinary programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(2), </volume> <month> April </month> <year> 1992. </year>
Reference-contexts: We call this subset of processors a partition and let this partition be time-variant, meaning that processors may be added or removed by the operating system during the execution of the job. The program model is the hierarchical task graph <ref> [10] </ref>, or HTG, combined with autoscheduling [19]. The HTG is an intermediate program representation that encapsulates data and control dependences at various levels of task granularity. This structure is used to generate autoscheduling code, which includes the scheduling operations directly within the program code. <p> Information on control and data dependences allows the exploitation of functional (task-level) parallelism, in addition to data (loop-level) parallelism. A brief summary of the properties of the HTG is given here and details can be found in <ref> [3, 8, 9, 10, 19] </ref>. 2 The hierarchical task graph is a directed acyclic graph HTG = (HV ; HE ) with unique nodes START and STOP 2 HV , the set of vertices.
Reference: 11. <author> Gene H. Golub and Charles F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, MD, </address> <year> 1989. </year>
Reference-contexts: The code simply performs a straightforward implementation of the definition of matrix multiply (c ij = P k a ik b kj ). The two outermost loops are run in parallel as doall loops. Strassen's Matrix Multiply (SMM): An implementation of Strassen's matrix multiply algorithm <ref> [11] </ref> was also written in C++ using the nanoThreads library. Strassen's algorithm is a recursive algorithm that breaks down a matrix into four quadrants, recursively performs multiplies on the quadrants and performs a combination of matrix adds and subtracts. The corresponding task graph is shown in Figure 4.
Reference: 12. <author> Anoop Gupta, Andrew Tucker, and Luis Stevens. </author> <title> Making effective use of shared-memory multiprocessors: The process control approach. </title> <type> Technical Report CSL-TR-91-475A, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <year> 1991. </year>
Reference-contexts: 1 Introduction The magnitude to which runtime overhead affects performance has been widely demonstrated <ref> [2, 3, 12] </ref>. In order to alleviate this problem [12] and other subsequent studies provided an environment that allows the user to control the number of parallel tasks a given parallel application generates. <p> 1 Introduction The magnitude to which runtime overhead affects performance has been widely demonstrated [2, 3, 12]. In order to alleviate this problem <ref> [12] </ref> and other subsequent studies provided an environment that allows the user to control the number of parallel tasks a given parallel application generates. Given a fixed number of resources, a user or ? This work was supported by the Office of Naval Research under grant N00014-94-1-0234. <p> In our scheme, however, the exact task size is determined at runtime and depends on program properties as well as runtime conditions. The latter depends largely on workload characteristics. Modern processor scheduling systems, such as Process Control <ref> [12] </ref> and Scheduler Activations [2], space-share the processors in a multiprocessor to improve utilization and locality of reference. In this case, the number of processors allocated to a particular job depends on other jobs running at the same time. <p> The effect of granularity control. Scheduler Activations [2] and Process Control <ref> [12] </ref> both offer user-level scheduling and dynamic adaptation to changing environments. Scheduler activations supply an execution context that may be manipulated by a user-level scheduler or the kernel to allow for user-level scheduling with the flexibility and power of kernel-level threads.
Reference: 13. <author> S. F. Hummel and E. Schonberg. </author> <title> Low-overhead scheduling of nested parallelism. </title> <journal> IBM J. Res. Develp., </journal> 35(5/6):743-765, Sept/Nov 1991. 
Reference-contexts: Concurrency is detected dynamically from data access specifications in each task. The language also supports the declaration of hierarchical tasks. Techniques to exploit nested parallelism include switch-stacks [4] and process con 15 trol blocks (PCBs) <ref> [13] </ref>. A PCB for a parallel loop is used to schedule the iterations of that loop. Reference [13] discusses heuristics that strike a balance between efficient allocation of PCBs versus load balancing problems that arise from barrier synchronization in nested parallel loops. <p> The language also supports the declaration of hierarchical tasks. Techniques to exploit nested parallelism include switch-stacks [4] and process con 15 trol blocks (PCBs) <ref> [13] </ref>. A PCB for a parallel loop is used to schedule the iterations of that loop. Reference [13] discusses heuristics that strike a balance between efficient allocation of PCBs versus load balancing problems that arise from barrier synchronization in nested parallel loops. Switch-stacks handle nested parallelism by actually swapping stacks between processors so no one processor is left idle waiting for another to finish.
Reference: 14. <author> D. E. Knuth. </author> <title> The Art of Computer Programming, Vol. 3 Sorting and Searching. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1973. </year>
Reference-contexts: This code was written from a high-level description of the functionality of TRFD. The code operates on four-dimensional triangular matrices, performing several multiplies and rearrangements. Only three loops were parallelized. Two of the parallel loops are perfectly nested. Quicksort (QUICK): Quicksort <ref> [14] </ref> is a recursive divide and conquer algorithm that sorts a vector of elements. It does so by partitioning the vector into two parts, based on the value of the elements, and recursively applying the same technique to both parts. In the average case, Quicksort is O (n log n).
Reference: 15. <author> S. L. Lyons, T. J. Hanratty, and J. B. MacLaughlin. </author> <title> Large-scale computer simulation of fully developed channel flow with heat transfer. </title> <journal> International Journal of Numerical Methods for Fluids, </journal> <volume> 13 </volume> <pages> 999-1028, </pages> <year> 1991. </year>
Reference-contexts: This benchmark was coded in Cedar Fortran, and the autoscheduling compiler automatically generated C++ autoscheduling code. The measurements were taken with a vector of size 1048576 elements. Computational Fluid Dynamics (CFD): This is the kernel of a Fourier-Chebyshev spectral computational fluid dynamics code <ref> [15] </ref>. The task graph representing this code is shown in Figure 4. It consists of 4 stages that operate on matrices of size 128 fi 128.
Reference: 16. <author> Brian D. Marsh, Michael L. Scott, Thomas J. LeBlanc, and Evangelos P. Markatos. </author> <title> First-class user-level threads. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 110-121, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Switch-stacks handle nested parallelism by actually swapping stacks between processors so no one processor is left idle waiting for another to finish. The Psyche <ref> [16] </ref> system has facilities for user-level threads in which many tasks normally performed by the kernel, such as interrupt handling and preemptions, are handled at the user-level. Like nanoThreads, it relies on multiple virtual processors sharing the same address space.
Reference: 17. <author> C. D. Polychronopoulos, M. B. Girkar, Mohammad R. Haghighat, C. L. Lee, B. Leung, and D. A. Schouten. </author> <title> Parafrase-2: An environment for parallelizing, partitioning, synchronizing, and scheduling programs on multiprocessors. </title> <journal> International Journal of High Speed Computing, </journal> <volume> 1(1) </volume> <pages> 45-72, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: The actual body of the tasks can be coded either in C++ or in Fortran. The other implementation is a nanoThreads compiler based on the Parafrase-2 compiler <ref> [17] </ref>. This compiler uses the HTG representation of a Fortran program to automatically generate C++ code with an embedded scheduler. In both cases, system-level shared-address processes are created at the beginning of the program execution to implement the virtual processors, while all the nanoThreads operations are performed by user-level code.
Reference: 18. <author> Constantine Polychronopoulos, Nawaf Bitar, and Steve Kleiman. nanothreads: </author> <title> A user-level threads architecture. </title> <booktitle> In Proceedings of the ACM Symposium on Principles of Operating Systems, </booktitle> <year> 1993. </year>
Reference-contexts: This evaluation of "(x) is also performed by the drive code for all the successors of a task and the enabled tasks are placed in a task queue. Autoscheduling and the HTG effectively implement a macro dataflow model of execution on a conventional multiprocessor. 3 NanoThreads NanoThreads <ref> [18] </ref> is a threads architecture that combines low-overhead threads and autoscheduling. Each nanoThread corresponds to an HTG task. Since the tasks effectively schedule themselves, via the exit blocks of the HTG, the user-level scheduler is a simple loop.
Reference: 19. <author> Constantine D. Polychronopoulos. Autoscheduling: </author> <title> Control flow and data flow come together. </title> <type> Technical Report 1058, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <year> 1990. </year>
Reference-contexts: We call this subset of processors a partition and let this partition be time-variant, meaning that processors may be added or removed by the operating system during the execution of the job. The program model is the hierarchical task graph [10], or HTG, combined with autoscheduling <ref> [19] </ref>. The HTG is an intermediate program representation that encapsulates data and control dependences at various levels of task granularity. This structure is used to generate autoscheduling code, which includes the scheduling operations directly within the program code. <p> Information on control and data dependences allows the exploitation of functional (task-level) parallelism, in addition to data (loop-level) parallelism. A brief summary of the properties of the HTG is given here and details can be found in <ref> [3, 8, 9, 10, 19] </ref>. 2 The hierarchical task graph is a directed acyclic graph HTG = (HV ; HE ) with unique nodes START and STOP 2 HV , the set of vertices.
Reference: 20. <author> Shankar Ramaswamy and Prithviraj Banerjee. </author> <title> Processor allocation and scheduling of macro dataflow graphs on distributed memory multicomputers by the PARADIGM compiler. </title> <booktitle> In International Conference on Parallel Processing, pages II:134-138, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Processors are released by the user on request of the kernel at safe points to prevent deadlock and similar problems. Considerable work has been done on the exploitation of functional and loop parallelism by the Paradigm <ref> [20] </ref> group at the University of Illinois. They rely on a Macro Dataflow Graph which is a directed acyclic task graph in which the nodes are weighted with the communication and computation time. Their approach performs static partitioning for data and computations on a distributed system to minimize total runtime.
Reference: 21. <author> Martin C. Rinard, Daniel J. Scales, and Monica S. Lam. </author> <title> Jade: A high-level machine-independent language for parallel programming. </title> <journal> IEEE Computer, </journal> <volume> 26(6) </volume> <pages> 28-38, </pages> <month> June </month> <year> 1993. </year> <title> This article was processed using the L A T E X macro package with LLNCS style 17 </title>
Reference-contexts: They rely on a Macro Dataflow Graph which is a directed acyclic task graph in which the nodes are weighted with the communication and computation time. Their approach performs static partitioning for data and computations on a distributed system to minimize total runtime. Jade <ref> [21] </ref> is a high-level language designed for the exploitation of coarse-grain task (functional) parallelism. Concurrency is detected dynamically from data access specifications in each task. The language also supports the declaration of hierarchical tasks. Techniques to exploit nested parallelism include switch-stacks [4] and process con 15 trol blocks (PCBs) [13].
References-found: 21

