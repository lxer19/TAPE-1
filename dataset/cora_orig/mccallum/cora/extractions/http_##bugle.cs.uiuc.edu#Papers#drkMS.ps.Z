URL: http://bugle.cs.uiuc.edu/Papers/drkMS.ps.Z
Refering-URL: http://bugle.cs.uiuc.edu/Papers/drkMS.html
Root-URL: http://www.cs.uiuc.edu
Title: c  
Author: flCopyright by David R. Kohr, Jr. 
Date: 1996  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> J. Adams, W. Brainerd, J. Martin, B. Smith, and J. Wagener. </author> <title> Fortran 90 Handbook. </title> <publisher> McGraw-Hill, </publisher> <year> 1992. </year>
Reference-contexts: Because there is just one global thread of control, a data-parallel language can often be derived from an existing sequential one. HPF in particular is an extension of the sequential language Fortran 90 <ref> [1] </ref>. A major aid to achieving determinism in data-parallel programs is that all parallelism is implicit: programmers only specify the sequence of operations to perform and provide directives for how data should be distributed among the processors.
Reference: [2] <author> N. Carriero and D. Gelernter. </author> <title> Linda in context. </title> <journal> Communications of the ACM, </journal> <volume> 32(4) </volume> <pages> 444-458, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Partly for this reason, coordination libraries have long been a popular tool for parallel programming. Tasks in earlier coordination libraries, such as Linda <ref> [2] </ref> and PVM [31], consisted of sequential programs written in traditional languages such as C and Fortran. More recent systems, such as Meta-Chaos [7] and HPF/MPI, permit coupling of tasks written in data-parallel languages.
Reference: [3] <author> K. Chandy, I. Foster, C. Koelbel, K. Kennedy, and C.-W. Tseng. </author> <title> Integrated support for task and data parallelism. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 8(2) </volume> <pages> 80-98, </pages> <year> 1994. </year>
Reference-contexts: Because these mechanisms involve extensions to the base data 5 parallel language, they also require modification of existing compilers, though in many cases it is sufficient simply to translate the new constructs directly into calls to runtime primitives <ref> [3] </ref>. Another approach relies on compiler analysis and optimization to partition programs into tasks and to synthesize inter-task communication. Girkar and Polychronopoulos develop a scheme for automatically detecting task parallelism [14].
Reference: [4] <author> A. Choudhary, B. Narahari, D. Nicol, and R. Simha. </author> <title> Optimal processor assignment for pipeline computations. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(4) </volume> <pages> 439-445, </pages> <year> 1994. </year>
Reference-contexts: If we know how the stage computation rates and inter-stage communication overheads vary with the number of processors assigned to the stages, then we can compute an assignment that is optimal in terms of minimizing pipeline latency or throughput <ref> [4, 30] </ref>. To determine the communication overheads, we could in principle measure the array transfer times 42 for every possible combination of stage sizes. However, this would be extremely cumbersome.
Reference: [5] <author> P. Dinda, T. Gross, D. O'Hallaron, E. Segall, J. Stichnoth, J. Subhlok, J. Webb, and B. Yang. </author> <title> The CMU task parallel program suite. </title> <type> Technical Report CMU-CS-94-131, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1994. </year>
Reference-contexts: Using this mechanism, one may compose new HPF/MPI applications without recompilation, in a manner similar to that of Unix process pipelines. 25 3.4 A Typical HPF/MPI Application Many applications that mix task and data parallelism, such as those in the CMU task-parallel benchmark suite <ref> [5] </ref>, consist of pipelines of data-parallel tasks. A simple representative of this class is the two-dimensional fast Fourier transform (2-D FFT), which comprises a pipeline of two stages.
Reference: [6] <author> P. Dinda and D. O'Hallaron. </author> <title> The impact of address relation caching on the performance of deposit model message passing. </title> <booktitle> In Proceedings of the Third Workshop on Languages, Compilers, and Run-time Systems for Scalable Computers, </booktitle> <pages> pages 213-226, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Our approach to performance modeling builds upon that used by Foster [8]. While it is true that many others have developed analytic models of communication performance, to our knowledge the only other model for HPF array redistribution in particular is that of Dinda and O'Hallaron <ref> [6] </ref>.
Reference: [7] <author> G. Edjlali, A. Sussman, and J. Saltz. </author> <title> Interoperability of data parallel runtime libraries with Meta-Chaos. </title> <type> Technical Report CS-TR-3633, </type> <institution> University of Maryland, Department of Computer Science, </institution> <year> 1996. </year>
Reference-contexts: Partly for this reason, coordination libraries have long been a popular tool for parallel programming. Tasks in earlier coordination libraries, such as Linda [2] and PVM [31], consisted of sequential programs written in traditional languages such as C and Fortran. More recent systems, such as Meta-Chaos <ref> [7] </ref> and HPF/MPI, permit coupling of tasks written in data-parallel languages. Use of coordination libraries brings many of the same benefits and drawbacks as explicit language constructs: more control over task behavior at the expense of more effort on the part of the programmer.
Reference: [8] <author> I. Foster. </author> <title> Designing and Building Parallel Programs. </title> <publisher> Addison-Wesley, </publisher> <year> 1995. </year> <month> 66 </month>
Reference-contexts: In contrast, HPF/MPI emphasizes highly optimized transfers of distributed HPF arrays, with programmers supplying optimization hints to the system using MPI facilities. Our approach to performance modeling builds upon that used by Foster <ref> [8] </ref>. While it is true that many others have developed analytic models of communication performance, to our knowledge the only other model for HPF array redistribution in particular is that of Dinda and O'Hallaron [6]. <p> Nonetheless the model is sufficiently accurate for analyzing the communication behavior of many applications on contemporary multicomputers, because these machines' networks typically induce little additional latency per "hop" and provide sufficient connectivity to mitigate congestion under many traffic patterns <ref> [8] </ref>. 5.2 A Performance Model for Array Transfers As in the case of sequential message passing programs, to tune the performance of programs that combine task- and data-parallelism, it is useful to know how long it takes to transfer arrays between tasks.
Reference: [9] <author> I. Foster, B. Avalani, A. Choudhary, and M. Xu. </author> <title> A compilation system that integrates High Performance Fortran and Fortran M. </title> <booktitle> In Proceedings of the 1994 Scalable High-Performance Computing Conference, </booktitle> <pages> pages 293-300, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: For example, the Opus language augments HPF with features for performing remote procedure calls between tasks [23], while in the HPF/FM system tasks exchange data via explicit message passing across communication channels <ref> [9] </ref>. Use of such constructs places some additional burden on programmers to identify and coordinate tasks, but in return they gain control over processor allocation and task interaction.
Reference: [10] <author> I. Foster, D. Kohr, Jr., R. Krishnaiyer, and A. Choudhary. </author> <title> Double standards: Bringing task parallelism to HPF via the Message Passing Interface. </title> <booktitle> In Proceedings of Supercomputing '96, </booktitle> <month> November </month> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: However, the problem of incorrect usage of library routines can be ameliorated somewhat by runtime argument checking. Further arguments in favor of the coordination library approach appear in <ref> [10] </ref>. Of the systems discussed above, HPF/MPI is most similar to Meta-Chaos. The chief difference is that Meta-Chaos provides a very general framework for exchanging arbitrary data 6 structures between tasks, by means of a user-defined canonicalization of structures. <p> Descriptions of additional applications that combine task and data parallelism appear in <ref> [10] </ref>, along with an empirical comparison of the performance of HPF and HPF/MPI versions of 2-D FFT and other applications. 27 Chapter 4 Coordination Mechanism Design The previous chapter described the semantics of inter-task array transfer using the HPF/MPI coordination library. <p> A better test of the coordination library approach to task parallelism is to determine how much it improves the performance of real data-parallel applications. Elsewhere we show some preliminary results obtained with an older version of the library <ref> [10] </ref>. In the future, we wish to conduct performance studies of additional HPF/MPI applications that are more diverse in structure and more complex than our current ones. We validated our array transfer performance model merely by checking that its predictions fit the benchmark measurements used to generate the model's parameters.
Reference: [11] <author> I. Foster, D. Kohr, Jr., R. Krishnaiyer, and A. Choudhary. </author> <title> MPI as a coordination layer for communicating HPF tasks. </title> <booktitle> In Proceedings of the 1996 MPI Developers Conference. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> July </month> <year> 1996. </year>
Reference-contexts: In many situations, other MPI features could provide performance improvements. For example, non-blocking transfer operations could permit tasks to overlap computation and communication. We sketch out some techniques for implementing non-blocking operations in <ref> [11] </ref>. Finally, development of some kinds of applications could be made easier if the coordination library supported other interaction mechanisms in addition to explicit message passing. For example, if the library supported dynamic attachment and detachment between tasks, it would be possible to employ client-server structures in HPF/MPI programs.
Reference: [12] <author> I. Foster, D. Kohr, Jr., R. Olson, S. Tuecke, and M. Xu. </author> <title> Point-to-point communication using migrating ports. </title> <booktitle> In Proceedings of the Third Workshop on Languages, Compilers, and Run-time Systems for Scalable Computers, </booktitle> <pages> pages 199-212. </pages> <publisher> Kluwer Academic Publishers, </publisher> <month> May </month> <year> 1995. </year>
Reference-contexts: For large array transfers, the library achieved a bandwidth approximately half that of the underlying communication substrate, which is consistent with results for other communication libraries on our target machine that, like HPF/MPI, perform buffer copying <ref> [12] </ref>. Independent inter-processor transfers proceeded in parallel at close to the maximum possible rate, except in cases of network congestion. The primary sources of overhead for array transfers were copies performed during extrinsic calls and for message assembly and disassembly.
Reference: [13] <author> G. Fox, M. Johnson, G. Lyzenga, S. Otto, J. Salmon, and D. Walker. </author> <title> Solving Problems on Concurrent Processors, volume 1. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1988. </year>
Reference-contexts: Because message passing mechanisms are extremely general and flexible, one can use them to solve efficiently a wide variety of parallel programming problems <ref> [13] </ref>. In particular, using message-passing techniques, one may design solutions based on other paradigms, including task or data parallelism. Unfortunately, the use of message passing for data-parallel programming requires the programmer to manage many tedious details, such as complex interprocessor data transfers and translation between local and global array indices. <p> It has long been recognized that message passing mechanisms are extremely general and flexible, and permit efficient solutions to a wide variety of parallel programming problems <ref> [13, 19] </ref>. In particular, message-passing techniques can be used to design programs that exploit task or data parallelism. As discussed above, HPF/MPI provides an interface that is based on the MPI standard for explicit message passing.
Reference: [14] <author> M. Girkar and C. Polychronopoulos. </author> <title> Automatic extraction of functional parallelism from ordinary programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(2) </volume> <pages> 166-178, </pages> <year> 1992. </year>
Reference-contexts: Another approach relies on compiler analysis and optimization to partition programs into tasks and to synthesize inter-task communication. Girkar and Polychronopoulos develop a scheme for automatically detecting task parallelism <ref> [14] </ref>. Ramaswamy et al. present techniques for optimal processor allocation and task scheduling based on inter-task data dependences and communication costs [25]. In some cases, programmers use directives to guide the compiler.
Reference: [15] <author> W. Gropp, E. Lusk, N. Doss, and A. Skjellum. </author> <title> A high-performance, portable implementation of the MPI Message Passing Interface standard. </title> <type> Technical Report ANL/MCS-TM-213, </type> <institution> Argonne National Laboratory, </institution> <year> 1996. </year>
Reference-contexts: However, many implementations of MPI, including portable libraries like MPICH <ref> [15] </ref> that are layered atop a vendor's proprietary communication library, 38 do not provide this performance enhancement. <p> We also successfully tested the communication safety mechanism of Section 4.7 with pghpf. To generate the performance results presented in the next chapter, we used the portable MPICH implementation of MPI <ref> [15] </ref> as the underlying sequential communication library.
Reference: [16] <author> T. Gross, D. O'Hallaron, and J. Subhlok. </author> <title> Task parallelism in a High Performance Fortran framework. </title> <journal> IEEE Parallel & Distributed Technology, </journal> <volume> 2(2) </volume> <pages> 16-26, </pages> <month> Fall </month> <year> 1994. </year>
Reference-contexts: In the case of task parallelism, however, the patterns of communication between tasks are often considerably simpler. For example, in many pipeline codes, inter-task communication consists of sending or receiving whole arrays <ref> [16] </ref>. Therefore, it is reasonable to consider mechanisms based on explicit message passing, provided that the primitives are sufficiently powerful that simple patterns of data movement can be expressed using a simple sequence of calls to the coordination library. <p> In some cases, programmers use directives to guide the compiler. The Fx compiler is an example: it combines traditional dataflow analysis of HPF source code with program transformation based on directives to generate pipelines of data-parallel tasks <ref> [16] </ref>. In general, compiler-based techniques shift much of the effort of task-level parallelization from the programmer to the compiler, but require very sophisticated compiler technology. Moreover, the mapping from program source to executable code may be complex, so that reasoning about program behavior at runtime is more difficult. <p> Sensor resolutions are often modest in size (perhaps 1024-by-1024 or less). When a program that combines task and data parallelism operates on such datasets, parallel efficiency considerations often dictate that to achieve the best performance, its tasks should be only moderate in size <ref> [16] </ref>. Therefore we did not run ping-pong on all 128 nodes of the SP, but instead restricted task sizes to powers of 2 in the range 1 P 16.
Reference: [17] <author> W. Hillis and G. Steele Jr. </author> <title> Data parallel algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1170-1183, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: One paradigm for parallel programming that has achieved wide acceptance is data parallelism <ref> [17] </ref>. In a data-parallel program, data structures, such as arrays, are distributed across a machine's processors. When a program performs an operation on a distributed data structure, all processors operate in parallel on a portion of the entire structure, communicating with each other as necessary to compute the final result.
Reference: [18] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year> <month> 67 </month>
Reference-contexts: Unfortunately, the use of message passing for data-parallel programming requires the programmer to manage many tedious details, such as complex interprocessor data transfers and translation between local and global array indices. Because compiler technology has been developed that is able to handle such chores automatically <ref> [18] </ref>, it is reasonable to expect that high-level languages such as HPF will eventually supplant the use of message passing in many data-parallel applications. In the case of task parallelism, however, the patterns of communication between tasks are often considerably simpler. <p> A complete description of HPF's features appears in [22]. To generate code for implicitly parallel array operations such as an assignment specified using FORALL, compilers typically follow the owner-computes rule: the processor to which an element is mapped computes the new value for the element <ref> [18] </ref>. This implies that any off-processor data needed for the computation must be transferred to the owner.
Reference: [19] <author> C. A. R. Hoare. </author> <title> Communicating sequential processes. </title> <journal> Communications of the ACM, </journal> <volume> 21(8) </volume> <pages> 666-677, </pages> <month> August </month> <year> 1978. </year>
Reference-contexts: It has long been recognized that message passing mechanisms are extremely general and flexible, and permit efficient solutions to a wide variety of parallel programming problems <ref> [13, 19] </ref>. In particular, message-passing techniques can be used to design programs that exploit task or data parallelism. As discussed above, HPF/MPI provides an interface that is based on the MPI standard for explicit message passing.
Reference: [20] <author> D. Kahaner, C. Moler, and S. Nash. </author> <title> Numerical Methods and Software. </title> <publisher> Prentice Hall, </publisher> <year> 1989. </year>
Reference-contexts: Qualitatively, this solution is a close "fit" to the experimental data, and tends to smooths out the effects of errors. To compute least-squares solutions, we use the standard technique of QR Factorization <ref> [20] </ref>. Unfortunately, we cannot simply apply QR Factorization to the full system Ax b to obtain a high-quality solution. This is because the system is poorly conditioned due to being poorly scaled: the coefficients of ~ t b span a wide range of magnitudes.
Reference: [21] <author> E. Kalns and L. Ni. </author> <title> Processor mapping techniques toward efficient data redistribution. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 6(12) </volume> <pages> 1234-1247, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: The problem of computing schedules for transferring HPF arrays when the transfers may involve redistribution has been studied in depth by many other researchers <ref> [21, 24, 32] </ref>. We do not introduce new techniques here for computing schedules; instead, we utilize those developed by Ramaswamy and Banerjee [24]. The techniques of Ramaswamy and Banerjee are based on a representation for array sections called FALLS (FAmiLies of Line Segments).
Reference: [22] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Sophisticated compilers can translate high-level data-parallel operations into efficient executable code for particular target machines, easing program development and enhancing portability. The data-parallel model has proven so popular that several languages for programming in this style have recently been approved as standards, including High Performance Fortran (HPF) <ref> [22] </ref>. Commercial implementations of HPF have appeared, spanning a variety of target platforms. <p> A complete description of HPF's features appears in <ref> [22] </ref>. To generate code for implicitly parallel array operations such as an assignment specified using FORALL, compilers typically follow the owner-computes rule: the processor to which an element is mapped computes the new value for the element [18].
Reference: [23] <author> P. Mehrotra and M. Haines. </author> <title> An overview of the Opus language and runtime system. </title> <type> Technical Report 94-39, </type> <institution> Institute for Computer Applications in Science and Engineering, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: One class of mechanisms extends data-parallel languages such as HPF with constructs for explicit definition of tasks and specification of inter-task communication. For example, the Opus language augments HPF with features for performing remote procedure calls between tasks <ref> [23] </ref>, while in the HPF/FM system tasks exchange data via explicit message passing across communication channels [9]. Use of such constructs places some additional burden on programmers to identify and coordinate tasks, but in return they gain control over processor allocation and task interaction.
Reference: [24] <author> S. Ramaswamy and P. Banerjee. </author> <title> Automatic generation of efficient array redistribution routines for distributed memory multicomputers. </title> <booktitle> In Proceedings of the Fifth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 342-349, </pages> <address> McLean, VA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: The problem of computing schedules for transferring HPF arrays when the transfers may involve redistribution has been studied in depth by many other researchers <ref> [21, 24, 32] </ref>. We do not introduce new techniques here for computing schedules; instead, we utilize those developed by Ramaswamy and Banerjee [24]. The techniques of Ramaswamy and Banerjee are based on a representation for array sections called FALLS (FAmiLies of Line Segments). <p> The problem of computing schedules for transferring HPF arrays when the transfers may involve redistribution has been studied in depth by many other researchers [21, 24, 32]. We do not introduce new techniques here for computing schedules; instead, we utilize those developed by Ramaswamy and Banerjee <ref> [24] </ref>. The techniques of Ramaswamy and Banerjee are based on a representation for array sections called FALLS (FAmiLies of Line Segments). <p> Therefore senders and receivers implicitly agree regarding the ordering of communication, without need for negotiation messages. The transfer ordering algorithm appears in an implementation by Ramaswamy of FALLS-based redistribution that was adapted for use in the HPF/MPI library, but is not mentioned in published descriptions of FALLS <ref> [24, 26] </ref>. In addition, we have been unable to find a prior description of this algorithm in the literature. Because there are subtle features of its operation that impact the performance of HPF/MPI, we describe the algorithm here. P = Q = 3 and all senders communicate with all receivers.
Reference: [25] <author> S. Ramaswamy, S. Sapatnekar, and P. Banerjee. </author> <title> A framework for exploiting data and functional parallelism on distributed memory multicomputers. </title> <type> Technical Report CRHC-94-10, </type> <institution> Center for Reliable and High-Performance Computing, University of Illinois, </institution> <year> 1994. </year>
Reference-contexts: Girkar and Polychronopoulos develop a scheme for automatically detecting task parallelism [14]. Ramaswamy et al. present techniques for optimal processor allocation and task scheduling based on inter-task data dependences and communication costs <ref> [25] </ref>. In some cases, programmers use directives to guide the compiler. The Fx compiler is an example: it combines traditional dataflow analysis of HPF source code with program transformation based on directives to generate pipelines of data-parallel tasks [16].
Reference: [26] <author> S. Ramaswamy, B. Simons, and P. Banerjee. </author> <title> Optimizations for efficient array redistribution on distributed memory multicomputers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 1996. Accepted for publication. </note>
Reference-contexts: Therefore senders and receivers implicitly agree regarding the ordering of communication, without need for negotiation messages. The transfer ordering algorithm appears in an implementation by Ramaswamy of FALLS-based redistribution that was adapted for use in the HPF/MPI library, but is not mentioned in published descriptions of FALLS <ref> [24, 26] </ref>. In addition, we have been unable to find a prior description of this algorithm in the literature. Because there are subtle features of its operation that impact the performance of HPF/MPI, we describe the algorithm here. P = Q = 3 and all senders communicate with all receivers.
Reference: [27] <author> J. Saltz, K. Crowley, R. Mirchandaney, and H. Berryman. </author> <title> Run-time scheduling and execution of loops on message passing machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(4) </volume> <pages> 303-312, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Note that our model is specific to the HPF/MPI design presented in the previous chapter: if we were to implement array transfers differently, we might need to change the model. For example, if we computed communication schedules using runtime resolution <ref> [27] </ref> instead of the FALLS-based algorithms, we would expect the corresponding array transfer performance model to include a term proportional to the array size N , because runtime resolution requires each processor to execute a loop that iterates once per element in an array. 5.3 The ping-pong Communication Benchmark The simple
Reference: [28] <author> M. Snir, S. Otto, S. Huss-Lederman, D. Walker, and J. Dongarra. </author> <title> MPI: The Complete Reference. </title> <publisher> MIT Press, </publisher> <year> 1996. </year> <month> 68 </month>
Reference-contexts: Early libraries for explicit message passing tended to offer similar sets of features but through different interfaces, and in many cases these libraries were not portable. Recently, the Message Passing Interface (MPI) has emerged as a standard for explicit message passing <ref> [28] </ref>, and has been implemented on most major platforms. <p> Using a simple task-parallel example, we now illustrate how basic message-passing operations are invoked in MPI. We also examine some MPI features for optimization and safety that are important to the programming interface and internal design of HPF/MPI. For a complete description of MPI, see <ref> [28] </ref>. We wish to construct a processing pipeline that contains the first P of the available Q processors, with each processor constituting a separate task. This pipeline processes N items of data. <p> When such a datatype is used as an argument to a send or receive routine, a gather or scatter operation is performed automatically by MPI <ref> [28] </ref>. When the communication library's implementation of gather-scatter operations is tightly integrated into the network interface driver, it is possible to avoid completely the use of a temporary contiguous buffer by copying data directly between the network interface and its non-contiguous source or location.
Reference: [29] <author> C. Stunkel, D. Shea, D. Grice, P. Hochschild, and M. Tsao. </author> <title> The SP1 high-performance switch. </title> <booktitle> In Proceedings of the 1994 Scalable High-Performance Computing Conference, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: However, the only active user process on each node was a benchmark process, so there was no additional CPU contention. The SP's interconnection network possesses a multistage topology constructed from 4-by-4 crossbar switches <ref> [29] </ref>. This topology provides several features that are useful for benchmark-ing. The switches partition the system into "frames" of 16 nodes each, such that transmission of a message does not utilize any hardware resources of a frame if none of the frame's processors are the sender or receiver.
Reference: [30] <author> J. Subhlok and G. Vondran. </author> <title> Optimal latency-throughput tradeoffs for data parallel pipelines. </title> <booktitle> In Eighth Annual ACM Symposium on Parallel Algorithms and Architecture, </booktitle> <month> June </month> <year> 1996. </year>
Reference-contexts: If we know how the stage computation rates and inter-stage communication overheads vary with the number of processors assigned to the stages, then we can compute an assignment that is optimal in terms of minimizing pipeline latency or throughput <ref> [4, 30] </ref>. To determine the communication overheads, we could in principle measure the array transfer times 42 for every possible combination of stage sizes. However, this would be extremely cumbersome.
Reference: [31] <author> V. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concurrency: Practice & Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: Partly for this reason, coordination libraries have long been a popular tool for parallel programming. Tasks in earlier coordination libraries, such as Linda [2] and PVM <ref> [31] </ref>, consisted of sequential programs written in traditional languages such as C and Fortran. More recent systems, such as Meta-Chaos [7] and HPF/MPI, permit coupling of tasks written in data-parallel languages.
Reference: [32] <author> R. Thakur, A. Choudhary, and G. Fox. </author> <title> Runtime array redistribution in HPF programs. </title> <booktitle> In Proceedings of the 1994 Scalable High-Performance Computing Conference, </booktitle> <pages> pages 309-316, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: The problem of computing schedules for transferring HPF arrays when the transfers may involve redistribution has been studied in depth by many other researchers <ref> [21, 24, 32] </ref>. We do not introduce new techniques here for computing schedules; instead, we utilize those developed by Ramaswamy and Banerjee [24]. The techniques of Ramaswamy and Banerjee are based on a representation for array sections called FALLS (FAmiLies of Line Segments).
Reference: [33] <institution> The Portland Group, Inc. </institution> <note> pghpf Reference Manual. 9150 SW Pioneer Ct., Suite H, Wilsonville, Oregon 97070. 69 </note>
Reference-contexts: As a result, we detected network contention only in a few experiments in which 47 many large messages were exchanged between two frames during each iteration of ping-pong (see Section 6.4). Our prototype HPF/MPI library operates with version 2.0 of the commercial HPF compilation system pghpf <ref> [33] </ref>, developed by the Portland Group, Inc. For access to distributed array data, we have implemented two different versions of HPF/MPI. One version, which we call "direct", operates directly upon arrays stored in pghpf's proprietary memory layout.
References-found: 33

