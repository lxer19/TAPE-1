URL: http://www.cs.cornell.edu/cs614-sp98/papers/al91.ps
Refering-URL: http://www.cs.cornell.edu/cs614-sp98/Readings.html
Root-URL: 
Title: Virtual Memory Primitives for User Programs  
Author: Andrew W. Appel and Kai Li 
Affiliation: Department of Computer Science Princeton University  
Pubnum: CS-TR-276-90  
Abstract: We survey several user-level algorithms that make use of page-protection techniques, and analyze their common characteristics, in an attempt to answer the question, "What virtual-memory primitives should the operating system provide to user processes, and how well do today's operating systems provide them?" 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Mike Accetta, Robert Baron, William Bolosky, David Golub, Richard Rashid, Avadis Tevanian, and Michael Young. </author> <title> Mach: A new kernel foundation for UNIX development. </title> <booktitle> In Proc. Summer Usenix, </booktitle> <month> July </month> <year> 1986. </year>
Reference-contexts: For a persistent store with transactions, it might be useful to pin a page [16] in core so that it is not written back to the backing store until the transaction is complete. The Mach external-pager interface <ref> [1] </ref> provides at least one facility which is lacking from the primitives we describe: the operating system can tell the client which pages are least-recently-used and (therefore) about to be paged out. The client might choose to destroy those pages rather than have them written to disk.
Reference: [2] <author> Andrew W. Appel. </author> <title> Garbage collection can be faster than stack allocation. </title> <journal> Information Processing Letters, </journal> <volume> 25(4) </volume> <pages> 275-279, </pages> <year> 1987. </year>
Reference-contexts: This technique requires trap, protN and unprot. But since the faults are quite rare (most processes don't use much stack space), efficiency is not a concern. The same technique can be used to detect heap overflow in a garbage-collected system <ref> [2] </ref>. Ordinarily, heap overflow in such a system is detect by a compare and conditional-branch performed on each memory allocation. By having the user process allocate new records in a region of memory terminated by a guard page, the compare and conditional-branch can be eliminated.
Reference: [3] <author> Andrew W. Appel. </author> <title> Simple generational garbage collection and fast allocation. </title> <journal> Software|Practice/Experience, </journal> <volume> 19(2) </volume> <pages> 171-183, </pages> <year> 1989. </year>
Reference-contexts: This checking can be done by special hardware [25, 35], or by compilers [34]. In the latter case, two or more instructions are required. Fortunately, non-initializing assignments are rare in Lisp, Smalltalk, and similar languages <ref> [25, 35, 30, 3] </ref> but the overhead of the instruction sequence for checking (without special hardware) is still on the order of 5-10% of total execution time. Virtual memory hardware can detect assignments to old objects.
Reference: [4] <author> Andrew W. Appel, John R. Ellis, and Kai Li. </author> <title> Real-time concurrent collection on stock multiprocessors. </title> <booktitle> SIG-PLAN Notices (Proc. SIGPLAN '88 Conf. on Prog. Lang. Design and Implementation), </booktitle> <volume> 23(7) </volume> <pages> 11-20, </pages> <year> 1988. </year>
Reference-contexts: We survey several algorithms so that we may attempt to draw general conclusions about what user programs require from the operating system and hardware. Concurrent garbage collection A concurrent, real-time, copying garbage collection algorithm can use the page fault mechanism to achieve medium-grain synchronization between collector and mutator threads <ref> [4] </ref>. The paging mechanism provides synchronization that is coarse enough to be efficient and yet fine enough to make the latency low. The algorithm is based on the Baker's sequential, real-time copying collector algorithm [6]. Baker's algorithm divides the memory heap into two regions, from-space and to-space. <p> Furthermore, the mutator and the collector must alternate; they cannot operate truly concurrently because they might simultaneously try to copy the same object to different places. Instead of checking every pointer fetched from memory, the concurrent collector <ref> [4] </ref> uses virtual-memory page protections to detect from-space memory references and to synchronize the collector and mutator threads. <p> This is what makes traditional disk paging efficient; in different ways, it makes the algorithms described here efficient as well. For example, the concurrent garbage collection algorithm must scan and copy the same amount of data regardless of the mutator's access pattern <ref> [4] </ref>, but the mutator's locality of reference reduces the fault-handling overhead. The "write barrier" in the generational collection algorithm, concurrent checkpointing, and persistent store algorithms takes advantage of locality if some small subset of objects accounts for most of the updates.
Reference: [5] <author> Malcom Atkinson, Ken Chisholm, Paul Cockshott, and Richard Marshall. </author> <title> Algorithms for a persistent heap. </title> <journal> Software|Practice and Experience, </journal> <volume> 13(3) </volume> <pages> 259-271, </pages> <year> 1983. </year>
Reference-contexts: Persistent stores A persistent store <ref> [5] </ref> is a dynamic allocation heap that persists from one program-invocation to the next. An execution of a program may traverse data structures in the persistent store just as it would in its own (in-core) heap.
Reference: [6] <author> H. G. Baker. </author> <title> List processing in real time on a serial computer. </title> <journal> Communications of the ACM, </journal> <volume> 21(4) </volume> <pages> 280-294, </pages> <year> 1978. </year>
Reference-contexts: The paging mechanism provides synchronization that is coarse enough to be efficient and yet fine enough to make the latency low. The algorithm is based on the Baker's sequential, real-time copying collector algorithm <ref> [6] </ref>. Baker's algorithm divides the memory heap into two regions, from-space and to-space. At the beginning of a collection, all objects are in from-space, and to-space is empty.
Reference: [7] <author> David L. Black, Richard F. Rashid, David B. Golub, Charles R. Hill, and Robert V. Brown. </author> <title> Translation lookaside buffer consistency: A software approach. </title> <booktitle> In Proc. 3rd Int'l Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 113-122, </pages> <year> 1989. </year>
Reference-contexts: To prevent this, it is necessary to flush the page from each TLB where it might reside. This "shootdown" can be done in software by interrupting each of the other processors and requesting it to flush the page from its TLB, or in hardware by various bus-based schemes <ref> [7, 32] </ref>. Software shootdown can be very expensive if there are many processors to interrupt. <p> large reserve of unused physical pages, then it can do its paging-out in batches (to replenish the reserve); this will amortize the shootdown cost over the entire batch. 2 Thus, while it has been claimed that software solutions work reasonably well but might need to be supplanted with hardware assist <ref> [7] </ref>, with batching it is likely that hardware would not be necessary. Optimal page size In many of the algorithms described here, page faults are handled entirely in the CPU, and the fault-handling time (exclusive of overhead) is a small constant times the page size.
Reference: [8] <author> David R. Cheriton. </author> <title> The vmp multiprocessor: Initial ex-perience, refinements and performance evaluation. </title> <booktitle> In Proceedings of the 14th Annual Symposium on Computer Architecture, </booktitle> <year> 1988. </year>
Reference-contexts: The various algorithms described here might perform best at different page sizes. The effect of a varying page size can be accomplished on hardware with a small page size. (In the VMP system, the translation buffer and the cache are the same thing, with a 128-byte line size <ref> [8] </ref>; this architecture might be well-suited to many of the algorithms described in this paper.) For prot and unprot operations, the small pages would be used; for disk paging, contiguous multi-page blocks would be used (as is now common on the Vax).
Reference: [9] <author> D. W. Clark and C. C. Green. </author> <title> An empirical study of list structure in Lisp. </title> <journal> IEEE Trans. Software Eng., </journal> <volume> SE-5(1):51-59, </volume> <year> 1977. </year>
Reference-contexts: In short, the information-theoretic entropy of the average word is small; furthermore, a garbage collector can be made to put objects that point to each other in nearby locations, thus reducing the entropy per word to as little as 7 bits <ref> [9] </ref>. By the use of a data-compression algorithm, then, a page of 32-bit words might be compressible to about one-quarter of a page.
Reference: [10] <author> Douglas W. Clark. </author> <title> Pipelining and performance in the VAX 8800 processor. </title> <booktitle> In Proc. 2nd Intl. Conf. Architectural Support for Prog. Lang. and Operating Systems, </booktitle> <pages> pages 173-179, </pages> <year> 1987. </year>
Reference-contexts: The previously-faulting instruction is re-executed, but this time it won't fault because it's storing to a different location. The behaviour is unacceptable on a highly-pipelined machine (unless, as on the VAX 8800 <ref> [10] </ref>, there is hardware for "undoing" those subsequent instructions or addressing-mode side-effects that have already completed). In fact, even on the Motorola 68020 the use of page faults to detect heap overflow is not reliable.
Reference: [11] <author> Brian Cook. </author> <title> Four garbage collectors for Oberon. </title> <type> Undergraduate thesis, </type> <institution> Princeton University, </institution> <year> 1989. </year>
Reference-contexts: At garbage-collection time the collector will need to scan the pages on the trap-list for possible pointers into the youngest generation. Variants of this algorithm have exhibited quite good performance <ref> [30, 11] </ref>: as heaps and memories get larger the this scheme begins to dominate other techniques [37]. This technique uses the trap, protN, and unprot features, or just dirty. <p> For example, if the client program must never be interrupted for more than a millisecond, then a fault-handler computation time of 500 microseconds doesn't leave room for an operating-system overhead of 1200 microseconds! (This issue gets more complicated when we consider multiple consecutive faults; see <ref> [11] </ref> for an analysis.) In order to compare virtual memory primitives on different architectures, we have normalized the measurements by processor speed. Figure 4 shows the number of adds each processor could have done in the time it takes to protect a page, fault, and unprotect a page.
Reference: [12] <author> Eric Cooper. </author> <type> personal communication, </type> <year> 1990. </year>
Reference-contexts: The client might choose to destroy those pages rather than have them written to disk. This would be particularly useful for data-compression paging, and extending addressibility. Also, in a system with garbage collection, the client might know that a certain region contains only garbage and can safely be destroyed <ref> [12] </ref>.
Reference: [13] <author> George Copeland, Michael Franklin, and Gerhard Weikum. </author> <title> Uniform object management. </title> <booktitle> In Advances in Database Technology|EDBT '90, </booktitle> <pages> pages 253-268. </pages> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: locking; such systems (sometimes called object-oriented databases) can be quickly traversed with fetch instructions but also can provide synchronization and locking; efficiency of access can be improved by using a garbage collector to group related objects on the same page, treat small objects differently than large objects, and so on <ref> [13] </ref>. These schemes requires the use of trap and unprot as well as file-mapping with copy-on-write (which, if not otherwise available, can be simulated using protN, un-prot, and map2.
Reference: [14] <author> A.L. Cox and R.J. Fowler. </author> <title> The implementation of a coherent memory abstraction on a numa multiprocessor: Experiences with platinum. </title> <booktitle> In Proceedings of the Twelfth Symposium on Operating Systems Principles, </booktitle> <pages> pages 32-44, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Alternatives to multiple-mapping are discussed in section 5. Shared virtual memory The access protection paging mechanism has been used to implement shared virtual memory on a network of computers, on a multicomputer without shared memories [21], and on a multiprocessor based on interconnection networks <ref> [14] </ref>. The essential idea of shared virtual memory is to use the paging mechanism to control and maintain single-writer and multiple-reader coherence at the page level. system. On a multicomputer, each node in the system consists of a processor and its memory.
Reference: [15] <author> Peter J. Denning. </author> <title> Working sets past and present. </title> <journal> IEEE Trans. Software Engineering, </journal> <volume> SE-6(1):64-84, </volume> <year> 1980. </year>
Reference-contexts: The memory mapping manager views its local memory as a big cache of the SVM address space for its associated processors. Like the traditional virtual memory <ref> [15] </ref>, the shared memory itself exists only virtually. A memory reference may cause a page fault when the page containing the memory location is not in a processor's current physical memory. When this happens, the memory mapping manager retrieves the page from either disk or the memory of another processor.
Reference: [16] <author> Jeffrey L. Eppinger. </author> <title> Virtual Memory Management for Transaction Processing Systems. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <month> February </month> <year> 1989. </year>
Reference-contexts: Other primitives There are other virtual memory primitives that operating systems can provide. For a persistent store with transactions, it might be useful to pin a page <ref> [16] </ref> in core so that it is not written back to the backing store until the transaction is complete.
Reference: [17] <author> S. Feldman and C. Brown. Igor: </author> <title> A system for program debugging via reversible execution. </title> <booktitle> ACM SIGPLAN Notices, Workshop on Parallel and Distributed Debugging, </booktitle> <volume> 24(1) </volume> <pages> 112-123, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: This method also applies to taking incremental checkpoints; saving the pages that have been changed since the last checkpoint. Instead of protecting all the pages with "read-only," the algorithm can protect only "dirtied" pages since the previous checkpoint. Feldman and Brown <ref> [17] </ref> implemented and measured a sequential version for a debugging system by using reversible executions. They proposed and implemented the system call dirty. This algorithm uses trap, prot1, protN, unprot, and dirty ; a medium pagesize may be appropriate.
Reference: [18] <author> R. Fitzgerald and R.F. Rashid. </author> <title> The integration of virtual memory management and interprocess communication in accent. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(2) </volume> <pages> 147-177, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: But virtual memory has been used for many other purposes. Operating systems can share pages between processes, make instruction-spaces read-only (and thus guaranteed re-entrant), make portions of memory zeroed-on-demand or copy-on-write, and so on <ref> [18] </ref>. In fact, there is a large class of "tricks" that operating systems can perform using the page protection hardware. Modern operating systems allow user programs to perform such tricks too, by allowing user programs to provide "handlers" for protection violations.
Reference: [19] <author> Ralph Johnson and Paul R. Wilson. </author> <type> personal communication, </type> <year> 1990. </year>
Reference-contexts: The idea of having short pointers in core and long pointers on disk, with a translation table for only that subset of objects used in one session, originated in the LOOM system of Smalltalk-80 [20]. The use of a page-fault mechanism to implement it is more recent <ref> [19] </ref>. This algorithm uses trap, unprot, prot1 or protN, and (in a multi-threaded environment) map2, and might work well with a smaller pagesize. Data-compression paging In a typical linked data structure, many words point to nearby objects; many words are nil.
Reference: [20] <author> Glenn Krasner. </author> <title> Smalltalk-80: Bits of History, Words of Advice. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1983. </year>
Reference-contexts: The idea of having short pointers in core and long pointers on disk, with a translation table for only that subset of objects used in one session, originated in the LOOM system of Smalltalk-80 <ref> [20] </ref>. The use of a page-fault mechanism to implement it is more recent [19]. This algorithm uses trap, unprot, prot1 or protN, and (in a multi-threaded environment) map2, and might work well with a smaller pagesize.
Reference: [21] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Alternatives to multiple-mapping are discussed in section 5. Shared virtual memory The access protection paging mechanism has been used to implement shared virtual memory on a network of computers, on a multicomputer without shared memories <ref> [21] </ref>, and on a multiprocessor based on interconnection networks [14]. The essential idea of shared virtual memory is to use the paging mechanism to control and maintain single-writer and multiple-reader coherence at the page level. system.
Reference: [22] <author> Kai Li, Jeffrey Naughton, and James Plank. </author> <title> Concurrent real-time checkpoint for parallel programs. </title> <booktitle> In Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 79-88, </pages> <address> Seattle, Wash-ington, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: This algorithm uses trap, prot1, and unprot; the trap-handler needs access to memory that is still protected from the client threads (map2), and a small pa gesize may be appropriate. Concurrent checkpointing The access protection page fault mechanism has been used successfully in making checkpointing concurrent and real-time <ref> [22] </ref>. This algorithm for shared-memory multiprocessors runs concurrently with the target program, interrupts the target program for small, fixed amounts of time and is transparent to the checkpointed program and its compiler.
Reference: [23] <author> Henry Lieberman and Carl Hewitt. </author> <title> A real-time garbage collector based on the lifetimes of objects. </title> <journal> Communications of the ACM, </journal> <volume> 23(6) </volume> <pages> 419-429, </pages> <year> 1983. </year>
Reference-contexts: They proposed and implemented the system call dirty. This algorithm uses trap, prot1, protN, unprot, and dirty ; a medium pagesize may be appropriate. Generational garbage collection An important application of memory protection is in generational garbage collection <ref> [23] </ref>, a very efficient algorithm that depends on two properties of dynamically allocated records in LISP and other programming languages: 1. Younger records are much more likely to die soon than older records.
Reference: [24] <author> Raymond A. Lorie. </author> <title> Physical integrity in a large segmented database. </title> <journal> ACM Trans. on Database Systems, </journal> <volume> 2(1) </volume> <pages> 91-104, </pages> <year> 1977. </year>
Reference-contexts: A database is a storage management system that may provide, among other things, locking of objects, transactions with abort/commit, checkpointing and recovery. The integration of virtual memory techniques into database implementations has long been studied <ref> [24, 31] </ref>. Compiled programs can traverse the data in their heaps very quickly and easily, since each access operation is just a compiled fetch instruction.
Reference: [25] <author> David A. Moon. </author> <title> Garbage collection in a large LISP system. </title> <booktitle> In ACM Symposium on LISP and Functional Programming, </booktitle> <pages> pages 235-246, </pages> <year> 1984. </year>
Reference-contexts: If it does, the from-space object is copied to to-space and the pointer updated; only then is the pointer returned to the mutator. This checking requires hardware support to be implemented efficiently <ref> [25] </ref>, since otherwise a few extra instructions must be performed on every fetch. Furthermore, the mutator and the collector must alternate; they cannot operate truly concurrently because they might simultaneously try to copy the same object to different places. <p> The only way that an older generation can point to a younger one is by an assignment to an already-existing record. To detect such assignments, each modification of a heap object must be examined to see whether it violates property 2. This checking can be done by special hardware <ref> [25, 35] </ref>, or by compilers [34]. In the latter case, two or more instructions are required. <p> This checking can be done by special hardware [25, 35], or by compilers [34]. In the latter case, two or more instructions are required. Fortunately, non-initializing assignments are rare in Lisp, Smalltalk, and similar languages <ref> [25, 35, 30, 3] </ref> but the overhead of the instruction sequence for checking (without special hardware) is still on the order of 5-10% of total execution time. Virtual memory hardware can detect assignments to old objects.
Reference: [26] <author> Mike O'Dell. </author> <title> Putting UNIX on very fast computers. </title> <booktitle> In Proc. Summer 1990 USENIX Conf., </booktitle> <pages> pages 239-246, </pages> <year> 1990. </year>
Reference-contexts: The signal handler can access machine registers completely synchronously, change the memory map or machine registers, and then restart the faulting instruction. However, on a highly pipelined machine there may be several outstanding page faults <ref> [26] </ref>, and many instructions after the faulting one may have written their results to registers even before the fault is noticed; instructions can be resumed, but not restarted. <p> This ... serves as the courier of an engraved invitation to Hell <ref> [26] </ref> If the algorithms described are indeed incompatible with fast, pipelined machines, it would be a serious 9 a a Methods a trap a prot1 a protN a unprot a map2 a dirty a pagesize a a a Concurrent GC a p p p p p a SVM a a a
Reference: [27] <author> John Ousterhout. </author> <title> Why aren't operating systems getting faster as fast as hardware? In Proc. </title> <booktitle> Summer 1990 USENIX Conf., </booktitle> <pages> pages 247-256, </pages> <year> 1990. </year>
Reference-contexts: Where we have the data, we also show the time for a trap-handler that does not change any memory protections; this would be useful for heap-overflow detection. The results are shown in Table 1. Note that this benchmark is not an "overall operating system throughput" benchmark <ref> [27] </ref> and should not be influenced by disk speeds; it is measuring the performance of CPU-handled virtual memory services for user-level programs.
Reference: [28] <author> Karin Petersen. </author> <type> personal communication, </type> <year> 1990. </year>
Reference-contexts: Of course, compressed pages could be sent out to disk after a long period of disuse. Of course, data-compression paging might be done inside the operating system transparently to the user process <ref> [28] </ref>. But since a garbage collector can move objects to minimize their entropy, much better results might be obtained if the user process can have some control over how and when compression is done. This algorithm requires trap, prot1 (or perhaps protN with careful buffering), trap, and unprot.
Reference: [29] <author> Paul Pierce. </author> <booktitle> The NX/2 Operating System, </booktitle> <pages> pages 51-57. </pages> <publisher> Intel Corporation, </publisher> <year> 1988. </year>
Reference-contexts: Figure 4 shows the number of adds each processor could have done in the time it takes to protect a page, fault, and unprotect a page. Our benchmark shows that there is a wide range of efficiency in implementing virtual memory primitives. Intel 80386-based machine running NX/2 operating system <ref> [29] </ref> (a simple operating system for the iPSC/2 hypercube multicomputer) is the best in our benchmark. Its normalized benchmark performance is about ten times better than the worst performer (Mach on the Sparcstation). Clearly, there is no inherent reason that these primitives must be slow.
Reference: [30] <author> Robert A. Shaw. </author> <title> Improving garbage collector performance in virtual memory. </title> <type> Technical Report CSL-TR-87-323, </type> <institution> Stanford University, </institution> <year> 1987. </year>
Reference-contexts: This checking can be done by special hardware [25, 35], or by compilers [34]. In the latter case, two or more instructions are required. Fortunately, non-initializing assignments are rare in Lisp, Smalltalk, and similar languages <ref> [25, 35, 30, 3] </ref> but the overhead of the instruction sequence for checking (without special hardware) is still on the order of 5-10% of total execution time. Virtual memory hardware can detect assignments to old objects. <p> Virtual memory hardware can detect assignments to old objects. If dirty is available, the collector can examine dirtied pages to derive pointers from older generations to younger generations and process them. In the absence of such a service, the collector can use the page protection mechanism <ref> [30] </ref>: the older generations can be write-protected so that any store into them will cause a trap. The user trap-handler can save the address of the trapping page on a list for the garbage collector; then the page must be unprotected to allow the store instruction to proceed. <p> At garbage-collection time the collector will need to scan the pages on the trap-list for possible pointers into the youngest generation. Variants of this algorithm have exhibited quite good performance <ref> [30, 11] </ref>: as heaps and memories get larger the this scheme begins to dominate other techniques [37]. This technique uses the trap, protN, and unprot features, or just dirty.
Reference: [31] <author> Michael Stonebraker. </author> <title> Virtual memory transaction management. </title> <journal> Operating Systems Review, </journal> <volume> 18(2) </volume> <pages> 8-16, </pages> <month> April </month> <year> 1984. </year>
Reference-contexts: A database is a storage management system that may provide, among other things, locking of objects, transactions with abort/commit, checkpointing and recovery. The integration of virtual memory techniques into database implementations has long been studied <ref> [24, 31] </ref>. Compiled programs can traverse the data in their heaps very quickly and easily, since each access operation is just a compiled fetch instruction.
Reference: [32] <author> Patricia J. Teller. </author> <title> Translation-lookaside buffer consistency. </title> <journal> IEEE Computer, </journal> <volume> 23(6) </volume> <pages> 26-36, </pages> <year> 1990. </year>
Reference-contexts: To prevent this, it is necessary to flush the page from each TLB where it might reside. This "shootdown" can be done in software by interrupting each of the other processors and requesting it to flush the page from its TLB, or in hardware by various bus-based schemes <ref> [7, 32] </ref>. Software shootdown can be very expensive if there are many processors to interrupt.
Reference: [33] <author> Charles Thacker, Lawrence Stewart, and Edwin Sat-terthwaite. Firefly: </author> <title> A multiprocessor workstation. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(8) </volume> <pages> 909-920, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: At this point the copying thread immediately copies the page and sets the access for the page to "read/write," and restarts the faulting thread. Several benchmark programs have been used to measure the performance of this algorithm on the DEC Firefly multiprocessors <ref> [33] </ref>. The measurements show that about 90% of the checkpoint work is executed concurrently with the target program while no thread is ever interrupted for more than .1 second at a time.
Reference: [34] <author> David Ungar. </author> <title> Generation scavenging: a non-disruptive high performance storage reclamation algorithm. </title> <journal> SIG-PLAN Notices (Proc. ACM SIGSOFT/SIGPLAN Software Eng. </journal> <volume> Symp. </volume> <booktitle> on Practical Software Development Environments), </booktitle> <volume> 19(5) </volume> <pages> 157-167, </pages> <year> 1984. </year>
Reference-contexts: To detect such assignments, each modification of a heap object must be examined to see whether it violates property 2. This checking can be done by special hardware [25, 35], or by compilers <ref> [34] </ref>. In the latter case, two or more instructions are required. Fortunately, non-initializing assignments are rare in Lisp, Smalltalk, and similar languages [25, 35, 30, 3] but the overhead of the instruction sequence for checking (without special hardware) is still on the order of 5-10% of total execution time.
Reference: [35] <author> David M. Ungar. </author> <title> The Design and Evaluation of a High Performance Smalltalk System. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1986. </year>
Reference-contexts: The only way that an older generation can point to a younger one is by an assignment to an already-existing record. To detect such assignments, each modification of a heap object must be examined to see whether it violates property 2. This checking can be done by special hardware <ref> [25, 35] </ref>, or by compilers [34]. In the latter case, two or more instructions are required. <p> This checking can be done by special hardware [25, 35], or by compilers [34]. In the latter case, two or more instructions are required. Fortunately, non-initializing assignments are rare in Lisp, Smalltalk, and similar languages <ref> [25, 35, 30, 3] </ref> but the overhead of the instruction sequence for checking (without special hardware) is still on the order of 5-10% of total execution time. Virtual memory hardware can detect assignments to old objects.
Reference: [36] <author> Paul R. Wilson. </author> <type> personal communication, </type> <year> 1989. </year>
Reference: [37] <author> Benjamin Zorn. </author> <title> Comparative Performance Evaluation of Garbage Collection Algorithms. </title> <type> PhD thesis, </type> <institution> University of California at Berkeley, </institution> <month> November </month> <year> 1989. </year> <month> 12 </month>
Reference-contexts: At garbage-collection time the collector will need to scan the pages on the trap-list for possible pointers into the youngest generation. Variants of this algorithm have exhibited quite good performance [30, 11]: as heaps and memories get larger the this scheme begins to dominate other techniques <ref> [37] </ref>. This technique uses the trap, protN, and unprot features, or just dirty.
References-found: 37

