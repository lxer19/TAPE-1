URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-438.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: dkroy@media.mit.edu  
Phone: (617) 253-0378, (617) 253-8874 (fax)  
Title: Multimodal Adaptive Interfaces  
Author: Deb Roy and Alex Pentland 
Date: October 16, 1997  
Address: 20 Ames Street, Rm. E15-388 Cambridge, MA 01239  
Affiliation: Perceptual Computing Group MIT Media Lab  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> T.M. Cover and J. A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: Similar to [3] we set the weight vectors of cluster l to the mutual information <ref> [1] </ref> between the observation of word cluster l and each attribute vector: w i ( j j V l ) j ) (4) where V l signifies the presence of an spoken word belonging to word class l.
Reference: [2] <author> G.W. Furnas, T.K. Landauer, L.M. Gomez, and S.T. Dumais. </author> <title> The vocabulary problem in human-system communications. </title> <journal> Communications of the Association for Compuring Machinery, </journal> <volume> 30 </volume> <pages> 964-972, </pages> <year> 1987. </year>
Reference-contexts: The problem with this method is that there is no set of actions and associated meanings which will match the expectations of all potential users of the system. For example Furnas et. al. <ref> [2] </ref> did a series of experiments on object naming and found that: 2 People use a suprisingly great variety of words to refer to the same thing.
Reference: [3] <author> A.L. Gorin, S.E. Levinson, and A. Sankar. </author> <title> An experiment in spoken language acquisition. </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> 2(1) </volume> <pages> 224-240, </pages> <month> january </month> <year> 1994. </year>
Reference-contexts: Similar to <ref> [3] </ref> we set the weight vectors of cluster l to the mutual information [1] between the observation of word cluster l and each attribute vector: w i ( j j V l ) j ) (4) where V l signifies the presence of an spoken word belonging to word class l. <p> Since the elements of the attributes are binary variables, we are able to use simple smoothed relative frequencies to estimate the probabilities in Equation 4 <ref> [3] </ref>. Equation 4 has some intuitively satisfying properties.
Reference: [4] <author> H. Hermansky and N. Morgan. </author> <title> Rasta processing of speech. </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <month> October </month> <year> 1994. </year>
Reference-contexts: The audio signal from the microphone is sampled using a 16-bit 16 kHz analog to digital converter and them processed using the Relative Spectral (RASTA) <ref> [4] </ref> algorithm. RASTA provides a spectral representation of the audio signal which is relatively invariant to background noise 1 .
Reference: [5] <author> Kai-Fu Lee and Hsiao-Wuen Hon. </author> <title> Speaker-independent phone recognition using hidden markov models. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> 37(11) </volume> <pages> 1641-1648, </pages> <month> November </month> <year> 1989. </year> <month> 16 </month>
Reference-contexts: The RNN produces a 40-dimensional phoneme probability vector every 10ms. Thirty nine of the outputs estimate the probability of the 39 phonemes which constitute spoken English. We use the 39 phoneme class grouping described in <ref> [5] </ref>. The final RNN output is the probability estimate for silence. We use this output to detect "speech events" (i.e. when the user says something) as described in the next section. 4.2.2 Speech Event Detection The phoneme recognizer runs continuously (there is no push-to-talk button).
Reference: [6] <author> Nelson Morgan and Herve Bourlard. </author> <title> An introduction to hybrid hmm/connectionist continuous speech recognition. </title> <journal> Signal Processing Magazine, </journal> <pages> pages 25-43, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: background noise. 2 i.e. the probability estimate of silence from the RNN is lower than the probability estimate of one of the other 39 RNN outputs 6 7 4.2.3 Converting Speech Events to Phoneme Strings We use the RNN outputs as emission probabilities within an Hidden Markov Model (HMM) framework <ref> [6] </ref>. When a speech event is detected, a Viterbi search is performed on the output from the neural network representing the speech event to find the most likely phoneme sequence for the event. The Viterbi search allows any sequence of the 39 phonemes or silence.
Reference: [7] <author> Alex Pentland. Smart desks, desks, and clothes. </author> <booktitle> In Proceedings of ICASSP, </booktitle> <pages> pages 171-174, </pages> <address> Munich, Germany, April 1997. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: 1 Introduction Our group is interested in creating human machine interfaces which use natural modalities such as vision and speech to sense and interpret a user's actions <ref> [7] </ref>. In this paper we describe recent work on multimodal adaptive interfaces which combine automatic speech recognition, computer vision for gesture tracking, and machine learning techniques. Speech is the primary mode of communication between people and should also be used in computer human communication. <p> We now describe the various components of our system which lead to the interaction described above. 4 A Multimodal Sensory Environment for Adap tive Interfaces We have created an environment to facilitate development of multimodal adaptive interfaces based on the smart desk environment <ref> [7] </ref>. In its current configuration, the user sits at a desk facing a 70" color projection screen. The screen is used to display a graphical embodiment of the interface and virtual objects (described in Section 5).
Reference: [8] <author> Lawrence R. Rabiner. </author> <title> A tutorial on hidden markov models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77(2) </volume> <pages> 257-285, </pages> <year> 1989. </year>
Reference-contexts: Finally, we can define the distance between the event and the reference string to be: d (ref; e) = log (p (ref j e)) log (p (phonemeloop j e)) (1) The details for computing the probability of an observation sequence through a HMM state space may be found in <ref> [8] </ref>.
Reference: [9] <author> B. Reeves and C. Nass. </author> <title> The Media Equation. </title> <publisher> Cambridge Univeristy Press, </publisher> <year> 1996. </year>
Reference-contexts: For the interface to be usable it is crucial that the protocol for teaching the machine be easy to use and not significantly burden the user. Reeves and Nass <ref> [9] </ref> have performed numerous studies of social interactions between humans and machines and have found that best way to design an interface is to ensure the the interface follows social conventions which people already know: 3 When media conform to social and natural rules...no instruction is necessary. <p> We believe that by embodying our system in an interactive character, we can evoke a set of assumptions from the user about what to expect from the interface. If the character can meet these expectations, then the interface may be understood and used with minimal instructions <ref> [9] </ref>. 6 Learning in the Interface The most natural way to deal with the problem of reference resolution is to learn the set of words and gestures which a user prefers to use, and also learn their semantics within the context of some application.
Reference: [10] <author> Tony Robinson. </author> <title> An application of recurrent nets to phone probability estimation. </title> <journal> IEEE Trans. Neural Networks, </journal> <volume> 5(3), </volume> <year> 1994. </year>
Reference-contexts: RASTA provides a spectral representation of the audio signal which is relatively invariant to background noise 1 . The RASTA coefficients are computed on 20ms frames of audio (recomputed every 10ms) and fed into a recurrent neural network (RNN) similar to the system described in <ref> [10] </ref> to produce phoneme probability estimates at a rate of 100Hz. <p> The duration and bigram transition statistics were estimated from the TIMIT training data set. Using the Viterbi search, our system recognizes phonemes with 68% accuracy on the standard speaker independent TIMIT recognition task. This performance is close to current state of the art recognition for context independent phoneme recognition <ref> [10] </ref>. 4.2.4 A Metric for Comparing Speech Events to Phoneme Strings In this section we define a distance metric for comparing a speech events to a reference phoneme string. Typically the phoneme string is generated by the Viterbi search although it may also be generated manually in some cases.
Reference: [11] <author> Richard Rose. </author> <title> Word Spotting from Continuous Speech Utterances, </title> <booktitle> chapter 13, </booktitle> <pages> pages 303-329. </pages> <publisher> Kluwer Academic, </publisher> <year> 1996. </year>
Reference-contexts: The reference phoneme string may be thought of as a hidden Markov model (HMM) which can generate arbitrary speech events. We can then compute a confidence measure that an event was generated by an HMM using standard Viterbi search. Following methods developed for keyword spotting confidence measures <ref> [11] </ref> we normalize this measure across utterance as follows. First we compute the log probability of an event e using a forced Viterbi alignment with phoneme transitions determined by the reference phoneme string. We denote this as log (p (ref erence j e)).
Reference: [12] <author> Stephanie Seneff and Victor Zue. </author> <title> Transcription and alignment of the timit database. </title> <booktitle> In Proceedings of the Second Symposium on Advanced Man-Machine Interface through Spoken Language, Oahu, Hawaii, </booktitle> <month> November </month> <year> 1988. </year>
Reference-contexts: The RNN has been trained using back propagation in time [13] on the TIMIT database <ref> [12] </ref> which is a database of phonetically labeled speech recorded from 630 adult male and female native English speakers from all the major dialect regions of the United States. The RNN produces a 40-dimensional phoneme probability vector every 10ms.
Reference: [13] <author> Paul Werbos. </author> <title> Backpropagation through time: what it does and how to do it. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78 </volume> <pages> 1150-1160, </pages> <year> 1990. </year> <month> 17 </month>
Reference-contexts: The RASTA coefficients are computed on 20ms frames of audio (recomputed every 10ms) and fed into a recurrent neural network (RNN) similar to the system described in [10] to produce phoneme probability estimates at a rate of 100Hz. The RNN has been trained using back propagation in time <ref> [13] </ref> on the TIMIT database [12] which is a database of phonetically labeled speech recorded from 630 adult male and female native English speakers from all the major dialect regions of the United States. The RNN produces a 40-dimensional phoneme probability vector every 10ms.
References-found: 13

