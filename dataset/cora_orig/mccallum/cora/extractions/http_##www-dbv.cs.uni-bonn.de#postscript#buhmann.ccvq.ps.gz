URL: http://www-dbv.cs.uni-bonn.de/postscript/buhmann.ccvq.ps.gz
Refering-URL: http://www-dbv.cs.uni-bonn.de/abstracts/buhmann.ccvq.html
Root-URL: http://cs.uni-bonn.de
Title: Vector Quantization with Complexity Costs  
Author: Joachim Buhmann Hans Kuhnel 
Keyword: Vector quantization, complexity costs, maximum entropy estimation, image compression, neural networks  
Note: published in IEEE Transactions on Information Theory  Supported by the German Federal Ministry of Science and Technology (ITR-8800-H1) and by the Air Force Office of Scientific Research (88-0274) (C. von der Malsburg, PI) while working at the  Supported by a graduate fellowship of the  
Address: P.O.Box 808, L-270, Livermore, CA 94550  Boltzmann Strae, D-8046 Garching, Fed. Rep. Germany  Los Angeles, Ca 90089. Present address: Rheinische  Romerstrae 164, D-5300 Bonn 1, Fed. Rep. Germany.  
Affiliation: Lawrence Livermore National Laboratory, Computational Physics Division,  Physik Department, T30, Technische Universitat Munchen,  Center for Neural Engineering, University of Southern California,  Friedrich-Wilhelms-Universitat, Institut fur Informatik II,  Technische Universitat Munchen.  
Email: jb@mozart.llnl.gov  kuehnel@physik.tu-muenchen.de  
Date: 39:1133-1145 (1993)  
Abstract: Vector quantization is a data compression method where a set of data points is encoded by a reduced set of reference vectors, the codebook. We discuss a vector quantization strategy which jointly optimizes distortion errors and the codebook complexity, thereby, determining the size of the codebook. A maximum entropy estimation of the cost function yields an optimal number of reference vectors, their positions and their assignment probabilities. The dependence of the codebook density on the data density for different complexity functions is investigated in the limit of asymptotic quantization levels. How different complexity measures influence the efficiency of vector quantizers is studied for the task of image compression, i.e., we quantize the wavelet coefficients of gray level images and measure the reconstruction error. Our approach establishes a unifying framework for different quantization methods like K-means clustering and its fuzzy version, entropy constrained vector quantization or topological feature maps and competitive neural networks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. C. Ahalt, A. K. Krishnamurthy, P. Chen, and D. E. Melton. </author> <title> Competitive learning algorithms for vector quantization. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 277-290, </pages> <year> 1990. </year>
Reference-contexts: It should be mentioned that algorithms which minimize the above cost function under the constraint of unique assignment are also known as "hard clustering" algorithms. In contrast, procedures which return continuous assignments M iff 2 <ref> [0; 1] </ref> of a data point to several "cluster centers", are called "soft" or "fuzzy clustering" algorithms [17, 4]. 2 How large should the "magic" number K of reference vectors be? It is clear that the result of the optimization of cost function (1) depends on the number of codebook vectors <p> the optimal codebook size but the resulting codebooks have comparable quantization costs to codebooks found in batch optimization. 9 Discussion Vector quantization and data clustering have a wide spectrum of engineering applications ranging from data compression for transmission and storage purposes to image and speech processing for pattern recognition tasks <ref> [1] </ref>. When we assign partitions of a data set to a reduced set of reference vectors we have to make a compromise between the simplicity 19 and precision of the resulting representation, e.g., between the size of the codebook and the distortion error due to data quantization.
Reference: [2] <author> A. G. Andreou, K. A. Boahen, P. O. Pouliquen, A. Pavasovic, R. E. Jenkins, and K. Strohbehn. </author> <title> Current mode subthreshold MOS circuits for analog VLSI neural systems. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2 </volume> <pages> 205-213, </pages> <year> 1991. </year>
Reference-contexts: The structure of the reestimation equations suggests hardware implementations in analog VLSI as they are known from neural network research. Complexity optimized vector quantization maps onto a two layer neural network with a winner-take-all architecture as discussed in [6]. Hardware implementations of such network architectures have been successfully tested <ref> [2, 31] </ref>. Our study of entropy optimized quantization of wavelet transformed gray level images revealed the interesting fact that entropy optimized codebooks reproduce sparse image features like edges more faithfully than the conventional K-means clustering approach.
Reference: [3] <author> G. Ball and D. Hall. </author> <title> A clustering technique for summarizing multivariate data. </title> <journal> Behavioral Sciences, </journal> <volume> 12 </volume> <pages> 153-155, </pages> <year> 1967. </year>
Reference-contexts: Algorithms like K-means clustering assume that the size of the codebook is determined a priori, i.e., that the number K of reference vectors with K o N is prespecified. Other procedures like ISODATA <ref> [3] </ref> stop adding new reference vectors as soon as the residual distortion error falls below a fixed threshold.
Reference: [4] <author> J. C. Bezdek. </author> <title> A convergence theorem for the fuzzy ISODATA clustering algorithms. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 2(1) </volume> <pages> 1-8, </pages> <year> 1980. </year>
Reference-contexts: In contrast, procedures which return continuous assignments M iff 2 [0; 1] of a data point to several "cluster centers", are called "soft" or "fuzzy clustering" algorithms <ref> [17, 4] </ref>. 2 How large should the "magic" number K of reference vectors be? It is clear that the result of the optimization of cost function (1) depends on the number of codebook vectors K or, more generally speaking, on the complexity of the codebook.
Reference: [5] <author> L. Bobrowski and J. C. Bezdek. </author> <title> C-means clustering with the l 1 and l 1 norms. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 21(3) </volume> <pages> 545-554, </pages> <year> 1991. </year>
Reference-contexts: The Minkowski l p -norms D (x; y) = s=1 ! 1=p offer another alternative to quantify distortion errors <ref> [5] </ref>. Both measures (6) and (7) leave E K (fM iff g) invariant under permutations of the reference vectors y ff .
Reference: [6] <author> J. Buhmann and H. Kuhnel. </author> <title> Complexity optimized data clustering by competitive neural networks. Neural Computation, </title> <publisher> 5(1):(in press), </publisher> <year> 1992. </year>
Reference-contexts: The structure of the reestimation equations suggests hardware implementations in analog VLSI as they are known from neural network research. Complexity optimized vector quantization maps onto a two layer neural network with a winner-take-all architecture as discussed in <ref> [6] </ref>. Hardware implementations of such network architectures have been successfully tested [2, 31]. Our study of entropy optimized quantization of wavelet transformed gray level images revealed the interesting fact that entropy optimized codebooks reproduce sparse image features like edges more faithfully than the conventional K-means clustering approach. <p> The sharp appearence of the reconstruction in Fig. 3b supports this finding. In a related study we have extended the maximum entropy approach for vector quantization to the case of supervised data clustering <ref> [8, 6] </ref>. An additional cost term is used to penalize partitionings of data space which are in conflict with a priori known class knowledge. The respective algorithm is implemented by a three layer neural network with classification units in the third layer [6]. <p> An additional cost term is used to penalize partitionings of data space which are in conflict with a priori known class knowledge. The respective algorithm is implemented by a three layer neural network with classification units in the third layer <ref> [6] </ref>. We consider the similarity of the discussed algorithms and the underlying reestimation equations for codebook parameters with neural network systems as a very fruitful direction for future research which might not only produce better information processing systems but might also lead to a more fundamental understanding of perception.
Reference: [7] <author> J. Buhmann and H. Kuhnel. </author> <title> Complexity optimized vector quantization: A neural network approach. </title> <editor> In J. Storer, editor, </editor> <booktitle> Data Compression Conference '92, </booktitle> <pages> pages 12-21, </pages> <address> Los Alamitos, CA, 1992. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: As a "real-world" application of the new vector quantization algorithm we discuss the entropy optimized compression of wavelet decomposed images in Section 7 (see also <ref> [7] </ref>). This quantization scheme preserves psychophysically important image features like edges more accurately than the K-means clustering algorithm. <p> In this section we use the image compression task to compare the efficiency of the K-means clustering scheme (C ff = 1=p ff ) with entropy optimized vector quantization (C ff = log p ff ). The compression results have been published in part <ref> [7] </ref>. The images are decomposed with the wavelet algorithm of Mallat [36] which uses quadrature mirror filtering for the subband decomposition. The partitioning of the Fourier plane by Mallat's algorithm is shown in Fig. 2a.
Reference: [8] <author> J. Buhmann and H. Kuhnel. </author> <title> Unsupervised and supervised data clustering with competitive neural networks. </title> <booktitle> In IJCNN International Conference on Neural Networks, </booktitle> <address> Baltimore, pages IV-796 - IV-801. </address> <publisher> IEEE, </publisher> <year> 1992. </year>
Reference-contexts: The sharp appearence of the reconstruction in Fig. 3b supports this finding. In a related study we have extended the maximum entropy approach for vector quantization to the case of supervised data clustering <ref> [8, 6] </ref>. An additional cost term is used to penalize partitionings of data space which are in conflict with a priori known class knowledge. The respective algorithm is implemented by a three layer neural network with classification units in the third layer [6].
Reference: [9] <author> P. A. Chou, T. Lookabaugh, and R. M. Gray. </author> <title> Entropy-constrained vector quantization. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 37 </volume> <pages> 31-42, </pages> <year> 1989. </year>
Reference-contexts: The data assignments to reference vectors are estimated in the maximum entropy sense, a strategy first proposed by Rose et al. [38, 39] for K-means clustering. The special case of jointly minimizing suitable distortion costs with constrained entropy of the codebook vectors has been discussed by Chou et al. <ref> [9] </ref>. The traditional VQ design strategy, as followed in the well-known K-means clustering algorithm [35] or the LBG algorithm by Linde et al. [32], exclusively relies on the distortion errors for placing the reference vectors. <p> For example, entropic costs for codebook design have been shown to yield superior quantization performance in speech compression <ref> [9, 26] </ref>. We, therefore, suggest to extend the cost function (1) by an application dependent complexity measure which has to reflect the inherent costs of too complex vector quantizers. The complexity term limits the number of reference vectors. <p> Lagrangian optimization of a vector quantization cost function with entropic complexity costs is extensively discussed in <ref> [9] </ref>. We will apply entropy optimized quantization to the problem of image compression in Section 7. <p> The random ordering of codebook vectors ff fl in step 1 can be replaced by an application dependent heuristics, e.g., to order the clusters according to size. The proposed algorithm is a generalization of the vector quantization algorithm discussed by Chou et al. <ref> [9] </ref> in two respects: (i) we optimize the codebook size K by a systematic search for the lowest free energy F K ; (ii) temperature variation in the spirit of simulating annealing allows us to avoid suboptimal local minima of F K . <p> The specific choice of the logarithmic complexity measure causes the homogeneous density of reference vectors which is known to yield the smallest entropy per codebook vector in the limit of asymptotic quantization levels <ref> [9, 23] </ref>. This demonstrates how different complexity measures can drastically modify the codebook structure. We will analyse this observation in section 5. 10 5 Asymptotic Quantization Level Density Different complexity measures influence the distributions of reference vectors, even if we choose the same data distribution and the same distortion measure.
Reference: [10] <author> P. A. Chou, T. Lookabaugh, and R. M. Gray. </author> <title> Optimal pruning with applications to tree-structured source-coding and modeling. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 35(2) </volume> <pages> 299-315, </pages> <year> 1989. </year>
Reference-contexts: Various other distortion measures have been proposed in the literature [24], but generalizations of the squared Euclidean distortions (r 6= 2) are most common for many 1 Computational complexity issues like the generation of optimally searchable codebooks in tree-structured vector quantizers <ref> [10] </ref> are not within the scope of this paper. 1 signal processing applications, e.g., r = 1 supposedly is in accordance with the sensitivity profile of the human visual system [15] and is claimed to be a natural choice for image processing applications.
Reference: [11] <author> T. M. </author> <title> Cover. Elements of Information Theory. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: The codebook vectors y ff are determined by the centroid condition X M iff @y ff which is optimal according to rate distortion theory (see <ref> [11] </ref>). <p> an information source emitting letters of a K-ary alphabet with probabilities p ff ; ff = 1; : : : ; K. hCi sets a lower bound for the minimal possible average codeword length if the messages ff are subject to data compaction algorithms like arithmetic coding or Huffman coding <ref> [11] </ref>. Lagrangian optimization of a vector quantization cost function with entropic complexity costs is extensively discussed in [9]. We will apply entropy optimized quantization to the problem of image compression in Section 7.
Reference: [12] <author> C. Darken and J. Moody. </author> <title> Fast adaptive K-means clustering: Some empirical results. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <address> San Diego, </address> <booktitle> volume II, </booktitle> <pages> pages 233-238. </pages> <publisher> IEEE, </publisher> <month> June 17-21, </month> <year> 1990 1990. </year>
Reference-contexts: The question if there exists any faster learning rate schedule c=(N P fl T flff p (N) fl ) with c &gt; 1 than the schedule proposed by (47) is still open although numerical simulations <ref> [12, 13] </ref> suggest that it is advisable to choose c &gt; 1 to speed up the convergence. Online optimization of the codebook size K relies on a heuristics for cluster merging and cluster creation.
Reference: [13] <author> C. Darken and J. Moody. </author> <title> Towards faster stochastic gradient search. </title> <booktitle> In Neural Information Processing Systems 4, </booktitle> <address> San Mateo, California, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The question if there exists any faster learning rate schedule c=(N P fl T flff p (N) fl ) with c &gt; 1 than the schedule proposed by (47) is still open although numerical simulations <ref> [12, 13] </ref> suggest that it is advisable to choose c &gt; 1 to speed up the convergence. Online optimization of the codebook size K relies on a heuristics for cluster merging and cluster creation.
Reference: [14] <author> J. Daugman. </author> <title> Complete discrete 2-d Gabor transforms by neural networks for image analysis and compression. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 36(7) </volume> <pages> 1169-1179, </pages> <year> 1988. </year>
Reference-contexts: Image compression based on orthogonal [45, 36, 15] or nonorthogo-nal <ref> [14, 44] </ref> wavelet decompositions has witnessed increasing popularity in recent years. The wavelet data format is supposedly optimal for natural image representation since the coefficients are statistically independent if we average over a large set of natural images [21, 44].
Reference: [15] <author> R. A. DeVore, B. Jawerth, and B. J. Lucier. </author> <title> Image compression through wavelet trans-form coding. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 38 </volume> <pages> 719-746, </pages> <year> 1992. </year>
Reference-contexts: are most common for many 1 Computational complexity issues like the generation of optimally searchable codebooks in tree-structured vector quantizers [10] are not within the scope of this paper. 1 signal processing applications, e.g., r = 1 supposedly is in accordance with the sensitivity profile of the human visual system <ref> [15] </ref> and is claimed to be a natural choice for image processing applications. The codebook vectors y ff are determined by the centroid condition X M iff @y ff which is optimal according to rate distortion theory (see [11]). <p> Image compression based on orthogonal <ref> [45, 36, 15] </ref> or nonorthogo-nal [14, 44] wavelet decompositions has witnessed increasing popularity in recent years. The wavelet data format is supposedly optimal for natural image representation since the coefficients are statistically independent if we average over a large set of natural images [21, 44]. <p> The wavelet data format is supposedly optimal for natural image representation since the coefficients are statistically independent if we average over a large set of natural images [21, 44]. Quantization of wavelet coefficients yields a psychophysically more pleasing image quality <ref> [15] </ref> than quantization schemes which are based on raw pixel blocks or on the discrete cosine transform of pixel blocks. <p> The reconstruction error is measured as the absolute difference between the original pixel value I (i) and its reconstructed value I c (i), i.e., 16 1 P N i=1 jI (i) I c (i)j. Such an error measure supposedly is compatible with the sensitivity of the visual system <ref> [15] </ref>. In the first series of compression experiments we set = 5 for entropy optimized vector quantization of L 1 and L 2 and = 0:5 for entropy optimized scalar quantization of A 2 .
Reference: [16] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: We 2 In the spirit of the work on K-means clustering and refering to the close relationship between hard and fuzzy clustering in the maximum entropy framework we will use the terms "cluster center" and "reference vector" synonymously throughout this paper, although we are aware of approaches like hierarchical clustering <ref> [16] </ref> or Bayesian clustering [25] which are designed to uncover "natural" structures in a data set and which do not necessarily optimize the cost function (1). 2 also address the question in Section 6 what gain in compression efficiency results from an entropic complexity measure for special data distributions with outliers.
Reference: [17] <author> J. C. Dunn. </author> <title> A fuzzy relative of the ISODATA process and its use in detecting compact well-separated clusters. </title> <journal> J. Cybernetics, </journal> <volume> 3(3) </volume> <pages> 32-57, </pages> <year> 1974. </year>
Reference-contexts: In contrast, procedures which return continuous assignments M iff 2 [0; 1] of a data point to several "cluster centers", are called "soft" or "fuzzy clustering" algorithms <ref> [17, 4] </ref>. 2 How large should the "magic" number K of reference vectors be? It is clear that the result of the optimization of cost function (1) depends on the number of codebook vectors K or, more generally speaking, on the complexity of the codebook.
Reference: [18] <author> R. Durbin and D. Willshaw. </author> <title> An analogue approach to the travelling salesman problem using an elastic net method. </title> <journal> Nature, </journal> <volume> 326 </volume> <pages> 689-691, </pages> <year> 1987. </year>
Reference-contexts: Consequently, a search strategy for assignments fM iff g based on the maximum entropy principle promises robustness and fast convergence. The success of simulating annealing [29] and of neural optimization algorithms <ref> [18, 41, 42, 47] </ref> supports this 5 design philosophy for optimization algorithms. Another substantial advantage of maximum entropy inference is the inherent parallelism of the method which facilitates the mapping of resulting algorithms to parallel hardware or VLSI implementations on chips.
Reference: [19] <author> N. Farvardin. </author> <title> A study of vector quantization for noisy channels. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 36(4) </volume> <pages> 799-809, </pages> <year> 1990. </year>
Reference-contexts: The complexity term limits the number of reference vectors. The admissible class of distortion measures is generalized to include topology preserving vector quantization schemes, also known as source-channel-coding, which reduce the detrimental effect of channel noise on the quantized data set <ref> [19, 34, 50] </ref>. The new cost function with a discussion of different choices for distortion and complexity measures is introduced in Section 2. A maximum entropy solution for this cost function is derived in Section 3. <p> The channel noise breaks the permutation symmetry of the reference vectors and imposes a topology on the set of indices fffjff = 1; : : : ; Kg. Codeword assignments which take the characteristics of the channel noise into account yield superior results <ref> [19, 50] </ref>. Such a procedure is also known as source-channel-coding. Following the same line of thought Luttrell [34] established a connection to a class of topological vector quantization algorithms known as self-organizing feature maps [30, 37].
Reference: [20] <author> T. Feder and D. H. Greene. </author> <title> Optimal algorithms for approximate clustering. </title> <booktitle> In Proceedings of the 20th Annual ACM Symposium on Theory of Computing (STOC), </booktitle> <pages> pages 434-444, </pages> <year> 1988. </year>
Reference-contexts: A datapoint x i is assigned to cluster ff if and only if E iff + dC dp p ; 8 6= ff. 4 An Algorithm for the Design of Complexity Opti mized Codebooks Finding the maximum entropy estimation of the cost function (3) is a N P -hard problem <ref> [20] </ref> plagued by the characteristic non-convexity of the free energy. In the following we propose an iterative cluster splitting algorithm which approximates the maximum entropy solution and which determines the optimal size K of the codebook.
Reference: [21] <author> D.J. </author> <title> Field. Relations between the statistics of natural images and the response properties of cortical cells. </title> <journal> Journal of the Optical Society of America A, </journal> <volume> 4(12) </volume> <pages> 2379-2394, </pages> <year> 1987. </year>
Reference-contexts: Image compression based on orthogonal [45, 36, 15] or nonorthogo-nal [14, 44] wavelet decompositions has witnessed increasing popularity in recent years. The wavelet data format is supposedly optimal for natural image representation since the coefficients are statistically independent if we average over a large set of natural images <ref> [21, 44] </ref>. Quantization of wavelet coefficients yields a psychophysically more pleasing image quality [15] than quantization schemes which are based on raw pixel blocks or on the discrete cosine transform of pixel blocks.
Reference: [22] <author> A. Gersho. </author> <title> Asymptotically optimal block quantization. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 25 </volume> <pages> 373-380, </pages> <month> July </month> <year> 1979. </year>
Reference-contexts: In this section, we study the asymptotic level density for codebooks with high complexity, i.e., dense quantization levels, and we derive the dependency of the cluster density (x) on the probability density (x) for the case of hard clustering. The analysis follows Zador's [49], Gersho's <ref> [22] </ref> and Yamada et al.'s [46] line of reasoning, although we use a variational approach to determine the density of quantization levels as a function of the probability density instead of searching bound for the distortion costs. <p> The total distortion costs can be written as hDi = ff=1 S ff K X (y ff ) S ff The approximation holds for a smoothly varying probability density (x) (y ff ) for x 2 S ff . Following Gersho <ref> [22] </ref> we make the basic assumption that for large K the partitions S ff approximate the optimal polytope S fl for dimension d and distortion measure D (x; y).
Reference: [23] <author> H. Gish and J. N. Pierce. </author> <title> Asymptotically efficient quantizing. </title> <journal> IEEE Transactions on Information Theory IT, </journal> <volume> 14 </volume> <pages> 676-683, </pages> <year> 1968. </year>
Reference-contexts: An optimization algorithm and simulation results are summarized in Section 4. The asymptotic level density of vector quantizers in the high complexity limit is studied in Section 5. These calculations determine the functional dependence of the codebook density on the data density and are related to <ref> [23, 49] </ref>. <p> The specific choice of the logarithmic complexity measure causes the homogeneous density of reference vectors which is known to yield the smallest entropy per codebook vector in the limit of asymptotic quantization levels <ref> [9, 23] </ref>. This demonstrates how different complexity measures can drastically modify the codebook structure. We will analyse this observation in section 5. 10 5 Asymptotic Quantization Level Density Different complexity measures influence the distributions of reference vectors, even if we choose the same data distribution and the same distortion measure. <p> In the special case of K-means clustering or entropic complexity costs the solutions reduce to the classic results of Zador [49] and of Gish and Pierce <ref> [23] </ref>. Our results are restricted to rth-power Euclidean distortions D (x; y) = kx yk r , although the analysis can be generalized to the class of difference distortion measure [46]. We denote the codebook size by K. <p> (x) = K r I (d; r) s (x) (30) ent (x) = K r I (d; r) : (31) The (in)dependence of on for the K-means density (Eq. 30 for s = 1) and the entropic density (31) have been derived in the classic papers of Gish and Pierce <ref> [23] </ref> and Zador [49]. 6 Performance Comparison between K -means and En tropy Optimized Clustering The quantization results shown in Fig. 1 and the analysis of the asymptotic quantization level density (30,31) demonstrate that the complexity measure drastically influences the placement of the codebook vectors.
Reference: [24] <author> R. M. Gray. </author> <title> Vector quantization. </title> <journal> IEEE Acoustics, Speech and Signal Processing Magazine, </journal> <pages> pages 4-29, </pages> <month> April </month> <year> 1984. </year>
Reference-contexts: A widely used distortion measure is the squared Euclidean distance D iff (x i ; y ff ) j kx i y ff k r , r = 2. Various other distortion measures have been proposed in the literature <ref> [24] </ref>, but generalizations of the squared Euclidean distortions (r 6= 2) are most common for many 1 Computational complexity issues like the generation of optimally searchable codebooks in tree-structured vector quantizers [10] are not within the scope of this paper. 1 signal processing applications, e.g., r = 1 supposedly is in
Reference: [25] <author> R. Hanson, J. Stutz, and P. Cheeseman. </author> <title> Bayesian classification theory. </title> <type> Technical Report FIA-90-12-7-01, </type> <institution> NASA Ames Research Center, </institution> <year> 1991. </year>
Reference-contexts: spirit of the work on K-means clustering and refering to the close relationship between hard and fuzzy clustering in the maximum entropy framework we will use the terms "cluster center" and "reference vector" synonymously throughout this paper, although we are aware of approaches like hierarchical clustering [16] or Bayesian clustering <ref> [25] </ref> which are designed to uncover "natural" structures in a data set and which do not necessarily optimize the cost function (1). 2 also address the question in Section 6 what gain in compression efficiency results from an entropic complexity measure for special data distributions with outliers.
Reference: [26] <author> Y. Hussain and N. Farvardin. </author> <title> Adaptive block transform coding of speech based on LPC vector quantization. </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 39(12) </volume> <pages> 2611-2620, </pages> <year> 1991. </year>
Reference-contexts: For example, entropic costs for codebook design have been shown to yield superior quantization performance in speech compression <ref> [9, 26] </ref>. We, therefore, suggest to extend the cost function (1) by an application dependent complexity measure which has to reflect the inherent costs of too complex vector quantizers. The complexity term limits the number of reference vectors.
Reference: [27] <author> E. T. </author> <title> Jaynes. </title> <journal> Information theory and statistical mechanics. Physical Review, </journal> <volume> 106 </volume> <pages> 620-630, </pages> <year> 1957. </year>
Reference-contexts: Furthermore, the maximum entropy principle is the least biased inference method, being maximally noncommittal with respect to missing data as Jaynes formulated it <ref> [27, 28] </ref>. Consequently, a search strategy for assignments fM iff g based on the maximum entropy principle promises robustness and fast convergence. The success of simulating annealing [29] and of neural optimization algorithms [18, 41, 42, 47] supports this 5 design philosophy for optimization algorithms.
Reference: [28] <author> E. T. Jaynes. </author> <title> On the rationale of maximum-entropy methods. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 70 </volume> <pages> 939-952, </pages> <year> 1982. </year>
Reference-contexts: Furthermore, the maximum entropy principle is the least biased inference method, being maximally noncommittal with respect to missing data as Jaynes formulated it <ref> [27, 28] </ref>. Consequently, a search strategy for assignments fM iff g based on the maximum entropy principle promises robustness and fast convergence. The success of simulating annealing [29] and of neural optimization algorithms [18, 41, 42, 47] supports this 5 design philosophy for optimization algorithms.
Reference: [29] <author> S. Kirkpatrick, </author> <title> C.D. Gelatt, and M.P. Vecchi. Optimization by simulated annealing. </title> <journal> Science, </journal> <volume> 220 </volume> <pages> 671-680, </pages> <year> 1983. </year>
Reference-contexts: Consequently, a search strategy for assignments fM iff g based on the maximum entropy principle promises robustness and fast convergence. The success of simulating annealing <ref> [29] </ref> and of neural optimization algorithms [18, 41, 42, 47] supports this 5 design philosophy for optimization algorithms. Another substantial advantage of maximum entropy inference is the inherent parallelism of the method which facilitates the mapping of resulting algorithms to parallel hardware or VLSI implementations on chips. <p> Rose et al. advocated a deterministic annealing approach for K-means clustering in the spirit of <ref> [29] </ref>. They did not include complexity costs to limit the number of codebook vectors but proposed to start with many codebook vectors at high temperature, to lower the temperature to a finite value and to use the resulting set of different vectors as the codebook. <p> In case that D iff and G ff are not differentiable we have to minimize (10,21) directly. The problem of local minima can be alleviated by simulated annealing, an optimization strategy which was introduced by Kirkpatrick <ref> [29] </ref> and which proved to be successful in a large variety of difficult non-convex optimization problems like the traveling salesman problem. The reestimation equations (18), 3 Note, that fuzziness should be understood as a partial membership of data points in clusters.
Reference: [30] <author> T. Kohonen. </author> <title> Self-organization and Associative Memory. </title> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1984. </year>
Reference-contexts: Codeword assignments which take the characteristics of the channel noise into account yield superior results [19, 50]. Such a procedure is also known as source-channel-coding. Following the same line of thought Luttrell [34] established a connection to a class of topological vector quantization algorithms known as self-organizing feature maps <ref> [30, 37] </ref>. <p> Online vector quantization algorithms process a data stream sequentially by updating y ff and p ff in an iterative fashion <ref> [35, 30] </ref>. But how do we have to change the reference vectors and the cluster probabilities after data point x N has been processed? This question is normally studied in stochastic approximation theory. <p> The learning rate 1=(N P fl T flff p (N) fl ) treats different clusters according to their history. That generalizes conventional topological feature maps which suggest the same learning rate for all clusters <ref> [30] </ref>.
Reference: [31] <author> J. Lazzaro, R. Ryckebusch, M. A. Mahowald, and C. A. Mead. </author> <title> Winner-take-all networks of O(n) complexity. </title> <booktitle> In Neural Information Processing Systems 1, </booktitle> <pages> pages 703-711, </pages> <address> San Mateo, California, 1989. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 24 </pages>
Reference-contexts: The structure of the reestimation equations suggests hardware implementations in analog VLSI as they are known from neural network research. Complexity optimized vector quantization maps onto a two layer neural network with a winner-take-all architecture as discussed in [6]. Hardware implementations of such network architectures have been successfully tested <ref> [2, 31] </ref>. Our study of entropy optimized quantization of wavelet transformed gray level images revealed the interesting fact that entropy optimized codebooks reproduce sparse image features like edges more faithfully than the conventional K-means clustering approach.
Reference: [32] <author> Y. Linde, A. Buzo, and R. M. Gray. </author> <title> An algorithm for vector quantizer design. </title> <journal> IEEE Transactions on Communications COM, </journal> <volume> 28 </volume> <pages> 84-95, </pages> <year> 1980. </year>
Reference-contexts: The special case of jointly minimizing suitable distortion costs with constrained entropy of the codebook vectors has been discussed by Chou et al. [9]. The traditional VQ design strategy, as followed in the well-known K-means clustering algorithm [35] or the LBG algorithm by Linde et al. <ref> [32] </ref>, exclusively relies on the distortion errors for placing the reference vectors. The size of the codebook is fixed (K-means) or it is incrementally increased until the distortion error drops below a predefined threshold (LBG). <p> distortion measure D iff (x i ; y ff ) is differentiable in y ff ; for non-differentiable distortions D iff (x i ; ff ff ) we replace Eqs. (2) by the constraints E Km (fM iff g) = min z ff i P N P K j (see <ref> [32] </ref>). Variation of M iff implicitly determines the reference vectors y ff if we enforce Eqs. (2) strictly. Provided, the optimum set of reference vectors has been found, then each data point is represented by its closest reference vector. <p> The special case s = 1 with complexity costs strictly proportional to the number of clusters (C ff = 1=p ff ) P P ff M iff =p ff = N K) corresponds to the K-means clustering cost function <ref> [32] </ref>. For s = 1 all clusters have the same cost term independent of the number of data points assigned to them. We will use this case as a standard to compare with the entropy optimized vector quantizer in section 6.
Reference: [33] <author> S. P. Lloyd. </author> <title> Least squares quantization in PCM. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 28(2) </volume> <pages> 129-137, </pages> <year> 1982. </year>
Reference-contexts: We denote the codebook size by K. S ff , ff = 1; : : : ; K are the partitions of &lt; d which define the quantizer. Following <ref> [33] </ref> we define the codebook density function as g K (x) = KV (S ff ) where V (S ff ) is the volume of partition S ff .
Reference: [34] <author> S. P. Luttrell. </author> <title> Hierarchical vector quantisation. </title> <booktitle> IEE Proceedings, </booktitle> <volume> 136 </volume> <pages> 405-413, </pages> <year> 1989. </year>
Reference-contexts: The complexity term limits the number of reference vectors. The admissible class of distortion measures is generalized to include topology preserving vector quantization schemes, also known as source-channel-coding, which reduce the detrimental effect of channel noise on the quantized data set <ref> [19, 34, 50] </ref>. The new cost function with a discussion of different choices for distortion and complexity measures is introduced in Section 2. A maximum entropy solution for this cost function is derived in Section 3. <p> Codeword assignments which take the characteristics of the channel noise into account yield superior results [19, 50]. Such a procedure is also known as source-channel-coding. Following the same line of thought Luttrell <ref> [34] </ref> established a connection to a class of topological vector quantization algorithms known as self-organizing feature maps [30, 37].
Reference: [35] <author> J. MacQueen. </author> <title> Some methods for classification and analysis of multivariate observations. </title> <booktitle> In Proceedings of the 5th Berkeley Symposium on Mathematical Statistics and Probability, </booktitle> <pages> pages 281-297, </pages> <year> 1967. </year>
Reference-contexts: The special case of jointly minimizing suitable distortion costs with constrained entropy of the codebook vectors has been discussed by Chou et al. [9]. The traditional VQ design strategy, as followed in the well-known K-means clustering algorithm <ref> [35] </ref> or the LBG algorithm by Linde et al. [32], exclusively relies on the distortion errors for placing the reference vectors. The size of the codebook is fixed (K-means) or it is incrementally increased until the distortion error drops below a predefined threshold (LBG). <p> Online vector quantization algorithms process a data stream sequentially by updating y ff and p ff in an iterative fashion <ref> [35, 30] </ref>. But how do we have to change the reference vectors and the cluster probabilities after data point x N has been processed? This question is normally studied in stochastic approximation theory. <p> of squared Euclidean distor tion costs (D iff = kx i y ff k 2 ) equation (46) reduces to the form y ff = N fl T flff p N fl fl T flff hM N;fl i i ff (47) which corresponds to MacQueens update rule for K-means clustering <ref> [35] </ref> and which is similar to learning in competitive neural networks. y (N1) ff is moved towards the most recent data point x N proportionally to the error (x N y (N1) ff ) and proportional to x N 's effective membership P fl T flff hM N;fl i in cluster
Reference: [36] <author> S. G. Mallat. </author> <title> A theory for multiresolution signal decomposition: The wavelet representation. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 11 </volume> <pages> 674-693, </pages> <year> 1989. </year>
Reference-contexts: Image compression based on orthogonal <ref> [45, 36, 15] </ref> or nonorthogo-nal [14, 44] wavelet decompositions has witnessed increasing popularity in recent years. The wavelet data format is supposedly optimal for natural image representation since the coefficients are statistically independent if we average over a large set of natural images [21, 44]. <p> The compression results have been published in part [7]. The images are decomposed with the wavelet algorithm of Mallat <ref> [36] </ref> which uses quadrature mirror filtering for the subband decomposition. The partitioning of the Fourier plane by Mallat's algorithm is shown in Fig. 2a. <p> The resulting coefficients are combined to three dimensional vectors for the bandpassed signals and scalar values for the lowpassed image. In each frequency band we combine the three wavelet coefficients (D 2 ae I) (i), = 1; 2; 3 at position i (notation as in <ref> [36] </ref>) to a three-dimensional vector x i (see Fig. 2). The index ae denotes the reduction factor 2 ae of the bandpass filters.
Reference: [37] <author> H. Ritter, T. Martinetz, and K. Schulten. </author> <title> Neural Computation and Self-organizing Maps. </title> <publisher> Addison Wesley, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: Codeword assignments which take the characteristics of the channel noise into account yield superior results [19, 50]. Such a procedure is also known as source-channel-coding. Following the same line of thought Luttrell [34] established a connection to a class of topological vector quantization algorithms known as self-organizing feature maps <ref> [30, 37] </ref>.
Reference: [38] <author> K. Rose, E. Gurewitz, and G. Fox. </author> <title> A deterministic annealing approach to clustering. </title> <journal> Pattern Recognition Letters, </journal> <volume> 11(11) </volume> <pages> 589-594, </pages> <year> 1990. </year>
Reference-contexts: We focus on a Lagrangian variation principle to find optimal codebooks for a large class of distortion and complexity measures. The data assignments to reference vectors are estimated in the maximum entropy sense, a strategy first proposed by Rose et al. <ref> [38, 39] </ref> for K-means clustering. The special case of jointly minimizing suitable distortion costs with constrained entropy of the codebook vectors has been discussed by Chou et al. [9]. <p> The new cost function with a discussion of different choices for distortion and complexity measures is introduced in Section 2. A maximum entropy solution for this cost function is derived in Section 3. Our approach is inspired by the work of Rose et al. <ref> [38, 39] </ref> on K-means clustering. An optimization algorithm and simulation results are summarized in Section 4. The asymptotic level density of vector quantizers in the high complexity limit is studied in Section 5. <p> Depending on the data distribution the minimization in Eqs. (10,21) might be a highly non-convex optimization problem with many local minima. The statistical mechanics approach for the K-means clustering cost function (1) which corresponds to the case = 0 and fixed K has first been discussed in <ref> [38, 39] </ref>. Note, that 7 this case corresponds to the specific choice of C = 1=p ff where the conjugate potential ^p ff = =p ff = C ff exactly cancels the complexity term, adding essentially a constant cost term (chemical potential) K to the free energy F . <p> It has to be mentioned that step (i) does not require a complete cooling schedule from high temperature to zero temperature since cluster splitting and cooling are coupled <ref> [38] </ref>. We have abstained from temperature variations in the simulation examples presented below since a T = 0 strategy yielded sufficiently good estimates of the global minimum of F K .
Reference: [39] <author> K. Rose, E. Gurewitz, and G. Fox. </author> <title> Statistical mechanics and phase transitions in clustering. </title> <journal> Physical Review Letters, </journal> <volume> 65(8) </volume> <pages> 945-948, </pages> <year> 1990. </year>
Reference-contexts: We focus on a Lagrangian variation principle to find optimal codebooks for a large class of distortion and complexity measures. The data assignments to reference vectors are estimated in the maximum entropy sense, a strategy first proposed by Rose et al. <ref> [38, 39] </ref> for K-means clustering. The special case of jointly minimizing suitable distortion costs with constrained entropy of the codebook vectors has been discussed by Chou et al. [9]. <p> The new cost function with a discussion of different choices for distortion and complexity measures is introduced in Section 2. A maximum entropy solution for this cost function is derived in Section 3. Our approach is inspired by the work of Rose et al. <ref> [38, 39] </ref> on K-means clustering. An optimization algorithm and simulation results are summarized in Section 4. The asymptotic level density of vector quantizers in the high complexity limit is studied in Section 5. <p> We, therefore, have to assure that each cluster index represents a separate cluster. Cluster degeneration does not occur for hard clustering but is a problem for fuzzy clustering <ref> [39] </ref>. An appropriate complexity measure C depends on the particular information processing application at hand, i.e., it is a function of the variables y ff ; p ff . <p> Depending on the data distribution the minimization in Eqs. (10,21) might be a highly non-convex optimization problem with many local minima. The statistical mechanics approach for the K-means clustering cost function (1) which corresponds to the case = 0 and fixed K has first been discussed in <ref> [38, 39] </ref>. Note, that 7 this case corresponds to the specific choice of C = 1=p ff where the conjugate potential ^p ff = =p ff = C ff exactly cancels the complexity term, adding essentially a constant cost term (chemical potential) K to the free energy F .
Reference: [40] <author> T. Senoo and B. Girod. </author> <title> Vector quantization for entropy coding of image subbands. </title> <type> Preprint, </type> <year> 1992. </year>
Reference-contexts: The corresponding outlier regions of L 1 and L 2 are more accurately sampled by entropy optimized quantization than by K-means clustering. Differences in the peak signal to noise ratios (SNR) are listed in the table to make a comparison with published compression results <ref> [40] </ref> possible. Images of a compression and reconstruction experiment are shown in Fig. 3. We have used a complexity parameter = 50 to design entropy optimized codebooks for L 1 ; L 2 and = 5 to design an entropy optimized codebook for A 2 .
Reference: [41] <author> P.D. Simic. </author> <title> Statistical mechanics as the underlying theory of "elastic" and "neural" optimizations. </title> <journal> Network, </journal> <volume> 1 </volume> <pages> 89-103, </pages> <year> 1990. </year>
Reference-contexts: Consequently, a search strategy for assignments fM iff g based on the maximum entropy principle promises robustness and fast convergence. The success of simulating annealing [29] and of neural optimization algorithms <ref> [18, 41, 42, 47] </ref> supports this 5 design philosophy for optimization algorithms. Another substantial advantage of maximum entropy inference is the inherent parallelism of the method which facilitates the mapping of resulting algorithms to parallel hardware or VLSI implementations on chips.
Reference: [42] <author> P.D. Simic. </author> <title> Constrained nets for graph matching and other quadratic assignment problems. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 268-281, </pages> <year> 1991. </year>
Reference-contexts: Consequently, a search strategy for assignments fM iff g based on the maximum entropy principle promises robustness and fast convergence. The success of simulating annealing [29] and of neural optimization algorithms <ref> [18, 41, 42, 47] </ref> supports this 5 design philosophy for optimization algorithms. Another substantial advantage of maximum entropy inference is the inherent parallelism of the method which facilitates the mapping of resulting algorithms to parallel hardware or VLSI implementations on chips.
Reference: [43] <author> Y. Tikochinsky, N.Z. Tishby, and R. D. Levine. </author> <title> Alternative approach to maximum-entropy inference. </title> <journal> Physical Review A, </journal> <volume> 30 </volume> <pages> 2638-2644, </pages> <year> 1984. </year>
Reference-contexts: The inference principle which selects the most stable set of assignment variables with respect to random fluctuations in the assignment process is the maximum entropy principle <ref> [43] </ref>. Furthermore, the maximum entropy principle is the least biased inference method, being maximally noncommittal with respect to missing data as Jaynes formulated it [27, 28]. Consequently, a search strategy for assignments fM iff g based on the maximum entropy principle promises robustness and fast convergence.
Reference: [44] <author> B. Wegmann and C. Zetsche. </author> <title> Statistical dependence between orientation filter outputs used in an human vision based image code. </title> <editor> In M. Kunt, editor, </editor> <booktitle> SPIE Proceedings of the Visual Communications and Image Processing'90, </booktitle> <volume> volume 1360, </volume> <pages> pages 909-923, </pages> <year> 1990. </year>
Reference-contexts: Image compression based on orthogonal [45, 36, 15] or nonorthogo-nal <ref> [14, 44] </ref> wavelet decompositions has witnessed increasing popularity in recent years. The wavelet data format is supposedly optimal for natural image representation since the coefficients are statistically independent if we average over a large set of natural images [21, 44]. <p> Image compression based on orthogonal [45, 36, 15] or nonorthogo-nal [14, 44] wavelet decompositions has witnessed increasing popularity in recent years. The wavelet data format is supposedly optimal for natural image representation since the coefficients are statistically independent if we average over a large set of natural images <ref> [21, 44] </ref>. Quantization of wavelet coefficients yields a psychophysically more pleasing image quality [15] than quantization schemes which are based on raw pixel blocks or on the discrete cosine transform of pixel blocks.
Reference: [45] <author> P. H. Westerink, D. E. Boekee, J. Biemond, and J.W. Woods. </author> <title> Subband coding of images using vector quantization. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 36 </volume> <pages> 713-719, </pages> <year> 1988. </year>
Reference-contexts: Image compression based on orthogonal <ref> [45, 36, 15] </ref> or nonorthogo-nal [14, 44] wavelet decompositions has witnessed increasing popularity in recent years. The wavelet data format is supposedly optimal for natural image representation since the coefficients are statistically independent if we average over a large set of natural images [21, 44].
Reference: [46] <author> Y. Yamada, S. Tazaki, and R. Gray. </author> <title> Asymptotic performance of block quantizers with difference distortion measures. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 26 </volume> <pages> 6-14, </pages> <year> 1980. </year>
Reference-contexts: The analysis follows Zador's [49], Gersho's [22] and Yamada et al.'s <ref> [46] </ref> line of reasoning, although we use a variational approach to determine the density of quantization levels as a function of the probability density instead of searching bound for the distortion costs. <p> Our results are restricted to rth-power Euclidean distortions D (x; y) = kx yk r , although the analysis can be generalized to the class of difference distortion measure <ref> [46] </ref>. We denote the codebook size by K. S ff , ff = 1; : : : ; K are the partitions of &lt; d which define the quantizer.
Reference: [47] <author> A. L. Yuille. </author> <title> Generalized deformable models, statistical physics, and matching problems. </title> <journal> Neural Computation, </journal> <volume> 2(1) </volume> <pages> 1-24, </pages> <year> 1990. </year> <month> 25 </month>
Reference-contexts: Consequently, a search strategy for assignments fM iff g based on the maximum entropy principle promises robustness and fast convergence. The success of simulating annealing [29] and of neural optimization algorithms <ref> [18, 41, 42, 47] </ref> supports this 5 design philosophy for optimization algorithms. Another substantial advantage of maximum entropy inference is the inherent parallelism of the method which facilitates the mapping of resulting algorithms to parallel hardware or VLSI implementations on chips.
Reference: [48] <author> L. Zadeh. </author> <title> Fuzzy sets. </title> <journal> Inform. Contr., </journal> <volume> 8 </volume> <pages> 338-353, </pages> <year> 1965. </year>
Reference-contexts: The reestimation equations (18), 3 Note, that fuzziness should be understood as a partial membership of data points in clusters. Our approach is still a probabilistic one and is not related to fuzzy set theory as proposed by Zadeh <ref> [48] </ref>. 8 Zero temperature solutions of entropy optimized quantization C ff = log p ff are shown for the complexity weights = 5:0; 2:5; 0:4 in (b,c,d).
Reference: [49] <author> P. L. Zador. </author> <title> Asymptotic quantization error of continuous signals and the quantization dimension. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 28(2) </volume> <pages> 139-149, </pages> <year> 1982. </year>
Reference-contexts: An optimization algorithm and simulation results are summarized in Section 4. The asymptotic level density of vector quantizers in the high complexity limit is studied in Section 5. These calculations determine the functional dependence of the codebook density on the data density and are related to <ref> [23, 49] </ref>. <p> In this section, we study the asymptotic level density for codebooks with high complexity, i.e., dense quantization levels, and we derive the dependency of the cluster density (x) on the probability density (x) for the case of hard clustering. The analysis follows Zador's <ref> [49] </ref>, Gersho's [22] and Yamada et al.'s [46] line of reasoning, although we use a variational approach to determine the density of quantization levels as a function of the probability density instead of searching bound for the distortion costs. <p> The variational approach is required since we treat the case of general complexity costs which balance the monotonous decrease of distortion costs with increasing codebook vector density. In the special case of K-means clustering or entropic complexity costs the solutions reduce to the classic results of Zador <ref> [49] </ref> and of Gish and Pierce [23]. Our results are restricted to rth-power Euclidean distortions D (x; y) = kx yk r , although the analysis can be generalized to the class of difference distortion measure [46]. We denote the codebook size by K. <p> r I (d; r) s (x) (30) ent (x) = K r I (d; r) : (31) The (in)dependence of on for the K-means density (Eq. 30 for s = 1) and the entropic density (31) have been derived in the classic papers of Gish and Pierce [23] and Zador <ref> [49] </ref>. 6 Performance Comparison between K -means and En tropy Optimized Clustering The quantization results shown in Fig. 1 and the analysis of the asymptotic quantization level density (30,31) demonstrate that the complexity measure drastically influences the placement of the codebook vectors.
Reference: [50] <author> K. Zeger and A. Gersho. </author> <title> Pseudo-gray coding. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 38(12) </volume> <pages> 2147-2158, </pages> <year> 1990. </year> <title> reconstruction from wavelet coefficients which were quantized with entropic complexity. (c) Reconstruction from wavelet coefficients quantized by K-means clustering. (d) Reconstruction error of image (b). (e) Reconstruction error of image (c). Black is normalized in image (d) and (e) to a deviation of 92 gray values. Note the large errors near edges in (e). </title> <type> 27 </type>
Reference-contexts: The complexity term limits the number of reference vectors. The admissible class of distortion measures is generalized to include topology preserving vector quantization schemes, also known as source-channel-coding, which reduce the detrimental effect of channel noise on the quantized data set <ref> [19, 34, 50] </ref>. The new cost function with a discussion of different choices for distortion and complexity measures is introduced in Section 2. A maximum entropy solution for this cost function is derived in Section 3. <p> The channel noise breaks the permutation symmetry of the reference vectors and imposes a topology on the set of indices fffjff = 1; : : : ; Kg. Codeword assignments which take the characteristics of the channel noise into account yield superior results <ref> [19, 50] </ref>. Such a procedure is also known as source-channel-coding. Following the same line of thought Luttrell [34] established a connection to a class of topological vector quantization algorithms known as self-organizing feature maps [30, 37].
References-found: 50

