URL: http://bugle.cs.uiuc.edu/Papers/SCHPF.ps.Z
Refering-URL: http://bugle.cs.uiuc.edu/Pablo.html
Root-URL: http://www.cs.uiuc.edu
Title: An Integrated Compilation and Performance Analysis Environment for Data Parallel Programs  
Author: Vikram S. Adve John Mellor-Crummey Mark Anderson Ken Kennedy Jhy-Chun Wang Daniel A. Reed 
Address: Houston, Texas 77251-1892  Urbana, Illinois 61801  
Affiliation: Center for Research on Parallel Computation Rice University  Department of Computer Science University of Illinois  
Abstract: Supporting source-level performance analysis of programs written in data-parallel languages requires a unique degree of integration between compilers and performance analysis tools. Since compilers for data-parallel languages such as High Performance Fortran infer parallelism and communication from data distribution directives, performance tools cannot meaningfully relate measurements about these key aspects of execution performance to source-level constructs without substantial compiler support. In this paper, we describe an integrated system for performance analysis of data-parallel programs written in Fortran D, based on substantial extensions to the Rice Fortran 77D compiler and the Illinois Pablo performance analysis environment. During code generation, the Fortran D compiler records semantic information describing the relationship between performance instrumentation and the original source program. The Pablo performance analysis substrate uses this information to correlate the program's dynamic behavior with the data parallel source. The integrated system provides detailed source-level performance feedback to programmers via a pair of graphical interfaces. We expect that our strategy can serve as a model for integration of other data parallel compilers and performance tools.
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Applied Parallel Research. </institution> <note> Forge 90 Distributed Memory Parallelizer: User's Guide, version 8.0 ed. </note> <institution> Placerville, </institution> <address> CA, </address> <year> 1992. </year>
Reference-contexts: Notable exceptions include include Prism [12] and NV [4] for CM-Fortran, Forge90 <ref> [1] </ref> for Fortran 90 and HPF, and the MPP-Apprentice performance tool, which supports C, Fortran and Fortran 90 on the Cray T3D [13]. The CM Fortran compiler lacks aggressive inter-statement optimization, making the mapping of dynamic performance data to source lines for Prism and data objects for NV straightforward.
Reference: [2] <author> Aydt, R. A. SDDF: </author> <title> The Pablo Self-Describing Data Format. </title> <type> Tech. rep., </type> <institution> Department of Computer Science, University of Illinois, </institution> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: When data dependences prevent full 2 parallelism, the compiler often can achieve partial parallelism by pipelining the computation. The base Pablo environment's principal components are an extensible data capture library [10], a data meta-format <ref> [2] </ref> for describing the structure of performance data records without constraining their contents, and a user-extensible graphical data analysis toolkit [8]. The data capture library can trace, count, or time code fragments as well as capture entry to and exit from procedures, loops, and message-passing library calls. <p> In addition to dynamic SDDF records from runtime measurements, new families of SDDF data records have been designed to hold static and symbolic information. Thus, SDDF <ref> [2] </ref> provides a flexible medium for information interchange between components of the integrated system. A post-mortem trace analysis facility interprets the dynamic trace recorded during a program execution using the static and symbolic information.
Reference: [3] <author> Hiranandani, S., Kennedy, K., and Tseng, C.-W. </author> <title> Preliminary Experiences with the Fortran D Compiler. </title> <booktitle> In Proceedings of Supercomputing '93 (Nov. </booktitle> <year> 1993), </year> <institution> Association for Computing Machinery. </institution>
Reference-contexts: 1 Introduction High-level, data-parallel languages such as High Performance Fortran [5] and Fortran D <ref> [3] </ref> have attracted considerable attention because they offer a simple and portable programming model for parallel, scientific programs. In such languages, programmers specify parallelism abstractly using data layout directives, and a compiler uses these directives as the basis for synthesizing a program with explicit parallelism and inter-processor communication and synchronization.
Reference: [4] <author> Irvin, R. B., and Miller, B. P. </author> <title> A Performance Tool for High-Level Parallel Programming Languages. In Programming Environments for Massively Parallel Distributed Systems (Basel, </title> <address> Switzerland, 1994), </address> <publisher> Birkhauser Verlag. </publisher>
Reference-contexts: Notable exceptions include include Prism [12] and NV <ref> [4] </ref> for CM-Fortran, Forge90 [1] for Fortran 90 and HPF, and the MPP-Apprentice performance tool, which supports C, Fortran and Fortran 90 on the Cray T3D [13].
Reference: [5] <author> Koelbel, C., Loveman, D., Schreiber, R., Steele, Jr., G., and Zosel, M. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction High-level, data-parallel languages such as High Performance Fortran <ref> [5] </ref> and Fortran D [3] have attracted considerable attention because they offer a simple and portable programming model for parallel, scientific programs.
Reference: [6] <author> Mellor-Crummey, J. M., Adve, V. S., and Koelbel, C. </author> <title> The Compiler's Role in Analysis and Tuning of Data-Parallel Programs. </title> <booktitle> In Proceedings of The Second Workshop on Environments and Tools for Parallel Scientific Computing (Townsend, </booktitle> <address> TN, </address> <month> May </month> <year> 1994), </year> <pages> pp. 211-220. </pages> <note> Also available via anonymous ftp from softlib.cs.rice.edu in pub/CRPC-TRs/reports/CRPC-TR94405.ps. </note>
Reference-contexts: Finally, the compiler can exploit dynamic performance data to generate more efficient code, as described elsewhere <ref> [6] </ref>. 6 3 Integrated Compilation and Performance Analysis As illustrated in x2, a simple composition of compiler and performance tools is insufficient to support high-level performance analysis and software tuning of Fortran D programs.
Reference: [7] <author> Miller, B. P., Clark, M., Hollingsworth, J., Kierstead, S., Lim, S.-S., and Torzewski, T. IPS-2: </author> <title> The Second Generation of a Parallel Program Measurement System. </title> <journal> IEEE Transactions on Computers 1, </journal> <month> 2 (Apr. </month> <year> 1990), </year> <pages> 206-217. </pages>
Reference-contexts: Because the goal of data-parallel languages is to insulate software developers from the idiosyncrasies of message passing, performance tuning should not require them to understand the details of the compiler-generated code. Unfortunately, current performance instrumentation and analysis tools <ref> [9, 8, 7, 11] </ref> for distributed memory parallel systems only capture and present performance data from the generated Fortran 77 code of Figure 3.
Reference: [8] <author> Reed, D. A. </author> <title> Performance Instrumentation Techniques for Parallel Systems. In Models and Techniques for Performance Evaluation of Computer and Communications Systems, </title> <editor> L. Donatiello and R. Nelson, Eds. </editor> <booktitle> Springer-Verlag Lecture Notes in Computer Science, </booktitle> <year> 1993, </year> <pages> pp. 463-490. </pages>
Reference-contexts: The base Pablo environment's principal components are an extensible data capture library [10], a data meta-format [2] for describing the structure of performance data records without constraining their contents, and a user-extensible graphical data analysis toolkit <ref> [8] </ref>. The data capture library can trace, count, or time code fragments as well as capture entry to and exit from procedures, loops, and message-passing library calls. A group of library extension interfaces allows instrumentation software developers to incrementally extend the library's functionality. <p> Because the goal of data-parallel languages is to insulate software developers from the idiosyncrasies of message passing, performance tuning should not require them to understand the details of the compiler-generated code. Unfortunately, current performance instrumentation and analysis tools <ref> [9, 8, 7, 11] </ref> for distributed memory parallel systems only capture and present performance data from the generated Fortran 77 code of Figure 3.
Reference: [9] <author> Reed, D. A. </author> <title> Experimental Performance Analysis of Parallel Systems: Techniques and Open Problems. </title> <booktitle> In Proceedings of the 7th International Conference on Modelling Techniques and Tools for Computer Performance Evaluation (May 1994), </booktitle> <pages> pp. 25-51. </pages>
Reference-contexts: Because the goal of data-parallel languages is to insulate software developers from the idiosyncrasies of message passing, performance tuning should not require them to understand the details of the compiler-generated code. Unfortunately, current performance instrumentation and analysis tools <ref> [9, 8, 7, 11] </ref> for distributed memory parallel systems only capture and present performance data from the generated Fortran 77 code of Figure 3.
Reference: [10] <author> Reed, D. A., Aydt, R. A., Noe, R. J., Roth, P. C., Shields, K. A., Schwartz, B. W., and Tavera, L. F. </author> <title> Scalable Performance Analysis: The Pablo Performance Analysis Environment. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <editor> A. Skjellum, Ed. </editor> <publisher> IEEE Computer Society, </publisher> <year> 1993, </year> <pages> pp. 104-113. </pages>
Reference-contexts: When data dependences prevent full 2 parallelism, the compiler often can achieve partial parallelism by pipelining the computation. The base Pablo environment's principal components are an extensible data capture library <ref> [10] </ref>, a data meta-format [2] for describing the structure of performance data records without constraining their contents, and a user-extensible graphical data analysis toolkit [8]. <p> Also, the compiler adds instrumentation to record values of key symbolics unknown at compile time. Pablo's instrumentation library <ref> [10] </ref> supports counting or tracing dynamic events and time intervals as well as specialized instrumentation support for tracing procedure calls, loops, and message passing library calls. The library has been augmented, via its extension interfaces, to support integration with the Fortran D compiler.
Reference: [11] <author> Ries, B., Anderson, R., Auld, W., Breazeal, D., Callaghan, K., Richards, E., and Smith, 21 W. </author> <title> The Paragon Performance Monitoring Environment. </title> <booktitle> In Proceedings of Supercomputing '93 (Nov. 1993), Association for Computing Machinery, </booktitle> <pages> pp. 850-859. </pages>
Reference-contexts: Because the goal of data-parallel languages is to insulate software developers from the idiosyncrasies of message passing, performance tuning should not require them to understand the details of the compiler-generated code. Unfortunately, current performance instrumentation and analysis tools <ref> [9, 8, 7, 11] </ref> for distributed memory parallel systems only capture and present performance data from the generated Fortran 77 code of Figure 3.
Reference: [12] <author> TMC. </author> <title> Prism User's Guide, </title> <institution> V1.2. Thinking Machines Corporation, Cambridge, Massachusetts, </institution> <month> Mar. </month> <year> 1993. </year>
Reference-contexts: Notable exceptions include include Prism <ref> [12] </ref> and NV [4] for CM-Fortran, Forge90 [1] for Fortran 90 and HPF, and the MPP-Apprentice performance tool, which supports C, Fortran and Fortran 90 on the Cray T3D [13].
Reference: [13] <author> Williams, W., Hoel, T., and Pase, D. </author> <title> The MPP Apprentice Performance Tool: Delivering the Performance of the Cray T3D. In Programming Environments for Massively Parallel Distributed Systems (Basel, </title> <address> Switzerland, 1994), </address> <publisher> Birkhauser Verlag. </publisher> <pages> 22 </pages>
Reference-contexts: Notable exceptions include include Prism [12] and NV [4] for CM-Fortran, Forge90 [1] for Fortran 90 and HPF, and the MPP-Apprentice performance tool, which supports C, Fortran and Fortran 90 on the Cray T3D <ref> [13] </ref>. The CM Fortran compiler lacks aggressive inter-statement optimization, making the mapping of dynamic performance data to source lines for Prism and data objects for NV straightforward. Forge90 supports performance analysis at the level of compiler-generated SPMD code; its performance annotations show the cost of invocations of their communication library.
References-found: 13

