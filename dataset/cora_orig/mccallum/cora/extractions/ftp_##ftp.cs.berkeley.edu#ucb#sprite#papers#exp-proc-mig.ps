URL: ftp://ftp.cs.berkeley.edu/ucb/sprite/papers/exp-proc-mig.ps
Refering-URL: http://www.cs.berkeley.edu/projects/sprite/sprite.papers.html
Root-URL: 
Email: douglis@sprite.Berkeley.EDU  
Title: Experience with Process Migration in Sprite  
Author: Fred Douglis 
Address: Berkeley, CA 94720  
Affiliation: Computer Science Division Electrical Engineering and Computer Sciences University of California  
Abstract: This paper reports on experience with the Sprite process migration facility. Sprite provides transparent remote execution to support load sharing through the use of idle workstations. Process migration is used to reclaim workstations when their owners return. On Sun 3/75 workstations, the cost of selecting an idle host and invoking a remote process is about 400 milliseconds. This time is substantially greater than the cost of creating the same process locally, but it is much less than the typical execution time of programs that are run remotely, such as compilations and text formatting. The cost of migrating an active process is a function of the number of dirty pages it has, the number of file blocks that must be flushed from the host's file cache, and the number of open files it has. This time ranges from 110 milliseconds to migrate a small process with no open files, to several seconds to migrate a process with many dirty pages and file blocks and several open files. Remote execution has been used regularly for approximately 9 months to perform compilations in parallel. I draw conclusions about the usefulness of remote execution for parallel compilation, and I present lessons we learned about process migration and system building in general. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Y. Artsy and R. Finkel. </author> <title> Simplicity, efficiency, and functionality in designing a process migration facility. </title> <booktitle> In The 2nd Israel Conference on Computer Systems, </booktitle> <month> May </month> <year> 1987. </year>
Reference-contexts: Measurements of the cost of cache flushing are presented below in Section 4. To transfer a process's virtual memory, Sprite writes the process's dirty pages to a shared file server. The pages are retrieved from the server as the process page-faults. By comparison, Locus, V [11], and Charlotte <ref> [1] </ref> transfer the entire address space, which may take orders of magnitude more time than transferring the rest of the process's state.
Reference: [2] <author> F. Douglis and J. Ousterhout. </author> <title> Process migration in the Sprite operating system. </title> <booktitle> In Proceedings of the 7th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 18-25, </pages> <address> Berlin, West Germany, </address> <month> September </month> <year> 1987. </year> <note> IEEE. </note>
Reference-contexts: over the past 9 months. fl This work was supported in part by the Defense Advanced Research Projects Agency under contract N00039-85-C-0269 and in part by the National Science Foundation under grant ECS-8351961. 1 The next section provides some background on Sprite's process migration facility, sum-marizing what has appeared elsewhere <ref> [2, 3] </ref>. In Section 3, I discuss the history of process migration in Sprite, from its initial implementation to its current state. We found that migration was much harder to get working than we had expected, and even harder to keep working as the rest of the system evolved.
Reference: [3] <author> F. Douglis and J. Ousterhout. </author> <title> Process migration in Sprite: A status report. </title> <journal> IEEE Computer Society Technical Committee on Operating Systems Newsletter, </journal> <volume> 3(1) </volume> <pages> 8-10, </pages> <month> Winter </month> <year> 1989. </year>
Reference-contexts: over the past 9 months. fl This work was supported in part by the Defense Advanced Research Projects Agency under contract N00039-85-C-0269 and in part by the National Science Foundation under grant ECS-8351961. 1 The next section provides some background on Sprite's process migration facility, sum-marizing what has appeared elsewhere <ref> [2, 3] </ref>. In Section 3, I discuss the history of process migration in Sprite, from its initial implementation to its current state. We found that migration was much harder to get working than we had expected, and even harder to keep working as the rest of the system evolved.
Reference: [4] <author> D. L. Eager, E. D. Lazowska, and J. Zahorjan. </author> <title> The limited performance benefits of migrating active processes for load sharing. </title> <booktitle> In ACM SIGMETRICS 1988, </booktitle> <month> May </month> <year> 1988. </year>
Reference-contexts: that migrating processes for load sharing performance does not generally yield significant improvement over policies with only remote invocation, and they suggested that "costlier but simpler" migration may be appropriate if migration is done primarily for purposes other than load sharing (such as permitting workstation owners to reclaim their hosts) <ref> [4] </ref>. Remote invocation in Sprite is inexpensive enough to provide performance improvements for all but extremely short-lived processes, assuming that the local host is already highly utilized. Migrating active processes, on the other hand, is often measured in seconds rather than milliseconds.
Reference: [5] <author> R. Finkel and Y. Artsy. </author> <title> The process migration mechanism of Charlotte. </title> <journal> IEEE Computer Society Technical Committee on Operating Systems Newsletter, </journal> <volume> 3(1) </volume> <pages> 11-14, </pages> <month> Winter </month> <year> 1989. </year>
Reference-contexts: Finkel and Artsy, on the other hand, report that they were able to keep migration sufficiently modular to keep changes to migration from breaking other parts of the kernel and changes elsewhere in the kernel from breaking migration <ref> [5] </ref>. Although file encapsulation proved to be a thorn in the side of process migration for some time, migration has evolved to be generally orthogonal to the rest of the system. Many kernel modules in Sprite maintain state on behalf of each process.
Reference: [6] <author> P. E. Krueger. </author> <title> Distributed Scheduling for a Changing Environment. </title> <type> PhD thesis, </type> <institution> University of Wisconsin, Madison, Wisconsin, </institution> <month> June </month> <year> 1988. </year> <note> Computer Sciences Technical Report #780. </note>
Reference-contexts: Methods of selecting hosts for distributing load, with and without process migration, have been discussed at length in the literature (e.g., <ref> [6, 11] </ref>). Sprite uses a shared file that contains the load average and idle time of each host, as well as information about the number of foreign tasks currently using the host.
Reference: [7] <author> M. Nelson, B. Welch, and J. Ousterhout. </author> <title> Caching in the Sprite network file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 134-154, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: File transfer is costly primarily because of Sprite's file system cache consistency algorithm (described in detail in <ref> [7] </ref>). Read-only files are cachable on multiple hosts simultaneously, and if a file is read and written by only one host then that host may cache the file.
Reference: [8] <author> J. Ousterhout, A. Cherenson, F. Douglis, M. Nelson, and B. Welch. </author> <title> The Sprite network operating system. </title> <journal> IEEE Computer, </journal> <volume> 21(2) </volume> <pages> 23-36, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: 1 Introduction By executing independent tasks in parallel on idle workstations, applications may substantially reduce turnaround time. However, the usefulness of remote execution is limited if processes must be terminated to reclaim a workstation when its owner returns, or if processes behave differently when they are run remotely. Sprite <ref> [8] </ref> provides a transparent process migration facility to allow noninvasive access to idle workstations. An application invokes a program remotely by performing a system call that combines migration with exec, replacing the process's execution image with a new program on the other host.
Reference: [9] <author> G. J. Popek and B. J. Walker, </author> <title> editors. The LOCUS Distributed System Architecture. </title> <booktitle> Computer Systems Series. </booktitle> <publisher> The MIT Press, </publisher> <year> 1985. </year>
Reference-contexts: Furthermore, the home host alone is responsible for knowing the current location of all processes that are tied to it; this host is similar to the LOCUS "origin site", which is the host on which a process is created <ref> [9] </ref>. However, the home host in Sprite is inherited, so children of remote processes behave as though they were created on the same host as their parent. Processes are migrated by encapsulating their state on the source and transferring the state to the target via kernel-to-kernel remote procedure calls (RPC). <p> We plan to port migration to Sun 4 workstations, and if possible, provide the ability to perform remote execs between machines of different types. The ability to perform heterogeneous remote execs, along the lines of the LOCUS rexec system call <ref> [9] </ref>, could considerably expand the pool of idle hosts available to a single program. We would also like to add automatic remigration after eviction to keep eviction from degrading the performance of the home host.
Reference: [10] <author> E. Roberts and J. Ellis. </author> <title> parmake and dp: Experience with a distributed, parallel implementation of make. </title> <booktitle> In Proceedings from the Second Workshop on Large-Grained Parallelism. </booktitle> <institution> Software Engineering Institute, Carnegie-Mellon University, </institution> <month> November </month> <year> 1987. </year> <note> Report CMU/SEI-87-SR-5. </note>
Reference-contexts: The total time to select an idle workstation and start a program on it compares favorably to the cost of other remote execution facilities, such as the Digital Systems Research Center distant process (dp) facility <ref> [10] </ref>. Dp takes 1 second on Firefly workstations (using multiple MicroVAX-II processors) to start a new distant process. However, dp takes 6 seconds to initialize before being usable, so the SRC parallel make facility does not use dp unless enough tasks may be o*oaded to amortize the overhead.
Reference: [11] <author> M. Theimer. </author> <title> Preemptable Remote Execution Facilities for Loosely-Coupled Distributed Systems. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1986. </year>
Reference-contexts: Measurements of the cost of cache flushing are presented below in Section 4. To transfer a process's virtual memory, Sprite writes the process's dirty pages to a shared file server. The pages are retrieved from the server as the process page-faults. By comparison, Locus, V <ref> [11] </ref>, and Charlotte [1] transfer the entire address space, which may take orders of magnitude more time than transferring the rest of the process's state. <p> Methods of selecting hosts for distributing load, with and without process migration, have been discussed at length in the literature (e.g., <ref> [6, 11] </ref>). Sprite uses a shared file that contains the load average and idle time of each host, as well as information about the number of foreign tasks currently using the host. <p> Theimer refers to migration facilities as being "fragile": in an environment in which the kernel is often modified, migration can break unless everyone modifying the kernel keeps the migration facility in step with other kernel changes <ref> [11] </ref>. Finkel and Artsy, on the other hand, report that they were able to keep migration sufficiently modular to keep changes to migration from breaking other parts of the kernel and changes elsewhere in the kernel from breaking migration [5]. <p> For example, the V System preemptable remote execution facility is restricted to applications that execute "only operations whose output is independent of the location at which they are executed" <ref> [11] </ref>.
Reference: [12] <author> B. B. Welch and J. K. Ousterhout. </author> <title> Prefix tables: A simple mechanism for locating files in a distributed filesystem. </title> <booktitle> In Proc. of the 6th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 184-189, </pages> <address> Boston, Mass., </address> <month> May </month> <year> 1986. </year> <note> IEEE. </note>
Reference-contexts: The original implementation forwarded nearly all system calls home, including calls that involved locating files, because each host maintained a distinct prefix table that mapped file system domains to servers <ref> [12] </ref>. Rather than keeping copies of the prefix table consistent between multiple hosts, naming was performed on the home host using its prefix table. Forcing naming operations to be redirected via the process's home slowed down compilation benchmarks by approximately 20%.
Reference: [13] <author> B. B. Welch and J. K. Ousterhout. </author> <title> Pseudo devices: User-level extensions to the Sprite file system. </title> <booktitle> In USENIX 1988 Summer Conference, </booktitle> <pages> pages 37-49, </pages> <address> San Francisco, CA, </address> <month> June </month> <year> 1988. </year> <month> 13 </month>
Reference-contexts: Communication with other processes is performed using file system objects such as pipes and pseudo-devices <ref> [13] </ref>. Pseudo-devices are used for system services such as the X Window System and access to the internet, for which location transparency would otherwise 2 present a problem.
Reference: [14] <author> E. Zayas. </author> <title> Attacking the process migration bottleneck. </title> <booktitle> In Proceedings of the Eleventh ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 13-22, </pages> <address> Austin, TX, </address> <month> Novem-ber </month> <year> 1987. </year> <month> 14 </month>
Reference-contexts: Accent addresses the "process migration bottleneck" by transferring virtual memory in a lazy fashion: the target of the migration retrieves memory from the source as it is referenced, thus amortizing the cost of memory transfer over the execution of the process <ref> [14] </ref>. Although lazy virtual memory transfer makes the act of migration faster than direct memory-to-memory transfer, it requires that the source of a migration dedicate memory to the process after the migration has completed.
References-found: 14

