URL: http://www.cs.washington.edu/homes/tullsen/ISCA93.ps
Refering-URL: http://www.cs.washington.edu/homes/tullsen/research.html
Root-URL: 
Title: Limitations of Cache Prefetching on a Bus-Based Multiprocessor  
Author: Dean M. Tullsen and Susan J. Eggers 
Address: Seattle WA 98195  
Affiliation: Department of Computer Science and Engineering FR-35 University of Washington  
Note: To appear in Proceedings of the 20th Annual International Symposium on Computer Architecture  
Abstract: current and future high-performance processors. However, prefetching is not without costs, particularly on a multiprocessor. Prefetching can negatively affect bus utilization, overall cache miss rates, memory latencies and data sharing. We simulated the effects of a particular compiler-directed prefetching algorithm, running on a bus-based multiprocessor. We showed that, despite a high memory latency, this architecture is not very well-suited for prefetching. For several variations on the architecture, speedups for five parallel programs were no greater than 39%, and degradations were as high as 7%, when prefetching was added to the workload. We examined the sources of cache misses, in light of several different prefetching strategies, and pinpointed the causes of the performance changes. Invalidation misses pose a particular problem for current compiler-directed prefetchers. We applied two techniques that reduced their impact: a special prefetching heuristic tailored to write-shared data, and restructuring shared data to reduce false sharing, thus allowing traditional prefetching algorithms to work well. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Callahan, K. Kennedy, and A. Porterfield. </author> <title> Software prefetching. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 40-52, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Although the need to make processors tolerant of high memory latency is much more severe in multiprocessors than in uniprocessors, most studies of cache prefetching have concentrated on uniprocessor architectures <ref> [1, 2, 18] </ref>. DASH [13] has hardware support for cache prefetching, but to date they have only published the results of micro-benchmark throughput tests.
Reference: [2] <author> W.Y. Chen, S.A. Mahlke, P.P. Chang, and W.W. Hwu. </author> <title> Data access microarchitectures for superscalar processors with compiler-assisted data prefetching. </title> <booktitle> In Proceedings of 24th International Symposium on Microarchitecture, </booktitle> <year> 1991. </year>
Reference-contexts: Although the need to make processors tolerant of high memory latency is much more severe in multiprocessors than in uniprocessors, most studies of cache prefetching have concentrated on uniprocessor architectures <ref> [1, 2, 18] </ref>. DASH [13] has hardware support for cache prefetching, but to date they have only published the results of micro-benchmark throughput tests.
Reference: [3] <author> S. Devadas and A.R. </author> <title> Newton. Topological optimization of multiple level array logic. </title> <journal> IEEE Transactions on Computer-Aided Design, </journal> <month> November </month> <year> 1987. </year>
Reference-contexts: We will refer to the latter as an exclusive prefetch. Our simu-lations support both types of prefetches, as in [17]. 3.2 Workload The address traces were generated with MPTrace [7] on a Sequent Symmetry [14], running the following coarse-grained parallel applications, all written in C (see Table 1). Topopt <ref> [3] </ref> performs topological optimization on VLSI circuits using a parallel simulated annealing algorithm. Pverify [15] determines whether two boolean circuits are functionally identical. Statistics on the amount of shared data for these programs can be found in [5]. LocusRoute is a commercial quality VLSI standard cell router.
Reference: [4] <author> S.J. Eggers. </author> <title> Simulation analysis of data sharing in shared memory multiprocessors. </title> <type> Technical Report No. UCB/CSD 89/501 (Ph.D. thesis), </type> <institution> University of California, Berkeley, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: Data Processes Topopt apla.lomim 20 KB 9 Pverify C880.21.berk1/2 130 KB 12 LocusRoute Primary1 1.6 MB 12 Mp3d 10,000 molecules 1.9 MB 12 Water 343 molecules 227 KB 12 Table 1: Workload used in experiments 3.3 Multiprocessor Simulations After the prefetch accesses were added, the traces were run through Charlie <ref> [4] </ref>, a multiprocessor cache simulator, that was modified to handle prefetching and lockup-free caches.
Reference: [5] <author> S.J. Eggers. </author> <title> Simplicity versus accuracy in a model of cache coherency overhead. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 40(8) </volume> <pages> 893-906, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Topopt [3] performs topological optimization on VLSI circuits using a parallel simulated annealing algorithm. Pverify [15] determines whether two boolean circuits are functionally identical. Statistics on the amount of shared data for these programs can be found in <ref> [5] </ref>. LocusRoute is a commercial quality VLSI standard cell router. Mp3d solves a problem involving particle flow at extremely low density. Water evaluates the forces and potentials in a system of water molecules in liquid state. The latter three are part of the Stanford SPLASH benchmarks [21].
Reference: [6] <author> S.J. Eggers and T.E. Jeremiassen. </author> <title> Eliminating false sharing. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <volume> volume I, </volume> <pages> pages 377-381, </pages> <address> St. Charles IL, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Current compiler-directed prefetching algorithms are becoming more effective at predicting non-sharing misses; however, on many multiprocessor workloads, the majority of cache misses are caused by invalidations of (either true or false) shared cache lines <ref> [6] </ref>. Predicting invalidation misses so that they can be accurately prefetched will be more difficult than predicting other types of misses, due to the nondeterministic nature of invalidation traffic. <p> We record a false sharing miss if an invalidation miss is caused by a write from another processor to a word in the local cache line that the local processor has not accessed. Previous results <ref> [23, 6] </ref> demonstrate that false sharing goes up significantly with larger block sizes. In [9], an algorithm is presented for restructuring shared data to reduce false sharing.
Reference: [7] <author> S.J. Eggers, D.R. Keppel, E.J. Koldinger, and H.M. Levy. </author> <title> Techniques for inline tracing on a shared-memory multiprocessor. </title> <booktitle> In Proceedings of the 1990 ACM Sigmetrics, </booktitle> <pages> pages 37-47, </pages> <address> Santa Fe NM, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: We will refer to the latter as an exclusive prefetch. Our simu-lations support both types of prefetches, as in [17]. 3.2 Workload The address traces were generated with MPTrace <ref> [7] </ref> on a Sequent Symmetry [14], running the following coarse-grained parallel applications, all written in C (see Table 1). Topopt [3] performs topological optimization on VLSI circuits using a parallel simulated annealing algorithm. Pverify [15] determines whether two boolean circuits are functionally identical.
Reference: [8] <author> J.L. Hennessy and N.P. Jouppi. </author> <title> Computer technology and architecture: An evolving interaction. </title> <journal> IEEE Computer, </journal> <volume> 24(9) </volume> <pages> 18-29, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Several factors contribute to the increasing need for processors to tolerate high memory latencies, particularly in multiprocessor systems. Certainly the widening gap in speed between CPU's and memory increases memory latencies in uniprocessors and multiprocessors alike <ref> [8] </ref>. But the discrepancy has an added effect on memory subsystem contention in multiprocessors, lengthening the actual latency seen by CPUs, because of CPU queuing for the interconnect. Second, parallel workloads exhibit more interconnect operations, This research was supported by ONR Grant No. N00014-92-J-1395 and NSF PYI Award No.
Reference: [9] <author> T.E. Jeremiassen and S.J. Eggers. </author> <title> Computing per-process summary side-effect information. </title> <booktitle> In Preliminary Proceedings of the Fifth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 115-122, </pages> <address> New Haven CT, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: We record a false sharing miss if an invalidation miss is caused by a write from another processor to a word in the local cache line that the local processor has not accessed. Previous results [23, 6] demonstrate that false sharing goes up significantly with larger block sizes. In <ref> [9] </ref>, an algorithm is presented for restructuring shared data to reduce false sharing. While the technique has promise for improving overall performance, for the purpose of this study we are only interested in whether doing so makes prefetching more viable.
Reference: [10] <author> N.P. Jouppi. </author> <title> Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers. </title> <booktitle> In 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 364-373, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: This implies that the degree of conflict between the prefetched data and the current working set can be significant. The magnitude of this conflict, however, would likely be reduced by a victim cache <ref> [10] </ref> or a set-associative cache. The primary result of a reduction in the number of conflict misses introduced by prefetching would be a reduction in the performance degradations seen in bus saturation.
Reference: [11] <author> A.C. Klaiber and H.M. Levy. </author> <title> An architecture for software-controlled data prefetching. </title> <booktitle> In 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 43-53, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Using a prefetch buffer eliminates conflicts with the current working set in the cache. Although they have been shown to be effective in uniprocessors <ref> [11] </ref>, they are less useful in bus-based shared memory machines. Prefetch buffers typically don't snoop on the bus; therefore, no shared data can be prefetched, unless it can be guaranteed not to be written during the interval in which the load might be executed.
Reference: [12] <author> D. Kroft. </author> <title> Lockup-free instruction fetch/prefetch cache organization. </title> <booktitle> In 8th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 81-87, </pages> <year> 1981. </year>
Reference-contexts: In the best case, the data arrives at the cache before it is needed by the CPU, and the CPU sees its load as a hit. Lockup-free caches <ref> [12, 16, 20, 22] </ref>, which allow the CPU to continue execution during the prefetch, hide the prefetch latency from the CPU.
Reference: [13] <author> D. Lenoski, J. Laudon, T. Joe, D. Nakahira, L. Stevens, A. Gupta, and J. Hennessy. </author> <title> The DASH prototype: Logic overhead and performance. </title> <journal> IEEE Transactions on Parallel and Distributed Systems. </journal> <note> To appear. </note>
Reference-contexts: Although the need to make processors tolerant of high memory latency is much more severe in multiprocessors than in uniprocessors, most studies of cache prefetching have concentrated on uniprocessor architectures [1, 2, 18]. DASH <ref> [13] </ref> has hardware support for cache prefetching, but to date they have only published the results of micro-benchmark throughput tests. A noteworthy exception is the work by Mowry and Gupta [17], in which simulations were driven by three parallel programs, providing analysis of potential speedups with programmer-directed cache prefetching.
Reference: [14] <author> R. Lovett and S. Thakkar. </author> <title> The symmetry multiprocessor system. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages 303-310, </pages> <address> University Park PA, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: We will refer to the latter as an exclusive prefetch. Our simu-lations support both types of prefetches, as in [17]. 3.2 Workload The address traces were generated with MPTrace [7] on a Sequent Symmetry <ref> [14] </ref>, running the following coarse-grained parallel applications, all written in C (see Table 1). Topopt [3] performs topological optimization on VLSI circuits using a parallel simulated annealing algorithm. Pverify [15] determines whether two boolean circuits are functionally identical.
Reference: [15] <author> H-K. T. Ma, S. Devadas, R. Wei, and A. Sangiovanni-Vincentelli. </author> <title> Logic verification algorithms and their parallel implementation. </title> <booktitle> In Proceedings of the 24th Design Automation Conference, </booktitle> <pages> pages 283-290, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: Topopt [3] performs topological optimization on VLSI circuits using a parallel simulated annealing algorithm. Pverify <ref> [15] </ref> determines whether two boolean circuits are functionally identical. Statistics on the amount of shared data for these programs can be found in [5]. LocusRoute is a commercial quality VLSI standard cell router. Mp3d solves a problem involving particle flow at extremely low density.
Reference: [16] <author> Motorola. </author> <title> MC88100 RISC Microprocessor User's Manual. </title> <publisher> Prentice Hall, </publisher> <year> 1990. </year>
Reference-contexts: In the best case, the data arrives at the cache before it is needed by the CPU, and the CPU sees its load as a hit. Lockup-free caches <ref> [12, 16, 20, 22] </ref>, which allow the CPU to continue execution during the prefetch, hide the prefetch latency from the CPU.
Reference: [17] <author> T.C. Mowry and A. Gupta. </author> <title> Tolerating latency through software-controlled prefetching in shared-memory multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12(2) </volume> <pages> 87-106, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: DASH [13] has hardware support for cache prefetching, but to date they have only published the results of micro-benchmark throughput tests. A noteworthy exception is the work by Mowry and Gupta <ref> [17] </ref>, in which simulations were driven by three parallel programs, providing analysis of potential speedups with programmer-directed cache prefetching. <p> We will refer to the latter as an exclusive prefetch. Our simu-lations support both types of prefetches, as in <ref> [17] </ref>. 3.2 Workload The address traces were generated with MPTrace [7] on a Sequent Symmetry [14], running the following coarse-grained parallel applications, all written in C (see Table 1). Topopt [3] performs topological optimization on VLSI circuits using a parallel simulated annealing algorithm. <p> For the other workloads, the average processor utilization for Topopt ranged from .65 to .59, Locus-Route ranged from .64 to .54, and Pverify from .41 to .18. We report much smaller multiprocessor performance improvements than Mowry and Gupta <ref> [17] </ref>, particularly for our lower-throughput memory subsystems. The reasons stem from several differences in our approaches. First, they eliminated bus contention from their model by simulating only one processor per cluster. <p> This is because most of the leading references to shared lines are not writes. A compiler might recognize when a read is followed immediately by a write and make more effective use of the exclusive prefetch feature. Mowry and Gupta <ref> [17] </ref> take advantage of this in their programmer-directed prefetching study. 4.4 Invalidation Misses The most striking result from Figure 3 is that the limit to effective prefetching (in fact, the limit to performance in general) is invalidation misses on shared data.
Reference: [18] <author> T.C. Mowry, M.S. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 62-73, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Although the need to make processors tolerant of high memory latency is much more severe in multiprocessors than in uniprocessors, most studies of cache prefetching have concentrated on uniprocessor architectures <ref> [1, 2, 18] </ref>. DASH [13] has hardware support for cache prefetching, but to date they have only published the results of micro-benchmark throughput tests. <p> Section 4 presents the results. The conclusions appear in section 5. 2 The Dangers of Prefetching Although prefetching has been shown to be effective for certain applications and architectures, even on a uniprocessor it can cause a loss in performance if not done carefully <ref> [18] </ref>. To understand what makes prefetching more challenging on a multiprocessor, we must understand its potential drawbacks, both on uniprocessor and multiprocessor systems and work-loads. Some prefetching costs affect both types of machines. <p> Also, prefetching results in code expansion from both the prefetch instructions and code for techniques, such as loop splitting, that are used to implement many prefetching algorithms. The former can significantly increase execution time <ref> [18] </ref> and the latter can increase, though likely minimally, the miss rate of instruction or unified caches. Prefetching has additional costs on shared memory multiprocessors. Prefetching attempts to increase processor utilization by lowering the CPU miss rate, usually a win in a uniprocessor. <p> Our results indicate that increasing the prefetch distance to the point that virtually all prefetches complete does not pay off in performance. Mowry, et al. <ref> [18] </ref> also studied prefetch distance, noting that only one of their programs degraded with increasing prefetch distance, but they had to restructure four others to eliminate the conflicts that are causing this phenomenon.
Reference: [19] <author> M.S. Papamarcos and J.H. Patel. </author> <title> A low-overhead coherence solution for multiprocessors with private cache memories. </title> <booktitle> In 11th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 348-354, </pages> <year> 1984. </year>
Reference-contexts: Our simulations included both private and shared data, in order to include the effects of interference between the two in the cache. The cache coherency scheme is the Illinois coherency protocol <ref> [19] </ref>. Its most important feature for our purposes is that it has a private-clean state for exclusive prefetches. We simulate a 16-deep prefetch instruction buffer, which was sufficiently large to almost always prevent the processor from stalling because the buffer was full.
Reference: [20] <author> C. Scheurich and M. Dubois. </author> <title> Lockup-free caches in high-performance multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 11(1) </volume> <pages> 25-36, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: In the best case, the data arrives at the cache before it is needed by the CPU, and the CPU sees its load as a hit. Lockup-free caches <ref> [12, 16, 20, 22] </ref>, which allow the CPU to continue execution during the prefetch, hide the prefetch latency from the CPU.
Reference: [21] <author> J.P. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared-memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <year> 1991. </year>
Reference-contexts: LocusRoute is a commercial quality VLSI standard cell router. Mp3d solves a problem involving particle flow at extremely low density. Water evaluates the forces and potentials in a system of water molecules in liquid state. The latter three are part of the Stanford SPLASH benchmarks <ref> [21] </ref>. Restricted by the practical limit on trace lengths in multiprocessor trace-driven simulations, a balance must be struck between the desire for large data sets that don't fit in the cache and tracing a reasonable portion of the program.
Reference: [22] <author> G.S. Sohi and M. Franklin. </author> <title> High-bandwidth data memory systems for superscalar processor. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 53-62, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: In the best case, the data arrives at the cache before it is needed by the CPU, and the CPU sees its load as a hit. Lockup-free caches <ref> [12, 16, 20, 22] </ref>, which allow the CPU to continue execution during the prefetch, hide the prefetch latency from the CPU.
Reference: [23] <author> J. Torrellas, M.S. Lam, and J.L. Hennessy. </author> <title> Shared data placement optimizations to reduce multiprocessor cache miss rates. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 266-270, </pages> <address> St. Charles IL, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: We record a false sharing miss if an invalidation miss is caused by a write from another processor to a word in the local cache line that the local processor has not accessed. Previous results <ref> [23, 6] </ref> demonstrate that false sharing goes up significantly with larger block sizes. In [9], an algorithm is presented for restructuring shared data to reduce false sharing.
References-found: 23

