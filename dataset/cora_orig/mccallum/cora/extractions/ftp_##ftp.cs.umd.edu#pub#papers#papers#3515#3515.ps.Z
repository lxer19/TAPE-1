URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3515/3515.ps.Z
Refering-URL: http://www.cs.indiana.edu/cstr/search/?Knowledge+discovery+MINK%3D2
Root-URL: 
Title: Fast Sequential and Parallel Algorithms for Association Rule Mining: A Comparison 1  
Author: Andreas Mueller 
Address: Maryland-College Park College Park, MD 20742  
Affiliation: Department of Computer Science University of  
Date: August 1995  
Pubnum: CS-TR-3515  
Abstract: The field of knowledge discovery in databases, or "Data Mining", has received increasing attention during recent years as large organizations have begun to realize the potential value of the information that is stored implicitly in their databases. One specific data mining task is the mining of Association Rules, particularly from retail data. The task is to determine patterns (or rules) that characterize the shopping behavior of customers from a large database of previous consumer transactions. The rules can then be used to focus marketing efforts such as product placement and sales promotions. Because early algorithms required an unpredictably large number of IO operations, reducing IO cost has been the primary target of the algorithms presented in the literature. One of the most recent proposed algorithms, called PARTITION, uses a new TID-list data representation and a new partitioning technique. The partitioning technique reduces IO cost to a constant amount by processing one database portion at a time in memory. We implemented an algorithm called SPTID that incorporates both TID-lists and partitioning to study their benefits. For comparison, a non-partitioning algorithm called SEAR, which is based on a new prefix-tree data structure, is used. Our experiments with SPTID and SEAR indicate that TID-lists have inherent inefficiencies; furthermore, because all of the algorithms tested tend to be CPU-bound, trading CPU-overhead against I/O operations by partitioning did not lead to better performance. In order to scale mining algorithms to the huge databases (e.g., multiple Terabytes) that large organizations will manage in the near future, we implemented parallel versions of SEAR and SPEAR (its partitioned counterpart). The performance results show that, while both algorithms parallelize easily and obtain good speedup and scale-up results, the parallel SEAR version performs better than parallel SPEAR, despite the fact that it uses more communication. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Rakesh Agrawal, Sakti Ghosh, Tomasz Imielinski, and Arun Swami. </author> <title> An interval classifier for database mining applications. </title> <booktitle> In 18th Int'l Conf. on Very Large Databases (VLDB), </booktitle> <address> Vancouver, Canada, </address> <pages> pages 560-573, </pages> <year> 1992. </year>
Reference-contexts: For this reason, neural nets prohibit the use of a database query language for optimized retrieval of items that match a rule. Instead of fast indexed selection, a scan over the entire database is necessary. As argued in <ref> [1] </ref> symbolic learning outruns neural nets because multiple passes over the training set are necessary in the learning process. <p> The reader is referred to <ref> [1, 2, 14, 31] </ref> for more information. A basic familiarity with decision trees as constructed by classification algorithms and general AI search techniques is assumed. The supervised learning case is presupposed because both approaches presented here solve this problem.
Reference: [2] <author> Rakesh Agrawal, Tomasz Imielinski, and Arun Swami. </author> <title> Database mining: a performance perspective. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> Vol. 5, No. 6, </volume> <month> December </month> <year> 1993:914 </year> <month> - 925, </month> <year> 1993. </year>
Reference-contexts: Frawley in [15] demands that the statement of the discovered pattern be somewhat simpler than the subset of data objects it describes. What simpler means is left vague intentionally. Length of encoding or other information theoretic measures seem reasonable and have been used widely <ref> [11, 36, 2] </ref>. In short, we are interested in facts about data, and the term knowledge shall be used with this meaning from now on. The last requirement potentially useful is highly dependent on the application and even on the special focus of the current mining task. <p> The last requirement potentially useful is highly dependent on the application and even on the special focus of the current mining task. This issue will be revisited in later sections when background knowledge and related work are discussed (Section 1.1.3 and Section 7.2.1). R. Agrawal in <ref> [2] </ref> identifies the three types of knowledge to be discovered in databases: classification, association and sequences. Classification tries to divide the given data set into disjoint classes using supervised or unsupervised learning, the latter also being referred to as clustering. <p> Associations can be arbitrary rules of the form X ! Y , X and Y being conjunctions of attribute value restrictions. We have already introduced this area and will treat it in depth in Section 2.1.1. <ref> [2] </ref> proposes a unified framework for all three areas, arguing that they all are variations of a basic rule discovery task and can be reduced to a set of standard operations. <p> Note that this computation is based on the set inclusion above. In general, confidence values cannot be multiplied in this manner. 13 Property 2.7 (Inferring Whether Rules Hold) <ref> [2] </ref> show that if a rule A ! (L A) does not have minimum confidence, neither does B ! (L B) for itemsets L; A; B and B A . <p> The reader is referred to <ref> [1, 2, 14, 31] </ref> for more information. A basic familiarity with decision trees as constructed by classification algorithms and general AI search techniques is assumed. The supervised learning case is presupposed because both approaches presented here solve this problem.
Reference: [3] <author> Rakesh Agrawal, Tomasz Imielinski, and Arun Swami. </author> <title> Mining association rules between sets of items in large databases. </title> <booktitle> In SIGMOD, </booktitle> <address> Washington D.C., </address> <pages> pages 207-216, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: First, however, we introduce frequent sets which form the basis for generating the actual association rules. Except where stated otherwise we follow the initial terminology used by <ref> [3] </ref>. 2.1.1 Definition of Frequent Sets Given a set I = fI 1 ; I 2 ; : : : ; I m g of items (e.g. a set of items sold by a retail store) a transaction 1 T is defined as any subset of items in I ( I). <p> Different from its usual context in DBMS, the term applies to data about consumer transactions. 2 <ref> [3] </ref> in D and other early work refer to frequent sets as large sets, but since this has lead to confusion with the cardinality of the set we chose to adopt the change in terminology proposed recently in [37]. 11 Property 2.1 (Support for Subsets) If A B for itemsets A; <p> We describe their advantages and disadvantages as they are introduced and argue how the choice of representation influences the performance of the algorithms that use them. 2.3.1 AIS The problem of association rules was first introduced in <ref> [3] </ref> along with an algorithm that was later called AIS by the authors [4]. To find frequent sets, AIS creates candidates on-the-fly while it reads the database. Several passes are necessary, and during one pass, the entire database is read one transaction after the other. <p> For k-extensions, for example, only maximal frequent sets become frontier sets. For the details we refer the reader to <ref> [3] </ref> 16 counting support by means of aggregation and then deleting infrequent sets.
Reference: [4] <author> Rakesh Agrawal and Ramakrishnan Srikant. </author> <title> Fast algorithms for mining association rules in large databases. </title> <booktitle> In 20th Int'l Conf. on Very Large Databases (VLDB), </booktitle> <address> Santiago, Chile, </address> <month> Sept. </month> <year> 1994. </year> <note> Expanded version available as IBM Research Report RJ9839, </note> <month> June </month> <year> 1994. </year>
Reference-contexts: Unfortunately, the number of possible association rules grows exponentially with the number of items considered. For 1000 items, for example, more than 2 1000 rules have to be considered in a naive approach. Several algorithms have been proposed in the literature to make this search more effective, i.e. Apriori <ref> [4] </ref> and PARTITION [34]. Algorithms differ mainly with respect to the internal data representations used for intermediate results and the IO cost and CPU-overhead they incur. Like all algorithms before it, Apriori needs to scan the entire database several times. <p> We describe their advantages and disadvantages as they are introduced and argue how the choice of representation influences the performance of the algorithms that use them. 2.3.1 AIS The problem of association rules was first introduced in [3] along with an algorithm that was later called AIS by the authors <ref> [4] </ref>. To find frequent sets, AIS creates candidates on-the-fly while it reads the database. Several passes are necessary, and during one pass, the entire database is read one transaction after the other. A candidate is created by adding items to sets that were found to be frequent in previous passes. <p> But these candidates are never considered by SETM. In spite of this advantage, as reported in <ref> [4] </ref>, the inefficiencies of SETM outweigh by far those of AIS. 2.3.3 Apriori, AprioriTid and AprioriHybrid Candidate Generation: Apriori-gen The vast number of candidates AIS creates caused its authors to develop a new candidate generation strategy called Apriori-gen 4 as part of the algorithms Apriori and AprioriTid [4]. <p> as reported in <ref> [4] </ref>, the inefficiencies of SETM outweigh by far those of AIS. 2.3.3 Apriori, AprioriTid and AprioriHybrid Candidate Generation: Apriori-gen The vast number of candidates AIS creates caused its authors to develop a new candidate generation strategy called Apriori-gen 4 as part of the algorithms Apriori and AprioriTid [4]. Apriori-gen has been so successful in reducing the number candidates that it was used in every algorithm that was proposed since it was published [19, 32, 34, 37]. <p> AprioriTid The shortcoming of Apriori, that it could not remove unwanted parts of the database during later passes, has prompted its authors to develop AprioriTid, which uses a different data representation than the item-lists used by Apriori. AprioriTid 5 <ref> [4] </ref> can be considered an optimized version of SETM that does not rely on standard database operations and uses Apriori-gen for faster candidate generation. Furthermore, AprioriTid reads the data only once and tries to buffer the data for all other passes. <p> Unfortunately, it is slowed mainly in the second pass if its data does not fit in memory as a consequence of the candidate-list representation and swapping is necessary. In this case Apriori beats AprioriTid. For this reason another algorithm, AprioriHybrid, is proposed in <ref> [4] </ref>, that uses Apriori for the initial passes and switches to AprioriTid as soon as the data is expected to fit in memory. The switch takes an extra effort to transform one representation into the other that has to be balanced by savings in later passes. <p> In fact, the pseudo-code for SEAR (Algorithm 3.1) is the same as for Apriori. However, SEAR uses the prefix-tree data structure for itemsets, which we developed to improve the data structure used by Apriori, and the pass bundling optimization, which is mentioned briefly in the literature <ref> [4, 28] </ref>, but has not been investigated more closely. Prefix-trees and pass bundling are both described below. 3.1.1 Prefix Trees: Storage for Sets and Candidates Recall how Apriori stores candidates in the tree-like data structure depicted in Figure 2.4. <p> In this structure, nodes do not contain sets, but only information about sets (e.g. counters). Each edge in the tree is labeled with an item, and each node contains the information for the set of items labeling 1 In <ref> [4, 28] </ref> this optimization is mentioned, but not given a specific name. 28 Algorithm 3.1 SEAR L 0 = ;; k = 1 C 1 = ffigji 2 Ig fall 1-itemsetsg while ( C k 6= ; ) do fcount supportg forall transactions T 2 D forall k-subsets t T if <p> To avoid this waste of effort, SEAR allows the expansion of several levels of the tree before counting candidates. Thus, all three candidates in our example 31 will be created and counted together. This technique, which we call pass bundling, was first mentioned in [28] and <ref> [4] </ref>, but its effects were not examined there. There is a tradeoff involved between the number of IO operations we save and the additional computation necessary to count the additional candidates. Clearly, pass bundling of several levels is only desirable in later passes when the number of candidates is small. <p> Of course, the clock is turned off for these writes. 4.1 Synthetic Data Generation All the following experiments were performed on synthetic data generated according to the procedure outlined in <ref> [4] </ref> which was designed to model the buying behavior of consumers in a retail environment. Note that our notation varies slightly from the one used there. <p> Since this distribution determines how similar frequent sets are, the parameter c is called correlation level. The effects of different corruption levels and correlation levels were reported to be rather insignificant in <ref> [4] </ref>, which is why we did not conduct experiments that investigate these issues and used the values proposed there. Table 4.1 lists the different parameters. A naming convention for data sets is customary for easier reference.
Reference: [5] <author> Marko Bohanec and Ivan Bratko. </author> <title> Trading accuracy for simplicity in decision trees. </title> <journal> Machine Learning, </journal> <volume> 15 </volume> <pages> 223-250, </pages> <year> 1994. </year>
Reference-contexts: It is an open question as to which methods are optimal for deciding which leaves to expand and when to stop expanding the tree altogether to avoid poor performance due to over-fitting on the one hand or incomplete trees on the other <ref> [5, 27, 35, 39] </ref>.
Reference: [6] <author> H. Boral, W. Alexander, L. Clay, G. Copeland, S. Danforth, M. Franklin, B. Hart, M. Smith, and P. Valduriez. </author> <title> Prototyping bubba, a highly parallel database system. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 2(1), </volume> <month> March </month> <year> 1990. </year>
Reference-contexts: We assume the 2 short for Single Program Multiple Data 5 shared-nothing paradigm that is commonly used in parallel databases <ref> [12, 13, 6] </ref> and the nature of the problem suits this assumption well.
Reference: [7] <author> Yandong Cai, Nick Cercone, and Jaiwei Han. </author> <title> Attribute-oriented induction in relational databases. </title> <editor> In Gregory Piatetsky-Shapiro and William J. Frawley, editors, </editor> <booktitle> Knowledge Discovery in Databases, </booktitle> <pages> pages 213-228. </pages> <publisher> AAAI/MIT, </publisher> <year> 1991. </year>
Reference-contexts: This is necessary when the current structure is too general, specific or unbalanced and thus causes distorted results. Concept hierarchies have been used in classification mining before, the most prominent example of which is attribute oriented induction that is realized in the DBLEARN system <ref> [7, 17] </ref>. Here aggregate relations are built successively by replacing values by their ancestors in the hierarchy. In the context of association mining, concept hierarchies can be viewed as directed acyclic graphs, the nodes of which are labeled with the literals from I.
Reference: [8] <author> Philip K. Chan and Salvatore J.Stolfo. </author> <title> Toward multi-strategy parallel and distributed learning in sequence analysis. </title> <booktitle> In Proc. First Intl. Conf. Intel. Sys. Molecular Biology, </booktitle> <pages> pages 65-73, </pages> <year> 1993. </year>
Reference-contexts: Since we assume a shared-nothing architecture and the ARM algorithms are more row- than column-oriented, vertical partitioning of the database is unlikely to yield satisfactory results for association mining. 7.3.3 Meta-Learning and Multi-Strategy-Learning Unlike the previous approach, meta-learning <ref> [8, 9, 10] </ref> 2 does not parallelize the mining algorithm itself but involves horizontally dividing the database into subsets and applying sequential learning algorithms to each subset. The result is a set of different decision trees each depending on the data it was trained on. <p> Nevertheless, the strength of this approach lies in the possibility to incorporate different classification methods as "leaf"-classifiers to improve classification performance by means of diversity. This version of meta-learning is then referred to as multi-strategy-learning. 2 Sequence analysis in <ref> [8] </ref> refers to discovery in DNA sequences, but classification algorithms are used. 69 To our knowledge, no attempts have been made to apply the meta-learning principle to ARM, and although the partitioning principle is used in PARTITION (see Section 2.3.4), intermediate and not final results are coalesced here which constitutes a
Reference: [9] <author> Philip K. Chan and Salvatore J.Stolfo. </author> <title> Toward parallel and distributed learning by meta-learning. </title> <booktitle> In Working Notes AAAI Work. Knowledge Discovery in Databases, </booktitle> <pages> pages 227-24, </pages> <year> 1993. </year>
Reference-contexts: Since we assume a shared-nothing architecture and the ARM algorithms are more row- than column-oriented, vertical partitioning of the database is unlikely to yield satisfactory results for association mining. 7.3.3 Meta-Learning and Multi-Strategy-Learning Unlike the previous approach, meta-learning <ref> [8, 9, 10] </ref> 2 does not parallelize the mining algorithm itself but involves horizontally dividing the database into subsets and applying sequential learning algorithms to each subset. The result is a set of different decision trees each depending on the data it was trained on.
Reference: [10] <author> Philip K. Chan and Salvatore J. Stolfo. </author> <title> Toward scalable and parallel learning: A case study in splice junction prediction. </title> <type> Technical Report CUCS-032-94, </type> <institution> Columbia University, </institution> <year> 1994. </year>
Reference-contexts: Since we assume a shared-nothing architecture and the ARM algorithms are more row- than column-oriented, vertical partitioning of the database is unlikely to yield satisfactory results for association mining. 7.3.3 Meta-Learning and Multi-Strategy-Learning Unlike the previous approach, meta-learning <ref> [8, 9, 10] </ref> 2 does not parallelize the mining algorithm itself but involves horizontally dividing the database into subsets and applying sequential learning algorithms to each subset. The result is a set of different decision trees each depending on the data it was trained on. <p> Therefore, while building the individual classifiers in parallel is done efficiently, a considerable amount of time has to be spent to build a suitable arbitrator. Theoretical speedups of O (p= lgp) can be obtained on p processors and results reported in <ref> [10] </ref> reach this boundary. Apart from the rather moderate speedups, we see two shortcomings of this approach: One is that using the classifier for unseen data is slow, because all classifiers have to be applied to the data item, and these results have to be processed by the arbiter tree.
Reference: [11] <author> David K. Chiu, Andrew K.C. Wong, and Benny Cheung. </author> <title> Information discovery through hierarchical maximum enthropy discretization and synthesis. </title> <editor> In Gregory Piatetsky-Shapiro and William J. Frawley, editors, </editor> <booktitle> Knowledge Discovery in Databases, </booktitle> <pages> pages 125-140. </pages> <publisher> AAAI/MIT, </publisher> <year> 1991. </year>
Reference-contexts: Frawley in [15] demands that the statement of the discovered pattern be somewhat simpler than the subset of data objects it describes. What simpler means is left vague intentionally. Length of encoding or other information theoretic measures seem reasonable and have been used widely <ref> [11, 36, 2] </ref>. In short, we are interested in facts about data, and the term knowledge shall be used with this meaning from now on. The last requirement potentially useful is highly dependent on the application and even on the special focus of the current mining task.
Reference: [12] <author> David DeWitt and J. Gray. </author> <title> Parallel database systems: The future of database processing or a passing fad? sigmod, </title> <booktitle> 19(4) </booktitle> <pages> 104-112, </pages> <month> December </month> <year> 1990. </year> <month> 74 </month>
Reference-contexts: We assume the 2 short for Single Program Multiple Data 5 shared-nothing paradigm that is commonly used in parallel databases <ref> [12, 13, 6] </ref> and the nature of the problem suits this assumption well. <p> Figure 6.5 confirms this expectation for a range from 500,000 to 40 million transactions. All other parameters were chosen as above. 1 In addition to speed-up and scale-up, this term is used in <ref> [12] </ref> as third criteria to evaluate parallel database performance for the case when the number of processors remains constant while the problem size is increased. 63 64 Chapter 7 Extensions and Other Related Work The following sections outline the current work on concept hierarchies and parallel classification, which is not directly
Reference: [13] <author> David J. DeWitt, Shaharm Ghandeharizadeh, Donovan A. Schneider, Allan Bricker, Hui i Hsiao, and Rick Rasmusen. </author> <title> The gamma database machine project. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 2(1) </volume> <pages> 44-62, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: We assume the 2 short for Single Program Multiple Data 5 shared-nothing paradigm that is commonly used in parallel databases <ref> [12, 13, 6] </ref> and the nature of the problem suits this assumption well.
Reference: [14] <author> Tapio Elomaa. </author> <title> In defense of c4.5: Notes on learning one-level decision trees. </title> <editor> In W.Cohen and H. Hirsh, editors, </editor> <booktitle> Machine Learning: Proc. 11th Int'l Conference, </booktitle> <pages> pages 62-60. </pages> <publisher> Morgan Kaufmann, </publisher> <month> July </month> <year> 1994. </year>
Reference-contexts: The reader is referred to <ref> [1, 2, 14, 31] </ref> for more information. A basic familiarity with decision trees as constructed by classification algorithms and general AI search techniques is assumed. The supervised learning case is presupposed because both approaches presented here solve this problem.
Reference: [15] <author> William J. Frawley, Gregory Piatetsky-Shapiro, and Christopher J. Matheus. </author> <title> Knowledge discovery in databases: An overview. </title> <editor> In Gregory Piatetsky-Shapiro and William J. Frawley, editors, </editor> <booktitle> Knowledge Discovery in Databases, </booktitle> <pages> pages 1-30. </pages> <publisher> AAAI/MIT, </publisher> <year> 1991. </year>
Reference-contexts: This kind of knowledge could also be called "meta-knowledge": patterns that characterize the data | hidden laws and structures that need not be strict functional dependencies, but may hold only with a certain probability. Frawley in <ref> [15] </ref> demands that the statement of the discovered pattern be somewhat simpler than the subset of data objects it describes. What simpler means is left vague intentionally. Length of encoding or other information theoretic measures seem reasonable and have been used widely [11, 36, 2]. <p> A bank, for example, might want to classify its credit customers to determine whether to give loans or not. In <ref> [15] </ref> W. Frawley and G. Piatetski-Shapiro subdivide this task into summarization, that searches for common characteristic features of one class only, and discrimination where the goal is to find features that help distinguish different classes or alternatively one class from all others. <p> Hybrid approaches or mining algorithms that rely heavily on database functionality [16] are also possible of course. Some argue that mining capabilities should not be provided in applications separate from the actual database but be an integral part of a DBMS's capabilities <ref> [15] </ref>. According to this vision future DBMS should be able to provide the user with more insight on the data than just selection and aggregation.
Reference: [16] <author> Jiawei Han, Yandong Cai, and Nick Cercone. </author> <title> Knowledge discovery in databases: An attribute-oriented approach. </title> <booktitle> In 18th Int'l Conf. on Very Large Databases (VLDB), </booktitle> <address> Vancouver, Canada, </address> <year> 1992. </year>
Reference-contexts: The attribute can therefore be removed entirely before the mining algorithm is invoked. The problem of selecting and exploiting this knowledge is being investigated. Examples of domain knowledge are the use of concept hierarchies, user-defined predicates, and automated selection of relevant attributes to reduce data sizes and running time <ref> [16] </ref>. 1.1.4 KDD System Architecture Internal and External Miners A mining tool can be viewed as an application that uses the DBMS as server for its data management requirements. <p> Hybrid approaches or mining algorithms that rely heavily on database functionality <ref> [16] </ref> are also possible of course. Some argue that mining capabilities should not be provided in applications separate from the actual database but be an integral part of a DBMS's capabilities [15].
Reference: [17] <author> Jiawei Han, Yandong Cai, and Nick Cercone. </author> <title> Data-driven discovery of quantitative rules in relational databases. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 5(1), </volume> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: This is necessary when the current structure is too general, specific or unbalanced and thus causes distorted results. Concept hierarchies have been used in classification mining before, the most prominent example of which is attribute oriented induction that is realized in the DBLEARN system <ref> [7, 17] </ref>. Here aggregate relations are built successively by replacing values by their ancestors in the hierarchy. In the context of association mining, concept hierarchies can be viewed as directed acyclic graphs, the nodes of which are labeled with the literals from I.
Reference: [18] <author> Jiawei Han and Yongjian Fu. </author> <title> Dynamic generation and refinement of concept hierarchies for knowledge discovery in databases. </title> <booktitle> In AAAI'94 Workshop on Knowledge Discovery in Databases (KDD'94), </booktitle> <address> Seattle, WA, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: If neither is feasible, methods are available to create concept hierarchies from data automatically and to modify existing hierarchies to suit the current mining task if for example only a part of the data is being examined <ref> [18] </ref>. This is necessary when the current structure is too general, specific or unbalanced and thus causes distorted results. Concept hierarchies have been used in classification mining before, the most prominent example of which is attribute oriented induction that is realized in the DBLEARN system [7, 17].
Reference: [19] <author> Jiawei Han and Yongjian Fu. </author> <title> Discovery of multiple-level association rules from large databases. </title> <booktitle> In 21st Int'l Conf. on Very Large Databases (VLDB), </booktitle> <address> Zurich, Switzerland, </address> <month> Sept. </month> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: Association rule mining (ARM) is one problem in KDD that has received considerable attention during the past year, as demonstrated by the large number of new publications <ref> [19, 32, 34, 37] </ref>. As with many other problems in KDD, a certain type of database and a special application has lead to specialized algorithms for this problem. <p> Apriori-gen has been so successful in reducing the number candidates that it was used in every algorithm that was proposed since it was published <ref> [19, 32, 34, 37] </ref>. The underlying principle, based on Property 2.7, is to generate only those candidates for which all subsets have been previously determined to be frequent. In particular, a (k+1)-candidate will be accepted only if all its k-subsets are frequent. <p> Therefore larger numbers of items become tractable. The content of this chapter is based on <ref> [19, 37] </ref>where different algorithms to include concept hierarchies 1 . After general remarks we provide a short sketch of these algorithms in Section 7.2.2 7.2.1 New Definitions for Interestingness Based on concept hierarchies new criteria for the interestingness of a rule can be applied. <p> Since the ancestor rule is more general, it should be preferred. Recalling the cereal example, rules involving individual cereal brands can be considered redundant unless their confidence or support deviate significantly from the ancestor rule Cereal ! Milk. Details can be found in [37]. <ref> [19] </ref> introduce the notion of strong rules to focus the search in several hierarchy levels. A rule is strong if every ancestor of every item in its antecedent and consequent is frequent on its level in the taxonomy. <p> This can dramatically increase the size of transactions and the number of possible 1 [37] refer to their treatment of taxonomies as generalized association rules while <ref> [19] </ref> use the term multiple-level association rules are proposed. The two are used synonymously here, along with hierarchical association rules. 67 frequent sets. Therefore several methods including sampling are proposed to prune the candidate set and avoid unnecessary passes over the database due to the large number of candidates. <p> The two are used synonymously here, along with hierarchical association rules. 67 frequent sets. Therefore several methods including sampling are proposed to prune the candidate set and avoid unnecessary passes over the database due to the large number of candidates. The algorithms in <ref> [19] </ref> encode the hierarchy in the representation of the leaf literal and work on this encoded transaction table after the initial translation step. <p> Moreover, speed-ups improve further as the database size is increased and the mining problem is made more difficult. 71 Unfortunately, we were limited to experiments on synthetic data sets (like many other previous studies <ref> [19, 32, 34] </ref>), so the next task is to evaluate the performance of our algorithms against a real-life data set.
Reference: [20] <author> Marcel Holsheimer and Arno P.J.M. Siebes. </author> <title> Architectural support for data mining. </title> <type> Technical Report CS-R9429, </type> <institution> CWI, </institution> <year> 1994. </year>
Reference-contexts: Two approaches are presented here: one parallel system features a sequential external mining tool that uses a parallel databases server, and the other approach, called meta-learning, allows the actual mining to be done in parallel. 7.3.2 A Parallel Mining Engine The classification tool developed at CWI <ref> [20] </ref> merely directs the discovery process; the expensive histogram building procedures are incorporated into the extendable parallel DBMS Monet, which is designed to exploit large amounts of main memory on shared-memory multi-processors. Experiments were run on a 6-node SGI machine with 256 MBytes of main memory. <p> Hence, selection to create partitions and the construction of histograms can be done in parallel. Also the successive subdivision of horizontal partitions suggests assigning individual processors to different partitions. Results in <ref> [20] </ref> show "considerable", although not linear speed-up (factor 2.5 on 4 nodes on a 25K database) , but the authors are rather brief on this issue. In particular, issues like scalability, skew and load balancing and the impact of communication overhead are not addressed.
Reference: [21] <author> Marcel Holsheimer and Arno P.J.M. Siebes. </author> <title> Data mining: the search for knowledge in databases. </title> <type> Technical Report CS-R9406, </type> <institution> CWI, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: We proceed to list the additional difficulties that arise with databases as data source for pattern discovery algorithms. This section ends with more general issues and a proposed architecture for KDD systems. For more detailed introductions to KDD see <ref> [21, 40] </ref>. 1.1.1 Types of Knowledge Recalling the aforementioned definition of KDD by W.Frawley that discovered information is implicit, previously unknown and potentially useful we can state what "knowledge" means in our context.
Reference: [22] <author> Houtsma and Arun Swami. </author> <title> Set-oriented mining of association rules. </title> <type> Technical Report RJ 9567, </type> <institution> IBM Research Report, </institution> <month> Oct. </month> <year> 1993. </year>
Reference-contexts: This limitation was not justified by the algorithm itself because it finds all frequent sets, and this information is enough to produce rules without this limitation. 2.3.2 SETM SETM <ref> [22] </ref> is designed to use only standard database operations to find frequent sets. For this reason, it uses its own data representation that stores every itemset supported by a transaction along with the transaction's TID.
Reference: [23] <institution> IBM Corporation, Kingston, NJ 12401-1099. IBM AIX Parallel Environment, Release 2.0, </institution> <year> 1994. </year>
Reference: [24] <institution> IBM Thomas J. Watson Research Center, </institution> <address> P.O. Box 218, Yorktown Heights, NY 10598. </address> <booktitle> The SP2 Communication Subsystem, </booktitle> <month> Aug. </month> <year> 1994. </year> <note> Also available via http://ibm.tc.cornell.edu/bm/pps/doc/css/css.ps. </note>
Reference: [25] <author> Kenneth Kaufman, Ryszard S. Michalski, and Larry Kerschberg. </author> <title> Mining for knowledge in databases: Goals and general description of the inlen system. </title> <editor> In Gregory Piatetsky-Shapiro and William J. Frawley, editors, </editor> <booktitle> Knowledge Discovery in Databases, </booktitle> <pages> pages 449-464. </pages> <publisher> AAAI/MIT, </publisher> <year> 1991. </year>
Reference-contexts: Interesting questions that arise in this context are concerned with the tradeoffs between support for OLTP and the mining algorithm's needs for fast access to the entire database. In other words, the question is how to couple knowledge base and database ? <ref> [25] </ref> A Model for KDD We conclude this chapter by presenting a model which reflects most of the observations made in previous sections. This section follows roughly the idealized model for KDD systems proposed in [30].
Reference: [26] <author> Mika Klemettinen, Heikki Mannila, Pirjo Ronkainen, Hannu Toivonen, and A. Inkeri Verkamo. </author> <title> Finding interesting rules from large sets of discovered association rules. </title> <booktitle> In 3 rd Internat. Conf. on Information and Knowledge Management, </booktitle> <address> Maryland. </address> <publisher> ACM Press, </publisher> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: An example from association rule discovery that investigates rule templates and visualization of results can be found in <ref> [26] </ref>. It is not clear just how much user interaction is desired or necessary, and the answer to this question may very well be dependent on the mining domain.
Reference: [27] <author> Wei Zhong Liu and Allan P. White. </author> <title> The importance of attribute selection measures in decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 15 </volume> <pages> 25-41, </pages> <year> 1994. </year>
Reference-contexts: It is an open question as to which methods are optimal for deciding which leaves to expand and when to stop expanding the tree altogether to avoid poor performance due to over-fitting on the one hand or incomplete trees on the other <ref> [5, 27, 35, 39] </ref>.
Reference: [28] <author> Heikki Mannila, Hannu Toivonen, and A. Inkeri Verkamo. </author> <title> Improved methods for finding association rules. </title> <booktitle> In AAAI Worshop on Knowledge Discovery, </booktitle> <address> Seattle, Washington, </address> <pages> pages 181-192, </pages> <month> July </month> <year> 1994. </year> <month> 75 </month>
Reference-contexts: The rapid growth of database sizes also calls for higher mining speeds than are currently available. Sampling is one solution to this problem <ref> [28] </ref>, but looking at only a portion of the database will not produce exact results. <p> Since the 1-candidates are simply all the sets that contain only one item, this procedure is not necessary to generate them. Apriori-gen is first used to generate C 2 from L 1 . 4 Very much the same idea was suggested independently in <ref> [28] </ref> where it was called off-line candidate determination (OCD) 18 The second novelty of Apriori-gen (and the one leading to its name) is that candidate generation is done prior to and separate from the counting step, and the algorithm is only called once to create the candidates of a given size. <p> In fact, the pseudo-code for SEAR (Algorithm 3.1) is the same as for Apriori. However, SEAR uses the prefix-tree data structure for itemsets, which we developed to improve the data structure used by Apriori, and the pass bundling optimization, which is mentioned briefly in the literature <ref> [4, 28] </ref>, but has not been investigated more closely. Prefix-trees and pass bundling are both described below. 3.1.1 Prefix Trees: Storage for Sets and Candidates Recall how Apriori stores candidates in the tree-like data structure depicted in Figure 2.4. <p> In this structure, nodes do not contain sets, but only information about sets (e.g. counters). Each edge in the tree is labeled with an item, and each node contains the information for the set of items labeling 1 In <ref> [4, 28] </ref> this optimization is mentioned, but not given a specific name. 28 Algorithm 3.1 SEAR L 0 = ;; k = 1 C 1 = ffigji 2 Ig fall 1-itemsetsg while ( C k 6= ; ) do fcount supportg forall transactions T 2 D forall k-subsets t T if <p> To avoid this waste of effort, SEAR allows the expansion of several levels of the tree before counting candidates. Thus, all three candidates in our example 31 will be created and counted together. This technique, which we call pass bundling, was first mentioned in <ref> [28] </ref> and [4], but its effects were not examined there. There is a tradeoff involved between the number of IO operations we save and the additional computation necessary to count the additional candidates.
Reference: [29] <author> Heikki Mannila, Hannu Toivonen, and A. Inkeri Verkamo. </author> <title> Discovering frequent episodes in sequences. In Proc. Knowledge Discovery and Data Mining (KDD'95), </title> <note> (to appear), 1995. Also: Technical Report C-1995-10, </note> <institution> University of Helsinki, Department of Computer Science, Finland, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: For special algorithms for sequence discovery that use the Discrete Fourier Transform to speed up the pattern search we refer the reader to [33]. An interesting problem involving sequences is the discovery of episodes, frequent partially ordered sets of events that occur within a given time window <ref> [29] </ref>. The algorithms for finding episodes are very similar to those used in ARM which is why they are described in a comparatively detailed way in Section 7.1. 3 in machine learning terminology: concept descriptions 6 The third type of knowledge are association rules. <p> Also, several occurrences of the same event within one window are not considered, as for example event C in our example window. For these reasons, <ref> [29] </ref> define an episode to be a partially ordered multi-set of events to capture the time relation between events. Events that are not ordered with respect to each other are considered parallel. <p> Similar to association rules, prediction rules E ! F can be constructed that predict the occurrence of an episode F if E took place. The confidence of such rules is defined in a manner similar to association rules. 7.1.2 Algorithm Like the ARM algorithm, the algorithm presented in <ref> [29] </ref> alternates between candidate generation and counting steps. Candidate episodes are initially built from elementary events; the support is then counted, the frequent episodes are used to create new candidate episodes, and so on. Candidate episodes are created in the same way candidate sets are built by Apriori-gen.
Reference: [30] <author> Christopher J. Matheus, Philip K.Chan, and Gregory Piatetsky-Shapiro. </author> <title> Systems for knowledge discovery in databases. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 5(6), </volume> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: This is not the best possible input to standard learning algorithms. Probabilistic methods are needed to deal with these difficulties. Redundant Information While discovery algorithms are supposed to detect patterns in the database, this is undesirable for previously known dependencies. <ref> [30] </ref> identifies two cases where trivial patterns are wrongly reported as "knowledge" due to redundant information stored in the database. One case is strong functional dependencies when one field is a function of one or several other fields, for example: Profit = Sales Expenses. <p> Systems with various degrees of independence exist and the tradeoff between versatility and independence of a discovery system was even used in <ref> [30] </ref> to typify KDD systems. The "human in the loop" requires fast response times, and execution times of several minutes cannot be tolerated. The demands users make on discovery tools create additional challenges for KDD algorithms and deserve further attention. <p> In other words, the question is how to couple knowledge base and database ? [25] A Model for KDD We conclude this chapter by presenting a model which reflects most of the observations made in previous sections. This section follows roughly the idealized model for KDD systems proposed in <ref> [30] </ref>. Actual systems may more or less map to this scheme, but most major components will be represented. As depicted in Figure 1.1, input to the system comes from the user who runs mining queries against the database, in the form of domain knowledge and from the database itself. <p> Note that this model precludes neither internal nor external mining. Pattern extraction denotes the set of actual mining algorithms, and the evaluation component filters the discovered patterns according to their interestingness. To the model proposed in <ref> [30] </ref>, we added the user interface and display components to emphasize the need for and the non-triviality of convenient and effective user interaction.
Reference: [31] <author> Gur-Ali Ozden and William A. Wallace. </author> <title> Induction of rules subject to a quality constraint: Probabilistic inductive learning. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 5(6), </volume> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: The reader is referred to <ref> [1, 2, 14, 31] </ref> for more information. A basic familiarity with decision trees as constructed by classification algorithms and general AI search techniques is assumed. The supervised learning case is presupposed because both approaches presented here solve this problem.
Reference: [32] <author> Jong Soo Park, Mink-Syan Chen, and Philip S. Yu. </author> <title> An effective hash-based algorithm for mining association rules. </title> <booktitle> In SIGMOD, </booktitle> <address> San Jose, CA, </address> <pages> pages 175-186. </pages> <publisher> ACM, </publisher> <year> 1995. </year>
Reference-contexts: Association rule mining (ARM) is one problem in KDD that has received considerable attention during the past year, as demonstrated by the large number of new publications <ref> [19, 32, 34, 37] </ref>. As with many other problems in KDD, a certain type of database and a special application has lead to specialized algorithms for this problem. <p> Apriori-gen has been so successful in reducing the number candidates that it was used in every algorithm that was proposed since it was published <ref> [19, 32, 34, 37] </ref>. The underlying principle, based on Property 2.7, is to generate only those candidates for which all subsets have been previously determined to be frequent. In particular, a (k+1)-candidate will be accepted only if all its k-subsets are frequent. <p> Moreover, speed-ups improve further as the database size is increased and the mining problem is made more difficult. 71 Unfortunately, we were limited to experiments on synthetic data sets (like many other previous studies <ref> [19, 32, 34] </ref>), so the next task is to evaluate the performance of our algorithms against a real-life data set.
Reference: [33] <author> Arun Swami Rakesh Agrawal, Christos Faloutsos. </author> <title> Efficient similarity search in sequence databases. </title> <booktitle> In 4th Int'l Conf. on Foundations of Data Organization and Algorithms, </booktitle> <address> Chicago, </address> <month> Oct. </month> <year> 1993. </year> <booktitle> Also in Lecture Notes in Computer Science 730, </booktitle> <publisher> Springer Verlag, </publisher> <year> 1993, </year> <pages> 69-84. </pages>
Reference-contexts: Examples can be found in stock market data or consumer behavior. For special algorithms for sequence discovery that use the Discrete Fourier Transform to speed up the pattern search we refer the reader to <ref> [33] </ref>. An interesting problem involving sequences is the discovery of episodes, frequent partially ordered sets of events that occur within a given time window [29].
Reference: [34] <author> Ashok Sarasere, Edward Omiecinsky, and Shamkant Navathe. </author> <title> An efficient algorithm for mining association rules in large databases. </title> <booktitle> In 21st Int'l Conf. on Very Large Databases (VLDB), </booktitle> <address> Zurich, Switzerland, </address> <month> Sept. </month> <year> 1995. </year> <note> Also Gatech Technical Report No. GIT-CC-95-04. </note>
Reference-contexts: Association rule mining (ARM) is one problem in KDD that has received considerable attention during the past year, as demonstrated by the large number of new publications <ref> [19, 32, 34, 37] </ref>. As with many other problems in KDD, a certain type of database and a special application has lead to specialized algorithms for this problem. <p> For 1000 items, for example, more than 2 1000 rules have to be considered in a naive approach. Several algorithms have been proposed in the literature to make this search more effective, i.e. Apriori [4] and PARTITION <ref> [34] </ref>. Algorithms differ mainly with respect to the internal data representations used for intermediate results and the IO cost and CPU-overhead they incur. Like all algorithms before it, Apriori needs to scan the entire database several times. <p> The second novelty in PARTITION is the use of the new TID-list structure to store a partition in memory (as opposed to the item-list representation used by Apriori). <ref> [34] </ref> report that PARTITION is superior to Apriori because it needs less IO and less CPU 1 sometimes also referred to as basket data 4 overhead due to these two new features. <p> In contrast, item-lists with pass bundling also achieve low IO and CPU cost without the partitioning overhead. Unfortunately, because the bypass optimization was not mentioned in the paper on PARTITION <ref> [34] </ref>, we could not add it to SPTID to compare both concepts directly to determine which performs better, and further research is required to provide a definitive answer to this question. <p> Apriori-gen has been so successful in reducing the number candidates that it was used in every algorithm that was proposed since it was published <ref> [19, 32, 34, 37] </ref>. The underlying principle, based on Property 2.7, is to generate only those candidates for which all subsets have been previously determined to be frequent. In particular, a (k+1)-candidate will be accepted only if all its k-subsets are frequent. <p> As far as the implementation is concerned, this test for set inclusion can be optimized by storing the sets as bitmaps, one bit for each item. As observed in <ref> [34] </ref>, these bitmaps can become quite large for many items (128 Bytes for 1000 items) and cause considerable overhead. 19 Internal nodes are implemented as hash tables to allow fast selection of the next node. <p> Furthermore, the size of the database that can be mined by the hybrid version is not limited any more as it was by AprioriTid. 2.3.4 PARTITION While all the algorithms presented so far are more or less variations of the same scheme, the PARTITION algorithm <ref> [34] </ref> 8 takes a somewhat different approach. <p> Unfortunately, the size of intermediate data structures is not known and heuristics have to be applied for choosing the partition size. These heuristics are not detailed in <ref> [34] </ref>. Any such heuristic must take into account the memory size, the average transaction size, the current minimum support threshold and the number of items m. Both the number of items and minimum support are necessary to estimate the number of frequent sets. <p> In our experiments, typically between 30% and 60% of the items were not part of any candidate in later passes. 3.2 SPTID: A Partitioning Algorithm This short section describes the implementation details of SPTID, which we implemented according to the description of PARTITION in <ref> [34] </ref>. However, in our experiments, we were unable to obtain results comparable to those reported there. The reason was that the algorithm used to obtain the results in [34] uses important optimization which is not mentioned in the paper. <p> A Partitioning Algorithm This short section describes the implementation details of SPTID, which we implemented according to the description of PARTITION in <ref> [34] </ref>. However, in our experiments, we were unable to obtain results comparable to those reported there. The reason was that the algorithm used to obtain the results in [34] uses important optimization which is not mentioned in the paper. This optimization basically skips the second pass, counting the support for 2-candidates directly. Although we suspected this, we could not confirm it early enough to include it in this report. <p> Although we suspected this, we could not confirm it early enough to include it in this report. We will return to this issue in the performance section on TID-lists (Section 4.3.2). To distinguish the two algorithms we refer to our implementation (conforming with the description in <ref> [34] </ref>) as SPTID and use the name "PARTITION" for the optimized version of the algorithm. Being similar to PARTITION, SPTID works with the TID-list representation and partitions the data to ensure the database is scanned at most two times. <p> The partitioning approach allows a reduction in both the overall amount of communication and the number of synchronizations, as we describe in this section on PPAR, the parallel version of SPEAR. This paralel version of a partitioned algorithm was briefly outlined along with the PARTITION algorithm in <ref> [34] </ref>. 5.3.1 Algorithm Description SPEAR processes each partition independently; their order is not relevant, and results from one partition are not needed in another. Therefore, the straightforward approach, and also the one we choose here, is to consider the data given to a processor as one partition. <p> In contrast, item-lists with pass bundling also achieve low IO and CPU cost without the partitioning overhead. Because the bypassing optimization was not mentioned in the paper on PARTITION <ref> [34] </ref>, both techniques could not be compared directly to find a definitive answer to the question which performs better. However, we plan to conduct future research on this issue, which requires implementing the bypass optimization for SPTID and SEAR. <p> Moreover, speed-ups improve further as the database size is increased and the mining problem is made more difficult. 71 Unfortunately, we were limited to experiments on synthetic data sets (like many other previous studies <ref> [19, 32, 34] </ref>), so the next task is to evaluate the performance of our algorithms against a real-life data set.
Reference: [35] <author> Cullen Schaffer. </author> <title> Overfitting avoidance as bias. </title> <journal> Machine Learning, </journal> <volume> 10 </volume> <pages> 153-178, </pages> <year> 1993. </year>
Reference-contexts: It is an open question as to which methods are optimal for deciding which leaves to expand and when to stop expanding the tree altogether to avoid poor performance due to over-fitting on the one hand or incomplete trees on the other <ref> [5, 27, 35, 39] </ref>.
Reference: [36] <author> Padhraic Smyth and Rodney M. Goodman. </author> <title> Rule induction using information theory. </title> <editor> In Gregory Piatetsky-Shapiro and William J. Frawley, editors, </editor> <booktitle> Knowledge Discovery in Databases, </booktitle> <pages> pages 159-176. </pages> <publisher> AAAI/MIT, </publisher> <year> 1991. </year>
Reference-contexts: Frawley in [15] demands that the statement of the discovered pattern be somewhat simpler than the subset of data objects it describes. What simpler means is left vague intentionally. Length of encoding or other information theoretic measures seem reasonable and have been used widely <ref> [11, 36, 2] </ref>. In short, we are interested in facts about data, and the term knowledge shall be used with this meaning from now on. The last requirement potentially useful is highly dependent on the application and even on the special focus of the current mining task.
Reference: [37] <author> Ramakrishnan Srikant and Rakesh Agrawal. </author> <title> Mining generalized association rules. </title> <booktitle> In 21st Int'l Conf. on Very Large Databases (VLDB), </booktitle> <address> Zurich, Switzerland, </address> <month> Sept. </month> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: Association rule mining (ARM) is one problem in KDD that has received considerable attention during the past year, as demonstrated by the large number of new publications <ref> [19, 32, 34, 37] </ref>. As with many other problems in KDD, a certain type of database and a special application has lead to specialized algorithms for this problem. <p> context in DBMS, the term applies to data about consumer transactions. 2 [3] in D and other early work refer to frequent sets as large sets, but since this has lead to confusion with the cardinality of the set we chose to adopt the change in terminology proposed recently in <ref> [37] </ref>. 11 Property 2.1 (Support for Subsets) If A B for itemsets A; B, then supp (A) supp (B) because all transactions in D that support B necessarily support A also. <p> Apriori-gen has been so successful in reducing the number candidates that it was used in every algorithm that was proposed since it was published <ref> [19, 32, 34, 37] </ref>. The underlying principle, based on Property 2.7, is to generate only those candidates for which all subsets have been previously determined to be frequent. In particular, a (k+1)-candidate will be accepted only if all its k-subsets are frequent. <p> Therefore larger numbers of items become tractable. The content of this chapter is based on <ref> [19, 37] </ref>where different algorithms to include concept hierarchies 1 . After general remarks we provide a short sketch of these algorithms in Section 7.2.2 7.2.1 New Definitions for Interestingness Based on concept hierarchies new criteria for the interestingness of a rule can be applied. <p> Since the ancestor rule is more general, it should be preferred. Recalling the cereal example, rules involving individual cereal brands can be considered redundant unless their confidence or support deviate significantly from the ancestor rule Cereal ! Milk. Details can be found in <ref> [37] </ref>. [19] introduce the notion of strong rules to focus the search in several hierarchy levels. A rule is strong if every ancestor of every item in its antecedent and consequent is frequent on its level in the taxonomy. <p> This is possible because different minimum support and confidence levels are permitted on the various levels. 7.2.2 Algorithms The basic approach in <ref> [37] </ref> is to add ancestor literals to each transaction and run the algorithms used for mining single level rules. This can dramatically increase the size of transactions and the number of possible 1 [37] refer to their treatment of taxonomies as generalized association rules while [19] use the term multiple-level association <p> minimum support and confidence levels are permitted on the various levels. 7.2.2 Algorithms The basic approach in <ref> [37] </ref> is to add ancestor literals to each transaction and run the algorithms used for mining single level rules. This can dramatically increase the size of transactions and the number of possible 1 [37] refer to their treatment of taxonomies as generalized association rules while [19] use the term multiple-level association rules are proposed. The two are used synonymously here, along with hierarchical association rules. 67 frequent sets.
Reference: [38] <author> Craig B. Stunkel, Denis G. Shea, Don G. Grice, Peter H. Hochschild, and Michael Tsao. </author> <title> The sp1 high-performance switch. </title> <booktitle> In Proc. 1994 Scalable High-Performance Computing Conference, </booktitle> <pages> pages 150-157, </pages> <month> May </month> <year> 1994. </year>
Reference: [39] <author> Allan P. White and Wei Zhong Liu. </author> <title> Bias in information-based measures in decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 15 </volume> <pages> 321-329, </pages> <year> 1994. </year>
Reference-contexts: It is an open question as to which methods are optimal for deciding which leaves to expand and when to stop expanding the tree altogether to avoid poor performance due to over-fitting on the one hand or incomplete trees on the other <ref> [5, 27, 35, 39] </ref>.
Reference: [40] <author> Beat Wuthrich. </author> <title> Knowledge discovery in databases. </title> <type> Draft course manuscript, </type> <institution> Hong Kong University of Science and Technology, </institution> <month> May </month> <year> 1994. </year> <month> 76 </month>
Reference-contexts: We proceed to list the additional difficulties that arise with databases as data source for pattern discovery algorithms. This section ends with more general issues and a proposed architecture for KDD systems. For more detailed introductions to KDD see <ref> [21, 40] </ref>. 1.1.1 Types of Knowledge Recalling the aforementioned definition of KDD by W.Frawley that discovered information is implicit, previously unknown and potentially useful we can state what "knowledge" means in our context.
References-found: 40

