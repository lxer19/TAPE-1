URL: ftp://ftp.cscs.ch/pub/CSCS/techreports/1994/CSCS-TR-94-01.ps.gz
Refering-URL: 
Root-URL: 
Title: Application-Driven Development of an Integrated Tool Environment for Distributed Memory Parallel Processors  
Author: Clemen~con, K. M. Decker, A. Endo, J. Fritscher, G. Jost, N. Masuda, A. Muller, R. Ruhl, W. Sawyer, E. de Sturler, B. J. N. Wylie 
Address: La Galleria, CH-6928 Manno, Switzerland  SX-Center, Switzerland  
Affiliation: CSCS-ETH, Section of Research and Development (SeRD), Swiss Scientific Computing Center,  NEC  
Note: C.  
Pubnum: CSCS-TR-94-01  Technical Report  
Email: decker@cscs.ch  
Date: April 6, 1994  
Abstract: The Joint CSCS-ETH/NEC Collaboration in Parallel Processing comprises the development of an integrated tool environment together with applications and algorithms for distributed memory parallel processors (DMPPs). Tool and application developers interact closely: the requirements of the tools are defined by the needs of the application developers, and once an application requirement becomes an integral part of the tool environment, the tools ease parallelization of similar applications and whole application classes. Additional features of the project are the use of a standardized DMPP high-level programming language (HPF) and low-level message passing interface (MPI). The tool environment integrates parallelization support, a parallel debugger, and a performance monitor and analyzer. Applications already investigated include some of those currently considered difficult to parallelize on DMPPs. In this paper we summarize the tool and application development efforts and show preliminary performance results of three applications effectively parallelized on two DMPP platforms with the assistance of our tool environment. 
Abstract-found: 1
Intro-found: 1
Reference: <institution> References </institution>
Reference: [ABJ + 91] <author> D. Allen, R. Bowker, K. Jourdenais, J. Simons, S. Sistare, and R. </author> <title> Title. The Prism programming environment. </title> <booktitle> In Proc. Supercomputer Debugging Workshop '91, </booktitle> <pages> pages 1-7, </pages> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference: [APR89] <author> M. Annaratone, C. Pommerell, and R. Ruhl. </author> <title> Interprocessor communication speed and performance in distributed-memory parallel processors. </title> <booktitle> In Proc. 16th Symp. on Computer Architecture, </booktitle> <pages> pages 315-324, </pages> <address> Jerusalem, Israel, </address> <month> June </month> <year> 1989. </year> <pages> IEEE-ACM. </pages>
Reference-contexts: The performance curve on the Paragon appears to flatten off faster than that of the Cenju-2: this can be explained through differences in the communication/computation ratio <ref> [APR89, AR92] </ref>. FE1 and FE2 (Finite Element) Fig. 4 shows the performance measured for the finite-element application. We see that on the Cenju-2 the performance of the MPI versions are about a factor of two better than that of the PST versions. On the Paragon the differences are slightly smaller.
Reference: [APR93] <author> APR (Applied Parallel Research). </author> <title> HPF parallelization tools for clustered workstations and distributed memory multi-processor systems. HPC Select News, </title> <type> article 914, </type> <month> August </month> <year> 1993. </year> <note> Further information available from APR, Placerville, CA. </note>
Reference: [AR92] <author> M. Annaratone and R. Ruhl. </author> <title> Balancing interprocessor communication and computation on torus-connected multicomputers running compiler-parallelized code. </title> <booktitle> In Proc. </booktitle> <address> SHPCC 92, Williamsburg VA, </address> <month> March </month> <year> 1992. </year> <note> IEEE. </note>
Reference-contexts: The performance curve on the Paragon appears to flatten off faster than that of the Cenju-2: this can be explained through differences in the communication/computation ratio <ref> [APR89, AR92] </ref>. FE1 and FE2 (Finite Element) Fig. 4 shows the performance measured for the finite-element application. We see that on the Cenju-2 the performance of the MPI versions are about a factor of two better than that of the PST versions. On the Paragon the differences are slightly smaller.
Reference: [BBDS92] <author> D. H. Bailey, E. Barszcz, L. Dagum, and H. D. Simon. </author> <title> NAS Parallel Benchmarks Results. </title> <type> Technical Report RNR-93-016, </type> <institution> NASA Ames Research Center, </institution> <address> CA, </address> <month> Dec </month> <year> 1992. </year> <title> With supplementary NAS parallel benchmark update tables of Feb. </title> <year> 1994. </year>
Reference-contexts: The NAS benchmark kernels consist of eight codes, known as Embarrassingly Parallel (EP), Multigrid (MG), Conjugate Gradient (CG), 3DFFT PDE (FT), Integer Sorting (IS), LU-solver (LU), Pentadiagonal Solver (SP), and Block Tridiagonal Solver (BT) <ref> [BBDS92] </ref>. Here, we consider two codes among them: MG: calculates an approximate solution to the discrete Poisson problem using four iterations of the V-cycle multigrid algorithm on a n fi n fi n grid with periodic boundary conditions.
Reference: [BSS91] <author> H. Berryman, J. Saltz, and J. Schroggs. </author> <title> Execution time support for adaptive scientific algorithms on distributed memory machines. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 3(3), </volume> <month> June </month> <year> 1991. </year>
Reference-contexts: This has already resulted in performance optimizations in PST, higher usability in PDT, and the provision of additional information by PMA. The inclusion of new functionality is necessary for our research in unstructured problems. Currently support for such problems is provided mainly by external libraries, e.g., <ref> [BSS91] </ref>. We feel that it is imperative for the parallelization tool environment itself to provide functionality to support irregularly distributed code and data structures, to debug parallel code efficiently, and to visualize sparse matrices and their distribution.
Reference: [CEF + 94] <author> C. Clemen~con, A. Endo, J. Fritscher, A. Muller, R. Ruhl, and B. J. N. Wylie. </author> <title> An environment for portable distributed memory parallel programming. </title> <booktitle> Proc. IFIP WG 10.3 Working Conf. on Programming Environments for Massively Parallel Distributed Systems (Ascona, </booktitle> <address> Switzerland, </address> <month> April </month> <year> 1994), 1994. </year> <note> To be published by Birkhauser (Basel). </note>
Reference: [Che93] <author> D. Y. Cheng. </author> <title> A Survey of Parallel Programming Languages and Tools. </title> <type> Technical Report RND-93-005, </type> <institution> NASA Ames Research Center, Moffet Field, </institution> <address> CA, </address> <month> March </month> <year> 1993. </year>
Reference: [Dec93] <author> K. M. Decker. </author> <title> Methods and Tools for Programming Massively Parallel Distributed Systems. </title> <journal> SPEEDUP Journal, </journal> <volume> 7(2), </volume> <month> November </month> <year> 1993. </year>
Reference: [Esc93] <author> P. Esclangon. </author> <title> Support theorique d'un benchmark supercalculateur. </title> <type> Technical Report 93NJ00009, </type> <institution> Service Informatique et Mathematiques Appliquee, Electricite de France, </institution> <month> April </month> <year> 1993. </year> <note> ISSN 1161-059X. </note>
Reference-contexts: A Finite Element Code for Unstructured Problems This application, made available to us by Electricite de France (EDF), solves the two-dimensional heat equation with mixed Dirichlet and von Neumann boundary conditions on a rectangular plate using a finite element method (see <ref> [Esc93] </ref>). The resulting sparse linear system is solved by the conjugate gradient (CG) method [HS52, GL89], and requires indirect addressing because of the sparse matrix storage. Both a stationary and a time-dependent problem are solved, and the average performance is shown.
Reference: [GL89] <author> G. Golub and C. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference-contexts: The resulting sparse linear system is solved by the conjugate gradient (CG) method <ref> [HS52, GL89] </ref>, and requires indirect addressing because of the sparse matrix storage. Both a stationary and a time-dependent problem are solved, and the average performance is shown. We consider a small problem (FE1) with approximately 7,000 CSCS-TR-94-01 9 3. <p> Conjugate Gradient NAS Kernel (CG) The conjugate gradients method is well described in the literature <ref> [HS52, GL89] </ref>. Our approach to parallelization of the NAS CG kernel was the same in both the MPI and PST versions: starting with the example Fortran source code, the sparse matrix was generated by sorting elements by their column index.
Reference: [Hac85] <author> W. Hackbusch. </author> <title> Multi-Grid Methods and Applications. </title> <publisher> Springer Verlag, </publisher> <year> 1985. </year>
Reference-contexts: The description of the algorithm can be found in <ref> [Hac85] </ref>. The parallelization of this benchmark arises directly from the nature of the problem: the cubic grid is partitioned into blocks. The communication required is then only the exchange of elements between adjacent block faces.
Reference: [HF93] <author> M. T. Heath and J. A. E. Finger. ParaGraph: </author> <title> A Tool for Visualizing Performance of Parallel Programs. </title> <type> Technical report, </type> <institution> UIUC/ORNL, </institution> <year> 1993. </year>
Reference: [HKT91] <author> S. Hirandani, K. Kennedy, and C. W. Tseng. </author> <title> Compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proc. Supercomputing 91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year> <note> ACM-IEEE. CSCS-TR-94-01 19 REFERENCES </note>
Reference: [HPF93] <author> HPFF (High Performance Fortran Forum). </author> <title> High Performance Fortran Language Specification: Version 1.0. </title> <journal> Scientific Programming, </journal> <volume> 2(1&2), </volume> <year> 1993. </year> <note> Special Issue. </note>
Reference: [HS52] <author> M. Hestenes and E. </author> <title> Stiefel. Methods of conjugate gradients for solving linear systems. </title> <journal> J. Res. Nat. Bur. Stand., </journal> <volume> 49 </volume> <pages> 409-436, </pages> <year> 1952. </year>
Reference-contexts: The resulting sparse linear system is solved by the conjugate gradient (CG) method <ref> [HS52, GL89] </ref>, and requires indirect addressing because of the sparse matrix storage. Both a stationary and a time-dependent problem are solved, and the average performance is shown. We consider a small problem (FE1) with approximately 7,000 CSCS-TR-94-01 9 3. <p> Conjugate Gradient NAS Kernel (CG) The conjugate gradients method is well described in the literature <ref> [HS52, GL89] </ref>. Our approach to parallelization of the NAS CG kernel was the same in both the MPI and PST versions: starting with the example Fortran source code, the sparse matrix was generated by sorting elements by their column index.
Reference: [JMSd93] <author> G. Jost, N. Masuda, W. Sawyer, and E. de Sturler. </author> <title> Application Suite Document. </title> <note> Technical Note SeRD-CSCS-TN-93-15, CSCS, </note> <month> December </month> <year> 1993. </year>
Reference: [KMv90] <author> C. Koelbel, P. Mehrotra, and J. van Rosendale. </author> <title> Supporting shared memory data structures on distributed memory architectures. </title> <booktitle> In 2nd Symp. on Principles and Practice of Parallel Programming, </booktitle> <pages> page 177, </pages> <address> Seattle, WA, </address> <month> March </month> <year> 1990. </year> <journal> ACM SIGPLAN. </journal>
Reference: [KRYG82] <author> D. R. Kincaid, J. R. Respess, D. M. Yound, and R. G. Grimes. </author> <title> ITPACK 2C: </title>
Reference-contexts: One was obtained using the PST directives, the other has the communication hand-coded using MPI routines. The most time-consuming part is the matrix-vector multiplication in CG which uses indirect addressing, because the sparse matrix is stored in compressed form (ITPACK format <ref> [KRYG82] </ref>). For the MPI version, we used the so-called replicated data approach. That means that all processors hold all data. The computation itself is decomposed in the sense that each processor updates only a certain subdomain.
References-found: 20

