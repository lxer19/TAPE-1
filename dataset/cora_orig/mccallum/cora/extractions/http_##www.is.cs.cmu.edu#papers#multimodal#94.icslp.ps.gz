URL: http://www.is.cs.cmu.edu/papers/multimodal/94.icslp.ps.gz
Refering-URL: http://www.is.cs.cmu.edu/papers/multimodal/94.icslp.abstract.html
Root-URL: 
Title: SEE ME, HEAR ME: INTEGRATING AUTOMATIC SPEECH RECOGNITION AND  
Author: LIP-READING Paul Duchnowski Uwe Meier Alex Waibel ; 
Address: Pittsburgh PA, USA  
Affiliation: 1 University of Karlsruhe, Karlsruhe, Germany 2 Carnegie Mellon University,  
Abstract: We present recent work on integration of visual information (automatic lip-reading) with acoustic speech for better overall speech recognition. A Multi-State Time Delay Neural Network performs the recognition of spelled letter sequences taking advantage of lip images from a standard camera. The problems addressed include efficient but effective representation of the visual information and optimum manner of combining the two modalities when rendering a decision. We show results for several alternatives to direct gray level image as the visual evidence. These are: Principal Components, Linear Discriminants, and DFT coefficients. Dimensionality of the input is decreased by a factor of 12 while maintaining recognition rates. Combination of the visual and acoustic information is performed at three different levels of abstraction. Results suggest that integration of higher order input features works best. On a continuous spelling task, visual-alone recognition of 45-55%, when combined with acoustic data, lowers audio-alone error rates by 30-40%. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L.D. Braida. </author> <title> Crossmodal Integration in the Identification of Consonant Segments. </title> <journal> The Quart. J. of Exp. Psych., </journal> <volume> 43A(3), </volume> <year> 1991, </year> <month> pp.647-677. </month>
Reference-contexts: Visual-only recognition rates for different data representations. further prevents the recognizer from taking advantage of lower level correlations between acoustic and visual events such as inter-modal timing relationships. There is evidence that humans integrate the bi-modal inputs to take advantage of such cues <ref> [1, 13] </ref>. layers: output DTW phoneme hidden input visual visual Net I acoustic acoustic nation. Two combination alternatives are illustrated in Figure 2.
Reference: [2] <author> C. Bregler, H. Hild, S. Manke, and A. Waibel. </author> <title> Improving Connected Letter Recognition by Lipreading. </title> <booktitle> in Proc. ICASSP '93. </booktitle>
Reference-contexts: The audio-visual ASR system under development in our laboratory was first described in <ref> [2] </ref>. It is designed to recognize continuously spelled names and nonsense letter sequences of arbitrary length using the German alphabet. The task is thus equivalent to contiuous recognition with a small but highly confusable vocabulary. 2. SYSTEM DESCRIPTION AND DEVELOPMENT 2.1. <p> This component remains invariant for all experiments described below. The visual evidence is obtained by "frame-grabbing" the output of a conventional camcorder camera at 30 frames/sec, with 8-bit gray level resolution. In our first work <ref> [2] </ref>, pictures of the lip region were manually extracted from the image. Currently, speakers are asked to position themselves such that their lips appear within a rectangle that is simultaneously shown on the screen of a workstation. However, no special markers, restraints or position indicators are used. <p> The weights in the parallel networks are trained by backpropagation. There are 15 hidden units in both sub-nets. The combination weights are computed dynamically during recognition to reflect the estimated reliability of each modality. These "entropy weights" <ref> [2] </ref>, A for the acoustic side and V for the visual are given by: A = b + S maxoverdata (1) The entropy quantities S A and S V are computed for the acoustic and visual phone/viseme activations by normalizing these to sum to one and treating them as probability mass
Reference: [3] <author> K. Fukunaga. </author> <title> Introduction to Statistical Pattern Recognition. </title> <address> San Diego: </address> <publisher> Academic Press, </publisher> <year> 1990. </year>
Reference-contexts: On the other hand, the goal of recognition is rather the discrimination among several input classes (e.g. phones). A related method of data rate reduction that is better geared towards this goal is Linear Discriminant Analysis (LDA) <ref> [3] </ref>. Here the original data is also projected onto a set of vectors and only the most significant of the resulting coefficients are used further. However, the projection vectors, calculated as the eigenvectors of so-called scatter matrices, maximize the separability of different input classes in the reduced-dimensional representation.
Reference: [4] <author> A.J. Goldschen. </author> <title> Continuous Automatic Speech Recognition by Lipreading. </title> <type> Ph.D. Dissertation. </type> <institution> George Wash-ington University, </institution> <year> 1993. </year>
Reference-contexts: Automatic speech recognition (ASR) systems' performance is, if anything, even more sensitive to degradation of the acoustic input. Therefore, it is only natural to try to supplement the acoustic data with lip movement information. Related work on this concept has been reported by other researchers in <ref> [4, 9, 10, 11, 12, 15] </ref>. These studies clearly indicated that combined audio-visual speech recognition was feasible. However, most of the experiments have relied on such simplifications as head-mounted cameras, reflective markers placed on the speaker's lips, or manual extraction of the relevant part of the face image.
Reference: [5] <author> H. Hild and A. Waibel. </author> <title> Connected Letter Recognition with a Multi-State Time Delay Neural Network. </title> <booktitle> Neural Information Processing Systems (NIPS-5), </booktitle> <year> 1993. </year>
Reference-contexts: The resulting sequence of 384-dimensional "normalized pixel vectors" (one vector for each lip image frame) is then used as the input to the recognition algorithm. In the basic system a modular Multi-State Time De lay Neural Network (MS-TDNN) <ref> [5] </ref> is used to perform the recognition. Figure 1 schematically shows the architecture. Through the first three layers (input-hidden-phoneme/viseme) the acoustic and visual inputs are processed separately.
Reference: [6] <author> H.M. Hunke. Lokalisieren von Gesichtern mit Hilfe von neuronalen Netzen. </author> <title> M.S. </title> <type> Thesis, </type> <institution> University of Karls-ruhe, </institution> <year> 1994. </year>
Reference-contexts: The training effort, however, becomes impractical. A more elegant collective solution to these challenges involves automatic control of the camera to track the face of the speaker in a room and an algorithm to accurately locate the lips within the face image <ref> [6] </ref>. Prototypes of both the face-tracker and lip-locator have already been developed in our laboratory and we're in the process of integrating them into a single system. Together they allow for reasonable movement and enable size- and location-normalization of the input lip image.
Reference: [7] <author> I.T. Joliffe. </author> <title> Principal Component Analysis. </title> <address> New York: </address> <publisher> Springer-Verlag, </publisher> <year> 1986. </year>
Reference-contexts: Equalizing the visual and acoustic input vector dimensions also seems advantageous for low-level integration of the information (see Sec. 2.3). Storage savings would also result. 2.2.1. Principal Components A well-known method that can accomplish this is Princi pal Component Analysis <ref> [7] </ref> (also known as Karhunen-Loeve expansion). In this approach the original vectors are pro jected onto the eigenvectors of their covariance matrix and only the coefficients corresponding to the largest N eigen-values are retained (N &lt; 384). This preserves most of the variance in the original data.
Reference: [8] <author> J.S. Lim. </author> <title> Two-Dimensional Signal and Image Processing. </title> <address> Englewood Cliffs, N.J.: </address> <publisher> Prentice-Hall, </publisher> <year> 1990. </year>
Reference-contexts: In our case we labelled the data vectors by one of 62 phones. Again, preliminary experiments led us to further use of 32 LDA coefficients. 2.2.3. Fourier Transfrom It is known that almost all typical images are uniquely specified by the magnitude of their Fourier Transform <ref> [8] </ref>. This parameterization is also potentially resistant against translation of the input image and offers several methods of reducing the data count by grouping the DFT coefficients.
Reference: [9] <author> K. Mase and A. Pentland. </author> <title> Automatic Lipreading by Optical-Flow Analysis. </title> <journal> Systems and Computers in Japan, </journal> <volume> 22(6), </volume> <year> 1991, </year> <pages> pp. 67-76. </pages>
Reference-contexts: Automatic speech recognition (ASR) systems' performance is, if anything, even more sensitive to degradation of the acoustic input. Therefore, it is only natural to try to supplement the acoustic data with lip movement information. Related work on this concept has been reported by other researchers in <ref> [4, 9, 10, 11, 12, 15] </ref>. These studies clearly indicated that combined audio-visual speech recognition was feasible. However, most of the experiments have relied on such simplifications as head-mounted cameras, reflective markers placed on the speaker's lips, or manual extraction of the relevant part of the face image.
Reference: [10] <author> E.D. Petajan. </author> <title> Automatic lipreading to enhance speech recognition. </title> <booktitle> in Proc. IEEE Communications Society Global Telecom. Conf., </booktitle> <address> Atlanta GA, </address> <month> Nov. </month> <year> 1984. </year>
Reference-contexts: Automatic speech recognition (ASR) systems' performance is, if anything, even more sensitive to degradation of the acoustic input. Therefore, it is only natural to try to supplement the acoustic data with lip movement information. Related work on this concept has been reported by other researchers in <ref> [4, 9, 10, 11, 12, 15] </ref>. These studies clearly indicated that combined audio-visual speech recognition was feasible. However, most of the experiments have relied on such simplifications as head-mounted cameras, reflective markers placed on the speaker's lips, or manual extraction of the relevant part of the face image.
Reference: [11] <author> P. Silsbee and A. Bovik. </author> <title> Audio-visual speech recognition for a vowel discrimination task. </title> <booktitle> Proceedings of SPIE, </booktitle> <volume> vol. </volume> <year> 2094, 1993, </year> <month> pp.84-95. </month>
Reference-contexts: Automatic speech recognition (ASR) systems' performance is, if anything, even more sensitive to degradation of the acoustic input. Therefore, it is only natural to try to supplement the acoustic data with lip movement information. Related work on this concept has been reported by other researchers in <ref> [4, 9, 10, 11, 12, 15] </ref>. These studies clearly indicated that combined audio-visual speech recognition was feasible. However, most of the experiments have relied on such simplifications as head-mounted cameras, reflective markers placed on the speaker's lips, or manual extraction of the relevant part of the face image.
Reference: [12] <author> D.G. Stork, G. Wolff, and E. Levine. </author> <title> Neural network lipreading system for improved speech recognition. </title> <booktitle> in Proc. </booktitle> <address> IJCNN'92. </address>
Reference-contexts: Automatic speech recognition (ASR) systems' performance is, if anything, even more sensitive to degradation of the acoustic input. Therefore, it is only natural to try to supplement the acoustic data with lip movement information. Related work on this concept has been reported by other researchers in <ref> [4, 9, 10, 11, 12, 15] </ref>. These studies clearly indicated that combined audio-visual speech recognition was feasible. However, most of the experiments have relied on such simplifications as head-mounted cameras, reflective markers placed on the speaker's lips, or manual extraction of the relevant part of the face image.
Reference: [13] <author> Q. Summerfield. </author> <title> Audio-visual Speech Perception, Lipreading and Artificial Stimulation. in Hearing Science and Hearing Disorders, M.E. </title> <editor> Lutman and M.P. Haggard eds., </editor> <address> New York: </address> <publisher> Academic Press, </publisher> <year> 1983. </year>
Reference-contexts: The usefulness of lip movement information stems in large part from its rough complementariness to the acoustic signal: the former is most reliable for distinguishing the place of articulation, the latter conveys most robustly manner and voicing information (e.g. <ref> [13] </ref>). Automatic speech recognition (ASR) systems' performance is, if anything, even more sensitive to degradation of the acoustic input. Therefore, it is only natural to try to supplement the acoustic data with lip movement information. <p> Visual-only recognition rates for different data representations. further prevents the recognizer from taking advantage of lower level correlations between acoustic and visual events such as inter-modal timing relationships. There is evidence that humans integrate the bi-modal inputs to take advantage of such cues <ref> [1, 13] </ref>. layers: output DTW phoneme hidden input visual visual Net I acoustic acoustic nation. Two combination alternatives are illustrated in Figure 2.
Reference: [14] <author> A. Waibel, M.T. Vo, P. Duchnowski, and S. Manke. </author> <title> Multimodal Interfaces. </title> <note> to appear in Artificial Intelligence Review Journal, special issue, </note> <year> 1994. </year>
Reference-contexts: We are pursuing machine understanding of speech, lip motion, gesture, eye gaze, hand writing, face recognition and tracking and sound localization. Overviews of some of these projects can be found in <ref> [14] </ref>. In this paper we concentrate on the problem of integrating acoustic and visual information for better speech recognition. It is well known that hearing-impaired listeners and those listening in adverse acoustic environments (noise, reverberation, multiple speakers) rely heavily on the visual input to disambiguate among acoustically confusable speech elements.
Reference: [15] <author> B.P. Yuhas, M.H. Goldstein, Jr., and T.J. Sejnowski. </author> <title> Integration of acoustic and visual speech signals using neural networks. </title> <journal> IEEE Communications Magazine, </journal> <month> Nov. </month> <year> 1989. </year>
Reference-contexts: Automatic speech recognition (ASR) systems' performance is, if anything, even more sensitive to degradation of the acoustic input. Therefore, it is only natural to try to supplement the acoustic data with lip movement information. Related work on this concept has been reported by other researchers in <ref> [4, 9, 10, 11, 12, 15] </ref>. These studies clearly indicated that combined audio-visual speech recognition was feasible. However, most of the experiments have relied on such simplifications as head-mounted cameras, reflective markers placed on the speaker's lips, or manual extraction of the relevant part of the face image.
References-found: 15

