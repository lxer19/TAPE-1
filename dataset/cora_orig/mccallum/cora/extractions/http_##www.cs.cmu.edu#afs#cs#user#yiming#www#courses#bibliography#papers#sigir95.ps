URL: http://www.cs.cmu.edu/afs/cs/user/yiming/www/courses/bibliography/papers/sigir95.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs/user/yiming/www/courses/bibliography/papers/
Root-URL: 
Title: Noise Reduction in a Statistical Approach to Text Categorization  
Author: Yiming Yang 
Address: Rochester, Minnesota 55905 USA  
Affiliation: Section of Medical Information Resources Mayo Clinic/Foundation  
Abstract: This paper studies noise reduction for computational efficiency improvements in a statistical learning method for text categorization, the Linear Least Squares Fit (LLSF) mapping. Multiple noise reduction strategies are proposed and evaluated, including: an aggressive removal of non-informative words from texts before training; the use of a truncated singular value decomposition to cut off noisy latent semantic structures during training; the elimination of non-influential components in the LLSF solution (a word-concept association matrix) after training. Text collections in different domains were used for evaluation. Significant improvements in computational efficiency without losing categorization accuracy were evident in the testing results. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Yang Y, </author> <title> Chute CG. (1992) A Linear Least Squares Fit mapping method for information retrieval from natural language texts. </title> <booktitle> Proc 14th International Conference on Computational Linguistics (COLING 92), </booktitle> <pages> 447-453. </pages>
Reference-contexts: That is, the vocabulary gap between free texts and the controlled indexing language of a particular database is usually large. Consequently, search methods based on shared words between free text and category names typically exhibit poor performance <ref> [1] </ref> [2] [3]. The importance of using human knowledge to solve the vocabulary gap problem has been recognized, and statistical learning of text-to-categories mapping based on human assignments has been a major focus in recent research in text categorization [3] [4] [5] [6] [7].
Reference: [2] <author> Yang Y, Chute CG. </author> <year> (1993, </year> <title> July) An application of Least Squares Fit Mapping to text information retrieval. </title> <booktitle> Proc 16th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 93), </booktitle> <pages> 281-290. </pages>
Reference-contexts: That is, the vocabulary gap between free texts and the controlled indexing language of a particular database is usually large. Consequently, search methods based on shared words between free text and category names typically exhibit poor performance [1] <ref> [2] </ref> [3]. The importance of using human knowledge to solve the vocabulary gap problem has been recognized, and statistical learning of text-to-categories mapping based on human assignments has been a major focus in recent research in text categorization [3] [4] [5] [6] [7]. <p> The LLSF mapping is a successful learner relying on past human relevance judgments, and can be used for both text retrieval <ref> [2] </ref> and text categorization. Significant improvements of LLSF mapping have been observed in previous evaluations, compared to alternatives such as word-based matching methods which do not use any human knowledge, and thesaurus-based methods which are heavily dependent on manually coded human knowledge [3].
Reference: [3] <author> Yang Y, </author> <title> Chute CG. (1994) An example-based mapping method for text categorization and retrieval. </title> <booktitle> ACM Transaction on Information Systems (TOIS 94): </booktitle> <pages> 253-277. </pages>
Reference-contexts: That is, the vocabulary gap between free texts and the controlled indexing language of a particular database is usually large. Consequently, search methods based on shared words between free text and category names typically exhibit poor performance [1] [2] <ref> [3] </ref>. The importance of using human knowledge to solve the vocabulary gap problem has been recognized, and statistical learning of text-to-categories mapping based on human assignments has been a major focus in recent research in text categorization [3] [4] [5] [6] [7]. <p> words between free text and category names typically exhibit poor performance [1] [2] <ref> [3] </ref>. The importance of using human knowledge to solve the vocabulary gap problem has been recognized, and statistical learning of text-to-categories mapping based on human assignments has been a major focus in recent research in text categorization [3] [4] [5] [6] [7]. The LLSF mapping is a successful learner relying on past human relevance judgments, and can be used for both text retrieval [2] and text categorization. <p> Significant improvements of LLSF mapping have been observed in previous evaluations, compared to alternatives such as word-based matching methods which do not use any human knowledge, and thesaurus-based methods which are heavily dependent on manually coded human knowledge <ref> [3] </ref>. LLSF uses a corpus of manually categorized texts for training. The training data are represented using two matrices, A and B, where A is a text-word matrix and B is a text-category matrix. <p> More precisely, the problem is to find a matrix X which minimizes the Frobenius norm of the residual matrix E = AX B where E is n fi l, and the Frobenius matrix norm is kEk F = u t i=1 j=1 ij (see <ref> [3] </ref> for a more detailed presentation of LLSF). The intuitive interpretation is to find X which minimizes the squared error in the mapping from training texts to their categories. The solution X is a word-category association matrix whose elements are the weights of these associations. <p> We refer to the 10-point average precision as categorization accuracy. Average precision is the most commonly used measure in evaluations of categorization systems, because many systems provide a ranked list of categories for a given text instead of binary decisions over categories <ref> [3] </ref> [4] [5] [6] [7]. A ranked list, of course, can be used to obtain binary decisions by setting a threshold. There are discussions about using alternative measures [12] [3]; these issues, however, are open research questions, and are not the focus of this paper. <p> categorization systems, because many systems provide a ranked list of categories for a given text instead of binary decisions over categories <ref> [3] </ref> [4] [5] [6] [7]. A ranked list, of course, can be used to obtain binary decisions by setting a threshold. There are discussions about using alternative measures [12] [3]; these issues, however, are open research questions, and are not the focus of this paper. <p> Most systems apply the same generic stop-word list to all document collections without change. While the generic stop words are relatively safe to remove in the sense that their removal rarely causes a significant accuracy loss, the chance of significant accuracy improvement is also small <ref> [3] </ref> [15]. Since a generic stop-word list is often much smaller than the vocabularies of real-world document collections, only a limited number of words can be removed from texts, and the improvement in computational efficiency is therefore very limited.
Reference: [4] <author> Fuhr N, Hartmann S, Lustig G, et al. </author> <title> (1991) AIR/X a rule-based multistage indexing systems for large subject fields. </title> <booktitle> Proceedings of the RIAO'91, </booktitle> <pages> 606-623. </pages>
Reference-contexts: The importance of using human knowledge to solve the vocabulary gap problem has been recognized, and statistical learning of text-to-categories mapping based on human assignments has been a major focus in recent research in text categorization [3] <ref> [4] </ref> [5] [6] [7]. The LLSF mapping is a successful learner relying on past human relevance judgments, and can be used for both text retrieval [2] and text categorization. <p> We refer to the 10-point average precision as categorization accuracy. Average precision is the most commonly used measure in evaluations of categorization systems, because many systems provide a ranked list of categories for a given text instead of binary decisions over categories [3] <ref> [4] </ref> [5] [6] [7]. A ranked list, of course, can be used to obtain binary decisions by setting a threshold. There are discussions about using alternative measures [12] [3]; these issues, however, are open research questions, and are not the focus of this paper.
Reference: [5] <author> Tzeras K, Hartmann S. </author> <title> (1993) Automatic indexing based on Bayesian inference networks. </title> <booktitle> Proc 16th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 93), </booktitle> <pages> 22-34. </pages>
Reference-contexts: The importance of using human knowledge to solve the vocabulary gap problem has been recognized, and statistical learning of text-to-categories mapping based on human assignments has been a major focus in recent research in text categorization [3] [4] <ref> [5] </ref> [6] [7]. The LLSF mapping is a successful learner relying on past human relevance judgments, and can be used for both text retrieval [2] and text categorization. <p> We refer to the 10-point average precision as categorization accuracy. Average precision is the most commonly used measure in evaluations of categorization systems, because many systems provide a ranked list of categories for a given text instead of binary decisions over categories [3] [4] <ref> [5] </ref> [6] [7]. A ranked list, of course, can be used to obtain binary decisions by setting a threshold. There are discussions about using alternative measures [12] [3]; these issues, however, are open research questions, and are not the focus of this paper.
Reference: [6] <author> Masand B., Linoff G., Waltz D. </author> <title> (1992) Classifying News Stories using Memory Based Reasoning. </title> <booktitle> 15th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 92): </booktitle> <pages> 59-64. </pages>
Reference-contexts: The importance of using human knowledge to solve the vocabulary gap problem has been recognized, and statistical learning of text-to-categories mapping based on human assignments has been a major focus in recent research in text categorization [3] [4] [5] <ref> [6] </ref> [7]. The LLSF mapping is a successful learner relying on past human relevance judgments, and can be used for both text retrieval [2] and text categorization. <p> We refer to the 10-point average precision as categorization accuracy. Average precision is the most commonly used measure in evaluations of categorization systems, because many systems provide a ranked list of categories for a given text instead of binary decisions over categories [3] [4] [5] <ref> [6] </ref> [7]. A ranked list, of course, can be used to obtain binary decisions by setting a threshold. There are discussions about using alternative measures [12] [3]; these issues, however, are open research questions, and are not the focus of this paper.
Reference: [7] <author> Yang Y. </author> <title> (1994) Expert Network: Effective and Efficient Learning from Human Decisions in Text Categorization and Retrieval. </title> <booktitle> 17th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 94): </booktitle> <pages> 11-21. </pages>
Reference-contexts: The importance of using human knowledge to solve the vocabulary gap problem has been recognized, and statistical learning of text-to-categories mapping based on human assignments has been a major focus in recent research in text categorization [3] [4] [5] [6] <ref> [7] </ref>. The LLSF mapping is a successful learner relying on past human relevance judgments, and can be used for both text retrieval [2] and text categorization. <p> We refer to the 10-point average precision as categorization accuracy. Average precision is the most commonly used measure in evaluations of categorization systems, because many systems provide a ranked list of categories for a given text instead of binary decisions over categories [3] [4] [5] [6] <ref> [7] </ref>. A ranked list, of course, can be used to obtain binary decisions by setting a threshold. There are discussions about using alternative measures [12] [3]; these issues, however, are open research questions, and are not the focus of this paper.
Reference: [8] <author> Golub GH, Van Loan CE. </author> <title> (1989) Matrix Computations, </title> <booktitle> 2nd Edition. </booktitle> <address> Baltimore, MD: </address> <publisher> The Johns Hopkins University Press. </publisher>
Reference-contexts: A practical and crucial question about LLSF is the computational complexity when applying LLSF to very large text collections. A conventional method for solving a LLSF problem employs a singular value decomposition (SVD) <ref> [8] </ref> of the input matrix A (the text-word matrix) as a part of the computation. <p> If m and n are large, this cubic complexity can become a computational bottleneck. Alternate algorithms were developed for very large and sparse matrices. The Lanczos methods <ref> [8] </ref> [10] [11], for example, are particularly efficient in situations where relatively few singular values are desired and relatively few orthogonalizations are needed. To take full advantage of Lanczos, however, the validity of using a truncated SVD instead of a complete SVD in LLSF needs to be established.
Reference: [9] <author> Dongarra JJ, Moler CB, Bunch JR, Stewart GW. </author> <title> (1979) LIN-PACK Users' Guide. </title> , <address> Philadelphia, PA: </address> <publisher> SIAM. </publisher>
Reference-contexts: A conventional method for solving a LLSF problem employs a singular value decomposition (SVD) [8] of the input matrix A (the text-word matrix) as a part of the computation. The standard SVD algorithm, as implemented in LINPACK <ref> [9] </ref>, has a time complexity O (m 2 n), where m is the number of texts in the training corpus and n is the number of distinct words in these texts (assuming m n). If m and n are large, this cubic complexity can become a computational bottleneck.
Reference: [10] <author> Cullum JK and Willoughby RA. </author> <title> (1985) Lanczos Algorithm for Large Symmetric Eigenvalue Computations. Vol.1: Theory. </title> <address> Boston: </address> <publisher> Birkhauser. </publisher>
Reference-contexts: If m and n are large, this cubic complexity can become a computational bottleneck. Alternate algorithms were developed for very large and sparse matrices. The Lanczos methods [8] <ref> [10] </ref> [11], for example, are particularly efficient in situations where relatively few singular values are desired and relatively few orthogonalizations are needed. To take full advantage of Lanczos, however, the validity of using a truncated SVD instead of a complete SVD in LLSF needs to be established.
Reference: [11] <author> Berry MW. </author> <title> (1992) Large-Scale Sparse Singular Value Computations. </title> <journal> The International Journal of Super-computer Applications Vol. </journal> <volume> 6 No.1: </volume> <pages> 13-49. </pages>
Reference-contexts: If m and n are large, this cubic complexity can become a computational bottleneck. Alternate algorithms were developed for very large and sparse matrices. The Lanczos methods [8] [10] <ref> [11] </ref>, for example, are particularly efficient in situations where relatively few singular values are desired and relatively few orthogonalizations are needed. To take full advantage of Lanczos, however, the validity of using a truncated SVD instead of a complete SVD in LLSF needs to be established. <p> Figure 5 shows the observed time savings in the SVD computation on the three document collections using a Lanczos algorithm, namely the Sparse Singular Value Decomposition or SSVD algorithm <ref> [11] </ref>. While no time savings were observed with up to 50% SV cuts, around 70% time savings were obtained at the 80% SV truncation threshold. Table 1 summarizes the effects of combined use of word cutting and SV cutting in solving the LLSF on SURCL, MEDCL and CACMCL.
Reference: [12] <editor> Lewis DD. </editor> <booktitle> (1991) Evaluating Text Categorization Proceedings of the Speech and Natural Language Workshop Asilomar, </booktitle> <publisher> Morgan Kaufmann:312-318. </publisher>
Reference-contexts: A ranked list, of course, can be used to obtain binary decisions by setting a threshold. There are discussions about using alternative measures <ref> [12] </ref> [3]; these issues, however, are open research questions, and are not the focus of this paper.
Reference: [13] <author> Fox EA. (Ed.). </author> <title> (1990) Virginia Disc One. </title> <institution> Virginia Polytechnic Institute and State University, Nimbus Records. </institution>
Reference-contexts: The chance of a random assignment being correct was 17 in 4020, or 0.42%. (3) CACMCL is a subset of documents derived from the CACM collection which is one of the standard information retrieval test collections <ref> [13] </ref>. The CACM collection consists of 3703 records each of which contains fields of title, abstract, keys (keywords), categories, author, etc. We take the words in the fields of title, abstract and keys without distinction, and call these words together a document.
Reference: [14] <institution> ACM Guide to Computing Literature Baltimore, MD: Association for Computing Machinery, </institution> <year> 1984: </year> <pages> 657-658. </pages>
Reference-contexts: We take the words in the fields of title, abstract and keys without distinction, and call these words together a document. The categories are defined in the Classification System for Computing Reviews (CSCR) and were assigned by humans to documents <ref> [14] </ref>. We used a subset of the 3703 documents in our experiments. We eliminated documents with an empty category field as obviously unsuitable for the study. We also eliminated documents with empty abstract fields because our primary interest is in documents with the potential for significant word removal.
Reference: [15] <author> Buckley C, Salton G, Allan J. </author> <title> (1993) Automatic Retrieval With Locality Information Using SMART. </title> <editor> In: DK Harman, Ed. </editor> <booktitle> The First Text REtrieval Conference (TREC-1):59-65. </booktitle>
Reference-contexts: Most systems apply the same generic stop-word list to all document collections without change. While the generic stop words are relatively safe to remove in the sense that their removal rarely causes a significant accuracy loss, the chance of significant accuracy improvement is also small [3] <ref> [15] </ref>. Since a generic stop-word list is often much smaller than the vocabularies of real-world document collections, only a limited number of words can be removed from texts, and the improvement in computational efficiency is therefore very limited. <p> Experiments in adding collection-specific high-frequency words to a generic stop-word list have shown no reliable improvements <ref> [15] </ref>. In contrast to using generic stop words, Wilbur and Sirotkin developed a novel stop-word identification method which allows a far more aggressive removal of words from documents without losing retrieval accuracy [16].
Reference: [16] <author> Wilbur JW, Sirotkin K. </author> <title> (1992) The automatic identification of stop words. </title> <institution> J. Inf. Sci. 18:4555. </institution>
Reference-contexts: Experiments in adding collection-specific high-frequency words to a generic stop-word list have shown no reliable improvements [15]. In contrast to using generic stop words, Wilbur and Sirotkin developed a novel stop-word identification method which allows a far more aggressive removal of words from documents without losing retrieval accuracy <ref> [16] </ref>. This method uses a collection of training documents to estimate word importance using a score, namely word strength. Clearly, word strengths are domain specific or application specific. <p> If the word strength is not at least two standard deviations above the strength of such a randomly distributed word, it is designated a stop word (refer to <ref> [16] </ref> for details). This method was tested using a training set of 71,311 documents from the MEDLINE database in the area of molecular biology/genetics.
Reference: [17] <author> Salton G. </author> <title> (1989) Automatic Text Processing: The Transformation, Analysis, </title> <booktitle> and Retrieval of Information by Computer. </booktitle> <address> Reading, Pennsylvania: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: This method uses a collection of training documents to estimate word importance using a score, namely word strength. Clearly, word strengths are domain specific or application specific. An important difference between word strength and other corpus-dependent word weighting schemes such as the Inverse Document Frequency <ref> [17] </ref> is that word strength is not computed based on word occurrencies in documents, but based on word co-occurrencies in pairs of related documents. Word strength measures how informative a word is in identifying two related documents.
Reference: [18] <author> Yang Y, W.J. Wilbur. </author> <title> (1995) Using Corpus Statistics to Remove Redundant Words in Text Categorization, </title> <journal> J Amer Soc Inf Sci (accepted). </journal>
Reference-contexts: In general the proportion of removable words may be less but still vary significant. Yang and Wilbur have applied the aggressive word removal method to document categorization to remove non-informative words from documents before applying a categorization method to these documents <ref> [18] </ref>. The effects on several categorization methods on different document collections have been studied and the effectiveness has been evident in the experiments.
Reference: [19] <author> Jackson JE. </author> <title> (1978) Review of Methods for statistical data analysis of multivariate observations Technometrics Vol 20: </title> <type> 210-211. </type>
Reference-contexts: was measured using the CPU minutes of the LINPACK algorithm. 4 Singular Values Truncation The idea of using truncated SVD to reduce the noise level in training documents is inspired by the Latent Structure Analysis theory, a well-known statistical method for analyzing causal factors or hidden reasons behind observed events <ref> [19] </ref>. A latent structure is defined as a linear combination of the independent variables, and can be computed using the SVD of a matrix which represents correspondences between independent and dependent variables.
Reference: [20] <author> Deerwester S., Dumais ST, Furnas GW, Landauer TK, Harsh-man R. </author> <title> (1990) Indexing by Latent Semantic analysis. </title> <journal> J Amer Soc Inf Sci 41, </journal> <volume> 6, </volume> <pages> 391-407. </pages>
Reference-contexts: One set of the singular vectors has independent variables as the dimensions, and the other set of singular vectors has dependent variables as the dimensions. These singular vectors are interpreted as the orthogonal factors, artificial concepts or latent semantic structures (LSS) <ref> [20] </ref>. The word latent is used because it is often difficult to give an intuitive interpretation about the meanings of these structures. It would be helpful, however, to have some understanding of the LSS. <p> In general, the orthogonal basis of a given matrix consists of linear combinations of the row or column vectors of the matrix. In our problem, the rows are words, the columns are documents, and an LSS is a linear combination of documents <ref> [20] </ref>. Since each document is a vector of word weights, a linear combination of documents is again a vector of word weights. Therefore, we can say that an LSS is a word combination which does not mean a phrase, a sentence, or any continuous piece of text. <p> For convenience, I do not distinguish between a complete LLSF and a pseudo-LLSF, unless specified. I want to point out the similarities of the pseudo-LLSF and the Latent Semantic Indexing (LSI) in document retrieval <ref> [20] </ref>, and the fundamental difference between my hypothesis about truncated SVD and the claim in LSI. <p> Evaluation results, however, have shown no reliable improvement of LSI over baseline text matching <ref> [20] </ref> [21] [22]. My hypothesis about truncated SVD is different from the hypothesis in LSI. I do not claim an improvement of synonym representation in a document matrix by using such an approach.
Reference: [21] <author> Chute CG, Yang Y. </author> <title> (1991) Latent semantic indexing of medical diagnoses using UMLS Semantic Structures. </title> <booktitle> Proceedings of the Fifteenth Annual Symposium on Computer Applications in Medical Care (SCAMC </booktitle> 91):185-189. 
Reference-contexts: Evaluation results, however, have shown no reliable improvement of LSI over baseline text matching [20] <ref> [21] </ref> [22]. My hypothesis about truncated SVD is different from the hypothesis in LSI. I do not claim an improvement of synonym representation in a document matrix by using such an approach.
Reference: [22] <author> Dumais S. </author> <title> (1994) Latent Semantic Indexing (LSI) and TREC-2. </title> <editor> In: DK Harman, Ed. </editor> <booktitle> The Second Text REtrieval Conference (TREC-2):105116. </booktitle>
Reference-contexts: Evaluation results, however, have shown no reliable improvement of LSI over baseline text matching [20] [21] <ref> [22] </ref>. My hypothesis about truncated SVD is different from the hypothesis in LSI. I do not claim an improvement of synonym representation in a document matrix by using such an approach.
References-found: 22

