URL: http://www.cs.uni-bonn.de/III/lehre/seminare/ServiceRobotic/WS97/MULTI/Multiagent-survey.ps.gz
Refering-URL: http://www.cs.uni-bonn.de/III/lehre/seminare/ServiceRobotic/WS97/
Root-URL: http://cs.uni-bonn.de
Title: Multiagent Systems: A Survey from a Machine Learning Perspective  
Author: Peter Stone and Manuela Veloso 
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Date: December 4, 1997  
Pubnum: CMU-CS-97-193  
Abstract: Distributed Artificial Intelligence (DAI) has existed as a subfield of AI for less than two decades. DAI is concerned with systems that consist of multiple independent entities that interact in a domain. Traditionally, DAI has been divided into two sub-disciplines: Distributed Problem Solving (DPS) focusses on the information management aspects of systems with several branches working together towards a common goal; Multiagent Systems (MAS) deals with behavior management in collections of several independent entities, or agents. This survey of MAS is intended to serve as an introduction to the field and as an organizational framework. A series of increasingly complex general multiagent scenarios are presented. For each scenario, the issues that arise are described along with a sampling of the techniques that exist to deal with them. The presented techniques are not exhaustive, but they highlight how multiagent systems can be and have been used to build complex systems. When options exist, the techniques presented are biased towards machine learning approaches. Additional opportunities for applying machine learning to MAS are highlighted and robotic soccer is presented as an appropriate testbed for MAS. This research is sponsored in part by the DARPA/RL Knowledge Based Planning and Scheduling Initiative under grant number F30602-95-1-0018. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies or endorsements, either expressed or implied, of the U. S. Government. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> AAAI. </editor> <booktitle> Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <address> Menlo Park,CA, June 1995. </address> <publisher> AAAI Press. Victor LessorGeneral Chair. </publisher>
Reference-contexts: MAS is an active field with many open issues. Continuing research is presented at dedicated conferences and workshops such as the International Conference on Multi-Agent Systems <ref> [95, 2, 1] </ref>. MAS work also appears in many of the DAI conferences and workshops [22, 94]. This survey provides a framework within which the reader can situate both existing and future work.
Reference: [2] <author> AAAI. </author> <title> Adaptation, Coevolution and Learning in Multiagent Systems: </title> <booktitle> Papers from the 1996 AAAI Spring Symposium, </booktitle> <address> Menlo Park,CA, </address> <month> March </month> <year> 1996. </year> <note> AAAI Press. AAAI Technical Report SS-96-01. Sandip SenChair. </note>
Reference-contexts: The techniques presented are not exhaustive, but they highlight how multiagent systems can be and have been used to build complex systems. Because of the inherent complexity of MAS, there is much interest in using machine learning techniques to help deal with this complexity <ref> [95, 2] </ref>. When several different systems exist that could illustrate the same or similar MAS techniques, the systems presented here are biased towards those that use machine learning (ML) approaches. Furthermore, every effort is made to highlight additional opportunities for applying ML to MAS. <p> MAS is an active field with many open issues. Continuing research is presented at dedicated conferences and workshops such as the International Conference on Multi-Agent Systems <ref> [95, 2, 1] </ref>. MAS work also appears in many of the DAI conferences and workshops [22, 94]. This survey provides a framework within which the reader can situate both existing and future work.
Reference: [3] <author> Sorin Achim, Peter Stone, and Manuela Veloso. </author> <title> Building a dedicated robotic soccer system. </title> <booktitle> In Proceedings of the IROS-96 Workshop on RoboCup, </booktitle> <month> November </month> <year> 1996. </year>
Reference-contexts: Sahota et al. built a 1 vs. 1 version of the game. Asada et al. have used vision-based RL with their soccer playing robots [5]. Veloso et al. discuss some of the robotic issues involved in building robotic soccer players <ref> [3, 89] </ref>. Some robotic issues can only be studied in the real-world instantiation, but there are also many issues that can be studied in simulation. A particularly good simulator for this purpose is the soccerserver developed by Noda [56] and pictured in Figure 12.
Reference: [4] <author> Neeraj Arora and Sandip Sen. </author> <title> Resolving social dilemmas using genetic algorithms. In Adaptation, Coevolution and Learning in Multiagent Systems: </title> <booktitle> Papers from the 1996 AAAI Spring Symposium, </booktitle> <pages> pages 15, </pages> <address> Menlo Park,CA, </address> <month> March </month> <year> 1996. </year> <note> AAAI Press. AAAI Technical Report SS-96-01. </note>
Reference-contexts: Permpoontanalarp [59] * Model as a team (individual ! role). Tambe [85, 86] * Social reasoning: depend on others for goal (6= game theory). Sichman and Demazeau [79] * GAs to deal with Braes' paradox (more resource ! worse). Arora and Sen <ref> [4] </ref> * Multiagent RL for adaptive Load Balancing. Schaerf, Shoham, and Tennenholtz [76] * Focal points/Emergent conventions. Fenster et al./Walker and Woolridge [26, 91] * Design agents play different roles. <p> Glance and Hogg use GAs to represent different parts of a contrived sample network. Arora and Sen then improve the GA representation slightly and show that with the new representation, the system is actually able to find the globally optimal traffic flow <ref> [4] </ref>. Adaptive load balancing has been studied as a multiagent problem by allowing different agents to decide which processor to use at a given time. Using RL, Schaerf et al. show that the heterogeneous agents can achieve reasonable load balance without any central control and without communication among agents [76].
Reference: [5] <author> M. Asada, E. Uchibe, S. Noda, S. Tawaratsumida, and K. Hosoda. </author> <title> Coordination of multiple behaviors acquired by vision-based reinforcement learning. </title> <booktitle> In Proc. of IEEE/RSJ/GI International Conference on Intelligent Robots and Systems 1994 (IROS '94), </booktitle> <pages> pages 917924, </pages> <year> 1994. </year>
Reference-contexts: The first robotic soccer system was the Dynamo system [70]. Sahota et al. built a 1 vs. 1 version of the game. Asada et al. have used vision-based RL with their soccer playing robots <ref> [5] </ref>. Veloso et al. discuss some of the robotic issues involved in building robotic soccer players [3, 89]. Some robotic issues can only be studied in the real-world instantiation, but there are also many issues that can be studied in simulation.
Reference: [6] <author> T. Balch. </author> <title> Learning roles: Behavioral diversity in robot teams. </title> <institution> College of Computing Technical Report GIT-CC-97-12, Georgia Institute of Technology, Atlanta, Georgia, </institution> <month> March </month> <year> 1997. </year> <month> 28 </month>
Reference-contexts: For example, Balch uses a behavioral diversity measure to encourage role learning in a RL framework, finding that providing a uniform reinforcement to the entire team is more effective than providing local reinforcements to individual players <ref> [6] </ref>. Luke et al. use genetic programming to evolve cooperative behaviors within a team of players [50]. 8 Conclusion This survey is presented as a description of the field of MAS.
Reference: [7] <author> Tucker Balch and Ronald C. Arkin. </author> <title> Motor schema-based formation control for multiagent robot teams. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <pages> pages 1016, </pages> <address> Menlo Park, California, June 1995. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: The agents' goals, actions, and/or domain knowledge are all identical as indicated by the identical fonts. Homogeneous Non-Communicating Issues * Reactive vs. deliberative agents * Local or global perspective * Modeling of other agents' states * How to affect others Techniques * Reactive Behaviors for Formation maintenance. Balch <ref> [7] </ref> * Local knowledge sometimes better. Roychowdhury [67] * (limited) Recursive Modeling Method (RMM). Durfee [24] * Don't model othersjust pay attention to reward. Schmidhuber [77] * Stigmergy. Holland/Goldman and Rosenschein [39, 31] * Q-learning for behaviors like foraging, homing, etc. <p> Here we describe one system at each extreme as well as two others that mix reactive and deliberative reasoning. Balch and Arkin use homogeneous, reactive, non-communicating agents to study formation maintenance in autonomous robots <ref> [7] </ref>. The robots' goal is to move together in a military formation such as a diamond, column, or wedge. They periodically come across obstacles which prevent one or more of the robots from moving in a straight line.
Reference: [8] <author> Mihai Barbuceanu and Mark S. Fox. </author> <title> Cool: A language for describing coordination in multi agent systems. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <pages> pages 1724, </pages> <address> Menlo Park, California, June 1995. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Independent 22 aspects of protocols are information content, message format, and coordination conventions. Among many others, existing language protocols for these three levels are: KIF for content [29], KQML for message format [28], and, more recently, COOL for coordination <ref> [8] </ref>. There has been a lot of research done on refining these and other communication protocols.
Reference: [9] <author> M. Benda, V. Jagannathan, and R. Dodhiawala. </author> <title> On optimal cooperation of knowledge sources an empirical investigation. </title> <type> Technical Report BCSG201028, </type> <institution> Boeing Advanced Technology Center, Boeing Computing Services, </institution> <address> Seattle, Washington, </address> <month> July </month> <year> 1986. </year>
Reference-contexts: However by first increasing heterogeneity and then communication, all of the important issues and techniques in MAS are encountered. For each multiagent scenario presented, a single example domain is presented in an appropriate instan-tiation for the purpose of illustration. In this extensively-studied domain, the Predator/Prey or Pursuit domain <ref> [9] </ref>, many MAS issues arise. Nevertheless, it is a toy domain. At the end of the article, a much more complex domainrobotic socceris presented in order to illustrate the full power of MAS. 1 The article is organized as follows. <p> It is not presented as a complex real-world domain, but rather as a toy domain that helps concretize many concepts. For discussion of a domain that has the full range of complexities characteristic of more real-world domains, see Section 7. The pursuit domain was introduced by Benda et al. <ref> [9] </ref>. Over the years, researchers have studied several variations of its original formulation. In this section, a single instantiation of the domain is presented. However, care is taken to point out the parameters that can be varied. The pursuit domain is usually studied with four predators and one prey. <p> Observe that by taking MAS to the extreme of full communication, we may arrive at a single-agent system. Benda et al., in the original presentation of the pursuit domain, also consider the full range of communication possibilities, all the way up to the central strategy <ref> [9] </ref>. They consider the possible organizations of the four predators when any pair can either exchange data, exchange data and goals, or have one control the other. The tradeoff between lower communication costs and better decisions is described.
Reference: [10] <author> Alan H. Bond and Les Gasser. </author> <title> An analysis of problems and research in DAI. </title> <editor> In Alan H. Bond and Les Gasser, editors, </editor> <booktitle> Readings in Distributed Artificial Intelligence, </booktitle> <pages> pages 335. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: DAI has existed as a subfield of AI for less than two decades. Traditionally, DAI is broken into two sub-disciplines: Distributed Problem Solving (DPS) and MAS <ref> [10] </ref>. The main topics considered in DPS are information management issues such as task decomposition and solution synthesis. For example, a constraint satisfaction problem can often be decomposed into several not entirely independent subproblems that can be solved on different processors. <p> The goal of this section is to underscore the need for and usefulness of MAS while giving characteristics of typical domains that can benefit from it. For a more extensive discussion, see <ref> [10] </ref>. The most important reason to use MAS when designing a system is that some domains require it. In particular, if there are different people or organizations with different (possibly conflicting) goals and proprietary information, then a multiagent system is needed to handle their interactions. <p> From this perspective, the important characteristics of MAS are: * System function; * Agent architecture (degree of heterogeneity, reactive vs. deliberative); * System architecture (communication, protocols, human involvement). A useful contribution is that the dimensions are divided into agent and system characteristics. Other overviews of DAI and/or MAS include <ref> [47, 23, 25, 10] </ref>. This article contributes a taxonomy specifically focussed on MAS along with a detailed chronicle of existing systems as they fit in to this taxonomy.
Reference: [11] <author> L. Bull, T.C. Fogarty, and M. Snaith. </author> <title> Evolution in multi-agent systems: Evolving communicating classifier systems for gait in a quadrupedal robot. </title> <editor> In Stephanie Forrest, editor, </editor> <booktitle> Proceedings of the Sixth International Conference on Genetic Algorithms, </booktitle> <pages> pages 382388, </pages> <address> San Mateo,CA, July 1995. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Mataric [53] * Bayesian learning in negotiation: model others. Zeng and Sycara [96] * Multiagent Q-learning. Weiss [93] * Training other agents' Q-functions (track driving). Clouse [13] * Minimize the need for training. Potter and Grefenstette [61] * Cooperative co-evolution. Bull et al. <ref> [11] </ref> * Contract nets for electronic commerce. Sandholm and Lesser [73] * Market-based systems. Huberman and Clearwater [41] * Generalized Partial Global Planning (GPGP). Decker [21] * Internal, Social, and Collective (role) commitments. Castelfranchi [12] * Commitment states (potential, pre, and actual) as planning states. <p> Subpopulations of rules are seeded to be more effective in different situations. Thus specialized subpopulations of rules corresponding to shaped behaviors tend to emerge. Rather than competitive co-evolution Bull et al. build a system system which uses cooperative coevolution <ref> [11] </ref>. They use GAs to evolve separate communicating agents to control different legs of a quadrapedal robot. 23 Drawing inspiration from competition in human societies, several researchers have designed systems based on the law of supply and demand.
Reference: [12] <author> Cristiano Castelfranchi. </author> <title> Commitments: From individual intentions to groups and organizations. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <pages> pages 4148, </pages> <address> Menlo Park, California, June 1995. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Potter and Grefenstette [61] * Cooperative co-evolution. Bull et al. [11] * Contract nets for electronic commerce. Sandholm and Lesser [73] * Market-based systems. Huberman and Clearwater [41] * Generalized Partial Global Planning (GPGP). Decker [21] * Internal, Social, and Collective (role) commitments. Castelfranchi <ref> [12] </ref> * Commitment states (potential, pre, and actual) as planning states. Haddadi [34] * Belief/Desire/Intention (BDI) model: OASIS. Rao and Georgeff [63] * BDI commitments only over intentions. Rao and Georgeff [63] * Coalitions. Zlotkin and Rosenschein/Shehory and Kraus/Sandholm and Lesser [97, 78, 72] * Reasoning about truthfulness. <p> The theory of commitment and decommitment (when the commitment terminates) has consequently drawn considerable attention. For example, Castelfranchi defines three types of commitment: internal commitmentan agent binds itself to do something, social commitmentan agent commits to another agent, and collective commitment an agent agrees to fill a certain role <ref> [12] </ref>. Setting an alarm clock is an example of internal commitment to wake up at a certain time. Haddadi discusses commitment states as planning states: potential cooperation, pre-commitment, and commitment [34]. Agents can then use means-ends analysis to plan for goals in terms of commitment opportunities.
Reference: [13] <author> Jeffery A. Clouse. </author> <title> Learning from an automated training agent. </title> <editor> In Gerhard Weiand Sandip Sen, editors, </editor> <title> Adaptation and Learning in Multiagent Systems. </title> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1996. </year>
Reference-contexts: Cohen and Levesque/Lux and Steiner [14, 51] * Learning social behaviors. Mataric [53] * Bayesian learning in negotiation: model others. Zeng and Sycara [96] * Multiagent Q-learning. Weiss [93] * Training other agents' Q-functions (track driving). Clouse <ref> [13] </ref> * Minimize the need for training. Potter and Grefenstette [61] * Cooperative co-evolution. Bull et al. [11] * Contract nets for electronic commerce. Sandholm and Lesser [73] * Market-based systems. Huberman and Clearwater [41] * Generalized Partial Global Planning (GPGP). Decker [21] * Internal, Social, and Collective (role) commitments. <p> Eventually, the learner is able to perform the task without any guidance. Clouse studies the effect of different levels of advice in a road-following domain <ref> [13] </ref>. He concludes that moderate advice improves performance and speeds up learning, while too much advice leads to worse performance because the learner does not experience enough negative examples while training.
Reference: [14] <author> Philip R. Cohen and Hector J. Levesque. </author> <title> Communicative actions for artificial agents. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <pages> pages 6572, </pages> <address> Menlo Park, California, June 1995. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Cohen and Levesque/Lux and Steiner <ref> [14, 51] </ref> * Learning social behaviors. Mataric [53] * Bayesian learning in negotiation: model others. Zeng and Sycara [96] * Multiagent Q-learning. Weiss [93] * Training other agents' Q-functions (track driving). Clouse [13] * Minimize the need for training. Potter and Grefenstette [61] * Cooperative co-evolution. <p> When combined with a model of other agents, the effect of a communication act might be to alter an agent's belief about the state of another agent or agents. The theory of communication as action is called speech acts <ref> [14, 51] </ref>. Mataric adds a learning dimension to the idea of speech acts. Starting with the foraging behavior mentioned above [52], the agents can then learn to choose among a set of social behaviors that include broadcasting and listening [53].
Reference: [15] <author> Kerstin Dautenhahn. </author> <title> Getting to know each otherartificial social intelligence for autonomous robots. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <address> 16:333356, </address> <year> 1995. </year>
Reference-contexts: As Gerhard Wei put it: Intelligence is deeply and inevitably coupled with interaction [94]. In fact, it has been proposed that the best way to develop intelligent machines at all might be to start by creating social machines <ref> [15] </ref>. This theory is based on the socio-biological theory that primate intelligence first evolved because of the need to deal with social interactions. Reasons presented above to use MAS are summarized in Table 1.
Reference: [16] <author> Keith S. Decker. </author> <title> Distributed problem solving: A survey. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 17(5):729740, </volume> <month> September </month> <year> 1987. </year>
Reference-contexts: Single-agent systems should be used in such cases. Finally, multiagent systems can be useful for their illucidation of intelligence <ref> [16] </ref>. As Gerhard Wei put it: Intelligence is deeply and inevitably coupled with interaction [94]. In fact, it has been proposed that the best way to develop intelligent machines at all might be to start by creating social machines [15]. <p> For example, Decker presents four dimensions of DAI <ref> [16] </ref>: 1. Agent granularity (coarse vs. fine); 2. Heterogeneity of agent knowledge (redundant vs. specialized); 3. Methods of distributing control (benevolent vs. competitive, team vs. hierarchical, static vs. shifting roles); 4. and Communication possibilities (blackboard vs. messages, low-level vs. high-level, content).
Reference: [17] <author> Keith S. Decker. </author> <title> Environment Centered Analysis and Design of Coordination Mechanisms. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <year> 1995. </year>
Reference-contexts: In this case, the designer must consider the issue of communication between the human and computer agents [71]. Another example of user involvement is user feedback in an information filtering domain [27]. 8 Decker distinguishes three different sources of uncertainty in a domain <ref> [17] </ref>. The transitions in the domain itself might be non-deterministic; agents might not know the actions of other agents; and agents might not know the outcomes of their own actions. This and the other domain characteristics are summarized in Table 4. <p> Table 4: Domain characteristics that are important when designing MAS * Number of agents * Amount of time pressure (real time?) * Dynamically arriving goals? * Cost of communication * Cost of failure * User involvement * Environmental uncertainty: Decker <ref> [17] </ref> a priori in the domain in the actions of other agents in outcomes of an agent's own actions 4 Homogeneous Non-Communicating Multiagent Systems The simplest multiagent scenario involves homogeneous non-communicating agents.
Reference: [18] <author> Keith S. Decker. </author> <title> Distributed artificial intelligence testbeds. </title> <editor> In G. M. P. O'Hare and N. R. Jennings, editors, </editor> <booktitle> Foundations of Distributed Artificial Intelligence, </booktitle> <pages> pages 119138. </pages> <publisher> Wiley Interscience, </publisher> <year> 1996. </year>
Reference-contexts: Furthermore, teams of agents can be evaluated by playing against each other, or perhaps against standard teams. The simulator was successfully used for a competition among twenty-nine teams from around the world in 1997 [44]. Thus robotic soccer satisfies Decker's criteria for DAI testbeds <ref> [18] </ref>. 25 7.2 MAS in Robotic Soccer The main goal of any testbed is to facilitate the trial and evaluation of ideas that have promise in the real world. A wide variety of MAS issues can be studied in simulated robotic soccer.
Reference: [19] <author> Keith S. Decker. </author> <type> Personal correspondence, </type> <month> May </month> <year> 1996. </year>
Reference-contexts: Of course there are some domains that are more naturally approached from an omniscient perspectivebecause a global view is givenor with centralized controlbecause no parallel actions are possible and there is no action uncertainty <ref> [19] </ref>. Single-agent systems should be used in such cases. Finally, multiagent systems can be useful for their illucidation of intelligence [16]. As Gerhard Wei put it: Intelligence is deeply and inevitably coupled with interaction [94].
Reference: [20] <author> Keith S. Decker. </author> <title> Task environment centered simulation. </title> <editor> In M. Prietula, K. Carley, and L. Gasser, editors, </editor> <title> Simulating Organizations: Computational Models of Institutions and Groups. </title> <publisher> AAAI Press/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: They must then be combined into a multiagent system with the aid of some of the techniques described in this article. Another example of a domain that requires MAS is hospital scheduling as presented in <ref> [20] </ref>. This domain from an actual case study requires different agents to represent the interests of different people within the hospital. Hospital employees have different interests, from nurses who want to minimize the patient's time in the hospital, to x-ray operators who want to maximize the throughput on their machines.
Reference: [21] <author> Keith S. Decker and Victor R. Lesser. </author> <title> Designing a family of coordination algorithms. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <pages> pages 7380, </pages> <address> Menlo Park, California, June 1995. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Clouse [13] * Minimize the need for training. Potter and Grefenstette [61] * Cooperative co-evolution. Bull et al. [11] * Contract nets for electronic commerce. Sandholm and Lesser [73] * Market-based systems. Huberman and Clearwater [41] * Generalized Partial Global Planning (GPGP). Decker <ref> [21] </ref> * Internal, Social, and Collective (role) commitments. Castelfranchi [12] * Commitment states (potential, pre, and actual) as planning states. Haddadi [34] * Belief/Desire/Intention (BDI) model: OASIS. Rao and Georgeff [63] * BDI commitments only over intentions. Rao and Georgeff [63] * Coalitions. <p> In the current scenario, agents can also coordinate schedules. Decker's Generalized Partial Global Planning (GPGP) allows several heterogeneous agents to post constraints, or commitments to do a task by some time, to each other's local schedulers and thus coordinate without the aid of any centralized agent <ref> [21] </ref>. 6.3.5 Commitment/decommitment When agents communicate, they may decide to cooperate on a given task or for a given amount of time. In so doing, they make commitments to each other.
Reference: [22] <institution> Proceedings of the 10th International Workshop on Distributed Artificial Intelligence, Bandera, Texas, </institution> <month> October </month> <year> 1990. </year> <month> 29 </month>
Reference-contexts: MAS is an active field with many open issues. Continuing research is presented at dedicated conferences and workshops such as the International Conference on Multi-Agent Systems [95, 2, 1]. MAS work also appears in many of the DAI conferences and workshops <ref> [22, 94] </ref>. This survey provides a framework within which the reader can situate both existing and future work. Acknowledgements We would like to thank Keith Decker, Rala Stone, Russell Stone, Astro Teller, and the anonymous reviewers for their helpful comments and suggestions.
Reference: [23] <author> Edmund H. Durfee. </author> <title> What your computer really needs to know, you learned in kindergarten. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <address> Philadelphia, PA, 1992. </address> <publisher> Morgan Kaufman. Invited Talk. </publisher>
Reference-contexts: From this perspective, the important characteristics of MAS are: * System function; * Agent architecture (degree of heterogeneity, reactive vs. deliberative); * System architecture (communication, protocols, human involvement). A useful contribution is that the dimensions are divided into agent and system characteristics. Other overviews of DAI and/or MAS include <ref> [47, 23, 25, 10] </ref>. This article contributes a taxonomy specifically focussed on MAS along with a detailed chronicle of existing systems as they fit in to this taxonomy.
Reference: [24] <author> Edmund H. Durfee. </author> <title> Blissful ignorance: Knowing just enough to coordinate well. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <pages> pages 406413, </pages> <address> Menlo Park, California, June 1995. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Homogeneous Non-Communicating Issues * Reactive vs. deliberative agents * Local or global perspective * Modeling of other agents' states * How to affect others Techniques * Reactive Behaviors for Formation maintenance. Balch [7] * Local knowledge sometimes better. Roychowdhury [67] * (limited) Recursive Modeling Method (RMM). Durfee <ref> [24] </ref> * Don't model othersjust pay attention to reward. Schmidhuber [77] * Stigmergy. Holland/Goldman and Rosenschein [39, 31] * Q-learning for behaviors like foraging, homing, etc. <p> Better performance by agents with less knowledge is occasionally summarized by the cliche Ignorance is Bliss. 4.3.3 Modeling of other agents' states Durfee gives another example of Blissful Ignorance, mentioning it explicitly in the title of his paper: Blissful Ignorance: Knowing Just Enough to Coordinate Well <ref> [24] </ref>. Now rather than referring to resource usage, the saying applies to the limited Recursive Modeling Method (RMM). As mentioned above in the context of the pursuit domain, RMM could recurse indefinitely. <p> Durfee contends that for coordination to be possible, some potential knowledge must be ignored. As well as illustrating this concept in the pursuit domain [90], Durfee goes into more detail and offers more generally applicable methodology in <ref> [24] </ref>. The point of the RMM is to model the internal state of another agent in order to predict its actions. Even though the agents know each other's goals and structure (they are homogeneous), they may not know each other's future actions.
Reference: [25] <author> Edmund H. Durfee, Victor R. Lesser, and Daniel D. Corkill. </author> <title> Trends in cooperative distributed problem solving. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 1(1):6383, </volume> <month> March </month> <year> 1989. </year>
Reference-contexts: From this perspective, the important characteristics of MAS are: * System function; * Agent architecture (degree of heterogeneity, reactive vs. deliberative); * System architecture (communication, protocols, human involvement). A useful contribution is that the dimensions are divided into agent and system characteristics. Other overviews of DAI and/or MAS include <ref> [47, 23, 25, 10] </ref>. This article contributes a taxonomy specifically focussed on MAS along with a detailed chronicle of existing systems as they fit in to this taxonomy.
Reference: [26] <author> Maier Fenster, Sarit Kraus, and Jeffrey S. Rosenschein. </author> <title> Coordination without communication: Experimental validation of focal point techniques. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <pages> pages 102108, </pages> <address> Menlo Park, California, June 1995. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Sichman and Demazeau [79] * GAs to deal with Braes' paradox (more resource ! worse). Arora and Sen [4] * Multiagent RL for adaptive Load Balancing. Schaerf, Shoham, and Tennenholtz [76] * Focal points/Emergent conventions. Fenster et al./Walker and Woolridge <ref> [26, 91] </ref> * Design agents play different roles. <p> Thus even without communicating, people are sometimes able to coordinate actions. Apparently features that have been seen or used often present themselves as obvious choices. In the context of MAS, Fenster et al. define the Focal Point method <ref> [26] </ref>. They discuss the phenomenon of cultural (or programmed) preferences allowing agents to meet without communicating. They propose that, all else being equal, agents who need to meet should choose rare or extreme options. On a similar note, conventions might emerge over time.
Reference: [27] <author> Innes A. Ferguson and Grigoris J. Karakoulas. </author> <title> Multiagent learning and adaptation in an information filtering market. In Adaptation, Coevolution and Learning in Multiagent Systems: </title> <booktitle> Papers from the 1996 AAAI Spring Symposium, </booktitle> <pages> pages 2832, </pages> <address> Menlo Park,CA, </address> <month> March </month> <year> 1996. </year> <note> AAAI Press. AAAI Technical Report SS-96-01. </note>
Reference-contexts: Several multiagent systems include humans as one or more of the agents. In this case, the designer must consider the issue of communication between the human and computer agents [71]. Another example of user involvement is user feedback in an information filtering domain <ref> [27] </ref>. 8 Decker distinguishes three different sources of uncertainty in a domain [17]. The transitions in the domain itself might be non-deterministic; agents might not know the actions of other agents; and agents might not know the outcomes of their own actions.
Reference: [28] <author> Tim Finin, Don McKay, Rich Fritzson, and Robin McEntire. </author> <title> Kqml: An information and knowledge exchange protocol. </title> <editor> In Kazuhiro Fuchi and Toshio Yokoi, editors, </editor> <title> Knowledge Building and Knowledge Sharing. </title> <publisher> Ohmsha and IOS Press, </publisher> <year> 1994. </year>
Reference-contexts: Heterogeneous Communicating Issues * Understanding each other * Planning communicative acts * Benevolence vs. competitiveness * Resource management (schedule coordination) * Commitment/decommitment * Truth in communication Techniques * Language protocols: KIF for content (Genesreth and Fikes [29]), KQML for message format (Finin et al. <ref> [28] </ref>). * Speech acts. Cohen and Levesque/Lux and Steiner [14, 51] * Learning social behaviors. Mataric [53] * Bayesian learning in negotiation: model others. Zeng and Sycara [96] * Multiagent Q-learning. Weiss [93] * Training other agents' Q-functions (track driving). Clouse [13] * Minimize the need for training. <p> Independent 22 aspects of protocols are information content, message format, and coordination conventions. Among many others, existing language protocols for these three levels are: KIF for content [29], KQML for message format <ref> [28] </ref>, and, more recently, COOL for coordination [8]. There has been a lot of research done on refining these and other communication protocols.
Reference: [29] <author> M. R. Genesereth and R. E. Fikes. </author> <title> Knowledge interchange format, version 3.0 reference manual. </title> <type> Technical Report Logic-92-1, </type> <institution> Computer Science Department, Stanford University, </institution> <year> 1992. </year>
Reference-contexts: Communication can either be broadcast or transmitted point-to-point. Heterogeneous Communicating Issues * Understanding each other * Planning communicative acts * Benevolence vs. competitiveness * Resource management (schedule coordination) * Commitment/decommitment * Truth in communication Techniques * Language protocols: KIF for content (Genesreth and Fikes <ref> [29] </ref>), KQML for message format (Finin et al. [28]). * Speech acts. Cohen and Levesque/Lux and Steiner [14, 51] * Learning social behaviors. Mataric [53] * Bayesian learning in negotiation: model others. Zeng and Sycara [96] * Multiagent Q-learning. Weiss [93] * Training other agents' Q-functions (track driving). <p> Independent 22 aspects of protocols are information content, message format, and coordination conventions. Among many others, existing language protocols for these three levels are: KIF for content <ref> [29] </ref>, KQML for message format [28], and, more recently, COOL for coordination [8]. There has been a lot of research done on refining these and other communication protocols.
Reference: [30] <author> Natalie S. Glance and Tad Hogg. </author> <title> Dilemmas in computational societies. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <pages> pages 117124, </pages> <address> Menlo Park, California, June 1995. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Designers of multiagent systems with limited resources must decide how the agents will share the resources. One interesting network traffic problem called Braess' paradox has been studied from a multiagent perspective using GAs <ref> [30] </ref>. Braess' paradox is the phenomenon of adding more resources to a network but getting worse performance. <p> Glance and Hogg claim that under certain conditions, including usage-dependent resource costs, agents that are sharing the network and reasoning separately about which path of the network to use cannot achieve global optimal performance <ref> [30] </ref>. Glance and Hogg use GAs to represent different parts of a contrived sample network. Arora and Sen then improve the GA representation slightly and show that with the new representation, the system is actually able to find the globally optimal traffic flow [4].
Reference: [31] <author> Claudia Goldman and Jeffrey Rosenschein. </author> <title> Emergent coordination through the use of cooperative state-changing rules. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 408413, </pages> <address> Philadelphia, PA, 1994. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Balch [7] * Local knowledge sometimes better. Roychowdhury [67] * (limited) Recursive Modeling Method (RMM). Durfee [24] * Don't model othersjust pay attention to reward. Schmidhuber [77] * Stigmergy. Holland/Goldman and Rosenschein <ref> [39, 31] </ref> * Q-learning for behaviors like foraging, homing, etc. Mataric [52] Learning opportunities * Enable others' actions * Sensor data ! Other agent's sensor data Table 5: The issues, techniques, and learning opportunities for homogeneous MAS as reflected in the literature. state. <p> For example, a robotic agent might leave a marker behind it for other agents to observe. Goldman and Rosenschein demonstrate an effective form of active stigmergy in which agents heuristically alter the environment in order to facilitate future unknown plans of other agents <ref> [31] </ref>. Second, passive stigmergy involves altering the environment so that the effects of another agent's actions change. <p> Even if they have different goals, the agents can be benevolent if they are willing to help each other achieve their respective goals <ref> [31] </ref>. On the other hand, the agents may be selfish and only consider their own goals when acting. In the extreme, the agents may be involved in a zero-sum situation so that they must actively oppose other agents' goals in order to achieve their own.
Reference: [32] <author> John Grefenstette and Robert Daley. </author> <title> Methods for competitive and cooperative co-evolution. In Adaptation, Coevolution and Learning in Multiagent Systems: </title> <booktitle> Papers from the 1996 AAAI Spring Symposium, </booktitle> <pages> pages 4550, </pages> <address> Menlo Park,CA, </address> <month> March </month> <year> 1996. </year> <note> AAAI Press. AAAI Technical Report SS-96-01. </note>
Reference-contexts: Mor and Rosenschein/Sandholm and Crites [55, 75] * Minimax-Q. Littman [49] * Competitive co-evolution. Haynes and Sen/Grefenstette and Daley/Rosin and Belew <ref> [37, 32, 66] </ref> * Deduce intentions through observation. Huber and Durfee [40] * Autoepistemic reasoning (ignorance). Permpoontanalarp [59] * Model as a team (individual ! role). Tambe [85, 86] * Social reasoning: depend on others for goal (6= game theory). <p> The evolution of both predator and prey agents by Haynes and Sen [37] qualifies as competitive co-evolution. Grefenstette and Daley conduct a preliminary study of competitive and cooperative co-evolution in a domain that is loosely related to the pursuit domain <ref> [32] </ref>. Their domain has two robots that can move continuously and one morsel of (stationary) food that appears randomly in the world. In the cooperative task, both robots must be at the food in order to capture it. <p> In a competitive task in the same domain, agents try to be the first to reach the food <ref> [32] </ref>. Again, different GA evaluation methods are considered for use in evolving rule sets to control the agents. One problem to contend with in competitive rather than cooperative co-evolution is the possibility of an escalating arms race with no end.
Reference: [33] <author> Fred Gruau. </author> <title> Announcement for participation of the first autonomous robotics football tournament, </title> <note> 1997. Accessible from http://www.dcs.qmw.ac.uk/research/ai/robot/football/FirstARFTannouncement.html. </note>
Reference-contexts: In this section a single domain which embodies most multiagent issues is presented. Robotic soccer is a particularly good domain for studying MAS. Originated by Alan Mackworth [70], it has been gaining popularity in recent years, with several international competitions taking place <ref> [43, 44, 33] </ref>. It is also the subject of an official IJCAI-97 Challenge [45]. It can be used to evaluate different MAS techniques in a direct manner: teams implemented with different techniques can play against each other.
Reference: [34] <author> Afsaneh Haddadi. </author> <title> Towards a pragmatic theory of interactions. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <pages> pages 133139, </pages> <address> Menlo Park, California, June 1995. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Bull et al. [11] * Contract nets for electronic commerce. Sandholm and Lesser [73] * Market-based systems. Huberman and Clearwater [41] * Generalized Partial Global Planning (GPGP). Decker [21] * Internal, Social, and Collective (role) commitments. Castelfranchi [12] * Commitment states (potential, pre, and actual) as planning states. Haddadi <ref> [34] </ref> * Belief/Desire/Intention (BDI) model: OASIS. Rao and Georgeff [63] * BDI commitments only over intentions. Rao and Georgeff [63] * Coalitions. Zlotkin and Rosenschein/Shehory and Kraus/Sandholm and Lesser [97, 78, 72] * Reasoning about truthfulness. <p> Setting an alarm clock is an example of internal commitment to wake up at a certain time. Haddadi discusses commitment states as planning states: potential cooperation, pre-commitment, and commitment <ref> [34] </ref>. Agents can then use means-ends analysis to plan for goals in terms of commitment opportunities. This work is conducted within a model called Belief/Desire/Intention, or BDI. BDI is a popular technique for modeling other agents.
Reference: [35] <author> Barbara Hayes-Roth, Lee Brownston, and Robert van Gent. </author> <title> Multiagent collaboration in directed improvisation. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <pages> pages 148154, </pages> <address> Menlo Park, California, June 1995. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: With respect to cost of failure, an example of a domain with high cost of failure is air-traffic control [63]. On the other hand, the directed improvisation domain considered by Hayes-Roth et al. has a very low cost of failure <ref> [35] </ref>. In this domain, entertainment agents accept all improvisation suggestions from each other. The idea is that the agents should not be afraid to make mistakes, but rather should just let the words flow [35]. Several multiagent systems include humans as one or more of the agents. <p> the directed improvisation domain considered by Hayes-Roth et al. has a very low cost of failure <ref> [35] </ref>. In this domain, entertainment agents accept all improvisation suggestions from each other. The idea is that the agents should not be afraid to make mistakes, but rather should just let the words flow [35]. Several multiagent systems include humans as one or more of the agents. In this case, the designer must consider the issue of communication between the human and computer agents [71].
Reference: [36] <author> Thomas Haynes, Kit Lau, and Sandip Sen. </author> <title> Learning cases to compliment rules for conflict resolution in multiagent systems. In Adaptation, Coevolution and Learning in Multiagent Systems: </title> <booktitle> Papers from the 1996 AAAI Spring Symposium, </booktitle> <pages> pages 5156, </pages> <address> Menlo Park,CA, </address> <month> March </month> <year> 1996. </year> <note> AAAI Press. AAAI Technical Report SS-96-01. 30 </note>
Reference-contexts: More pressingly, if Korf's claim that the pursuit domain is easily solved with local greedy heuristics were true, there would be no point in studying the pursuit domain any further. Fortunately, Haynes and Sen show that Korf's heuristics do not work for certain instantiations of the domain <ref> [36] </ref> (see Section 5). 4.2 General Homogeneous MAS The general multiagent scenario with homogeneous agents is illustrated in Figure 7. There are several different agents with identical structure (sensors, effectors, domain knowledge, and decision functions), but they have different sensor input and effector output. <p> In place of communicating planned actions to each other, the predators can evolve to know, or at least act as if knowing, each other's future actions. In a separate study, Haynes et al. use case-based reasoning to allow predators to learn to cooperate <ref> [36] </ref>. They begin with identical agents controlling each of the predators. The predators move simultaneously to their closest capture positions. But because predators that try to occupy the same position all remain stationary, cases of deadlock arise.
Reference: [37] <author> Thomas Haynes and Sandip Sen. </author> <title> Evolving behavioral strategies in predators and prey. </title> <editor> In Ger--hard Weiand Sandip Sen, editors, </editor> <booktitle> Adaptation and Learning in Multiagent Systems, </booktitle> <pages> pages 113126. </pages> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1996. </year>
Reference-contexts: Over time, the predators learn to stay out of each other's way while approaching the prey. Finally, Haynes and Sen explore the possibility of evolving both the predators and the prey so that they all try to improve their behaviors <ref> [37] </ref>. Working in a toroidal world and starting with predator behaviors such as Korf's greedy heuristic and their own evolved GP predators, they then evolve the prey to behave more effectively than randomly. <p> Although there may yet be greedy solutions that can deal with different types of prey behavior, they have not yet been discovered. Thus the predator domain retains value for researchers in MAS. Although Haynes and Sen convince the reader that the pursuit domain is still worth studying <ref> [37] </ref>, the co-evolutionary results are less than satisfying. As mentioned above, one would intuitively expect the predators to be able to adapt to the linearly moving prey. <p> Mor and Rosenschein/Sandholm and Crites [55, 75] * Minimax-Q. Littman [49] * Competitive co-evolution. Haynes and Sen/Grefenstette and Daley/Rosin and Belew <ref> [37, 32, 66] </ref> * Deduce intentions through observation. Huber and Durfee [40] * Autoepistemic reasoning (ignorance). Permpoontanalarp [59] * Model as a team (individual ! role). Tambe [85, 86] * Social reasoning: depend on others for goal (6= game theory). <p> Such systems that use competitive evolving agents are said to use a technique called competitive co-evolution. Systems that evolve benevolent agents are said to use cooperative co-evolution. The evolution of both predator and prey agents by Haynes and Sen <ref> [37] </ref> qualifies as competitive co-evolution. Grefenstette and Daley conduct a preliminary study of competitive and cooperative co-evolution in a domain that is loosely related to the pursuit domain [32]. Their domain has two robots that can move continuously and one morsel of (stationary) food that appears randomly in the world.
Reference: [38] <author> Thomas Haynes, Roger Wainwright, Sandip Sen, and Dale Schoenefeld. </author> <title> Strongly typed genetic programming in evolving cooperation strategies. </title> <editor> In Stephanie Forrest, editor, </editor> <booktitle> Proceedings of the Sixth International Conference on Genetic Algorithms, </booktitle> <pages> pages 271278, </pages> <address> San Mateo, CA, July 1995. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Haynes and colleagues have done various studies with heterogeneous agents in the pursuit domain. They have evolved teams of predators, equipped predators with case bases, and competitively evolved the predators and the prey. First, Haynes et al. use genetic programming (GP) to evolve teams of four predators <ref> [38] </ref>. Rather than evolving predator agents in a single evolutionary pool and then combining them into teams to test performance, each individual in the population is actually a team of four agents already specifically assigned to different predators. Thus the predators can evolve to cooperate.
Reference: [39] <author> O.E. Holland. </author> <title> Multiagent systems: Lessons from social insects and collective robotics. In Adaptation, Coevolution and Learning in Multiagent Systems: </title> <booktitle> Papers from the 1996 AAAI Spring Symposium, </booktitle> <pages> pages 5762, </pages> <address> Menlo Park,CA, </address> <month> March </month> <year> 1996. </year> <note> AAAI Press. AAAI Technical Report SS-96-01. </note>
Reference-contexts: Balch [7] * Local knowledge sometimes better. Roychowdhury [67] * (limited) Recursive Modeling Method (RMM). Durfee [24] * Don't model othersjust pay attention to reward. Schmidhuber [77] * Stigmergy. Holland/Goldman and Rosenschein <ref> [39, 31] </ref> * Q-learning for behaviors like foraging, homing, etc. Mataric [52] Learning opportunities * Enable others' actions * Sensor data ! Other agent's sensor data Table 5: The issues, techniques, and learning opportunities for homogeneous MAS as reflected in the literature. state. <p> Actively, they can be sensed by other agents, or they may be able to change the state of another agent by, for example, pushing it. More indirectly, agents can affect other agents by one of two types of stigmergy <ref> [39] </ref>. First, active stigmergy occurs when an agent alters the environment so as to affect the sensory input of another agent. For example, a robotic agent might leave a marker behind it for other agents to observe. <p> turns off the main water valve to a building, the effect of another agent turning on the kitchen faucet is altered. 12 Holland illustrates the concept of passive stigmergy with a robotic system designed to model the behavior of an ant colony confronted with many dead ants around its nest <ref> [39] </ref>. An ant from such a colony tends to periodically pick up a dead ant, carry it for a short distance, and then drop it. Although the behavior appears to be random, after several hours, the dead ants are clustered in a small number of heaps. <p> Although the ants behave homogeneously and, at least in this case, we have no evidence that they communicate explicitly, the ants manage to cooperate in achieving a task. Holland models this situation with a number of identical robots in a small area scattered with pucks <ref> [39] </ref>. The robots are programmed reactively to move straight (turning at walls) until they are pushing three or more pucks. At that point, the robots back up and turn away, leaving the three pucks in a cluster.
Reference: [40] <author> Marcus J. Huber and Edmund H. Durfee. </author> <title> Deciding when to commit to action during observation-based coordination. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <pages> pages 163170, </pages> <address> Menlo Park, California, June 1995. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Mor and Rosenschein/Sandholm and Crites [55, 75] * Minimax-Q. Littman [49] * Competitive co-evolution. Haynes and Sen/Grefenstette and Daley/Rosin and Belew [37, 32, 66] * Deduce intentions through observation. Huber and Durfee <ref> [40] </ref> * Autoepistemic reasoning (ignorance). Permpoontanalarp [59] * Model as a team (individual ! role). Tambe [85, 86] * Social reasoning: depend on others for goal (6= game theory). Sichman and Demazeau [79] * GAs to deal with Braes' paradox (more resource ! worse). <p> Without communication, agents are forced to model each other strictly through observation. Huber and Durfee consider a case of coordinated motion control among multiple mobile robots under the assumption that communication is prohibitively expensive <ref> [40] </ref>. Thus the agents try to deduce each other's plans by observing their actions. In particular, each robot (simulated or real) tries to figure out the destinations of the other robots by watching how they move.
Reference: [41] <author> Bernardo Huberman and Scott H. Clearwater. </author> <title> A multi-agent system for controlling building environments. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <pages> pages 171176, </pages> <address> Menlo Park, California, June 1995. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Weiss [93] * Training other agents' Q-functions (track driving). Clouse [13] * Minimize the need for training. Potter and Grefenstette [61] * Cooperative co-evolution. Bull et al. [11] * Contract nets for electronic commerce. Sandholm and Lesser [73] * Market-based systems. Huberman and Clearwater <ref> [41] </ref> * Generalized Partial Global Planning (GPGP). Decker [21] * Internal, Social, and Collective (role) commitments. Castelfranchi [12] * Commitment states (potential, pre, and actual) as planning states. Haddadi [34] * Belief/Desire/Intention (BDI) model: OASIS. Rao and Georgeff [63] * BDI commitments only over intentions. <p> Agents must pay to contract their tasks out and thus shop around for the lowest bidder. Sandholm and Lesser discuss some of the issues that arise in contract nets [73]. In a similar spirit is an implemented multiagent system that controls air temperature in different rooms of a building <ref> [41] </ref>. A person can set one's thermostat to any temperature. Then depending on the actual air temperature, the agent for that room tries to buy either hot or cold air from another room that has an excess.
Reference: [42] <author> Leslie Pack Kaelbling, Michael L. Littman, and Andrew W. Moore. </author> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4:237285, </volume> <month> May </month> <year> 1996. </year>
Reference-contexts: Typical RL situations with delayed reward encourage agents to learn to achieve their goals directly by propagating local reinforcement back to past states and actions <ref> [42] </ref>. However if an action leads to a reward by another agent, the acting agent may have no way of reinforcing that action. Techniques to deal with such a problem would be useful for building multiagent systems.
Reference: [43] <editor> Jong-Hwan Kim, editor. </editor> <booktitle> Proceedings of the Micro-Robot World Cup Soccer Tournament, </booktitle> <address> Taejon, Korea, </address> <month> November </month> <year> 1996. </year>
Reference-contexts: In this section a single domain which embodies most multiagent issues is presented. Robotic soccer is a particularly good domain for studying MAS. Originated by Alan Mackworth [70], it has been gaining popularity in recent years, with several international competitions taking place <ref> [43, 44, 33] </ref>. It is also the subject of an official IJCAI-97 Challenge [45]. It can be used to evaluate different MAS techniques in a direct manner: teams implemented with different techniques can play against each other.
Reference: [44] <author> H. Kitano, Y. Kuniyoshi, I. Noda, M. Asada, H. Matsubara, and E. Osawa. </author> <title> Robocup: A challenge problem for AI. </title> <journal> AI Magazine, </journal> <volume> 18(1):7385, </volume> <month> Spring </month> <year> 1997. </year>
Reference-contexts: In this section a single domain which embodies most multiagent issues is presented. Robotic soccer is a particularly good domain for studying MAS. Originated by Alan Mackworth [70], it has been gaining popularity in recent years, with several international competitions taking place <ref> [43, 44, 33] </ref>. It is also the subject of an official IJCAI-97 Challenge [45]. It can be used to evaluate different MAS techniques in a direct manner: teams implemented with different techniques can play against each other. <p> The simulator provides a domain and supports users who wish to build their own agents. Furthermore, teams of agents can be evaluated by playing against each other, or perhaps against standard teams. The simulator was successfully used for a competition among twenty-nine teams from around the world in 1997 <ref> [44] </ref>. Thus robotic soccer satisfies Decker's criteria for DAI testbeds [18]. 25 7.2 MAS in Robotic Soccer The main goal of any testbed is to facilitate the trial and evaluation of ideas that have promise in the real world.
Reference: [45] <author> Hiroaki Kitano, Milind Tambe, Peter Stone, Manuela Veloso, Silvia Coradeschi, Eiichi Osawa, Hitoshi Matsubara, Itsuki Noda, and Minoru Asada. </author> <title> The robocup synthetic agent challenge 97. </title> <booktitle> In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, </booktitle> <address> San Francisco, CA, 1997. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Robotic soccer is a particularly good domain for studying MAS. Originated by Alan Mackworth [70], it has been gaining popularity in recent years, with several international competitions taking place [43, 44, 33]. It is also the subject of an official IJCAI-97 Challenge <ref> [45] </ref>. It can be used to evaluate different MAS techniques in a direct manner: teams implemented with different techniques can play against each other. Although the pursuit domain serves us well for purposes of illustration, robotic soccer is much more complex and interesting as a general testbed for MAS.
Reference: [46] <author> Richard E. Korf. </author> <title> A simple solution to pursuit games. </title> <booktitle> In Working Papers of the 11th International Workshop on Distributed Artificial Intelligence, </booktitle> <pages> pages 183194, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: A requirement for their model is that each predator has full information about the location of other predators. Their game model mixes game-theoretical cooperative and non-cooperative games. Korf also takes the approach that each agent should try to greedily maximize its own local utility <ref> [46] </ref>. He introduces a policy for each predator based on an attractive force to the prey and a repulsive force from the other predators. Thus the predators tend to approach the prey from different sides. <p> Richard Korf <ref> [46] </ref> However, whether or not altruism occurs in nature, there is certainly some use for benevolent agents in MAS, as shown below. <p> As we have already seen in the pursuit domain, Korf advocates using greedy agents that minimize their own distance to the prey <ref> [46] </ref>, and similarly, Levy and Rosenschein use Game Theory to study how the predators can cooperate despite maximizing their own utilities [48]. Some advocates of selfish agents point to nature for their justification, claiming that animals are not altruistic, but rather act always in their own self-interest [46]. <p> to the prey <ref> [46] </ref>, and similarly, Levy and Rosenschein use Game Theory to study how the predators can cooperate despite maximizing their own utilities [48]. Some advocates of selfish agents point to nature for their justification, claiming that animals are not altruistic, but rather act always in their own self-interest [46]. On 15 Heterogeneous Non-Communicating Issues * Benevolence vs. competitiveness * Stable vs. evolving agents (arms race, credit/blame) * Modeling of others' goals, actions, and knowledge * Resource management (interdependent actions) * Social conventions * Roles Techniques * Game theory, iterative play.
Reference: [47] <author> Victor R. Lesser. </author> <title> Multiagent systems: An emerging subdiscipline of AI. </title> <journal> ACM Computing Surveys, </journal> <volume> 27(3):340342, </volume> <month> September </month> <year> 1995. </year>
Reference-contexts: From this perspective, the important characteristics of MAS are: * System function; * Agent architecture (degree of heterogeneity, reactive vs. deliberative); * System architecture (communication, protocols, human involvement). A useful contribution is that the dimensions are divided into agent and system characteristics. Other overviews of DAI and/or MAS include <ref> [47, 23, 25, 10] </ref>. This article contributes a taxonomy specifically focussed on MAS along with a detailed chronicle of existing systems as they fit in to this taxonomy.
Reference: [48] <author> Ran Levy and Jeffrey S. Rosenschein. </author> <title> A game theoretic approach to the pursuit problem. </title> <booktitle> In Working Papers of the 11th International Workshop on Distributed Artificial Intelligence, </booktitle> <pages> pages 195213, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Vidal and Durfee's Limited Rationality RMM algorithm is designed to take such considerations into account [90]. Levy and Rosenschein use a game theoretical approach to the pursuit domain <ref> [48] </ref>. They use a payoff function that allows selfish agents to cooperate. A requirement for their model is that each predator has full information about the location of other predators. Their game model mixes game-theoretical cooperative and non-cooperative games. <p> The actual robot motion is a simple weighted sum of these vectors. At the other extreme is the pursuit domain work by Levy and Rosenschein that is mentioned above <ref> [48] </ref>. Their agents assume that each will act in service of its own goals. They use game theoretic techniques to find equilibrium points and thus to decide how to act [48]. These agents are clearly deliberative, as they search for actions rather than simply retrieving them. <p> At the other extreme is the pursuit domain work by Levy and Rosenschein that is mentioned above <ref> [48] </ref>. Their agents assume that each will act in service of its own goals. They use game theoretic techniques to find equilibrium points and thus to decide how to act [48]. These agents are clearly deliberative, as they search for actions rather than simply retrieving them. There are also several existing systems and techniques that mix reactive and deliberative behaviors. <p> As we have already seen in the pursuit domain, Korf advocates using greedy agents that minimize their own distance to the prey [46], and similarly, Levy and Rosenschein use Game Theory to study how the predators can cooperate despite maximizing their own utilities <ref> [48] </ref>. Some advocates of selfish agents point to nature for their justification, claiming that animals are not altruistic, but rather act always in their own self-interest [46].
Reference: [49] <author> Michael L. Littman. </author> <title> Markov games as a framework for multi-agent reinforcement learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 157163, </pages> <address> San Mateo, CA, 1994. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Mor and Rosenschein/Sandholm and Crites [55, 75] * Minimax-Q. Littman <ref> [49] </ref> * Competitive co-evolution. Haynes and Sen/Grefenstette and Daley/Rosin and Belew [37, 32, 66] * Deduce intentions through observation. Huber and Durfee [40] * Autoepistemic reasoning (ignorance). Permpoontanalarp [59] * Model as a team (individual ! role). <p> The players can block each other by trying to move to the same space. Littman introduces a variant of Q-learning called Minimax-Q which is designed to work on Markov games as opposed to Markov Decision Processes <ref> [49] </ref>. The competitive agents learn probabilistic policies since any deterministic policy can be completely counteracted by the opponent. The issue of benevolence (willingness to cooperate) vs. competitiveness comes up repeatedly in the systems described below.
Reference: [50] <author> Sean Luke, Charles Hohn, Jonathan Farris, Gary Jackson, and James Hendler. </author> <title> Co-evolving soccer softbot team coordination with genetic programming. </title> <booktitle> In Proceedings of the First International Workshop on RoboCup, </booktitle> <pages> pages 115118, </pages> <address> Nagoya,Japan, </address> <month> August </month> <year> 1997. </year>
Reference-contexts: Luke et al. use genetic programming to evolve cooperative behaviors within a team of players <ref> [50] </ref>. 8 Conclusion This survey is presented as a description of the field of MAS. It is designed to serve both as an introduction for people unfamiliar with the field and as an organizational framework for system designers.
Reference: [51] <author> Andreas Lux and Donald Steiner. </author> <title> Understanding cooperation: an agent's perspective. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <pages> pages 261268, </pages> <address> Menlo Park, California, June 1995. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Cohen and Levesque/Lux and Steiner <ref> [14, 51] </ref> * Learning social behaviors. Mataric [53] * Bayesian learning in negotiation: model others. Zeng and Sycara [96] * Multiagent Q-learning. Weiss [93] * Training other agents' Q-functions (track driving). Clouse [13] * Minimize the need for training. Potter and Grefenstette [61] * Cooperative co-evolution. <p> When combined with a model of other agents, the effect of a communication act might be to alter an agent's belief about the state of another agent or agents. The theory of communication as action is called speech acts <ref> [14, 51] </ref>. Mataric adds a learning dimension to the idea of speech acts. Starting with the foraging behavior mentioned above [52], the agents can then learn to choose among a set of social behaviors that include broadcasting and listening [53].
Reference: [52] <editor> Maja J. Mataric. </editor> <title> Interaction and intelligent behavior. </title> <type> MIT EECS PhD Thesis AITR-1495, </type> <institution> MIT AI Lab, </institution> <month> August </month> <year> 1994. </year> <month> 31 </month>
Reference-contexts: Balch [7] * Local knowledge sometimes better. Roychowdhury [67] * (limited) Recursive Modeling Method (RMM). Durfee [24] * Don't model othersjust pay attention to reward. Schmidhuber [77] * Stigmergy. Holland/Goldman and Rosenschein [39, 31] * Q-learning for behaviors like foraging, homing, etc. Mataric <ref> [52] </ref> Learning opportunities * Enable others' actions * Sensor data ! Other agent's sensor data Table 5: The issues, techniques, and learning opportunities for homogeneous MAS as reflected in the literature. state. <p> Like the ants, the robots use passive stigmergy to affect each other's behavior. A similar scenario with more deliberative robots is explored by Mataric. In this case, the robots use Q-learning to learn behaviors including foraging for pucks as well as homing and following <ref> [52] </ref>. The robots learn independent policies, dealing with the high-dimensional state space with the aid of progress estimators that give intermediate rewards, and with the aid of boolean value predicates that condense many states into one. <p> The theory of communication as action is called speech acts [14, 51]. Mataric adds a learning dimension to the idea of speech acts. Starting with the foraging behavior mentioned above <ref> [52] </ref>, the agents can then learn to choose among a set of social behaviors that include broadcasting and listening [53]. Q-learning is extended so that reinforcement can be received for direct rewards or for rewards to other agents.
Reference: [53] <editor> Maja J. Mataric. </editor> <title> Learning to behave socially. </title> <booktitle> In Third International Conference on Simulation of Adaptive Behavior, </booktitle> <year> 1994. </year>
Reference-contexts: Cohen and Levesque/Lux and Steiner [14, 51] * Learning social behaviors. Mataric <ref> [53] </ref> * Bayesian learning in negotiation: model others. Zeng and Sycara [96] * Multiagent Q-learning. Weiss [93] * Training other agents' Q-functions (track driving). Clouse [13] * Minimize the need for training. Potter and Grefenstette [61] * Cooperative co-evolution. Bull et al. [11] * Contract nets for electronic commerce. <p> The theory of communication as action is called speech acts [14, 51]. Mataric adds a learning dimension to the idea of speech acts. Starting with the foraging behavior mentioned above [52], the agents can then learn to choose among a set of social behaviors that include broadcasting and listening <ref> [53] </ref>. Q-learning is extended so that reinforcement can be received for direct rewards or for rewards to other agents. When using communication as a planning action, the possibility arises of communicating misinformation in order to satisfy a particular goal.
Reference: [54] <author> Hitoshi Matsubara, Itsuki Noda, and Kazuo Hiraki. </author> <title> Learning of cooperative actions in multi-agent systems: a case study of pass play in soccer. In Adaptation, Coevolution and Learning in Multiagent Systems: </title> <booktitle> Papers from the 1996 AAAI Spring Symposium, </booktitle> <pages> pages 6367, </pages> <address> Menlo Park,CA, </address> <month> March </month> <year> 1996. </year> <note> AAAI Press. AAAI Technical Report SS-96-01. </note>
Reference-contexts: They use similar techniques in the soccerserver system as well, extending the learned behavior as a part of a hierarchical learning system [84]. Matsubara et al. also use a Neural Network to allow a player to learn when to shoot and when to pass in the soccerserver system <ref> [54] </ref>. Uchibe et al. have successfully combined RL modules for shooting and for avoiding opponents using real robots [88]. Once low-level behaviors have been developed, the opportunity to use ML techniques at the strategy level is particularly exciting.
Reference: [55] <author> Yishay Mor and Jeffrey Rosenschein. </author> <title> Time and the prisoner's dilemma. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <pages> pages 276282, </pages> <address> Menlo Park, California, June 1995. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: On 15 Heterogeneous Non-Communicating Issues * Benevolence vs. competitiveness * Stable vs. evolving agents (arms race, credit/blame) * Modeling of others' goals, actions, and knowledge * Resource management (interdependent actions) * Social conventions * Roles Techniques * Game theory, iterative play. Mor and Rosenschein/Sandholm and Crites <ref> [55, 75] </ref> * Minimax-Q. Littman [49] * Competitive co-evolution. Haynes and Sen/Grefenstette and Daley/Rosin and Belew [37, 32, 66] * Deduce intentions through observation. Huber and Durfee [40] * Autoepistemic reasoning (ignorance). Permpoontanalarp [59] * Model as a team (individual ! role). <p> Whether or not altruism exists, in some situations it may be in an animal's (or agent's) interest to cooperate with other agents. Mor and Rosenschein illustrate this possibility in the context of the prisoner's dilemma <ref> [55] </ref>. In the prisoner's dilemma, two agents try to act so as to maximize their own individual rewards. They are not actively out to thwart each other since it is not a zero-sum game, yet they place no inherent value on the other receiving reward.
Reference: [56] <author> Itsuki Noda. </author> <title> Soccer server : a simulator of robocup. </title> <booktitle> In Proceedings of AI symposium '95, pages 2934. Japanese Society for Artificial Intelligence, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: Some robotic issues can only be studied in the real-world instantiation, but there are also many issues that can be studied in simulation. A particularly good simulator for this purpose is the soccerserver developed by Noda <ref> [56] </ref> and pictured in Figure 12.
Reference: [57] <author> Ei-Ichi Osawa. </author> <title> A metalevel coordination strategy for reactive cooperative planning. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <pages> pages 297303, </pages> <address> Menlo Park, California, June 1995. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Another way to frame this tradeoff is as one between cost and freedom: as communication cost (time) increases, freedom decreases. Osawa suggests that the predators should move through four phases. In increasing order of cost (decreasing freedom), they are: autonomy, communication, negotiation, and control <ref> [57] </ref>. When the predators stop making sufficient progress toward the prey using one strategy, they should move to the next most expensive strategy. Thus they can close in on the prey efficiently and effectively.
Reference: [58] <author> H. Van Dyke Parunak. </author> <booktitle> Applications of distributed artificial intelligence in industry. </booktitle> <editor> In G. M. P. O'Hare and N. R. Jennings, editors, </editor> <booktitle> Foundations of Distributed Artificial Intelligence, </booktitle> <pages> pages 139164. </pages> <publisher> Wiley Interscience, </publisher> <year> 1996. </year>
Reference-contexts: In fact, the remaining dimensions are very prominent in this article: degree of heterogeneity is a major MAS dimension and all the methods of distributing control appear here as major issues. More recently, Parunak has presented a taxonomy of MAS from an application perspective <ref> [58] </ref>. From this perspective, the important characteristics of MAS are: * System function; * Agent architecture (degree of heterogeneity, reactive vs. deliberative); * System architecture (communication, protocols, human involvement). A useful contribution is that the dimensions are divided into agent and system characteristics.
Reference: [59] <author> Yongyuth Permpoontanalarp. </author> <title> Generalised proof-theory for multi-agent autoepistemic reasoning. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <pages> pages 304 311, </pages> <address> Menlo Park, California, June 1995. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Mor and Rosenschein/Sandholm and Crites [55, 75] * Minimax-Q. Littman [49] * Competitive co-evolution. Haynes and Sen/Grefenstette and Daley/Rosin and Belew [37, 32, 66] * Deduce intentions through observation. Huber and Durfee [40] * Autoepistemic reasoning (ignorance). Permpoontanalarp <ref> [59] </ref> * Model as a team (individual ! role). Tambe [85, 86] * Social reasoning: depend on others for goal (6= game theory). Sichman and Demazeau [79] * GAs to deal with Braes' paradox (more resource ! worse). Arora and Sen [4] * Multiagent RL for adaptive Load Balancing. <p> When modeling other agents, it may be useful to reason not only about what is true and what is false, but also about what is not known. Such reasoning about ignorances is called autoepistemic reasoning. For a theoretical presentation of an autoepistemic reasoning method in MAS, see <ref> [59] </ref>. Just as RMM is useful for modeling the states of homogeneous agents, it can be used in the heterogeneous scenario as well. Tambe takes it one step further, studying how agents can learn models of teams of agents.
Reference: [60] <author> Dean A. Pormerleau. </author> <title> Neural Network Perception for Mobile Robot Guidance. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1993. </year>
Reference-contexts: In the past several years, AI techniques have become more and more robust and complex. To mention just one of the many exciting successes, a car recently steered itself more than 95% of the way across the United States using the ALVINN system <ref> [60] </ref>. By meeting this and other such daunting challenges, AI researchers have earned the right to start examining the implications of multiple autonomous agents interacting in the real world. In fact, they have rendered this examination indispensable. If there is one self-steering car, there will surely be more.
Reference: [61] <author> Mitchell A. Potter, Kenneth A. De Jong, and John J. Grefenstette. </author> <title> A coevolutionary approach to learning sequential decision rules. </title> <editor> In Stephanie Forrest, editor, </editor> <booktitle> Proceedings of the Sixth International Conference on Genetic Algorithms, </booktitle> <pages> pages 366372, </pages> <address> San Mateo,CA, July 1995. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Cohen and Levesque/Lux and Steiner [14, 51] * Learning social behaviors. Mataric [53] * Bayesian learning in negotiation: model others. Zeng and Sycara [96] * Multiagent Q-learning. Weiss [93] * Training other agents' Q-functions (track driving). Clouse [13] * Minimize the need for training. Potter and Grefenstette <ref> [61] </ref> * Cooperative co-evolution. Bull et al. [11] * Contract nets for electronic commerce. Sandholm and Lesser [73] * Market-based systems. Huberman and Clearwater [41] * Generalized Partial Global Planning (GPGP). Decker [21] * Internal, Social, and Collective (role) commitments. <p> Potter and Grefenstette illustrate this effect in the domain described above in which two robots compete for a stationary pellet of food <ref> [61] </ref>. Subpopulations of rules are seeded to be more effective in different situations. Thus specialized subpopulations of rules corresponding to shaped behaviors tend to emerge. Rather than competitive co-evolution Bull et al. build a system system which uses cooperative coevolution [11].
Reference: [62] <author> M V Nagendra Prasad, Victor R. Lesser, and Susan E. Lander. </author> <title> Learning organizational roles in a heterogeneous multi-agent system. In Adaptation, Coevolution and Learning in Multiagent Systems: </title> <booktitle> Papers from the 1996 AAAI Spring Symposium, </booktitle> <pages> pages 7277, </pages> <address> Menlo Park,CA, </address> <month> March </month> <year> 1996. </year> <note> AAAI Press. AAAI Technical Report SS-96-01. </note>
Reference-contexts: Arora and Sen [4] * Multiagent RL for adaptive Load Balancing. Schaerf, Shoham, and Tennenholtz [76] * Focal points/Emergent conventions. Fenster et al./Walker and Woolridge [26, 91] * Design agents play different roles. Prasad et al. <ref> [62] </ref> Learning opportunities * Credit/blame in competitive scenarios * Behaviors that blend well with team * Prediction of others' actions * Dynamic role assumption Table 6: The issues, techniques, and learning opportunities for heterogeneous MAS as reflected in the literature. the other hand, Ridley provides a detailed chronicle and explanation of <p> However in some domains, the agents are flexible enough to interchange roles. The multiagent design of a steam pump is one such domain. Prasad et al. study design agents that can either initiate a design or extend a design <ref> [62] </ref>. In different situations, different agents are more effective at initiation and at extension. Thus a supervised learning technique is used to help agents learn what roles they should fill in different situations.
Reference: [63] <author> Anand S. Rao and Michael P. Georgeff. </author> <title> Bdi agents: From theory to practice. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <pages> pages 312319, </pages> <address> Menlo Park, California, June 1995. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: The first several of these characteristics are self-explanatory and do not need further mention. With respect to cost of failure, an example of a domain with high cost of failure is air-traffic control <ref> [63] </ref>. On the other hand, the directed improvisation domain considered by Hayes-Roth et al. has a very low cost of failure [35]. In this domain, entertainment agents accept all improvisation suggestions from each other. <p> There are also several existing systems and techniques that mix reactive and deliberative behaviors. One example is Rao and Georgeff's OASIS system (see Section 6) which reasons about when to be reactive and 11 when to follow goal-directed plans <ref> [63] </ref>. Another example is Sahota's reactive deliberation technique [69]. As the name implies it mixes reactive and deliberative behavior: an agent reasons about which reactive behavior to follow under the constraint that it must choose actions at a rate of 60 Hz. <p> Sandholm and Lesser [73] * Market-based systems. Huberman and Clearwater [41] * Generalized Partial Global Planning (GPGP). Decker [21] * Internal, Social, and Collective (role) commitments. Castelfranchi [12] * Commitment states (potential, pre, and actual) as planning states. Haddadi [34] * Belief/Desire/Intention (BDI) model: OASIS. Rao and Georgeff <ref> [63] </ref> * BDI commitments only over intentions. Rao and Georgeff [63] * Coalitions. Zlotkin and Rosenschein/Shehory and Kraus/Sandholm and Lesser [97, 78, 72] * Reasoning about truthfulness. <p> Decker [21] * Internal, Social, and Collective (role) commitments. Castelfranchi [12] * Commitment states (potential, pre, and actual) as planning states. Haddadi [34] * Belief/Desire/Intention (BDI) model: OASIS. Rao and Georgeff <ref> [63] </ref> * BDI commitments only over intentions. Rao and Georgeff [63] * Coalitions. Zlotkin and Rosenschein/Shehory and Kraus/Sandholm and Lesser [97, 78, 72] * Reasoning about truthfulness. <p> Rao and Georgeff use the BDI model to build a system for air-traffic control, OASIS, which has been implemented for testing (in parallel with human operators who retain full control) at the airport in Sydney, Australia <ref> [63] </ref>. Each aircraft is represented by a controlling agent which deals with a global sequencing agent. OASIS mixes reactive and deliberative actions in the agents: they can break out of planned sequences when coming across situations that demand immediate reaction.
Reference: [64] <author> Matt Ridley. </author> <title> The Origins of Virtue : Human Instincts and the Evolution of Cooperation. </title> <publisher> Viking Press, </publisher> <month> April </month> <year> 1997. </year>
Reference-contexts: * Prediction of others' actions * Dynamic role assumption Table 6: The issues, techniques, and learning opportunities for heterogeneous MAS as reflected in the literature. the other hand, Ridley provides a detailed chronicle and explanation of apparent altruism in nature (usually explainable as kin selection) and cooperation in human societies <ref> [64] </ref>. Whether or not altruism exists, in some situations it may be in an animal's (or agent's) interest to cooperate with other agents. Mor and Rosenschein illustrate this possibility in the context of the prisoner's dilemma [55].
Reference: [65] <author> Jeffrey S Rosenschein and Gilad Zlotkin. </author> <title> Rules of Encounter. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Haddadi [34] * Belief/Desire/Intention (BDI) model: OASIS. Rao and Georgeff [63] * BDI commitments only over intentions. Rao and Georgeff [63] * Coalitions. Zlotkin and Rosenschein/Shehory and Kraus/Sandholm and Lesser [97, 78, 72] * Reasoning about truthfulness. Sandholm and Lesser/ Rosenschein <ref> [74, 65] </ref> Learning opportunities * Evolving language * Effects of speech acts on global dynamics * Communication utility and truthfulness * Commitment utility Table 7: The issues, techniques, and learning opportunities for communicating multiagent systems as reflected in the literature. 6.3.1 Understanding each other In all communicating multiagent systems, and particularly
Reference: [66] <author> Christopher D. Rosin and Richard K. Belew. </author> <title> Methods for competitive co-evolution: Finding opponents worth beating. </title> <editor> In Stephanie Forrest, editor, </editor> <booktitle> Proceedings of the Sixth International Conference on Genetic Algorithms, </booktitle> <pages> pages 373380, </pages> <address> San Mateo,CA, July 1995. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Mor and Rosenschein/Sandholm and Crites [55, 75] * Minimax-Q. Littman [49] * Competitive co-evolution. Haynes and Sen/Grefenstette and Daley/Rosin and Belew <ref> [37, 32, 66] </ref> * Deduce intentions through observation. Huber and Durfee [40] * Autoepistemic reasoning (ignorance). Permpoontanalarp [59] * Model as a team (individual ! role). Tambe [85, 86] * Social reasoning: depend on others for goal (6= game theory). <p> Of course this method encourages the arms race more than ever. Nevertheless, Rosin and Belew use this technique, along with an interesting method for maintaining diversity in genetic populations, to evolve agents that can play TicTacToe, Nim, and a simple version of Go <ref> [66] </ref>. When it is a given agent's turn to evolve, it executes a standard GA generation. Individuals are tested against individuals from the competing population, but a technique called competitive fitness sharing is used to maintain diversity.
Reference: [67] <author> Shounak Roychowdhury, Neeraj Arora, and Sandip Sen. </author> <title> Effects of local information on group behavior. In Adaptation, Coevolution and Learning in Multiagent Systems: </title> <booktitle> Papers from the 1996 AAAI Spring Symposium, </booktitle> <pages> pages 7883, </pages> <address> Menlo Park,CA, </address> <month> March </month> <year> 1996. </year> <note> AAAI Press. AAAI Technical Report SS-96-01. </note>
Reference-contexts: Homogeneous Non-Communicating Issues * Reactive vs. deliberative agents * Local or global perspective * Modeling of other agents' states * How to affect others Techniques * Reactive Behaviors for Formation maintenance. Balch [7] * Local knowledge sometimes better. Roychowdhury <ref> [67] </ref> * (limited) Recursive Modeling Method (RMM). Durfee [24] * Don't model othersjust pay attention to reward. Schmidhuber [77] * Stigmergy. Holland/Goldman and Rosenschein [39, 31] * Q-learning for behaviors like foraging, homing, etc. <p> Roychowdhury et al. consider a case of multiple agents sharing a set of identical resources in which they have to learn (adapt) their resource usage policies <ref> [67] </ref>. Since the agents are identical and do not communicate, if they all have a global view of the current resource usage, they will all move simultaneously to the most under-used resource.
Reference: [68] <author> Stuart J. Russell and Peter Norvig. </author> <title> Artificial Intelligence: A Modern Approach. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1995. </year> <month> 32 </month>
Reference-contexts: Multiagent Systems (MAS) is the emerging subfield of AI that aims to provide both principles for construction of complex systems involving multiple agents and mechanisms for coordination of independent agents' behaviors. While there is no generally accepted definition of agent in AI <ref> [68] </ref>, for the purposes of this article, we consider an agent to be an entity with goals, actions, and domain knowledge, situated in an environment.
Reference: [69] <author> Michael K. Sahota. </author> <title> Reactive deliberation: An architecture for real-time intelligent control in dynamic environments. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 13031308, </pages> <year> 1994. </year>
Reference-contexts: There are also several existing systems and techniques that mix reactive and deliberative behaviors. One example is Rao and Georgeff's OASIS system (see Section 6) which reasons about when to be reactive and 11 when to follow goal-directed plans [63]. Another example is Sahota's reactive deliberation technique <ref> [69] </ref>. As the name implies it mixes reactive and deliberative behavior: an agent reasons about which reactive behavior to follow under the constraint that it must choose actions at a rate of 60 Hz.
Reference: [70] <author> Michael K. Sahota, Alan K. Mackworth, Rod A. Barman, and Stewart J. Kingdon. </author> <title> Real-time control of soccer-playing robots using off-board vision: the dynamite testbed. </title> <booktitle> In IEEE International Conference on Systems, Man, and Cybernetics, </booktitle> <pages> pages 36903663, </pages> <year> 1995. </year>
Reference-contexts: In this section a single domain which embodies most multiagent issues is presented. Robotic soccer is a particularly good domain for studying MAS. Originated by Alan Mackworth <ref> [70] </ref>, it has been gaining popularity in recent years, with several international competitions taking place [43, 44, 33]. It is also the subject of an official IJCAI-97 Challenge [45]. <p> Although more costly and time consuming to develop, a number of groups have developed real robotic systems. The first robotic soccer system was the Dynamo system <ref> [70] </ref>. Sahota et al. built a 1 vs. 1 version of the game. Asada et al. have used vision-based RL with their soccer playing robots [5]. Veloso et al. discuss some of the robotic issues involved in building robotic soccer players [3, 89].
Reference: [71] <author> J. Alfredo Sanchez, Flavio S. Azevedo, and John J. Leggett. Paragente: </author> <title> Exploring the issues in agent-based user interfaces. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <pages> pages 320327, </pages> <address> Menlo Park, California, June 1995. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Several multiagent systems include humans as one or more of the agents. In this case, the designer must consider the issue of communication between the human and computer agents <ref> [71] </ref>. Another example of user involvement is user feedback in an information filtering domain [27]. 8 Decker distinguishes three different sources of uncertainty in a domain [17].
Reference: [72] <author> Tuomas Sandholm and Victor Lesser. </author> <title> Coalition formation among bounded rational agents. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 662669, </pages> <address> Los Angeles, CA, 1995. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Castelfranchi [12] * Commitment states (potential, pre, and actual) as planning states. Haddadi [34] * Belief/Desire/Intention (BDI) model: OASIS. Rao and Georgeff [63] * BDI commitments only over intentions. Rao and Georgeff [63] * Coalitions. Zlotkin and Rosenschein/Shehory and Kraus/Sandholm and Lesser <ref> [97, 78, 72] </ref> * Reasoning about truthfulness. <p> Sandholm and Lesser use a vehicle routing domain to illustrate a method by which agents can form valuable coalitions when it is intractable to discover the optimal coalitions <ref> [72] </ref>. 24 6.4 Further Learning Opportunities Once again, there are many possible ways in the current scenario to enhance MAS with ML techniques. Within this heterogeneous communicating multiagent scenario there is a clear need to pre-define a language and communication protocol for use by the agents.
Reference: [73] <author> Tuomas Sandholm and Victor Lesser. </author> <title> Issues in automated negotiation and electronic commerce: Extending the contract net framework. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <pages> pages 328335, </pages> <address> Menlo Park, California, June 1995. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Zeng and Sycara [96] * Multiagent Q-learning. Weiss [93] * Training other agents' Q-functions (track driving). Clouse [13] * Minimize the need for training. Potter and Grefenstette [61] * Cooperative co-evolution. Bull et al. [11] * Contract nets for electronic commerce. Sandholm and Lesser <ref> [73] </ref> * Market-based systems. Huberman and Clearwater [41] * Generalized Partial Global Planning (GPGP). Decker [21] * Internal, Social, and Collective (role) commitments. Castelfranchi [12] * Commitment states (potential, pre, and actual) as planning states. Haddadi [34] * Belief/Desire/Intention (BDI) model: OASIS. <p> Agents must pay to contract their tasks out and thus shop around for the lowest bidder. Sandholm and Lesser discuss some of the issues that arise in contract nets <ref> [73] </ref>. In a similar spirit is an implemented multiagent system that controls air temperature in different rooms of a building [41]. A person can set one's thermostat to any temperature.
Reference: [74] <author> Tuomas Sandholm and Victor Lesser. </author> <title> Advantages of a leveled commitment contracting protocol. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 126133, </pages> <address> Menlo Park,California, 1996. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Haddadi [34] * Belief/Desire/Intention (BDI) model: OASIS. Rao and Georgeff [63] * BDI commitments only over intentions. Rao and Georgeff [63] * Coalitions. Zlotkin and Rosenschein/Shehory and Kraus/Sandholm and Lesser [97, 78, 72] * Reasoning about truthfulness. Sandholm and Lesser/ Rosenschein <ref> [74, 65] </ref> Learning opportunities * Evolving language * Effects of speech acts on global dynamics * Communication utility and truthfulness * Commitment utility Table 7: The issues, techniques, and learning opportunities for communicating multiagent systems as reflected in the literature. 6.3.1 Understanding each other In all communicating multiagent systems, and particularly <p> Rather than actually making it true, the agent might just say that it is true. For example, Sandholm and Lesser analyze a framework in which agents are allowed to decommit from agreements with other agents by paying a penalty to these other agents <ref> [74] </ref>. They consider the case in which an agent might not be truthful in its decommitment, hoping that the other agent will decommit first.
Reference: [75] <author> Tuomas W. Sandholm and Robert H. Crites. </author> <title> On multiagent q-learning in a semi-competitive domain. </title> <editor> In Gerhard Weiand Sandip Sen, editors, </editor> <title> Adaptation and Learning in Multiagent Systems. </title> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1996. </year>
Reference-contexts: On 15 Heterogeneous Non-Communicating Issues * Benevolence vs. competitiveness * Stable vs. evolving agents (arms race, credit/blame) * Modeling of others' goals, actions, and knowledge * Resource management (interdependent actions) * Social conventions * Roles Techniques * Game theory, iterative play. Mor and Rosenschein/Sandholm and Crites <ref> [55, 75] </ref> * Minimax-Q. Littman [49] * Competitive co-evolution. Haynes and Sen/Grefenstette and Daley/Rosin and Belew [37, 32, 66] * Deduce intentions through observation. Huber and Durfee [40] * Autoepistemic reasoning (ignorance). Permpoontanalarp [59] * Model as a team (individual ! role). <p> Of course in a dynamic environment, it may not be feasible or even desirable to evolve a stable behavior. Applying RL to the iterated prisoner's dilemma, Sandholm and Crites find that a learning agent is able to perform optimally against a fixed opponent <ref> [75] </ref>. But when both agents are learning, there is no stable solution. Another issue in competitive co-evolution is the credit/blame assignment problem.
Reference: [76] <author> Andrea Schaerf, Yoav Shoham, and Moshe Tennenholtz. </author> <title> Adaptive load balancing: A study in multi-agent learning. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2:475500, </volume> <year> 1995. </year>
Reference-contexts: Tambe [85, 86] * Social reasoning: depend on others for goal (6= game theory). Sichman and Demazeau [79] * GAs to deal with Braes' paradox (more resource ! worse). Arora and Sen [4] * Multiagent RL for adaptive Load Balancing. Schaerf, Shoham, and Tennenholtz <ref> [76] </ref> * Focal points/Emergent conventions. Fenster et al./Walker and Woolridge [26, 91] * Design agents play different roles. <p> Adaptive load balancing has been studied as a multiagent problem by allowing different agents to decide which processor to use at a given time. Using RL, Schaerf et al. show that the heterogeneous agents can achieve reasonable load balance without any central control and without communication among agents <ref> [76] </ref>.
Reference: [77] <author> Jurgen Schmidhuber. </author> <title> A general method for multi-agent reinforcement learning in unrestricted environments. In Adaptation, Coevolution and Learning in Multiagent Systems: </title> <booktitle> Papers from the 1996 AAAI Spring Symposium, </booktitle> <pages> pages 8487, </pages> <address> Menlo Park,CA, </address> <month> March </month> <year> 1996. </year> <note> AAAI Press. AAAI Technical Report SS-96-01. </note>
Reference-contexts: Balch [7] * Local knowledge sometimes better. Roychowdhury [67] * (limited) Recursive Modeling Method (RMM). Durfee [24] * Don't model othersjust pay attention to reward. Schmidhuber <ref> [77] </ref> * Stigmergy. Holland/Goldman and Rosenschein [39, 31] * Q-learning for behaviors like foraging, homing, etc. Mataric [52] Learning opportunities * Enable others' actions * Sensor data ! Other agent's sensor data Table 5: The issues, techniques, and learning opportunities for homogeneous MAS as reflected in the literature. state. <p> Although it may be useful to build models of other agents in the environment, agent modeling is not done universally. Schmidhuber advocates a form of multiagent reinforcement learning (RL) with which agents do not model each other as agents <ref> [77] </ref>. Instead they consider each other as parts of the environment and affect each other's policies only as sensed objects. The agents pay attention to the reward they receive using a given policy and checkpoint their policies so they can return to successful ones.
Reference: [78] <author> Onn Shehory and Sarit Kraus. </author> <title> Task allocation via coalition formation among autonomous agents. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 655661, </pages> <address> Los Angeles, CA, 1995. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Castelfranchi [12] * Commitment states (potential, pre, and actual) as planning states. Haddadi [34] * Belief/Desire/Intention (BDI) model: OASIS. Rao and Georgeff [63] * BDI commitments only over intentions. Rao and Georgeff [63] * Coalitions. Zlotkin and Rosenschein/Shehory and Kraus/Sandholm and Lesser <ref> [97, 78, 72] </ref> * Reasoning about truthfulness. <p> Shehory and Kraus present a a distributed algorithm for task allocation when coalitions are either needed to perform tasks or more efficient that single agents <ref> [78] </ref>.
Reference: [79] <author> Jaime Simao Sichman and Yves Demazeau. </author> <title> Exploiting social reasoning to deal with agency level inconistency. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <pages> pages 352359, </pages> <address> Menlo Park, California, June 1995. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Huber and Durfee [40] * Autoepistemic reasoning (ignorance). Permpoontanalarp [59] * Model as a team (individual ! role). Tambe [85, 86] * Social reasoning: depend on others for goal (6= game theory). Sichman and Demazeau <ref> [79] </ref> * GAs to deal with Braes' paradox (more resource ! worse). Arora and Sen [4] * Multiagent RL for adaptive Load Balancing. Schaerf, Shoham, and Tennenholtz [76] * Focal points/Emergent conventions. Fenster et al./Walker and Woolridge [26, 91] * Design agents play different roles. <p> For example, two robots may be needed to successfully push a box, or, as in the pursuit domain, several agents may be needed to capture an opponent. Sichman and Demazeau analyze how the case of conflicting mutual models of different co-dependent agents can arise and be dealt with <ref> [79] </ref>. 5.3.4 Resource management Heterogeneous agents may have interdependent actions due to limited resources needed by several of the agents.
Reference: [80] <author> Reid G. Smith. </author> <title> The contract net protocol: High-level communication and control in a distributed problem solver. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-29(12):11041113, </volume> <month> December </month> <year> 1980. </year>
Reference-contexts: In the contract nets framework, agents all have their own goals, are self-interested, and have limited reasoning resources <ref> [80] </ref>. They bid to accept tasks from other agents and then can either perform the tasks (if they have the proper resources) or subcontract them to still other agents. Agents must pay to contract their tasks out and thus shop around for the lowest bidder.
Reference: [81] <author> Larry M. Stephens and Matthias B. Merx. </author> <title> The effect of agent control strategy on the performance of a dai pursuit problem. </title> <booktitle> In Proceedings of the 10th International Workshop on Distributed Artificial Intelligence, </booktitle> <address> Bandera, Texas, </address> <month> October </month> <year> 1990. </year>
Reference-contexts: The pursuit domain with homogeneous agents is illustrated in Figure 6. may have (the same amount of) limited information about other agents' internal states. Within this framework, Stephens and Merx propose a simple heuristic behavior for each agent that is based on local information <ref> [81] </ref>. They define capture positions as the four positions adjacent to the prey. They then propose a local strategy whereby each predator agent determines the capture position to which it is closest and moves towards that position. <p> Recall the local strategy defined by Stephens and Merx in which each predator simply moved to its closest capture position. In their instantiation of the domain, the predators can see the prey, but not each other. With communication possible, they define two more possible strategies for the predators <ref> [81] </ref>. When 20 using a distributed strategy, the agents are still homogeneous, but they communicate to insure that each moves toward a different capture position. In particular, the predator farthest from the prey chooses the capture position closest to it, and announces that it will approach that position. <p> A distributed strategy, it is much more effective than the local policy and does not require very much communication. However there are situations in which it does not succeed. Stephens and Merx then present one more strategy that always succeeds but requires much more communication: the central strategy <ref> [81] </ref>. The central strategy is effectively a single agent system. Three predators transmit all of their sensory inputs to one central agent which then decides where all the predators should move and transmits its decision back to them.
Reference: [82] <author> Peter Stone and Manuela Veloso. </author> <title> Beating a defender in robotic soccer: Memory-based learning of a continuous function. </title> <editor> In David S. Touretzky, Michael C. Mozer, and Michael E. Hasselmo, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <pages> pages 896902, </pages> <address> Cambridge, MA, 1996. </address> <publisher> MIT press. </publisher>
Reference-contexts: In another soccer simulator, Stone and Veloso use Memory-based Learning to allow a player to learn when to shoot and when to pass the ball <ref> [82] </ref>. They then use Neural Networks to teach a player to shoot a moving ball into the goal [83]. They use similar techniques in the soccerserver system as well, extending the learned behavior as a part of a hierarchical learning system [84].
Reference: [83] <author> Peter Stone and Manuela Veloso. </author> <title> Towards collaborative and adversarial learning: A case study in robotic soccer. </title> <note> To appear in International Journal of Human-Computer Systems (IJHCS), 1997. 33 </note>
Reference-contexts: In another soccer simulator, Stone and Veloso use Memory-based Learning to allow a player to learn when to shoot and when to pass the ball [82]. They then use Neural Networks to teach a player to shoot a moving ball into the goal <ref> [83] </ref>. They use similar techniques in the soccerserver system as well, extending the learned behavior as a part of a hierarchical learning system [84]. Matsubara et al. also use a Neural Network to allow a player to learn when to shoot and when to pass in the soccerserver system [54].
Reference: [84] <author> Peter Stone and Manuela Veloso. </author> <title> A layered approach to learning client behaviors in the robocup soccer server. </title> <note> To appear in Applied AI Journal, </note> <year> 1998. </year>
Reference-contexts: They then use Neural Networks to teach a player to shoot a moving ball into the goal [83]. They use similar techniques in the soccerserver system as well, extending the learned behavior as a part of a hierarchical learning system <ref> [84] </ref>. Matsubara et al. also use a Neural Network to allow a player to learn when to shoot and when to pass in the soccerserver system [54]. Uchibe et al. have successfully combined RL modules for shooting and for avoiding opponents using real robots [88].
Reference: [85] <author> Milind Tambe. </author> <title> Recursive agent and agent-group tracking in a real-time , dynamic environment. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <pages> pages 368375, </pages> <address> Menlo Park, California, June 1995. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Mor and Rosenschein/Sandholm and Crites [55, 75] * Minimax-Q. Littman [49] * Competitive co-evolution. Haynes and Sen/Grefenstette and Daley/Rosin and Belew [37, 32, 66] * Deduce intentions through observation. Huber and Durfee [40] * Autoepistemic reasoning (ignorance). Permpoontanalarp [59] * Model as a team (individual ! role). Tambe <ref> [85, 86] </ref> * Social reasoning: depend on others for goal (6= game theory). Sichman and Demazeau [79] * GAs to deal with Braes' paradox (more resource ! worse). Arora and Sen [4] * Multiagent RL for adaptive Load Balancing. Schaerf, Shoham, and Tennenholtz [76] * Focal points/Emergent conventions. <p> Tambe takes it one step further, studying how agents can learn models of teams of agents. In an air combat domain, agents can use RMM to try to deduce an opponents' plan based on its observable actions <ref> [85] </ref>. For example, a fired missile may not be visible, but the observation of a preparatory maneuver commonly used before firing could indicate that a missile has been launched. When teams of agents are involved, the situation becomes more complicated.
Reference: [86] <author> Milind Tambe. </author> <title> Tracking dynamic team activity. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <address> Menlo Park, California, 1996. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Mor and Rosenschein/Sandholm and Crites [55, 75] * Minimax-Q. Littman [49] * Competitive co-evolution. Haynes and Sen/Grefenstette and Daley/Rosin and Belew [37, 32, 66] * Deduce intentions through observation. Huber and Durfee [40] * Autoepistemic reasoning (ignorance). Permpoontanalarp [59] * Model as a team (individual ! role). Tambe <ref> [85, 86] </ref> * Social reasoning: depend on others for goal (6= game theory). Sichman and Demazeau [79] * GAs to deal with Braes' paradox (more resource ! worse). Arora and Sen [4] * Multiagent RL for adaptive Load Balancing. Schaerf, Shoham, and Tennenholtz [76] * Focal points/Emergent conventions. <p> When teams of agents are involved, the situation becomes more complicated. In this case, an opponent's actions may not make sense except in the context of a team maneuver. Then the agent's role within the team must be modeled. Tambe discusses the advantages of team modeling <ref> [86] </ref>. One reason that modeling other agents might be useful is that agents sometimes depend on each other for achieving their goals. Unlike in game theory where agents can cooperate or not depending on their utility estimation, there may be actions that require cooperation for successful execution. <p> When an agent is faced with an opposing team of agents, it may be useful to model individual agents as filling roles within a team action rather than as acting independently <ref> [86] </ref>. 5.4 Further Learning Opportunities Throughout the above investigation of issues and techniques in the heterogeneous non-communicating multiagent scenario, many learning approaches are described. A few of the other most obvious future ML applications to this scenario are described here and summarized in Table 6.
Reference: [87] <author> Ming Tan. </author> <title> Multi-agent reinforcement learning: Independent vs. </title> <booktitle> cooperative agents. In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 330337, </pages> <year> 1993. </year>
Reference-contexts: However the predators can freely exchange information in order to help them capture the prey more effectively. The current situation is illustrated in Figure 10. now the predators can communicate with one another. Tan uses communicating agents in the pursuit domain to conduct some interesting multiagent Q-learning experiments <ref> [87] </ref>. In his instantiation of the domain, there are several prey agents and the predators have limited vision so that they may not always know where the prey are. Thus the predators can help each other by informing each other of their sensory input. <p> Zeng and Sycara study a competitive negotiation scenario in which agents use Bayesian Learning techniques to update models of each other based on bids and counter bids in a negotiation process [96]. Similar to Tan's work on multiagent RL in the pursuit domain <ref> [87] </ref> is Wei's work with competing Q-learners. The agents compete with each other to earn the right to control a single system [93]. The highest bidder pays a certain amount to be allowed to act, then receives any reward that results from the action.
Reference: [88] <author> Eiji Uchibe, Minoru Asada, and Koh Hosoda. </author> <title> Behavior coordination for a mobile robot using modular reinforcement learning. </title> <booktitle> In Proc. of IEEE/RSJ International Conference on Intelligent Robots and Systems 1996 (IROS '96), </booktitle> <pages> pages 13291336, </pages> <year> 1996. </year>
Reference-contexts: Matsubara et al. also use a Neural Network to allow a player to learn when to shoot and when to pass in the soccerserver system [54]. Uchibe et al. have successfully combined RL modules for shooting and for avoiding opponents using real robots <ref> [88] </ref>. Once low-level behaviors have been developed, the opportunity to use ML techniques at the strategy level is particularly exciting.
Reference: [89] <author> Manuela Veloso, Peter Stone, Kwun Han, and Sorin Achim. Cmunited: </author> <title> A team of robotic soccer agents collaborating in an adversarial environment. </title> <booktitle> In Proceedings of the First International Workshop on RoboCup, </booktitle> <address> Nagoya,Japan, </address> <month> August </month> <year> 1997. </year>
Reference-contexts: Sahota et al. built a 1 vs. 1 version of the game. Asada et al. have used vision-based RL with their soccer playing robots [5]. Veloso et al. discuss some of the robotic issues involved in building robotic soccer players <ref> [3, 89] </ref>. Some robotic issues can only be studied in the real-world instantiation, but there are also many issues that can be studied in simulation. A particularly good simulator for this purpose is the soccerserver developed by Noda [56] and pictured in Figure 12.
Reference: [90] <author> Jose M. Vidal and Edmund H. Durfee. </author> <title> Recursive agent modeling using limited rationality. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <pages> pages 376 383, </pages> <address> Menlo Park, California, June 1995. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Such prediction can be useful when the agents move simultaneously and would like to base their actions on where the other predators will be at the next time step. Vidal and Durfee analyze such a situation using the Recursive Modeling Method (RMM) <ref> [90] </ref>. RMM is discussed in more detail below, but the basic idea is that predator A bases its move on the predicted move of predator B and vice versa. <p> Since the resulting reasoning can recurse indefinitely, it is important for the agents to bound the amount of reasoning they use either in terms of time or in terms of levels of recursion. Vidal and Durfee's Limited Rationality RMM algorithm is designed to take such considerations into account <ref> [90] </ref>. Levy and Rosenschein use a game theoretical approach to the pursuit domain [48]. They use a payoff function that allows selfish agents to cooperate. A requirement for their model is that each predator has full information about the location of other predators. <p> Durfee contends that for coordination to be possible, some potential knowledge must be ignored. As well as illustrating this concept in the pursuit domain <ref> [90] </ref>, Durfee goes into more detail and offers more generally applicable methodology in [24]. The point of the RMM is to model the internal state of another agent in order to predict its actions.
Reference: [91] <author> Adam Walker and Michael Wooldridge. </author> <title> Understanding the emergence of conventions in multi-agent systems. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <pages> pages 384389, </pages> <address> Menlo Park, California, June 1995. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Sichman and Demazeau [79] * GAs to deal with Braes' paradox (more resource ! worse). Arora and Sen [4] * Multiagent RL for adaptive Load Balancing. Schaerf, Shoham, and Tennenholtz [76] * Focal points/Emergent conventions. Fenster et al./Walker and Woolridge <ref> [26, 91] </ref> * Design agents play different roles. <p> They propose that, all else being equal, agents who need to meet should choose rare or extreme options. On a similar note, conventions might emerge over time. Walker and Woolridge propose biasing agents towards options that have been chosen, for example, most recently or most frequently in the past <ref> [91] </ref>. Rather than coming from pre-analysis of the options as in the Focal Point method, conventions emerge over time. 5.3.6 Roles When agents have similar goals, they can be organized into a team. Each agent then plays a separate role within the team.
Reference: [92] <author> Xuemei Wang. </author> <title> Planning while learning operators. </title> <booktitle> In Proceedings of the Third International Conference on AI Planning Systems, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: In addition to modeling agents' goals through observation, it is also possible to learn their actions. Wang's OBSERVER system allows an agent to incrementally learn the preconditions and effects of planning actions by observing domain experts <ref> [92] </ref>. After observing for a time, the agent can then experimentally refine its model by practicing the actions itself. When modeling other agents, it may be useful to reason not only about what is true and what is false, but also about what is not known.
Reference: [93] <author> Gerhard Wei. </author> <title> Distributed reinforcement learning. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <address> 15:135142, </address> <year> 1995. </year>
Reference-contexts: However multiagent learning is more concerned with learning issues that arise because of the multiagent aspect of a given domain. As described by Wei, multiagent learning is learning that is done by several agents and that becomes possible only because several agents are present <ref> [93] </ref>. This type of learning is emphasized in the sections entitled Further Learning Opportunities. <p> Cohen and Levesque/Lux and Steiner [14, 51] * Learning social behaviors. Mataric [53] * Bayesian learning in negotiation: model others. Zeng and Sycara [96] * Multiagent Q-learning. Weiss <ref> [93] </ref> * Training other agents' Q-functions (track driving). Clouse [13] * Minimize the need for training. Potter and Grefenstette [61] * Cooperative co-evolution. Bull et al. [11] * Contract nets for electronic commerce. Sandholm and Lesser [73] * Market-based systems. Huberman and Clearwater [41] * Generalized Partial Global Planning (GPGP). <p> Similar to Tan's work on multiagent RL in the pursuit domain [87] is Wei's work with competing Q-learners. The agents compete with each other to earn the right to control a single system <ref> [93] </ref>. The highest bidder pays a certain amount to be allowed to act, then receives any reward that results from the action. Another Q-learning approach, this time with benevolent agents, has been to explore the interesting idea of having one agent teach another agent through communication.
Reference: [94] <author> Gerhard Wei. </author> <booktitle> Ecai-96 workshop on learning in distributed artificial intelligence. Call For Papers, </booktitle> <year> 1996. </year>
Reference-contexts: Single-agent systems should be used in such cases. Finally, multiagent systems can be useful for their illucidation of intelligence [16]. As Gerhard Wei put it: Intelligence is deeply and inevitably coupled with interaction <ref> [94] </ref>. In fact, it has been proposed that the best way to develop intelligent machines at all might be to start by creating social machines [15]. This theory is based on the socio-biological theory that primate intelligence first evolved because of the need to deal with social interactions. <p> MAS is an active field with many open issues. Continuing research is presented at dedicated conferences and workshops such as the International Conference on Multi-Agent Systems [95, 2, 1]. MAS work also appears in many of the DAI conferences and workshops <ref> [22, 94] </ref>. This survey provides a framework within which the reader can situate both existing and future work. Acknowledgements We would like to thank Keith Decker, Rala Stone, Russell Stone, Astro Teller, and the anonymous reviewers for their helpful comments and suggestions.
Reference: [95] <author> Gerhard Wei and Sandip Sen, </author> <title> editors. Adaptation and Learning in Multiagent Systems. </title> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1996. </year>
Reference-contexts: The techniques presented are not exhaustive, but they highlight how multiagent systems can be and have been used to build complex systems. Because of the inherent complexity of MAS, there is much interest in using machine learning techniques to help deal with this complexity <ref> [95, 2] </ref>. When several different systems exist that could illustrate the same or similar MAS techniques, the systems presented here are biased towards those that use machine learning (ML) approaches. Furthermore, every effort is made to highlight additional opportunities for applying ML to MAS. <p> MAS is an active field with many open issues. Continuing research is presented at dedicated conferences and workshops such as the International Conference on Multi-Agent Systems <ref> [95, 2, 1] </ref>. MAS work also appears in many of the DAI conferences and workshops [22, 94]. This survey provides a framework within which the reader can situate both existing and future work.
Reference: [96] <author> Dajun Zeng and Katia Sycara. </author> <title> Bayesian learning in negotiation. In Adaptation, Coevolution and Learning in Multiagent Systems: </title> <booktitle> Papers from the 1996 AAAI Spring Symposium, </booktitle> <pages> pages 99104, </pages> <address> Menlo Park,CA, </address> <month> March </month> <year> 1996. </year> <note> AAAI Press. AAAI Technical Report SS-96-01. </note>
Reference-contexts: Cohen and Levesque/Lux and Steiner [14, 51] * Learning social behaviors. Mataric [53] * Bayesian learning in negotiation: model others. Zeng and Sycara <ref> [96] </ref> * Multiagent Q-learning. Weiss [93] * Training other agents' Q-functions (track driving). Clouse [13] * Minimize the need for training. Potter and Grefenstette [61] * Cooperative co-evolution. Bull et al. [11] * Contract nets for electronic commerce. Sandholm and Lesser [73] * Market-based systems. <p> In the current scenario, there are many more examples of competitive agents. Zeng and Sycara study a competitive negotiation scenario in which agents use Bayesian Learning techniques to update models of each other based on bids and counter bids in a negotiation process <ref> [96] </ref>. Similar to Tan's work on multiagent RL in the pursuit domain [87] is Wei's work with competing Q-learners. The agents compete with each other to earn the right to control a single system [93].
Reference: [97] <author> Gilad Zlotkin and Jeffrey S. Rosenschein. </author> <title> Coalition, cryptography, and stability: Mechanisms for coalition formation in task oriented domains. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 432437, </pages> <address> Menlo Park, California, </address> <month> August </month> <year> 1994. </year> <note> AAAI Press. 34 </note>
Reference-contexts: Castelfranchi [12] * Commitment states (potential, pre, and actual) as planning states. Haddadi [34] * Belief/Desire/Intention (BDI) model: OASIS. Rao and Georgeff [63] * BDI commitments only over intentions. Rao and Georgeff [63] * Coalitions. Zlotkin and Rosenschein/Shehory and Kraus/Sandholm and Lesser <ref> [97, 78, 72] </ref> * Reasoning about truthfulness. <p> Finally, groups of agents may decide to commit to each other. Rather than the more usual two-agent or all-agent commitment scenarios, Zlotkin and Rosenschein study situations in which agents may want to form coalitions <ref> [97] </ref>. Since this work is conducted in a game theory framework, agents consider the utility of joining a coalition in which they are bound to try to advance the utility of other members in exchange for reciprocal consideration.
References-found: 97

