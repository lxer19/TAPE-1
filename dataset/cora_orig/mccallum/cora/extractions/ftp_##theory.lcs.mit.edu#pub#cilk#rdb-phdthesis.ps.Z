URL: ftp://theory.lcs.mit.edu/pub/cilk/rdb-phdthesis.ps.Z
Refering-URL: http://theory.lcs.mit.edu/~cilk/papers.html
Root-URL: 
Title: Executing Multithreaded Programs Efficiently  
Author: by Robert D. Blumofe 
Degree: (1992) Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Doctor of Philosophy at the  c Robert D. Blumofe, MCMXCV. All rights reserved. The author hereby grants to MIT permission to reproduce and distribute publicly paper and electronic copies of this thesis document in whole or in part, and to grant others the right to do so. Author  Certified by Charles E. Leiserson Professor Thesis Supervisor Accepted by Frederic R. Morgenthaler Chairman, Departmental Committee on Graduate Students  
Date: September 1995  August 18, 1995  
Address: (1988)  
Affiliation: Sc.B., Brown University  S.M., Massachusetts Institute of Technology  MASSACHUSETTS INSTITUTE OF TECHNOLOGY  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 0
Reference: [1] <author> Noga Alon and Joel H. Spencer. </author> <title> The Probabilistic Method. </title> <publisher> John Wiley & Sons, </publisher> <year> 1992. </year>
Reference-contexts: The spawn tree is analogous to a conventional call 56 Chapter 5. Parallel programming in Cilk cilk int cilk main (int argc, char *argv []) f int n, result; n = atoi (argv <ref> [1] </ref>); result = spawn Fib (n); sync; printf ("Fib (%d) = %d."n", n, result); return 0; g tree, and it is equivalent to the spawn tree described in Chapter 2 except that in Cilk we use the term procedure instead of thread. <p> Letting the random variable X denote the number of these steal attempts that do choose processor p, we bound the probability that X is less than 3n l by using a Cher-noff bound <ref> [1] </ref> on the lower tail of a binomial distribution with mean m: Pr fX &lt; m ag e a 2 =2m for any a &gt; 0.
Reference: [2] <author> Andrew W. Appel. </author> <title> Compiling with Continuations. </title> <publisher> Cambridge University Press, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: The accumulate statement is the same as send argument except that it uses the function op to accumulate value into the argument slot. This style of linking Cilk threads is called continuation-passing style <ref> [2] </ref> and contrasts with the spawn/return style that links procedures. In spawn/return style, a spawned child always returns to its parent, and the parent, after performing the spawn, can suspend, waiting for the child to return.
Reference: [3] <author> Arvind, Rishiyur S. Nikhil, and Keshav K. Pingali. I-structures: </author> <title> Data structures for parallel computing. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 11(4) </volume> <pages> 598-632, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Many multithreaded programming languages and runtime systems are based on heuristic 1.3. Contributions of this thesis 7 scheduling techniques. Though systems such as Charm [91], COOL [27, 28], Id <ref> [3, 37, 80] </ref>, Olden [22], and others [29, 31, 38, 44, 54, 55, 63, 88, 98] are based on sound heuristics that seem to perform well in practice and generally have wider applicability than Cilk, none are able to provide any sort of performance guarantee or accurate machine-independent performance model.
Reference: [4] <author> Brian N. Bershad, Matthew J. Zekauskas, and Wayne A. Sawdon. </author> <title> The Midway distributed shared memory system. </title> <booktitle> In Digest of Papers from the Thirty-Eighth IEEE Computer Society International Conference (Spring COMPCON), </booktitle> <pages> pages 528-537, </pages> <address> San Francisco, California, </address> <month> February </month> <year> 1993. </year>
Reference-contexts: In this section, we look at other theoretical results and systems that address scheduling issues for dynamic parallel computation. We shall not look at data-parallel systems [8, 53] nor at systems focused 6 Chapter 1. Executing multithreaded programs efficiently on infrastructure such as distributed shared memory <ref> [4, 6, 29, 39, 59, 60, 66, 73, 87, 92, 93] </ref> or message passing [43, 96, 104, 105]. Substantial research has been reported in the theoretical literature concerning the scheduling of dynamic computations.
Reference: [5] <author> Sandeep Bhatt, David Greenberg, Tom Leighton, and Pangfeng Liu. </author> <title> Tight bounds for on-line tree embeddings. </title> <booktitle> In Proceedings of the Second Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 344-350, </pages> <address> San Francisco, California, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: In contrast to our research on multithreaded computations, however, other theoretical research has tended to ignore space requirements and communication costs. Related work in this area includes a randomized work-stealing algorithm for load balancing independent jobs [89]; algorithms for dynamically embedding trees in fixed-connection networks <ref> [5, 71] </ref>; and algorithms for backtrack search and branch-and-bound [61, 65, 75, 86, 109].
Reference: [6] <author> Andrew Birrell, Greg Nelson, Susan Owicki, and Edward Wobber. </author> <title> Network objects. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Operating Systems Principles (SOSP 14), </booktitle> <pages> pages 217-230, </pages> <address> Asheville, North Carolina, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: In this section, we look at other theoretical results and systems that address scheduling issues for dynamic parallel computation. We shall not look at data-parallel systems [8, 53] nor at systems focused 6 Chapter 1. Executing multithreaded programs efficiently on infrastructure such as distributed shared memory <ref> [4, 6, 29, 39, 59, 60, 66, 73, 87, 92, 93] </ref> or message passing [43, 96, 104, 105]. Substantial research has been reported in the theoretical literature concerning the scheduling of dynamic computations.
Reference: [7] <author> Andrew D. Birrell. </author> <title> An introduction to programming with threads. </title> <type> Technical Report 35, </type> <institution> Digital Equipment Corporation Systems Research Center, </institution> <month> January </month> <year> 1989. </year>
Reference-contexts: Nevertheless, this was a one-time effort that we expect will reap performance rewards for a long time to come. The job broker, CilkJobBroker, and node manager, CilkNodeManager, are implemented using remote procedure calls (RPC) <ref> [7] </ref> in the standard client/server configuration with the job broker as the server. When the node manager on a machine finds that its machine is idle, it makes a remote procedure call to the job broker to obtain a Cilk job description.
Reference: [8] <author> Guy E. Blelloch. </author> <title> Programming parallel algorithms. </title> <booktitle> In Proceedings of the 1992 Dartmouth Institute for Advanced Graduate Studies (DAGS) Symposium on Parallel Computation, </booktitle> <pages> pages 11-18, </pages> <address> Hanover, New Hampshire, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Likewise, the Cilk programming model and runtime systemincluding Cilk-NOWbuild on ideas found in earlier systems. In this section, we look at other theoretical results and systems that address scheduling issues for dynamic parallel computation. We shall not look at data-parallel systems <ref> [8, 53] </ref> nor at systems focused 6 Chapter 1. Executing multithreaded programs efficiently on infrastructure such as distributed shared memory [4, 6, 29, 39, 59, 60, 66, 73, 87, 92, 93] or message passing [43, 96, 104, 105]. <p> The use of work and critical path length to analyze parallel algorithms and model application performance is also not new. Work and critical path have been used in the theory community for years to analyze parallel algorithms [64]. Blelloch <ref> [8] </ref> has developed a performance model for data-parallel computations based on these same two abstract measures. He cites many advantages to such a model over machine-based models. Cilk provides a similar performance model for the domain of multithreaded computation.
Reference: [9] <author> Guy E. Blelloch, Phillip B. Gibbons, and Yossi Matias. </author> <title> Provably efficient scheduling for languages with fine-grained parallelism. </title> <booktitle> In Proceedings of the Seventh Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 1-12, </pages> <address> Santa Barbara, California, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: These analyses focus on the tree-growing stage of divide-and-conquer programs, so they do not consider the case when synchronization is involved. For programs with nested fine-grained parallelism, Blelloch, Gibbons, and Matias <ref> [9] </ref> give a scheduling algorithm and prove that it is efficient with respect to both time and space.
Reference: [10] <author> Robert D. Blumofe, Matteo Frigo, Christopher F. Joerg, Bradley C. Kuszmaul, Charles E. Leiserson, Rob Miller, Keith H. Randall, and Yuli Zhou. </author> <title> Cilk 2.0 Reference Manual. </title> <institution> MIT Laboratory for Computer Science, 545 Technology Square, </institution> <address> Cambridge, Massachusetts 02139, </address> <month> June </month> <year> 1995. </year> <note> 115 116 Bibliography </note>
Reference-contexts: To date, all of the applications that we have coded are fully strict. 5.1 The Cilk language and runtime system The Cilk language <ref> [10] </ref> extends C with primitives to express parallelism, and the Cilk run-time system maps the expressed parallelism into parallel execution.
Reference: [11] <author> Robert D. Blumofe, Matteo Frigo, Christopher F. Joerg, Charles E. Leiserson, and Keith H. Randall. </author> <title> Dag-consistent distributed shared memory. </title> <booktitle> In Proceedings of the 10th International Parallel Processing Symposium (IPPS), </booktitle> <month> April </month> <year> 1996. </year> <note> to appear. </note>
Reference-contexts: The analysis builds on the work of Chapter 4. In Chapter 6, we present the implementation of adaptive parallelism and fault tolerance in Cilk-NOW. Finally, in Chapter 7, we conclude and discuss current and planed work to add distributed shared memory to Cilk using dag consistency <ref> [11] </ref>. The reader 1.3. Contributions of this thesis 9 interested only in the system-building contributions of this thesis may safely skip ahead to Chapter 5 and pass over Section 5.3. Information about the current and forthcoming Cilk software releases can be found in Appendix B. <p> By charging S 1 space for each busy leaf, we may be overcharging. For some computations, by knowing that the schedule preserves the busy-leaves property, we can appeal directly to the fact that the spawn subtree never has more than P leaves to obtain tight bounds on space usage <ref> [11] </ref>. We finish this chapter by showing that for strict computations, the Busy-Leaves Algorithm computes a schedule that is both greedy and maintains the busy-leaves property. Thus, we show that every strict computation has execution schedules that are simultaneously efficient with respect to time and space. <p> Some forms of nonstrictness, for example, might allow Cilk to efficiently execute more synchronous types of applications. Our current work is focused on incorporating in Cilk a distributed shared memory using dag consistency <ref> [11] </ref>. A distributed shared memory provides a global virtual address space, so an instruction may load from or store to a virtual address that has meaning independent of which processor executes the instruction. With deterministic dag consistency, we specify the value returned when an instruction performs a load as follows.
Reference: [12] <author> Robert D. Blumofe, Christopher F. Joerg, Bradley C. Kuszmaul, Charles E. Leiser-son, Keith H. Randall, and Yuli Zhou. Cilk: </author> <title> An efficient multithreaded runtime system. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), </booktitle> <pages> pages 207-216, </pages> <address> Santa Barbara, California, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Some of the material in this chapter was previously published in <ref> [12] </ref>. 53 54 Chapter 5. Parallel programming in Cilk processed to C using the cilk2c translator 1 [76] and then compiled and linked with a run-time library to run on the target platform. <p> In this section, we show how these proofs must be modified Some of the research reported in this section is joint work with Charles Leiserson and Keith Randall both of MIT's Laboratory for Computer Science. The material in this section generalizes results previously published in <ref> [12] </ref>. 76 Chapter 5. Parallel programming in Cilk to account for the Cilk model. Recall that a Cilk computation models the execution of a Cilk program as a tree of procedures and a dag of threads that unfold dynamically during program execution.
Reference: [13] <author> Robert D. Blumofe and Charles E. Leiserson. </author> <title> Space-efficient scheduling of multi-threaded computations. </title> <booktitle> In Proceedings of the Twenty Fifth Annual ACM Symposium on Theory of Computing (STOC), </booktitle> <pages> pages 362-371, </pages> <address> San Diego, California, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: The research reported in this chapter is joint work with Charles Leiserson of MIT's Laboratory for Computer Science and was first published in <ref> [13] </ref>. 11 12 Chapter 2. A model of multithreaded computation and 6 threads G 1 ; G 2 ; : : :; G 6 . An execution schedule for a multithreaded computation determines which processors of a parallel computer execute which instructions at each step. <p> This lower Some of the research reported in this chapter is joint work with Charles Leiserson of MIT's Laboratory for Computer Science and was first published in <ref> [13] </ref> and [14]. 25 26 Chapter 3. Strict multithreaded computations bound motivates our consideration, in later sections, of computations with more structure, namely, strict multithreaded computations. Before going on to a more formal statement of the lower bound, it is worth noting the difference between useless and excess parallelism.
Reference: [14] <author> Robert D. Blumofe and Charles E. Leiserson. </author> <title> Scheduling multithreaded computations by work stealing. </title> <booktitle> In Proceedings of the 35th Annual Symposium on Foundations of Computer Science (FOCS), </booktitle> <pages> pages 356-368, </pages> <address> Santa Fe, New Mexico, </address> <month> Novem-ber </month> <year> 1994. </year>
Reference-contexts: This lower Some of the research reported in this chapter is joint work with Charles Leiserson of MIT's Laboratory for Computer Science and was first published in [13] and <ref> [14] </ref>. 25 26 Chapter 3. Strict multithreaded computations bound motivates our consideration, in later sections, of computations with more structure, namely, strict multithreaded computations. Before going on to a more formal statement of the lower bound, it is worth noting the difference between useless and excess parallelism. <p> Threads can be inserted on the bottom and removed from either end. A processor treats its ready The research reported in this chapter is joint work with Charles Leiserson of MIT's Laboratory for Computer Science and was first published in <ref> [14] </ref>. 37 38 Chapter 4. Work stealing deque like a call stack, pushing and popping from the bottom. Threads that are migrated to other processors are removed from the top. In general, a processor obtains work by removing the thread at the bottom of its ready deque.
Reference: [15] <author> Robert D. Blumofe and David S. Park. </author> <title> Scheduling large-scale parallel computations on networks of workstations. </title> <booktitle> In Proceedings of the Third International Symposium on High Performance Distributed Computing (HPDC), </booktitle> <pages> pages 96-105, </pages> <address> San Francisco, California, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Some of the material in this chapter was previously published in <ref> [15] </ref>. At that time, Cilk-NOW was in the prototype stage and was called Phish. 89 90 Chapter 6. Cilk on a network of workstations the Cilk-NOW runtime system, explaining the operation of each component program and their interactions.
Reference: [16] <author> Richard P. Brent. </author> <title> The parallel evaluation of general arithmetic expressions. </title> <journal> Journal of the ACM, </journal> <volume> 21(2) </volume> <pages> 201-206, </pages> <month> April </month> <year> 1974. </year>
Reference-contexts: A computer with P processors can execute at most P instructions per step, and since the computation has T 1 instructions, we have the lower bound T P T 1 =P. And, of course, we also have the lower bound T P T . Early independent work by Brent <ref> [16, Lemma 2] </ref> and Graham [48, 49] yields the upper bound T P T 1 =P + T .
Reference: [17] <author> Eric A. Brewer and Robert Blumofe. Strata: </author> <title> A multi-layer communications library. </title> <note> Technical Report to appear, </note> <institution> MIT Laboratory for Computer Science. </institution> <note> Available as ftp://ftp.lcs.mit.edu/pub/supertech/strata/strata.tar.Z. </note>
Reference-contexts: All experiments were run on a CM5 supercomputer. The CM5 is a massively parallel computer based on 32MHz SPARC processors with a fat-tree interconnection network [72]. The Cilk runtime system on the CM5 performs communication among processors using the Strata <ref> [17] </ref> active-message library. 5.2.2 Application performance By running our applications and measuring a suite of performance parameters, we empirically answer a number of questions about the effectiveness of the Cilk runtime system. We focus on the following questions.
Reference: [18] <author> F. Warren Burton. </author> <title> Storage management in virtual tree machines. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(3) </volume> <pages> 321-328, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: For programs with nested fine-grained parallelism, Blelloch, Gibbons, and Matias [9] give a scheduling algorithm and prove that it is efficient with respect to both time and space. Burton <ref> [18] </ref> shows how to limit space in certain parallel computations without causing deadlock, and Burton and Simpson [19] give an offline algorithm that is efficient with respect to time and space in a very general model of multithreaded computation.
Reference: [19] <author> F. Warren Burton and David J. Simpson. </author> <title> Space efficient execution of deterministic parallel programs. </title> <type> Unpublished manuscript, </type> <year> 1994. </year>
Reference-contexts: For programs with nested fine-grained parallelism, Blelloch, Gibbons, and Matias [9] give a scheduling algorithm and prove that it is efficient with respect to both time and space. Burton [18] shows how to limit space in certain parallel computations without causing deadlock, and Burton and Simpson <ref> [19] </ref> give an offline algorithm that is efficient with respect to time and space in a very general model of multithreaded computation.
Reference: [20] <author> F. Warren Burton and M. Ronan Sleep. </author> <title> Executing functional programs on a virtual tree of processors. </title> <booktitle> In Proceedings of the 1981 Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 187-194, </pages> <address> Portsmouth, New Hampshire, </address> <month> October </month> <year> 1981. </year>
Reference-contexts: The work-stealing idea dates back at least as far as Burton and Sleep's research <ref> [20] </ref> on parallel execution of functional programs and Halstead's implementation of Multilisp [51, 52]. <p> First, to lower communication costs, we would like to steal large amounts of work, and in a tree-structured computation, shallow threads are likely to spawn more work than deep ones. This heuristic notion is the justification cited by earlier researchers <ref> [20, 42, 52, 77, 103] </ref> who proposed stealing work that is shallow in the spawn tree. We cannot, however, prove that shallow threads are more likely to spawn work than deep ones. What we prove in Section 5.3 is the following algorithmic property.
Reference: [21] <author> Clemens H. Cap and Volker Strumpen. </author> <title> Efficient parallel computing in distributed workstation environments. </title> <journal> Parallel Computing, </journal> <volume> 19 </volume> <pages> 1221-1234, </pages> <year> 1993. </year>
Reference-contexts: Massively parallel supercomputers such as the Cray Research T3D or the Thinking Machines CM5, for example, either dedicate themselves to a single user at a time or gang-timeshare within fixed size partitions [72]. Systems such as Charm [91], the Parform <ref> [21] </ref>, PVM/Hence [96], and others [29, 42, 44, 97] support parallel computing on a network of workstations. In these systems, the set of machines on which the program runs is chosen statically by the user.
Reference: [22] <author> Martin C. Carlisle, Anne Rogers, John H. Reppy, and Laurie J. Hendren. </author> <title> Early experiences with Olden. </title> <booktitle> In Proceedings of the Sixth Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, Oregon, </address> <month> August </month> <year> 1993. </year> <note> Bibliography 117 </note>
Reference-contexts: Many multithreaded programming languages and runtime systems are based on heuristic 1.3. Contributions of this thesis 7 scheduling techniques. Though systems such as Charm [91], COOL [27, 28], Id [3, 37, 80], Olden <ref> [22] </ref>, and others [29, 31, 38, 44, 54, 55, 63, 88, 98] are based on sound heuristics that seem to perform well in practice and generally have wider applicability than Cilk, none are able to provide any sort of performance guarantee or accurate machine-independent performance model. <p> Each Cilk thread runs to completion without suspending and leaves the C runtime stack empty when it dies. A common alternative <ref> [22, 47, 52, 63, 77, 81] </ref> is to directly support spawn/return threads (or procedures) in the runtime system, possibly with stack-allocated activation frames. In such a system, threads can suspend waiting for synchronization and leave temporary values on the calling stack.
Reference: [23] <author> Nicholas Carriero, Eric Freeman, and David Gelernter. </author> <title> Adaptive parallelism on multiprocessors: Preliminary experience with Piranha on the CM-5. </title> <booktitle> In Proceedings of the Sixth Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, Oregon, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Distributed operating systems [30, 82, 98, 99] and other systems [32, 40, 68, 74, 79, 110] provide transparent process placement and (in some cases) migration, but these systems are geared towards large serial programs or coarse-grain distributed programs. A system that does provide adaptive parallelism is Piranha <ref> [23, 46, 62] </ref>. (The creators of Piranha appear to have coined the term adaptive parallelism.) Based on Linda [24], Piranha's adaptive parallelism leverages structure in the Linda programming model much as Cilk-NOW leverages structure in the Cilk programming model.
Reference: [24] <author> Nicholas Carriero and David Gelernter. </author> <title> Linda in context. </title> <journal> Communications of the ACM, </journal> <volume> 32(4) </volume> <pages> 444-458, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: A system that does provide adaptive parallelism is Piranha [23, 46, 62]. (The creators of Piranha appear to have coined the term adaptive parallelism.) Based on Linda <ref> [24] </ref>, Piranha's adaptive parallelism leverages structure in the Linda programming model much as Cilk-NOW leverages structure in the Cilk programming model. Adaptive parallelism is also present in the Benevolent Bandit Laboratory (BBL) [40], a PC-based system. The BBL system architecture is closely related to Cilk-NOW's.
Reference: [25] <author> Vint Cerf and Robert Kahn. </author> <title> A protocol for packet network intercommunication. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 22(5) </volume> <pages> 637-648, </pages> <month> May </month> <year> 1974. </year>
Reference-contexts: Beyond this level and in a wide-area network, we may require multiple clearinghouses configured in a hierarchy. All of the communication between workers and between workers and the clearinghouse is implemented with UDP/IP [95]. UDP/IP is an unreliable datagram protocol built on top of the internet protocol <ref> [25] </ref>. The protocols implemented in the Cilk-NOW runtime system all use UDP/IP to perform split-phase communication, so except in the case of work stealing, a worker never sits idle waiting for a reply or an acknowledgment.
Reference: [26] <author> Rohit Chandra, Scott Devine, Ben Verghese, Anoop Gupta, and Mendel Rosenblum. </author> <title> Scheduling and page migration for multiprocessor compute servers. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 12-24, </pages> <address> San Jose, California, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: The BBL system architecture is closely related to Cilk-NOW's. The Prospero resource manager [78] also employs a similar system architecture. Runtime systems for the programming language COOL [28] running on symmetric multiprocessors [101, 102] and cache-coherent, distributed, shared-memory machines <ref> [26, 69] </ref> use process control to support adaptive parallelism. These systems rely on special-purpose operating system and hardware support. In contrast, Cilk-NOW supports adaptive parallelism entirely in user-level software on top of commercial hardware and operating systems.
Reference: [27] <author> Rohit Chandra, Anoop Gupta, and John L. Hennessy. </author> <title> Data locality and load balancing in COOL. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), </booktitle> <pages> pages 249-259, </pages> <address> San Diego, California, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Many multithreaded programming languages and runtime systems are based on heuristic 1.3. Contributions of this thesis 7 scheduling techniques. Though systems such as Charm [91], COOL <ref> [27, 28] </ref>, Id [3, 37, 80], Olden [22], and others [29, 31, 38, 44, 54, 55, 63, 88, 98] are based on sound heuristics that seem to perform well in practice and generally have wider applicability than Cilk, none are able to provide any sort of performance guarantee or accurate machine-independent
Reference: [28] <author> Rohit Chandra, Anoop Gupta, and John L. Hennessy. </author> <title> COOL: An object-based language for parallel programming. </title> <journal> IEEE Computer, </journal> <volume> 27(8) </volume> <pages> 13-26, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: Many multithreaded programming languages and runtime systems are based on heuristic 1.3. Contributions of this thesis 7 scheduling techniques. Though systems such as Charm [91], COOL <ref> [27, 28] </ref>, Id [3, 37, 80], Olden [22], and others [29, 31, 38, 44, 54, 55, 63, 88, 98] are based on sound heuristics that seem to perform well in practice and generally have wider applicability than Cilk, none are able to provide any sort of performance guarantee or accurate machine-independent <p> Adaptive parallelism is also present in the Benevolent Bandit Laboratory (BBL) [40], a PC-based system. The BBL system architecture is closely related to Cilk-NOW's. The Prospero resource manager [78] also employs a similar system architecture. Runtime systems for the programming language COOL <ref> [28] </ref> running on symmetric multiprocessors [101, 102] and cache-coherent, distributed, shared-memory machines [26, 69] use process control to support adaptive parallelism. These systems rely on special-purpose operating system and hardware support. In contrast, Cilk-NOW supports adaptive parallelism entirely in user-level software on top of commercial hardware and operating systems.
Reference: [29] <author> Jeffrey S. Chase, Franz G. Amador, Edward D. Lazowska, Henry M. Levy, and Richard J. Littlefield. </author> <title> The Amber system: Parallel programming on a network of multiprocessors. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles (SOSP 12), </booktitle> <pages> pages 147-158, </pages> <address> Litchfield Park, Arizona, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: In this section, we look at other theoretical results and systems that address scheduling issues for dynamic parallel computation. We shall not look at data-parallel systems [8, 53] nor at systems focused 6 Chapter 1. Executing multithreaded programs efficiently on infrastructure such as distributed shared memory <ref> [4, 6, 29, 39, 59, 60, 66, 73, 87, 92, 93] </ref> or message passing [43, 96, 104, 105]. Substantial research has been reported in the theoretical literature concerning the scheduling of dynamic computations. <p> Many multithreaded programming languages and runtime systems are based on heuristic 1.3. Contributions of this thesis 7 scheduling techniques. Though systems such as Charm [91], COOL [27, 28], Id [3, 37, 80], Olden [22], and others <ref> [29, 31, 38, 44, 54, 55, 63, 88, 98] </ref> are based on sound heuristics that seem to perform well in practice and generally have wider applicability than Cilk, none are able to provide any sort of performance guarantee or accurate machine-independent performance model. <p> Massively parallel supercomputers such as the Cray Research T3D or the Thinking Machines CM5, for example, either dedicate themselves to a single user at a time or gang-timeshare within fixed size partitions [72]. Systems such as Charm [91], the Parform [21], PVM/Hence [96], and others <ref> [29, 42, 44, 97] </ref> support parallel computing on a network of workstations. In these systems, the set of machines on which the program runs is chosen statically by the user.
Reference: [30] <author> David R. Cheriton. </author> <title> The V distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 31(3) </volume> <pages> 314-333, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: Systems such as Charm [91], the Parform [21], PVM/Hence [96], and others [29, 42, 44, 97] support parallel computing on a network of workstations. In these systems, the set of machines on which the program runs is chosen statically by the user. Distributed operating systems <ref> [30, 82, 98, 99] </ref> and other systems [32, 40, 68, 74, 79, 110] provide transparent process placement and (in some cases) migration, but these systems are geared towards large serial programs or coarse-grain distributed programs.
Reference: [31] <author> Andrew A. Chien and William J. Dally. </author> <title> Concurrent Aggregates (CA). </title> <booktitle> In Proceedings of the Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), </booktitle> <pages> pages 187-196, </pages> <address> Seattle, Washington, </address> <month> March </month> <year> 1990. </year> <note> Also: MIT Artificial Intelligence Laboratory Technical Report MIT/AI/TR-1248. </note>
Reference-contexts: Many multithreaded programming languages and runtime systems are based on heuristic 1.3. Contributions of this thesis 7 scheduling techniques. Though systems such as Charm [91], COOL [27, 28], Id [3, 37, 80], Olden [22], and others <ref> [29, 31, 38, 44, 54, 55, 63, 88, 98] </ref> are based on sound heuristics that seem to perform well in practice and generally have wider applicability than Cilk, none are able to provide any sort of performance guarantee or accurate machine-independent performance model.
Reference: [32] <author> Henry Clark and Bruce McMillin. </author> <title> DAWGSa distributed compute server utilizing idle workstations. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 14(2) </volume> <pages> 175-186, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: In these systems, the set of machines on which the program runs is chosen statically by the user. Distributed operating systems [30, 82, 98, 99] and other systems <ref> [32, 40, 68, 74, 79, 110] </ref> provide transparent process placement and (in some cases) migration, but these systems are geared towards large serial programs or coarse-grain distributed programs.
Reference: [33] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: The left-to-right depth-first order is then the order of instructions visited by a preorder walk of this tree <ref> [33] </ref>. For example, the left-to-right depth-first order for the computation of Figure 2.9 is v 1 ; v 2 ; : : :; v 13 . Now considering the dependency edges again, a multithreaded computation is depth-first if this left-to-right depth-first order yields a 1-processor execution schedule.
Reference: [34] <author> David E. Culler. </author> <title> Resource management for the tagged token dataflow architecture. </title> <type> Master's thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Mass-achusetts Institute of Technology, </institution> <month> January </month> <year> 1980. </year> <note> Also: </note> <institution> MIT Laboratory for Computer Science Technical Report MIT/LCS/TR-332. </institution> <address> 118 Bibliography </address>
Reference-contexts: Our results specialize to match theirs. Likewise, most of the systems-oriented work on multithreading has ignored the space issue. Notable exceptions include the k-bounded loops of Culler and Arvind <ref> [34, 35, 36] </ref> and the throttling mechanism of Ruggiero and Sargeant [90]. These techniques and others [56, 57] have met with some success, though none have any algorithmic foundation. <p> While Culler and Arvind argued convincingly that the observed useless parallelism is in fact useless, they came short of a proof, and they left open the possibility of a clever scheduler that might be able to exploit this parallelism without using excessive amounts of space. With loop-bounding <ref> [34, 35, 36] </ref> techniques, they were able to eliminate the useless parallelism with only a small decrease in the average parallelism. Their applications had only small amounts of useless parallelism. In this section we show that multithreaded computations may contain vast quantities of provably useless parallelism.
Reference: [35] <author> David E. Culler. </author> <title> Managing Parallelism and Resources in Scientific Dataflow Programs. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> March </month> <year> 1990. </year> <note> Also: </note> <institution> MIT Laboratory for Computer Science Technical Report MIT/LCS/TR-446. </institution>
Reference-contexts: Our results specialize to match theirs. Likewise, most of the systems-oriented work on multithreading has ignored the space issue. Notable exceptions include the k-bounded loops of Culler and Arvind <ref> [34, 35, 36] </ref> and the throttling mechanism of Ruggiero and Sargeant [90]. These techniques and others [56, 57] have met with some success, though none have any algorithmic foundation. <p> While Culler and Arvind argued convincingly that the observed useless parallelism is in fact useless, they came short of a proof, and they left open the possibility of a clever scheduler that might be able to exploit this parallelism without using excessive amounts of space. With loop-bounding <ref> [34, 35, 36] </ref> techniques, they were able to eliminate the useless parallelism with only a small decrease in the average parallelism. Their applications had only small amounts of useless parallelism. In this section we show that multithreaded computations may contain vast quantities of provably useless parallelism.
Reference: [36] <author> David E. Culler and Arvind. </author> <title> Resource requirements of dataflow programs. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 141-150, </pages> <address> Honolulu, Hawaii, </address> <month> May </month> <year> 1988. </year> <note> Also: </note> <institution> MIT Laboratory for Computer Science, </institution> <note> Computation Structures Group Memo 280. </note>
Reference-contexts: Our results specialize to match theirs. Likewise, most of the systems-oriented work on multithreading has ignored the space issue. Notable exceptions include the k-bounded loops of Culler and Arvind <ref> [34, 35, 36] </ref> and the throttling mechanism of Ruggiero and Sargeant [90]. These techniques and others [56, 57] have met with some success, though none have any algorithmic foundation. <p> (Chapter 4) employed by the Cilk runtime system (Chapter 5) and in the implementation of adaptive parallelism and fault tolerance on a network of workstations (Chapter 6). 3.1 A lower bound for the general case In a study of resource requirements for dataflow programs published in 1988, Culler and Arvind <ref> [36] </ref> observed applications with parallelism that they conjectured to be useless. Useless parallelism is parallelism that requires excessive amounts of space resource to exploit. <p> While Culler and Arvind argued convincingly that the observed useless parallelism is in fact useless, they came short of a proof, and they left open the possibility of a clever scheduler that might be able to exploit this parallelism without using excessive amounts of space. With loop-bounding <ref> [34, 35, 36] </ref> techniques, they were able to eliminate the useless parallelism with only a small decrease in the average parallelism. Their applications had only small amounts of useless parallelism. In this section we show that multithreaded computations may contain vast quantities of provably useless parallelism.
Reference: [37] <author> David E. Culler, Anurag Sah, Klaus Erik Schauser, Thorsten von Eicken, and John Wawrzynek. </author> <title> Fine-grain parallelism with minimal hardware support: A compiler-controlled threaded abstract machine. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 164-175, </pages> <address> Santa Clara, California, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Many multithreaded programming languages and runtime systems are based on heuristic 1.3. Contributions of this thesis 7 scheduling techniques. Though systems such as Charm [91], COOL [27, 28], Id <ref> [3, 37, 80] </ref>, Olden [22], and others [29, 31, 38, 44, 54, 55, 63, 88, 98] are based on sound heuristics that seem to perform well in practice and generally have wider applicability than Cilk, none are able to provide any sort of performance guarantee or accurate machine-independent performance model.
Reference: [38] <author> William J. Dally, Linda Chao, Andrew Chien, Soha Hassoun, Waldemar Horwat, Jon Kaplan, Paul Song, Brian Totty, and Scott Wills. </author> <title> Architecture of a message-driven processor. </title> <booktitle> In Proceedings of the 14th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 189-196, </pages> <address> Pittsburgh, Pennsylvania, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: Many multithreaded programming languages and runtime systems are based on heuristic 1.3. Contributions of this thesis 7 scheduling techniques. Though systems such as Charm [91], COOL [27, 28], Id [3, 37, 80], Olden [22], and others <ref> [29, 31, 38, 44, 54, 55, 63, 88, 98] </ref> are based on sound heuristics that seem to perform well in practice and generally have wider applicability than Cilk, none are able to provide any sort of performance guarantee or accurate machine-independent performance model.
Reference: [39] <author> Babak Falsafi, Alvin R. Lebeck, Steven K. Reinhardt, Ioannis Schoinas, Mark D. Hill, James R. Larus, Anne Rogers, and David A. Wood. </author> <title> Application-specific protocols for user-level shared memory. </title> <booktitle> In Supercomputing '94, </booktitle> <pages> pages 380-389, </pages> <address> Wash-ington, D.C., </address> <month> November </month> <year> 1994. </year>
Reference-contexts: In this section, we look at other theoretical results and systems that address scheduling issues for dynamic parallel computation. We shall not look at data-parallel systems [8, 53] nor at systems focused 6 Chapter 1. Executing multithreaded programs efficiently on infrastructure such as distributed shared memory <ref> [4, 6, 29, 39, 59, 60, 66, 73, 87, 92, 93] </ref> or message passing [43, 96, 104, 105]. Substantial research has been reported in the theoretical literature concerning the scheduling of dynamic computations.
Reference: [40] <author> Robert E. Felderman, Eve M. Schooler, and Leonard Kleinrock. </author> <title> The Benevolent Bandit Laboratory: A testbed for distributed algorithms. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 7(2) </volume> <pages> 303-311, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: In these systems, the set of machines on which the program runs is chosen statically by the user. Distributed operating systems [30, 82, 98, 99] and other systems <ref> [32, 40, 68, 74, 79, 110] </ref> provide transparent process placement and (in some cases) migration, but these systems are geared towards large serial programs or coarse-grain distributed programs. <p> Adaptive parallelism is also present in the Benevolent Bandit Laboratory (BBL) <ref> [40] </ref>, a PC-based system. The BBL system architecture is closely related to Cilk-NOW's. The Prospero resource manager [78] also employs a similar system architecture.
Reference: [41] <author> Rainer Feldmann, Peter Mysliwietz, and Burkhard Monien. </author> <title> Studying overheads in massively parallel min/max-tree evaluation. </title> <booktitle> In Proceedings of the Sixth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 94-103, </pages> <address> Cape May, New Jersey, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: These researchers observed that heuristically, by having each processor work as if it is the only one (i.e., in serial, depth-first order) and having idle processors steal threads from others, space requirements and communication requirements should be curbed. Since then, many researchers have implemented variants on this strategy <ref> [41, 42, 44, 50, 67, 70, 77, 81, 84, 94, 103] </ref>. Cilk's work-stealing scheduler is very similar to the schedulers in some of these other systems, though Cilk's algorithm uses randomness and is provably efficient. Many multithreaded programming languages and runtime systems are based on heuristic 1.3.
Reference: [42] <author> Raphael Finkel and Udi Manber. </author> <title> DIBa distributed implementation of backtracking. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(2) </volume> <pages> 235-256, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: These researchers observed that heuristically, by having each processor work as if it is the only one (i.e., in serial, depth-first order) and having idle processors steal threads from others, space requirements and communication requirements should be curbed. Since then, many researchers have implemented variants on this strategy <ref> [41, 42, 44, 50, 67, 70, 77, 81, 84, 94, 103] </ref>. Cilk's work-stealing scheduler is very similar to the schedulers in some of these other systems, though Cilk's algorithm uses randomness and is provably efficient. Many multithreaded programming languages and runtime systems are based on heuristic 1.3. <p> Massively parallel supercomputers such as the Cray Research T3D or the Thinking Machines CM5, for example, either dedicate themselves to a single user at a time or gang-timeshare within fixed size partitions [72]. Systems such as Charm [91], the Parform [21], PVM/Hence [96], and others <ref> [29, 42, 44, 97] </ref> support parallel computing on a network of workstations. In these systems, the set of machines on which the program runs is chosen statically by the user. <p> First, to lower communication costs, we would like to steal large amounts of work, and in a tree-structured computation, shallow threads are likely to spawn more work than deep ones. This heuristic notion is the justification cited by earlier researchers <ref> [20, 42, 52, 77, 103] </ref> who proposed stealing work that is shallow in the spawn tree. We cannot, however, prove that shallow threads are more likely to spawn work than deep ones. What we prove in Section 5.3 is the following algorithmic property.
Reference: [43] <author> The MPI Forum. </author> <title> MPI: A message passing interface. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 878-883, </pages> <address> Portland, Oregon, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: We shall not look at data-parallel systems [8, 53] nor at systems focused 6 Chapter 1. Executing multithreaded programs efficiently on infrastructure such as distributed shared memory [4, 6, 29, 39, 59, 60, 66, 73, 87, 92, 93] or message passing <ref> [43, 96, 104, 105] </ref>. Substantial research has been reported in the theoretical literature concerning the scheduling of dynamic computations. In contrast to our research on multithreaded computations, however, other theoretical research has tended to ignore space requirements and communication costs. <p> We chose to build Cilk-NOW's communication protocols using an unreliable message-passing layer instead of a reliable one for two reasons. First, reliable layers such as TCP/IP [95], PVM [96], and MPI <ref> [43] </ref> all perform implicit acknowledgments and retries to achieve reliability. Therefore, such layers either preclude the use of split-phase communication or require extra buffering and copying.
Reference: [44] <author> Vincent W. Freeh, David K. Lowenthal, and Gregory R. Andrews. </author> <title> Distributed Filaments: Efficient fine-grain parallelism on a cluster of workstations. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 201-213, </pages> <address> Monterey, California, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: These researchers observed that heuristically, by having each processor work as if it is the only one (i.e., in serial, depth-first order) and having idle processors steal threads from others, space requirements and communication requirements should be curbed. Since then, many researchers have implemented variants on this strategy <ref> [41, 42, 44, 50, 67, 70, 77, 81, 84, 94, 103] </ref>. Cilk's work-stealing scheduler is very similar to the schedulers in some of these other systems, though Cilk's algorithm uses randomness and is provably efficient. Many multithreaded programming languages and runtime systems are based on heuristic 1.3. <p> Many multithreaded programming languages and runtime systems are based on heuristic 1.3. Contributions of this thesis 7 scheduling techniques. Though systems such as Charm [91], COOL [27, 28], Id [3, 37, 80], Olden [22], and others <ref> [29, 31, 38, 44, 54, 55, 63, 88, 98] </ref> are based on sound heuristics that seem to perform well in practice and generally have wider applicability than Cilk, none are able to provide any sort of performance guarantee or accurate machine-independent performance model. <p> Massively parallel supercomputers such as the Cray Research T3D or the Thinking Machines CM5, for example, either dedicate themselves to a single user at a time or gang-timeshare within fixed size partitions [72]. Systems such as Charm [91], the Parform [21], PVM/Hence [96], and others <ref> [29, 42, 44, 97] </ref> support parallel computing on a network of workstations. In these systems, the set of machines on which the program runs is chosen statically by the user.
Reference: [45] <author> Matteo Frigo, </author> <month> June </month> <year> 1995. </year> <title> Private communication. Bibliography 119 </title>
Reference-contexts: With p T 1 processors, we again have the space-per-processor growing proportional to p In more recent work, Frigo, using a somewhat more general model of multithreaded computation coupled with the same technique as used in the proof of Theorem 3.1, has obtained the following stronger result <ref> [45] </ref>.
Reference: [46] <author> David Gelernter and David Kaminsky. </author> <title> Supercomputing out of recycled garbage: Preliminary experience with Piranha. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <pages> pages 417-427, </pages> <address> Washington, D.C., </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Distributed operating systems [30, 82, 98, 99] and other systems [32, 40, 68, 74, 79, 110] provide transparent process placement and (in some cases) migration, but these systems are geared towards large serial programs or coarse-grain distributed programs. A system that does provide adaptive parallelism is Piranha <ref> [23, 46, 62] </ref>. (The creators of Piranha appear to have coined the term adaptive parallelism.) Based on Linda [24], Piranha's adaptive parallelism leverages structure in the Linda programming model much as Cilk-NOW leverages structure in the Cilk programming model.
Reference: [47] <author> Seth Copen Goldstein, Klaus Erik Schauser, and David Culler. </author> <title> Enabling primitives for compiling parallel languages. </title> <booktitle> In Third Workshop on Languages, Compilers, and Run-Time Systems for Scalable Computers, </booktitle> <address> Troy, New York, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: Each Cilk thread runs to completion without suspending and leaves the C runtime stack empty when it dies. A common alternative <ref> [22, 47, 52, 63, 77, 81] </ref> is to directly support spawn/return threads (or procedures) in the runtime system, possibly with stack-allocated activation frames. In such a system, threads can suspend waiting for synchronization and leave temporary values on the calling stack. <p> For example, the fib program executed over 17 million threads but migrated only 6170 (24:10 per processor) when run with 256 processors. Taking advantage of this property, other researchers <ref> [47, 63, 77] </ref> have developed techniques for implementing spawns such that when the child thread executes on the same processor as its parent, the cost of the spawn operation is roughly equal the cost of a function call. We hope to incorporate such techniques into future implementations of Cilk.
Reference: [48] <author> R. L. Graham. </author> <title> Bounds for certain multiprocessing anomalies. </title> <journal> The Bell System Technical Journal, </journal> <volume> 45 </volume> <pages> 1563-1581, </pages> <month> November </month> <year> 1966. </year>
Reference-contexts: And, of course, we also have the lower bound T P T . Early independent work by Brent [16, Lemma 2] and Graham <ref> [48, 49] </ref> yields the upper bound T P T 1 =P + T . <p> This observation was first made by Graham <ref> [48] </ref>. Second, the greedy-scheduling theorem tells us when we can obtain linear speedup, that is, when we can find a P-processor execution schedule X such that T (X ) = Q (T 1 =P).
Reference: [49] <author> R. L. Graham. </author> <title> Bounds on multiprocessing timing anomalies. </title> <journal> SIAM Journal on Applied Mathematics, </journal> <volume> 17(2) </volume> <pages> 416-429, </pages> <month> March </month> <year> 1969. </year>
Reference-contexts: And, of course, we also have the lower bound T P T . Early independent work by Brent [16, Lemma 2] and Graham <ref> [48, 49] </ref> yields the upper bound T P T 1 =P + T .
Reference: [50] <author> Michael Halbherr, Yuli Zhou, and Chris F. Joerg. </author> <title> MIMD-style parallel programming with continuation-passing threads. </title> <booktitle> In Proceedings of the 2nd International Workshop on Massive Parallelism: Hardware, Software, and Applications, </booktitle> <address> Capri, Italy, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: These researchers observed that heuristically, by having each processor work as if it is the only one (i.e., in serial, depth-first order) and having idle processors steal threads from others, space requirements and communication requirements should be curbed. Since then, many researchers have implemented variants on this strategy <ref> [41, 42, 44, 50, 67, 70, 77, 81, 84, 94, 103] </ref>. Cilk's work-stealing scheduler is very similar to the schedulers in some of these other systems, though Cilk's algorithm uses randomness and is provably efficient. Many multithreaded programming languages and runtime systems are based on heuristic 1.3.
Reference: [51] <author> Robert H. Halstead, Jr. </author> <title> Implementation of Multilisp: Lisp on a multiprocessor. </title> <booktitle> In Conference Record of the 1984 ACM Symposium on Lisp and Functional Programming, </booktitle> <pages> pages 9-17, </pages> <address> Austin, Texas, </address> <month> August </month> <year> 1984. </year>
Reference-contexts: The work-stealing idea dates back at least as far as Burton and Sleep's research [20] on parallel execution of functional programs and Halstead's implementation of Multilisp <ref> [51, 52] </ref>. These researchers observed that heuristically, by having each processor work as if it is the only one (i.e., in serial, depth-first order) and having idle processors steal threads from others, space requirements and communication requirements should be curbed.
Reference: [52] <author> Robert H. Halstead, Jr. </author> <title> Multilisp: A language for concurrent symbolic computation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(4) </volume> <pages> 501-538, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: The work-stealing idea dates back at least as far as Burton and Sleep's research [20] on parallel execution of functional programs and Halstead's implementation of Multilisp <ref> [51, 52] </ref>. These researchers observed that heuristically, by having each processor work as if it is the only one (i.e., in serial, depth-first order) and having idle processors steal threads from others, space requirements and communication requirements should be curbed. <p> Each Cilk thread runs to completion without suspending and leaves the C runtime stack empty when it dies. A common alternative <ref> [22, 47, 52, 63, 77, 81] </ref> is to directly support spawn/return threads (or procedures) in the runtime system, possibly with stack-allocated activation frames. In such a system, threads can suspend waiting for synchronization and leave temporary values on the calling stack. <p> First, to lower communication costs, we would like to steal large amounts of work, and in a tree-structured computation, shallow threads are likely to spawn more work than deep ones. This heuristic notion is the justification cited by earlier researchers <ref> [20, 42, 52, 77, 103] </ref> who proposed stealing work that is shallow in the spawn tree. We cannot, however, prove that shallow threads are more likely to spawn work than deep ones. What we prove in Section 5.3 is the following algorithmic property.
Reference: [53] <author> W. Hillis and G. Steele. </author> <title> Data parallel algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1170-1183, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: Likewise, the Cilk programming model and runtime systemincluding Cilk-NOWbuild on ideas found in earlier systems. In this section, we look at other theoretical results and systems that address scheduling issues for dynamic parallel computation. We shall not look at data-parallel systems <ref> [8, 53] </ref> nor at systems focused 6 Chapter 1. Executing multithreaded programs efficiently on infrastructure such as distributed shared memory [4, 6, 29, 39, 59, 60, 66, 73, 87, 92, 93] or message passing [43, 96, 104, 105].
Reference: [54] <author> Waldemar Horwat, Andrew A. Chien, and William J. Dally. </author> <title> Experience with CST: </title> <booktitle> Programming and implementation. In Proceedings of the ACM SIGPLAN '89 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 101-109, </pages> <address> Portland, Oregon, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Many multithreaded programming languages and runtime systems are based on heuristic 1.3. Contributions of this thesis 7 scheduling techniques. Though systems such as Charm [91], COOL [27, 28], Id [3, 37, 80], Olden [22], and others <ref> [29, 31, 38, 44, 54, 55, 63, 88, 98] </ref> are based on sound heuristics that seem to perform well in practice and generally have wider applicability than Cilk, none are able to provide any sort of performance guarantee or accurate machine-independent performance model.
Reference: [55] <author> Wilson C. Hsieh, Paul Wang, and William E. Weihl. </author> <title> Computation migration: Enhancing locality for distributed-memory parallel systems. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), </booktitle> <pages> pages 239-248, </pages> <address> San Diego, California, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Many multithreaded programming languages and runtime systems are based on heuristic 1.3. Contributions of this thesis 7 scheduling techniques. Though systems such as Charm [91], COOL [27, 28], Id [3, 37, 80], Olden [22], and others <ref> [29, 31, 38, 44, 54, 55, 63, 88, 98] </ref> are based on sound heuristics that seem to perform well in practice and generally have wider applicability than Cilk, none are able to provide any sort of performance guarantee or accurate machine-independent performance model.
Reference: [56] <author> Suresh Jagannathan and Jim Philbin. </author> <title> A customizable substrate for concurrent languages. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 55-67, </pages> <address> San Francisco, California, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Our results specialize to match theirs. Likewise, most of the systems-oriented work on multithreading has ignored the space issue. Notable exceptions include the k-bounded loops of Culler and Arvind [34, 35, 36] and the throttling mechanism of Ruggiero and Sargeant [90]. These techniques and others <ref> [56, 57] </ref> have met with some success, though none have any algorithmic foundation. In algorithmic work that considers space requirements or communication costs, most prior work has focused on cases like backtrack search with no synchronization or has focused on time and space to the exclusion of communication costs.
Reference: [57] <author> Suresh Jagannathan and Jim Philbin. </author> <title> A foundation for an efficient multi-threaded Scheme system. </title> <booktitle> In Proceedings of the 1992 ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pages 345-357, </pages> <address> San Francisco, California, </address> <month> June </month> <year> 1992. </year> <note> 120 Bibliography </note>
Reference-contexts: Our results specialize to match theirs. Likewise, most of the systems-oriented work on multithreading has ignored the space issue. Notable exceptions include the k-bounded loops of Culler and Arvind [34, 35, 36] and the throttling mechanism of Ruggiero and Sargeant [90]. These techniques and others <ref> [56, 57] </ref> have met with some success, though none have any algorithmic foundation. In algorithmic work that considers space requirements or communication costs, most prior work has focused on cases like backtrack search with no synchronization or has focused on time and space to the exclusion of communication costs.
Reference: [58] <author> Chris Joerg and Bradley C. Kuszmaul. </author> <title> Massively parallel chess. </title> <booktitle> In Proceedings of the Third DIMACS Parallel Implementation Challenge, </booktitle> <institution> Rutgers University, </institution> <address> New Jersey, </address> <month> October </month> <year> 1994. </year> <note> Available as ftp://theory.lcs.mit.edu/ pub/cilk/dimacs94.ps.Z. </note>
Reference-contexts: At each node of the tree, the program runs an empty for loop for 400 iterations. * ?Socrates is a parallel chess program that uses the Jamboree search algorithm <ref> [58, 70] </ref> to parallelize a minmax tree search.
Reference: [59] <author> Kirk L. Johnson, M. Frans Kaashoek, and Deborah A. Wallach. </author> <title> CRL: High-performance all-software distributed shared memory. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 213-228, </pages> <address> Copper Mountain Resort, Colorado, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: In this section, we look at other theoretical results and systems that address scheduling issues for dynamic parallel computation. We shall not look at data-parallel systems [8, 53] nor at systems focused 6 Chapter 1. Executing multithreaded programs efficiently on infrastructure such as distributed shared memory <ref> [4, 6, 29, 39, 59, 60, 66, 73, 87, 92, 93] </ref> or message passing [43, 96, 104, 105]. Substantial research has been reported in the theoretical literature concerning the scheduling of dynamic computations.
Reference: [60] <author> Eric Jul, Henry Levy, Norman Hutchinson, and Andrew Black. </author> <title> Fine-grained mobility in the Emerald system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6 </volume> <pages> 109-133, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: In this section, we look at other theoretical results and systems that address scheduling issues for dynamic parallel computation. We shall not look at data-parallel systems [8, 53] nor at systems focused 6 Chapter 1. Executing multithreaded programs efficiently on infrastructure such as distributed shared memory <ref> [4, 6, 29, 39, 59, 60, 66, 73, 87, 92, 93] </ref> or message passing [43, 96, 104, 105]. Substantial research has been reported in the theoretical literature concerning the scheduling of dynamic computations.
Reference: [61] <author> Christos Kaklamanis and Giuseppe Persiano. </author> <title> Branch-and-bound and backtrack search on mesh-connected arrays of processors. </title> <booktitle> In Proceedings of the Fourth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 118-126, </pages> <address> San Diego, California, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Related work in this area includes a randomized work-stealing algorithm for load balancing independent jobs [89]; algorithms for dynamically embedding trees in fixed-connection networks [5, 71]; and algorithms for backtrack search and branch-and-bound <ref> [61, 65, 75, 86, 109] </ref>.
Reference: [62] <author> David Louis Kaminsky. </author> <title> Adaptive Parallelism with Piranha. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Distributed operating systems [30, 82, 98, 99] and other systems [32, 40, 68, 74, 79, 110] provide transparent process placement and (in some cases) migration, but these systems are geared towards large serial programs or coarse-grain distributed programs. A system that does provide adaptive parallelism is Piranha <ref> [23, 46, 62] </ref>. (The creators of Piranha appear to have coined the term adaptive parallelism.) Based on Linda [24], Piranha's adaptive parallelism leverages structure in the Linda programming model much as Cilk-NOW leverages structure in the Cilk programming model.
Reference: [63] <author> Vijay Karamcheti and Andrew Chien. </author> <title> Concertefficient runtime support for concurrent object-oriented programming languages on stock hardware. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 598-607, </pages> <address> Portland, Oregon, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Many multithreaded programming languages and runtime systems are based on heuristic 1.3. Contributions of this thesis 7 scheduling techniques. Though systems such as Charm [91], COOL [27, 28], Id [3, 37, 80], Olden [22], and others <ref> [29, 31, 38, 44, 54, 55, 63, 88, 98] </ref> are based on sound heuristics that seem to perform well in practice and generally have wider applicability than Cilk, none are able to provide any sort of performance guarantee or accurate machine-independent performance model. <p> Each Cilk thread runs to completion without suspending and leaves the C runtime stack empty when it dies. A common alternative <ref> [22, 47, 52, 63, 77, 81] </ref> is to directly support spawn/return threads (or procedures) in the runtime system, possibly with stack-allocated activation frames. In such a system, threads can suspend waiting for synchronization and leave temporary values on the calling stack. <p> For example, the fib program executed over 17 million threads but migrated only 6170 (24:10 per processor) when run with 256 processors. Taking advantage of this property, other researchers <ref> [47, 63, 77] </ref> have developed techniques for implementing spawns such that when the child thread executes on the same processor as its parent, the cost of the spawn operation is roughly equal the cost of a function call. We hope to incorporate such techniques into future implementations of Cilk.
Reference: [64] <author> Richard M. Karp and Vijaya Ramachandran. </author> <title> Parallel algorithms for shared-memory machines. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science Volume A: Algorithms and Complexity, chapter 17, </booktitle> <pages> pages 869-941. </pages> <publisher> MIT Press, </publisher> <address> Cam-bridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: The use of work and critical path length to analyze parallel algorithms and model application performance is also not new. Work and critical path have been used in the theory community for years to analyze parallel algorithms <ref> [64] </ref>. Blelloch [8] has developed a performance model for data-parallel computations based on these same two abstract measures. He cites many advantages to such a model over machine-based models. Cilk provides a similar performance model for the domain of multithreaded computation.
Reference: [65] <author> Richard M. Karp and Yanjun Zhang. </author> <title> Randomized parallel algorithms for backtrack search and branch-and-bound computation. </title> <journal> Journal of the ACM, </journal> <volume> 40(3) </volume> <pages> 765-789, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Related work in this area includes a randomized work-stealing algorithm for load balancing independent jobs [89]; algorithms for dynamically embedding trees in fixed-connection networks [5, 71]; and algorithms for backtrack search and branch-and-bound <ref> [61, 65, 75, 86, 109] </ref>. <p> Backtrack search can be viewed as a multithreaded computation with no synchronization, and in the work just cited, the only algorithm with reasonable space bounds is the random work-stealing algorithm of Karp and Zhang <ref> [65, 109] </ref>, though they did not make this observation until the later work of Zhang and Ortynski [108]. Our results specialize to match theirs. Likewise, most of the systems-oriented work on multithreading has ignored the space issue. <p> Our analysis assumes that concurrent accesses to the same data structure are serially queued by an adversary, as in the atomic message-passing model of [75]. This assumption is more stringent than that in the model of Karp and Zhang <ref> [65] </ref>. They assume that if concurrent steal requests are made to a deque, in one time step, one request is satisfied and all the others are denied.
Reference: [66] <author> Pete Keleher, Alan L. Cox, Sandhya Dwarkadas, and Willy Zwaenepoel. </author> <title> Tread-Marks: Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In USENIX Winter 1994 Conference Proceedings, </booktitle> <pages> pages 115-132, </pages> <address> San Francisco, California, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: In this section, we look at other theoretical results and systems that address scheduling issues for dynamic parallel computation. We shall not look at data-parallel systems [8, 53] nor at systems focused 6 Chapter 1. Executing multithreaded programs efficiently on infrastructure such as distributed shared memory <ref> [4, 6, 29, 39, 59, 60, 66, 73, 87, 92, 93] </ref> or message passing [43, 96, 104, 105]. Substantial research has been reported in the theoretical literature concerning the scheduling of dynamic computations.
Reference: [67] <author> David A. Kranz, Robert H. Halstead, Jr., and Eric Mohr. Mul-T: </author> <title> A high-performance parallel Lisp. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 81-90, </pages> <address> Portland, Oregon, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: These researchers observed that heuristically, by having each processor work as if it is the only one (i.e., in serial, depth-first order) and having idle processors steal threads from others, space requirements and communication requirements should be curbed. Since then, many researchers have implemented variants on this strategy <ref> [41, 42, 44, 50, 67, 70, 77, 81, 84, 94, 103] </ref>. Cilk's work-stealing scheduler is very similar to the schedulers in some of these other systems, though Cilk's algorithm uses randomness and is provably efficient. Many multithreaded programming languages and runtime systems are based on heuristic 1.3.
Reference: [68] <author> Phillip Krueger and Rohit Chawla. </author> <title> The Stealth distributed scheduler. </title> <booktitle> In Proceedings of the 11th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 336-343, </pages> <address> Arlington, Texas, </address> <month> May </month> <year> 1991. </year> <note> Bibliography 121 </note>
Reference-contexts: In these systems, the set of machines on which the program runs is chosen statically by the user. Distributed operating systems [30, 82, 98, 99] and other systems <ref> [32, 40, 68, 74, 79, 110] </ref> provide transparent process placement and (in some cases) migration, but these systems are geared towards large serial programs or coarse-grain distributed programs.
Reference: [69] <author> Jeffrey Kuskin, David Ofelt, Mark Heinrich, John Heinlein, Richard Simoni, Kourosh Gharachorloo, John Chapin, David Nakahira, Joel Baxter, Mark Horowitz, Anoop Gupta, Mendel Rosenblum, and John Hennessy. </author> <title> The Stanford Flash multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <address> Chicago, Illinois, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: The BBL system architecture is closely related to Cilk-NOW's. The Prospero resource manager [78] also employs a similar system architecture. Runtime systems for the programming language COOL [28] running on symmetric multiprocessors [101, 102] and cache-coherent, distributed, shared-memory machines <ref> [26, 69] </ref> use process control to support adaptive parallelism. These systems rely on special-purpose operating system and hardware support. In contrast, Cilk-NOW supports adaptive parallelism entirely in user-level software on top of commercial hardware and operating systems.
Reference: [70] <author> Bradley C. Kuszmaul. </author> <title> Synchronized MIMD Computing. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> May </month> <year> 1994. </year> <note> Available as MIT Laboratory for Computer Science Technical Report MIT/LCS/TR-645 or ftp://theory.lcs.mit.edu/pub/bradley/ phd.ps.Z. </note>
Reference-contexts: These researchers observed that heuristically, by having each processor work as if it is the only one (i.e., in serial, depth-first order) and having idle processors steal threads from others, space requirements and communication requirements should be curbed. Since then, many researchers have implemented variants on this strategy <ref> [41, 42, 44, 50, 67, 70, 77, 81, 84, 94, 103] </ref>. Cilk's work-stealing scheduler is very similar to the schedulers in some of these other systems, though Cilk's algorithm uses randomness and is provably efficient. Many multithreaded programming languages and runtime systems are based on heuristic 1.3. <p> At each node of the tree, the program runs an empty for loop for 400 iterations. * ?Socrates is a parallel chess program that uses the Jamboree search algorithm <ref> [58, 70] </ref> to parallelize a minmax tree search.
Reference: [71] <author> Tom Leighton, Mark Newman, Abhiram G. Ranade, and Eric Schwabe. </author> <title> Dynamic tree embeddings in butterflies and hypercubes. </title> <booktitle> In Proceedings of the 1989 ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 224-234, </pages> <address> Santa Fe, New Mexico, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: In contrast to our research on multithreaded computations, however, other theoretical research has tended to ignore space requirements and communication costs. Related work in this area includes a randomized work-stealing algorithm for load balancing independent jobs [89]; algorithms for dynamically embedding trees in fixed-connection networks <ref> [5, 71] </ref>; and algorithms for backtrack search and branch-and-bound [61, 65, 75, 86, 109].
Reference: [72] <author> Charles E. Leiserson, Zahi S. Abuhamdeh, David C. Douglas, Carl R. Feynman, Ma-hesh N. Ganmukhi, Jeffrey V. Hill, W. Daniel Hillis, Bradley C. Kuszmaul, Margaret A. St. Pierre, David S. Wells, Monica C. Wong, Shaw-Wen Yang, and Robert Zak. </author> <title> The network architecture of the Connection Machine CM-5. </title> <booktitle> In Proceedings of the Fourth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 272-285, </pages> <address> San Diego, California, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Massively parallel supercomputers such as the Cray Research T3D or the Thinking Machines CM5, for example, either dedicate themselves to a single user at a time or gang-timeshare within fixed size partitions <ref> [72] </ref>. Systems such as Charm [91], the Parform [21], PVM/Hence [96], and others [29, 42, 44, 97] support parallel computing on a network of workstations. In these systems, the set of machines on which the program runs is chosen statically by the user. <p> The whiter the pixel, the longer ray worked to compute the corresponding pixel value. All experiments were run on a CM5 supercomputer. The CM5 is a massively parallel computer based on 32MHz SPARC processors with a fat-tree interconnection network <ref> [72] </ref>. The Cilk runtime system on the CM5 performs communication among processors using the Strata [17] active-message library. 5.2.2 Application performance By running our applications and measuring a suite of performance parameters, we empirically answer a number of questions about the effectiveness of the Cilk runtime system.
Reference: [73] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: In this section, we look at other theoretical results and systems that address scheduling issues for dynamic parallel computation. We shall not look at data-parallel systems [8, 53] nor at systems focused 6 Chapter 1. Executing multithreaded programs efficiently on infrastructure such as distributed shared memory <ref> [4, 6, 29, 39, 59, 60, 66, 73, 87, 92, 93] </ref> or message passing [43, 96, 104, 105]. Substantial research has been reported in the theoretical literature concerning the scheduling of dynamic computations.
Reference: [74] <author> Michael J. Litzkow, Miron Livny, and Matt W. </author> <title> Mutka. Condora hunter of idle workstations. </title> <booktitle> In Proceedings of the 8th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 104-111, </pages> <address> San Jose, California, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: In these systems, the set of machines on which the program runs is chosen statically by the user. Distributed operating systems [30, 82, 98, 99] and other systems <ref> [32, 40, 68, 74, 79, 110] </ref> provide transparent process placement and (in some cases) migration, but these systems are geared towards large serial programs or coarse-grain distributed programs.
Reference: [75] <author> Pangfeng Liu, William Aiello, and Sandeep Bhatt. </author> <title> An atomic model for message-passing. </title> <booktitle> In Proceedings of the Fifth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 154-163, </pages> <address> Velen, Germany, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Related work in this area includes a randomized work-stealing algorithm for load balancing independent jobs [89]; algorithms for dynamically embedding trees in fixed-connection networks [5, 71]; and algorithms for backtrack search and branch-and-bound <ref> [61, 65, 75, 86, 109] </ref>. <p> We assume that the machine is an asynchronous parallel computer with P processors, and its memory can be either distributed or shared. Our analysis assumes that concurrent accesses to the same data structure are serially queued by an adversary, as in the atomic message-passing model of <ref> [75] </ref>. This assumption is more stringent than that in the model of Karp and Zhang [65]. They assume that if concurrent steal requests are made to a deque, in one time step, one request is satisfied and all the others are denied.
Reference: [76] <author> Robert C. Miller. </author> <title> A type-checking preprocessor for Cilk 2, a multithreaded C language. </title> <type> Master's thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: Some of the material in this chapter was previously published in [12]. 53 54 Chapter 5. Parallel programming in Cilk processed to C using the cilk2c translator 1 <ref> [76] </ref> and then compiled and linked with a run-time library to run on the target platform. Currently supported targets include the Connection Machine CM5 MPP, the Intel Paragon MPP, the Sun SparcStation SMP, the Silicon Graphics Power Challenge SMP, and the Cilk-NOW network of workstations (Chapter 6).
Reference: [77] <author> Eric Mohr, David A. Kranz, and Robert H. Halstead, Jr. </author> <title> Lazy task creation: A technique for increasing the granularity of parallel programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 264-280, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: These researchers observed that heuristically, by having each processor work as if it is the only one (i.e., in serial, depth-first order) and having idle processors steal threads from others, space requirements and communication requirements should be curbed. Since then, many researchers have implemented variants on this strategy <ref> [41, 42, 44, 50, 67, 70, 77, 81, 84, 94, 103] </ref>. Cilk's work-stealing scheduler is very similar to the schedulers in some of these other systems, though Cilk's algorithm uses randomness and is provably efficient. Many multithreaded programming languages and runtime systems are based on heuristic 1.3. <p> Each Cilk thread runs to completion without suspending and leaves the C runtime stack empty when it dies. A common alternative <ref> [22, 47, 52, 63, 77, 81] </ref> is to directly support spawn/return threads (or procedures) in the runtime system, possibly with stack-allocated activation frames. In such a system, threads can suspend waiting for synchronization and leave temporary values on the calling stack. <p> First, to lower communication costs, we would like to steal large amounts of work, and in a tree-structured computation, shallow threads are likely to spawn more work than deep ones. This heuristic notion is the justification cited by earlier researchers <ref> [20, 42, 52, 77, 103] </ref> who proposed stealing work that is shallow in the spawn tree. We cannot, however, prove that shallow threads are more likely to spawn work than deep ones. What we prove in Section 5.3 is the following algorithmic property. <p> For example, the fib program executed over 17 million threads but migrated only 6170 (24:10 per processor) when run with 256 processors. Taking advantage of this property, other researchers <ref> [47, 63, 77] </ref> have developed techniques for implementing spawns such that when the child thread executes on the same processor as its parent, the cost of the spawn operation is roughly equal the cost of a function call. We hope to incorporate such techniques into future implementations of Cilk.
Reference: [78] <author> B. Clifford Neuman and Santosh Rao. </author> <title> The Prospero resource manager: A scalable framework for processor allocation in distributed systems. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 6(4) </volume> <pages> 339-355, </pages> <month> June </month> <year> 1994. </year> <note> 122 Bibliography </note>
Reference-contexts: Adaptive parallelism is also present in the Benevolent Bandit Laboratory (BBL) [40], a PC-based system. The BBL system architecture is closely related to Cilk-NOW's. The Prospero resource manager <ref> [78] </ref> also employs a similar system architecture. Runtime systems for the programming language COOL [28] running on symmetric multiprocessors [101, 102] and cache-coherent, distributed, shared-memory machines [26, 69] use process control to support adaptive parallelism. These systems rely on special-purpose operating system and hardware support.
Reference: [79] <author> David A. Nichols. </author> <title> Using idle workstations in a shared computing environment. </title> <booktitle> In Proceedings of the Eleventh ACM Symposium on Operating Systems Principles (SOSP 11), </booktitle> <pages> pages 5-12, </pages> <address> Austin, Texas, </address> <month> November </month> <year> 1987. </year>
Reference-contexts: In these systems, the set of machines on which the program runs is chosen statically by the user. Distributed operating systems [30, 82, 98, 99] and other systems <ref> [32, 40, 68, 74, 79, 110] </ref> provide transparent process placement and (in some cases) migration, but these systems are geared towards large serial programs or coarse-grain distributed programs.
Reference: [80] <author> Rishiyur S. Nikhil. </author> <title> A multithreaded implementation of Id using P-RISC graphs. </title> <booktitle> In Proceedings of the Sixth Annual Workshop on Languages and Compilers for Parallel Computing, number 768 in Lecture Notes in Computer Science, </booktitle> <pages> pages 390-405, </pages> <address> Portland, Oregon, </address> <month> August </month> <year> 1993. </year> <note> Springer-Verlag. </note>
Reference-contexts: Many multithreaded programming languages and runtime systems are based on heuristic 1.3. Contributions of this thesis 7 scheduling techniques. Though systems such as Charm [91], COOL [27, 28], Id <ref> [3, 37, 80] </ref>, Olden [22], and others [29, 31, 38, 44, 54, 55, 63, 88, 98] are based on sound heuristics that seem to perform well in practice and generally have wider applicability than Cilk, none are able to provide any sort of performance guarantee or accurate machine-independent performance model.
Reference: [81] <author> Rishiyur S. Nikhil. Cid: </author> <title> A parallel, shared-memory C for distributed-memory machines. </title> <booktitle> In Proceedings of the Seventh Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: These researchers observed that heuristically, by having each processor work as if it is the only one (i.e., in serial, depth-first order) and having idle processors steal threads from others, space requirements and communication requirements should be curbed. Since then, many researchers have implemented variants on this strategy <ref> [41, 42, 44, 50, 67, 70, 77, 81, 84, 94, 103] </ref>. Cilk's work-stealing scheduler is very similar to the schedulers in some of these other systems, though Cilk's algorithm uses randomness and is provably efficient. Many multithreaded programming languages and runtime systems are based on heuristic 1.3. <p> Each Cilk thread runs to completion without suspending and leaves the C runtime stack empty when it dies. A common alternative <ref> [22, 47, 52, 63, 77, 81] </ref> is to directly support spawn/return threads (or procedures) in the runtime system, possibly with stack-allocated activation frames. In such a system, threads can suspend waiting for synchronization and leave temporary values on the calling stack.
Reference: [82] <author> John K. Ousterhout, Andrew R. Cherenson, Frederick Douglis, Michael N. Nel-son, and Brent B. Welch. </author> <title> The Sprite network operating system. </title> <journal> IEEE Computer, </journal> <volume> 21(2) </volume> <pages> 23-36, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Systems such as Charm [91], the Parform [21], PVM/Hence [96], and others [29, 42, 44, 97] support parallel computing on a network of workstations. In these systems, the set of machines on which the program runs is chosen statically by the user. Distributed operating systems <ref> [30, 82, 98, 99] </ref> and other systems [32, 40, 68, 74, 79, 110] provide transparent process placement and (in some cases) migration, but these systems are geared towards large serial programs or coarse-grain distributed programs.
Reference: [83] <author> Vijay S. Pande, Christopher F. Joerg, Alexander Yu Grosberg, and Toyoichi Tanaka. </author> <title> Enumerations of the hamiltonian walks on a cubic sublattice. </title> <journal> Journal of Physics A, </journal> <volume> 27, </volume> <year> 1994. </year>
Reference-contexts: The Cilk program is based on serial code by R. Sargent of MIT's Media Laboratory. Thread length was enhanced by serializing the bottom 7 levels of the search tree. * pfold is a protein-folding program that finds hamiltonian paths in a three-dimensional grid using backtrack search <ref> [83] </ref>. Written by Chris Joerg of MIT's Laboratory for Computer Science and V. Pande of MIT's Center for Material Sciences and Engineering, pfold was the first program to enumerate all hamiltonian paths in a 3 fi 4 fi 4 grid.
Reference: [84] <author> Joseph D. Pehoushek and Joseph S. Weening. </author> <title> Low-cost process creation and dynamic partitioning in Qlisp. </title> <booktitle> In Parallel Lisp: Languages and Systems. Proceedings of the US/Japan Workshop, number 441 in Lecture Notes in Computer Science, </booktitle> <pages> pages 182-199, </pages> <address> Sendai, Japan, June 1989. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: These researchers observed that heuristically, by having each processor work as if it is the only one (i.e., in serial, depth-first order) and having idle processors steal threads from others, space requirements and communication requirements should be curbed. Since then, many researchers have implemented variants on this strategy <ref> [41, 42, 44, 50, 67, 70, 77, 81, 84, 94, 103] </ref>. Cilk's work-stealing scheduler is very similar to the schedulers in some of these other systems, though Cilk's algorithm uses randomness and is provably efficient. Many multithreaded programming languages and runtime systems are based on heuristic 1.3.
Reference: [85] <author> C. Gregory Plaxton, </author> <month> August </month> <year> 1994. </year> <title> Private communication. </title>
Reference-contexts: Thus, we have d = P r=2 1 Greg Plaxton of the University of Texas, Austin has improved this bound to O (M) for the case when 1=e is at most polynomial in M and P <ref> [85] </ref>. 44 Chapter 4. Work stealing We now prove an important property of these indicator random variables. Consider any set S of pairs (i; r), each of which corresponds to the event that the ith cycle of ball 1 is delayed by ball r. <p> Communication also occurs whenever a dependency edge enters a parent thread from one of its children and the parent has been stolen, but since each dependency edge accounts for at most a constant number of 2 With Plaxton's bound <ref> [85] </ref> for Lemma 4.3, this bound becomes T P = O (T 1 =P + T ), whenever 1=e is at most polynomial in M and P. 52 Chapter 4. Work stealing bytes, the communication incurred is at most O (n d ) per steal.
Reference: [86] <author> Abhiram Ranade. </author> <title> Optimal speedup for backtrack search on a butterfly network. </title> <booktitle> In Proceedings of the Third Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 40-48, </pages> <address> Hilton Head, South Carolina, </address> <month> July </month> <year> 1991. </year>
Reference-contexts: Related work in this area includes a randomized work-stealing algorithm for load balancing independent jobs [89]; algorithms for dynamically embedding trees in fixed-connection networks [5, 71]; and algorithms for backtrack search and branch-and-bound <ref> [61, 65, 75, 86, 109] </ref>.
Reference: [87] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-level shared memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325-336, </pages> <address> Chicago, Illinois, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: In this section, we look at other theoretical results and systems that address scheduling issues for dynamic parallel computation. We shall not look at data-parallel systems [8, 53] nor at systems focused 6 Chapter 1. Executing multithreaded programs efficiently on infrastructure such as distributed shared memory <ref> [4, 6, 29, 39, 59, 60, 66, 73, 87, 92, 93] </ref> or message passing [43, 96, 104, 105]. Substantial research has been reported in the theoretical literature concerning the scheduling of dynamic computations.
Reference: [88] <author> Martin C. Rinard, Daniel J. Scales, and Monica S. Lam. </author> <title> Jade: A high-level, machine-independent language for parallel programming. </title> <journal> Computer, </journal> <volume> 26(6) </volume> <pages> 28-38, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Many multithreaded programming languages and runtime systems are based on heuristic 1.3. Contributions of this thesis 7 scheduling techniques. Though systems such as Charm [91], COOL [27, 28], Id [3, 37, 80], Olden [22], and others <ref> [29, 31, 38, 44, 54, 55, 63, 88, 98] </ref> are based on sound heuristics that seem to perform well in practice and generally have wider applicability than Cilk, none are able to provide any sort of performance guarantee or accurate machine-independent performance model.
Reference: [89] <author> Larry Rudolph, Miriam Slivkin-Allalouf, and Eli Upfal. </author> <title> A simple load balancing scheme for task allocation in parallel machines. </title> <booktitle> In Proceedings of the Third Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 237-245, </pages> <address> Hilton Head, South Carolina, </address> <month> July </month> <year> 1991. </year>
Reference-contexts: In contrast to our research on multithreaded computations, however, other theoretical research has tended to ignore space requirements and communication costs. Related work in this area includes a randomized work-stealing algorithm for load balancing independent jobs <ref> [89] </ref>; algorithms for dynamically embedding trees in fixed-connection networks [5, 71]; and algorithms for backtrack search and branch-and-bound [61, 65, 75, 86, 109].
Reference: [90] <author> Carlos A. Ruggiero and John Sargeant. </author> <title> Control of parallelism in the Manchester dataflow machine. </title> <booktitle> In Functional Programming Languages and Computer Architecture, number 274 in Lecture Notes in Computer Science, </booktitle> <pages> pages 1-15. </pages> <publisher> Springer-Verlag, </publisher> <year> 1987. </year> <note> Bibliography 123 </note>
Reference-contexts: Our results specialize to match theirs. Likewise, most of the systems-oriented work on multithreading has ignored the space issue. Notable exceptions include the k-bounded loops of Culler and Arvind [34, 35, 36] and the throttling mechanism of Ruggiero and Sargeant <ref> [90] </ref>. These techniques and others [56, 57] have met with some success, though none have any algorithmic foundation.
Reference: [91] <author> Vikram A. Saletore, J. Jacob, and M. Padala. </author> <title> Parallel computations on the CHARM heterogeneous workstation cluster. </title> <booktitle> In Proceedings of the Third International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 203-210, </pages> <address> San Francisco, California, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Cilk's work-stealing scheduler is very similar to the schedulers in some of these other systems, though Cilk's algorithm uses randomness and is provably efficient. Many multithreaded programming languages and runtime systems are based on heuristic 1.3. Contributions of this thesis 7 scheduling techniques. Though systems such as Charm <ref> [91] </ref>, COOL [27, 28], Id [3, 37, 80], Olden [22], and others [29, 31, 38, 44, 54, 55, 63, 88, 98] are based on sound heuristics that seem to perform well in practice and generally have wider applicability than Cilk, none are able to provide any sort of performance guarantee or <p> Massively parallel supercomputers such as the Cray Research T3D or the Thinking Machines CM5, for example, either dedicate themselves to a single user at a time or gang-timeshare within fixed size partitions [72]. Systems such as Charm <ref> [91] </ref>, the Parform [21], PVM/Hence [96], and others [29, 42, 44, 97] support parallel computing on a network of workstations. In these systems, the set of machines on which the program runs is chosen statically by the user.
Reference: [92] <author> Daniel J. Scales and Monica S. Lam. </author> <title> The design and evaluation of a shared object system for distributed memory machines. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 101-114, </pages> <address> Monterey, Califor-nia, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: In this section, we look at other theoretical results and systems that address scheduling issues for dynamic parallel computation. We shall not look at data-parallel systems [8, 53] nor at systems focused 6 Chapter 1. Executing multithreaded programs efficiently on infrastructure such as distributed shared memory <ref> [4, 6, 29, 39, 59, 60, 66, 73, 87, 92, 93] </ref> or message passing [43, 96, 104, 105]. Substantial research has been reported in the theoretical literature concerning the scheduling of dynamic computations.
Reference: [93] <author> Daniel J. Scales and Monica S. Lam. </author> <title> An efficient shared memory system for distributed memory machines. </title> <type> Technical Report CSL-TR-94-627, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: In this section, we look at other theoretical results and systems that address scheduling issues for dynamic parallel computation. We shall not look at data-parallel systems [8, 53] nor at systems focused 6 Chapter 1. Executing multithreaded programs efficiently on infrastructure such as distributed shared memory <ref> [4, 6, 29, 39, 59, 60, 66, 73, 87, 92, 93] </ref> or message passing [43, 96, 104, 105]. Substantial research has been reported in the theoretical literature concerning the scheduling of dynamic computations.
Reference: [94] <author> Jaswinder Pal Singh, Anoop Gupta, and Marc Levoy. </author> <title> Parallel visualization algorithms: Performance and architectural implications. </title> <journal> IEEE Computer, </journal> <volume> 27(7) </volume> <pages> 45-55, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: These researchers observed that heuristically, by having each processor work as if it is the only one (i.e., in serial, depth-first order) and having idle processors steal threads from others, space requirements and communication requirements should be curbed. Since then, many researchers have implemented variants on this strategy <ref> [41, 42, 44, 50, 67, 70, 77, 81, 84, 94, 103] </ref>. Cilk's work-stealing scheduler is very similar to the schedulers in some of these other systems, though Cilk's algorithm uses randomness and is provably efficient. Many multithreaded programming languages and runtime systems are based on heuristic 1.3.
Reference: [95] <author> W. Richard Stevens. </author> <title> UNIX Network Programming. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1990. </year>
Reference-contexts: Beyond this level and in a wide-area network, we may require multiple clearinghouses configured in a hierarchy. All of the communication between workers and between workers and the clearinghouse is implemented with UDP/IP <ref> [95] </ref>. UDP/IP is an unreliable datagram protocol built on top of the internet protocol [25]. The protocols implemented in the Cilk-NOW runtime system all use UDP/IP to perform split-phase communication, so except in the case of work stealing, a worker never sits idle waiting for a reply or an acknowledgment. <p> We chose to build Cilk-NOW's communication protocols using an unreliable message-passing layer instead of a reliable one for two reasons. First, reliable layers such as TCP/IP <ref> [95] </ref>, PVM [96], and MPI [43] all perform implicit acknowledgments and retries to achieve reliability. Therefore, such layers either preclude the use of split-phase communication or require extra buffering and copying.
Reference: [96] <author> V. S. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concur-rency: Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: We shall not look at data-parallel systems [8, 53] nor at systems focused 6 Chapter 1. Executing multithreaded programs efficiently on infrastructure such as distributed shared memory [4, 6, 29, 39, 59, 60, 66, 73, 87, 92, 93] or message passing <ref> [43, 96, 104, 105] </ref>. Substantial research has been reported in the theoretical literature concerning the scheduling of dynamic computations. In contrast to our research on multithreaded computations, however, other theoretical research has tended to ignore space requirements and communication costs. <p> Massively parallel supercomputers such as the Cray Research T3D or the Thinking Machines CM5, for example, either dedicate themselves to a single user at a time or gang-timeshare within fixed size partitions [72]. Systems such as Charm [91], the Parform [21], PVM/Hence <ref> [96] </ref>, and others [29, 42, 44, 97] support parallel computing on a network of workstations. In these systems, the set of machines on which the program runs is chosen statically by the user. <p> We chose to build Cilk-NOW's communication protocols using an unreliable message-passing layer instead of a reliable one for two reasons. First, reliable layers such as TCP/IP [95], PVM <ref> [96] </ref>, and MPI [43] all perform implicit acknowledgments and retries to achieve reliability. Therefore, such layers either preclude the use of split-phase communication or require extra buffering and copying.
Reference: [97] <author> V. S. Sunderam and Vernon J. Rego. </author> <title> EcliPSe: A system for high performance concurrent simulation. </title> <journal> SoftwarePractice and Experience, </journal> <volume> 21(11) </volume> <pages> 1189-1219, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Massively parallel supercomputers such as the Cray Research T3D or the Thinking Machines CM5, for example, either dedicate themselves to a single user at a time or gang-timeshare within fixed size partitions [72]. Systems such as Charm [91], the Parform [21], PVM/Hence [96], and others <ref> [29, 42, 44, 97] </ref> support parallel computing on a network of workstations. In these systems, the set of machines on which the program runs is chosen statically by the user.
Reference: [98] <author> Andrew S. Tanenbaum, Henri E. Bal, and M. Frans Kaashoek. </author> <title> Programming a distributed system using shared objects. </title> <booktitle> In Proceedings of the Second International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 5-12, </pages> <address> Spokane, Washington, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: Many multithreaded programming languages and runtime systems are based on heuristic 1.3. Contributions of this thesis 7 scheduling techniques. Though systems such as Charm [91], COOL [27, 28], Id [3, 37, 80], Olden [22], and others <ref> [29, 31, 38, 44, 54, 55, 63, 88, 98] </ref> are based on sound heuristics that seem to perform well in practice and generally have wider applicability than Cilk, none are able to provide any sort of performance guarantee or accurate machine-independent performance model. <p> Systems such as Charm [91], the Parform [21], PVM/Hence [96], and others [29, 42, 44, 97] support parallel computing on a network of workstations. In these systems, the set of machines on which the program runs is chosen statically by the user. Distributed operating systems <ref> [30, 82, 98, 99] </ref> and other systems [32, 40, 68, 74, 79, 110] provide transparent process placement and (in some cases) migration, but these systems are geared towards large serial programs or coarse-grain distributed programs.
Reference: [99] <author> Andrew S. Tanenbaum, Robbert van Renesse, Hans van Staveren, Gregory J. Sharp, Sape J. Mullender, Jack Jansen, and Guido van Rossum. </author> <title> Experiences with the Amoeba distributed operating system. </title> <journal> Communications of the ACM, </journal> <volume> 33(12) </volume> <pages> 46-63, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: Systems such as Charm [91], the Parform [21], PVM/Hence [96], and others [29, 42, 44, 97] support parallel computing on a network of workstations. In these systems, the set of machines on which the program runs is chosen statically by the user. Distributed operating systems <ref> [30, 82, 98, 99] </ref> and other systems [32, 40, 68, 74, 79, 110] provide transparent process placement and (in some cases) migration, but these systems are geared towards large serial programs or coarse-grain distributed programs.
Reference: [100] <author> Kenneth R. Traub. </author> <title> Implementation of Non-Strict Functional Programming Languages. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1991. </year>
Reference-contexts: Thus, in a strict computation, no function can be invoked until all of its arguments have been evaluated, although the arguments can be evaluated in parallel. In other words, a strict computation requires a strict evaluation order (as opposed to a lenient evaluation order) <ref> [100] </ref>. In later chapters we will also consider fully strict multithreaded computations. In a fully strict computation, every dependency goes from a thread to its parent. The strict computation in Figure 3.1 (b) is also fully strict.
Reference: [101] <author> Andrew Tucker. </author> <title> Efficient Scheduling on Multiprogrammed Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: Adaptive parallelism is also present in the Benevolent Bandit Laboratory (BBL) [40], a PC-based system. The BBL system architecture is closely related to Cilk-NOW's. The Prospero resource manager [78] also employs a similar system architecture. Runtime systems for the programming language COOL [28] running on symmetric multiprocessors <ref> [101, 102] </ref> and cache-coherent, distributed, shared-memory machines [26, 69] use process control to support adaptive parallelism. These systems rely on special-purpose operating system and hardware support. In contrast, Cilk-NOW supports adaptive parallelism entirely in user-level software on top of commercial hardware and operating systems.
Reference: [102] <author> Andrew Tucker and Anoop Gupta. </author> <title> Process control and scheduling issues for mul-tiprogrammed shared-memory multiprocessors. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles (SOSP 12), </booktitle> <pages> pages 159-166, </pages> <address> Litchfield Park, Arizona, </address> <month> December </month> <year> 1989. </year> <note> 124 Bibliography </note>
Reference-contexts: Adaptive parallelism is also present in the Benevolent Bandit Laboratory (BBL) [40], a PC-based system. The BBL system architecture is closely related to Cilk-NOW's. The Prospero resource manager [78] also employs a similar system architecture. Runtime systems for the programming language COOL [28] running on symmetric multiprocessors <ref> [101, 102] </ref> and cache-coherent, distributed, shared-memory machines [26, 69] use process control to support adaptive parallelism. These systems rely on special-purpose operating system and hardware support. In contrast, Cilk-NOW supports adaptive parallelism entirely in user-level software on top of commercial hardware and operating systems.
Reference: [103] <author> Mark T. Vandevoorde and Eric S. Roberts. WorkCrews: </author> <title> An abstraction for controlling parallelism. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 17(4) </volume> <pages> 347-366, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: These researchers observed that heuristically, by having each processor work as if it is the only one (i.e., in serial, depth-first order) and having idle processors steal threads from others, space requirements and communication requirements should be curbed. Since then, many researchers have implemented variants on this strategy <ref> [41, 42, 44, 50, 67, 70, 77, 81, 84, 94, 103] </ref>. Cilk's work-stealing scheduler is very similar to the schedulers in some of these other systems, though Cilk's algorithm uses randomness and is provably efficient. Many multithreaded programming languages and runtime systems are based on heuristic 1.3. <p> First, to lower communication costs, we would like to steal large amounts of work, and in a tree-structured computation, shallow threads are likely to spawn more work than deep ones. This heuristic notion is the justification cited by earlier researchers <ref> [20, 42, 52, 77, 103] </ref> who proposed stealing work that is shallow in the spawn tree. We cannot, however, prove that shallow threads are more likely to spawn work than deep ones. What we prove in Section 5.3 is the following algorithmic property.
Reference: [104] <author> Thorsten von Eicken, Anindya Basu, </author> <title> and Vineet Buch. Low-latency communication over ATM networks using active messages. </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 46-53, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: We shall not look at data-parallel systems [8, 53] nor at systems focused 6 Chapter 1. Executing multithreaded programs efficiently on infrastructure such as distributed shared memory [4, 6, 29, 39, 59, 60, 66, 73, 87, 92, 93] or message passing <ref> [43, 96, 104, 105] </ref>. Substantial research has been reported in the theoretical literature concerning the scheduling of dynamic computations. In contrast to our research on multithreaded computations, however, other theoretical research has tended to ignore space requirements and communication costs.
Reference: [105] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: We shall not look at data-parallel systems [8, 53] nor at systems focused 6 Chapter 1. Executing multithreaded programs efficiently on infrastructure such as distributed shared memory [4, 6, 29, 39, 59, 60, 66, 73, 87, 92, 93] or message passing <ref> [43, 96, 104, 105] </ref>. Substantial research has been reported in the theoretical literature concerning the scheduling of dynamic computations. In contrast to our research on multithreaded computations, however, other theoretical research has tended to ignore space requirements and communication costs.
Reference: [106] <author> I-Chen Wu. </author> <title> Efficient parallel divide-and-conquer for a class of interconnection topologies. </title> <booktitle> In Proceedings of the 2nd International Symposium on Algorithms, number 557 in Lecture Notes in Computer Science, </booktitle> <pages> pages 229-240, </pages> <address> Taipei, Republic of China, </address> <month> December </month> <year> 1991. </year> <note> Springer-Verlag. </note>
Reference-contexts: Besides the work of Karp, Zhang, and Ortynski already mentioned, Wu and Kung [107] proved a lower bound on communication requirements in parallel divide-and-conquer programs, and Wu <ref> [106] </ref> gives a distributed algorithm for scheduling parallel divide-and-conquer programs on fixed-connection networks. These analyses focus on the tree-growing stage of divide-and-conquer programs, so they do not consider the case when synchronization is involved.
Reference: [107] <author> I-Chen Wu and H. T. Kung. </author> <title> Communication complexity for parallel divide-and-conquer. </title> <booktitle> In Proceedings of the 32nd Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 151-162, </pages> <address> San Juan, Puerto Rico, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: In algorithmic work that considers space requirements or communication costs, most prior work has focused on cases like backtrack search with no synchronization or has focused on time and space to the exclusion of communication costs. Besides the work of Karp, Zhang, and Ortynski already mentioned, Wu and Kung <ref> [107] </ref> proved a lower bound on communication requirements in parallel divide-and-conquer programs, and Wu [106] gives a distributed algorithm for scheduling parallel divide-and-conquer programs on fixed-connection networks. These analyses focus on the tree-growing stage of divide-and-conquer programs, so they do not consider the case when synchronization is involved. <p> The communication bounds in this theorem are existentially tight, in that there exist fully strict computations that require W (PT (1 + n d )S max ) total communication for any execution schedule that achieves linear speedup. This result follows directly from a theorem of Wu and Kung <ref> [107] </ref>, who showed that divide-and-conquer computationsa special case of fully strict computations with n d = 1require this much communication.
Reference: [108] <author> Y. Zhang and A. Ortynski. </author> <title> The efficiency of randomized parallel backtrack search. </title> <booktitle> In Proceedings of the 6th IEEE Symposium on Parallel and Distributed Processing, </booktitle> <address> Dallas, Texas, </address> <month> October </month> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: search can be viewed as a multithreaded computation with no synchronization, and in the work just cited, the only algorithm with reasonable space bounds is the random work-stealing algorithm of Karp and Zhang [65, 109], though they did not make this observation until the later work of Zhang and Ortynski <ref> [108] </ref>. Our results specialize to match theirs. Likewise, most of the systems-oriented work on multithreading has ignored the space issue. Notable exceptions include the k-bounded loops of Culler and Arvind [34, 35, 36] and the throttling mechanism of Ruggiero and Sargeant [90].
Reference: [109] <author> Yanjun Zhang. </author> <title> Parallel Algorithms for Combinatorial Search Problems. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, University of Califor-nia at Berkeley, </institution> <month> November </month> <year> 1989. </year> <note> Also: </note> <institution> University of California at Berkeley, Computer Science Division, </institution> <type> Technical Report UCB/CSD 89/543. </type>
Reference-contexts: Related work in this area includes a randomized work-stealing algorithm for load balancing independent jobs [89]; algorithms for dynamically embedding trees in fixed-connection networks [5, 71]; and algorithms for backtrack search and branch-and-bound <ref> [61, 65, 75, 86, 109] </ref>. <p> Backtrack search can be viewed as a multithreaded computation with no synchronization, and in the work just cited, the only algorithm with reasonable space bounds is the random work-stealing algorithm of Karp and Zhang <ref> [65, 109] </ref>, though they did not make this observation until the later work of Zhang and Ortynski [108]. Our results specialize to match theirs. Likewise, most of the systems-oriented work on multithreading has ignored the space issue.
Reference: [110] <author> Songnian Zhou, Jingwen Wang, Xiaohu Zheng, and Pierre Delisle. </author> <title> Utopia: A load sharing facility for large, heterogeneous distributed computer systems. </title> <journal> Software Practice and Experience, </journal> <volume> 23(12) </volume> <pages> 1305-1336, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: In these systems, the set of machines on which the program runs is chosen statically by the user. Distributed operating systems [30, 82, 98, 99] and other systems <ref> [32, 40, 68, 74, 79, 110] </ref> provide transparent process placement and (in some cases) migration, but these systems are geared towards large serial programs or coarse-grain distributed programs.
References-found: 110

