URL: ftp://cse.ogi.edu/pub/neural/papers/kambhatlaLeen94.nips.ps.Z
Refering-URL: http://www.cse.ogi.edu/~tleen/
Root-URL: http://www.cse.ogi.edu
Email: nanda@cse.ogi.edu, tleen@cse.ogi.edu  
Title: Classifying with Gaussian Mixtures and Clusters  
Author: Nanda Kambhatla and Todd K. Leen 
Address: P.O. Box 91000 Portland, OR 97291-1000  
Affiliation: Department of Computer Science and Engineering Oregon Graduate Institute of Science Technology  
Abstract: In this paper, we derive classifiers which are winner-take-all (WTA) approximations to a Bayes classifier with Gaussian mixtures for class conditional densities. The derived classifiers include clustering based algorithms like LVQ and k-Means. We propose a constrained rank Gaussian mixtures model and derive a WTA algorithm for it. Our experiments with two speech classification tasks indicate that the constrained rank model and the WTA approximations improve the performance over the unconstrained models.
Abstract-found: 1
Intro-found: 1
Reference: <author> R.A. Cole, D.G. Novick, D. Burnett, B. Hansen, S. Sutton, M. Fanty. </author> <title> (1994) Towards Automatic Collection of the U.S. Census. </title> <booktitle> Proceedings of the International Conference on Acoustics, Speech and Signal Processing 1994. </booktitle>
Reference-contexts: The data was drawn from the CENSUS speech corpus <ref> (Cole et al 1994) </ref>. Each feature vector was 70 dimensional (perceptual linear prediction (PLP) coefficients (Hermansky 1990) over the vowel and surrounding context). We partitioned the data into a training set (8997 vectors), a validation set (1362 vectors) for model selection, and a test set (1638 vectors).
Reference: <author> A.P. Dempster, N.M. Laird, and D.B. Rubin. </author> <title> (1977) Maximum Likelihood from Incomplete Data via the EM Algorithm. </title> <journal> J. Royal Statistical Society Series B, </journal> <volume> vol. 39, </volume> <pages> pp. 1-38. </pages>
Reference-contexts: In this example, the class conditional densities are modelled as a mixture of two Gaussians and equal priors are assumed. To implement the Gaussian mixture Bayes classifier (GMB) we first separate the training data into the different classes. We then use the EM algorithm <ref> (Dempster et al 1977, Nowlan 1991) </ref> to determine the parameters for the Gaussian mixture density for each class. 3 Winner-take-all approximations to GMB classifiers In this section, we derive winner-take-all (WTA) approximations to GMB classifiers.
Reference: <author> R.O. Duda and P.E. Hart. </author> <title> (1973) Pattern Classification and Scene Analysis. </title> <publisher> John Wiley and Sons Inc. </publisher>
Reference-contexts: is ffi I (x) = p ( I ) p (x j I ) : (1) An input feature vector x is assigned to class I if ffi I (x) &gt; ffi J (x) 8J 6= I : Given the class conditional densities, this choice minimizes the classification error rate <ref> (Duda and Hart 1973) </ref>. We model each class conditional density by a mixture composed of Q I component Gaussians.
Reference: <author> W.M Fisher and G.R Doddington. </author> <title> (1986) The DARPA speech recognition database: specification and status. </title> <booktitle> In Proceedings of the DARPA Speech Recognition Workshop, </booktitle> <address> p93-99, Palo Alto CA. </address>
Reference-contexts: The measure used is the classification accuracy. 5.1 TIMIT data The first task is the classification of 12 monothongal vowels from the TIMIT database <ref> (Fisher and Doddington 1986) </ref>. Each feature vector consists of the lowest 32 DFT coefficients, time-averaged over the central third of the vowel. We partitioned the data into a training set (1200 vectors), a validation set (408 vectors) for model selection, and a test set (408 vectors).
Reference: <author> H. Hermansky. </author> <title> (1990) Perceptual Linear Predictive (PLP) analysis of speech. </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 87(4) </volume> <pages> 1738-1752. </pages>
Reference-contexts: The data was drawn from the CENSUS speech corpus (Cole et al 1994). Each feature vector was 70 dimensional (perceptual linear prediction (PLP) coefficients <ref> (Hermansky 1990) </ref> over the vowel and surrounding context). We partitioned the data into a training set (8997 vectors), a validation set (1362 vectors) for model selection, and a test set (1638 vectors). The training set had close to a 1000 vectors per class.
Reference: <author> T. Kohonen. </author> <title> (1989) Self-Organization and Associative Memory (3rd edition). </title> <publisher> Berlin: Springer-Verlag. </publisher>
Reference: <author> S.J. Nowlan. </author> <title> (1991) Soft Competitive Adaptation: Neural Network Learning Algorithms based on Fitting Statistical Mixtures. </title> <type> CMU-CS-91-126 PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University. </institution>
Reference-contexts: In this paper, we assume that the class conditional densities are modeled by mixtures of Gaussians. Based on Nowlan's work relating Gaussian mixtures and clustering <ref> (Nowlan 1991) </ref>, we derive winner-take-all (WTA) algorithms which approximate a Gaussian mixtures Bayes classifier. We also show the relationship of these algorithms to non-Bayesian cluster-based techniques like LVQ and k-Means. <p> We also show the relationship of these algorithms to non-Bayesian cluster-based techniques like LVQ and k-Means. 3.1 The WTA model for GMB The WTA assumptions (relating hard clustering to Gaussian mixtures; see <ref> (Nowlan 1991) </ref>) are: * p (x j I ) are mixtures of Gaussians as in (2). * The summation in (2) is dominated by the largest term. <p> <ref> (Nowlan 1991) </ref>) are: * p (x j I ) are mixtures of Gaussians as in (2). * The summation in (2) is dominated by the largest term. This is "equivalent to assigning all of the responsibility for an observation to the Gaussian with the highest probability of generating that observation" (Nowlan 1991). To draw the relation between GMB and cluster-based classifiers, we further assume that: * The mixing proportions (ff I j ) are equal for a given class. * The number of mixture components Q I is proportional to p ( I ).
References-found: 7

