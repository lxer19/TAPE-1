URL: ftp://wol.ra.phy.cam.ac.uk/pub/mackay/ensemblePaper.ps.gz
Refering-URL: http://131.111.48.24/mackay/README.html
Root-URL: 
Email: mackay@mrao.cam.ac.uk  
Title: Ensemble Learning for Hidden Markov Models  
Author: David J.C. MacKay 
Address: Cambridge CB3 0HE. UK  
Affiliation: Cavendish Laboratory  
Abstract: The standard method for training Hidden Markov Models optimizes a point estimate of the model parameters. This estimate, which can be viewed as the maximum of a posterior probability density over the model parameters, may be susceptible to over-fitting, and contains no indication of parameter uncertainty. Also, this maximum may be unrepresentative of the posterior probability distribution. In this paper we study a method in which we optimize an ensemble which approximates the entire posterior probability distribution. The ensemble learning algorithm requires the same The traditional training algorithm for hidden Markov models is an expectation-maximization (EM) algorithm (Dempster et al. 1977) known as the Baum-Welch algorithm. It is a maximum likelihood method, or, with a simple modification, a penalized maximum likelihood method, which can be viewed as maximizing a posterior probability density over the model parameters. Recently, Hinton and van Camp (1993) developed a technique known as ensemble learning (see also MacKay (1995) for a review). Whereas maximum a posteriori methods optimize a point estimate of the parameters, in ensemble learning an ensemble is optimized, so that it approximates the entire posterior probability distribution over the parameters. The objective function that is optimized is a variational free energy (Feynman 1972) which measures the relative entropy between the approximating ensemble and the true distribution. In this paper we derive and test an ensemble learning algorithm for hidden Markov models, building on Neal resources as the traditional Baum-Welch algorithm.
Abstract-found: 1
Intro-found: 1
Reference: <author> Dempster, A., Laird, N., and Rubin, D. </author> <title> (1977) Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society B 39: </journal> <pages> 1-38. </pages>
Reference: <author> Feynman, R. P. </author> <title> (1972) Statistical Mechanics. </title> <editor> W. A. </editor> <publisher> Benjamin, Inc. </publisher>
Reference: <author> Hinton, G. E., and van Camp, D. </author> <title> (1993) Keeping neural networks simple by minimizing the description length of the weights. </title> <booktitle> In Proc. 6th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pp. 5-13. </pages> <publisher> ACM Press, </publisher> <address> New York, NY. </address>
Reference: <author> MacKay, D. J. C. </author> <title> (1995) Developments in probabilistic modelling with neural networks|ensemble learning. </title> <booktitle> In Neural Networks: Artificial Intelligence and Industrial Applications. Proceedings of the 3rd Annual Symposium on Neural Networks, </booktitle> <address> Nijmegen, Netherlands, </address> <month> 14-15 September </month> <year> 1995 </year> <month> , pp. </month> <pages> 191-198, </pages> <address> Berlin. </address> <publisher> Springer. </publisher>
Reference: <author> MacKay, D. J. C., </author> <title> (1996) Choice of basis for Laplace approximation. </title> <note> Submitted to Machine Learning. </note>
Reference-contexts: choosing these priors is that they give a direct correspondence between the standard penalized maximum likelihood method in which `initial counts' or `offsets' u i are placed in the bins of the Baum-Welch algorithm and a maximum a posteriori method, if the posterior density is maximized in the `softmax' basis <ref> (MacKay 1996) </ref> where each probability vector p is represented by parameters a such that p i (a) = e a i / i 0 e a i 0 : (7) 1.1 The standard Baum-Welch optimization The Baum-Welch algorithm (with penalty terms U) is an iterative algorithm that increases P (jX) at
Reference: <author> MacKay, D. J. C., and Peto, L. </author> <title> (1995) A hierarchical Dirichlet language model. </title> <booktitle> Natural Language Engineering 1 (3): </booktitle> <pages> 1-19. </pages>
Reference: <author> Neal, R. M., and Hinton, G. E. </author> <title> (1993) A new view of the EM algorithm that justifies incremental and other variants. </title> <journal> Biometrika. </journal> <note> submitted. </note>
Reference: <author> Rabiner, L. R., and Juang, B. H. </author> <title> (1986) An introduction to hidden Markov models. </title> <journal> IEEE ASSP Magazine pp. </journal> <pages> 4-16. </pages>
References-found: 8

