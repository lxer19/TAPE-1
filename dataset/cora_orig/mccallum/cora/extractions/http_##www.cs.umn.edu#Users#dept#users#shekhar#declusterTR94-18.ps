URL: http://www.cs.umn.edu/Users/dept/users/shekhar/declusterTR94-18.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/shekhar/
Root-URL: http://www.cs.umn.edu
Email: dliu@cs.umn.edu shekhar@cs.umn.edu  
Phone: TEL: (612) 6248307 FAX: (612) 6250572  
Title: Partitioning Similarity Graphs A Framework for Declustering Problems  
Author: Duen-Ren Liu and Shashi Shekhar 
Keyword: Keyword: Similarity Graph, Geographic Databases, Declustering, Grid-File and Parallel Databases  
Address: 4-192 EE/CS, 200 Union St. SE, Minneapolis, MN 55455  
Affiliation: Department of Computer Science, University of Minnesota  
Abstract: Declustering problems are well-known in the databases for parallel computing environments. In this paper, we propose a new similarity-based technique for declustering data. The proposed method can adapt to the available information about query distribution (e.g. size, shape and frequency) and can work with alternative atomic data-types. Furthermore, the proposed method is flexible and can work with alternative data distributions, data sizes and partition-size constraints. The method is based on max-cut partitioning of a similarity graph defined over the given set of data, under constraints on the partition sizes. It maximizes the chances that a pair of atomic data-items that are frequently accessed together by queries are allocated to distinct disks. We describe the application of the proposed method to parallelizing Grid Files at the data page level. Detailed experiments in this context show that the proposed method adapts to query distribution and data distribution, and that it outperforms traditional mapping-function-based methods for many interesting query distributions as well for several non-uniform data distributions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E.R. Barnes. </author> <title> "An Algorithm for Partitioning the Nodes of a Graph". </title> <journal> SIAM Journal Alg. Disc. Meth., </journal> <volume> 3(4) </volume> <pages> 541-550, </pages> <month> December </month> <year> 1982. </year>
Reference-contexts: We note that the minimum-spanning-trees algorithm can also be used as a simple heuristic for the graph-partitioning approach. However, the ratio-cut [5], KL [22] and other methods [21] are superior [5, 21, 22]. These heuristics are based on spectral partitioning and iterative approaches. In spectral techniques <ref> [1] </ref>, eigenvalues and eigenvectors of the matrix representation of the graph are computed. Partition results are derived by mapping the information provided by eigenvectors onto actual partitions.
Reference: [2] <author> W. J. Camp, S. J. Plimpton, and et. al. </author> <title> "Massively Parallel Methods for Engineering and Science Problems". </title> <journal> Communication of ACM, </journal> <volume> 37(4) </volume> <pages> 31-41, </pages> <year> 1994. </year>
Reference-contexts: Similar run-times (e.g. within 50 seconds on a sparc 2 workstation for partitioning a graph with 170,000 nodes and 230 K edges into 64 partitions) are also reported in <ref> [2] </ref>. An incremental declustering algorithm is presented in Section 3.1 and is experimentally evaluated for a large data set in Section 6.3.4.
Reference: [3] <author> C.C. Chang and C.Y. Cheng. </author> <title> "Performance of Two-Disk Partition Data Allocations". </title> <journal> BIT, </journal> <volume> 27(3) </volume> <pages> 306-314, </pages> <year> 1987. </year>
Reference-contexts: Several single-attribute functions including round robin, hash-partitioning, key-range partitioning [6, 15], and a hybrid of these [16], as well as multi-attribute functions including grid-based multi-dimensional key-range-partitioning [17], disk modulo [8, 27], generalized disk modulo (linear) [8, 36], field-wise-XOR [24], Hilbert [9], error-correcting code [11], latin-square [23, 36], random <ref> [3] </ref>, vector-based declustering [4] and lattice [36] have been proposed and evaluated. A survey of multi-attribute functions can be found in [11, 9]. These methods are limited in the case of managing updates, non-uniform data-distributions and non-uniform data-sizes.
Reference: [4] <author> L. T. Chen and D. Rotem. </author> <title> "Declustering Objects for Visualization". </title> <booktitle> In Proc. of Intl Conference on Very Large Data Bases, </booktitle> <pages> pages 85-96, </pages> <year> 1993. </year>
Reference-contexts: single-attribute functions including round robin, hash-partitioning, key-range partitioning [6, 15], and a hybrid of these [16], as well as multi-attribute functions including grid-based multi-dimensional key-range-partitioning [17], disk modulo [8, 27], generalized disk modulo (linear) [8, 36], field-wise-XOR [24], Hilbert [9], error-correcting code [11], latin-square [23, 36], random [3], vector-based declustering <ref> [4] </ref> and lattice [36] have been proposed and evaluated. A survey of multi-attribute functions can be found in [11, 9]. These methods are limited in the case of managing updates, non-uniform data-distributions and non-uniform data-sizes. <p> The MF-CMD method is also known as the CMD method [27] or the Disk Modulo method (DM) [8] in a two-dimensional space. The specific mapping function method, MF-GDM, that we choose in the experiment, belongs to the class of Generalized Disk Modulo methods (GDM) [8], vector based declustering methods <ref> [4] </ref> and lattice allocation methods [36]. Figure 8 illustrates the disk assignments produced by these methods in a 8x8 dense grid directory. Two global max-cut declustering strategies are derived which use different approaches to estimate the weights.
Reference: [5] <author> C.K. Cheng and Y.C. Wei. </author> <title> "An Improved Two-Way Partitioning Algorithm with Stable Performance". </title> <journal> IEEE Trans. on Computer-Aided Design, </journal> <volume> 10(12) </volume> <pages> 1502-1511, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: Many good heuristic algorithms which have been developed for the min-cut graph partitioning problem can also be applied to efficiently solve the max-cut graph-partitioning problem. We note that the minimum-spanning-trees algorithm can also be used as a simple heuristic for the graph-partitioning approach. However, the ratio-cut <ref> [5] </ref>, KL [22] and other methods [21] are superior [5, 21, 22]. These heuristics are based on spectral partitioning and iterative approaches. In spectral techniques [1], eigenvalues and eigenvectors of the matrix representation of the graph are computed. <p> We note that the minimum-spanning-trees algorithm can also be used as a simple heuristic for the graph-partitioning approach. However, the ratio-cut [5], KL [22] and other methods [21] are superior <ref> [5, 21, 22] </ref>. These heuristics are based on spectral partitioning and iterative approaches. In spectral techniques [1], eigenvalues and eigenvectors of the matrix representation of the graph are computed. Partition results are derived by mapping the information provided by eigenvectors onto actual partitions. <p> These heuristics are based on spectral partitioning and iterative approaches. In spectral techniques [1], eigenvalues and eigenvectors of the matrix representation of the graph are computed. Partition results are derived by mapping the information provided by eigenvectors onto actual partitions. Iterative approaches <ref> [5, 13, 22, 35] </ref> start from an initial partition, then iteratively apply pairwise swapping or moving of nodes across partitions to minimize edge cuts. In general, graph partitioning algorithms are expensive and their CPU and memory costs often grow non-linearly with the size of the graph. <p> In general, N-way partitioning can be implemented via generalizing the two-way partitioning or via repeated applications of the two-way partitioning [22]. We implement the N-way max-cut graph partitioning by repeated applications of two-way graph partitioning, using the modified ratio-cut heuristic <ref> [5] </ref> which is described in Appendix B. The cost metric in the two-way ratio-cut algorithm is modified to be W c fl jAj fl jBj, and the sign of the weight on the edge is changed to be negative, to maximize the weight on the edges in the cut-set. <p> The 2-way-maxcut-partition algorithm adapts the iterative approach <ref> [5] </ref>, which starts from an initial partition (i.e. two subsets), and then iteratively moves nodes across subsets in an attempt to achieve a global minimum weight on the edges in the cut set. <p> The process repeats until no further accumulated positive gain is possible. The implementation of 2-way-maxcut-partition algorithm is based on the bucket-list data structure and requires a time complexity of O (jEj) [13] with respect to the number of edges jEj. We have chosen a competent algorithm, ratio-cut heuristic algorithm <ref> [5] </ref>. Our max-cut partitioning scheme can adapt any existing graph partitioning algorithms as the basis for declustering. Readers are referred to [21] for a survey and comparison of alternate graph partitioning algorithms. the procedure find-initial-partition ().
Reference: [6] <author> D.J. DeWitt and et. al. </author> <title> "The Gamma Database Machine Project". </title> <journal> IEEE Trans. on Knowledge and Data Engineering, </journal> <volume> 2(1), </volume> <year> 1990. </year>
Reference-contexts: These methods provide a mapping function from the domain of data-items to the set of disk-ids, assuming that all data-items and queries are equiprobable. Several single-attribute functions including round robin, hash-partitioning, key-range partitioning <ref> [6, 15] </ref>, and a hybrid of these [16], as well as multi-attribute functions including grid-based multi-dimensional key-range-partitioning [17], disk modulo [8, 27], generalized disk modulo (linear) [8, 36], field-wise-XOR [24], Hilbert [9], error-correcting code [11], latin-square [23, 36], random [3], vector-based declustering [4] and lattice [36] have been proposed and evaluated.
Reference: [7] <author> H. C. Du. </author> <title> "Disk Allocation Methods for Binary Cartesian Product Files". </title> <journal> BIT, </journal> <volume> 26 </volume> <pages> 138-147, </pages> <year> 1986. </year>
Reference-contexts: We note that no methods can achieve strict optimality for all query-sets <ref> [7, 18, 36] </ref>. For example, no method can be strictly optimal for all range queries if the number of disks is greater than 5 [18, 36]. However, some of the existing declustering methods have been proved to be strictly optimal for simpler query sets [8, 11, 36].
Reference: [8] <author> H. C. Du and J. S. Sobolewski. </author> <title> "Disk Allocation for Product Files on Multiple Disk Systems". </title> <journal> ACM Trans. on Database Systems, </journal> <volume> 7, </volume> <month> March </month> <year> 1982. </year>
Reference-contexts: Several single-attribute functions including round robin, hash-partitioning, key-range partitioning [6, 15], and a hybrid of these [16], as well as multi-attribute functions including grid-based multi-dimensional key-range-partitioning [17], disk modulo <ref> [8, 27] </ref>, generalized disk modulo (linear) [8, 36], field-wise-XOR [24], Hilbert [9], error-correcting code [11], latin-square [23, 36], random [3], vector-based declustering [4] and lattice [36] have been proposed and evaluated. A survey of multi-attribute functions can be found in [11, 9]. <p> Several single-attribute functions including round robin, hash-partitioning, key-range partitioning [6, 15], and a hybrid of these [16], as well as multi-attribute functions including grid-based multi-dimensional key-range-partitioning [17], disk modulo [8, 27], generalized disk modulo (linear) <ref> [8, 36] </ref>, field-wise-XOR [24], Hilbert [9], error-correcting code [11], latin-square [23, 36], random [3], vector-based declustering [4] and lattice [36] have been proposed and evaluated. A survey of multi-attribute functions can be found in [11, 9]. <p> For example, no method can be strictly optimal for all range queries if the number of disks is greater than 5 [18, 36]. However, some of the existing declustering methods have been proved to be strictly optimal for simpler query sets <ref> [8, 11, 36] </ref>. We use a weighted similarity graph to capture the similarity relationship between data items. The nodes in the graph represent the data items. The similarity between two data items is quantified as the weight on the edge connecting them. <p> Corollary 2 The max-cut declustering method is a strictly optimal allocation method with respect to the Row queries over one dimension of a k-dimensional grid. Proof: The Disk Modulo method is strictly optimal for all partial match queries, with only one unspecified attribute <ref> [8] </ref>. Therefore, the corollary is derived from Theorem 1. Example: We give an example to illustrate Theorem 1. Table 3 shows the declustering of a 6x6 grid into 6 and 4 disks respectively, using max-cut partitioning with respect to Row/Col queries. <p> For the experiments on the effects of page-sharing, data was inserted into the grid file with an initial 1x1 grid directory structure. 23 Declustering Methods : We compare max-cut declustering with two well-known mapping--function-based methods, the Hilbert (MF-Hilb) allocation method [9] and the Linear method <ref> [27, 8, 36] </ref>. The Hilbert method is chosen for comparison, since it achieves good performance for square-shaped range queries [9]. Two types of linear methods are used in our experiments. The first one, denoted as MF-CMD, uses the modulo function (x + y) mod N. <p> The first one, denoted as MF-CMD, uses the modulo function (x + y) mod N. The second one, represented as MF-GDM, uses the modulo function (x + 5y) mod N. The MF-CMD method is also known as the CMD method [27] or the Disk Modulo method (DM) <ref> [8] </ref> in a two-dimensional space. The specific mapping function method, MF-GDM, that we choose in the experiment, belongs to the class of Generalized Disk Modulo methods (GDM) [8], vector based declustering methods [4] and lattice allocation methods [36]. <p> The MF-CMD method is also known as the CMD method [27] or the Disk Modulo method (DM) <ref> [8] </ref> in a two-dimensional space. The specific mapping function method, MF-GDM, that we choose in the experiment, belongs to the class of Generalized Disk Modulo methods (GDM) [8], vector based declustering methods [4] and lattice allocation methods [36]. Figure 8 illustrates the disk assignments produced by these methods in a 8x8 dense grid directory. Two global max-cut declustering strategies are derived which use different approaches to estimate the weights.
Reference: [9] <author> C. Faloutsos and P. Bhagwat. </author> <title> "Declustering Using Fractals". </title> <booktitle> In Proc. of Intl Symposium on Databases in Parallel and Distributed Systems, </booktitle> <pages> pages 18-25, </pages> <year> 1993. </year>
Reference-contexts: Several single-attribute functions including round robin, hash-partitioning, key-range partitioning [6, 15], and a hybrid of these [16], as well as multi-attribute functions including grid-based multi-dimensional key-range-partitioning [17], disk modulo [8, 27], generalized disk modulo (linear) [8, 36], field-wise-XOR [24], Hilbert <ref> [9] </ref>, error-correcting code [11], latin-square [23, 36], random [3], vector-based declustering [4] and lattice [36] have been proposed and evaluated. A survey of multi-attribute functions can be found in [11, 9]. These methods are limited in the case of managing updates, non-uniform data-distributions and non-uniform data-sizes. <p> A survey of multi-attribute functions can be found in <ref> [11, 9] </ref>. These methods are limited in the case of managing updates, non-uniform data-distributions and non-uniform data-sizes. Furthermore, they are limited in their ability to adapt to available information about query distribution and size constraints. Lastly, these techniques are not designed for schema partitioning. <p> We consider several types of queries, including square-shaped, row/Column and diagonal range 22 queries. The square-shaped queries represent range queries with equal lengths on both sides and have been used to evaluate declustering methods in recent studies <ref> [9, 18] </ref>. In our experiments, we examine all possible square-range queries. Suppose that the grid file is an MxM grid. The row-query (Row) set is a set of 1xM rectangle-shaped queries, and the column-query (Col) set is a set of Mx1 rectangle-shaped queries. <p> For the experiments on the effects of page-sharing, data was inserted into the grid file with an initial 1x1 grid directory structure. 23 Declustering Methods : We compare max-cut declustering with two well-known mapping--function-based methods, the Hilbert (MF-Hilb) allocation method <ref> [9] </ref> and the Linear method [27, 8, 36]. The Hilbert method is chosen for comparison, since it achieves good performance for square-shaped range queries [9]. Two types of linear methods are used in our experiments. The first one, denoted as MF-CMD, uses the modulo function (x + y) mod N. <p> file with an initial 1x1 grid directory structure. 23 Declustering Methods : We compare max-cut declustering with two well-known mapping--function-based methods, the Hilbert (MF-Hilb) allocation method <ref> [9] </ref> and the Linear method [27, 8, 36]. The Hilbert method is chosen for comparison, since it achieves good performance for square-shaped range queries [9]. Two types of linear methods are used in our experiments. The first one, denoted as MF-CMD, uses the modulo function (x + y) mod N. The second one, represented as MF-GDM, uses the modulo function (x + 5y) mod N.
Reference: [10] <author> C. Faloutsos and I. Kamel. </author> <title> "Beyond Uniformity and Independence : Analysis of R-trees Using the Concept of Fractal Dimension". </title> <booktitle> In Proc. Symp. on Principles of Database Systems, </booktitle> <pages> pages 4-13. </pages> <address> SIGMOD-SIGACT PODS, </address> <year> 1994. </year>
Reference-contexts: We note that the hot-spot data-set is not factorizable, and that it has been used in the literature [26] to simulate skewed distributions. The data-page distribution over the 20x20 grid directory is shown in Appendix F, Table 6. It is possible to use the "fractal dimension" <ref> [10] </ref> to generate experimental data. However, it is not likely to change the major trends (e.g. the proposed method outperforms others for nonuniform data distributions). In fact, the proposed methods are already being used for real data sets (e.g. the polygonal map of Killeen, Texas) [32].
Reference: [11] <author> C. Faloutsos and D. Metaxas. </author> <title> "Disk Allocation Methods Using Error Correcting Codes". </title> <journal> IEEE Trans. on Computers, </journal> <volume> 40(8), </volume> <month> August </month> <year> 1991. </year> <month> 31 </month>
Reference-contexts: 1 Introduction With an increasing performance gap between processors and I/O systems, parallelizing I/O operations by declustering <ref> [12, 11, 29] </ref> data is becoming essential for high performance applications. Database machines, multi-processors and parallel computers can all benefit from effective declus-tering. <p> Unfortunately, this problem is NP-complete in several contexts, which include partial match queries on cartesian product files <ref> [11] </ref> and join queries on a set of relations [29]. Thus any method to solve this problem in polynomial time will be heuristic. We address the declustering problem in a single processor with a multi-disk environment. <p> Several single-attribute functions including round robin, hash-partitioning, key-range partitioning [6, 15], and a hybrid of these [16], as well as multi-attribute functions including grid-based multi-dimensional key-range-partitioning [17], disk modulo [8, 27], generalized disk modulo (linear) [8, 36], field-wise-XOR [24], Hilbert [9], error-correcting code <ref> [11] </ref>, latin-square [23, 36], random [3], vector-based declustering [4] and lattice [36] have been proposed and evaluated. A survey of multi-attribute functions can be found in [11, 9]. These methods are limited in the case of managing updates, non-uniform data-distributions and non-uniform data-sizes. <p> A survey of multi-attribute functions can be found in <ref> [11, 9] </ref>. These methods are limited in the case of managing updates, non-uniform data-distributions and non-uniform data-sizes. Furthermore, they are limited in their ability to adapt to available information about query distribution and size constraints. Lastly, these techniques are not designed for schema partitioning. <p> For example, no method can be strictly optimal for all range queries if the number of disks is greater than 5 [18, 36]. However, some of the existing declustering methods have been proved to be strictly optimal for simpler query sets <ref> [8, 11, 36] </ref>. We use a weighted similarity graph to capture the similarity relationship between data items. The nodes in the graph represent the data items. The similarity between two data items is quantified as the weight on the edge connecting them.
Reference: [12] <author> M.T. Fang, R.C.T. Lee, and C.C. Chang. </author> <title> "The Idea of Declustering and its Applications". </title> <booktitle> In Proc. of Intl Conference on Very Large Databases. VLDB, </booktitle> <year> 1986. </year>
Reference-contexts: 1 Introduction With an increasing performance gap between processors and I/O systems, parallelizing I/O operations by declustering <ref> [12, 11, 29] </ref> data is becoming essential for high performance applications. Database machines, multi-processors and parallel computers can all benefit from effective declus-tering. <p> B-tree, R-tree) in the face of updates, non-uniform data distributions and non-uniform access frequencies to data-items. However, they do not take advantage of query distribution information, beyond looking at the access frequencies of the individual data-items. Fang et. al. <ref> [12] </ref> introduced the idea of similarity (e.g. the nearest neighbor) for declustering. <p> Minimal spanning trees and shortest spanning paths were proposed in <ref> [12] </ref> to divide a set of given data-items into two "similar" groups. Similarity for more than two groups is not addressed. Furthermore, it is not obvious that the nearest-neighbor based notion of similarity is appropriate for all kinds of query and data types. <p> Section 6 presents the application of the proposed method to parallelizing Grid Files. Experiments in this context are also presented. Finally, Section 7 presents the conclusion and suggests future work. 2 Similarity-Based Approach to Declustering Fang, et al. <ref> [12] </ref> introduced the similarity idea for declustering as follows: Given a set of data, partition it into two groups such that these two groups are similar to each other. <p> Alternatively, the incremental max-cut declustering technique presented in Section 3.1 can also be used to create the initial partition. Declustering Effect: We demonstrate the declustering capability of the proposed declustering methods in the following examples from <ref> [12] </ref>, and also illustrate that maximizing the weight on the cut-set leads to reduced average response time. Example: Table 2 shows a two-attribute cartesian product file and two ways of allocating the records into the two disks. We consider partial-match queries in this example.
Reference: [13] <author> C.M. Fiduccia and R.M. Mattheyses. </author> <title> "A Linear Time Heuristic for Improving Network Partitions". </title> <booktitle> In Proc. of 19th Design Automation Conference, </booktitle> <pages> pages 175-181, </pages> <year> 1982. </year>
Reference-contexts: These heuristics are based on spectral partitioning and iterative approaches. In spectral techniques [1], eigenvalues and eigenvectors of the matrix representation of the graph are computed. Partition results are derived by mapping the information provided by eigenvectors onto actual partitions. Iterative approaches <ref> [5, 13, 22, 35] </ref> start from an initial partition, then iteratively apply pairwise swapping or moving of nodes across partitions to minimize edge cuts. In general, graph partitioning algorithms are expensive and their CPU and memory costs often grow non-linearly with the size of the graph. <p> The process repeats until no further accumulated positive gain is possible. The implementation of 2-way-maxcut-partition algorithm is based on the bucket-list data structure and requires a time complexity of O (jEj) <ref> [13] </ref> with respect to the number of edges jEj. We have chosen a competent algorithm, ratio-cut heuristic algorithm [5]. Our max-cut partitioning scheme can adapt any existing graph partitioning algorithms as the basis for declustering.
Reference: [14] <author> M.R. Garey and D.S. Johnson. </author> <title> "Computers and Intractability: A Guide to the Theory of NP-Completeness". W.H. </title> <publisher> Freeman and Company, </publisher> <address> San Francisco, </address> <year> 1979. </year>
Reference-contexts: Unfortunately, the max-cut graph-partitioning problem, as stated, remains NP-complete <ref> [14] </ref>, which can be shown by reducing it to the complementary min-cut graph partitioning problem [22].
Reference: [15] <author> S. Ghandeharizadeh and D.J. DeWitt. </author> <title> "A Mutltiuser Performance Analysis of Alternative Declustering Strategies". </title> <booktitle> In Proc. of the 6th Intl Conference on Data Engineering. IEEE, </booktitle> <year> 1990. </year>
Reference-contexts: These methods provide a mapping function from the domain of data-items to the set of disk-ids, assuming that all data-items and queries are equiprobable. Several single-attribute functions including round robin, hash-partitioning, key-range partitioning <ref> [6, 15] </ref>, and a hybrid of these [16], as well as multi-attribute functions including grid-based multi-dimensional key-range-partitioning [17], disk modulo [8, 27], generalized disk modulo (linear) [8, 36], field-wise-XOR [24], Hilbert [9], error-correcting code [11], latin-square [23, 36], random [3], vector-based declustering [4] and lattice [36] have been proposed and evaluated.
Reference: [16] <author> S. Ghandeharizadeh and D.J. DeWitt. </author> <title> "Hybrid-range Partitioning Strategy: A New Declustering Strategy for Multiprocessor Database Machine". </title> <booktitle> In Proc. of Intl Conference on Very Large Databases. VLDB, </booktitle> <year> 1990. </year>
Reference-contexts: These methods provide a mapping function from the domain of data-items to the set of disk-ids, assuming that all data-items and queries are equiprobable. Several single-attribute functions including round robin, hash-partitioning, key-range partitioning [6, 15], and a hybrid of these <ref> [16] </ref>, as well as multi-attribute functions including grid-based multi-dimensional key-range-partitioning [17], disk modulo [8, 27], generalized disk modulo (linear) [8, 36], field-wise-XOR [24], Hilbert [9], error-correcting code [11], latin-square [23, 36], random [3], vector-based declustering [4] and lattice [36] have been proposed and evaluated. <p> The size-constraint predicate L i for each disk may be determined based on the disk load-balance criterion, available disk capacity, N and sizeof (V). It could represent either disk-capacity constraints, P v2G i size (v) Capacity (disk i ), or storage-load balance constraints. It could balance the "heat" <ref> [16] </ref> or frequency of access to data-items in each group. We note that the max-cut partitioning scheme explicitly accounts for available information about query distribution via the weights on the edges. It also accounts for data sizes, data distribution, and partition size constraints.
Reference: [17] <author> S. Ghandeharizadeh, D.J. DeWitt, and W. Qureshi. </author> <title> "A Performance Analysis of Alternative Multi-attribute Declustering Strategies". </title> <booktitle> In Proc. of Intl Conference on Management of Data. ACM SIGMOD, </booktitle> <year> 1992. </year>
Reference-contexts: Thus the storage system can reduce the response time for large I/O volumes by a factor of N, where N is the number of disks in the system. We focus on I/O cost only. Readers are referred to MAGIC <ref> [17] </ref> for a more general cost model that includes communication costs. Furthermore, the data items are assumed to be atomic, i.e., a data item will not be split across disks. Data items like records, objects, pages and page-clusters are likely to satisfy this assumption. <p> Several single-attribute functions including round robin, hash-partitioning, key-range partitioning [6, 15], and a hybrid of these [16], as well as multi-attribute functions including grid-based multi-dimensional key-range-partitioning <ref> [17] </ref>, disk modulo [8, 27], generalized disk modulo (linear) [8, 36], field-wise-XOR [24], Hilbert [9], error-correcting code [11], latin-square [23, 36], random [3], vector-based declustering [4] and lattice [36] have been proposed and evaluated. A survey of multi-attribute functions can be found in [11, 9].
Reference: [18] <author> Bhaskar Himmatsingka and Jaideep Srivastava. </author> <title> "Performance Evaluation of Grid Based Multi-Attribute Record Declustering Methods". </title> <booktitle> In Proc. of the Tenth Intl Conference on Data Engineering. IEEE, </booktitle> <year> 1994. </year>
Reference-contexts: We note that no methods can achieve strict optimality for all query-sets <ref> [7, 18, 36] </ref>. For example, no method can be strictly optimal for all range queries if the number of disks is greater than 5 [18, 36]. However, some of the existing declustering methods have been proved to be strictly optimal for simpler query sets [8, 11, 36]. <p> We note that no methods can achieve strict optimality for all query-sets [7, 18, 36]. For example, no method can be strictly optimal for all range queries if the number of disks is greater than 5 <ref> [18, 36] </ref>. However, some of the existing declustering methods have been proved to be strictly optimal for simpler query sets [8, 11, 36]. We use a weighted similarity graph to capture the similarity relationship between data items. The nodes in the graph represent the data items. <p> The result can be generalized to higher dimensions. Queries on 20 multi-dimensional data represent various subsets of the data. To process a query, the database manager has to retrieve the data points contained inside each cell which intersects with the query. Most existing declustering methods for grid files <ref> [18, 36] </ref> are based on coordinate mapping of the data pages in the grid-directory to the disk-id, assuming that the data are uniformly distributed. <p> We consider several types of queries, including square-shaped, row/Column and diagonal range 22 queries. The square-shaped queries represent range queries with equal lengths on both sides and have been used to evaluate declustering methods in recent studies <ref> [9, 18] </ref>. In our experiments, we examine all possible square-range queries. Suppose that the grid file is an MxM grid. The row-query (Row) set is a set of 1xM rectangle-shaped queries, and the column-query (Col) set is a set of Mx1 rectangle-shaped queries.
Reference: [19] <author> H.V. Jagadish. </author> <title> "Linear Clustering of Objects with Multiple Attributes". </title> <booktitle> In Proc. of Intl Conference on Management of Data, </booktitle> <pages> pages 332-342. </pages> <booktitle> ACM SIGMOD, </booktitle> <year> 1990. </year>
Reference-contexts: We specify our implementation decision and provide some of the grids to facilitate the repeatability of results. The data was inserted into the grid file in the order of a space-filling curve function, Z-curve <ref> [19] </ref>. If one of the data pages overflows, the row or column of the grid that contains that data page is split. The split policy alternates between splitting rows and columns. The data pages may be shared by multiple cells after the split, as is natural in grid files.
Reference: [20] <author> I. Kamel and C. Faloutsos. </author> <title> "Parallel R-Trees". </title> <booktitle> In Proc. of Intl Conference on Management of Data. ACM SIGMOD, </booktitle> <year> 1992. </year>
Reference-contexts: Furthermore, they are limited in their ability to adapt to available information about query distribution and size constraints. Lastly, these techniques are not designed for schema partitioning. Index-specific load-balancing based declustering methods have been proposed for B-tree [30], R-tree <ref> [20] </ref> and the temporal index [25], etc. Dynamic file allocation methods are proposed in the FIVE system [34]. <p> For example, in decision support systems, several reports are generated periodically (daily, weekly or monthly). In a terrain-visualization system, the simulation of a ground vehicle usually issues a fixed size (e.g. 8 km * 8 km) range queries [32]. In spatial databases, the proximity index <ref> [20] </ref> can be used to estimate the probability of a random query retrieving a given pair of rectangles. We view the declustering not as a one-time operation, but as an operation that may need to be executed periodically to reorganize the allocation of data items, if query profiles change substantially. <p> It is likely to achieve better quality of declustering; however, it will have a higher cost. 3.1 Incremental Max-cut Declustering The incremental max-cut allocates data items to the disks in a greedy manner, using the max-cut similarity criterion and local load-balancing. Different from the classical local load-balancing strategy <ref> [20, 30] </ref>, which allocates a data item to the disk with the lowest load (e.g. storage) over a local window, Incremental max-cut declustering aims at allocating a data item to a disk, such that the max-cut similarity criterion within a local window is best fulfilled.
Reference: [21] <author> G. Karypis and V. Kumar. </author> <title> "A Fast and High Quality Multilevel Scheme for Partitioning Irregular Graphs". </title> <booktitle> In Proc. Intl Conference on Parallel Processing, </booktitle> <year> 1995, </year> <note> also Csci TR 95-035, available via WWW at URL: http://www.cs.umn.edu/users/kumar/papers.html. </note>
Reference-contexts: We note that the minimum-spanning-trees algorithm can also be used as a simple heuristic for the graph-partitioning approach. However, the ratio-cut [5], KL [22] and other methods <ref> [21] </ref> are superior [5, 21, 22]. These heuristics are based on spectral partitioning and iterative approaches. In spectral techniques [1], eigenvalues and eigenvectors of the matrix representation of the graph are computed. Partition results are derived by mapping the information provided by eigenvectors onto actual partitions. <p> We note that the minimum-spanning-trees algorithm can also be used as a simple heuristic for the graph-partitioning approach. However, the ratio-cut [5], KL [22] and other methods [21] are superior <ref> [5, 21, 22] </ref>. These heuristics are based on spectral partitioning and iterative approaches. In spectral techniques [1], eigenvalues and eigenvectors of the matrix representation of the graph are computed. Partition results are derived by mapping the information provided by eigenvectors onto actual partitions. <p> A survey of graph-partitioning algorithms, along with their costs and quality of partition is available in <ref> [21] </ref>. The latest developments have yielded algorithms that can partition graphs with a hundred thousand nodes in a couple of minutes [21] on workstations. <p> A survey of graph-partitioning algorithms, along with their costs and quality of partition is available in <ref> [21] </ref>. The latest developments have yielded algorithms that can partition graphs with a hundred thousand nodes in a couple of minutes [21] on workstations. Similar run-times (e.g. within 50 seconds on a sparc 2 workstation for partitioning a graph with 170,000 nodes and 230 K edges into 64 partitions) are also reported in [2]. <p> We have chosen a competent algorithm, ratio-cut heuristic algorithm [5]. Our max-cut partitioning scheme can adapt any existing graph partitioning algorithms as the basis for declustering. Readers are referred to <ref> [21] </ref> for a survey and comparison of alternate graph partitioning algorithms. the procedure find-initial-partition (). Then, it repeatedly applies the 2-way-maxcut-partition procedure to pairs of subsets and makes the partition as close as possible to being pairwise optimal (pairwise max-cut).
Reference: [22] <author> B.W. Kernighan and S. Lin. </author> <title> "An Efficient Heuristic Procedure for Partitioning Graphs". </title> <journal> Bell Syst. Tech. J., </journal> <volume> 49(2) </volume> <pages> 291-307, </pages> <month> February </month> <year> 1970. </year>
Reference-contexts: Unfortunately, the max-cut graph-partitioning problem, as stated, remains NP-complete [14], which can be shown by reducing it to the complementary min-cut graph partitioning problem <ref> [22] </ref>. The min-cut graph partitioning problem is to partition the nodes of a graph with weights on its edges into subsets of given sizes, so as to minimize the sum of the weights on all of the edges in the cut-set. Kernighan and Lin [22] have shown that by changing the <p> the complementary min-cut graph partitioning problem <ref> [22] </ref>. The min-cut graph partitioning problem is to partition the nodes of a graph with weights on its edges into subsets of given sizes, so as to minimize the sum of the weights on all of the edges in the cut-set. Kernighan and Lin [22] have shown that by changing the signs of all the edge weights, the max-cut graph partitioning problem can be transformed into the min-cut graph partitioning problem 1 . <p> Many good heuristic algorithms which have been developed for the min-cut graph partitioning problem can also be applied to efficiently solve the max-cut graph-partitioning problem. We note that the minimum-spanning-trees algorithm can also be used as a simple heuristic for the graph-partitioning approach. However, the ratio-cut [5], KL <ref> [22] </ref> and other methods [21] are superior [5, 21, 22]. These heuristics are based on spectral partitioning and iterative approaches. In spectral techniques [1], eigenvalues and eigenvectors of the matrix representation of the graph are computed. Partition results are derived by mapping the information provided by eigenvectors onto actual partitions. <p> We note that the minimum-spanning-trees algorithm can also be used as a simple heuristic for the graph-partitioning approach. However, the ratio-cut [5], KL [22] and other methods [21] are superior <ref> [5, 21, 22] </ref>. These heuristics are based on spectral partitioning and iterative approaches. In spectral techniques [1], eigenvalues and eigenvectors of the matrix representation of the graph are computed. Partition results are derived by mapping the information provided by eigenvectors onto actual partitions. <p> These heuristics are based on spectral partitioning and iterative approaches. In spectral techniques [1], eigenvalues and eigenvectors of the matrix representation of the graph are computed. Partition results are derived by mapping the information provided by eigenvectors onto actual partitions. Iterative approaches <ref> [5, 13, 22, 35] </ref> start from an initial partition, then iteratively apply pairwise swapping or moving of nodes across partitions to minimize edge cuts. In general, graph partitioning algorithms are expensive and their CPU and memory costs often grow non-linearly with the size of the graph. <p> Those data items which belong to the same group are allocated to the same disk. In general, N-way partitioning can be implemented via generalizing the two-way partitioning or via repeated applications of the two-way partitioning <ref> [22] </ref>. We implement the N-way max-cut graph partitioning by repeated applications of two-way graph partitioning, using the modified ratio-cut heuristic [5] which is described in Appendix B. <p> The pairwise optimization process may converge quickly, if the number of disks (groups) is not very large. The CPU cost for partitioning can also be controlled by T , which limits the number of passes for pairwise optimization. As mentioned in <ref> [22] </ref>, pairwise optimality is only a necessary condition for global optimality. There may be situations when some complex interchange of three or more elements from three or more groups is required to achieve global optimum. <p> Other multi-way graph partitioning methods [35] can also be used as the basis of our scheme to further improve the result of partitioning, if computation complexity and CPU cost is not a concern. To find the initial partition, we use the following two approaches suggested in <ref> [22] </ref> 2 . In the case that N is a power of 2, we apply the 2-way-maxcut-partition () procedure to partition the initial data set into two equally balanced subsets, then we repeatedly partition each of the subsets into two equally balanced subsets until N subsets are found.
Reference: [23] <author> K. Kim and V.K. Prasanna. </author> <title> "Latin Squares for Parallel Array Access". </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 4(4) </volume> <pages> 361-370, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: Several single-attribute functions including round robin, hash-partitioning, key-range partitioning [6, 15], and a hybrid of these [16], as well as multi-attribute functions including grid-based multi-dimensional key-range-partitioning [17], disk modulo [8, 27], generalized disk modulo (linear) [8, 36], field-wise-XOR [24], Hilbert [9], error-correcting code [11], latin-square <ref> [23, 36] </ref>, random [3], vector-based declustering [4] and lattice [36] have been proposed and evaluated. A survey of multi-attribute functions can be found in [11, 9]. These methods are limited in the case of managing updates, non-uniform data-distributions and non-uniform data-sizes.
Reference: [24] <author> M.H. Kim and S. Pramanik. </author> <title> "Optimal File Distribution for Partial Match Queries". </title> <booktitle> In Proc. of SIGMOD Conference on Management of Data, </booktitle> <pages> pages 173-182. </pages> <publisher> ACM, </publisher> <year> 1988. </year>
Reference-contexts: Several single-attribute functions including round robin, hash-partitioning, key-range partitioning [6, 15], and a hybrid of these [16], as well as multi-attribute functions including grid-based multi-dimensional key-range-partitioning [17], disk modulo [8, 27], generalized disk modulo (linear) [8, 36], field-wise-XOR <ref> [24] </ref>, Hilbert [9], error-correcting code [11], latin-square [23, 36], random [3], vector-based declustering [4] and lattice [36] have been proposed and evaluated. A survey of multi-attribute functions can be found in [11, 9]. These methods are limited in the case of managing updates, non-uniform data-distributions and non-uniform data-sizes.
Reference: [25] <author> V. Kouramajian, R. Elmasri, and A. Chaudhry. </author> <title> "Declustering Techniques for Parallelizing Temporal Access Structures". </title> <booktitle> In Proc. of the Tenth Intl Conference on Data Engineering. IEEE, </booktitle> <year> 1994. </year>
Reference-contexts: Furthermore, they are limited in their ability to adapt to available information about query distribution and size constraints. Lastly, these techniques are not designed for schema partitioning. Index-specific load-balancing based declustering methods have been proposed for B-tree [30], R-tree [20] and the temporal index <ref> [25] </ref>, etc. Dynamic file allocation methods are proposed in the FIVE system [34]. These methods are incremental in nature to balance the load (e.g. storage, I/O time) in various partitions for a local window (i.e. a subset of existing data-items in partitions) around the new data-item.
Reference: [26] <author> R. Krishnamurthy and K.-Y. Whang. </author> <title> "Multilevel Grid Files". </title> <institution> IBM Research Report, </institution> <year> 1985. </year>
Reference-contexts: We then generate and insert 3K/4 other points from the normal distribution, with a small standard deviation. We note that the hot-spot data-set is not factorizable, and that it has been used in the literature <ref> [26] </ref> to simulate skewed distributions. The data-page distribution over the 20x20 grid directory is shown in Appendix F, Table 6. It is possible to use the "fractal dimension" [10] to generate experimental data.
Reference: [27] <author> J. Li, J. Srivastava, and D. Rotem. "CMD: </author> <title> A Multidimensional Declustering Method for Parallel Database Systems". </title> <booktitle> In Proc. of Intl Conference on Very Large Data Bases, </booktitle> <year> 1992. </year>
Reference-contexts: Several single-attribute functions including round robin, hash-partitioning, key-range partitioning [6, 15], and a hybrid of these [16], as well as multi-attribute functions including grid-based multi-dimensional key-range-partitioning [17], disk modulo <ref> [8, 27] </ref>, generalized disk modulo (linear) [8, 36], field-wise-XOR [24], Hilbert [9], error-correcting code [11], latin-square [23, 36], random [3], vector-based declustering [4] and lattice [36] have been proposed and evaluated. A survey of multi-attribute functions can be found in [11, 9]. <p> For the experiments on the effects of page-sharing, data was inserted into the grid file with an initial 1x1 grid directory structure. 23 Declustering Methods : We compare max-cut declustering with two well-known mapping--function-based methods, the Hilbert (MF-Hilb) allocation method [9] and the Linear method <ref> [27, 8, 36] </ref>. The Hilbert method is chosen for comparison, since it achieves good performance for square-shaped range queries [9]. Two types of linear methods are used in our experiments. The first one, denoted as MF-CMD, uses the modulo function (x + y) mod N. <p> The first one, denoted as MF-CMD, uses the modulo function (x + y) mod N. The second one, represented as MF-GDM, uses the modulo function (x + 5y) mod N. The MF-CMD method is also known as the CMD method <ref> [27] </ref> or the Disk Modulo method (DM) [8] in a two-dimensional space. The specific mapping function method, MF-GDM, that we choose in the experiment, belongs to the class of Generalized Disk Modulo methods (GDM) [8], vector based declustering methods [4] and lattice allocation methods [36].
Reference: [28] <author> J. Nievergelt, H. Hinteberger, and K.D. Sevcik. </author> <title> "The Grid File: An Adaptable, Symmetric Multi-Key File Structure". </title> <journal> ACM Trans. on Database Systems, </journal> <volume> 9(1) </volume> <pages> 38-71, </pages> <year> 1984. </year>
Reference-contexts: These experiments also facilitate the characterization of the dominance region of the proposed method in the context of multi-dimensional data. In the future, we intend to extend the comparisons to other domains. 6.1 Parallelizing Grid Files We apply the max-cut declustering scheme to parallelizing Grid files <ref> [28] </ref>, a well known access method for multi-dimensional and spatial data. Multi-dimensional data refers to a collection of data values embedded in a coordinate space which has dimension 2. The Grid file partitions the coordinate space into rectangular grids called cells. <p> Most existing declustering methods for grid files [18, 36] are based on coordinate mapping of the data pages in the grid-directory to the disk-id, assuming that the data are uniformly distributed. However, for many nonuniform distributions, multiple grid cells may need to share a disk block <ref> [28] </ref>, and the mapping-function based methods will then need to resolve conflicts. In addition, the mapping-based methods need to be extended to deal with the splitting and merging that result from updates, since these events may change the grid-coordinates of existing data-pages.
Reference: [29] <author> D. Rotem, G.A. Schloss, and A. Segev. </author> <title> "Data Allocation for Multidisk Databases". </title> <journal> IEEE Trans. on Knowledge and Data Engineering, </journal> <volume> 5(5), </volume> <month> October </month> <year> 1993. </year>
Reference-contexts: 1 Introduction With an increasing performance gap between processors and I/O systems, parallelizing I/O operations by declustering <ref> [12, 11, 29] </ref> data is becoming essential for high performance applications. Database machines, multi-processors and parallel computers can all benefit from effective declus-tering. <p> Unfortunately, this problem is NP-complete in several contexts, which include partial match queries on cartesian product files [11] and join queries on a set of relations <ref> [29] </ref>. Thus any method to solve this problem in polynomial time will be heuristic. We address the declustering problem in a single processor with a multi-disk environment. We abstract the properties of multi-disk secondary storage systems in terms of their capability of carrying out N-independent disk operations in parallel.
Reference: [30] <author> B. Seeger and P.A. Larson. </author> <title> "Multi-Disk B-trees". </title> <booktitle> In Proc. of Intl Conference on Management of Data. ACM SIGMOD, </booktitle> <year> 1991. </year> <month> 32 </month>
Reference-contexts: Furthermore, they are limited in their ability to adapt to available information about query distribution and size constraints. Lastly, these techniques are not designed for schema partitioning. Index-specific load-balancing based declustering methods have been proposed for B-tree <ref> [30] </ref>, R-tree [20] and the temporal index [25], etc. Dynamic file allocation methods are proposed in the FIVE system [34]. <p> It is likely to achieve better quality of declustering; however, it will have a higher cost. 3.1 Incremental Max-cut Declustering The incremental max-cut allocates data items to the disks in a greedy manner, using the max-cut similarity criterion and local load-balancing. Different from the classical local load-balancing strategy <ref> [20, 30] </ref>, which allocates a data item to the disk with the lowest load (e.g. storage) over a local window, Incremental max-cut declustering aims at allocating a data item to a disk, such that the max-cut similarity criterion within a local window is best fulfilled. <p> The load on individual disks represents the objective function of declustering. The LoadBal method uses the storage load, i.e., the number of pages within the local window residing on the disk, as the 24 measure <ref> [30] </ref>. Choice of Parameters : Our experiments with the grid file focus on the effect of query distribution and data distribution. We leave the exploration of the effect of partition constraints out of this study, as it has not been explored in related work.
Reference: [31] <author> S. Shekhar and D. R. Liu. </author> <title> "CCAM : A Connectivity-Clustered Access Method for Aggregate Queries on Transportation Networks : A Summary of Results". </title> <booktitle> In Proc. of the Eleventh Intl Conference on Data Engineering. IEEE, </booktitle> <month> March </month> <year> 1995, </year> <note> (An extended version is also accepted for IEEE Trans. on Knowledge and Data Engineering). </note>
Reference-contexts: In future work, we would like to evaluate the suitability of the latest graph-partitioning algorithm for declustering. 1 Interestingly, min-cut graph partitioning can be used as a clustering technique <ref> [33, 31] </ref> to put similar records into the same disk page to reduce the number of disk pages that need to be accessed, since it minimizes the weight on the cut-set, i.e., maximizes the intra-similarity.
Reference: [32] <author> S. Shekhar, S. Ravada, and et. al. </author> <title> "Load-Balancing in High Performance GIS: Partitioning Polygonal Maps". </title> <booktitle> In Proc. of 4th Symposium on Spatial Databases, </booktitle> <address> SSD'95, </address> <year> 1995, </year> <note> available via ftp: ftp.cs.umn.edu, /dept/users/shekhar/SSD95.ps. </note>
Reference-contexts: For example, in decision support systems, several reports are generated periodically (daily, weekly or monthly). In a terrain-visualization system, the simulation of a ground vehicle usually issues a fixed size (e.g. 8 km * 8 km) range queries <ref> [32] </ref>. In spatial databases, the proximity index [20] can be used to estimate the probability of a random query retrieving a given pair of rectangles. <p> Example: We use a simple example to illustrate the application of max-cut partitioning to handling data-items of different sizes. The proposed scheme has been applied to decluster polygonal maps which contain polygons of different sizes <ref> [32] </ref>. In Figure 5 (a), we show a database for four polygons. For simplicity, we assume that all the queries in this particular database are polygons-intersection queries involving two polygons (spatial join queries). The polygons are shown as vertices of the graph in Figures 5 (a) and (b). <p> However, it is not likely to change the major trends (e.g. the proposed method outperforms others for nonuniform data distributions). In fact, the proposed methods are already being used for real data sets (e.g. the polygonal map of Killeen, Texas) <ref> [32] </ref>. The relative performance and rankings of declustering methods have been found to be the same as those predicted in this paper. Grid File Creation : These data sets are stored into the grid file via a sequence of insert operations with the grid access method.
Reference: [33] <author> M. M. Tsangaris and Jeffrey.F. Naughton. </author> <title> "A Stochastic Approach for Clustering in Object Bases". </title> <booktitle> In Proc. of SIGMOD Conference on Management of Data, </booktitle> <pages> pages 12-21. </pages> <publisher> ACM, </publisher> <year> 1991. </year>
Reference-contexts: In future work, we would like to evaluate the suitability of the latest graph-partitioning algorithm for declustering. 1 Interestingly, min-cut graph partitioning can be used as a clustering technique <ref> [33, 31] </ref> to put similar records into the same disk page to reduce the number of disk pages that need to be accessed, since it minimizes the weight on the cut-set, i.e., maximizes the intra-similarity.
Reference: [34] <author> G. Weikum, P. Zabback, and P. Scheuermann. </author> <title> "Dynamic File Allocation in Disk Arrays". </title> <booktitle> In Proc. of Intl Conference on Management of Data. ACM SIGMOD, </booktitle> <year> 1991. </year>
Reference-contexts: Lastly, these techniques are not designed for schema partitioning. Index-specific load-balancing based declustering methods have been proposed for B-tree [30], R-tree [20] and the temporal index [25], etc. Dynamic file allocation methods are proposed in the FIVE system <ref> [34] </ref>. These methods are incremental in nature to balance the load (e.g. storage, I/O time) in various partitions for a local window (i.e. a subset of existing data-items in partitions) around the new data-item. The incremental nature of the load-balancing methods allows them to work well with indexing methods (e.g.
Reference: [35] <author> C.W. Yeh, C.K. Cheng, and T.T. Y. Lin. </author> <title> "A General Purpose Multiple Way Partitioning Algorithm". </title> <booktitle> In Proc. of 28th ACM/IEEE Design Automation Conference, </booktitle> <pages> pages 421-426, </pages> <year> 1991. </year>
Reference-contexts: These heuristics are based on spectral partitioning and iterative approaches. In spectral techniques [1], eigenvalues and eigenvectors of the matrix representation of the graph are computed. Partition results are derived by mapping the information provided by eigenvectors onto actual partitions. Iterative approaches <ref> [5, 13, 22, 35] </ref> start from an initial partition, then iteratively apply pairwise swapping or moving of nodes across partitions to minimize edge cuts. In general, graph partitioning algorithms are expensive and their CPU and memory costs often grow non-linearly with the size of the graph. <p> As mentioned in [22], pairwise optimality is only a necessary condition for global optimality. There may be situations when some complex interchange of three or more elements from three or more groups is required to achieve global optimum. Other multi-way graph partitioning methods <ref> [35] </ref> can also be used as the basis of our scheme to further improve the result of partitioning, if computation complexity and CPU cost is not a concern. To find the initial partition, we use the following two approaches suggested in [22] 2 .
Reference: [36] <author> Y. Zhou, S. Shekhar, and M. Coyle. </author> <title> "Disk Allocation Methods for Parallelizing Grid Files". </title> <booktitle> In Proc. of the Tenth Intl Conference on Data Engineering. IEEE, </booktitle> <year> 1994. </year> <month> 33 </month>
Reference-contexts: Several single-attribute functions including round robin, hash-partitioning, key-range partitioning [6, 15], and a hybrid of these [16], as well as multi-attribute functions including grid-based multi-dimensional key-range-partitioning [17], disk modulo [8, 27], generalized disk modulo (linear) <ref> [8, 36] </ref>, field-wise-XOR [24], Hilbert [9], error-correcting code [11], latin-square [23, 36], random [3], vector-based declustering [4] and lattice [36] have been proposed and evaluated. A survey of multi-attribute functions can be found in [11, 9]. <p> Several single-attribute functions including round robin, hash-partitioning, key-range partitioning [6, 15], and a hybrid of these [16], as well as multi-attribute functions including grid-based multi-dimensional key-range-partitioning [17], disk modulo [8, 27], generalized disk modulo (linear) [8, 36], field-wise-XOR [24], Hilbert [9], error-correcting code [11], latin-square <ref> [23, 36] </ref>, random [3], vector-based declustering [4] and lattice [36] have been proposed and evaluated. A survey of multi-attribute functions can be found in [11, 9]. These methods are limited in the case of managing updates, non-uniform data-distributions and non-uniform data-sizes. <p> round robin, hash-partitioning, key-range partitioning [6, 15], and a hybrid of these [16], as well as multi-attribute functions including grid-based multi-dimensional key-range-partitioning [17], disk modulo [8, 27], generalized disk modulo (linear) [8, 36], field-wise-XOR [24], Hilbert [9], error-correcting code [11], latin-square [23, 36], random [3], vector-based declustering [4] and lattice <ref> [36] </ref> have been proposed and evaluated. A survey of multi-attribute functions can be found in [11, 9]. These methods are limited in the case of managing updates, non-uniform data-distributions and non-uniform data-sizes. Furthermore, they are limited in their ability to adapt to available information about query distribution and size constraints. <p> We note that no methods can achieve strict optimality for all query-sets <ref> [7, 18, 36] </ref>. For example, no method can be strictly optimal for all range queries if the number of disks is greater than 5 [18, 36]. However, some of the existing declustering methods have been proved to be strictly optimal for simpler query sets [8, 11, 36]. <p> We note that no methods can achieve strict optimality for all query-sets [7, 18, 36]. For example, no method can be strictly optimal for all range queries if the number of disks is greater than 5 <ref> [18, 36] </ref>. However, some of the existing declustering methods have been proved to be strictly optimal for simpler query sets [8, 11, 36]. We use a weighted similarity graph to capture the similarity relationship between data items. The nodes in the graph represent the data items. <p> For example, no method can be strictly optimal for all range queries if the number of disks is greater than 5 [18, 36]. However, some of the existing declustering methods have been proved to be strictly optimal for simpler query sets <ref> [8, 11, 36] </ref>. We use a weighted similarity graph to capture the similarity relationship between data items. The nodes in the graph represent the data items. The similarity between two data items is quantified as the weight on the edge connecting them. <p> Proof: A linear allocation method, Disk (x, y) = (px + qy + r) mod N, where GCD (p,N) = GCD (q,N) = 1, is strictly optimal with respect to Row/Col queries over a two-dimensional grid <ref> [36] </ref>. Therefore, the corollary follows from Theorem 1. The above result can be extended to Row/Col type queries over a k-dimensional grid. The partial match queries with only one unspecified attribute over a cartesian product file are instances of Row/Col type queries over a k-dimensional grid. <p> Theorem 1 demonstrates that the max-cut declustering (partition) is capable of achieving strictly optimal declustering if a strictly optimal declustering exists. However, strictly optimal declustering does not exist for all query-sets, for example range queries <ref> [36] </ref>. In this situation, no declustering method can achieve theoretically optimal response time. <p> The result can be generalized to higher dimensions. Queries on 20 multi-dimensional data represent various subsets of the data. To process a query, the database manager has to retrieve the data points contained inside each cell which intersects with the query. Most existing declustering methods for grid files <ref> [18, 36] </ref> are based on coordinate mapping of the data pages in the grid-directory to the disk-id, assuming that the data are uniformly distributed. <p> Suppose that the grid file is an MxM grid. The row-query (Row) set is a set of 1xM rectangle-shaped queries, and the column-query (Col) set is a set of Mx1 rectangle-shaped queries. The diagonal queries include the Principal Diagonal (PD) queries and the Anti-Diagonal (PA) queries <ref> [36] </ref>. <p> For the experiments on the effects of page-sharing, data was inserted into the grid file with an initial 1x1 grid directory structure. 23 Declustering Methods : We compare max-cut declustering with two well-known mapping--function-based methods, the Hilbert (MF-Hilb) allocation method [9] and the Linear method <ref> [27, 8, 36] </ref>. The Hilbert method is chosen for comparison, since it achieves good performance for square-shaped range queries [9]. Two types of linear methods are used in our experiments. The first one, denoted as MF-CMD, uses the modulo function (x + y) mod N. <p> The specific mapping function method, MF-GDM, that we choose in the experiment, belongs to the class of Generalized Disk Modulo methods (GDM) [8], vector based declustering methods [4] and lattice allocation methods <ref> [36] </ref>. Figure 8 illustrates the disk assignments produced by these methods in a 8x8 dense grid directory. Two global max-cut declustering strategies are derived which use different approaches to estimate the weights. <p> We broke the tie in a fair manner by choosing a disk at random. This is consistent with the tie-breaking approaches used previously in mapping-function based methods to decluster grid files <ref> [36] </ref>. Each query is executed by searching qualified grid cells to retrieve data pages.
References-found: 36

