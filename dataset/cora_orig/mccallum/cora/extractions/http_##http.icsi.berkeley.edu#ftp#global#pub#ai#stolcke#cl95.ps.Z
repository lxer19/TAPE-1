URL: http://http.icsi.berkeley.edu/ftp/global/pub/ai/stolcke/cl95.ps.Z
Refering-URL: http://http.icsi.berkeley.edu/ftp/global/pub/ai/stolcke/
Root-URL: http://http.icsi.berkeley.edu
Title: An Efficient Probabilistic Context-Free Parsing Algorithm that Computes Prefix Probabilities  
Author: Andreas Stolcke 
Affiliation: University of California at Berkeley and International Computer Science Institute  
Abstract: We describe an extension of Earley's parser for stochastic context-free grammars that computes the following quantities given a stochastic context-free grammar and an input string: a) probabilities of successive prefixes being generated by the grammar; b) probabilities of substrings being generated by the nonterminals, including the entire string being generated by the grammar; c) most likely (Viterbi) parse of the string; d) posterior expected number of applications of each grammar production, as required for reestimating rule probabilities. Probabilities (a) and (b) are computed incrementally in a single left-to-right pass over the input. Our algorithm compares favorably to standard bottom-up parsing methods for SCFGs in that it works efficiently on sparse grammars by making use of Earley's top-down control structure. It can process any context-free rule format without conversion to some normal form, and combines computations for (a) through (d) in a single algorithm. Finally, the algorithm has simple extensions for processing partially bracketed inputs, and for finding partial parses and their likelihoods on ungrammatical inputs. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aho, Alfred V., and Ullman, Jeffrey D. </author> <year> (1972). </year> <title> The Theory of Parsing, Translation, and Compiling. Volume1: Parsing. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J. </address>
Reference-contexts: mapping between partial derivations and Earley paths, such that each production X ! applied in a derivation corresponds to a predicted Earley state X ! :. (a) is the invariant underlying the correctness and completeness of Earley's algorithm; it can be proved by induction on the length of a derivation <ref> (Aho and Ullman 1972, Theorem 4.9) </ref>. The slightly stronger form (b) follows from (a) and the way possible prediction steps are defined. Since we have established that paths correspond to derivations, it is convenient to associate derivation probabilities directly with paths. <p> As in prediction, forward and inner probabilities are multiplied by the corresponding *-expansion probabilities. 4.7.4 Eliminating null productions. Given these added complications one might consider simply eliminating all *-productions in a preprocessing step. This is mostly straightforward and analogous to the corresponding procedure for non-probabilistic CFGs <ref> (Aho and Ullman 1972, Algorithm 2.10) </ref>. The main difference is the updating of rule probabilities, for which the *-expansion probabilities are again needed. 1. Delete all null productions, except on the start symbol (in case the grammar as a whole produces * with non-zero probability). <p> This approach is a subject of ongoing work, in the context of tight-coupling SCFGs with speech decoders (Jurafsky et al. 1995). 6.2 Relation to probabilistic LR parsing One of the major alternative context-free parsing paradigms besides Earley's algorithm is LR parsing <ref> (Aho and Ullman 1972) </ref>. A comparison of the two approaches, both in their probabilistic and non-probabilistic aspects, is interesting and provides useful insights. The following remarks assume familiarity with both approaches.
Reference: <author> Bahl, Lalit R.; Jelinek, Frederick; and Mercer, Robert L. </author> <year> (1983). </year> <title> A maximum likelihood approach to continuous speech recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 5(2) </volume> <pages> 179-190. </pages>
Reference-contexts: These conditional probabilities can then be used as word transition probabilities in a Viterbi-style decoder or to incrementally compute the cost function for a stack decoder <ref> (Bahl, Jelinek, and Mercer 1983) </ref>. Another application where prefix probabilities play a central role is the extraction of n-gram probabilities from SCFGs (Stolcke and Segal 1994). Here, too, efficient incremental computation saves time since the work for common prefix strings can be shared.
Reference: <author> Baker, James K. </author> <year> (1979). </year> <title> Trainable grammars for speech recognition. In Speech Communication Papers for the 97th Meeting of the Acoustical Society of America, edited by Jared J. </title> <editor> Wolf and Dennis H. Klatt, </editor> <address> 547-550, </address> <publisher> MIT, </publisher> <address> Cambridge, Mass. </address>
Reference-contexts: Partial parses are assembled just as in non-probabilistic parsing (modulo possible pruning based on probabilities), while substring probabilities (also known as inside probabilities) can be computed in a straightforward way. Thus, the CYK chart parser underlies the standard solutions to problems (1) and (4) <ref> (Baker 1979) </ref>, as well as (2) (Jelinek 1985). While the Jelinek and Lafferty (1991) solution to problem (3) is not a direct extension of CYK parsing they nevertheless present their algorithm in terms of its similarities to the computation of inside probabilities. <p> This constitutes the main distinguishing feature of Earley parsing compared to the strict bottom-up computation used in the standard inside probability computation <ref> (Baker 1979) </ref>. There, inside probabilities for all positions and nonterminals are computed, regardless of possible prefixes. 6 The same technical complication was noticed by Wright (1990) in the computation of probabilistic LR parser tables. The relation to LR parsing will be discussed in Section 6.2. <p> The total time is therefore O (l 3 ) for an input of length l, which is also the complexity of the standard Inside/Outside <ref> (Baker 1979) </ref> and LRI (Jelinek and Lafferty 1991) algorithms. For grammars of bounded ambiguity, the incremental per-word cost reduces to O (l), O (l 2 ) total. For deterministic CFGs the incremental cost is constant, O (l) total.
Reference: <author> Baum, Leonard E.; Petrie, Ted; Soules, George; and Weiss, </author> <title> Norman (1970). A maximization technique occuring in the statistical analysis of probabilistic functions in Markov chains. </title> <journal> The Annals of Mathematical Statistics, </journal> <volume> 41(1) </volume> <pages> 164-171. </pages>
Reference-contexts: EM is a generalization of the well-known Baum-Welch algorithm for HMM estimation <ref> (Baum et al. 1970) </ref>; the original formulation for the case of SCFGs is due to Baker (1979). For SCFGs, the E-step involves computing the expected number of times each production is applied in generating the training corpus.
Reference: <author> Booth, Taylor L., and Thompson, Richard A. </author> <year> (1973). </year> <title> Applying probability measures to abstract languages. </title> <journal> IEEE Transactions on Computers, C-22(5):442-450. </journal>
Reference: <author> Briscoe, Ted, and Carroll, </author> <title> John (1993). Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars. </title> <journal> Computational Linguistics, </journal> <volume> 19(1) </volume> <pages> 25-59. </pages>
Reference: <author> Casacuberta, F., and Vidal, E. </author> <year> (1988). </year> <title> A parsing algorithm for weighted grammars and substring recognition. In Syntactic and Structural Pattern Recognition, edited by Gabriel Ferrat e, </title> <editor> Theo Pavlidis, Alberto Sanfeliu, and Horst Bunke, </editor> <booktitle> volume F45 of NATO ASI Series. </booktitle> <publisher> Springer Verlag, Berlin, </publisher> <pages> 51-67. </pages>
Reference: <author> Corazza, Anna; De Mori, Renato; Gretter, Roberto; and Satta, </author> <title> Giorgio (1991). Computation of probabilities for an island-driven parser. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(9) </volume> <pages> 936-950. </pages>
Reference-contexts: which defines a language as a probability distribution over strings, they have been used in a variety of applications: for the selection of parses for ambiguous inputs (Fujisaki et al. 1991); to guide the rule choice efficiently during parsing (Jones and Eisner 1992); to compute island probabilities for non-linear parsing <ref> (Corazza et al. 1991) </ref>. In speech recognition, probabilistic context-free grammars play a central role in integrating low-level word models with higher-level language models (Ney 1992), as well as in non-finite state acoustic and phonotactic modeling (Lari and Young 1991).
Reference: <author> Dempster, A. P.; Laird, N. M.; and Rubin, D. B. </author> <year> (1977). </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 34 </volume> <month> 1-38. </month> <title> Andreas Stolcke Efficient Probabilistic Context-Free Parsing Earley, Jay (1970). An efficient context-free parsing algorithm. </title> <journal> Communications of the ACM, </journal> <volume> 6(8) </volume> <pages> 451-455. </pages>
Reference-contexts: :Y ) as well as T 0 = Viterbi-parse (i : j Y ! :) Adjoin T 0 to T as the right-most child at the root, and return T . 5.2 Rule probability estimation The rule probabilities in a SCFG can be iteratively estimated using the EM (Expectation-Maximization) algorithm <ref> (Dempster, Laird, and Rubin 1977) </ref>.
Reference: <author> Fujisaki, T.; Jelinek, F.; Cocke, J.; Black, E.; and Nishino, T. </author> <year> (1991). </year> <title> A probabilistic parsing method for sentence disambiguation. In Current Issues in Parsing Technology, edited by Masaru Tomita. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <note> chapter 10, 139-152. </note>
Reference-contexts: 1. Introduction Context-free grammars are widely used as models of natural language syntax. In their probabilistic version, which defines a language as a probability distribution over strings, they have been used in a variety of applications: for the selection of parses for ambiguous inputs <ref> (Fujisaki et al. 1991) </ref>; to guide the rule choice efficiently during parsing (Jones and Eisner 1992); to compute island probabilities for non-linear parsing (Corazza et al. 1991).
Reference: <author> Graham, Susan L.; Harrison, Michael A.; and Ruzzo, Walter L. </author> <year> (1980). </year> <title> An improved context-free recognizer. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 2(3) </volume> <pages> 415-462. </pages>
Reference: <author> Jelinek, </author> <title> Frederick (1985). Markov source modeling of text generation. In The Impact of Processing Techniques on Communications, edited by J. </title> <editor> K. Skwirzynski, </editor> <booktitle> volume E91 of NATO ASI Series. </booktitle> <publisher> Nijhoff, Dordrecht, </publisher> <pages> 569-598. </pages> <booktitle> Proceedings of the NATO Advanced Study Institute, </booktitle> <address> Bonas, France, </address> <month> July </month> <year> 1983. </year>
Reference-contexts: Thus, the CYK chart parser underlies the standard solutions to problems (1) and (4) (Baker 1979), as well as (2) <ref> (Jelinek 1985) </ref>. While the Jelinek and Lafferty (1991) solution to problem (3) is not a direct extension of CYK parsing they nevertheless present their algorithm in terms of its similarities to the computation of inside probabilities.
Reference: <author> Jelinek, Frederick, and Lafferty, John D. </author> <year> (1991). </year> <title> Computation of the probability of initial substring generation by stochastic context-free grammars. </title> <journal> Computational Linguistics, </journal> <volume> 17(3) </volume> <pages> 315-323. </pages>
Reference-contexts: The total time is therefore O (l 3 ) for an input of length l, which is also the complexity of the standard Inside/Outside (Baker 1979) and LRI <ref> (Jelinek and Lafferty 1991) </ref> algorithms. For grammars of bounded ambiguity, the incremental per-word cost reduces to O (l), O (l 2 ) total. For deterministic CFGs the incremental cost is constant, O (l) total.
Reference: <author> Jelinek, Frederick; Lafferty, John D.; and Mercer, Robert L. </author> <year> (1992). </year> <title> Basic methods of probabilistic context free grammars. In Speech Recognition and Understanding. Recent Advances, Trends, and Applications, edited by Pietro Laface and Renato De Mori, </title> <booktitle> volume F75 of NATO ASI Series. </booktitle> <publisher> Springer Verlag, Berlin, </publisher> <pages> 345-360. </pages> <booktitle> Proceedings of the NATO Advanced Study Institute, </booktitle> <address> Cetraro, Italy, </address> <month> July </month> <year> 1990. </year>
Reference: <author> Jones, Mark A., and Eisner, Jason M. </author> <year> (1992). </year> <title> A probabilistic parser and its applications. </title> <booktitle> In AAAI Workshop on Statistically-Based NLP Techniques, </booktitle> <pages> 20-27, </pages> <address> San Jose, CA. </address>
Reference-contexts: In their probabilistic version, which defines a language as a probability distribution over strings, they have been used in a variety of applications: for the selection of parses for ambiguous inputs (Fujisaki et al. 1991); to guide the rule choice efficiently during parsing <ref> (Jones and Eisner 1992) </ref>; to compute island probabilities for non-linear parsing (Corazza et al. 1991). In speech recognition, probabilistic context-free grammars play a central role in integrating low-level word models with higher-level language models (Ney 1992), as well as in non-finite state acoustic and phonotactic modeling (Lari and Young 1991).
Reference: <author> Jurafsky, Daniel; Wooters, Chuck; Segal, Jonathan; Stolcke, Andreas; Fosler, Eric; Tajchman, Gary; and Morgan, </author> <title> Nelson (1995). Using a stochastic context-free grammar as a language model for speech recognition. </title> <booktitle> In Proceedings IEEE Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> volume 1, </volume> <pages> 189-192, </pages> <address> Detroit. </address>
Reference-contexts: On the contrary, by using Earley-style parsing with a set of carefully designed and estimated fault tolerant top-level productions, it should be possible to use probabilities to better advantage in robust parsing. This approach is a subject of ongoing work, in the context of tight-coupling SCFGs with speech decoders <ref> (Jurafsky et al. 1995) </ref>. 6.2 Relation to probabilistic LR parsing One of the major alternative context-free parsing paradigms besides Earley's algorithm is LR parsing (Aho and Ullman 1972). A comparison of the two approaches, both in their probabilistic and non-probabilistic aspects, is interesting and provides useful insights. <p> The parser now uses the method described here to provide exact SCFG prefix and next-word probabilities to a tightly-coupled speech decoder <ref> (Jurafsky et al. 1995) </ref>. An essential idea in the probabilistic formulation of Earley's algorithm is the collapsing of recursive predictions and unit completion chains, replacing both with lookups in precomputed matrices. This idea arises in our formulation out of the need to compute probability sums given as infinite series.
Reference: <author> Jurafsky, Daniel; Wooters, Chuck; Tajchman, Gary; Segal, Jonathan; Stolcke, Andreas; Fosler, Eric; and Morgan, </author> <title> Nelson (1994). The Berkeley Restaurant Project. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing, </booktitle> <volume> volume 4, </volume> <pages> 2139-2142, </pages> <address> Yokohama. </address>
Reference-contexts: A similar queuing scheme, with the start index order reversed, can be used for the reverse completion step needed in the computation of outer probabilities (Section 5.2). B.3 Efficient parsing with large sparse grammars During work with a moderate-sized, application-specific natural language grammar taken from the BeRP speech system <ref> (Jurafsky et al. 1994) </ref> we had opportunity to optimize our implementation of the algorithm. Below we relate some of the lessons learned in the process. 32 Andreas Stolcke Efficient Probabilistic Context-Free Parsing B.3.1 Speeding up matrix inversions.
Reference: <author> Kupiec, </author> <title> Julian (1992). Hidden Markov estimation for unrestricted stochastic context-free grammars. </title> <booktitle> In Proceedings IEEE Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> volume 1, </volume> <pages> 177-180, </pages> <address> San Francisco. </address>
Reference-contexts: In this respect the Earley approach contrasts with both the CNF-oriented I/O and LRI algorithms. Another approach to avoiding the CNF constraint is a formulation based on probabilistic Recursive Transition Networks (RTNs) <ref> (Kupiec 1992) </ref>.
Reference: <author> Lari, K., and Young, S. J. </author> <year> (1990). </year> <title> The estimation of stochastic context-free grammars using the Inside-Outside algorithm. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 4 </volume> <pages> 35-56. </pages>
Reference-contexts: Unfortunately, it seems that in the case of unconstrained SCFG estimation local maxima present a very real problem, and make success dependent on chance and initial conditions <ref> (Lari and Young 1990) </ref>. Pereira and Schabes (1992) showed that partially bracketed input samples can alleviate the problem in certain cases. The bracketing information constrains the parse of the inputs, and therefore the parameter estimates, steering it clear from some of the suboptimal solutions that could otherwise be found.
Reference: <author> Lari, K., and Young, S. J. </author> <year> (1991). </year> <title> Applications of stochastic context-free grammars using the Inside-Outside algorithm. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 5 </volume> <pages> 237-257. </pages>
Reference-contexts: In speech recognition, probabilistic context-free grammars play a central role in integrating low-level word models with higher-level language models (Ney 1992), as well as in non-finite state acoustic and phonotactic modeling <ref> (Lari and Young 1991) </ref>. In some work, context-free grammars are combined with scoring functions that are not strictly probabilistic (Nakagawa 1987), or they are used with context-sensitive and/or semantic probabilities (Magerman and Marcus 1991; Magerman and Weir 1992; Jones and Eisner 1992; Briscoe and Carroll 1993).
Reference: <author> Magerman, David M., and Marcus, Mitchell P. </author> <year> (1991). </year> <title> Pearl: A probabilistic chart parser. </title> <booktitle> In Proceedings of the 2nd International Workshop on Parsing Technologies, </booktitle> <pages> 193-199, </pages> <address> Cancun, Mexico. </address>
Reference: <author> Magerman, David M., and Weir, </author> <title> Carl (1992). Efficiency, robustness and accuracy in Picky chart parsing. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> 40-47, </pages> <institution> University of Delaware, Newark, Delaware. </institution>
Reference-contexts: Another advantage is that our probabilistic Earley parser has been extended to take advantage of partially bracketed input, and to return partial parses on ungrammatical input. The latter extension removes one of the common objections against top-down, predictive (as opposed to bottom-up) parsing approaches <ref> (Magerman and Weir 1992) </ref>. 2. Overview The remainder of the article proceeds as follows. Section 3 briefly reviews the workings of an Earley parser without regard to probabilities. Section 4 describes how the parser needs to be extended to compute sentence and prefix probabilities. <p> Traditionally it was seen as a drawback of top-down parsing algorithms such as Earley's that they sacrifice robustness, i.e., the ability to find partial parses in an ungrammatical input, for the efficiency gained from top-down prediction <ref> (Magerman and Weir 1992) </ref>. 25 Computational Linguistics Volume 21, Number 2 One approach to the problem is to build robustness into the grammar itself. In the simplest case one could add top-level productions S ! XS where X can expand to any nonterminal, including an unknown word category.
Reference: <author> Nakagawa, </author> <month> Sei-ichi </month> <year> (1987). </year> <title> Spoken sentence recognition by time-synchronous parsing algorithm of context-free grammar. </title> <booktitle> In Proceedings IEEE Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> volume 2, </volume> <pages> 829-832, </pages> <address> Dallas, Texas. </address>
Reference-contexts: In some work, context-free grammars are combined with scoring functions that are not strictly probabilistic <ref> (Nakagawa 1987) </ref>, or they are used with context-sensitive and/or semantic probabilities (Magerman and Marcus 1991; Magerman and Weir 1992; Jones and Eisner 1992; Briscoe and Carroll 1993).
Reference: <author> Ney, </author> <title> Hermann (1992). Stochastic grammars and pattern recognition. In Speech Recognition and Understanding. Recent Advances, Trends, and Applications, edited by Pietro Laface and Renato De Mori, </title> <booktitle> volume F75 of NATO ASI Series. </booktitle> <publisher> Springer Verlag, Berlin, </publisher> <pages> 319-344. </pages> <booktitle> Proceedings of the NATO Advanced Study Institute, </booktitle> <address> Cetraro, Italy, </address> <month> July </month> <year> 1990. </year> <title> P aseler, Annedore (1988). Modification of Earley's algorithm for speech recognition. In Recent Advances in Speech Understanding and Dialog Systems, edited by H. </title> <editor> Niemann, M. Lang, and G. Sagerer, </editor> <booktitle> volume F46 of NATO ASI Series. </booktitle> <publisher> Springer Verlag, Berlin, </publisher> <pages> 466-472. </pages> <booktitle> Proceedings of the NATO Advanced Study Institute, </booktitle> <address> Bad Windsheim, Germany, </address> <month> July </month> <year> 1987. </year>
Reference-contexts: In speech recognition, probabilistic context-free grammars play a central role in integrating low-level word models with higher-level language models <ref> (Ney 1992) </ref>, as well as in non-finite state acoustic and phonotactic modeling (Lari and Young 1991).
Reference: <author> Pereira, Fernando, and Schabes, </author> <title> Yves (1992). Inside-outside reestimation from partially bracketed corpora. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> 128-135, </pages> <institution> University of Delaware, Newark, Delaware. </institution>
Reference: <author> Pereira, Fernando C. N., and Shieber, Stuart M. </author> <year> (1987). </year> <title> Prolog and Natural-Language Analysis. </title> <booktitle> Number 10 in 35 Computational Linguistics Volume 21, Number 2 CSLI Lecture Notes Series. Center for the Study of Language and Information, </booktitle> <address> Stanford, CA. </address>
Reference: <author> Rabiner, L. R., and Juang, B. H. </author> <year> (1986). </year> <title> An introduction to hidden Markov models. </title> <journal> IEEE ASSP Magazine, </journal> <volume> 3(1) </volume> <pages> 4-16. </pages>
Reference-contexts: This can be accomplished by attaching two probabilistic quantities to each Earley state, as follows. The terminology is derived from analogous or similar quantities commonly used in the literature on Hidden Markov Models (HMMs) <ref> (Rabiner and Juang 1986) </ref> and in Baker (1979). <p> Both the definition of Viterbi parse, and its computation are straightforward generalizations of the corresponding notion for Hidden Markov Models <ref> (Rabiner and Juang 1986) </ref>, where one computes the Viterbi path (state sequence) through an HMM. Precisely the same approach can be used in the Earley parser, using the fact that each derivation corresponds to a path. The standard computational technique for Viterbi parses is applicable here.
Reference: <author> Schabes, </author> <title> Yves (1991). An inside-outside algorithm for estimating the parameters of a hidden stochastic context-free grammar based on Earley's algorithm. Unpublished mss. </title> <booktitle> Presented at the Second Workshop on Mathematics of Language, </booktitle> <address> Tarritown, N.Y., </address> <month> May </month> <year> 1991. </year>
Reference: <author> Stolcke, </author> <title> Andreas (1993). An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. </title> <type> Technical Report TR-93-065, </type> <institution> International Computer Science Institute, Berkeley, CA. </institution> <note> Revised 1994. </note>
Reference: <author> Stolcke, Andreas, and Segal, </author> <title> Jonathan (1994). Precise n-gram probabilities from stochastic context-free grammars. </title> <booktitle> In Proceedings of the 32th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> 74-79, </pages> <address> New Mexico State University, Las Cruces, NM. </address>
Reference-contexts: These conditional probabilities can then be used as word transition probabilities in a Viterbi-style decoder or to incrementally compute the cost function for a stack decoder (Bahl, Jelinek, and Mercer 1983). Another application where prefix probabilities play a central role is the extraction of n-gram probabilities from SCFGs <ref> (Stolcke and Segal 1994) </ref>. Here, too, efficient incremental computation saves time since the work for common prefix strings can be shared. The key to most of the features of our algorithm is that it is based on the top-down parsing method for non-probabilistic CFGs developed by Earley (1970).
Reference: <author> Tomita, </author> <title> Masaru (1986). Efficient Parsing for Natural Language. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston. </address>
Reference-contexts: Generalized LR parsing is an extension that allows parallel tracking of multiple state transitions and stack actions by using a graph-structured stack <ref> (Tomita 1986) </ref>. Probabilistic LR parsing (Wright 1990) is based on LR items augmented with certain conditional probabilities. <p> However, the size of LR parser tables can be exponential in the size of the grammar (due to the number of potential item subsets). Furthermore, if the generalized LR method is used for dealing with non-deterministic grammars <ref> (Tomita 1986) </ref> the runtime on arbitrary inputs may also grow exponentially. The bottom line is that each application's needs have to be evaluated against the pros and cons of both approaches to find the best solution.
Reference: <author> Wright, J. H. </author> <year> (1990). </year> <title> LR parsing of probabilistic grammars with input uncertainty for speech recognition. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 4 </volume> <pages> 297-323. 36 </pages>
Reference-contexts: Generalized LR parsing is an extension that allows parallel tracking of multiple state transitions and stack actions by using a graph-structured stack (Tomita 1986). Probabilistic LR parsing <ref> (Wright 1990) </ref> is based on LR items augmented with certain conditional probabilities.
References-found: 32

