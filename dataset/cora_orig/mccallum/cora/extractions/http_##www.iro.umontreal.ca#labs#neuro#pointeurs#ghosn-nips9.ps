URL: http://www.iro.umontreal.ca/labs/neuro/pointeurs/ghosn-nips9.ps
Refering-URL: http://www.iro.umontreal.ca/labs/neuro/proc.html
Root-URL: http://www.iro.umontreal.ca
Email: ghosn@iro.umontreal.ca  bengioy@iro.umontreal.ca  
Title: Multi-Task Learning for Stock Selection  
Author: Joumana Ghosn Yoshua Bengio 
Address: Montreal, Qc H3C-3J7  Montreal, Qc H3C-3J7  
Affiliation: Dept. Informatique et Recherche Operationnelle Universite de Montreal  Dept. Informatique et Recherche Operationnelle Universite de Montreal  
Abstract: Artificial Neural Networks can be used to predict future returns of stocks in order to take financial decisions. Should one build a separate network for each stock or share the same network for all the stocks? In this paper we also explore other alternatives, in which some layers are shared and others are not shared. When the prediction of future returns for different stocks are viewed as different tasks, sharing some parameters across stocks is a form of multi-task learning. In a series of experiments with Canadian stocks, we obtain yearly returns that are more than 14% above various benchmarks. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Baxter, J. </author> <year> (1995). </year> <title> Learning internal representations. </title> <booktitle> In Proceedings of the Eighth International Conference on Computational Learning Theory, </booktitle> <pages> pages 311-320, </pages> <address> Santa Cruz, California. </address> <publisher> ACM Press. </publisher>
Reference-contexts: Our objective is not to use multi-task learning to improve the speed of learning the training data (Pratt, 1993; Silver and Mercer, 1995), but instead to improve generalization performance. For example, in <ref> (Baxter, 1995) </ref>, several neural networks (one for each task) are trained simultaneouly. The networks share their first hidden layers, while all the remaining layers are specific to each network.
Reference: <author> Bengio, Y. </author> <year> (1996). </year> <title> Using a financial training criterion rather than a prediction criterion. </title> <type> Technical Report #1019, </type> <institution> Dept. Informatique et Recherche Operationnelle, Universite de Montreal. </institution>
Reference-contexts: These experiments were performed on 9 years of data concerning 35 large capitalization companies of the Toronto Stock Exchange (TSE). Following the results of previous experiments <ref> (Bengio, 1996) </ref>, the networks were not trained to predict the future return of stocks, but instead to directly optimize a financial criterion. This has been found to yield returns that are significantly superior to training the ANNs to minimize the mean squared prediction error. <p> Furthermore, multiple such experiments with different initial weights were performed to verify that we did not obtain "lucky" results due to particular initial weights. The 5 concatenated test periods make an overall 5-year test period from February 1989 to January 1994. The training algorithm is described in <ref> (Bengio, 1996) </ref> and is based on the optimization of the neural network parameters with respect to a financial criterion (here maximizing the overall profit). The outputs of the neural network feed a trading module. <p> Therefore, a gradient step is performed only after presenting the whole training sequence (in order, of course). In <ref> (Bengio, 1996) </ref>, we have found this procedure to yield significantly larger profits (around 4% better annual return), at comparable risks, in comparison to training the neural network to predict expected future returns with the mean squared error criterion.
Reference: <author> Caruana, R. </author> <year> (1995). </year> <title> Learning many related tasks at the same time with backpropagation. </title> <editor> In Tesauro, G., Touretzky, D. S., and Leen, T. K., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 7, </volume> <pages> pages 657-664, </pages> <address> Cambridge, MA. </address> <publisher> MIT Press. </publisher>
Reference-contexts: The shared layers use the knowledge provided from the training examples of all the tasks to build an internal representation suitable for all these tasks. The remaining layers of each network use the internal representation to learn a specific task. In the multitask learning method used by Caruana <ref> (Caruana, 1995) </ref>, all the hidden layers are shared. They serve as mutual sources of inductive bias.
Reference: <author> Caruana, R., Baluja, S., and Mitchell, T. </author> <year> (1996). </year> <title> Using the future to "sort out" the present: Rankprop and multitask learning for medical risk evaluation. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> volume 8. </volume>
Reference: <author> Intrator, N. and Edelman, S. </author> <year> (1996). </year> <title> How to make a low-dimensional representation suitable for diverse tasks. </title> <journal> Connection Science, Special issue on Transfer in Neural Networks. </journal> <note> to appear. </note>
Reference: <author> Moody, J., Levin, U., and Rehfuss, S. </author> <year> (1993). </year> <title> Predicting the U.S. index of industrial production. </title> <booktitle> Neural Network World, </booktitle> <volume> 3(6) </volume> <pages> 791-794. </pages>
Reference: <author> Omohundro, S. </author> <year> (1996). </year> <title> Family discovery. </title> <editor> In Mozer, M., Touretzky, D., and Perrone, M., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8. </booktitle> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA. </address>
Reference-contexts: In the family discovery method <ref> (Omohundro, 1996) </ref>, a parameterized family of models is built. Several learners are trained separately on different but related tasks and their parameters are used to construct a manifold of parameters. <p> Secondly, we would like to generalize the type of multi-task learning by allowing for more freedom in the way the different tasks influence each other. Following <ref> (Omohundro, 1996) </ref>, the basic idea is to re-parameterize the parameters i 2 R n 1 of the i th model, for all n models in the following way: i = f (p i ; !) where p i 2 R n 2 , ! 2 R n 3 , and n
Reference: <author> Pratt, L. Y. </author> <year> (1993). </year> <title> Discriminability-based transfer between neural networks. </title> <editor> In Giles, C. L., Hanson, S. J., and Cowan, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 204-211, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Refenes, A. </author> <year> (1994). </year> <title> Stock performance modeling using neural networks: a comparative study with regression models. </title> <booktitle> Neural Networks, </booktitle> <volume> 7(2) </volume> <pages> 375-388. </pages>
Reference: <author> Sharpe, W. </author> <year> (1964). </year> <title> Capital asset prices: A theory of market equilibrium under conditions of risk. </title> <journal> Journal of Finance, </journal> <volume> 19 </volume> <pages> 425-442. </pages>
Reference-contexts: Beta is simply the ratio of the covariance between the portfolio return and the market return with the variance of the market. According to the Capital Asset Pricing Model <ref> (Sharpe, 1964) </ref>, beta gives a measure of "systematic" risk, i.e., as it relates to the risk of the market, whereas the variance of the return gives a measure of total risk.
Reference: <author> Sharpe, W. </author> <year> (1966). </year> <title> Mutual fund performance. </title> <journal> Journal of Business, </journal> <volume> 39(1) </volume> <pages> 119-138. </pages>
Reference-contexts: The hypothesis that alpha = 0 is clearly rejected in all cases (with t-statistics above 9, and corresponding p-values very close to 0). The reward to variability (or "Sharpe ratio") as defined in <ref> (Sharpe, 1966) </ref>, is another risk adjusted measure of performance: (r p r i ) oe p , where oe p is the standard deviation of the portfolio return (monthly returns were used here).
Reference: <author> Silver, D. L. and Mercer, R. E. </author> <year> (1995). </year> <title> Toward a model of consolidation: The retention and transfer of neural net task knowledge. </title> <booktitle> In Proceedings of the INNS World Congress on Neural Networks, </booktitle> <volume> volume 3, </volume> <pages> pages 164-169, </pages> <address> Washington, DC. </address>
References-found: 12

