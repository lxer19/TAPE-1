URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/tm.outbox/MIT-LCS-TM-548.ps.gz
Refering-URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/listings/catatm.html
Root-URL: 
Title: Cilk: An Efficient Multithreaded Runtime System  
Author: ROBERT D. BLUMOFE, CHRISTOPHER F. JOERG, BRADLEY C. KUSZMAUL, CHARLES E. LEISERSON, KEITH H. RANDALL, AND YULI ZHOU 
Date: December 19, 1995  
Address: 545 Technology Square, Cambridge MA 02139  
Affiliation: MIT Laboratory for Computer Science,  
Abstract: Cilk (pronounced silk) is a C-based runtime system for multithreaded parallel programming. In this paper, we document the efficiency of the Cilk work-stealing scheduler, both empirically and analytically. We show that on real and synthetic applications, the work and critical-path length of a Cilk computation can be used to model performance accurately. Consequently, a Cilk programmer can focus on reducing the computation's work and critical-path length, insulated from load balancing and other runtime scheduling issues. We also prove that for the class of fully strict (well-structured) programs, the Cilk scheduler achieves space, time, and communication bounds all within a constant factor of optimal. The Cilk runtime system currently runs on the Connection Machine CM5 MPP, the Intel Paragon MPP, the Sun Sparcstation SMP, and the Cilk-NOW network of workstations. Applications written in Cilk include protein folding, graphic rendering, backtrack search, and the ?Socrates chess program, which won second prize in the 1995 World Computer Chess Championship. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anderson, T. E., Bershad, B. N., Lazowska, E. D., and Levy, H. M. </author> <title> Scheduler activations: Effective kernel support for the user-level management of parallelism. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pp. 95109, </pages> <address> Pacific Grove, California, </address> <month> Oct. </month> <year> 1991. </year>
Reference: [2] <author> Blelloch, G. E. </author> <title> Programming parallel algorithms. </title> <booktitle> In Proceedings of the 1992 Dartmouth Institute for Advanced Graduate Studies (DAGS) Symposium on Parallel Computation, </booktitle> <pages> pp. 1118, </pages> <address> Hanover, New Hampshire, </address> <month> Jun. </month> <year> 1992. </year>
Reference-contexts: Many of these applications pose problems for more traditional parallel environments, such as message passing [41] and data parallel <ref> [2, 22] </ref>, because of the unpredictability of the dynamic workloads on processors. Analytically, we prove that for fully strict (well-structured) programs, Cilk's work-stealing scheduler achieves execution space, time, and communication bounds all within a constant factor of optimal. <p> Cilk provides a programming model in which work and critical-path length are observable quantities, and it delivers guaranteed performance as a function of these quantities. Work and critical-path length have been used in the theory community for years to analyze parallel algorithms [28]. Blelloch <ref> [2] </ref> has developed a performance model for data-parallel computations based on these same two abstract measures. He cites many advantages to such a model over machine-based models. Cilk provides a similar performance model for the domain of asynchronous, multi-threaded computation.
Reference: [3] <author> Blumofe, R. D. </author> <title> Executing Multithreaded Programs Efficiently. </title> <type> Ph.D. thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> Sep. </month> <year> 1995. </year>
Reference-contexts: The C code is then compiled and linked with a runtime library to run on the target platform. Currently supported targets include the Connection Machine CM5 MPP, the Intel Paragon MPP, the Sun Sparcstation SMP, and the Cilk-NOW <ref> [3, 5] </ref> network of workstations. In this paper, we focus on the Connection Machine CM5 implementation of Cilk. The Cilk scheduler on the CM5 is written in about 40 pages of C, and it performs communication among processors using the Strata [7] active-message library. <p> The expected-time bound and the expected-communication bound can be converted into high-probability bounds at the cost of only a small additive term in both cases. Full proofs of these bounds, using generalizations of the techniques developed in [4], can be found in <ref> [3] </ref>. The space bound can be obtained from a busy-leaves property [4] that characterizes the allocated closures at all times during the execution. In order to state this property simply, we first define some terms. <p> Proof sketch: The proof follows the delay-sequence argument of [4], but with some differences that we shall point out. Full details can be found in <ref> [3] </ref>, which generalizes to the situation in which a thread can have more than one successor. At any given time during the execution, we say that a thread is critical if it has not yet been executed but all of its predecessors in the dag have been executed. <p> That is, there must be some path P in the dag such that each of the s steal attempts occurs while some thread of P is critical. We do not give the construction here, but rather refer the reader to <ref> [3, 4] </ref> for directly analogous arguments. 19 The last step of the proof is to show that a delay sequence with s = (P T 1 ) is unlikely to occur. The key to this step is a lemma, which describes the structure of threads the processors' ready pools. <p> The analysis and bounds we have derived apply to fully strict programs in the case when each thread spawns at most one successor. Some programs, such as ?Socrates, contain threads that spawn several successors. In <ref> [3] </ref>, the theorems above are generalized to handle this situation as follows. Let n l denote the maximum number of threads belonging to any one procedure such that all the threads are simultaneously living during some execution. <p> The O (S 1 P ) bound on space is unchanged. Analogous high-probability bounds for time and communication can be found in <ref> [3] </ref>. 7 Conclusion To produce high-performance parallel applications, programmers often focus on communication costs and execution time, quantities that are dependent on specific machine configurations.
Reference: [4] <author> Blumofe, R. D. and Leiserson, C. E. </author> <title> Scheduling multithreaded computations by work stealing. </title> <booktitle> In Proceedings of the 35th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 356368, </pages> <address> Santa Fe, New Mexico, </address> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: With P processors, the execution time cannot be less than T 1 =P or less than T 1 . The Cilk scheduler uses work stealing <ref> [4, 8, 14, 15, 16, 21, 29, 30, 31, 37, 43] </ref> to achieve execution time very near to the sum of these two measures. <p> Other features include various abilities to override the sched-uler's decisions, including on which processor a thread should be placed and how to pack and unpack data when a closure is migrated from one processor to another. 3 The Cilk work-stealing scheduler Cilk's scheduler uses the technique of work stealing <ref> [4, 8, 14, 15, 16, 21, 29, 30, 31, 37, 43] </ref> in which a processor (the thief) who runs out of work selects another processor (the victim) from whom to steal work, and then steals the shallowest ready thread in the victim's spawn tree. <p> Cilk's strategy is for thieves to choose victims at random <ref> [4, 29, 40] </ref>. We shall now present the implementation of Cilk's work-stealing scheduler. At runtime, each processor maintains a local ready pool to hold ready closures. Each closure has an associated level, which corresponds to the thread's depth in the spawn tree. <p> This bound is existentially optimal to within a constant factor <ref> [4] </ref>. Time With P processors, the expected execution time, including scheduling overhead, is bounded by O (T 1 =P +T 1 ). <p> This bound is existentially optimal to within a constant factor [44]. The expected-time bound and the expected-communication bound can be converted into high-probability bounds at the cost of only a small additive term in both cases. Full proofs of these bounds, using generalizations of the techniques developed in <ref> [4] </ref>, can be found in [3]. The space bound can be obtained from a busy-leaves property [4] that characterizes the allocated closures at all times during the execution. In order to state this property simply, we first define some terms. <p> Full proofs of these bounds, using generalizations of the techniques developed in <ref> [4] </ref>, can be found in [3]. The space bound can be obtained from a busy-leaves property [4] that characterizes the allocated closures at all times during the execution. In order to state this property simply, we first define some terms. <p> Our strategy is to mimic the theorems of <ref> [4] </ref> for a more restricted model of multithreaded computation. As in [4], the bounds assume a communication model in which messages are delayed only by contention at destination processors, but no assumptions are made about the order in which contending messages are delivered [33]. <p> Our strategy is to mimic the theorems of <ref> [4] </ref> for a more restricted model of multithreaded computation. As in [4], the bounds assume a communication model in which messages are delayed only by contention at destination processors, but no assumptions are made about the order in which contending messages are delivered [33]. <p> Proof: The computation contains a total of T 1 instructions. Lemma 4 When the execution of a fully strict Cilk computation ends, the expected number of dollars in the WAIT bucket is less than the number of dollars in the STEAL bucket. Proof: Lemma 5 of <ref> [4] </ref> shows that if P processors make M random steal requests during the course of a computation, where requests with the same destination are serially queued at the destination, then the expected total delay is less than M . <p> Proof sketch: The proof follows the delay-sequence argument of <ref> [4] </ref>, but with some differences that we shall point out. Full details can be found in [3], which generalizes to the situation in which a thread can have more than one successor. <p> That is, there must be some path P in the dag such that each of the s steal attempts occurs while some thread of P is critical. We do not give the construction here, but rather refer the reader to <ref> [3, 4] </ref> for directly analogous arguments. 19 The last step of the proof is to show that a delay sequence with s = (P T 1 ) is unlikely to occur. The key to this step is a lemma, which describes the structure of threads the processors' ready pools. <p> Thus, the sum of the dollars is T 1 + O (P T 1 ), and the bound on execution time is obtained by dividing by P . In fact, it can be shown using the techniques of <ref> [4] </ref> that for any * &gt; 0, with probability at least 1 *, the execution time on P processors is O (T 1 =P + T 1 + lg P + lg (1=*)).
Reference: [5] <author> Blumofe, R. D. and Park, D. S. </author> <title> Scheduling large-scale parallel computations on networks of workstations. </title> <booktitle> In Proceedings of the Third International Symposium on High Performance Distributed Computing, </booktitle> <pages> pp. 96105, </pages> <address> San Francisco, California, </address> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: The C code is then compiled and linked with a runtime library to run on the target platform. Currently supported targets include the Connection Machine CM5 MPP, the Intel Paragon MPP, the Sun Sparcstation SMP, and the Cilk-NOW <ref> [3, 5] </ref> network of workstations. In this paper, we focus on the Connection Machine CM5 implementation of Cilk. The Cilk scheduler on the CM5 is written in about 40 pages of C, and it performs communication among processors using the Strata [7] active-message library.
Reference: [6] <author> Brent, R. P. </author> <title> The parallel evaluation of general arithmetic expressions. </title> <journal> Journal of the ACM, </journal> <volume> 21(2):201206, </volume> <month> Apr. </month> <year> 1974. </year>
Reference-contexts: The Cilk scheduler uses work stealing [4, 8, 14, 15, 16, 21, 29, 30, 31, 37, 43] to achieve execution time very near to the sum of these two measures. Off-line techniques for computing such efficient schedules have been known for a long time <ref> [6, 18, 19] </ref>, but this efficiency 2 has been difficult to achieve on-line in a distributed environment while simultaneously using small amounts of space and communication. We demonstrate the efficiency of the Cilk scheduler both empirically and analytically.
Reference: [7] <author> Brewer, E. A. and Blumofe, R. Strata: </author> <title> A multi-layer communications library. </title> <note> Technical Report to appear, </note> <institution> MIT Laboratory for Computer Science. </institution> <note> Available as ftp:// ftp.lcs.mit.edu/pub/supertech/strata/strata.tar.Z. </note>
Reference-contexts: In this paper, we focus on the Connection Machine CM5 implementation of Cilk. The Cilk scheduler on the CM5 is written in about 40 pages of C, and it performs communication among processors using the Strata <ref> [7] </ref> active-message library. This description of the Cilk environment corresponds to the version as of March 1995. The remainder of this paper is organized as follows. Section 2 describes Cilk's runtime data structures and the C language extensions that are used for programming. Section 3 describes the work-stealing scheduler. <p> All experiments were run on a CM5 supercomputer. The CM5 is a massively parallel computer based on 32MHz SPARC processors with a fat-tree interconnection network [32]. The Cilk runtime system on the CM5 performs communication among processors using the Strata <ref> [7] </ref> active-message library. Application performance By running our applications and measuring a suite of performance parameters, we empirically answer many questions about the effectiveness of the Cilk runtime system. We focus on the following questions.
Reference: [8] <author> Burton, F. W. and Sleep, M. R. </author> <title> Executing functional programs on a virtual tree of processors. </title> <booktitle> In Proceedings of the 1981 Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pp. 187194, </pages> <address> Portsmouth, New Hampshire, </address> <month> Oct. </month> <year> 1981. </year>
Reference-contexts: With P processors, the execution time cannot be less than T 1 =P or less than T 1 . The Cilk scheduler uses work stealing <ref> [4, 8, 14, 15, 16, 21, 29, 30, 31, 37, 43] </ref> to achieve execution time very near to the sum of these two measures. <p> Other features include various abilities to override the sched-uler's decisions, including on which processor a thread should be placed and how to pack and unpack data when a closure is migrated from one processor to another. 3 The Cilk work-stealing scheduler Cilk's scheduler uses the technique of work stealing <ref> [4, 8, 14, 15, 16, 21, 29, 30, 31, 37, 43] </ref> in which a processor (the thief) who runs out of work selects another processor (the victim) from whom to steal work, and then steals the shallowest ready thread in the victim's spawn tree. <p> First, to lower communication costs, we would like to steal large amounts of work, and in a tree-structured computation, shallow threads are likely to spawn more work than deep ones. This heuristic notion is the justification cited by earlier researchers <ref> [8, 15, 21, 35, 43] </ref> who proposed stealing work that is shallow in the spawn tree. We cannot, however, prove that shallow threads are more likely to spawn work than deep ones. What we prove in Section 6 is the following algorithmic property.
Reference: [9] <author> Carlisle, M. C., Rogers, A., Reppy, J. H., and Hendren, L. J. </author> <title> Early experiences with Olden. </title> <booktitle> In Proceedings of the Sixth Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, Oregon, </address> <month> Aug. </month> <year> 1993. </year>
Reference: [10] <author> Chandra, R., Gupta, A., and Hennessy, J. L. </author> <title> COOL: An object-based language for parallel programming. </title> <journal> IEEE Computer, </journal> <volume> 27(8):1326, </volume> <month> Aug. </month> <year> 1994. </year>
Reference: [11] <author> Chase, J. S., Amador, F. G., Lazowska, E. D., Levy, H. M., and Littlefield, R. J. </author> <title> The Amber system: Parallel programming on a network of multiprocessors. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pp. 147158, </pages> <address> Litchfield Park, Arizona, </address> <month> Dec. </month> <year> 1989. </year> <month> 22 </month>
Reference: [12] <author> Cooper, E. C. and Draves, R. P. </author> <title> C Threads. </title> <type> Tech. Rep. </type> <institution> CMU-CS-88-154, School of Computer Science, Carnegie-Mellon University, </institution> <month> Jun. </month> <year> 1988. </year>
Reference: [13] <author> Culler, D. E., Sah, A., Schauser, K. E., von Eicken, T., and Wawrzynek, J. </author> <title> Fine-grain parallelism with minimal hardware support: A compiler-controlled threaded abstract machine. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 164175, </pages> <address> Santa Clara, California, </address> <month> Apr. </month> <year> 1991. </year>
Reference: [14] <author> Feldmann, R., Mysliwietz, P., and Monien, B. </author> <title> Studying overheads in massively parallel min/max-tree evaluation. </title> <booktitle> In Proceedings of the Sixth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 94103, </pages> <address> Cape May, New Jersey, </address> <month> Jun. </month> <year> 1994. </year>
Reference-contexts: With P processors, the execution time cannot be less than T 1 =P or less than T 1 . The Cilk scheduler uses work stealing <ref> [4, 8, 14, 15, 16, 21, 29, 30, 31, 37, 43] </ref> to achieve execution time very near to the sum of these two measures. <p> Other features include various abilities to override the sched-uler's decisions, including on which processor a thread should be placed and how to pack and unpack data when a closure is migrated from one processor to another. 3 The Cilk work-stealing scheduler Cilk's scheduler uses the technique of work stealing <ref> [4, 8, 14, 15, 16, 21, 29, 30, 31, 37, 43] </ref> in which a processor (the thief) who runs out of work selects another processor (the victim) from whom to steal work, and then steals the shallowest ready thread in the victim's spawn tree.
Reference: [15] <author> Finkel, R. and Manber, U. </author> <title> DIBa distributed implementation of backtracking. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(2):235256, </volume> <month> Apr. </month> <year> 1987. </year>
Reference-contexts: With P processors, the execution time cannot be less than T 1 =P or less than T 1 . The Cilk scheduler uses work stealing <ref> [4, 8, 14, 15, 16, 21, 29, 30, 31, 37, 43] </ref> to achieve execution time very near to the sum of these two measures. <p> Other features include various abilities to override the sched-uler's decisions, including on which processor a thread should be placed and how to pack and unpack data when a closure is migrated from one processor to another. 3 The Cilk work-stealing scheduler Cilk's scheduler uses the technique of work stealing <ref> [4, 8, 14, 15, 16, 21, 29, 30, 31, 37, 43] </ref> in which a processor (the thief) who runs out of work selects another processor (the victim) from whom to steal work, and then steals the shallowest ready thread in the victim's spawn tree. <p> First, to lower communication costs, we would like to steal large amounts of work, and in a tree-structured computation, shallow threads are likely to spawn more work than deep ones. This heuristic notion is the justification cited by earlier researchers <ref> [8, 15, 21, 35, 43] </ref> who proposed stealing work that is shallow in the spawn tree. We cannot, however, prove that shallow threads are more likely to spawn work than deep ones. What we prove in Section 6 is the following algorithmic property.
Reference: [16] <author> Freeh, V. W., Lowenthal, D. K., and Andrews, G. R. </author> <title> Distributed Filaments: Efficient fine-grain parallelism on a cluster of workstations. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pp. </pages> <address> 201213, Monterey, California, </address> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: With P processors, the execution time cannot be less than T 1 =P or less than T 1 . The Cilk scheduler uses work stealing <ref> [4, 8, 14, 15, 16, 21, 29, 30, 31, 37, 43] </ref> to achieve execution time very near to the sum of these two measures. <p> Other features include various abilities to override the sched-uler's decisions, including on which processor a thread should be placed and how to pack and unpack data when a closure is migrated from one processor to another. 3 The Cilk work-stealing scheduler Cilk's scheduler uses the technique of work stealing <ref> [4, 8, 14, 15, 16, 21, 29, 30, 31, 37, 43] </ref> in which a processor (the thief) who runs out of work selects another processor (the victim) from whom to steal work, and then steals the shallowest ready thread in the victim's spawn tree.
Reference: [17] <author> Goldstein, S. C., Schauser, K. E., and Culler, D. </author> <title> Enabling primitives for compiling parallel languages. </title> <booktitle> In Third Workshop on Languages, Compilers, and Run-Time Systems for Scalable Computers, </booktitle> <address> Troy, New York, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: For example, the fib program executed over 17 million threads but migrated only 6170 (24:10 per processor) when run with 256 processors. Taking advantage of this property, other researchers <ref> [17, 27, 35] </ref> have developed techniques for implementing spawns such that when the child thread executes on the same processor as its parent, the cost of the spawn operation is roughly equal the cost of a function call. We hope to incorporate such techniques into future implementations of Cilk.
Reference: [18] <author> Graham, R. L. </author> <title> Bounds for certain multiprocessing anomalies. </title> <journal> The Bell System Technical Journal, </journal> <volume> 45:15631581, </volume> <month> Nov. </month> <year> 1966. </year>
Reference-contexts: The Cilk scheduler uses work stealing [4, 8, 14, 15, 16, 21, 29, 30, 31, 37, 43] to achieve execution time very near to the sum of these two measures. Off-line techniques for computing such efficient schedules have been known for a long time <ref> [6, 18, 19] </ref>, but this efficiency 2 has been difficult to achieve on-line in a distributed environment while simultaneously using small amounts of space and communication. We demonstrate the efficiency of the Cilk scheduler both empirically and analytically.
Reference: [19] <author> Graham, R. L. </author> <title> Bounds on multiprocessing timing anomalies. </title> <journal> SIAM Journal on Applied Mathematics, </journal> <volume> 17(2):416429, </volume> <month> Mar. </month> <year> 1969. </year>
Reference-contexts: The Cilk scheduler uses work stealing [4, 8, 14, 15, 16, 21, 29, 30, 31, 37, 43] to achieve execution time very near to the sum of these two measures. Off-line techniques for computing such efficient schedules have been known for a long time <ref> [6, 18, 19] </ref>, but this efficiency 2 has been difficult to achieve on-line in a distributed environment while simultaneously using small amounts of space and communication. We demonstrate the efficiency of the Cilk scheduler both empirically and analytically.
Reference: [20] <author> Halbherr, M., Zhou, Y., and Joerg, C. F. </author> <title> MIMD-style parallel programming with continuation-passing threads. </title> <booktitle> In Proceedings of the 2nd International Workshop on Massive Parallelism: Hardware, Software, and Applications, </booktitle> <address> Capri, Italy, </address> <month> Sep. </month> <year> 1994. </year>
Reference-contexts: Recent information about Cilk is maintained on the World Wide Web in page http://theory.lcs.mit.edu/~cilk. Acknowledgments We gratefully acknowledge the inspiration of Michael Halbherr, now of the Boston Consulting Group in Zurich, Switzerland. Mike's PCM runtime system <ref> [20] </ref> developed at MIT was the precursor of Cilk, and many of the design decisions in Cilk are owed to him. We thank Shail Aditya and Sivan Toledo of MIT and Larry Rudolph of Hebrew University for helpful discussions.
Reference: [21] <author> Halstead, Jr., R. H. </author> <title> Multilisp: A language for concurrent symbolic computation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(4):501538, </volume> <month> Oct. </month> <year> 1985. </year>
Reference-contexts: With P processors, the execution time cannot be less than T 1 =P or less than T 1 . The Cilk scheduler uses work stealing <ref> [4, 8, 14, 15, 16, 21, 29, 30, 31, 37, 43] </ref> to achieve execution time very near to the sum of these two measures. <p> Each Cilk thread leaves the C runtime stack empty when it completes. Thus, Cilk can run on top of a vanilla C runtime system. A common alternative <ref> [21, 27, 35, 37] </ref> is to support a programming style in which a thread suspends whenever it discovers that required values have not yet been computed, resuming when the values become available. <p> Other features include various abilities to override the sched-uler's decisions, including on which processor a thread should be placed and how to pack and unpack data when a closure is migrated from one processor to another. 3 The Cilk work-stealing scheduler Cilk's scheduler uses the technique of work stealing <ref> [4, 8, 14, 15, 16, 21, 29, 30, 31, 37, 43] </ref> in which a processor (the thief) who runs out of work selects another processor (the victim) from whom to steal work, and then steals the shallowest ready thread in the victim's spawn tree. <p> First, to lower communication costs, we would like to steal large amounts of work, and in a tree-structured computation, shallow threads are likely to spawn more work than deep ones. This heuristic notion is the justification cited by earlier researchers <ref> [8, 15, 21, 35, 43] </ref> who proposed stealing work that is shallow in the spawn tree. We cannot, however, prove that shallow threads are more likely to spawn work than deep ones. What we prove in Section 6 is the following algorithmic property.
Reference: [22] <author> Hillis, W. and Steele, G. </author> <title> Data parallel algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 29(12):11701183, </volume> <month> Dec. </month> <year> 1986. </year>
Reference-contexts: Many of these applications pose problems for more traditional parallel environments, such as message passing [41] and data parallel <ref> [2, 22] </ref>, because of the unpredictability of the dynamic workloads on processors. Analytically, we prove that for fully strict (well-structured) programs, Cilk's work-stealing scheduler achieves execution space, time, and communication bounds all within a constant factor of optimal.
Reference: [23] <author> Hsieh, W. C., Wang, P., and Weihl, W. E. </author> <title> Computation migration: Enhancing locality for distributed-memory parallel systems. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), </booktitle> <pages> pp. 239248, </pages> <address> San Diego, California, </address> <month> May </month> <year> 1993. </year>
Reference: [24] <author> Jagannathan, S. and Philbin, J. </author> <title> A customizable substrate for concurrent languages. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 5567, </pages> <address> San Francisco, California, </address> <month> Jun. </month> <year> 1992. </year>
Reference: [25] <author> Joerg, C. and Kuszmaul, B. C. </author> <title> Massively parallel chess. </title> <booktitle> In Proceedings of the Third DIMACS Parallel Implementation Challenge, </booktitle> <institution> Rutgers University, </institution> <address> New Jersey, </address> <month> Oct. </month> <year> 1994. </year> <note> Available as ftp://theory.lcs.mit.edu/pub/cilk/dimacs94.ps.Z. </note>
Reference-contexts: At each node of the tree, the program runs an empty for loop for 400 iterations. * ?Socrates is a parallel chess program that uses the Jamboree search algorithm <ref> [25, 31] </ref> to parallelize a minmax tree search.
Reference: [26] <author> Kale, L. V. </author> <title> The Chare kernel parallel programming system. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, Volume II: Software, </booktitle> <pages> pp. 1725, </pages> <month> Aug. </month> <year> 1990. </year> <month> 23 </month>
Reference: [27] <author> Karamcheti, V. and Chien, A. </author> <title> Concertefficient runtime support for concurrent object--oriented programming languages on stock hardware. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pp. 598607, </pages> <address> Portland, Oregon, </address> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: Each Cilk thread leaves the C runtime stack empty when it completes. Thus, Cilk can run on top of a vanilla C runtime system. A common alternative <ref> [21, 27, 35, 37] </ref> is to support a programming style in which a thread suspends whenever it discovers that required values have not yet been computed, resuming when the values become available. <p> For example, the fib program executed over 17 million threads but migrated only 6170 (24:10 per processor) when run with 256 processors. Taking advantage of this property, other researchers <ref> [17, 27, 35] </ref> have developed techniques for implementing spawns such that when the child thread executes on the same processor as its parent, the cost of the spawn operation is roughly equal the cost of a function call. We hope to incorporate such techniques into future implementations of Cilk.
Reference: [28] <author> Karp, R. M. and Ramachandran, V. </author> <title> Parallel algorithms for shared-memory machines. </title> <editor> In van Leeuwen, J., (Ed.), </editor> <booktitle> Handbook of Theoretical Computer ScienceVolume A: Algorithms and Complexity, chapter 17, </booktitle> <pages> pp. 869941. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: Cilk provides a programming model in which work and critical-path length are observable quantities, and it delivers guaranteed performance as a function of these quantities. Work and critical-path length have been used in the theory community for years to analyze parallel algorithms <ref> [28] </ref>. Blelloch [2] has developed a performance model for data-parallel computations based on these same two abstract measures. He cites many advantages to such a model over machine-based models. Cilk provides a similar performance model for the domain of asynchronous, multi-threaded computation.
Reference: [29] <author> Karp, R. M. and Zhang, Y. </author> <title> Randomized parallel algorithms for backtrack search and branch-and-bound computation. </title> <journal> Journal of the ACM, </journal> <volume> 40(3):765789, </volume> <month> Jul. </month> <year> 1993. </year>
Reference-contexts: With P processors, the execution time cannot be less than T 1 =P or less than T 1 . The Cilk scheduler uses work stealing <ref> [4, 8, 14, 15, 16, 21, 29, 30, 31, 37, 43] </ref> to achieve execution time very near to the sum of these two measures. <p> Other features include various abilities to override the sched-uler's decisions, including on which processor a thread should be placed and how to pack and unpack data when a closure is migrated from one processor to another. 3 The Cilk work-stealing scheduler Cilk's scheduler uses the technique of work stealing <ref> [4, 8, 14, 15, 16, 21, 29, 30, 31, 37, 43] </ref> in which a processor (the thief) who runs out of work selects another processor (the victim) from whom to steal work, and then steals the shallowest ready thread in the victim's spawn tree. <p> Cilk's strategy is for thieves to choose victims at random <ref> [4, 29, 40] </ref>. We shall now present the implementation of Cilk's work-stealing scheduler. At runtime, each processor maintains a local ready pool to hold ready closures. Each closure has an associated level, which corresponds to the thread's depth in the spawn tree.
Reference: [30] <author> Kranz, D. A., Halstead, Jr., R. H., and Mohr, E. Mul-T: </author> <title> A high-performance parallel Lisp. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 8190, </pages> <address> Portland, Oregon, </address> <month> Jun. </month> <year> 1989. </year>
Reference-contexts: With P processors, the execution time cannot be less than T 1 =P or less than T 1 . The Cilk scheduler uses work stealing <ref> [4, 8, 14, 15, 16, 21, 29, 30, 31, 37, 43] </ref> to achieve execution time very near to the sum of these two measures. <p> Other features include various abilities to override the sched-uler's decisions, including on which processor a thread should be placed and how to pack and unpack data when a closure is migrated from one processor to another. 3 The Cilk work-stealing scheduler Cilk's scheduler uses the technique of work stealing <ref> [4, 8, 14, 15, 16, 21, 29, 30, 31, 37, 43] </ref> in which a processor (the thief) who runs out of work selects another processor (the victim) from whom to steal work, and then steals the shallowest ready thread in the victim's spawn tree.
Reference: [31] <author> Kuszmaul, B. C. </author> <title> Synchronized MIMD Computing. </title> <type> Ph.D. thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> May </month> <year> 1994. </year> <note> Available as MIT Laboratory for Computer Science Technical Report MIT/LCS/TR-645 or ftp://theory.lcs.mit.edu/pub/bradley/phd.ps.Z. </note>
Reference-contexts: With P processors, the execution time cannot be less than T 1 =P or less than T 1 . The Cilk scheduler uses work stealing <ref> [4, 8, 14, 15, 16, 21, 29, 30, 31, 37, 43] </ref> to achieve execution time very near to the sum of these two measures. <p> Other features include various abilities to override the sched-uler's decisions, including on which processor a thread should be placed and how to pack and unpack data when a closure is migrated from one processor to another. 3 The Cilk work-stealing scheduler Cilk's scheduler uses the technique of work stealing <ref> [4, 8, 14, 15, 16, 21, 29, 30, 31, 37, 43] </ref> in which a processor (the thief) who runs out of work selects another processor (the victim) from whom to steal work, and then steals the shallowest ready thread in the victim's spawn tree. <p> At each node of the tree, the program runs an empty for loop for 400 iterations. * ?Socrates is a parallel chess program that uses the Jamboree search algorithm <ref> [25, 31] </ref> to parallelize a minmax tree search.
Reference: [32] <author> Leiserson, C. E., Abuhamdeh, Z. S., Douglas, D. C., Feynman, C. R., Ganmukhi, M. N., Hill, J. V., Hillis, W. D., Kuszmaul, B. C., Pierre, M. A. S., Wells, D. S., Wong, M. C., Yang, S.-W., and Zak, R. </author> <title> The network architecture of the Connection Machine CM-5. </title> <booktitle> In Proceedings of the Fourth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 272285, </pages> <address> San Diego, California, </address> <month> Jun. </month> <year> 1992. </year>
Reference-contexts: In all of these cases, high performance demands efficient, dynamic load balancing at runtime. All experiments were run on a CM5 supercomputer. The CM5 is a massively parallel computer based on 32MHz SPARC processors with a fat-tree interconnection network <ref> [32] </ref>. The Cilk runtime system on the CM5 performs communication among processors using the Strata [7] active-message library. Application performance By running our applications and measuring a suite of performance parameters, we empirically answer many questions about the effectiveness of the Cilk runtime system. We focus on the following questions.
Reference: [33] <author> Liu, P., Aiello, W., and Bhatt, S. </author> <title> An atomic model for message-passing. </title> <booktitle> In Proceedings of the Fifth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 154163, </pages> <address> Velen, Germany, </address> <month> Jun. </month> <year> 1993. </year>
Reference-contexts: As in [4], the bounds assume a communication model in which messages are delayed only by contention at destination processors, but no assumptions are made about the order in which contending messages are delivered <ref> [33] </ref>. For technical reasons in our analysis of execution time, the critical path is calculated assuming that all threads spawned by a parent thread are spawned at the end of the parent thread. In our analysis of execution time, we use an accounting argument.
Reference: [34] <author> Miller, R. C. </author> <title> A type-checking preprocessor for Cilk 2, a multithreaded C language. </title> <type> Master's thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: To date, all of the applications that we have coded are fully strict. The Cilk language extends C with primitives to express parallelism, and the Cilk runtime system maps the expressed parallelism into a parallel execution. A Cilk program is preprocessed to C using our cilk2c type-checking preprocessor <ref> [34] </ref>. The C code is then compiled and linked with a runtime library to run on the target platform. Currently supported targets include the Connection Machine CM5 MPP, the Intel Paragon MPP, the Sun Sparcstation SMP, and the Cilk-NOW [3, 5] network of workstations. <p> The procedure itself is not explicitly specified in the language. Rather, it is defined implicitly in terms of its constituent threads. A thread T is defined in a manner similar to a C function definition: thread T (arg-decls ...) f stmts ...g The cilk2c type-checking preprocessor <ref> [34] </ref> translates T into a C function of one argument and void return type. The one argument is a pointer to a closure data structure, illustrated in Figure 2, which holds the arguments for T.
Reference: [35] <author> Mohr, E., Kranz, D. A., and Halstead, Jr., R. H. </author> <title> Lazy task creation: A technique for increasing the granularity of parallel programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3):264280, </volume> <month> Jul. </month> <year> 1991. </year>
Reference-contexts: Each Cilk thread leaves the C runtime stack empty when it completes. Thus, Cilk can run on top of a vanilla C runtime system. A common alternative <ref> [21, 27, 35, 37] </ref> is to support a programming style in which a thread suspends whenever it discovers that required values have not yet been computed, resuming when the values become available. <p> First, to lower communication costs, we would like to steal large amounts of work, and in a tree-structured computation, shallow threads are likely to spawn more work than deep ones. This heuristic notion is the justification cited by earlier researchers <ref> [8, 15, 21, 35, 43] </ref> who proposed stealing work that is shallow in the spawn tree. We cannot, however, prove that shallow threads are more likely to spawn work than deep ones. What we prove in Section 6 is the following algorithmic property. <p> For example, the fib program executed over 17 million threads but migrated only 6170 (24:10 per processor) when run with 256 processors. Taking advantage of this property, other researchers <ref> [17, 27, 35] </ref> have developed techniques for implementing spawns such that when the child thread executes on the same processor as its parent, the cost of the spawn operation is roughly equal the cost of a function call. We hope to incorporate such techniques into future implementations of Cilk.
Reference: [36] <author> Nikhil, R. S. </author> <title> A multithreaded implementation of Id using P-RISC graphs. </title> <booktitle> In Proceedings of the Sixth Annual Workshop on Languages and Compilers for Parallel Computing, number 768 in Lecture Notes in Computer Science, </booktitle> <pages> pp. 390405, </pages> <address> Portland, Oregon, Aug. 1993. </address> <publisher> Springer-Verlag. </publisher>
Reference: [37] <author> Nikhil, R. S. Cid: </author> <title> A parallel, shared-memory C for distributed-memory machines. </title> <booktitle> In Proceedings of the Seventh Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: With P processors, the execution time cannot be less than T 1 =P or less than T 1 . The Cilk scheduler uses work stealing <ref> [4, 8, 14, 15, 16, 21, 29, 30, 31, 37, 43] </ref> to achieve execution time very near to the sum of these two measures. <p> Each Cilk thread leaves the C runtime stack empty when it completes. Thus, Cilk can run on top of a vanilla C runtime system. A common alternative <ref> [21, 27, 35, 37] </ref> is to support a programming style in which a thread suspends whenever it discovers that required values have not yet been computed, resuming when the values become available. <p> Other features include various abilities to override the sched-uler's decisions, including on which processor a thread should be placed and how to pack and unpack data when a closure is migrated from one processor to another. 3 The Cilk work-stealing scheduler Cilk's scheduler uses the technique of work stealing <ref> [4, 8, 14, 15, 16, 21, 29, 30, 31, 37, 43] </ref> in which a processor (the thief) who runs out of work selects another processor (the victim) from whom to steal work, and then steals the shallowest ready thread in the victim's spawn tree.
Reference: [38] <author> Pande, V. S., Joerg, C. F., Grosberg, A. Y., and Tanaka, T. </author> <title> Enumerations of the hamiltonian walks on a cubic sublattice. </title> <journal> Journal of Physics A, </journal> <volume> 27, </volume> <year> 1994. </year>
Reference-contexts: Sargent of the MIT Media Laboratory. Thread length was enhanced by serializing the bottom 7 levels of the search tree. * pfold (x,y,z) is a protein-folding program that finds hamiltonian paths in a three-dimensional grid of size x fi y fi z using backtrack search <ref> [38] </ref>. Written by Chris Joerg of MIT's Laboratory for Computer Science and V. Pande of MIT's Center for Material Sciences and Engineering, pfold was the first program to enumerate all hamiltonian paths in a 3 fi 4 fi 4 grid.
Reference: [39] <author> Rinard, M. C., Scales, D. J., and Lam, M. S. </author> <title> Jade: A high-level, machine-independent language for parallel programming. </title> <booktitle> Computer, </booktitle> <address> 26(6):2838, </address> <month> Jun. </month> <year> 1993. </year>
Reference: [40] <author> Rudolph, L., Slivkin-Allalouf, M., and Upfal, E. </author> <title> A simple load balancing scheme for task allocation in parallel machines. </title> <booktitle> In Proceedings of the Third Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 237245, </pages> <address> Hilton Head, South Carolina, </address> <month> Jul. </month> <year> 1991. </year> <month> 24 </month>
Reference-contexts: Cilk's strategy is for thieves to choose victims at random <ref> [4, 29, 40] </ref>. We shall now present the implementation of Cilk's work-stealing scheduler. At runtime, each processor maintains a local ready pool to hold ready closures. Each closure has an associated level, which corresponds to the thread's depth in the spawn tree.
Reference: [41] <author> Sunderam, V. S. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concurrency: Prac--tice and Experience, </journal> <volume> 2(4):315339, </volume> <month> Dec. </month> <year> 1990. </year>
Reference-contexts: Many of these applications pose problems for more traditional parallel environments, such as message passing <ref> [41] </ref> and data parallel [2, 22], because of the unpredictability of the dynamic workloads on processors. Analytically, we prove that for fully strict (well-structured) programs, Cilk's work-stealing scheduler achieves execution space, time, and communication bounds all within a constant factor of optimal.
Reference: [42] <author> Tanenbaum, A. S., Bal, H. E., and Kaashoek, M. F. </author> <title> Programming a distributed system using shared objects. </title> <booktitle> In Proceedings of the Second International Symposium on High Performance Distributed Computing, </booktitle> <pages> pp. 512, </pages> <address> Spokane, Washington, </address> <month> Jul. </month> <year> 1993. </year>
Reference: [43] <author> Vandevoorde, M. T. and Roberts, E. S. WorkCrews: </author> <title> An abstraction for controlling parallelism. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 17(4):347366, </volume> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: With P processors, the execution time cannot be less than T 1 =P or less than T 1 . The Cilk scheduler uses work stealing <ref> [4, 8, 14, 15, 16, 21, 29, 30, 31, 37, 43] </ref> to achieve execution time very near to the sum of these two measures. <p> Other features include various abilities to override the sched-uler's decisions, including on which processor a thread should be placed and how to pack and unpack data when a closure is migrated from one processor to another. 3 The Cilk work-stealing scheduler Cilk's scheduler uses the technique of work stealing <ref> [4, 8, 14, 15, 16, 21, 29, 30, 31, 37, 43] </ref> in which a processor (the thief) who runs out of work selects another processor (the victim) from whom to steal work, and then steals the shallowest ready thread in the victim's spawn tree. <p> First, to lower communication costs, we would like to steal large amounts of work, and in a tree-structured computation, shallow threads are likely to spawn more work than deep ones. This heuristic notion is the justification cited by earlier researchers <ref> [8, 15, 21, 35, 43] </ref> who proposed stealing work that is shallow in the spawn tree. We cannot, however, prove that shallow threads are more likely to spawn work than deep ones. What we prove in Section 6 is the following algorithmic property.

References-found: 43

