URL: http://www.rpi.edu/~bennek/geometry2.ps
Refering-URL: http://www.rpi.edu/~bennek/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: bennek@rpi.edu bredee@rpi.edu  
Title: Geometry in Learning  
Author: Kristin P. Bennett Erin J. Bredensteiner 
Address: Troy, NY 12180  
Affiliation: Department of Mathematical Sciences Rensselaer Polytechnic Institute  
Abstract: One of the fundamental problems in learning is identifying members of two different classes. For example, to diagnose cancer, one must learn to discriminate between benign and malignant tumors. Through examination of tumors with previously determined diagnosis, one learns some function for distinguishing the benign and malignant tumors. Then the acquired knowledge is used to diagnose new tumors. The perceptron is a simple biologically inspired model for this two-class learning problem. The perceptron is trained or constructed using examples from the two classes. Then the perceptron is used to classify new examples. We describe geometrically what a perceptron is capable of learning. Using duality, we develop a framework for investigating different methods of training a perceptron. Depending on how we define the "best" perceptron, different minimization problems are developed for training the perceptron. The effectiveness of these methods is evaluated empirically on four practical applications: breast cancer diagnosis, detection of heart disease, political voting habits, and sonar recognition. This paper does not assume prior knowledge of machine learning or pattern recognition.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. P. Bennett. </author> <title> Decision tree construction via linear programming. </title> <editor> In M. Evans, editor, </editor> <booktitle> Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society Conference, </booktitle> <pages> pages 97-101, </pages> <address> Utica, Illinois, </address> <year> 1992. </year>
Reference-contexts: If the parts contain points all or largely all of one class the algorithm stops. If not, the process is repeated in each of the half spaces until the desired accuracy is achieved. Perceptrons are one such way to construct the decision <ref> [26, 4, 1, 17] </ref>. The final method is to construct nonlinear mappings of the attributes and then construct a linear discriminant in the augmented attribute space. The resulting problem is linear in its parameters but a nonlinear decision surface is created.
Reference: [2] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Robust linear programming discrimination of two linearly inseparable sets. </title> <journal> Optimization Methods and Software, </journal> <volume> 1 </volume> <pages> 23-34, </pages> <year> 1992. </year>
Reference-contexts: Both MSM (10 and 11) and RLP (13) have been successfully used in practice [34, 19, 35]. They do, however, have some limitations. MSM works well for the separable case. As noted above on problems with noisy data, the method performs poorly since it minimizes the maximum error <ref> [2] </ref>. RLP achieves very strong results for inseparable problems [3, 35]. For separable problems, however, any separating plane, if scaled appropriately, is optimal for RLP since e 0 y = e 0 z = 0. So RLP is not very well defined in the separable case.
Reference: [3] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Bilinear separation of two sets in n-space. </title> <journal> Computational Optimization and Applications, </journal> <volume> 2 </volume> <pages> 207-227, </pages> <year> 1993. </year>
Reference-contexts: They do, however, have some limitations. MSM works well for the separable case. As noted above on problems with noisy data, the method performs poorly since it minimizes the maximum error [2]. RLP achieves very strong results for inseparable problems <ref> [3, 35] </ref>. For separable problems, however, any separating plane, if scaled appropriately, is optimal for RLP since e 0 y = e 0 z = 0. So RLP is not very well defined in the separable case. The next two approaches preserve the benefits of both RLP and MSM.
Reference: [4] <author> C. E. Brodley and P. E. Utgoff. </author> <title> Multivariate decision trees. </title> <journal> Machine Learning, </journal> <volume> 19(1) </volume> <pages> 45-77, </pages> <year> 1995. </year>
Reference-contexts: If the parts contain points all or largely all of one class the algorithm stops. If not, the process is repeated in each of the half spaces until the desired accuracy is achieved. Perceptrons are one such way to construct the decision <ref> [26, 4, 1, 17] </ref>. The final method is to construct nonlinear mappings of the attributes and then construct a linear discriminant in the augmented attribute space. The resulting problem is linear in its parameters but a nonlinear decision surface is created.
Reference: [5] <author> C. Cortes and V. N. Vapnik. </author> <title> Support vector networks. </title> <journal> Machine Learning, </journal> <volume> 20 </volume> <pages> 273-297, </pages> <year> 1995. </year>
Reference-contexts: The term 2 w 0 w decreases the maximum classification error which corresponds to the distance between the relaxed supporting planes. Since this problem is a very minor variation of the Generalized Optimal Plane of Cortes and Vapnik <ref> [5, 33] </ref>, we refer to it as GOP. The problem is a nonlinear perturbation of the RLP. When is close to 0 the RLP objective is emphasized. <p> The dual problem of the Generalized Optimal Plane is constructed as follows <ref> [33, 5] </ref>: Theorem A.4 (Dual of generalized optimal plane) The dual of problem min (1 )( 1 k e 0 z) + s:t: Aw ffe + y 0 ff fi = 2 (28) min 1 2 s:t: e 0 u = e 0 v = ffi m e u 0 1
Reference: [6] <author> R. Detrano, A. Janosi, W. Steinbrunn, M. Pfisterer, J. Schmid, S. Sandhu, K. Guppy, S. Lee, and V. Froelicher. </author> <title> International application of a new probability algorithm for the diagnosis of coronary artery disease. </title> <journal> American Journal of Cardiology, </journal> <volume> 64 </volume> <pages> 304-310, </pages> <year> 1989. </year>
Reference-contexts: By evaluating a new patient's attributes with respect to the separating plane a diagnosis is made. The Cleveland Heart Disease Database (Heart) is a publicly available dataset that contains information on 297 patients using 13 attributes <ref> [6] </ref>. A second application, as discussed previously, is the diagnosis of breast cancer. To evaluate whether a tumor is benign or malignant, a fine needle aspiration is performed collecting a small amount of tissue from the tumor for examination.
Reference: [7] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1973. </year> <month> 21 </month>
Reference: [8] <author> H. Edelsbrunner. </author> <title> Computing the extreme distances between two convex polytopes. </title> <journal> Journal of Algorithms, </journal> <volume> 6 </volume> <pages> 213-224, </pages> <year> 1985. </year>
Reference: [9] <author> K. Fukunaga. </author> <title> Statistical Pattern Recognition. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1990. </year>
Reference: [10] <author> S. Gallant. </author> <title> Optimal linear discriminants. </title> <booktitle> In Proceedings of the International Conference on Pattern Recognition, </booktitle> <pages> pages 849-852. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1986. </year>
Reference: [11] <author> F. Glover. </author> <title> Improved linear programming models for discriminant analysis. </title> <journal> Decision Sciences, </journal> <volume> 21 </volume> <pages> 771-785, </pages> <year> 1990. </year>
Reference: [12] <author> R.P. Gorman and T.J. Sejnowski. </author> <title> Analysis of hidden units in a layered network trained to classify sonar targets. </title> <booktitle> Neural Networks, </booktitle> <volume> 1 </volume> <pages> 75-89, </pages> <year> 1988. </year>
Reference-contexts: Mine attributes are obtained by bouncing sonar signals off a metal cylinder. The sonar signal is transmitted at various angles with rises in frequency. A similar procedure is performed to obtain the rock attributes. The publicly available Sonar dataset represents 208 mines and rocks <ref> [12] </ref>. Sixty real-valued attributes between 0.0 and 1.0 are collected for each mine or rock. The value of the attribute represents the amount of energy within a particular frequency band, integrated over a certain period of time.
Reference: [13] <author> J. Hertz, A. Krogh, and R. G. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, California, </address> <year> 1991. </year>
Reference-contexts: We provide a few references to three major ways to extend the perceptron model to nonlinear separators. The most popular is the multilayer perceptron or neural network. A neural network is created from an interconnecting network of threshold/perceptron type units. We invite the reader to consult <ref> [13, 17] </ref> or one of the many books on this subject. Another approach is decision trees. In decision trees, a linear separation is constructed that divides the attribute space into two parts. If the parts contain points all or largely all of one class the algorithm stops.
Reference: [14] <author> O. L. Mangasarian. </author> <title> Linear and nonlinear separation of patterns by linear programming. </title> <journal> Operations Research, </journal> <volume> 13 </volume> <pages> 444-452, </pages> <year> 1965. </year>
Reference: [15] <author> O. L. Mangasarian. </author> <title> Multi-surface method of pattern separation. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-14:801-807, </volume> <year> 1968. </year>
Reference-contexts: The dual problem maximizes the Lagrangian function of (19), L (w; ff; fi; u; v), subject to the constraints that the partial derivatives of the Lagrangian with respect to the primal variables are equal to zero <ref> [15] </ref>. Specifically, the dual of (20) is: max L (w; ff; fi; u; v) = 1 2 s:t: @L @L @L u 0; v 0 where ff; fi 2 R; w 2 R n ; u 2 R k ; and v 2 R m . <p> First we will show that any solution of Problem (24) can be used to construct a solution of Problem (19). Let u; v; w; fl be an optimal solution of Problem (24), then the following Karush-Kuhn-Tucker optimality conditions are satisfied <ref> [15] </ref>: A w (fl + 1)e 0 u 0 (A w (fl + 1)e) = 0 w = A 0 u B 0 v u 0; v 0: Let ffi = e 0 u = e 0 v.
Reference: [16] <author> O. L. Mangasarian. </author> <title> Nonlinear Programming. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1969. </year>
Reference-contexts: Gordan's Theorem of the Alternative <ref> [16] </ref> states: For each given matrix C, either I Cx &gt; 0 has a solution x or II C 0 y = 0, y 0 has a solution y but never both. Theorem A.1 follows directly with C = " B e , x = w # " v .
Reference: [17] <author> O.L. Mangasarian. </author> <title> Mathematical programming in neural networks. </title> <journal> ORSA Journal on Computing, </journal> <volume> 5(4) </volume> <pages> 349-360, </pages> <year> 1993. </year>
Reference-contexts: We provide a few references to three major ways to extend the perceptron model to nonlinear separators. The most popular is the multilayer perceptron or neural network. A neural network is created from an interconnecting network of threshold/perceptron type units. We invite the reader to consult <ref> [13, 17] </ref> or one of the many books on this subject. Another approach is decision trees. In decision trees, a linear separation is constructed that divides the attribute space into two parts. If the parts contain points all or largely all of one class the algorithm stops. <p> If the parts contain points all or largely all of one class the algorithm stops. If not, the process is repeated in each of the half spaces until the desired accuracy is achieved. Perceptrons are one such way to construct the decision <ref> [26, 4, 1, 17] </ref>. The final method is to construct nonlinear mappings of the attributes and then construct a linear discriminant in the augmented attribute space. The resulting problem is linear in its parameters but a nonlinear decision surface is created.
Reference: [18] <author> O.L. Mangasarian and R.R. Meyer. </author> <title> Nonlinear perturbation of linear programs. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 17(6) </volume> <pages> 745-752, </pages> <month> November </month> <year> 1979. </year>
Reference-contexts: The problem is a nonlinear perturbation of the RLP. When is close to 0 the RLP objective is emphasized. Using thereoms on nonlinear perturbation of linear programming in <ref> [18] </ref>, we know that there exists some positive number such that if 2 (0; ], there exists a solution of GOP (14) that also solves RLP (13).
Reference: [19] <author> O.L. Mangasarian, R. Setiono, </author> <title> and W.H. Wolberg. Pattern recognition via linear programming: Theory and application to medical diagnosis. </title> <booktitle> In Proceedings of the Workshop on Large-Scale Numerical Optimization, </booktitle> <year> 1989, </year> <pages> pages 22-31, </pages> <address> Philadelphia, Pennsylvania, </address> <year> 1990. </year> <note> SIAM. </note>
Reference-contexts: introduction [32, 35]. Both MSM (10 and 11) and RLP (13) have been successfully used in practice <ref> [34, 19, 35] </ref>. They do, however, have some limitations. MSM works well for the separable case. As noted above on problems with noisy data, the method performs poorly since it minimizes the maximum error [2]. RLP achieves very strong results for inseparable problems [3, 35].
Reference: [20] <author> O.L. Mangasarian, W. N. </author> <title> Street, and W.H. Wolberg. Breast cancer diagnosis and prognosis via linear programming. </title> <journal> Operations Research, </journal> <volume> 43(4) </volume> <pages> 570-577, </pages> <year> 1995. </year>
Reference-contexts: Several measurements such as clump thickness, uniformity of cell size, and uniformity of cell shape are collected. A mathematical programming approach incorporating the RLP has been employed in clinical practice <ref> [20, 34] </ref>. This data was collected before the computer imaging techniques were used for measuring attributes as discussed in the introduction. They report 100% correctness on 131 new cases that have been diagnosed. Additionally, many studies have been made on the Wisconsin Breast Cancer Database (Cancer) [34].
Reference: [21] <author> M. Minsky and S. Papert. </author> <title> Perceptrons: An Introduction to Computational Geometry. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1969. </year>
Reference: [22] <author> J. J. More and S. J. Wright. </author> <title> Optimization Software Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1993. </year>
Reference: [23] <author> T. S. Motzkin and I. J. </author> <title> Schoenberg. The relaxation method for linear inequalities. </title> <journal> Canadian Journal of Mathematics, </journal> <volume> 6 </volume> <pages> 393-404, </pages> <year> 1954. </year>
Reference: [24] <author> P.M. Murphy and D.W. Aha. </author> <title> UCI repository of machine learning databases. </title> <institution> Department of Information and Computer Science, University of California, Irvine, California, </institution> <year> 1992. </year>
Reference-contexts: Most datasets for practical applications are linearly inseparable. We have limited this investigation to datasets that are publicly available via the World Wide Web. All of the above datasets are available via anonymous file transfer protocol (ftp) from the UCI Repository of Machine Learning Databases and Domain Theories <ref> [24] </ref> at ftp://ftp.ics.uci.edu/pub/machine-learning-databases. The following section contains computational results for the MSM, RLP, RLP-P, and GOP methods on these datasets. 7 Computational Results This section contains a computational comparison of the four methods, MSM, RLP, RLP-P and GOP. We report results on four datasets: Heart, Cancer, Sonar, and House Votes.
Reference: [25] <author> B.A. Murtagh and M.A. Saunders. </author> <title> MINOS 5.4 user's guide. </title> <type> Technical Report SOL 83.20, </type> <institution> Stanford University, </institution> <year> 1993. </year>
Reference-contexts: We report results on four datasets: Heart, Cancer, Sonar, and House Votes. The previous section contains a description of each dataset and its application. All four methods were implemented using the MINOS <ref> [25] </ref> mathematical programming software package. Other optimization packages could easily be substituted. The results of GOP and RLP-P are dependent on the parameter . We report results for GOP using = :1 and = :05 and for RLP-P using = :05 and = :02.
Reference: [26] <author> S. Murthy, S. Kasif, and S. Salzberg. </author> <title> A system for induction of oblique decision trees. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 1-32, </pages> <year> 1994. </year>
Reference-contexts: If the parts contain points all or largely all of one class the algorithm stops. If not, the process is repeated in each of the half spaces until the desired accuracy is achieved. Perceptrons are one such way to construct the decision <ref> [26, 4, 1, 17] </ref>. The final method is to construct nonlinear mappings of the attributes and then construct a linear discriminant in the augmented attribute space. The resulting problem is linear in its parameters but a nonlinear decision surface is created.
Reference: [27] <author> F. Rosenblatt. </author> <title> The perceptron-a perceiving and recognizing automaton. </title> <type> Technical Report 85-460-1, </type> <institution> Cornell Aeronautical Laboratory, </institution> <address> Itahca, New York, </address> <month> January </month> <year> 1957. </year>
Reference: [28] <author> A. Roy, S. Govil, and R. Miranda. </author> <title> An algorithm to generate radial basis function (rbf)-like nets for classification problems. </title> <booktitle> Neural Networks, </booktitle> <volume> 8(2) </volume> <pages> 179-202, </pages> <year> 1995. </year>
Reference-contexts: The separating surface would now be x 1 w 1 + x 2 w 2 + x 2 1 w 3 + x 2 Support Vector Networks [33] and Polynomial Networks <ref> [29, 28] </ref> are examples of this type 16 of approach. 9 Conclusions We have studied the problem of training a perceptron to classify points from two sets.
Reference: [29] <author> A. Roy, L. S. Kim, and S. Mukhopadhyay. </author> <title> A polynomial time algorithm for the construction and training of a class of multilayer perceptrons. </title> <booktitle> Neural Networks, </booktitle> <volume> 6 </volume> <pages> 535-545, </pages> <year> 1993. </year>
Reference-contexts: The separating surface would now be x 1 w 1 + x 2 w 2 + x 2 1 w 3 + x 2 Support Vector Networks [33] and Polynomial Networks <ref> [29, 28] </ref> are examples of this type 16 of approach. 9 Conclusions We have studied the problem of training a perceptron to classify points from two sets.
Reference: [30] <author> J.C. Schlimmer. </author> <title> Concept acquisition through representational adjustment. </title> <type> PhD thesis, </type> <institution> Department of Information and Computer Science, University of California, </institution> <address> Irvine, CA, </address> <year> 1987. </year>
Reference-contexts: This publicly available dataset contains information on 682 patients using nine integer attributes. The voting patterns of congressmen can be used to determine party affiliation. A specific example of this application is the 1984 United States Congressional Voting Records Database (House Votes) <ref> [30] </ref>. This is a publicly available dataset. Each instance of the dataset represents a U.S. House of Representatives Congressman. Information on 435 congressmen is given. Each congressman is classified as either democrat or republican. The attributes consist of 16 key votes.
Reference: [31] <author> M. Stone. </author> <title> Cross-validatory choice and assessment of statistical predictions. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> 36 </volume> <pages> 111-147, </pages> <year> 1974. </year>
Reference-contexts: The best algorithm is the one that generalizes best, i.e., the algorithm most accurate on future points. Since the future points are unknown, we use an experimental technique called cross validation <ref> [31] </ref> to estimate the accuracy on future points. In 10-fold cross validation a dataset is divided randomly into ten disjoint parts of equal size. The method is then trained on nine of these parts. These nine parts are called the training set.
Reference: [32] <author> W.N. </author> <title> Street, W.H. Wolberg, and O.L. Mangasarian. Nuclear feature extraction for breast tumor diagnosis. </title> <booktitle> In IS&T/SPIE 1993 International Symposium on Electronic Imaging: Science and Technology, volume 1905, </booktitle> <pages> pages 861-870, </pages> <address> San Jose, California, </address> <year> 1993. </year>
Reference-contexts: introduction <ref> [32, 35] </ref>. Both MSM (10 and 11) and RLP (13) have been successfully used in practice [34, 19, 35]. They do, however, have some limitations. MSM works well for the separable case.
Reference: [33] <author> V. N. Vapnik. </author> <title> The Nature of Statistical Learning Theory. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: The term 2 w 0 w decreases the maximum classification error which corresponds to the distance between the relaxed supporting planes. Since this problem is a very minor variation of the Generalized Optimal Plane of Cortes and Vapnik <ref> [5, 33] </ref>, we refer to it as GOP. The problem is a nonlinear perturbation of the RLP. When is close to 0 the RLP objective is emphasized. <p> The objective still minimizes the distance between these points. Each u i &gt; 0 corresponds to an example or vector that determines the convex combination c of A . Vapnik refers to these examples with positive multipliers as support vectors <ref> [33] </ref>. Recall that the convex combination c of A is c = A 0 u with e u 0 and e 0 u = 1. <p> The separating surface would now be x 1 w 1 + x 2 w 2 + x 2 1 w 3 + x 2 Support Vector Networks <ref> [33] </ref> and Polynomial Networks [29, 28] are examples of this type 16 of approach. 9 Conclusions We have studied the problem of training a perceptron to classify points from two sets. <p> kA 0 u B 0 vk 2 = (u 0 A v 0 B)(A 0 u B 0 v), this simplifies to: min 1 2 u; v 0 The dual Problem (19) of finding the closest two points is equivalent to the Optimal Separating Plane problem (24) proposed by Vapnik <ref> [33] </ref>. Specifically, every solution of one problem can be used to construct a corresponding solution of the other. Theorem A.3 (Equivalence of two separating problems) For separable sets, Problem (19) and the following problem are equivalent min 1 2 Bw + (fl 1)e 0 Proof. <p> The dual problem of the Generalized Optimal Plane is constructed as follows <ref> [33, 5] </ref>: Theorem A.4 (Dual of generalized optimal plane) The dual of problem min (1 )( 1 k e 0 z) + s:t: Aw ffe + y 0 ff fi = 2 (28) min 1 2 s:t: e 0 u = e 0 v = ffi m e u 0 1
Reference: [34] <author> W. H. Wolberg and O.L. Mangasarian. </author> <title> Multisurface method of pattern separation for medical diagnosis applied to breast cytology. </title> <booktitle> Proceedings of the National Academy of Sciences,U.S.A., </booktitle> <volume> 87 </volume> <pages> 9193-9196, </pages> <year> 1990. </year>
Reference-contexts: introduction [32, 35]. Both MSM (10 and 11) and RLP (13) have been successfully used in practice <ref> [34, 19, 35] </ref>. They do, however, have some limitations. MSM works well for the separable case. As noted above on problems with noisy data, the method performs poorly since it minimizes the maximum error [2]. RLP achieves very strong results for inseparable problems [3, 35]. <p> Several measurements such as clump thickness, uniformity of cell size, and uniformity of cell shape are collected. A mathematical programming approach incorporating the RLP has been employed in clinical practice <ref> [20, 34] </ref>. This data was collected before the computer imaging techniques were used for measuring attributes as discussed in the introduction. They report 100% correctness on 131 new cases that have been diagnosed. Additionally, many studies have been made on the Wisconsin Breast Cancer Database (Cancer) [34]. <p> This data was collected before the computer imaging techniques were used for measuring attributes as discussed in the introduction. They report 100% correctness on 131 new cases that have been diagnosed. Additionally, many studies have been made on the Wisconsin Breast Cancer Database (Cancer) <ref> [34] </ref>. This publicly available dataset contains information on 682 patients using nine integer attributes. The voting patterns of congressmen can be used to determine party affiliation. A specific example of this application is the 1984 United States Congressional Voting Records Database (House Votes) [30]. This is a publicly available dataset.
Reference: [35] <author> W.H. Wolberg, W. N. </author> <title> Street, and O.L. Mangasarian. Image analysis and machine learning applied to breast cancer diagnosis and prognosis. </title> <journal> Quantitative Cytology and Histology, </journal> <volume> 17(2) </volume> <pages> 77-87, </pages> <year> 1995. </year> <month> 23 </month>
Reference-contexts: introduction <ref> [32, 35] </ref>. Both MSM (10 and 11) and RLP (13) have been successfully used in practice [34, 19, 35]. They do, however, have some limitations. MSM works well for the separable case. <p> introduction [32, 35]. Both MSM (10 and 11) and RLP (13) have been successfully used in practice <ref> [34, 19, 35] </ref>. They do, however, have some limitations. MSM works well for the separable case. As noted above on problems with noisy data, the method performs poorly since it minimizes the maximum error [2]. RLP achieves very strong results for inseparable problems [3, 35]. <p> They do, however, have some limitations. MSM works well for the separable case. As noted above on problems with noisy data, the method performs poorly since it minimizes the maximum error [2]. RLP achieves very strong results for inseparable problems <ref> [3, 35] </ref>. For separable problems, however, any separating plane, if scaled appropriately, is optimal for RLP since e 0 y = e 0 z = 0. So RLP is not very well defined in the separable case. The next two approaches preserve the benefits of both RLP and MSM.
References-found: 35

