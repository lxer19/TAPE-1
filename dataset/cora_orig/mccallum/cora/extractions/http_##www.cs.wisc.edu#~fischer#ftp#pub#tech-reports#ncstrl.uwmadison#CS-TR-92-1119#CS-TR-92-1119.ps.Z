URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-92-1119/CS-TR-92-1119.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-92-1119/
Root-URL: http://www.cs.wisc.edu
Title: CODE GENERATION TECHNIQUES  
Author: By Todd Alan Proebsting 
Degree: A thesis submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy (Computer Sciences) at the  
Date: 1992  
Address: WISCONSIN MADISON  
Affiliation: UNIVERSITY OF  
Abstract-found: 0
Intro-found: 1
Reference: [AG85] <author> Alfred V. Aho and Mahedevan Ganapathi. </author> <title> Efficient tree pattern matching: An aid to code generation. </title> <booktitle> In Proceedings of the 12th Annual Symposium on Principles of Programming Languages, </booktitle> <pages> pages 334-340, </pages> <month> January </month> <year> 1985. </year>
Reference-contexts: If the tree patterns that describe different instructions are given weights to describe their relative costs, dynamic programming can be used to select the optimal set of instructions to evaluate the tree ([AGT89], [AJ76], [PLG88], [BDB90], and <ref> [AG85] </ref>). Dynamic programming is an expensive operation since it finds all optimal sub-solutions before finding a solution for the entire tree. Fortunately, Bottom-Up Rewrite System (BURS) technology, can hide this cost from the compiler [PLG88]. <p> Burg has been used to construct fast optimal instruction selectors for use in code generation. Burg addresses many of the problems addressed by Twig <ref> [AG85, App87] </ref>, but it is somewhat less flexible and much faster. Burg is available via anonymous ftp from kaese.cs.wisc.edu. The compressed shar file pub/burg.shar.Z holds the complete distribution.
Reference: [AGT89] <author> Alfred V. Aho, Mahedevan Ganapathi, and Steven W. K. Tjiang. </author> <title> Code generation using tree matching and dynamic programming. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 11(4) </volume> <pages> 491-516, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Equally important, finding a least-cost covering (based on costs associated with the patterns) is also efficient. 4.1 Overview Tree pattern matching combined with dynamic programming can be used in code generators to create locally optimal code for expression trees <ref> [AGT89] </ref>. Code generators based on bottom-up rewrite system (BURS) theory can be extremely fast because all dynamic programming is done when the BURS automaton is built. <p> They differ primarily in what technology they use to do tree pattern matching, and in the fact that they do dynamic programming at compile time rather than compile-compile time. 119 4.11.1 Twig Aho, Ganapathi, and Tjiang <ref> [AGT89] </ref> created a tree manipulation language and system called Twig. Given a specification of tree patterns and associated costs, Twig generates a tree automaton that will find the least-cost cover of a subject tree.
Reference: [AJ76] <author> A. V. Aho and S. C. Johnson. </author> <title> Optimal code generation for expressions trees. </title> <journal> Journal of the ACM, </journal> <volume> 23(3) </volume> <pages> 488-501, </pages> <month> July </month> <year> 1976. </year>
Reference-contexts: If the tree patterns that describe different instructions are given weights to describe their relative costs, dynamic programming can be used to select the optimal set of instructions to evaluate the tree ([AGT89], <ref> [AJ76] </ref>, [PLG88], [BDB90], and [AG85]). Dynamic programming is an expensive operation since it finds all optimal sub-solutions before finding a solution for the entire tree. Fortunately, Bottom-Up Rewrite System (BURS) technology, can hide this cost from the compiler [PLG88]. <p> This course may be wise if, say, the tree structure is defined in a large header file with symbols that might collide with Burm's. Burm selects an optimal parse without doing dynamic programming at compile time <ref> [AJ76] </ref>. Instead, Burg does the dynamic programming at compile-compile time, as it builds Burm. Consequently, Burm parses quickly. Similar labellers have taken as few as 15 instructions per node, and reducers as few as 35 per node visited [FH91c].
Reference: [AJU77] <author> A. V. Aho, S. C. Johnson, and J. D. Ullman. </author> <title> Code generation for expressions with common subexpressions. </title> <journal> Journal of the ACM, </journal> <volume> 24(1) </volume> <pages> 146-160, </pages> <month> January </month> <year> 1977. </year>
Reference-contexts: Introduction The three main problems in code generation are what instructions to use, in what order to do the computations, and what values to keep in registers. Aho, Johnson, and Ullman <ref> [AJU77] </ref>. 1.1 Overview This thesis describes the following issues in code generation theory and technology: Chapter 2 develops an optimal instruction scheduler and register allocator for delayed-load architectures, Chapter 3 describes probabilistic register allocation, a new global register allocation heuristic that is simpler and more effective than widely-used graph-coloring techniques, and
Reference: [App87] <author> Andrew W. Appel. </author> <title> Concise specifications of locally optimal code generators. </title> <type> Technical Report CS-TR-080-87, </type> <institution> Princeton University, Dept. of Computer Science, Princeton, </institution> <address> New Jersey, </address> <month> February </month> <year> 1987. </year>
Reference-contexts: Burg has been used to construct fast optimal instruction selectors for use in code generation. Burg addresses many of the problems addressed by Twig <ref> [AG85, App87] </ref>, but it is somewhat less flexible and much faster. Burg is available via anonymous ftp from kaese.cs.wisc.edu. The compressed shar file pub/burg.shar.Z holds the complete distribution.
Reference: [BCKT89] <author> Preston Briggs, Keith D. Cooper, Ken Kennedy, and Linda Torczon. </author> <title> Coloring heuristics for register allocation. </title> <booktitle> In Proceedings of the SIGPLAN '89 138 139 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 275-284, </pages> <year> 1989. </year>
Reference-contexts: Previous global register allocation methods have concentrated on casting register allocation as a graph-coloring problem ([CAC + 81], [CH90], <ref> [BCKT89] </ref>, [LH86]). Since no two simultaneously live values may be assigned to the same register, an interference graph can be built where nodes represent register candidate values, and arcs exist between simultaneously live values. <p> A closely related problem is that of register assignment. Register assignment is the problem of determining which actual physical register will hold a particular value (that has previously been allocated a register). 3.1 Overview The dominant paradigm in modern global register allocation is graph coloring ([CH90], [CAC + 81], <ref> [BCKT89] </ref>, [LH86]). Unfortunately, graph coloring does not really address the issue of register allocation, but rather the related issue of register assignment. That is, graph coloring tells us how to assign registers so that simultaneously live values aren't assigned the same register. <p> yet identifies those values that can readily and profitably reside in a register. 3.2 Graph Coloring Allocators The basic graph coloring technique involves creating a register interference graph and then pruning nodes from that graph that can be trivially colored (assigned a physical register) ([CAC + 81], [Cha82], [CH90], [LH86], <ref> [BCKT89] </ref>, [CK91]). The nodes of the graph represent the live ranges of the different register candidates (variables and temporaries). The live range of a candidate is the set of all program points where that candidate is live|as computed by data-flow analysis. <p> If, however, within a tile, the graph pruning algorithm fails to find a coloring, their technique resorts to the graph-coloring spill techniques outlined in <ref> [BCKT89] </ref>. Therefore, while succeeding in biasing register allocation within a loop to variables used within that loop, spill decisions must still be made via ad hoc heuristic methods. Few graph coloring techniques do local (basic block level) register allocation as well as established local allocation algorithms ([HFG89], [Fre74], [FL88]). <p> Probabilities present the foundation for a global register allocator that combines the advantages of excellent local allocation with effective global allocation. Probabilistic register allocation avoids the problems of live-range splitting that plague graph-coloring techniques [Cha82], <ref> [BCKT89] </ref>, [CH90] by implicitly (and automatically) splitting ranges where the probability and benefit of residing in a register are low. <p> The previous allocation phases guarantee that there will never be a point in the program that is over-allocated, but so far no legal assignment has been found. All of the variables that have been allocated registers are assigned registers using graph-coloring techniques [Cha82], <ref> [BCKT89] </ref>, [CH90]. An important difference between using graph-coloring for allocation (as other algorithms do) and for assignment (as we do) is that failure to find a legal coloring (unlikely) does not necessitate spilling a value.
Reference: [BDB90] <author> A. Balachandran, D. M. Dhamdhere, and S. Biswas. </author> <title> Efficient retargetable code generation using bottom-up tree pattern matching. </title> <journal> Computer Languages, </journal> <volume> 15(3) </volume> <pages> 127-140, </pages> <year> 1990. </year>
Reference-contexts: If the tree patterns that describe different instructions are given weights to describe their relative costs, dynamic programming can be used to select the optimal set of instructions to evaluate the tree ([AGT89], [AJ76], [PLG88], <ref> [BDB90] </ref>, and [AG85]). Dynamic programming is an expensive operation since it finds all optimal sub-solutions before finding a solution for the entire tree. Fortunately, Bottom-Up Rewrite System (BURS) technology, can hide this cost from the compiler [PLG88]. <p> This allowed the specification of commutativity transformations, for instance. Subsequent BURS systems, including the techniques described here, do not allow general rewrites, but instead defer that responsibility to another phase of the compilation process. Balachandran, Dhamdhere, and Biswas <ref> [BDB90] </ref> simplified Pelegri's model by disallowing rewrite rules, and also generalized Chase's ideas to use cost information. Henry [Hen89] developed optimization techniques to limit the number of BURS states produced during table generation. With fewer states, a smaller automaton is produced more quickly. <p> the cost associated 94 with rule #1 is 0, the cost of the derivation is 1 | the sum of the costs of complete derivation of Int from goal. 4.3.1 Normal Form Patterns To simplify the generation of BURS tables, all patterns are put into the canonical form introduced in <ref> [BDB90] </ref>. <p> Representer states are constructed from an itemset by retaining only those nonterminals that may contribute to a match in the given dimension for the given operator ([Cha87], <ref> [BDB90] </ref>). Suppose that, for a given grammar, there is no rule with a tree pattern for the binary operator, , that has a left child of nonterminal n. <p> Readers interested in more detail might start with <ref> [BDB90] </ref>. Other relevant documents include [Kro75, HO82, HC86, Cha87, PLG88, PL88, BMW87, Hen89, FH91c, Pro92]. A.2 Input Burg accepts a tree grammar and emits a BURS tree parser. Figure A.1 shows a sample grammar that implements a very simple instruction selector. Burg grammars are structurally similar to Yacc's. <p> The command: burg [ arguments ] [ file ] invokes Burg. If a file is named, Burg expects its grammar there; otherwise it reads the standard input. The options include: -c N Abort if any relative cost exceeds N , which keeps Burg from looping on diverging grammars. <ref> [PLG88, Hen89, BDB90, Pro92] </ref> explain relative costs. -d Report a few statistics and flag unused rules and terminals. -o file Write parser into file. Otherwise it writes to the standard output. -p prefix Start exported names with prefix.
Reference: [Bea74] <author> J. C. Beatty. </author> <title> Register assignment algorithm for generation of highly optimized object code. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 18(1) </volume> <pages> 20-39, </pages> <month> January </month> <year> 1974. </year>
Reference-contexts: The implementation simply happened to break the tie between the candidates of merit 25 differently. 3.4.3 Probabilities Improve Beatty's Algorithm Our algorithm is an improvement to Beatty's register allocation scheme <ref> [Bea74] </ref>. His algorithm does local allocation followed by global allocation through the removal of loads and stores to loop pre-headers and post-exits. Our algorithm differs from his in two important ways: ours uses probabilities to provide better global allocation, and ours separates register assignment from register allocation.
Reference: [BEH91] <author> David G. Bradlee, Susan J. Eggers, and Robert R. Henry. </author> <title> Integrating register allocation and instruction scheduling for riscs. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: Given a DAG, IPS schedules instructions using CSP and maintains a count of live registers. When the count exceeds a threshold, IPS switches to CSR to reduce register usage. Once reduced appropriately, IPS reverts to CSP. This oscillation continues until the scheduling process is complete. Bradlee, Eggers, and Henry <ref> [BEH91] </ref> describe another integrated system, Register Allocation with Schedule Estimates (RASE), and compare it to IPS. RASE works in three sequential passes: PRESCHED, GRA, and FINALSCHED. For each basic block, PRESCHED estimates the cost of evaluating that basic block with n registers available, for all legal register counts.
Reference: [BJR89] <author> David Bernstein, Jeffrey M. Jaffe, and Michael Rodeh. </author> <title> Scheduling arithmetic and load operations in parallel with no spilling. </title> <journal> SIAM Journal on Computing, </journal> <volume> 18(6) </volume> <pages> 1098-1127, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: For an architecture with 2 functional units, one for loads and one for operations, with identical pipeline constraints, Bernstein et. al. have investigated code scheduling with register allocation for trees ([BPR84] and <ref> [BJR89] </ref>). Although applicable to a much different machine, Bernstein's results and algorithms are similar to ours 1 |both minimize pipeline interlocks and register usage, and both run in O (n) time (where n is the number of nodes in the expression).
Reference: [BL92] <author> Thomas Ball and James R. Larus. </author> <title> Optimally profiling and tracing programs. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Principles of Programming Languages, </booktitle> <pages> pages 59-70, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: This can be determined heuristically from loop and conditional nesting levels, or empirically through profiling information from previous executions of the program <ref> [BL92] </ref>. Once a particular use has been allocated a register, there is no need to do a load of the variable at that use, thus saving time and space. Allocating a register causes the probabilities for other inter-block variables to change.
Reference: [BMW87] <author> Jurgen Borstler, Ulrich Monche, and Reinhard Wilhelm. </author> <title> Table compression for tree automata. </title> <type> Technical Report Aachener Informatik-Berichte 87-12, </type> <institution> Fachgruppe Informatik, Aachen, Fed. </institution> <type> Rep. </type> <institution> of Germany, </institution> <year> 1987. </year> <month> 140 </month>
Reference-contexts: Readers interested in more detail might start with [BDB90]. Other relevant documents include <ref> [Kro75, HO82, HC86, Cha87, PLG88, PL88, BMW87, Hen89, FH91c, Pro92] </ref>. A.2 Input Burg accepts a tree grammar and emits a BURS tree parser. Figure A.1 shows a sample grammar that implements a very simple instruction selector. Burg grammars are structurally similar to Yacc's. Comments follow C conventions.
Reference: [BPR84] <author> David Bernstein, Ron Y. Pinter, and Michael Rodeh. </author> <title> Optimal scheduling of arithmetic operations in parallel with memory access. </title> <booktitle> In Proceedings of the 12th Annual Symposium on Principles of Programming Languages, </booktitle> <pages> pages 325-333, </pages> <month> January </month> <year> 1984. </year>
Reference: [CAC + 81] <author> G. J. Chaitin, M. A. Auslander, A. K. Chandra, J. Cooke, M. E. Hopkins, and P. W. Markstein. </author> <title> Register allocation via graph coloring. </title> <journal> Computer Languages, </journal> <volume> 6 </volume> <pages> 47-57, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: A closely related problem is that of register assignment. Register assignment is the problem of determining which actual physical register will hold a particular value (that has previously been allocated a register). 3.1 Overview The dominant paradigm in modern global register allocation is graph coloring ([CH90], <ref> [CAC + 81] </ref>, [BCKT89], [LH86]). Unfortunately, graph coloring does not really address the issue of register allocation, but rather the related issue of register assignment. That is, graph coloring tells us how to assign registers so that simultaneously live values aren't assigned the same register.
Reference: [CH90] <author> Fred C. Chow and John L. Hennessy. </author> <title> The priority-based coloring approach to register allocation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12(4) </volume> <pages> 501-536, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Previous global register allocation methods have concentrated on casting register allocation as a graph-coloring problem ([CAC + 81], <ref> [CH90] </ref>, [BCKT89], [LH86]). Since no two simultaneously live values may be assigned to the same register, an interference graph can be built where nodes represent register candidate values, and arcs exist between simultaneously live values. <p> simple and yet identifies those values that can readily and profitably reside in a register. 3.2 Graph Coloring Allocators The basic graph coloring technique involves creating a register interference graph and then pruning nodes from that graph that can be trivially colored (assigned a physical register) ([CAC + 81], [Cha82], <ref> [CH90] </ref>, [LH86], [BCKT89], [CK91]). The nodes of the graph represent the live ranges of the different register candidates (variables and temporaries). The live range of a candidate is the set of all program points where that candidate is live|as computed by data-flow analysis. <p> Probabilities present the foundation for a global register allocator that combines the advantages of excellent local allocation with effective global allocation. Probabilistic register allocation avoids the problems of live-range splitting that plague graph-coloring techniques [Cha82], [BCKT89], <ref> [CH90] </ref> by implicitly (and automatically) splitting ranges where the probability and benefit of residing in a register are low. <p> The previous allocation phases guarantee that there will never be a point in the program that is over-allocated, but so far no legal assignment has been found. All of the variables that have been allocated registers are assigned registers using graph-coloring techniques [Cha82], [BCKT89], <ref> [CH90] </ref>. An important difference between using graph-coloring for allocation (as other algorithms do) and for assignment (as we do) is that failure to find a legal coloring (unlikely) does not necessitate spilling a value. It is possible that a program that is not over-allocated may not have a legal assignment. <p> The higher probabilities for A and B indicate that they put less pressure on the allocator, and would therefore be good candidates for removal. Chow's priority-based graph coloring algorithm <ref> [CH90] </ref> used the size of a live range (measured in instructions) as a crude measure of this competition. Of course, the expected benefit of allocating a register to a value must also be weighed when deciding between two candidates.
Reference: [Cha82] <author> G. J. Chaitin. </author> <title> Register allocation & spilling via graph coloring. </title> <booktitle> In Proceedings of the ACM SIGPLAN '82 Symposium on Compiler Construction, </booktitle> <pages> pages 98-101, </pages> <month> June </month> <year> 1982. </year>
Reference-contexts: is simple and yet identifies those values that can readily and profitably reside in a register. 3.2 Graph Coloring Allocators The basic graph coloring technique involves creating a register interference graph and then pruning nodes from that graph that can be trivially colored (assigned a physical register) ([CAC + 81], <ref> [Cha82] </ref>, [CH90], [LH86], [BCKT89], [CK91]). The nodes of the graph represent the live ranges of the different register candidates (variables and temporaries). The live range of a candidate is the set of all program points where that candidate is live|as computed by data-flow analysis. <p> Various graph coloring techniques differ precisely in what they do when pruning blocks. When the pruning heuristic blocks, Chaitin's techniques ([CAC + 81], <ref> [Cha82] </ref>) take the simplest approach. A node (register candidate) is picked based on a cost measure, removed from the graph, and assigned permanently to a memory location. All subsequent references to that value must be from memory. <p> Probabilities present the foundation for a global register allocator that combines the advantages of excellent local allocation with effective global allocation. Probabilistic register allocation avoids the problems of live-range splitting that plague graph-coloring techniques <ref> [Cha82] </ref>, [BCKT89], [CH90] by implicitly (and automatically) splitting ranges where the probability and benefit of residing in a register are low. <p> The previous allocation phases guarantee that there will never be a point in the program that is over-allocated, but so far no legal assignment has been found. All of the variables that have been allocated registers are assigned registers using graph-coloring techniques <ref> [Cha82] </ref>, [BCKT89], [CH90]. An important difference between using graph-coloring for allocation (as other algorithms do) and for assignment (as we do) is that failure to find a legal coloring (unlikely) does not necessitate spilling a value.
Reference: [Cha87] <author> David R. Chase. </author> <title> An improvement to bottom-up tree pattern matching. </title> <booktitle> In Proceedings of the 14th Annual Symposium on Principles of Programming Languages, </booktitle> <pages> pages 168-177, </pages> <year> 1987. </year>
Reference-contexts: Chase demonstrated that these maps can be produced on-the-fly during table generation so that no superfluous work need be performed <ref> [Cha87] </ref>. Pelegri-Llopart, the originator of BURS theory ([PLG88], [PL88]), encorporated Chase's ideas into a system that added cost information for dynamic programming at table generation time. <p> Readers interested in more detail might start with [BDB90]. Other relevant documents include <ref> [Kro75, HO82, HC86, Cha87, PLG88, PL88, BMW87, Hen89, FH91c, Pro92] </ref>. A.2 Input Burg accepts a tree grammar and emits a BURS tree parser. Figure A.1 shows a sample grammar that implements a very simple instruction selector. Burg grammars are structurally similar to Yacc's. Comments follow C conventions.
Reference: [CK91] <author> David Callahan and Brian Koblenz. </author> <title> Register allocation via hierarchical graph coloring. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 192-203, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: identifies those values that can readily and profitably reside in a register. 3.2 Graph Coloring Allocators The basic graph coloring technique involves creating a register interference graph and then pruning nodes from that graph that can be trivially colored (assigned a physical register) ([CAC + 81], [Cha82], [CH90], [LH86], [BCKT89], <ref> [CK91] </ref>). The nodes of the graph represent the live ranges of the different register candidates (variables and temporaries). The live range of a candidate is the set of all program points where that candidate is live|as computed by data-flow analysis. <p> Complex heuristics are used to split live ranges to minimize the costs of spilling and reloading the values across the boundaries to the new, smaller live-ranges. This heuristic is repeated until all the register candidates are assigned registers. Callahan and Koblenz allocate registers globally by doing graph coloring "hierarchically" <ref> [CK91] </ref>. They treat the program as a hierarchy of nested "tiles." Tiles may be basic blocks, conditionals, or loops. They assign registers using graph pruning techniques, but start by assigning registers in innermost tiles and progressively assigning registers in enclosing tiles.
Reference: [Cof76] <author> E. G. Coffman, Jr., </author> <title> editor. Computer and Job-Shop Scheduling Theory. </title> <publisher> John Wiley and Sons, </publisher> <year> 1976. </year> <month> 141 </month>
Reference-contexts: DLS is as an attractive, simple, fast and effective alternative to more complicated, slower heuristic solutions. 2.2 Previous Work An adaptation of Hu's algorithm [Hu61] gives an optimal solution to scheduling a tree-structured task system on multiple identical processors if each task has unit execution time <ref> [Cof76] </ref>, but the algorithm does not handle register allocation constraints. For an architecture with 2 functional units, one for loads and one for operations, with identical pipeline constraints, Bernstein et. al. have investigated code scheduling with register allocation for trees ([BPR84] and [BJR89]).
Reference: [ESL89] <author> Helmut Emmelmann, Friedrich-Wilhelm Schroer, and Rudolf Landwehr. </author> <title> BEG|a generator for efficient back ends. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 227-237, </pages> <year> 1989. </year>
Reference-contexts: Thus, the applicability of Twig's patterns is context-sensitive. BURS cannot have this flexibility since all costs must be compile-compile time constants to precompute dynamic programming decisions. 4.11.2 BEG A code generator generator based on tree pattern matching was developed by Emmel-mann, et al. <ref> [ESL89] </ref>. The Back End Generator (BEG) uses naive pattern matching to find pattern matches within the tree IR to do instruction selection. The least-cost cover of the tree is found using dynamic programming techniques that are essentially identical to Twig's. Like Twig, BEG can guard patterns with semantic predicates.
Reference: [FH91a] <author> Christopher W. Fraser and David R. Hanson. </author> <title> A code generation interface for ANSI C. </title> <journal> Software|Practice and Experience, </journal> <volume> 21(9) </volume> <pages> 963-988, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: the testing of this register allocation technique, this occurred only for the SPEC89 benchmark, nasa, when it was compiled with only 6 integer registers available. 3.5 Implementation Results A prototype probabilistic register allocator has been built as part of an experimental code generator for an ANSI C compiler ("lcc" [FH91b] <ref> [FH91a] </ref>). The code generator produces MIPS R2000 assembler. 3.5.1 Stanford Benchmarks The tables in Figure 3.10-3.11 summarizes the results of running the compiler on the Stanford benchmarks suite. Each program was run with three different register configurations for both integer and floating point registers. <p> The first two grammars (used to generate code generators for lcc <ref> [FH91a] </ref>) are for the VAX and the MIPS R3000 RISC processor. Two others that were developed as part of the CodeGen project are integer (byte, word, and long) subsets of the VAX and Motorola 68000 processors.
Reference: [FH91b] <author> Christopher W. Fraser and David R. Hanson. </author> <title> A retargetable compiler for ANSI C. </title> <journal> SIGPLAN Notices, </journal> <volume> 26(10), </volume> <month> October </month> <year> 1991. </year>
Reference-contexts: Since its development, BURG has been made publicly available, and is being used at AT&T Bell Labs to develop code generators for an ANSI C compiler <ref> [FH91b] </ref>. Chapter 2 Delayed-Load Scheduling Modern RISC architectures are characterized by small, simple instruction sets, and general-purpose registers. While simple functionally, many of the instructions are complicated by instruction scheduling requirements. <p> In the testing of this register allocation technique, this occurred only for the SPEC89 benchmark, nasa, when it was compiled with only 6 integer registers available. 3.5 Implementation Results A prototype probabilistic register allocator has been built as part of an experimental code generator for an ANSI C compiler ("lcc" <ref> [FH91b] </ref> [FH91a]). The code generator produces MIPS R2000 assembler. 3.5.1 Stanford Benchmarks The tables in Figure 3.10-3.11 summarizes the results of running the compiler on the Stanford benchmarks suite. Each program was run with three different register configurations for both integer and floating point registers. <p> If the underlying grammar defines all legal tree structures, this can be used to quickly audit trees to ensure that they are not malformed. In an experiment, a BURS pattern matcher audited the intermediate representation generated by the front-end of lcc <ref> [FH91b] </ref>, an ANSI C compiler. The IR trees were tested to see if they were correctly formed with respect to basic types. A small portion of the grammar is given in Figure 4.18. If the matcher finds a parse, then the expression tree is legal, otherwise it is malformed.
Reference: [FH91c] <author> Christopher W. Fraser and Robert R. Henry. </author> <title> Hard-coding bottom-up code generation tables to save time and space. </title> <journal> Software|Practice and Experience, </journal> <volume> 21(1) </volume> <pages> 1-12, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: BURS technology pre-processes the tree patterns and their costs to build automata that can drive instruction selection very quickly. BURS generated instruction selectors can be built that execute fewer than 50 VAX instructions per node of an expression tree <ref> [FH91c] </ref>. BURS code generators are fast for two reasons: they use bottom-up tree pattern matching technology (the theoretically fastest possible [HO82]), and they do all dynamic programming at compile-compile time (i.e., when the patterns are pre-processed to build the code generator). <p> At compile-time, it is only necessary to make two traversals of the subject tree: one bottom-up traversal to label each node with a state that encodes all optimal matches, and a second top-down traversal that uses these states to select and emit code. Fraser and Henry <ref> [FH91c] </ref> report that careful encodings can produce an automaton that executes fewer than 50 88 89 VAX instructions ( 90 RISC instructions) per node to do both traversals. The automaton that labels the tree is a simple state transition machine. <p> A solution to the encoding problem is described by Fraser and Henry in <ref> [FH91c] </ref>. 90 This chapter describes a new simple and efficient table generation algorithm whose implementation is an order of magnitude faster than the best current systems. Simplicity has increased, not decreased, efficiency. <p> As output the program creates C routines and tables for labeling and reducing a subject tree. The program can output either a simple table-driven tree-labeler and reducer, or a hard-coded labeler and reducer. The hard-coded routines incorporate the time and space saving techniques in <ref> [FH91c] </ref>. The entire program is under 4000 lines of code that splits evenly between table generation routines and input/output routines. <p> Readers interested in more detail might start with [BDB90]. Other relevant documents include <ref> [Kro75, HO82, HC86, Cha87, PLG88, PL88, BMW87, Hen89, FH91c, Pro92] </ref>. A.2 Input Burg accepts a tree grammar and emits a BURS tree parser. Figure A.1 shows a sample grammar that implements a very simple instruction selector. Burg grammars are structurally similar to Yacc's. Comments follow C conventions. <p> Instead, Burg does the dynamic programming at compile-compile time, as it builds Burm. Consequently, Burm parses quickly. Similar labellers have taken as few as 15 instructions per node, and reducers as few as 35 per node visited <ref> [FH91c] </ref>. A.4 Debugging Burm invokes PANIC when an error prevents it from proceeding. PANIC has the same signature as printf. It should pass its arguments to printf if diagnostics are desired and then either abort (say via exit) or recover (say via longjmp). If it returns, Burm aborts.
Reference: [FHP91] <author> Christopher W. Fraser, Robert R. Henry, and Todd A. Proebsting. </author> <title> BURG | fast optimal instruction selection and tree parsing. </title> <journal> SIGPLAN Notices, </journal> <volume> 27(4) </volume> <pages> 68-76, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Chapter 4 describes an extremely fast BURS automata generator. The algorithm described is a work-list algorithm that employs simple optimizations. The implementation of the simple algorithm is a code generator generator, BURG, that runs 10 to 7 30 times faster than the previous best system <ref> [FHP91] </ref>. That increase in speed lowered the time to pre-process a VAX grammar from over 7 minutes to under 15 seconds on a DECstation 5000. <p> These fields indicate which rule produces the given nonterminal. There is no need for the cost field at compile-time. 4.9 Implementation Results Our algorithm has been implemented in a system called "BURG" <ref> [FHP91] </ref>. The input has two parts: a description of the operators (including the arity and identifying value of each), and a list of grammar rules.
Reference: [FL88] <author> Charles N. Fischer and Richard J. Leblanc, Jr. </author> <title> Crafting a Compiler. </title> <address> Ben-jamin/Cummings, Menlo Park, California, </address> <year> 1988. </year>
Reference-contexts: Therefore, while succeeding in biasing register allocation within a loop to variables used within that loop, spill decisions must still be made via ad hoc heuristic methods. Few graph coloring techniques do local (basic block level) register allocation as well as established local allocation algorithms ([HFG89], [Fre74], <ref> [FL88] </ref>). Unlike graph coloring algorithms, local allocation techniques are able to exploit information about the simple sequential nature of register usage in the block to minimize local spill code. <p> a register for that loop. 48 3.3.1 Local Register Allocation and Probabilities Most local register allocators share the basic principle of deciding what value should stay in a register (or when a spill is necessary) by checking the closeness of the next use of values already in registers ([HFG89], [Fre74], <ref> [FL88] </ref>). If a value in a register has only distant next uses then it will be spilled before a value to be used sooner.
Reference: [Fre74] <author> R. A. </author> <title> Freiburghouse. Register allocation via usage counts. </title> <journal> Communications of the ACM, </journal> <volume> 17(11), </volume> <month> November </month> <year> 1974. </year>
Reference-contexts: Therefore, while succeeding in biasing register allocation within a loop to variables used within that loop, spill decisions must still be made via ad hoc heuristic methods. Few graph coloring techniques do local (basic block level) register allocation as well as established local allocation algorithms ([HFG89], <ref> [Fre74] </ref>, [FL88]). Unlike graph coloring algorithms, local allocation techniques are able to exploit information about the simple sequential nature of register usage in the block to minimize local spill code. <p> allocated a register for that loop. 48 3.3.1 Local Register Allocation and Probabilities Most local register allocators share the basic principle of deciding what value should stay in a register (or when a spill is necessary) by checking the closeness of the next use of values already in registers ([HFG89], <ref> [Fre74] </ref>, [FL88]). If a value in a register has only distant next uses then it will be spilled before a value to be used sooner.
Reference: [GH88] <author> James R. Goodman and Wei-Chung Hsu. </author> <title> Code scheduling and register allocation in large basic blocks. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1988. </year> <month> 142 </month>
Reference-contexts: Given the DAG, they attempt to schedule the instructions while both obeying pipeline constraints and minimizing registers. Since both optimal scheduling and register allocation on DAGs are NP-complete problems, their solutions to the integrated problem are heuristic. Goodman and Hsu <ref> [GH88] </ref> describe a system, Integrated Prepass Scheduling (IPS), that combines register allocation and instruction scheduling. IPS is conceptually simple. 2 We will use operations to denote non-load instructions. 12 The input is an instruction DAG for which registers have not been assigned.
Reference: [GJ79] <author> M. R. Garey and D. S. Johnson. </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <year> 1979. </year>
Reference-contexts: The intractability of finding an optimal schedule holds even if an unlimited number of registers is available. Optimal local register allocation in itself is also NP-complete in the presence of common subexpressions <ref> [GJ79] </ref>. Such negative results have led to the belief that generating good quality code for RISC machines with pipeline constraints is too difficult to do well except in complex optimizing compilers. 10 Fast, optimal algorithms, however, can be devised for simpler, yet realistic architectures.
Reference: [GM86] <author> Phillip B. Gibbons and Steven S. Muchnick. </author> <title> Efficient instruction scheduling for a pipelined architecture. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <pages> pages 11-16, </pages> <year> 1986. </year>
Reference-contexts: On the R2000, it is necessary to put an explicit NOP in the instruction stream; on the SPARC, the pipeline will automatically interlock and stall the processor for the additional cycle. Previously, most instruction schedulers handled delayed-load scheduling by solving a more general problem of arbitrary instruction scheduling ([HG83], <ref> [GM86] </ref>, [War90], [LLM + 87], and [PS90]). Arbitrary instruction scheduling considers operations other than loads with delays (such as multiplies/divides) that can take many cycles to complete. <p> the attention to code scheduling has been 1 Ours can issue only one instruction per cycle. 11 directed at scheduling expressions represented by directed acyclic graphs (DAGs) for architectures with pipeline constraints after both loads and operations. 2 Heuristic attacks on this general problem can be found in [HG82], [HG83], <ref> [GM86] </ref>, [War90], [LLM + 87], and [PS90]. These techniques are similar in spirit; they schedule instructions from the bottom of the DAG based on differing priority heuristics.
Reference: [Han90] <author> David R. Hanson. </author> <title> Fast allocation and deallocation of memory based on object lifetimes. </title> <journal> Software|Practice and Experience, </journal> <volume> 20(1) </volume> <pages> 5-12, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Redundant itemsets really must be deallocated|for a 68000 grammar the program computed over 100,000 redundant itemsets. Fortunately, knowledge of the the allocation/deallocation pattern of particular data can lead to very efficient memory management <ref> [Han90] </ref>. This is the case with itemsets. Itemsets, after allocation, are computed and then either retained forever or immediately released. It can never be the case, therefore, that two itemset deallocations occur sequentially without an intervening allocation. This allows the creation of specialized deallocation and allocation routines for itemsets.
Reference: [HC86] <author> Philip J. Hatcher and Thomas W. Christopher. </author> <title> High-quality code generation via bottom-up tree pattern matching. </title> <booktitle> In Proceedings of the 13th Annual Symposium on Principles of Programming Languages, </booktitle> <pages> pages 119-130, </pages> <year> 1986. </year>
Reference-contexts: Readers interested in more detail might start with [BDB90]. Other relevant documents include <ref> [Kro75, HO82, HC86, Cha87, PLG88, PL88, BMW87, Hen89, FH91c, Pro92] </ref>. A.2 Input Burg accepts a tree grammar and emits a BURS tree parser. Figure A.1 shows a sample grammar that implements a very simple instruction selector. Burg grammars are structurally similar to Yacc's. Comments follow C conventions.
Reference: [Hen89] <author> Robert R. Henry. </author> <title> Encoding optimal pattern selection in a table-driven bottom-up tree-pattern matcher. </title> <type> Technical Report 89-02-04, </type> <institution> University of Washington, </institution> <year> 1989. </year>
Reference-contexts: Subsequent BURS systems, including the techniques described here, do not allow general rewrites, but instead defer that responsibility to another phase of the compilation process. Balachandran, Dhamdhere, and Biswas [BDB90] simplified Pelegri's model by disallowing rewrite rules, and also generalized Chase's ideas to use cost information. Henry <ref> [Hen89] </ref> developed optimization techniques to limit the number of BURS states produced during table generation. With fewer states, a smaller automaton is produced more quickly. Henry's techniques are much more aggressive than Chase's simple index map techniques, but at the cost of increased complexity. In [Hen89], Henry states, "The table builder <p> Henry <ref> [Hen89] </ref> developed optimization techniques to limit the number of BURS states produced during table generation. With fewer states, a smaller automaton is produced more quickly. Henry's techniques are much more aggressive than Chase's simple index map techniques, but at the cost of increased complexity. In [Hen89], Henry states, "The table builder uses space and time voraciously, even though it uses very complex algorithms designed to minimize these resources." Our algorithm generalizes and simplifies his work. <p> A nonterminal is unessential (in a particular state) if it can be proven that it will never be needed to produce a least-cost cover of any subject tree. Henry devised two ad hoc techniques, "sibling," and "demand" trimming <ref> [Hen89] </ref>, to identify when one "f cost, rule g" item (representing a nonterminal) can be safely removed from a state because another item subsumes it. Triangle Trimming By generalizing Henry's trimming techniques, we have developed triangle trimming for safely removing unessential nonterminals from an itemset. <p> Because state minimization is a post-pass, it cannot make the program faster|it must make it slower. 3 We decided the space savings was not worth the additional complexity or time and, therefore, did not attempt to add a state minimization 3 Henry <ref> [Hen89] </ref> found that the additional time for the post-pass was negligible (&lt; 1%) in his system. 113 pass. 4.7.3 Normalize Specialization When an itemset is normalized, the relative costs of all the nonterminals are retained. <p> We compare our system to Henry's table generator that was derived from the CodeGen system <ref> [Hen89] </ref>. His system consists of over 20,000 lines of C code. <p> Readers interested in more detail might start with [BDB90]. Other relevant documents include <ref> [Kro75, HO82, HC86, Cha87, PLG88, PL88, BMW87, Hen89, FH91c, Pro92] </ref>. A.2 Input Burg accepts a tree grammar and emits a BURS tree parser. Figure A.1 shows a sample grammar that implements a very simple instruction selector. Burg grammars are structurally similar to Yacc's. Comments follow C conventions. <p> The command: burg [ arguments ] [ file ] invokes Burg. If a file is named, Burg expects its grammar there; otherwise it reads the standard input. The options include: -c N Abort if any relative cost exceeds N , which keeps Burg from looping on diverging grammars. <ref> [PLG88, Hen89, BDB90, Pro92] </ref> explain relative costs. -d Report a few statistics and flag unused rules and terminals. -o file Write parser into file. Otherwise it writes to the standard output. -p prefix Start exported names with prefix.
Reference: [HFG89] <author> Wei-Chung Hsu, Charles N. Fischer, and James R. Goodman. </author> <title> On the minimization of loads/stores in local register allocation. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 15(10) </volume> <pages> 1252-1260, </pages> <year> 1989. </year>
Reference-contexts: Ultimately, when an operand is actually 43 used it must be in a register, and once a value is in a register, it is easy to reuse within a basic block. Simple, fast and nearly optimal local register allocators are known <ref> [HFG89] </ref>. Once local register needs are met, the effects of global allocation can be estimated ([Bea74], [Mor91]).
Reference: [HG82] <author> John L. Hennessy and Thomas R. Gross. </author> <title> Code generation and reorganization in the presence of pipeline constraints. </title> <booktitle> In Proceedings of the 9th Annual Symposium on Principles of Programming Languages, </booktitle> <pages> pages 120-127, </pages> <year> 1982. </year> <month> 143 </month>
Reference-contexts: Because registers are scarce, and can be advantageously used to hold temporary and global values, it is important not to overuse them when scheduling instructions. 2.1 Overview The problem of optimally scheduling instructions under arbitrary pipeline constraints is NP-complete ([GJ79], [LLM + 87], <ref> [HG82] </ref>, and [PS90]). Many heuristics have been proposed for scheduling pipelined code; all assume, however, that pipeline constraints can occur after any instruction, and that operators may share common subexpressions. The intractability of finding an optimal schedule holds even if an unlimited number of registers is available. <p> Most of the attention to code scheduling has been 1 Ours can issue only one instruction per cycle. 11 directed at scheduling expressions represented by directed acyclic graphs (DAGs) for architectures with pipeline constraints after both loads and operations. 2 Heuristic attacks on this general problem can be found in <ref> [HG82] </ref>, [HG83], [GM86], [War90], [LLM + 87], and [PS90]. These techniques are similar in spirit; they schedule instructions from the bottom of the DAG based on differing priority heuristics.
Reference: [HG83] <author> John L. Hennessy and Thomas R. Gross. </author> <title> Postpass code optimization of pipeline constraints. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(3) </volume> <pages> 422-448, </pages> <month> July </month> <year> 1983. </year>
Reference-contexts: of the attention to code scheduling has been 1 Ours can issue only one instruction per cycle. 11 directed at scheduling expressions represented by directed acyclic graphs (DAGs) for architectures with pipeline constraints after both loads and operations. 2 Heuristic attacks on this general problem can be found in [HG82], <ref> [HG83] </ref>, [GM86], [War90], [LLM + 87], and [PS90]. These techniques are similar in spirit; they schedule instructions from the bottom of the DAG based on differing priority heuristics.
Reference: [HO82] <author> Christoph M. Hoffmann and Michael J. O'Donnell. </author> <title> Pattern matching in trees. </title> <journal> Journal of the ACM, </journal> <volume> 29(1) </volume> <pages> 68-95, </pages> <month> January </month> <year> 1982. </year>
Reference-contexts: BURS generated instruction selectors can be built that execute fewer than 50 VAX instructions per node of an expression tree [FH91c]. BURS code generators are fast for two reasons: they use bottom-up tree pattern matching technology (the theoretically fastest possible <ref> [HO82] </ref>), and they do all dynamic programming at compile-compile time (i.e., when the patterns are pre-processed to build the code generator). By doing dynamic programming at compile-compile time, a BURS code generator can anticipate all possible input trees with information stored in tables. <p> Triangle trimming is an un-complicated optimization that, for complex grammars, reduces both the table generation time and table sizes by over 50%. We also describe optimizations that take advantage of special properties of BURS states. 4.2 Related Work Bottom-up tree pattern matching was developed by Hoffman and O'Donnell <ref> [HO82] </ref>. Bottom-up pattern matching is the theoretically fastest possible|relying on a single bottom-up tree walk with a simple table lookup at each node in a tree to do the matching. BURS technology relies on this technology for much of its speed. <p> Given a specification of tree patterns and associated costs, Twig generates a tree automaton that will find the least-cost cover of a subject tree. Twig uses fast top-down Hoffmann-O'Donnell <ref> [HO82] </ref> pattern matching in parallel with dynamic programming to find the least-cost cover in O (patno fi jtreej) time (where patno is the number of patterns in the grammar, and jtreej is the size of the tree to be parsed). <p> Readers interested in more detail might start with [BDB90]. Other relevant documents include <ref> [Kro75, HO82, HC86, Cha87, PLG88, PL88, BMW87, Hen89, FH91c, Pro92] </ref>. A.2 Input Burg accepts a tree grammar and emits a BURS tree parser. Figure A.1 shows a sample grammar that implements a very simple instruction selector. Burg grammars are structurally similar to Yacc's. Comments follow C conventions.
Reference: [Hu61] <author> T. C. Hu. </author> <title> Parallel sequencing and assembly line problems. </title> <journal> Operations Research, </journal> <volume> 9(6) </volume> <pages> 841-848, </pages> <year> 1961. </year>
Reference-contexts: It is restricted to handling expression trees in which all leaf nodes are direct memory references. DLS is as an attractive, simple, fast and effective alternative to more complicated, slower heuristic solutions. 2.2 Previous Work An adaptation of Hu's algorithm <ref> [Hu61] </ref> gives an optimal solution to scheduling a tree-structured task system on multiple identical processors if each task has unit execution time [Cof76], but the algorithm does not handle register allocation constraints.
Reference: [KFP92] <author> Steven M. Kurlander, Charles N. Fischer, and Todd A. Proebsting. </author> <title> Extended delayed-load scheduling. </title> <note> Technical report (in preparation), </note> <institution> University of Wisconsin, </institution> <year> 1992. </year>
Reference-contexts: A more realistic machine model must be able to handle unary nodes, leaf instructions without delays, and delayed loads at internal nodes. Kurlander, Fischer, and Proebsting <ref> [KFP92] </ref> have extended DLS to optimally handle unary nodes and non-delayed leaf nodes. The improvements are called Extended DLS (EDLS). In addition, they give a simple heuristic for scheduling trees with internal delayed loads. 2.7.1 Non-Delayed Leaf Nodes Not all leaf instructions on real machines incur delay cycles. <p> The detailed algorithm and proof of correctness can be found in <ref> [KFP92] </ref>. 2.7.3 Register Variables As presented, the DLS algorithm cannot handle register variables (i.e., leaf nodes that do not represent load instructions). With register variables, it is not always the case that 32 leaf nodes allocate registers or that operations will decrease the number of registers in use. <p> A DAG is split by computing shared internal nodes to temporary storage prior to computing ancestor nodes. Proceeding in a bottom-up fashion, evaluating a DAG then reduces to evaluating a sequence of trees (a forest). Furthermore, Kurlander <ref> [KFP92] </ref> has used this splitting idea to handle delayed loads that are internal to a tree (or DAG). Whenever a non-leaf load is encountered, the tree is split so that that load is now at the frontier of the original tree.
Reference: [Kro75] <author> H. Kron. </author> <title> Tree Templates and Subtree Transformational Grammars. </title> <type> PhD thesis, </type> <institution> University of California, Santa Cruz, </institution> <year> 1975. </year>
Reference-contexts: Readers interested in more detail might start with [BDB90]. Other relevant documents include <ref> [Kro75, HO82, HC86, Cha87, PLG88, PL88, BMW87, Hen89, FH91c, Pro92] </ref>. A.2 Input Burg accepts a tree grammar and emits a BURS tree parser. Figure A.1 shows a sample grammar that implements a very simple instruction selector. Burg grammars are structurally similar to Yacc's. Comments follow C conventions.
Reference: [LH86] <author> J. R. Larus and P. N. Hilfinger. </author> <title> Register allocation in the SPUR lisp compiler. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <pages> pages 255-263, </pages> <year> 1986. </year>
Reference-contexts: Previous global register allocation methods have concentrated on casting register allocation as a graph-coloring problem ([CAC + 81], [CH90], [BCKT89], <ref> [LH86] </ref>). Since no two simultaneously live values may be assigned to the same register, an interference graph can be built where nodes represent register candidate values, and arcs exist between simultaneously live values. <p> Register assignment is the problem of determining which actual physical register will hold a particular value (that has previously been allocated a register). 3.1 Overview The dominant paradigm in modern global register allocation is graph coloring ([CH90], [CAC + 81], [BCKT89], <ref> [LH86] </ref>). Unfortunately, graph coloring does not really address the issue of register allocation, but rather the related issue of register assignment. That is, graph coloring tells us how to assign registers so that simultaneously live values aren't assigned the same register. <p> and yet identifies those values that can readily and profitably reside in a register. 3.2 Graph Coloring Allocators The basic graph coloring technique involves creating a register interference graph and then pruning nodes from that graph that can be trivially colored (assigned a physical register) ([CAC + 81], [Cha82], [CH90], <ref> [LH86] </ref>, [BCKT89], [CK91]). The nodes of the graph represent the live ranges of the different register candidates (variables and temporaries). The live range of a candidate is the set of all program points where that candidate is live|as computed by data-flow analysis. <p> A node (register candidate) is picked based on a cost measure, removed from the graph, and assigned permanently to a memory location. All subsequent references to that value must be from memory. Priority-based coloring ([CH90], <ref> [LH86] </ref>) also builds an interference graph and attempts to color it by pruning nodes. If this pruning blocks, a heuristic is employed to split large, costly live ranges into smaller ranges in an attempt to produce a graph that can be further pruned.
Reference: [LLM + 87] <author> Eugene Lawler, Jan Karel Lenstra, Charles Martel, Barbara Simons, and Larry Stockmeyer. </author> <title> Pipeline scheduling: A survey. </title> <institution> Computer science research report, IBM Research Division, </institution> <year> 1987. </year>
Reference-contexts: Previously, most instruction schedulers handled delayed-load scheduling by solving a more general problem of arbitrary instruction scheduling ([HG83], [GM86], [War90], <ref> [LLM + 87] </ref>, and [PS90]). Arbitrary instruction scheduling considers operations other than loads with delays (such as multiplies/divides) that can take many cycles to complete. This thesis describes the delayed-load scheduling (DLS) algorithm for doing instruction scheduling for computations in which the only instructions facing pipeline constraints are loads. <p> Because registers are scarce, and can be advantageously used to hold temporary and global values, it is important not to overuse them when scheduling instructions. 2.1 Overview The problem of optimally scheduling instructions under arbitrary pipeline constraints is NP-complete ([GJ79], <ref> [LLM + 87] </ref>, [HG82], and [PS90]). Many heuristics have been proposed for scheduling pipelined code; all assume, however, that pipeline constraints can occur after any instruction, and that operators may share common subexpressions. The intractability of finding an optimal schedule holds even if an unlimited number of registers is available. <p> to code scheduling has been 1 Ours can issue only one instruction per cycle. 11 directed at scheduling expressions represented by directed acyclic graphs (DAGs) for architectures with pipeline constraints after both loads and operations. 2 Heuristic attacks on this general problem can be found in [HG82], [HG83], [GM86], [War90], <ref> [LLM + 87] </ref>, and [PS90]. These techniques are similar in spirit; they schedule instructions from the bottom of the DAG based on differing priority heuristics.
Reference: [Mor91] <author> W. G. Morris. CCG: </author> <title> A prototype coagulating code generator. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 45-58, </pages> <year> 1991. </year> <month> 144 </month>
Reference-contexts: Simple, fast and nearly optimal local register allocators are known [HFG89]. Once local register needs are met, the effects of global allocation can be estimated ([Bea74], <ref> [Mor91] </ref>). In particular, a good global allocation improves upon good local allocation by eliminating unnecessary loads at the entrance to a basic block and by eliminating unnecessary stores at the exit from a basic block.
Reference: [PF91] <author> Todd A. Proebsting and Charles N. Fischer. </author> <title> Linear-time optimal code scheduling for delayed-load architectures. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1991. </year>
Reference: [PF92] <author> Todd A. Proebsting and Charles N. Fischer. </author> <title> Probabilistic register allocation. </title> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1992. </year>
Reference: [PH90] <author> David A. Patterson and John L. Hennessy. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> Palo Alto, California, </address> <year> 1990. </year>
Reference-contexts: Our simple machine's instruction set is given in Figure 2.2. This architecture is an approximation of the integer functional units of many modern RISC processors such as the SPARC and MIPS R3000 <ref> [PH90] </ref>. A delayed load requires that the destination of a load not be accessed by subsequent instructions for some number of instruction cycles, although other, unrelated instructions may execute.
Reference: [PL88] <author> Eduardo Pelegri-Llopart. </author> <title> Rewrite Systems, Pattern Matching, and Code Generation. </title> <type> Phd Thesis, Technical Report UCB/CSD 88/423, </type> <institution> Computer Science Division, University of California, Berkeley, </institution> <year> 1988. </year>
Reference-contexts: Chase demonstrated that these maps can be produced on-the-fly during table generation so that no superfluous work need be performed [Cha87]. Pelegri-Llopart, the originator of BURS theory ([PLG88], <ref> [PL88] </ref>), encorporated Chase's ideas into a system that added cost information for dynamic programming at table generation time. In addition to recognizing that dynamic programming could be done prior to compile time, he developed the theoretical foundation for showing that the process is theoretically feasible for typical machine grammars. <p> To do this, there must be only a finite number of states. Grammars that do not produce a finite number of states are said to diverge <ref> [PL88] </ref>. A grammar diverges when it is possible for the derivation costs of a pair of non-terminals in the same state to become arbitrarily distant. To prevent the BURS table generation algorithm from attempting to enumerate an infinite set of states for diverging grammars, a simple threshold test is used. <p> Readers interested in more detail might start with [BDB90]. Other relevant documents include <ref> [Kro75, HO82, HC86, Cha87, PLG88, PL88, BMW87, Hen89, FH91c, Pro92] </ref>. A.2 Input Burg accepts a tree grammar and emits a BURS tree parser. Figure A.1 shows a sample grammar that implements a very simple instruction selector. Burg grammars are structurally similar to Yacc's. Comments follow C conventions. <p> If a bonafide machine grammar appears to make Burg loop, try a host with more memory. To apply Burg to problems other than instruction selection, be prepared to consult the literature on cost-divergence <ref> [PL88] </ref>. A.5 Running Burg Burg reads a tree grammar and writes a Burm in C. Burm can be compiled by itself or included in another file. When suitably named with the -p option, disjoint instances of Burm should link together without name conflicts.
Reference: [PLG88] <author> Eduardo Pelegri-Llopart and Susan L. Graham. </author> <title> Optimal code generation for expression trees: An application of BURS theory. </title> <booktitle> In Proceedings of the 15th Annual Symposium on Principles of Programming Languages, </booktitle> <pages> pages 294-308, </pages> <year> 1988. </year>
Reference-contexts: If the tree patterns that describe different instructions are given weights to describe their relative costs, dynamic programming can be used to select the optimal set of instructions to evaluate the tree ([AGT89], [AJ76], <ref> [PLG88] </ref>, [BDB90], and [AG85]). Dynamic programming is an expensive operation since it finds all optimal sub-solutions before finding a solution for the entire tree. Fortunately, Bottom-Up Rewrite System (BURS) technology, can hide this cost from the compiler [PLG88]. <p> to select the optimal set of instructions to evaluate the tree ([AGT89], [AJ76], <ref> [PLG88] </ref>, [BDB90], and [AG85]). Dynamic programming is an expensive operation since it finds all optimal sub-solutions before finding a solution for the entire tree. Fortunately, Bottom-Up Rewrite System (BURS) technology, can hide this cost from the compiler [PLG88]. BURS technology pre-processes the tree patterns and their costs to build automata that can drive instruction selection very quickly. BURS generated instruction selectors can be built that execute fewer than 50 VAX instructions per node of an expression tree [FH91c]. <p> Readers interested in more detail might start with [BDB90]. Other relevant documents include <ref> [Kro75, HO82, HC86, Cha87, PLG88, PL88, BMW87, Hen89, FH91c, Pro92] </ref>. A.2 Input Burg accepts a tree grammar and emits a BURS tree parser. Figure A.1 shows a sample grammar that implements a very simple instruction selector. Burg grammars are structurally similar to Yacc's. Comments follow C conventions. <p> The command: burg [ arguments ] [ file ] invokes Burg. If a file is named, Burg expects its grammar there; otherwise it reads the standard input. The options include: -c N Abort if any relative cost exceeds N , which keeps Burg from looping on diverging grammars. <ref> [PLG88, Hen89, BDB90, Pro92] </ref> explain relative costs. -d Report a few statistics and flag unused rules and terminals. -o file Write parser into file. Otherwise it writes to the standard output. -p prefix Start exported names with prefix.
Reference: [Pro92] <author> Todd A. Proebsting. </author> <title> Simple and efficient burs table generation. </title> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: Readers interested in more detail might start with [BDB90]. Other relevant documents include <ref> [Kro75, HO82, HC86, Cha87, PLG88, PL88, BMW87, Hen89, FH91c, Pro92] </ref>. A.2 Input Burg accepts a tree grammar and emits a BURS tree parser. Figure A.1 shows a sample grammar that implements a very simple instruction selector. Burg grammars are structurally similar to Yacc's. Comments follow C conventions. <p> The command: burg [ arguments ] [ file ] invokes Burg. If a file is named, Burg expects its grammar there; otherwise it reads the standard input. The options include: -c N Abort if any relative cost exceeds N , which keeps Burg from looping on diverging grammars. <ref> [PLG88, Hen89, BDB90, Pro92] </ref> explain relative costs. -d Report a few statistics and flag unused rules and terminals. -o file Write parser into file. Otherwise it writes to the standard output. -p prefix Start exported names with prefix.
Reference: [PS90] <author> Krishna Palem and Barbara Simons. </author> <title> Scheduling time-critical instructions on RISC machines. </title> <booktitle> In Proceedings of the 17th Annual Symposium on Principles of Programming Languages, </booktitle> <pages> pages 270-280, </pages> <year> 1990. </year> <month> 145 </month>
Reference-contexts: Previously, most instruction schedulers handled delayed-load scheduling by solving a more general problem of arbitrary instruction scheduling ([HG83], [GM86], [War90], [LLM + 87], and <ref> [PS90] </ref>). Arbitrary instruction scheduling considers operations other than loads with delays (such as multiplies/divides) that can take many cycles to complete. This thesis describes the delayed-load scheduling (DLS) algorithm for doing instruction scheduling for computations in which the only instructions facing pipeline constraints are loads. <p> Because registers are scarce, and can be advantageously used to hold temporary and global values, it is important not to overuse them when scheduling instructions. 2.1 Overview The problem of optimally scheduling instructions under arbitrary pipeline constraints is NP-complete ([GJ79], [LLM + 87], [HG82], and <ref> [PS90] </ref>). Many heuristics have been proposed for scheduling pipelined code; all assume, however, that pipeline constraints can occur after any instruction, and that operators may share common subexpressions. The intractability of finding an optimal schedule holds even if an unlimited number of registers is available. <p> been 1 Ours can issue only one instruction per cycle. 11 directed at scheduling expressions represented by directed acyclic graphs (DAGs) for architectures with pipeline constraints after both loads and operations. 2 Heuristic attacks on this general problem can be found in [HG82], [HG83], [GM86], [War90], [LLM + 87], and <ref> [PS90] </ref>. These techniques are similar in spirit; they schedule instructions from the bottom of the DAG based on differing priority heuristics.
Reference: [SU70] <author> Ravi Sethi and J. D. Ullman. </author> <title> The generation of optimal code for arithmetic expressions. </title> <journal> Journal of the ACM, </journal> <volume> 17(4) </volume> <pages> 715-728, </pages> <month> October </month> <year> 1970. </year>
Reference: [Wai91] <author> William Waite. </author> <type> Personal communication. </type> <institution> (Electronic mail), </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: For instance, BURS can be used to do simple type inferencing, data structure auditing, and tree simplification. 4.10.1 Simple Type Inferencing Waite has used BURG to automate simple type inferencing <ref> [Wai91] </ref>. In many Algol-like languages, arithmetic operators are overloaded and may operate on different types. For example, in Pascal, "+" may operate on sets, reals, or integers|but both operands must be the same type.
Reference: [Wal86] <author> David W. Wall. </author> <title> Global register allocation at link time. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <pages> pages 264-275, </pages> <year> 1986. </year>
Reference-contexts: Wall <ref> [Wal86] </ref> built a system that allocated registers interprocedurally, at link-time when the entire program was available. All local and global variables had been previously allocated to memory, and his allocator attempted to allocate these values|when most profitable|to registers.
Reference: [War90] <author> H. S. Warren, Jr. </author> <title> Instruction scheduling for the IBM RISC system/6000 processor. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 34(1) </volume> <pages> 85-92, </pages> <year> 1990. </year>
Reference-contexts: Previously, most instruction schedulers handled delayed-load scheduling by solving a more general problem of arbitrary instruction scheduling ([HG83], [GM86], <ref> [War90] </ref>, [LLM + 87], and [PS90]). Arbitrary instruction scheduling considers operations other than loads with delays (such as multiplies/divides) that can take many cycles to complete. This thesis describes the delayed-load scheduling (DLS) algorithm for doing instruction scheduling for computations in which the only instructions facing pipeline constraints are loads. <p> attention to code scheduling has been 1 Ours can issue only one instruction per cycle. 11 directed at scheduling expressions represented by directed acyclic graphs (DAGs) for architectures with pipeline constraints after both loads and operations. 2 Heuristic attacks on this general problem can be found in [HG82], [HG83], [GM86], <ref> [War90] </ref>, [LLM + 87], and [PS90]. These techniques are similar in spirit; they schedule instructions from the bottom of the DAG based on differing priority heuristics.
References-found: 53

