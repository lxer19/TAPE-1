URL: http://www.cs.ubc.ca/spider/cebly/Papers/_download_/planning.ps
Refering-URL: http://www.cs.ubc.ca/spider/cebly/papers.html
Root-URL: 
Email: cebly@cs.ubc.ca  tld@cs.brown.edu  hanks@cs.washington.edu  
Phone: Tel. (604) 822-4632  Tel. (401) 863-7645  Tel. (206) 543-4784  
Title: Planning Under Uncertainty: Structural Assumptions and Computational Leverage  
Author: Craig Boutilier Thomas Dean Steve Hanks 
Keyword: decision-theoretic planning, action representation, uncertainty, stochastic domains  
Note: This research was supported by NSERC Research Grant OGP0121843, and the NCE IRIS-II program Project IC-7. This work was supported in part by a National Science Foundation Presidential Young Investigator Award IRI-8957601 and by the Air Force and the Advanced Research Projects Agency of the Department of Defense under Contract No. F30602-91-C-0041. This work was supported in part by NSF grant IRI-9008670 and in part by a grant from the University of Washington Royalty Research Fund.  
Address: Vancouver, BC V6T 1Z4  Providence, RI 02912  Seattle, WA 98195  
Affiliation: Dept. of Comp. Science Univ. of British Columbia  Dept. of Comp. Science Brown University  Dept. of Comp. Sci. and Eng. Univ. of Washington  
Abstract: The problem of planning under uncertainty has been addressed by researchers in many different fields, adopting rather different perspectives on the problem. Unfortunately, these researchers are not always aware of the relationships among these different problem formulations, often resulting in confusion and duplicated effort. Many probabilistic planning or decision making problems can be characterized as a class of Markov decision processes that allow for significant compression in representing the underlying system dynamics. It is for this class of problems that we as experts in intensional representations are advantageously positioned to contribute efficient solution methods. This paper provides a general characterization of the representational requirements for this class of problems, and we describe how to achieve computational leverage using representations that make different types of dependency information explicit. Notice: This paper has not already been accepted by and is not currently under review for a journal or another conference. Nor will it be submitted for such consideration during IJCAI's review period. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <year> 1957. </year>
Reference-contexts: Originally proposed in <ref> [1] </ref>, the DP technique is based on the following principle of optimality. <p> The principle of optimality can be applied to infinite-horizon problems as well. These include expected average-per-stage reward problems and classic expected total discounted reward problems <ref> [1, 12] </ref>, where the value function is the discounted sum of rewards incurred at every stage of execution: V (h) = t=0 where fl is a fixed discount rate (0 &lt; fl &lt; 1). Optimal policies for these problems, assuming FOMDPs, are stationary, therefore requiring only O (jSj) space.
Reference: [2] <author> D. P. Bertsekas and D. A. Castanon. </author> <title> Adaptive aggregation for infinite horizon dynamic programming. </title> <journal> IEEE Trans. on Aut. Control, </journal> <volume> 34(6) </volume> <pages> 589-598, </pages> <year> 1989. </year>
Reference-contexts: Boutilier et al. [5] describe similar ideas using 2TBNs with decision trees for infinite horizon problems and compact policy representations. Shachter and Peot [22] describe how value nodes can be replaced by chance nodes 13 See <ref> [2] </ref> for adaptive aggregation in flat states spaces. 13 to solve influence diagrams using probabilistic inference; see Savage [21] on the foundational relationship between utilities and probabilities. Wellman [25] describes a STRIPS assumption for planning under uncertainty, while Hanks and McDermott [11] introduce probabilistic state-space operators.
Reference: [3] <author> D. P. Bertsekas. </author> <title> Dynamic Programming. </title> <publisher> Prentice-Hall, </publisher> <year> 1987. </year>
Reference-contexts: Space limitations prevent the comprehensive discussion of related work that is warranted; but we briefly mention some of the especially relevant research. Bertsekas <ref> [3] </ref> and Puterman [19] provide excellent and extensive coverage of Markov decision processes and dynamic programming. Dean and Wellman [8] present the view of planning problems in terms of expected value over histories, while Dean et al. [6] propose MDPs as a model for planning in stochastic domains.
Reference: [4] <author> C. Boutilier and R. Dearden. </author> <title> Using abstractions for decision theoretic planning with time constraints. </title> <booktitle> AAAI-94. </booktitle> <address> Seattle, </address> <year> 1994. </year> <month> 14 </month>
Reference-contexts: One drawback of this representation is the need to specify mutually exclusive propositional partitions for each condition under which the action induces a distinct joint distribution. Network representations, on the other hand, only require partitions for each distinct variable (though see <ref> [4] </ref> for factored PSOs). Structure in the Value Function: Value functions, in the general case arbitrary functions of histories, also typically exhibit useful structure. <p> In classical hierarchical planning [20], state-space operators suggest the criticality of various propositions, allowing one to plan for the most important propositions. In MDPs, 2TBNs with value nodes or PSOs can be used to identify the relative contribution variables make to overall value <ref> [4] </ref>. By ignoring variables, we generally cannot guarantee the discovery of an optimal (or even satisficing) policy, unless these variables are strictly irrelevant. In the classical setting, an abstract plan is used to guide the search for a concrete plan [20] with this guarantee. <p> In the classical setting, an abstract plan is used to guide the search for a concrete plan [20] with this guarantee. In general MDPs, nonoptimal plans will have some value; by identifying and eliminating variables with relatively little contribution to value, sat-isficing policies are thus feasible <ref> [4] </ref>. Ultimately, more adaptive, nonuniform prior aggregation techniques should offer the advantages of both forms of aggregation. 5 Related Work The general model we have presented here and the particular representations we described draw tremendously from existing work. <p> Sacerdoti [20] uses abstraction for planning in classical settings, while Knoblock [15] addresses the question of generating abstractions using STRIPS action descriptions. Boutilier and Dearden <ref> [4] </ref> propose a nonadaptive abstraction method for MDPs that exploits factored representations of actions and value functions. 6 Conclusion Research in decision-theoretic planning and planning under uncertainty has taken many different directions, and each line of research adopted a particular set of assumptions about the underlying problem and about the target
Reference: [5] <author> Craig B., R. Dearden, and M. Goldszmidt. </author> <title> Exploiting structure in policy construction. </title> <booktitle> To appear AAAI Spr. Symp. on Extending Theories of Action, </booktitle> <year> 1995. </year>
Reference-contexts: DP techniques can be employed to solve problems of this sort [24]: while the same basic principle of optimality is used, computational leverage can be gained by algorithms that use the structured representation to identify relevant or irrelevant variables at various stages of the process, given decisions at later stages <ref> [24, 5] </ref>. <p> DP over a finite horizon is undirected: since all (reachable) states have potential impact, all must be considered when constructing a policy. DP is simply an efficient way to consider all states; but even if structured system dynamics reduce the effective state space <ref> [24, 5] </ref>, all groups of states must be visited. In a goal-based setting, no value is accrued for states visited in the process of achieving the goal nor for anything that happens after the goal is achieved. 12 An optimal plan must reach a goal state. <p> Luenberger [18] provides a definition and discussion of separable value functions. Tatman and Shachter [24] show how separable value functions are incorporated in influence diagrams and their relation to dynamic programming. Boutilier et al. <ref> [5] </ref> describe similar ideas using 2TBNs with decision trees for infinite horizon problems and compact policy representations.
Reference: [6] <author> T. Dean, L. P. Kaelbling, J. Kirman, and A. Nicholson. </author> <title> Planning with deadlines in stochastic domains. </title> <booktitle> AAAI-93, </booktitle> <address> pp.574-579. Washington, DC, </address> <year> 1993. </year>
Reference-contexts: Bertsekas [3] and Puterman [19] provide excellent and extensive coverage of Markov decision processes and dynamic programming. Dean and Wellman [8] present the view of planning problems in terms of expected value over histories, while Dean et al. <ref> [6] </ref> propose MDPs as a model for planning in stochastic domains. Dean and Kanazawa [7, 14] introduce the notion of 2TBNs and probabilistic action networks based on influence diagrams. Luenberger [18] provides a definition and discussion of separable value functions.
Reference: [7] <author> T. Dean and K. </author> <title> Kanazawa. A model for reasoning about persistence and causation. </title> <journal> Comp. Intel., </journal> <volume> 5(3) </volume> <pages> 142-150, </pages> <year> 1989. </year>
Reference-contexts: Dean and Wellman [8] present the view of planning problems in terms of expected value over histories, while Dean et al. [6] propose MDPs as a model for planning in stochastic domains. Dean and Kanazawa <ref> [7, 14] </ref> introduce the notion of 2TBNs and probabilistic action networks based on influence diagrams. Luenberger [18] provides a definition and discussion of separable value functions. Tatman and Shachter [24] show how separable value functions are incorporated in influence diagrams and their relation to dynamic programming.
Reference: [8] <author> T. Dean and M. Wellman. </author> <title> Planning and Control. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: Space limitations prevent the comprehensive discussion of related work that is warranted; but we briefly mention some of the especially relevant research. Bertsekas [3] and Puterman [19] provide excellent and extensive coverage of Markov decision processes and dynamic programming. Dean and Wellman <ref> [8] </ref> present the view of planning problems in terms of expected value over histories, while Dean et al. [6] propose MDPs as a model for planning in stochastic domains. Dean and Kanazawa [7, 14] introduce the notion of 2TBNs and probabilistic action networks based on influence diagrams.
Reference: [9] <author> D. Draper, S. Hanks, and D. Weld. </author> <title> Probabilistic planning with information gathering and contingent execution. </title> <booktitle> 2nd Intl. Conf. AI Planning Systems, </booktitle> <year> 1994. </year>
Reference-contexts: Partially observable problems are captured by highlighting observable variables or using information arcs into decision nodes. 11 Though most classical planning problems have been cast in deterministic terms, the Buridan and C-Buridan planners <ref> [16, 9] </ref> reformulate the problem in probabilistically: given a distribution over states, a set of actions, a goal expression, and a probability threshold t , generate a sequence of actions that when executed will leave the system in a goal state with probability at least t . <p> Wellman [25] describes a STRIPS assumption for planning under uncertainty, while Hanks and McDermott [11] introduce probabilistic state-space operators. Kushmerick et al. [16] propose a method for open-loop probabilistic planning using PSOs in nonobservable domains, while Draper et al. <ref> [9] </ref> address closed-loop probabilistic planning in partially-observable settings (see also [23, 17] for the classical definition of POMDPs). Sacerdoti [20] uses abstraction for planning in classical settings, while Knoblock [15] addresses the question of generating abstractions using STRIPS action descriptions.
Reference: [10] <author> R. Fikes and N. J. Nilsson. </author> <title> Strips: A new approach to the application of theorem proving to problem solving. </title> <booktitle> Art. Intel., </booktitle> <volume> 2 </volume> <pages> 189-208, </pages> <year> 1971. </year>
Reference-contexts: In models with boolean variables, we can describe states using logical propositions. A variant of the decision tree representation of actions are probabilistic state space operators (PSOs) an extension of the classical STRIPS operators <ref> [10] </ref>. A PSO ff is a set of triples of the form h; ; !i where is a set of propositions (preconditions) that describe a subset of S, is a probability, and ! is a set of propositions (postconditions) describing another subset of S.
Reference: [11] <author> S. Hanks and D. V. McDermott. </author> <title> Modeling a dynamic and uncertain world i: Symbolic and probabilistic reasoning about change. Art. </title> <publisher> Intel., </publisher> <year> 1994. </year>
Reference-contexts: Wellman [25] describes a STRIPS assumption for planning under uncertainty, while Hanks and McDermott <ref> [11] </ref> introduce probabilistic state-space operators. Kushmerick et al. [16] propose a method for open-loop probabilistic planning using PSOs in nonobservable domains, while Draper et al. [9] address closed-loop probabilistic planning in partially-observable settings (see also [23, 17] for the classical definition of POMDPs).
Reference: [12] <author> R. A. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1960. </year>
Reference-contexts: The principle of optimality can be applied to infinite-horizon problems as well. These include expected average-per-stage reward problems and classic expected total discounted reward problems <ref> [1, 12] </ref>, where the value function is the discounted sum of rewards incurred at every stage of execution: V (h) = t=0 where fl is a fixed discount rate (0 &lt; fl &lt; 1). Optimal policies for these problems, assuming FOMDPs, are stationary, therefore requiring only O (jSj) space.
Reference: [13] <author> R. A. Howard and J. E. Matheson. </author> <title> Influence diagrams. </title> <editor> In R. A. Howard, J. E. Matheson, editors, </editor> <title> The Principles and Applications of Decision Analysis. Strategic Decisions Group, </title> <address> Menlo Park, </address> <year> 1984. </year>
Reference-contexts: introduction of a new multivalued parent variable; and savings are possible when actions share similar effects on given variables. It is common to make explicit the actions (and value, as we describe below) using a generalization of Bayes nets called influence diagrams <ref> [13] </ref>.
Reference: [14] <author> K. Kanazawa and T. Dean. </author> <title> A model for projection and action. </title> <booktitle> IJCAI-89, </booktitle> <address> pp.985-990. Detroit, </address> <year> 1989. </year>
Reference-contexts: Dean and Wellman [8] present the view of planning problems in terms of expected value over histories, while Dean et al. [6] propose MDPs as a model for planning in stochastic domains. Dean and Kanazawa <ref> [7, 14] </ref> introduce the notion of 2TBNs and probabilistic action networks based on influence diagrams. Luenberger [18] provides a definition and discussion of separable value functions. Tatman and Shachter [24] show how separable value functions are incorporated in influence diagrams and their relation to dynamic programming.
Reference: [15] <author> C. A. Knoblock. </author> <title> Generating Abstraction Hierarchies: An Automated Approach to Reducing Search in Planning. </title> <publisher> Kluwer, </publisher> <year> 1993. </year>
Reference-contexts: Kushmerick et al. [16] propose a method for open-loop probabilistic planning using PSOs in nonobservable domains, while Draper et al. [9] address closed-loop probabilistic planning in partially-observable settings (see also [23, 17] for the classical definition of POMDPs). Sacerdoti [20] uses abstraction for planning in classical settings, while Knoblock <ref> [15] </ref> addresses the question of generating abstractions using STRIPS action descriptions.
Reference: [16] <author> N. Kushmerick, S. Hanks, and D. Weld. </author> <title> An algorithm for probabilistic planning. </title> <booktitle> AAAI-94. </booktitle> <address> Seattle, </address> <year> 1994. </year>
Reference-contexts: Partially observable problems are captured by highlighting observable variables or using information arcs into decision nodes. 11 Though most classical planning problems have been cast in deterministic terms, the Buridan and C-Buridan planners <ref> [16, 9] </ref> reformulate the problem in probabilistically: given a distribution over states, a set of actions, a goal expression, and a probability threshold t , generate a sequence of actions that when executed will leave the system in a goal state with probability at least t . <p> Wellman [25] describes a STRIPS assumption for planning under uncertainty, while Hanks and McDermott [11] introduce probabilistic state-space operators. Kushmerick et al. <ref> [16] </ref> propose a method for open-loop probabilistic planning using PSOs in nonobservable domains, while Draper et al. [9] address closed-loop probabilistic planning in partially-observable settings (see also [23, 17] for the classical definition of POMDPs).
Reference: [17] <author> W. S. Lovejoy. </author> <title> A survey of algorithmic methods for partially observed markov decision processes. </title> <journal> Annals of Op. Res., </journal> <volume> 28 </volume> <pages> 47-66, </pages> <year> 1991. </year>
Reference-contexts: Using DP to solve POMDPs (with an additive discounted value function) is currently practical only for very small problems <ref> [17] </ref>. Although DP techniques were developed using flat state-space and action rep resentations, the same basic technique can be applied to factored representations, 10 exploiting problem structure. Figure 4 illustrates a general influence diagram (in contrast to the 2TBN structures in Figure 3). <p> Wellman [25] describes a STRIPS assumption for planning under uncertainty, while Hanks and McDermott [11] introduce probabilistic state-space operators. Kushmerick et al. [16] propose a method for open-loop probabilistic planning using PSOs in nonobservable domains, while Draper et al. [9] address closed-loop probabilistic planning in partially-observable settings (see also <ref> [23, 17] </ref> for the classical definition of POMDPs). Sacerdoti [20] uses abstraction for planning in classical settings, while Knoblock [15] addresses the question of generating abstractions using STRIPS action descriptions.
Reference: [18] <author> D. G. Luenberger. </author> <title> Introduction to Linear and Nonlinear Programming. </title> <publisher> Addison-Wesley, </publisher> <year> 1973. </year>
Reference-contexts: Dean and Kanazawa [7, 14] introduce the notion of 2TBNs and probabilistic action networks based on influence diagrams. Luenberger <ref> [18] </ref> provides a definition and discussion of separable value functions. Tatman and Shachter [24] show how separable value functions are incorporated in influence diagrams and their relation to dynamic programming. Boutilier et al. [5] describe similar ideas using 2TBNs with decision trees for infinite horizon problems and compact policy representations.
Reference: [19] <author> M. L. Puterman. </author> <title> Markov Decision Processes. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: Space limitations prevent the comprehensive discussion of related work that is warranted; but we briefly mention some of the especially relevant research. Bertsekas [3] and Puterman <ref> [19] </ref> provide excellent and extensive coverage of Markov decision processes and dynamic programming. Dean and Wellman [8] present the view of planning problems in terms of expected value over histories, while Dean et al. [6] propose MDPs as a model for planning in stochastic domains.
Reference: [20] <author> E. D. Sacerdoti. </author> <title> Planning in a hierarchy of abstraction spaces. </title> <booktitle> Art. Intel., </booktitle> <volume> 5 </volume> <pages> 115-135, </pages> <year> 1974. </year>
Reference-contexts: Once again the structured representation of actions and value functions often allow one to determine the relevance of variables and action prior to planning. In classical hierarchical planning <ref> [20] </ref>, state-space operators suggest the criticality of various propositions, allowing one to plan for the most important propositions. In MDPs, 2TBNs with value nodes or PSOs can be used to identify the relative contribution variables make to overall value [4]. <p> By ignoring variables, we generally cannot guarantee the discovery of an optimal (or even satisficing) policy, unless these variables are strictly irrelevant. In the classical setting, an abstract plan is used to guide the search for a concrete plan <ref> [20] </ref> with this guarantee. In general MDPs, nonoptimal plans will have some value; by identifying and eliminating variables with relatively little contribution to value, sat-isficing policies are thus feasible [4]. <p> Kushmerick et al. [16] propose a method for open-loop probabilistic planning using PSOs in nonobservable domains, while Draper et al. [9] address closed-loop probabilistic planning in partially-observable settings (see also [23, 17] for the classical definition of POMDPs). Sacerdoti <ref> [20] </ref> uses abstraction for planning in classical settings, while Knoblock [15] addresses the question of generating abstractions using STRIPS action descriptions.
Reference: [21] <author> L. J. Savage. </author> <title> The Foundations of Statistics. </title> <publisher> Dover, </publisher> <year> 1972. </year>
Reference-contexts: Shachter and Peot [22] describe how value nodes can be replaced by chance nodes 13 See [2] for adaptive aggregation in flat states spaces. 13 to solve influence diagrams using probabilistic inference; see Savage <ref> [21] </ref> on the foundational relationship between utilities and probabilities. Wellman [25] describes a STRIPS assumption for planning under uncertainty, while Hanks and McDermott [11] introduce probabilistic state-space operators.
Reference: [22] <author> R. Shachter and M. Peot. </author> <title> Decision making using probabilistic inference models. </title> <publisher> UAI-92, Stanford, </publisher> <year> 1992. </year>
Reference-contexts: Tatman and Shachter [24] show how separable value functions are incorporated in influence diagrams and their relation to dynamic programming. Boutilier et al. [5] describe similar ideas using 2TBNs with decision trees for infinite horizon problems and compact policy representations. Shachter and Peot <ref> [22] </ref> describe how value nodes can be replaced by chance nodes 13 See [2] for adaptive aggregation in flat states spaces. 13 to solve influence diagrams using probabilistic inference; see Savage [21] on the foundational relationship between utilities and probabilities.
Reference: [23] <author> R. D. Smallwood and E. J. Sondik. </author> <title> The optimal control of partially observable markov processes over a finite horizon. Op. </title> <journal> Res., </journal> <volume> 21 </volume> <pages> 1071-1088, </pages> <year> 1973. </year>
Reference-contexts: Optimal policies for these problems, assuming FOMDPs, are stationary, therefore requiring only O (jSj) space. DP techniques can be applied in partially-observable settings as well <ref> [23] </ref>, but the problem is that the state space becomes the set of all probability distributions over S, which can, in the worst case, grow exponentially with the number of stages in the problem. <p> Wellman [25] describes a STRIPS assumption for planning under uncertainty, while Hanks and McDermott [11] introduce probabilistic state-space operators. Kushmerick et al. [16] propose a method for open-loop probabilistic planning using PSOs in nonobservable domains, while Draper et al. [9] address closed-loop probabilistic planning in partially-observable settings (see also <ref> [23, 17] </ref> for the classical definition of POMDPs). Sacerdoti [20] uses abstraction for planning in classical settings, while Knoblock [15] addresses the question of generating abstractions using STRIPS action descriptions.
Reference: [24] <author> J. A. Tatman and R. D. Shachter. </author> <title> Dynamic programming and influence diagrams. </title> <journal> IEEE Trans. on Sys., Man, and Cyber., </journal> <volume> 20(2) </volume> <pages> 365-379, </pages> <year> 1990. </year>
Reference-contexts: This influence diagram represents a finite-horizon, fully observable 10 problem with a separable value function the horizon is inherently fixed by the number of stages in the graph. DP techniques can be employed to solve problems of this sort <ref> [24] </ref>: while the same basic principle of optimality is used, computational leverage can be gained by algorithms that use the structured representation to identify relevant or irrelevant variables at various stages of the process, given decisions at later stages [24, 5]. <p> DP techniques can be employed to solve problems of this sort [24]: while the same basic principle of optimality is used, computational leverage can be gained by algorithms that use the structured representation to identify relevant or irrelevant variables at various stages of the process, given decisions at later stages <ref> [24, 5] </ref>. <p> DP over a finite horizon is undirected: since all (reachable) states have potential impact, all must be considered when constructing a policy. DP is simply an efficient way to consider all states; but even if structured system dynamics reduce the effective state space <ref> [24, 5] </ref>, all groups of states must be visited. In a goal-based setting, no value is accrued for states visited in the process of achieving the goal nor for anything that happens after the goal is achieved. 12 An optimal plan must reach a goal state. <p> Dean and Kanazawa [7, 14] introduce the notion of 2TBNs and probabilistic action networks based on influence diagrams. Luenberger [18] provides a definition and discussion of separable value functions. Tatman and Shachter <ref> [24] </ref> show how separable value functions are incorporated in influence diagrams and their relation to dynamic programming. Boutilier et al. [5] describe similar ideas using 2TBNs with decision trees for infinite horizon problems and compact policy representations.
Reference: [25] <author> M. P. Wellman. </author> <title> The STRIPS assumption for planning under uncertainty. </title> <booktitle> AAAI-90, </booktitle> <address> pp.198-203. Boston, </address> <year> 1990. </year>
Reference-contexts: Shachter and Peot [22] describe how value nodes can be replaced by chance nodes 13 See [2] for adaptive aggregation in flat states spaces. 13 to solve influence diagrams using probabilistic inference; see Savage [21] on the foundational relationship between utilities and probabilities. Wellman <ref> [25] </ref> describes a STRIPS assumption for planning under uncertainty, while Hanks and McDermott [11] introduce probabilistic state-space operators.
Reference: [26] <author> M. Williamson and S. Hanks. </author> <title> Optimal planning with a goal-directed utility model. </title> <booktitle> 2nd Intl. Conf. on AI Planning Systems, </booktitle> <year> 1994. </year> <month> 15 </month>
Reference-contexts: problem (or the computation) may be different from those at another 11 The Buridan planner solves a non-observable process, and generates straight-line plans; C-Buridan extends the action representation to allow observational actions and the plan representation allows actions to be executed contingent on the result of observations. 12 Though see <ref> [26] </ref> for a system that uses backchaining in the context of a value function that also takes into account the costs and benefits of alternative plans that achieve the goal. 12 stage.
References-found: 26

