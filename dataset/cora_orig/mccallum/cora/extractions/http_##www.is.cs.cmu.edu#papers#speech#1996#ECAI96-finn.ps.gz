URL: http://www.is.cs.cmu.edu/papers/speech/1996/ECAI96-finn.ps.gz
Refering-URL: http://www.is.cs.cmu.edu/ISL.speech.publications.html
Root-URL: 
Title: Search in a Learnable Spoken Language Parser  
Author: Finn Dag But and Alex Waibel 
Abstract: We describe and experimentally evaluate a system, FeasPar, that learns parsing spontaneous speech. The FeasPar architecture consists of neural networks and a search. The neural networks learns the parsing task, and the search improves performance by finding the most probable and consistent feature structure. This paper focuses on the search component, and shows how the search improves overall performance considerably. N-best lists of feature structure fragments and agendas are used to speed up the search. To train and run FeasPar (Feature Structure Parser), only limited handmodeled knowledge is required. FeasPar with the search component performs better than a hand modeled LR-parser in all six comparisons that are made. FeasPar is trained, tested and evaluated in the Time Scheduling Domain, and compared with the LR-parser. The handmodeling effort for FeasPar is 2 weeks. The handmodeling effort for the LR-parser was 4 months. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Finn Dag But, </author> <title> FeasPar A Feature Structure Parser Learning to Parse Spontaneous Speech, </title> <type> Ph.D. dissertation, </type> <institution> University of Karlsruhe, </institution> <month> upcoming </month> <year> 1996. </year>
Reference-contexts: Waibel true hybrid mechanism, where symbolic knowledge is included when the neural network signals so. The Chunk Relation Finder determines how a chunk relates to its parent chunk. It has one network per chunk level and chunk relation element. Further details on the baseline parser can be found in <ref> [2, 1] </ref>. 3.1 Lexicon and Neural Architecture FeasPar uses a full word form lexicon. The lexicon consists of three parts: one, a syntactic and semantic microfeature vector per word, second, lexical feature values, and three, statistical microfeatures. <p> These tables are generated automatically from the training data, and can easily be extended by hand for more generality and new words. An automatic ambiguity checker warns if similar words or phrases map to ambiguous lexical feature values. Further information on the lexicon can be found in <ref> [1] </ref>. All neural networks have one hidden layer, and are conventional feed-forward networks. The learning is done with standard back-propagation, combined with the constructive learning algorithm PCL [7], where learning starts using a small context, which is increased later in the learning process. <p> The learning is done with standard back-propagation, combined with the constructive learning algorithm PCL [7], where learning starts using a small context, which is increased later in the learning process. This causes local dependencies to be learned first. Further techniques for improving performance are described in <ref> [1] </ref>. For the neural networks, the average test set performance is 95.4 %. 4 Consistency Checking Search The complete parse depends on many neural networks. Most networks have a certain error rate; only a few networks are perfect.
Reference: [2] <author> Finn Dag But and Alex Waibel, </author> <title> `Search in a Learnable Spoken Language Parser', </title> <booktitle> in Proceedings of the 12th European Conference on Artificial Intelligence, </booktitle> <month> (August </month> <year> 1996). </year>
Reference-contexts: Waibel true hybrid mechanism, where symbolic knowledge is included when the neural network signals so. The Chunk Relation Finder determines how a chunk relates to its parent chunk. It has one network per chunk level and chunk relation element. Further details on the baseline parser can be found in <ref> [2, 1] </ref>. 3.1 Lexicon and Neural Architecture FeasPar uses a full word form lexicon. The lexicon consists of three parts: one, a syntactic and semantic microfeature vector per word, second, lexical feature values, and three, statistical microfeatures.
Reference: [3] <author> J. Dowding, J. M. Gawron, D. Appelt, J. Bear, L. Cherny, R. Moore, and D. Moran, </author> <title> `Gemini: A Natural Language System for Spoken-Language Understanding', </title> <booktitle> in Proceedings ARPA Workshop on Human Language Technology, </booktitle> <pages> pp. 43-48, </pages> <address> Princeton, New Jersey, (March 1993). </address> <publisher> Morgan Kaufmann Publisher. </publisher>
Reference-contexts: Therefore, a mixture of 'hard' and 'soft' rules (scores and penalties, probabilistic rules, and constraints) is applied. In most parsers, the core consists of hand modeled rules. With great success, these rules have been annotated with 'soft' information <ref> [3, 10, 8, 6] </ref>. In this paper, we present a parser, FeasPar, that learns to parse, instead of having hand modeled rules. The FeasPar architecture consists of neural networks and a search. The search finds the best feature structure based on the neural network outputs, and feature structure constraints.
Reference: [4] <author> G. Gazdar, E. Klein, G. K. Pullum, and I. A. Sag, </author> <title> `A theory of syntactic features', in Generalized Phrase Structure Grammar, chapter 2, </title> <publisher> Blackwell Publishing, Oxford, England and Harvard University Press, </publisher> <address> Cambridge, MA, USA, </address> <year> (1985). </year>
Reference-contexts: Second, we describe the parser architecture and how it works. Then the search algorithm is motivated and explained in detail. Finally, results and conclusion follow. 2 Building a Feature Structure Feature structures <ref> [4, 9] </ref> are used as output formalism for FeasPar. Their core syntactic properties and terminology are: 1. A feature structure is a set of none, one or several feature pairs. 2.
Reference: [5] <author> P. Geutner, B. Suhm, F. D. But, T. Kemp, L. Mayfield, A. E. McNair, I. Rogina, T. Schultz, T. Sloboda, W. Ward, M. Woszczyna, and A. Waibel, </author> <title> `Integrating Different Learning Approaches into a Multilingual Spoken Language Translation System', </title> <booktitle> in Workshop on New Approaches to Learning for Natural Language Processing, International Joint Conference on Artificial Intelligence, </booktitle> <address> Montreal, Canada, </address> <month> (August </month> <year> 1995). </year>
Reference-contexts: For clarity, the example sentences in this paper are among the simpler in the training set. The parser produces feature structures, holding semantic information. These feature structures are used as interlingua in the speech-to-speech translation system JANUS <ref> [5] </ref>. Within our research team, the design of the interlingua ILT was determined by the needs of unification based parser and generator writers. Consequently, the interlingua design was not tuned towards connectionist systems.
Reference: [6] <author> Sunil Issar and Wayne Ward, </author> <title> `CMU's robust spoken language understanding system', </title> <booktitle> in Proceedings of Eurospeech, </booktitle> <year> (1993). </year>
Reference-contexts: Therefore, a mixture of 'hard' and 'soft' rules (scores and penalties, probabilistic rules, and constraints) is applied. In most parsers, the core consists of hand modeled rules. With great success, these rules have been annotated with 'soft' information <ref> [3, 10, 8, 6] </ref>. In this paper, we present a parser, FeasPar, that learns to parse, instead of having hand modeled rules. The FeasPar architecture consists of neural networks and a search. The search finds the best feature structure based on the neural network outputs, and feature structure constraints.
Reference: [7] <author> Ajay N. Jain, </author> <title> A Connectionist Learning Architecture for Parsing Spoken Language, </title> <type> Ph.D. dissertation, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> Dec </month> <year> 1991. </year>
Reference-contexts: Further information on the lexicon can be found in [1]. All neural networks have one hidden layer, and are conventional feed-forward networks. The learning is done with standard back-propagation, combined with the constructive learning algorithm PCL <ref> [7] </ref>, where learning starts using a small context, which is increased later in the learning process. This causes local dependencies to be learned first. Further techniques for improving performance are described in [1].
Reference: [8] <author> A. Lavie and M. Tomita, </author> <title> `GLR* An Efficient Noise-skipping Parsing Algorithm for Context-free Grammars', </title> <booktitle> in Proceedings of Third International Workshop on Parsing Technologies, </booktitle> <pages> pp. 123-134, </pages> <year> (1993). </year>
Reference-contexts: Therefore, a mixture of 'hard' and 'soft' rules (scores and penalties, probabilistic rules, and constraints) is applied. In most parsers, the core consists of hand modeled rules. With great success, these rules have been annotated with 'soft' information <ref> [3, 10, 8, 6] </ref>. In this paper, we present a parser, FeasPar, that learns to parse, instead of having hand modeled rules. The FeasPar architecture consists of neural networks and a search. The search finds the best feature structure based on the neural network outputs, and feature structure constraints. <p> The handmodel-ing effort for the LR-parser was 4 months. The evaluation environment is the JANUS speech translation system for the Spontaneous Scheduling Task. The system have one parser and one generator per language. All parsers and generators are written using CMU's GLR/GLR* system <ref> [8] </ref>. They all share the same interlingua, ILT, which is a special case of LFG or feature structures. All Performance measures are run with transcribed (T) sentences and with speech (S) sentences containing speech recognition errors.
Reference: [9] <author> C. Pollard and I. Sag, </author> <title> `Formal Foundations', in An Information-Based Syntax and Semantics, </title> <booktitle> chapter 2, CSLI Lecture Notes No.13, </booktitle> <year> (1987). </year>
Reference-contexts: Second, we describe the parser architecture and how it works. Then the search algorithm is motivated and explained in detail. Finally, results and conclusion follow. 2 Building a Feature Structure Feature structures <ref> [4, 9] </ref> are used as output formalism for FeasPar. Their core syntactic properties and terminology are: 1. A feature structure is a set of none, one or several feature pairs. 2.
Reference: [10] <author> Stephanie Seneff, `TINA: </author> <title> A Natural Language System for Spoken Language Applications', </title> <journal> Computational linguistics, </journal> <volume> 18(1), </volume> <year> (1992). </year> <title> Natural Language Processing 566 F.D. But and A. </title> <type> Waibel </type>
Reference-contexts: Therefore, a mixture of 'hard' and 'soft' rules (scores and penalties, probabilistic rules, and constraints) is applied. In most parsers, the core consists of hand modeled rules. With great success, these rules have been annotated with 'soft' information <ref> [3, 10, 8, 6] </ref>. In this paper, we present a parser, FeasPar, that learns to parse, instead of having hand modeled rules. The FeasPar architecture consists of neural networks and a search. The search finds the best feature structure based on the neural network outputs, and feature structure constraints.
References-found: 10

