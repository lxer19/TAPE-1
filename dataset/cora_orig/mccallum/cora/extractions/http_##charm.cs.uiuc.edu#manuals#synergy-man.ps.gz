URL: http://charm.cs.uiuc.edu/manuals/synergy-man.ps.gz
Refering-URL: http://charm.cs.uiuc.edu/manuals/
Root-URL: http://www.cs.uiuc.edu
Title: Building Scalable Parallel Processors Using Networked Computers A Tutorial For Synergy V2.0  
Author: Yuan Shi 
Affiliation: Temple University SYNERGY  
Date: January 1994 @1994  
Abstract-found: 0
Intro-found: 1
Reference: 1. <author> Express: </author> <title> Parasoft Corporation, "Express C User's Guide," </title> <year> 1988, 1989, 1990. </year>
Reference-contexts: size G for each wave */ R = R - G * P; /* Pass along the remainder to next wave */ if (G&gt;0) - for (i=0; i&lt;P; i++) - /* Put out P tuples per wave */ ituple [0] = (double) G; /* ith wave grain size */ ituple <ref> [1] </ref> = (double) ix; /* Starting colume index */ 19 ituple [2] = xmin; /* complex domain def. */ ituple [3] = xmax; ituple [4] = ymin; ituple [5] = ymax; ituple [6] = (double) XRES; /* Screen resolution def. */ ituple [7] = (double) YRES; ituple [8] = (double) iterat; <p> [7] = (double) YRES; ituple [8] = (double) iterat; /* Loop limit def. */ sprintf ( tpname, "i%d"0",ix); /* Define a tuple name */ status = cnf_tsput (tsd,tpname,ituple,tplength); ix +=G; /* Increment colum index by G */ - - ituple [0] = (double) R; /* Complete the leftover */ ituple <ref> [1] </ref> = (double) ix; ituple [2] = xmin; ituple [3] = xmax; ituple [4] = ymin; ituple [5] = ymax; ituple [6] = (double) XRES; ituple [7] = (double) YRES; ituple [8] = (double) iterat; sprintf ( tpname, "i%d"0",ix); status = cnf_tsput (tsd,tpname,ituple,tplength); /* Send the leftover */ if (R&gt;0) - <p> */ ituple [0] = (float) 0; sprintf ( tpname, "i%d"0", XRES+1); status = cnf_tsput (tsd,tpname,ituple,tplength); - /* Now receive the results */ received = 0; while ( received &lt; XRES ) - strcpy ( tpname, "*" ); len = cnf_tsget ( res, tpname, otuple, 0 ); ix = (int) otuple <ref> [1] </ref>; /* Unpacking */ G = (int) otuple [0]; iy = 2; received ++; for ( j= 0; j&lt;YRES; j++ ) - ip = (int)otuple [iy]; if (DISPLAY) - /* Paint the screen */ XSetForeground ( dpy, gc, ip ); XDrawPoint ( dpy, win, gc, ix, j ); - - - <p> /* max line length of text data file */ #define NULL 0 main () - int fin, sorter [N], i, j, k, buf [N]; char *nread, linbuf [LINELEN]; fin = cnf_open ("infile","r" ); /* Open a file object */ sorter [0] = cnf_open ("out1",0); /* Open pipe objects */ sorter <ref> [1] </ref> = cnf_open ("out2",0); /* Distribute file to the sorters */ i = 0; nread = cnf_fgets (fin, linbuf, LINELEN); while (nread != NULL) - j = atoi (linbuf); k = cnf_write (sorter [i++], &j, sizeof (int)); nread = cnf_fgets (fin, linbuf, LINELEN); if (i == N) i = 0; cnf_term <p> 20 /* max line length of text data file */ main () - int nread, fout, merger [N], i, j, k, buf [N], l; char linbuf [LINELEN]; 24 fout = cnf_open ("out","w"); /* Open the output file object */ merger [0] = cnf_open ("in1",0); /* Open input pipes */ merger <ref> [1] </ref> = cnf_open ("in2",0); cnf_read (merger [0], &buf [0], sizeof (int)); /* Read input pipes */ cnf_read (merger [1], &buf [1], sizeof (int)); while ((buf [0] != MAX) || (buf [1] != MAX)) ) /* Mergesort inputs */ - sprintf (linbuf, "%7d"n", buf [i]); l = cnf_fputs (fout, linbuf); l = <p> i, j, k, buf [N], l; char linbuf [LINELEN]; 24 fout = cnf_open ("out","w"); /* Open the output file object */ merger [0] = cnf_open ("in1",0); /* Open input pipes */ merger <ref> [1] </ref> = cnf_open ("in2",0); cnf_read (merger [0], &buf [0], sizeof (int)); /* Read input pipes */ cnf_read (merger [1], &buf [1], sizeof (int)); while ((buf [0] != MAX) || (buf [1] != MAX)) ) /* Mergesort inputs */ - sprintf (linbuf, "%7d"n", buf [i]); l = cnf_fputs (fout, linbuf); l = cnf_read (merger [i], &j , sizeof (int)); buf [i] = j; if (l==0) buf [i] = MAX; /* <p> k, buf [N], l; char linbuf [LINELEN]; 24 fout = cnf_open ("out","w"); /* Open the output file object */ merger [0] = cnf_open ("in1",0); /* Open input pipes */ merger <ref> [1] </ref> = cnf_open ("in2",0); cnf_read (merger [0], &buf [0], sizeof (int)); /* Read input pipes */ cnf_read (merger [1], &buf [1], sizeof (int)); while ((buf [0] != MAX) || (buf [1] != MAX)) ) /* Mergesort inputs */ - sprintf (linbuf, "%7d"n", buf [i]); l = cnf_fputs (fout, linbuf); l = cnf_read (merger [i], &j , sizeof (int)); buf [i] = j; if (l==0) buf [i] = MAX; /* End of <p> cnf_open ("out","w"); /* Open the output file object */ merger [0] = cnf_open ("in1",0); /* Open input pipes */ merger <ref> [1] </ref> = cnf_open ("in2",0); cnf_read (merger [0], &buf [0], sizeof (int)); /* Read input pipes */ cnf_read (merger [1], &buf [1], sizeof (int)); while ((buf [0] != MAX) || (buf [1] != MAX)) ) /* Mergesort inputs */ - sprintf (linbuf, "%7d"n", buf [i]); l = cnf_fputs (fout, linbuf); l = cnf_read (merger [i], &j , sizeof (int)); buf [i] = j; if (l==0) buf [i] = MAX; /* End of pipe */ - cnf_term (); - 5.3.3 File Object Programming
Reference: 2. <author> Linda: D.Gelernter, </author> <title> "Generative Communication in Linda," </title> <journal> ACM TOPLAS, </journal> <volume> 7.1, </volume> <month> January </month> <year> 1985. </year>
Reference-contexts: The Synergy architecture was invented to tackle the following problems: Individual distributed computers halt more often than centrally managed computers. For many applications it is desirable to have host-independent programs (in binary) that can be quickly restarted (without recompilation) when a subset of computers are down. "Tuple space programming" <ref> [2] </ref> is simpler (and more powerful) than message passing programming. Load balancing is natural to the tuple space concept. However, the implementation overhead of tuple space across multiple networked computers is inherently higher than message passing, even if using compiler optimization [2,3]. <p> However, complex synchronization schemes, such as load balancing, are difficult to develop using the rigid "spaghetti" controls. 1 Linda is a tuple space parallel programming system lead by Dr. David Gelenter, Yale University. It's commercial version is distributed by the Scientific Computing Associates, New Heaven, NH. (See <ref> [2] </ref> for details) 2 PVM is a message-passing parallel programming system by Oak Ridge National Laboratory, Unversity of Tennassee and Emory University. (See [4] for details) 3 Express is a commercial message-passing parallel programming system by ParaSoft, CA. (See [4] for details) 3 Parallel programming can be greatly simplified if there <p> This pattern is used in all subsequent operations throughout the caller's lifetime. 5.3.1 Tuple Space Object Programming Although tuple space objects can be used for many purposes, this section only illustrates its use for coarse grain SIMD programming. The Synergy tuple space object differs from Linda's <ref> [2] </ref> (and Piranha's [3]) tuple space in the following: 14 Unique tuples names. Synergy tuple space objects hold only unique tuples. Only one copy of redundant (by name) tuples can exist in an object. The original sequential program already provides a nonambiguous name-base. FIFO ordering. <p> * P; /* Pass along the remainder to next wave */ if (G&gt;0) - for (i=0; i&lt;P; i++) - /* Put out P tuples per wave */ ituple [0] = (double) G; /* ith wave grain size */ ituple [1] = (double) ix; /* Starting colume index */ 19 ituple <ref> [2] </ref> = xmin; /* complex domain def. */ ituple [3] = xmax; ituple [4] = ymin; ituple [5] = ymax; ituple [6] = (double) XRES; /* Screen resolution def. */ ituple [7] = (double) YRES; ituple [8] = (double) iterat; /* Loop limit def. */ sprintf ( tpname, "i%d"0",ix); /* Define <p> [8] = (double) iterat; /* Loop limit def. */ sprintf ( tpname, "i%d"0",ix); /* Define a tuple name */ status = cnf_tsput (tsd,tpname,ituple,tplength); ix +=G; /* Increment colum index by G */ - - ituple [0] = (double) R; /* Complete the leftover */ ituple [1] = (double) ix; ituple <ref> [2] </ref> = xmin; ituple [3] = xmax; ituple [4] = ymin; ituple [5] = ymax; ituple [6] = (double) XRES; ituple [7] = (double) YRES; ituple [8] = (double) iterat; sprintf ( tpname, "i%d"0",ix); status = cnf_tsput (tsd,tpname,ituple,tplength); /* Send the leftover */ if (R&gt;0) - /* Insert the sentinel, if
Reference: 3. <author> Piranha: D. Kaminsky, </author> <title> "Adaptive Parallelism with Piranha," </title> <type> Ph.D Dissertation, </type> <institution> CIS Department, Yale University, </institution> <year> 1994. </year>
Reference-contexts: This pattern is used in all subsequent operations throughout the caller's lifetime. 5.3.1 Tuple Space Object Programming Although tuple space objects can be used for many purposes, this section only illustrates its use for coarse grain SIMD programming. The Synergy tuple space object differs from Linda's [2] (and Piranha's <ref> [3] </ref>) tuple space in the following: 14 Unique tuples names. Synergy tuple space objects hold only unique tuples. Only one copy of redundant (by name) tuples can exist in an object. The original sequential program already provides a nonambiguous name-base. FIFO ordering. <p> wave */ if (G&gt;0) - for (i=0; i&lt;P; i++) - /* Put out P tuples per wave */ ituple [0] = (double) G; /* ith wave grain size */ ituple [1] = (double) ix; /* Starting colume index */ 19 ituple [2] = xmin; /* complex domain def. */ ituple <ref> [3] </ref> = xmax; ituple [4] = ymin; ituple [5] = ymax; ituple [6] = (double) XRES; /* Screen resolution def. */ ituple [7] = (double) YRES; ituple [8] = (double) iterat; /* Loop limit def. */ sprintf ( tpname, "i%d"0",ix); /* Define a tuple name */ status = cnf_tsput (tsd,tpname,ituple,tplength); ix <p> /* Loop limit def. */ sprintf ( tpname, "i%d"0",ix); /* Define a tuple name */ status = cnf_tsput (tsd,tpname,ituple,tplength); ix +=G; /* Increment colum index by G */ - - ituple [0] = (double) R; /* Complete the leftover */ ituple [1] = (double) ix; ituple [2] = xmin; ituple <ref> [3] </ref> = xmax; ituple [4] = ymin; ituple [5] = ymax; ituple [6] = (double) XRES; ituple [7] = (double) YRES; ituple [8] = (double) iterat; sprintf ( tpname, "i%d"0",ix); status = cnf_tsput (tsd,tpname,ituple,tplength); /* Send the leftover */ if (R&gt;0) - /* Insert the sentinel, if the */ /* leftover
Reference: 4. <author> PVM: V.S.Sunderam, </author> <title> "PVM: A framework for parallel distributed computing," </title> <journal> Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: David Gelenter, Yale University. It's commercial version is distributed by the Scientific Computing Associates, New Heaven, NH. (See [2] for details) 2 PVM is a message-passing parallel programming system by Oak Ridge National Laboratory, Unversity of Tennassee and Emory University. (See <ref> [4] </ref> for details) 3 Express is a commercial message-passing parallel programming system by ParaSoft, CA. (See [4] for details) 3 Parallel programming can be greatly simplified if there exists a coherent processor interconnection interface. <p> is distributed by the Scientific Computing Associates, New Heaven, NH. (See [2] for details) 2 PVM is a message-passing parallel programming system by Oak Ridge National Laboratory, Unversity of Tennassee and Emory University. (See <ref> [4] </ref> for details) 3 Express is a commercial message-passing parallel programming system by ParaSoft, CA. (See [4] for details) 3 Parallel programming can be greatly simplified if there exists a coherent processor interconnection interface. Parallel processors employing spaceswitched interconnection devices, such as hypercubes, mesh and systolic arrays, do not offer such an interface naturally. Majority practical applications do not scale well on parallel machines [13]. <p> - for (i=0; i&lt;P; i++) - /* Put out P tuples per wave */ ituple [0] = (double) G; /* ith wave grain size */ ituple [1] = (double) ix; /* Starting colume index */ 19 ituple [2] = xmin; /* complex domain def. */ ituple [3] = xmax; ituple <ref> [4] </ref> = ymin; ituple [5] = ymax; ituple [6] = (double) XRES; /* Screen resolution def. */ ituple [7] = (double) YRES; ituple [8] = (double) iterat; /* Loop limit def. */ sprintf ( tpname, "i%d"0",ix); /* Define a tuple name */ status = cnf_tsput (tsd,tpname,ituple,tplength); ix +=G; /* Increment colum <p> */ sprintf ( tpname, "i%d"0",ix); /* Define a tuple name */ status = cnf_tsput (tsd,tpname,ituple,tplength); ix +=G; /* Increment colum index by G */ - - ituple [0] = (double) R; /* Complete the leftover */ ituple [1] = (double) ix; ituple [2] = xmin; ituple [3] = xmax; ituple <ref> [4] </ref> = ymin; ituple [5] = ymax; ituple [6] = (double) XRES; ituple [7] = (double) YRES; ituple [8] = (double) iterat; sprintf ( tpname, "i%d"0",ix); status = cnf_tsput (tsd,tpname,ituple,tplength); /* Send the leftover */ if (R&gt;0) - /* Insert the sentinel, if the */ /* leftover is nonzero. */ ituple
Reference: 5. <author> Synergy: K.Blathras,J.Dougherty and Y.Shi, </author> <title> "The Synergy System - Tools for Computing the Future," Cluster Workshop'92, </title> <institution> Florida State University, </institution> <note> retrievable from ftp.scri.fsu.edu (144.174.128.34) under the name "cluster-workshop92/ Synergy.ps.Z" </note>
Reference-contexts: i++) - /* Put out P tuples per wave */ ituple [0] = (double) G; /* ith wave grain size */ ituple [1] = (double) ix; /* Starting colume index */ 19 ituple [2] = xmin; /* complex domain def. */ ituple [3] = xmax; ituple [4] = ymin; ituple <ref> [5] </ref> = ymax; ituple [6] = (double) XRES; /* Screen resolution def. */ ituple [7] = (double) YRES; ituple [8] = (double) iterat; /* Loop limit def. */ sprintf ( tpname, "i%d"0",ix); /* Define a tuple name */ status = cnf_tsput (tsd,tpname,ituple,tplength); ix +=G; /* Increment colum index by G */ <p> "i%d"0",ix); /* Define a tuple name */ status = cnf_tsput (tsd,tpname,ituple,tplength); ix +=G; /* Increment colum index by G */ - - ituple [0] = (double) R; /* Complete the leftover */ ituple [1] = (double) ix; ituple [2] = xmin; ituple [3] = xmax; ituple [4] = ymin; ituple <ref> [5] </ref> = ymax; ituple [6] = (double) XRES; ituple [7] = (double) YRES; ituple [8] = (double) iterat; sprintf ( tpname, "i%d"0",ix); status = cnf_tsput (tsd,tpname,ituple,tplength); /* Send the leftover */ if (R&gt;0) - /* Insert the sentinel, if the */ /* leftover is nonzero. */ ituple [0] = (float) 0;
Reference: 6. <author> Y. Shi, K. Blathras, J. Wang, G. Valdez, </author> <title> "A Heuristic Partition Method For Creating Coarse Grain Parallel Programs - A Report on Parallelizing a Coupled Electron-Photon Simulator," Cluster Workshop'93, </title> <institution> Supercomputer Computation Research Institute, Florida State University, </institution> <note> December 1993 (retrievable from ftp.scri.fsu.edu (144.174.128.34) under the name "cluster-workshop93/ partition.ps.Z") </note>
Reference-contexts: out P tuples per wave */ ituple [0] = (double) G; /* ith wave grain size */ ituple [1] = (double) ix; /* Starting colume index */ 19 ituple [2] = xmin; /* complex domain def. */ ituple [3] = xmax; ituple [4] = ymin; ituple [5] = ymax; ituple <ref> [6] </ref> = (double) XRES; /* Screen resolution def. */ ituple [7] = (double) YRES; ituple [8] = (double) iterat; /* Loop limit def. */ sprintf ( tpname, "i%d"0",ix); /* Define a tuple name */ status = cnf_tsput (tsd,tpname,ituple,tplength); ix +=G; /* Increment colum index by G */ - - ituple [0] <p> tuple name */ status = cnf_tsput (tsd,tpname,ituple,tplength); ix +=G; /* Increment colum index by G */ - - ituple [0] = (double) R; /* Complete the leftover */ ituple [1] = (double) ix; ituple [2] = xmin; ituple [3] = xmax; ituple [4] = ymin; ituple [5] = ymax; ituple <ref> [6] </ref> = (double) XRES; ituple [7] = (double) YRES; ituple [8] = (double) iterat; sprintf ( tpname, "i%d"0",ix); status = cnf_tsput (tsd,tpname,ituple,tplength); /* Send the leftover */ if (R&gt;0) - /* Insert the sentinel, if the */ /* leftover is nonzero. */ ituple [0] = (float) 0; sprintf ( tpname, "i%d"0",
Reference: 7. <author> C. Polychronopoulos and D. Kuck, </author> <title> "Guided Self-Scheduling: A Practical Scheduling Scheme for Parallel Computers," </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36, </volume> <month> 12 (December </month> <year> 1987), </year> <pages> 1425-1439 </pages>
Reference-contexts: For example, if N=1000, P i =1 (all processors of equal power), P=10, the optimal tuple size is 100. Since the measures of P i 's are never accurate, it is difficult to approach the optimal using this algorithm without many repetitive experiments. Guided Self-Scheduling <ref> [7] </ref>. <p> G; /* ith wave grain size */ ituple [1] = (double) ix; /* Starting colume index */ 19 ituple [2] = xmin; /* complex domain def. */ ituple [3] = xmax; ituple [4] = ymin; ituple [5] = ymax; ituple [6] = (double) XRES; /* Screen resolution def. */ ituple <ref> [7] </ref> = (double) YRES; ituple [8] = (double) iterat; /* Loop limit def. */ sprintf ( tpname, "i%d"0",ix); /* Define a tuple name */ status = cnf_tsput (tsd,tpname,ituple,tplength); ix +=G; /* Increment colum index by G */ - - ituple [0] = (double) R; /* Complete the leftover */ ituple [1] <p> cnf_tsput (tsd,tpname,ituple,tplength); ix +=G; /* Increment colum index by G */ - - ituple [0] = (double) R; /* Complete the leftover */ ituple [1] = (double) ix; ituple [2] = xmin; ituple [3] = xmax; ituple [4] = ymin; ituple [5] = ymax; ituple [6] = (double) XRES; ituple <ref> [7] </ref> = (double) YRES; ituple [8] = (double) iterat; sprintf ( tpname, "i%d"0",ix); status = cnf_tsput (tsd,tpname,ituple,tplength); /* Send the leftover */ if (R&gt;0) - /* Insert the sentinel, if the */ /* leftover is nonzero. */ ituple [0] = (float) 0; sprintf ( tpname, "i%d"0", XRES+1); status = cnf_tsput (tsd,tpname,ituple,tplength);
Reference: 8. <author> S.F.Hummel, E. Schonberg and L. E. </author> <title> Flynn ., "Factoring - A Method for Scheduling Parallel Loops," </title> <journal> CACM, </journal> <volume> Vol., 35, </volume> <month> No.8 (August </month> <year> 1992), </year> <pages> 90-101 </pages>
Reference-contexts: Until Ri = 1. 17 For example, if N=1000, P=2, we have tuples of the following sizes: 500,250,125,63,32,16,8,4,2,1 GSS puts too much work in the beginning. It performs poorly when employing processors with large capacity variances or computing problems with large computing density variances [8,9]. Factoring <ref> [8] </ref>. Assuming there are P parallel workers, a threshold t&gt;0 and a real value (0&lt;f&lt;=1), the factored tuple sizes are calculated as follows: R0 = N. Ri+1 = Ri - (P*Gi) until Ri &lt; t. <p> size */ ituple [1] = (double) ix; /* Starting colume index */ 19 ituple [2] = xmin; /* complex domain def. */ ituple [3] = xmax; ituple [4] = ymin; ituple [5] = ymax; ituple [6] = (double) XRES; /* Screen resolution def. */ ituple [7] = (double) YRES; ituple <ref> [8] </ref> = (double) iterat; /* Loop limit def. */ sprintf ( tpname, "i%d"0",ix); /* Define a tuple name */ status = cnf_tsput (tsd,tpname,ituple,tplength); ix +=G; /* Increment colum index by G */ - - ituple [0] = (double) R; /* Complete the leftover */ ituple [1] = (double) ix; ituple [2] <p> Increment colum index by G */ - - ituple [0] = (double) R; /* Complete the leftover */ ituple [1] = (double) ix; ituple [2] = xmin; ituple [3] = xmax; ituple [4] = ymin; ituple [5] = ymax; ituple [6] = (double) XRES; ituple [7] = (double) YRES; ituple <ref> [8] </ref> = (double) iterat; sprintf ( tpname, "i%d"0",ix); status = cnf_tsput (tsd,tpname,ituple,tplength); /* Send the leftover */ if (R&gt;0) - /* Insert the sentinel, if the */ /* leftover is nonzero. */ ituple [0] = (float) 0; sprintf ( tpname, "i%d"0", XRES+1); status = cnf_tsput (tsd,tpname,ituple,tplength); - /* Now receive the
Reference: 9. <author> J. Dougherty, </author> <title> "Monte Carlo integration in a heterogeneous distributed environment," </title> <booktitle> Proceedings of the 26th Annual Hawaiian International Conference on System Sciences, </booktitle> <address> Maui, HI, </address> <month> January </month> <year> 1993. </year>
Reference: 10. <author> Y. Shi and K. Blathras, </author> <title> "The SAG Distributed Programming Model and Its Application to Scientific Computing," </title> <booktitle> Proceedings of the SIAM 5th International Conference on Scientific Parallel Programming, </booktitle> <month> March </month> <year> 1991 </year>
Reference-contexts: Considering the communication overhead, however, finding the optimal granularity becomes a nonlinear optimization problem <ref> [10] </ref>. We looked for heuristics. Here we present the results of our studies with the following algorithms: Fixed chunking. Each tuple (work batch) contains the same work size measured by data items. <p> showed that the optimal performance, assuming all data items carry the same work load, can be achieved if the work size is P i P , where N is the total number of data items to be computed, P i is the estimated processing power measured in relative index value <ref> [10] </ref> and P is the number of parallel workers. For example, if N=1000, P i =1 (all processors of equal power), P=10, the optimal tuple size is 100.
Reference: 11. <author> Y.Shi, </author> <title> "System for Generating Efficient Client/Server Operating Environments using Multiple Operating Systems," </title> <type> U.S. Patent Application, </type> <note> January 1993 cip revision </note> . 
Reference-contexts: 1. Introduction The Synergy system was designed <ref> [11, U.S. Patent pending] </ref> to provide an efficient and stable platform for distributed and parallel processing. This goal includes the development, execution and maintenance of parallel applications. Clearly it also implies the use of multiple computers connected via some interconnection network (s).
Reference: 12. <author> Y.Shi, </author> <title> "CIS750 - Distributed and Parallel Systems Seminar Case Studies," </title> <type> Technical Report, </type> <institution> Center for Advanced Computing and Communications, Temple University, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: example, if N=1000, P=2, f=0.5, t=1, we have the following tuple sizes: 250,250,125,125,63,63,32,32,16,16,8,8,4,4,2,2,1,1 Since different program input can significantly change the work distribution using the same tuple size calculation algorithm, the presence of a "knob" (0&lt;f&lt;=1), gives the programs much latitude to adapt to both input and resource status changes <ref> [12] </ref>. We concluded that factoring is the most adaptable for fluctuating environments. Synergy V2.0 supports the specification of factor (f) and threshold (t) values at runtime. The number of parallel workers is calculated automatically. Examples Coarse grain SIMD parallel processing can be best illustrated using the Mandelbrot display program.
Reference: 13. <author> Y.Shi, </author> <title> "Quantifying Efficiency and Speed Up for Parallel Programs," </title> <type> Private communication, </type> <month> January </month> <year> 1994. </year> <month> 39 </month>
Reference-contexts: Parallel processors employing spaceswitched interconnection devices, such as hypercubes, mesh and systolic arrays, do not offer such an interface naturally. Majority practical applications do not scale well on parallel machines <ref> [13] </ref>. For most applications, computing efficiency drops sharply beyond a small number (10s - 100s) of parallel processors. It is hard to make a massively parallel processor ( MPP ) useful at all times.
References-found: 13

