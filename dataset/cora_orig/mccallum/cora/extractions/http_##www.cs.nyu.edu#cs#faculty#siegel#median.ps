URL: http://www.cs.nyu.edu/cs/faculty/siegel/median.ps
Refering-URL: http://www.cs.nyu.edu/cs/faculty/siegel/index.html
Root-URL: http://www.cs.nyu.edu
Title: Median Bounds and their Application*  
Author: Alan Siegely 
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. Anderson, </author> <type> personal communication. </type>
Reference-contexts: The basic probing strategy, however, has yet to be analyzed by elementary means. Indeed, it has been open whether a simple analysis could establish a performance bound of log 2 log 2 n + O (1) <ref> [1] </ref>, [2]. Pearl and Reingold, in their presentation of a less efficient (but easily analyzed) search strategy, speculate that the difficulty of the analysis is responsible for the absence of this remarkable search procedure from algorithms texts [12].
Reference: [2] <author> R. Cole, </author> <type> personal commulication. </type>
Reference-contexts: Let ^ F (x) F (x)+F (2x) 2 . By supposition, ^ F is moustache-shaped on <ref> [0; 2] </ref>: It is symmetric by construction. Since f (0) &lt; f (2), it is decreasing at the origin from the value ^ F (0) = F (2) 2 . <p> Finally, 2 0 ) prevents f (x) from equaling f (2 x) at four points in <ref> [0; 2] </ref>, whence 2) must hold. <p> The basic probing strategy, however, has yet to be analyzed by elementary means. Indeed, it has been open whether a simple analysis could establish a performance bound of log 2 log 2 n + O (1) [1], <ref> [2] </ref>. Pearl and Reingold, in their presentation of a less efficient (but easily analyzed) search strategy, speculate that the difficulty of the analysis is responsible for the absence of this remarkable search procedure from algorithms texts [12]. <p> Suppose that a1) E [X] = &lt; 1. a2) g (t) is maximized at . a3) g satisfies the (simplified) right skew condition g (t) g (2 t), for t 2 [0; ], and g has only one local extremum in <ref> [0; 2] </ref>. Furthermore, suppose that b1) f (x) f (2 x), has at most one zero for t in the open interval (0; ). b2) f (0) &lt; f (2). <p> Condition a4 00 is quite mild, has many alternative formulations, and is unlikely to be an impediment in most applications. Proof of Theorem 2.2. (Sketch) First, create a unimodal symmetrization of g on <ref> [0; 2] </ref> via leftward shiftings of area, which is lossy in terms of the integral. Then 6 symmetrize F into a moustache ^ F as in Theorem 2.1. Now flatten ^ F to its average on [0; 2], which is lossy for the integral as symmetrized. <p> Proof of Theorem 2.2. (Sketch) First, create a unimodal symmetrization of g on <ref> [0; 2] </ref> via leftward shiftings of area, which is lossy in terms of the integral. Then 6 symmetrize F into a moustache ^ F as in Theorem 2.1. Now flatten ^ F to its average on [0; 2], which is lossy for the integral as symmetrized. The result is at least R 1 g (x) 2 . Probability application 2: conditional Poisson distributions. The following is an illustrative application of Theorem 2.2, although the bound can be established by alternative means as explained below.
Reference: [3] <author> G. Gonnet, L. Rogers and J. George, </author> <title> An algorithmic and complexity analysis of interpolation search, </title> <journal> Acta Inf., </journal> <volume> 13, 1 (1980), </volume> <pages> pp. 39-52. </pages>
Reference-contexts: They also characterized the behavior of the difference as n ! 1 with np fixed [6]. Before presenting some additional probability applications, we give an easy algorithmic application. Interpolation Search. Interpolation Search was introduced by Peterson [13], and its performance has been extensively analyzed [18], <ref> [3] </ref>, [11]. Moreover, the underlying scheme has been extended to support more general search [17] and data access systems [9]. The basic probing strategy, however, has yet to be analyzed by elementary means. <p> For completeness, we note that the tight bound of log 2 log 2 n + fi (1) for Interpolation Search originates with Yao and Yao [18]. Gonnet, Rogers and George also analyze Interpolation Search <ref> [3] </ref>. Their study is by no means elementary, and uses extensive asymptotics.
Reference: [4] <author> W. Hoeffding, </author> <title> On the distribution of the number of successes in independent trials, </title> <journal> Ann. Math. Stat., </journal> <volume> 27 (1956), </volume> <pages> pp. </pages> <month> 713-721. </month> <title> [5] , Probability Inequalities for Sums of Bounded Random Variables, </title> <journal> J. Am. Stat. Ass., </journal> <volume> 58 (1963), </volume> <pages> pp. 13-30. </pages>
Reference-contexts: Bernoulli Trials, are maximized (and minimized) at the extremal distributions where some Bernoulli Trials are stuck at 1 (with individual probability of success equal to 1), some are stuck at zero (with probability of success equal to 0), and all other probabilities of success set to some identical constant p <ref> [4] </ref>. Jogdeo and Samuels gave a subtle but very elegant argument specific to Bernoulli Trials to attain a slightly but remarkably stronger result. <p> For example, Hoeffding established the first peakedness properties to show that for a sum of Bernoulli Trials, deviations from the mean are maximum when all Trials (apart from some stuck at one or zero) all have the same probability of success <ref> [4] </ref>. Hoeffding also showed E [f (X)] E [f (Y )], for convex f , where X is a sum of values randomly selected without replacement, and Y is the same with replacement [5].
Reference: [6] <author> K. Jogdeo and S. Samuels, </author> <title> Monotone Convergence of Binomial Probabilities and a Generalization of Ram-anujan's Equation, </title> <address> Ann.Math.Stat.,39 (1968), pp.1191-1195. </address>
Reference-contexts: Probability application 1a: Bernoulli Trials. We can now give a generic proof of the following result by Jogdeo and Samuels <ref> [6] </ref>: Theorem [Jogdeo and Samuels] Let X n = x 1 + x 2 + + x n be the sum of n independent Bernoulli Trials, where Prfx i = 1g = 1 e i . Suppose that E [X n ] = np is an integer. <p> They also characterized the behavior of the difference as n ! 1 with np fixed <ref> [6] </ref>. Before presenting some additional probability applications, we give an easy algorithmic application. Interpolation Search. Interpolation Search was introduced by Peterson [13], and its performance has been extensively analyzed [18], [3], [11]. Moreover, the underlying scheme has been extended to support more general search [17] and data access systems [9].
Reference: [7] <author> G. Lueker and M. Molodowitch, </author> <title> More Analysis of Double Hashing, </title> <journal> Combinatorica, </journal> <volume> 13 (1993), </volume> <pages> pp. 83-96. </pages>
Reference-contexts: For completeness, we observe that a little more work can eliminate the factor of 2 in Theorem 4.1. 4.2 Double Hashing. In an analysis of double hashing by Lueker and Molodowitch <ref> [7] </ref>, a subproblem arose that can be abstracted as follows. Let S be a collection of some subsets of U fx 1 ; x 2 ; : : : ; x n g, where each element x i is a Boolean variable.
Reference: [8] <author> A. W. Marshall and I. Olkin, </author> <title> Inequalities: Theory of Majorization and Its Applications, </title> <publisher> Acad. Press, </publisher> <year> 1979. </year>
Reference-contexts: These formulations have been widely generalized (principally for symmetric functions that exhibit some kind of convexity) via a variety of majorization concepts (cf. <ref> [8] </ref>).
Reference: [9] <author> K. Mehlhorn and A. Tsakalidis, </author> <title> Dynamic Interpolation Search, </title> <journal> JACM, </journal> <volume> 40, 3 (1993), </volume> <pages> pp. 621-634. </pages>
Reference-contexts: Before presenting some additional probability applications, we give an easy algorithmic application. Interpolation Search. Interpolation Search was introduced by Peterson [13], and its performance has been extensively analyzed [18], [3], [11]. Moreover, the underlying scheme has been extended to support more general search [17] and data access systems <ref> [9] </ref>. The basic probing strategy, however, has yet to be analyzed by elementary means. Indeed, it has been open whether a simple analysis could establish a performance bound of log 2 log 2 n + O (1) [1], [2].
Reference: [10] <author> A. Panconesi and A. Srinivasan, </author> <title> Randomized distributed edge coloring via an extension of the Chernoff-Hoeffding bounds, </title> <journal> SIAM J. Comput., </journal> <volume> 26 (1997), </volume> <pages> pp. 350-368. </pages>
Reference: [11] <author> Y. Pearl, A. Itai and H. Avni, </author> <title> Interpolation search-A log log N search, </title> <journal> CACM, </journal> <volume> 21, 7 (1978), </volume> <pages> pp. 550-554. </pages>
Reference-contexts: They also characterized the behavior of the difference as n ! 1 with np fixed [6]. Before presenting some additional probability applications, we give an easy algorithmic application. Interpolation Search. Interpolation Search was introduced by Peterson [13], and its performance has been extensively analyzed [18], [3], <ref> [11] </ref>. Moreover, the underlying scheme has been extended to support more general search [17] and data access systems [9]. The basic probing strategy, however, has yet to be analyzed by elementary means. <p> Analysis 2. More interesting is a log 2 log 2 n+O (1) 4 bound for the probe count. The proof has two parts. The first half originates from an elegant, short (but sophisticated) analysis by Pearl, Itai, and Avni <ref> [11] </ref>. These authors presented a three-part analysis of Interpolation Search that was technically incomplete.
Reference: [12] <author> Y. Pearl and E. M. Reingold, </author> <title> Understanding the complexity of interpolation search, </title> <journal> IPL, </journal> <volume> 6 (1977), </volume> <pages> pp. 219-222. </pages>
Reference-contexts: Pearl and Reingold, in their presentation of a less efficient (but easily analyzed) search strategy, speculate that the difficulty of the analysis is responsible for the absence of this remarkable search procedure from algorithms texts <ref> [12] </ref>. The basic probe strategy assumes that data comprises independent values drawn from a fixed uniformly distributed range.
Reference: [13] <author> W. W. Peterson, </author> <title> Addressing for random access storage, </title> <institution> IBM J. Res. Dev. </institution> <month> 1 </month> <year> (1957), </year> <pages> pp. 130-146. </pages>
Reference-contexts: They also characterized the behavior of the difference as n ! 1 with np fixed [6]. Before presenting some additional probability applications, we give an easy algorithmic application. Interpolation Search. Interpolation Search was introduced by Peterson <ref> [13] </ref>, and its performance has been extensively analyzed [18], [3], [11]. Moreover, the underlying scheme has been extended to support more general search [17] and data access systems [9]. The basic probing strategy, however, has yet to be analyzed by elementary means.
Reference: [14] <author> S. </author> <title> Ramanujan, Collected Papers, </title> <publisher> Cambridge University Press, </publisher> <year> 1927. </year>
Reference-contexts: For completeness, we note that Ramanujan conjectured the very precise bound as follows, and used messy asymptotics to give a partial proof <ref> [14] </ref>. Theorem [Ramanujan] Let P n , for n &gt; 0, be a Poisson distributed random variable so that PrfP n = jg = e n n j j! , and E [P n ] = n, and suppose that n is an integer.
Reference: [15] <author> A. G. Ranade, </author> <title> How To Emulate Shared Memory, </title> <journal> JCSS Vol 42, </journal> <volume> 3 (1991), </volume> <pages> pp, 307-326. </pages>
Reference-contexts: Consequently, this formulation requires the factor 1 PrfZk 0 g , although it may actually be unnecessary for many applications. 4 Additional Algorithmic Applications. 4.1 Concurrent sorting for PRAM emulation. Ranade showed how to emulate a Common PRAM via an n fi 2 n butterfly network of processors <ref> [15] </ref>.
Reference: [16] <author> A. Siegel, </author> <title> On universal classes of fast high performance hash functions, their time-space tradeoff, and their applications, </title> <booktitle> 30 th FOCS, </booktitle> <year> (1989), </year> <pages> pp. 20-25. </pages>
Reference-contexts: In this algorithm, an n2 n -processor machine has a running time, for emulating one parallel step of an n2 n - processor Common PRAM, that is fi (n) with very high probability and on average. In the fast hashing work <ref> [16] </ref>, steps 1,3, and 4 were shown to work in O (n) expected time for a machine comprising n fi 2 n switches, but with only one column of 2 n processors, where each processor runs n virtual processes, so that the n2 n parallel computations can still be executed in
Reference: [17] <author> D. E. Willard, </author> <title> Searching unindexed and nonuniformly generated files in log log N time, </title> <journal> SIAM J. Comput., </journal> <volume> 14 (1985), </volume> <pages> pp. 1013-1029. </pages>
Reference-contexts: Before presenting some additional probability applications, we give an easy algorithmic application. Interpolation Search. Interpolation Search was introduced by Peterson [13], and its performance has been extensively analyzed [18], [3], [11]. Moreover, the underlying scheme has been extended to support more general search <ref> [17] </ref> and data access systems [9]. The basic probing strategy, however, has yet to be analyzed by elementary means. Indeed, it has been open whether a simple analysis could establish a performance bound of log 2 log 2 n + O (1) [1], [2].
Reference: [18] <author> A. C. Yao and F. F. Yao, </author> <title> The complexity of searching an ordered random table, </title> <booktitle> 17 th FOCS, </booktitle> <year> (1976), </year> <pages> pp. 173-175. </pages>
Reference-contexts: They also characterized the behavior of the difference as n ! 1 with np fixed [6]. Before presenting some additional probability applications, we give an easy algorithmic application. Interpolation Search. Interpolation Search was introduced by Peterson [13], and its performance has been extensively analyzed <ref> [18] </ref>, [3], [11]. Moreover, the underlying scheme has been extended to support more general search [17] and data access systems [9]. The basic probing strategy, however, has yet to be analyzed by elementary means. <p> For completeness, we note that the tight bound of log 2 log 2 n + fi (1) for Interpolation Search originates with Yao and Yao <ref> [18] </ref>. Gonnet, Rogers and George also analyze Interpolation Search [3]. Their study is by no means elementary, and uses extensive asymptotics.
References-found: 17

