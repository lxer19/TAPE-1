URL: http://www.ius.cs.cmu.edu/IUS/har2/har/idb/har/nips/paper.ps.gz
Refering-URL: http://www.cs.cmu.edu/~har/faces-old-papers.html
Root-URL: 
Email: har@cs.cmu.edu  baluja@cs.cmu.edu  tk@cs.cmu.edu  
Title: Human Face Detection in Visual Scenes  
Author: Henry A. Rowley Shumeet Baluja Takeo Kanade 
Address: Pittsburgh, PA 15213, USA  
Affiliation: School of Computer Science, Carnegie Mellon University,  
Abstract: We present a neural network-based face detection system. A retinally connected neural network examines small windows of an image, and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We use a bootstrap algorithm for training, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting non-face training examples, which must be chosen to span the entire space of non-face images. Comparisons with another state-of-the-art face detection system are presented; our system has better performance in terms of detection and false-positive rates.
Abstract-found: 1
Intro-found: 1
Reference: [ Baluja and Pomerleau, 1995 ] <author> Shumeet Baluja and Dean Pomerleau. </author> <title> Encouraging distributed input reliance in spatially constrained artificial neural networks: Applications to visual scene analysis and control. </title> <note> Submitted, </note> <year> 1995. </year>
Reference-contexts: to detect faces, and then present the error rates of the system over two large test sets. 3.1 SENSITIVITY ANALYSIS In order to determine which part of the input image the network uses to decide whether the input is a face, we performed a sensitivity analysis using the method of <ref> [ Baluja and Pomerleau, 1995 ] </ref> .
Reference: [ Le Cun et al., 1989 ] <author> Y. Le Cun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hub-bard, and L. D. Jackel. </author> <title> Backpropogation applied to handwritten zip code recognition. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 541-551, </pages> <year> 1989. </year>
Reference-contexts: Although the figure shows a single hidden unit for each subregion of the input, these units can be replicated. Similar architectures are commonly used in speech and character recognition tasks <ref> [ Waibel et al., 1989, Le Cun et al., 1989 ] </ref> . Examples of output from a single filter are shown in Figure 2. In the figure, each box represents the position and size of a window to which the neural network gave a positive response.
Reference: [ Rowley et al., 1995 ] <author> Henry A. Rowley, Shumeet Baluja, and Takeo Kanade. </author> <title> Human face detection in visual scenes. </title> <institution> CMU-CS-95-158R, Carnegie Mellon University, </institution> <month> November </month> <year> 1995. </year> <note> Also available at http://www.cs.cmu.edu/har/faces.html. </note>
Reference-contexts: Because of space restrictions only a few results are reported here; further results are presented in <ref> [ Rowley et al., 1995 ] </ref> . <p> Figure 6 shows some example output images from the system, produced by merging the detections from networks 1 and 2, and ANDing the results. Using another neural network to arbitrate among the two networks gives about the same performance as the simpler schemes presented above <ref> [ Rowley et al., 1995 ] </ref> .
Reference: [ Sung and Poggio, 1994 ] <author> Kah-Kay Sung and Tomaso Poggio. </author> <title> Example-based learning for view-based human face detection. </title> <journal> A.I. </journal> <note> Memo 1521, CBCL Paper 112, </note> <institution> MIT, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: The size of the training set for the second class can grow very quickly. We avoid the problem of using a huge training set of non-faces by selectively adding images to the training set as training progresses <ref> [ Sung and Poggio, 1994 ] </ref> . This bootstrapping method reduces the size of the training set needed. Detailed descriptions of this training method, along with the network architecture are given in Section 2. In Section 3 the performance of the system is examined. <p> It consists of two main steps: a preprocessing step, followed by a forward pass through a neural network. The preprocessing consists of lighting correction, which equalizes the intensity values across the window, followed by histogram equalization, which expands the range of intensities in the window <ref> [ Sung and Poggio, 1994 ] </ref> . The preprocessed window is used as the input to the neural network. The network has retinal connections to its input layer; the receptive fields of each hidden unit are shown in the figure. <p> A few example images are shown in Figure 3. It is difficult to collect a representative set of non-faces. Instead of collecting the images before training is started, the images are collected during training, as follows <ref> [ Sung and Poggio, 1994 ] </ref> : 1. Create 1000 non-face images using random pixel intensities. 2. Train a neural network to produce an output of 1 for the face examples, and -1 for the non-face examples. 3. Run the system on an image of scenery which contains no faces. <p> Test Set A was collected at CMU, and consists of 42 scanned photographs, newspaper pictures, images collected from the World Wide Web, and digitized television pictures. Test set B consists of 23 images provided by Sung and Poggio; it was used in <ref> [ Sung and Poggio, 1994 ] </ref> to measure the accuracy of their system. These test sets require the system to analyze 22,053,124 and 9,678,084 windows, respectively. <p> two networks 5) Networks 1 and 2 ! AND ! merge detections 52 69.2% 34 78.1% 6) Networks 1 and 2 ! merge detections ! AND 36 78.7% 20 87.1% 7) Networks 1 and 2 ! merge ! OR ! merge 26 84.6% 11 92.9% 4 COMPARISON TO OTHER SYSTEMS <ref> [ Sung and Poggio, 1994 ] </ref> reports a face-detection system based on clustering techniques. Their system, like ours, passes a small window over all portions of the image, and determines whether a face exists in each window. <p> The second distance metric is the Euclidean distance between the test pattern and its projection in the 75 dimensional subspace. These distance measures have close ties with Principal Components Analysis (PCA), as described in <ref> [ Sung and Poggio, 1994 ] </ref> . The last step in their system is to use either a perceptron or a neural network with a hidden layer, trained to classify points using the two distances to each of the clusters (a total of 24 inputs). <p> Images P and B correspond to Figures 2A and 2B. system using a variety of arbitration heuristics. In <ref> [ Sung and Poggio, 1994 ] </ref> , only 149 faces were labelled in the test set, while we labelled 155 (some are difficult for either system to detect). The number of missed faces is therefore six more than the values listed in their paper. <p> In <ref> [ Sung and Poggio, 1994 ] </ref> , only 149 faces were labelled in the test set, while we labelled 155 (some are difficult for either system to detect). The number of missed faces is therefore six more than the values listed in their paper. Also note that [ Sung and Poggio, 1994 ] check a slightly smaller number of windows over the entire test set; this is taken into account when computing the false detection rates. The table shows that we can achieve higher detection rates with fewer false detections. <p> Also note that <ref> [ Sung and Poggio, 1994 ] </ref> check a slightly smaller number of windows over the entire test set; this is taken into account when computing the false detection rates. The table shows that we can achieve higher detection rates with fewer false detections. Table 2: Comparison of [ Sung and Poggio, 1994 ] and Our System on Test Set B Missed Detect False System faces rate detects Rate 5) Networks 1 and 2 ! AND ! merge 34 78.1% 3 1/3226028 6) Networks 1 and 2 ! merge ! AND 20 87.1% 15 1/645206 7) Networks 1 and <p> B Missed Detect False System faces rate detects Rate 5) Networks 1 and 2 ! AND ! merge 34 78.1% 3 1/3226028 6) Networks 1 and 2 ! merge ! AND 20 87.1% 15 1/645206 7) Networks 1 and 2 ! merge ! OR ! merge 11 92.9% 64 1/151220 <ref> [ Sung and Poggio, 1994 ] </ref> (Multi-layer network) 36 76.8% 5 1/1929655 [ Sung and Poggio, 1994 ] (Perceptron) 28 81.9% 13 1/742175 5 CONCLUSIONS AND FUTURE RESEARCH Our algorithm can detect up to 92.9% of faces in a set of test images with an acceptable number of false positives. <p> and 2 ! AND ! merge 34 78.1% 3 1/3226028 6) Networks 1 and 2 ! merge ! AND 20 87.1% 15 1/645206 7) Networks 1 and 2 ! merge ! OR ! merge 11 92.9% 64 1/151220 <ref> [ Sung and Poggio, 1994 ] </ref> (Multi-layer network) 36 76.8% 5 1/1929655 [ Sung and Poggio, 1994 ] (Perceptron) 28 81.9% 13 1/742175 5 CONCLUSIONS AND FUTURE RESEARCH Our algorithm can detect up to 92.9% of faces in a set of test images with an acceptable number of false positives. This is a higher detection rate than [ Sung and Poggio, 1994 ] <p> network) 36 76.8% 5 1/1929655 <ref> [ Sung and Poggio, 1994 ] </ref> (Perceptron) 28 81.9% 13 1/742175 5 CONCLUSIONS AND FUTURE RESEARCH Our algorithm can detect up to 92.9% of faces in a set of test images with an acceptable number of false positives. This is a higher detection rate than [ Sung and Poggio, 1994 ] . The system can be made more conservative by varying the arbitration heuristics or thresholds. Currently, the system does not use temporal coherence to focus attention on particular portions of the image.
Reference: [ Waibel et al., 1989 ] <author> Alex Waibel, Toshiyuki Hanazawa, Geoffrey Hinton, Kiyohiro Shikano, and Kevin J. Lang. </author> <title> Phoneme recognition using time-delay neural networks. </title> <booktitle> Readings in Speech Recognition, </booktitle> <pages> pages 393-404, </pages> <year> 1989. </year>
Reference-contexts: Although the figure shows a single hidden unit for each subregion of the input, these units can be replicated. Similar architectures are commonly used in speech and character recognition tasks <ref> [ Waibel et al., 1989, Le Cun et al., 1989 ] </ref> . Examples of output from a single filter are shown in Figure 2. In the figure, each box represents the position and size of a window to which the neural network gave a positive response.
References-found: 5

