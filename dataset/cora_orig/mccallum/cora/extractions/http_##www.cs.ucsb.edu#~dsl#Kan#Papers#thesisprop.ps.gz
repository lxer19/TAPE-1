URL: http://www.cs.ucsb.edu/~dsl/Kan/Papers/thesisprop.ps.gz
Refering-URL: http://www.cs.ucsb.edu/~dsl/Kan/Papers/
Root-URL: http://www.cs.ucsb.edu
Title: Thesis Proposal Compositional Fault-Tolerant Distributed Object Systems  
Author: Jerry James 
Address: Santa Barbara  
Affiliation: Department of Computer Science University of California at  
Abstract: Research is proposed into the theory and practice of distributed shared object systems. Specitc points of inquiry are the application of compositional techniques to such systems, and techniques for constructing fault tolerant objects. In particular, we give an object-oriented model of concurrent systems, and show how to support proof reuse by applying existing compositional proof techniques to the model. We demonstrate the technique with an example, that of synchronizing nodes in a distributed system with gossip. We analyze the achievable fault tolerances of distributed shared memories in a totally asynchronous system. The analysis is in terms of t-resilience, the ability to operate correctly without blocking in spite of t node failures. We remark on current work on extending this analysis to general objects, and relationships to work on deriving lower bounds (in terms of network latencies) for various operations, as well as work on wait-free hierarchies. Finally, we present a design for an implementation supporting the model that provides transparent fault tolerance for distributed objects. The implementation exhibits compositionality, as it is based on a small collection of inherently fault-tolerant objects. It combines reasoning for the fault tolerance, load balancing, and consistency modules so that fault tolerance checkpoints can also be used as consistency replicas.
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Proceedings of the 17th Annual International Symposium on Computer Architecture, </institution> <address> Seattle, WA, USA, 2831 May 1990. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference: [2] <author> Martn Abadi and Leslie Lamport. </author> <title> Conjoining specitcations. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 17(3):50734, </volume> <month> May </month> <year> 1995. </year> <note> Also SRC Research Report 118. </note>
Reference-contexts: Stark [100] gives an incomplete rule for rely-guarantee properties that involves construction of a set of implications, to show that the processes do not interfere with each other. Abadi and Lamport give a composition rule in the context of TLA <ref> [2] </ref>. A proof system for the parallel language POOL is given by de Boer [38], and contains a composition rule. However, that rule can only compose pure safety properties (i.e., only a property that is its own closure may be composed). Other works in this area are as follows.
Reference: [3] <editor> ACM SIGACT. </editor> <booktitle> Proceedings of the 25th Annual ACM Symposium on the Theory of Computing, </booktitle> <address> San Diego, CA, USA, 1618 May 1993. </address> <publisher> ACM Press. </publisher>
Reference: [4] <editor> ACM SIGACT, ACM SIGARCH, EATCS. </editor> <booktitle> 5th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <address> Velen, Germany, 30 June2 July 1993. </address> <publisher> ACM Press. </publisher>
Reference: [5] <editor> ACM SIGACT, </editor> <booktitle> ACM SIGOPS. Proceedings of the 13th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <address> Los Angeles, CA, USA, </address> <month> 1417 August </month> <year> 1994. </year> <note> ACM Press. </note>
Reference: [6] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Weak ordering: A new detnition. </title> <booktitle> In AH90 [1], </booktitle> <pages> pages 214. </pages>
Reference-contexts: An attempt at realizing the best features of strong and weak memories was made by Attiya and Friedman [17] in the detnition of hybrid consistency. They showed that data-race free programs also execute on hybrid consistent memory as though on sequentially consistent memory. In <ref> [6] </ref>, Adve and Hill detne data-race-free-0 (DRF0), which distinguishes synchronization accesses from data accesses. A closely related idea is release consistency, detned by Gharachorloo et al. in [44]. They show that a class of properly-labeled programs execute on release consistent memory as though on sequentially consistent memory.
Reference: [7] <author> Sarita V. Adve and Mark D. Hill. </author> <title> A united formalization of four shared-memory models. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(6):61324, </volume> <month> June </month> <year> 1993. </year>
Reference-contexts: Bershad, Zekauskas, and Sawdon generalized release consistency to entry consistency [24]. Agrawal et al. detne mixed consistency [11] and show some classes of programs that execute on it as though on sequentially consistent memory. In <ref> [7] </ref>, Adve and Hill unify previous work on DRF0, release consistency, and the VAX memory model to produce DRF1, the weakest consistency condition for which data-race free programs behave as though on sequentially consistent memory. Other results obtained in this area are as follows.
Reference: [8] <author> Sarita V. Adve, Mark D. Hill, Barton P. Miller, and Robert H. B. Netzer. </author> <title> Detecting data races on weak memory systems. </title> <booktitle> In ISCA '91, </booktitle> <pages> pages 23443, </pages> <address> Toronto, Ontario, Canada, 2730 May 1991. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Raynal and Schiper [97] strengthened the result on data-race free programs for causal memory by showing other classes of programs that also execute on causal memory as though on sequentially consistent memory. Adve et al. <ref> [8] </ref> demonstrated a method for detecting a data race during the execution of a program on weak memory. Gibbons and Korach [45] give a method for online testing of a shared 13 memory implementation.
Reference: [9] <author> Divyakant Agrawal, Manhoi Choy, Hong Va Leong, and Ambuj K. Singh. </author> <title> Investigating weak memories using Maya. </title> <booktitle> In HPDC '94, </booktitle> <pages> pages 12330, </pages> <address> San Francisco, CA, USA, </address> <month> 25 August </month> <year> 1994. </year> <institution> IEEE Computer Society Technical Committee on Distributed Processing, Northeast Parallel Architectures Center at Syracuse University, ACM SIGCOMM, </institution> <address> Rome Laboratory, </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Adve et al. [8] demonstrated a method for detecting a data race during the execution of a program on weak memory. Gibbons and Korach [45] give a method for online testing of a shared 13 memory implementation. Systems supporting multiple consistency conditions were described by Agrawal et al. <ref> [10, 9] </ref>, Carter et al. [30], Keleher et al. [64], and Kohli et al. [65]. Attiya et al. considered the eect of letting a single node issue multiple shared memory operations in parallel [16].
Reference: [10] <author> Divyakant Agrawal, Manhoi Choy, Hong Va Leong, and Ambuj K. Singh. Maya: </author> <title> A simulation platform for distributed shared memories. </title> <editor> In D. K. Arvind, Rajive Bagrodia, and Jason Yi-Bing Lin, editors, </editor> <booktitle> PADS '94, volume 24 of SIGSIM newsletter, </booktitle> <pages> pages 1515, </pages> <address> Ed-inburgh, Scotland, UK, </address> <month> 68 July </month> <year> 1994. </year> <booktitle> ACM SIGSIM, the Society for Computer Simulation International, IEEE Computer Society, Society for Computer Simulation. </booktitle>
Reference-contexts: Adve et al. [8] demonstrated a method for detecting a data race during the execution of a program on weak memory. Gibbons and Korach [45] give a method for online testing of a shared 13 memory implementation. Systems supporting multiple consistency conditions were described by Agrawal et al. <ref> [10, 9] </ref>, Carter et al. [30], Keleher et al. [64], and Kohli et al. [65]. Attiya et al. considered the eect of letting a single node issue multiple shared memory operations in parallel [16].
Reference: [11] <author> Divyakant Agrawal, Manhoi Choy, Hong Va Leong, and Ambuj K. Singh. </author> <title> Mixed consistency: A model for parallel programming. </title> <booktitle> In ACLS94c [5], </booktitle> <pages> pages 10110. 15 </pages>
Reference-contexts: Weaker conditions have also been proposed, such as processor consistency [12], causal memory [13], release consistency [44], pipelined RAM [78], hybrid consistency [17], and mixed consistency <ref> [11] </ref>. Each such condition corresponds to some optimization that can lead to non-sequential behavior. As some have observed (e.g., [37]) the message-passing and shared memory paradigms can be united by introducing objects. <p> They show that a class of properly-labeled programs execute on release consistent memory as though on sequentially consistent memory. Bershad, Zekauskas, and Sawdon generalized release consistency to entry consistency [24]. Agrawal et al. detne mixed consistency <ref> [11] </ref> and show some classes of programs that execute on it as though on sequentially consistent memory.
Reference: [12] <author> Mustaque Ahamad, Rida Bazzi, Ranjit John, Prince Kohli, and Gil Neiger. </author> <title> The power of processor consistency. </title> <booktitle> In ABJKN93 [4], </booktitle> <pages> pages 25160. </pages>
Reference-contexts: The conditions of atomic memory [70, 71] (or linearizability [53]) and sequential consistency [69] require the shared memory to behave like a single memory module, where every operation is atomic. Weaker conditions have also been proposed, such as processor consistency <ref> [12] </ref>, causal memory [13], release consistency [44], pipelined RAM [78], hybrid consistency [17], and mixed consistency [11]. Each such condition corresponds to some optimization that can lead to non-sequential behavior. As some have observed (e.g., [37]) the message-passing and shared memory paradigms can be united by introducing objects. <p> In [56], Hutto and Ahamad detne slow memory. Goodman detned cache consistency (also called coherence) and processor consistency in [46]. However, his detnition of processor consistency was understood dierently by various researchers, as noted by Ahamad et al. in <ref> [12] </ref>. Causal memory is detned by Ahamad et al. in [13], where they also show that the class of data-race free programs execute on causal memory as though on sequentially consistent memory.
Reference: [13] <author> Mustaque Ahamad, Gil Neiger, James E. Burns, Prince Kohli, and Phillip W. Hutto. </author> <title> Causal memory: </title> <booktitle> Detnitions, implementation and programming. Distributed Computing, </booktitle> <address> 9(1):3749, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: The conditions of atomic memory [70, 71] (or linearizability [53]) and sequential consistency [69] require the shared memory to behave like a single memory module, where every operation is atomic. Weaker conditions have also been proposed, such as processor consistency [12], causal memory <ref> [13] </ref>, release consistency [44], pipelined RAM [78], hybrid consistency [17], and mixed consistency [11]. Each such condition corresponds to some optimization that can lead to non-sequential behavior. As some have observed (e.g., [37]) the message-passing and shared memory paradigms can be united by introducing objects. <p> In [56], Hutto and Ahamad detne slow memory. Goodman detned cache consistency (also called coherence) and processor consistency in [46]. However, his detnition of processor consistency was understood dierently by various researchers, as noted by Ahamad et al. in [12]. Causal memory is detned by Ahamad et al. in <ref> [13] </ref>, where they also show that the class of data-race free programs execute on causal memory as though on sequentially consistent memory. Attiya and Friedman showed that weak consistency conditions such as these are frequently incapable of supporting common parallel programming practices [19].
Reference: [14] <author> Hagit Attiya. </author> <title> Ecient and robust sharing of memory in message-passing systems. </title> <editor> In zalp Babaoglu and Keith Marzullo, editors, </editor> <booktitle> WDAG '96, volume 1151 of Lecture Notes in Computer Science, </booktitle> <pages> pages 5670, </pages> <address> Bologna, Italy, </address> <month> 911 October </month> <year> 1996. </year> <note> Springer-Verlag. </note>
Reference-contexts: Attiya et al. [15] give a fault-tolerant implementation of 1-reader mul-tiwriter atomic registers on top of a message-passing system (Attiya also provided an alternate implementation <ref> [14] </ref>). Lencevicius and Singh [73] give a lower bound on the latencies suered by implementations of linearizability and sequential consistency, which is strengthened by us [60] to tight bounds on the fault tolerance of several consistency conditions in a totally asynchronous setting.
Reference: [15] <author> Hagit Attiya, Amotz Bar-Noy, and Danny Dolev. </author> <title> Sharing memory robustly in message-passing systems. </title> <journal> Journal of the ACM, </journal> <volume> 42(1):12442, </volume> <month> January </month> <year> 1995. </year>
Reference-contexts: Attiya et al. <ref> [15] </ref> give a fault-tolerant implementation of 1-reader mul-tiwriter atomic registers on top of a message-passing system (Attiya also provided an alternate implementation [14]).
Reference: [16] <author> Hagit Attiya, Soma Chaudhuri, Roy Friedman, and Jennifer L. Welch. </author> <title> Shared memory consistency conditions for non-sequential execution: Detnitions and programming strategies. </title> <booktitle> In ACFW93 [4], </booktitle> <pages> pages 24150. </pages>
Reference-contexts: Systems supporting multiple consistency conditions were described by Agrawal et al. [10, 9], Carter et al. [30], Keleher et al. [64], and Kohli et al. [65]. Attiya et al. considered the eect of letting a single node issue multiple shared memory operations in parallel <ref> [16] </ref>. A description of the DEC Alpha's memory consistency model is given by Attiya and Friedman in [18]. Chandra et al. describe a language for writing memory consistency protocols [32]. Dill et al. developed a formal language for describing memory consistency conditions [39].
Reference: [17] <author> Hagit Attiya and Roy Friedman. </author> <title> A correctness condition for high-performance multiprocessors. </title> <booktitle> In STOC '92, </booktitle> <pages> pages 67990, </pages> <address> Victoria, British Columbia, Canada, </address> <month> 46 May </month> <year> 1992. </year> <booktitle> ACM SIGACT, </booktitle> <publisher> ACM Press. </publisher>
Reference-contexts: Weaker conditions have also been proposed, such as processor consistency [12], causal memory [13], release consistency [44], pipelined RAM [78], hybrid consistency <ref> [17] </ref>, and mixed consistency [11]. Each such condition corresponds to some optimization that can lead to non-sequential behavior. As some have observed (e.g., [37]) the message-passing and shared memory paradigms can be united by introducing objects. <p> Attiya and Friedman showed that weak consistency conditions such as these are frequently incapable of supporting common parallel programming practices [19]. An attempt at realizing the best features of strong and weak memories was made by Attiya and Friedman <ref> [17] </ref> in the detnition of hybrid consistency. They showed that data-race free programs also execute on hybrid consistent memory as though on sequentially consistent memory. In [6], Adve and Hill detne data-race-free-0 (DRF0), which distinguishes synchronization accesses from data accesses.
Reference: [18] <author> Hagit Attiya and Roy Friedman. </author> <title> Programming DEC-Alpha based multiprocessors the easy way. </title> <booktitle> In SPAA '94, </booktitle> <pages> pages 15766, </pages> <address> Cape May, NJ, USA, </address> <month> 2729 June </month> <year> 1994. </year>
Reference-contexts: Attiya et al. considered the eect of letting a single node issue multiple shared memory operations in parallel [16]. A description of the DEC Alpha's memory consistency model is given by Attiya and Friedman in <ref> [18] </ref>. Chandra et al. describe a language for writing memory consistency protocols [32]. Dill et al. developed a formal language for describing memory consistency conditions [39]. Friedman [43] showed that some powerful synchronization operations greatly simplify the task of implementing hybrid consistency.
Reference: [19] <author> Hagit Attiya and Roy Friedman. </author> <title> Limitations of fast consistency conditions for distributed shared memories. </title> <journal> Information Processing Letters, </journal> <volume> 57(5):2438, </volume> <month> March </month> <year> 1996. </year>
Reference-contexts: Programming methodologies and proof systems have been designed for the strong conditions, but are lacking for the weak conditions (but see [99] for program transformations that work around this problem), and some common programming methodologies cannot be supported on weak memories <ref> [19] </ref>. Furthermore, our research has shown that the knowledge that a program has certain properties with respect to one consistency condition gives only limited information on properties of the program with respect to other consistency conditions [57]. <p> Attiya and Friedman showed that weak consistency conditions such as these are frequently incapable of supporting common parallel programming practices <ref> [19] </ref>. An attempt at realizing the best features of strong and weak memories was made by Attiya and Friedman [17] in the detnition of hybrid consistency. They showed that data-race free programs also execute on hybrid consistent memory as though on sequentially consistent memory.
Reference: [20] <author> Hagit Attiya and Jennifer L. Welch. </author> <title> Sequential consistency versus linearizability. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 12(2):91122, </volume> <month> May </month> <year> 1994. </year>
Reference-contexts: This topic will be discussed further in Section 6. 6 5 Consistency The plethora of shared memory consistency conditions mentioned in the introduction can be confusing (e.g., see [96]). While linearizability and sequential consistency provide the clearest semantics for the programmer, they are also limited in their achievable eciency <ref> [20] </ref> and fault tolerance [60, 73]. The weaker consistency conditions can be implemented more eciently and more robustly, but suffer from less clear semantics. <p> They also outlined a proof methodology for linearizable objects, and proved that linearizability is a local property (i.e., a collection of linearizable objects forms a linearizable system). A formal comparison of linearizability and sequential consistency was carried out by Attiya and Welch in <ref> [20] </ref>. They determined that under certain conditions, sequential consistency can be implemented more eciently than linearizability. They also derived a tight lower bound on the number of network latencies implementations of the two conditions must suer.
Reference: [21] <author> Carlos Baquero and Francisco Moura. </author> <title> Concurrency annotations in C++. </title> <journal> SIGPLAN Notices, </journal> <volume> 29(7):617, </volume> <month> July </month> <year> 1994. </year>
Reference-contexts: A number of common modes of information sharing are directly supported, including read-only variables, accumulator variables, monotonic variables, write-once variables, and distributed tables. It is based on a message-passing model. CA/C++ (Concurrency Annotations in C++) <ref> [21] </ref> is an extension to C++ for coordinating threads in a shared memory system. Both synchronous and asynchronous method calls are supported, with futures provided for the asynchronous calls. To avoid the inheritance anomaly, synchronization constraints are detned in a separate class hierarchy. A guard-like construct is also available.
Reference: [22] <author> Howard Barringer, Ruurd Kuiper, and Amir Pnueli. </author> <title> Now you may compose temporal logic specitcations. </title> <booktitle> In STOC '84, </booktitle> <pages> pages 5163, </pages> <address> Washington, D.C., USA, 30 April2 May 1984. </address> <publisher> ACM SIGACT, ACM Press. </publisher>
Reference-contexts: Milner describes CCS [87], a process algebra, and later developed the -calculus [88, 89], a formalism based on the idea of naming. The ability to prove properties of the composition of smaller systems is considered in various works. Barringer, Kuiper and Pnueli <ref> [22] </ref> give a complete composition rule for mixed (i.e., not rely-guarantee) properties. However, the rule is extremely dicult to apply in practice. Pnueli [95] later gave an incomplete rule for rely-guarantee properties, then gave a rule for a mixed properties when each variable is modited by at most one process.
Reference: [23] <author> Peter Beckman, Dennis Gannon, and Elizabeth Johnson. </author> <title> Portable parallel programming in HPC++. </title> <editor> In Howard J. Siegel, editor, </editor> <booktitle> ICPP Workshop on Challenges for Parallel Processing '96, </booktitle> <pages> pages 1329, </pages> <address> Ithaca, NY, USA, </address> <month> 12 August </month> <year> 1996. </year> <note> Int. Assoc. </note> <institution> Comput. & Commun., Pennsylvania State Univ., IEEE Computer Society Press. </institution>
Reference-contexts: The idea is that any large data structure that can have its elements distributed across processors is a candidate for the kind of processing performed by pC++. The usual data parallel constructs are translated into an environment containing objects. HPC++ (High Performance C++) <ref> [23] </ref> arises from a merging of the pC++ and CC++ languages. It supports loop-level parallelism and a parallel version of the C++ STL. The designers intend to support parallel active objects as well. A CORBA interface is planned. The underlying model is shared memory.
Reference: [24] <author> Brian N. Bershad, Matthew J. Zekauskas, and Wayne A. Sawdon. </author> <title> The Midway distributed shared memory system. </title> <booktitle> In COMPCON Spring '93, </booktitle> <pages> pages 52837, </pages> <address> San Francisco, CA, USA, </address> <month> 2226 February </month> <year> 1993. </year> <booktitle> IEEE Computer Society, </booktitle> <publisher> IEEE Computer Society Press. </publisher> <pages> 16 </pages>
Reference-contexts: A closely related idea is release consistency, detned by Gharachorloo et al. in [44]. They show that a class of properly-labeled programs execute on release consistent memory as though on sequentially consistent memory. Bershad, Zekauskas, and Sawdon generalized release consistency to entry consistency <ref> [24] </ref>. Agrawal et al. detne mixed consistency [11] and show some classes of programs that execute on it as though on sequentially consistent memory.
Reference: [25] <author> Robert D. Blumofe, Matteo Frigo, Christopher F. Joerg, Charles E. Leiserson, and Keith H. Randall. </author> <title> An analysis of dag-consistent distributed shared-memory algorithms. </title> <booktitle> In SPAA '96, </booktitle> <pages> pages 297308, </pages> <address> Padua, Italy, </address> <month> 2426 June </month> <year> 1996. </year> <journal> ACM SIGACT, ACM SIGARCH, </journal> <note> EATCS, ACM Press. </note>
Reference-contexts: The dependencies among threads form a rooted DAG. Load sharing takes place through randomized work stealing, in which a node with nothing to do asks a random neighbor for a task. There is an online data-race detector. The system guarantees DAG consistency <ref> [25] </ref> at a minimum, and something stronger if system support is available. Thor [79] is a distributed object-oriented database. It provides objects to applications written in dierent programming languages. Access to object state can occur only through method calls. Techniques for safely sharing objects between clients are explored.
Reference: [26] <author> Robert D. Blumofe, Christopher F. Joerg, Bradley C. Kuszmaul, Charles E. Leiserson, Keith H. Randall, and Yuli Zhou. Cilk: </author> <title> An ecient multithreaded runtime system. </title> <booktitle> In PPOPP '95, pages 20716, </booktitle> <address> Santa Barbara, CA, USA, </address> <year> 1921 </year> <month> July </month> <year> 1995. </year> <booktitle> ACM SIGPLAN, </booktitle> <publisher> ACM Press. </publisher>
Reference-contexts: Objects in the Amoeba system have a txed location. Method calls are made via an RPC mechanism. Consistency is supported by totally-ordered, causally-ordered, and unordered group communication. Compilers have been built for several languages that generate code for the Amoeba system. Cilk <ref> [26] </ref> is an algorithmic, multithreaded language. The runtime system guarantees predictable performance. It features an architecture- and language-independent checkpointing facility based on source-to-source translations. Code is structured into procedures. Each procedure consists of one or more nonblocking threads. The dependencies among threads form a rooted DAG.
Reference: [27] <author> Franois Bodin, Peter Beckman, Dennis Gannon, Srinivas Narayana, and Shelby X. Yang. </author> <title> Distributed pC++: Basic ideas for an object parallel language. </title> <booktitle> Scientitc Programming, </booktitle> <address> 2(3):722, </address> <month> Fall </month> <year> 1993. </year>
Reference-contexts: Mentat uses objects to mask communication, synchronization and scheduling from the programmer. It is based on C++. The granule of computation is the method. The programmer chooses the granularity by appropriately designing methods, and the run-time system automatically detects data dependencies while parallelizing the program. The pC++ language <ref> [27] </ref> is a data-parallel extension to C++. The Sage++ compiler preprocessor toolkit is used to compile pC++ programs to ordinary C++. It is based on the idea of distributed aggregates, where an aggregate is a vector, array, matrix, grid, tree, DAG, etc.
Reference: [28] <author> Elizabeth Borowsky and Eli Gafni. </author> <title> Generalized FLP impossibility result for t-resilient asynchronous computations (extended abstract). </title> <booktitle> In BG93 [3], </booktitle> <pages> pages 91100. </pages>
Reference-contexts: analyzed the network latencies that must be suered by linearizability, sequential consistency, and hybrid consistency in a partially synchronous setting. 7.4 Fault Tolerance The notion of t-resilience, the ability to withstand t node failures, is explored in various contexts by Dolev, Dwork and Stockmeyer [40] (consensus protocols), Borowsky and Gafni <ref> [28] </ref> (asynchronous tasks), Herlihy and Shavit [52] (asynchronous tasks), Chandra et al. [33] (shared objects), and Lo [82] (shared objects). Attiya et al. [15] give a fault-tolerant implementation of 1-reader mul-tiwriter atomic registers on top of a message-passing system (Attiya also provided an alternate implementation [14]).
Reference: [29] <author> Luca Cardelli. </author> <title> A language with distributed scope. </title> <booktitle> Computing Systems, </booktitle> <address> 8(1):2759, </address> <month> Winter </month> <year> 1995. </year>
Reference-contexts: Each object can have several attributes set at object creation time, including cached/non-cached, invalidate/update, array distribution, and nonblocking accesses. Processes are considered as separate entities from objects. The system guarantees release consistency, and uses a multiple-writer protocol. Barrier, mutex, and semaphore synchronization operators are provided. Obliq <ref> [29] </ref> is a lexically-scoped untyped interpreted language that supports distributed object-oriented computation. It handles multiple threads per address space, multiple address spaces per 11 machine, heterogeneous machines on a network, and multiple networks on an Internet. Objects exist on a single site, but threads can migrate.
Reference: [30] <author> John B. Carter, John K. Bennett, and Willy Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In SOSP '91, pages 15264, Asilomar Conference Center, </booktitle> <address> Pacitc Grove, CA, USA, </address> <month> 1316 October </month> <year> 1991. </year> <booktitle> ACM SIGOPS, </booktitle> <publisher> ACM Press. </publisher>
Reference-contexts: Gibbons and Korach [45] give a method for online testing of a shared 13 memory implementation. Systems supporting multiple consistency conditions were described by Agrawal et al. [10, 9], Carter et al. <ref> [30] </ref>, Keleher et al. [64], and Kohli et al. [65]. Attiya et al. considered the eect of letting a single node issue multiple shared memory operations in parallel [16]. A description of the DEC Alpha's memory consistency model is given by Attiya and Friedman in [18].
Reference: [31] <author> Rohit Chandra, Anoop Gupta, and John Hennessy. </author> <title> COOL: An object-based language for parallel programming. </title> <journal> IEEE Computer, </journal> <volume> 27(8):1326, </volume> <month> August </month> <year> 1994. </year>
Reference-contexts: Methods can be called on nonlocal objects with the RMI (Remote Method Invocation) API. However, the only kind of interaction possible is client-server; there is no support for object replication or fragmentation. The COOL language <ref> [31] </ref> has constructs for expressing parallel tasks, synchronization (monitors and condition variables) and exclusive access to an object (mutex methods). It is based on a shared memory model, and provides cache consistency by default.
Reference: [32] <author> Satish Chandra, Brad Richards, and James R. Larus. Teapot: </author> <title> Language support for writing memory coherence protocols. </title> <type> Technical Report CS-TR961291, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, WI 53706, </institution> <month> February </month> <year> 1996. </year>
Reference-contexts: Attiya et al. considered the eect of letting a single node issue multiple shared memory operations in parallel [16]. A description of the DEC Alpha's memory consistency model is given by Attiya and Friedman in [18]. Chandra et al. describe a language for writing memory consistency protocols <ref> [32] </ref>. Dill et al. developed a formal language for describing memory consistency conditions [39]. Friedman [43] showed that some powerful synchronization operations greatly simplify the task of implementing hybrid consistency.
Reference: [33] <author> Tushar Chandra, Vassos Hadzilacos, Prasad Jayanti, and Sam Toueg. </author> <title> Wait-freedom vs. t-resiliency and the robustness of wait-free hierarchies (extended abstract). </title> <booktitle> In CHJT94 [5], </booktitle> <pages> pages 33443. </pages>
Reference-contexts: and hybrid consistency in a partially synchronous setting. 7.4 Fault Tolerance The notion of t-resilience, the ability to withstand t node failures, is explored in various contexts by Dolev, Dwork and Stockmeyer [40] (consensus protocols), Borowsky and Gafni [28] (asynchronous tasks), Herlihy and Shavit [52] (asynchronous tasks), Chandra et al. <ref> [33] </ref> (shared objects), and Lo [82] (shared objects). Attiya et al. [15] give a fault-tolerant implementation of 1-reader mul-tiwriter atomic registers on top of a message-passing system (Attiya also provided an alternate implementation [14]).
Reference: [34] <author> K. Mani Chandy and Carl Kesselman. </author> <title> Compositional C++: Compositional parallel programming. </title> <editor> In Utpal Banerjee et al., editor, </editor> <booktitle> Languages and Compilers for Parallel Computing '92, volume 757 of Lecture Notes in Computer Science, </booktitle> <pages> pages 12444, </pages> <address> New Haven, CT, USA, </address> <month> 35 August </month> <year> 1992. </year> <note> Springer-Verlag. </note>
Reference-contexts: Load balancing is automatic, but the programmer can provide hints to the system. Facilities are provided for allocating an object on a specitc node, migrating all or part of an object to a specitc node, and tnding the node that holds an object. The CC++ (Compositional C++) language <ref> [34] </ref> has constructs for expressing parallel blocks (both synchronous and asynchronous), parallel loops, synchronization, and global pointers. Each main process can be treated as an object. It is based on a shared memory model, and provides cache consistency.
Reference: [35] <author> K. Mani Chandy and Jayadev Misra. </author> <title> Parallel Program Design: A Foundation. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, USA, </address> <year> 1988. </year> <note> Reprinted with corrections, </note> <month> May </month> <year> 1989. </year>
Reference-contexts: A partial list follows. Owicki and Gries [93] give an axiomatic approach. Hoare [55] introduced CSP, which features a guarded statement semantics, and a rendezvous mechanism for process communication. Chandy and Misra describe UNITY in <ref> [35] </ref>. This system is based on shared variables and assignment statements. Lamport detned the Temporal Logic of Actions (TLA) in [72]. It is based on shared variables and actions, events that change state. Chu spaces, an automaton-based approach, are described by Pratt and Gupta in [48, 49].
Reference: [36] <author> Pierre Collette. </author> <title> Composition of assumption-commitment specitcations in a UNITY style. </title> <booktitle> Science of Computer Programming, </booktitle> <address> 23(23):10725, </address> <month> December </month> <year> 1994. </year> <booktitle> Proceedings of TAPSOFT '93. </booktitle>
Reference-contexts: They give a composition 12 rule for such properties and show that it is unable to prove properties of a gossip algorithm. They blame this on the modited rely-guarantee properties. However, we have been able to show that the culprit is actually their weak composition rule. Collette <ref> [36] </ref> shows how to adapt Abadi and Lamport's logic-independent composition rule to the UNITY logic. 7.3 Shared Memory Consistency Conditions The foundational paper for this area is Lamport's detnition of sequential consistency [69]. The stronger notion of atomic memory was explored in [70, 71].
Reference: [37] <author> Lawrence A. Crowl. </author> <title> A uniform object model for parallel programming. </title> <editor> In Gul Agha, Peter Wegner, and Akinori Yonezawa, editors, </editor> <booktitle> Workshop on Object-Based Concurrent Programming, volume 24 of SIGPLAN Notices, </booktitle> <pages> pages 257, </pages> <address> San Diego, CA, USA, </address> <month> 2627 September </month> <year> 1988. </year> <month> 17 </month>
Reference-contexts: Weaker conditions have also been proposed, such as processor consistency [12], causal memory [13], release consistency [44], pipelined RAM [78], hybrid consistency [17], and mixed consistency [11]. Each such condition corresponds to some optimization that can lead to non-sequential behavior. As some have observed (e.g., <ref> [37] </ref>) the message-passing and shared memory paradigms can be united by introducing objects. A general concurrent system, for which message-passing and shared memory are both special cases, is composed of two fundamental entities: objects and threads.
Reference: [38] <author> Frank S. de Boer. </author> <title> A proof system for the parallel object-oriented language POOL. </title> <editor> In M. S. Paterson, editor, </editor> <booktitle> ICALP '90, volume 443 of Lecture Notes in Computer Science, </booktitle> <pages> pages 57285, </pages> <institution> Warwick University, </institution> <address> England, 1620 July 1990. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Abadi and Lamport give a composition rule in the context of TLA [2]. A proof system for the parallel language POOL is given by de Boer <ref> [38] </ref>, and contains a composition rule. However, that rule can only compose pure safety properties (i.e., only a property that is its own closure may be composed). Other works in this area are as follows.
Reference: [39] <author> David L. Dill, Seungjoon Park, and Andreas G. Nowatzyk. </author> <title> Formal specitcation of abstract memory models. </title> <editor> In Gaetano Borriello and Carl Ebeling, editors, </editor> <booktitle> Research on Integrated Systems '93, </booktitle> <pages> pages 3852, </pages> <address> Cambridge, MA, USA, March 1993. </address> <publisher> MIT Press. </publisher>
Reference-contexts: A description of the DEC Alpha's memory consistency model is given by Attiya and Friedman in [18]. Chandra et al. describe a language for writing memory consistency protocols [32]. Dill et al. developed a formal language for describing memory consistency conditions <ref> [39] </ref>. Friedman [43] showed that some powerful synchronization operations greatly simplify the task of implementing hybrid consistency.
Reference: [40] <author> Danny Dolev, Cynthia Dwork, and Larry Stockmeyer. </author> <title> On the minimal synchronism needed for distributed consensus. </title> <journal> Journal of the ACM, </journal> <volume> 34(1):7797, </volume> <month> January </month> <year> 1987. </year>
Reference-contexts: Kosa [67] analyzed the network latencies that must be suered by linearizability, sequential consistency, and hybrid consistency in a partially synchronous setting. 7.4 Fault Tolerance The notion of t-resilience, the ability to withstand t node failures, is explored in various contexts by Dolev, Dwork and Stockmeyer <ref> [40] </ref> (consensus protocols), Borowsky and Gafni [28] (asynchronous tasks), Herlihy and Shavit [52] (asynchronous tasks), Chandra et al. [33] (shared objects), and Lo [82] (shared objects).
Reference: [41] <author> M. J. Fischer, Nancy A. Lynch, and M. S. Paterson. </author> <title> Impossibility of distributed consensus with one faulty process. </title> <journal> Journal of the ACM, </journal> <volume> 32(2):37482, </volume> <month> April </month> <year> 1985. </year>
Reference-contexts: Hence, it is impossible to dierentiate between a very slow node and a failed node. It is known that some objects, such as those capable of solving the Consensus problem, cannot tolerate any node failures in a totally asynchronous system <ref> [41] </ref>. This leads us to the following question: given an object specitcation, what is the maximum number of faulty nodes that can be tolerated by some implementation of the object? To answer this question, we use the notion of t-resilience, the ability to withstand t node failures.
Reference: [42] <author> Matthew I. Frank and Mary K. Vernon. </author> <title> A hybrid shared memory/message passing parallel machine. </title> <editor> In C. Y. R. Chen and P. Bruce Berra, editors, </editor> <volume> ICPP '93, volume 1: Architecture, </volume> <pages> pages 2326, </pages> <address> St. Charles, IL, USA, </address> <month> 1620 August </month> <year> 1993. </year> <title> Pennsylvania State University, </title> <publisher> CRC Press. </publisher>
Reference-contexts: We model shared memory systems by having a collection of ReadWrite objects. The abstract state of each such object consists of an integer or machine word. Other kinds of systems can also be modeled, including hybrids of the message-passing and shared memory paradigms such as those described in <ref> [42, 50, 54, 68, 75] </ref>. The proposed research is to investigate the use of compositional techniques in constructing transparently fault-tolerant distributed object systems and proving properties of objects in such a system. In particular, the following points will be addressed: * Compositional reasoning techniques for concurrent objects.
Reference: [43] <author> Roy Friedman. </author> <title> Implementing hybrid consistency with high-level synchronization operations. </title> <booktitle> Distributed Computing, </booktitle> <address> 9(3):11929, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: A description of the DEC Alpha's memory consistency model is given by Attiya and Friedman in [18]. Chandra et al. describe a language for writing memory consistency protocols [32]. Dill et al. developed a formal language for describing memory consistency conditions [39]. Friedman <ref> [43] </ref> showed that some powerful synchronization operations greatly simplify the task of implementing hybrid consistency.
Reference: [44] <author> Kourosh Gharachorloo, Daniel Lenoski, James Laudon, Phillip Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In GLLGGH90 [1], </booktitle> <pages> pages 1526. </pages> <note> Revised in Stanford CSL Technical Report CSL-TR93568. </note>
Reference-contexts: The conditions of atomic memory [70, 71] (or linearizability [53]) and sequential consistency [69] require the shared memory to behave like a single memory module, where every operation is atomic. Weaker conditions have also been proposed, such as processor consistency [12], causal memory [13], release consistency <ref> [44] </ref>, pipelined RAM [78], hybrid consistency [17], and mixed consistency [11]. Each such condition corresponds to some optimization that can lead to non-sequential behavior. As some have observed (e.g., [37]) the message-passing and shared memory paradigms can be united by introducing objects. <p> They showed that data-race free programs also execute on hybrid consistent memory as though on sequentially consistent memory. In [6], Adve and Hill detne data-race-free-0 (DRF0), which distinguishes synchronization accesses from data accesses. A closely related idea is release consistency, detned by Gharachorloo et al. in <ref> [44] </ref>. They show that a class of properly-labeled programs execute on release consistent memory as though on sequentially consistent memory. Bershad, Zekauskas, and Sawdon generalized release consistency to entry consistency [24].
Reference: [45] <author> Phillip B. Gibbons and Ephraim Korach. </author> <title> Testing shared memories. </title> <note> To appear in SIAM Journal on Computing. </note>
Reference-contexts: Adve et al. [8] demonstrated a method for detecting a data race during the execution of a program on weak memory. Gibbons and Korach <ref> [45] </ref> give a method for online testing of a shared 13 memory implementation. Systems supporting multiple consistency conditions were described by Agrawal et al. [10, 9], Carter et al. [30], Keleher et al. [64], and Kohli et al. [65].
Reference: [46] <author> James R. Goodman. </author> <title> Cache consistency and sequential consistency. </title> <type> Technical Report CS-TR911006, </type> <institution> Computer Sciences Department, University of Wisconsin - Madison, Madison, Wisconsin 53706, </institution> <month> February </month> <year> 1991. </year>
Reference-contexts: Weak shared memories were devised to increase opportunities for optimizations. A very weak condition, pipelined RAM, is detned by Lipton and Sandberg in [78]. In [56], Hutto and Ahamad detne slow memory. Goodman detned cache consistency (also called coherence) and processor consistency in <ref> [46] </ref>. However, his detnition of processor consistency was understood dierently by various researchers, as noted by Ahamad et al. in [12].
Reference: [47] <author> James Gosling, Bill Joy, and Guy Steele. </author> <title> The Java Language Specitcation. The Java Series. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1996. </year>
Reference-contexts: We also make some remarks on ongoing research into extending these results to general objects. Practice: We have designed and will build a reference implementation of the object model, based on the Java language <ref> [47] </ref>. This implementation will be used to experiment with techniques for implementing fault tolerance. It will be based on a small substrate of inherently fault tolerant objects, thereby using compositional techniques. <p> This process should yield insights into the relative diculties of programming for each of the consistency conditions involved. 7 Related Work/Reading List The proposed research is related to work in several telds. For convenience, we consider the various telds separately. 7.1 Concurrent Object-Oriented Languages and Systems The Java language <ref> [47] </ref> and system [77] has deep roots in C++, but adds many new features. The bytecoded nature of the language is intended to enhance portability. Mutual exclusion is supported through the synchronized label; for each object, at most one thread is executing in a synchronized method at any given time.
Reference: [48] <author> Vineet Gupta. </author> <title> Chu Spaces: A Model for Concurrency. </title> <type> PhD thesis, </type> <institution> Stanford University, Stanford, </institution> <address> CA 94305, </address> <month> August </month> <year> 1994. </year> <month> CS-TR941521. </month>
Reference-contexts: This system is based on shared variables and assignment statements. Lamport detned the Temporal Logic of Actions (TLA) in [72]. It is based on shared variables and actions, events that change state. Chu spaces, an automaton-based approach, are described by Pratt and Gupta in <ref> [48, 49] </ref>. Lynch et al. describe I/O Automata in [84, 85]. Milner describes CCS [87], a process algebra, and later developed the -calculus [88, 89], a formalism based on the idea of naming. The ability to prove properties of the composition of smaller systems is considered in various works.
Reference: [49] <author> Vineet Gupta and Vaughan Pratt. </author> <title> Gates accept concurrent behavior. </title> <booktitle> In FOCS '93, </booktitle> <pages> pages 6271, </pages> <address> Palo Alto, CA, USA, </address> <month> 35 November </month> <year> 1993. </year> <booktitle> IEEE Computer Society, </booktitle> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: This system is based on shared variables and assignment statements. Lamport detned the Temporal Logic of Actions (TLA) in [72]. It is based on shared variables and actions, events that change state. Chu spaces, an automaton-based approach, are described by Pratt and Gupta in <ref> [48, 49] </ref>. Lynch et al. describe I/O Automata in [84, 85]. Milner describes CCS [87], a process algebra, and later developed the -calculus [88, 89], a formalism based on the idea of naming. The ability to prove properties of the composition of smaller systems is considered in various works.
Reference: [50] <author> John Heinlein, Kourosh Gharachorloo, Scott A. Dresser, and Anoop Gupta. </author> <title> Integration of message passing and shared memory in the Stanford FLASH multiprocessor. </title> <booktitle> In ASPLOS-VI, </booktitle> <pages> pages 3850, </pages> <address> San Jose, CA, USA, </address> <month> 47 October </month> <year> 1994. </year> <journal> ACM, IEEE Computer Society, ACM Press. Published as SIGARCH Computer Architecture News 22(special issue): Oct 1994; SIGOPS Operating Systems Review 28(5): Oct 1994; SIGPLAN Notices 29(11): </journal> <month> Nov </month> <year> 1994. </year> <month> 18 </month>
Reference-contexts: We model shared memory systems by having a collection of ReadWrite objects. The abstract state of each such object consists of an integer or machine word. Other kinds of systems can also be modeled, including hybrids of the message-passing and shared memory paradigms such as those described in <ref> [42, 50, 54, 68, 75] </ref>. The proposed research is to investigate the use of compositional techniques in constructing transparently fault-tolerant distributed object systems and proving properties of objects in such a system. In particular, the following points will be addressed: * Compositional reasoning techniques for concurrent objects.
Reference: [51] <author> Maurice P. Herlihy. </author> <title> Wait-free synchronization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(1):12449, </volume> <month> January </month> <year> 1991. </year>
Reference-contexts: This analysis is performed in terms of t-resilience, the ability of an operation to complete correctly in spite of t node failures 1 . We detne a hierarchy of 1 An operation that can tolerate the failure of N 1 out of N nodes is called wait-free <ref> [51] </ref>, since it need not wait 2 resilient objects, and give some general proofs and techniques for proving both upper and lower resilience bounds. We give tight bounds for shared memory objects (with several kinds of consistency conditions), and some other simple objects. <p> An object at level i of this hierarchy can be implemented so that every method on it is at least i-resilient. We have proved certain relationships between this hierarchy and Jayanti's wait-free hierarchy h r m [61] (which is based on Herlihy's consensus hierarchy <ref> [51] </ref>). We can show, for example, that every object with consensus number 1 is 0-resilient. We have identited an object, which we call the View object, that is very useful in constructing these proofs.
Reference: [52] <author> Maurice P. Herlihy and Nir Shavit. </author> <title> The asynchronous computability theorem for t-resilient tasks. </title> <booktitle> In HS93 [3], </booktitle> <pages> pages 11120. </pages>
Reference-contexts: be suered by linearizability, sequential consistency, and hybrid consistency in a partially synchronous setting. 7.4 Fault Tolerance The notion of t-resilience, the ability to withstand t node failures, is explored in various contexts by Dolev, Dwork and Stockmeyer [40] (consensus protocols), Borowsky and Gafni [28] (asynchronous tasks), Herlihy and Shavit <ref> [52] </ref> (asynchronous tasks), Chandra et al. [33] (shared objects), and Lo [82] (shared objects). Attiya et al. [15] give a fault-tolerant implementation of 1-reader mul-tiwriter atomic registers on top of a message-passing system (Attiya also provided an alternate implementation [14]).
Reference: [53] <author> Maurice P. Herlihy and Jeannette M. Wing. </author> <title> Linearizability: A correctness condition for concurrent objects. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12(3):463 92, </volume> <month> July </month> <year> 1990. </year>
Reference-contexts: A DSM system provides a contract to the programmer in the form of a consistency condition, which is a guarantee on how the views of the shared memory seen by each node relate to each other. The conditions of atomic memory [70, 71] (or linearizability <ref> [53] </ref>) and sequential consistency [69] require the shared memory to behave like a single memory module, where every operation is atomic. Weaker conditions have also been proposed, such as processor consistency [12], causal memory [13], release consistency [44], pipelined RAM [78], hybrid consistency [17], and mixed consistency [11]. <p> The stronger notion of atomic memory was explored in [70, 71]. Atomic memory was generalized to objects and named linearizability by Herlihy and Wing in <ref> [53] </ref>. They also outlined a proof methodology for linearizable objects, and proved that linearizability is a local property (i.e., a collection of linearizable objects forms a linearizable system). A formal comparison of linearizability and sequential consistency was carried out by Attiya and Welch in [20].
Reference: [54] <author> Mark D. Hill, James R. Larus, and David A. Wood. </author> <title> Tempest: A substrate for portable parallel programs. </title> <booktitle> In COMPCON '95, </booktitle> <pages> pages 32733, </pages> <address> San Francisco, CA, USA, 59 March 1995. </address> <publisher> IEEE Computer Society, IEEE Computer Society Press. </publisher>
Reference-contexts: We model shared memory systems by having a collection of ReadWrite objects. The abstract state of each such object consists of an integer or machine word. Other kinds of systems can also be modeled, including hybrids of the message-passing and shared memory paradigms such as those described in <ref> [42, 50, 54, 68, 75] </ref>. The proposed research is to investigate the use of compositional techniques in constructing transparently fault-tolerant distributed object systems and proving properties of objects in such a system. In particular, the following points will be addressed: * Compositional reasoning techniques for concurrent objects.
Reference: [55] <author> C. A. R. Hoare. </author> <title> Communicating sequential processes. </title> <journal> Communications of the ACM, </journal> <volume> 21(8):66677, </volume> <month> August </month> <year> 1978. </year>
Reference-contexts: Thor is supported by the Theta object-oriented language [80], a sequential, strongly-typed language. 7.2 Proof Systems A number of approaches have been taken to constructing concurrent proof systems. A partial list follows. Owicki and Gries [93] give an axiomatic approach. Hoare <ref> [55] </ref> introduced CSP, which features a guarded statement semantics, and a rendezvous mechanism for process communication. Chandy and Misra describe UNITY in [35]. This system is based on shared variables and assignment statements. Lamport detned the Temporal Logic of Actions (TLA) in [72].
Reference: [56] <author> Phillip W. Hutto and Mustaque Ahamad. </author> <title> Slow memory: Weakening consistency to enhance concurrency in distributed shared memory. </title> <booktitle> In ICDCS-10, </booktitle> <pages> pages 3029, </pages> <address> Paris, France, </address> <month> 28 May 1 June </month> <year> 1990. </year> <institution> Institut National de Recherche en Informatique et en Automatique, IEEE Computer Society Technical Committee on Distributed Processing, IEEE Computer Society Press. </institution>
Reference-contexts: Li and Hudak [74] consider several problems that arise when attempting to eciently implement sequentially consistent shared memory. Weak shared memories were devised to increase opportunities for optimizations. A very weak condition, pipelined RAM, is detned by Lipton and Sandberg in [78]. In <ref> [56] </ref>, Hutto and Ahamad detne slow memory. Goodman detned cache consistency (also called coherence) and processor consistency in [46]. However, his detnition of processor consistency was understood dierently by various researchers, as noted by Ahamad et al. in [12].
Reference: [57] <author> Jerry James and Ambuj K. Singh. </author> <title> Unpublished results. </title>
Reference-contexts: Furthermore, our research has shown that the knowledge that a program has certain properties with respect to one consistency condition gives only limited information on properties of the program with respect to other consistency conditions <ref> [57] </ref>. The end result is that weak shared memories, while potentially providing eciency and fault tolerance gains, are dicult to program and dicult to reason about. The situation is further confused when objects are introduced. Distributed objects are making their way into everyday parlance.
Reference: [58] <author> Jerry James and Ambuj K. Singh. </author> <title> The impact of hardware models on shared memory consistency conditions. </title> <editor> In Ugo Montanari and Vladimiro Sassone, editors, </editor> <booktitle> CONCUR '96, volume 1119 of Lecture Notes in Computer Science, </booktitle> <pages> pages 71934, </pages> <address> Pisa, Italy, 2629 August 1996. </address> <publisher> Springer. </publisher>
Reference-contexts: In Section 7, we discuss related work. Finally, we make some concluding remarks. A specitcation of the proposed system is found in the Appendix. There are also three attached papers detailing accomplished research, corresponding to <ref> [58] </ref>, [60], and [59]. 2 Model Concurrent objects are of interest for several reasons. First, many resources are available only in certain geographical places, or on specitc nodes of a network. Encapsulating such resources as objects is a convenient and intuitive way of abstracting them. <p> There are also program transformations 7 that map a correct program that meets some condition on one consistency condition into a correct program for another. As part of our study of consistency, we developed a notion of a relatively complete implementation <ref> [58] </ref>. This is an implementation whose possible behaviors range over all behaviors allowed by the consistency condition. That is, a complete implementation is a maximally correct implementation for its consistency condition. We developed algorithms that are complete for several consistency conditions under various assumptions about the system.
Reference: [59] <author> Jerry James and Ambuj K. Singh. </author> <title> Compositional proofs for concurrent objects. </title> <type> Technical Report TRCS9704, </type> <institution> Computer Science Dept., UCSB, Santa Barbara, </institution> <address> CA, </address> <month> May </month> <year> 1997. </year>
Reference-contexts: In Section 7, we discuss related work. Finally, we make some concluding remarks. A specitcation of the proposed system is found in the Appendix. There are also three attached papers detailing accomplished research, corresponding to [58], [60], and <ref> [59] </ref>. 2 Model Concurrent objects are of interest for several reasons. First, many resources are available only in certain geographical places, or on specitc nodes of a network. Encapsulating such resources as objects is a convenient and intuitive way of abstracting them. <p> In our model, we view objects as encapsulated state holders, and also introduce classes (groups of objects with a shared implementation), threads (agents of computation), and a guarded statement semantics. Each concept is explained in more detail below. See <ref> [59] </ref> (attached) for a more thorough explanation. For simplicity, we contne our attention to systems with an interleaving semantics. That is, we assume that there is some set of atomic actions. We want to be able to express varying granularities of atomic actions. <p> Hence, we need a proof system. Furthermore, we would like to reuse proofs, as much as possible, when the corresponding code is reused. We have developed a proof theory for our model which enables us to achieve these goals <ref> [59] </ref> by adapting existing work on compositional proof systems to the model. We adopt the rely/guarantee style of properties. That is, an object or program guarantees to provide a certain property only when it can rely on the environment to provide another property. <p> See the attached paper <ref> [59] </ref> for more details. The proof theory could have been developed using any one of a number of formalisms. Since our model has elements of both message passing and shared memory in it, a formalism that concentrates on only one or the other may prove awkward for us.
Reference: [60] <author> Jerry James and Ambuj K. Singh. </author> <title> Fault tolerance bounds for memory consistency. </title> <note> To appear in WDAG, </note> <year> 1997. </year>
Reference-contexts: In Section 7, we discuss related work. Finally, we make some concluding remarks. A specitcation of the proposed system is found in the Appendix. There are also three attached papers detailing accomplished research, corresponding to [58], <ref> [60] </ref>, and [59]. 2 Model Concurrent objects are of interest for several reasons. First, many resources are available only in certain geographical places, or on specitc nodes of a network. Encapsulating such resources as objects is a convenient and intuitive way of abstracting them. <p> Note that this refers to the failure of any t nodes. No node is considered to be reliable in this model. In <ref> [60] </ref> we showed that the Read and Write operations in any implementation of atomic shared memory can withstand the failure of only a minority of the nodes. <p> While linearizability and sequential consistency provide the clearest semantics for the programmer, they are also limited in their achievable eciency [20] and fault tolerance <ref> [60, 73] </ref>. The weaker consistency conditions can be implemented more eciently and more robustly, but suffer from less clear semantics. <p> Lencevicius and Singh [73] give a lower bound on the latencies suered by implementations of linearizability and sequential consistency, which is strengthened by us <ref> [60] </ref> to tight bounds on the fault tolerance of several consistency conditions in a totally asynchronous setting. The Condor project [81] provides checkpointing and recovery in distributed systems. Since a process can be checkpointed, killed, and then recovered on a dierent machine, Condor also supports process migration.
Reference: [61] <author> Prasad Jayanti. </author> <title> On the robustness of Herlihy's hierarchy. </title> <booktitle> In PODC '93, </booktitle> <pages> pages 14557, </pages> <address> Ithaca, NY, USA, </address> <month> 1518 August </month> <year> 1993. </year> <booktitle> ACM SIGACT, ACM SIGOPS, </booktitle> <publisher> ACM Press. </publisher>
Reference-contexts: We have constructed a t-resilient hierarchy. An object at level i of this hierarchy can be implemented so that every method on it is at least i-resilient. We have proved certain relationships between this hierarchy and Jayanti's wait-free hierarchy h r m <ref> [61] </ref> (which is based on Herlihy's consensus hierarchy [51]). We can show, for example, that every object with consensus number 1 is 0-resilient. We have identited an object, which we call the View object, that is very useful in constructing these proofs.
Reference: [62] <author> Ranjit John and Mustaque Ahamad. </author> <title> Evaluation of causal distributed shared memory for data-race-free programs. </title> <type> Technical Report GIT-CC94/34, </type> <institution> College of Computing, Georgia Institute of Technology, Atlanta, Georgia 303320280, </institution> <year> 1994. </year>
Reference-contexts: In [7], Adve and Hill unify previous work on DRF0, release consistency, and the VAX memory model to produce DRF1, the weakest consistency condition for which data-race free programs behave as though on sequentially consistent memory. Other results obtained in this area are as follows. John and Ahamad <ref> [62] </ref> showed that data-race-free programs execute on causal memory as though on sequentially consistent memory. Raynal and Schiper [97] strengthened the result on data-race free programs for causal memory by showing other classes of programs that also execute on causal memory as though on sequentially consistent memory.
Reference: [63] <author> L. V. Kale and Sanjeev Krishnan. CHARM++: </author> <title> A portable concurrent object-oriented system based on C++. </title> <editor> In Andreas Paepcke, editor, </editor> <booktitle> OOPSLA '93, volume 28 of SIGPLAN Notices, </booktitle> <pages> pages 91108, </pages> <address> Washington, DC, USA, </address> <month> 26 September1 October </month> <year> 1993. </year> <booktitle> ACM SIGPLAN, </booktitle> <publisher> ACM Press. </publisher> <pages> 19 </pages>
Reference-contexts: It supports active objects, remote method invocation, futures, and parametric shared regions. Any object can be marked as shared, and then becomes visible to all processes. However, the user has to follow certain locking rules for sequential consistency to be guaranteed. 10 The CHARM++ language <ref> [63] </ref> detnes computation in terms of a basic unit, the chare. Each chare can be located on a specitc node, or can be a branch oce chare, with local components on each node.
Reference: [64] <author> Pete Keleher, Sandhya Dwarkadas, Alan Cox, and Willy Zwaenepoel. TreadMarks: </author> <title> Dis--tributed shared memory on standard workstations and operating systems. </title> <booktitle> In USENIX Winter '94, </booktitle> <pages> pages 11531, </pages> <address> San Francisco, CA, USA, </address> <month> 1721 January </month> <year> 1994. </year> <institution> USENIX Association. </institution>
Reference-contexts: Gibbons and Korach [45] give a method for online testing of a shared 13 memory implementation. Systems supporting multiple consistency conditions were described by Agrawal et al. [10, 9], Carter et al. [30], Keleher et al. <ref> [64] </ref>, and Kohli et al. [65]. Attiya et al. considered the eect of letting a single node issue multiple shared memory operations in parallel [16]. A description of the DEC Alpha's memory consistency model is given by Attiya and Friedman in [18].
Reference: [65] <author> Prince Kohli, Mustaque Ahamad, and Karsten Schwan. Indigo: </author> <title> User-level support for building distributed shared abstractions. </title> <journal> Concurrency: Practice and Experience, </journal> <note> 1997. To appear. </note>
Reference-contexts: Gibbons and Korach [45] give a method for online testing of a shared 13 memory implementation. Systems supporting multiple consistency conditions were described by Agrawal et al. [10, 9], Carter et al. [30], Keleher et al. [64], and Kohli et al. <ref> [65] </ref>. Attiya et al. considered the eect of letting a single node issue multiple shared memory operations in parallel [16]. A description of the DEC Alpha's memory consistency model is given by Attiya and Friedman in [18]. Chandra et al. describe a language for writing memory consistency protocols [32].
Reference: [66] <author> Leonidas I. Kontothanassis, Robert W. Wisniewski, and Michael L. Scott. </author> <title> Scheduler-conscious synchronization. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 15(1):340, </volume> <month> February </month> <year> 1997. </year>
Reference-contexts: We will experiment with a variety of techniques, to see how they perform under varying network conditions and application domains. We are especially interested in techniques that take into account other characteristics of the system, such as the thread scheduler and the consistency and fault tolerance mechanisms. (See <ref> [66] </ref> for synchronization algorithms that interact with the thread scheduler.) 8 The evaluation of guards is another problem. In a typical stack-based implementation of a language, the local variables of a method are placed in a certain stack frame.
Reference: [67] <author> Martha J. Kosa. </author> <title> Making operations of concurrent data types fast. </title> <booktitle> In Ko94 [5], </booktitle> <pages> pages 3241. </pages>
Reference-contexts: Chandra et al. describe a language for writing memory consistency protocols [32]. Dill et al. developed a formal language for describing memory consistency conditions [39]. Friedman [43] showed that some powerful synchronization operations greatly simplify the task of implementing hybrid consistency. Kosa <ref> [67] </ref> analyzed the network latencies that must be suered by linearizability, sequential consistency, and hybrid consistency in a partially synchronous setting. 7.4 Fault Tolerance The notion of t-resilience, the ability to withstand t node failures, is explored in various contexts by Dolev, Dwork and Stockmeyer [40] (consensus protocols), Borowsky and Gafni
Reference: [68] <author> David Kranz, Kirk Johnson, Anant Agarwal, John Kubiatowicz, and Beng-Hong Lim. </author> <title> Integrating message-passing and shared-memory: Early experience. </title> <type> Technical Report MIT/LCS/TM478, </type> <institution> Laboratory for Computer Science, MIT, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: We model shared memory systems by having a collection of ReadWrite objects. The abstract state of each such object consists of an integer or machine word. Other kinds of systems can also be modeled, including hybrids of the message-passing and shared memory paradigms such as those described in <ref> [42, 50, 54, 68, 75] </ref>. The proposed research is to investigate the use of compositional techniques in constructing transparently fault-tolerant distributed object systems and proving properties of objects in such a system. In particular, the following points will be addressed: * Compositional reasoning techniques for concurrent objects.
Reference: [69] <author> Leslie Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 28(9):6901, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: A DSM system provides a contract to the programmer in the form of a consistency condition, which is a guarantee on how the views of the shared memory seen by each node relate to each other. The conditions of atomic memory [70, 71] (or linearizability [53]) and sequential consistency <ref> [69] </ref> require the shared memory to behave like a single memory module, where every operation is atomic. Weaker conditions have also been proposed, such as processor consistency [12], causal memory [13], release consistency [44], pipelined RAM [78], hybrid consistency [17], and mixed consistency [11]. <p> However, we have been able to show that the culprit is actually their weak composition rule. Collette [36] shows how to adapt Abadi and Lamport's logic-independent composition rule to the UNITY logic. 7.3 Shared Memory Consistency Conditions The foundational paper for this area is Lamport's detnition of sequential consistency <ref> [69] </ref>. The stronger notion of atomic memory was explored in [70, 71]. Atomic memory was generalized to objects and named linearizability by Herlihy and Wing in [53].
Reference: [70] <author> Leslie Lamport. </author> <title> On interprocess communication, part I: Basic formalism. </title> <booktitle> Distributed Computing, </booktitle> <address> 1(2):7785, </address> <month> April </month> <year> 1986. </year>
Reference-contexts: A DSM system provides a contract to the programmer in the form of a consistency condition, which is a guarantee on how the views of the shared memory seen by each node relate to each other. The conditions of atomic memory <ref> [70, 71] </ref> (or linearizability [53]) and sequential consistency [69] require the shared memory to behave like a single memory module, where every operation is atomic. <p> Collette [36] shows how to adapt Abadi and Lamport's logic-independent composition rule to the UNITY logic. 7.3 Shared Memory Consistency Conditions The foundational paper for this area is Lamport's detnition of sequential consistency [69]. The stronger notion of atomic memory was explored in <ref> [70, 71] </ref>. Atomic memory was generalized to objects and named linearizability by Herlihy and Wing in [53]. They also outlined a proof methodology for linearizable objects, and proved that linearizability is a local property (i.e., a collection of linearizable objects forms a linearizable system).
Reference: [71] <author> Leslie Lamport. </author> <title> On interprocess communication, part II: Algorithms. </title> <booktitle> Distributed Computing, </booktitle> <address> 1(2):86101, </address> <month> April </month> <year> 1986. </year>
Reference-contexts: A DSM system provides a contract to the programmer in the form of a consistency condition, which is a guarantee on how the views of the shared memory seen by each node relate to each other. The conditions of atomic memory <ref> [70, 71] </ref> (or linearizability [53]) and sequential consistency [69] require the shared memory to behave like a single memory module, where every operation is atomic. <p> Collette [36] shows how to adapt Abadi and Lamport's logic-independent composition rule to the UNITY logic. 7.3 Shared Memory Consistency Conditions The foundational paper for this area is Lamport's detnition of sequential consistency [69]. The stronger notion of atomic memory was explored in <ref> [70, 71] </ref>. Atomic memory was generalized to objects and named linearizability by Herlihy and Wing in [53]. They also outlined a proof methodology for linearizable objects, and proved that linearizability is a local property (i.e., a collection of linearizable objects forms a linearizable system).
Reference: [72] <author> Leslie Lamport. </author> <title> The temporal logic of actions. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 16(3):872923, </volume> <month> May </month> <year> 1994. </year>
Reference-contexts: Since our model has elements of both message passing and shared memory in it, a formalism that concentrates on only one or the other may prove awkward for us. For example, I/O Automata [84, 85] have the avor of message passing and the Temporal Logic of Actions (TLA) <ref> [72] </ref> has the avor of shared memory. Due to the power of the composition rule in TLA, we chose it for our underlying formalism. <p> Hoare [55] introduced CSP, which features a guarded statement semantics, and a rendezvous mechanism for process communication. Chandy and Misra describe UNITY in [35]. This system is based on shared variables and assignment statements. Lamport detned the Temporal Logic of Actions (TLA) in <ref> [72] </ref>. It is based on shared variables and actions, events that change state. Chu spaces, an automaton-based approach, are described by Pratt and Gupta in [48, 49]. Lynch et al. describe I/O Automata in [84, 85].
Reference: [73] <author> Raimondas Lencevicius and Ambuj K. Singh. </author> <title> Latency bounds for memory consistency. </title> <note> Submitted for publication, </note> <year> 1997. </year>
Reference-contexts: While linearizability and sequential consistency provide the clearest semantics for the programmer, they are also limited in their achievable eciency [20] and fault tolerance <ref> [60, 73] </ref>. The weaker consistency conditions can be implemented more eciently and more robustly, but suffer from less clear semantics. <p> Attiya et al. [15] give a fault-tolerant implementation of 1-reader mul-tiwriter atomic registers on top of a message-passing system (Attiya also provided an alternate implementation [14]). Lencevicius and Singh <ref> [73] </ref> give a lower bound on the latencies suered by implementations of linearizability and sequential consistency, which is strengthened by us [60] to tight bounds on the fault tolerance of several consistency conditions in a totally asynchronous setting. The Condor project [81] provides checkpointing and recovery in distributed systems.
Reference: [74] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4):32159, </volume> <month> November </month> <year> 1989. </year>
Reference-contexts: In the shared memory paradigm, the system automatically handles the sharing of data between nodes, and can automatically provide some forms of error recovery as well. We are particularly interested in Distributed Shared Memory (DSM) <ref> [74] </ref>, which is a software layer over a distributed (message-passing) system and provides the shared memory abstraction to application programs. <p> They determined that under certain conditions, sequential consistency can be implemented more eciently than linearizability. They also derived a tight lower bound on the number of network latencies implementations of the two conditions must suer. Li and Hudak <ref> [74] </ref> consider several problems that arise when attempting to eciently implement sequentially consistent shared memory. Weak shared memories were devised to increase opportunities for optimizations. A very weak condition, pipelined RAM, is detned by Lipton and Sandberg in [78]. In [56], Hutto and Ahamad detne slow memory.
Reference: [75] <author> Xi Li, Sotirios G. Ziavras, and Constantine N. Manikopoulos. </author> <title> Parallel DSP algorithms on TurboNet: An experimental system with hybrid message-passing/shared-memory architecture. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 8(5):387411, </volume> <month> June </month> <year> 1996. </year>
Reference-contexts: We model shared memory systems by having a collection of ReadWrite objects. The abstract state of each such object consists of an integer or machine word. Other kinds of systems can also be modeled, including hybrids of the message-passing and shared memory paradigms such as those described in <ref> [42, 50, 54, 68, 75] </ref>. The proposed research is to investigate the use of compositional techniques in constructing transparently fault-tolerant distributed object systems and proving properties of objects in such a system. In particular, the following points will be addressed: * Compositional reasoning techniques for concurrent objects.
Reference: [76] <author> Wen-Yew Liang, Chun-Ta King, and Feipei Lai. Adsmith: </author> <title> An ecient object-based distributed shared memory system on PVM. </title> <editor> In Guo-Jie Li, D. F. Hsu, S. Horiguchi, and B. Maggs, editors, </editor> <volume> I-SPAN '96, </volume> <pages> pages 1739, </pages> <address> Peking, China, </address> <month> 1214 June </month> <year> 1996. </year> <title> Chinese National Research Center for Intelligent Computing Systems (NCIC), </title> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: All method calls are synchronous; new threads of control arise by creating a new active object. A garbage collector reclaims unreferenced objects. There is no notion of shared state; all communication is by method calls on objects. Adsmith <ref> [76] </ref> is an object-based distributed shared memory system built on PVM and programmed in C++. Each object can have several attributes set at object creation time, including cached/non-cached, invalidate/update, array distribution, and nonblocking accesses. Processes are considered as separate entities from objects.
Reference: [77] <author> Tim Lindholm and Frank Yellin. </author> <title> The Java Virtual Machine Specitcation. The Java Series. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, USA, </address> <year> 1997. </year>
Reference-contexts: For convenience, we consider the various telds separately. 7.1 Concurrent Object-Oriented Languages and Systems The Java language [47] and system <ref> [77] </ref> has deep roots in C++, but adds many new features. The bytecoded nature of the language is intended to enhance portability. Mutual exclusion is supported through the synchronized label; for each object, at most one thread is executing in a synchronized method at any given time.
Reference: [78] <author> Richard J. Lipton and Jonathan S. Sandberg. </author> <title> PRAM: A scalable shared memory. </title> <type> Technical Report CS-TR18088, </type> <institution> Department of Computer Science, Princeton University, Princeton, </institution> <address> NJ 08544, </address> <month> September </month> <year> 1988. </year> <month> 20 </month>
Reference-contexts: The conditions of atomic memory [70, 71] (or linearizability [53]) and sequential consistency [69] require the shared memory to behave like a single memory module, where every operation is atomic. Weaker conditions have also been proposed, such as processor consistency [12], causal memory [13], release consistency [44], pipelined RAM <ref> [78] </ref>, hybrid consistency [17], and mixed consistency [11]. Each such condition corresponds to some optimization that can lead to non-sequential behavior. As some have observed (e.g., [37]) the message-passing and shared memory paradigms can be united by introducing objects. <p> Li and Hudak [74] consider several problems that arise when attempting to eciently implement sequentially consistent shared memory. Weak shared memories were devised to increase opportunities for optimizations. A very weak condition, pipelined RAM, is detned by Lipton and Sandberg in <ref> [78] </ref>. In [56], Hutto and Ahamad detne slow memory. Goodman detned cache consistency (also called coherence) and processor consistency in [46]. However, his detnition of processor consistency was understood dierently by various researchers, as noted by Ahamad et al. in [12].
Reference: [79] <author> B. Liskov, A. Adya, M. Castro, M. Day, S. Ghemawat, R. Gruber, U. Maheshwari, A. C. Myers, and L. Shrira. </author> <title> Safe and ecient sharing of persistent objects in Thor. </title> <editor> In H. V. Jagadish and Inderpal Singh Mumick, editors, </editor> <booktitle> SIGMOD '96: Management of Data, volume 25 of SIGMOD Record, </booktitle> <pages> pages 31829, </pages> <address> Montreal, Quebec, Canada, </address> <month> 46 June </month> <year> 1996. </year> <booktitle> ACM SIGMOD, </booktitle> <publisher> ACM Press. </publisher>
Reference-contexts: Load sharing takes place through randomized work stealing, in which a node with nothing to do asks a random neighbor for a task. There is an online data-race detector. The system guarantees DAG consistency [25] at a minimum, and something stronger if system support is available. Thor <ref> [79] </ref> is a distributed object-oriented database. It provides objects to applications written in dierent programming languages. Access to object state can occur only through method calls. Techniques for safely sharing objects between clients are explored.
Reference: [80] <author> Barbara Liskov, Dorothy Curtis, Mark Day, Sanjay Ghemawat, Robert Gruber, Paul John-son, and Andrew C. Myers. </author> <title> Theta Reference Manual. Programming Methodology Group, </title> <institution> MIT Laboratory for Computer Science, </institution> <address> Cambridge, MA 02139, </address> <note> preliminary version edition, </note> <month> February </month> <year> 1995. </year>
Reference-contexts: Thor [79] is a distributed object-oriented database. It provides objects to applications written in dierent programming languages. Access to object state can occur only through method calls. Techniques for safely sharing objects between clients are explored. Thor is supported by the Theta object-oriented language <ref> [80] </ref>, a sequential, strongly-typed language. 7.2 Proof Systems A number of approaches have been taken to constructing concurrent proof systems. A partial list follows. Owicki and Gries [93] give an axiomatic approach. Hoare [55] introduced CSP, which features a guarded statement semantics, and a rendezvous mechanism for process communication.
Reference: [81] <author> Michael Litzkow, Todd Tannenbaum, Jim Basney, and Miron Livny. </author> <title> Checkpoint and migration of Unix processes in the Condor distributed processing system. </title> <type> Technical Report CS-TR97 1346, </type> <institution> Computer Sciences Department, University of Wisconsin-Madison, Madison, Wisconsin 53706, </institution> <month> April </month> <year> 1997. </year>
Reference-contexts: Lencevicius and Singh [73] give a lower bound on the latencies suered by implementations of linearizability and sequential consistency, which is strengthened by us [60] to tight bounds on the fault tolerance of several consistency conditions in a totally asynchronous setting. The Condor project <ref> [81] </ref> provides checkpointing and recovery in distributed systems. Since a process can be checkpointed, killed, and then recovered on a dierent machine, Condor also supports process migration. It farms out jobs to idle machines, so that workstation power is not wasted.
Reference: [82] <author> Wai-Kau Lo. </author> <title> More on t-resilience vs. wait-freedom (extended abstract). </title> <booktitle> In PODC '95, </booktitle> <pages> pages 1109, </pages> <address> Ottawa, Ontario, Canada, </address> <year> 2023 </year> <month> August </month> <year> 1995. </year> <booktitle> ACM SIGACT, ACM SIGOPS, </booktitle> <publisher> ACM Press. </publisher>
Reference-contexts: partially synchronous setting. 7.4 Fault Tolerance The notion of t-resilience, the ability to withstand t node failures, is explored in various contexts by Dolev, Dwork and Stockmeyer [40] (consensus protocols), Borowsky and Gafni [28] (asynchronous tasks), Herlihy and Shavit [52] (asynchronous tasks), Chandra et al. [33] (shared objects), and Lo <ref> [82] </ref> (shared objects). Attiya et al. [15] give a fault-tolerant implementation of 1-reader mul-tiwriter atomic registers on top of a message-passing system (Attiya also provided an alternate implementation [14]).
Reference: [83] <author> Nancy A. Lynch. </author> <title> Distributed Algorithms. Morgan Kaufmann Series in Data Management Systems. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA, </address> <year> 1996. </year>
Reference-contexts: Rich telds of research have arisen from the study of these problems (see <ref> [83] </ref> for an overview). Programming paradigms for concurrent systems oer various tradeos. In the message-passing paradigm, each node has a name, and a mechanism exists for transmitting data to a named destination. The programmer uses these mechanisms to send messages directly between nodes. However, shared data must be explicitly managed.
Reference: [84] <author> Nancy A. Lynch and Frits Vaandrager. </author> <title> Forward and backward simulations, I: </title> <journal> Untimed systems. Information and Computation, </journal> <volume> 121(2):21433, </volume> <month> September </month> <year> 1995. </year>
Reference-contexts: The proof theory could have been developed using any one of a number of formalisms. Since our model has elements of both message passing and shared memory in it, a formalism that concentrates on only one or the other may prove awkward for us. For example, I/O Automata <ref> [84, 85] </ref> have the avor of message passing and the Temporal Logic of Actions (TLA) [72] has the avor of shared memory. Due to the power of the composition rule in TLA, we chose it for our underlying formalism. <p> Lamport detned the Temporal Logic of Actions (TLA) in [72]. It is based on shared variables and actions, events that change state. Chu spaces, an automaton-based approach, are described by Pratt and Gupta in [48, 49]. Lynch et al. describe I/O Automata in <ref> [84, 85] </ref>. Milner describes CCS [87], a process algebra, and later developed the -calculus [88, 89], a formalism based on the idea of naming. The ability to prove properties of the composition of smaller systems is considered in various works.
Reference: [85] <author> Nancy A. Lynch and Frits Vaandrager. </author> <title> Forward and backward simulations, II: </title> <journal> Timing-based systems. Information and Computation, </journal> <volume> 128(1):125, </volume> <month> July </month> <year> 1996. </year>
Reference-contexts: The proof theory could have been developed using any one of a number of formalisms. Since our model has elements of both message passing and shared memory in it, a formalism that concentrates on only one or the other may prove awkward for us. For example, I/O Automata <ref> [84, 85] </ref> have the avor of message passing and the Temporal Logic of Actions (TLA) [72] has the avor of shared memory. Due to the power of the composition rule in TLA, we chose it for our underlying formalism. <p> Lamport detned the Temporal Logic of Actions (TLA) in [72]. It is based on shared variables and actions, events that change state. Chu spaces, an automaton-based approach, are described by Pratt and Gupta in [48, 49]. Lynch et al. describe I/O Automata in <ref> [84, 85] </ref>. Milner describes CCS [87], a process algebra, and later developed the -calculus [88, 89], a formalism based on the idea of naming. The ability to prove properties of the composition of smaller systems is considered in various works.
Reference: [86] <author> Rajit Manohar and Paolo A. G. Sivilotti. </author> <title> Composing processes using modited rely-guarantee specitcations. </title> <type> Technical Report CS-TR9622, </type> <institution> California Institute of Technology, Pasadena, </institution> <address> CA 91125, </address> <month> 12 June </month> <year> 1996. </year>
Reference-contexts: However, that rule can only compose pure safety properties (i.e., only a property that is its own closure may be composed). Other works in this area are as follows. Manohar and Sivilotti <ref> [86] </ref> detne modited rely-guarantee properties, which are simply properties that refer to only local variables. They give a composition 12 rule for such properties and show that it is unable to prove properties of a gossip algorithm. They blame this on the modited rely-guarantee properties.
Reference: [87] <author> Robin Milner. </author> <title> A Calculus of Communicating Systems, </title> <booktitle> volume 92 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, USA, </address> <year> 1980. </year>
Reference-contexts: Lamport detned the Temporal Logic of Actions (TLA) in [72]. It is based on shared variables and actions, events that change state. Chu spaces, an automaton-based approach, are described by Pratt and Gupta in [48, 49]. Lynch et al. describe I/O Automata in [84, 85]. Milner describes CCS <ref> [87] </ref>, a process algebra, and later developed the -calculus [88, 89], a formalism based on the idea of naming. The ability to prove properties of the composition of smaller systems is considered in various works.
Reference: [88] <author> Robin Milner, Joachim Parrow, and David Walker. </author> <title> A calculus of mobile processes. I. Information and Computation, </title> <address> 100(1):140, </address> <month> September </month> <year> 1992. </year>
Reference-contexts: It is based on shared variables and actions, events that change state. Chu spaces, an automaton-based approach, are described by Pratt and Gupta in [48, 49]. Lynch et al. describe I/O Automata in [84, 85]. Milner describes CCS [87], a process algebra, and later developed the -calculus <ref> [88, 89] </ref>, a formalism based on the idea of naming. The ability to prove properties of the composition of smaller systems is considered in various works. Barringer, Kuiper and Pnueli [22] give a complete composition rule for mixed (i.e., not rely-guarantee) properties.
Reference: [89] <author> Robin Milner, Joachim Parrow, and David Walker. </author> <title> A calculus of mobile processes. II. Information and Computation, </title> <address> 100(1):4177, </address> <month> September </month> <year> 1992. </year>
Reference-contexts: It is based on shared variables and actions, events that change state. Chu spaces, an automaton-based approach, are described by Pratt and Gupta in [48, 49]. Lynch et al. describe I/O Automata in [84, 85]. Milner describes CCS [87], a process algebra, and later developed the -calculus <ref> [88, 89] </ref>, a formalism based on the idea of naming. The ability to prove properties of the composition of smaller systems is considered in various works. Barringer, Kuiper and Pnueli [22] give a complete composition rule for mixed (i.e., not rely-guarantee) properties.
Reference: [90] <author> Jayadev Misra. </author> <title> New UNITY. </title> <note> Unpublished book. </note>
Reference-contexts: However, the rule is extremely dicult to apply in practice. Pnueli [95] later gave an incomplete rule for rely-guarantee properties, then gave a rule for a mixed properties when each variable is modited by at most one process. Misra <ref> [90] </ref> considers mixed properties trst, then rely-guarantee (conditional) properties. He gives a complete rule for composition in the context of the UNITY formalism.
Reference: [91] <author> Rishiyur S. Nikhil. Cid: </author> <title> A parallel shared-memory C for distributed memory machines. </title> <editor> In K. Pingali et al., editor, </editor> <booktitle> Languages and Compilers for Parallel Computing '94, volume 892 of Lecture Notes in Computer Science, </booktitle> <pages> pages 36790, </pages> <address> Ithaca, NY, USA, </address> <month> 810 August </month> <year> 1994. </year> <note> Springer-Verlag. 21 </note>
Reference-contexts: It supports loop-level parallelism and a parallel version of the C++ STL. The designers intend to support parallel active objects as well. A CORBA interface is planned. The underlying model is shared memory. The Cid system <ref> [91] </ref> is based on a shared memory model. It extends C by providing MIMD threads plus shared passive objects. Sequential consistency is maintained through a multiple-reader single-writer semantics. Arbitrary thred forking is supported. As in our model, a thread is created through an asynchronous method call.
Reference: [92] <author> William O'Farrell, Frank Eigler, Ivan Kalas, and Gregory Wilson. </author> <title> ABC++: An Introduction to the IBM Parallel Class Library for C++. </title> <institution> IBM, </institution> <year> 1995. </year>
Reference-contexts: Each main process can be treated as an object. It is based on a shared memory model, and provides cache consistency. Currently, it compiles down to standard C++ with some library calls to implement the parallelism. The ABC++ language <ref> [92] </ref> is a class library for standard C++. It provides a run-time system for a uniform shared-data model on distributed systems and massively parallel computers. It supports active objects, remote method invocation, futures, and parametric shared regions.
Reference: [93] <author> Susan Owicki and David Gries. </author> <title> An axiomatic proof technique for parallel programs I. </title> <journal> Acta Informatica, </journal> <volume> 6(4):319340, </volume> <year> 1976. </year>
Reference-contexts: Techniques for safely sharing objects between clients are explored. Thor is supported by the Theta object-oriented language [80], a sequential, strongly-typed language. 7.2 Proof Systems A number of approaches have been taken to constructing concurrent proof systems. A partial list follows. Owicki and Gries <ref> [93] </ref> give an axiomatic approach. Hoare [55] introduced CSP, which features a guarded statement semantics, and a rendezvous mechanism for process communication. Chandy and Misra describe UNITY in [35]. This system is based on shared variables and assignment statements. Lamport detned the Temporal Logic of Actions (TLA) in [72].
Reference: [94] <author> G. D. Parrington, S. K. Shrivastava, S. M. Wheater, and M. C. </author> <title> Little. </title> <booktitle> The design and implementation of Arjuna. USENIX Computing Systems Journal, </booktitle> <address> 8(3):255308, </address> <month> Summer </month> <year> 1995. </year>
Reference-contexts: Condor would be an ideal substrate for our system, except for one detail. Condor provides checkpointing, recovery, and migration of processes. We need to deal with threads. Similar techniques will apply, but Condor is not directly usable for our needs. The Arjuna system <ref> [94] </ref> provides an object store on stable storage. The use of this store enables the programmer to produce fault tolerant distributed applications. Nested atomic transactions on abstract data types are provided by way of an RPC mechanism.
Reference: [95] <author> Amir Pnueli. </author> <title> In transition from global to modular temporal reasoning about programs. </title> <editor> In Krzysztof R. Apt, editor, </editor> <booktitle> Logics and Models of Concurrent Systems, volume 13 of NATO ASI Series F, </booktitle> <pages> pages 12344. </pages> <publisher> Springer-Verlag, </publisher> <address> Heidelberg, </address> <year> 1985. </year>
Reference-contexts: The ability to prove properties of the composition of smaller systems is considered in various works. Barringer, Kuiper and Pnueli [22] give a complete composition rule for mixed (i.e., not rely-guarantee) properties. However, the rule is extremely dicult to apply in practice. Pnueli <ref> [95] </ref> later gave an incomplete rule for rely-guarantee properties, then gave a rule for a mixed properties when each variable is modited by at most one process. Misra [90] considers mixed properties trst, then rely-guarantee (conditional) properties.
Reference: [96] <author> Michel Raynal and Masaaki Mizuno. </author> <title> How to tnd his way in the jungle of consistency criteria for distributed shared memories (or how to escape from Minos' labyrinth). </title> <booktitle> In 4th Workshop on Future Trends of Distributed Computing Systems, </booktitle> <pages> pages 3406, </pages> <address> Lisbon, Portugal, </address> <month> 2224 September </month> <year> 1993. </year> <booktitle> IEEE Computer Society Technical Committee on Distributed Processing, </booktitle> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: This topic will be discussed further in Section 6. 6 5 Consistency The plethora of shared memory consistency conditions mentioned in the introduction can be confusing (e.g., see <ref> [96] </ref>). While linearizability and sequential consistency provide the clearest semantics for the programmer, they are also limited in their achievable eciency [20] and fault tolerance [60, 73]. The weaker consistency conditions can be implemented more eciently and more robustly, but suffer from less clear semantics.
Reference: [97] <author> Michel Raynal and Andre Schiper. </author> <title> From causal consistency to sequential consistency in shared memory systems. </title> <editor> In P. S. Thiagarajan, editor, </editor> <booktitle> FST&TCS '95, volume 1026 of Lecture Notes in Computer Science, </booktitle> <pages> pages 18094, </pages> <address> Bangalore, India, </address> <month> 1820 December </month> <year> 1995. </year> <note> Springer-Verlag. </note>
Reference-contexts: Other results obtained in this area are as follows. John and Ahamad [62] showed that data-race-free programs execute on causal memory as though on sequentially consistent memory. Raynal and Schiper <ref> [97] </ref> strengthened the result on data-race free programs for causal memory by showing other classes of programs that also execute on causal memory as though on sequentially consistent memory. Adve et al. [8] demonstrated a method for detecting a data race during the execution of a program on weak memory.
Reference: [98] <author> Judi Romijn and Frits Vaandrager. </author> <title> A note on fairness in I/O automata. </title> <journal> Information Processing Letters, </journal> <volume> 59(5):24550, </volume> <month> September </month> <year> 1996. </year>
Reference-contexts: We give constructions for both a weakly fair scheduler (a continuously enabled thread eventually runs) and a strongly fair scheduler (an intnitely often enabled thread eventually runs). See also <ref> [98] </ref>, which considers fairness for I/O Automata. 3 Compositional Proofs One reason for introducing objects into a system is to enable code reuse. To achieve code reuse, we need to know the properties of an object. Hence, we need a proof system.
Reference: [99] <author> Ambuj K. Singh. </author> <title> A framework for programming with non-atomic memories. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 26(2):211224, </volume> <month> April </month> <year> 1995. </year>
Reference-contexts: The weaker consistency conditions can be implemented more eciently and more robustly, but suffer from less clear semantics. Programming methodologies and proof systems have been designed for the strong conditions, but are lacking for the weak conditions (but see <ref> [99] </ref> for program transformations that work around this problem), and some common programming methodologies cannot be supported on weak memories [19].
Reference: [100] <author> Eugene W. Stark. </author> <title> A proof technique for rely/guarantee properties. </title> <editor> In S. N. Maheshwari, editor, </editor> <booktitle> FST&TCS '85, volume 206 of Lecture Notes in Computer Science, </booktitle> <pages> pages 36991, </pages> <address> New Delhi, India, </address> <month> 1618 December </month> <year> 1985. </year> <note> Springer-Verlag. </note>
Reference-contexts: Misra [90] considers mixed properties trst, then rely-guarantee (conditional) properties. He gives a complete rule for composition in the context of the UNITY formalism. Stark <ref> [100] </ref> gives an incomplete rule for rely-guarantee properties that involves construction of a set of implications, to show that the processes do not interfere with each other. Abadi and Lamport give a composition rule in the context of TLA [2].
Reference: [101] <author> Bjarne Steensgaard and Eric Jul. </author> <title> Object and native code thread mobility among heterogeneous computers. </title> <booktitle> In SOSP '95, </booktitle> <pages> pages 6878, </pages> <address> Copper Mountain Resort, CO, USA, </address> <month> 36 December </month> <year> 1995. </year> <booktitle> ACM SIGOPS, </booktitle> <publisher> ACM Press. </publisher>
Reference-contexts: Arbitrary thred forking is supported. As in our model, a thread is created through an asynchronous method call. However, threads cannot migrate between machines. Threads are implemented entirely at user level. The coherence protocol is also entirely at user level. Emerald <ref> [101] </ref> is a strongly typed, pure object-oriented language. Both threads and objects can migrate between nodes, although threads only migrate when carried by a migrating object. Read-only objects can be declared immutable, which simplites sharing. There is both inter-object and intra-object concurrency.
Reference: [102] <author> Andrew S. Tanenbaum. </author> <title> The Amoeba distributed operating system. </title> <editor> In C. E. Vandoni, editor, </editor> <volume> CERN 9608, </volume> <pages> pages 10915, </pages> <address> Egmond aan Zee, The Netherlands, </address> <month> 821 September </month> <year> 1996. </year> <pages> CERN. </pages>
Reference-contexts: Objects exist on a single site, but threads can migrate. The underlying model is message passing. It is an extension of Modula-3. A guard statement exists in the language. Amoeba <ref> [102] </ref> is a complete Unix-like operating system for distributed systems. Objects in the Amoeba system have a txed location. Method calls are made via an RPC mechanism. Consistency is supported by totally-ordered, causally-ordered, and unordered group communication.
References-found: 102

