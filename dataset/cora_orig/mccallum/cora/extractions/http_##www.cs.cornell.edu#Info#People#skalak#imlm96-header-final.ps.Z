URL: http://www.cs.cornell.edu/Info/People/skalak/imlm96-header-final.ps.Z
Refering-URL: http://www.cs.cornell.edu/Info/People/skalak/
Root-URL: 
Email: skalak@cs.umass.edu  
Title: The Sources of Increased Accuracy for Two Proposed Boosting Algorithms  
Author: David B. Skalak 
Address: Amherst, MA 01003-4610  
Affiliation: Dept. of Computer Science University of Massachusetts  
Abstract: We introduce two boosting algorithms that aim to increase the generalization accuracy of a given classifier by incorporating it as a level-0 component in a stacked generalizer. Both algorithms construct a complementary level-0 classifier that can only generate coarse hypotheses for the training data. We show that the two algorithms boost generalization accuracy on a representative collection of data sets. The two algorithms are distinguished in that one of them modifies the class targets of selected training instances in order to train the complementary classifier. We show that the two algorithms achieve approximately equal generalization accuracy, but that they create complementary classifiers that display different degrees of accuracy and diversity. Our study provides evidence that it may be useful to investigate families of boosting algorithms that incorporate varying levels of accuracy and diversity, so as to achieve an appropriate mix for a given task and domain. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ali, K.M. and Pazzani, M.J. </author> <year> 1995. </year> <title> Error reduction through learning multiple descriptions. </title> <note> Machine Learning (To appear). Also published as Dept. of Information and Computer Science Technical Report 95-39, </note> <institution> University of California, </institution> <address> Irvine, CA. </address>
Reference: <author> Battiti, R. and Colla, A.M. </author> <year> 1994. </year> <title> Democracy in Neu--ral Nets: Voting Schemes for Classification. </title> <booktitle> Neural Networks 7 </booktitle> <pages> 691-707. </pages>
Reference: <author> Breiman, L. </author> <year> 1992. </year> <title> Stacked Regressions. </title> <type> Technical Report 367, </type> <institution> Department of Statistics, University of California, Berkeley, </institution> <address> CA. </address>
Reference: <author> Breiman, L. </author> <year> 1994a. </year> <title> Bagging predictors. </title> <type> Technical Report 421, </type> <institution> Department of Statistics, University of California, Berkeley, </institution> <address> CA. </address>
Reference-contexts: Breiman has shown that bootstrap aggregating (bagging) nearest neighbor classifiers in general will not be effective because they are stable: small perturbations in the training data will not change the hypothesis very much <ref> (Breiman 1994a) </ref>. Therefore we have adopted a policy of what might be called radical destabilization of the nearest neighbor algorithm, enforced by choosing a very small number of prototypes.
Reference: <author> Breiman, L. </author> <year> 1994b. </year> <title> NIPS*94 Tutorial, Statistics and Nets: Understanding Nonlinear Models from Their Linear Relatives. </title> <booktitle> Neural Information Processing Systems, 1994, </booktitle> <address> Denver, </address> <publisher> CO. </publisher>
Reference: <author> Cameron-Jones, M. </author> <year> 1995. </year> <title> Instance Selection by Encoding Length Heuristic with Random Mutation Hill Climbing. </title> <booktitle> In Proceedings of the Eighth Australian Joint Conference on Artificial Intelligence. World Scientific. </booktitle> <pages> 99-106. </pages>
Reference: <author> Chan, P.K. and Stolfo, S.J. </author> <year> 1993a. </year> <title> Experiments on multistrategy learning by meta-learning. </title> <booktitle> In Proceedings of the Second International Conference on Information and Knowledge Management. </booktitle> <pages> 314-323. </pages>
Reference: <author> Chan, P.K. and Stolfo, S.J. </author> <year> 1993b. </year> <title> Toward Parallel and Distributed Learning by Meta-Learning. </title> <booktitle> In Working Notes, AAAI Workshop on Knowledge Discovery in Databases, </booktitle> <address> San Mateo, CA. </address> <publisher> AAAI Press/MIT Press. </publisher> <pages> 227-240. </pages>
Reference: <author> Chan, P.K. and Stolfo, S.J. </author> <year> 1995. </year> <title> A Comparative Evaluation of Voting and Meta-Learning on Partitioned Data. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 90-98. </pages>
Reference: <author> Drucker, H. and Cortes, C. </author> <year> 1996. </year> <title> Boosting Decision Trees. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> 8. </volume> <publisher> MIT Press, </publisher> <address> Las Vegas, NV. </address>
Reference: <author> Drucker, H.; Cortes, C.; Jackel, L.D.; LeCun, Y.; and Vapnik, V. </author> <year> 1994. </year> <title> Boosting and other ensemble methods. </title> <booktitle> Neural Computation 6(6) </booktitle> <pages> 1289-1301. </pages>
Reference: <author> Freund, Y. and Schapire, R.E. </author> <year> 1995. </year> <title> A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting. </title> <booktitle> In Proceedings of the Second European Conference on Computational Learning Theory. </booktitle> <address> Barcelona, Spain. </address>
Reference: <author> Hansen, L.K. and Salamon, P. </author> <year> 1990. </year> <title> Neural Network Ensembles. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 12 </journal> <pages> 993-1001. </pages>
Reference: <author> Jacobs, R. A.; Jordan, M. I.; and Barto, A. G. </author> <year> 1991. </year> <title> Task Decomposition through Competition in a Modular Connectionist Architecture: The What and Where Vision Tasks. </title> <booktitle> Cognitive Science 15 </booktitle> <pages> 219-250. </pages>
Reference: <author> Kong, E.B. and Dietterich, T.G. </author> <year> 1995. </year> <title> Error-correcting output coding corrects bias and variance. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA. </address> <pages> 313-321. </pages>
Reference: <author> Krogh, A. and Vedelsby, J. </author> <year> 1995. </year> <title> Neural Network Ensembles, Cross Validation and Active Learning. </title> <editor> In Tesauro, G.; Touretzky, D.; and Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> 7. </volume> <publisher> MIT Press. </publisher>
Reference: <author> Maclin, R. and Shavlik, J.W. </author> <year> 1995. </year> <title> Combining the predictions of multiple classifiers: Using competitive learning to initialize neural networks. </title> <editor> In Mellish, C.S., editor, </editor> <booktitle> Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address> <pages> 524-530. </pages>
Reference: <author> Opitz, D.W. and Shavlik, J.W. </author> <year> 1995. </year> <title> Generating Accurate and Diverse Members of a Neural-Network Ensemble. </title> <editor> In Mozer, M.C. and Hasselmo, M.E., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> 8. </volume> <publisher> MIT Press. </publisher>
Reference: <author> Schapire, R.E. </author> <year> 1990. </year> <title> The Strength of Weak Learn-ability. </title> <booktitle> Machine Learning 5 </booktitle> <pages> 197-227. </pages>
Reference: <author> Selfridge, O. G. </author> <year> 1959. </year> <title> Pandemonium: A Paradigm for Learning. </title> <booktitle> In Proceedings of the Symposium on the Mechanization of Thought Processes, </booktitle> <address> Teddington, England. </address> <institution> National Physical Laboratory, H.M. Stationery Office, </institution> <address> London. </address> <pages> 511-529. </pages>
Reference: <author> Skalak, D. B. </author> <year> 1994. </year> <title> Prototype and Feature Selection by Sampling and Random Mutation Hill Climbing Algorithms. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 293-301. </pages>
Reference-contexts: We have previously shown that prototype sampling can effectively construct small, accurate nearest neighbor classifiers in some domains <ref> (Skalak 1994) </ref>. The algorithms are different in only one respect. The Deliberate Misclassifi-cation algorithm deliberately modifies the target class labels of selected instances in the training set, while the Coarse Reclassification algorithm does not.
Reference: <author> Skalak, D.B. </author> <year> 1995. </year> <title> Prototype selection for composite nearest neighbor classifiers. </title> <type> Technical Report 95-74, </type> <institution> Department of Computer Science, University of Mas-sachusetts, </institution> <address> Amherst, MA. </address>
Reference: <author> Skalak, D.B. </author> <year> 1996. </year> <title> Prototype Selection for Composite Nearest Neighbor Classifiers. </title> <type> Ph.D. Dissertation, </type> <institution> Dept. of Computer Science, University of Mas-sachusetts, </institution> <address> Amherst, MA (forthcoming). </address>
Reference-contexts: Both algorithms create a complementary level-0 classifier drawn from a model class of coarse-hypothesis or high-bias (Breiman 1994b; Kong and Dietterich paper. 1995) nearest neighbor classifiers that incorporate only one prototype for each class. These algorithms are described in detail in a longer piece <ref> (Skalak 1996) </ref>. <p> While not guaranteeing that only "helpful" instances will have their targets changed, altering class assignments near boundaries where the errors are asymmetric appears to be a useful heuristic <ref> (Skalak 1996) </ref>. By concentrating on asymmetric boundaries, the algorithm raises the possibility that a complementary classifier may correctly classify a larger number of instances at the expense of misclassifying a smaller number, resulting in a net gain in accuracy.
Reference: <author> Wolpert, D. </author> <year> 1992. </year> <title> Stacked Generalization. </title> <booktitle> Neural Networks 5 </booktitle> <pages> 241-259. </pages>
Reference-contexts: These negative results may be judged against Wolpert's observation that no generalizing scheme will always improve accuracy <ref> (Wolpert 1992, p.242) </ref>. Referring to Table 1, we see that these four data sets have the lowest average number of training instances per class.
Reference: <author> Zhang, X.; Mersirov, J.P.; and D.L.Waltz, </author> <year> 1992. </year> <title> A Hybrid System for Protein Secondary Structure Prediction. </title> <journal> Journal of Molecular Biology 225 </journal> <pages> 1049-1063. </pages>
Reference: <author> Zheng, Z. </author> <year> 1993. </year> <title> A Benchmark for Classifier Learning. </title> <journal> In Proceedings of the Sixth Australian Joint Conference on Artificial Intelligence. </journal> <note> World Scientific Publisher (An extended version is available as Technical Report TR474, </note> <institution> Basser Department of Computer Science, The University of Sydney, </institution> <note> available by anonymous ftp to ftp.cs.su.oz.au in the /pub/tr directory). 281-286. </note>
Reference-contexts: We ran these two algorithms on 13 data sets, selected mostly from the representative data sets for the U.C.I. Repository identified by Zheng <ref> (Zheng 1993) </ref>. Some characteristics of the training data are given in Table 1.
References-found: 26

