URL: http://www.cs.cornell.edu/Info/People/csun/papers/psqr.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/csun/sun.html
Root-URL: 
Email: csun@cs.cornell.edu  
Title: DISTRIBUTED-MEMORY MULTIPROCESSORS 1  
Author: Chunguang Sun 
Keyword: PARALLEL SPARSE ORTHOGONAL FACTORIZATION  Key words. parallel algorithms, sparse matrix, orthogonal factorization, multifrontal method, block partitioning scheme, distributed-memory multiprocessors  Abbreviated title: Parallel Sparse Orthogonal Factorization  
Web: 65F05, 65F50  
Address: Ithaca, NY 14853-3801  
Affiliation: Advanced Computing Research Institute Cornell Theory Center Cornell University  
Note: ON  AMS subject classifications.  
Abstract: This paper appears in SIAM Journal on Scientific Computing, Vol 17, No. 3, pp. 666-685, May 1996. Abstract In this paper, we propose a new parallel multifrontal algorithm for the orthogonal factorization of large sparse matrices on distributed-memory multiprocessors. We explore the use of block partitioning schemes in parallel sparse orthogonal factorization. Our block-oriented parallel algorithm for sparse orthogonal factorization achieves high performance by incurring strictly less communication overhead than the conventional non-block algorithm, maintaining relatively balanced load distribution among processors and accelerating the parallel numerical kernel via increased cache utilization. We analyze the performance of our parallel algorithm and present its arithmetic and communication complexities for regular grid problems. We report the experimental results of an implementation of our parallel algorithm on an Intel iPSC/860 machine. Through our theoretical analysis and experimental results, we demonstrate that our new block-oriented algorithm outperforms the conventional non-block algorithm impressively. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Ashcraft, </author> <title> A vector implementation of the multifrontal method for large sparse, symmetric positive definite linear systems, </title> <type> Tech. Report ECA-TR-51, </type> <institution> Boeing Computer Services, </institution> <address> Seattle, WA, </address> <year> 1987. </year>
Reference-contexts: Step 2 can be accomplished by using an algorithm described in [18]. The symbolic factorization step is discussed in [9]. There are several slightly different definitions of supernode <ref> [1, 2, 3, 10, 20] </ref>. Any of those definitions can be used in our approach. In an implementation of our algorithm, the fundamental supernode [2] is used. Let R j denote the structure of row j of R|i.e., the set of nonzero column indices in row j of R.
Reference: [2] <author> C. Ashcraft and R. G. Grimes, </author> <title> The influence of relaxed supernode partitions on the multifrontal method, </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 15 (1989), </volume> <pages> pp. 291-309. </pages>
Reference-contexts: Step 2 can be accomplished by using an algorithm described in [18]. The symbolic factorization step is discussed in [9]. There are several slightly different definitions of supernode <ref> [1, 2, 3, 10, 20] </ref>. Any of those definitions can be used in our approach. In an implementation of our algorithm, the fundamental supernode [2] is used. Let R j denote the structure of row j of R|i.e., the set of nonzero column indices in row j of R. <p> The symbolic factorization step is discussed in [9]. There are several slightly different definitions of supernode [1, 2, 3, 10, 20]. Any of those definitions can be used in our approach. In an implementation of our algorithm, the fundamental supernode <ref> [2] </ref> is used. Let R j denote the structure of row j of R|i.e., the set of nonzero column indices in row j of R.
Reference: [3] <author> C. Ashcraft, R. G. Grimes, J. G. Lewis, B. W. Peyton, and H. D. Simon, </author> <title> Progress in sparse matrix methods for large linear systems on vector supercomputers, </title> <journal> Int. J. Supercomputer Appl., </journal> <volume> 1 (1987), </volume> <pages> pp. 10-30. </pages>
Reference-contexts: Step 2 can be accomplished by using an algorithm described in [18]. The symbolic factorization step is discussed in [9]. There are several slightly different definitions of supernode <ref> [1, 2, 3, 10, 20] </ref>. Any of those definitions can be used in our approach. In an implementation of our algorithm, the fundamental supernode [2] is used. Let R j denote the structure of row j of R|i.e., the set of nonzero column indices in row j of R.
Reference: [4] <author> A. Bj orck, </author> <title> Stability analysis of the method of seminormal equations for linear least squares problems, </title> <journal> Linear Algebra Appl., </journal> <volume> 88/89 (1987), </volume> <pages> pp. 31-48. </pages>
Reference-contexts: This approach is numerically backward stable. However, the right hand side vector must be available before A is factorized. The method of corrected semi-normal equations (CSNE) proposed by Bjorck <ref> [4] </ref> can be used to handle new right sides without storing Q. Bjorck [4] has shown that the CSNE method is, in general, as accurate as the QR factorization method. However, it is not always backward stable and may not be accurate for "stiff" problems. <p> This approach is numerically backward stable. However, the right hand side vector must be available before A is factorized. The method of corrected semi-normal equations (CSNE) proposed by Bjorck <ref> [4] </ref> can be used to handle new right sides without storing Q. Bjorck [4] has shown that the CSNE method is, in general, as accurate as the QR factorization method. However, it is not always backward stable and may not be accurate for "stiff" problems.
Reference: [5] <author> E. Chu and J. A. George, </author> <title> Sparse orthogonal decomposition on a hypercube multiprocessor, </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 11 (1990), </volume> <pages> pp. 453-465. </pages>
Reference-contexts: Usually Q is not formed explicitly. A row merging scheme and a general row merging scheme for sparse QR factorization are proposed in [8] and [19], respectively. Sequential algorithms for sparse QR factorization are also considered in [11, 17, 22, 23]. Parallel algorithms are described in <ref> [5, 24, 27] </ref> for implementing the numeric phase of the general row merging scheme [19] on MIMD distributed-memory multiprocessors. A parallel algorithm is discussed in [16] for implementing the numeric phase of the row merging scheme [8] on SIMD architecture. <p> A parallel algorithm is discussed in [16] for implementing the numeric phase of the row merging scheme [8] on SIMD architecture. Previously-proposed parallel sparse QR factorization algorithms <ref> [5, 16, 24, 27] </ref> are based on the non-block or single row partition of the dense matrices involved in the numerical computation. Block-oriented approach to parallel sparse QR factorization has not been explored before. <p> Our block-oriented approach to parallel sparse QR factorization achieves significant improvement in performance over the conventional non-block approach described in <ref> [5] </ref>. In x2, a multifrontal sparse QR factorization scheme is reviewed. A new parallel multifrontal algorithm for sparse QR factorization is proposed in x3. The complexity analysis of our algorithm for regular grid problems is presented in x4. Experimental results are discussed in x5. <p> The numeric phase of a sequential multifrontal sparse QR factorization algorithm 3. A parallel multifrontal sparse QR factorization algorithm. Several parallel algorithms have been described in <ref> [5, 24, 27] </ref> for implementing the numeric phase of sparse QR factor ization on distributed-memory multiprocessors. * In the algorithm discussed in [5], a frontal matrix F is mapped to a ring of processors by assigning the rows of F to the processors in a wrap-around manner. <p> The numeric phase of a sequential multifrontal sparse QR factorization algorithm 3. A parallel multifrontal sparse QR factorization algorithm. Several parallel algorithms have been described in [5, 24, 27] for implementing the numeric phase of sparse QR factor ization on distributed-memory multiprocessors. * In the algorithm discussed in <ref> [5] </ref>, a frontal matrix F is mapped to a ring of processors by assigning the rows of F to the processors in a wrap-around manner. An update matrix U which needs to be merged into F is shifted among the ring of processors in a circular fashion. <p> Because of the message start-up time, the overhead associated with a short message is proportionally higher than that associated with a long message. To reduce communication costs, a block approach is suggested as a future research topic in <ref> [5] </ref>. A frontal matrix is partitioned into blocks with same number of rows. If F is an h fi h upper triangular matrix and is mapped to p processors, block size equal to h=p 2 is recommended in [5]. <p> costs, a block approach is suggested as a future research topic in <ref> [5] </ref>. A frontal matrix is partitioned into blocks with same number of rows. If F is an h fi h upper triangular matrix and is mapped to p processors, block size equal to h=p 2 is recommended in [5]. As shown in x5, the communication volume of this block approach is still relatively high. To the best of our knowledge, experimental results of the row-oriented approach and the block-oriented approach described in [5] have not been reported in the literature. <p> matrix and is mapped to p processors, block size equal to h=p 2 is recommended in <ref> [5] </ref>. As shown in x5, the communication volume of this block approach is still relatively high. To the best of our knowledge, experimental results of the row-oriented approach and the block-oriented approach described in [5] have not been reported in the literature. In x5, performance results of these two approaches will be discussed. 3.1. An overall framework for parallel sparse QR factorization. We propose a new parallel algorithm for the numeric phase of the multifrontal sparse QR factorization described in x2. <p> This scheme is referred to as the equal-row partitioning scheme. The special cases s = 1 and s = h=p 2 are considered in <ref> [5] </ref>. The number of blocks k determines the arithmetic work distribution of merging V into W . The larger the number of blocks, the more balanced the arithmetic work distribution. The worst and the best arithmetic work distributions are achieved for k = 1 and k = h, respectively. <p> Analysis of regular grid problems. In this section, we present the complexity analysis of our parallel sparse QR factorization algorithm based on a special equal-volume partitioning scheme for regular grid problems. We compare our results with those provided in <ref> [5] </ref>. To be consistent with the results in [5], we count only the number of multiplications in our analysis. We focus on the equal-volume partitioning scheme in which an h fi h frontal matrix distributed among p processors is partitioned into p blocks. <p> Analysis of regular grid problems. In this section, we present the complexity analysis of our parallel sparse QR factorization algorithm based on a special equal-volume partitioning scheme for regular grid problems. We compare our results with those provided in <ref> [5] </ref>. To be consistent with the results in [5], we count only the number of multiplications in our analysis. We focus on the equal-volume partitioning scheme in which an h fi h frontal matrix distributed among p processors is partitioned into p blocks. <p> The results of the row-oriented algorithm are taken from <ref> [5] </ref>. Under a reasonable assumption that p is less than the number of columns k 2 , the communication complexity of our algorithm with the special block partition is much better. <p> The versions of our parallel sparse QR factorization algorithm, which use these two instances of the revised equal-volume partitioning scheme in their kernels, are referred to as algorithm EVP and EVPP, respectively. Note that ERP1 is the row-oriented approach proposed in <ref> [5] </ref> and ERP2 is only suggested as a future research topic in [5]. ERP and EVP are algorithms with optimal block partition. We evaluate the practical performance of these six parallel sparse QR factorization algorithms on a 32-node Intel iPSC/860 machine. <p> Note that ERP1 is the row-oriented approach proposed in <ref> [5] </ref> and ERP2 is only suggested as a future research topic in [5]. ERP and EVP are algorithms with optimal block partition. We evaluate the practical performance of these six parallel sparse QR factorization algorithms on a 32-node Intel iPSC/860 machine. <p> We have presented both complexity results of our parallel algorithm for regular grid problems and experimental results of an implementation of our parallel algorithm on an Intel iPSC/860 machine. We have demonstrated that our new block-oriented algorithm achieves significant improvement in performance over the conventional non-block algorithm <ref> [5] </ref>. Our approach is especially suitable for those machines which communicate long messages much more efficiently than they communicate short messages such as the IBM SP1 system since relatively long messages are used in our algorithm.
Reference: [6] <author> T. H. Dunigan, </author> <title> Performance of the Intel iPSC/860 hypercube, </title> <type> Tech. Report 11491, </type> <institution> Mathematical Sciences Section, Oak Ridge National Laboratory, Oak Ridge, TN, </institution> <year> 1990. </year>
Reference-contexts: If t 22, the overall complexity of our algorithm with the special block partition is better. Dunigan <ref> [6] </ref> has shown that the values of t for iPSC/2 and iPSC/860 are roughly 59 and 1000, respectively. Note that the communication cost dominates the arithmetic cost in the row-oriented approach.
Reference: [7] <author> J. A. George, </author> <title> Nested dissection of a regular finite element mesh, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 10 (1973), </volume> <pages> pp. 345-363. 20 </pages>
Reference-contexts: Compute the supernodal elimination tree of R from the elimination tree and the symbolic structure of R. 5. Perform numerical factorization by processing the supernodes in a topological ordering. Fig. 1. Sparse QR factorization Step 1 is done by applying a symmetric ordering such as the nested dissection ordering <ref> [7] </ref> or the minimum degree ordering [12] to G (A T A). Step 2 can be accomplished by using an algorithm described in [18]. The symbolic factorization step is discussed in [9]. There are several slightly different definitions of supernode [1, 2, 3, 10, 20]. <p> Analysis of the parallel sparse QR factorization algorithm. Now we analyze the performance of our parallel sparse QR factorization algorithm applied to regular grid problems with nested dissection ordering <ref> [7] </ref>. Consider a k fi k grid with (k 1) 2 small squares. Associated with each square is a set of four equations involving the four variables at the corners of the square. <p> The regular grid problems and the problems for modelling turbulent combustion are ordered by the nested dissection ordering <ref> [7] </ref>. The general sparse problem is ordered by the minimum degree ordering [12].
Reference: [8] <author> J. A. George and M. T. Heath, </author> <title> Solution of sparse linear least squares problems using Givens rotations, Linear Algebra and its Appl., </title> <booktitle> 34 (1980), </booktitle> <pages> pp. 69-83. </pages>
Reference-contexts: Usually Q is not formed explicitly. A row merging scheme and a general row merging scheme for sparse QR factorization are proposed in <ref> [8] </ref> and [19], respectively. Sequential algorithms for sparse QR factorization are also considered in [11, 17, 22, 23]. Parallel algorithms are described in [5, 24, 27] for implementing the numeric phase of the general row merging scheme [19] on MIMD distributed-memory multiprocessors. <p> Parallel algorithms are described in [5, 24, 27] for implementing the numeric phase of the general row merging scheme [19] on MIMD distributed-memory multiprocessors. A parallel algorithm is discussed in [16] for implementing the numeric phase of the row merging scheme <ref> [8] </ref> on SIMD architecture. Previously-proposed parallel sparse QR factorization algorithms [5, 16, 24, 27] are based on the non-block or single row partition of the dense matrices involved in the numerical computation. Block-oriented approach to parallel sparse QR factorization has not been explored before.
Reference: [9] <author> J. A. George and J. W. H. Liu, </author> <title> An optimal algorithm for symbolic factorization of symmetric matrices, </title> <journal> SIAM J. Comput., </journal> <volume> 9 (1980), </volume> <pages> pp. </pages> <month> 583-593. </month> <title> [10] , Computer Solution of Large Sparse Positive Definite Systems, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year> <title> [11] , Householder reflections versus Givens rotations in sparse orthogonal decomposition, Linear Algebra and its Appl., </title> <booktitle> 88/89 (1987), </booktitle> <pages> pp. </pages> <month> 223-238. </month> <title> [12] , The evolution of the minimum degree algorithm, </title> <journal> SIAM Review, </journal> <volume> 31 (1989), </volume> <pages> pp. 1-19. </pages>
Reference-contexts: Sparse QR factorization Step 1 is done by applying a symmetric ordering such as the nested dissection ordering [7] or the minimum degree ordering [12] to G (A T A). Step 2 can be accomplished by using an algorithm described in [18]. The symbolic factorization step is discussed in <ref> [9] </ref>. There are several slightly different definitions of supernode [1, 2, 3, 10, 20]. Any of those definitions can be used in our approach. In an implementation of our algorithm, the fundamental supernode [2] is used.
Reference: [13] <author> J. A. George, J. W. H. Liu, and E. G. Y. Ng, </author> <title> Communication results for parallel sparse Cholesky factorization on a hypercube, </title> <booktitle> Parallel Computing, 10 (1989), </booktitle> <pages> pp. 287-298. </pages>
Reference-contexts: Our goal is to obtain an upper bound on the overall execution time of our parallel sparse QR factorization algorithm. There are two levels of analysis. In the outer level, the supernodal elimination tree is mapped onto node processors by the subtree-to-subcube mapping scheme <ref> [13] </ref>. We identify a heavest path of the supernodal elimination tree. The heavest path consists of a sequence of computational tasks each of which is the computation of a frontal matrix. Careful exploration of the heavest path leads to an upper bound on the execution time of our algorithm. <p> The supernodal elimination tree of a grid problem is a complete binary tree and is mapped onto node processors by the subtree-to-subcube mapping scheme <ref> [13] </ref>. The frontal matrices are partitioned by our special equal-volume partitioning scheme. A frontal matrix F K is computed by merging the two update matrices U 1 and U 2 of the children of K.
Reference: [14] <author> G. Golub, </author> <title> Numerical methods for solving linear least squares problems, </title> <journal> Numer. Math., </journal> <volume> 7 (1965), </volume> <pages> pp. 206-216. </pages>
Reference-contexts: The QR factorization method described in <ref> [14] </ref> is used to solve these sparse linear least squares problems on an Intel iPSC/860 machine. The sparse QR factorization is computed by our parallel multifrontal approach described in x3. Both the equal-row partitioning scheme and the revised equal-volume partitioning scheme are used.
Reference: [15] <author> Intel, </author> <title> iPSC/2 and iPSC/860 User's Guide, </title> <publisher> Intel SSD, </publisher> <address> Beaverton, OR, </address> <year> 1991. </year>
Reference-contexts: Once the corresponding receive is issued by ^, processor ^ waits until the message arrives in the specified buffer. An example of this model is the communication model used on an Intel iPSC/2 or Intel iPSC/860 machine <ref> [15] </ref>. In this model, the cost of transmitting a message is charged to the receiver. Let t denote the ratio of the time for transmitting one floating-point number from one processor to another processor to the time for one floating-point multiplication. 4.1. Analysis of the parallel numerical kernel.
Reference: [16] <author> S. G. Kratzer, </author> <title> Massively parallel sparse matrix computations, </title> <type> Tech. Report SRC-TR-90-008, </type> <institution> Supercomputing Research Center, Bowie, MD, </institution> <month> February </month> <year> 1990. </year>
Reference-contexts: Sequential algorithms for sparse QR factorization are also considered in [11, 17, 22, 23]. Parallel algorithms are described in [5, 24, 27] for implementing the numeric phase of the general row merging scheme [19] on MIMD distributed-memory multiprocessors. A parallel algorithm is discussed in <ref> [16] </ref> for implementing the numeric phase of the row merging scheme [8] on SIMD architecture. Previously-proposed parallel sparse QR factorization algorithms [5, 16, 24, 27] are based on the non-block or single row partition of the dense matrices involved in the numerical computation. <p> A parallel algorithm is discussed in [16] for implementing the numeric phase of the row merging scheme [8] on SIMD architecture. Previously-proposed parallel sparse QR factorization algorithms <ref> [5, 16, 24, 27] </ref> are based on the non-block or single row partition of the dense matrices involved in the numerical computation. Block-oriented approach to parallel sparse QR factorization has not been explored before.
Reference: [17] <author> J. G. Lewis, D. J. Pierce, and D. K. Wah, </author> <title> Multifrontal Householder QR factorization, </title> <type> Tech. Report ECA-TR-127, </type> <institution> Boeing Computer Services, </institution> <address> Seattle, WA, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: Usually Q is not formed explicitly. A row merging scheme and a general row merging scheme for sparse QR factorization are proposed in [8] and [19], respectively. Sequential algorithms for sparse QR factorization are also considered in <ref> [11, 17, 22, 23] </ref>. Parallel algorithms are described in [5, 24, 27] for implementing the numeric phase of the general row merging scheme [19] on MIMD distributed-memory multiprocessors. A parallel algorithm is discussed in [16] for implementing the numeric phase of the row merging scheme [8] on SIMD architecture. <p> Experimental results are discussed in x5. Finally, concluding remarks are contained in x6. 2. A multifrontal sparse QR factorization algorithm. We briefly review a multifrontal approach to sparse QR factorization in this section. We refer the reader to <ref> [11, 17, 19] </ref> for further details. Let A T A = LL T , where L is the Cholesky factor of A T A. It is well-known that R T is equal to L, apart from possible sign differences in the rows.
Reference: [18] <author> J. W. H. Liu, </author> <title> A compact row storage scheme for Cholesky factors using elimination trees, </title> <journal> ACM Trans. on Math. Software, </journal> <volume> 12 (1986), </volume> <pages> pp. </pages> <month> 127-148. </month> <title> [19] , On general row merging schemes for sparse Givens transformations, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 7 (1986), </volume> <pages> pp. </pages> <month> 1190-1211. </month> <title> [20] , The multifrontal method for sparse matrix solution: </title> <journal> theory and practice, SIAM Review, </journal> <volume> 34 (1992), </volume> <pages> pp. 82-109. </pages>
Reference-contexts: Fig. 1. Sparse QR factorization Step 1 is done by applying a symmetric ordering such as the nested dissection ordering [7] or the minimum degree ordering [12] to G (A T A). Step 2 can be accomplished by using an algorithm described in <ref> [18] </ref>. The symbolic factorization step is discussed in [9]. There are several slightly different definitions of supernode [1, 2, 3, 10, 20]. Any of those definitions can be used in our approach. In an implementation of our algorithm, the fundamental supernode [2] is used.
Reference: [21] <author> S. Lu and J. L. Barlow, </author> <title> Parallel computation of orthogonal factors of sparse matrices, </title> <booktitle> in Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1993, </year> <pages> pp. 486-490. </pages>
Reference-contexts: The practical problems include large and sparse linear least squares problems arising from particle methods for modelling turbulent combustion [25] and a large problem NIMBUS from the Bramley's test set used in <ref> [21] </ref>. The sparse linear least squares problems for modelling turbulent combustion correspond to three-dimensional k fi k fi k twenty-seven point grids. There are a number of particles associated with each cubic element. A particle corresponds to an equation involving the eight variables at the corners of the cubic element.
Reference: [22] <author> P. Matstoms, </author> <title> The Multifrontal Solution of Sparse Linear Least Squares Problems, </title> <type> PhD thesis, </type> <institution> Linkoping University, Sweden, </institution> <year> 1991. </year>
Reference-contexts: Usually Q is not formed explicitly. A row merging scheme and a general row merging scheme for sparse QR factorization are proposed in [8] and [19], respectively. Sequential algorithms for sparse QR factorization are also considered in <ref> [11, 17, 22, 23] </ref>. Parallel algorithms are described in [5, 24, 27] for implementing the numeric phase of the general row merging scheme [19] on MIMD distributed-memory multiprocessors. A parallel algorithm is discussed in [16] for implementing the numeric phase of the row merging scheme [8] on SIMD architecture.
Reference: [23] <author> D. J. Pierce and J. G. Lewis, </author> <title> Sparse rank revealing QR factorization, </title> <type> Tech. Report MEA-TR-193, </type> <institution> Boeing Computer Services, </institution> <address> Seattle, WA, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Usually Q is not formed explicitly. A row merging scheme and a general row merging scheme for sparse QR factorization are proposed in [8] and [19], respectively. Sequential algorithms for sparse QR factorization are also considered in <ref> [11, 17, 22, 23] </ref>. Parallel algorithms are described in [5, 24, 27] for implementing the numeric phase of the general row merging scheme [19] on MIMD distributed-memory multiprocessors. A parallel algorithm is discussed in [16] for implementing the numeric phase of the row merging scheme [8] on SIMD architecture.
Reference: [24] <author> P. E. Plassmann, </author> <title> Sparse Jacobian estimation and factorization on a multiprocessor, in Large-Scale Numerical Optimization, </title> <editor> T. F. Coleman and Y. Li, eds., </editor> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1990, </year> <pages> pp. 152-179. </pages>
Reference-contexts: Usually Q is not formed explicitly. A row merging scheme and a general row merging scheme for sparse QR factorization are proposed in [8] and [19], respectively. Sequential algorithms for sparse QR factorization are also considered in [11, 17, 22, 23]. Parallel algorithms are described in <ref> [5, 24, 27] </ref> for implementing the numeric phase of the general row merging scheme [19] on MIMD distributed-memory multiprocessors. A parallel algorithm is discussed in [16] for implementing the numeric phase of the row merging scheme [8] on SIMD architecture. <p> A parallel algorithm is discussed in [16] for implementing the numeric phase of the row merging scheme [8] on SIMD architecture. Previously-proposed parallel sparse QR factorization algorithms <ref> [5, 16, 24, 27] </ref> are based on the non-block or single row partition of the dense matrices involved in the numerical computation. Block-oriented approach to parallel sparse QR factorization has not been explored before. <p> The numeric phase of a sequential multifrontal sparse QR factorization algorithm 3. A parallel multifrontal sparse QR factorization algorithm. Several parallel algorithms have been described in <ref> [5, 24, 27] </ref> for implementing the numeric phase of sparse QR factor ization on distributed-memory multiprocessors. * In the algorithm discussed in [5], a frontal matrix F is mapped to a ring of processors by assigning the rows of F to the processors in a wrap-around manner. <p> This shift is continued until U is completely merged into F . This approach is referred to as the row-oriented approach. * In the approach described in <ref> [24] </ref>, a global row reduction algorithm is introduced to compute a frontal matrix. Parallel computation of the tree structure required for the multifrontal method is also discussed.
Reference: [25] <author> S. B. Pope, </author> <title> Particle methods for turbulent combustion. </title> <type> private communication, </type> <year> 1991. </year>
Reference-contexts: The practical problems include large and sparse linear least squares problems arising from particle methods for modelling turbulent combustion <ref> [25] </ref> and a large problem NIMBUS from the Bramley's test set used in [21]. The sparse linear least squares problems for modelling turbulent combustion correspond to three-dimensional k fi k fi k twenty-seven point grids. There are a number of particles associated with each cubic element.
Reference: [26] <author> A. Pothen and C. Sun, </author> <title> A mapping algorithm for parallel sparse Cholesky factorization, </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 14 (1993), </volume> <pages> pp. 1253-1257. </pages>
Reference-contexts: The symbolic phase of the sparse QR factorization is done sequentially. After the symbolic structure of R and the corresponding supernodal elimination tree are determined, the arithmetic work associated with each subtree is estimated. The supernodal elimination tree is mapped onto processors by a proportional mapping scheme described in <ref> [26] </ref>. The root of the supernodal elimination tree is partitioned among all processors.
Reference: [27] <author> P. Raghavan, </author> <title> Distributed Sparse Matrix Factorization: QR and Cholesky Decompositions, </title> <type> PhD thesis, </type> <institution> Penn-sylvania State University, University Park, </institution> <address> PA, </address> <year> 1991. </year>
Reference-contexts: Usually Q is not formed explicitly. A row merging scheme and a general row merging scheme for sparse QR factorization are proposed in [8] and [19], respectively. Sequential algorithms for sparse QR factorization are also considered in [11, 17, 22, 23]. Parallel algorithms are described in <ref> [5, 24, 27] </ref> for implementing the numeric phase of the general row merging scheme [19] on MIMD distributed-memory multiprocessors. A parallel algorithm is discussed in [16] for implementing the numeric phase of the row merging scheme [8] on SIMD architecture. <p> A parallel algorithm is discussed in [16] for implementing the numeric phase of the row merging scheme [8] on SIMD architecture. Previously-proposed parallel sparse QR factorization algorithms <ref> [5, 16, 24, 27] </ref> are based on the non-block or single row partition of the dense matrices involved in the numerical computation. Block-oriented approach to parallel sparse QR factorization has not been explored before. <p> The numeric phase of a sequential multifrontal sparse QR factorization algorithm 3. A parallel multifrontal sparse QR factorization algorithm. Several parallel algorithms have been described in <ref> [5, 24, 27] </ref> for implementing the numeric phase of sparse QR factor ization on distributed-memory multiprocessors. * In the algorithm discussed in [5], a frontal matrix F is mapped to a ring of processors by assigning the rows of F to the processors in a wrap-around manner. <p> Parallel computation of the tree structure required for the multifrontal method is also discussed. In this approach, the amount of arithmetic work may increase as the number of processors increases. 3 * A parallel algorithm is discussed in <ref> [27] </ref> for implementing the sparse QR factorization scheme proposed in [11] in parallel. In this scheme, a frontal matrix is computed by row-oriented Householder transformations. Before the parallel computation of a frontal matrix is started, the relevant update matrices are redistributed among processors to ensure relatively balanced load distribution.

References-found: 22

