URL: http://www.cogs.susx.ac.uk/users/christ/papers/best-friend.ps
Refering-URL: http://www.cogs.susx.ac.uk/users/christ/index-noframes.html
Root-URL: 
Email: Email: Chris.Thornton@cogs.susx.ac.uk  
Phone: Tel: (44)1273 678856  
Title: Separability is a Learner's Best Friend  
Author: Chris Thornton 
Date: August 22, 1997  
Web: WWW: http://www.cogs.susx.ac.uk  
Address: Brighton BN1 9QH  
Affiliation: Cognitive and Computing Sciences University of Sussex  
Abstract: Geometric separability is a generalisation of linear separability, familiar to many from Minsky and Papert's analysis of the Perceptron learning method. The concept forms a novel dimension along which to conceptualise learning methods. The present paper shows how geometric separability can be defined and demonstrates that it accurately predicts the performance of a at least one empirical learning method.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Rendell, L. and Seshu, R. </author> <year> (1990). </year> <title> Learning hard concepts through con-structive induction. </title> <booktitle> Computational Intelligence, </booktitle> <pages> 6 (pp. 247-270). </pages>
Reference-contexts: But these region- or boundary-oriented methods all make a rather strong assumption about the input data and the underlying task. They assume, in 1 effect, that the function is `smooth' <ref> [1] </ref> and that input points with the same target output will therefore tend to cluster together in the same region of the input space. Unfortunately, this assumption turns out to be valid only in certain situations. In other situations, it is totally invalid.
Reference: [2] <author> Minsky, M. and Papert, S. </author> <year> (1969). </year> <title> Perceptrons. </title> <address> Cambridge, Mass.: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Unfortunately, this assumption turns out to be valid only in certain situations. In other situations, it is totally invalid. The general idea that boundary methods have severe limitations is familiar to the Cognitive Science community via the work of Minsky and Papert <ref> [2, 3] </ref> on the Perceptron learning method. The Perceptron is a neural-network method which, in its simplest manifestation, attempts to acquire a target function using the simplest boundary method of all.
Reference: [3] <author> Minsky, M. and Papert, S. </author> <year> (1988). </year> <title> Perceptrons: An Introduction to Computational Geometry (expanded edn). </title> <address> Cambridge, Mass.: MIT Press. </address> <note> 5 The terms type-1 and type-2 were also used to label the non-relational and relational cases respectively. 7 </note>
Reference-contexts: Unfortunately, this assumption turns out to be valid only in certain situations. In other situations, it is totally invalid. The general idea that boundary methods have severe limitations is familiar to the Cognitive Science community via the work of Minsky and Papert <ref> [2, 3] </ref> on the Perceptron learning method. The Perceptron is a neural-network method which, in its simplest manifestation, attempts to acquire a target function using the simplest boundary method of all.
Reference: [4] <author> Thornton, C. </author> <year> (1996). </year> <title> Parity: the problem that won't go away. </title> <editor> In G. McCalla (Ed.), </editor> <booktitle> Proceeding of AI-96 (Toronto, </booktitle> <pages> Canada) (pp. 362-374). </pages> <publisher> Springer. </publisher>
Reference-contexts: Boundary methods produce poor performance when geometric separability is low. The geometric separability of any parity problem is 3 zero. Thus, Perceptron's should not be able to learn parity problems. In fact, any boundary method tends to produce poor performance on parity problems <ref> [4] </ref>. <p> Widespread (if often implicit) recognition of this fact has meant that characteristically relational problems are typically addressed using special-purpose relational learning methods (e.g., ILP methods [7]). In those rare cases where a boundary method is tested on a relational task the performance tends to be poor <ref> [4] </ref>. Interestingly, the argument which allows us to deduce that low geometric separability is associated with relational problems can be turned around to demonstrate that problems with high geometric separability are always efficiently processed using boundary methods.
Reference: [5] <author> Holte, R. </author> <year> (1993). </year> <title> Very simple classification rules perform well on most commonly used datasets. </title> <booktitle> Machine learning, </booktitle> <pages> 3 (pp. 63-91). </pages>
Reference-contexts: But what about other common learning problems? Do frequently used learning problems such as those which reside in the UCI repository 2 , tend to have high, low, or indifferent geometric separability? To investigate this issue, the geometric separability of all 16 `frequently used datasets' (as featured in the Holte <ref> [5] </ref> study) was tested. The results are shown in Table 1.
Reference: [6] <author> Fisher, D. and McKusick, K. </author> <year> (1989). </year> <title> An empirical comparison of ID3 and back-propagation. </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 788-793). </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: But this turns out to be just the tip of an iceberg. in computing the GSI. 4 Of course, C4.5 and its near relation ID3 have been shown on numerous occasions to produce performance which is comparable to that of many other empirical learning methods, e.g., the connectionist MLP method <ref> [6] </ref> 5 By reasoning backwards from what we know about geometric separability, we can demonstrate that any task which involves the recognition or testing of a relationship will typically exhibit negligable geometric separability.
Reference: [7] <author> Muggleton, S. (Ed.) </author> <year> (1992). </year> <title> Inductive Logic Programming. </title> <publisher> Academic Press. </publisher>
Reference-contexts: Widespread (if often implicit) recognition of this fact has meant that characteristically relational problems are typically addressed using special-purpose relational learning methods (e.g., ILP methods <ref> [7] </ref>). In those rare cases where a boundary method is tested on a relational task the performance tends to be poor [4].
Reference: [8] <author> Holland, J. </author> <year> (1975). </year> <booktitle> Adaptation in Natural and Artificial Systems. </booktitle> <address> Ann Arbor: </address> <publisher> University of Michigan Press. </publisher> <pages> 8 </pages>
Reference-contexts: ID3 and its near relation C4.5 constructs hyper-rectangular, axis-aligned regions. The MLP, in its vanilla form employing one layer of hidden units, uses multiple linear boundaries (cf. Lipmann figure) to identify more or less arbitrarily-shaped regions. The crossover-based genetic algorithm can be viewed as manipulating hyperplanes <ref> [8] </ref>. 4 Concluding comments The concept of geometric separability gets close to the heart of what empirical learning is really all about. Empirical learning is the exploitation of geometric separability.
References-found: 8

