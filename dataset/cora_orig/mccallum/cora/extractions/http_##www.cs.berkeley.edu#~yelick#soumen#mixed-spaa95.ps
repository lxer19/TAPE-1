URL: http://www.cs.berkeley.edu/~yelick/soumen/mixed-spaa95.ps
Refering-URL: http://www.cs.berkeley.edu/~yelick/papers.html
Root-URL: 
Title: Modeling the Benefits of Mixed Data and Task Parallelism  
Author: Soumen Chakrabarti James Demmel Katherine Yelick 
Abstract: Mixed task and data parallelism exists naturally in many applications, but utilizing it may require sophisticated scheduling algorithms and software support. Recently, significant research effort has been applied to exploiting mixed parallelism in both theory and systems communities. In this paper, we ask how much mixed parallelism will improve performance in practice, and how architectural evolution impacts these estimates. First, we build and validate a performance model for a class of mixed task and data parallel problems based on machine and problem parameters. Second, we use this model to estimate the gains from mixed parallelism for some scientific applications on current machines. This quantifies our intuition that mixed parallelism is best when either communication is slow or the number of processors is large. Third, we show that, for balanced divide and conquer trees, a simple one-time switch between data and task parallelism gets most of the benefit of general mixed parallelism. Fourth, we establish upper bounds to the benefits of mixed parallelism for irregular task graphs. Apart from these detailed analyses, we provide a framework in which other applications and machines can be evaluated. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Arpaci, D. Culler, A. Krishnamurthy, S. Steinberg, and K. Yelick. </author> <title> Empirical evaluation of the CRAY-T3D: A compiler perspective. </title> <booktitle> In International Symposium on Computer Architecture. ACM SIGARCH, </booktitle> <year> 1995. </year>
Reference-contexts: The curves were fit for 2 P 500 and 100 n = N 1=2 10000. An estimate of memory per processor in megabytes is given in the column marked M/P. Estimates for ff and fi are in part from <ref> [23, 27, 18, 1] </ref>. of this magnitude (which we will call "packing loss") may substantially mask the benefits which would otherwise be obtained from mixed parallelism. Furthermore, we know of no tighter analysis of this constant for a given graph. We are thus faced with the following problem.
Reference: [2] <author> S. B. Baden. </author> <title> Programming abstractions for dynamically partitioning and coordinating localized scientific calculations running on multiprocessors. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 12(1) </volume> <pages> 145-157, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction Mixed parallelism exists naturally in many applications. In adaptive mesh refinement (AMR) algorithms, there is task parallelism between meshes and data-parallelism within a mesh <ref> [2] </ref>. In computing eigenvalues of nonsymmetric matrices, the sign function algorithm does divide and conquer with matrix factorizations at each division [3]. In timing-level circuit simulation there is parallelism between separate subcircuits and parallelism within the model evaluation of each subcircuit [26].
Reference: [3] <author> Z. Bai and J. Demmel. </author> <title> Design of a parallel nonsymmetric eigenroutine toolbox, Part I. </title> <booktitle> In Proceedings of the Sixth SIAM Conference on Parallel Proceesing for Scientific Computing. </booktitle> <publisher> SIAM, </publisher> <year> 1993. </year>
Reference-contexts: 1 Introduction Mixed parallelism exists naturally in many applications. In adaptive mesh refinement (AMR) algorithms, there is task parallelism between meshes and data-parallelism within a mesh [2]. In computing eigenvalues of nonsymmetric matrices, the sign function algorithm does divide and conquer with matrix factorizations at each division <ref> [3] </ref>. In timing-level circuit simulation there is parallelism between separate subcircuits and parallelism within the model evaluation of each subcircuit [26]. In sparse matrix factorization, multi-frontal algorithms expose task parallelism between separate dense sub-matrices and data parallelism within those dense matrices [16]. <p> Finally, we compute the running times of full divide and conquer trees using these three kinds of parallelism and apply the results to our examples. 4.1.1 Applications Eigenvalue algorithms. Eigenvalue algorithms exhibit mixed parallelism. For example, a recent implementation of a dense nonsymmetric algorithm <ref> [3] </ref> proceeds by successively separating the matrix into two submatrices, the union of whose eigenvalues are the eigenvalues of the original matrix. The root node has size N = n 2 . If the separation is perfect, each child is of size n 2 fi n 2 , or N=4.
Reference: [4] <author> K. Belkhale and P. Banerjee. </author> <title> An approximate algorithm for the partitionable independent task scheduling problem. </title> <booktitle> In International Conference on Parallel Processing (ICPP). IEEE, </booktitle> <month> August </month> <year> 1990. </year> <note> Full version in technical reports UILU-ENG-90-2253 and CRHC-90-15, </note> <institution> University of Illinois, Urbana. </institution>
Reference-contexts: Several researchers have proposed support to take advantage of this mixed parallelism. In the theory area, the best known on-line scheduling algorithm for mixed parallelism is 2:62-optimal <ref> [4, 11] </ref>, and the best off-line algorithm is 2-optimal [25, 17].
Reference: [5] <author> C. Bischof, S. Huss-Lederman, X. Sun, A. Tsao, and T. Turnbull. </author> <title> Parallel performance of a symmetric eigensolver based on the invariant subspace decomposition approach. </title> <booktitle> In Scalable High Performance Computing Conference, </booktitle> <pages> pages 32-39, </pages> <address> Knoxville, TN, </address> <month> May </month> <year> 1994. </year> <note> IEEE. </note>
Reference-contexts: This successive separation process forms a binary tree with c = 2, d = 4, and a = 3=2. (We scale time so that the constant in O (N 3=2 ) becomes one.) For symmetric matrices, an algorithm similar in spirit is the beta-function technique of Bischof et al <ref> [5] </ref>. An eigenvalue algorithm of a different flavor, but still from the divide and conquer category, is Cuppen's method for symmetric tridiagonal matrices, where we can actually split the matrix exactly in half all the time [7, 21] (although the costs of the children are not so simple). Sparse Cholesky. <p> Thus, switched parallelism will not be as efficient as optimal mixed parallelism, but it is much simpler to implement, so if its efficiency is nearly as good, it is an attractive option. Switched parallelism is used, for example, by Bischof et al <ref> [5] </ref>.
Reference: [6] <author> S. Chatterjee. </author> <title> Compiling data-parallel programs for efficient execution on shared-memory multiprocessors. </title> <type> Technical Report CMU-CS-91-189, </type> <address> CMU, Pittsburgh, PA 15213, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: In the theory area, the best known on-line scheduling algorithm for mixed parallelism is 2:62-optimal [4, 11], and the best off-line algorithm is 2-optimal [25, 17]. In the systems area, the Paradigm compiler [20], iWarp compiler [24], and NESL compiler <ref> [6] </ref> all support limited forms of mixed task and data parallelism, and there are plans to merge data Fortran D with Fortran M [12] and pC++ with CC++ [15] to support mixed parallelism.
Reference: [7] <author> J. Cuppen. </author> <title> A divide and conquer method for the symmetric tridiagonal eigenproblem. </title> <journal> Numer. Math., </journal> <volume> 36 </volume> <pages> 177-195, </pages> <year> 1981. </year>
Reference-contexts: An eigenvalue algorithm of a different flavor, but still from the divide and conquer category, is Cuppen's method for symmetric tridiagonal matrices, where we can actually split the matrix exactly in half all the time <ref> [7, 21] </ref> (although the costs of the children are not so simple). Sparse Cholesky. We consider the regular but important special case of the matrix arising from the 5-point Laplacian on a square grid, ordered using the nested dissection ordering [13]. <p> For this compound data parallel task, estimates of for various machines are shown in column SF of table 1 (e 1 = 1). If Cuppen's eigenvalue algorithm is used, and the effect of "deflation" is small <ref> [7] </ref>, the task tree has the same parameters as the sign function example above, although is different. Comments. From table 1, typical values of are all in the 10 2 to 10 6 range.
Reference: [8] <author> J. Demmel, J. Dongarra, R. van de Geijn, and D. Walker. </author> <title> LAPACK for distributed memory machines: the next generation. </title> <booktitle> In Proceedings of the Sixth SIAM Conference on Parallel Proceesing for Scientific Computing. </booktitle> <publisher> SIAM, </publisher> <year> 1993. </year>
Reference-contexts: N=P 1 e 1 within a single task. 2.1.2 Validation We validated our model using experimental data. In figure 2, we consider three ScaLAPACK programs: LU, QR and Cholesky factorizations, and three machines: the Delta, Paragon and iPSC/860 <ref> [8] </ref>. Each graph plots performance in GFLOPS per processor versus N=P , including experimental data (the circles), as well as the prediction of the asymptotic model. The iPSC/860 experiments were run with 128 processors, and the Paragon and Delta experiments were run with both 128 and 512 processors. <p> The asymptotic model is a good fit for the actual efficiency profiles: the mean relative error is 6-11%. Estimates of are important for performance analysis as well as runtime scheduling decisions, as we shall see later. To this end, we collect values of for some parallel scientific libraries <ref> [8] </ref>, using existing analytical performance models [9, 10]. For each of these routines, we have available the communication and computation time as functions of problem size, number of processors, network latency, and network bandwidth.
Reference: [9] <author> J. Demmel and K. Stanley. </author> <title> The performance of finding eigenvalues and eigenvectors of dense symmetric matrices on distributed memory computers. </title> <booktitle> In Proceedings of the Seventh SIAM Conference on Parallel Proceesing for Scientific Computing. </booktitle> <publisher> SIAM, </publisher> <year> 1994. </year>
Reference-contexts: Estimates of are important for performance analysis as well as runtime scheduling decisions, as we shall see later. To this end, we collect values of for some parallel scientific libraries [8], using existing analytical performance models <ref> [9, 10] </ref>. For each of these routines, we have available the communication and computation time as functions of problem size, number of processors, network latency, and network bandwidth. <p> Parameters ff (latency) and fi (inverse bandwidth) are normalized to a BLAS-3 FLOP, and the model is fit to data generated from analytical models <ref> [9, 10, 23] </ref>. The curves were fit for 2 P 500 and 100 n = N 1=2 10000. An estimate of memory per processor in megabytes is given in the column marked M/P.
Reference: [10] <author> F. Desprez, B. Tourancheau, and J. J. Dongarra. </author> <title> Performance complexity of LU factorization with efficient pipelin-ing and overlap on a multiprocessor. </title> <type> Technical report, </type> <institution> University of Tennessee, Knoxville, </institution> <month> Feb </month> <year> 1994. </year> <note> (LAPACK Working Note #67). </note>
Reference-contexts: Estimates of are important for performance analysis as well as runtime scheduling decisions, as we shall see later. To this end, we collect values of for some parallel scientific libraries [8], using existing analytical performance models <ref> [9, 10] </ref>. For each of these routines, we have available the communication and computation time as functions of problem size, number of processors, network latency, and network bandwidth. <p> Parameters ff (latency) and fi (inverse bandwidth) are normalized to a BLAS-3 FLOP, and the model is fit to data generated from analytical models <ref> [9, 10, 23] </ref>. The curves were fit for 2 P 500 and 100 n = N 1=2 10000. An estimate of memory per processor in megabytes is given in the column marked M/P.
Reference: [11] <author> A. Feldmann, J. Sgall, and S.-H. Teng. </author> <title> Dynamic scheduling on parallel machines. </title> <booktitle> In Foundations of Computer Science (FOCS), </booktitle> <pages> pages 111-120, </pages> <year> 1992. </year>
Reference-contexts: Several researchers have proposed support to take advantage of this mixed parallelism. In the theory area, the best known on-line scheduling algorithm for mixed parallelism is 2:62-optimal <ref> [4, 11] </ref>, and the best off-line algorithm is 2-optimal [25, 17].
Reference: [12] <author> I. Foster, M. Xu, B. Avalani, and A. Chowdhary. </author> <title> A compilation system that integrates high performance Fortran and Fortran M. </title> <booktitle> In Scalable High Performance Computing Conference, </booktitle> <pages> pages 293-300. </pages> <publisher> IEEE, </publisher> <year> 1994. </year>
Reference-contexts: In the systems area, the Paradigm compiler [20], iWarp compiler [24], and NESL compiler [6] all support limited forms of mixed task and data parallelism, and there are plans to merge data Fortran D with Fortran M <ref> [12] </ref> and pC++ with CC++ [15] to support mixed parallelism. In this paper, we step back from these algorithmic and systems issues and address the question of how much benefit should be expected, and what impact architectural evolution has on these estimates.
Reference: [13] <author> A. George and J. Liu. </author> <title> Computer Solution of Large Sparse Positive Definite Systems. </title> <publisher> Prentice-Hall Inc., </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1981. </year>
Reference-contexts: Sparse Cholesky. We consider the regular but important special case of the matrix arising from the 5-point Laplacian on a square grid, ordered using the nested dissection ordering <ref> [13] </ref>. In this case one may think of dividing the matrix into 4 independent subproblems, corresponding to dividing the square grid into 4 subsquares, each of half the perimeter.
Reference: [14] <author> J. Gilbert and R. Tarjan. </author> <title> The analysis of a nested dissection algorithm. </title> <journal> Numerische Mathematik, </journal> <volume> 50 </volume> <pages> 377-404, </pages> <year> 1987. </year>
Reference-contexts: Thus the above bound is overly optimistic. Even so, it is useful for deriving heuristic bounds to performance in some irregular graphs. E.g., Gilbert and Tarjan study nested dissection algorithms to solve sparse systems on planar graphs <ref> [14] </ref>, where a problem of size N is divided into d = 2 subproblems, where each part is no bigger than 2N=3.
Reference: [15] <author> X. Li and H. Huang. </author> <title> On the concurrency of C++. </title> <booktitle> In Proceedings ICCI '93. Fifth International Conference on Computing and Information, </booktitle> <pages> pages 215-19, </pages> <address> Ontario, Canada, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: In the systems area, the Paradigm compiler [20], iWarp compiler [24], and NESL compiler [6] all support limited forms of mixed task and data parallelism, and there are plans to merge data Fortran D with Fortran M [12] and pC++ with CC++ <ref> [15] </ref> to support mixed parallelism. In this paper, we step back from these algorithmic and systems issues and address the question of how much benefit should be expected, and what impact architectural evolution has on these estimates.
Reference: [16] <author> J. W. H. Liu. </author> <title> The multifrontal method for sparse matrix solution: </title> <journal> theory and practice. SIAM Review, </journal> <volume> 34(1) </volume> <pages> 82-109, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: In timing-level circuit simulation there is parallelism between separate subcircuits and parallelism within the model evaluation of each subcircuit [26]. In sparse matrix factorization, multi-frontal algorithms expose task parallelism between separate dense sub-matrices and data parallelism within those dense matrices <ref> [16] </ref>. In global climate modeling [19], there are fl Computer Science Division, U. C. Berkeley, CA 94720. Supported in part by ARPA/DOD (DABT63-92-C-0026), DOE (DE-FG03-94ER25206), and NSF (CCR-9210260, CDA-8722788 and CDA-9401156).
Reference: [17] <author> W. Ludwig and P. Tiwari. </author> <title> Scheduling malleable and nonmalleable parallel tasks. </title> <booktitle> In Symposium on Discrete Algorithms (SODA), </booktitle> <pages> pages 167-176. ACM-SIAM, </pages> <year> 1994. </year>
Reference-contexts: Several researchers have proposed support to take advantage of this mixed parallelism. In the theory area, the best known on-line scheduling algorithm for mixed parallelism is 2:62-optimal [4, 11], and the best off-line algorithm is 2-optimal <ref> [25, 17] </ref>. In the systems area, the Paradigm compiler [20], iWarp compiler [24], and NESL compiler [6] all support limited forms of mixed task and data parallelism, and there are plans to merge data Fortran D with Fortran M [12] and pC++ with CC++ [15] to support mixed parallelism.
Reference: [18] <author> S. Luna. </author> <title> Implementing an efficient portable global memory layer on distributed memory multiprocessors. </title> <type> Technical Report UCB/CSD-94-810, </type> <institution> University of California, Berkeley, </institution> <address> CA 94720, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: The curves were fit for 2 P 500 and 100 n = N 1=2 10000. An estimate of memory per processor in megabytes is given in the column marked M/P. Estimates for ff and fi are in part from <ref> [23, 27, 18, 1] </ref>. of this magnitude (which we will call "packing loss") may substantially mask the benefits which would otherwise be obtained from mixed parallelism. Furthermore, we know of no tighter analysis of this constant for a given graph. We are thus faced with the following problem.
Reference: [19] <author> C. R. Mechoso, C.-C. Ma, J. Farrara, J. A. Spahr, and R. W. Moore. </author> <title> Parallelization and distribution of a coupled atmosphere-ocean general circulation model. </title> <journal> Monthly Weather Review, </journal> <volume> 121(7) </volume> <pages> 2062-2076, </pages> <year> 1993. </year>
Reference-contexts: In timing-level circuit simulation there is parallelism between separate subcircuits and parallelism within the model evaluation of each subcircuit [26]. In sparse matrix factorization, multi-frontal algorithms expose task parallelism between separate dense sub-matrices and data parallelism within those dense matrices [16]. In global climate modeling <ref> [19] </ref>, there are fl Computer Science Division, U. C. Berkeley, CA 94720. Supported in part by ARPA/DOD (DABT63-92-C-0026), DOE (DE-FG03-94ER25206), and NSF (CCR-9210260, CDA-8722788 and CDA-9401156). The information presented here does not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred. <p> Another extension could be to deal with non-homogeneous processor networks or nonhomogeneous tasks, for example where one task is much better suited to one machine than another <ref> [19] </ref>. 7 Conclusions We have built a simple model to evaluate the utility of mixed data and task parallelism, with the goal of identifying how the communication cost and graph structure of the program, and network performance of the machine, affect the performance gain from using mixed parallelism.
Reference: [20] <author> S. Ramaswamy, S. Sapatnekar, and P. Banerjee. </author> <title> A convex programming approach for exploiting data and functional parallelism on distributed memory multiprocessors. </title> <booktitle> In International Conference on Parallel Processing (ICPP). IEEE, </booktitle> <year> 1994. </year>
Reference-contexts: Several researchers have proposed support to take advantage of this mixed parallelism. In the theory area, the best known on-line scheduling algorithm for mixed parallelism is 2:62-optimal [4, 11], and the best off-line algorithm is 2-optimal [25, 17]. In the systems area, the Paradigm compiler <ref> [20] </ref>, iWarp compiler [24], and NESL compiler [6] all support limited forms of mixed task and data parallelism, and there are plans to merge data Fortran D with Fortran M [12] and pC++ with CC++ [15] to support mixed parallelism. <p> is not optimal, the relative improvement of using mixed parallelism over pure data parallelism for the batch problem is e M = P + 1 P + 1 (3) Example 3.3 We apply this analysis to complex matrix multiplication, which was reported as a benchmark for the Illinois Paradigm compiler <ref> [20] </ref>. The task is MM, the machine is the CM5 without vector units, and L = 4. From figure 1 we obtain = 53 and e 1 = 1 for this problem. <p> E.g., if P = 64 and * = 0:5, then n &lt; 42, a tiny problem indeed. It is interesting that the experiments reported in <ref> [20] </ref> for the CM5 use P 2 f64; 128g processors and n = 64. <p> The motivation to study divide and conquer problems arises out of the relatively small degree of task parallelism available in applications. Static task graphs, such as those generated from control flow graphs by parallelizing compilers, have a fixed small degree of task parallelism. For example, the benchmarks in <ref> [20] </ref> have 4-7 fold effective task parallelism and the signal processing applications in [24] have a 2-5 fold task parallelism. The task parallelism in climate modeling applications is typically no more than 4-6.
Reference: [21] <author> J. Rutter. </author> <title> A serial implementation of Cuppen's divide and conquer algorithm for the symmetric eigenvalue problem. </title> <institution> Mathematics Dept. </institution> <note> Master's Thesis available by anonymous ftp to tr-ftp.cs.berkeley.edu, directory pub/tech-reports/cs/csd-94-799, file all.ps, </note> <institution> University of California, </institution> <year> 1994. </year>
Reference-contexts: An eigenvalue algorithm of a different flavor, but still from the divide and conquer category, is Cuppen's method for symmetric tridiagonal matrices, where we can actually split the matrix exactly in half all the time <ref> [7, 21] </ref> (although the costs of the children are not so simple). Sparse Cholesky. We consider the regular but important special case of the matrix arising from the 5-point Laplacian on a square grid, ordered using the nested dissection ordering [13].
Reference: [22] <author> D. B. Shmoys and D. S. Hochbaum. </author> <title> Using dual approximation algorithms for scheduling problems: theoretical and practical results. </title> <journal> Journal of the ACM, </journal> <volume> 34(1) </volume> <pages> 144-162, </pages> <month> Jan-uary </month> <year> 1987. </year>
Reference-contexts: Let Pack (P; S) be the makespan (length of schedule) generated by packing tasks from set S in task parallel mode into P processors. There are heuristics that return Pack (1 + *) Pack OPT for any given * &gt; 0, within time that is polynomial in jSj <ref> [22] </ref>. It is easy to see that Pack OPT 1 P s2S f s + max s2S f s . Consider the following heuristic. Prefix-Suffix Sort tasks in decreasing order: N 1 &gt; N 2 &gt; &gt; N L .
Reference: [23] <author> K. Stanley and J. Demmel. </author> <title> Modeling the performance of linear systems solvers on distributed memory multiprocessors. </title> <type> Technical report, </type> <institution> University of California, Berkeley, </institution> <address> CA 94720, </address> <year> 1994. </year> <note> In preparation. </note>
Reference-contexts: Parameters ff (latency) and fi (inverse bandwidth) are normalized to a BLAS-3 FLOP, and the model is fit to data generated from analytical models <ref> [9, 10, 23] </ref>. The curves were fit for 2 P 500 and 100 n = N 1=2 10000. An estimate of memory per processor in megabytes is given in the column marked M/P. <p> The curves were fit for 2 P 500 and 100 n = N 1=2 10000. An estimate of memory per processor in megabytes is given in the column marked M/P. Estimates for ff and fi are in part from <ref> [23, 27, 18, 1] </ref>. of this magnitude (which we will call "packing loss") may substantially mask the benefits which would otherwise be obtained from mixed parallelism. Furthermore, we know of no tighter analysis of this constant for a given graph. We are thus faced with the following problem.
Reference: [24] <author> J. Subhlok, J. Stichnoth, D. O'Hallaron, and T. Gross. </author> <title> Exploiting task and data parallelism on a multicomputer. </title> <booktitle> In Principles and Practice of Parallel Programming (PPoPP), </booktitle> <pages> pages 13-22, </pages> <address> San Diego, </address> <month> May </month> <year> 1993. </year> <pages> ACM-SIGPLAN. </pages>
Reference-contexts: Several researchers have proposed support to take advantage of this mixed parallelism. In the theory area, the best known on-line scheduling algorithm for mixed parallelism is 2:62-optimal [4, 11], and the best off-line algorithm is 2-optimal [25, 17]. In the systems area, the Paradigm compiler [20], iWarp compiler <ref> [24] </ref>, and NESL compiler [6] all support limited forms of mixed task and data parallelism, and there are plans to merge data Fortran D with Fortran M [12] and pC++ with CC++ [15] to support mixed parallelism. <p> Static task graphs, such as those generated from control flow graphs by parallelizing compilers, have a fixed small degree of task parallelism. For example, the benchmarks in [20] have 4-7 fold effective task parallelism and the signal processing applications in <ref> [24] </ref> have a 2-5 fold task parallelism. The task parallelism in climate modeling applications is typically no more than 4-6.
Reference: [25] <author> J. Turek, J. L. Wolf, and P. S. Yu. </author> <title> Approximate algorithms for scheduling parallelizable tasks. </title> <booktitle> In Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <pages> pages 323-332, </pages> <year> 1992. </year>
Reference-contexts: Several researchers have proposed support to take advantage of this mixed parallelism. In the theory area, the best known on-line scheduling algorithm for mixed parallelism is 2:62-optimal [4, 11], and the best off-line algorithm is 2-optimal <ref> [25, 17] </ref>. In the systems area, the Paradigm compiler [20], iWarp compiler [24], and NESL compiler [6] all support limited forms of mixed task and data parallelism, and there are plans to merge data Fortran D with Fortran M [12] and pC++ with CC++ [15] to support mixed parallelism.
Reference: [26] <author> C.-P. Wen and K. Yelick. </author> <title> Parallel timing simulation on a distributed memory multiprocessor. </title> <booktitle> In International Conference on CAD, </booktitle> <address> Santa Clara, CA, </address> <month> November </month> <year> 1993. </year> <note> An earlier version appeared as UCB Technical Report CSD-93-723. </note>
Reference-contexts: In computing eigenvalues of nonsymmetric matrices, the sign function algorithm does divide and conquer with matrix factorizations at each division [3]. In timing-level circuit simulation there is parallelism between separate subcircuits and parallelism within the model evaluation of each subcircuit <ref> [26] </ref>. In sparse matrix factorization, multi-frontal algorithms expose task parallelism between separate dense sub-matrices and data parallelism within those dense matrices [16]. In global climate modeling [19], there are fl Computer Science Division, U. C. Berkeley, CA 94720.
Reference: [27] <author> R. C. Whaley. </author> <title> Basic linear algebra communication subprograms: Analysis and implementation across multiple parallel architectures. </title> <note> Technical Report LAPACK working note 73, </note> <institution> University of Tennessee, Knoxville, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: The curves were fit for 2 P 500 and 100 n = N 1=2 10000. An estimate of memory per processor in megabytes is given in the column marked M/P. Estimates for ff and fi are in part from <ref> [23, 27, 18, 1] </ref>. of this magnitude (which we will call "packing loss") may substantially mask the benefits which would otherwise be obtained from mixed parallelism. Furthermore, we know of no tighter analysis of this constant for a given graph. We are thus faced with the following problem.
References-found: 27

