URL: ftp://ftp.cs.umass.edu/pub/techrept/techreport/1996/UM-CS-1996-013.ps
Refering-URL: http://www.cs.umass.edu/~potter/LC/spring96.doc.html
Root-URL: 
Email: clouse@cs.umass.edu  
Title: An Introspection Approach to Querying a Trainer  
Author: Jeffery A. Clouse 
Address: Amherst, MA 01003-4610  
Affiliation: Department of Computer Science Lederle Graduate Research Center University of Massachusetts  
Abstract: Technical Report 96-13 January 22, 1996 Abstract This paper introduces the Introspection Approach, a method by which a learning agent employing reinforcement learning can decide when to ask a training agent for instruction. When using our approach, we find that the same number of trainer's responses produced significantly faster learners than by having the learner ask for aid randomly. Guidance received via our approach is more informative than random guidance. Thus, we can reduce the interaction that the training agent has with the learning agent without reducing the speed with which the learner develops its policy. In fact, by being intelligent about when the learner asks for help, we can even increase the learning speed for the same level of trainer interaction. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A. G., Bradtke, S. J., & Singh, S. P. </author> <year> (1995). </year> <title> Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence, </journal> <volume> 72, </volume> <pages> 81-138. </pages>
Reference: <author> Clouse, J. A., & Utgoff, P. E. </author> <year> (1992). </year> <title> A teaching method for reinforcement learning. </title> <booktitle> Machine Learning: Proceedings of the Ninth International Conference (pp. </booktitle> <pages> 92-101). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The learner then updates its decision policy An Introspection Approach to Querying a Trainer 2 as if it itself had performed the training sequences. In another system <ref> (Clouse & Utgoff, 1992) </ref>, a human training agent interacts with the learner, occasionally providing actions that the learner then performs. Maclin and Shavlik (1994) employ if-then rules, compiling the trainer's instruction directly into the learner's policy via knowledge-based neural network techniques.
Reference: <author> Clouse, J. A. </author> <year> (1995). </year> <title> Learning from an automated training agent. Proceedings: ML95 Workshop on `Agents that Learn from Other Agents'. </title>
Reference-contexts: Gordon and Subramanian (1994) provided the trainer's domain information before training begins, and not during the on-line learning. The final system described above <ref> (Clouse, 1995) </ref> takes a controlled stochastic approach to providing training information, allowing the training agent to offer training direction a fixed percentage of the time. <p> In the experiments described below, we set this width parameter at different values to analyze different learners. An Introspection Approach to Querying a Trainer 4 4 Empirical Study The experiments described below test the Introspection Approach against an approach in which the learning agent request help randomly <ref> (similar to Clouse 1995) </ref>. In the following section we lay out the details of the experimental study, describing the problem domain, the training agents, and the learning algorithm.
Reference: <author> Gordon, D., & Subramanian, D. </author> <year> (1994). </year> <title> A multistrategy learning scheme for agent knowledge acquisition. </title> <journal> Informatica, </journal> <volume> 17, </volume> <pages> 331-346. </pages>
Reference: <author> Lin, </author> <month> Long-Ji </month> <year> (1992). </year> <title> Self-improving reactive agents based on reinforcement learning, planning and teaching. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 293-321. </pages>
Reference: <author> Lin, </author> <month> Long-Ji </month> <year> (1993). </year> <title> Scaling up reinforcement learning for robot control. </title> <booktitle> Machine Learning: Proceedings of the Tenth International Conference (pp. </booktitle> <pages> 182-189). </pages> <address> Amherst, MA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Maclin, R., & Shavlik, J. W. </author> <year> (1994). </year> <title> Incorporating advice into agents that learn from reinforcements. </title> <booktitle> Proceedings of the Twelfth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 694-699). </pages> <address> Seattle, WA: MIT Press. </address> <note> An Introspection Approach to Querying a Trainer 11 Sutton, </note> <author> R. S. </author> <year> (1984). </year> <title> Temporal credit assignment in reinforcement learning. </title> <type> Doctoral dissertation, </type> <institution> Department of Computer and Information Science, University of Massachusetts, </institution> <address> Amherst, MA. </address>
Reference: <author> Watkins, C.J.C.H. </author> <year> (1989). </year> <title> Learning with delayed rewards. </title> <type> Doctoral dissertation, </type> <institution> Psychology Department, Cambridge University. </institution>
Reference-contexts: In the case of the optimal trainer, this action is optimal. The other trainers may provide an optimal action, but they may also provide an action that is not optimal. 4.3 Learning Algorithm In all of the experiments, the learner employs the reinforcement learning technique Q-learning <ref> (Watkins, 1989) </ref> to develop its policy. The Q-functions for each of the four actions are stored in separate tables. The Q-value for a particular state and action, Q (s; a), is just the value in the sth location of the table for action a.
References-found: 8

