URL: http://www.cse.unsw.edu.au/~quinlan/miniboost.ps
Refering-URL: http://www.cse.unsw.edu.au/~quinlan/
Root-URL: 
Email: quinlan@cse.unsw.edu.au  
Title: MiniBoosting Decision Trees  
Author: J. R. Quinlan 
Date: 2052  
Address: Sydney Australia  
Affiliation: School of Computer Science and Engineering University of New South Wales  
Note: Journal of Artificial Intelligence Research X (1998) XX-XX Submitted 7/98; published XX/98  
Abstract: Boosting, introduced by Freund and Schapire, is a method for generating an ensemble of classifiers by successive reweightings of the training cases. We study boosting in the context of small ensembles of decision trees, showing that the reweighting procedure can be improved and that the resulting ensemble can be represented by a single decision tree. This tree is large, attesting to the complexity of boosted classifiers, but simplifications that do not affect its performance on the training data destroy the benefits of boosting.
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W., Kibler, D., & Albert, M. K. </author> <year> (1991). </year> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 37-66. </pages>
Reference-contexts: 1. Introduction Learning to predict the categories to which objects belong by analysing a collection of training cases is one of the most studied areas of Machine Learning. Well-tried methods for constructing a classifier or function that maps from object descriptions to class names include instance-based approaches <ref> (Aha, Kibler, & Albert, 1991) </ref>, neural networks (Mc-Clelland & Rumelhart, 1988), decision trees (Breiman, Friedman, Olshen, & Stone, 1984; Quinlan, 1993), decision lists (Rivest, 1989), naive Bayesian learners (Domingos & Pazzani, 1996), and inference nets (Pearl, 1988).
Reference: <author> Bauer, E., & Kohavi, R. </author> <year> (1998). </year> <title> An empirical comparison of voting classification algorithms: bagging, boosting, and variants. </title> <note> Machine Learning (to appear). </note>
Reference-contexts: The number of leaves of the decision tree (or, equivalently, the number of separate regions) is a useful measure of the complexity of the classifier. 4. MiniBoosting Most studies of boosting use a substantial ensemble of classifiers. For example, <ref> (Bauer & Kohavi, 1998) </ref> uses 25 classifiers, (Freund & Schapire, 1996) and (Breiman, 1998) use 100 classifiers, and (Schapire, Freund, Bartlett, & Lee, 1998) extends this to 1000. In contrast to this assumption, the intention here is to explore boosting with a minimal number of trials.
Reference: <author> Breiman, L. </author> <year> (1996). </year> <title> Bagging predictors. </title> <journal> Machine Learning, </journal> <volume> 24, </volume> <pages> 123-140. </pages>
Reference-contexts: The sample is enlarged by including some training cases on which the classifier performs poorly, a new classifier is generated from the enlarged sample, and the process repeats. * Bagging <ref> (Breiman, 1996) </ref>: Repeated bootstrap samples are drawn with replacement from the training cases, and a classifier constructed from each. * Boosting (Freund & Schapire, 1997): Each training case has an associated weight. <p> Experiments and Results The effectiveness of MiniBoost has been evaluated using the same datasets and experimental procedure as a previous study (Quinlan, 1996a) comparing AdaBoost with bagging <ref> (Breiman, 1996) </ref>. Twenty-seven classification tasks, whose characteristics are summarized in the Appendix, cover a broad spectrum of properties such as size, numbers and types of attributes, numbers of classes, and accuracy achievable.
Reference: <author> Breiman, L. </author> <year> (1998). </year> <title> Bias, variance, and arcing classifiers. </title> <journal> Machine Learning, </journal> <note> (to appear). </note>
Reference-contexts: MiniBoosting Most studies of boosting use a substantial ensemble of classifiers. For example, (Bauer & Kohavi, 1998) uses 25 classifiers, (Freund & Schapire, 1996) and <ref> (Breiman, 1998) </ref> use 100 classifiers, and (Schapire, Freund, Bartlett, & Lee, 1998) extends this to 1000. In contrast to this assumption, the intention here is to explore boosting with a minimal number of trials. With two trials, the boosted classifier is equivalent to whichever of the classifiers has 1.
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth. </publisher>
Reference: <author> Dietterich, T. G. </author> <year> (1997). </year> <journal> Machine-learning research. AI Magazine, </journal> <volume> 18:4, </volume> <pages> 97-136. </pages>
Reference: <author> Domingos, P., & Pazzani, M. J. </author> <year> (1996). </year> <title> Beyond independence: conditions for the optimality of the simple bayesian classifier. </title> <booktitle> In Proceedings Thirteenth International Conference on Machine Learning, </booktitle> <pages> pp. 105-112. </pages> <address> San Francisco: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Well-tried methods for constructing a classifier or function that maps from object descriptions to class names include instance-based approaches (Aha, Kibler, & Albert, 1991), neural networks (Mc-Clelland & Rumelhart, 1988), decision trees (Breiman, Friedman, Olshen, & Stone, 1984; Quinlan, 1993), decision lists (Rivest, 1989), naive Bayesian learners <ref> (Domingos & Pazzani, 1996) </ref>, and inference nets (Pearl, 1988). The "standard" application of one of these methods results in a single classifier, but there has recently been considerable interest in ensembles of classifiers (Bauer & Kohavi, 1998; Dietterich, 1997).
Reference: <author> Freund, Y., & Schapire, R. E. </author> <year> (1996). </year> <title> Experiments with a new boosting algorithm. </title> <booktitle> In Proceedings Thirteenth International Conference on Machine Learning, </booktitle> <pages> pp. 148-156. </pages> <address> San Francisco: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The number of leaves of the decision tree (or, equivalently, the number of separate regions) is a useful measure of the complexity of the classifier. 4. MiniBoosting Most studies of boosting use a substantial ensemble of classifiers. For example, (Bauer & Kohavi, 1998) uses 25 classifiers, <ref> (Freund & Schapire, 1996) </ref> and (Breiman, 1998) use 100 classifiers, and (Schapire, Freund, Bartlett, & Lee, 1998) extends this to 1000. In contrast to this assumption, the intention here is to explore boosting with a minimal number of trials.
Reference: <author> Freund, Y., & Schapire, R. E. </author> <year> (1997). </year> <title> A decision-theoretic generalization of on-line learning and an application to boosting. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 55, </volume> <pages> 119-139. </pages>
Reference-contexts: sample is enlarged by including some training cases on which the classifier performs poorly, a new classifier is generated from the enlarged sample, and the process repeats. * Bagging (Breiman, 1996): Repeated bootstrap samples are drawn with replacement from the training cases, and a classifier constructed from each. * Boosting <ref> (Freund & Schapire, 1997) </ref>: Each training case has an associated weight. At every iteration a classifier is built from the weighted training cases and each case is then reweighted according to whether or not it is misclassified. c fl1998 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved. <p> Boosting has so far proved to be the most effective of these because it focuses attention on the difficult cases using a multiplicative reweighting strategy derived from Winnow (Littlestone & Warmuth, 1994). This description of boosting follows Freund and Schapire's AdaBoost.M1 <ref> (Freund & Schapire, 1997) </ref>. We assume a set of training cases i = 1,2,...,N . At each repetition or trial t = 1,2,...,T , case i has weight w t [i], where w 1 [i] = 1/N for all i.
Reference: <author> Littlestone, N., & Warmuth, M. K. </author> <year> (1994). </year> <title> The weighted majority algorithm. </title> <journal> Information and Computation, </journal> <volume> 108, </volume> <pages> 212-261. </pages>
Reference-contexts: Boosting has so far proved to be the most effective of these because it focuses attention on the difficult cases using a multiplicative reweighting strategy derived from Winnow <ref> (Littlestone & Warmuth, 1994) </ref>. This description of boosting follows Freund and Schapire's AdaBoost.M1 (Freund & Schapire, 1997). We assume a set of training cases i = 1,2,...,N .
Reference: <author> McClelland, J. L., & Rumelhart, D. E. </author> <year> (1988). </year> <title> Experiments in Parallel Distributed Processing. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic Reasoning in Intelligent Systems. </title> <address> San Francisco: </address> <publisher> Morgan Kaufmann. 13 Quinlan Quinlan, </publisher> <editor> J. R. </editor> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: a classifier or function that maps from object descriptions to class names include instance-based approaches (Aha, Kibler, & Albert, 1991), neural networks (Mc-Clelland & Rumelhart, 1988), decision trees (Breiman, Friedman, Olshen, & Stone, 1984; Quinlan, 1993), decision lists (Rivest, 1989), naive Bayesian learners (Domingos & Pazzani, 1996), and inference nets <ref> (Pearl, 1988) </ref>. The "standard" application of one of these methods results in a single classifier, but there has recently been considerable interest in ensembles of classifiers (Bauer & Kohavi, 1998; Dietterich, 1997).
Reference: <author> Quinlan, J. R. </author> <year> (1996a). </year> <title> Bagging, boosting, </title> <booktitle> and c4.5. In Proceedings Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pp. 725-730. </pages> <publisher> AAAI Press. </publisher>
Reference-contexts: The procedure incorporating these small changes and using uniform voting weights will be referred to as MiniBoost to distinguish it from three-trial AdaBoost. 4 MiniBoosting Decision Trees 5. Experiments and Results The effectiveness of MiniBoost has been evaluated using the same datasets and experimental procedure as a previous study <ref> (Quinlan, 1996a) </ref> comparing AdaBoost with bagging (Breiman, 1996). Twenty-seven classification tasks, whose characteristics are summarized in the Appendix, cover a broad spectrum of properties such as size, numbers and types of attributes, numbers of classes, and accuracy achievable.
Reference: <author> Quinlan, J. R. </author> <year> (1996b). </year> <title> Improved use of continuous attributes in c4.5. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4, </volume> <pages> 77-90. </pages>
Reference-contexts: Twenty-seven classification tasks, whose characteristics are summarized in the Appendix, cover a broad spectrum of properties such as size, numbers and types of attributes, numbers of classes, and accuracy achievable. Ten repetitions of ten-fold cross-validation were carried out with each dataset and C4.5 Release 8 <ref> (Quinlan, 1996b) </ref> was used as the underlying decision tree generator. A summary of the results on unseen test cases appears in Table 1.
Reference: <author> Rivest, R. </author> <year> (1989). </year> <title> Learning decision lists. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <pages> 229-246. </pages>
Reference-contexts: Well-tried methods for constructing a classifier or function that maps from object descriptions to class names include instance-based approaches (Aha, Kibler, & Albert, 1991), neural networks (Mc-Clelland & Rumelhart, 1988), decision trees (Breiman, Friedman, Olshen, & Stone, 1984; Quinlan, 1993), decision lists <ref> (Rivest, 1989) </ref>, naive Bayesian learners (Domingos & Pazzani, 1996), and inference nets (Pearl, 1988). The "standard" application of one of these methods results in a single classifier, but there has recently been considerable interest in ensembles of classifiers (Bauer & Kohavi, 1998; Dietterich, 1997).
Reference: <author> Schapire, R. E., Freund, Y., Bartlett, P., & Lee, W. S. </author> <year> (1998). </year> <title> Boosting the margin: a new explanation for the effectiveness of voting methods. </title> <note> Annals of Statistics (to appear). </note>
Reference-contexts: MiniBoosting Most studies of boosting use a substantial ensemble of classifiers. For example, (Bauer & Kohavi, 1998) uses 25 classifiers, (Freund & Schapire, 1996) and (Breiman, 1998) use 100 classifiers, and <ref> (Schapire, Freund, Bartlett, & Lee, 1998) </ref> extends this to 1000. In contrast to this assumption, the intention here is to explore boosting with a minimal number of trials. With two trials, the boosted classifier is equivalent to whichever of the classifiers has 1. <p> But MML will select C 1:2:3 only if it can be encoded in a shorter message than C 0 1:2:3 , even though the latter is much simpler (by a factor of about 20, on average). This difficulty echoes Webb's arguments against Occam's Razor (Webb, 1996). * <ref> (Schapire et al., 1998) </ref> ascribes the power of boosting and other voting methods to improving the distribution of margin for the training cases. Roughly speaking, the margin of a case is determined by the proportion of votes amassed by the correct class.
Reference: <author> Wallace, C. S., & Patrick, J. D. </author> <year> (1993). </year> <title> Coding decision trees. </title> <journal> Machine Learning, </journal> <volume> 11, </volume> <pages> 7-22. </pages>
Reference-contexts: The reduced form C 0 1:2:3 of this merged tree raises some interesting philosophical issues: * Consider a collection of alternative theories derived from the same training data. By the Minimum Message Length Principle <ref> (Wallace & Patrick, 1993) </ref>, the best choice is the theory that minimizes the cost of a message encoding the theory and the training data given the theory.
Reference: <author> Webb, G. I. </author> <year> (1996). </year> <title> Further experimental evidence against the utility of occam's razor. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4, </volume> <pages> 397-417. </pages>
Reference-contexts: This last point is reminiscent of the findings reported in <ref> (Webb, 1996) </ref>. Rather than eliminating empty regions of the instance space, Webb created them intentionally by introducing additional tests at some leaves. <p> But MML will select C 1:2:3 only if it can be encoded in a shorter message than C 0 1:2:3 , even though the latter is much simpler (by a factor of about 20, on average). This difficulty echoes Webb's arguments against Occam's Razor <ref> (Webb, 1996) </ref>. * (Schapire et al., 1998) ascribes the power of boosting and other voting methods to improving the distribution of margin for the training cases. Roughly speaking, the margin of a case is determined by the proportion of votes amassed by the correct class.
Reference: <author> Wolpert, D. H. </author> <year> (1992). </year> <title> Stacked generalization. </title> <booktitle> Neural Networks, </booktitle> <volume> 5, </volume> <pages> 241-259. </pages> <booktitle> 14 MiniBoosting Decision Trees </booktitle>
Reference-contexts: Multiple classifiers have been shown to lead to improved predictive accuracy when classifying objects that are not among the training cases. Once again, there is considerable diversity in the methods used to assemble the ensembles, including: * Stacking <ref> (Wolpert, 1992) </ref>: The descriptions of the training cases are extended to include the results of classifying the cases with an initial selection of classifiers.
References-found: 19

