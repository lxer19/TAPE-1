URL: ftp://ftp.cs.virginia.edu/pub/techreports/IPC-94-02.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/techreports/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Bayesian Estimation and the Kalman Filter  
Author: Allen L. Barker Donald E. Brown Worthy N. Martin 
Note: This research was sponsored in part by the Jet Propulsion Laboratory under grant number 95772.  
Address: Charlottesville, VA 22901  
Affiliation: Institute for Parallel Computation School of Engineering and Applied Science University of Virginia  
Date: July 15, 1994 (Revised Sept. 19, 1994)  
Pubnum: IPC-TR-94-002  
Abstract-found: 0
Intro-found: 1
Reference: [Ber85] <author> James O. Berger. </author> <title> Statistical Decision Theory and Bayesian Analysis. </title> <publisher> Springer-Verlag, </publisher> <year> 1985. </year>
Reference-contexts: The results we present are really just a repackaging of standard results in optimal estimation theory and Bayesian analysis, following mainly from references <ref> [Med69, JH69, Sal89, Ber85] </ref>. We hope, though, that this paper will provide useful results which can be put to immediate practical use. We adopt a Bayesian approach because it lends itself to a straightforward, intuitive derivation.
Reference: [Ber92] <author> L. Mark Berliner. </author> <title> Statistics, probability and chaos. </title> <journal> Statistical Science, </journal> <volume> 7(1) </volume> <pages> 69-122, </pages> <year> 1992. </year>
Reference-contexts: The problem with t q &lt; t n is a smoothing problem, which can be handled similarly to the filtering problem. 8. Equation (1) is an iterated function system; such systems have received much recent study in relation to nonlinear dynamics and chaos theory. See <ref> [Ber92, CY92] </ref> for reviews of such systems from a statistical viewpoint. 3 Calculating the Posterior Density In this section we derive an expression for the desired posterior density p (x q jZ) in terms of the known density functions.
Reference: [BSF88] <author> Yaakov Bar-Shalom and Thomas E. Fortmann. </author> <title> Tracking and Data Association. </title> <publisher> Academic Press, </publisher> <year> 1988. </year>
Reference-contexts: There are a number of variations and extensions of the basic Kalman filter algorithm to address, among other things, nonlinearities in the motion model and numerical stability. See [CT84] for a survey. See also [MS83] for another Bayesian derivation of the Kalman filter, and <ref> [BSF88] </ref> for a least squares approach and many additional references. 6 Examples In this section we give some examples applying the preceeding results. A general approach for a practical (as opposed to theoretical) application follows.
Reference: [CT84] <author> Chaw-Bing Chang and John A. Tabaczynski. </author> <title> Application of state estimation to target tracking. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> AC-29(2), </volume> <year> 1984. </year>
Reference-contexts: The algorithm changes in that k+1 k+1;k k + u k in (22) and (23). 8. There are a number of variations and extensions of the basic Kalman filter algorithm to address, among other things, nonlinearities in the motion model and numerical stability. See <ref> [CT84] </ref> for a survey. See also [MS83] for another Bayesian derivation of the Kalman filter, and [BSF88] for a least squares approach and many additional references. 6 Examples In this section we give some examples applying the preceeding results.
Reference: [CY92] <author> Sangit Chattergee and Mustafa R. Yilmaz. </author> <title> Chaos, </title> <journal> fractals and statistics. Statistical Science, </journal> <volume> 7(1) </volume> <pages> 49-121, </pages> <year> 1992. </year>
Reference-contexts: The problem with t q &lt; t n is a smoothing problem, which can be handled similarly to the filtering problem. 8. Equation (1) is an iterated function system; such systems have received much recent study in relation to nonlinear dynamics and chaos theory. See <ref> [Ber92, CY92] </ref> for reviews of such systems from a statistical viewpoint. 3 Calculating the Posterior Density In this section we derive an expression for the desired posterior density p (x q jZ) in terms of the known density functions.
Reference: [Dil94] <author> Dan Dill. </author> <title> Interactive TeX/Mathematica documents, </title> <month> Feb. </month> <year> 1994. </year>
Reference: [Gil77] <author> John Gill. </author> <title> Computational complexity of probabilistic turing machines. </title> <journal> SIAM Journal on Computing, </journal> <volume> 6(4) </volume> <pages> 675-695, </pages> <year> 1977. </year>
Reference-contexts: Note that the size of the tape increases with each time step to give a constructively infinite tape. A probabilistic Turing machine [San69] can be characterized as a Tur-ing machine where the function M ove q takes an additional, discrete, independent random variable as an argument <ref> [Gil77] </ref>. This random variable is restricted to have finite range, i.e., it can have only finitely many possible values. Thus, for example, in (29) M ove q is replaced with M ove q (q k ; c k ; ~), so the next state is also a random variable, etc. <p> Thus, for example, in (29) M ove q is replaced with M ove q (q k ; c k ; ~), so the next state is also a random variable, etc. Note that our formulation is slightly different from those in [San69] and <ref> [Gil77] </ref>. A nondeterministic Turing machine, in standard computer science terminology, can be characterized as a Turing machine where M ove q is multiple-valued. The nondeterministic machine essentially branches and computes the results for all possible q values at each time step.
Reference: [GJ79] <author> Michael R. Garey and David S. Johnson. </author> <title> Computers and Intractability. </title> <publisher> Freeman, </publisher> <year> 1979. </year>
Reference-contexts: As another example, the observer may not be able to read some tape cells at all, but can read all other state information perfectly. This type of model relates to another characterization of nondeterministic machines, where the machine is allowed to "guess an input structure" <ref> [GJ79] </ref>. 6.2 Example 2: Dynamic classification of an input signal Consider a discrete-time signal s (t k ) 2 &lt; d , k = 0; 1; 2; : : :.
Reference: [HU79] <author> John E. Hopcroft and Jeffrey D. Ullman. </author> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison-Wesley, </publisher> <year> 1979. </year>
Reference-contexts: Example 3 concerns tracking a moving object, and is worked out in some detail. In this example the model is linear-Gaussian, thus all random variables are continuous and the Kalman filtering algorithm can be applied. 6.1 Example 1: An imperfectly observed, probabilistic Tur ing machine A Turing machine <ref> [Men64, HU79] </ref> is a model of effective computation; no known deterministic computations have been shown to be non-computable in principle by a Turing machine. Informally, a Turing machine can be thought of as a semi-infinite tape of symbols scanned by a tape head.
Reference: [JH69] <author> Arthur E. Bryson Jr. and Yu-Chi Ho. </author> <title> Applied Optimal Control. </title> <publisher> Ginn and Company, </publisher> <year> 1969. </year>
Reference-contexts: The results we present are really just a repackaging of standard results in optimal estimation theory and Bayesian analysis, following mainly from references <ref> [Med69, JH69, Sal89, Ber85] </ref>. We hope, though, that this paper will provide useful results which can be put to immediate practical use. We adopt a Bayesian approach because it lends itself to a straightforward, intuitive derivation.
Reference: [JW88] <author> Richard A. Johnson and Dean W. Wichern. </author> <title> Applied Multivari-ate Statistical Analysis. </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: The Gaussian in Figure 3 is not normalized; from Theorem 2 we know its normalizing constant is the reciprocal of another Gaussian independent of x. We will also need the following result, that linear transformations of Gaussian random vectors are Gaussian random vectors. See e.g. <ref> [JW88] </ref> for a proof. Theorem 3 Let ~x, x, b, B and Q be defined as above. Let d be an s fi 1 matrix of constants. Let ~x have density p (x) = G s (b; B; x).
Reference: [MCTW86] <author> Shozo Mori, Chee-Yee Chong, Edison Tse, and Richard P. Wishner. </author> <title> Tracking and classifying multiple targets without a priori identification. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> AC-31(5), </volume> <year> 1986. </year>
Reference-contexts: In this case it is more convenient to define the recursion in terms of a pair of mutually recursive equations for p (x k jZ k1 ) and p (x k jZ k ) (see note 2). Notes: 1. See <ref> [MCTW86] </ref> for a measure-theoretic treatment of mixed continuous and discrete random vectors. 2.
Reference: [Med69] <author> J. S. Meditch. </author> <title> Stochastic Optimal Linear Estimation and Control. </title> <publisher> McGraw Hill, </publisher> <year> 1969. </year> <month> 25 </month>
Reference-contexts: The results we present are really just a repackaging of standard results in optimal estimation theory and Bayesian analysis, following mainly from references <ref> [Med69, JH69, Sal89, Ber85] </ref>. We hope, though, that this paper will provide useful results which can be put to immediate practical use. We adopt a Bayesian approach because it lends itself to a straightforward, intuitive derivation. <p> For this reason computing (22) is often called the prediction step and computing (24) the measurement update step. 2. The variable lettering in (18) and (19) follows that in <ref> [Med69] </ref>. 3. Equation (12) is usually preferable to (11) for evaluating C since it uses fewer inverses. Similarly (14) is preferable to (13) for computing 12 c. The form of the equations for computing c and C starting with the Kalman gain matrix, given in [Med69] and elsewhere, is preferable computationally <p> and (19) follows that in <ref> [Med69] </ref>. 3. Equation (12) is usually preferable to (11) for evaluating C since it uses fewer inverses. Similarly (14) is preferable to (13) for computing 12 c. The form of the equations for computing c and C starting with the Kalman gain matrix, given in [Med69] and elsewhere, is preferable computationally to the form we have presented.
Reference: [Men64] <author> Elliot Mendelson. </author> <title> Introduction to Mathematical Logic. </title> <publisher> Van Nostrand, </publisher> <year> 1964. </year>
Reference-contexts: Example 3 concerns tracking a moving object, and is worked out in some detail. In this example the model is linear-Gaussian, thus all random variables are continuous and the Kalman filtering algorithm can be applied. 6.1 Example 1: An imperfectly observed, probabilistic Tur ing machine A Turing machine <ref> [Men64, HU79] </ref> is a model of effective computation; no known deterministic computations have been shown to be non-computable in principle by a Turing machine. Informally, a Turing machine can be thought of as a semi-infinite tape of symbols scanned by a tape head.
Reference: [MS83] <author> Richard J. Meinhold and Nozer D. Singpurwalla. </author> <title> Understanding the kalman filter. </title> <journal> The American Statistician, </journal> <volume> 37(2) </volume> <pages> 123-127, </pages> <year> 1983. </year>
Reference-contexts: There are a number of variations and extensions of the basic Kalman filter algorithm to address, among other things, nonlinearities in the motion model and numerical stability. See [CT84] for a survey. See also <ref> [MS83] </ref> for another Bayesian derivation of the Kalman filter, and [BSF88] for a least squares approach and many additional references. 6 Examples In this section we give some examples applying the preceeding results. A general approach for a practical (as opposed to theoretical) application follows.
Reference: [Rab89] <author> Lawrence R. Rabiner. </author> <title> A tutorial on hidden markov models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77(2) </volume> <pages> 257-285, </pages> <year> 1989. </year>
Reference-contexts: Since finding the optimal parameter vector is typically intractable, approximate algorithms like hill-climbing or annealing are used. See <ref> [Rab89] </ref> for a tutorial introduction to Markov models in speech recognition and a discussion of the many practical problems that arise. 18 6.3 Example 3: Tracking a moving object For this example we modify our notation slightly.
Reference: [Sal89] <author> D. J. Salmond. </author> <title> Tracking in uncertain environments. </title> <type> Technical report, </type> <institution> Royal Aerospace Establishment, Farnborough, Hants, UK, </institution> <month> September </month> <year> 1989. </year> <title> From a Ph.D. </title> <type> thesis, </type> <institution> University of Sussex. </institution>
Reference-contexts: The results we present are really just a repackaging of standard results in optimal estimation theory and Bayesian analysis, following mainly from references <ref> [Med69, JH69, Sal89, Ber85] </ref>. We hope, though, that this paper will provide useful results which can be put to immediate practical use. We adopt a Bayesian approach because it lends itself to a straightforward, intuitive derivation. <p> Then M r (a; A; Qx) + M s (b; B; x) = M s (c; C; x) + M r (a; A + QBQ 0 ; Qb): This theorem was taken from <ref> [Sal89] </ref>, Appendix A, and a proof can be found there. The proof is straightforward, though somewhat tedious, and involves completing the square and applying the matrix inversion lemma (see note 1).
Reference: [San69] <author> Eugene S. Santos. </author> <title> Probabilistic turing machines and computability. </title> <journal> Proceedings of the American Mathematical Society, </journal> <volume> 22 </volume> <pages> 704-710, </pages> <year> 1969. </year>
Reference-contexts: Note that the size of the tape increases with each time step to give a constructively infinite tape. A probabilistic Turing machine <ref> [San69] </ref> can be characterized as a Tur-ing machine where the function M ove q takes an additional, discrete, independent random variable as an argument [Gil77]. This random variable is restricted to have finite range, i.e., it can have only finitely many possible values. <p> Thus, for example, in (29) M ove q is replaced with M ove q (q k ; c k ; ~), so the next state is also a random variable, etc. Note that our formulation is slightly different from those in <ref> [San69] </ref> and [Gil77]. A nondeterministic Turing machine, in standard computer science terminology, can be characterized as a Turing machine where M ove q is multiple-valued. The nondeterministic machine essentially branches and computes the results for all possible q values at each time step.
Reference: [Wol91] <author> Stephen Wolfram. </author> <title> Mathematica: A System for Doing Mathematics by Computer. </title> <publisher> Addison Wesley, </publisher> <year> 1991. </year> <month> 26 </month>
References-found: 19

