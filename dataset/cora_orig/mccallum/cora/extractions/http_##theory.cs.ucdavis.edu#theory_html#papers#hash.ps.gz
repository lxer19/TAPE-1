URL: http://theory.cs.ucdavis.edu/theory_html/papers/hash.ps.gz
Refering-URL: http://theory.cs.ucdavis.edu/
Root-URL: http://www.cs.ucdavis.edu
Email: qi@cs.ucdavis.edu  martel@cs.ucdavis.edu  
Title: Design and Analysis of Hashing Algorithms with Cache Effects  
Author: Hongbin Qi Charles U. Martel 
Note: This work was supported by NSF grant CCR 94-03651.  
Date: August 10, 1998  
Address: Davis  Davis  
Affiliation: Department of Computer Science University of California,  Department of Computer Science University of California,  
Abstract: This paper investigates the performance of hashing algorithms by both an experimental and an analytical approach. We examine the performance of three classical hashing algorithms: chaining, double hashing and linear probing. Our experimental results show that, despite the theoretical superiority of chaining and double hashing, linear probing outperforms both for random lookups. We explore variations on the data structures used by these traditional algorithms to improve their spatial locality and hence cache performance. Our results also help determine the optimal table size for a given key set size. In addition to time, we also study the average number of probes and cache misses incurred by these algorithms. For most of the algorithms studied in this paper, our analysis agrees with the experimental results. As a supplementary result, we examine the behavior of random lookups to a hash table. This provides a simple way to estimate the cache miss penalties of different machines. Two conclusions can be drawn from this study. First, cache effects have a significant influence on the performance of hashing algorithms. Second, it is possible to predict fairly accurately the performance of different hashing algorithms based on the algorithm configurations and cache structures. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Digital Semiconductor 21164 ALPHA Microprocessor Hardware Reference Manual. Digital Equipment Corporation, Maynard, </institution> <address> MA, </address> <year> 1997. </year>
Reference-contexts: The ALPHA has an 8K byte on-chip direct mapped L1 data-cache and a 96K 3-way set-associative on-chip L2 cache, both of which use 32-byte cache blocks <ref> [1] </ref>. Please refer to appendix A for other relevant system parameters. The DECstation is an older (and slower) architecture while the ALPHA is a faster and more modern machine. <p> All the hash tables are cache-aligned, that is, no table entry spans two cache blocks. On the DECstation, the memory allocating function malloc () returns cache-aligned memory blocks automatically. On the ALPHA, however, some pointer adjustment must be done explicitly by the user to achieve alignment <ref> [1] </ref>. 4 Results of Random Lookups Before examing the performance of hash tables, we explore the behavior of random lookups to a hash table.
Reference: [2] <author> A. Srivastava and A. Eustace. </author> <title> ATOM: A system for building customized program analysis tools. </title> <booktitle> In Proceedings of the 1994 ACM Symposium on Programming Language Design and Implementation, </booktitle> <pages> pages 196-205, </pages> <year> 1994. </year>
Reference-contexts: simpler and makes linear probing more attractive. 6.4.1 Analysis of Other Settings Chaining has some of the same complications as linear probing: memory locations at the start of long chains are more likely to be in cache than others. 6.5 Simulation of the Number of Cache misses We used Atom <ref> [2] </ref> to simulate the cache behavior of each hashing algorithm. We simulated a direct-mapped single-level cache which has exactly the same configuration as the DECstation cache. We chose to simulate the DECstation cache because a single-level cache would make our experimental results easier to analyze and more representative.
Reference: [3] <author> John Black, Charles Martel. </author> <title> Designing fast graph data structures: an experimental approach. </title> <note> Preprint 1997. </note>
Reference-contexts: They also introduced an analytical model called collective analysis that helps predict cache performance [7]. Black and Martel presented simple alternatives to standard graph representations that substantially improved the performance of breadth-first-search and depth-first-search <ref> [3] </ref>. A recent hashing paper [11] develops a collision resolution scheme which can reduce the probes compared to double hashing for some very specialized settings.
Reference: [4] <author> Thomas Cormen, Charles Leiserson and Ronald Rivest. </author> <title> Introduction to algorithms. </title> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Thus it is important to find the best practical hashing schemes and to understand the empirical behavior of hashing. While hashing algorithms have been studied extensively under traditional cost models, there has been little prior work focusing on their cache effects. Chaining, double hashing and linear probing <ref> [4, 6] </ref> are the three most classic hashing algorithms. Traditionally, chaining and double hashing are considered superior to linear probing because they disperse the keys better and thus require fewer probes.
Reference: [5] <author> William Feller. </author> <title> An introduction to probability theory and its applications. Volume 1, second edition. </title> <publisher> John Wiley and Sons Publishing Company, </publisher> <year> 1957. </year>
Reference-contexts: the same for both, and the expected number of bounces to insert all items is exactly the same as the expected number of double hashes done in PDH. 8 We can easily analyze round 1 using known results on the probability of a certain number of balls in a bin <ref> [5] </ref>: Let P j be the probability we end up with j balls in a bin at the end of Round 1. P j = j! Let P B+ denote the expected fraction of slots which are full after Round 1.
Reference: [6] <author> Donald Knuth. </author> <title> Sorting and searching, </title> <booktitle> the art of computer programming, </booktitle> <volume> Volume 3. </volume> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1973. </year>
Reference-contexts: Thus it is important to find the best practical hashing schemes and to understand the empirical behavior of hashing. While hashing algorithms have been studied extensively under traditional cost models, there has been little prior work focusing on their cache effects. Chaining, double hashing and linear probing <ref> [4, 6] </ref> are the three most classic hashing algorithms. Traditionally, chaining and double hashing are considered superior to linear probing because they disperse the keys better and thus require fewer probes. <p> In the following subsections, we give a brief analysis of the number of probes and jumps of each algorithm. Although we rely partly on approximations, our analysis mostly agrees with the experimental results. 6.1.1 Analysis of Linear Probing It is shown in <ref> [6] </ref> that the average number of probes incurred by an unsuccessful search in a table with load factor ff = n=T is given by 1 + 1=(1 ff) 2 (1) The expected number of probes per insertion into a table whose final load factor is is thus given by Z 1 <p> but are not as good as for estimating the jumps. 6.2.1 Analysis of Chaining The probability that a linked list is of length j, is just P j as in equation 6 of the prior section, and the expected number of probes for successfull and unsuccessful search are well known <ref> [6] </ref>. In our experiments each linked list node took up an entire cache block, so each probe is a jump. <p> For T &gt; C, for each probe into the table, the probability that the location probed is not in the cache can be approximated by 10 T C (9) And it is well known that the expected number of probes for a random unsuccessful search is 1 1ff <ref> [6] </ref>.
Reference: [7] <author> Anthony LaMarca and Richard Ladner. </author> <title> The influence of caches on the performance of heaps. </title> <journal> Journal of Experimental Algorithms, </journal> <volume> Volume 1, </volume> <year> 1996. </year>
Reference-contexts: They used these techniques to tune cache performance of the SPEC92 benchmarks and achieved significant speedups. The prototypes of the two major techniques used in this paper, packing and aligning, can be found in [9]. LaMarca and Ladner examined the influence of caches on the performance of heaps <ref> [7] </ref>. They presented optimizations that significantly reduced cache misses and improved the overall performance, and were able to extend their results to improve the performance of sorting algorithms [8]. They also introduced an analytical model called collective analysis that helps predict cache performance [7]. <p> of caches on the performance of heaps <ref> [7] </ref>. They presented optimizations that significantly reduced cache misses and improved the overall performance, and were able to extend their results to improve the performance of sorting algorithms [8]. They also introduced an analytical model called collective analysis that helps predict cache performance [7]. Black and Martel presented simple alternatives to standard graph representations that substantially improved the performance of breadth-first-search and depth-first-search [3]. A recent hashing paper [11] develops a collision resolution scheme which can reduce the probes compared to double hashing for some very specialized settings.
Reference: [8] <author> Anthony LaMarca and Richard Ladner. </author> <title> The influence of caches on the performance of sorting. </title> <booktitle> In the Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <year> 1997. </year>
Reference-contexts: LaMarca and Ladner examined the influence of caches on the performance of heaps [7]. They presented optimizations that significantly reduced cache misses and improved the overall performance, and were able to extend their results to improve the performance of sorting algorithms <ref> [8] </ref>. They also introduced an analytical model called collective analysis that helps predict cache performance [7]. Black and Martel presented simple alternatives to standard graph representations that substantially improved the performance of breadth-first-search and depth-first-search [3].
Reference: [9] <author> Alvin Lebeck, David Wood. </author> <title> Cache profiling and the SPEC benchmarks: a case study. </title> <note> Preprint 1997. </note>
Reference-contexts: It is also possible to combine these predictions to get an accurate estimate of the time expense of hashing algorithms. 2 Related Work Lebeck and Wood showed several techniques that could be employed to improve cache performance <ref> [9] </ref>. They used these techniques to tune cache performance of the SPEC92 benchmarks and achieved significant speedups. The prototypes of the two major techniques used in this paper, packing and aligning, can be found in [9]. LaMarca and Ladner examined the influence of caches on the performance of heaps [7]. <p> Work Lebeck and Wood showed several techniques that could be employed to improve cache performance <ref> [9] </ref>. They used these techniques to tune cache performance of the SPEC92 benchmarks and achieved significant speedups. The prototypes of the two major techniques used in this paper, packing and aligning, can be found in [9]. LaMarca and Ladner examined the influence of caches on the performance of heaps [7]. They presented optimizations that significantly reduced cache misses and improved the overall performance, and were able to extend their results to improve the performance of sorting algorithms [8].
Reference: [10] <author> Ronald Rivest. </author> <title> The RC5 encryption algorithm. </title> <booktitle> Proceedings of the Second International Workshop on Fast Software Encryption. 1994, </booktitle> <address> Leuven, Belgium. </address>
Reference-contexts: A key term in hashing algorithms is load factor, which is the ratio of the number of keys stored in a table to the table size. Load factor describes the storage density of a hash table. We used two random integer generators. One was RC5 <ref> [10] </ref>, an encryption-decryption function suite, whose encryption function can be used as a random number generator. This function accepts a parameter, and generates the same random integer if passed the same parameter. <p> We can also use the ALT chart to estimate the cache miss penalties on machines with multiple caches, although more computation is needed. 5 Results of Hashing Algorithms We study successful searches and unsuccessful searches. To look at successful searches, we use RC5 <ref> [10] </ref> as the random integer generator. RC5 is an encryption-decryption function suite, whose encryption function can be used as a random number generator.
Reference: [11] <author> B. Smith, G. Heileman, and C. Abdallah. </author> <title> The Exponential Hash Function. </title> <journal> Journal of Experimental Algorithms, Vol.2, 1997. </journal> <volume> 13 14 15 16 </volume>
Reference-contexts: They also introduced an analytical model called collective analysis that helps predict cache performance [7]. Black and Martel presented simple alternatives to standard graph representations that substantially improved the performance of breadth-first-search and depth-first-search [3]. A recent hashing paper <ref> [11] </ref> develops a collision resolution scheme which can reduce the probes compared to double hashing for some very specialized settings.
References-found: 11

