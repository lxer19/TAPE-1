URL: http://www.eecs.umich.edu/~pmchen/papers/sigmetrics93.ps
Refering-URL: http://www.eecs.umich.edu/~pmchen/otherPapers.html
Root-URL: http://www.eecs.umich.edu
Email: pmchen@cs.Berkeley.EDU, pattrsn@cs.Berkeley.EDU  
Title: on Measurement and Modeling of Computer Systems. A New Approach to I/O Performance Evaluation Self-Scaling
Author: Peter M. Chen David A. Patterson 
Address: Berkeley  
Affiliation: Computer Science Division, University of California at  
Note: 1993 ACM SIGMETRICS Conference  
Abstract: Current I/O benchmarks suffer from several chronic problems: they quickly become obsolete, they do not stress the I/O system, and they do not help in understanding I/O system performance. We propose a new approach to I/O performance analysis. First, we propose a self-scaling benchmark that dynamically adjusts aspects of its workload according to the performance characteristic of the system being measured. By doing so, the benchmark automatically scales across current and future systems. The evaluation aids in understanding system performance by reporting how performance varies according to each of five workload parameters. Second, we propose predicted performance, a technique for using the results from the self-scaling evaluation to quickly estimate the performance for workloads that have not been measured. We show that this technique yields reasonably accurate performance estimates and argue that this method gives a far more accurate comparative performance evaluation than traditional single point benchmarks. We apply our new evaluation technique by measuring a SPARCstation 1+ with one SCSI disk, an HP 730 with one SCSI-II disk, a Sprite LFS DECstation 5000/200 with a three-disk disk array, a Convex C240 minisupercomputer with a four-disk disk array, and a Sol-bourne 5E/905 fileserver with a two-disk disk array. 
Abstract-found: 1
Intro-found: 1
Reference: [Anon85] <editor> Anon and et al., </editor> <title> ``A Measure of Transaction Processing Power'', </title> <journal> Datamation, </journal> <volume> 31, </volume> <month> 7 (April </month> <year> 1985), </year> <pages> 112-118. </pages>
Reference-contexts: Current I/O Benchmarks In this section, we examine current benchmarks used to evaluate I/O systems. The benchmarks we consider are Andrew [Howard88], TPC-B <ref> [Anon85, TPCA89, TPCB90] </ref>, Sdet [Gaede81, Gaede82, SPEC91b, SPEC91a], Bonnie [Bray90], and IOStone [Park90] 1 .
Reference: [Baker91] <author> M. G. Baker, J. H. Hartman, M. D. Kupfer, K. W. Shirriff and J. K. Ousterhout, </author> <title> ``Measurements of a Distributed File System'', </title> <booktitle> Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <month> October </month> <year> 1991. </year>
Reference: [Bechtolsheim90] <author> A. V. Bechtolsheim and E. H. Frank, </author> <title> ``Sun's SPARCstation 1: A Workstation for the 1990s'', </title> <booktitle> Procedures of the IEEE Computer Society International Conference (COMPCON), Spring 1990, </booktitle> <pages> 184-188. </pages>
Reference: [Berry89] <author> M. Berry and et al., </author> <title> ``The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers'', </title> <journal> International Journal of Supercomputing Applications, </journal> <month> Fall </month> <year> 1989. </year>
Reference-contexts: Good benchmarks also assist users in purchasing machines by allowing fair, relevant comparisons. Recent efforts to standardize benchmarks, such as SPEC [Scott90] and Perfect Club <ref> [Berry89] </ref>, have increased our understanding of computing performance and helped create a fair playing field on which companies can compete. These standardization efforts have focused on CPU-intensive applications [Scott90], however, and intentionally avoided I/O intensive applications [Berry89]. <p> Recent efforts to standardize benchmarks, such as SPEC [Scott90] and Perfect Club <ref> [Berry89] </ref>, have increased our understanding of computing performance and helped create a fair playing field on which companies can compete. These standardization efforts have focused on CPU-intensive applications [Scott90], however, and intentionally avoided I/O intensive applications [Berry89]. In this paper, we develop criteria for ideal I/O benchmarks and show how current I/O benchmarks fall short of these.
Reference: [Bray90] <author> T. Bray, </author> <title> Bonnie source code, </title> <booktitle> netnews posting, </booktitle> <year> 1990. </year>
Reference-contexts: Current I/O Benchmarks In this section, we examine current benchmarks used to evaluate I/O systems. The benchmarks we consider are Andrew [Howard88], TPC-B [Anon85, TPCA89, TPCB90], Sdet [Gaede81, Gaede82, SPEC91b, SPEC91a], Bonnie <ref> [Bray90] </ref>, and IOStone [Park90] 1 .
Reference: [Chen90] <author> P. M. Chen and D. A. Patterson, </author> <title> ``Maximizing Performance in a Striped Disk Array'', </title> <booktitle> Proceedings of the 1990 International Symposium on Computer Architecture, </booktitle> <address> Seattle WA, </address> <month> May </month> <year> 1990, </year> <pages> 322-331. </pages>
Reference-contexts: For instance, IOStone tries to exercise the memory hierarchy but touches only 1 MB of user data. Perhaps at the time IOStone was written 1 MB was a lot of data but no longer. One recent example of how I/O systems are evolving is disk arrays <ref> [Patterson88, Gibson91, Chen90, Salem86] </ref>. Disk arrays allow multiple I/Os to be in progress simultaneously. Most current I/O benchmark do not scale the number of processes issuing I/O, and hence are unable to properly stress disk arrays. <p> The DECstation [DECstation90] uses a three disk RAID disk array [Patterson88] with a 16 KB striping unit <ref> [Chen90] </ref> and is configured without redundancy. The SPECmark rating is a measure of the processor speed; ratings are relative to the speed of a VAX 11/780. The full name of the HP 730 is the HP Series 700 Model 730 [HP730]. 4.
Reference: [Chen92] <author> P. M. Chen, </author> <title> ``Input-Output Performance Evaluation: Self-Scaling Benchmarks, Predicted Performance'', </title> <institution> UCB/Computer Science Dpt. 92/714, University of California, </institution> <month> November </month> <year> 1992. </year> <type> Ph.D. dissertation. </type>
Reference-contexts: In this section, we set this performance point at 75% of the maximum performance. Using a simple iterative approach, it is possible to find a focal vector for which each workload parameter is simultaneously at its 75% performance point <ref> [Chen92] </ref>. Figure 5 shows results from a benchmark that self-scales all parameters. The system being measured is the one disk SPARCstation of Figure 2. <p> HP 730, Convex C240, Solbourne 5E/905 Figures 9 and 10 give selected graphs from self-scaling benchmark runs on an HP 730, Convex C240, and Solbourne 5E/905 (complete results can be found in <ref> [Chen92] </ref>. In this section, we highlight some insights gained from these benchmark results. extremely fast, is quite small (3 MB). <p> Figures 9a and 10 shows results from the self-scaling benchmark of a Convex C240. The curves are similar to the SPARCstation 1+, with three main differences: g Absolute performance is very high. File cache performance reaches 25 MB/s (see <ref> [Chen92] </ref>); disk performance reaches almost 10 MB/s (Figure 10) This high performance is due to Convex's 200 MB/s memory system and performance-focused (as opposed to cost-performance) implementation. g The effective file cache for the Convex is 800 MB. <p> Error is most closely correlated to the value of uniqueBytes (the analogous graphs of error versus sizeMean, readFrac, processNum and seqFrac are flat <ref> [Chen92] </ref>). Prediction is particularly poor near the border between performance regions. <p> Error is most closely correlated to the value of uniqueBytes (see <ref> [Chen92] </ref> for a full set of graphs). Prediction is particularly poor near the border between performance regions. As expected, sharp drops in performance lead to unstable throughput and poor prediction. This confirms our use of two distinct uniqueBytes regions for prediction versus a single focal point.
Reference: [DECstation90] <institution> DECstation 5000 Model 200 Technical Overview, Digital Equipment Corporation, </institution> <year> 1990. </year>
Reference-contexts: The DECstation <ref> [DECstation90] </ref> uses a three disk RAID disk array [Patterson88] with a 16 KB striping unit [Chen90] and is configured without redundancy. The SPECmark rating is a measure of the processor speed; ratings are relative to the speed of a VAX 11/780.
Reference: [Ferrari84] <author> D. Ferrari, </author> <booktitle> ``On the Foundations of Artificial Workload Design'', Proceedings of the 1984 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <year> 1984, </year> <pages> 8-14. </pages>
Reference-contexts: In this paper, a workload refers to a user-level program with parameter values for each of the above five parameters. This program spawns and controls several processes if necessary. The most important question in developing a synthetic workload is the question of representativeness <ref> [Ferrari84] </ref>.
Reference: [Gaede81] <author> S. </author> <title> Gaede, ``Tools for Research in Computer Workload Characterization'', Experimental Computer Performance and Evaluation, 1981. </title> <editor> D. Ferrari, M. Spadoni, </editor> <publisher> eds.. </publisher>
Reference-contexts: Current I/O Benchmarks In this section, we examine current benchmarks used to evaluate I/O systems. The benchmarks we consider are Andrew [Howard88], TPC-B [Anon85, TPCA89, TPCB90], Sdet <ref> [Gaede81, Gaede82, SPEC91b, SPEC91a] </ref>, Bonnie [Bray90], and IOStone [Park90] 1 .
Reference: [Gaede82] <author> S. </author> <title> Gaede, ``A Scaling Technique for Comparing Interactive System Capacities'', </title> <booktitle> 13th International Conference on Management and Performance Evaluation of Computer Systems, </booktitle> <year> 1982, </year> <pages> 62-67. </pages> <note> CMG 1982. </note>
Reference-contexts: Current I/O Benchmarks In this section, we examine current benchmarks used to evaluate I/O systems. The benchmarks we consider are Andrew [Howard88], TPC-B [Anon85, TPCA89, TPCB90], Sdet <ref> [Gaede81, Gaede82, SPEC91b, SPEC91a] </ref>, Bonnie [Bray90], and IOStone [Park90] 1 .
Reference: [Gibson91] <author> G. A. Gibson, </author> <title> ``Redundant Disk Arrays: Reliable, Parallel Secondary Storage'', </title> <institution> UCB/Computer Science Dpt. 91/613, University of California at Berkeley, </institution> <month> December </month> <year> 1991. </year> <note> also available from MIT Press, </note> <year> 1992. </year>
Reference-contexts: For instance, IOStone tries to exercise the memory hierarchy but touches only 1 MB of user data. Perhaps at the time IOStone was written 1 MB was a lot of data but no longer. One recent example of how I/O systems are evolving is disk arrays <ref> [Patterson88, Gibson91, Chen90, Salem86] </ref>. Disk arrays allow multiple I/Os to be in progress simultaneously. Most current I/O benchmark do not scale the number of processes issuing I/O, and hence are unable to properly stress disk arrays.
Reference: [HP730] <institution> HP Apollo Series 700 Model 730 PA-RISC Workstation, Hewlett-Packard, </institution> <year> 1992. </year>
Reference-contexts: The SPECmark rating is a measure of the processor speed; ratings are relative to the speed of a VAX 11/780. The full name of the HP 730 is the HP Series 700 Model 730 <ref> [HP730] </ref>. 4. A New Approach for I/O BenchmarksAn Overview We propose two new ideas in I/O benchmarks. First, we propose a benchmark that automatically scales its workload to the system being measured.
Reference: [Horning91] <author> R. Horning, L. Johnson, L. Thayer, D. Li, V. Meier, C. Dowdell and D. Roberts, </author> <title> ``System Design for a Low Cost PA-RISC Desktop Workstation'', </title> <booktitle> Procedures of the IEEE Computer Society International Conference (COMPCON), Spring 1991, </booktitle> <pages> 208-213. </pages>
Reference-contexts: This high performance is due to the fast memory system of the HP 730 (peak memory bandwidth is 264 MB/s) and to the use of a VLSI memory controller to accelerate cache-memory write backs <ref> [Horning91] </ref>. Figures 9a and 10 shows results from the self-scaling benchmark of a Convex C240. The curves are similar to the SPARCstation 1+, with three main differences: g Absolute performance is very high.
Reference: [Howard88] <author> J. H. Howard, M. L. Kazar, S. G. Menees, D. A. Nichols, M. Satyanarayanan, R. N. Sidebotham and M. J. West, </author> <title> ``Scale and Performance in a Distributed File System'', </title> <journal> ACM Transactions on Computer Systems 6, </journal> <month> 1 (February </month> <year> 1988), </year> <pages> 51-81. </pages>
Reference-contexts: Current I/O Benchmarks In this section, we examine current benchmarks used to evaluate I/O systems. The benchmarks we consider are Andrew <ref> [Howard88] </ref>, TPC-B [Anon85, TPCA89, TPCB90], Sdet [Gaede81, Gaede82, SPEC91b, SPEC91a], Bonnie [Bray90], and IOStone [Park90] 1 .
Reference: [Nielsen91] <author> M. J. K. Nielsen, </author> <title> ``DECstation 5000 Model 200'', </title> <booktitle> Procedures of the IEEE Computer Society International Conference (COMPCON), Spring 1991, </booktitle> <pages> 220-225. </pages>
Reference: [Ousterhout88] <author> J. K. Ousterhout, A. Cherenson, F. Douglis and M. Nelson, </author> <title> ``The Sprite Network Operating System'', </title> <booktitle> IEEE Computer 21, </booktitle> <month> 2 (February </month> <year> 1988), </year> <pages> 23-36. </pages>
Reference-contexts: We show a qualitative evaluation of today's I/O benchmarks in Figure 1 and make the following observations: g Many I/O benchmarks are not I/O limited. On a DECstation 5000/200 running the Sprite Operating System <ref> [Ousterhout88] </ref>, Andrew, Sdet 2 , and IOStone spend 25% or less of their time doing I/O. Further, many of the benchmarks touch very little data. IOStone touches only 1 MB of user data; Andrew touches only 4.5 MB. g Today's I/O benchmarks do not help in understanding system performance.
Reference: [Ousterhout89] <author> J. K. Ousterhout and F. Douglis, </author> <title> ``Beating the I/O Bottleneck: A Case for Log-Structured File Systems'', </title> <type> SIGOPS 23, </type> <month> 1 (January </month> <year> 1989), </year> <pages> 11-28. </pages>
Reference: [Park90] <author> A. Park and J. C. Becker, ``IOStone: </author> <title> A synthetic file system benchmark'', Computer Architecture News 18, </title> <month> 2 (June </month> <year> 1990), </year> <pages> 45-52. </pages>
Reference-contexts: Current I/O Benchmarks In this section, we examine current benchmarks used to evaluate I/O systems. The benchmarks we consider are Andrew [Howard88], TPC-B [Anon85, TPCA89, TPCB90], Sdet [Gaede81, Gaede82, SPEC91b, SPEC91a], Bonnie [Bray90], and IOStone <ref> [Park90] </ref> 1 .
Reference: [Patterson88] <author> D. A. Patterson, G. Gibson and R. H. Katz, </author> <title> ``A Case for Redundant Arrays of Inexpensive Disks (RAID)'', </title> <booktitle> International Conference on Management of Data (SIGMOD), </booktitle> <month> June </month> <year> 1988, </year> <pages> 109-116. </pages>
Reference-contexts: 1. Introduction As processors continue to improve their performance faster than I/O devices <ref> [Patterson88] </ref>, I/O will increasingly become the system bottleneck. There is therefore an increased need to understand and compare the performance of I/O systems, hence the need for I/O-intensive benchmarks. <p> For instance, IOStone tries to exercise the memory hierarchy but touches only 1 MB of user data. Perhaps at the time IOStone was written 1 MB was a lot of data but no longer. One recent example of how I/O systems are evolving is disk arrays <ref> [Patterson88, Gibson91, Chen90, Salem86] </ref>. Disk arrays allow multiple I/Os to be in progress simultaneously. Most current I/O benchmark do not scale the number of processes issuing I/O, and hence are unable to properly stress disk arrays. <p> The DECstation [DECstation90] uses a three disk RAID disk array <ref> [Patterson88] </ref> with a 16 KB striping unit [Chen90] and is configured without redundancy. The SPECmark rating is a measure of the processor speed; ratings are relative to the speed of a VAX 11/780. The full name of the HP 730 is the HP Series 700 Model 730 [HP730]. 4.
Reference: [Rosenblum91] <author> M. Rosenblum and J. K. Ousterhout, </author> <title> ``The Design and Implementation of a Log-Structured File System'', </title> <booktitle> Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <month> October </month> <year> 1991. </year>
Reference: [Rosenblum92] <author> M. Rosenblum, </author> <title> Sprite LFS Write Cache Size, </title> <type> personal communication, </type> <month> July </month> <year> 1992. </year>
Reference: [SPEC91a] <editor> SPEC SDM Release 1.0 Manual, </editor> <title> System Performance Evaluation Cooperative, </title> <year> 1991. </year>
Reference-contexts: Current I/O Benchmarks In this section, we examine current benchmarks used to evaluate I/O systems. The benchmarks we consider are Andrew [Howard88], TPC-B [Anon85, TPCA89, TPCB90], Sdet <ref> [Gaede81, Gaede82, SPEC91b, SPEC91a] </ref>, Bonnie [Bray90], and IOStone [Park90] 1 . <p> First TrySelf-Scaling All Workload Parameters A self-scaling benchmark is one that adjusts the workloads that it runs and reports based on the capabilities of the system being measured. Sdet and TPC-B both do this for one aspect of the workload, that is, load (processNum) <ref> [SPEC91a, TPCB90] </ref>. Sdet reports the maximum throughput, which occurs at different loads for different systems. TPC-B reports maximum throughput subject to a response time constraint; this also occurs at different loads for different systems.
Reference: [SPEC91b] <institution> SPEC SDM Release 1.0 Technical Fact Sheet, Franson and Haggerty Associates, </institution> <year> 1991. </year>
Reference-contexts: Current I/O Benchmarks In this section, we examine current benchmarks used to evaluate I/O systems. The benchmarks we consider are Andrew [Howard88], TPC-B [Anon85, TPCA89, TPCB90], Sdet <ref> [Gaede81, Gaede82, SPEC91b, SPEC91a] </ref>, Bonnie [Bray90], and IOStone [Park90] 1 .
Reference: [Saavedra-Barrera89] <author> R. H. Saavedra-Barrera, A. J. Smith and E. Miya, </author> <title> ``Machine Characterization Based on an Abstract High-Level Language Machine'', </title> <journal> IEEE Transactions on Computers 38, </journal> <month> 12 (December </month> <year> 1989), </year> <pages> 1659-1679. </pages>
Reference-contexts: A more attractive approach is to use the graphs output by the self-scaling evaluation (such as This is similar in concept to work done by Saavedra-Barrera, who predicts CPU performance by measuring the performance for a small set of FORTRAN operations <ref> [Saavedra-Barrera89] </ref>. We estimate performance for unmeasured workloads by assuming the shape of a performance curve for one parameter is independent of the values of the other parameters.
Reference: [Salem86] <author> K. Salem and H. Garcia-Molina, </author> <title> ``Disk Striping'', </title> <booktitle> Proceedings of the Second International Conference on Data Engineering, </booktitle> <year> 1986, </year> <pages> 336-342. </pages>
Reference-contexts: For instance, IOStone tries to exercise the memory hierarchy but touches only 1 MB of user data. Perhaps at the time IOStone was written 1 MB was a lot of data but no longer. One recent example of how I/O systems are evolving is disk arrays <ref> [Patterson88, Gibson91, Chen90, Salem86] </ref>. Disk arrays allow multiple I/Os to be in progress simultaneously. Most current I/O benchmark do not scale the number of processes issuing I/O, and hence are unable to properly stress disk arrays.
Reference: [Scott90] <author> V. Scott, </author> <title> ``Is Standardization of Benchmarks Feasible?'', </title> <booktitle> Proceedings of the BUSCON Conference, </booktitle> <address> Long Beach, CA, </address> <month> February </month> <year> 1990, </year> <pages> 139-147. </pages>
Reference-contexts: The benefits of good benchmarks are well understoodwhen benchmarks are representative of users' applications, they channel vendor optimization and research efforts into improvements that benefit users. Good benchmarks also assist users in purchasing machines by allowing fair, relevant comparisons. Recent efforts to standardize benchmarks, such as SPEC <ref> [Scott90] </ref> and Perfect Club [Berry89], have increased our understanding of computing performance and helped create a fair playing field on which companies can compete. These standardization efforts have focused on CPU-intensive applications [Scott90], however, and intentionally avoided I/O intensive applications [Berry89]. <p> Recent efforts to standardize benchmarks, such as SPEC <ref> [Scott90] </ref> and Perfect Club [Berry89], have increased our understanding of computing performance and helped create a fair playing field on which companies can compete. These standardization efforts have focused on CPU-intensive applications [Scott90], however, and intentionally avoided I/O intensive applications [Berry89]. In this paper, we develop criteria for ideal I/O benchmarks and show how current I/O benchmarks fall short of these. <p> Results should be reproducible; optimizations that are allowed and disallowed must be explicitly stated; the machine environment on which the benchmarking takes place must be well-defined and reported, and so on. In this paper, we leave this aspect of benchmarking to standardization organizations such as SPEC <ref> [Scott90] </ref> and the Transaction Processing Performance Council [TPCA89, TPCB90].
Reference: [TPCA89] <editor> TPC Benchmark A Standard Specification, </editor> <booktitle> Transaction Processing Performance Council, </booktitle> <month> November </month> <year> 1989. </year>
Reference-contexts: In this paper, we leave this aspect of benchmarking to standardization organizations such as SPEC [Scott90] and the Transaction Processing Performance Council <ref> [TPCA89, TPCB90] </ref>. <p> Current I/O Benchmarks In this section, we examine current benchmarks used to evaluate I/O systems. The benchmarks we consider are Andrew [Howard88], TPC-B <ref> [Anon85, TPCA89, TPCB90] </ref>, Sdet [Gaede81, Gaede82, SPEC91b, SPEC91a], Bonnie [Bray90], and IOStone [Park90] 1 .
Reference: [TPCB90] <editor> TPC Benchmark B Standard Specification, </editor> <booktitle> Transaction Processing Performance Council, </booktitle> <month> August </month> <year> 1990. </year>
Reference-contexts: In this paper, we leave this aspect of benchmarking to standardization organizations such as SPEC [Scott90] and the Transaction Processing Performance Council <ref> [TPCA89, TPCB90] </ref>. <p> Current I/O Benchmarks In this section, we examine current benchmarks used to evaluate I/O systems. The benchmarks we consider are Andrew [Howard88], TPC-B <ref> [Anon85, TPCA89, TPCB90] </ref>, Sdet [Gaede81, Gaede82, SPEC91b, SPEC91a], Bonnie [Bray90], and IOStone [Park90] 1 . <p> First TrySelf-Scaling All Workload Parameters A self-scaling benchmark is one that adjusts the workloads that it runs and reports based on the capabilities of the system being measured. Sdet and TPC-B both do this for one aspect of the workload, that is, load (processNum) <ref> [SPEC91a, TPCB90] </ref>. Sdet reports the maximum throughput, which occurs at different loads for different systems. TPC-B reports maximum throughput subject to a response time constraint; this also occurs at different loads for different systems.
References-found: 29

