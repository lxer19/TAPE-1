URL: ftp://arch.cs.ucdavis.edu/papers/sm-vs-mp.ps.Z
Refering-URL: http://arch.cs.ucdavis.edu/~chong/pubs.html
Root-URL: http://www.cs.ucdavis.edu
Email: fftchong,agarwalg@lcs.mit.edu  
Title: Shared Memory versus Message Passing for Iterative Solution of Sparse, Irregular Problems  
Author: Frederic T. Chong and Anant Agarwal 
Keyword: multiprocessors, shared memory, message passing, bulk transfer, iterative solution, irregular, sparse matrix  
Date: October 27, 1996  
Affiliation: Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: The benefits of hardware support for shared memory versus those for message passing are difficult to evaluate without an in-depth study of real applications on a common platform. We evaluate the communication mechanisms of the MIT Alewife machine, a multiprocessor which provides integrated cache-coherent shared memory, message passing, and DMA. We perform this evaluation with best-effort implementations which solve several sparse, irregular benchmark problems with a preconditioned conjugate gradient sparse matrix solver (ICCG). We find that machines with fast global memory operations do not need message passing or bulk transfer to support our irregular problems. This is primarily due to three reasons. First, a 5-to-1 ratio between global and local cache misses makes memory copies in bulk communication expensive relative to communication via shared memory. Second, although message passing has synchronization semantics superior to shared memory for data-driven computation, efficient shared memory can overcome this handicap by using global read-modify-writes to change from the traditional owner-computes model to a producer-computes model. Third, bulk transfers can result in high processor idle times in irregular applications.
Abstract-found: 1
Intro-found: 1
Reference: [ABC + 95] <author> Anant Agarwal, Ricardo Bianchini, David Chaiken, Kirk Johnson, David Kranz, John Kubiatowicz, Beng-Hong Lim, Ken Mackenzie, and Donald Yeung. </author> <title> The MIT Alewife machine: Architecture and performance. </title> <booktitle> In Proc. 22nd Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Distributed shared memory and message passing are two dominant communication mechanisms in parallel systems. Most machines incorporate either message 1 passing [Thi93a] [Int91] [Che93] or shared memory [LT88] [LLG + 92] [BFKR92] [SGI94]. A few machines support both mechanisms <ref> [ABC + 95] </ref> [HKO + 94]. The precise benefit of shared memory versus message passing has remained an open question, largely due to the difficulty of finding experimental platforms supporting an apples to apples comparison and the difficulty of writing applications in both styles without bias. <p> This paper addresses this open question using the MIT Alewife machine <ref> [ABC + 95] </ref> and sparse, irregular applications. Alewife integrates distributed, cache-coherent shared memory and message passing, providing an ideal platform for our experiments. <p> Colors indicates the number of colors required in the multicolor ordering described in Section 4.4. The larger the ratio between Order and Colors, the better the parallelism and message aggregation. 3 Experimental Platform We conduct our experiments on the MIT Alewife multiprocessor <ref> [ABC + 95] </ref>, shown in Figure 1.
Reference: [ACM88] <author> Arvind, David E. Culler, and Gino K. Maa. </author> <title> Assessing the benefits of fine-grained parallelism in dataflow programs. </title> <booktitle> In Supercomputing `88. IEEE, </booktitle> <year> 1988. </year>
Reference-contexts: The next two sections describe our message-passing and shared-memory implementations of this computation. 4.1.1 Message Passing Our DAG is essentially a dataflow computation <ref> [ACM88] </ref> and is easily implemented via active messages. In our example, let us assume that DAG node 0 is on processor 0 and node 4 is on processor 4. The edge from node 0 to node 4 is a non-local edge, an edge between nodes on different processors.
Reference: [BCL + 95] <author> Eric A. Brewer, Frederic T. Chong, Lok T. Liu, Shamik D. Sharma, and John Kubiatowicz. </author> <title> Remote queues: Exposing message queues for optimization and atomicity. </title> <booktitle> In 1995 Symposium on Parallel Architectures and Algorithms, </booktitle> <address> Santa Barbara, California, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: This paper addresses this open question using the MIT Alewife machine [ABC + 95] and sparse, irregular applications. Alewife integrates distributed, cache-coherent shared memory and message passing, providing an ideal platform for our experiments. Although this study evolved from research [CSBS95] <ref> [BCL + 95] </ref> [CS95] and researchers 1 with a clear message-passing bias, our results point to shared memory as the most useful communication mechanism for a multiprocessor. Our kernels were originally optimized for message passing and were rewritten for shared memory. <p> However, they were able to gain many of the benefits of bulk transfer by using large cache lines and prefetching. Our use of a producer-computes model to cope with shared memory synchronization is similar to the Remote Queues concept presented in <ref> [BCL + 95] </ref>. Mukherjee et al [MSH + 95] recently studied fine-grain, irregular applications on the Chaos system [SMC91] for message passing and software distributed shared memory on the CM5.
Reference: [BFKR92] <author> H. Burkhardt, S. Frank, B. Knobe, and J. Rothnie. </author> <title> Overview of the KSR1 Computer System. </title> <type> Technical Report KSR-TR-9202001, </type> <institution> Kendall Square Research, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Distributed shared memory and message passing are two dominant communication mechanisms in parallel systems. Most machines incorporate either message 1 passing [Thi93a] [Int91] [Che93] or shared memory [LT88] [LLG + 92] <ref> [BFKR92] </ref> [SGI94]. A few machines support both mechanisms [ABC + 95] [HKO + 94].
Reference: [CB96] <author> David Conroy and Lance Berc. </author> <type> Personal communication, </type> <institution> Digitial Systems Research Center, </institution> <month> March </month> <year> 1996. </year>
Reference-contexts: While a quad-issue, 300 MHz Alpha is more than 30 times faster than Alewife's 20 MHz Sparcle processor, many current memory systems are no faster than Alewife's 500 ns miss time. Even the fastest current prototypes, such as Digital's cache-less workstation <ref> [CB96] </ref>, still require 90 ns to reference main memory. Memory latency limitations will remain severe. Synchronous DRAMs and wide datapaths will not help with irregular accesses. The key is that network technology scales at least as well as DRAM speeds.
Reference: [Che93] <author> Chesney. </author> <title> The Meiko CS-2 system architecture. </title> <booktitle> In Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1993. </year>
Reference-contexts: 1 Introduction Distributed shared memory and message passing are two dominant communication mechanisms in parallel systems. Most machines incorporate either message 1 passing [Thi93a] [Int91] <ref> [Che93] </ref> or shared memory [LT88] [LLG + 92] [BFKR92] [SGI94]. A few machines support both mechanisms [ABC + 95] [HKO + 94].
Reference: [CLR94] <author> Satish Chandra, James R. Larus, and Anne Rogers. </author> <title> Where is time spent in message-passing and shared-memory programs. </title> <booktitle> In ASPLOS VI, </booktitle> <pages> pages 61-73, </pages> <address> San Jose, Cali-fornia, </address> <year> 1994. </year>
Reference-contexts: This was a convenient option which was available because the Alewife system allows the use of both message passing and shared memory on the same data structures in the same program. Previous work by Chandra, Larus, and Rogers <ref> [CLR94] </ref> studied four benchmarks running on simulations of separate message-passing and shared-memory machines based upon the CM5.
Reference: [CS95] <author> Frederic T. Chong and Robert Schreiber. </author> <title> Parallel sparse triangular solution with partitioned inverses and prescheduled DAGs. </title> <booktitle> In 1995 Workshop on Solving Irregular Problems on Distributed Memory Machines, </booktitle> <address> Santa Barbara, California, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: This paper addresses this open question using the MIT Alewife machine [ABC + 95] and sparse, irregular applications. Alewife integrates distributed, cache-coherent shared memory and message passing, providing an ideal platform for our experiments. Although this study evolved from research [CSBS95] [BCL + 95] <ref> [CS95] </ref> and researchers 1 with a clear message-passing bias, our results point to shared memory as the most useful communication mechanism for a multiprocessor. Our kernels were originally optimized for message passing and were rewritten for shared memory.
Reference: [CSBS95] <author> Frederic T. Chong, Shamik D. Sharma, Eric A. Brewer, and Joel Saltz. </author> <title> Multiprocessor runtime support for irregular DAGs. </title> <journal> Parallel Processing Letters: Special Issue on Partitioning and Scheduling for Parallel and Distributed Systems, </journal> <pages> pages 671-683, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: This paper addresses this open question using the MIT Alewife machine [ABC + 95] and sparse, irregular applications. Alewife integrates distributed, cache-coherent shared memory and message passing, providing an ideal platform for our experiments. Although this study evolved from research <ref> [CSBS95] </ref> [BCL + 95] [CS95] and researchers 1 with a clear message-passing bias, our results point to shared memory as the most useful communication mechanism for a multiprocessor. Our kernels were originally optimized for message passing and were rewritten for shared memory. <p> The traditional method would be to schedule the nodes into phases (levels of a topological sort) and use barrier synchronization between levels. However, this has been found to be less efficient than finer-grain scheduling and synchronization <ref> [CSBS95] </ref>. <p> However, buffering cost reduces these savings by 22 percent. More importantly, buffering an edge for later transmission can cost nearly half as much as directly communicating the data via shared memory. 4.2 Overview of ICCG Unlike previous studies <ref> [CSBS95] </ref>, we have extensively studied the entire ICCG algorithm to ensure that our results are consistent with a complete, best-effort implementation. <p> This approach produces better mappings and application performance than previous DAG-oriented approaches <ref> [CSBS95] </ref>. For data placement, we use Chaco, a well-established sequential code from Sandia National Laboratories which provides a range of mapping algorithms. We use a multilevel algorithm which uses a coarsening heuristic to condense the graph, and then partitions the condensed graph with recursive spectral bisection. <p> The increased parallelism allows our triangular solve speedups scale to much higher number of processors than with other orderings in previous studies <ref> [CSBS95] </ref>. 5 Results In this section, we present performance results from our experiments on Alewife. Our experiments focus on three implementations. Each implementation uses a different mechanism to communicate data along non-local edges of our computation DAG, edges between DAG nodes on different processors.
Reference: [DCB + 94] <author> Andr e DeHon, Frederic Chong, Matthew Becker, Eran Egozy, Henry Minsky, Samuel Peretz, and Thomas F. Knight, Jr. METRO: </author> <title> A router architecture for high-performance, short-haul routing networks. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <pages> pages 266-277, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: It takes one local-miss-time to service a remote miss at the receiver. It takes much less than one local-miss-time to get the request off the sender. That leaves more than three local-miss-times for the network transit time. With current VLSI switch technology <ref> [DCB + 94] </ref> keeping pace with processor cycle times, future networks should have no trouble keeping up with local memory speeds. Not only is a fast remote miss feasible, it is necessary to make shared-memory machines easy to use and viable for a wider range of applications.
Reference: [DGL92] <author> Ian S. Duff, Roger G. Grimes, and John G. Lewis. </author> <title> User's guide for the Harwell-Boeing sparse matrix collection. </title> <type> Technical Report TR/PA/92/86, CERFACS, 42 Ave G. Coriolis, </type> <address> 31057 Toulouse Cedex, France, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: This assumption avoids super-linear speedups which can result from sequential times based upon a workstation paging to disk or a single multiprocessor node using the memory from multiple nodes. Most of our benchmarks are from the Harwell-Boeing benchmark suite <ref> [DGL92] </ref>. Most of the matrices, especially the large ones, only come with the pattern of the nonzero entries, not the actual floating point values. BUS1138 and BCCSTK18 are the two largest power system and structures matrices in the HB suite with included values.
Reference: [DM89] <author> Iain S. Duff and Ge rard A. Meurant. </author> <title> The effect of ordering on preconditioned conjugate gradients. </title> <journal> BIT, </journal> <volume> 29 </volume> <pages> 635-657, </pages> <year> 1989. </year>
Reference-contexts: implementations, a processor may perform varying amounts of the computation of a node that it owns, but it will always complete every owned node by checking input dependencies and passing results along outgoing edges. 14 How do multicolor orderings affect convergence and what are the benefits of increased parallelism? Studies <ref> [DM89] </ref>, on similar data-sets to ours, indicate that the convergence rate of multicolor orderings is within roughly a factor of two of the best reorderings optimized for convergence.
Reference: [E + 92] <author> Thorsten von Eicken et al. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <address> Queensland, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: However, we found shared memory to be an extremely efficient communication mechanism even without the benefits of caching. Although we expected clear gains from the superior synchronization semantics of message-passing with Active Messages <ref> [E + 92] </ref>, we were able to overcome the synchronization difficulties of shared memory by shifting from an owner-computes model of computation to a producer-computes model (see Section 4.1).
Reference: [FLR + 94] <author> Babak Falsafi, Alvin R. Lebeck, Steven K. Reinhardt, Ioannis Schoinas, Mark D. Hill James R. Larus, Anne Rogers, and David A. Wood. </author> <title> Application-specific protocols for user-level shared memory. </title> <booktitle> In Supercomputing 94, </booktitle> <year> 1994. </year>
Reference-contexts: Unfortunately, aggregation techniques are not applicable to our applications and only hardware-supported shared memory can achieve acceptable performance. On the plus side, efficient hardware support can provide good performance without relaxing memory models. While relaxed models such as Wisconsin's delayed-update model <ref> [FLR + 94] </ref> are complementary to this study and could improve our application performance, our results show that we can still achieve acceptable performance on an architecture which chooses not to support application-specific models.
Reference: [GvL83] <author> G. Golub and C. F. van Loan. </author> <title> Matrix Computations. </title> <publisher> John Hopkins University Press, </publisher> <address> Baltimore, </address> <year> 1983. </year> <month> 22 </month>
Reference-contexts: Our kernels were originally optimized for message passing and were rewritten for shared memory. We chose especially challenging fine-grain, irregular benchmarks based upon the solution of sparse linear systems via ICCG (conjugate gradient preconditioned with incomplete Cholesky factors) <ref> [GvL83] </ref>. We expected ICCG to favor message passing because it is an iterative algorithm with little data reuse. However, we found shared memory to be an extremely efficient communication mechanism even without the benefits of caching.
Reference: [HKO + 94] <author> Mark Heinrish, Jeffrey Kuskin, David Ofelt, John Heinlein, Joel Baxter, Jaswinder Pal Singh, Richard Simoni, Kourosh Gharachorloo, David Nakahira, Mark Horowitz, Anoop Gupta, Mendel Rosenblum, and John Hennessy. </author> <title> The performance impact of flexibility in the Stanford FLASH multiprocessor. </title> <booktitle> In ASPLOS VI, </booktitle> <pages> pages 274-285, </pages> <address> San Jose, California, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction Distributed shared memory and message passing are two dominant communication mechanisms in parallel systems. Most machines incorporate either message 1 passing [Thi93a] [Int91] [Che93] or shared memory [LT88] [LLG + 92] [BFKR92] [SGI94]. A few machines support both mechanisms [ABC + 95] <ref> [HKO + 94] </ref>. The precise benefit of shared memory versus message passing has remained an open question, largely due to the difficulty of finding experimental platforms supporting an apples to apples comparison and the difficulty of writing applications in both styles without bias.
Reference: [HL95] <author> Bruce Hendrickson and Robert Leland. </author> <title> The Chaco user's guide. </title> <type> Technical Report SAND94-2692, </type> <institution> Sandia National Laboratories, </institution> <month> July </month> <year> 1995. </year>
Reference-contexts: We obtain supporting data by instrumenting two state-of-the-art scientific packages: Chaco <ref> [HL95] </ref>, a graph partitioner and mapper from Sandia National Laboratories; and BlockSolve [JP94], a conjugate gradient linear systems solver from Argonne National Laboratory.
Reference: [Int91] <institution> Paragon XP/S product overview. Intel Corporation, </institution> <year> 1991. </year>
Reference-contexts: 1 Introduction Distributed shared memory and message passing are two dominant communication mechanisms in parallel systems. Most machines incorporate either message 1 passing [Thi93a] <ref> [Int91] </ref> [Che93] or shared memory [LT88] [LLG + 92] [BFKR92] [SGI94]. A few machines support both mechanisms [ABC + 95] [HKO + 94].
Reference: [JP94] <author> Mark T. Jones and Paul E. Plassman. BlockSolve v2.0: </author> <title> Scalable library software for the parallel solution of sparse linear systems. </title> <type> ANL Report (updated draft) 92-46, </type> <institution> Argonne National Laboratory, </institution> <month> October </month> <year> 1994. </year>
Reference-contexts: We obtain supporting data by instrumenting two state-of-the-art scientific packages: Chaco [HL95], a graph partitioner and mapper from Sandia National Laboratories; and BlockSolve <ref> [JP94] </ref>, a conjugate gradient linear systems solver from Argonne National Laboratory.
Reference: [KL70] <author> B. W. Kernighan and S. Lin. </author> <title> An efficient heuristic procedure for partitioning graphs. </title> <journal> The Bell System Technical Journal, </journal> <pages> pages 291-307, </pages> <month> February </month> <year> 1970. </year>
Reference-contexts: We use a multilevel algorithm which uses a coarsening heuristic to condense the graph, and then partitions the condensed graph with recursive spectral bisection. Chaco assigns a mapping from partitions to processors on the parallel system and then refines it with the Kernighan-Lin algorithm <ref> [KL70] </ref>. 12 and incomplete Cholesky factorization. 4.4 Multicolor Reordering and Message Aggregation After the data is mapped to processors, we hold the mapping fixed but we can renumber the nodes. The mapping works on the undirected graph, which corresponds to treating the A-matrix as an adjacency matrix.
Reference: [LDCZ95] <author> Honghui Lu, Sandhya Dwarkadas, Alan L. Cox, and Willy Zwaenepoel. </author> <title> Message passing versus distributed shared memory on networks of workstations. </title> <booktitle> In Supercomputing 95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: Mukherjee et al [MSH + 95] recently studied fine-grain, irregular applications on the Chaos system [SMC91] for message passing and software distributed shared memory on the CM5. Lu et al <ref> [LDCZ95] </ref> looked at different applications coded for PVM message passing and Treadmarks software DSM. 20 These studies deal with an entirely different regime of overheads associated with software DSM. These overheads make communication aggregation and relaxed consistency models crucial to achieving acceptable application performance.
Reference: [LLG + 92] <author> Daniel Lenoski, James Laudon, Kourosh Gharachorloo, Wolf-Dietrich Weber, Anoop Gupta, John Hennessy, Mark Horowitz, and Monica S. Lam. </author> <title> The stanford Dash multiprocessor. </title> <journal> Computer, </journal> <volume> 25(3) </volume> <pages> 63-80, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Distributed shared memory and message passing are two dominant communication mechanisms in parallel systems. Most machines incorporate either message 1 passing [Thi93a] [Int91] [Che93] or shared memory [LT88] <ref> [LLG + 92] </ref> [BFKR92] [SGI94]. A few machines support both mechanisms [ABC + 95] [HKO + 94].
Reference: [LT88] <author> T. Lovett and S. Thakkar. </author> <title> The Symmetry multiprocessor system. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <volume> Vol. I Architecture, </volume> <pages> pages 303-310. </pages> , <address> University Park, Pennsylvania, </address> <month> [8] </month> <year> 1988. </year>
Reference-contexts: 1 Introduction Distributed shared memory and message passing are two dominant communication mechanisms in parallel systems. Most machines incorporate either message 1 passing [Thi93a] [Int91] [Che93] or shared memory <ref> [LT88] </ref> [LLG + 92] [BFKR92] [SGI94]. A few machines support both mechanisms [ABC + 95] [HKO + 94].
Reference: [Mes93] <author> Message Passing Interface Forum. </author> <title> MPI: A message passing interface. </title> <booktitle> In Supercomputing `93, </booktitle> <pages> pages 878-883. </pages> <publisher> IEEE, </publisher> <year> 1993. </year>
Reference-contexts: In particular, it shows that many iterations are required to solve these linear systems and that triangular solve takes at least 40 percent of runtime out of a total runtime which includes incomplete Cholesky and iterative solution. Iteration counts and timings are from BlockSolve running on top of MPI <ref> [Mes93] </ref> on a collection of Sun workstations connected by Ethernet. This system runs the same computation as would a full implementation of ICCG on Alewife. The data, partitions, and convergence would be the same. However, the overheads and latencies are substantially higher in the cluster than on Alewife.
Reference: [MSH + 95] <author> Shubhendu S. Mukherjee, Shamik D. Sharma, Mark D. Hill, James R. Larus, Anne Rogers, and Joel Saltz. </author> <title> Efficient support for irregular applications on distributed-memory machines. </title> <booktitle> In Proc. 5th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP'95, </booktitle> <pages> pages 68-79, </pages> <address> Santa Barbara, California, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: However, they were able to gain many of the benefits of bulk transfer by using large cache lines and prefetching. Our use of a producer-computes model to cope with shared memory synchronization is similar to the Remote Queues concept presented in [BCL + 95]. Mukherjee et al <ref> [MSH + 95] </ref> recently studied fine-grain, irregular applications on the Chaos system [SMC91] for message passing and software distributed shared memory on the CM5.
Reference: [RG93] <author> Edward Rothberg and Anoop Gupta. </author> <title> An efficient block-oriented approach to parallel sparse Cholesky factorization. </title> <booktitle> In Supercomputing `93, </booktitle> <pages> pages 503-512. </pages> <publisher> IEEE, </publisher> <year> 1993. </year>
Reference-contexts: Coarse-grain blocking approaches <ref> [RG93] </ref> perform well for this approach. Unfortunately, L and U can be expensive to compute and contain significantly more nonzeros than the original A. For this reason, an approximate factorization is often used.
Reference: [SGI94] <institution> Power Challenge technical report. </institution> <type> Technical report, </type> <institution> Silicon Graphics Inc., </institution> <address> 2011 N. Shoreline Blvd., Mountain View, CA 94043, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction Distributed shared memory and message passing are two dominant communication mechanisms in parallel systems. Most machines incorporate either message 1 passing [Thi93a] [Int91] [Che93] or shared memory [LT88] [LLG + 92] [BFKR92] <ref> [SGI94] </ref>. A few machines support both mechanisms [ABC + 95] [HKO + 94].
Reference: [SHH95] <author> Jaswinder Pal Singh, Chris Holt, and John Hennessy. </author> <title> Load balancing and data locality in adaptive hierarchical N-body methods: Barnes-hut, fast multipole, and radiosity. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 27(2), </volume> <month> June </month> <year> 1995. </year>
Reference-contexts: Many studies have examined shared memory without thorough comparison with message passing and DMA. Yeung and Agarwal [YA93] explored fine-grain synchronization and language support for preconditioned conjugate gradient on regular problems on Alewife. Singh, Holt, and Hennessy <ref> [SHH95] </ref> studied hierarchical N-body methods on distributed shared memory machines. They found significant benefits from caching due to re-use in their applications. Woo, Sing, and Hennessy [WSH94] also found the advantages of bulk transfer over efficient shared memory to be limited.
Reference: [SMC91] <author> Joel H. Saltz, Ravi Mirchandaney, and Kay Crowley. </author> <title> Run-time parallelization and scheduling of loops. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 603-611, </pages> <year> 1991. </year>
Reference-contexts: Our use of a producer-computes model to cope with shared memory synchronization is similar to the Remote Queues concept presented in [BCL + 95]. Mukherjee et al [MSH + 95] recently studied fine-grain, irregular applications on the Chaos system <ref> [SMC91] </ref> for message passing and software distributed shared memory on the CM5. Lu et al [LDCZ95] looked at different applications coded for PVM message passing and Treadmarks software DSM. 20 These studies deal with an entirely different regime of overheads associated with software DSM.
Reference: [ST82] <author> R. Schreiber and W. Tang. </author> <title> Vectorizing the conjugate gradient method. </title> <booktitle> In Proceedings Symposium CYBER 205 Applications, </booktitle> <address> Ft. Collins, CO, </address> <year> 1982. </year>
Reference-contexts: Renumbering can also result in greater slackness, amount of computation able to proceed between communications, which creates more data available to be aggregated into long messages. Long messages are often used to try to amortize communication overhead. We use the multicolor reordering algorithm <ref> [ST82] </ref>, described later in this section, which is one of the best known algorithms for enhancing parallelism and message aggregation. Multicolor reorderings are similar to red-black relaxation. Nodes are colored to ensure independence and then the computation works on the nodes one color at a time.
Reference: [Thi93a] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA. </address> <note> CM-5 Technical Summary, Novem-ber 1993. </note>
Reference-contexts: 1 Introduction Distributed shared memory and message passing are two dominant communication mechanisms in parallel systems. Most machines incorporate either message 1 passing <ref> [Thi93a] </ref> [Int91] [Che93] or shared memory [LT88] [LLG + 92] [BFKR92] [SGI94]. A few machines support both mechanisms [ABC + 95] [HKO + 94].
Reference: [Thi93b] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA. </address> <note> CMMD Reference Manual (Version 3.0), </note> <month> May </month> <year> 1993. </year>
Reference-contexts: They also argue that shared memory performance is comparable to message passing, but shared memory was nearly a factor 1 For example, one of the original developers of the CMMD message-passing library <ref> [Thi93b] </ref> on the CM5. matrix Description Order Nonzeros Seq MFLOPS Values Avg Deg Colors BUS1138 Power system 1138 4054 0.7 Y 1.3 5 CAN1072 Aircraft model 1072 12444 0.7 N 5.3 7 BCSPWR10 Eastern US power system 5300 21842 0.7 N 1.6 4 BCSSTK18 Nuclear power plant model 11948 149090 *0.7
Reference: [WSH94] <author> Steven Cameron Woo, Jaswinder Pal Singh, and John L. Hennessy. </author> <title> The performance advantages of integrating block data transfer in cache-coherent multiprocessors. </title> <booktitle> In Asplos VI, </booktitle> <pages> pages 219-229, </pages> <address> San Jose, California, </address> <year> 1994. </year> <month> 23 </month>
Reference-contexts: Yeung and Agarwal [YA93] explored fine-grain synchronization and language support for preconditioned conjugate gradient on regular problems on Alewife. Singh, Holt, and Hennessy [SHH95] studied hierarchical N-body methods on distributed shared memory machines. They found significant benefits from caching due to re-use in their applications. Woo, Sing, and Hennessy <ref> [WSH94] </ref> also found the advantages of bulk transfer over efficient shared memory to be limited. Their applications were not as irregular and aggregation was not as expensive. However, they were able to gain many of the benefits of bulk transfer by using large cache lines and prefetching.

References-found: 33

