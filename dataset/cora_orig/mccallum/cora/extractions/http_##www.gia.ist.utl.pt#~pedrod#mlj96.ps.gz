URL: http://www.gia.ist.utl.pt/~pedrod/mlj96.ps.gz
Refering-URL: http://www.gia.ist.utl.pt/~pedrod/
Root-URL: 
Email: pedrod@ics.uci.edu  
Title: Unifying Instance-Based and Rule-Based Induction  
Author: PEDRO DOMINGOS Editor: Raymond J. Mooney 
Keyword: Concept learning, multi-strategy learning, rule induction, instance-based learning, nearest-neighbor classification, case-based reasoning  
Address: Irvine, CA 92717  
Affiliation: Department of Information and Computer Science, University of California,  
Note: 1-31 c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Abstract: Several well-developed approaches to inductive learning now exist, but each has specific limitations that are hard to overcome. Multi-strategy learning attempts to tackle this problem by combining multiple methods in one algorithm. This article describes a unification of two widely-used empirical approaches: rule induction and instance-based learning. In the new algorithm, instances are treated as maximally specific rules, and classification is performed using a best-match strategy. Rules are learned by gradually generalizing instances until no improvement in apparent accuracy is obtained. Theoretical analysis shows this approach to be efficient. It is implemented in the RISE 3.1 system. In an extensive empirical study, RISE consistently achieves higher accuracies than state-of-the-art representatives of both its parent approaches (PEBLS and CN2), as well as a decision tree learner (C4.5). Lesion studies show that each of RISE's components is essential to this performance. Most significantly, in 14 of the 30 domains studied, RISE is more accurate than the best of PEBLS and CN2, showing that a significant synergy can be obtained by combining multiple empirical methods. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W. </author> <year> (1990). </year> <title> A study of instance-based learning algorithms for supervised learning tasks: Mathematical, empirical, </title> <type> and psychological evaluations (Technical Report 90-42). </type> <institution> Irvine, CA: University of California at Irvine, Department of Information and Computer Science. </institution>
Reference: <author> Aha, D. W. (Ed.) </author> <note> (in press). Special issue on lazy learning. Artificial Intelligence Review. </note>
Reference: <author> Aha, D. W., & Bankert, R. L. </author> <year> (1994). </year> <title> Feature selection for case-based classification of cloud types: An empirical comparison. </title> <booktitle> Proceedings of the 1994 AAAI Workshop on Case-Based Reasoning (pp. </booktitle> <pages> 106-112). </pages> <address> Seattle, WA: </address> <publisher> AAAI. </publisher>
Reference: <author> Aha, D. W., & Goldstone, R. L. </author> <year> (1992). </year> <title> Concept learning and flexible weighting. </title> <booktitle> Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 534-539). </pages> <address> Bloomington, </address> <publisher> IN: Lawrence Erlbaum. </publisher>
Reference: <author> Aha, D. W., Kibler, D., & Albert, M. K. </author> <year> (1991). </year> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 37-66. </pages>
Reference-contexts: As a result, research in this field has blossomed, and several mature approaches to inductive learning are now available to the practitioner. These include induction of decision trees (Quinlan, 1986), rule induction (Michalski, 1983), instance-based learning <ref> (Aha, Kibler & Albert, 1991) </ref>, Bayesian classification (Bun 2 tine, 1989), back-propagation (Rumelhart, Hinton & Williams, 1986), and genetic algorithms (Booker, Goldberg & Holland, 1989). Empirical comparison of these different approaches in a variety of application domains has shown that each performs best in some, but not all, domains. <p> Symbolic attributes pose a more difficult problem. Most IBL systems <ref> (e.g., Aha et al., 1991) </ref> use a simple overlap metric: ffi (x i ; x j ) = 0 if i = j 1 otherwise (3) This measure is obviously less informative than its numeric counterpart, and, although it is appropriate in some cases, its use can lead to poor performance <p> Another issue in IBL methods is their sensitivity to noise. Incorrect instances are liable to create a region around them where new examples will also be misclassified. Several methods have been successfully introduced to deal with this problem. IB3 <ref> (Aha et al., 1991) </ref> retains only reliable instances, reliability being judged by the instance's classification performance over a "probation period." Cameron-Jones (1992) uses instead a criterion based on the minimum description length principle to decide which instances to retain. <p> With multiple instances of each class, the frontier will be composed of a number of hyperplanar sections, and can thus become quite complex even when few instances are present <ref> (Aha et al., 1991) </ref>. The introduction of weights further increases this complexity, turning the hyperplanes into hyperquadrics (Cost & Salzberg, 1993). Another extension to the basic IBL paradigm consists in using the k nearest neighbors for classification, instead of just the nearest one (Duda & Hart, 1973).
Reference: <author> Atkeson, C. G., Moore, A. W., & Schaal, S. </author> <title> (in press). Locally weighted learning. </title> <journal> Artificial Intelligence Review. </journal>
Reference: <author> Belew, R. K, McInerney, J., & Schraudolph, N. N. </author> <year> (1992). </year> <title> Evolving networks: Using the genetic algorithm with connectionist learning. </title> <editor> In C. G. Langton, C. Taylor, J. D. Farmer, & S. Rasmussen (Eds.), </editor> <booktitle> Artificial Life II. </booktitle> <address> Redwood City, CA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Several induction algorithms proposed in the literature can be seen as empirical multi-strategy learners, but combining different paradigms from RISE's: 26 decision trees and rules (Quinlan, 1987), decision trees and perceptrons (Utgoff, 1989b), rules and Bayesian classification (Smyth, Goodman & Higgins, 1990), backpropagation and genetic algorithms <ref> (Belew, McInerney & Schraudolph, 1992) </ref>, instances and prototypes (Scott & Sage, 1992), decision trees and kernel density estimation (Smyth, Gray & Fayyad, 1995), etc.
Reference: <author> Biberman, Y. </author> <year> (1994). </year> <title> A context similarity measure. </title> <booktitle> Proceedings of the Ninth European Conference on Machine Learning (pp. </booktitle> <pages> 49-63). </pages> <address> Catania, Italy: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: A number of methods of this type have been proposed in the literature (Aha & Goldstone, 1992; Atkeson, Moore & Schaal, in press). Non-metric, context-sensitive distance measures for IBL are discussed in <ref> (Biberman, 1994) </ref>. Another important aspect of RISE is its policy for combining numeric and symbolic attributes, using SVDM for the former and Euclidean distance for the latter. Alternative approaches have been explored by Ting (1994) and Mohri and Tanaka (1994).
Reference: <author> Booker, L. B, Goldberg, D. E., & Holland, J. H. </author> <year> (1989). </year> <title> Classifier systems and genetic algorithms. </title> <journal> Artificial Intelligence, </journal> <volume> 40, </volume> <pages> 235-282. </pages>
Reference-contexts: These include induction of decision trees (Quinlan, 1986), rule induction (Michalski, 1983), instance-based learning (Aha, Kibler & Albert, 1991), Bayesian classification (Bun 2 tine, 1989), back-propagation (Rumelhart, Hinton & Williams, 1986), and genetic algorithms <ref> (Booker, Goldberg & Holland, 1989) </ref>. Empirical comparison of these different approaches in a variety of application domains has shown that each performs best in some, but not all, domains.
Reference: <author> Brodley, C. E. </author> <year> (1995). </year> <title> Recursive automatic bias selection for classifier construction. </title> <journal> Machine Learning, </journal> <volume> 20, </volume> <pages> 63-94. </pages>
Reference-contexts: Empirical comparison of these different approaches in a variety of application domains has shown that each performs best in some, but not all, domains. This has been termed the "selective superiority" problem <ref> (Brodley, 1995) </ref>, and presents a dilemma to the knowledge engineer approaching a new task: which induction paradigm should be used? One solution is to try each one in turn, and use cross-validation to choose the one that appears to perform best (Schaffer, 1994a). <p> Another important aspect of RISE is its policy for combining numeric and symbolic attributes, using SVDM for the former and Euclidean distance for the latter. Alternative approaches have been explored by Ting (1994) and Mohri and Tanaka (1994). MCS <ref> (Brodley, 1995) </ref> has the most similar aims to RISE's, but uses an entirely different approach (applying meta-knowledge to detect when one algorithm should be used instead of another), and combines instead decision trees with IBL and linear discriminant functions.
Reference: <author> Buntine, W. </author> <year> (1989). </year> <title> Learning classification rules using Bayes. </title> <booktitle> Proceedings of the Sixth International Workshop on Machine Learning (pp. </booktitle> <pages> 94-98). </pages> <address> Ithaca, NY: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Cameron-Jones, R. M. </author> <year> (1992). </year> <title> Minimum description length instance-based learning. </title> <booktitle> Proceedings of the Fifth Australian Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 368-373). </pages> <address> Hobart, Australia: </address> <publisher> World Scientific. </publisher>
Reference: <author> Catlett, J. </author> <year> (1991). </year> <title> Megainduction: A test flight. </title> <booktitle> Proceedings of the Eighth International Conference on Machine Learning (pp. </booktitle> <pages> 589-604). </pages> <address> Evanston, IL: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: internal parameter of the algorithm (Clark & Niblett, 1989). 2 The worst-case complexity of growing a decision tree is O (ea 2 ) when only symbolic attributes are present (Utgoff, 1989a), but becomes quadratic or worse in e with numeric ones, mainly due to the need for repeated sorting operations <ref> (Catlett, 1991) </ref>. The final pruning stage used in C4.5 and several rule induction systems may also in general be worse than quadratic in e (Cohen, 1995). Thus RISE's worst-case time complexity is comparable to that of other rule and decision tree learners. <p> Thus windowing may or may not decrease running time. Its effect on accuracy can also be positive or negative. A more detailed discussion of these issues in the context of decision tree induction can be found in <ref> (Catlett, 1991) </ref> and (Quinlan, 1993a). In the empirical study described below, win-dowing improved RISE's running time in all the datasets where it was used (those with more than 3000 examples), sometimes by a large factor, while improving accuracy in all but one (where it had no significant effect).
Reference: <author> Clark, P., & Boswell, R. </author> <year> (1991). </year> <title> Rule induction with CN2: Some recent improvements. </title> <booktitle> Proceedings of the Sixth European Working Session on Learning (pp. </booktitle> <pages> 151-163). </pages> <address> Porto, Portugal: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: This measure tends to the uncorrected accuracy when the rule has strong statistical support (i.e., when it covers many examples), but tends to 1=c (i.e., "maximum ignorance") when it covers few. It is used in recent versions of CN2 <ref> (Clark & Boswell, 1991) </ref>. Classification of a new example is performed by matching each rule against it, and selecting those it satisfies. If there is only one such rule, its class is assigned to the example. <p> The other is to let the different rules vote, and select the class receiving the most votes. Recent versions of CN2 attach to each rule the number of examples of each class that it covers, and use these numbers as votes at classification time <ref> (Clark & Boswell, 1991) </ref>. Other voting schemes are possible (e.g., Michalski et al., 1986). The use of unordered rules has been found to generally produce higher accuracy (Clark & Boswell, 1991), and also has the advantage of greater comprehensibility, since in a decision list each rule body is implicitly conjoined with <p> attach to each rule the number of examples of each class that it covers, and use these numbers as votes at classification time <ref> (Clark & Boswell, 1991) </ref>. Other voting schemes are possible (e.g., Michalski et al., 1986). The use of unordered rules has been found to generally produce higher accuracy (Clark & Boswell, 1991), and also has the advantage of greater comprehensibility, since in a decision list each rule body is implicitly conjoined with the negations of all those that precede it. <p> PEBLS 2.1's inability to deal with missing values was overcome by grafting onto it an approach similar to the one selected for RISE (see Section 4). A recent version of CN2 (6.1) was used, one incorporating Laplace accuracy and unordered rules <ref> (Clark & Boswell, 1991) </ref>. To gauge its position in the overall spectrum of induction methods, RISE was also compared with a system that learns rule sets by way of decision trees, C4.5/C4.5RULES (Quinlan, 1993a).
Reference: <author> Clark, P., & Niblett, T. </author> <year> (1989). </year> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 261-283. </pages>
Reference-contexts: Given a class, its members in the training set are called positive examples, and the remainder are negative examples. 7 The "best" rule in each covering cycle (see Table 1) may also be found by beam search <ref> (e.g., Clark & Niblett, 1989) </ref>. In this case a list of the b best rule bodies found so far is maintained, instead of a single body. <p> The AQ series of algorithms (Michalski et al., 1986) uses apparent accuracy (i.e., the accuracy of the rule on the training set): H (n ; n ) = n + n The CN2 system <ref> (Clark & Niblett, 1989) </ref> originally used the entropy of the rule (Quinlan, 1986). However, the problem with both these measures is that they tend to favor overly specific rules: they attain their maximum value with a rule covering a single example. <p> The time complexity of CN2 and AQ-style algorithms is O (be 2 a 2 ), where b is the beam size, an integer-valued internal parameter of the algorithm <ref> (Clark & Niblett, 1989) </ref>. 2 The worst-case complexity of growing a decision tree is O (ea 2 ) when only symbolic attributes are present (Utgoff, 1989a), but becomes quadratic or worse in e with numeric ones, mainly due to the need for repeated sorting operations (Catlett, 1991). <p> The remainder of this section describes the characteristics and reports the results of this study. 6.1. Experimental design RISE was compared with a representative of each of its parent approaches: PEBLS for IBL (Cost & Salzberg, 1993), and CN2 for rule induction <ref> (Clark & Niblett, 1989) </ref>. PEBLS is a state-of-the art system, as opposed to the skeleton nearest-neighbor implementations typically used in empirical comparisons. PEBLS 2.1's inability to deal with missing values was overcome by grafting onto it an approach similar to the one selected for RISE (see Section 4).
Reference: <author> Cohen, W. W. </author> <year> (1995). </year> <title> Fast effective rule induction. </title> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning (pp. </booktitle> <pages> 115-123). </pages> <address> Tahoe City, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The final pruning stage used in C4.5 and several rule induction systems may also in general be worse than quadratic in e <ref> (Cohen, 1995) </ref>. Thus RISE's worst-case time complexity is comparable to that of other rule and decision tree learners.
Reference: <author> Cost, S., & Salzberg, S. </author> <year> (1993). </year> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <journal> Machine Learning, </journal> <volume> 10, </volume> <pages> 57-78. </pages>
Reference-contexts: Aha et al., 1991) use a simple overlap metric: ffi (x i ; x j ) = 0 if i = j 1 otherwise (3) This measure is obviously less informative than its numeric counterpart, and, although it is appropriate in some cases, its use can lead to poor performance <ref> (Cost & Salzberg, 1993) </ref>. A more sophisticated alternative consists of considering two symbolic values to be similar if they make similar predictions (i.e., if they correlate similarly with the class feature). <p> IB3 (Aha et al., 1991) retains only reliable instances, reliability being judged by the instance's classification performance over a "probation period." Cameron-Jones (1992) uses instead a criterion based on the minimum description length principle to decide which instances to retain. PEBLS <ref> (Cost & Salzberg, 1993) </ref> assign weights to instances, making their apparent distance to new examples increase with their misclassification rate. Given two instances of different classes, the frontier between classes induced by them is a hyperplane perpendicular to the line connecting the two instances, and bisecting it. <p> With multiple instances of each class, the frontier will be composed of a number of hyperplanar sections, and can thus become quite complex even when few instances are present (Aha et al., 1991). The introduction of weights further increases this complexity, turning the hyperplanes into hyperquadrics <ref> (Cost & Salzberg, 1993) </ref>. Another extension to the basic IBL paradigm consists in using the k nearest neighbors for classification, instead of just the nearest one (Duda & Hart, 1973). <p> The remainder of this section describes the characteristics and reports the results of this study. 6.1. Experimental design RISE was compared with a representative of each of its parent approaches: PEBLS for IBL <ref> (Cost & Salzberg, 1993) </ref>, and CN2 for rule induction (Clark & Niblett, 1989). PEBLS is a state-of-the art system, as opposed to the skeleton nearest-neighbor implementations typically used in empirical comparisons.
Reference: <author> Cover, T. M., & Hart, P. E. </author> <year> (1967). </year> <title> Nearest neighbor pattern classification. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 13, </volume> <pages> 21-27. </pages>
Reference-contexts: Two issues are involved in Wettschereck and Dietterich's study, and they should be clearly distinguished. One is whether using one nearest neighbor is preferable to using several. This has been studied for many years in the nearest neighbor literature <ref> (e.g., Cover & Hart, 1967) </ref>. Another issue, of more interest here, is whether or not generalization of instances to hyperrectangles is beneficial.
Reference: <author> DeGroot, M. H. </author> <year> (1986). </year> <title> Probability and statistics (Second Edition). </title> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher> <address> 29 Domingos, P. </address> <year> (1994). </year> <title> The RISE system: Conquering without separating. </title> <booktitle> Proceedings of the Sixth IEEE International Conference on Tools with Artificial Intelligence (pp. </booktitle> <pages> 704-707). </pages> <address> New Orleans, LA: </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: The Wilcoxon signed-ranks test is a procedure that addresses this issue <ref> (DeGroot, 1986) </ref>. It takes into account the signs of the observed differences, and their ranking (i.e., the largest difference counts more than the second largest one and so forth, but it does not matter by how much).
Reference: <author> Domingos, P. </author> <year> (1995a). </year> <title> Rule induction and instance-based learning: A unified approach. </title> <booktitle> Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 1226-1232). </pages> <address> Montreal, Canada: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This methodology is implemented in the RISE 3.1 system (Rule Induction from a Set of Exemplars; earlier versions are described in (Domingos, 1994) and <ref> (Domingos, 1995a) </ref>). One of its basic features is that rules and instances are treated uniformly; no distinction is made between the two.
Reference: <author> Domingos, P. </author> <year> (1995b). </year> <title> The RISE 2.0 system: A case study in multistrategy learning (Technical Report 95-2). </title> <address> Irvine, CA: </address> <institution> University of California at Irvine, Department of Information and Computer Science. </institution>
Reference-contexts: In the event the accuracies 10 are the same, RISE chooses the most frequent class among those represented, and if there is still a draw, the winner is chosen at random. Other policies were also tried, and a comparative evaluation is described in <ref> (Domingos, 1995b) </ref>. A rule is said to cover an example if all its conditions are true for the example; a rule is said to win an example if it is the nearest rule to the example according to the distance metric and conflict resolution policy just described. <p> However, this is extremely unlikely. The two policies were empirically compared <ref> (see Domingos, 1995b) </ref>, showing no appreciable difference between the two in accuracy or time. Multiplying the values above by the cost of a single cycle yields a total time complexity of O (e 2 a 2 ) or O (e 3 a 2 ) respectively. <p> When no clear differences were present, the default was retained. For RISE this process is described in more detail in <ref> (Domingos, 1995b) </ref>, and resulted in the system described in the previous sections, with q = 1 (Equation 4), s = 2 (Equation 7), and global stopping (Section 5).
Reference: <author> Domingos, P. </author> <year> (1995c). </year> <title> Two-way induction. </title> <booktitle> Proceedings of the Seventh IEEE International Conference on Tools with Artificial Intelligence (pp. </booktitle> <pages> 182-189). </pages> <address> Herndon, VA: </address> <publisher> IEEE Computer Society Press. </publisher>
Reference: <author> Domingos, P. </author> <title> (in press). Context-sensitive feature selection for lazy learners. </title> <journal> Artificial Intelligence Review. </journal>
Reference: <author> Duda, R. O., & Hart, P. E. </author> <year> (1973). </year> <title> Pattern classification and scene analysis. </title> <address> New York, NY: </address> <publisher> Wiley. </publisher>
Reference-contexts: The introduction of weights further increases this complexity, turning the hyperplanes into hyperquadrics (Cost & Salzberg, 1993). Another extension to the basic IBL paradigm consists in using the k nearest neighbors for classification, instead of just the nearest one <ref> (Duda & Hart, 1973) </ref>. The class assigned is then that of the majority of those k neighbors, or the class receiving the most votes, with a neighbor's vote decreasing with its distance from the test example. <p> In regions of overlap, EACH arbitrarily assigns all examples to the class of the most specific hyperrectangle. In contrast, RISE's learning strategy approximates the optimal decision rule of placing the boundary between two classes at the point where the density of examples from one overtakes that of the other <ref> (Duda & Hart, 1973) </ref>. This is because a rule is started from each example, and its generalization halts when it would include more examples of other classes than of the example's one.
Reference: <author> Golding, A. R., & Rosenbloom, P. S. </author> <year> (1991). </year> <title> Improving rule-based systems through case-based reasoning. </title> <booktitle> Proceedings of the Ninth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 22-27). </pages> <address> Anaheim, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Holte, R. C. </author> <year> (1993). </year> <title> Very simple classification rules perform well on most commonly used datasets. </title> <journal> Machine Learning, </journal> <volume> 11, </volume> <pages> 63-91. </pages>
Reference-contexts: All datasets were drawn from the UCI repository (Murphy & Aha, 1995), and are obtainable by anonymous ftp from ics.uci.edu, subdirectory pub/machine-learning-databases. Table 4 lists the datasets used, and summarizes some of their main characteristics. 3 Datasets included in the listing of empirical results in <ref> (Holte, 1993) </ref> are referred to by the same codes. In the first phase of the study, the first 15 datasets in Table 4 (from breast cancer to wine) were used to fine-tune the algorithms, choosing by 10-fold cross-validation the most accurate version of each.
Reference: <author> Holte, R. C., Acker, L. E., & Porter, B. W. </author> <year> (1989). </year> <title> Concept learning and the problem of small disjuncts. </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 813-818). </pages> <address> Detroit, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: suited to symbolic domains, and can often dispose easily of irrelevant attributes; but they can have difficulty with non-axis-parallel frontiers, and suffer from the fragmentation problem (i.e., the available data dwindles as induction progresses) and the small disjuncts problem (i.e., rules covering few training examples have a high error rate <ref> (Holte, Acker & Porter, 1989) </ref>). (The two paradigms also share a number of characteristics, of course, most notably the assumption that the example space contains large continuous regions of constant class membership|the similarity hypothesis (Rendell, 1986).) Instances and rules also form the basis of two competing approaches to reasoning: case-based reasoning
Reference: <author> Kelly, J. D., & Davis, L. </author> <year> (1991). </year> <title> A hybrid genetic algorithm for classification. </title> <booktitle> Proceedings of the Twelfth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 645-650). </pages> <address> Sydney, Australia: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kohavi, R., & Li, C. </author> <year> (1995). </year> <title> Oblivious decision trees, graphs, and top-down pruning. </title> <booktitle> Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 1071-1077). </pages> <address> Montreal, Canada: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kolodner, J. </author> <year> (1993). </year> <title> Case-based reasoning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Acker & Porter, 1989)). (The two paradigms also share a number of characteristics, of course, most notably the assumption that the example space contains large continuous regions of constant class membership|the similarity hypothesis (Rendell, 1986).) Instances and rules also form the basis of two competing approaches to reasoning: case-based reasoning <ref> (Kolodner, 1993) </ref> and the rule-based reasoning more often found in expert systems.
Reference: <author> Michalski, R. S. </author> <year> (1983). </year> <title> A theory and methodology of inductive learning. </title> <journal> Artificial Intelligence, </journal> <volume> 20, </volume> <pages> 111-161. </pages>
Reference-contexts: As a result, research in this field has blossomed, and several mature approaches to inductive learning are now available to the practitioner. These include induction of decision trees (Quinlan, 1986), rule induction <ref> (Michalski, 1983) </ref>, instance-based learning (Aha, Kibler & Albert, 1991), Bayesian classification (Bun 2 tine, 1989), back-propagation (Rumelhart, Hinton & Williams, 1986), and genetic algorithms (Booker, Goldberg & Holland, 1989).
Reference: <author> Michalski, R. S., Mozetic, I., Hong, J., & Lavrac, N. </author> <year> (1986). </year> <title> The multi-purpose incremental learning system AQ15 and its testing application to three medical domains. </title> <booktitle> Proceedings of the Fifth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 1041-1045). </pages> <address> Philadelphia, PA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Given a rule, H should increase with n , the number of positive examples that satisfy the rule, and decrease with n , the number of negative examples that satisfy it. The AQ series of algorithms <ref> (Michalski et al., 1986) </ref> uses apparent accuracy (i.e., the accuracy of the rule on the training set): H (n ; n ) = n + n The CN2 system (Clark & Niblett, 1989) originally used the entropy of the rule (Quinlan, 1986). <p> Recent versions of CN2 attach to each rule the number of examples of each class that it covers, and use these numbers as votes at classification time (Clark & Boswell, 1991). Other voting schemes are possible <ref> (e.g., Michalski et al., 1986) </ref>. The use of unordered rules has been found to generally produce higher accuracy (Clark & Boswell, 1991), and also has the advantage of greater comprehensibility, since in a decision list each rule body is implicitly conjoined with the negations of all those that precede it.
Reference: <editor> Michalski, R. S., & Tecuci, G. (Eds.) </editor> <booktitle> (1993). Proceedings of the Second International Workshop on Multistrategy Learning. </booktitle> <address> Harpers Ferry, VA: </address> <institution> Office of Naval Research/George Mason University. </institution>
Reference: <editor> Michalski, R. S., & Tecuci, G. (Eds.) </editor> <year> (1994). </year> <title> Machine learning: A multistrategy approach. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: especially considering the large number of algorithms and systems now available, and the fact that each typically has options and parameters that themselves need to be fine-tuned by cross-validation or a similar method before the system can be said to be doing its "best." Another approach, known as multi-strategy learning <ref> (Michalski & Tecuci, 1994) </ref>, attempts to combine two or more different paradigms in a single algorithm. <p> Related work The RISE approach should be seen in the context of previous work in inductive learning and related areas. The AQ15 system employed best-match classification (Michalski et al, 1986; a more recent version is AQ17-HCI <ref> (Wnek & Michalski, 1994) </ref>). It differed from RISE in that it was a general-to-specific, separate-and-conquer system that learned purely logical rules, and only introduced the best-match policy in a post-processing step, with the goal of removing rules from the set without adversely affecting accuracy.
Reference: <author> Mitchell, T. M. </author> <year> (1980). </year> <title> The need for biases in learning generalizations (Technical Report). </title> <address> New Brunswick, NJ: </address> <institution> Rutgers University, Computer Science Department. </institution>
Reference-contexts: Here a theoretical question arises. It is well known that no induction algorithm can be the best in all possible domains; each algorithm contains an explicit or implicit bias <ref> (Mitchell, 1980) </ref> that leads it to prefer certain generalizations over others, and it will be successful only insofar as this bias matches the characteristics of the application domain.
Reference: <author> Mohri, T., and Tanaka, H. </author> <year> (1994). </year> <title> An optimal weighting criterion of case indexing for both numeric and symbolic attributes. </title> <booktitle> Proceedings of the 1994 AAAI Workshop on Case-Based Reasoning (pp. </booktitle> <pages> 123-127), </pages> <address> Seattle, WA: </address> <publisher> AAAI. </publisher>
Reference: <author> Murphy, P. M., & Aha, D. W. </author> <year> (1995). </year> <title> UCI repository of machine learning databases (Machine-readable data repository). </title> <address> Irvine, CA: </address> <institution> University of California, Department of Information and Computer Science. </institution>
Reference-contexts: All datasets were drawn from the UCI repository <ref> (Murphy & Aha, 1995) </ref>, and are obtainable by anonymous ftp from ics.uci.edu, subdirectory pub/machine-learning-databases. Table 4 lists the datasets used, and summarizes some of their main characteristics. 3 Datasets included in the listing of empirical results in (Holte, 1993) are referred to by the same codes.
Reference: <author> Niblett, T. </author> <year> (1987). </year> <title> Constructing decision trees in noisy domains. </title> <booktitle> Proceedings of the Second European Working Session on Learning (pp. </booktitle> <pages> 67-78). </pages> <address> Bled, Yugoslavia: Sigma. </address>
Reference-contexts: However, the problem with both these measures is that they tend to favor overly specific rules: they attain their maximum value with a rule covering a single example. This can be overcome by use of the Laplace correction <ref> (Niblett, 1987) </ref>: n + 1 (6) where c is the number of classes. This measure tends to the uncorrected accuracy when the rule has strong statistical support (i.e., when it covers many examples), but tends to 1=c (i.e., "maximum ignorance") when it covers few.
Reference: <author> Oliveira, A. L., and Sangiovanni-Vincentelli, A. </author> <year> (1995). </year> <title> Inferring reduced ordered decision graphs of minimum description length. </title> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning (pp. </booktitle> <pages> 421-429). </pages> <address> Tahoe City, CA: </address> <publisher> Morgan Kaufmann. 30 Ourston, </publisher> <editor> D., & Mooney, R. J. </editor> <year> (1994). </year> <title> Theory refinement combining analytical and empirical methods. </title> <journal> Artificial Intelligence, </journal> <volume> 66, </volume> <pages> 273-309. </pages>
Reference: <author> Pagallo, G., & Haussler, D. </author> <year> (1990). </year> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 71-99. </pages>
Reference-contexts: Unlike RISE, FCLS employs different representations for rules and exemplars, and it uses the separate-and-conquer strategy of its AQ ancestors. RISE addresses the fragmentation problem in rule induction by employing a "conquering without separating" induction strategy. Other approaches to this problem include constructing new attributes <ref> (Pagallo & Haussler, 1990) </ref> and converting decision trees to decision graphs (Oliveira & Sangiovanni-Vincentelli, 1995; Kohavi & Li, 1995). Viewed as an instance-based learner, RISE performs context-sensitive feature selection, which can be seen as an extreme form of context-sensitive feature weighting.
Reference: <author> Pazzani, M., & Kibler, D. </author> <year> (1992). </year> <title> The utility of knowledge in inductive learning. </title> <journal> Machine Learning, </journal> <volume> 9, </volume> <pages> 57-94. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference-contexts: As a result, research in this field has blossomed, and several mature approaches to inductive learning are now available to the practitioner. These include induction of decision trees <ref> (Quinlan, 1986) </ref>, rule induction (Michalski, 1983), instance-based learning (Aha, Kibler & Albert, 1991), Bayesian classification (Bun 2 tine, 1989), back-propagation (Rumelhart, Hinton & Williams, 1986), and genetic algorithms (Booker, Goldberg & Holland, 1989). <p> The AQ series of algorithms (Michalski et al., 1986) uses apparent accuracy (i.e., the accuracy of the rule on the training set): H (n ; n ) = n + n The CN2 system (Clark & Niblett, 1989) originally used the entropy of the rule <ref> (Quinlan, 1986) </ref>. However, the problem with both these measures is that they tend to favor overly specific rules: they attain their maximum value with a rule covering a single example.
Reference: <author> Quinlan, J. R. </author> <year> (1987). </year> <title> Generating production rules from decision trees. </title> <booktitle> Proceedings of the Tenth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 304-307). </pages> <address> Milan, Italy: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Several induction algorithms proposed in the literature can be seen as empirical multi-strategy learners, but combining different paradigms from RISE's: 26 decision trees and rules <ref> (Quinlan, 1987) </ref>, decision trees and perceptrons (Utgoff, 1989b), rules and Bayesian classification (Smyth, Goodman & Higgins, 1990), backpropagation and genetic algorithms (Belew, McInerney & Schraudolph, 1992), instances and prototypes (Scott & Sage, 1992), decision trees and kernel density estimation (Smyth, Gray & Fayyad, 1995), etc.
Reference: <author> Quinlan, J. R. </author> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 239-266. </pages>
Reference-contexts: given, where each example (also called observation or case) is described by a vector of features or attribute values, and the goal is to form a description that can be used to classify previously unseen examples with high accuracy. (Examples can also be described in other languages, like first-order logic <ref> (Quinlan, 1990) </ref>, and other goals are often also important, like comprehensibility of the description.) The last decade has witnessed renewed interest in this area, largely due to its relevance to the "knowledge acquisition bottleneck" problem: the costliest component in the creation and deployment of an expert system is the construction of
Reference: <author> Quinlan, J. R. </author> <year> (1993a). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: Thus windowing may or may not decrease running time. Its effect on accuracy can also be positive or negative. A more detailed discussion of these issues in the context of decision tree induction can be found in (Catlett, 1991) and <ref> (Quinlan, 1993a) </ref>. In the empirical study described below, win-dowing improved RISE's running time in all the datasets where it was used (those with more than 3000 examples), sometimes by a large factor, while improving accuracy in all but one (where it had no significant effect). <p> A recent version of CN2 (6.1) was used, one incorporating Laplace accuracy and unordered rules (Clark & Boswell, 1991). To gauge its position in the overall spectrum of induction methods, RISE was also compared with a system that learns rule sets by way of decision trees, C4.5/C4.5RULES <ref> (Quinlan, 1993a) </ref>. The default classifier (always choosing the most frequent class) was also included in the study to provide a baseline.
Reference: <author> Quinlan, J. R. </author> <year> (1993b). </year> <title> Combining instance-based and model-based learning. </title> <booktitle> Proceedings of the Tenth International Conference on Machine Learning (pp. </booktitle> <pages> 236-243). </pages> <address> Amherst, MA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Rao, R. B., Gordon, D., & Spears, W. </author> <year> (1995). </year> <title> For every generalization action, is there really an equal and opposite reaction? Analysis of the conservation law for generalization performance. </title> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning (pp. </booktitle> <pages> 471-479). </pages> <address> Tahoe City, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A distinction should be made between all the mathematically possible domains, which are simply a product of the representation languages used, and the domains that occur in the real world, and are therefore the ones of primary interest <ref> (Rao, Gordon & Spears, 1995) </ref>. Without doubt there are many domains 3 in the former set that are not in the latter, and average accuracy in the real-world domains can be increased at the expense of accuracy in the domains that never occur in practice.
Reference: <author> Rendell, L. </author> <year> (1986). </year> <title> A general framework for induction and a study of selective induction. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 177-226. </pages>
Reference-contexts: and the small disjuncts problem (i.e., rules covering few training examples have a high error rate (Holte, Acker & Porter, 1989)). (The two paradigms also share a number of characteristics, of course, most notably the assumption that the example space contains large continuous regions of constant class membership|the similarity hypothesis <ref> (Rendell, 1986) </ref>.) Instances and rules also form the basis of two competing approaches to reasoning: case-based reasoning (Kolodner, 1993) and the rule-based reasoning more often found in expert systems.
Reference: <author> Riesbeck, C. K., & Schank, R. C. </author> <year> (1989). </year> <title> Inside case-based reasoning. </title> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum. </publisher>
Reference-contexts: In recent years, case-based reasoning has gained popularity as an alternative to rule systems, but its proponents recognize that there is a wide spectrum from specific cases to the very general rules typically used <ref> (Riesbeck & Schank, 1989) </ref>, and it deserves to be further explored. This article describes and evaluates the RISE algorithm, an approach to inductive learning that produces knowledge bases spanning this entire spectrum.
Reference: <author> Rivest, R. L. </author> <year> (1987). </year> <title> Learning decision lists. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <pages> 229-246. </pages>
Reference-contexts: Finally, if more than one rule covers the example, then two strategies are possible. One is to order the rules into a "decision list," and select only the first rule that fires <ref> (Rivest, 1987) </ref>. The other is to let the different rules vote, and select the class receiving the most votes. Recent versions of CN2 attach to each rule the number of examples of each class that it covers, and use these numbers as votes at classification time (Clark & Boswell, 1991).
Reference: <author> Rumelhart, D. E., Hinton, G. E., & Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart & J. L. McClelland (Eds.), </editor> <booktitle> Parallel distributed processing: Explorations in the microstructure of cognition (Vol. 2). </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: As a result, research in this field has blossomed, and several mature approaches to inductive learning are now available to the practitioner. These include induction of decision trees (Quinlan, 1986), rule induction (Michalski, 1983), instance-based learning (Aha, Kibler & Albert, 1991), Bayesian classification (Bun 2 tine, 1989), back-propagation <ref> (Rumelhart, Hinton & Williams, 1986) </ref>, and genetic algorithms (Booker, Goldberg & Holland, 1989). Empirical comparison of these different approaches in a variety of application domains has shown that each performs best in some, but not all, domains. <p> The default classifier (always choosing the most frequent class) was also included in the study to provide a baseline. Backpropagation <ref> (Rumelhart et al., 1986) </ref>, although a widely-used learning algorithm, was left out because its need for extensive fine-tuning and very long running times would make a large-scale study of this type difficult to carry out. Thirty datasets were used in the study.
Reference: <author> Salzberg, S. </author> <year> (1991). </year> <title> A nearest hyperrectangle learning method. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 251-276. </pages>
Reference-contexts: A commonly used approach <ref> (see, e.g., Salzberg, 1991) </ref> is to normalize the difference by its largest observed value: ffi (x i ; x j ) = fi fi x max x min fi fi (1) If there are a attributes, the distance between two instances E 1 = (e 11 ; e 12 ; . <p> This definition of ffi num (i) is also used in EACH <ref> (Salzberg, 1991) </ref>. The distance from a missing numeric value to any other is defined as 0. If a symbolic attribute's value is missing, it is assigned the special value "?". <p> In form, the most similar system to RISE in the literature is EACH <ref> (Salzberg, 1991) </ref>, which generalizes instances to hyperrectangles, and classifies each test example according to its nearest hyperrectangle.
Reference: <author> Schaffer, C. </author> <year> (1994a). </year> <title> Cross-validation, stacking, and bi-level stacking: Meta-methods for classification learning. </title> <editor> In P. Cheeseman & R. W. Oldford (Eds.), </editor> <title> Selecting models from data: </title> <booktitle> Artificial intelligence and statistics IV. </booktitle> <address> New York, NY: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: This has been termed the "selective superiority" problem (Brodley, 1995), and presents a dilemma to the knowledge engineer approaching a new task: which induction paradigm should be used? One solution is to try each one in turn, and use cross-validation to choose the one that appears to perform best <ref> (Schaffer, 1994a) </ref>.
Reference: <author> Schaffer, C. </author> <year> (1994b). </year> <title> A conservation law for generalization performance. </title> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning (pp. </booktitle> <pages> 259-265). </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Further, recent results <ref> (Schaffer, 1994b) </ref> show that performance over the set of all possible domains is subject to a "conservation law": if one algorithm is better than another in some domains, then there are necessarily other domains in which this relationship is reversed.
Reference: <author> Scott, P. D., & Sage, K. H. </author> <year> (1992). </year> <title> Why generalize? Hybrid representations and instance-based learning. </title> <booktitle> Proceedings of the Tenth European Conference on Artificial Intelligence (pp. </booktitle> <pages> 484-486). </pages> <address> Vienna, Austria: </address> <publisher> Wiley. </publisher>
Reference-contexts: in the literature can be seen as empirical multi-strategy learners, but combining different paradigms from RISE's: 26 decision trees and rules (Quinlan, 1987), decision trees and perceptrons (Utgoff, 1989b), rules and Bayesian classification (Smyth, Goodman & Higgins, 1990), backpropagation and genetic algorithms (Belew, McInerney & Schraudolph, 1992), instances and prototypes <ref> (Scott & Sage, 1992) </ref>, decision trees and kernel density estimation (Smyth, Gray & Fayyad, 1995), etc. In form, the most similar system to RISE in the literature is EACH (Salzberg, 1991), which generalizes instances to hyperrectangles, and classifies each test example according to its nearest hyperrectangle.
Reference: <author> Smyth, P. R., Goodman, M., & Higgins, C. </author> <year> (1990). </year> <title> A hybrid rule-based/Bayesian classifier. </title> <booktitle> Proceedings of the Seventh European Conference on Artificial Intelligence (pp. </booktitle> <pages> 610-615). </pages> <address> Stock-holm, Sweden: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Several induction algorithms proposed in the literature can be seen as empirical multi-strategy learners, but combining different paradigms from RISE's: 26 decision trees and rules (Quinlan, 1987), decision trees and perceptrons (Utgoff, 1989b), rules and Bayesian classification <ref> (Smyth, Goodman & Higgins, 1990) </ref>, backpropagation and genetic algorithms (Belew, McInerney & Schraudolph, 1992), instances and prototypes (Scott & Sage, 1992), decision trees and kernel density estimation (Smyth, Gray & Fayyad, 1995), etc.
Reference: <author> Smyth, P., Gray, A., & Fayyad, U. </author> <year> (1995). </year> <title> Retrofitting decision tree classifiers using kernel density estimation. </title> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning (pp. </booktitle> <pages> 506-514). </pages> <address> Tahoe City, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: but combining different paradigms from RISE's: 26 decision trees and rules (Quinlan, 1987), decision trees and perceptrons (Utgoff, 1989b), rules and Bayesian classification (Smyth, Goodman & Higgins, 1990), backpropagation and genetic algorithms (Belew, McInerney & Schraudolph, 1992), instances and prototypes (Scott & Sage, 1992), decision trees and kernel density estimation <ref> (Smyth, Gray & Fayyad, 1995) </ref>, etc. In form, the most similar system to RISE in the literature is EACH (Salzberg, 1991), which generalizes instances to hyperrectangles, and classifies each test example according to its nearest hyperrectangle.
Reference: <author> Stanfill, C., & Waltz, D. </author> <year> (1986). </year> <title> Toward memory-based reasoning. </title> <journal> Communications of the ACM, </journal> <volume> 29, </volume> <pages> 1213-1228. </pages>
Reference: <author> Ting, K. M. </author> <year> (1994). </year> <title> Discretization of continuous-valued attributes and instance-based learning (Technical Report 491). </title> <address> Sydney, Australia: </address> <institution> Basser Department of Computer Science, University of Sydney. </institution>
Reference: <author> Towell, G. G., & Shavlik, J. W. </author> <year> (1994). </year> <title> Knowledge-based artificial neural networks. </title> <journal> Artificial Intelligence, </journal> <volume> 70, </volume> <pages> 119-165. </pages>
Reference: <author> Townsend-Weber, T., & Kibler, D. </author> <year> (1994). </year> <title> Instance-based prediction of continuous values. </title> <booktitle> Proceedings of the 1994 AAAI Workshop on Case-Based Reasoning (pp. </booktitle> <pages> 30-35). </pages> <address> Seattle, WA: </address> <publisher> AAAI. </publisher>
Reference: <author> Utgoff, P. E. </author> <year> (1989a). </year> <title> Incremental induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 4, </volume> <pages> 161-186. </pages> <note> 31 Utgoff, </note> <author> P. E. </author> <year> (1989b). </year> <title> Perceptron trees: A case study in hybrid concept representations. </title> <journal> Connection Science, </journal> <volume> 1, </volume> <pages> 377-391. </pages>
Reference-contexts: complexity of CN2 and AQ-style algorithms is O (be 2 a 2 ), where b is the beam size, an integer-valued internal parameter of the algorithm (Clark & Niblett, 1989). 2 The worst-case complexity of growing a decision tree is O (ea 2 ) when only symbolic attributes are present <ref> (Utgoff, 1989a) </ref>, but becomes quadratic or worse in e with numeric ones, mainly due to the need for repeated sorting operations (Catlett, 1991). The final pruning stage used in C4.5 and several rule induction systems may also in general be worse than quadratic in e (Cohen, 1995).
Reference: <author> Wettschereck, D. </author> <year> (1994). </year> <title> A hybrid nearest-neighbor and nearest-hyperrectangle algorithm. </title> <booktitle> Proceedings of the Ninth European Conference on Machine Learning (pp. </booktitle> <pages> 323-335). </pages> <address> Catania, Italy: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Recently Wettschereck and Dietterich (1995) carried out a detailed comparison of EACH and k-nearest-neighbor (kNN), and designed an algorithm that combines the two <ref> (Wettschereck, 1994) </ref>, but does not achieve greater accuracy than kNN alone. They found EACH to be less accurate than kNN in most of the domains studied, and the chief cause of this to be EACH's use of overlapping rectangles.
Reference: <author> Wettschereck, D., & Dietterich, T. </author> <year> (1995). </year> <title> An experimental comparison of the nearest-neighbor and nearest-hyperrectangle algorithms. </title> <journal> Machine Learning, </journal> <volume> 19, </volume> <pages> 5-27. </pages>
Reference: <author> Wnek, J., & R. S. </author> <title> Michalski (1994). Hypothesis-driven constructive induction in AQ17-HCI: A method and experiments. </title> <journal> Machine Learning, </journal> <volume> 14, </volume> <pages> 139-168. </pages>
Reference-contexts: Related work The RISE approach should be seen in the context of previous work in inductive learning and related areas. The AQ15 system employed best-match classification (Michalski et al, 1986; a more recent version is AQ17-HCI <ref> (Wnek & Michalski, 1994) </ref>). It differed from RISE in that it was a general-to-specific, separate-and-conquer system that learned purely logical rules, and only introduced the best-match policy in a post-processing step, with the goal of removing rules from the set without adversely affecting accuracy.
Reference: <author> Zhang, J. </author> <year> (1990). </year> <title> A method that combines inductive learning with exemplar-based learning. </title> <booktitle> Proceedings of the Second IEEE International Conference on Tools for Artificial Intelligence (pp. </booktitle> <pages> 31-37). </pages> <address> San Jose, CA: </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: It also used a different distance measure. Its approach is carried further in the FCLS system <ref> (Zhang, 1990) </ref>, which combines rules with exemplars in an attempt to alleviate the small disjuncts problem. Unlike RISE, FCLS employs different representations for rules and exemplars, and it uses the separate-and-conquer strategy of its AQ ancestors.
References-found: 66

