URL: http://www.cs.toronto.edu/~ftp/pub/reports/na/transpose.99.ps.Z
Refering-URL: http://www.cs.toronto.edu/NA/reports.html
Root-URL: http://www.cs.toronto.edu
Title: An Efficient Transposition Algorithm for Distributed Memory Computers  
Author: Christina Christara Xiaoliang Ding and Ken Jackson 
Keyword: data transposition, shallow-water equations, Cray T3E, cluster of workstations, MPI, FFT, Helmholtz equation, d-dimensional mesh of processors.  
Date: January 1999  
Abstract: Data transposition is required in many numerical applications, including both Fast Direct Solvers and the Alternating Direction (ADI) Method for linear algebraic systems as well as Spectral Methods for PDEs. When implemented on a distributed-memory computer, data transposition requires all-to-all communication, a time consuming operation. The Direct Exchange algorithm, commonly used for this task, is inefficient if the number of processors is large. We investigate a series of more sophisticated techniques: the Ring Exchange, Mesh Exchange and Cube Exchange algorithms. These data transposition schemes were incorporated into a parallel solver for the shallow-water equations. We compare the performance of these schemes with that of the Direct Exchange Algorithm and the MPI all-to-all communication routine. The numerical experiments were performed on a Cray T3E computer with 512 processors and on an ethernet-connected cluster of 36 Sun workstations. Both the analysis and the numerical results indicate that the more sophisticated Mesh and Cube Exchange algorithms perform better than either the simpler well-known Direct and Ring Exchange schemes or the MPI all-to-all communication routine. fl This work was supported in part by the Natural Sciences and Engineering Research Council (NSERC) of Canada and the Information Technology Research Centre (ITRC) of Ontario. We also thank Cray Research Inc. for allowing us to use one of their T3E computers to obtain our numerical results. y Computer Science Department, University of Toronto, Toronto, Ontario M5S 3G4, Canada. Email: fccc, ding, krjg@cs.toronto.edu. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Choi, J. Dongarra and D. Walker, </author> <title> Parallel Matrix Transpose Algorithms on Distributed Memory Concurrent Computers, </title> <journal> Parallel Computing, </journal> <volume> vol. 21, </volume> <year> 1995, </year> <pages> pp. </pages> <month> 1387-1405. </month> <title> 19 code using various transposition algorithms on the Cray T3E with 1024 fi 1025 grid points. points. </title>
Reference-contexts: That is, data transposition is required between the FFT and tridiagonal system solve phases of the Fast Direct Solver. This implies that each processor must exchange data with each other, a potentially time-consuming task. Several researchers have considered this data transposition problem; see, for example, <ref> [1, 5, 7] </ref> and the references therein. In this paper, we propose the Mesh Exchange and Cube Exchange data transposition algorithms designed to exploit, respectively, a two- or three-dimensional processor interconnection network. <p> In Phase 2, processor p holds blocks blk (p; j), j = 0; : : : ; P 1. In Phase 3, it holds blocks blk (i; p), i = 0; : : : ; P 1. We start by describing the well-known Direct Exchange and Ring Exchange algorithms <ref> [1, 7] </ref>. Each requires O (P ) messages per processor. To reduce the number of messages sent by each processor, we extend the Ring Exchange algorithm to the Mesh Exchange and Cube Exchange algorithms that exploit, respectively, a two- or three-dimensional processor interconnection network.
Reference: [2] <author> Xiaoliang Ding, </author> <title> Numerical Solution of the Shallow-Water Equations on Distributed Memory Systems, M.Sc. </title> <type> Thesis, </type> <institution> Computer Science Department, University of Toronto, </institution> <year> 1998. </year> <note> Available from http://www.cs.toronto.edu/NA/reports.html. </note>
Reference-contexts: For a more detailed description of the equations and the numerical scheme, see <ref> [2, 8, 9] </ref>. The shallow-water equations are three time-dependent PDEs in two space dimensions that model the inviscid flow of a thin layer of fluid. <p> Then the matrix associated with the linear algebraic systems takes a tensor-product form <ref> [2, 9] </ref> and each linear algebraic system can be solved by applying a Fast Direct Solver. <p> Each processor is assigned the data and computation associated with the points of one subdomain. Such a decomposition is favorable during all steps of the shallow-water algorithm, except Steps 5 and 7. For further details, see <ref> [2, 9] </ref>. Thus, the parallel algorithm for solving the shallow-water equations has three different phases of data access modes: Phase 1 requires a 2D x-y block partition of data; Phase 2 requires a 1D x-direction block partition of data; Phase 3 requires a 1D y-direction block partition of data. <p> This code is based on the numerical methods briefly described in x2. For more details, see <ref> [2, 9] </ref>. Our numerical experiments were performed on both a Cray T3E machine with 512 processors and an ethernet-connected cluster of 36 Sun workstations. The Cray T3E is a MIMD parallel machine and provides shared physical address space for up to 2048 processors over a 3D torus interconnection. <p> The elapsed times include computing time, communication time, idle time and all other overhead. To investigate the scalability of the algorithms, numerical experiments were performed for several problem sizes and numbers of processors. A theoretical scalability analysis of the algorithms is provided in <ref> [2] </ref>. <p> Our numerical results confirm this analysis. However, as shown in <ref> [2] </ref>, the parallel algorithm for the shallow-water equations remains essentially unscalable even with our more sophisticated transposition algorithms. Further research can be done to improve the performance of the Mesh and Cube Exchange algorithms. Our schemes were implemented using high level MPI communication routines.
Reference: [3] <author> Ian Foster, </author> <title> Designing and Building Parallel Programs: Concepts and Tools for Parallel Software Engineering, </title> <publisher> Addison-Wesley, </publisher> <year> 1995. </year>
Reference-contexts: In practice, however, a parallel machine may not have sufficiently many links to guarantee that each processor can exchange messages with each other concurrently without contention. For a cluster of workstations connected by an ethernet, for example, all P processors share one communication channel. According to Foster <ref> [3] </ref>, the communication cost for such a cluster (and many other distributed memory computers) is approximated more accurately by T dir (P 1)(t s + m b P t w ) P t s + M N t w (2) 3.2 The Ring Exchange Algorithm A linear array of processors with
Reference: [4] <author> William Gropp, </author> <title> Using MPI: portable parallel programming with the message-passing interface, </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Moreover, they can be extended to a d-dimensional processor interconnection network, for any d. A parallel numerical method for the shallow-water equations incorporating these data transposition schemes was implemented using MPI <ref> [4] </ref>. The 2 method was run on both a Cray T3E computer with 512 processors and an ethernet-connected cluster of 36 Sun workstations. <p> worth noting that, just as the Ring Exchange algorithm is extended to the Mesh and Cube Exchange algorithms, it can also be extended in a similar way to an algorithm exploiting a d-dimensional mesh of processors for any d &gt; 1. 13 4 Numerical Experiments and Results We used MPI <ref> [4] </ref> to implement the transpose schemes described in the previous section and incorporated these schemes into the shallow-water code developed by Steve Thomas at Environment Canada. This code is based on the numerical methods briefly described in x2. For more details, see [2, 9].
Reference: [5] <author> S. L. Johnsson and C. T. Ho, </author> <title> Algorithms for Matrix Transposition on Boolean N-Cube Configured Ensemble Architectures, </title> <journal> SIAM J. Matrix Anal. and Applic., </journal> <volume> vol. 9 (3), </volume> <month> July </month> <year> 1988, </year> <pages> pp. 419-454. </pages>
Reference-contexts: That is, data transposition is required between the FFT and tridiagonal system solve phases of the Fast Direct Solver. This implies that each processor must exchange data with each other, a potentially time-consuming task. Several researchers have considered this data transposition problem; see, for example, <ref> [1, 5, 7] </ref> and the references therein. In this paper, we propose the Mesh Exchange and Cube Exchange data transposition algorithms designed to exploit, respectively, a two- or three-dimensional processor interconnection network.
Reference: [6] <author> Charles Van Loan, </author> <title> Computational Frameworks for the Fast Fourier Transform, </title> <institution> Society for Industrial and Applied Mathematics, </institution> <year> 1992. </year>
Reference-contexts: This process is commonly referred to as a Fast Direct Solver <ref> [6, 9] </ref>. On a distributed-memory parallel computer, it is highly desirable to have each processor perform one or more FFTs using data stored locally on that processor and similarly to perform one or more tridiagonal system solves using data stored locally on that processor.
Reference: [7] <author> David M. Nicol and Shahid H. Bokhari, </author> <title> Optimal Multiphase Complete Exchange on Circuit-Switched Hypercube Architectures, </title> <journal> SIGMETRICS, ACM, </journal> <volume> vol. 22, no. 1, </volume> <month> May </month> <year> 1994, </year> <pages> pp. 252-260. </pages>
Reference-contexts: That is, data transposition is required between the FFT and tridiagonal system solve phases of the Fast Direct Solver. This implies that each processor must exchange data with each other, a potentially time-consuming task. Several researchers have considered this data transposition problem; see, for example, <ref> [1, 5, 7] </ref> and the references therein. In this paper, we propose the Mesh Exchange and Cube Exchange data transposition algorithms designed to exploit, respectively, a two- or three-dimensional processor interconnection network. <p> In Phase 2, processor p holds blocks blk (p; j), j = 0; : : : ; P 1. In Phase 3, it holds blocks blk (i; p), i = 0; : : : ; P 1. We start by describing the well-known Direct Exchange and Ring Exchange algorithms <ref> [1, 7] </ref>. Each requires O (P ) messages per processor. To reduce the number of messages sent by each processor, we extend the Ring Exchange algorithm to the Mesh Exchange and Cube Exchange algorithms that exploit, respectively, a two- or three-dimensional processor interconnection network.
Reference: [8] <author> A. Robert, </author> <title> A Semi-Lagrangian and Semi-Implicit Numerical Integration Scheme for the Primitive Meteorological Equations, Jpn. </title> <journal> Meteor. Soc., </journal> <volume> vol. 60, </volume> <year> 1982, </year> <pages> pp. 319-325. </pages>
Reference-contexts: For a more detailed description of the equations and the numerical scheme, see <ref> [2, 8, 9] </ref>. The shallow-water equations are three time-dependent PDEs in two space dimensions that model the inviscid flow of a thin layer of fluid. <p> Such a scheme allows for relatively large time-steps, while maintaining stability. If a semi-implicit scheme is coupled with a semi-Lagrangian technique, the time-steps can be increased even further <ref> [8] </ref>. A semi-Lagrangian treatment of advection applies an implicit correction to the explicit scheme by tracing the trajectory backwards and computing the departure point using a fixed-point iteration.
Reference: [9] <author> S. Thomas, J. Cote, A. Staniforth, I. Lie and R. Skalin, </author> <title> A Semi-Implicit Semi-Lagrangian Shallow-Water Model for Massively Parallel Processors, </title> <booktitle> Proceedings of the 6th ECMWF Workshop on the Use of Parallel Processors in Meteorology, </booktitle> <month> Nov. </month> <year> 1994, </year> <pages> pp. </pages> <month> 407-423. </month> <title> 20 grid points. using various transposition algorithms on the CDF system with 150 fi 151 grid points. code using various transposition algorithms on the CDF system with 150 fi 151 grid points. points. </title> <type> 21 </type>
Reference-contexts: This process is commonly referred to as a Fast Direct Solver <ref> [6, 9] </ref>. On a distributed-memory parallel computer, it is highly desirable to have each processor perform one or more FFTs using data stored locally on that processor and similarly to perform one or more tridiagonal system solves using data stored locally on that processor. <p> For a more detailed description of the equations and the numerical scheme, see <ref> [2, 8, 9] </ref>. The shallow-water equations are three time-dependent PDEs in two space dimensions that model the inviscid flow of a thin layer of fluid. <p> Then the matrix associated with the linear algebraic systems takes a tensor-product form <ref> [2, 9] </ref> and each linear algebraic system can be solved by applying a Fast Direct Solver. <p> Each processor is assigned the data and computation associated with the points of one subdomain. Such a decomposition is favorable during all steps of the shallow-water algorithm, except Steps 5 and 7. For further details, see <ref> [2, 9] </ref>. Thus, the parallel algorithm for solving the shallow-water equations has three different phases of data access modes: Phase 1 requires a 2D x-y block partition of data; Phase 2 requires a 1D x-direction block partition of data; Phase 3 requires a 1D y-direction block partition of data. <p> This code is based on the numerical methods briefly described in x2. For more details, see <ref> [2, 9] </ref>. Our numerical experiments were performed on both a Cray T3E machine with 512 processors and an ethernet-connected cluster of 36 Sun workstations. The Cray T3E is a MIMD parallel machine and provides shared physical address space for up to 2048 processors over a 3D torus interconnection.
References-found: 9

