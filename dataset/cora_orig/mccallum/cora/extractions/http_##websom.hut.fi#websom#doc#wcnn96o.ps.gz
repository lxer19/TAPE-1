URL: http://websom.hut.fi/websom/doc/wcnn96o.ps.gz
Refering-URL: 
Root-URL: 
Email: Samuel.Kaski@hut.fi  
Title: Creating an Order in Digital Libraries with Self-Organizing Maps  
Author: Samuel Kaski, Timo Honkela, Krista Lagus, and Teuvo Kohonen 
Address: Rakentajanaukio 2 C, FIN-02150 Espoo, Finland  
Affiliation: Helsinki University of Technology Neural Networks Research Centre  
Note: In Proc. WCNN'96, World Congress on Neural Networks, pp. 814-817. Lawrence Erlbaum and INNS Press, Mahwah, NJ, 1996.  
Abstract: Formulation of suitable search expressions for information retrieval from large full-text databases may currently require considerable efforts. Changing the scope of the search when, e.g., too many or too few hits have been obtained, requires re-formulation of the search expression. For an alternative scheme we suggest an explorative full-text information retrieval method, where the Self-Organizing Map (SOM) algorithm is used to order documents based on their full textual contents. The visualized order can then be utilized for an explorative search or exploration of novel knowledge areas, whereby the scope can be changed interactively. The ordering of the documents is achieved by a two-level analysis: First, word categories are extracted from the text by a "semantic" SOM. Second, the textual context of the documents is encoded on the basis of the histograms of words formed on the word category map. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Honkela, V. Pulkki, and T. Kohonen. </author> <title> Contextual relations of words in Grimm tales analyzed by self-organizing map. </title> <editor> In F. Fogelman-Soulie and P. Gallinari, eds., </editor> <booktitle> Proc. ICANN-95, Int. Conf. on Artificial Neural Networks, </booktitle> <volume> vol. II, </volume> <pages> pp. 3-7. </pages> <institution> EC2 et Cie, Paris, </institution> <year> 1995. </year>
Reference-contexts: An extra benefit from the use of word category histograms instead of simple word histograms is that the dimensionality of the input to the document map is reduced by an order of magnitude. Several studies have been published on SOMs that map words into grammatical and semantic categories <ref> [1] </ref>, [7], [8], [9], [12]. The SOM has also been utilized previously to form a small map based on titles of scientific documents by Lin et al. [4]. Scholtes has developed, based on the SOM, a neural filter and a neural interest map for information retrieval [10], [11], [12].
Reference: [2] <author> T. Kohonen. </author> <title> Self-organized formation of topologically correct feature maps. </title> <journal> Biol. Cybern., </journal> <volume> 43 </volume> <pages> 59-69, </pages> <year> 1982. </year>
Reference-contexts: However, while the amount of available textual information increases progressively, automatic methods for its management become necessary. The previous nonexistence of such methods has been due to a lack of effective means for encoding and ordering free-form documents. The Self-Organizing Map (SOM) <ref> [2] </ref>, [3] is a general unsupervised tool for ordering high-dimensional statistical data so that alike inputs are in general mapped close to each other. To utilize the SOM on texts, a document might, for example, be represented as the histogram of its words.
Reference: [3] <author> T. Kohonen. </author> <title> Self-Organizing Maps. </title> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1995. </year>
Reference-contexts: However, while the amount of available textual information increases progressively, automatic methods for its management become necessary. The previous nonexistence of such methods has been due to a lack of effective means for encoding and ordering free-form documents. The Self-Organizing Map (SOM) [2], <ref> [3] </ref> is a general unsupervised tool for ordering high-dimensional statistical data so that alike inputs are in general mapped close to each other. To utilize the SOM on texts, a document might, for example, be represented as the histogram of its words.
Reference: [4] <author> X. Lin, D. Soergel, and G. Marchionini. </author> <title> A self-organizing semantic map for information retrieval. </title> <booktitle> In Proc. 14th Ann. Int. ACM/SIGIR Conf. on R & D In Information Retrieval, </booktitle> <pages> pp. 262-269, </pages> <year> 1991. </year>
Reference-contexts: Several studies have been published on SOMs that map words into grammatical and semantic categories [1], [7], [8], [9], [12]. The SOM has also been utilized previously to form a small map based on titles of scientific documents by Lin et al. <ref> [4] </ref>. Scholtes has developed, based on the SOM, a neural filter and a neural interest map for information retrieval [10], [11], [12]. Merkl [5], [6] has used the SOM to cluster textual descriptions of software library components.
Reference: [5] <author> D. Merkl. </author> <title> Structuring software for reuse|the case of self-organizing maps. </title> <booktitle> In Proc. IJCNN-93-Nagoya, Int. Joint Conf. on Neural Networks, </booktitle> <volume> vol. III, </volume> <pages> pp. 2468-2471. </pages> <publisher> IEEE Service Center. </publisher> <address> Piscataway, NJ, </address> <year> 1993. </year>
Reference-contexts: The SOM has also been utilized previously to form a small map based on titles of scientific documents by Lin et al. [4]. Scholtes has developed, based on the SOM, a neural filter and a neural interest map for information retrieval [10], [11], [12]. Merkl <ref> [5] </ref>, [6] has used the SOM to cluster textual descriptions of software library components. In this work we introduce a new architecture in which the semantic SOM is first used to extract statistical contextual information from free-form documents, and another SOM transforms the categorial statistics into an ordered document map.
Reference: [6] <author> D. Merkl, A. M. Tjoa, and G. Kappel. </author> <title> A self-organizing map that learns the semantic similarity of reusable software components. </title> <booktitle> In Proc. ACNN'94, 5th Australian Conf. on Neural Networks, </booktitle> <pages> pp. 13-16, </pages> <year> 1994. </year>
Reference-contexts: The SOM has also been utilized previously to form a small map based on titles of scientific documents by Lin et al. [4]. Scholtes has developed, based on the SOM, a neural filter and a neural interest map for information retrieval [10], [11], [12]. Merkl [5], <ref> [6] </ref> has used the SOM to cluster textual descriptions of software library components. In this work we introduce a new architecture in which the semantic SOM is first used to extract statistical contextual information from free-form documents, and another SOM transforms the categorial statistics into an ordered document map.
Reference: [7] <author> R. Miikkulainen. </author> <title> Subsymbolic Natural Language Processing: An Integrated Model of Scripts, Lexicon, and Memory. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: An extra benefit from the use of word category histograms instead of simple word histograms is that the dimensionality of the input to the document map is reduced by an order of magnitude. Several studies have been published on SOMs that map words into grammatical and semantic categories [1], <ref> [7] </ref>, [8], [9], [12]. The SOM has also been utilized previously to form a small map based on titles of scientific documents by Lin et al. [4]. Scholtes has developed, based on the SOM, a neural filter and a neural interest map for information retrieval [10], [11], [12].
Reference: [8] <author> H. Ritter and T. Kohonen. </author> <title> Self-organizing semantic maps. </title> <journal> Biol. Cybern., </journal> <volume> 61 </volume> <pages> 241-254, </pages> <year> 1989. </year>
Reference-contexts: To utilize the SOM on texts, a document might, for example, be represented as the histogram of its words. A more practical method is to first use the so-called semantic SOM <ref> [8] </ref> for word categorization. The semantic SOM organizes the words into grammatical and semantic categories represented on a two-dimensional array. The relative similarity of the categories is reflected in their distance relationships on the array. <p> Several studies have been published on SOMs that map words into grammatical and semantic categories [1], [7], <ref> [8] </ref>, [9], [12]. The SOM has also been utilized previously to form a small map based on titles of scientific documents by Lin et al. [4]. Scholtes has developed, based on the SOM, a neural filter and a neural interest map for information retrieval [10], [11], [12].
Reference: [9] <author> H. Ritter and T. Kohonen. </author> <title> Learning "semantotopic maps" from context. </title> <editor> In M. Caudill, ed., </editor> <booktitle> Proc. IJCNN-90-WASH-DC, Int. Joint Conf. on Neural Networks, </booktitle> <volume> vol. I, </volume> <pages> pp. 23-26. </pages> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, NJ, </address> <year> 1990. </year>
Reference-contexts: Several studies have been published on SOMs that map words into grammatical and semantic categories [1], [7], [8], <ref> [9] </ref>, [12]. The SOM has also been utilized previously to form a small map based on titles of scientific documents by Lin et al. [4]. Scholtes has developed, based on the SOM, a neural filter and a neural interest map for information retrieval [10], [11], [12].
Reference: [10] <author> J. C. Scholtes. </author> <title> Kohonen feature maps in full-text data bases: A case study of the 1987 Pravda. </title> <booktitle> In Proc. Informatiewetenschap, </booktitle> <pages> pp. 203-220. </pages> <address> STINFON, Nijmegen, Netherlands, </address> <year> 1991. </year>
Reference-contexts: The SOM has also been utilized previously to form a small map based on titles of scientific documents by Lin et al. [4]. Scholtes has developed, based on the SOM, a neural filter and a neural interest map for information retrieval <ref> [10] </ref>, [11], [12]. Merkl [5], [6] has used the SOM to cluster textual descriptions of software library components.
Reference: [11] <author> J. C. Scholtes. </author> <title> Unsupervised learning and the information retrieval problem. </title> <booktitle> In Proc. IJCNN'91, Int. Joint Conf. on Neural Networks, </booktitle> <pages> pp. 95-100. </pages> <publisher> IEEE Service Center, </publisher> <address> Piscataway, NJ, </address> <year> 1991. </year>
Reference-contexts: The SOM has also been utilized previously to form a small map based on titles of scientific documents by Lin et al. [4]. Scholtes has developed, based on the SOM, a neural filter and a neural interest map for information retrieval [10], <ref> [11] </ref>, [12]. Merkl [5], [6] has used the SOM to cluster textual descriptions of software library components.
Reference: [12] <author> J. C. Scholtes. </author> <title> Neural Networks in Natural Language Processing and Information Retrieval. </title> <type> PhD thesis, </type> <institution> Universiteit van Amsterdam, Netherlands, </institution> <year> 1993. </year>
Reference-contexts: Several studies have been published on SOMs that map words into grammatical and semantic categories [1], [7], [8], [9], <ref> [12] </ref>. The SOM has also been utilized previously to form a small map based on titles of scientific documents by Lin et al. [4]. Scholtes has developed, based on the SOM, a neural filter and a neural interest map for information retrieval [10], [11], [12]. <p> semantic categories [1], [7], [8], [9], <ref> [12] </ref>. The SOM has also been utilized previously to form a small map based on titles of scientific documents by Lin et al. [4]. Scholtes has developed, based on the SOM, a neural filter and a neural interest map for information retrieval [10], [11], [12]. Merkl [5], [6] has used the SOM to cluster textual descriptions of software library components.
References-found: 12

