URL: http://www.robotics.stanford.edu/~koller/papers/apn.ps
Refering-URL: http://www.robotics.stanford.edu/~koller/papers/apn.html
Root-URL: http://www.robotics.stanford.edu
Title: Adaptive Probabilistic Networks with Hidden Variables  
Author: JOHN BINDER DAPHNE KOLLER STUART RUSSELL Editor: Padhraic Smyth 
Keyword: Bayesian networks, learning, gradient descent, prior knowledge, dynamic networks, hybrid networks  
Address: Berkeley, CA 94720-1776  Stanford, CA 94305-9010  Berkeley, CA 94720-1776 KEIJI KANAZAWA  One Microsoft Way, Redmond, WA 98052-6399  
Affiliation: Computer Science Division, University of California,  Computer Science Department, Stanford University,  Computer Science Division, University of California,  Microsoft Corporation,  
Note: 1-33 c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Email: binder@cs.berkeley.edu  koller@cs.stanford.edu  russell@cs.berkeley.edu  keijik@microsoft.com  
Date: Received July 23, 1996; Revised March 24, 1997  
Abstract: Probabilistic networks (also known as Bayesian belief networks) allow a compact description of complex stochastic relationships among several random variables. They are rapidly becoming the tool of choice for uncertain reasoning in artificial intelligence. In this paper, we investigate the problem of learning probabilistic networks with known structure and hidden variables. This is an important problem, because structure is much easier to elicit from experts than numbers, and the world is rarely fully observable. We present a gradient-based algorithm and show that the gradient can be computed locally, using information that is available as a byproduct of standard probabilistic network inference algorithms. Our experimental results demonstrate that using prior knowledge about the structure, even with hidden variables, can significantly improve the learning rate of probabilistic networks. We extend the method to networks in which the conditional probability tables are described using a small number of parameters. Examples include noisy-OR nodes and dynamic probabilistic networks. We show how this additional structure can be exploited by our algorithm to speed up the learning even further. We also outline an extension to hybrid networks, in which some of the nodes take on values in a continuous domain. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Andersen, S. K., K. G. Olesen, F. V. Jensen, and F. Jensen (1989, </author> <month> August). </month> <title> HUGIN|a shell for building Bayesian belief universes for expert systems. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (IJCAI-89), Volume 2, </booktitle> <address> Detroit, </address> <publisher> Michigan, </publisher> <pages> pp. 1080-1085. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This algorithm uses repeated line minimizations (with direction chosen by the Polak-Ribiere method) and a heuristic termination condition to signal a maximum. We use the Hugin package <ref> (Andersen, Olesen, Jensen, and Jensen, 1989) </ref>, which uses a clustering algorithm, for most of the inference computations. 11 Table 1.
Reference: <author> Apolloni, B. and D. </author> <title> de Falco (1991). Learning by asymmetric parallel boltzmann machines. </title> <booktitle> Neural Computation 3 (3), </booktitle> <pages> 402-408. </pages>
Reference: <author> Baum, E. B. and F. </author> <month> Wilczek </month> <year> (1988). </year> <title> Supervised learning of probability distributions by neural networks. </title> <editor> In D. Z. Anderson (Ed.), </editor> <booktitle> Neural Information Processing Systems, </booktitle> <pages> pp. 52-61. </pages> <address> New York: </address> <publisher> American Institute of Physics. </publisher>
Reference-contexts: Reported results were then averaged over the ten training sets at each training set size. Prediction performance for the APN algorithm was compared against that of a feedforward neural network. The neural network was trained using the update rule for cross-entropy minimization <ref> (Baum and Wilczek, 1988) </ref>, which allows the output unit values to be interpreted as probabilities. To ensure that the distribution over the output units for each output variable summed to 1, the output layer used the softmax parameterization (Bridle, 1990).
Reference: <author> Bishop, C. M. </author> <year> (1995). </year> <title> Neural Networks for Pattern Recognition. </title> <publisher> Oxford: Oxford University Press. </publisher>
Reference: <author> Bridle, J. S. </author> <year> (1990). </year> <title> Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. </title> <editor> In F. Fogelman Soulie and J. Herault (Eds.), Neurocomputing: </editor> <booktitle> Algorithms, Architectures and Applications. </booktitle> <address> Berlin: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: To ensure that the distribution over the output units for each output variable summed to 1, the output layer used the softmax parameterization <ref> (Bridle, 1990) </ref>. Because a feedforward neural network does not represent correlations among output nodes, the joint probability on the output variables was approximated by the product of the marginals from the output units. <p> Since the function being maximized is a likelihood, the EM algorithm can also be used, as discussed in Section 7. 3. One can also use the softmax reparameterization <ref> (Bridle, 1990) </ref> which uses exp (fi) instead of fi 2 . 31 4. It is possible to compute the gradient for the likelihood of specified output variables, rather than for the likelihood of all the variables. This form of optimization is discussed by Spiegelhalter and Cowell (1992). 5.
Reference: <author> Buntine, W. L. </author> <year> (1994). </year> <title> Operations for learning with graphical models. </title> <journal> Journal of Artificial Intelligence Research 2, </journal> <pages> 159-225. </pages>
Reference: <author> Buntine, W. L. </author> <year> (1996). </year> <title> A guide to the literature on learning probabilistic networks from data. </title> <journal> IEEE Transactions on Knowledge and Data Engineering 8, </journal> <pages> 195-210. </pages>
Reference: <author> Cooper, G. and E. </author> <title> Herskovits (1992). A Bayesian method for the induction of probabilistic networks from data. </title> <booktitle> Machine Learning 9, </booktitle> <pages> 309-347. </pages>
Reference: <author> Daganzo, C. </author> <year> (1979). </year> <title> Multinomial probit: The theory and its application to demand forecasting. </title> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference-contexts: Again, gradients with respect to and can be computed easily. For example, we have @w (F; x) = N ; (x) The probit model can be extended to the multinomial case, where there are more than two discrete choices <ref> (Daganzo, 1979) </ref>. We can also include multiple parents. There is a large body of work on constructing and fitting complex probit models for a variety of applications.
Reference: <author> Dasgupta, S. </author> <year> (1997). </year> <title> The sample complexity of learning Bayesian nets. </title> <note> Machine Learning this issue. </note>
Reference: <author> Dean, T. and K. </author> <title> Kanazawa (1988). Probabilistic temporal reasoning. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence (AAAI-88), </booktitle> <address> St. Paul, </address> <publisher> Minnesota, </publisher> <pages> pp. 524-528. </pages> <booktitle> American Association for Artificial Intelligence. </booktitle>
Reference-contexts: As expected, the noisy-OR model results in faster learning than the corresponding model with explicitly represented CPTs. 6.3. Dynamic probabilistic networks One of the most important applications of Equation 5 is in learning the behavior of stochastic temporal processes. Such processes are typically represented using dynamic probabilistic networks (DPNs) <ref> (Dean and Kanazawa, 1988) </ref>. A DPN is structured as a sequence of time slices, where the nodes at each slice encode the state at the corresponding time point. Figure 8 shows the coarse structure of a generic DPN.
Reference: <author> Dempster, A., N. Laird, and D. </author> <title> Rubin (1977). Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society 39 (Series B), </journal> <pages> 1-38. </pages>
Reference-contexts: Thiesson (1995b) analyzes this class of models, which he calls recursive exponential models (REMs). DPN models can be viewed as a special case of REMs. For the specific case of maximizing likelihood by altering parameters of a probability distribution, the EM (Expectation Maximization) algorithm <ref> (Dempster, Laird, and Rubin, 1977) </ref> can be used. This observation was made by Lauritzen (1991, Lauritzen (1995)), who discussed its application to general probabilistic networks (see also Spiegelhalter, Dawid, Lauritzen, and Cowell, 1993; Olesen, Lauritzen, and Jensen, 1992; Spiegelhalter and Cowell, 1992; Heckerman, 1995).
Reference: <author> Finney, D. J. </author> <year> (1947). </year> <title> Probit analysis; a statistical treatment of the sigmoid response curve. </title> <publisher> Cambridge: Cambridge University Press. </publisher>
Reference: <author> Friedman, N., D. Geiger, and M. </author> <title> Goldszmidt (1997). Bayesian network classifiers. </title> <note> Machine Learning this issue. </note>
Reference: <author> Friedman, N. and M. </author> <title> Goldszmidt (1996). Learning bayesian networks with local structure. </title> <booktitle> In Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence (UAI-96), </booktitle> <address> Portland, Oregon. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Friedman, N. and M. </author> <month> Yakhini </month> <year> (1996). </year> <title> On the sample complexity of learning bayesian networks. </title> <note> Submitted to UAI '96. </note>
Reference: <author> Fung, R. and K. C. </author> <title> Chang (1989). Weighting and integrating evidence for stochastic simulation in Bayesian networks. </title> <booktitle> In Proceedings of the Fifth Conference on Uncertainty in Artificial Intelligence (UAI-89), </booktitle> <address> Windsor, Ontario. </address> <publisher> Morgan Kaufmann. 32 Geiger, </publisher> <editor> D., D. Heckerman, and C. </editor> <title> Meek (1996). Asymptotic model selection for directed networks with hidden variables. </title> <booktitle> In Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence (UAI-96), </booktitle> <address> Portland, Oregon. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Ghahramani, Z. and M. I. </author> <title> Jordan (1995). Factorial hidden Markov models. </title> <type> Technical Report 9502, </type> <institution> MIT Computational Cognitive Science Report. </institution>
Reference: <author> Golmard, J.-L. and A. </author> <month> Mallet </month> <year> (1991). </year> <title> Learning probabilities in causal trees from incomplete databases. </title> <journal> Revue d'Intelligence Artificielle 5, </journal> <pages> 93-106. </pages>
Reference: <author> Haddawy, P. </author> <year> (1994). </year> <title> Generating bayesian networks from probability logic knowledge bases. </title> <booktitle> In Proceedings of the Tenth Annual Conference on Uncertainty in Artificial Intelligence (UAI '94), </booktitle> <pages> pp. 262-269. </pages>
Reference: <author> Heckerman, D. </author> <year> (1995). </year> <title> A tutorial on learning with Bayesian networks. </title> <type> Technical Report MSR-TR-95-06, </type> <institution> Microsoft Research, </institution> <address> Redmond, Washington. </address> <month> Revised June </month> <year> 1996. </year>
Reference-contexts: Probabilistic networks have been shown to perform well in complex decision-making domains such as medical diagnosis, fault diagnosis, image analysis, natural language understanding, robot control, and real-time monitoring <ref> (Heckerman and Wellman, 1995 and accompanying articles) </ref>. While the compact and natural representation considerably facilitates the knowledge acquisition, the process of eliciting a probabilistic network from experts is still a slow one, largely due to the need to obtain numerical parameters. <p> EM, like gradient descent, can be used to find local maxima on the likelihood surface defined by the network parameters. EM can also be used in the context of maximum a posteriori (MAP) learning, where we have a prior over the set of parameters <ref> (Heckerman, 1995) </ref>. Thiesson (1995a) shows that a similar analysis can be used to do MAP learning with gradient-based methods. It can be shown that EM converges faster than simple gradient ascent, but the comparison between EM and conjugate gradient remains unclear.
Reference: <author> Heckerman, D., D. Geiger, and M. </author> <title> Chickering (1994). Learning Bayesian networks: The combination of knowledge and statistical data. </title> <type> Technical Report MSR-TR-94-09, </type> <institution> Microsoft Research, </institution> <address> Redmond, Washington. </address>
Reference: <author> Heckerman, D. and M. Wellman (1995, </author> <month> March). </month> <title> Bayesian networks. </title> <booktitle> Communications of the Association for Computing Machinery 38 (3), </booktitle> <pages> 27-30. </pages>
Reference-contexts: Probabilistic networks have been shown to perform well in complex decision-making domains such as medical diagnosis, fault diagnosis, image analysis, natural language understanding, robot control, and real-time monitoring <ref> (Heckerman and Wellman, 1995 and accompanying articles) </ref>. While the compact and natural representation considerably facilitates the knowledge acquisition, the process of eliciting a probabilistic network from experts is still a slow one, largely due to the need to obtain numerical parameters. <p> EM, like gradient descent, can be used to find local maxima on the likelihood surface defined by the network parameters. EM can also be used in the context of maximum a posteriori (MAP) learning, where we have a prior over the set of parameters <ref> (Heckerman, 1995) </ref>. Thiesson (1995a) shows that a similar analysis can be used to do MAP learning with gradient-based methods. It can be shown that EM converges faster than simple gradient ascent, but the comparison between EM and conjugate gradient remains unclear.
Reference: <author> Koller, D. and A. </author> <title> Pfeffer (1996). Learning the parameters of first order probabilistic rules. </title> <booktitle> In Working Notes of the AAAI Fall Symposium on Learning Complex Behaviors in Adaptive Intelligent Systems, </booktitle> <address> Stanford, California. </address>
Reference: <author> Kwoh, C.-K. and D. F. </author> <title> Gillies (1996). Using hidden nodes in Bayesian networks. </title> <booktitle> Artificial Intelligence 88 (1-2), </booktitle> <pages> 1-38. </pages>
Reference: <author> Laskey, K. B. </author> <year> (1990). </year> <title> Adapting connectionist learning to Bayes networks. </title> <journal> International Journal of Approximate Reasoning 4, </journal> <pages> 261-282. </pages>
Reference: <author> Lauritzen, S. L. </author> <year> (1991). </year> <title> The EM algorithm for graphical association models with missing data. </title> <type> Technical Report TR-91-05, </type> <institution> Department of Statistics, Aalborg University. </institution>
Reference: <author> Lauritzen, S. L. </author> <year> (1995). </year> <title> The EM algorithm for graphical association models with missing data. </title> <journal> Computational Statistics and Data Analysis 19, </journal> <pages> 191-201. </pages>
Reference: <author> Lauritzen, S. L. and D. J. </author> <month> Spiegelhalter </month> <year> (1988). </year> <title> Local computations with probabilities on graphical structures and their application to expert systems. </title> <journal> Journal of the Royal Statistical Society B 50 (2), </journal> <pages> 157-224. </pages>
Reference-contexts: However, a number of algorithms have been developed that take advantage of network structure to perform the inference process effectively, often allowing the solution of large networks in practice. The most widely used exact inference algorithms are clustering algorithhms <ref> (Lauritzen and Spiegelhalter, 1988) </ref>, which modify the network topology, transforming it into a Markov random field. Stochastic simulation algorithms have also been developed (e.g., Shachter and Peot, 1989; Fung and Chang, 1989), allowing for an anytime approximation of the solution. <p> We are therefore able to use standard probabilistic network packages as a substrate for our learning system. For example, clustering algorithms <ref> (Lauritzen and Spiegelhalter, 1988) </ref> compute a posterior for each clique in the Markov network corresponding to the original probabilistic network; since a node and its parents always appear together in at least one clique, the required probabilities can be found by summing out the other variables in that clique.
Reference: <author> Lauritzen, S. L. and N. </author> <title> Wermuth (1989). Graphical models for associations between variables, some of which are qualitative and some quantitative. </title> <journal> Annals of Statistics 17, </journal> <pages> 31-57. </pages>
Reference-contexts: For example, as we mentioned above, probabilistic networks can also contain continuous-valued nodes. The "CPT" for such nodes must be parametrically defined, for example as a Gaussian distribution with parameters for the mean and the variance <ref> (Lauritzen and Wermuth, 1989) </ref>. Dynamic probabilistic networks also require a parametrized representation. These are potentially infinite networks that represent temporal processes. The same parameters, e.g., those encoding the stochastic model of state evolution, appear many times in the network. <p> The CPT is then defined by the parameters of the linear function and the variance. 6 In a conditional Gaussian (CG) distribution <ref> (Lauritzen and Wermuth, 1989) </ref>, which describes the case where a continuous node has both discrete and continuous parents, one set of parameters is provided for each possible instantiation of the discrete parents. The full parameterized description is rather complicated, so we begin by illustrating the idea with a simple example.
Reference: <author> MacKay, D. J. C. </author> <year> (1992). </year> <title> A practical Bayesian framework for back-propagation networks. </title> <booktitle> Neural Computation 4 (3), </booktitle> <pages> 448-472. </pages>
Reference: <author> Neal, R. M. </author> <year> (1992a). </year> <title> Asymmetric parallel boltzmann machines are belief networks. </title> <booktitle> Neural Computation 4 (6), </booktitle> <pages> 832-834. </pages>
Reference: <author> Neal, R. M. </author> <year> (1992b). </year> <title> Connectionist learning of belief networks. </title> <booktitle> Artificial Intelligence 56, </booktitle> <pages> 71-113. </pages>
Reference: <author> Neal, R. M. and G. E. </author> <title> Hinton (1993). A new view of the EM algorithm that justifies incremental and other variants. </title> <type> unpublished manuscript. </type>
Reference-contexts: However, for more complex problems with generalized parameters, the M-step may itself require an iterative optimization algorithm. Dempster et al. (1977) discuss generalized EM (GEM) algorithms that execute only a partial M-step; as long as this step increases the likelihood, the algorithm will converge <ref> (Neal and Hinton, 1993) </ref>. Thus, a gradient-based approach is closely related to a GEM algorithm. There has also been some work on the more complex problem of learning with hidden variables when the structural properties of the network are not all known.
Reference: <author> Ngo, L., P. Haddawy, and J. </author> <title> Helwig (1995). A theoretical framework for context-sensitive temporal probability model construction with application to plan projection. </title> <booktitle> In Proceedings of the Eleventh Annual Conference on Uncertainty in Artificial Intelligence (UAI '95), </booktitle> <pages> pp. 419-426. </pages>
Reference: <author> Olesen, K. G., S. L. Lauritzen, and F. V. </author> <title> Jensen (1992). aHUGIN: A system for creating adaptive causal probabilistic networks. </title> <booktitle> In Proceedings of the Eighth Conference on Uncertainty in Artificial Intelligence (UAI-92), </booktitle> <address> Stanford, California. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For example, given observations and test results, one can compute the probability that the patient has lung cancer. Unfortunately, an explicit description of the joint distribution requires a number of parameters that is exponential in n, the number of variables. Probabilistic networks <ref> (Pearl, 1988) </ref> derive their power from the ability to represent conditional independences among variables, which allows them to take advantage of the "locality" of causal influences. Intuitively, a variable is independent of its indirect causal influences given its direct causal influences. <p> More precisely, H (P fl ; P w ) = e;y To compute the joint probability of the output variables using a standard probabilistic network inference algorithm, we can decompose it into marginals using the chain rule <ref> (Pearl, 1988, p.226) </ref>: P w (y j e) = i 5.2.2. Methodology and comparison algorithm Data was generated randomly from a target probabilistic network and then partitioned into training and test sets. The training data was used to train probabilistic networks that were initialized with random parameter values.
Reference: <author> Poggio, T. and F. </author> <title> Girosi (1990). Regularization algorithms for learning that are equivalent to multilayer networks. </title> <booktitle> Science 247, </booktitle> <pages> 978-982. </pages>
Reference-contexts: It is straightforward to extend the methods described below to solve this problem, as shown by Thiesson (1995a). This extension can also be performed for feedforward neural networks, resulting in the regularization method <ref> (Poggio and Girosi, 1990) </ref>. Finally, it is possible to perform a full Bayesian analysis, which uses the posterior distribution over the weights, P (w j D), to make predictions for new cases by integrating over the predictions made for each possible weight setting.
Reference: <author> Pradhan, M., G. M. Provan, B. Middleton, and M. </author> <month> Henrion </month> <year> (1994). </year> <title> Knowledge engineering for large belief networks. </title> <booktitle> In Proceedings of Uncertainty in Artificial Intelligence, </booktitle> <address> Seattle, Washing-ton. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In very large networks, however, this reduction may not be enough. For example, the CPCS network <ref> (Pradhan, Provan, Middleton, and Henrion, 1994) </ref> would require 133,931,430 parameters if defined using explicit conditional probability tables. Instead, CPCS uses parametric descriptions of the conditional distributions in the network, such as noisy-OR and noisy-MAX, thereby reducing the network to only 8,254 parameters.
Reference: <author> Price, W. H. </author> <year> (1992). </year> <title> Numerical Recipes in C. </title> <publisher> Cambridge: Cambridge University Press. 33 Russell, </publisher> <editor> S. J. and B. </editor> <month> Grosof </month> <year> (1987). </year> <title> A declarative approach to bias in concept learning. </title> <booktitle> In Proceedings of the Sixth National Conference on Artificial Intelligence (AAAI-87), </booktitle> <address> Seattle, Washington. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: As discussed above, the algorithm can (and, in many cases, should) be extended to incorporate nonuniform priors over parameters, and to take advantage of more sophisticated methods for function optimization. In the experiments described below, we have adapted the conjugate gradient algorithm <ref> (Price, 1992) </ref> to keep the probabilistic variables in the legal [0,1] range as described above. This algorithm uses repeated line minimizations (with direction chosen by the Polak-Ribiere method) and a heuristic termination condition to signal a maximum.
Reference: <author> Shachter, R. D. and M. A. </author> <title> Peot (1989). Simulation approaches to general probabilistic inference on belief networks. </title> <booktitle> In Proceedings of the Fifth Conference on Uncertainty in Artificial Intelligence (UAI-89), </booktitle> <address> Windsor, Ontario. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Shachter, R. S. and C. R. </author> <month> Kenley </month> <year> (1989). </year> <title> Gaussian influence diagrams. </title> <booktitle> Management Science 35 (5), </booktitle> <pages> 527-550. </pages>
Reference: <author> Smyth, P., D. Heckerman, and M. </author> <title> Jordan (1996). Probabilistic independence networks for hidden markov probability models. </title> <type> Technical Report MSR-TR-96-03, </type> <institution> Microsoft Research, </institution> <address> Redmond, Washington. </address>
Reference: <author> Spiegelhalter, D., P. Dawid, S. Lauritzen, and R. </author> <title> Cowell (1993). Bayesian analysis in expert systems. </title> <booktitle> Statistical Science 8, </booktitle> <pages> 219-282. </pages>
Reference: <author> Spiegelhalter, D. J. and R. G. </author> <title> Cowell (1992). Learning in probabilistic expert systems. </title> <note> In J. M. </note>
Reference: <author> Bernardo, J. O. Berger, A. P. Dawid, and A. F. M. Smith (Eds.), </author> <title> Bayesian Statistics 4, </title> <publisher> Oxford. Oxford University Press. </publisher>
Reference: <author> Spirtes, P., C. Glymour, and R. </author> <month> Scheines </month> <year> (1993). </year> <title> Causation, prediction, and search. </title> <publisher> Berlin: Springer-Verlag. </publisher>
Reference: <author> Tadepalli, P. </author> <year> (1993). </year> <title> Learning from queries and examples with tree-structured bias. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <address> Amherst, Massachusetts. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Thiesson, B. </author> <year> (1995a). </year> <title> Accelerated quantification of bayesian networks with incomplete data. </title> <booktitle> In Proceedings of the First International Conference on Knowledge Discovery and Data Mining (KDD-95), </booktitle> <address> Montreal, Canada, </address> <pages> pp. 306-311. </pages> <publisher> AAAI Press. </publisher>
Reference: <author> Thiesson, B. </author> <year> (1995b). </year> <title> Score and information for recursive exponential models with incomplete data. </title> <type> Technical Report R-95-2020, </type> <institution> Institute for Electronic Systems, Aalborg University, Den-mark. </institution>
Reference: <author> Titterington, D., A. Smith, and U. </author> <month> Makov </month> <year> (1985). </year> <title> Statistical analysis of finite mixture distributions. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: There are several reasons for this. First, it is not necessarily the case that any particular variable is hidden in all the observed cases (although we do not rule this out). Second, the hidden variable might be one which we are interested in querying, as in learning mixture models <ref> (Titterington, Smith, and Makov, 1985) </ref>. Finally, networks with hidden variables can be more compact than the corresponding fully observable network (see Figure 2). <p> Notes 1. In this case, knowledge of structure is taken to include knowledge of the number of values of each variable. In other settings, particularly in learning mixture models <ref> (Titterington, Smith, and Makov, 1985) </ref>, finding the number of values of a class variable may be part of the learning task. 2. Since the function being maximized is a likelihood, the EM algorithm can also be used, as discussed in Section 7. 3.
Reference: <author> Towell, G. and J. </author> <title> Shavlik (1994). </title> <booktitle> Knowledge-based artificial neural networks. Artificial Intelligence 70, </booktitle> <pages> 119-165. </pages>
Reference: <author> Wellman, M. P. </author> <year> (1990). </year> <title> Fundamental concepts of qualitative probabilistic networks. </title> <booktitle> Artificial Intelligence 44 (3), </booktitle> <pages> 257-303. </pages>
Reference-contexts: These results are currently being extended to cover DPN models, continuous variables, and so on. Another possible improvement over the basic APN model is to allow the user or domain expert to prespecify constraints on the conditional distributions. One possibility might be to specify a qualitative probabilistic network <ref> (Wellman, 1990) </ref> that provides appropriate monotonicity constraints on the conditional distributions. For example, the higher one's driving skill, the less likely it is that one has had an accident.
Reference: <author> Werbos, P. J. </author> <year> (1990). </year> <title> Backpropagation through time: What it does and how to do it. </title> <booktitle> Proceedings of the IEEE 78 (10), </booktitle> <pages> 1550-1560. </pages>
Reference: <author> Zweig, G. </author> <year> (1996). </year> <title> Methods for learning dynamic probabilistic networks and a comparison with hidden Markov models. </title> <type> MS report, </type> <institution> Computer Science Division, UC Berkeley. </institution> <note> Received Date Accepted Date Final Manuscript Date </note>
References-found: 55

