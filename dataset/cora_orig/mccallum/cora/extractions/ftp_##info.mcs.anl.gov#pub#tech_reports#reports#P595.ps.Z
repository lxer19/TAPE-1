URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P595.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts96.htm
Root-URL: http://www.mcs.anl.gov
Email: ffoster,geisler,tueckeg@mcs.anl.gov  
Title: MPI on the I-WAY: A Wide-Area, Multimethod Implementation of the Message Passing Interface  
Author: Ian Foster, Jonathan Geisler, Steven Tuecke 
Address: Argonne, IL 60439, U.S.A.  
Affiliation: Argonne National Laboratory  
Abstract: High-speed wide-area networks enable innovative applications that integrate geographically distributed computing, database, graphics, and networking resources. The Message Passing Interface (MPI) can be used as a portable, high-performance programming model for such systems. However, the wide-area environment introduces challenging problems for the MPI implementor, because of the heterogeneity of both the underlying physical infrastructure and the authentication and software environment at different sites. In this article, we describe an MPI implementation that incorporates solutions to these problems. This implementation, which was developed for the I-WAY distributed-computing experiment, was constructed by layering MPICH on the Nexus multithreaded runtime system. Nexus provides automatic configuration mechanisms that can be used to select and configure authentication, process creation, and communication mechanisms in heterogeneous systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. M. Chandy and C. Kesselman. </author> <title> CC ++ : A declarative concurrent object oriented programming notation. In Research Directions in Object Oriented Programming. </title> <publisher> The MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: Nexus provides a low-level interface to multithreading and communication mechanisms in homogeneous and heterogeneous systems. It is designed for use by library writers and compiler writers; in addition to MPI, systems that use Nexus facilities include parallel object-oriented languages (for example, CC++ <ref> [1] </ref> and Fortran M [5]), parallel scripting languages (nPerl [10]), and communication libraries (CAVEcomm [4] and a Java library). 3.1. Nexus overview Nexus is structured in terms of five basic abstractions: nodes, contexts, threads, global pointers, and remote service requests. <p> Other systems used Nexus mechanisms in the same manner, notably the parallel language CC++ <ref> [1] </ref> and the parallel scripting language nPerl [10], used to write the I-WAY sched-uler. A significant difficulty revealed by the I-WAY experiment related to the mechanisms used to generate and maintain the configuration information used by Nexus.
Reference: [2] <author> C. Cruz-Neira, D. Sandin, T. DeFanti, R. Kenyon, and J. Hart. </author> <title> The CAVE: Audio visual experience automatic virtual environment. </title> <journal> Communications of the ACM, </journal> <volume> 35(6) </volume> <pages> 65-72, </pages> <year> 1992. </year>
Reference-contexts: When demonstrated at the Supercomputing conference in San Diego in Decem-ber 1995, the I-WAY network connected multiple high-end display devices (including immersive CAVE TM and ImmersaDesk TM virtual reality devices <ref> [2] </ref>); mass storage systems; specialized instruments (such as microscopes and satellite downlinks); and supercomputers of different architectures, including distributed-memory multicomputers (IBM SP, Intel Paragon, Cray T3D, etc.), shared-memory multiprocessors (SGI Challenge, Convex Exemplar), and vector multiprocessors (Cray C90, Y-MP). These devices were located at seventeen different sites across North America.
Reference: [3] <author> T. DeFanti, I. Foster, M. Papka, R. Stevens, and T. Kuhfuss. </author> <title> Overview of the I-WAY: Wide area visual 7 supercomputing. </title> <journal> International Journal of Supercom--puter Applications, </journal> <note> 1996. in press. </note>
Reference-contexts: 1. Introduction The I-WAY networking experiment <ref> [3] </ref> provided the largest testbed developed to date for high-performance distributed computing. Over sixty groups used this testbed to develop applications that connected supercomputers, advanced display devices, storage systems, and/or scientific instruments located across North America. <p> Then, we introduce Nexus and the techniques that it uses to support multimethod communication. Subsequent sections describe the Nexus implementation of MPI and the techniques used to support automatic configuration of MPI computations on the I-WAY. 2. The I-WAY Experiment The I-WAY, or Information Wide Area Year <ref> [3] </ref>, was a wide-area computing experiment conducted throughout 1995 with the goal of providing a large-scale testbed in which innovative high-performance and geographically distributed applications could be deployed.
Reference: [4] <author> T. L. Disz, M. E. Papka, M. Pellegrino, and R. Stevens. </author> <title> Sharing visualization experiences among remote virtual environments. </title> <booktitle> In International Workshop on High Performance Computing for Computer Graphics and Visualization, </booktitle> <pages> pages 217-237. </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: It is designed for use by library writers and compiler writers; in addition to MPI, systems that use Nexus facilities include parallel object-oriented languages (for example, CC++ [1] and Fortran M [5]), parallel scripting languages (nPerl [10]), and communication libraries (CAVEcomm <ref> [4] </ref> and a Java library). 3.1. Nexus overview Nexus is structured in terms of five basic abstractions: nodes, contexts, threads, global pointers, and remote service requests. <p> For example, a key TCP BUFFER SIZE might be used to specify the buffer size to be used on a particular communicator. A second benefit that accrues from the Nexus implementation of MPI is interoperability with other Nexus-based tools. For example, on the I-WAY, numerous applications used the CAVEcomm <ref> [4] </ref> client-server package to transfer data among one or more virtual reality systems and a scientific simulation running on a supercomputer. When the simulation itself was developed with MPI, the need arose to integrate the polling required to detect communication from either source. This integration is supported within Nexus.
Reference: [5] <author> I. Foster and K. M. Chandy. </author> <title> Fortran M: A language for modular parallel programming. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 25(1), </volume> <year> 1994. </year>
Reference-contexts: Nexus provides a low-level interface to multithreading and communication mechanisms in homogeneous and heterogeneous systems. It is designed for use by library writers and compiler writers; in addition to MPI, systems that use Nexus facilities include parallel object-oriented languages (for example, CC++ [1] and Fortran M <ref> [5] </ref>), parallel scripting languages (nPerl [10]), and communication libraries (CAVEcomm [4] and a Java library). 3.1. Nexus overview Nexus is structured in terms of five basic abstractions: nodes, contexts, threads, global pointers, and remote service requests.
Reference: [6] <author> I. Foster, J. Geisler, C. Kesselman, and S. Tuecke. </author> <title> Multimethod communication for high-performance metacomputing applications. </title> <type> Preprint, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <year> 1996. </year>
Reference-contexts: Multimethod communication From the point of view of the I-WAY, the Nexus features that are most interesting are those that support multimethod communication <ref> [6] </ref>. These mechanisms are based around the global pointer construct, which is used to maintain information about the methods that can be used to perform communications directed to a particular remote location. <p> This sort of cost differential allows an infrequently used, expensive method to impose significant overhead on a frequently used, inexpensive method. These overheads can be reduced by using optimizations that, for example, perform TCP polls less frequently <ref> [6] </ref>. The results presented in this section are for a non-threaded implementation of Nexus.
Reference: [7] <author> I. Foster, J. Geisler, W. Nickless, W. Smith, and S. Tuecke. </author> <title> Software infrastructure for the I-WAY high-performance distributed computing experiment. </title> <booktitle> In Proc. 5th IEEE Symp. on High Performance Distributed Computing. IEEE, </booktitle> <year> 1996. </year>
Reference-contexts: The need to configure both IP routing tables and ATM virtual circuits in this highly heterogeneous environment was a significant source of implementation complexity. An innovative aspect of the I-WAY project was the development of a system management and application programming environment called I-Soft <ref> [7] </ref> that provided uniform authentication, resource reservation, process creation, and communication functions across I-WAY resources. A novel aspect of this approach was the deployment of a dedicated I-WAY Point of Presence, or I-POP, machine at each participating site. <p> The I-WAY scheduler was configured so that, when scheduling resources to users, it would also generate database entries describing the resources and the network configuration <ref> [7] </ref>. Nexus (and hence MPI) could then use this information when creating a user computation.
Reference: [8] <author> I. Foster, C. Kesselman, and M. Snir. </author> <title> Generalized communicators in the Message Passing Interface. </title> <booktitle> In Proceedings of the 1996 MPI Developers Conference. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1996. </year>
Reference-contexts: The multithreaded MPI also has its limitations. In particular, it is not possible to define a collective operation that involves more than one thread per process. This functionality requires extensions to the MPI model <ref> [8, 15, 18] </ref>. Finally, we note that Nexus support for dynamic resource management and multithreading also provides a framework for implementing new features proposed for MPI-2, such as dynamic process management, single-sided communication, and multicast. 5.
Reference: [9] <author> I. Foster, C. Kesselman, and S. Tuecke. </author> <title> The Nexus approach to integrating multithreading and communication. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 1996. To appear. </note>
Reference-contexts: Second, it requires access to correct, up-to-date information about the software and hardware environment at different sites. We addressed these two challenges by layering the Argonne/Mississippi State MPICH library [13] on top of a runtime library called Nexus <ref> [9] </ref>. MPICH provides a portable, high-performance implementation of MPI that incorporates some support for heterogeneous environments, but that in its current instantiation is designed primarily for homogeneous massively parallel processing (MPP) systems. Nexus is a portable, mul-tithreaded communication library that we have constructed to support wide-area, heterogeneous computations. <p> TCP communications incur the high latencies typically observed in other environments, and so multiple SP partitions can be used to provide a controlled testbed for experimentation with multimethod communication in networked systems. Nexus performance experiments, reported elsewhere <ref> [9] </ref>, reveal that on the Argonne SP2, a "ping-pong" benchmark that performs RSRs back and forth between two processors obtains a one-way cost of 82.8 sec for a zero-length message; in contrast, the SP2's low-level MPL communication library takes 61.4 sec. <p> The principal sources of the 21.4 sec difference between NexusLite and MPL are the setup and communication of the 32-byte header contained in a Nexus message (about 8 sec) and the lookup and dispatch of the handler on the receive side (about 7 sec) <ref> [9] </ref>. We evaluated the performance of the Nexus implementation of MPI by using the ping-pong benchmark provided by the MPI mpptest program [13]. <p> Various approaches to the integration of multithreading into a message-passing framework have been proposed (see <ref> [9] </ref> for a discussion). The Nexus implementation of MPI supports a particularly simple and elegant model that does not require that explicit thread identifiers be exported from MPI processes.
Reference: [10] <author> I. Foster and R. Olson. </author> <title> A guide to parallel and distributed programming in nPerl. </title> <type> Technical report, </type> <institution> Argonne National Laboratory, </institution> <year> 1995. </year> <note> http://www.mcs.anl.gov/nexus/nperl/. </note>
Reference-contexts: It is designed for use by library writers and compiler writers; in addition to MPI, systems that use Nexus facilities include parallel object-oriented languages (for example, CC++ [1] and Fortran M [5]), parallel scripting languages (nPerl <ref> [10] </ref>), and communication libraries (CAVEcomm [4] and a Java library). 3.1. Nexus overview Nexus is structured in terms of five basic abstractions: nodes, contexts, threads, global pointers, and remote service requests. <p> Other systems used Nexus mechanisms in the same manner, notably the parallel language CC++ [1] and the parallel scripting language nPerl <ref> [10] </ref>, used to write the I-WAY sched-uler. A significant difficulty revealed by the I-WAY experiment related to the mechanisms used to generate and maintain the configuration information used by Nexus.
Reference: [11] <author> W. Gropp and E. Lusk. </author> <title> An abstract device definition to support the implementation of a high-level point-to-point message-passing interface. </title> <type> Preprint MCS-P342-1193, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <year> 1994. </year>
Reference-contexts: A D I other devices N e x u s multiple communication methods constructed by defining a Nexus instantiation of the MPICH channel device, a specialization of the abstract device interface. abstract device interface (ADI) that defines low-level communication-related functions that can be implemented in different ways on different machines <ref> [11, 12] </ref>. The Nexus implementation of MPI is constructed by providing a Nexus implementation of this device. The use of the ADI simplifies implementation but has some performance implications, which we discuss below. 4.2.
Reference: [12] <author> W. Gropp and E. Lusk. </author> <title> MPICH working note: Creating a new MPICH device using the channel interface. </title> <type> Technical Report ANL/MCS-TM-213, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <year> 1995. </year>
Reference-contexts: A D I other devices N e x u s multiple communication methods constructed by defining a Nexus instantiation of the MPICH channel device, a specialization of the abstract device interface. abstract device interface (ADI) that defines low-level communication-related functions that can be implemented in different ways on different machines <ref> [11, 12] </ref>. The Nexus implementation of MPI is constructed by providing a Nexus implementation of this device. The use of the ADI simplifies implementation but has some performance implications, which we discuss below. 4.2.
Reference: [13] <author> W. Gropp, E. Lusk, N. Doss, and A. Skjellum. </author> <title> A high-performance, portable implementation of the MPI message passing interface standard. </title> <type> Technical Report ANL/MCS-TM-213, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Ar-gonne, Ill., </institution> <year> 1996. </year>
Reference-contexts: Second, it requires access to correct, up-to-date information about the software and hardware environment at different sites. We addressed these two challenges by layering the Argonne/Mississippi State MPICH library <ref> [13] </ref> on top of a runtime library called Nexus [9]. MPICH provides a portable, high-performance implementation of MPI that incorporates some support for heterogeneous environments, but that in its current instantiation is designed primarily for homogeneous massively parallel processing (MPP) systems. <p> The communicator construct combines a group of processes and a unique tag space and can be used to ensure that communications associated with different parts of a program are not confused. MPICH <ref> [13] </ref> is a portable, high-performance implementation of MPI. <p> A typical implementation of the ADI will map some functions directly to low-level mechanisms and implement others via library calls. The mapping of MPICH functions to ADI mechanisms is achieved via macros and preprocessors, not function calls. Hence, the overhead associated with this organization is often small or nonexistent <ref> [13] </ref>. The ADI provides a fairly high-level abstraction of a communication device: for example, it assumes that the device handles the buffering and queuing of messages. The lower-level channel interface defines simpler functions for moving data from one processor to another. <p> We evaluated the performance of the Nexus implementation of MPI by using the ping-pong benchmark provided by the MPI mpptest program <ref> [13] </ref>. We executed this program using both "native" MPICH and the Nexus implementation of MPI, in the later case comparing performance both with MPL support only and with MPL and TCP support. Figure 3 shows our results.
Reference: [14] <author> W. Gropp, E. Lusk, and A. Skjellum. </author> <title> Using MPI: Portable Parallel Programming with the Message Passing Interface. </title> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: MPI and MPICH We first review important features of MPI and of the MPICH implementation on which this work is based. The Message Passing Interface defines a standard set of functions for interprocess communication <ref> [14] </ref>. It defines functions for sending messages from one process to another (point-to-point communication), for communication operations that involve groups of processes (collective communication, such as reduction), and for obtaining information about the environment in which a program executes (enquiry functions).
Reference: [15] <author> M. Haines, P. Mehrotra, and D. Cronk. Ropes: </author> <title> Support for collective operations among distributed threads. </title> <type> Technical Report 95-36, </type> <institution> Institute for Computer Application in Science and Engineering, </institution> <year> 1995. </year>
Reference-contexts: The multithreaded MPI also has its limitations. In particular, it is not possible to define a collective operation that involves more than one thread per process. This functionality requires extensions to the MPI model <ref> [8, 15, 18] </ref>. Finally, we note that Nexus support for dynamic resource management and multithreading also provides a framework for implementing new features proposed for MPI-2, such as dynamic process management, single-sided communication, and multicast. 5.
Reference: [16] <author> C. Lee, C. Kesselman, and S. Schwab. </author> <title> Near-real-time satellite image processing: Metacomputing in CC++. </title> <journal> Computer Graphics and Applications, </journal> <note> 1996. to appear. </note>
Reference-contexts: These devices were located at seventeen different sites across North America. The I-WAY distributed supercomputing environment was used by over sixty application groups for experiments in high-performance computing (e.g., [17]), collaborative design, and the coupling of remote supercomputers and databases into local environments (e.g., <ref> [16] </ref>). A primary thrust was applications that use multiple supercomputers and virtual reality devices to explore collaborative technologies in which shared virtual spaces are used to perform computational science.
Reference: [17] <author> M. Norman et al. </author> <title> Galaxies collide on the I-WAY: An example of heterogeneous wide-area collaborative supercomputing. </title> <journal> International Journal of Supercomputer Applications, </journal> <note> 1996. in press. </note>
Reference-contexts: These devices were located at seventeen different sites across North America. The I-WAY distributed supercomputing environment was used by over sixty application groups for experiments in high-performance computing (e.g., <ref> [17] </ref>), collaborative design, and the coupling of remote supercomputers and databases into local environments (e.g., [16]). A primary thrust was applications that use multiple supercomputers and virtual reality devices to explore collaborative technologies in which shared virtual spaces are used to perform computational science.
Reference: [18] <author> A. Skjellum, N. Doss, K. Viswanathan, A. Chow-dappa, and P. </author> <title> Bangalore. Extending the message passing interface. </title> <booktitle> In Proc. 1994 Scalable Parallel Libraries Conf. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year> <month> 8 </month>
Reference-contexts: The multithreaded MPI also has its limitations. In particular, it is not possible to define a collective operation that involves more than one thread per process. This functionality requires extensions to the MPI model <ref> [8, 15, 18] </ref>. Finally, we note that Nexus support for dynamic resource management and multithreading also provides a framework for implementing new features proposed for MPI-2, such as dynamic process management, single-sided communication, and multicast. 5.
References-found: 18

