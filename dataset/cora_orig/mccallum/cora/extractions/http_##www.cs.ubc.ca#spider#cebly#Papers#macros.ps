URL: http://www.cs.ubc.ca/spider/cebly/Papers/macros.ps
Refering-URL: http://www.cs.ubc.ca/spider/cebly/papers.html
Root-URL: 
Email: tldg@cs.brown.edu  cebly@cs.ubc.ca  
Title: Hierarchical Solution of Markov Decision Processes using Macro-actions  
Author: Milos Hauskrecht, Nicolas Meuleau Leslie Pack Kaelbling, Thomas Dean Craig Boutilier 
Date: 1910  
Address: Box  Providence, RI 02912 fmilos, nm, lpk,  Vancouver, BC V6T 1Z4, Canada  
Affiliation: Computer Science Department,  Brown University,  Department of Computer Science University of British Columbia  
Abstract: We investigate the use of temporally abstract actions, or macro-actions, in the solution of Markov decision processes. Unlike current models that combine both primitive actions and macro-actions and leave the state space unchanged, we propose a hierarchical model (using an abstract MDP) that works with macro-actions only, and that significantly reduces the size of the state space. This is achieved by treating macro-actions as local policies that act in certain regions of state space, and by restricting states in the abstract MDP to those at the boundaries of regions. The abstract MDP approximates the original and can be solved more efficiently. We discuss several ways in which macro-actions can be generated to ensure good solution quality. Finally, we consider ways in which macro-actions can be reused to solve multiple, related MDPs; and we show that this can justify the computational over head of macro-action generation.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. E. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, Princeton, </publisher> <year> 1957. </year>
Reference-contexts: 2 A Hierarchical Model of Macro-actions 2.1 Markov Decision Processes A (finite) Markov decision process is a tuple hS; A; T; Ri where: S is a finite set of states; A is a finite set of actions; T is a transition distribution T : S fi A fi S ! <ref> [0; 1] </ref>, such that T (s; a; ) is a probability distribution over S for any s 2 S and a 2 A; and R : S fi A ! IR is a bounded reward function. <p> The optimal value function V fl is the value function for any optimal policy. A number of techniques for constructing optimal policies exist. An especially simple algorithm is value iteration <ref> [1] </ref>. We produce a sequence of value functions V n by starting from an arbitrary V 0 , and defining V i+1 (s) = max fR (s; a) + fi t2S The sequence of functions V i converges to V fl in the limit. <p> They propose the following method of modeling macros. Definition 3 A discounted transition model T i (; i ; ) for macro i (defined on region S i ) is a mapping T i : S i fi XPer (S i ) ! <ref> [0; 1] </ref> such that T i (s; i ; s 0 ) = E t (fi t1 Pr (s t = s 0 j s 0 = s; i )); 1 X fi t1 Pr t = t; s t = s 0 j s 0 = s; i where the
Reference: [2] <author> D. P. Bertsekas and J.. N. Tsitsiklis. </author> <title> Neuro-dynamic Programming. </title> <publisher> Athena, </publisher> <year> 1996. </year>
Reference-contexts: Considerable research has been directed toward the solution of Markov decision processes (MDPs) with large state and action spaces. These include function approximation <ref> [2] </ref>, reach-ability analyses [5] and aggregation techniques [7, 3, 4]. Despite these advances, little attention has been paid to the reuse of policies or value functions generated for one MDP in the solution of a related MDP. <p> The resulting value function is used as a seed to generate a new set of macros (again one per region), and the new abstract MDP is solved. This iterative macro-refinement method is a special case of asynchronous policy iteration <ref> [2] </ref> and is similar to Dantzig-Wolfe (D-W) decom position techniques [6, 14]. D-W techniques can be viewed as iterative schemes for evaluating and modifying macro sets generated by assigning values to peripheral states.
Reference: [3] <author> C. Boutilier, R. Dearden, and M. Goldszmidt. </author> <title> Exploiting structure in policy construction. </title> <booktitle> IJCAI-95, </booktitle> <address> pp.11041111, Montreal, </address> <year> 1995. </year>
Reference-contexts: Considerable research has been directed toward the solution of Markov decision processes (MDPs) with large state and action spaces. These include function approximation [2], reach-ability analyses [5] and aggregation techniques <ref> [7, 3, 4] </ref>. Despite these advances, little attention has been paid to the reuse of policies or value functions generated for one MDP in the solution of a related MDP.
Reference: [4] <author> T. Dean and R. Givan. </author> <title> Model minimization in Markov decision processes. </title> <address> AAAI-97, pp.106111, Providence, </address> <year> 1997. </year>
Reference-contexts: Considerable research has been directed toward the solution of Markov decision processes (MDPs) with large state and action spaces. These include function approximation [2], reach-ability analyses [5] and aggregation techniques <ref> [7, 3, 4] </ref>. Despite these advances, little attention has been paid to the reuse of policies or value functions generated for one MDP in the solution of a related MDP.
Reference: [5] <author> T. Dean, L. P. Kaelbling, J. Kirman, and A. Nicholson. </author> <title> Planning under time constraints in stochastic domains. </title> <address> Artif. In-tell., 76:3574, </address> <year> 1995. </year>
Reference-contexts: Considerable research has been directed toward the solution of Markov decision processes (MDPs) with large state and action spaces. These include function approximation [2], reach-ability analyses <ref> [5] </ref> and aggregation techniques [7, 3, 4]. Despite these advances, little attention has been paid to the reuse of policies or value functions generated for one MDP in the solution of a related MDP.
Reference: [6] <author> T. Dean and S.-H. Lin. </author> <title> Decomposition techniques for planning in stochastic domains. </title> <booktitle> IJCAI-95, </booktitle> <address> pp.11211127, Mon-treal, </address> <year> 1995. </year>
Reference-contexts: Formally, our model relies on a region-based decomposition of a given MDP hS; A; T; Ri as defined by Dean and Lin <ref> [6] </ref>. Definition 1 A region-based decomposition of an MDP M = hS; A; T; Ri is a partitioning = fS 1 ; ; S n g of the state space S. We call the elements S i of the regions of M . <p> The resulting value function is used as a seed to generate a new set of macros (again one per region), and the new abstract MDP is solved. This iterative macro-refinement method is a special case of asynchronous policy iteration [2] and is similar to Dantzig-Wolfe (D-W) decom position techniques <ref> [6, 14] </ref>. D-W techniques can be viewed as iterative schemes for evaluating and modifying macro sets generated by assigning values to peripheral states.
Reference: [7] <author> R. Dearden and C. Boutilier. </author> <title> Abstraction and approximate decision theoretic planning. </title> <booktitle> Artif. Intell., </booktitle> <address> 89:219283, </address> <year> 1997. </year>
Reference-contexts: Considerable research has been directed toward the solution of Markov decision processes (MDPs) with large state and action spaces. These include function approximation [2], reach-ability analyses [5] and aggregation techniques <ref> [7, 3, 4] </ref>. Despite these advances, little attention has been paid to the reuse of policies or value functions generated for one MDP in the solution of a related MDP.
Reference: [8] <author> R. Fikes, P. Hart, and N. Nilsson. </author> <title> Learning and executing generalized robot plans. </title> <booktitle> Artif. Intell., </booktitle> <address> 3:251288, </address> <year> 1972. </year>
Reference-contexts: Despite these advances, little attention has been paid to the reuse of policies or value functions generated for one MDP in the solution of a related MDP. While such reasoning is common in classical planningfor instance, through the use of macros <ref> [8, 16, 13] </ref> or plan repair strategies [15]its application in stochastic settings is less common. Suitable techniques of this type could lead to the amortization of solution costs over a large number of problems, and the ability to solve future problem instances quickly, which is critical to on-line reasoning.
Reference: [9] <author> J. P. Forestier, P. Varaiya. </author> <title> Multilayer control of large Markov chains. </title> <journal> IEEE Trans. on Aut. Control, </journal> <volume> 23 </volume> <pages> 298-304, </pages> <year> 1978. </year>
Reference-contexts: We show how an abstract MDP can be constructed that consists only of states that lie on the borders of adjacent regions, and whose solution determines a policy that consists of macros only. Hierarchical models similar to the one we propose have been investigated by Forestier and Varaiya <ref> [9] </ref>, and recently by Parr [18, 19]. Two limitations of this model are then addressed. The first relates to solution quality. Since the policy generated by solving the abstract MDP can contain only macros, certain behaviors cannot be realized, thus the resulting policy may be suboptimal.
Reference: [10] <author> M. Hauskrecht. </author> <title> Planning with temporally abstract actions. </title> <type> Technical report, </type> <institution> CS-98-01, Brown University, Providence, </institution> <year> 1998. </year>
Reference-contexts: We also note that, depending on the initial value function used to begin value iteration, macros can actually increase the number of steps required for convergence compared to the value iteration with primitive actions alone. Specifically, Hauskrecht <ref> [10] </ref> showed that if V 0 is an upper bound on the optimal value function V fl , value iteration in the augmented MDP is guaranteed to require at least as many iterations as in the original MDP (the same holds for a lower bound and minimization of costs). <p> This reflects the increased action space of the augmented MDP and the reduced action generate a markovian policy for all states in the region (see <ref> [10] </ref> for details). augmented MDP) are shown on the left. Results for a poor initial function are shown on the right.
Reference: [11] <author> R. A. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> MIT Press, </publisher> <year> 1960. </year>
Reference-contexts: 1 Introduction Markov decision processes (MDPs) <ref> [11, 22] </ref> have proven tremendously useful as models of stochastic planning and decision problems. However, traditional dynamic programming remains computationally intractable for practical problems, requiring time polynomial in the size of the state and action spaces, but where these spaces are generally too large to be explicitly enumerated. <p> In such a setting, we restrict our attention to stationary policies of the form : S ! A, with (s) denoting the action to be executed in state s. The value of a policy can be shown to satisfy <ref> [11] </ref> V (s) = R (s; (s)) + fi t2S A policy is optimal if V (s) V 0 (s) for all s 2 S and policies 0 . The optimal value function V fl is the value function for any optimal policy.
Reference: [12] <author> L. </author> <title> Pack Kaelbling. Hierarchical reinforcement learning: Preliminary results. </title> <address> ICML-93, pp.167173, Amherst, </address> <year> 1993. </year>
Reference-contexts: To capture this intuition, we consider the hierarchical application of macro operators within a high-level, or abstract, MDP. This model is closely related to the landmark technique developed by Kael bling <ref> [12] </ref> for learning policies for hierarchical stochastic shortest path problems.
Reference: [13] <author> R. Korf. Macro-operators: </author> <title> A weak method for learning. </title> <booktitle> Artif. Intell., </booktitle> <address> 26:3577, </address> <year> 1985. </year>
Reference-contexts: Despite these advances, little attention has been paid to the reuse of policies or value functions generated for one MDP in the solution of a related MDP. While such reasoning is common in classical planningfor instance, through the use of macros <ref> [8, 16, 13] </ref> or plan repair strategies [15]its application in stochastic settings is less common. Suitable techniques of this type could lead to the amortization of solution costs over a large number of problems, and the ability to solve future problem instances quickly, which is critical to on-line reasoning.
Reference: [14] <author> H. J. Kushner and C.-H. Chen. </author> <title> Decomposition of systems governed by Markov chains. </title> <journal> IEEE Trans. Automatic Control, </journal> <volume> 19(5):501507, </volume> <year> 1974. </year>
Reference-contexts: The resulting value function is used as a seed to generate a new set of macros (again one per region), and the new abstract MDP is solved. This iterative macro-refinement method is a special case of asynchronous policy iteration [2] and is similar to Dantzig-Wolfe (D-W) decom position techniques <ref> [6, 14] </ref>. D-W techniques can be viewed as iterative schemes for evaluating and modifying macro sets generated by assigning values to peripheral states.
Reference: [15] <author> J. E. Laird, A. Newell, P. S. Rosenbloom. </author> <title> SOAR: An architecture for general intelligence. </title> <booktitle> Art. Intell., </booktitle> <address> 33:164, </address> <year> 1987. </year>
Reference: [16] <author> S. Minton. </author> <title> Selectively generalizing plans for problem solving. </title> <booktitle> IJCAI-85, </booktitle> <address> pp.596599, Boston, </address> <year> 1985. </year>
Reference-contexts: Despite these advances, little attention has been paid to the reuse of policies or value functions generated for one MDP in the solution of a related MDP. While such reasoning is common in classical planningfor instance, through the use of macros <ref> [8, 16, 13] </ref> or plan repair strategies [15]its application in stochastic settings is less common. Suitable techniques of this type could lead to the amortization of solution costs over a large number of problems, and the ability to solve future problem instances quickly, which is critical to on-line reasoning.
Reference: [17] <author> R. Parr and S. Russell. </author> <title> Reinforcement learning with hierarchies of machines. </title> <editor> In M. Mozer, M. Jordan, T. Petsche, eds., NIPS-11. </editor> <publisher> MIT Press, </publisher> <year> 1998. </year>
Reference-contexts: To deal with this problem, Precup, Sutton and Singh [23, 20, 21] have developed multi-time models and applied them to planning with MDPs. In what follows, we draw heavily on the use of these multi-time models. Parr and Russell <ref> [17] </ref> have proposed a related model in which a (partial) policy is modeled using a finite-state machine. These policies are then abstracted hierarchically and treated as primitive actions to be invoked by higher-level behaviors.
Reference: [18] <author> R. Parr. </author> <title> Flexible Decomposition Algorithms for Weakly Coupled Markov Decision Processes. </title> <booktitle> In this proceedings, </booktitle> <year> 1998. </year>
Reference-contexts: Hierarchical models similar to the one we propose have been investigated by Forestier and Varaiya [9], and recently by Parr <ref> [18, 19] </ref>. Two limitations of this model are then addressed. The first relates to solution quality. Since the policy generated by solving the abstract MDP can contain only macros, certain behaviors cannot be realized, thus the resulting policy may be suboptimal. <p> The Skills model of Thrun and Schwartz [24] has a similar motivation, though they do not address the use of multi-time models for learned skills. Parr <ref> [18, 19] </ref> has independently investigated the use of hierarchical models with an eye toward macro generation, and has considered many of the same problems we address here. Formally, our model relies on a region-based decomposition of a given MDP hS; A; T; Ri as defined by Dean and Lin [6]. <p> First, the number of macros is usually smaller than the number of grid points covering [V min ; V max ]. Thus it is often more appropriate to search a local policy space. One technique for doing so was suggested recently by Parr <ref> [18] </ref>. Second, we can apply various forms of domain-specific knowledge.
Reference: [19] <author> R. Parr. </author> <title> Hierarchical control and learning with hierarchies of machines. </title> <note> Chapters 1-3, under preparation, </note> <year> 1998. </year>
Reference-contexts: Hierarchical models similar to the one we propose have been investigated by Forestier and Varaiya [9], and recently by Parr <ref> [18, 19] </ref>. Two limitations of this model are then addressed. The first relates to solution quality. Since the policy generated by solving the abstract MDP can contain only macros, certain behaviors cannot be realized, thus the resulting policy may be suboptimal. <p> The Skills model of Thrun and Schwartz [24] has a similar motivation, though they do not address the use of multi-time models for learned skills. Parr <ref> [18, 19] </ref> has independently investigated the use of hierarchical models with an eye toward macro generation, and has considered many of the same problems we address here. Formally, our model relies on a region-based decomposition of a given MDP hS; A; T; Ri as defined by Dean and Lin [6]. <p> Issues related to the macro-model construction are discussed also in <ref> [19] </ref>. Let i be a macro defined on S i .
Reference: [20] <author> D. Precup and R. S. Sutton. </author> <title> Multi-time models for temporally abstract planning. </title> <editor> In M. Mozer, M. Jordan, and T. Petsche, eds., NIPS-11. </editor> <publisher> MIT Press, </publisher> <year> 1998. </year>
Reference-contexts: One of the few models to deal with solution reuse within the MDP framework is the Skills model of Thrun and Schwartz [24], which attempts to learn how to reuse policy fragments (or skills) for different tasks. Another is found in the work of Sutton and his colleagues <ref> [23, 20, 21] </ref>, who have developed models of macro-actions for MDPs that can be reused to solve multiple MDPs when objectives (or goals) change. <p> Integrating such a partial policy into the decision process is a difficult task given that: (a) the robot usually commits to the execution of this program; and (b) the program extends over some period of time. To deal with this problem, Precup, Sutton and Singh <ref> [23, 20, 21] </ref> have developed multi-time models and applied them to planning with MDPs. In what follows, we draw heavily on the use of these multi-time models. Parr and Russell [17] have proposed a related model in which a (partial) policy is modeled using a finite-state machine. <p> Because all base level actions (those in A) are present, the policy so constructed is guaranteed to be optimal. Furthermore, the presence of macros can enhance the convergence of value iteration, as demonstrated by Sutton et al. <ref> [20, 21] </ref>. This is due to the fact that the single application of a macro can propagate values through a large number of states and over a large period of time in a single step. <p> Notice that the abstract MDP induced by a given decomposition can be substantially smaller than the original MDP, especially if the problem can be decomposed into a number of regions with relatively small peripheriesthis is the case in our running example, and in the types of domains considered in <ref> [20, 21] </ref>. We call a policy 0 : S 0 ! A 0 for M 0 that maps peripheral states to macro-actions a macro-policy.
Reference: [21] <author> D. Precup, R. S. Sutton, and S. Singh. </author> <title> Theoretical results on reinforcement learning with temporally abstract behaviors. </title> <booktitle> 10th Eur. Conf. Mach. Learn., </booktitle> <address> Chemnitz, </address> <year> 1998. </year>
Reference-contexts: One of the few models to deal with solution reuse within the MDP framework is the Skills model of Thrun and Schwartz [24], which attempts to learn how to reuse policy fragments (or skills) for different tasks. Another is found in the work of Sutton and his colleagues <ref> [23, 20, 21] </ref>, who have developed models of macro-actions for MDPs that can be reused to solve multiple MDPs when objectives (or goals) change. <p> In this paper, we continue the investigation of the use of macros in MDPs; specifically, we focus on the problem of planning with macro-actions addressed by Precup, Sutton and Singh <ref> [21] </ref>. Our main aim is the development of a different model for planning with macros that deals with some of the computational problems associated with this earlier model (the PSS model). <p> Integrating such a partial policy into the decision process is a difficult task given that: (a) the robot usually commits to the execution of this program; and (b) the program extends over some period of time. To deal with this problem, Precup, Sutton and Singh <ref> [23, 20, 21] </ref> have developed multi-time models and applied them to planning with MDPs. In what follows, we draw heavily on the use of these multi-time models. Parr and Russell [17] have proposed a related model in which a (partial) policy is modeled using a finite-state machine. <p> Because all base level actions (those in A) are present, the policy so constructed is guaranteed to be optimal. Furthermore, the presence of macros can enhance the convergence of value iteration, as demonstrated by Sutton et al. <ref> [20, 21] </ref>. This is due to the fact that the single application of a macro can propagate values through a large number of states and over a large period of time in a single step. <p> Notice that the abstract MDP induced by a given decomposition can be substantially smaller than the original MDP, especially if the problem can be decomposed into a number of regions with relatively small peripheriesthis is the case in our running example, and in the types of domains considered in <ref> [20, 21] </ref>. We call a policy 0 : S 0 ! A 0 for M 0 that maps peripheral states to macro-actions a macro-policy. <p> Thus, unless tight constraints are known on the value function, this can involve substantial overhead and, in many instances, be unprofitable. Heuristic methods for macro generation can alleviate this difficulty if they require the construction of a small number of macros. One such strategy, suggested by Sutton et al. <ref> [21] </ref> for robot navigation problems such as our example, involves creating macros for each region S i that try to lead the agent out of S i via different exit states.
Reference: [22] <author> M. L. Puterman. </author> <title> Markov Decision Processes: Discrete Stochastic Dynamic Programming. </title> <publisher> Wiley, </publisher> <year> 1994. </year>
Reference-contexts: 1 Introduction Markov decision processes (MDPs) <ref> [11, 22] </ref> have proven tremendously useful as models of stochastic planning and decision problems. However, traditional dynamic programming remains computationally intractable for practical problems, requiring time polynomial in the size of the state and action spaces, but where these spaces are generally too large to be explicitly enumerated.
Reference: [23] <author> R. S. Sutton. </author> <title> TD models: Modeling the world at a mixture of time scales. </title> <booktitle> In ICML-95, </booktitle> <address> pp.531539, Lake Tahoe, </address> <year> 1995. </year>
Reference-contexts: One of the few models to deal with solution reuse within the MDP framework is the Skills model of Thrun and Schwartz [24], which attempts to learn how to reuse policy fragments (or skills) for different tasks. Another is found in the work of Sutton and his colleagues <ref> [23, 20, 21] </ref>, who have developed models of macro-actions for MDPs that can be reused to solve multiple MDPs when objectives (or goals) change. <p> Each iteration is known as a Bellman backup. After some finite number n of iterations, the choice of maximizing ac tion for each s forms an optimal policy and V n approxi mates its value. 2.2 Macro-actions and their Models Sutton <ref> [23] </ref> has argued that it is crucial to be able to model MDPs at multiple time scales. The ability to determine the valuewithin an underlying MDPof a complex sequence of actions or program is important in, say for example, robot programming. <p> Integrating such a partial policy into the decision process is a difficult task given that: (a) the robot usually commits to the execution of this program; and (b) the program extends over some period of time. To deal with this problem, Precup, Sutton and Singh <ref> [23, 20, 21] </ref> have developed multi-time models and applied them to planning with MDPs. In what follows, we draw heavily on the use of these multi-time models. Parr and Russell [17] have proposed a related model in which a (partial) policy is modeled using a finite-state machine. <p> A key insight of PSS (which finds its roots in earlier work by Sutton <ref> [23] </ref>) is that one can treat a macro-action of this type as a primitive action in the original MDP if one has an appropriate reward and transition model for the macro. They propose the following method of modeling macros.
Reference: [24] <author> S. Thrun and A. Schwartz. </author> <title> Finding structure in reinforcement learning. </title> <editor> In G. Tesauro, D. Touretzky, and T. Leen, eds., NIPS-7, pp.385392, </editor> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: One of the few models to deal with solution reuse within the MDP framework is the Skills model of Thrun and Schwartz <ref> [24] </ref>, which attempts to learn how to reuse policy fragments (or skills) for different tasks. Another is found in the work of Sutton and his colleagues [23, 20, 21], who have developed models of macro-actions for MDPs that can be reused to solve multiple MDPs when objectives (or goals) change. <p> Thus the effort required to generate these macros (something not considered in the PSS model) will pay for itself either with decreased reaction time to changing circumstances, or with total computational savings over multiple problem instances. The Skills model of Thrun and Schwartz <ref> [24] </ref> has a similar motivation, though they do not address the use of multi-time models for learned skills. Parr [18, 19] has independently investigated the use of hierarchical models with an eye toward macro generation, and has considered many of the same problems we address here.
References-found: 24

