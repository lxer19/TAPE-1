URL: http://www.cs.washington.edu/homes/rgrimm/research/papers/mth.ps
Refering-URL: http://www.cs.washington.edu/homes/rgrimm/research/papers.html
Root-URL: 
Title: Exodisk: maximizing application control over storage management  
Author: by Robert Grimm Gregory R. Ganger M. Frans Kaashoek 
Degree: Submitted to the Department of Electrical Engineering and Computer Science in Partial Fulfillment of the Requirements for the Degrees of Bachelor of Science in Computer Science and Engineering and Master of Engineering in Electrical Engineering and Computer Science  MIT. All rights reserved. Author  Certified by  Thesis Supervisor Certified by  Thesis Supervisor Accepted by Frederic R. Morgenthaler Chairman, Department Committee on Graduate Theses  
Note: Copyright  
Date: May 28, 1996  1996  May 17, 1996  
Affiliation: at the Massachusetts Institute of Technology  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Mary G. Baker, John H. Hartman, Michael D. Kupfer, Ken W. Shirriff, and John K. Ousterhout. </author> <title> Measurements of a distributed file system. </title> <booktitle> In Proceedings of the 13th Symposium on Operating Systems Principles, </booktitle> <pages> pages 198-212, </pages> <address> Pacific Grove, California, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: The experiments are summarized in Table 5.2. The above experiments are modeled after similar experiments used to evaluate the performance of the Sprite log-structured file system [44] and the Logical Disk [8]. The file sizes of 1 KByte and 10 KByte are also consistent with <ref> [1, 9, 23, 37] </ref> which show that roughly between 70% and 80% of all files in the measured Unix, Sprite and AFS file systems are less than 10 KByte in size. 45 5.2.2 File Organizations The following file organizations and interfaces are used in the small and large file I/O experiments.
Reference: [2] <author> Pei Cao, Edward W. Felten, Anna R. Karlin, and Kai Li. </author> <title> Implementation and performance of integrated application-controlled caching, prefetching and disk scheduling. </title> <type> Technical report, </type> <institution> Princeton University, </institution> <year> 1994. </year> <month> CS-TR-94-493. </month>
Reference-contexts: Research on disk storage management provides ample evidence that disk access times can greatly benefit from application-specific resource management: Stonebraker [50] argues that inappropriate file system implementations can have considerable impact on the performance of database managers. Cao et al. <ref> [3, 2] </ref> show that application-level control over file caching, prefetching and disk scheduling policies can reduce average application running time by 46%. Patterson et al. [39] use application provided access patterns to dynamically regulate prefetching and caching, and show that application running time can be reduced by up to 42%. <p> Recent work in application-controlled storage management has focused on allowing applications to direct caching and prefetching activity, either by giving applications direct control over caching and prefetching <ref> [3, 2] </ref>, or by using a cost-benefit model in connection with application supplied hints [39]. This leads to significant performance improvements (e.g., over 40% reduction in the execution time of some applications), but addresses only one part of the spectrum of storage management issues.
Reference: [3] <author> Pei Cao, Edward W. Felten, and Kai Li. </author> <title> Implementation and performance of application-controlled file caching. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 165-177, </pages> <address> Monterey, California, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: Research on disk storage management provides ample evidence that disk access times can greatly benefit from application-specific resource management: Stonebraker [50] argues that inappropriate file system implementations can have considerable impact on the performance of database managers. Cao et al. <ref> [3, 2] </ref> show that application-level control over file caching, prefetching and disk scheduling policies can reduce average application running time by 46%. Patterson et al. [39] use application provided access patterns to dynamically regulate prefetching and caching, and show that application running time can be reduced by up to 42%. <p> A future implementation may thus choose to introduce a more sophisticated exodisk cache design: A mechanism similar to the LRU-SP policy suggested by Cao et al. in <ref> [3] </ref> can be used to fairly distribute exodisk cache entries among applications and to offer protection against greedy applications. Alternatively, a cost-benefit model in connection with application supplied hints as suggested by Patterson et al. in [39] can be used to regulate the caching and prefetching within the exodisk cache. <p> Recent work in application-controlled storage management has focused on allowing applications to direct caching and prefetching activity, either by giving applications direct control over caching and prefetching <ref> [3, 2] </ref>, or by using a cost-benefit model in connection with application supplied hints [39]. This leads to significant performance improvements (e.g., over 40% reduction in the execution time of some applications), but addresses only one part of the spectrum of storage management issues.
Reference: [4] <author> D. L. Chaum and R. S. Fabry. </author> <title> Implementing capability-based protection using encryption. </title> <type> Technical Report UCB/ERL M78/46, </type> <institution> University of California at Berkeley, </institution> <month> July </month> <year> 1978. </year>
Reference-contexts: Fine grain multiplexing avoids both the inflexibility of statically allocated partitions and the performance overhead associated with long seeks between partitions. Each such extent (a 14 single sector is an extent of length one) is protected by self-authenticating capabilities <ref> [4] </ref> for read and write access. Such an extent is called a self-protected extent. The metadata necessary to support this mapping from self-protected extents to capabilities is stored at the beginning of each individual extent within the extent header.
Reference: [5] <author> Peter M. Chen. </author> <title> Optimizing delay in delayed-write file systems. </title> <type> Technical report, </type> <institution> University of Michigan, </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: Applications can choose which model to use for each file and thus make their own tradeoffs between the strength of the persistence model and application performance. They can also dynamically alter and fine-tune their choices, depending on their specific performance requirements and computing environment. For example, Chen shows in <ref> [5] </ref> that the optimal 58 delay for writing dirty file system data from the buffer cache back to disk is dependent on the size of the file cache and the disk transfer rate.
Reference: [6] <author> Peter M. Chen, Wee Teck Ng, Gurushankar Rajamani, and Christopher M. Aycock. </author> <title> The Rio file cache: Surviving operating system crashes. </title> <type> Technical Report CSE-TR-286-96, </type> <institution> University of Michigan, </institution> <month> March </month> <year> 1996. </year>
Reference-contexts: However, it does not use a buffer to store the file data. Memory shows a factor of 21 better performance than Data. This result is (roughly) comparable to performance results for the Andrew file system benchmark under the different persistence models reported by Chen et al. in <ref> [6] </ref> (which show a factor of 25 performance difference between the Memory File System [30] and the Unix File System with write-through after each write).
Reference: [7] <author> Chia Choa, Robert English, David Jacobson, Alexander Stepanov, and John Wilkes. Mime: </author> <title> a high performance parallel storage device with strong recovery guarantees. </title> <type> Technical Report HPL-CSP-92-9rev1, </type> <institution> Hewlett Packard, </institution> <year> 1992. </year> <month> 67 </month>
Reference-contexts: This approach prevents the exodisk system from transparently reorganizing data on disk or avoiding block-overwrite semantics, as is done in some logical representations of disk storage (for example, in Mime <ref> [7] </ref> or the Logical Disk [8]). But it also does not suffer from the overheads associated with such logical-to-physical mappings, and it gives applications the flexibility to utilize the policies that are best suited for the given application. <p> Alternatively, it can use one exonode, storing the FAT in the exonode as application-defined data and using fixed-size extent entries for the individual blocks. Logical representations of disk storage such as Mime <ref> [7] </ref> or the Logical Disk [8] map logical block numbers to physical disk locations. <p> Loge [12], Mime <ref> [7] </ref>, the Logical Disk [8, 20], the HP AutoRAID System [52] and Network-Attached Secure Disks (NASD) [17, 18] all use a logical representation of disk storage through a level of indirection.
Reference: [8] <author> Wiebren de Jonge, M. Frans Kaashoek, and Wilson C. Hsieh. </author> <title> The logical disk: A new approach to improving file systems. </title> <booktitle> In Proceedings of the 14th Symposium on Operating Systems Principles, </booktitle> <pages> pages 15-28, </pages> <address> Ashville, North Carolina, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: This approach prevents the exodisk system from transparently reorganizing data on disk or avoiding block-overwrite semantics, as is done in some logical representations of disk storage (for example, in Mime [7] or the Logical Disk <ref> [8] </ref>). But it also does not suffer from the overheads associated with such logical-to-physical mappings, and it gives applications the flexibility to utilize the policies that are best suited for the given application. <p> Alternatively, it can use one exonode, storing the FAT in the exonode as application-defined data and using fixed-size extent entries for the individual blocks. Logical representations of disk storage such as Mime [7] or the Logical Disk <ref> [8] </ref> map logical block numbers to physical disk locations. Such a mapping can be efficiently implemented using an exonode: The logical block number can be an index into the array of 38 exonode entries and the physical block information (again, a fixed-size extent) is stored in that exonode entry. <p> The experiments are summarized in Table 5.2. The above experiments are modeled after similar experiments used to evaluate the performance of the Sprite log-structured file system [44] and the Logical Disk <ref> [8] </ref>. <p> Loge [12], Mime [7], the Logical Disk <ref> [8, 20] </ref>, the HP AutoRAID System [52] and Network-Attached Secure Disks (NASD) [17, 18] all use a logical representation of disk storage through a level of indirection.
Reference: [9] <author> Maria R. Ebling and M. Satyanarayanan. SynRGen: </author> <title> An extensible file reference generator. </title> <booktitle> In Proceedings of the 1994 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <address> Nashville, Tennessee, </address> <month> May </month> <year> 1994. </year> <note> Also available as Technical Report CMU-CS-94-119. </note>
Reference-contexts: The experiments are summarized in Table 5.2. The above experiments are modeled after similar experiments used to evaluate the performance of the Sprite log-structured file system [44] and the Logical Disk [8]. The file sizes of 1 KByte and 10 KByte are also consistent with <ref> [1, 9, 23, 37] </ref> which show that roughly between 70% and 80% of all files in the measured Unix, Sprite and AFS file systems are less than 10 KByte in size. 45 5.2.2 File Organizations The following file organizations and interfaces are used in the small and large file I/O experiments.
Reference: [10] <author> Dawson R. Engler. </author> <title> The design and implementation of a prototype exokernel operating system. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1995. </year>
Reference-contexts: The disk system design is guided by the exokernel design principles [11], and the prototype is implemented as an in-kernel component of the Aegis exokernel operating system <ref> [10, 11] </ref>. However, it could just as easily be used in other operating systems. Exodisk, the disk system described in this thesis, safely and efficiently space-multiplexes disk storage among applications at a very fine grain (down to a single disk sector), while avoiding resource management wherever possible. <p> when issuing 128 requests at a time. (3) Application-specific persistence models allow application-writers to make the trade-off between persistence guarantees and application performance on a per-file basis and greatly increase the flexibility of applications. 1.4 Exokernels The exodisk system is designed as a component of Aegis, an exokernel operating system <ref> [10, 11] </ref>. <p> To allocate disk space, an application asks the exodisk system for specific disk locations. If any of the requested sectors are already allocated, the exodisk system fails the allocation request. Otherwise, the disk space is associated with a (possibly new) self-protected extent or exonode, which establishes a secure binding <ref> [10] </ref>. The capabilities used to protect the self-protected extent or exonode are provided by the application. The exodisk system uses a free-list to make the current status of disk allocation visible to all disk system clients. <p> The idea of allowing applications to control resource management in a protected way 60 is not new and has been suggested, pursued and applied for a variety of other computer system resources. The exodisk system takes this philosophy directly from the exokernel operating system architecture <ref> [10, 11] </ref>, which in turn builds upon a variety of other work.
Reference: [11] <author> Dawson R. Engler, M. Frans Kaashoek, and James O'Toole Jr. Exokernel: </author> <title> an operating system architecture for application-level resource management. </title> <booktitle> In Proceedings of the 15th Symposium on Operating Systems Principles, </booktitle> <pages> pages 251-266, </pages> <address> Copper Mountain Resort, Colorado, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Applications can thus implement their own specialized disk storage abstractions on top of the disk system and (potentially) achieve better performance and greater flexibility than in current system architectures. The disk system design is guided by the exokernel design principles <ref> [11] </ref>, and the prototype is implemented as an in-kernel component of the Aegis exokernel operating system [10, 11]. However, it could just as easily be used in other operating systems. <p> The disk system design is guided by the exokernel design principles [11], and the prototype is implemented as an in-kernel component of the Aegis exokernel operating system <ref> [10, 11] </ref>. However, it could just as easily be used in other operating systems. Exodisk, the disk system described in this thesis, safely and efficiently space-multiplexes disk storage among applications at a very fine grain (down to a single disk sector), while avoiding resource management wherever possible. <p> when issuing 128 requests at a time. (3) Application-specific persistence models allow application-writers to make the trade-off between persistence guarantees and application performance on a per-file basis and greatly increase the flexibility of applications. 1.4 Exokernels The exodisk system is designed as a component of Aegis, an exokernel operating system <ref> [10, 11] </ref>. <p> The following principles, which are derived from this fundamental goal, guide the design of the exodisk system (this discussion follows the discussion of the general exokernel design principles in <ref> [11] </ref>). Each principle is presented along with the related components of the exodisk system. <p> Since a buffer cache will typically cache more and larger data than the exodisk cache, this scheme avoids large areas of physical memory being reserved for future use in the cache. At the same time, through visible resource revocation <ref> [11] </ref>, applications can still completely control their buffer cache management. 2.4 Limitations The exodisk system space-multiplexes disk storage but does not provide a mechanism to time-multiplex the available disk bandwidth. <p> Furthermore, the exodisk system uses asynchronous I/O, and leaves the caching of disk blocks to applications. 28 Chapter 3 Implementation The prototype implementation of the exodisk system is implemented as a component of the Aegis exokernel <ref> [11] </ref>. Since it is independent of the rest of the exokernel, it could easily be incorporated into other operating systems, with the same benefits. The prototype runs on DECstation 3100 and 5000 computers. It consists of approximately 4300 lines of well-commented C-code. <p> exodisk request system call, the exodisk system first verifies that any application-supplied data (the data structures pointed to by the prot, reqbp and stuff pointers, as well as buffer space for disk I/O and for application 33 defined data) have virtual-to-physical mappings in the processor's TLB (or the exokernel STLB <ref> [11] </ref>). If this is not the case, a TLB fault is signaled to the application, and the system call is restarted after the mapping has been installed in the TLB. <p> The idea of allowing applications to control resource management in a protected way 60 is not new and has been suggested, pursued and applied for a variety of other computer system resources. The exodisk system takes this philosophy directly from the exokernel operating system architecture <ref> [10, 11] </ref>, which in turn builds upon a variety of other work.
Reference: [12] <author> Robert M. English and Alexander A. Stepanov. Loge: </author> <title> a self-organizing disk controller. </title> <booktitle> In Proceedings of 1992 Winter USENIX, </booktitle> <pages> pages 237-251, </pages> <address> San Francisco, California, </address> <month> January </month> <year> 1992. </year>
Reference-contexts: Loge <ref> [12] </ref>, Mime [7], the Logical Disk [8, 20], the HP AutoRAID System [52] and Network-Attached Secure Disks (NASD) [17, 18] all use a logical representation of disk storage through a level of indirection.
Reference: [13] <institution> Digital Equipment. </institution> <note> DECstation 5000/133 Product Description. http://ftp.digital. com/pub/Digital/DECinfo/SOC/Feb94/ch-3-b.txt. </note>
Reference-contexts: Section 5.5 summarizes the evaluation of the exodisk system. 5.1 Experimental Environment The experiments described in this Chapter were conducted on a DECstation 5000/133 using a RZ25 disk drive. The characteristics of the computer system and the disk drive shown in Table 5.1 are taken from Digital Equipment's specifications <ref> [13, 14] </ref>. The exodisk cache for experiments using the exodisk system contains up to 281 entries, i.e. up to 281 extent 43 DECstation 5000/133: 33 MHz MIPS R3000A. 64 KByte I-cache, 128 KByte D-cache. 48 MByte main memory. 26.5 SPECmark89.
Reference: [14] <institution> Digital Equipment. </institution> <note> The RZ25: Digital's Highest Performing 3.5-Inch SCSI Disk - 426 MBF. S30108 Sales Update Dated 15-JUL-91. </note>
Reference-contexts: Section 5.5 summarizes the evaluation of the exodisk system. 5.1 Experimental Environment The experiments described in this Chapter were conducted on a DECstation 5000/133 using a RZ25 disk drive. The characteristics of the computer system and the disk drive shown in Table 5.1 are taken from Digital Equipment's specifications <ref> [13, 14] </ref>. The exodisk cache for experiments using the exodisk system contains up to 281 entries, i.e. up to 281 extent 43 DECstation 5000/133: 33 MHz MIPS R3000A. 64 KByte I-cache, 128 KByte D-cache. 48 MByte main memory. 26.5 SPECmark89.
Reference: [15] <author> Gregory R. Ganger, Robert Grimm, M. Frans Kaashoek, and Dawson R. Engler. </author> <title> Application-controlled storage management. </title> <type> Technical Report TM-551, </type> <institution> MIT Laboratory for Computer Science, </institution> <year> 1996. </year>
Reference: [16] <author> Gregory R. Ganger and Yale N. Patt. </author> <title> Metadata update performance in file systems. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 49-60, </pages> <address> Monterey, California, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: Temporary, memory-based file systems [30, 35] have been added to many systems because conventional file creation/deletion semantics are too heavy-weight for some applications, costing more than a factor of two in application performance <ref> [16] </ref>.
Reference: [17] <author> Garth Gibson. </author> <title> Network-attached secure disks (NASD). </title> <address> http://www.cs.cmu.edu/ afs/cs/project/pdl/WWW/Rpt/NASD_Feb96.ps, </address> <month> February </month> <year> 1996. </year> <month> 68 </month>
Reference-contexts: Loge [12], Mime [7], the Logical Disk [8, 20], the HP AutoRAID System [52] and Network-Attached Secure Disks (NASD) <ref> [17, 18] </ref> all use a logical representation of disk storage through a level of indirection.
Reference: [18] <author> Garth Gibson. </author> <title> Network-attached secure disks (NASD). </title> <address> http://www.cs.cmu.edu/ afs/cs/project/pdl/WWW/ftp/NASD_ARPA_Feb16_96.ps%, </address> <month> February </month> <year> 1996. </year>
Reference-contexts: Loge [12], Mime [7], the Logical Disk [8, 20], the HP AutoRAID System [52] and Network-Attached Secure Disks (NASD) <ref> [17, 18] </ref> all use a logical representation of disk storage through a level of indirection.
Reference: [19] <author> David K. Gifford, Pierre Jouvelot, Mark A. Sheldon, and James W. O'Toole. </author> <title> Semantic file systems. </title> <booktitle> In Proceedings of the 13th Symposium on Operating Systems Principles, </booktitle> <pages> pages 16-25, </pages> <address> Pacific Grove, California, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Examples are immediate files [33], temporary and memory-based file systems [35, 30], log-structured file systems [42, 44, 46], clustered files [40, 31], semantic file systems <ref> [19] </ref>, and stackable file systems [21]. Abstractions have been developed to allow these multiple storage management schemes to co-exist, 61 ranging from common interface definitions (for example, vnodes as described in [25]) to file systems that propose to incorporate them all (for example, [34, 49]).
Reference: [20] <author> Robert Grimm, Wilson C. Hsieh, Wiebren de Jonge, and M. Frans Kaashoek. </author> <title> Atomic recovery units: Failure atomicity for logical disks. </title> <booktitle> In Proceedings of the 16th International Conference on Distributed Computing Systems, </booktitle> <address> Hong Kong, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Loge [12], Mime [7], the Logical Disk <ref> [8, 20] </ref>, the HP AutoRAID System [52] and Network-Attached Secure Disks (NASD) [17, 18] all use a logical representation of disk storage through a level of indirection.
Reference: [21] <author> John Heidemann and Gerald Popek. </author> <title> Performance of cache coherence in stackable filing. </title> <booktitle> In Proceedings of the 15th Symposium on Operating Systems Principles, </booktitle> <pages> pages 127-142, </pages> <address> Copper Mountain Resort, Colorado, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Finally, applications can create user-augmented file systems by adding new functionality (such as encryption and compression) on top of given storage layouts by expanding the storage manager (similarly to stackable file systems <ref> [21] </ref>). The MS-DOS file system does not use i-nodes to organize disk storage but rather uses a so-called File Allocation Table (FAT) to map disk blocks to successors [51]. <p> Examples are immediate files [33], temporary and memory-based file systems [35, 30], log-structured file systems [42, 44, 46], clustered files [40, 31], semantic file systems [19], and stackable file systems <ref> [21] </ref>. Abstractions have been developed to allow these multiple storage management schemes to co-exist, 61 ranging from common interface definitions (for example, vnodes as described in [25]) to file systems that propose to incorporate them all (for example, [34, 49]).
Reference: [22] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture, A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, California, first edition, </address> <year> 1990. </year>
Reference-contexts: level of abstraction and doing the work "behind the scene"| Section 6.2), and work related to expanding the set of options available for applications to choose among (Section 6.3). 6.1 Giving Control to Applications The storage subsystems of IBM mainframes (for example, the IBM 3990 I/O Subsystem as described in <ref> [22] </ref>) allowed users to construct I/O control programs to be executed directly on channel processors. This provided a great deal of freedom to applications, but did not provide for protection (e.g., applications could easily interfere with one another).
Reference: [23] <author> Gordon Irlam. </author> <title> Unix file size survey|1993. World-Wide Web, </title> <note> 1993. http://www. base.com/gordoni/ufs93.html. </note>
Reference-contexts: The experiments are summarized in Table 5.2. The above experiments are modeled after similar experiments used to evaluate the performance of the Sprite log-structured file system [44] and the Logical Disk [8]. The file sizes of 1 KByte and 10 KByte are also consistent with <ref> [1, 9, 23, 37] </ref> which show that roughly between 70% and 80% of all files in the measured Unix, Sprite and AFS file systems are less than 10 KByte in size. 45 5.2.2 File Organizations The following file organizations and interfaces are used in the small and large file I/O experiments.
Reference: [24] <author> David M. Jacobson and John Wilkes. </author> <title> Disk scheduling algorithms based on rotational position. </title> <type> Technical Report HPL-CSP-91-7rev1, </type> <institution> Hewlett-Packard, </institution> <year> 1991. </year>
Reference-contexts: This approach can significantly improve performance in modern disk systems, which are characterized by (1) multiple disks disguised as a single disk (i.e., disk arrays [38]), or (2) disks that aggressively schedule requests based on information not available to the host system (e.g., both the seek time and rotational latency <ref> [47, 24] </ref>). The low-level interface provided by the exodisk system makes using disk-directed I/O trivial. One can simply initiate multiple requests, wait for notification of each request's completion and process it. To illustrate this, a very simple disk-directed I/O application is used.
Reference: [25] <author> S. R. Kleiman. Vnodes: </author> <title> An architecture for multiple file system types in Sun UNIX. </title> <booktitle> In Proceedings of 1986 Summer USENIX, </booktitle> <pages> pages 238-247, </pages> <address> Atlanta, Georgia, </address> <month> June </month> <year> 1986. </year>
Reference-contexts: Abstractions have been developed to allow these multiple storage management schemes to co-exist, 61 ranging from common interface definitions (for example, vnodes as described in <ref> [25] </ref>) to file systems that propose to incorporate them all (for example, [34, 49]).
Reference: [26] <author> David Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 61-74, </pages> <address> Monterey, California, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: The flexi 8 bility of the exodisk system is illustrated with fast file insertion, disk-directed I/O <ref> [26] </ref> and application-specific persistence. 1.1 Problem and Motivation CPU speed, network bandwidth, and primary and secondary storage capacities have increased rapidly over the last decade. At the same time, disk access times have only improved slowly [36, 38]. <p> Furthermore, it shows that a fast insert operation performs up to a factor of six better than insertion by copying (for large files), that disk-directed I/O <ref> [26] </ref> can improve the request-processing rate in an issue-wait-process cycle (by 39% when issuing 128 requests at the same time), and that applications can make their own trade-offs between persistence and performance by providing their own persistence models. This chapter is organized as follows. <p> They show 40.7% more latency than the Copy experiments for 10 KByte files (due to the slower read performance), a factor of four better performance for 100 KByte files, and a factor of six better performance for 1,000 KByte files. 5.4.2 Disk-Directed I/O Disk-directed I/O <ref> [26] </ref> is a straight-forward attack on the mechanical disk bottleneck. <p> The performance difference should be considerably larger when using modern disk drives that internally implement very aggressive scheduling algorithms. Of course, this is a very simple example designed to illustrate the ease of using this approach with the exodisk system. Kotz <ref> [26] </ref> and Patterson et al. [39] much more thoroughly demonstrate the benefits of doing so. 5.4.3 Application-Specific Persistence Because applications can implement their own file abstractions (or choose from a library of pre-developed ones), varying persistence models can be easily implemented on top of 56 the exodisk system.
Reference: [27] <author> Samuel J. Le*er, Marshall Kirk McKusick, Michael J. Karels, and John S. Quarterman. </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1989. </year>
Reference-contexts: Seltzer et al. show in [48], by comparing the 4.4BSD log-structured file system (which is based on ideas first explored in Sprite LFS [42, 44]) and the BSD fast file system <ref> [29, 41, 27] </ref>, that the highest performing data organization is highly workload dependent. Temporary, memory-based file systems [30, 35] have been added to many systems because conventional file creation/deletion semantics are too heavy-weight for some applications, costing more than a factor of two in application performance [16].
Reference: [28] <author> David Mazieres. </author> <title> Resource protection. </title> <type> Personal Communication, </type> <month> May </month> <year> 1996. </year> <month> 69 </month>
Reference-contexts: An alternative solution, which avoids the level of indirection, places the capabilities themselves into a hierarchical space. David Mazieres, who proposed this idea, is currently exploring it in an implementation of an exokernel operating system on the 80x86 architecture <ref> [28] </ref>. 4.4 Summary The examples in this chapter illustrate that the exodisk system provides applications with the flexibility to conveniently implement many different storage abstractions and optimization that best meet their specific performance and functionality needs.
Reference: [29] <author> Marshall K. McKusick, William N. Joy, Samuel J. Le*er, and Robert S. Fabry. </author> <title> A fast file system for UNIX. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(3) </volume> <pages> 181-197, </pages> <month> August </month> <year> 1984. </year>
Reference-contexts: Seltzer et al. show in [48], by comparing the 4.4BSD log-structured file system (which is based on ideas first explored in Sprite LFS [42, 44]) and the BSD fast file system <ref> [29, 41, 27] </ref>, that the highest performing data organization is highly workload dependent. Temporary, memory-based file systems [30, 35] have been added to many systems because conventional file creation/deletion semantics are too heavy-weight for some applications, costing more than a factor of two in application performance [16]. <p> The large transfer size for the sequential write and read operations mimics the block clustering used in some file systems [31]; the transfer size for the random write and read operations reflects the typical block size in Unix file systems <ref> [29] </ref>. The experiments are summarized in Table 5.2. The above experiments are modeled after similar experiments used to evaluate the performance of the Sprite log-structured file system [44] and the Logical Disk [8]. <p> Most applications will likely choose a persistence model between the two extremes of the Memory and the Data model and use a buffer cache to delay writes (similarly to the Unix Fast File System <ref> [29] </ref>). The results show the impact of different persistence models on application performance. Applications can choose which model to use for each file and thus make their own tradeoffs between the strength of the persistence model and application performance.
Reference: [30] <author> Marshall Kirk McKusick, Michael J. Karels, and Keith Bostic. </author> <title> A pageable memory based filesystem. </title> <booktitle> In Proceedings of 1990 Summer USENIX, </booktitle> <pages> pages 137-143, </pages> <address> Annaheim, California, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Seltzer et al. show in [48], by comparing the 4.4BSD log-structured file system (which is based on ideas first explored in Sprite LFS [42, 44]) and the BSD fast file system [29, 41, 27], that the highest performing data organization is highly workload dependent. Temporary, memory-based file systems <ref> [30, 35] </ref> have been added to many systems because conventional file creation/deletion semantics are too heavy-weight for some applications, costing more than a factor of two in application performance [16]. <p> Furthermore, applications do not need to use the same persistence model for all files, but can use different persistence models for different files. The concept of allowing different files to have different integrity and persistence characteristics is not new. For example, temporary and memory-based file systems <ref> [35, 30] </ref> were added to systems so that files that are known to be short-lived do not have to suffer the high cost of file creation and deletion imposed by conventional file systems. <p> Memory shows a factor of 21 better performance than Data. This result is (roughly) comparable to performance results for the Andrew file system benchmark under the different persistence models reported by Chen et al. in [6] (which show a factor of 25 performance difference between the Memory File System <ref> [30] </ref> and the Unix File System with write-through after each write). Most applications will likely choose a persistence model between the two extremes of the Memory and the Data model and use a buffer cache to delay writes (similarly to the Unix Fast File System [29]). <p> Examples are immediate files [33], temporary and memory-based file systems <ref> [35, 30] </ref>, log-structured file systems [42, 44, 46], clustered files [40, 31], semantic file systems [19], and stackable file systems [21].
Reference: [31] <author> L. W. McVoy and S. R. Kleiman. </author> <title> Extent-like performance from a UNIX file system. </title> <booktitle> In Proceedings of 1991 Winter USENIX, </booktitle> <pages> pages 33-43, </pages> <address> Dallas, Texas, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: Each entry either contains an extent to point to the corresponding physical data block, or is unused to indicate that the logical block is not currently allocated (entry 2). be referred to by direct pointers and larger data transfer sizes can be used <ref> [40, 31] </ref>. Since the size of exonodes is not fixed, it is possible to use i-nodes of different sizes and avoid the use of indirect i-nodes for medium-sized files. <p> The large transfer size for the sequential write and read operations mimics the block clustering used in some file systems <ref> [31] </ref>; the transfer size for the random write and read operations reflects the typical block size in Unix file systems [29]. The experiments are summarized in Table 5.2. <p> Examples are immediate files [33], temporary and memory-based file systems [35, 30], log-structured file systems [42, 44, 46], clustered files <ref> [40, 31] </ref>, semantic file systems [19], and stackable file systems [21]. Abstractions have been developed to allow these multiple storage management schemes to co-exist, 61 ranging from common interface definitions (for example, vnodes as described in [25]) to file systems that propose to incorporate them all (for example, [34, 49]).
Reference: [32] <author> S. P. Miller, B. C. Neuman, J. I. Schiller, and J. H. Saltzer. </author> <title> Kerberos authentication and authorization system. </title> <type> Project Athena technical plan, </type> <institution> Massachusetts Institute of Technology, </institution> <month> October </month> <year> 1988. </year>
Reference-contexts: Other implementations may thus choose to provide a higher degree of security and use digital signatures, encrypt all disk data, or, instead of using simple capabilities, use an authentication system such as Kerberos <ref> [32] </ref> for authentication and authorization. An implementation must thus make a trade-off between security of the on-disk data and the complexity of the authentication scheme. 2.3 Issues in Multiplexing Disk Storage A number of issues other than access control arise when multiplexing disk storage among applications.
Reference: [33] <author> Sape J. Mullender and Andrew S. Tanenbaum. </author> <title> Immediate files. </title> <journal> Software|Practice and Experience, </journal> <volume> 14(4) </volume> <pages> 365-368, </pages> <month> April </month> <year> 1984. </year>
Reference-contexts: Since the metadata of all storage system layers can be stored in close proximity, it is possible to avoid maintaining several separate tree-like mappings from disk storage to metadata as described by Stonebraker [50]. Exonodes also support file system optimizations such as immediate files <ref> [33] </ref> which store the data of small files in the i-node. Exonodes support references to descendant exonodes, which are protected by their own, possibly different, capabilities. Their intended use is to create hierarchical protection spaces, possibly shared by several applications. <p> Several extensions to the conventional Unix file system structure can be easily implemented on top of the exodisk system. The data of very small files can be stored within the exonode, thus avoiding one disk operation for both read and write access to such a file <ref> [33] </ref>. The use of extents in the exodisk system makes it possible to allocate file blocks in larger contiguous units than single blocks, resulting in a (partial) extent-based file system structure. <p> Examples are immediate files <ref> [33] </ref>, temporary and memory-based file systems [35, 30], log-structured file systems [42, 44, 46], clustered files [40, 31], semantic file systems [19], and stackable file systems [21].
Reference: [34] <author> Keith Muller and Joseph Pasquale. </author> <title> A high performance multi-structured file system design. </title> <booktitle> In Proceedings of the 13th Symposium on Operating Systems Principles, </booktitle> <pages> pages 56-67, </pages> <address> Pacific Grove, California, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: It is therefore possible to efficiently interleave several file systems on disk as well as to stripe over several disks, thus allowing for multiple file implementations as suggested in [49] and for separation of control and data storage as suggested in <ref> [34] </ref>. In order to efficiently map disk sectors to capabilities, the exodisk system needs to maintain a cache of extent headers and exonodes within kernel memory. <p> Abstractions have been developed to allow these multiple storage management schemes to co-exist, 61 ranging from common interface definitions (for example, vnodes as described in [25]) to file systems that propose to incorporate them all (for example, <ref> [34, 49] </ref>).
Reference: [35] <author> Masataka Ohta and Hiroshi Tezuka. </author> <title> A fast /tmp file system by delay mount option. </title> <booktitle> In Proceedings of 1990 Summer USENIX, </booktitle> <pages> pages 145-150, </pages> <address> Anaheim, California, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Seltzer et al. show in [48], by comparing the 4.4BSD log-structured file system (which is based on ideas first explored in Sprite LFS [42, 44]) and the BSD fast file system [29, 41, 27], that the highest performing data organization is highly workload dependent. Temporary, memory-based file systems <ref> [30, 35] </ref> have been added to many systems because conventional file creation/deletion semantics are too heavy-weight for some applications, costing more than a factor of two in application performance [16]. <p> Furthermore, applications do not need to use the same persistence model for all files, but can use different persistence models for different files. The concept of allowing different files to have different integrity and persistence characteristics is not new. For example, temporary and memory-based file systems <ref> [35, 30] </ref> were added to systems so that files that are known to be short-lived do not have to suffer the high cost of file creation and deletion imposed by conventional file systems. <p> Examples are immediate files [33], temporary and memory-based file systems <ref> [35, 30] </ref>, log-structured file systems [42, 44, 46], clustered files [40, 31], semantic file systems [19], and stackable file systems [21].
Reference: [36] <author> John Ousterhout and Fred Douglis. </author> <title> Beating the I/O bottleneck: A case for log-structured file systems. </title> <journal> Operating Systems Review, </journal> <volume> 23(1) </volume> <pages> 11-28, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: At the same time, disk access times have only improved slowly <ref> [36, 38] </ref>. Disk I/O now presents a major bottleneck in many computer systems and will most likely continue to do so [43].
Reference: [37] <author> John K. Ousterhout, Herve Da Costa, David Harrison, John A. Kunze, Mike Kupfer, and James G. Thompson. </author> <title> A trace-driven analysis of the UNIX 4.2 BSD file system. </title> <booktitle> In Proceedings of the 10th Symposium on Operating Systems Principles, </booktitle> <pages> pages 15-24, </pages> <address> Orcas Island, Washington, </address> <month> December </month> <year> 1985. </year>
Reference-contexts: The experiments are summarized in Table 5.2. The above experiments are modeled after similar experiments used to evaluate the performance of the Sprite log-structured file system [44] and the Logical Disk [8]. The file sizes of 1 KByte and 10 KByte are also consistent with <ref> [1, 9, 23, 37] </ref> which show that roughly between 70% and 80% of all files in the measured Unix, Sprite and AFS file systems are less than 10 KByte in size. 45 5.2.2 File Organizations The following file organizations and interfaces are used in the small and large file I/O experiments.
Reference: [38] <author> David A. Patterson, Garth Gibson, and Randy H. Katz. </author> <title> A case for redundant arrays of inexpensive disks (RAID). </title> <booktitle> In Proceedings of SIGMOD, </booktitle> <pages> pages 109-116, </pages> <address> Chicago, Illinois, </address> <month> June </month> <year> 1988. </year> <month> 70 </month>
Reference-contexts: At the same time, disk access times have only improved slowly <ref> [36, 38] </ref>. Disk I/O now presents a major bottleneck in many computer systems and will most likely continue to do so [43]. <p> This approach can significantly improve performance in modern disk systems, which are characterized by (1) multiple disks disguised as a single disk (i.e., disk arrays <ref> [38] </ref>), or (2) disks that aggressively schedule requests based on information not available to the host system (e.g., both the seek time and rotational latency [47, 24]). The low-level interface provided by the exodisk system makes using disk-directed I/O trivial.
Reference: [39] <author> R. Hugo Patterson, Garth A. Gibson, Eka Ginting, Daniel Stodolsky, and Jim Zelenka. </author> <title> Informed prefetching and caching. </title> <booktitle> In Proceedings of the 15th Symposium on Operating Systems Principles, </booktitle> <pages> pages 79-95, </pages> <address> Copper Mountain Resort, Colorado, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Cao et al. [3, 2] show that application-level control over file caching, prefetching and disk scheduling policies can reduce average application running time by 46%. Patterson et al. <ref> [39] </ref> use application provided access patterns to dynamically regulate prefetching and caching, and show that application running time can be reduced by up to 42%. <p> Alternatively, a cost-benefit model in connection with application supplied hints as suggested by Patterson et al. in <ref> [39] </ref> can be used to regulate the caching and prefetching within the exodisk cache. While both mechanisms ensure better management of the shared cache and show good application performance, they clearly limit application control over the shared cache and impose complex in-kernel policies. <p> Aggressive prefetching based on application hints (e.g., <ref> [39] </ref>) is very similar in behavior; the application (or, more precisely, the file system) issues requests in advance and allows the disk system to service them out of order, but the application still processes them in order. <p> The performance difference should be considerably larger when using modern disk drives that internally implement very aggressive scheduling algorithms. Of course, this is a very simple example designed to illustrate the ease of using this approach with the exodisk system. Kotz [26] and Patterson et al. <ref> [39] </ref> much more thoroughly demonstrate the benefits of doing so. 5.4.3 Application-Specific Persistence Because applications can implement their own file abstractions (or choose from a library of pre-developed ones), varying persistence models can be easily implemented on top of 56 the exodisk system. <p> Recent work in application-controlled storage management has focused on allowing applications to direct caching and prefetching activity, either by giving applications direct control over caching and prefetching [3, 2], or by using a cost-benefit model in connection with application supplied hints <ref> [39] </ref>. This leads to significant performance improvements (e.g., over 40% reduction in the execution time of some applications), but addresses only one part of the spectrum of storage management issues.
Reference: [40] <author> J. Kent Peacock. </author> <title> The Counterpoint fast file system. </title> <booktitle> In Proceedings of 1988 Winter USENIX, </booktitle> <pages> pages 243-249, </pages> <address> Dallas, Texas, </address> <month> February </month> <year> 1988. </year>
Reference-contexts: Each entry either contains an extent to point to the corresponding physical data block, or is unused to indicate that the logical block is not currently allocated (entry 2). be referred to by direct pointers and larger data transfer sizes can be used <ref> [40, 31] </ref>. Since the size of exonodes is not fixed, it is possible to use i-nodes of different sizes and avoid the use of indirect i-nodes for medium-sized files. <p> Examples are immediate files [33], temporary and memory-based file systems [35, 30], log-structured file systems [42, 44, 46], clustered files <ref> [40, 31] </ref>, semantic file systems [19], and stackable file systems [21]. Abstractions have been developed to allow these multiple storage management schemes to co-exist, 61 ranging from common interface definitions (for example, vnodes as described in [25]) to file systems that propose to incorporate them all (for example, [34, 49]).
Reference: [41] <author> John S. Quarterman, Abraham Silberschatz, and James L. Peterson. </author> <title> 4.2BSD and 4.3BSD as examples of the UNIX system. </title> <journal> Computing Surveys, </journal> <volume> 17(4) </volume> <pages> 379-418, </pages> <year> 1985. </year>
Reference-contexts: Seltzer et al. show in [48], by comparing the 4.4BSD log-structured file system (which is based on ideas first explored in Sprite LFS [42, 44]) and the BSD fast file system <ref> [29, 41, 27] </ref>, that the highest performing data organization is highly workload dependent. Temporary, memory-based file systems [30, 35] have been added to many systems because conventional file creation/deletion semantics are too heavy-weight for some applications, costing more than a factor of two in application performance [16].
Reference: [42] <author> Mendel Rosenblum. </author> <title> The Design and Implementation of a Log-structured File System. </title> <type> PhD thesis, </type> <institution> University of California at Berkeley, </institution> <year> 1992. </year> <note> Also available as Technical Report UCB/CSD 92/696. </note>
Reference-contexts: Seltzer et al. show in [48], by comparing the 4.4BSD log-structured file system (which is based on ideas first explored in Sprite LFS <ref> [42, 44] </ref>) and the BSD fast file system [29, 41, 27], that the highest performing data organization is highly workload dependent. <p> Examples are immediate files [33], temporary and memory-based file systems [35, 30], log-structured file systems <ref> [42, 44, 46] </ref>, clustered files [40, 31], semantic file systems [19], and stackable file systems [21].
Reference: [43] <author> Mendel Rosenblum, Edouard Bugnion, Stephen Alan Herrod, Emmett Witchel, and Anoop Gupta. </author> <title> The impact of architectural trends on operating system performance. </title> <booktitle> In Proceedings of the 15th Symposium on Operating Systems Principles, </booktitle> <address> Copper Mountain Resort, Colorado, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: At the same time, disk access times have only improved slowly [36, 38]. Disk I/O now presents a major bottleneck in many computer systems and will most likely continue to do so <ref> [43] </ref>.
Reference: [44] <author> Mendel Rosenblum and John K. Ousterhout. </author> <title> The design and implementation of a log-structured file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(1) </volume> <pages> 26-52, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Seltzer et al. show in [48], by comparing the 4.4BSD log-structured file system (which is based on ideas first explored in Sprite LFS <ref> [42, 44] </ref>) and the BSD fast file system [29, 41, 27], that the highest performing data organization is highly workload dependent. <p> Descendant exonodes can be used to divide the logical name-space into several smaller units and to thus avoid having an overly large exonode. Log-structured file systems such as Sprite LFS <ref> [44] </ref> and BSD LFS [46] can use an exo-node with several fixed-size extent entries, where each extent represents a segment of the file system log. <p> The experiments are summarized in Table 5.2. The above experiments are modeled after similar experiments used to evaluate the performance of the Sprite log-structured file system <ref> [44] </ref> and the Logical Disk [8]. <p> Examples are immediate files [33], temporary and memory-based file systems [35, 30], log-structured file systems <ref> [42, 44, 46] </ref>, clustered files [40, 31], semantic file systems [19], and stackable file systems [21].
Reference: [45] <author> J. H. Saltzer, D. P. Reed, and D. D. Clark. </author> <title> End-to-end arguments in system design. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(4) </volume> <pages> 277-288, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: For example, an application, even if it knows the exact layout of data and metadata on disk, can not implement its own allocation policy, since a standard policy is usually hard-wired into the file system and can not be modified. The "end-to-end" argument <ref> [45] </ref> thus applies to disk storage management as well as low-level communication protocols: Applications know better than operating systems which storage abstractions they require and how these abstractions should be managed. The overriding goal of a storage system should thus be to maximize application control over storage management.
Reference: [46] <author> Margo Seltzer, Keith Bostic, Marshall Kirk McKusick, and Carl Staelin. </author> <title> An implementation of a log-structured file system for UNIX. </title> <booktitle> In Proceedings of 1993 Winter USENIX, </booktitle> <pages> pages 307-326, </pages> <address> San Diego, California, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: Descendant exonodes can be used to divide the logical name-space into several smaller units and to thus avoid having an overly large exonode. Log-structured file systems such as Sprite LFS [44] and BSD LFS <ref> [46] </ref> can use an exo-node with several fixed-size extent entries, where each extent represents a segment of the file system log. <p> Examples are immediate files [33], temporary and memory-based file systems [35, 30], log-structured file systems <ref> [42, 44, 46] </ref>, clustered files [40, 31], semantic file systems [19], and stackable file systems [21].
Reference: [47] <author> Margo Seltzer, Peter Chen, and John Ousterhout. </author> <title> Disk scheduling revisited. </title> <booktitle> In Proceedings of 1990 Summer USENIX, </booktitle> <pages> pages 313-324, </pages> <address> Washington, DC, </address> <month> January </month> <year> 1990. </year>
Reference-contexts: This approach can significantly improve performance in modern disk systems, which are characterized by (1) multiple disks disguised as a single disk (i.e., disk arrays [38]), or (2) disks that aggressively schedule requests based on information not available to the host system (e.g., both the seek time and rotational latency <ref> [47, 24] </ref>). The low-level interface provided by the exodisk system makes using disk-directed I/O trivial. One can simply initiate multiple requests, wait for notification of each request's completion and process it. To illustrate this, a very simple disk-directed I/O application is used.
Reference: [48] <author> Margo Seltzer, Keith A. Smith, Hari Balakrishnan, Jacqueline Chang, Sara McMains, and Venkata Padmanabhan. </author> <title> File system logging versus clustering: A performance comparison. </title> <booktitle> In Proceedings of the 1995 USENIX Technical Conference, </booktitle> <pages> pages 325, 249-264, </pages> <address> New Orleans, Louisiana, </address> <month> January </month> <year> 1995. </year> <month> 71 </month>
Reference-contexts: Patterson et al. [39] use application provided access patterns to dynamically regulate prefetching and caching, and show that application running time can be reduced by up to 42%. Seltzer et al. show in <ref> [48] </ref>, by comparing the 4.4BSD log-structured file system (which is based on ideas first explored in Sprite LFS [42, 44]) and the BSD fast file system [29, 41, 27], that the highest performing data organization is highly workload dependent.
Reference: [49] <author> Raymie Stata. </author> <title> File systems with multiple file implementations. </title> <type> Master's thesis, </type> <institution> Mas--sachusetts Institute of Technology, </institution> <month> February </month> <year> 1992. </year> <note> Also available as Technical Report MIT/LCS/TR-528. </note>
Reference-contexts: It is therefore possible to efficiently interleave several file systems on disk as well as to stripe over several disks, thus allowing for multiple file implementations as suggested in <ref> [49] </ref> and for separation of control and data storage as suggested in [34]. In order to efficiently map disk sectors to capabilities, the exodisk system needs to maintain a cache of extent headers and exonodes within kernel memory. <p> Abstractions have been developed to allow these multiple storage management schemes to co-exist, 61 ranging from common interface definitions (for example, vnodes as described in [25]) to file systems that propose to incorporate them all (for example, <ref> [34, 49] </ref>).
Reference: [50] <author> Michael Stonebraker. </author> <title> Operating system support for database management. </title> <booktitle> Readings in Database Systems, </booktitle> <pages> pages 167-173, </pages> <year> 1988. </year>
Reference-contexts: Research on disk storage management provides ample evidence that disk access times can greatly benefit from application-specific resource management: Stonebraker <ref> [50] </ref> argues that inappropriate file system implementations can have considerable impact on the performance of database managers. Cao et al. [3, 2] show that application-level control over file caching, prefetching and disk scheduling policies can reduce average application running time by 46%. <p> Since the metadata of all storage system layers can be stored in close proximity, it is possible to avoid maintaining several separate tree-like mappings from disk storage to metadata as described by Stonebraker <ref> [50] </ref>. Exonodes also support file system optimizations such as immediate files [33] which store the data of small files in the i-node. Exonodes support references to descendant exonodes, which are protected by their own, possibly different, capabilities. <p> The database indices, the file system metadata, and the exodisk system protection information all represent a mapping from disk storage to metadata, each associated with different layers of storage management. However, maintaining these mappings in separate disk locations may have a large impact on database performance <ref> [50] </ref>. The exodisk system presents an elegant solution to this problem, since database managers can use application-defined data entries in exonodes to store the database index together with the file system and exodisk system metadata. All of the above examples avoid dependencies on fixed partitions. <p> While this method avoids the high overhead of copying the entire file, it forces the application to replicate functionality already present in the file system and may result in unfavorable performance since both mappings are maintained separately <ref> [50] </ref>. In contrast, the exodisk system allows applications to define and manipulate the bottom layer of metadata. One can exploit this freedom to efficiently implement a file insert operation.
Reference: [51] <author> Andrew S. Tanenbaum. </author> <title> Modern Operating Systems. </title> <publisher> Prentice-Hall Inc., </publisher> <address> Englewood Cliffs, N.J. 07632, </address> <year> 1992. </year>
Reference-contexts: The MS-DOS file system does not use i-nodes to organize disk storage but rather uses a so-called File Allocation Table (FAT) to map disk blocks to successors <ref> [51] </ref>. The resulting singly-linked lists indicate the blocks and the order of blocks belonging to a particular file while the file itself is simply represented by the initial index into the FAT.
Reference: [52] <author> John Wilkes, Richard Golding, Carl Staelin, and Tim Sullivan. </author> <title> The HP AutoRAID hierarchical storage system. </title> <booktitle> In Proceedings of the 15th Symposium on Operating Systems Principles, </booktitle> <pages> pages 96-108, </pages> <address> Copper Mountain Resort, Colorado, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Loge [12], Mime [7], the Logical Disk [8, 20], the HP AutoRAID System <ref> [52] </ref> and Network-Attached Secure Disks (NASD) [17, 18] all use a logical representation of disk storage through a level of indirection.
Reference: [53] <author> Bruce L. Worthington, Gregory R. Ganger, and Yale N. Patt. </author> <title> Scheduling algorithms for modern disk drives. </title> <booktitle> In Proceedings of the 1994 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 241-251, </pages> <address> Nashville, Tennessee, </address> <month> May </month> <year> 1994. </year> <month> 72 </month>
Reference-contexts: As a result, a mechanism for scheduling disk operations is needed. The prototype exodisk system is implemented on top of a disk driver that uses the cyclical scan algorithm (C-LOOK), which always schedules requests in ascending order and achieves very good performance 24 when compared to other seek-reducing algorithms <ref> [53] </ref>. Since the queue of disk operations is visible to applications, they can use this information to determine when to initiate disk operations. Future implementations may replace this global default with mechanisms for allowing application-control over disk scheduling.
References-found: 53

