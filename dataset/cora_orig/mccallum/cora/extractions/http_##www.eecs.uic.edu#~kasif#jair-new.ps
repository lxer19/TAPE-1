URL: http://www.eecs.uic.edu/~kasif/jair-new.ps
Refering-URL: http://www.eecs.uic.edu/~kasif/learn-research.html
Root-URL: 
Email: dpd@cs.princeton.edu  trux@cs.jhu.edu  kasif@cs.jhu.edu  salzberg@cs.jhu.edu  
Title: Induction of shallow decision trees  
Author: David Dobkin Truxton Fulton Simon Kasif Steven Salzberg 
Web: gunopulo@mpi-sb.mpg.de  
Note: Dimitrios Gunopulos 2  
Abstract: In this paper we describe efficient algorithms that induce shallow (i.e., low depth) decision trees. A key feature of these algorithms is their ability to induce decision trees over real-valued data that have multiple branches at each node (in contrast to algorithms that use binary splits). As a special case, we describe efficient algorithms for computing the optimal partitioning of one dimensional data. These algorithms can be used to discretize numerical datasets either for decision trees or other machine learning methods. We examine the empirical performance of our algorithms on several benchmarks. Several of the algorithms can be shown theoretically to converge quickly to the best fixed-depth decision tree. We also present algorithms to learn "exact" decision trees of minimal depth that correctly classify all training examples. This topic opens up some interesting possibilities for detecting simple patterns in data mining applications.
Abstract-found: 1
Intro-found: 1
Reference: [AHM95] <author> P. Auer, R. Holte, and W. Maass. </author> <title> Theory and applications of agnostic pac-learning with small decision trees. </title> <booktitle> In Proc. of the Twelfth Internatl. Conf. on Machine Learning, </booktitle> <pages> pages 21-29, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: This analysis is directly inspired by the ideas in <ref> [AHM95] </ref>. <p> See also Auer et al. <ref> [AHM95] </ref>.) Using that result, we can now prove the following theorem. <p> Thus, when the number of "errors" allowed on the training set is very small (i.e., a constant), this algorithm is asymptotically faster than the algorithm for depth two decision trees reported in <ref> [AHM95] </ref> whose running time is O (d 2 K 2 N log N ). The algorithm takes as input the values of K and *. <p> In particular, our algorithm improves on the running time of the previous agnostic PAC algorithm for depth two decision trees with a constant number of leaves. The motivation for computing low-depth decision trees that are agnostic PAC-learners was first given in <ref> [AHM95] </ref>. 4.1 Agnostic Learning and Decision Trees In this section we consider a specific hypothesis class, beginning with the classes T (1; K) and T (2; K). A decision tree in T (1; K) is simply a K-way split in one dimension, as defined in section 3. <p> A decision tree T is in T (2; K) if it has a binary split at the root node, and a K-way split at each of the nodes just below the root. Auer, Holte and Maass <ref> [AHM95] </ref> were the first to give an algorithm for T (2; K) along with some theoretical results. Their algorithm runs in O (d 2 K 2 N log N ) time. <p> In addition they showed that the VC-dimensions (see appendix B) of the hypothesis classes T (1; K) and T (2; K) are finite. We give this result in the following lemma. Lemma 3 <ref> [AHM95] </ref> The VC-dimension of the hypothesis classes T (D; K) in d dimensions, for constant D and K, is fi (log d). <p> Our agnostic PAC algorithm for depth-two decision trees (2K leaves) improves (in some cases) on the running time of the previous agnostic PAC algorithm. These algorithms have been shown to work well for a number of benchmarks in the Irvine database <ref> [AHM95, FKS95] </ref>. One of the reasons for using strictly binary trees in the past has been that they can be constructed efficiently. The algorithms presented here allow one to consider other types of trees without incurring excessive cost.
Reference: [BC85] <author> J.L. Bates and R.L. Constable. </author> <title> Proofs as programs. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7 </volume> <pages> 113-136, </pages> <year> 1985. </year>
Reference-contexts: In the next section we develop a linear-time algorithm to find the best K-split of N examples. This algorithm runs much faster, but is restricted to relatively simple impurity measures. We note that <ref> [BC85] </ref> gave a similar algorithm, though not in the context of classification algorithms, that can be modified to compute the optimal 3-split. 3.1.2 Linear Time Algorithm for 3-Splits First we review the standard binary split algorithm as it applies to Sum Minority, beginning with the computation of the best partition into
Reference: [BFOS84] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth Internatl. Group, </publisher> <year> 1984. </year>
Reference-contexts: exists a low depth decision tree that fits a data set exactly. 2 Definitions and Summary of Results Decision trees have been established to be a very useful tool in pattern recognition and machine learning, where they have been used for classification, diagnosis, and other tasks (for examples see, e.g., <ref> [Qui93, Cho91, BFOS84] </ref>). Geometrically, a decision tree corresponds to a recursive partitioning of instance space that divides the space into mutually disjoint regions. If the instances have d attributes, then the regions are d-dimensional hyperrectangles. <p> This notion can be naturally extended to multi-class classification. When constructing decision trees, most algorithms attempt to produce accurate and concise (small) trees. The standard approach is to build trees top down, recursively splitting the examples using tests on the attributes <ref> [Qui93, BFOS84] </ref>. The splitting stops when all the remaining examples belong to the same class. For examples with real-valued attributes, the decision tree algorithm considers splitting the examples into two groups, and picks the best "cut point" available. <p> In some cases the multi-split algorithm leads to much smaller trees than are found by binary decision tree methods such as C4.5 [Qui93] or CART <ref> [BFOS84] </ref>. * We present algorithms that find decision trees of constant depth that are optimal at minimizing training set error. * We provide a theoretical analysis of one of our algorithms for induction of shallow trees that provably converge (as the sample size is increased) efficiently (with a sub-exponential convergence rate) <p> For a related discussion see [DKS95, Maa94, ER96]. Our dynamic programming schema easily extends to most other impurity measures, including information gain [Qui93] and the Gini measure <ref> [BFOS84] </ref>. This dynamic programming approach works for any additive or linear scoring function; in fact, it is applicable to any function that assigns a value to a set of examples and preserves the standard dynamic programmign optimality criteria. <p> As a special case it works for information gain [Qui93], the twoing rule, and the Gini measure <ref> [BFOS84] </ref>. The algorithm is also simple to explain. However, it is less efficient (in the worst case) than the algorithms in the following sections, which are linear in N . The details of the algorithm are given in Appendix A. <p> This rule can be viewed as an extension of Breiman et al.'s Twoing Rule <ref> [BFOS84] </ref>. By analogy, we call this the K-ing Rule: this will attempt to split the examples into equal-size partitions that are as pure as possible, with a penalty term associated with larger values of K. <p> With the real data, we were testing the hypothesis that the examples in some real world domains can be modeled more accurately by a multi-way splitting method. For comparison, we considered a number of binary-split decision tree induction algorithms. C4.5 [Qui93] and CART <ref> [BFOS84] </ref> differ primarily in that they use different impurity (or goodness) measures: information gain for C4.5 and the GINI measure for CART. (They also differ in their pruning methods and in other ways; see the references for details.) OC1 [MKS94] produces binary trees but allows multi-variate tests; i.e., the test at
Reference: [BN92] <author> W. Buntine and T. Niblett. </author> <title> A further comparison of splitting rules for decision-tree induction. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 75-85, </pages> <year> 1992. </year>
Reference-contexts: Cancer diagnosis and liver disease: [BU95]. Cancer recurrence: [WK89]. Voting: <ref> [BN92] </ref>. 17 decision tree methods. For the voting data, it did just as well, and for the cancer recurrence data, it performed somewhat worse. This indicates that for some (but certainly not all) real domains, there may be advantages to looking explicitly for multi-way splits.
Reference: [BU95] <author> Carla E. Brodley and Paul E. Utgoff. </author> <title> Multivariate decision trees. </title> <journal> Machine Learning, </journal> <volume> 19(1) </volume> <pages> 45-78, </pages> <year> 1995. </year>
Reference-contexts: Cancer diagnosis and liver disease: <ref> [BU95] </ref>. Cancer recurrence: [WK89]. Voting: [BN92]. 17 decision tree methods. For the voting data, it did just as well, and for the cancer recurrence data, it performed somewhat worse. This indicates that for some (but certainly not all) real domains, there may be advantages to looking explicitly for multi-way splits.
Reference: [Cho91] <author> Philip Chou. </author> <title> Optimal partitioning for classification and regression trees. </title> <journal> IEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(4) </volume> <pages> 340-354, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: exists a low depth decision tree that fits a data set exactly. 2 Definitions and Summary of Results Decision trees have been established to be a very useful tool in pattern recognition and machine learning, where they have been used for classification, diagnosis, and other tasks (for examples see, e.g., <ref> [Qui93, Cho91, BFOS84] </ref>). Geometrically, a decision tree corresponds to a recursive partitioning of instance space that divides the space into mutually disjoint regions. If the instances have d attributes, then the regions are d-dimensional hyperrectangles.
Reference: [DGM95] <author> D. Dobkin, D. Gunopulos, and W. Maass. </author> <title> Computing the maximum bichromatic discrepancy, with applications in computer graphics and machine learning. </title> <note> Journal of Computer Systems Sciences (to appear), </note> <year> 1995. </year>
Reference-contexts: The first step in the development of an efficient algorithm for depth-2 trees is the linear time K-split algorithm given in Theorem 2. (Dobkin et al. <ref> [DGM95] </ref> give a dynamic algorithm of O (K 2 log N ) running time per update to solve the same problem. See also Auer et al. [AHM95].) Using that result, we can now prove the following theorem.
Reference: [DKS95] <author> J. Dougherty, R. Kohavi, and M. Sahami. </author> <title> Supervised and unsupervised discretiza-tion of continuous attributes. </title> <booktitle> In Proc. of the Twelfth Internatl. Conf. on Machine Learning, </booktitle> <pages> pages 194-202, </pages> <address> San Mateo, CA, July 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In particular, we describe simple and linear time algorithms for answering the query: is there a depth two or three decision tree that fits the data exactly? 3 Trees with K -Split Nodes Many recent papers have examined the use of decision trees for real-valued data <ref> [HKS93, MKS94, dM92, dM93, FI92, DKS95] </ref>. The standard approach to handling continuous values is to sort all the values according to each attribute i, and then consider the "cut points" between each successive pair of points in the sorted list. <p> Our implementation also considers splits of the form BA, BAB, etc., which incurs a factor of two in additional computation. 5 3.1.1 Quadratic time multi-split algorithm We begin with a dynamic programming approach that runs in O (KN 2 ) time. For a related discussion see <ref> [DKS95, Maa94, ER96] </ref>. Our dynamic programming schema easily extends to most other impurity measures, including information gain [Qui93] and the Gini measure [BFOS84].
Reference: [dM92] <author> T. Van de Merckt. Nfdt: </author> <title> A system that learns flexible concepts based on decision trees for numerical attributes. </title> <booktitle> In Proc. Ninth Internatl. Conf. on Machine Learning, </booktitle> <pages> pages 322-331, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 28 </pages>
Reference-contexts: In particular, we describe simple and linear time algorithms for answering the query: is there a depth two or three decision tree that fits the data exactly? 3 Trees with K -Split Nodes Many recent papers have examined the use of decision trees for real-valued data <ref> [HKS93, MKS94, dM92, dM93, FI92, DKS95] </ref>. The standard approach to handling continuous values is to sort all the values according to each attribute i, and then consider the "cut points" between each successive pair of points in the sorted list.
Reference: [dM93] <author> T. Van de Merckt. </author> <title> Decision trees in numerical attribute spaces. </title> <booktitle> In Proc. of the 13th Internatl. Joint Conf. on Artificial Intelligence, </booktitle> <pages> pages 1016-1021, </pages> <address> San Mateo, CA, August 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In particular, we describe simple and linear time algorithms for answering the query: is there a depth two or three decision tree that fits the data exactly? 3 Trees with K -Split Nodes Many recent papers have examined the use of decision trees for real-valued data <ref> [HKS93, MKS94, dM92, dM93, FI92, DKS95] </ref>. The standard approach to handling continuous values is to sort all the values according to each attribute i, and then consider the "cut points" between each successive pair of points in the sorted list.
Reference: [ER96] <author> T. Elomaa and J. Rousu. </author> <title> Finding optimal multi-splits for numerical attributes in decision tree learning. </title> <type> Technical report, NeuroCOLT Technical Report NC-TR-96-041, </type> <institution> Royal Holloway, University of London, </institution> <year> 1996. </year>
Reference-contexts: Our implementation also considers splits of the form BA, BAB, etc., which incurs a factor of two in additional computation. 5 3.1.1 Quadratic time multi-split algorithm We begin with a dynamic programming approach that runs in O (KN 2 ) time. For a related discussion see <ref> [DKS95, Maa94, ER96] </ref>. Our dynamic programming schema easily extends to most other impurity measures, including information gain [Qui93] and the Gini measure [BFOS84]. <p> KS-2 uses time O (KdN 2 ) per node of the tree; thus it can be slow for large datasets. Note that using the analysis provided in <ref> [FI93, ER96] </ref> we can apply our schema only at alternations which should substantially speed up the algorithm. We use a modified info gain measure which has the effect of penalizing larger values of K. <p> An interesting open question is whether multi-split trees using this measure can be constructed in linear (as opposed to quadratic) time. We also note that for most impurity measures it is sufficient to consider only the boundary points (alternations) as candidate splits. This usually yields better performance. In <ref> [ER96] </ref> it is formally shown that our dynamic programming algorithm can be applied at boundary points only for information gain and other commonly used impurity measures. This result follows on previous work along these lines by Fayyad [FI92, FI93].
Reference: [FI92] <author> U. M. Fayyad and K. B. Irani. </author> <title> On the handling of continuous-valued attributes in decision tree generation. </title> <journal> Machine Learning, </journal> <volume> 8(2) </volume> <pages> 87-102, </pages> <year> 1992. </year>
Reference-contexts: In particular, we describe simple and linear time algorithms for answering the query: is there a depth two or three decision tree that fits the data exactly? 3 Trees with K -Split Nodes Many recent papers have examined the use of decision trees for real-valued data <ref> [HKS93, MKS94, dM92, dM93, FI92, DKS95] </ref>. The standard approach to handling continuous values is to sort all the values according to each attribute i, and then consider the "cut points" between each successive pair of points in the sorted list. <p> The best of all such splits is used as the test for the current node of the decision tree. Thus the binary split made by most methods is locally optimal. In order to speed up decision tree algorithms, Fayyad and Irani <ref> [FI92, FI93] </ref> looked at what they called boundaries along each dimension. <p> This usually yields better performance. In [ER96] it is formally shown that our dynamic programming algorithm can be applied at boundary points only for information gain and other commonly used impurity measures. This result follows on previous work along these lines by Fayyad <ref> [FI92, FI93] </ref>. The experiments described here demonstrate that there exist artificial distributions for which the K-split algorithm produces much smaller trees than a standard algorithm. They show also that the K-split algorithm works well on several naturally occurring datasets.
Reference: [FI93] <author> U. M. Fayyad and K. B. Irani. </author> <title> Multi-interval discretization of continuous valued attributes for classification learning. </title> <booktitle> In Proc. 13th Internatl. Joint Conf. on Artificial Intelligence, </booktitle> <pages> pages 1022-1027, </pages> <address> Chambery, France, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The best of all such splits is used as the test for the current node of the decision tree. Thus the binary split made by most methods is locally optimal. In order to speed up decision tree algorithms, Fayyad and Irani <ref> [FI92, FI93] </ref> looked at what they called boundaries along each dimension. <p> Searching for such multi-way splits is a natural extension to binary decision tree algorithms, but considering all possible K-splits would be computationally very expensive. Although methods have been proposed to look at successive binary splits of the same dimension <ref> [FI93] </ref>, there was previously no efficient algorithm that directly compared the impurity of, e.g., the optimal 3-way split to 4 As a result, the majority of the complexity results in this paper can be restated in terms of the number of alternations. <p> KS-2 uses time O (KdN 2 ) per node of the tree; thus it can be slow for large datasets. Note that using the analysis provided in <ref> [FI93, ER96] </ref> we can apply our schema only at alternations which should substantially speed up the algorithm. We use a modified info gain measure which has the effect of penalizing larger values of K. <p> This usually yields better performance. In [ER96] it is formally shown that our dynamic programming algorithm can be applied at boundary points only for information gain and other commonly used impurity measures. This result follows on previous work along these lines by Fayyad <ref> [FI92, FI93] </ref>. The experiments described here demonstrate that there exist artificial distributions for which the K-split algorithm produces much smaller trees than a standard algorithm. They show also that the K-split algorithm works well on several naturally occurring datasets.
Reference: [FKS95] <author> T. Fulton, S. Kasif, and S. Salzberg. </author> <title> Efficient algorithms for finding multi-way splits for decision trees. </title> <booktitle> In Proc. of the Twelfth Internatl. Conf. on Machine Learning, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: We continue for K passes (for any K &lt; n), each time incurring only linear cost. At the end, we have the optimal K-way multi-split for all K. This algorithm is described below in Section 3.1 (see also <ref> [FKS95] </ref>). Clearly one could model a K-split with a succession of binary splits. However, an algorithm that considers binary splits one at a time will not necessarily discover a K-split, even though the K-way split might lead to a much smaller tree overall. <p> Because we have not yet investigated pruning methods for our k-split algorithms, we also ran some comparisons against unpruned trees produced by these methods. 5.3 Artificial data We ran experiments on numerous artificial datasets <ref> [FKS95] </ref>; for space considerations, we present just two of those datasets here. Both data sets were constructed to illustrate distributions for which a multiple-split capability should benefit a tree-building algorithm. We use 2-D data in order to illustrate pictorially how our algorithm classifies the data. <p> Our agnostic PAC algorithm for depth-two decision trees (2K leaves) improves (in some cases) on the running time of the previous agnostic PAC algorithm. These algorithms have been shown to work well for a number of benchmarks in the Irvine database <ref> [AHM95, FKS95] </ref>. One of the reasons for using strictly binary trees in the past has been that they can be constructed efficiently. The algorithms presented here allow one to consider other types of trees without incurring excessive cost.
Reference: [Hau92] <author> David Haussler. </author> <title> Decision theoretic generations of the PAC-model for neural nets and other applications. </title> <journal> Information and Computation, </journal> <volume> 100 </volume> <pages> 78-150, </pages> <year> 1992. </year>
Reference-contexts: B.2 Agnostic PAC-Learning The shortcoming of the original PAC-learning model of Valiant is that it relies on the assumption that the labels of the training examples arise from a target concept with an a priori known specific simple structure, an assumption rarely met in practice. Haussler in <ref> [Hau92] </ref> provided an important link between computational learning theory and the fields of non-parametric statistics and applied machine learning, when he introduced a variation of PAC-learning. <p> The empirical error measures how well a hypothesis predicts D for the given training sequence. The following two uniform convergence results provide a connection (see <ref> [Hau92] </ref>) between the required size of the training sequence and the difference of true and empirical errors. They say that we can bound the difference between the true error and the empirical error (with high probability) if we pick a large enough training sequence at random. <p> least m (*; ffi) examples drawn with regard to D, the following holds with probability at least 1 ffi: 8H 2 H; jError T (H) Error D (H)j * The following recent result by Talagrand ([Tal95], which holds under some rather weak measurability conditions, slightly improves a similar result in <ref> [Hau92] </ref>. This result applies if the VC-dimension of the set H, as it was defined by Vapnik and Chervonenkis ([VC71] is finite. K is some absolute constant that is conjectured to be not larger than 1000.
Reference: [HKS93] <author> D. Heath, S. Kasif, and S. Salzberg. </author> <title> Learning oblique decision trees. </title> <booktitle> In Proc. of the 13th Internatl. Joint Conf. on Artificial Intelligence, </booktitle> <pages> pages 1002-1007, </pages> <address> Chambery, France, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In particular, we describe simple and linear time algorithms for answering the query: is there a depth two or three decision tree that fits the data exactly? 3 Trees with K -Split Nodes Many recent papers have examined the use of decision trees for real-valued data <ref> [HKS93, MKS94, dM92, dM93, FI92, DKS95] </ref>. The standard approach to handling continuous values is to sort all the values according to each attribute i, and then consider the "cut points" between each successive pair of points in the sorted list.
Reference: [Hol93] <author> R. Holte. </author> <title> Very simple classification rules perform well on most commonly used datasets. </title> <journal> Machine Learning, </journal> <volume> 11(1) </volume> <pages> 63-90, </pages> <year> 1993. </year>
Reference-contexts: A decision tree in T (1; K) is simply a K-way split in one dimension, as defined in section 3. This class of hypotheses was first introduced by Holte <ref> [Hol93] </ref>, who showed that in many domains, even in data sets containing more than 10 dimensions, there exists one dimension that fairly accurately describes the concept.
Reference: [KSS92] <author> M. Kearns, R.E. Schapire, and L.M. Sellie. </author> <title> Toward efficient agnostic learning. </title> <booktitle> In ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 341-352, </pages> <year> 1992. </year>
Reference: [Maa94] <author> W. Maass. </author> <title> Efficient agnostic pac-learning with simple hypotheses. </title> <booktitle> In Proc. of the Seventh ACM Conf. on Computational Learning Theory, </booktitle> <pages> pages 67-75, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: Our implementation also considers splits of the form BA, BAB, etc., which incurs a factor of two in additional computation. 5 3.1.1 Quadratic time multi-split algorithm We begin with a dynamic programming approach that runs in O (KN 2 ) time. For a related discussion see <ref> [DKS95, Maa94, ER96] </ref>. Our dynamic programming schema easily extends to most other impurity measures, including information gain [Qui93] and the Gini measure [BFOS84].
Reference: [MKS94] <author> Sreerama K. Murthy, Simon Kasif, and Steven Salzberg. </author> <title> A system for induction of oblique decision trees. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 1-33, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: In particular, we describe simple and linear time algorithms for answering the query: is there a depth two or three decision tree that fits the data exactly? 3 Trees with K -Split Nodes Many recent papers have examined the use of decision trees for real-valued data <ref> [HKS93, MKS94, dM92, dM93, FI92, DKS95] </ref>. The standard approach to handling continuous values is to sort all the values according to each attribute i, and then consider the "cut points" between each successive pair of points in the sorted list. <p> C4.5 [Qui93] and CART [BFOS84] differ primarily in that they use different impurity (or goodness) measures: information gain for C4.5 and the GINI measure for CART. (They also differ in their pruning methods and in other ways; see the references for details.) OC1 <ref> [MKS94] </ref> produces binary trees but allows multi-variate tests; i.e., the test at each node can be a linear combination of any number of attributes. OC1 can also be set to use strictly 14 univariate tests, and can use any of six built-in impurity measures, including information gain. <p> Allowing pruning helped only slightly: it still generated 115 leaf nodes, and accuracy went down. On the right side of the figure, we show the tree generated OC1-AP using sum-minority. 15 (To be fair to the creators of OC1 <ref> [MKS94] </ref>, OC1-AP is not at all similar to OC1. OC1-AP uses strictly axis-parallel, univariate splits, while OC1 considers oblique, multivariate splits.
Reference: [MS95a] <author> S.K. Murthy and S. Salzberg. </author> <title> Decision tree induction: How effective is the greedy heuristic? In Proc. </title> <booktitle> First Internatl. Conf. on Knowledge Discovery and Data Mining (KDD-95), </booktitle> <address> Montreal, Canada, August 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: An important problem we have not considered is the statistical motivation for computing the smallest decision tree for a given training set. It has been shown by Quinlan and Cameron-Jones [QCJ95] and in related work by Murthy and Salzberg <ref> [MS95b, MS95a] </ref> that "oversearching" can in fact hurt the accuracy of practical learning algorithm. We are actively 23 investigating this subtle problem in the context of the bias/variance trade-off and other issues. Acknowledgements This material is based upon work supported in part by the National Science foundation under Grant Nos.
Reference: [MS95b] <author> S.K. Murthy and S. Salzberg. </author> <title> On lookahead for decision tree induction. </title> <booktitle> In Proc. 14th Internatl. Joint Conf. on Artificial Intelligence (IJCAI-95), </booktitle> <pages> pages 1025-1031. </pages> <publisher> Morgan Kaufmann, </publisher> <month> August </month> <year> 1995. </year> <month> 29 </month>
Reference-contexts: An important problem we have not considered is the statistical motivation for computing the smallest decision tree for a given training set. It has been shown by Quinlan and Cameron-Jones [QCJ95] and in related work by Murthy and Salzberg <ref> [MS95b, MS95a] </ref> that "oversearching" can in fact hurt the accuracy of practical learning algorithm. We are actively 23 investigating this subtle problem in the context of the bias/variance trade-off and other issues. Acknowledgements This material is based upon work supported in part by the National Science foundation under Grant Nos.
Reference: [QCJ95] <author> J.R. Quinlan and R.M. Cameron-Jones. </author> <title> Oversearching and layered search in empirical learning. </title> <booktitle> In Proc. 14th Internatl. Joint Conf. on Artificial Intelligence (IJCAI-95), </booktitle> <pages> pages 1019-1024. </pages> <publisher> Morgan Kaufmann, </publisher> <month> August </month> <year> 1995. </year>
Reference-contexts: An important problem we have not considered is the statistical motivation for computing the smallest decision tree for a given training set. It has been shown by Quinlan and Cameron-Jones <ref> [QCJ95] </ref> and in related work by Murthy and Salzberg [MS95b, MS95a] that "oversearching" can in fact hurt the accuracy of practical learning algorithm. We are actively 23 investigating this subtle problem in the context of the bias/variance trade-off and other issues.
Reference: [Qui93] <author> J. R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: exists a low depth decision tree that fits a data set exactly. 2 Definitions and Summary of Results Decision trees have been established to be a very useful tool in pattern recognition and machine learning, where they have been used for classification, diagnosis, and other tasks (for examples see, e.g., <ref> [Qui93, Cho91, BFOS84] </ref>). Geometrically, a decision tree corresponds to a recursive partitioning of instance space that divides the space into mutually disjoint regions. If the instances have d attributes, then the regions are d-dimensional hyperrectangles. <p> This notion can be naturally extended to multi-class classification. When constructing decision trees, most algorithms attempt to produce accurate and concise (small) trees. The standard approach is to build trees top down, recursively splitting the examples using tests on the attributes <ref> [Qui93, BFOS84] </ref>. The splitting stops when all the remaining examples belong to the same class. For examples with real-valued attributes, the decision tree algorithm considers splitting the examples into two groups, and picks the best "cut point" available. <p> In some cases the multi-split algorithm leads to much smaller trees than are found by binary decision tree methods such as C4.5 <ref> [Qui93] </ref> or CART [BFOS84]. * We present algorithms that find decision trees of constant depth that are optimal at minimizing training set error. * We provide a theoretical analysis of one of our algorithms for induction of shallow trees that provably converge (as the sample size is increased) efficiently (with a <p> Each cut point splits the examples into two groups, and the split is scored by applying a goodness criterion (e.g., Quinlan's information gain or gain ratio <ref> [Qui93] </ref>). The best of all such splits is used as the test for the current node of the decision tree. Thus the binary split made by most methods is locally optimal. <p> They showed that this result applies to the C4.5 <ref> [Qui93] </ref> algorithm as well. Intuitively, this is the way one would like an impurity measure to behave, and their result shows that their entropy measure is well-behaved. It also allows us to ignore any splits that do not occur at boundaries. <p> For a related discussion see [DKS95, Maa94, ER96]. Our dynamic programming schema easily extends to most other impurity measures, including information gain <ref> [Qui93] </ref> and the Gini measure [BFOS84]. This dynamic programming approach works for any additive or linear scoring function; in fact, it is applicable to any function that assigns a value to a set of examples and preserves the standard dynamic programmign optimality criteria. <p> This dynamic programming approach works for any additive or linear scoring function; in fact, it is applicable to any function that assigns a value to a set of examples and preserves the standard dynamic programmign optimality criteria. As a special case it works for information gain <ref> [Qui93] </ref>, the twoing rule, and the Gini measure [BFOS84]. The algorithm is also simple to explain. However, it is less efficient (in the worst case) than the algorithms in the following sections, which are linear in N . The details of the algorithm are given in Appendix A. <p> Therefore, one must employ a penalty function to prevent KS-1 from simply dividing the N examples into N different intervals. For this implementation, we used Quinlan's Gain Ratio criterion <ref> [Qui93] </ref> to penalize larger values of K. This works very simply: after choosing the best multi-split for each value of K, the values are ranked according to their gain ratio score. <p> This works very simply: after choosing the best multi-split for each value of K, the values are ranked according to their gain ratio score. The second version, called KS-2, also computes all the optimal multi-split partitions up to K, but uses a modified version of Quinlan's information gain criterion <ref> [Qui93] </ref> as 13 its impurity measure. Information gain for multiple splits is the natural extension of the standard information gain for multiple intervals (e.g,, see Quinlan's original definition of information gain in ID3). <p> With the real data, we were testing the hypothesis that the examples in some real world domains can be modeled more accurately by a multi-way splitting method. For comparison, we considered a number of binary-split decision tree induction algorithms. C4.5 <ref> [Qui93] </ref> and CART [BFOS84] differ primarily in that they use different impurity (or goodness) measures: information gain for C4.5 and the GINI measure for CART. (They also differ in their pruning methods and in other ways; see the references for details.) OC1 [MKS94] produces binary trees but allows multi-variate tests; i.e.,
Reference: [Sur] <author> S. Suri. </author> <title> Private communication. </title>
Reference-contexts: Next we give a more complicated but asymptotically faster algorithm. This algorithm is an extension of an algorithm given by Subash Suri, for D = 2 <ref> [Sur] </ref>. The algorithm is essentially based on the following observations. Let us assume that we "visit" the rectangles defined by the leafs of a depth-2 binary decision tree in a clockwise fashion (starting from the north-east rectangle). During each such tour we record the classes associated with each rectangle.
Reference: [Tal95] <author> M. Talagrand. </author> <title> Sharper bounds for empirical processes. </title> <note> Annals of Probability and its Applications (to appear), </note> <year> 1995. </year>
Reference: [Val84] <author> L. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27 </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: In this paper we are concerned with the problem of learning an unknown concept with a hypothesis in the form of a decision tree. B.1 PAC-Learning In Valiant's <ref> [Val84] </ref> model for Probably Approximately Correct learning ("PAC-learning"), the learning algorithm has access to positive and negative examples of an unknown target concept C. The learning algorithm does however know that C is in a set of hypotheses H, and the learning algorithm knows this class H.
Reference: [VC71] <author> V. N. Vapnik and A. Ya. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its Applications, </title> <address> XVI(2):264-280, </address> <year> 1971. </year>
Reference: [WK89] <author> S. Weiss and I. Kapouleas. </author> <title> An empirical comparison of pattern recognition, neural nets, and machine learning classification methods. </title> <booktitle> In Proc. of the Internatl. Joint Conf. of Artificial Intelligence, </booktitle> <pages> pages 781-787, </pages> <address> Detroit, MI, 1989. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 30 </pages>
Reference-contexts: Cancer diagnosis and liver disease: [BU95]. Cancer recurrence: <ref> [WK89] </ref>. Voting: [BN92]. 17 decision tree methods. For the voting data, it did just as well, and for the cancer recurrence data, it performed somewhat worse. This indicates that for some (but certainly not all) real domains, there may be advantages to looking explicitly for multi-way splits.
References-found: 29

