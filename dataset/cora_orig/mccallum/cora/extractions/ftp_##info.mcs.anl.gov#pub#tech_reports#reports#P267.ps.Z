URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P267.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts91.htm
Root-URL: http://www.mcs.anl.gov
Title: On the Calculation of Jacobian Matrices by the Markowitz Rule for Vertex Elimination  
Author: Andreas Griewank and Shawn Reese 
Date: October 1991  
Abstract: The evaluation of derivative vectors can be performed with optimal computational complexity by the forward or reverse mode of automatic differentiation. This approach may be applied to evaluate first and higher derivatives of any vector function that is defined as the composition of easily differentiated elementary functions, typically in the form of a computer program. The more general task of efficiently evaluating Jacobians or other derivative matrices leads to a combinatorial optimization problem, which is conjectured to be NP-hard. Here, we examine this vertex elimination problem and solve it approximately, using a greedy heuristic. Numerical experiments show the resulting Markowitz scheme for Jacobian evaluation to be more efficient than column by column or row by row evaluation using the forward or the reverse mode, respectively. 
Abstract-found: 1
Intro-found: 1
Reference: [Cole83a] <author> T. F. Coleman and J. J. </author> <title> More (1983) Estimation of sparse Jacobian matrices and graph coloring problems SIAM Journal on Numerical Analysis, </title> <journal> Vol. </journal> <volume> 16, </volume> <pages> pp. 368-375. </pages>
Reference-contexts: Under these reasonable assumptions we can thus conclude that in the sparse forward mode CostfJ (x)g=Costff (x)g 3 ^n : (12) This upper bound is still conservative, but much more competitive with sophisticated differencing schemes, for example, the graph coloring approach developed by Coleman and More <ref> [Cole83a] </ref>. In fact, their grouping of columns into n ^n mutually independent sets can also be exploited in the forward mode, with each vector rv i being compressed to n components. <p> A potential drawback of sparse implementations is the need for indirect addressing, which may represent a serious obstacle to vectorization and parallelization. To avoid this effect, one may instead employ Coleman and More's technique <ref> [Cole83a] </ref> of grouping columns into n ^n mutually independent sets and then apply the forward mode with each vector rv i being compressed to n components.
Reference: [Dixo91a] <author> L. W. C. </author> <title> Dixon The use of automatic ifferentiation to calculate Hessian matrices and Newton steps </title>
Reference-contexts: Even if the Jacobian is not sparse, the sparse forward mode may work well, provided the evaluation is organized such that the intermediate gradients rOE j fill in only towards the very end. This effect has been documented by <ref> [Dixo91a] </ref> on the Helmholtz energy function. Thus, the sparse mode can be competitive with the following reverse mode, which yields gradients at a fixed cost relative to that of evaluating the underlying function but may require significantly more storage. <p> In cases like our test problem Bratu1 where the vector function f is the gradient of a scalar function, its evaluation can be programmed such that the corresponding computational graph is symmetric, as described by Dixon. <ref> [Dixo91a] </ref>. Then it would seem natural to maintain this symmetry during the elimination process for accumulating the Hessian or for directly calculating the Newton step. Jacobians by Markowitz Elimination 11
Reference: [Duff86a] <author> I. S. Duff, A. M. Erisman, and J. K. </author> <title> Reid (1986) Direct methods for sparse matrices Oxford Science Publications, </title> <publisher> Clarendon Press, Oxford. </publisher>
Reference-contexts: First, there is the overhead of determining an appropriate elimination ordering according to the Markowitz rule or some other heuristic. As in the case of sparse linear system solving <ref> [Duff86a] </ref>, this overhead cost is highly dependent on the implementation and can be substantially reduced by suitably relaxing the pure selection criterion. Fortunately, in contrast to the Gaussian elimination case, numerical stability is not a problem, since accumulating the Jacobian requires only multiplications and additions.
Reference: [Grie89a] <author> A. </author> <title> Griewank (1989) On automatic differentiation, in Mathematical Programming: Recent Developments and Applications, </title> <editor> ed. M. Iri and K. Tanabe, </editor> <publisher> Kluwer Academic Publishers, </publisher> <pages> pp. 83-108. </pages>
Reference: [Grie90c] <author> A. </author> <title> Griewank (1990) Direct Calculation of Newton Steps without Accumulating Jacobians in Large-Scale Numerical Optimization, </title> <editor> T. F. Coleman and Yuying Li, eds., </editor> <publisher> SIAM, </publisher> <pages> pp. 115-137. </pages>
Reference: [Grie90a] <author> A. Griewank, D. Juedes, and J. </author> <title> Srinivasan (1990) ADOL-C, a package for the automatic differentiation of algorithms written in C/C++, </title> <type> Preprint MCS-180-1190, </type> <institution> Argonne National Laboratory, Argonne, Illinois. </institution>
Reference-contexts: This separation between a moderately increased, randomly accessed memory and the potentially very large sequential tape has for example been implemented in the C++ package ADOL-C <ref> [Grie90a] </ref>. The RAM requirements of the dynamically sparse forward and reverse mode are at most ^n n and ^m m larger than that of the corresponding scalar sweeps. This increase in memory from the scalar to the sparse vector mode mirrors exactly the corresponding growth in arithmetic operations. <p> Eventually, such an approach could be modified to compute Newton steps directly without forming the Jacobian, as suggested in [Muro87b] and <ref> [Grie90a] </ref>. In cases like our test problem Bratu1 where the vector function f is the gradient of a scalar function, its evaluation can be programmed such that the corresponding computational graph is symmetric, as described by Dixon. [Dixo91a].
Reference: [Grie91c] <author> A. </author> <title> Griewank (1991) Automatic Evaluation of First- and Higher-Derivative Vectors, </title> <booktitle> Proceedings of the Conference at Wurzburg, </booktitle> <month> Aug. </month> <year> 1990, </year> <title> Bifurcation and Chaos: Analysis, Algorithms, Applications, </title> <editor> R. Seydel, F. W. Schneider,T. Kupper, and H. Troger, eds., </editor> <publisher> Basel, Birkhauser, </publisher> <pages> pp. 124-137. </pages>
Reference: [Irim91a] <author> M. </author> <title> Iri (1991) Automatic differentiation and rounding error estimation overview and history This volume pp. </title>
Reference-contexts: The mixed modes discussed in the next session are likely to require substantially more in core storage than the forward or reverse mode. This is certainly true for our first experimental implementation. 3 Jacobian Accumulation as Elimination of Intermediate Vertices. As discussed by Iri <ref> [Irim91a] </ref>, the relation between the variables v j can be visualized by a computational graph with the integer vertices 0 &lt; j n + p + m. We shall use the attributes of corresponding variables v j and vertices j interchangeably. <p> J :l!k i where the paths J are of the form J j fl; j 1 ; j 2 ; : : : ; j i ; : : : ; kg : (24) This relation was apparently first established by Miller and Wrathall [Mill80a] and is also derived in <ref> [Irim91a] </ref>. Clearly, a separate evaluation of each Jacobian entry by the determinant-like explicit formula (23) would be extremely expensive, as common expressions are ignored and there may be an exponential number of paths.
Reference: [Mill80a] <author> W. Miller and C. </author> <title> Wrathall (1980) Software for Roundoff Analysis of Matrix Algorithms, </title> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference-contexts: arc values as c kl j J :l!k i where the paths J are of the form J j fl; j 1 ; j 2 ; : : : ; j i ; : : : ; kg : (24) This relation was apparently first established by Miller and Wrathall <ref> [Mill80a] </ref> and is also derived in [Irim91a]. Clearly, a separate evaluation of each Jacobian entry by the determinant-like explicit formula (23) would be extremely expensive, as common expressions are ignored and there may be an exponential number of paths.
Reference: [Muro87b] <author> K. </author> <title> Murota (1987) Menger-decomposition of a graph and its application to the structural analysis of a large-scale system of equations. </title> <journal> Discrete Applied Mathematics, </journal> <volume> Vol. </volume> <pages> 17 , pp. 107-134. </pages>
Reference-contexts: Eventually, such an approach could be modified to compute Newton steps directly without forming the Jacobian, as suggested in <ref> [Muro87b] </ref> and [Grie90a]. In cases like our test problem Bratu1 where the vector function f is the gradient of a scalar function, its evaluation can be programmed such that the corresponding computational graph is symmetric, as described by Dixon. [Dixo91a].
Reference: [Rose78a] <author> D. J. Rose and R. E. </author> <title> Tarjan (1978) Algorithmic aspects of vertex elimination on directed graphs. </title> <editor> SIAM J. A. M., </editor> <volume> Vol. 34, </volume> <pages> pp. 177-197. </pages>
Reference-contexts: Thus it would seem natural to look for an elimination ordering that minimizes this sum of Markowitz counts. Unfortunately, it appears that the exact solution of this combinatorial optimization problem is NP-hard. This conjecture is based on the close relationship to the Gaussian elimination problem considered in <ref> [Rose78a] </ref>. As is typical in combinatorial optimization, we may have to settle for a heuristic algorithm that is comparatively cheap to implement and that yields, in many cases, nearly optimal results. Here, the obvious greedy strategy is to eliminate a vertex whose Markowitz count is minimal at the current stage.
Reference: [Yosh87a] <author> T. </author> <title> Yoshida (1987). Derivation of a Computational Process for Partial Derivatives of Functions Using Transformations of a Graph. </title> <journal> Transactions of IPSJ, </journal> <volume> Vol. 11, </volume> <pages> pp. 1112-1120. </pages>
Reference-contexts: Hence, we may try to calculate the Jacobian matrix associated with a general linear dependency graph by successively eliminating all its intermediate vertices without altering the input-output characteristic Jacobians by Markowitz Elimination 7 between x 0 and y 0 . This idea was apparently first published in <ref> [Yosh87a] </ref>. Suppose we wish to eliminate one particular intermediate vertex j from the graph.
References-found: 12

