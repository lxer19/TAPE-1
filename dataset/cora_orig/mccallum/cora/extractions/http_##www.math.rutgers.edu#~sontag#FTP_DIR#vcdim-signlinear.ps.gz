URL: http://www.math.rutgers.edu/~sontag/FTP_DIR/vcdim-signlinear.ps.gz
Refering-URL: http://www.math.rutgers.edu/~sontag/papers.html
Root-URL: 
Email: bhaskar@dimacs.rutgers.edu  sontag@hilbert.rutgers.edu  
Title: Sample Complexity for Learning Recurrent Perceptron Mappings  
Author: Bhaskar Dasgupta Eduardo D. Sontag 
Keyword: perceptrons, recurrent models, neural networks, learning, Vapnik-Chervonenkis dimension  
Address: New Brunswick, NJ 08903  New Brunswick, NJ 08903  
Affiliation: Rutgers University  Department of Mathematics Rutgers University  
Pubnum: DIMACS  
Abstract: Recurrent perceptron classifiers generalize the classical perceptron model. They take into account those correlations and dependences among input coordinates which arise from linear digital filtering. This paper provides tight bounds on sample complexity associated to the fitting of such models to experimental data. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A.D. Back and A.C. Tsoi, </author> <title> FIR and IIR synapses, a new neural network architecture for time-series modeling, </title> <booktitle> Neural Computation, 3 (1991), </booktitle> <pages> pp. 375-385. </pages>
Reference-contexts: Some authors, particularly Back and Tsoi |see e.g. <ref> [1, 2] </ref>| have introduced these ideas in the neural network literature. There is also related work in control theory dealing with such classifying, or more generally quantized-output, linear systems; see [8, 13].
Reference: [2] <author> A.D. Back and A.C. Tsoi, </author> <title> A comparison of discrete-time operator models for nonlinear system identification, </title> <booktitle> Advances in Neural Information Processing Systems (NIPS'94), </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1995, </year> <note> to appear. </note>
Reference-contexts: Some authors, particularly Back and Tsoi |see e.g. <ref> [1, 2] </ref>| have introduced these ideas in the neural network literature. There is also related work in control theory dealing with such classifying, or more generally quantized-output, linear systems; see [8, 13].
Reference: [3] <author> A.M. Baksho, S. Dasgupta, J.S. Garnett, and C.R. Johnson, </author> <title> On the similarity of conditions for an open-eye channel and for signed filtered error adaptive filter stability, </title> <booktitle> Proc. IEEE Conf. Decision and Control, </booktitle> <address> Brighton, UK, Dec. 1991, </address> <publisher> IEEE Publications, </publisher> <year> 1991, </year> <pages> pp. 1786-1787. </pages>
Reference-contexts: Various dynamical system models for classification appear from instance when learning finite automata and languages |see e.g. [10]| and in signal processing as a channel equalization problem (at least in the simplest 2-level case) when modeling linear channels transmitting digital data from a quantized source |see <ref> [3] </ref> and also the related paper [15]. When dealing with linear dynamical classifiers, the inner product ~c:v represents a convolution by a separating vector ~c that is the impulse-response of a recursive digital filter of some order n t k.
Reference: [4] <author> S. Basu, R. Pollack, and M.-F. Roy, </author> <title> A New Algorithm to Find a Point in Every Cell Defined by a Family of Polynomials, in Quantifier Elimination and Cylindrical Algebraic Decomposition, </title> <editor> B. Caviness and J. Johnson eds., </editor> <publisher> Springer-Verlag, to appear. </publisher>
Reference-contexts: Our proof will consist of a simple application of several recent results and concepts, given in <ref> [4, 5, 16] </ref>, which deal with the computational complexity aspects of the first-order theory of real-closed fields.
Reference: [5] <author> S. Basu, R. Pollack, and M.-F. </author> <title> Roy,On the Combinatorial and Algebraic Complexity of Quantifier Elimination, </title> <booktitle> Proc. 35th IEEE Symp. on Foundations of Computer Science, </booktitle> <year> 1994. </year>
Reference-contexts: Our proof will consist of a simple application of several recent results and concepts, given in <ref> [4, 5, 16] </ref>, which deal with the computational complexity aspects of the first-order theory of real-closed fields.
Reference: [6] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth, </author> <title> Learnability and the Vapnik-Chervonenkis dimension, </title> <journal> J. of the ACM, </journal> <volume> 36 (1989), </volume> <pages> pp. 929-965. </pages>
Reference-contexts: Sample Complexity and VC Dimension We next very briefly review some (by now standard) notions regarding sample complexity, with the purpose of motivating the main results, which deal with the calculation of VC dimensions. For 2 more details see the book [20], the paper <ref> [6] </ref>, or the survey [14] (the particular terminology used here is as in the exposition [17]). <p> In fact, s ("; ffi) is bounded by a polynomial in 1=" and 1=ffi and is proportional to in the following precise sense (cf. <ref> [6] </ref>): s ("; ffi) max 8 log 13 4 log 2 (there is a similar lower bound), and this motivates the studies dealing with estimating VC dimension, as we pursue here. <p> We study the complexity of the learning problem for constant n (but varying q). The key step is treating consistency, since if the decision version of a consistency problem is NP-hard, then the corresponding class is not properly polynomially learnable under the complexity theoretic assumption RP6=NP, cf. <ref> [6] </ref>.
Reference: [7] <author> M. Coste and M.F. Roy, </author> <title> Thom's Lemma, the Coding of Real Algebraic Numbers and the Computation of the Topology of Semialgebraic sets, </title> <journal> J. Symbolic Computation, </journal> <volume> 5 (1988), </volume> <pages> pp. 121-129. </pages>
Reference-contexts: It is known (cf. <ref> [7] </ref>) that Th (ff; f ) uniquely characterizes ff among the roots of f .
Reference: [8] <author> D.F. Delchamps, </author> <title> Extracting State Information from a Quantized Output Record, </title> <journal> Systems and Control Letters, </journal> <volume> 13 (1989), </volume> <pages> pp. 365-372. </pages>
Reference-contexts: Some authors, particularly Back and Tsoi |see e.g. [1, 2]| have introduced these ideas in the neural network literature. There is also related work in control theory dealing with such classifying, or more generally quantized-output, linear systems; see <ref> [8, 13] </ref>.
Reference: [9] <author> R.O. Duda and P.E. Hart, </author> <title> Pattern Classification and Scene Analysis, </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: 1 Introduction One of the most popular approaches to binary pattern classification, underlying many statistical techniques, is based on perceptrons or linear discriminants; see for instance the classical reference <ref> [9] </ref>. In this context, one is interested in classifying k-dimensional input patterns v = (v 1 ; : : : ; v k ) into two disjoint classes A + and A .
Reference: [10] <author> C.E. Giles, G.Z. Sun, H.H. Chen, Y.C. Lee, and D. Chen, </author> <title> Higher order recurrent networks and grammatical inference, </title> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <editor> D.S. Touretzky, ed., </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: Various dynamical system models for classification appear from instance when learning finite automata and languages |see e.g. <ref> [10] </ref>| and in signal processing as a channel equalization problem (at least in the simplest 2-level case) when modeling linear channels transmitting digital data from a quantized source |see [3] and also the related paper [15].
Reference: [11] <author> P. Goldberg and M. Jerrum, </author> <title> Bounding the Vapnik-Chervonenkis dimension of concept classes parameterized by real numbers, </title> <journal> Machine Learning, </journal> <volume> 18, </volume> <year> (1995): </year> <pages> 131-148. </pages>
Reference: [12] <author> D. Haussler, </author> <title> Decision theoretic generalizations of the PAC model for neural nets and other learning applications, </title> <journal> Information and Computation, </journal> <volume> 100, </volume> <year> (1992): </year> <pages> 78-150. 15 </pages>
Reference-contexts: function ` is so that the class of functions (x; y) 7! `(f (x); y) is "permissible" in the sense of Haussler and Pollard. (This means that these functions must be Borel-measurable, and in addition measurability holds in a certain parametric sense uniformly over the class; the reader may consult <ref> [12] </ref> for details, but in any case, the assumptions hold in our application). <p> In partial analogy to the role of VC dimension for Boolean functions, the study of sample complexity for the present learning problem is related to another combinatorial quantity, namely Pollard's pseudo-dimension, which can be defined as follows (the property that we give is trivially equivalent to the one in e.g. <ref> [12, 14] </ref>, but it is somewhat easier to check): Given a class of functions F from X to Y, and a function ` : Yfi Y! R, one may introduce, for each f 2 F , the function A f;` : Xfi Yfi R ! f1; 1g : (x; y; t) <p> i=1 h2F i=1 fi fi fi " : (This entails, in practice, the approximate solution of an optimization problem over F , minimizing error over the training samples; in fact, a probabilistic algorithm can be used in that it is only necessary that this estimate holds with high probability, cf. <ref> [12, Lemma 1] </ref>).
Reference: [13] <author> R. Koplon and E.D. Sontag, </author> <title> Linear systems with sign-observations, </title> <journal> SIAM J. Control and Optimization, </journal> <volume> 31(1993): 1245 - 1266. </volume>
Reference-contexts: Some authors, particularly Back and Tsoi |see e.g. [1, 2]| have introduced these ideas in the neural network literature. There is also related work in control theory dealing with such classifying, or more generally quantized-output, linear systems; see <ref> [8, 13] </ref>.
Reference: [14] <author> W. Maass, </author> <booktitle> Perspectives of current research about the complexity of learning in neural nets, in Theoretical Advances in Neural Computation and Learning, </booktitle> <editor> V.P. Roychowdhury, K.Y. Siu, and A. Orlitsky, editors, </editor> <publisher> Kluwer, </publisher> <address> Boston, </address> <year> 1994, </year> <pages> pp. 295-336. </pages>
Reference-contexts: bounds give estimates of the number of fl This research was supported in part by US Air Force Grant AFOSR-94-0293. 1 random training samples needed so that a perceptron consistent with the seen samples will also, with high probability, perform well on unseen data; see in particular the exposition in <ref> [14] </ref>. Recurrent Perceptrons In signal processing and control applications, the size k of the input vectors v is typically very large, so the number of samples needed in order to accurately "learn" an appropriate classifying perceptron is in principle very large. <p> Sample Complexity and VC Dimension We next very briefly review some (by now standard) notions regarding sample complexity, with the purpose of motivating the main results, which deal with the calculation of VC dimensions. For 2 more details see the book [20], the paper [6], or the survey <ref> [14] </ref> (the particular terminology used here is as in the exposition [17]). <p> In partial analogy to the role of VC dimension for Boolean functions, the study of sample complexity for the present learning problem is related to another combinatorial quantity, namely Pollard's pseudo-dimension, which can be defined as follows (the property that we give is trivially equivalent to the one in e.g. <ref> [12, 14] </ref>, but it is somewhat easier to check): Given a class of functions F from X to Y, and a function ` : Yfi Y! R, one may introduce, for each f 2 F , the function A f;` : Xfi Yfi R ! f1; 1g : (x; y; t) <p> of F with respect to the loss function `, denoted by pd [F ; `], is defined as: pd [F ; `] := vc (A F;` ) : It is known (cf. [12, Lemma 1 in pp. 100 and Corollary 2 in pp. 114], as well as the discussion in <ref> [14, Theorem 4:1 and Remark 4:2b] </ref>) that a sufficient condition for F to be learnable with respect to ` is that pd [F ; `] &lt; 1, in which case one may pick s ("; ffi) " 2 2:pd [F ; `] ln " 8 Moreover, in analogy with the identifier
Reference: [15] <author> G.W. Pulford, R.A. Kennedy, and B.D.O. Anderson, </author> <title> Neural network structure for emulating decision feedback equalizers, </title> <booktitle> Proc. Int. Conf. Acoustics, Speech, and Signal Processing, </booktitle> <address> Toronto, Canada, </address> <month> May </month> <year> 1991, </year> <pages> pp. 1517-1520. </pages>
Reference-contexts: system models for classification appear from instance when learning finite automata and languages |see e.g. [10]| and in signal processing as a channel equalization problem (at least in the simplest 2-level case) when modeling linear channels transmitting digital data from a quantized source |see [3] and also the related paper <ref> [15] </ref>. When dealing with linear dynamical classifiers, the inner product ~c:v represents a convolution by a separating vector ~c that is the impulse-response of a recursive digital filter of some order n t k.
Reference: [16] <author> M.-F. Roy and A. Szpirglas, </author> <title> Complexity of Computation on Real Algebraic Numbers, </title> <editor> J. </editor> <booktitle> Symbolic Computation (1990) 10: </booktitle> <pages> 39-51. </pages>
Reference-contexts: Our proof will consist of a simple application of several recent results and concepts, given in <ref> [4, 5, 16] </ref>, which deal with the computational complexity aspects of the first-order theory of real-closed fields. <p> The evaluation can be done efficiently because of the following fact from <ref> [16] </ref>: There is an algorithm B with the following property.
Reference: [17] <author> E.D. Sontag, </author> <title> Neural networks for control, in Essays on Control: Perspectives in the Theory and its Applications (H.L. </title> <editor> Trentelman and J.C. Willems, eds.), </editor> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1993, </year> <pages> pp. 339-380. </pages>
Reference-contexts: For 2 more details see the book [20], the paper [6], or the survey [14] (the particular terminology used here is as in the exposition <ref> [17] </ref>). In the general classification problem, an input space X as well as a collection F of maps X! f1; 1g are assumed to have been given. (The set X is assumed to be either countable or an Euclidean space, and the maps in F are assumed to be measurable.
Reference: [18] <author> Gy orgy Tur an, </author> <title> Computational Learning Theory and Neural Networks: A Survey of Selected Topics, in Theoretical Advances in Neural Computation and Learning, V.P. Roychowdhury, K.Y. Siu, </title> <editor> and A. Orlitsky, editors, </editor> <publisher> Kluwer, </publisher> <address> Boston, </address> <year> 1994, </year> <pages> pp. 243-293. </pages>
Reference-contexts: It follows from Theorem 2 that vc (F n;q ) nblog (r)c, as desired. 5 The Consistency Problem We next briefly discuss polynomial time learnability of recurrent perceptron mappings. As discussed in e.g. <ref> [18] </ref>, in order to formalize this problem we need to first choose a data structure to represent the hypotheses in F n;q .
Reference: [19] <author> L.G. </author> <title> Valiant A theory of the learnable, </title> <journal> Comm. of the ACM, </journal> <volume> 27, </volume> <year> 1984, </year> <pages> pp. 1134-1142 </pages>
Reference-contexts: When there is an algorithm that allows computing such a function in time polynomial on the sample size, the class is said to be learnable in the PAC ("probably approximately correct") sense of Valiant (cf. <ref> [19] </ref>). In this paper we concentrate on the question of uniform learnability in the sample complexity sense, for recurrent perceptron concept classes, but we will also prove a result, in Section 5 regarding PAC learnability for such classes.
Reference: [20] <author> V.N. Vapnik, </author> <title> Estimation of Dependencies Based on Empirical Data, </title> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1982. </year> <month> 16 </month>
Reference-contexts: This philosophy can be made precise on the basis of sample complexity bounds ("VC dimension" as discussed below), and can be found in classical references (see e.g. <ref> [20] </ref>). <p> Sample Complexity and VC Dimension We next very briefly review some (by now standard) notions regarding sample complexity, with the purpose of motivating the main results, which deal with the calculation of VC dimensions. For 2 more details see the book <ref> [20] </ref>, the paper [6], or the survey [14] (the particular terminology used here is as in the exposition [17]). <p> It can also be proved that, if there is any identifier at all in the above sense, then one can always use the following naive identification procedure: pick any element which is consistent with the observed data. In the statistics literature |see <ref> [20] </ref>| this "naive technique" is a particular case of what is called empirical risk minimization.
References-found: 20

