URL: http://www.mcs.anl.gov/~thakur/papers/ipps99-thread-coll.ps.gz
Refering-URL: http://www.mcs.anl.gov/~thakur/papers.html
Root-URL: http://www.mcs.anl.gov
Title: Improving Collective I/O Performance Using Threads  
Author: Phillip M. Dickens Rajeev Thakur 
Affiliation: Department of Computer Science Illinois Institute of Technology  Mathematics and Computer Science Division Argonne National Laboratory  
Abstract: Massively parallel computers are increasingly being used to solve large, I/O intensive applications in many different fields. For such applications, the I/O requirements quite often present a significant obstacle in the way of achieving good performance, and an important area of current research is the development of techniques by which these costs can be reduced. One such approach is collective I/O, where the processors cooperatively develop an I/O strategy that reduces the number, and increases the size, of I/O requests, making a much better use of the I/O subsystem. Collective I/O has been shown to significantly reduce the cost of performing I/O in many large, parallel applications, and for this reason serves as an important base upon which we can explore other mechanisms which can further reduce these costs. One promising approach is to use threads to perform the collective I/O in the background while the main thread continues with other computation in the foreground. In this paper, we explore the issues associated with implementing collective I/O in the background using threads. The most natural approach is to simply spawn off an I/O thread to perform the collective I/O in the background while the main thread continues with other computation. However, our research demonstrates that this approach is frequently the worst implementation option, often performing much more poorly than just executing collective I/O completely in the foreground. To improve the performance of thread-based collective I/O, we developed an alternate approach where part of the collective I/O operation is performed in the background, and part is performed in the foreground. We demonstrate that this new technique can significantly improve the performance of thread-based collective I/O, providing up to an 80% improvement over sequential collective I/O (where there is no attempt to overlap computation with I/O). Also, we discuss one very important application of this research which is the implementation of the split-collective parallel I/O operations defined in MPI 2.0. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Acharya, A., Uysal, M., Bennett, R., Mendelson, A., Beynon, M., Hollingsworth, K., Saltz, J. and Alan Suss-man. </author> <title> Tuning the performance of I/O intensive parallel applications. </title> <booktitle> In Proceedings of the Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 15-27, </pages> <address> Philadelphia, May 1996. </address> <publisher> ACM Press. </publisher>
Reference-contexts: However, neither of these studies looks at thread-based collective I/O. Also, Bordawekar [4] provides an excellent discussion of the I/O characteristics of the HP Exemplar. Two-phase I/O is not the only approach that can significantly improve performance of I/O intensive applications. Acharya et al. <ref> [1] </ref> investigate code restructuring and other optimizations to improve the performance of I/O bound computations, and reported excellent performance without the use of collective I/O. There are other projects using collective I/O. For example, Passion has been extended to handle out-of-core arrays [19].
Reference: [2] <author> R. Bordawekar. </author> <title> Implementation of collective I/O in the Intel Paragon parallel file system: Initial experiences. </title> <booktitle> In Proceedings of the 11th ACM International Conference on Supercomputing. </booktitle> <publisher> ACM Press, </publisher> <month> July </month> <year> 1997. </year>
Reference-contexts: The application can use this knowledge to significantly improve its I/O performance. The technique of collective I/O has been developed to better utilize the parallel I/O subsystem <ref> [6, 19, 20, 2, 15, 18, 3] </ref>. In this approach, the processors exchange information about their individual I/O requests to develop a picture of the aggregate I/O request. <p> As noted above however, we found that the SP2 is the only architecture for which this approach performed well. The performance of two-phase I/O on the Intel Paragon has been studied extensively by both Dickens and Thakur [8] and by Bordawekar <ref> [2] </ref>. However, neither of these studies looks at thread-based collective I/O. Also, Bordawekar [4] provides an excellent discussion of the I/O characteristics of the HP Exemplar. Two-phase I/O is not the only approach that can significantly improve performance of I/O intensive applications.
Reference: [3] <author> Bordawekar, R., del Rosario, J. and Alok Choudhary. </author> <title> Design and evaluation of primitives for parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 452-461, </pages> <address> Portland, OR, 1993. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: The application can use this knowledge to significantly improve its I/O performance. The technique of collective I/O has been developed to better utilize the parallel I/O subsystem <ref> [6, 19, 20, 2, 15, 18, 3] </ref>. In this approach, the processors exchange information about their individual I/O requests to develop a picture of the aggregate I/O request.
Reference: [4] <author> Bordawekar, R. </author> <title> Quantitative Characterization and Analysis of the I/O Behavior of a Commercial Distributed-shared-memory Machine. </title> <institution> California Institute of Technology Technical Report CACR TR-157 March 1998. </institution>
Reference-contexts: The performance of two-phase I/O on the Intel Paragon has been studied extensively by both Dickens and Thakur [8] and by Bordawekar [2]. However, neither of these studies looks at thread-based collective I/O. Also, Bordawekar <ref> [4] </ref> provides an excellent discussion of the I/O characteristics of the HP Exemplar. Two-phase I/O is not the only approach that can significantly improve performance of I/O intensive applications.
Reference: [5] <author> Crandall, P., Aydt, R., Chien, A. and D. </author> <title> Reed Input-Output Characteristics of Scalable Parallel Applications, </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <publisher> ACM press, </publisher> <month> December </month> <year> 1995. </year>
Reference-contexts: The problem is generally not with the hardware; many parallel I/O subsystems offer excellent performance. Rather, the problem arises from other factors, primarily the I/O patterns exhibited by many parallel scientific applications <ref> [5, 14] </ref>. In particular, each processor tends to make a large number of small I/O requests, incurring the high cost of I/O on each such request. One reason for this access pattern is that parallel scientific codes frequently involve large arrays distributed across the processor's local memory.
Reference: [6] <author> DelRosario, J., Bordawekar, R. and Alok Choudhary. </author> <title> Improved parallel I/O via a two-phase run-time access strategy. </title> <booktitle> In Proceedings of the IPPS '93 Workshop on Input/Output in Parallel Computer Systems pages 56-70, </booktitle> <address> Newport Beach, CA, </address> <year> 1993. </year>
Reference-contexts: The application can use this knowledge to significantly improve its I/O performance. The technique of collective I/O has been developed to better utilize the parallel I/O subsystem <ref> [6, 19, 20, 2, 15, 18, 3] </ref>. In this approach, the processors exchange information about their individual I/O requests to develop a picture of the aggregate I/O request. <p> Based on this global knowledge, I/O requests are combined and submitted in their proper order, making a much more efficient use of the I/O subsystem. Two significant implementation techniques for collective I/O are two-phase I/O <ref> [6, 19, 20] </ref> and disk-directed I/O [13, 16]. In disk-directed I/O, the collective I/O request is sent to the I/O processors which collectively determine and carry out the optimal I/O strategy. In the two-phase approach, the application processors collectively determine and carry out the optimized approach.
Reference: [7] <author> DelRasario, J. and A. Choudhary. </author> <title> High performance I/O for parallel computers: Problems and prospects. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 59-68, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: There are other projects using collective I/O. For example, Passion has been extended to handle out-of-core arrays [19]. Also, a variation of the disk-directed I/O technique is used in the Panda runtime library [18]. Excellent overviews of the field of parallel I/O can be found in <ref> [9, 7, 10] </ref>. 5 Discussion and Conclusions The research presented here clearly demonstrates that it is possible to obtain good performance by overlapping computation with collective I/O, but it is not automatic.
Reference: [8] <author> Dickens, P. and R. Thakur. </author> <title> A Performance Study of Two-phase I/O. </title> <booktitle> In 4th International Euro-Par Conference Proceedings. In Lecture Notes of Computer Science, </booktitle> <pages> 1470. </pages> <note> Published by Springer, </note> <editor> D. Pritchard and J Reev Eds., </editor> <address> pages 959-965. </address>
Reference-contexts: As noted above however, we found that the SP2 is the only architecture for which this approach performed well. The performance of two-phase I/O on the Intel Paragon has been studied extensively by both Dickens and Thakur <ref> [8] </ref> and by Bordawekar [2]. However, neither of these studies looks at thread-based collective I/O. Also, Bordawekar [4] provides an excellent discussion of the I/O characteristics of the HP Exemplar. Two-phase I/O is not the only approach that can significantly improve performance of I/O intensive applications.
Reference: [9] <author> Feitelson, D., Corbett, P., Hsu, Y. and J. Prost. </author> <title> Parallel I/O systems and interfaces for parallel computers. </title> <booktitle> In Topics in Modern Operating Systems. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1997. </year>
Reference-contexts: There are other projects using collective I/O. For example, Passion has been extended to handle out-of-core arrays [19]. Also, a variation of the disk-directed I/O technique is used in the Panda runtime library [18]. Excellent overviews of the field of parallel I/O can be found in <ref> [9, 7, 10] </ref>. 5 Discussion and Conclusions The research presented here clearly demonstrates that it is possible to obtain good performance by overlapping computation with collective I/O, but it is not automatic.
Reference: [10] <author> Feitelson, D., Corbett, P., Baylor, S. and Yarson Hsu. </author> <title> Parallel I/O subsystems in massively parallel supercomputers. </title> <booktitle> In IEEE Parallel and Distributed Technology, </booktitle> <volume> 3(3) </volume> <pages> 33-47, </pages> <month> Fall </month> <year> 1995. </year>
Reference-contexts: There are other projects using collective I/O. For example, Passion has been extended to handle out-of-core arrays [19]. Also, a variation of the disk-directed I/O technique is used in the Panda runtime library [18]. Excellent overviews of the field of parallel I/O can be found in <ref> [9, 7, 10] </ref>. 5 Discussion and Conclusions The research presented here clearly demonstrates that it is possible to obtain good performance by overlapping computation with collective I/O, but it is not automatic.
Reference: [11] <author> Feitelson, D., Corbett, P. and J. Prost. </author> <title> Performance of the Vesta parallel file system. </title> <type> In Technical Report RC 19760, </type> <institution> IBM Watson Research Center, </institution> <address> Yorktown Heights, N.Y., </address> <month> September, </month> <year> 1994. </year>
Reference: [12] <author> Gropp, W., Lusk, E. and A. Skjellum. </author> <title> Using MPI. Portable Parallel Programming with the Message-Passing Interface. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts. </address> <year> 1996. </year>
Reference-contexts: Performance is enhanced because concurrent requests to different positions within the same file can be serviced in parallel by the I/O subsystem. 2.1. Split-Collective I/O Operations The Message Passing Interface (MPI) <ref> [12] </ref> is the emerging standard by which distributed computers communicate and synchronize. As such, it provides a platform upon which portable distributed codes can be developed. The recently released MPI 2.0 standard incorporates parallel I/O into the specification.
Reference: [13] <author> Kotz, D. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <journal> ACM Transactions on Computer Systems 15(1) </journal> <pages> 41-74, </pages> <month> February </month> <year> 1997. </year>
Reference-contexts: Based on this global knowledge, I/O requests are combined and submitted in their proper order, making a much more efficient use of the I/O subsystem. Two significant implementation techniques for collective I/O are two-phase I/O [6, 19, 20] and disk-directed I/O <ref> [13, 16] </ref>. In disk-directed I/O, the collective I/O request is sent to the I/O processors which collectively determine and carry out the optimal I/O strategy. In the two-phase approach, the application processors collectively determine and carry out the optimized approach. In this paper, we deal only with the two-phase approach.
Reference: [14] <author> Kotz, D. and N. Nieuwejaar. </author> <title> Dynamic file-access characteristics of a production parallel scientific workload. </title> <booktitle> In Supercomputing '94 pages 640-649, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: The problem is generally not with the hardware; many parallel I/O subsystems offer excellent performance. Rather, the problem arises from other factors, primarily the I/O patterns exhibited by many parallel scientific applications <ref> [5, 14] </ref>. In particular, each processor tends to make a large number of small I/O requests, incurring the high cost of I/O on each such request. One reason for this access pattern is that parallel scientific codes frequently involve large arrays distributed across the processor's local memory.
Reference: [15] <author> Kotz, D. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <journal> it ACM Transactions on Computer Systems, </journal> <volume> 15(1) </volume> <pages> 41-74, </pages> <month> February </month> <year> 1997. </year>
Reference-contexts: The application can use this knowledge to significantly improve its I/O performance. The technique of collective I/O has been developed to better utilize the parallel I/O subsystem <ref> [6, 19, 20, 2, 15, 18, 3] </ref>. In this approach, the processors exchange information about their individual I/O requests to develop a picture of the aggregate I/O request.
Reference: [16] <author> Kotz, D. </author> <title> Expanding the potential for disk-directed I/O. </title> <booktitle> In Proceedings of the 1995 IEEE Symposium on Parallel and Distributed Processing. </booktitle> <pages> Pages 490 - 495, </pages> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Based on this global knowledge, I/O requests are combined and submitted in their proper order, making a much more efficient use of the I/O subsystem. Two significant implementation techniques for collective I/O are two-phase I/O [6, 19, 20] and disk-directed I/O <ref> [13, 16] </ref>. In disk-directed I/O, the collective I/O request is sent to the I/O processors which collectively determine and carry out the optimal I/O strategy. In the two-phase approach, the application processors collectively determine and carry out the optimized approach. In this paper, we deal only with the two-phase approach.
Reference: [17] <author> More, S., Choudhary, A., Foster, I. and M. Xu. </author> <title> MTIO a multi-threaded parallel I/O system. </title> <booktitle> In Proceedings of the Eleventh International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1997. </year>
Reference-contexts: spawning a thread to perform only the write to disk appears to scale well as the number of processors, and consequently the total number of messages in the network, are simultaneously increased. 4 Related Work The research most closely related to this project is the development of the MTIO library <ref> [17] </ref>, which is a multi-threaded MPI-based I/O library. MTIO supports the overlap of computation with collective I/O by spawning an I/O thread to complete the whole collective routine in the background. The MTIO library is implemented on the IBM SP2.
Reference: [18] <author> Seamons, K., Chen, Y., Jones, P., Jozwial, J. and M. Winslett. </author> <title> Server-directed collective I/O in Panda. </title> <booktitle> In In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, CA, </address> <month> December </month> <year> 1995. </year> <note> IEEE Computer Science press. </note>
Reference-contexts: The application can use this knowledge to significantly improve its I/O performance. The technique of collective I/O has been developed to better utilize the parallel I/O subsystem <ref> [6, 19, 20, 2, 15, 18, 3] </ref>. In this approach, the processors exchange information about their individual I/O requests to develop a picture of the aggregate I/O request. <p> There are other projects using collective I/O. For example, Passion has been extended to handle out-of-core arrays [19]. Also, a variation of the disk-directed I/O technique is used in the Panda runtime library <ref> [18] </ref>. Excellent overviews of the field of parallel I/O can be found in [9, 7, 10]. 5 Discussion and Conclusions The research presented here clearly demonstrates that it is possible to obtain good performance by overlapping computation with collective I/O, but it is not automatic.
Reference: [19] <author> Thakur, R. and A. Choudhary. </author> <title> An Extended Two-Phase Method for Accessing Sections of Out-of-Core Arrays. </title> <booktitle> Scientific Programming 5(4) </booktitle> <pages> 301-317, </pages> <month> Winter </month> <year> 1996. </year>
Reference-contexts: The application can use this knowledge to significantly improve its I/O performance. The technique of collective I/O has been developed to better utilize the parallel I/O subsystem <ref> [6, 19, 20, 2, 15, 18, 3] </ref>. In this approach, the processors exchange information about their individual I/O requests to develop a picture of the aggregate I/O request. <p> Based on this global knowledge, I/O requests are combined and submitted in their proper order, making a much more efficient use of the I/O subsystem. Two significant implementation techniques for collective I/O are two-phase I/O <ref> [6, 19, 20] </ref> and disk-directed I/O [13, 16]. In disk-directed I/O, the collective I/O request is sent to the I/O processors which collectively determine and carry out the optimal I/O strategy. In the two-phase approach, the application processors collectively determine and carry out the optimized approach. <p> Acharya et al. [1] investigate code restructuring and other optimizations to improve the performance of I/O bound computations, and reported excellent performance without the use of collective I/O. There are other projects using collective I/O. For example, Passion has been extended to handle out-of-core arrays <ref> [19] </ref>. Also, a variation of the disk-directed I/O technique is used in the Panda runtime library [18].
Reference: [20] <author> Thakur, R., Choudhary, A., More, S and S. Kuditipudi. </author> <title> Passion: Optimized I/O for parallel applications. </title> <journal> IEEE Computer, </journal> <volume> 29(6) </volume> <pages> 70-78, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: The application can use this knowledge to significantly improve its I/O performance. The technique of collective I/O has been developed to better utilize the parallel I/O subsystem <ref> [6, 19, 20, 2, 15, 18, 3] </ref>. In this approach, the processors exchange information about their individual I/O requests to develop a picture of the aggregate I/O request. <p> Based on this global knowledge, I/O requests are combined and submitted in their proper order, making a much more efficient use of the I/O subsystem. Two significant implementation techniques for collective I/O are two-phase I/O <ref> [6, 19, 20] </ref> and disk-directed I/O [13, 16]. In disk-directed I/O, the collective I/O request is sent to the I/O processors which collectively determine and carry out the optimal I/O strategy. In the two-phase approach, the application processors collectively determine and carry out the optimized approach.
References-found: 20

