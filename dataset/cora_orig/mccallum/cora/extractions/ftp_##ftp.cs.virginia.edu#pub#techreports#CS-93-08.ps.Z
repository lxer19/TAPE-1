URL: ftp://ftp.cs.virginia.edu/pub/techreports/CS-93-08.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/techreports/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Hardware Support for Dynamic Access Ordering: Performance of Some Design Options  
Author: Sally A. McKee 
Note: This work was supported in part by the NSF under contract number MIP-9114110.  
Abstract: Computer Science Report No. CS-93-08 August 9, 1993 
Abstract-found: 1
Intro-found: 1
Reference: [Bae91] <author> Baer, J. L., and Chen, T. F., </author> <title> An Effective On-Chip Preloading Scheme To Reduce Data Access Penalty, </title> <address> Supercomputing91, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: These include altering the placement of data to exploit concurrency [Gup88], reordering the computation to increase locality, as in blocking [Lam91], address transformations for conict-free access to interleaved memory [Har89, Rau91, Val91], software prefetching data to the cache [Cal91, Kla91, Soh91], and hardware prefetching vector data to cache <ref> [Bae91, Fu91, Jou90, Skl92] </ref>. For a more detailed discussion of how these schemes relate to dynamic access ordering, see [McK93b].
Reference: [Bar92] <author> Baron, R.L., and Higbie, L., </author> <title> Computer Architecture, </title> <publisher> Addison-Wesley, </publisher> <year> 1992. </year>
Reference: [Ber89] <author> Berry, M., et. al., </author> <title> The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers, </title> <type> CSRD Report No. 827, </type> <institution> Center for Supercomputing Research and Development, Urbana, IL, </institution> <month> May </month> <year> 1989. </year>
Reference: [Bud71] <author> Budnik, P., and Kuck, D., </author> <title> The Organization and Use of Parallel Memories, </title> <journal> IEEE Trans. Comput., </journal> <volume> 20, 12, </volume> <year> 1971. </year>
Reference: [Cal91] <author> Callahan, D., et. al., </author> <title> Software Prefetching, </title> <booktitle> Fourth International Conference on Architectural Support for Programming Languages and Systems, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: These include altering the placement of data to exploit concurrency [Gup88], reordering the computation to increase locality, as in blocking [Lam91], address transformations for conict-free access to interleaved memory [Har89, Rau91, Val91], software prefetching data to the cache <ref> [Cal91, Kla91, Soh91] </ref>, and hardware prefetching vector data to cache [Bae91, Fu91, Jou90, Skl92]. For a more detailed discussion of how these schemes relate to dynamic access ordering, see [McK93b].
Reference: [Car89] <author> Carr, S., Kennedy, K., </author> <title> Blocking Linear Algebra Codes for Memory Hierarchies, </title> <booktitle> Proc. Fourth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <year> 1989. </year>
Reference-contexts: Furthermore, vectors leave large footprints in the cache. For computations in which vectors are reused, iteration space tiling <ref> [Car89, Wol89] </ref> can partition the problems into cache-size blocks, but this can create cache conicts for some block sizes and vector strides [Lam91], and the technique is difficult to automate.
Reference: [Dav90] <author> Davidson, Jack W., and Benitez, Manuel E., </author> <title> Code Generation for Streaming: An Access/Execute Mechanism, </title> <booktitle> Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: Another advantage is that this combined hardware/software scheme doesnt require heroic compiler technology the compiler need only detect the presence of streams, and Davidsons streaming algorithm <ref> [Dav90] </ref> can be used to do this. Note that we assume the processor can perform non-caching loads and stores so that non-unit stride streams can be accessed without concomitantly accessing extraneous data and wasting bandwidth.
Reference: [DEC92] <institution> Alpha Architecture Handbook, Digital Equipment Corporation, </institution> <year> 1992. </year>
Reference-contexts: Others, such as the DEC Alpha <ref> [DEC92] </ref>, provide a means of specifying some portions of memory as non-cacheable. 4.
Reference: [Don79] <author> Dongarra, J.J., et. al., </author> <title> Linpack Users Guide, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1979. </year>
Reference-contexts: Hardware Support for Dynamic Access Ordering: Performance of Some Design Options 8 5. Benchmark Suite The benchmark kernels used are described in Figure 2. Daxpy, copy, scale, and swap are from the BLAS (Basic Linear Algebra Subroutines) <ref> [Law79, Don79] </ref>. These vector and matrix computations occur frequently in scientific computations, thus they have been collected into libraries of highly optimized routines for various host architectures. Hydro and tridiag are the first and fifth Livermore Loops [McM86], a set of kernels culled from important scientific computations.
Reference: [Don90] <author> Dongarra, J.J., DuCroz, J., Duff, I., and Hammerling, S., </author> <title> A set of Level 3 Basic Linear Algebra Subprograms, </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 16 </volume> <pages> 1-17, </pages> <year> 1990. </year>
Reference-contexts: Note that although these computations do not reuse vector elements, they are often found in the inner loops of algorithms that do. Examples include the blocked algorithms of the Level 3 BLAS libraries <ref> [Don90] </ref>, as well as the matrix-multiply by diagonals operation mentioned above (which uses vaxpy). Whether or not the vectors are reused has no bearing on SMC performance, although lack of temporal locality greatly diminishes the effectiveness of caching.
Reference: [Fu91] <author> Fu, J. W. C., and Patel, J. H., </author> <title> Data Prefetching in Multiprocessor Vector Cache Memories, </title> <booktitle> 18th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: These include altering the placement of data to exploit concurrency [Gup88], reordering the computation to increase locality, as in blocking [Lam91], address transformations for conict-free access to interleaved memory [Har89, Rau91, Val91], software prefetching data to the cache [Cal91, Kla91, Soh91], and hardware prefetching vector data to cache <ref> [Bae91, Fu91, Jou90, Skl92] </ref>. For a more detailed discussion of how these schemes relate to dynamic access ordering, see [McK93b].
Reference: [Gol93] <author> Golub, G., and Ortega, J.M., </author> <title> Scientific Computation: An Introduction with Parallel Computing, </title> <publisher> Academic Press, Inc., </publisher> <year> 1993. </year>
Reference-contexts: Vaxpy is a vector axpy computation that occurs in matrix-vector multiplication by diagonals; this algorithm is useful for the diagonally sparse matrices that arise frequently when solving parabolic or elliptic partial differential equations by finite element or finite difference methods <ref> [Gol93] </ref>. Mul is a sparse matrix multiply, and msort is a merge sort. Here axpy refers to a computation involving some entity a times a vector x. plus a vector y.
Reference: [Goo85] <author> Goodman, J. R., et al, </author> <title> PIPE: A VLSI Decoupled Architecture, </title> <booktitle> Twelfth International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1985. </year>
Reference-contexts: In fact, the FIFO organization is almost identical to the stream units of the WM architecture [Wul92], or may be thought of as a special case of a decoupled access-execute architecture <ref> [Goo85, Smi87] </ref>. Another advantage is that this combined hardware/software scheme doesnt require heroic compiler technology the compiler need only detect the presence of streams, and Davidsons streaming algorithm [Dav90] can be used to do this.
Reference: [Gup88] <author> Gupta, R., Soffa, M., </author> <title> Compile-time Techniques for Efficient Utilization of Parallel Memories, </title> <journal> SIGPLAN Not., </journal> <volume> 23, 9, </volume> <year> 1988, </year> <pages> pp. </pages> <month> 235-246. </month> <title> Hardware Support for Dynamic Access Ordering: Performance of Some Design Options 109 </title>
Reference-contexts: There are a number of other hardware and software techniques that can help manage the imbalance between processor and memory speeds. These include altering the placement of data to exploit concurrency <ref> [Gup88] </ref>, reordering the computation to increase locality, as in blocking [Lam91], address transformations for conict-free access to interleaved memory [Har89, Rau91, Val91], software prefetching data to the cache [Cal91, Kla91, Soh91], and hardware prefetching vector data to cache [Bae91, Fu91, Jou90, Skl92].
Reference: [Har87] <author> Harper, D. T., Jump, J., </author> <title> Vector Access Performance in Parallel Memories Using a Skewed Storage Scheme, </title> <journal> IEEE Trans. Comput., </journal> <volume> 36, 12, </volume> <year> 1987. </year>
Reference: [Har89] <author> Harper, D. T., </author> <title> Address Transformation to Increase Memory Performance, </title> <booktitle> 1989 International Conference on Supercomputing. </booktitle>
Reference-contexts: These include altering the placement of data to exploit concurrency [Gup88], reordering the computation to increase locality, as in blocking [Lam91], address transformations for conict-free access to interleaved memory <ref> [Har89, Rau91, Val91] </ref>, software prefetching data to the cache [Cal91, Kla91, Soh91], and hardware prefetching vector data to cache [Bae91, Fu91, Jou90, Skl92]. For a more detailed discussion of how these schemes relate to dynamic access ordering, see [McK93b].
Reference: [Hay88] <author> Hayes, J.P., </author> <title> Computer Architecture and Organization, </title> <publisher> McGraw-Hill, </publisher> <year> 1988. </year>
Reference: [Hen90] <author> Hennessy, J., and Patterson, D., </author> <title> Computer Architecture: A Quantitative Approach, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference: [Hwa84] <author> Hwang, K., and Briggs, </author> <title> F.A., </title> <booktitle> Computer Architecture and Parallel Processing, </booktitle> <publisher> McGraw-Hill, Inc., </publisher> <year> 1984. </year>
Reference: [IEEE92] <author> High-speed DRAMs, </author> <title> Special Report, </title> <journal> IEEE Spectrum, </journal> <volume> vol. 29, no. 10, </volume> <month> October </month> <year> 1992. </year> <title> [Int91] i860 XP Microprocessor Data Book, </title> <publisher> Intel Corporation, </publisher> <year> 1991. </year>
Reference-contexts: Other common devices offer similar features, such as nibble-mode, static column mode, or a small amount of SRAM cache on chip. This sensitivity to the order of requests is exacerbated in several emerging technologies: for instance, Rambus [Ram92], Ramlink, and the new DRAM designs with high-speed sequential interfaces <ref> [IEEE92] </ref> provide high bandwidth for large transfers, but offer little performance benefit for single-word accesses. For multiple-module memory systems, the order of requests is important on yet another level: successive accesses to the same memory bank cannot be performed as quickly as accesses to different banks.
Reference: [Jou90] <author> Jouppi, N., </author> <title> Improving Direct-Mapped Cache Performance by the Addition of a Small Fully Associative Cache and Prefetch Buffers, </title> <booktitle> 17th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: These include altering the placement of data to exploit concurrency [Gup88], reordering the computation to increase locality, as in blocking [Lam91], address transformations for conict-free access to interleaved memory [Har89, Rau91, Val91], software prefetching data to the cache [Cal91, Kla91, Soh91], and hardware prefetching vector data to cache <ref> [Bae91, Fu91, Jou90, Skl92] </ref>. For a more detailed discussion of how these schemes relate to dynamic access ordering, see [McK93b].
Reference: [Kat89] <author> Katz, R., and Hennessy, J., </author> <title> High Performance Microprocessor Architectures, </title> <institution> University of California, Berkeley, </institution> <note> Report No. UCB/CSD 89/529, </note> <month> August, </month> <year> 1989. </year>
Reference-contexts: 1. Increasing Vector Memory Bandwidth Processor speeds are increasing much faster than memory speeds: microprocessor performance has increased by 50% to 100% per year in the last decade, while DRAM performance has risen only 10-15% per year <ref> [Kat89] </ref>. As a result, memory bandwidth is becoming the limiting performance factor for many applications, particularly scientific computations. Alleviating the growing disparity between processor and memory speeds is the subject of much current research.
Reference: [Kla91] <author> Klaiber, A., et. al., </author> <title> An Architecture for Software-Controlled Data Prefetching, </title> <booktitle> 18th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: These include altering the placement of data to exploit concurrency [Gup88], reordering the computation to increase locality, as in blocking [Lam91], address transformations for conict-free access to interleaved memory [Har89, Rau91, Val91], software prefetching data to the cache <ref> [Cal91, Kla91, Soh91] </ref>, and hardware prefetching vector data to cache [Bae91, Fu91, Jou90, Skl92]. For a more detailed discussion of how these schemes relate to dynamic access ordering, see [McK93b].
Reference: [Lam91] <author> Lam, Monica, et. al., </author> <title> The Cache Performance and Optimizations of Blocked Algorithms, </title> <booktitle> Fourth International Conference on Architectural Support for Programming Languages and Systems, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: Furthermore, vectors leave large footprints in the cache. For computations in which vectors are reused, iteration space tiling [Car89, Wol89] can partition the problems into cache-size blocks, but this can create cache conicts for some block sizes and vector strides <ref> [Lam91] </ref>, and the technique is difficult to automate. Caching nonunit stride vectors leaves even larger footprints, and may actually reduce a computations effective memory bandwidth by fetching extraneous data. The traditional scalar processor concern has been to minimize memory latency in order to maximize processor performance. <p> There are a number of other hardware and software techniques that can help manage the imbalance between processor and memory speeds. These include altering the placement of data to exploit concurrency [Gup88], reordering the computation to increase locality, as in blocking <ref> [Lam91] </ref>, address transformations for conict-free access to interleaved memory [Har89, Rau91, Val91], software prefetching data to the cache [Cal91, Kla91, Soh91], and hardware prefetching vector data to cache [Bae91, Fu91, Jou90, Skl92]. For a more detailed discussion of how these schemes relate to dynamic access ordering, see [McK93b].
Reference: [Law79] <editor> Lawson, et. al., </editor> <title> Basic Linear Algebra Subprograms for Fortran Usage, </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 5:3, </volume> <year> 1979. </year>
Reference-contexts: Hardware Support for Dynamic Access Ordering: Performance of Some Design Options 8 5. Benchmark Suite The benchmark kernels used are described in Figure 2. Daxpy, copy, scale, and swap are from the BLAS (Basic Linear Algebra Subroutines) <ref> [Law79, Don79] </ref>. These vector and matrix computations occur frequently in scientific computations, thus they have been collected into libraries of highly optimized routines for various host architectures. Hydro and tridiag are the first and fifth Livermore Loops [McM86], a set of kernels culled from important scientific computations.
Reference: [Lee90] <author> Lee, K., </author> <title> On the Floating Point Performance of the i860 Microprocessor, </title> <institution> NAS Systems Division, NASA Ames Research Center, </institution> <month> July </month> <year> 1990. </year>
Reference: [Low93] <author> Lowney, et. al., </author> <title> The Multiow Trace Scheduling Compiler, </title> <journal> Journal of Supercomputing, </journal> <volume> 7:1,2, </volume> <month> May </month> <year> 1993. </year>
Reference: [Mac93] <author> Maccabe, </author> <title> A.B., Computer Systems: Architecture, Organization, and Programming, </title> <editor> Richard D. Irwin, </editor> <publisher> Inc., </publisher> <year> 1993. </year>
Reference-contexts: Many computer architecture textbooks ([Bar92, Hay88, Hwa84, and Man82] among them) specifically cultivate this view. Others skirt the issue entirely <ref> [Mac93, Tom90] </ref>. Somewhat ironically, this assumption no longer applies to modern memory devices: most components manufactured in the last ten to fifteen years provide special capabilities that make it possible to perform some access sequences faster than others.
Reference: [Man82] <author> Mano, </author> <title> M.M., Computer System Architecture, 2nd ed., Prentice-Hall, Inc., Hardware Support for Dynamic Access Ordering: Performance of Some Design Options 110 1982 </title>
Reference: [McK93a] <author> McKee, S.A., Klenke, R.H., Schwab, A.J., Wulf, Wm.A., Moyer, S.A., Hitchcock, C., Aylor, J.H., </author> <title> Experimental Implementation of Dynamic Access Ordering, </title> <institution> University of Virginia, TR CS-93-42, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: One way to do this is via access ordering, which we define as any technique for changing the order of memory requests to increase bandwidth. Here we are especially concerned with ordering a set of vector-like stream accesses. For a more thorough discussion of access ordering, see <ref> [Moy92, Moy93, McK93a, McK93b] </ref>. The performance benefits of doing such static access ordering can be quite dramatic [Moy92, Moy93], but without the kinds of address alignment information that are usually only available at run time, the compiler cant generate the optimal access sequence. <p> The Stream Memory Controller The design space of access ordering systems and the rationale for the approach presented here is discussed in <ref> [McK93a, McK93b] </ref>. The approach we suggest is generally applicable to any uniprocessor computing system, but will be described based on the simplified architecture of Figure 1. Memory is interfaced to the processor through a controller labeled MSU for Memory Scheduling Unit.
Reference: [McK93b] <author> McKee, S.A., Moyer, S.A., Wulf, Wm.A., Hitchcock, C., </author> <title> Increasing Memory Bandwidth for Vector Computations, </title> <institution> University of Virginia, TR CS-93-34, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: One way to do this is via access ordering, which we define as any technique for changing the order of memory requests to increase bandwidth. Here we are especially concerned with ordering a set of vector-like stream accesses. For a more thorough discussion of access ordering, see <ref> [Moy92, Moy93, McK93a, McK93b] </ref>. The performance benefits of doing such static access ordering can be quite dramatic [Moy92, Moy93], but without the kinds of address alignment information that are usually only available at run time, the compiler cant generate the optimal access sequence. <p> For a more detailed discussion of how these schemes relate to dynamic access ordering, see <ref> [McK93b] </ref>. The main difference between these techniques and the complementary one we propose here is that we reorder stream accesses to exploit the architectural and component features that make memory systems sensitive to the sequence of requests. 3. <p> The Stream Memory Controller The design space of access ordering systems and the rationale for the approach presented here is discussed in <ref> [McK93a, McK93b] </ref>. The approach we suggest is generally applicable to any uniprocessor computing system, but will be described based on the simplified architecture of Figure 1. Memory is interfaced to the processor through a controller labeled MSU for Memory Scheduling Unit.
Reference: [McM86] <author> McMahon, F.H., </author> <title> The Livermore Fortran Kernels: A Computer Test of the Numerical Performance Range, </title> <institution> Lawrence Livermore National Laboratory, UCRL-53745, </institution> <month> December </month> <year> 1986. </year>
Reference-contexts: These vector and matrix computations occur frequently in scientific computations, thus they have been collected into libraries of highly optimized routines for various host architectures. Hydro and tridiag are the first and fifth Livermore Loops <ref> [McM86] </ref>, a set of kernels culled from important scientific computations. The former is a fragment of a hydrodynamics computation, and the latter is a tridiagonal elimination computation.
Reference: [Mea92] <author> Meadows, L., Nakamoto, S., and Schuster, V., </author> <title> A Vectorizing, Software Pipelining Compiler for LIW and Superscalar Architectures, </title> <booktitle> Proceedings of RISC92, </booktitle> <month> February </month> <year> 1992. </year>
Reference: [Moy91] <author> Moyer, S.A., </author> <title> Performance of the iPSC/860 Node Architecture, </title> <institution> University of Virginia, IPC-TR-91-007, </institution> <year> 1991. </year>
Reference: [Moy92] <author> Moyer, S., </author> <title> Access-Ordering Algorithms for a Single Memory Module, </title> <institution> University of Virginia, IPC-TR-92-002, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: One way to do this is via access ordering, which we define as any technique for changing the order of memory requests to increase bandwidth. Here we are especially concerned with ordering a set of vector-like stream accesses. For a more thorough discussion of access ordering, see <ref> [Moy92, Moy93, McK93a, McK93b] </ref>. The performance benefits of doing such static access ordering can be quite dramatic [Moy92, Moy93], but without the kinds of address alignment information that are usually only available at run time, the compiler cant generate the optimal access sequence. <p> Here we are especially concerned with ordering a set of vector-like stream accesses. For a more thorough discussion of access ordering, see [Moy92, Moy93, McK93a, McK93b]. The performance benefits of doing such static access ordering can be quite dramatic <ref> [Moy92, Moy93] </ref>, but without the kinds of address alignment information that are usually only available at run time, the compiler cant generate the optimal access sequence. The extent to which a compiler can perform this optimization is further constrained by such things as the size of the processor register file.
Reference: [Moy93] <author> Moyer, S., </author> <title> Access Ordering and Effective Memory Bandwidth, </title> <type> Ph.D. Dissertation, </type> <institution> Department of Computer Science, University of Virginia, </institution> <type> Technical Report CS-93-18, </type> <month> April </month> <year> 1993. </year>
Reference-contexts: One way to do this is via access ordering, which we define as any technique for changing the order of memory requests to increase bandwidth. Here we are especially concerned with ordering a set of vector-like stream accesses. For a more thorough discussion of access ordering, see <ref> [Moy92, Moy93, McK93a, McK93b] </ref>. The performance benefits of doing such static access ordering can be quite dramatic [Moy92, Moy93], but without the kinds of address alignment information that are usually only available at run time, the compiler cant generate the optimal access sequence. <p> Here we are especially concerned with ordering a set of vector-like stream accesses. For a more thorough discussion of access ordering, see [Moy92, Moy93, McK93a, McK93b]. The performance benefits of doing such static access ordering can be quite dramatic <ref> [Moy92, Moy93] </ref>, but without the kinds of address alignment information that are usually only available at run time, the compiler cant generate the optimal access sequence. The extent to which a compiler can perform this optimization is further constrained by such things as the size of the processor register file.
Reference: [Qui91] <author> Quinnell, R., </author> <title> High-speed DRAMs, </title> <type> EDN, </type> <month> May 23, </month> <year> 1991. </year>
Reference-contexts: Somewhat ironically, this assumption no longer applies to modern memory devices: most components manufactured in the last ten to fifteen years provide special capabilities that make it possible to perform some access sequences faster than others. For instance, nearly all current DRAMs implement a form of page-mode operation <ref> [Qui91] </ref>. These devices behave as if implemented with a single on-chip cache line, or page (this should not be confused with a virtual memory page). A memory access falling outside the address range of the current DRAM page forces a new page to be accessed.
Reference: [Ram92] <institution> Architectural Overview, Rambus Inc., Mountain View, </institution> <address> CA, </address> <year> 1992. </year>
Reference-contexts: Other common devices offer similar features, such as nibble-mode, static column mode, or a small amount of SRAM cache on chip. This sensitivity to the order of requests is exacerbated in several emerging technologies: for instance, Rambus <ref> [Ram92] </ref>, Ramlink, and the new DRAM designs with high-speed sequential interfaces [IEEE92] provide high bandwidth for large transfers, but offer little performance benefit for single-word accesses.
Reference: [Rau91] <author> Rau, B. R., </author> <title> Pseudo-Randomly Interleaved Memory, </title> <booktitle> 18th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: These include altering the placement of data to exploit concurrency [Gup88], reordering the computation to increase locality, as in blocking [Lam91], address transformations for conict-free access to interleaved memory <ref> [Har89, Rau91, Val91] </ref>, software prefetching data to the cache [Cal91, Kla91, Soh91], and hardware prefetching vector data to cache [Bae91, Fu91, Jou90, Skl92]. For a more detailed discussion of how these schemes relate to dynamic access ordering, see [McK93b].
Reference: [Skl92] <author> Sklenar, Ivan, </author> <title> Prefetch Unit for Vector Operation on Scalar Computers, Computer Architecture News, v 20, n 4, </title> <month> September </month> <year> 1992. </year>
Reference-contexts: These include altering the placement of data to exploit concurrency [Gup88], reordering the computation to increase locality, as in blocking [Lam91], address transformations for conict-free access to interleaved memory [Har89, Rau91, Val91], software prefetching data to the cache [Cal91, Kla91, Soh91], and hardware prefetching vector data to cache <ref> [Bae91, Fu91, Jou90, Skl92] </ref>. For a more detailed discussion of how these schemes relate to dynamic access ordering, see [McK93b].
Reference: [Smi87] <author> Smith, J. E., et al, </author> <title> The ZS-1 Central Processor, </title> <booktitle> The Second International Conference on Architectural Support for Programming Languages and Systems, </booktitle> <month> Oct. </month> <year> 1987 </year>
Reference-contexts: In fact, the FIFO organization is almost identical to the stream units of the WM architecture [Wul92], or may be thought of as a special case of a decoupled access-execute architecture <ref> [Goo85, Smi87] </ref>. Another advantage is that this combined hardware/software scheme doesnt require heroic compiler technology the compiler need only detect the presence of streams, and Davidsons streaming algorithm [Dav90] can be used to do this.
Reference: [Soh91] <author> Sohi, G. and Franklin, M., </author> <title> High Bandwidth Memory Systems for Superscalar Processors, </title> <booktitle> Fourth International Conference on Architectural Support for Programming Languages and Systems, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: These include altering the placement of data to exploit concurrency [Gup88], reordering the computation to increase locality, as in blocking [Lam91], address transformations for conict-free access to interleaved memory [Har89, Rau91, Val91], software prefetching data to the cache <ref> [Cal91, Kla91, Soh91] </ref>, and hardware prefetching vector data to cache [Bae91, Fu91, Jou90, Skl92]. For a more detailed discussion of how these schemes relate to dynamic access ordering, see [McK93b].
Reference: [Tom90] <editor> Tomek, I., </editor> <booktitle> The Foundations of Computer Architecture and Organization, </booktitle> <publisher> Computer Science Press, </publisher> <year> 1990. </year> <title> Hardware Support for Dynamic Access Ordering: Performance of Some Design Options 111 </title>
Reference-contexts: Many computer architecture textbooks ([Bar92, Hay88, Hwa84, and Man82] among them) specifically cultivate this view. Others skirt the issue entirely <ref> [Mac93, Tom90] </ref>. Somewhat ironically, this assumption no longer applies to modern memory devices: most components manufactured in the last ten to fifteen years provide special capabilities that make it possible to perform some access sequences faster than others.
Reference: [Val91] <author> Valero, M., et. al., </author> <title> Increasing the Number of Strides for Conict-Free Vector Access, </title> <booktitle> 19th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: These include altering the placement of data to exploit concurrency [Gup88], reordering the computation to increase locality, as in blocking [Lam91], address transformations for conict-free access to interleaved memory <ref> [Har89, Rau91, Val91] </ref>, software prefetching data to the cache [Cal91, Kla91, Soh91], and hardware prefetching vector data to cache [Bae91, Fu91, Jou90, Skl92]. For a more detailed discussion of how these schemes relate to dynamic access ordering, see [McK93b].
Reference: [Wal85] <author> Wallach, S., </author> <title> The CONVEX C-1 64-bit Supercomputer, </title> <booktitle> Compcon Spring 85, </booktitle> <month> February </month> <year> 1985. </year>
Reference-contexts: Note that we assume the processor can perform non-caching loads and stores so that non-unit stride streams can be accessed without concomitantly accessing extraneous data and wasting bandwidth. While not a common architectural feature, some commercial processors such as the Convex C-1 <ref> [Wal85] </ref> and Intel i860 [Int91] include such cache CPU mem mem state FIFO FIFO scalar accesses FIFO CACHE state state SBU Hardware Support for Dynamic Access Ordering: Performance of Some Design Options 7 bypassing.
Reference: [Wol89] <author> Wolfe, M., </author> <title> Optimizing Supercompilers for Supercomputers, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: Furthermore, vectors leave large footprints in the cache. For computations in which vectors are reused, iteration space tiling <ref> [Car89, Wol89] </ref> can partition the problems into cache-size blocks, but this can create cache conicts for some block sizes and vector strides [Lam91], and the technique is difficult to automate.
Reference: [Wul92] <author> Wulf, W. A., </author> <title> Evaluation of the WM Architecture, </title> <booktitle> 19th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: Hardware Support for Dynamic Access Ordering: Performance of Some Design Options 6 This organization is both simple and practical from an implementation standpoint: similar designs have been built. In fact, the FIFO organization is almost identical to the stream units of the WM architecture <ref> [Wul92] </ref>, or may be thought of as a special case of a decoupled access-execute architecture [Goo85, Smi87]. Another advantage is that this combined hardware/software scheme doesnt require heroic compiler technology the compiler need only detect the presence of streams, and Davidsons streaming algorithm [Dav90] can be used to do this.
References-found: 47

