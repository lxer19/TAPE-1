URL: ftp://ftp.cs.toronto.edu/pub/parallel/Krieger_Stumm_DAGS93.ps.Z
Refering-URL: http://www.eecg.toronto.edu/~okrieg/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: email: okrieg@eecg.toronto.edu  
Phone: phone: (416) 978-5036,  
Title: HFS: A Flexible File System for large-scale Multiprocessors  
Author: Orran Krieger and Michael Stumm 
Address: Toronto, Canada, M5S 1A4  
Affiliation: Department of Electrical and Computer Engineering University of Toronto  
Note: Proceedings of the 1993 DAGS/PC Symposium  
Abstract: The Hurricane File System (HFS) is a new file system being developed for large-scale shared memory multiprocessors with distributed disks. The main goal of this file system is scalability; that is, the file system is designed to handle demands that are expected to grow linearly with the number of processors in the system. To achieve this goal, HFS is designed using a new structuring technique called Hierarchical Clustering. HFS is also designed to be flexible in supporting a variety of policies for managing file data and for managing file system state. This flexibility is necessary to support in a scalable fashion the diverse workloads we expect for a multiprocessor file system. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Vadim Abrossimov, Marc Rozier, and Marc Shapiro. </author> <title> "Generic Virtual Memory Management for Operating System Kernels". </title> <booktitle> In Proc. 12th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 123-136, </pages> <address> Litchfield Park, Arizona, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: The principle of bounded overhead also applies to the space costs of the internal data structures of the system. While the data structures must grow at a rate proportional to the physical resources of the hardware <ref> [28, 1] </ref>, the principle of bounded space cost restricts growth to be no more than linear.
Reference: [2] <author> David Barach, Robert Wells, and Thomas Uban. </author> <title> Design of parallel virtual memory management on the TC2000. </title> <type> Technical Report 7296, </type> <institution> BBN Advanced Computers Inc., </institution> <address> 10 Moulton Street, Cam-bridge, Massachusetts, 02138, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Existing operating systems have typically been scaled to accommodate a large number of processors in an ad hoc manner, by repeatedly identifying and then removing the most contended bottlenecks <ref> [2, 18] </ref>. This is done either by splitting existing locks, or by replacing existing data structures with more elaborate, but concurrent ones.
Reference: [3] <author> Amnon Barak and Yoram Kornatzky. </author> <title> Design principles of operating systems for large scale multicom-puters. </title> <institution> Computer Science RC 13220 (#59114), IBM Research Division, T.J. Watson Research Center, </institution> <address> Yorktown Heights, NY 10598, </address> <month> October </month> <year> 1987. </year>
Reference-contexts: This also means that the file system should enact policies that balance the I/O demand across the system disks. Bounded overhead: The overhead for each independent system service request must be bounded by a constant <ref> [3] </ref>. If the overhead of each service call increases with the number of processors, then the system will ultimately saturate, so the demand on any single resource cannot increase with the number of processors.
Reference: [4] <author> D. Bitton and J. Gray. </author> <title> Disk shadowing. </title> <booktitle> In 14th International Conference on Very Large Data Bases, </booktitle> <pages> pages 331-338, </pages> <year> 1988. </year>
Reference-contexts: Finally, if I/O requests to a file are primarily reads, then replicating the file can be used both to obtain better performance <ref> [4, 20, 24] </ref> and to manage locality (since the more local copy of a disk block can be used to satisfy a read request).
Reference: [5] <author> Henry Burkhardt III, Steven Frank, Bruce Knobe, and James Rothnie. </author> <title> Overview of the KSR1 computer system. </title> <type> Technical Report KSR-TR-9202001, </type> <institution> Kendell Square Research, </institution> <address> Boston, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: A secondary goal is to be flexible in supporting a variety of policies for managing file data and for managing the internal file system state. Many current large scale multiprocessors achieve their scalability by using segmented architectures <ref> [5, 19, 29] </ref>. As new segments are added, there is an increase in: 1) the number of processors, 2) the network bandwidth, and 3) the number of memory banks and hence the memory bandwidth.
Reference: [6] <author> Thomas W. Crockett. </author> <title> File concepts for parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <pages> pages 574-579, </pages> <year> 1989. </year>
Reference-contexts: Several systems stripe file blocks across all disks in the system [8, 21, 22]. Crock-ett suggests six different distributions according to how processes of parallel applications generate I/O requests <ref> [6] </ref>.
Reference: [7] <author> Randall W. Dean and Francois Armand. </author> <title> Data Movement in Kernelized Systems. </title> <booktitle> In USENIX Workshop on Micro-kernels and Other Kernel Architectures, </booktitle> <pages> pages 243-262, </pages> <address> Seattle, Wa., </address> <month> April </month> <year> 1992. </year>
Reference-contexts: Also, having the code in the application library reduces the demand on possibly contended system servers. The fast LSI facility allows us to maintain state in the OFS that we would otherwise have to cache in the application level (as is done by Mach 3.0 <ref> [7] </ref>). This greatly simplifies our code for handling application errors and is probably also more robust. The name server allows locally accessed portions of the name space to be cached on each cluster.
Reference: [8] <author> P. C. Dibble and M. L. Scott. </author> <title> The Parallel Interleaved File System: A solution to the multiprocessor I/O bottleneck. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <note> 1992. To appear. </note>
Reference-contexts: If a system is to scale in its I/O capabilities, then the number of disks should also increase with the size of the system and these disks must also be distributed across the different segments <ref> [8, 13, 22] </ref>. As with memory, distributing the disks across the system has the advantage that some disks can be made more local to a processor, reducing the cost of accessing those disks for some processors. <p> Several systems stripe file blocks across all disks in the system <ref> [8, 21, 22] </ref>. Crock-ett suggests six different distributions according to how processes of parallel applications generate I/O requests [6].
Reference: [9] <author> High Performance Fortran Forum. </author> <title> DRAFT High Performance Fortran Language Specification. </title> <type> Technical report, </type> <institution> Rice University, </institution> <year> 1992. </year>
Reference-contexts: Hence, the file system for such a machine must be flexible so that it can be extended to support new I/O interfaces and so that it can adapt to applications with different types of I/O requirements. Even in the most recent draft language specification for High Performance Fortran <ref> [9] </ref>, no consensus was achieved on a set of parallel I/O extensions to the language. In this paper the architecture of the Hurricane File System is described. Section 2 describes our view of scalability and a new structuring technique for operating systems that aids in achieving scalability.
Reference: [10] <author> Andrew S. Grimshaw and Edmond C. Loyot, Jr. </author> <title> ELFS: object-oriented extensible file systems. </title> <booktitle> In Proceedings of the First International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 177-179, </pages> <year> 1991. </year>
Reference-contexts: For prefetching, Kotz and Ellis have proposed algorithms that track file accesses in order to determine candidate blocks for prefetching [15]. The ELFS file system prefetches by taking advantage of application specific knowledge of both the logical file structure (e.g., a two dimensional matrix) and the application access pattern <ref> [10, 11] </ref>. <p> A similar approach is used by Staelin et al for reorganizing data on disk [25], and by Kotz to prefetch data from disk [15]. Finally, an application can derive a new storage object from a file system storage object at run time. The ELFS file system <ref> [10, 11] </ref> uses such an approach both to invoke application specific policies and to close the semantic gap between the application programer and the file system by deriving a new storage object that matches the application view of the data in the file. 5 On disk data organization The main goal
Reference: [11] <author> Andrew S. Grimshaw and Edmond C. Loyot, Jr. </author> <title> ELFS: object-oriented extensible file systems. </title> <type> Technical Report TR-91-14, </type> <institution> Univ. of Virginia Computer Science Department, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: For prefetching, Kotz and Ellis have proposed algorithms that track file accesses in order to determine candidate blocks for prefetching [15]. The ELFS file system prefetches by taking advantage of application specific knowledge of both the logical file structure (e.g., a two dimensional matrix) and the application access pattern <ref> [10, 11] </ref>. <p> A similar approach is used by Staelin et al for reorganizing data on disk [25], and by Kotz to prefetch data from disk [15]. Finally, an application can derive a new storage object from a file system storage object at run time. The ELFS file system <ref> [10, 11] </ref> uses such an approach both to invoke application specific policies and to close the semantic gap between the application programer and the file system by deriving a new storage object that matches the application view of the data in the file. 5 On disk data organization The main goal
Reference: [12] <author> David Wayne Jensen. </author> <title> Disk I/O in High-Performance Computing Systems. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1993. </year>
Reference-contexts: Even if the time to transfer a disk block across the interconnection backplane is insignificant compared to the cost of getting the block from disk, the interconnection backplane will become contended if a large number of disks are concurrently transferring data over the backplane <ref> [12] </ref>. Hence, it is important that most of a processor's I/O requests be directed to nearby devices. This rules out simple disk striping as a means for distributing file data across the disks, where blocks of a file are uniformly striped across all disks in the system. <p> Jensen analyzed a series of distribution functions, and found that the performance of these functions depends on the total I/O load, the access pattern of the applications, the concurrency in the applications, and the number of files being accessed <ref> [12] </ref>. 4 Also, several file block distribution policies attempt to maximize locality. <p> This allows changes to the storage object on each super cluster to be made local to that super cluster. As a final example, Jensen has proposed a number of different ways of distributing data across disks <ref> [12] </ref>, each of which has advantages depending on how the data is being accessed. If read operations are dominant, then replicated storage objects could be constructed from multiple classes of distributed storage objects, each optimized for a particular type of read access.
Reference: [13] <author> David Kotz. </author> <title> Prefetching and Caching Techniques in File Systems for MIMD Multiprocessors. </title> <type> PhD thesis, </type> <institution> Duke University, </institution> <month> April </month> <year> 1991. </year> <note> Available as technical report CS-1991-016. </note>
Reference-contexts: If a system is to scale in its I/O capabilities, then the number of disks should also increase with the size of the system and these disks must also be distributed across the different segments <ref> [8, 13, 22] </ref>. As with memory, distributing the disks across the system has the advantage that some disks can be made more local to a processor, reducing the cost of accessing those disks for some processors.
Reference: [14] <author> David Kotz and Carla Schlatter Ellis. </author> <title> Caching and writeback policies in parallel file systems. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 17(1-2):140-145, January and February 1993. </note>
Reference-contexts: For caching, different policies can be used to determine which blocks should be replaced in the file cache (e.g., LRU, MRU) and to determine when dirty blocks should be written to disk <ref> [14] </ref>. For prefetching, Kotz and Ellis have proposed algorithms that track file accesses in order to determine candidate blocks for prefetching [15].
Reference: [15] <author> David Kotz and Carla Schlatter Ellis. </author> <title> Practical prefetching techniques for multiprocessor file systems. </title> <journal> Journal of Distributed and Parallel Databases, </journal> <volume> 1(1) </volume> <pages> 33-51, </pages> <month> January </month> <year> 1993. </year> <pages> Page 8 </pages>
Reference-contexts: For prefetching, Kotz and Ellis have proposed algorithms that track file accesses in order to determine candidate blocks for prefetching <ref> [15] </ref>. The ELFS file system prefetches by taking advantage of application specific knowledge of both the logical file structure (e.g., a two dimensional matrix) and the application access pattern [10, 11]. <p> A storage object can implement multiple policies and keep statistics (possibly stored in the meta data) to determine which policy is most effective for that data. A similar approach is used by Staelin et al for reorganizing data on disk [25], and by Kotz to prefetch data from disk <ref> [15] </ref>. Finally, an application can derive a new storage object from a file system storage object at run time.
Reference: [16] <author> O. Krieger, M. Stumm, and R. Unrau. </author> <title> Exploiting the Advantages of Mapped Files for Stream I/O. </title> <booktitle> In Winter USENIX, </booktitle> <pages> pages 27-42, </pages> <year> 1992. </year>
Reference-contexts: Dirty Harry (DH) is the only kernel level file system server. It collects dirty pages from the memory manager and makes requests to the BFS to write the pages to disk. The Alloc Stream Facility <ref> [16, 17] </ref> (ASF) is a user level library that maps files into the application's address space and translates read and write operations into accesses to the mapped regions. It supports a variety of I/O interfaces including Unix I/O (i.e., read/write), Stdio, and the Alloc Stream Interface [17].
Reference: [17] <author> Orran Krieger, Michael Stumm, and Ronald Un-rau. </author> <title> The Alloc Stream Facility: A Redesign of Application-level Stream I/O. </title> <type> Technical Report CSRI-275, </type> <institution> Computer Systems Research Institute, University of Toronto, Toronto, Canada, M5S 1A1, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: This has a number of advantages. For example, it allows all of main memory to be used as a cache of the file system. Also, mapped file I/O results in less overhead for accessing data in the file cache than for example Unix I/O <ref> [17] </ref>. However, mapped file I/O presents a challenge to the file system, since it must handle demands that are implicit due to accesses to memory rather than explicit due to calls to the file system. <p> Dirty Harry (DH) is the only kernel level file system server. It collects dirty pages from the memory manager and makes requests to the BFS to write the pages to disk. The Alloc Stream Facility <ref> [16, 17] </ref> (ASF) is a user level library that maps files into the application's address space and translates read and write operations into accesses to the mapped regions. It supports a variety of I/O interfaces including Unix I/O (i.e., read/write), Stdio, and the Alloc Stream Interface [17]. <p> It supports a variety of I/O interfaces including Unix I/O (i.e., read/write), Stdio, and the Alloc Stream Interface <ref> [17] </ref>. The file system servers (i.e., name server, OFS, and BFS) each maintain different state. The name server maintains logical directory state (e.g., directory size and access permissions) and directory contents (i.e., the mapping between file names and the identifiers used by the BFS). <p> We are currently in the process of extending this file system to handle multiple clusters and support a larger set of storage objects. The Alloc Stream Facility (i.e., the application level library) is quite mature and has been ported to a number of different systems <ref> [17] </ref>. The greatest handicap we are facing with this research is the lack of availability of (public domain) parallel applications that do real I/O.
Reference: [18] <author> Alan Langerman, Joseph Boykin, and Susan LoVerso. </author> <title> A Highly-Parallelized Mach-based Vnode Filesystem. </title> <booktitle> In Winter USENIX, </booktitle> <year> 1990. </year>
Reference-contexts: Existing operating systems have typically been scaled to accommodate a large number of processors in an ad hoc manner, by repeatedly identifying and then removing the most contended bottlenecks <ref> [2, 18] </ref>. This is done either by splitting existing locks, or by replacing existing data structures with more elaborate, but concurrent ones.
Reference: [19] <author> Daniel Lenoski, James Laudon, Kourosh Gharachor-loo, Wolf-Dietrich Weber, Anoop Gupta, John Hen-nessy, Mark Horowitz, and Monica Lam. </author> <title> "The Stan-ford Dash Multiprocessor". </title> <journal> Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: A secondary goal is to be flexible in supporting a variety of policies for managing file data and for managing the internal file system state. Many current large scale multiprocessors achieve their scalability by using segmented architectures <ref> [5, 19, 29] </ref>. As new segments are added, there is an increase in: 1) the number of processors, 2) the network bandwidth, and 3) the number of memory banks and hence the memory bandwidth.
Reference: [20] <author> Raymond Lo and Norman Matloff. </author> <title> A probabilistic limit on the virtual size of replicated file systems. </title> <type> Technical report, </type> <institution> Department of EE and CS, UC Davis, </institution> <year> 1989. </year>
Reference-contexts: Finally, if I/O requests to a file are primarily reads, then replicating the file can be used both to obtain better performance <ref> [4, 20, 24] </ref> and to manage locality (since the more local copy of a disk block can be used to satisfy a read request).
Reference: [21] <author> David Patterson, Garth Gibson, and Randy Katz. </author> <title> A case for redundant arrays of inexpensive disks (RAID). </title> <booktitle> In ACM SIGMOD Conference, </booktitle> <pages> pages 109-116, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Several systems stripe file blocks across all disks in the system <ref> [8, 21, 22] </ref>. Crock-ett suggests six different distributions according to how processes of parallel applications generate I/O requests [6].
Reference: [22] <author> Paul Pierce. </author> <title> A concurrent file system for a highly parallel mass storage system. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 155-160, </pages> <year> 1989. </year>
Reference-contexts: If a system is to scale in its I/O capabilities, then the number of disks should also increase with the size of the system and these disks must also be distributed across the different segments <ref> [8, 13, 22] </ref>. As with memory, distributing the disks across the system has the advantage that some disks can be made more local to a processor, reducing the cost of accessing those disks for some processors. <p> On the other hand, if a file is distributed across disks on a number of clusters, then to avoid one BFS from becoming a bottleneck, it may be worthwhile to cache the block map at the clusters where the file is accessed (as is done by CFS <ref> [22] </ref>). * If state is replicated, then it must be kept consistent. Different protocols could be used for this purpose, such as, invalidate or update. Similarly, message passing or shared memory can be used for communication. <p> Several systems stripe file blocks across all disks in the system <ref> [8, 21, 22] </ref>. Crock-ett suggests six different distributions according to how processes of parallel applications generate I/O requests [6].
Reference: [23] <author> M. Rosenblum and J. Ousterhout. </author> <title> The Design and Implementation of a Log-Structured File System. </title> <booktitle> In Proceedings of the Symposium on Operating System Principles, </booktitle> <year> 1990. </year>
Reference-contexts: To address both the file system recovery and the performance issues we decided to base our per-disk file system on the idea of log-structured file systems <ref> [23] </ref>. A log-structured file system allows for fast crash recovery on a single disk. Moreover, as long as the files are read and written in the same way, a log-structured file system results in good performance. Our file system differs from other log structured file systems in two important respects.
Reference: [24] <author> John A. Solworth and Cyril U. Orji. </author> <title> Distorted mirrors. </title> <booktitle> In Proceedings of the First International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 10-17, </pages> <year> 1991. </year>
Reference-contexts: Finally, if I/O requests to a file are primarily reads, then replicating the file can be used both to obtain better performance <ref> [4, 20, 24] </ref> and to manage locality (since the more local copy of a disk block can be used to satisfy a read request).
Reference: [25] <author> C. Staelin and H. Garcia-Molina. </author> <title> Smart Filesystems. </title> <booktitle> In Winter USENIX, </booktitle> <year> 1991. </year>
Reference-contexts: A storage object can implement multiple policies and keep statistics (possibly stored in the meta data) to determine which policy is most effective for that data. A similar approach is used by Staelin et al for reorganizing data on disk <ref> [25] </ref>, and by Kotz to prefetch data from disk [15]. Finally, an application can derive a new storage object from a file system storage object at run time.
Reference: [26] <author> Michael Stumm, Ron Unrau, and Orran Krieger. </author> <title> "Designing a Scalable Operating System for Shared Memory Multiprocessors". </title> <booktitle> In USENIX Workshop on Micro-kernels and Other Kernel Architectures, </booktitle> <pages> pages 285-303, </pages> <address> Seattle, Wa., </address> <month> April </month> <year> 1992. </year>
Reference-contexts: cluster, etc.). 1 Therefore, the cost of non-independent requests depends on the degree of sharing (or contention) of the applications making those requests and not on the size of the multiprocessor. 3 The File System Architecture The Hurricane File System is being developed as part of the Hurricane operating system <ref> [26] </ref>.
Reference: [27] <author> Ron Unrau, Michael Stumm, and Orran Krieger. </author> <title> Hierarchical Clustering: A Structure for Scalable Multiprocessor Operating System Design. </title> <type> Technical report, </type> <institution> Computer Systems Research Institute, University of Toronto, </institution> <month> 93. </month>
Reference-contexts: Page 2 2.2 Hierarchical Clustering The Hurricane File System is part of a larger effort to investigate operating system design for large scale multiprocessors. Our operating system structure is called Hierarchical Clustering. The goal of Hierarchical Clustering is to support large-scale applications without penalizing the performance of small-scale applications <ref> [27] </ref>. The basic unit of structuring within Hierarchical Clustering is the cluster, which provides the full functionality of a tightly coupled small-scale symmetric multiprocessor operating system.
Reference: [28] <author> Ronald C. Unrau. </author> <title> Scalable Memory Management through Hierarchical Symmetric Multiprocessing. </title> <type> PhD thesis, </type> <institution> Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: We have developed a more structured way of designing scalable operating systems called Hierarchical Clustering. This structuring technique was developed from a set of guidelines for designing scalable demand driven systems <ref> [28] </ref> (of which operating systems are an example). 2.1 Scalability Guidelines For a demand-driven system to be scalable, it must satisfy each of the following guidelines: Preserving parallelism: A demand driven system must preserve the parallelism afforded by the appli cations. <p> The principle of bounded overhead also applies to the space costs of the internal data structures of the system. While the data structures must grow at a rate proportional to the physical resources of the hardware <ref> [28, 1] </ref>, the principle of bounded space cost restricts growth to be no more than linear.
Reference: [29] <author> Zvonko G. Vranesic, Michael Stumm, Ron White, and David Lewis. </author> <title> "The Hector Multiprocessor". </title> <journal> Computer, </journal> <volume> 24(1), </volume> <month> January </month> <year> 1991. </year> <pages> Page 9 </pages>
Reference-contexts: A secondary goal is to be flexible in supporting a variety of policies for managing file data and for managing the internal file system state. Many current large scale multiprocessors achieve their scalability by using segmented architectures <ref> [5, 19, 29] </ref>. As new segments are added, there is an increase in: 1) the number of processors, 2) the network bandwidth, and 3) the number of memory banks and hence the memory bandwidth.
References-found: 29

